[
  {
    "id": 39678532,
    "title": "Apple Enables Direct App Downloads from Websites in EU",
    "originLink": "https://www.macrumors.com/2024/03/12/apple-announces-app-downloads-from-websites/",
    "originBody": "Apple Announces Ability to Download iPhone Apps From Websites in EU Tuesday March 12, 2024 5:00 am PDT by Hartley Charlton Apple today announced three further changes for developers in the European Union, allowing them to distribute apps directly from webpages, choose how to design in-app promotions, and more. Apple last week enabled alternative app stores in the EU in iOS 17.4, allowing third-party app stores to offer a catalog of other developers' apps as well as the marketplace developer's own apps. As of today, Apple is allowing third-party app stores to offer apps solely from their own catalog. For example, a games studio could create an app store on iOS that exclusively offers their own games. When directing users to complete a transaction on their website, developers can also now choose how to design their in-app promotions, discounts, and deals. Apple's templates for designing these links out to websites are optional as of today. In addition, developers will soon be able to distribute apps directly from their websites, providing they meet Apple's specific criteria, such as being a member of the Apple Developer Program for two continuous years or more and having an app with more than one million first installs on iOS in the EU in the prior year, and commit to ongoing requirements, such as publishing transparent data collection policies. Apps distributed in this way must meet Apple's notarization requirements like all other iOS apps and can only be installed from a web domain registered in App Store Connect. Authorized developers will gain access to APIs to facilitate app distribution from the web, integration with system functionality, back up and restore, and more. Web distribution will become available following a software update later in the spring – allowing app downloads directly from a website for the first time on iOS. The latest changes are part of Apple's Digital Markets Act (DMA) compliance plan and only apply within the European Union. Tag: European Union [ 385 comments ]",
    "commentLink": "https://news.ycombinator.com/item?id=39678532",
    "commentBody": "Apple announces ability to download apps directly from websites in EU (macrumors.com)653 points by Hamuko 21 hours agohidepastfavorite1209 comments dang 16 hours agoRelated: https://developer.apple.com/news/ (via https://news.ycombinator.com/item?id=39678555, but we merged that thread hither) docmars 9 hours agoprevOh the horror, it must be so hard for Apple to cave to an open system for these people. What will they ever do without their unbelievable tax on app profits simply for existing on their nearly unlimited real estate, that is, the web?! I have no sympathy for their concerns. I can download apps on my MacBook machines all day from many different sources, and it harms no one. While I understand the associated risks, computers have been this way long enough that the free and fair use of software on my devices is far more valuable than the brittle safety a marketplace offers. Apple's greed knows no bounds, and while I'm no big fan of the EU, there are some regulations like these that ensure these big bears in the industry can't abuse their positions for profit, at such unreasonable expense to consumers (not always monetarily, but be it fair access, availability, and choice), and developers especially. If commercial real estate charged XX% cuts of all sales from a business, every business would crumble with enough time, and only the big hitters would succeed with great resentment towards their gracious corporate overlords. reply bimguy 4 hours agoparentYou are not a big fan of the EU after this? They seem to care more about privacy and rights of the people within the majority of the countries that make up the EU then any other outside country that I could name. Then again, I'm not American so I can easily see the influence your country has on most other countries, so to say the \"EU have enormous capacity for overreach at the expense of participating countries and their citizens\" is completely ignorant and oblivious. reply chargingmarmot 2 hours agorootparentWell, yeah, but isn't the EU also responsible for all the trash cookie-consent notifications I get from every website now? Overall, I'm happy they're actively involved. The hands-off attitude in the US is terrible. reply frankvdwaal 2 hours agorootparentNo, it's the builders of the consent notifications who are responsible for that. They are often skirting or even breaking EU law to make it a headache to refuse. The GDPR says, for example, that refusal should be just as easy as acceptance. Having to click to another screen to do that is... not that. In reality a cookie consent notification can just as well be a small widget somewhere with an accept and refuse button, but it's the builders of these frameworks that have a vested interest in getting you to press accept. I've applied for a job at one of these companies about a year ago, and I asked them about it. They said to me that according to their metrics, there's about 30% more acceptance if they only bury their Refuse button, so it's a legal risk they are willing to take. Needless to say, when they invited me for a second conversation, I politely refused. No, the shitty cookie screens with dark patterns is not the responsibility of the EU - although you could make the argument that the EU should have been stricter or more prescriptive. reply mft_ 1 hour agorootparentIt's not just the dark-pattern cookie popups that are a problem - it's having any mandatory cookie popups --even the fairly-designed ones-- on virtually every website that you ever open. That's what's crappy about the implementation. I once read a light-hearted analysis of the cumulative time wasted by humanity due to the original USB plugs/sockets being unidirectional. I suspect a similar analysis of these cookie popups would be shocking. Hah, first Google hit: https://www.linkedin.com/pulse/billions-hours-now-being-wast.... (Not sure I agree with the numbers used, but the order of magnitude probably isn't too far wrong) reply MaKey 1 hour agorootparentCookie banners are not mandatory. If you're just using technical cookies you don't need a banner at all. Websites with them want to track you, that's why they have them. They need to ask for your permission to do so, which I think is a good thing. So instead of being mad at the EU we should be mad at those websites trying to get as much data as possible from their users. reply olivierduval 1 hour agorootparentprevActually, websites could \"not track\" BY DEFAULT (so no popup) and have a nice widget in a corner asking for consent to track, explaining why they need it, without this widget being obstructive... The problem is definitly NOT THE REGULATION but the way that websites have become a data/cash machine... reply account42 52 minutes agorootparent> Actually, websites could \"not track\" Yes, why not stop there? reply gcbirzan 1 hour agorootparentprevBut they're not mandatory. There is nothing stopping websites from not doing it, the previous poster was wrong. The GDPR requires consent, how you obtain that consent is irrelevant. Websites could not store cookies by default and you'd have to manually go and opt in. Maybe we even can have a per browser setting. reply account42 54 minutes agorootparentSpecifically, GDPR requires consent before you do (some) things the user might not want. You could simply not try to do those things and then you won't need to obtain consent at all. It's absurd how used we have become to wantonly collecting user data that some people can't even imagine not doing that. reply berkes 12 minutes agorootparentprevAlso, and too often overlooked or silently ignored: You don't need cookie popups! Really. You don't. You only need to get consent to track users with software you don't run yourself. Or when you sell your data off to other companies. Both are, unfortunately, the norm. But there's absolutely no technical reason to have these in place. Non at all. Plenty of alternatives for tracking that doesn't need consent. Or just not sell your customers' data off. I would be infuriated if I found the bakery down the street is selling its security footage with my face on it, next to my sales and spending in that bakery. I'd expect them to at least warn me about this at the door. So I can then buy my bread elsewhere. That's what a consent banner is! reply Sander_Marechal 1 hour agorootparentprev> The GDPR says, for example, that refusal should be just as easy as acceptance. Not true, actually! GDPR is a framework, and every EU country implements a national law according to that framework (e.g. the Dutch implementation is called \"AVG\"). The specific requirement that refusal must be as easy as acceptance is not in the GDPR, but several countries added it to their national implementation of the GDPR. reply frankvdwaal 25 minutes agorootparentThis is a misconception that I've seen going around, and I still wonder where it came from. The Dutch implementation is called \"Uitvoeringswet Algemene Verordening Gegevensbescherming\", which, as the title states, is the law that implements the GDPR. \"AVG\" is just a translation for \"GDPR\", not the name of the law that implements it. The Uitvoeringswet describes how the GDPR functions within Dutch law, for example, it describes the role that the Dutch Data Protection Authority plays. You can read the Uitvoeringswet right here: https://wetten.overheid.nl/BWBR0040940/2021-07-01 The GDPR (in Dutch AVG, in French RGPD, in Spanish RGPD, etc.) actually DOES state that it should be just \"as easy to withdraw as to give consent\" in Article 7. The directive (2016/679) can be found here: https://eur-lex.europa.eu/eli/reg/2016/679. reply capr 1 hour agorootparentprevYa, can't square the two? Check this out: violence actually works, so should we beat our kids? reply pg_1234 3 hours agorootparentprevThe EU should make a public service announcement. Something along the lines of: \"We urge all EU citizens with Apple devices to have an alternate means of accessing critical internet services like banking, to protect themselves in the event we are forced to block all Apple services EU-wide for legal non-compliance.\" ... then watch AAPL stock drop below NVDA ... ... and Apple come crawling back, suitably obedient. reply capr 1 hour agorootparent\"forced\" to block... seems like the only ones who can use force is them reply lobochrome 8 hours agoparentprevNot to disagree with your main argument - but high end real estate often works that way. The developer is often cut into the topline of the stores. reply quartesixte 8 hours agorootparentNot just high end real estate. Your local mom & pop in a strip mall somewhere (at least in the US) has a high chance of paying % of gross receipts to the landlord + rent. This may vary by location and landlord but it absolutely is a thing. And a guarantee for any high-end, high-traffic location. reply nojvek 6 hours agorootparentDemand and supply. As long as there is a marketplace and people have a choice, the market will balance. I have friends in strip mall businesses and they have moved locations for better landlords. reply illiac786 6 hours agorootparentSometimes there's something else at play though, because there are situations which cannot be explained by offer and demand. For example, developers are paid less in average than for example some consultant jobs while in my view (and I'm a consultant) both the skills required for a developer are higher while additionally the available workforce (supply) is lower for developers as well. I mean, consultants, they grow on trees and I know what I'm talking about, I sometines interview new hires as part of my job. Developers, less so. I don't have an explanation for this, it's a strange effect. But it's just an example, I have observed multiple times this unexpected deviation from the law of supply and demand. My point is, this law is not a sure fire way to explain any price. reply ffgjgf1 3 hours agorootparentThere is a higher demand for a certain kind of consultants (let’s call them “good”) and fairly low supply. There are plenty of people who call themselves consultants but nobody needs them since this is a bit of a winner takes all market (.e.g. like being a broker or even a lawyer to some extent) reply kuschku 7 hours agorootparentprevLuckily that's a rare thing and the vast majority if commercial real estate has no such rules whatsoever, even on main street. reply endisneigh 9 hours agoparentprevI’m very curious how you have this position yet have a MacBook. Why not support a better company like framework? reply docmars 6 hours agorootparentTo answer directly, I actually really love the Framework conceptually. What they're doing is immensely important for notebooks, and I'd love to see Apple follow suit one day (but I doubt that'll ever happen). I just can't stand Linux. I've tried several distros and after using macOS since 2009 and Windows since 1995, I just can't be bothered by all the things Linux distros lack, if muscle memory for the other two aren't already the biggest obstacle for me. I am insanely efficient on macOS, and I almost never have to think about global hotkeys, global search & calculations, managing apps & settings, and seeing nearly zero interruptions while I work -- including popups, notifications, performance dips (if I'm being reasonable with my usage), OS UI bugs, etc. These all do occasionally happen, but never to the extent I see in popular Linux distros and even Windows. It's just a fact, after nearly 30 years of first-hand usage and comparison. I also use macOS because it is as extensible and open as I need it to be for downloading and installing packages, and customizing the OS to suit my needs -- which is something iPhones can't do without jailbreaking and such. I've never owned an iPhone, and seldom use my iPad. I'm an Android guy, and being able to sideload apps in rare, but important, moments is important to me. The openness of Android has been important as well, namely the fact that Firefox has always been allowed to use its own browser engine since the start, enabling the same freedoms I have using it on desktop platforms, as a primary example. reply Nevermark 8 hours agorootparentprevI see this question a lot. People complain about the products that are best for them. Nobody (with the power to decide what they use) complains about a product long, unless it is still their best choice. And suppliers of products being complained about are not companies \"not worth supporting\". They are making the product that is the best fit for the complaining customer! They are not perfect. They can do better. So customers speak up. reply fennecbutt 8 hours agorootparentYeah that seems to be a common theme with Apple peeps, being fine with \"Well if you don't like what Apple gives you, go somewhere else!\" But like...if someone really does like the product but knows that product could be even better, wouldn't they naturally speak up about it? reply docmars 7 hours agorootparentYour last point is exactly where I'm coming from here. People complaining about a product is usually a sign of its wide, and possibly avid, use. After trying UI design tools and web development on Windows and Linux machines and finding the experience very sub-par for my needs, I've found macOS, and by extension, Apple's hardware quality, choice of keyboard layout, ease of use, etc. to be superior for my needs. I have almost no complaints about the Mac platform with the ways I've been able to customize it, and its nag-free experience. As they say, it just works™. ;) It feels made for UI design & development, with minimal to no configuration, no late nights fixing file permissions or access issues, fixing Linux subsystems, fiddling with very limited terminals, and suffering from buggy piecemeal UI shell packages that prioritize fancy, laggy animations over functionality. On the contrary, I've found not only doing these tasks and multitasking to be very frustrating when getting serious work done on other machines, primarily frequent interruptions (Windows) and major inconsistencies with global keyboard shortcuts. For what it's worth, I'm also a staunch Android user, never owned an iPhone, occasionally use an iPad for reading and other content creation, absolutely love my Windows PC for gaming and surfing on my TV, and work exclusively on my MacBook. I'm very particular about using the most suitable machines for the tasks at hand, but well-rounded enough not to be completely captured by Apple. reply jasonm23 5 hours agorootparentIf it's been a few years since you gave Linux a try, I'd strongly recommend giving it another go. I had 2 macbooks fail simultaneously over the new year, and instead of laying down the $$$ for a new m3 mackbook, I put a linux station together, with the intent for it to be a windows dual boot. At this point a couple of months later, windows is no more than a KVM/QEMU virtual machine (and runs its DAW/synth apps, significantly faster and with greater stability than either of my dead m1 macbooks ever did.) Best tool for the job has changed. An equipment manuifacturer who's goal is for hw failures to trigger a new purchase and not a repair should be enough incentive to ditch them. We all know Apple has fallen way further than that. They're a litigious, anti-consumer company that hides behind some fake, faded, John Lennon esque / hipster image. Time to cut them loose, isn't it? reply 9dev 3 hours agorootparent> If it's been a few years since you gave Linux a try, I'd strongly recommend giving it another go. I've heard this since around 2004, and did try it every once in a while. And while I have the utmost respect for the Linux desktop developers... the experience was never comparable to me. I'm a sucker for well-thought out and coherent user interfaces, and the rigid principles Apple developers have to follow are no match for a loose group of open source devs. I will continue to follow their progress, but as it stands, using a Linux desktop on my main machine feels like swapping a Mercedes with a home-built Gokart. reply chii 8 hours agorootparentprevand they're not \"supporting\" a company by merely buying their product. They are deciding that said product is the best fit for price on their individual criteria. To \"support\" would require you to make sacrifices - aka, buy an inferior/worse-fit product from a company you want to support, instead of from the company that actually offers the best-fit for yourself. reply docmars 7 hours agorootparentWhen I bought my MacBook Pro M1 Pro (ugh, stupid names, c'mon Apple!), it was probably the most confident I felt about a technology purchase in years, at least since Apple finally ditched the ridiculous touch bar and gave us back the Escape key and function row. Aside from me throwing too much at it (should've sprung for 32GB), it's the single best notebook I've ever owned, and the most reliable. To say it was the best fit for me is an understatement! It's truly great! reply fennecbutt 8 hours agorootparentprevBecause Apple is a company that does actually make great hardware, just marred by their idiot suit and tie MBA best schools social ties 1% tax dodging executive team that wants to foster the cultish attitude of their consumers and... well they succeeded in a way. reply crossroadsguy 8 hours agorootparentprevDid you entertain the possibility that Framework might not be available at OP’s place. Because that’s very much likely to be the case, just like it’s not available to a lot of us :) reply DanHulton 9 hours agorootparentprev\"You hate capitalism so much, _and yet you live under capitalism!_ How very hypocritical of you...\" Don't get me wrong, Framework makes a lot of neat stuff, but you can't swap out a Macbook to _anything_ without consequences. It does not take a lot of imagination and empathy to see that for some people, those consequences aren't acceptable at all, or not simply not worth the utterly undetectable sting that a company such as Apple would feel by us not buying a single Macbook from them. If someone has to alter their entire work environment and process, while Apple doesn't even notice, is that truly worth the moral superiority they'll feel? For a whole lot of people, that answer is \"no\", and I can't blame them. And yet we live under capitalism... reply gxs 8 hours agorootparentWhat a false equivalency. No, you can’t avoid capitalism, you’re born in the country your born and you don’t have that choice. This is more like moving to a capitalist country and then complaining that you live in a capitalist country. reply rezonant 8 hours agorootparentThe tech companies have boiled their frogs slowly and deliberately. Apple didn't start out the way it is today after all. reply docmars 6 hours agorootparentGood point -- With great power comes great inevitable irresponsibility and abuses. The human condition never fails to pollute and corrupt anything as untouchable as Apple. Too big to fail, by definition! reply smoldesu 8 hours agorootparentprevPotentially because Macbooks represent a more sustainable model for software distribution and don't prevent people from downloading apps directly from websites. reply docmars 6 hours agorootparentPrecisely! That's the key difference between iOS and macOS devices, essentially. I've never owned an iPhone primarily because its environment is so constrained, and the possibility of losing access to important apps due to failure of approval or other frivolous issues Apple hysterically deems unfit for publishing, is a huge single point of failure not worth risking. In reality, it's safer to assume that most or all major apps don't have problems with this, so I'm being a little facetious here. Regardless, after nearly 20 years on Android, nobody could possibly pry my muscle memory and features I've come to expect from my cold dead hands. :D reply xvector 9 hours agorootparentprevBecause MacBooks are just better. (Have owned both.) I have owned, and continue to own, all sorts of laptops and phones. I could rant about Apple all day long online, but in the end their product is simply superior. reply philistine 9 hours agorootparentWithin the corporate monopolist called Apple, that is to say within the minds of all its collective employees, lies an old idea still warm and vibrant after decades of waning indifference. This idea is called Apple Computer, and it makes the best gosh-darn computers in the world: the Mac. It is such a powerful and self-evident idea that those computers are still above and beyond the best ones in the world, even with all those years of indifference. reply radley 9 hours agorootparentHeh, Apple Computer also made the iPhone. Everything since launch has been incremental change, not innovative. Except Airpods. They're pretty sweet. reply lenkite 2 hours agorootparent> Except Airpods. They're pretty sweet. Sweet devices that keep falling off your ears. You need AirPod ear hooks to keep them on. reply crossroadsguy 8 hours agorootparentprevUntil battery life starts deteriorating any that happens real soon and you can’t even know when it started and where it is currently unlike that of, say, an iPhone. Then it’s unusable — hurrah, buy a new pair. The Apple way! :) reply yurishimo 8 hours agorootparentThis is not unique to Apple. All headphones with tiny batteries in the drivers take the piss after a few years aging. That said, my MacBooks have the best laptop batteries of any computer I’ve owned. My wife went through multiple laptops in the time I kept one in college. Turns out some electronics just suck! reply evilduck 8 hours agorootparentprevI have a pair of Pixel Buds, how do I see their current battery capacity and cycle counts? reply xvector 9 hours agorootparentprevIt's obviously an opinion. I have plenty of computers running Windows and a variety of Linux distros at my home. Laptops, desktop, servers, and weird hybrids of the former. Same with mobile - I have tried Android, iOS, PureOS, GrapheneOS. Apple's UX is so far ahead for me. It's just better. But, you obviously disagree. No need to be disparaging. reply rezonant 7 hours agorootparentConsider stating opinions as opinions, and not as facts. reply rezonant 7 hours agorootparentprevAnd yet if your statement were true, it would not explain why others who also own \"both\" disagree with you. Absolutism does not serve you, there are few subjects in this world that lack nuance. reply xvector 6 hours agorootparentMy statement is very clearly an opinion. reply gxs 8 hours agorootparentprevBecause people are incredibly entitled and want to have their cake and eat it too. So you bought an iPhone knowing you can’t download apps and then go cry because you can’t download apps? Then your argument is that well, all my friends have iPhones or there are some other good features, or whatever else you make up? So you obviously find value in the product, it’s missing a feature, but you will consciously buy it anyway, it doesn’t make any sense. Does the standard simply change when a company is big enough? Imagine ordering a steak salad even though the restaurant doesn’t allow modifications to the ingredients, then throwing a temper tantrum when you get it because it has steak. It’s unbelievable. reply rezonant 7 hours agorootparentYou might find this hard to believe, but people buy products based on a number of factors. For smart phones, the number of factors is dizzyingly complex, and yes, the effects it has on smoothness of communication with the people in your life is one of those factors. Sometimes a specific feature is one of those factors. What \"doesn't make sense\" is reducing a complex decision down to a specific factor, and then trying to create the narrative that your specific chosen factor is the sole reason anyone chooses a specific product. It is completely fair for people to prefer iPhone and also argue for Apple changing their policies. reply post_break 8 hours agorootparentprevImagine there are only two restaurants in the world and they both only serve steak, yet when you want a salad people say go to the other restaurant. reply gxs 8 hours agorootparentExcept in this case your beloved android lets you do whatever you want, so why not go use them? And there are multiple manufacturers that aren’t associated with Google who make phones. If this were truly such a shortcoming, more companies, in addition to already existing ones, would create phones with side loading apps. Imagine me pitching my idea to YC, it’s like an iPhone, but with side loading apps! It’s brilliant! You’d be laughed out of the room. The issue is you want those good Apple features, you want that Apple ecosystem, the blue bubbles, etc, but you also want to have a feature that the phone doesn’t have and people are crying that Apple won’t give them that feature. I don’t even care, and I even if I did, decisions already been made so there’s nothing to argue. This is simply an amusing situation, the grandstanding is simply funny. reply smoldesu 7 hours agorootparentImagine using YC as a corollary for consumer demand (or hell, corporate righteousness). > I don’t even care > This is simply an amusing situation, the grandstanding is simply funny. Wait till the Commission delivers the punch-line. reply gxs 7 hours agorootparentI don’t think you made the point you think you made here. It’s possible to not care about something and still submit an opinion. Or maybe it’s the degree of caring that is confusing you, I care enough to comment and have a viewpoint, but I don’t care to the degree that I am upset or will lose any sleep over it. There you go, hope this helps you understand what I meant there so that you are no so hung up on it so as to feel the need to quote it. Please do save me the suspense and share the punchline now! It’s perfectly reasonable to use YC here as at the end of the day they’ve helped launch of ton of companies that are popular with consumers. Regarding the irony in using them as an example of corporate righteousness, well you did get me there and I agree with you. reply endisneigh 8 hours agorootparentprevThis is a great metaphor because if we accept it then the salad is the web, yet no one wants that. reply docmars 6 hours agorootparentI think there is a great desire for web-like application distribution to work well on smartphones, but with none of the drawbacks like poor rendering performance and lack of native features. Of course, native apps that wrap web-based apps is almost the reverse of that, and we still often get laggy, sub-par experiences as a result of broader platform support for lower maintenance costs. PWAs fill the opposite gap where you get native-like apps at the expense of low performance, distributed any way you like. What we really need is for high-performance native applications to be distributable via the open web, and that's exactly what the EU is enforcing here, in a way. What would be better is for WebAssembly to take off and offer native performance in apps that can be visited at URLs, just like we're used to. reply fennecbutt 8 hours agorootparentprevWhy are you defending a trillion dollar company lmao? Why do you care so much that Apple has been forced to give consumers more choice, you can still just use the app store yourself, nobody is forcing you to use apps from alternative stores. This is the standard \"one true religion\" reaction imo. reply capr 1 hour agorootparentmaybe because forcing people or companies (property of people) is wrong? reply rightbyte 10 minutes agorootparentThe phones are not property of Apple though. The users should not be forced to run Apple approved applications. reply smoldesu 8 hours agorootparentprevApple is the only one acting entitled here. Why doesn't the App Store deserve competitors? Why should we accept Apple's fees and failures when they deliberately limit competition? They're acting like an anticompetitive wuss if you ask me. If Apple is the righteous one here (imagine that), they can pack up their bags and tell the whole EU to shove it. They can individually invite all 27 markets to kiss their ass and watch as the relevancy of Apple products plummets in the first world. Problem solved, Apple saves the day. 中国梦! Or, they can take the king's ransom of iPhone revenue and surrender their asinine software double-standard. This doesn't end well for them either way, there's no sense it making it last longer. reply docmars 6 hours agorootparentAgreed, and well worded. Never in computing history has a walled garden like Apple's existed until the iPhone. Distributing and finding apps wasn't always simple, but then again, the need for a central browsing experience to find and download apps was never truly a thing before -- maybe outside of Steam for games. The key difference with Steam is that the same games have always been available on other distribution platforms, generally, so it doesn't suffer from the same limitations. Show of hands, how many people actually spend multiple minutes (or hours) just swiping through their respective App Store just to find something new and interesting? Aside from the store experiences, the web's powerful (gasp!) ability to find and download content including Computer Applications™ has always been its greatest strength. App stores are a net detriment seeking to protect the lowest common denominator: the uneducated computer user who hasn't bothered to learn everyday security practices to avoid downloading malicious apps or vetting software developers on their popularity and/or security themselves. This takes a little knowledge and practice, but this isn't much different from shopping for good produce in a grocery store. Avoid the rotten fruit, and use your friends/family to help you judge what's best! That's the beauty of freedom on our devices, as it enables the power users and enthusiasts to enjoy these devices at their fullest, without senseless obstacles offering unsolicited \"protection\". reply throwaway2037 9 hours agoparentprevOverall -- a great post. This part: > If commercial real estate charged XX% cuts of all sales from a business As I understand, for luxury fashion brands, this type of contract is sometimes used. reply noapologies 8 hours agorootparentTrue, though the situation isn't exactly equivalent. Using another real estate analogy - Imagine you bought a house from Fruit Builders company. The house came with a pool. Now unlike every other pool in existence, this is a very special pool that just really cares about your privacy and security a lot. It won't let you use any random pool toy (it has lasers), no it must be a well-behaved toy that is rigorously tested and officially notarized by Fruit company themselves (= non-employee contractors taking one look to make sure Fruit's cut is not being circumvented). So you go to the supermarket, purchase a marked-up toy, the toy company reports its earning to Fruit, and Fruit takes their cut. All for your safety of course. reply monocularvision 8 hours agorootparentExcept in the real world, toys generally can’t do things like steal your private data and send confidential data to third parties. If you’re going to come up with analogies, at least do something that is remotely applicable. reply noapologies 7 hours agorootparentWhy not? You are presumably less-clothed in a pool, it could take pictures. Or record private conversations. Or both. But sure, here's another - You can run any company's software on a MBP, downloaded from the internet, without paying a dime to Apple. Similar situation applies to Windows / Microsoft. The iOS model is advocating for rent-seeking in MacOs and Windows binaries. reply fennecbutt 7 hours agorootparentprevWhy can apps installed from anywhere steal your data? Surely Apple's OS/framework that they tried to say they spent so much money developing, sandboxes and protects all running code from said data vacuuming behaviour? Or did you really believe Apple when they said it would reduce security™ Because the only reason it would is if Apple let it happen on purpose so they can create a consumer backlash by saying \"I told you so\". reply zakki 8 hours agorootparentprevWhat is the luxury of an app/software? reply nwbort 9 hours agoparentprevLook up 'turnover rent'... reply xwolfi 7 hours agoparentprevWhy are you no big fan of the EU ? reply docmars 6 hours agorootparentIn general I lean libertarian, and governing bodies like the EU have enormous capacity for overreach at the expense of participating countries and their citizens. I won't pretend to know exactly how their processes work in detail, as I'm an American citizen, so the EU's concerns generally don't interest me much, but if the U.S. Federal gov't versus State governments can be used an analogy, I have similar feelings, in principle. reply anomaly_ 8 hours agoparentprevI look forward to you having to support your parents, grandparents, and random relatives once they start installing all sorts of random dodgy apps. The EU is completely lost. No wonder Europe is a terrible place to build a business or innovate. The continent is basically an open-air museum at this point. reply docmars 6 hours agorootparentI do, in fact. My parents both use Android phones, and I'm more than happy to support their needs when something goes wrong. 99% of the time, they love their phones and use them regularly without issues. reply boringg 7 hours agorootparentprevSupporting families downloading bad software is in itself a job. Im always torn if i should get them a password manager or just stick with the handwritten notebook. I feel like the book approach is safest reply Sander_Marechal 59 minutes agorootparentIt depends on how tech-savvy they are. My dad used a password manager. My mom and grandparents keep a handwritten notebook. Just bookmark https://www.random.org/passwords/?num=1&len=12&format=plain&... for them to generate a password. reply sircastor 6 hours agorootparentprevPassword manager. Please, do it. It is a substantially better solution than a notebook. - It's always up to date - Previous values are kept (but hidden) - It encourages stronger passwords - Its automation makes it harder for people to put passwords into the wrong/phishing sites. - You can share passwords across a group - Remote administration - Distributed possession: You have them on your phone out and about, you have them at your desk. Really, a password manager is a really great tool. I don't know your family but I strongly recommend using and supporting one. reply docmars 6 hours agorootparentUnfortunately for many senior folks, using them is just too complicated. I've badgered my parents into using theirs, which is already setup, and it confuses them every time, so they prefer to memorize or write their passwords on a sheet of paper they never lose in their apartment. It's just easier than fiddling with a buggy mess of auto-fill prompts that only work half the time, and when they do show up, they fail to fill in the password, so then you have to open the app, hunt down the entry, copy it to the clipboard, and go back to the app or site you were signing into. Multi-tasking on phones is already very difficult for my parents, and they're well aware of it being a feature. Eventually all of these frustrations add up, and the path of least resistance is writing passwords down, as much as it kills me. reply bogwog 20 hours agoprev> Alternative app marketplaces. Marketplaces can choose to offer a catalog of apps solely from the developer of the marketplace. How does that count as a \"marketplace\"? > Web Distribution ... will let authorized developers distribute their iOS apps to EU users directly from a website owned by the developer All of this just makes it crystal clear what Apple's goal is: to prevent competition. It's not about security like they've been lying about; it's all about maintaining their app store monopoly. reply willseth 12 hours agoparent> All of this just makes it crystal clear what Apple's goal is: to prevent competition. Web Distribution requires stricter app and developer review than Marketplace distribution. reply xzjis 11 hours agorootparentApple makes more money from marketplaces than apps downloaded from the web. reply AnthonyMouse 12 hours agorootparentprevIsn't that kind of the point? The goal was to get out of Apple's clutches when your customers have their devices, so Apple made the thing meant to be independent even more dependent than the original in order to deter adoption. reply willseth 11 hours agorootparentThe parent comment cited Web Distribution as evidence that Apple doesn't actually care about safety and security, when in fact Web Distribution is more secured than Marketplace distribution. > The goal was to get out of Apple's clutches when your customers have their devices Whose goal? Read the DMA. It is very explicit that it expects Apple to maintain security of devices and apps. reply AnthonyMouse 11 hours agorootparent> The parent comment cited Web Distribution as evidence that Apple doesn't actually care about safety and security, when in fact Web Distribution is more secured than Marketplace distribution. Which goes to the parent's point that their intent is to prevent competition. Otherwise why would the alternative need more onerous security measures, if not to act as a deterrent through friction? > Read the DMA. It is very explicit that it expects Apple to maintain security of devices and apps. It also says that the security measures have to be \"strictly necessary\" and \"there are no less-restrictive means to safeguard the integrity of the hardware or operating system\" and \"[t]he gatekeeper should be prevented from implementing such measures as a default setting or as pre-installation\" etc. Which implies to me that you not only have to be able to turn them off, they have to be off by default. reply willseth 10 hours agorootparentThe comment literally says \"It's not about security like they've been lying about\", when the opposite is actually true. They were implying that Web Distribution was a way to get around security of a Marketplace, which is not possible. Without a kill switch, gatekeepers would lose control over apps, making them \"strictly necessary.\" Most interpretations of the DMA agree. reply AnthonyMouse 10 hours agorootparent> The comment literally says \"It's not about security like they've been lying about\" The comment literally says: \"All of this just makes it crystal clear what Apple's goal is: to prevent competition. It's not about security like they've been lying about; it's all about maintaining their app store monopoly.\" There is no reason for the security measures to be more onerous for the competing thing if they were sufficient for Apple's thing, unless the purpose of the security measures is to prevent competition. > Without a kill switch, gatekeepers would lose control over apps, making them \"strictly necessary.\" Gatekeepers having control over apps isn't necessary for security. The device's owner having control over apps is. They can opt into a particular gatekeeper's control if they choose to. How is it \"strictly necessary\" for the gatekeeper to force them to use one provider of vetting services over another? Isn't the point of the act to enable competition? reply willseth 9 hours agorootparent> There is no reason for the security measures to be more onerous for the competing thing if they were sufficient for Apple's thing, unless the purpose of the security measures is to prevent competition. Web Distribution means Apple is handing over responsibilities previously handled by the Marketplace directly to the developer. Allowing developers to police themselves is obviously riskier. > The device's owner having control over apps is. This is simply not true. Device owners are hopeless at maintaining the security of their devices. > How is it \"strictly necessary\" for the gatekeeper to force them to use one provider of vetting services over another? There are 2 tiers of \"vetting services\": 1. Marketplaces determine the appropriate content or type of apps allowed in their listings, 2. Apple determines if an app, developer, or marketplace is an outright threat, e.g. if an app turns out to be a scam, or if a bug in an app exposes an exploit, it is \"strictly necessary\" for Apple to be able to yank the app immediately. reply AnthonyMouse 8 hours agorootparent> Web Distribution means Apple is handing over responsibilities previously handled by the Marketplace directly to the developer. Allowing developers to police themselves is obviously riskier. Doesn't that depend on who the developer is? Certainly it isn't the case that no one exists who the user might trust at least as much as Apple. > This is simply not true. Device owners are hopeless at maintaining the security of their devices. \"Device owners\" includes substantially all people. Many of them are not hopeless and are entitled to make their own decisions. Some of them are even more qualified to do it than the people Apple has reviewing apps. The hopeless people may be better off sticking to trusted stores, but they can do that without prohibiting others from doing otherwise. > There are 2 tiers of \"vetting services\": 1. Marketplaces determine the appropriate content or type of apps allowed in their listings, 2. Apple determines if an app, developer, or marketplace is an outright threat, e.g. if an app turns out to be a scam, or if a bug in an app exposes an exploit, it is \"strictly necessary\" for Apple to be able to yank the app immediately. That doesn't change the question. How is it \"strictly necessary\" for Apple to do that, rather than whoever the owner of the device chooses to do it? It would obviously be possible for a third party like Symantec, Malwarebytes or the makers of uBlock to do the same thing. reply willseth 8 hours agorootparent> Doesn't that depend on who the developer is? Sure, the amount risk probably varies, but you are talking about going from a Marketplace that implements some level of app review to no-review. It's more risk. > Many of them are not hopeless ... Exactly, and \"many\" is not enough. It's not possible to design a special switch only for those qualified \"many\" - and only them. Platform owners and the EU insist on protecting the unqualified everyone else too. > How is it \"strictly necessary\" for Apple to do that, rather than whoever the owner of the device chooses to do it? It's not in the sense that someone else could do it, but the DMA doesn't require it, so obviously no gatekeeper will. Also, it's a terrible idea because there's no market for it. Everyone already expects it to be free. reply AnthonyMouse 6 hours agorootparent> Sure, the amount risk probably varies, but you are talking about going from a Marketplace that implements some level of app review to no-review. It's more risk. Only if the developer isn't as trustworthy as Apple. In fact, it could be lower risk even if they are less trustworthy than Apple, when it's their own app, because someone who is less competent but not overtly malicious who posts their own app is much less likely to be supplying malware than a general-purpose store that tries to vet everything but accepts submissions from just anyone at all including overtly malicious actors, and could thereby miss something. And the user, in choosing which alternate stores or developers to trust, can decide that. > It's not possible to design a special switch only for those qualified \"many\" - and only them. Well of course it is. In the worst case scenario you could make the switch irreversible and then once enabled the device could never add another store. But that's really no different than requiring a device wipe to change it back, because a wiped device should be no different than a new device that never had the switch enabled to begin with. > It's not in the sense that someone else could do it, but the DMA doesn't require it, so obviously no gatekeeper will. Isn't whether it's \"strictly necessary\" the condition on which they can demand it? > Also, it's a terrible idea because there's no market for it. Everyone already expects it to be free. How is it free? They're charging $100/year and a percentage on top of that. reply toyg 10 hours agorootparentprevI love how a never-used-by-courts-before regulation would supposedly already have \"most interpretations\" with any sort of authoritative value. I can probably walk into a pub tonight and get 27 other \"interpretations\", they will have the same value of yours. Technically speaking, even the Commissioner's own interpretation might well be flawed - we won't know until a court spends some time on it. I would humbly suggest, though, that when the very same lawmaker who wrote the law is publicly pulling your ears in public on related matters, your interpretations are probably not the right ones. Apple pay enough real lawyers to defend them, they really don't need pro-bono amateurs. reply willseth 9 hours agorootparentIt's not my interpretation, self-proclaimed humble person. Educated people have been discussing this ad nauseam for months. I would not-humbly suggest you actually read up on topics before breathlessly dismissing them deep down an HN comment thread. reply MatthiasPortzel 20 hours agoparentprevBefore this, if you had an alternative marketplace, you had to accept submissions from other developers. You are still allowed to accept submissions from other developers, but are no longer required to. reply rsynnott 19 hours agorootparentI suppose the point is that, if we're being pedantic (and after all, that is what the internet is _for_), you cannot have a single vendor marketplace based on the commonly understood meaning of the word 'marketplace'. (But yeah, this is just slightly silly naming from Apple). reply yard2010 13 hours agorootparentAre you demonstrating Cunningham's law because the internet is for porn reply stouset 11 hours agoparentprev> it's all about maintaining their app store monopoly. Does this only makes sense if you assume payments are tied to the App Store? They aren’t. If you remove payments from your list of motivations, what do you presume Apple’s motivation is to encourage apps to list themselves on the App Store and not a third-party marketplace? reply noapologies 10 hours agorootparentIt is much harder to explain to consumers why Apple should get a percentage-based rent (sorry Core Technology Fee that enables Privacy and Security™) if they go to a non-Apple website, download a non-Apple app, to do non-Apple-related things. Like literally the only participants in that business transaction are the consumer and the company, Apple does not even enter the picture. It would be like car manufacturers charging you a percentage for going to the grocery store, because they provide a Private and Secure™ transportation platform. Consumers will soon catch up, and if the EU does not put pressure on Apple about this, they definitely will. reply mrcode007 7 hours agorootparentIt’s more like car manufacturer charging license fees to the dealership for their use of the original manuals and tools to provide services that rely on their diagnostic tools and manuals. reply noapologies 3 hours agorootparentBut a car is used for more things than going to the dealership, and the dealership does not sell me groceries. Perhaps I want to race, or carry ikea furniture, or jump start another car - it is a general-purpose transportation device. Similarly, I dream of going to Epic's website to download some Fortnite, maybe charge a thousand vbucks to mom's credit card if I'm feeling adventurous, and that has nothing to do with Apple or iOS. This is how every single general-purpose computing platform (including Apple's MacOs) and the open internet has worked for multiple decades. reply mrcode007 2 hours agorootparentwe don’t care how the car is used. It’s the dealership that pays the fee on service manuals and access to tools, not the customer. The dealership can choose to pass the cost to customer but it doesn’t have to. reply noapologies 1 hour agorootparentOh but we do care. Not every app developer is a dealership, a car is used in a much broader context. Some may be like Uber, turning the car into a taxi service, or like Turo, allowing it to be rented. Others may be independent mechanics that can work on the car perfectly fine without access to blessed tools. There is no cost passed on to the customer because the car manufacturer does not enforce a percentage cut of Uber's or Turo's revenue. That said, there is likely no perfect analogy in cars. We can instead turn to MacOs / Windows / Linux etc., general purpose computing platforms that do not suffer from a gatekeeper's stranglehold. reply simondotau 8 hours agorootparentprev> Apple does not even enter the picture. Not exactly true, there are fragments of Apple intellectual property distributed with compiled binaries. reply fennecbutt 7 hours agorootparentMost platforms would offer the core libraries and services for free as an incentive to attract developers to the platform/make development easier. This is how it used to be, until Apple got too large and instead of being beholden to developers it flipped the other way around, and now releasing an app for Apple's platform is a supposed privilege. Take the games industry, where developers and publishers are often given huge incentives by a platform (mostly consoles) to develop for that platform; because games developers are providing value for the platform owner by making the platform more attractive because it has more content options for the consumer. Why is it so hard for people to wrap their heads around that concept. reply simondotau 7 hours agorootparent> Most platforms would offer the core libraries and services for free as an incentive Right, as an incentive. That's exactly right. Makers of other platforms chose a particular funding model to suit their commercial strategic environment, not because they were obligated to. Why should Apple be obligated to follow other (or even their own) prior business models? > This is how it used to be It really isn't. In fact this expectation of free full-featured developer tools for mainstream platforms is relatively new. https://www.itprotoday.com/windows-78/microsoft-sets-pricing... > Take the games industry Sure. Remind me where I can download the free developer kit for the PlayStation 5? Remind me who I need to pay in order to distribute a PlayStation game? reply int_19h 2 hours agorootparentEven back when Visual Studio did cost you an arm and a leg, you didn't need it to build and distribute software for Windows. Free options were always available; you paid for the comfort. In fact, Windows itself came with everything that you needed to build just about any userspace app in the box since Windows XP SP1 (the first one that included .NET Framework). reply fennecbutt 7 hours agorootparentprevWell most certainly in many cases a flat fee even annual and not a percentage. Also if I bought visual studio, MS couldn't tell me what I could make my program do, or outright refuse to let people use my program. reply callmeal 8 hours agorootparentprev>Not exactly true, there are fragments of Apple intellectual property distributed with compiled binaries. Which the annual tax (aka developer fee) presumably covers. reply simondotau 7 hours agorootparentA Happy Meal doesn't include a Sundae because you believe McDonald's is morally obligated to include one. reply asveikau 10 hours agorootparentprevApple fans would always claim that this was a security measure to prevent malware. I have always found the claim dubious. If you believe in that as a security measure, you could still have a signing requirement and apple could revoke trust on known-bad binaries. Which is probably what they will do. reply sandoze 9 hours agorootparentMind giving some high level clarification on how Apple would revoke entitlements on applications they’re not allowed to manage? Honestly curious about the infrastructure involved, is it really simple from a technological stand point? If the developer needs to use Apple resources to track and manage said entitlements, and the consumer expects Apple to police bad actors, then are we asking Apple to do this for free on the bad actor’s behalf (oops, I didn’t mean to use your microphone, GPS, BLE in order to sell the info to an enemy state, law enforcement, angry ex!) or should the cost of said infrastructure be passed to the customer when purchasing hardware? OR does Apple wait until an application is exposed, generally through an echo chamber after the damage is done and is made aware of the issue? reply asveikau 9 hours agorootparentI thought they already do this with notarized binaries on macOS. Conceptually it's no different from certificate revocation. The platform can phone home periodically to discover binaries for which notarization has been revoked. reply sandoze 9 hours agorootparentYou may be correct? Then the assumption would be developers need to pay the $99 fee to be part of the Apple dev program (pretty sure that’s the only way to get notarized). Next step in Apple’s playbook might be upping that fee for third party stores? reply thefounder 11 hours agorootparentprevAds reply nektro 12 hours agoparentprevdebating about how they run the store is totally valid, but there being only one store absolutely does make iOS safer overall reply jraph 11 hours agorootparentThe vast majority of Android users use the Play Store (or the Amazon thing) exclusively. So Android is not different than iOS in this regard. The vanishingly few remaining users use F-Droid (sometimes exclusively), which is probably the safest app store on Earth, with GNU/Linux and *BSD distros' base repositories. Open source only, reproducible builds with public recipes written independently, trackers removed (because they usually rely on non-free libs). I honestly don't see how having only one store makes an OS safer. That store could be an unchecked mess. We could talk about policies around app inclusion and permission management though. reply jhugo 10 hours agorootparent> The vast majority of Android users use the Play Store (or the Amazon thing) exclusively Are you sure? Android phones are pretty big in China, which is by far the world's largest smartphone market, and I guess Play Store & \"the Amazon thing\" (I don't remember the name either) adoption there is close to 0%. Anecdotally I have noticed a lot of people using phone vendor app stores in India (the second largest market, though half the number of devices as China) and Indonesia (another huge market). Taken together I'm very skeptical that Play Store + Amazon have a majority of Android users. reply throwaway2037 9 hours agorootparent> I guess Play Store & \"the Amazon thing\" (I don't remember the name either) adoption there is close to 0%. Woah. Is this true? If they don't use Google Play Store, what do they use? reply jhugo 1 hour agorootparentGoogle and its services are mostly blocked in China, so using the Play Store would require the use of a VPN or a foreign SIM card. There are a variety of local app stores. I've found that people often just use whatever came on their phone (which is often the phone manufacturer's own app store). reply fennecbutt 7 hours agorootparentprevhttps://digitalcreative.cn/blog/top-chinese-app-stores reply ben_w 11 hours agorootparentprevIf the argument is \"the number of stores is not a useful metric\", I agree. If the argument is \"Apple in particular has a huge vested interest in making sure that their first party App Store doesn't distribute malware\", that's somewhat stronger. I don't know which argument nektro was trying to make, I could read it either way. Personally, I lean towards the point about vested interests, although it is only \"lean towards\" not \"fully embrace\": what they care about isn't strictly security, but their bottom line, and being a US company with US moral norms and US payment providers, this can also be observed in the form of their content rules — they seem to treat sex as a much more important thing to hide than violence[0]. This does not sit well with people like me who think violence is bad and sex is good. [0] A bit over a decade ago, the app submission process flagged the word \"knopf\" in German translations, telling me it was a rude word and I might get in trouble if I was using inappropriate language. It's the German word for button… or knob (but in the sense of button, it's never a dick), and so I can only assume someone got a naughty words list in English and translated it literally rather than asking for a local list of naughty words. reply greazy 12 hours agorootparentprevBut does it? I haven't seen any hard evidence, and lots of anecdotal tales of technology illiterate grandparents, fathers and mothers being better off. reply throwaway2037 9 hours agorootparent> lots of anecdotal tales of technology illiterate grandparents, fathers and mothers being better off I'll bite. Is there anyone here that thinks overall security for elderly (and lower skilled users) will *not* be hurt by additional app stores? I find it hard to believe. And, I write this post an an uber geek is is neither an Apple fan boi, but is very impressive by their overall security and UX. For the geeks, it would be great to have more stores. For the average users... maybe... For the least tech-savvy users, I cannot believe it will benefit them. reply greazy 4 hours agorootparentI think you are overall correct that the iOS store does improve the experience of the elder. But I suspect it's more due to the lack of 'side loading' and locked user experience and less so do to do with apple inspection/code review. I have no evidence to support this. My original question was a request for hard evidence which I think is lacking in arguments of security and safety. I think I've seen an equal amount of press surrounding fake and useless apps on both android and apple platforms. But this is purely observational. reply fennecbutt 7 hours agorootparentprevI still feel like that argument is like a \"won't somebody please think of the children\" one. If app stores need to be locked down to protect the elderly, then surely the Internet needs to be locked down to protect all children. After all, Safari still navigates little Jimmy to pornhub if he clicks the link. I feel like the real solution, same as the one most parents should be using instead of forcing it into everyone else is the same it's always been; don't give young Jimmy unfettered access to the Internet (and use a child/safety filter in your own home/on your own devices) and for Apple to provide a setting that enables/disables alternative app stores, so that children of the elderly can choose for them in the same way they'd choose for their children. reply int_19h 2 hours agorootparentIn fact, Apple devices already ship with something called \"Assistive Access\", which is a mode that you can enable that limits what can be done with the phone. In particular, it limits the ability to install apps. https://support.apple.com/guide/assistive-access-iphone/set-... reply greazy 4 hours agorootparentprevThis was my reasoning as well. I guess the mention of the elderly side tracked the discussion of safety and security of app stores. reply rpdillon 7 hours agorootparentprev> For the least tech-savvy users, I cannot believe it will benefit them. My parents are in their 80s and use Android with F-Droid (I set it up for them). No scams. No account or password. No ads. Simple apps. They have definitely benefited from having more choices available to them, specifically a repo of software built with something other than profit motive in mind. Apple's not very good at offering that. reply AnthonyMouse 11 hours agorootparentprevParticularly when there are better alternatives. For example, put a physical hardware switch on the inside of the device that disables new stores from being added. Now you can set up your technically disinclined relatives with Apple's store, and a couple of others you trust if it pleases you, then flip the switch and they can't get into trouble because they can't add others. Move the switch back and the device won't boot without a factory wipe. That's going to deter both anyone who can't successfully disassemble the device to flip the switch (i.e. severely technically illiterate people) and the people who aren't willing to press YES to a prompt that says it's about to erase all their data (i.e. mildly technically illiterate people), while leaving it possible for exactly the people it should be possible for. reply chongli 11 hours agorootparentWhat happens when Meta, X, Google et al. move to their own stores where they distribute apps unencumbered by Apple's privacy policies? Your relatives then contact you and insist that you flip the switch for them so they can install Facebook and Instagram from the Meta store so they can continue scrolling cat memes. I have yet to hear a convincing argument (from multi-store proponents) about how to prevent this. If the big social media companies pull their apps from Apple's official store and move to their own stores (with unfettered access to spy on users) then they will be successful at dragging their users with them. Furthermore, there is no evidence that GDPR has had any success stopping them from siphoning up all the data they want. reply AnthonyMouse 10 hours agorootparentYou tell them to use the service's web page because their app isn't available from a trustworthy source. And if their web page sucks, you encourage them to use a competing service whenever possible and only use the inconvenient one when strictly necessary. Which, as others do the same, pressures the service to do what you want and put their app in the existing store. This is the same thing that Apple does if they refuse to follow the process as it is, right? You're being insufficiently stubborn. And excessively dismissive if you think users making choices have no power. There are demonstrably people committed to having it their way: https://news.ycombinator.com/item?id=39685700 Unless you think tech companies have gotten too big and people don't have a choice anymore. If you have a monopoly, what you want is not another monopoly to fight them over which gets to fleece you, it's to smash them both by any available means. One of which is resistance through personal choices, one of which is... anti-trust enforcement. reply chongli 10 hours agorootparentUsers don't have much power, individually. They express their power collectively through the political system. I'm just very skeptical of the approach taken by Europe with the DMA. It seems to be less about empowering individual users and more about letting other large players carve up the Apple/Google 2-slice pie into a few more big slices. reply throwaway2037 9 hours agorootparentI'm confused by this post. > about letting other large players carve up the Apple/Google 2-slice pie into a few more big slices Do you not believe that increasing competition for app stores will \"empower individual users\"? If yes, please provide an alternative to DMA that will benefits users more. reply chongli 7 hours agorootparentApple markets their offering on its privacy and security. In effect, they act as a bargaining agent on behalf of their users which says no to a lot of the tracking Google, Meta, et al. want to do. Due to Apple's marketshare and the nature of this arrangement (the walled garden), these trackers are forced to bargain with Apple as a unit. The DMA seeks to put an end to this arrangement and allow the trackers to bargain with users individually. So, to answer your question: no, I do not believe it will empower individual users. If we really want to empower individual users we should be looking to inject more competition into the social media markets as well. More \"app stores\" that do nothing but offer the same apps while bypassing Apple's protections will not benefit users. And if the 30% Apple tax is the real problem then why not legislate against that directly? reply AnthonyMouse 7 hours agorootparent> If we really want to empower individual users we should be looking to inject more competition into the social media markets as well. Sure, but you can do both. > More \"app stores\" that do nothing but offer the same apps while bypassing Apple's protections will not benefit users. It's not just the same apps though. For example, the license Apple uses for the app store is incompatible with the GPL, so no one can make an iPhone app under the GPL or use existing GPL code in one. That license is one of the things that allows collaborative projects to form and right now that can't happen for iPhones. Likewise, the $100/year fee deters hobbyists from creating apps. And Apple prohibits certain types of content in their store, e.g. adult content or P2P apps, which some users would want. > And if the 30% Apple tax is the real problem then why not legislate against that directly? Price controls are generally a bad idea. The cost of hosting the app installers is generally negligible, but a few apps could be huge, and then it isn't, so how much should it cost? Can they charge a flat percentage of sales or does it have to be per-GB of transfer? What happens when the market price of storage or bandwidth changes over time? What if it's different in different regions? Legislating rules to handle all the edge cases is a fool's errand when competition would handle it for you because anyone who charges too much would lose business to someone who charges less. reply AnthonyMouse 9 hours agorootparentprev> Users don't have much power, individually. Users have a lot of power individually. The most obvious example is when there is competition. You could be a single person and your counterparty could be the world's largest corporation, but if you have ten other viable alternatives, they can do no worse to you than the best of your other alternatives or you just choose the other one. But you can also do it by being stubborn. Some people seem to have completely forgotten how to do this. There is a transaction with a surplus of $100, the counterparty is some egregious monopolist and the deal they offer you is that they get $99 and you get $1. A lot of people take the deal, because $1 is better than nothing, but that's not it. What you do is flip over the table and walk away, because that costs you $1 but it costs them $99 (or $50 or whatever their share would be after offering whatever it would have taken to satisfy your sense of fairness). People are so lazy now, or they've been conditioned, so now they always just take the $1 even if the alternative is only a minor inconvenience for them. Okay, you have to use Signal instead of WhatsApp, so what? But being willing to walk away from an unfair offer can sometimes be to your advantage even in an individual negotiation, because you both know the other party has more to lose. It's definitely to your advantage when other similarly-situated people do the same thing at scale. See also: https://en.wikipedia.org/wiki/Superrationality > They express their power collectively through the political system. They express their power collectively however they want. Organizations (e.g. FSF, EFF) can do things like pool money to create competing systems. Even for-profit corporations can do this -- you don't like the incumbent? Start a competitor, and raise funding from all the other people who don't like the incumbent. But again this seems like something people have been conditioned to believe doesn't work, even though it obviously does. To take a simple example, the EFF created Let's Encrypt, which cut the legs out from under the certificate mafia and made TLS free for everybody. All it took was an organization to pool enough resources to develop the initial implementation. > I'm just very skeptical of the approach taken by Europe with the DMA. It seems to be less about empowering individual users and more about letting other large players carve up the Apple/Google 2-slice pie into a few more big slices. Government regulations often fail as a result of incompetent administration or some corruption. But some forms of anti-trust can only be fixed through the law because the trusts themselves were created that way. If government enforce contracts in restraint of trade then people will enter into contracts and form a cartel or enforce a monopoly. That is not acceptable, so then governments have to constrain what kinds of contracts they're willing to enforce, and somebody has to write down what \"restraint of trade\" means to establish how that works. It's not fun and they'll often get it wrong but the only alternatives are to either not have governments enforce contracts or allow cartels to form that become de facto private governments. So we do the best we can. The EU is not great at this, but the problem they're trying to address is real, so sometimes you just get to sit back and watch two entities you don't really like have a fight with each other. reply int_19h 2 hours agorootparent> Okay, you have to use Signal instead of WhatsApp, so what? When everyone you actually need to communicate to is on WhatsApp, Signal is pretty much useless. reply johnnyanmac 10 hours agorootparentprev>What happens when Meta, X, Google et al. move to their own stores where they distribute apps unencumbered by Apple's privacy policies? I guess pigs fly or hell freezes over. Musk and Zuckerburg had years after such changes to make their own store on Android (which put in similar privacy policies at the same time as Apple). It doesn't make any sense for them because being off the main store is worse than gleeming off a bit more data to sell. >I have yet to hear a convincing argument (from multi-store proponents) about how to prevent this. How about proving that the subjects in question are on multiple stores to begin with, or otherwise have shown interest? You're questioning GDPR's validity, but your own premise isn't a thing to begin with. reply skydhash 10 hours agorootparentprev> Your relatives then contact you and insist that you flip the switch for them so they can install Facebook and Instagram from the Meta store so they can continue scrolling cat memes. You should not have to police adults on what they're allowed to do with their property. If someone asks me to help them setup their computer, I may gave some advice and warning about things to avoid. If they asked me to do something that may be dangerous, I can refuse to do it, but I will not actively prevent them from doing so. They're not children. If someone is ok with putting their whole digital life at risk, then let him do so. Just like you can't prevent someone who wants to eat cake all day. It's not your life. reply xvector 9 hours agorootparent> You should not have to police adults on what they're allowed to do with their property. The fundamental problem with this \"power to the people\" mentality is that adults don't actually know how to use technology. The average person is technologically illiterate. You can go on about giving adults full control over their property, etc. etc. but we both know that this is how you get security disasters: old people getting scammed, people losing their life savings and what not. Part of being an effective security engineer, is realizing that you need to protect people themselves. 2FA is a prime example of security driven via this mindset: necessary because the technologically illiterate masses reuse passwords. There are other benefits, but that's the main reason. So you shouldn't have to police people, but practically, in the end you do. > If someone is ok with putting their whole digital life at risk, then let him do so. All fun and games until people lose their life savings and get forced into homelessness or whatever. Then these people start to blame you. Then technologically illiterate senators and regulators will also blame you. Lose-lose scenario. Crypto is a prime example of what happens when you give people control. \"Power to the people!,\" tons of people get scammed, and this prompts regulatory lockdown. TL;DR is that the EU regs wouldn't be a problem if Apple could hide the functionality behind developer settings, but they can't. Exciting times, people in the EU are gonna get totally fucked by shady apps. GG. reply AnthonyMouse 7 hours agorootparent> You can go on about giving adults full control over their property, etc. etc. but we both know that this is how you get security disasters: old people getting scammed, people losing their life savings and what not. This happens when senile people are legally authorized to exercise control over their assets. It has nothing to do with technology and has been happening since before computers existed. The general solution is to appoint a conservator who is required to authorize major transactions. Which hardly justifies using the same measures for someone of sound mind. > 2FA is a prime example of security driven via this mindset: necessary because the technologically illiterate masses reuse passwords. And then their phone number changes or they lose access to their email and you've locked them out of their account. This is particularly egregious when the second factor is required to be a phone number, because people in financial straits will have their service canceled for non-payment and now you've magnified their problems at the worst possible time. But phone numbers serve as a convenient tracking ID since most people only have one of them, which may explain the popularity of requiring them \"for your own protection\". > All fun and games until people lose their life savings and get forced into homelessness or whatever. We build insecure systems and then blame the users for it and offer to lock them in a cell to protect them from our bad choices. Why is it that anyone can charge a credit card or a bank account who has the account number? Public key cryptography has been a thing for decades. Put a USB-C connector on the credit card itself and require the card to be plugged in to the device the first time each merchant wants to charge the account. 99% of credit card fraud, gone, because you can't breach one merchant and use the card info at a different one without physical access to the card. Meanwhile anyone could trivially cancel a subscription because the list of authorized merchants would be listed on the bank's account webpage and the user could remove one at any time. > Crypto is a prime example of what happens when you give people control. Anybody can go to the bank, right now, and withdraw cash and hand it to a scammer. Sometimes they do. You can also give them your television or company ID badge. Cryptocurrency is no different. Most of the crypto scams are get rich quick schemes, which people have been getting scammed by since the invention of barter. What made cryptocurrency so susceptible to scams wasn't that people were in control, it was that some people were actually getting rich, which made others credulous, and that attracts con men. \"We have to protect people from themselves\" is only true for small children and the mentally ill. Adults get to make their own choices -- because there is no one else to make them. As soon as you appoint someone else to do it, that person has a conflict of interest and the incentive to defect, and the person affected needs the right to choose differently unless you can prove that this specific person is mentally incapable of exercising reason. \"Nobody is ever completely reasonable\" doesn't cut it because that applies to the gatekeepers too. reply josephg 11 hours agorootparentprevHaving only one website would also make the web safer. But it would also be super lame. Is that a trade you would make? Why would we want freedom to self publish on the web but not in mobile apps? reply coldtea 10 hours agorootparentI'd prefer zero websites, but I'd settle for one. reply mellutussa 11 hours agoparentprev> How does that count as a \"marketplace\"? I'm assuming that Apple is going to profit from that catalogue. reply bloppe 17 hours agoparentprevApple is just trying to protect users from scammers! I'm sure all this sensible authorization and notarization business will continue even after the fees are removed from the equation reply somat 15 hours agoprevThe whole app ecosystem(android and apple) is carefully constructed for maximum market owner value extraction, user value is a secondary consideration. Basically, it is what the web would look like if it were developed by corporate interests, conversely \"apps\" could have been a better designed web[1], but instead are this comparatively clunky gated process where you have to explicitly install the app first only then can you use it. 1. The web was designed to deliver pages, this was well designed, application like functionally grew organically afterwards and is quite the mess. reply cosmojg 14 hours agoparentOh, the tragedy of what could have been: https://en.wikipedia.org/wiki/Firefox_OS reply umeshunni 11 hours agorootparentBut that would have required someone other than Mozilla to run Firefox reply ryukafalz 10 hours agorootparentprevAlso MeeGo, which was killed off before it really had a chance: https://en.wikipedia.org/wiki/Nokia_N9 reply fsflover 12 hours agorootparentprevMobian, PureOS and pmOS are here today. Sent from my Librem 5. reply OCASMv2 8 hours agoparentprev> The whole app ecosystem(android and apple) is carefully constructed for maximum market owner value extraction, user value is a secondary consideration. And it has become the norm because both developers and consumers have readily and happily accepted that deal. reply catlikesshrimp 14 hours agoparentprevTo be fair, this evolved naturally. The TI calculators were progamable, my brother used those. Then the pocket pcs (windows ce) had 3rd party programs, those were distributed as files by the publisher. Program stores were webpages were people sold their files. I used the skyscape medical books; you installed the program as usual, then you bought a code specific for your version and file. All that done through a webpage Then we have android. Google had the Marketplace (now playstore) as we know it today, except packages didnt use google services to validate licenses, Many times it was just a package (a file) The main progress was ease of use. Then comes iOS and their extreme BS of not being able to \"sideload\" \"apps\" The store is no longer a convenience, it is a requirement. For your safety, of course. The main \"progress\" here is that they convinced many \"Americans\" that a commodity affordable phone with a painted cartoon of a bitten apple is \"Exclusive\", as VIP only. I compare it to the NFT phenomena, except the fruit cartoon did stick. reply redundantly 12 hours agorootparent> For your safety, of course. I know they have ulterior motives for their walled garden, but this is a product of said garden. The App Store is by far much safer to use than Google's Play Store. Plus the parental controls on android are essentially non-existent. I'm happy in this walled garden. reply AnthonyMouse 11 hours agorootparentThe premise of a walled garden is to keep unwanted things out, not to imprison you inside. Apple maintaining a store where they've vetted everything in it is fine, and if you like you can refuse to install anything from outside of it. That doesn't justify them prohibiting you from installing anything from outside of it. It should be up to you. If you wanted to, you could even configure your phone to not add any new stores without a factory wipe. But maybe first you want to add in the repositories that have only free and open source software, or the stores of some respected game publishers who offer lower prices if you use their own stores for their games. And maybe the existence of these stores would encourage Apple to charge lower fees, and then you benefit from the lower fees even if you choose never to install anything from those stores, since your option to exerts competitive pressure on the stores(s) you are willing to use. reply brikym 10 hours agorootparentA better metaphor would be the shops at an airport. The monopoly airport fleece the shops with high rent and in turn the shops fleece their customers with high prices. reply simondotau 8 hours agorootparentA better metaphor would be the shops anywhere in your country. Governments and banks charge taxes and fees and in turn, through an elaborate architecture of laws and consequences, their customers don't have to wonder if their glass of water contains rotaviruses, or if the silverware has high levels of lead, or if 0.000014 BTC is gross overpayment for a hamburger, or if people in the next town will decide to rape and pillage sometime in the next hour. reply fennecbutt 7 hours agorootparentFor-profit corporations aren't governments. Something something America. reply somat 3 hours agorootparentYes they are, this is literally the definition of a corporation. A group of people wanted to form a government to run their \"for profit endeavor\" so they incorporated, that is, they received a license from their parent public interest corporation(aka \"The Government\") that allows them to operate under rule of law. It's corporations all the way down. corporation is really just another word for government. reply simondotau 7 hours agorootparentprevSomething something there's no such thing as a perfect analogy reply AnthonyMouse 7 hours agorootparentThe distinction kind of matters though. Monopolies are terrible and to be avoided but if you're going to have one, e.g. because roads are a natural monopoly, then you damn well want it to be an elected body and not a for-profit corporation that will do everything it can to extract monopoly rents from everybody in its fiefdom. reply redundantly 11 hours agorootparentprevI'm not arguing that it shouldn't be opened up. I'm just stating that by being a walled garden it is safer. When things eventually open up, when Apple is finally forced to permit other app stores on their mobile devices, I'll take a hard pass on them. reply Nevermark 10 hours agorootparent\"Walled garden\" does not mean \"safe system\". And it is not a prerequisite for a safe system, or vice versa. You are saying you are happy in a \"safe secure system\". In contrast, a \"walled garden\" is a prohibition on alternatives, not a source of safety. The prohibition of alternatives does not make the App Store safer. If anything, it protects Apple from competing with safter alternatives! Like an app store only for children. Or an app store of formally verified apps. Please correct me if I am somehow missing something... reply catlikesshrimp 11 hours agorootparentprevConsider these two statements: 1) I happy having a walled garden, I feel safe 2) I am happy being imprisoned in a walled garden with no door, I feel safe reply vbezhenar 9 hours agorootparentprevThose who like AppStore actually benefit from it being the only store. It means that almost all developers will bend under Apple rules and users will get their apps. reply AnthonyMouse 8 hours agorootparentThese users can do the same thing by refusing to use any other store even if they are allowed to, and if there are many of them they'll have leverage. But what they want is to force other users, who would willingly use other stores, to also use only the same one as them. They have no right to force others to do that any more than Apple does. reply beeboobaa 11 hours agorootparentprev> The App Store is by far much safer to use than Google's Play Store By what metric? The warm fuzzy in your stomach because you believe apple's bullshit? Have you actually used the play store? They are identical. reply celticninja 11 hours agorootparentprevHave you tried parental controls on Android or are you just taking out of the side of your mouth? I have parental controls for my kids android devices and it works exceptionally well. I am not dissing the apple version because I have not used it, and based on your comment I have to assume you have not used the android parent controls and are just needing to convince yourself that apple are better and the apple premium you are paying is worth it. Spoiler: it isn't. reply redundantly 11 hours agorootparentI have tried it. More than one phone from different carriers. The parental controls are lacking. It's been a couple of years since I've last tried, but given Google's history regarding subpar controls I doubt it has gotten appreciably better. reply celticninja 11 hours agorootparentWhat were the subpar controls? I use it daily for my kids so would genuinely like to know what you feel didn't/doesn't work because for the last 4 or 5 years I have never had one issue using it. reply makeitdouble 10 hours agorootparentprev> More than one phone from different carriers. I'm confused by this. Did it come to play regarding parental controls ? Like an extra layer from the carrier ? reply celticninja 10 hours agorootparentYeah seems to detract from the Google angle if it's carrier related reply celticninja 11 hours agorootparentprevhttps://news.ycombinator.com/item?id=39685272 So safe in this walled garden where Apple reviews all apps for user safety and security. reply redundantly 11 hours agorootparentI didn't claim that it's perfect, just that it's safer. Regarding smartphone safety, the only truly safe thing to do is not not use one at all. reply celticninja 10 hours agorootparentEverybody who lives dies, so to avoid dying just dont ever live. Solid workable solution you have proposed there. reply megous 11 hours agorootparentprevBased on something \"real\" like scam/fraud metrics, or just \"this is what Tim Cook wants me to think\"? Both stores are walled gardens. One onboarding experience: https://news.ycombinator.com/item?id=39685272 :-D reply endisneigh 9 hours agoparentprevThere’s literally nothing stopping someone from distributing their app as a web app, and no PWA isn’t necessary for distribution. reply klaustopher 21 hours agoprevI am really impressed how much time and effort Apples legal department spends to find every single loop hole in the wording of the DMA. The 50ct per install for alternate app stores, 50ct per install for non-App Store apps after the millionth install, 1 million dollar in securities for alternate app stores, etc all follow the words of the DMA, but not the spirit. I am really interested to see the European Commissian drag Apple in front of a court and them having to legally defend their actions. I assume that all of those things they are setting up to circumvent people from using their rights will really blow up in their faces. reply sircastor 10 hours agoparent> I am really impressed how much time and effort Apples legal department spends to find every single loop hole in the wording of the DMA. Maybe this is an American trait, but I would be surprised at any company that wouldn't be doing this. A law has been made that affects our business: How do we comply with the law with as little impact as possible to us? Some of the comments here seem to expect Apple to simply give up, as though a parent just walked in the room and said \"You better do it or else.\" If it's really the spirit of the law that counts, then the law should require no specificity. A simple \"Treat everyone fairly, installs can come from anywhere\" would be sufficient. reply Nevermark 10 hours agorootparentPerhaps it seems unusual, as Apple has so much technical control, an unusually extensive legal budget, and doing a very effective job of castrating any \"threats\" or as the EU might say \"significant competition\". And Apple has the cash to play chicken with any potential fines if it comes to it, so its not hedging much if at all. It is clear that the EU is going to have to get very tough, before Apple is going to proactively take into account any of the \"spirit of the law\" that the EU would like it to understand. reply zb3 9 hours agorootparentprevCan't they just make their devices more expensive instead? reply internetter 20 hours agoparentprevThe EU has always been enthusiastic about the spirit of the law, and Apple is not used to this. You can see their temper tantrum unfold every time they find this out. reply procgen 20 hours agorootparentDisregarding the letter of the law seems arbitrary and capricious. reply internetter 20 hours agorootparentIs it? Developers used to determinism in software frequently don't understand that in all jurisdictions the law is ultimately interpreted by humans. I've been going through some legal processes myself, and my friend who is a lawyer reminded me more times than I care to admit that this is the case. In the US, SCOTUS's job is literally to interpret the spirit of the law in the event of ambiguity. reply AshamedCaptain 9 hours agorootparentDevelopers are fully used to this ambiguity and \"spirit of the law\" when interpreting standards. Search for WeirdNIX (popularly known as Windows NT and other names too). reply klaustopher 20 hours agorootparentprevThere's different ways to interpret laws for courts. One of them is called teleological interpretation where you follow the intent of the law. For this courts also look into the documentation the legislation provided when defining the law. This is usually not done by lower courts, but courts like the CJEU use those when the letter of the law is unclear to define this for the lower courts to follow. reply abigail95 10 hours agorootparentThis would be more valid if the law was passed with a message that says \"please interpret this law according to this documentation teleologically\" reply rekoil 1 hour agorootparentprevBut that's the thing, when your law is legally binding in 24 different languages it's really impractical if not entirely impossible to have a system based on letter-of-the-law interpretations... reply justin66 10 hours agorootparentprev> Disregarding the letter of the law seems arbitrary and capricious. There's a distinction to be made between principles-based and rules-based regulation which I bet you're unfamiliar with. reply Hamuko 20 hours agorootparentprevThe situation in the US seems to suggest that trying to finely analyze the exact sequence of words in a law or the consitution still leaves a whole lot of room for arbitrary decisions. Abortion was a constitutional right until it wasn't and the constitution was not changed between. reply anon373839 10 hours agorootparentAll language carries inherent ambiguity. However, developments in American constitutional law aren’t really about that. The Constitution is very general and it uses terms that lack an objective meaning (for example, “Due Process” - what counts as “process”? What process is “due”?) It can’t really be implemented without bringing in a pile of philosophy and policy making. At the same time, SCOTUS has been guilty of stretching its terms to include ideas that are clearly out of scope. (For example, the dubious invention of “substantive” due process - which all of the abortion stuff hinges on.) reply jquery 8 hours agorootparentOf all the examples you could've brought up and you thought a person's right to control their body is a stretch? Try \"qualified immunity\" if you want an example of justices reasoning with their bare ass showing. reply anon373839 7 hours agorootparentI was responding to the parent comment. Also, substantive due process was not invented for reproductive rights. It was invented in Dred Scott v. Sandford, to prevent “free” states from depriving slave owners of their “property”. reply isodev 20 hours agorootparentprevI’m so tired of this, instead of doing the right thing, Apple just keeps trying to brute force the legal framework. You don’t need fancy legal team to know this is not the way. reply klaustopher 20 hours agorootparentFrom a business point, I can totally understand what Apple is doing. Making this as painful and unpredictable (as a developer you never know if your app will be successfull and gain more than 1 million installs) is the way to keep developers using the old contract and keep them on the app store. This makes sense for Apple to find every loophole possible ... As a consumer, and an Apple users, I want them to be slapped as hard as possible for how they implement this. reply frizlab 20 hours agorootparentFunny how things go. As a consumer especially, but even as a developer I don’t want the DMA to succeed and purposefully want iOS to be a walled garden. It’s literally one of the reasons why I’m on iOS! reply klaustopher 20 hours agorootparentThat's the nice thing about the DMA ... Nobody forces you to install a 3rd party app store, nobody forces you to install apps from websites, nobody forces you out of the walled garden. For you nothing changes. Those that want to use their 1000€ device differently than you now have the chance to. reply frizlab 19 hours agorootparentAs the “tech guy” in the family things might change actually. (One of) the reasons why I like the walled garden is how it simplifies everything troubleshooting-wise. I have a few quirks to know, the rest is because of hardware failure and that’s it. My peer not being tech-savvy might install stupid things from stupid places and it might be a problem. The way it’s done it’s unlikely, but still it just complexify things for next to no reasons in my book. (Yes 30% is a lot; I personally don’t care, though I do recognize I’m a good position and I can afford not to–but then again, the most vocal about the 30% are not the most unwealthy…) reply klaustopher 19 hours agorootparentThat's also solveable. For android you need to enable deep inside of the settings to allow 3rd party installs. Nobody is preventing Apple to do something like this. Or that you can create a profile that disables that setting that you can install on your familys devices. Nothing in the DMA prevents this. Just because it makes your life easier as the family tech support is a pretty selfish reason to hope for a very good pro-consumer law to fail. reply frizlab 19 hours agorootparentThe way it’s going I’m actually pretty sure if they did that they’d get reprimanded… Also it makes my life annoying when I open Safari and am presented w/ what can be told as the worst pop-up ever and have to spend literally minutes dismissing it for something I neither wanted nor needed. It’s the cookie banner all over again. Does not seem like a lot, but as a developer I use devices in a factory configuration a lot, and it’s just as annoying as it’s useless. Basically it’s the cookie banner again. Served no-one (at least definitely not the consumers), but annoyed a lot. As for the “those that want to use their 1000€ device differently than you now have the chance to,” well……… nobody forced them to buy a 1000€ device did they?? They knew of the limitations; they had to, or they’re very dumb. The law is not pro-consumer contrary to people say, it’s anti-garden, which is definitely not the same, and I’ll die on this hill. reply ghusto 12 hours agorootparentNearly no sites comply with the cookie-banner law, if they did, you wouldn't mind it. It essentially says \"Tell the user you're tracking them, give them a button to click not allow you to do that\". If sites actually did that, I honestly couldn't care less about the extra second it would take to click \"No, fuck off\". reply user_7832 15 hours agorootparentprev> Also it makes my life annoying when I open Safari and am presented w/ what can be told as the worst pop-up ever and have to spend literally minutes dismissing it for something I neither wanted nor needed. It’s the cookie banner all over again. Know what's cool? Firefox on android supports ublock origin. There are some chromium forks too with desktop extension support (on android). Funny what an open(er) market and easy of installing apps does, huh? reply frizlab 6 hours agorootparentI have ads and pop up blockers already? What are you on about?? reply ulucs 16 hours agorootparentprev> Basically it’s the cookie banner again. Served no-one (at least definitely not the consumers), but annoyed a lot. Oh no, you have to be given the option to not permit your data to be shared with ~1000 different partners with \"legitimate\" interests. Honestly, the only thing that is wrong with GDPR is that it came out too late. reply frizlab 16 hours agorootparent90% of the websites today use google analytics which is not GDPR compliant, and yet nothing happens. Ironically Apple did more for privacy than GDPR ever did, and was able to enforce it… by having a walled garden! reply isodev 15 hours agorootparent> yet nothing happens Every time you dismiss a \"we care for your privacy\" banner, you're being made aware that your data is shared with hundreds or thousands of data brokers with \"legitimate interest\". The fact that vendors prefer to make your experience miserable rather than give up tracking is another example of \"malicious compliance\". What happens is that you now have the right to request a copy of the personal information a site has collected and ask them to delete it. You can also sue them if they don't fulfil your request. You're welcome to exercise your rights as an EU citizen at any time. reply latexr 14 hours agorootparentprev> and yet nothing happens. Not true. https://noyb.eu/en/noyb-win-first-major-fine-eu-1-million-us... reply _aavaa_ 18 hours agorootparentprevPeople (myself included) say the same thing about why they buy their tech illiterate relatives macOS computers. And it works. And guess what, it works despite Apple not getting a cut of every everything. reply skydhash 10 hours agorootparentMy girlfriend only install the handful of apps she wants both on her Mac and her iPhone and doesn't go back to the app store. She just put things on auto update. Most people don't fiddle with their computing device. And if installation steps are confusing, she just asked me to do it. I guess that's why Microsoft are enabling so many things on Windows as most users won't enable them by themselves. reply _aavaa_ 8 hours agorootparentThat's neither here nor there for whether Apple has the right to insert themselves into every transaction on their platform and gets to decide which apps are allowed to exist. And let's not kid ourself: Microsoft is enabling (and re-enabling and re-enabling and re-enabling) so many things because they are slowly turning their OS into spyware to make more money, not because they care at all about their users. I'll re-iterate Cory Doctorow's quote: \"Anytime someone puts a lock on something you own, against your wishes, and doesn't give you the key, they're not doing it for your benefit\". reply rchaud 17 hours agorootparentprev> My peer not being tech-savvy might install stupid things from stupid places and it might be a problem. Yes, and they may also respond to phishing emails served up by the Mail app. Do your peers consider you responsible for fixing that too? reply dariosalvi78 15 hours agorootparentprevIt's perfectly reasonable to create even more walled gardens than the Apple walled garden, once you open up for different markets. That's the beauty of choice. reply Nevermark 9 hours agorootparentprevI doubt it. \"Walled\" and \"Safety\" are getting confused here. I think you like the App Store for its safety. You trust it, enough to be happy with it. What does that have to do with wanting others to be denied alternatives? That deliver however much safety and different benefits that other people want? If safety is one of Apple store's selling points, then competitive app stores will push Apple to deliver even more safety. Perhaps new forms of safety others pioneer. Apple didn't invent security or sandboxes. While also encouraging it to loosen non-safety driven (and therefore quietly non-customer friendly) restrictions on innovation. That can only benefit you. reply internetter 20 hours agorootparentprevFor years Apple has placed deliberately c",
    "originSummary": [
      "Apple revealed new updates for developers in the European Union, enabling them to distribute apps directly from webpages, create in-app promotions, and provide apps exclusively from their own catalog.",
      "Developers must adhere to specific criteria and agree to continuous obligations to distribute apps from their websites, accessible post a software update in the spring.",
      "These modifications align with Apple's adherence to the Digital Markets Act in the EU."
    ],
    "commentSummary": [
      "The conversation delves into various subjects concerning Apple, such as EU regulations, privacy worries, developer fees, consumer choices, and competitiveness in the app store sector.",
      "Topics range from critiques of Apple's profit-focused strategies to discussions on cookie-consent notifications, views on Apple's security protocols, the ramifications of the Digital Markets Act on Apple, and the influence of individual users on tech firms.",
      "The debate broadly covers user control, privacy, security, competition, and the equilibrium between constraints and liberty in the tech field."
    ],
    "points": 653,
    "commentCount": 1209,
    "retryCount": 0,
    "time": 1710245322
  },
  {
    "id": 39680997,
    "title": "Meta Expands Investment in AI with 24k GPU Clusters for AGI Development",
    "originLink": "https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/",
    "originBody": "POSTED ON MARCH 12, 2024 TO AI Research, Data Center Engineering, ML Applications Building Meta’s GenAI Infrastructure By Kevin Lee, Adi Gangidi, Mathew Oldham Marking a major investment in Meta’s AI future, we are announcing two 24k GPU clusters. We are sharing details on the hardware, network, storage, design, performance, and software that help us extract high throughput and reliability for various AI workloads. We use this cluster design for Llama 3 training. We are strongly committed to open compute and open source. We built these clusters on top of Grand Teton, OpenRack, and PyTorch and continue to push open innovation across the industry. This announcement is one step in our ambitious infrastructure roadmap. By the end of 2024, we’re aiming to continue to grow our infrastructure build-out that will include 350,000 NVIDIA H100 GPUs as part of a portfolio that will feature compute power equivalent to nearly 600,000 H100s. To lead in developing AI means leading investments in hardware infrastructure. Hardware infrastructure plays an important role in AI’s future. Today, we’re sharing details on two versions of our 24,576-GPU data center scale cluster at Meta. These clusters support our current and next generation AI models, including Llama 3, the successor to Llama 2, our publicly released LLM, as well as AI research and development across GenAI and other areas . A peek into Meta’s large-scale AI clusters Meta’s long-term vision is to build artificial general intelligence (AGI) that is open and built responsibly so that it can be widely available for everyone to benefit from. As we work towards AGI, we have also worked on scaling our clusters to power this ambition. The progress we make towards AGI creates new products, new AI features for our family of apps, and new AI-centric computing devices. While we’ve had a long history of building AI infrastructure, we first shared details on our AI Research SuperCluster (RSC), featuring 16,000 NVIDIA A100 GPUs, in 2022. RSC has accelerated our open and responsible AI research by helping us build our first generation of advanced AI models. It played and continues to play an important role in the development of Llama and Llama 2, as well as advanced AI models for applications ranging from computer vision, NLP, and speech recognition, to image generation, and even coding. Under the hood Our newer AI clusters build upon the successes and lessons learned from RSC. We focused on building end-to-end AI systems with a major emphasis on researcher and developer experience and productivity. The efficiency of the high-performance network fabrics within these clusters, some of the key storage decisions, combined with the 24,576 NVIDIA Tensor Core H100 GPUs in each, allow both cluster versions to support models larger and more complex than that could be supported in the RSC and pave the way for advancements in GenAI product development and AI research. Network At Meta, we handle hundreds of trillions of AI model executions per day. Delivering these services at a large scale requires a highly advanced and flexible infrastructure. Custom designing much of our own hardware, software, and network fabrics allows us to optimize the end-to-end experience for our AI researchers while ensuring our data centers operate efficiently. With this in mind, we built one cluster with a remote direct memory access (RDMA) over converged Ethernet (RoCE) network fabric solution based on the Arista 7800 with Wedge400 and Minipack2 OCP rack switches. The other cluster features an NVIDIA Quantum2 InfiniBand fabric. Both of these solutions interconnect 400 Gbps endpoints. With these two, we are able to assess the suitability and scalability of these different types of interconnect for large-scale training, giving us more insights that will help inform how we design and build even larger, scaled-up clusters in the future. Through careful co-design of the network, software, and model architectures, we have successfully used both RoCE and InfiniBand clusters for large, GenAI workloads (including our ongoing training of Llama 3 on our RoCE cluster) without any network bottlenecks. Compute Both clusters are built using Grand Teton, our in-house-designed, open GPU hardware platform that we’ve contributed to the Open Compute Project (OCP). Grand Teton builds on the many generations of AI systems that integrate power, control, compute, and fabric interfaces into a single chassis for better overall performance, signal integrity, and thermal performance. It provides rapid scalability and flexibility in a simplified design, allowing it to be quickly deployed into data center fleets and easily maintained and scaled. Combined with other in-house innovations like our Open Rack power and rack architecture, Grand Teton allows us to build new clusters in a way that is purpose-built for current and future applications at Meta. We have been openly designing our GPU hardware platforms beginning with our Big Sur platform in 2015. Storage Storage plays an important role in AI training, and yet is one of the least talked-about aspects. As the GenAI training jobs become more multimodal over time, consuming large amounts of image, video, and text data, the need for data storage grows rapidly. The need to fit all that data storage into a performant, yet power-efficient footprint doesn’t go away though, which makes the problem more interesting. Our storage deployment addresses the data and checkpointing needs of the AI clusters via a home-grown Linux Filesystem in Userspace (FUSE) API backed by a version of Meta’s ‘Tectonic’ distributed storage solution optimized for Flash media. This solution enables thousands of GPUs to save and load checkpoints in a synchronized fashion (a challenge for any storage solution) while also providing a flexible and high-throughput exabyte scale storage required for data loading. We have also partnered with Hammerspace to co-develop and land a parallel network file system (NFS) deployment to meet the developer experience requirements for this AI cluster. Among other benefits, Hammerspace enables engineers to perform interactive debugging for jobs using thousands of GPUs as code changes are immediately accessible to all nodes within the environment. When paired together, the combination of our Tectonic distributed storage solution and Hammerspace enable fast iteration velocity without compromising on scale. The storage deployments in our GenAI clusters, both Tectonic- and Hammerspace-backed, are based on the YV3 Sierra Point server platform, upgraded with the latest high capacity E1.S SSD we can procure in the market today. Aside from the higher SSD capacity, the servers per rack was customized to achieve the right balance of throughput capacity per server, rack count reduction, and associated power efficiency. Utilizing the OCP servers as Lego-like building blocks, our storage layer is able to flexibly scale to future requirements in this cluster as well as in future, bigger AI clusters, while being fault-tolerant to day-to-day Infrastructure maintenance operations. Performance One of the principles we have in building our large-scale AI clusters is to maximize performance and ease of use simultaneously without compromising one for the other. This is an important principle in creating the best-in-class AI models. As we push the limits of AI systems, the best way we can test our ability to scale-up our designs is to simply build a system, optimize it, and actually test it (while simulators help, they only go so far). In this design journey, we compared the performance seen in our small clusters and with large clusters to see where our bottlenecks are. In the graph below, AllGather collective performance is shown (as normalized bandwidth on a 0-100 scale) when a large number of GPUs are communicating with each other at message sizes where roofline performance is expected. Our out-of-box performance for large clusters was initially poor and inconsistent, compared to optimized small cluster performance. To address this we made several changes to how our internal job scheduler schedules jobs with network topology awareness – this resulted in latency benefits and minimized the amount of traffic going to upper layers of the network. We also optimized our network routing strategy in combination with NVIDIA Collective Communications Library (NCCL) changes to achieve optimal network utilization. This helped push our large clusters to achieve great and expected performance just as our small clusters. In the figure we see that small cluster performance (overall communication bandwidth and utilization) reaches 90%+ out of the box, but an unoptimized large cluster performance has very poor utilization, ranging from 10% to 90%. After we optimize the full system (software, network, etc.), we see large cluster performance return to the ideal 90%+ range. In addition to software changes targeting our internal infrastructure, we worked closely with teams authoring training frameworks and models to adapt to our evolving infrastructure. For example, NVIDIA H100 GPUs open the possibility of leveraging new data types such as 8-bit floating point (FP8) for training. Fully utilizing larger clusters required investments in additional parallelization techniques and new storage solutions provided opportunities to highly optimize checkpointing across thousands of ranks to run in hundreds of milliseconds. We also recognize debuggability as one of the major challenges in large-scale training. Identifying a problematic GPU that is stalling an entire training job becomes very difficult at a large scale. We’re building tools such as desync debug, or a distributed collective flight recorder, to expose the details of distributed training, and help identify issues in a much faster and easier way Finally, we’re continuing to evolve PyTorch, the foundational AI framework powering our AI workloads, to make it ready for tens, or even hundreds, of thousands of GPU training. We have identified multiple bottlenecks for process group initialization, and reduced the startup time from sometimes hours down to minutes. Commitment to open AI innovation Meta maintains its commitment to open innovation in AI software and hardware. We believe open-source hardware and software will always be a valuable tool to help the industry solve problems at large scale. Today, we continue to support open hardware innovation as a founding member of OCP, where we make designs like Grand Teton and Open Rack available to the OCP community. We also continue to be the largest and primary contributor to PyTorch, the AI software framework that is powering a large chunk of the industry. We also continue to be committed to open innovation in the AI research community. We’ve launched the Open Innovation AI Research Community, a partnership program for academic researchers to deepen our understanding of how to responsibly develop and share AI technologies – with a particular focus on LLMs. An open approach to AI is not new for Meta. We’ve also launched the AI Alliance, a group of leading organizations across the AI industry focused on accelerating responsible innovation in AI within an open community. Our AI efforts are built on a philosophy of open science and cross-collaboration. An open ecosystem brings transparency, scrutiny, and trust to AI development and leads to innovations that everyone can benefit from that are built with safety and responsibility top of mind. The future of Meta’s AI infrastructure These two AI training cluster designs are a part of our larger roadmap for the future of AI. By the end of 2024, we’re aiming to continue to grow our infrastructure build-out that will include 350,000 NVIDIA H100s as part of a portfolio that will feature compute power equivalent to nearly 600,000 H100s. As we look to the future, we recognize that what worked yesterday or today may not be sufficient for tomorrow’s needs. That’s why we are constantly evaluating and improving every aspect of our infrastructure, from the physical and virtual layers to the software layer and beyond. Our goal is to create systems that are flexible and reliable to support the fast-evolving new models and research. Share this: Facebook Threads X LinkedIn Hacker News Email",
    "commentLink": "https://news.ycombinator.com/item?id=39680997",
    "commentBody": "Building Meta's GenAI infrastructure (fb.com)574 points by mootpt 18 hours agohidepastfavorite258 comments danielhanchen 16 hours agofloat8 got a mention! x2 more FLOPs! Also xformers has 2:4 sparsity support now so another x2? Is Llama3 gonna use like float8 + 2:4 sparsity for the MLP, so 4x H100 float16 FLOPs? Pytorch has fp8 experimental support, whilst attention is still complex to do in float8 due to precision issues, so maybe attention is in float16, and RoPE / layernorms in float16 / float32, whilst everything else is float8? reply GamerAlias 16 hours agoparentI was thinking why is this one guy on HN so deeply interested and discussing technical details from a minor remark. Then I clocked the name. Great work on Gemma bugs reply danielhanchen 16 hours agorootparentOh thanks :) I always like small details :) reply andy99 14 hours agoparentprevIs there float8 support in any common CPU intrinsics? It sounds interesting but curious what will be the impact if any on CPU inference. reply ashvardanian 11 hours agorootparentNope. Moreover, simulating it even with AVX-512 is quite an experience. Been postponing it for 2 years now... But first of all, you need to choose the version of float8 you want to implement, as the standards differ between GPU vendors. reply janwas 7 hours agorootparentWe use it in gemma.cpp [1]. This hybrid of E5M2 and E4M3 decodes to bf16 in ~14 instructions, so we can do that on the fly during dot products. [1]: github.com/google/gemma.cpp reply danielhanchen 7 hours agorootparentCongratulations on gemma.cpp!! reply teaearlgraycold 4 hours agorootparentprevI’m curious if there’s a meaningful quality difference between float8 and some uint8 alternative (fixed precision or a look up table). reply CraigJPerry 2 hours agorootparentA LUT could be a significant performance penalty would it not? Instead of a float8 (potentially multiple in simd case) in a register, you’re now having to head out to at least L1 cache to dereference the value in the LUT. Plain uint8 wouldn’t allow for the same accuracy range as float8 and it’s the accuracy not the precision (which uint would win for the largest values it can represent) that counts most. reply danielhanchen 2 hours agorootparentOh oh was just gonna comment as well, but saw this! I think x86 has like pshufb for LUTs (used them like ages ago, but forgot now :() I think also some game (was it Spiderman) used loads of lookup tables. The issue with LUTs is don't you have to update the LUT itself? You can select which memory address to load up, but the LUT itself has to be differentiable maybe? TBH I'm not an expert on LUTs. On fixed point - similarly ye you have to fix the precision ranges as well, so again I'm unsure on how one changes the fixed point numbers over time. I'll have to read more on fixed point. Maybe 1.58bit using (-1, 0, 1) which gets rid of multiplications and just additions might be more useful, although you'll only get a 2x FLOP boost since you still need fp8 or fp16 addition. reply protomolecule 59 minutes agorootparent>I think x86 has like pshufb for LUTs There is also VPERMI2B [0] which operates on a 128 byte LUT. [0] https://en.wikichip.org/wiki/x86/avx512_vbmi reply danielhanchen 50 minutes agorootparentOh I forgot about that!! But ye LUTs are very interesting and fascinating :) One of the hidden gems of CPU optimizations :) reply ipsum2 14 hours agoparentprevYou're still bounded by memory bandwidth, so adding multiples to FLOPs is not going to give you a good representation of overall speedup. reply jabl 14 hours agorootparentWell, those smaller floats require less BW to transfer back and forth as well. Perhaps not a reduction linear in the size of the float, as maybe smaller floats require more iterations and/or more nodes in the model graph to get an equivalent result. But rest assured there's an improvement, it's not like people would be doing it if there wasn't any benefit! reply andy99 12 hours agorootparentThe impact on bandwidth is the main reason smaller is better I belive, certainly when it's the bottleneck. I'm only really familiar with CPU but with say FP16 you might convert back to FP32 when you're doing the actual multiplication (so conversion plus multiplication is actually slower) but because you're moving half the data in and off you still get a huge speedup. reply danielhanchen 7 hours agorootparentI can't remember some research paper somewhere even if you do float32 multiplications, but keep the data in bfloat16 by just simply truncating the lower mantissa bits, and doing packing, you still get speedups, since matrix multiplication is bound both by compute and cache access. If you can optimize on the cache side of things, speedups are definitely there. reply danielhanchen 7 hours agorootparentprevI'm not sure exactly on how NVIDIA calculates FLOPs, but I do know for Intel's FLOPs, it's calculated from how many FMA units, how many loads can be done in tandem, and what the throughput is. And ye fp8 requires 2x less space. Sparse 2:4 might be less pronounced, since the matrix first needs to be constructed on the fly, and there is like a small matrix of indicator values. reply boywitharupee 8 hours agoparentprevcare to explain why attention has precision issues with fp8? reply danielhanchen 8 hours agorootparentOh so float8's L2 Norm from float32 is around I think 1e-4, whilst float16 is 1e-6. Sadly attention is quite sensitive. There are some hybrid methods which just before the attention kernel which is done in fp8, upcasts the Q and K from the RoPE kernel to become float16, then also leaves V to be in float8. Everything is done in fp8 on the fly, and the output is fp8. This makes errors go to 1e-6. reply j45 11 hours agoparentprevIs it safe to assume this is the same float16 that exists in Apple m2 chips but not m1? reply j45 8 hours agorootparentClarification: bfloat16 “bfloat16 data type and arithmetic instructions (AI and others)” https://eclecticlight.co/2024/01/15/why-the-m2-is-more-advan... reply dougdonohoe 10 hours agoprevHaving lived through the dot-com era, I find the AI-era slightly dispiriting because of the sheer capital cost of training models. At the start of the dot-com era, anyone could spin up an e-commerce site with relatively little infrastructure costs. Now, it seems, only the hyper-scale companies can build these AI models. Meta, Google, Microsoft, Open-AI, etc. reply herval 6 hours agoparentI’m not sure we went through the same dot-com era, but in my experience, it was extremely expensive to spin up anything. You’d have to run your own servers, buy your own T1 lines, develop with rudimentary cgi… it was a very expensive mess - just like AI today Which gives me hope that - like the web - hardware will catch up and stuff will become more and more accessible with time reply renegade-otter 10 hours agoparentprevNot everything has to be AI. You can run a small business infra for MUCH less than you did back then, especially if you adjust for inflation (!). Training AI models costs a fortune, but so far it's been just front-loading costs in hopes of a windfall. We'll see what actually happens. reply boringg 7 hours agorootparentFront loading costs to eventually extract rents on usage with one hell of a capital wall protecting the assets. Its easier to spin up a business for sure -- also easier to unwind it - there not as sticky as they used to be. reply whatshisface 7 hours agorootparentIf the government can stay back far enough that more than one AI company can train their models, it will end up working like steel mills - barely enough profit to pay the massive cost of capital due to competition. If the government regulates the industry into a monopoly, all bets are off. Their investors are going to push hard for shutting the door behind them so watch out. The only question is - what tactic? I don't really know, but one trick I am aware of is \"specifying to the vendor.\" In other words, the introduction of regulatory requirements that are at every step in the process a description of the most favored vendor's product. As the favored players add more features, potentially safety features, those features are required in new regulations, using very specific descriptions that more or less mandate that you reproduce the existing technology, to use a software engineer's term, bug-for-bug. If your product is better in some ways but worse in others, you might have a chance in the market - but to no avail, if the regulations demand exactly the advantages of the established suppliers. reply brookst 6 hours agorootparentprevThis is typically called a high fixed cost business, like airlines, hotels/apartments, SpaceX, etc. The dream may be barriers to entry that allow high margins (“rents” if you prefer the prejudicial), but all too often these huge capital costs bankrupt the company and lose money for investors (see: WeWork, Magic Leap). It is high risk, high return. Which seems fair. reply andy99 10 hours agoparentprevSo far it's been pretty \"democratic\" - I feel in no way disadvantaged because I can't train a foundation model myself. Actually the ecosystem is a lot better than 25 years ago - there are open source (or source available) versions of basically everything you'd want to participate in modern AI/ML. reply mewpmewp2 10 hours agorootparentBut none of those are remotely as good as GPT4 for example. reply to11mtm 9 hours agorootparentMixtral? reply ametrau 8 hours agorootparentObviously not even close reply richardw 2 hours agoparentprevI find the market way more open and competitive than dot-com. Everyone is throwing up a chatbot or RAG solution. There are tradesmen and secretaries and infinite 19 year olds who are now able to wire together a no-code app or low-code bot and add value to real businesses. The hyper scalars are making some money but absolutely don't have this locked up. Any Groq or Mistral could wander in and eat their lunch, and we haven't really started the race yet. The next decade will be ridiculous. reply danielhanchen 5 hours agoparentprevAnother way to compete with the big tech incumbents is instead of hardware, try maths and software hacks to level the playing field! Training models is still black magic, so making it faster on the software side can solve the capital cost issue somewhat! reply toxik 4 hours agorootparentThis kind of research is also incredibly capital intensive. You have to pay some of the smartest people around to work in it. reply djhn 24 minutes agorootparentThat's labour and human capital intensive, not capital intensive. And I don't mean this as a technically correct nitpick: in terms of economics it's more accurate to call it the exact opposite of capital intensive. reply danielmarkbruce 10 hours agoparentprevIt's not quite the same thing. A model is just one part of a product. You can spin up a product with zero infra and calling APIs hosting models. reply mindwok 7 hours agoparentprevWe will probably get there, it's just going to take time for hardware supply chains to catch up. I feel it's more comparable to mainframe eras - it took time for general purpose computing to become commoditised. reply hackerlight 4 hours agoparentprevFoundation models != application layer. The question is whether the application layer's lunch will be eaten by better foundation models. reply islewis 17 hours agoprevI know we won't get it this from FB, but I'd be really interested to see how the relationship of compute power to engineering hours scales. They mention custom building as much as they can. If FB magically has the option to 10x the compute power, would they need to re-engineer the whole stack? What about 100x? Is each of these re-writes just a re-write, or is it a whole order of magnitude more complex? My technical understanding of what's under the hood of these clusters is pretty surface level- super curious if anyone with relevant experience has thoughts? reply bilekas 17 hours agoparentI'm not 100% sure but I would.make an educated guess that that cluster in the first image for example is a sample of scalable clusters, so throwing more hardware at it could bring improvements but sooner or later the cost to improvements will call for an optimization or rewrite as you call it, so a bit of both usually. It seems a bit of a balancing act really! reply jvalencia 10 hours agoparentprevThe cost of training quickly outpaces the cost of development as context length increases. So hardware is cheap until it isn't anymore, by orders of magnitude. reply samstave 10 hours agorootparentBut there is still significant cost in the physical buildouts of new pods/DCs, whatever and the human engineering hours to physically build, even though its a mix of resources across the vendors and FB? - it still would be interesting to know man hours into the physical build of the HW. reply tintor 15 hours agoparentprev\"just a re-write\" reply mirekrusin 14 hours agorootparent...the idea is that at some point it \"just re-writes\" itself. reply ametrau 8 hours agorootparentThe day after that, we have true AGI. reply jvanderbot 14 hours agoprevSo, I'd love to work on optimizing pipelines like this. How does one \"get into\" it? It seems a ML scientist with some C/C++ and infra knowledge just dips down into the system when required? Or is it CUDA/SIMD experts who move \"up\" into ML? reply thegginthesky 10 hours agoparentI know someone who works on this in Meta. His resume is computer science heavy, with a masters in Machine Learning. On the previous experience side, before getting into Meta, he had about a decade working as a Software Engineer with Machine Learning system in multiple languages, such as Go, C++ and Python. To get the job he applied for a spot I'm Software Engineer applied in Machine Learning, he went through the multiple step interview process, and then when he got the job he did a few weeks of training and interviewing teams. One of the teams in charge of optimizing ML code in Meta picked him up and now he works there. Because of Meta's scale, optimizing code that saves a few ms or watts is a huge impact in the bottom line. In sum: - Get a formal education in the area - Get work experience somewhere - Apply for a big tech job in Software Engineer applied with ML - Hope they hire you and have a spot in one of the teams in charge of optimizing stuff reply jvanderbot 9 hours agorootparentThis is helpful thank you. There's always some luck. I have a PhD in CS, and lots of experience in optimization and some in throughput/speedups (in an amdahl sense) for planning problems. My biggest challenge is really getting something meaty with high constraints or large compute requirements. By the time I get a pipeline set up it's good enough and we move on. So it's tough to build up that skillset to get in the door where the big problems are. reply KaiserPro 13 hours agoparentprevA lot of the optimisation at this level is getting data into the right place at the right time, without killing the network. Its also a group effort to provide simple to use primitives that \"normal\" ML people can use, even if they've never used hyper scale clusters before. So you need a good scheduler, that understand dependencies (no, the k8s scheduler(s) are shit for this, plus it wont scale past 1k nodes without eating all of your network bandwidth), then you need a dataloader that can provide the dataset access, then you need the IPC that allows sharing/joining of GPUs together. all of that needs to be wrapped up into a python interface that fairly simple to use. Oh and it needs to be secure, pass an FCC audit (ie you need to prove that no user data is being used) have a high utilisation efficiency and uptime. the model stuff is the cherry on the top reply claytonjy 10 hours agorootparentcan you say more about the network issues with thousands of k8s nodes? I'm regularly running 2-3000 nodes in a GKE cluster, majority have GPUs, is this something I need to be worrying about? reply jvanderbot 12 hours agorootparentprevOk, but back to my main question, how do I get into this? reply willsmith72 12 hours agorootparentIt looks more like an infra problem than ML. \"Software architect\"s mixed with devops/infra/sre people reply jvanderbot 12 hours agorootparentWell since I'm not a ML engineer of any kind - that's good! reply zooq_ai 11 hours agorootparentat the end of the day, you are still moving, storing and manipulating 1's and 0's, whether you are a front end engineer or a backend engineer or systems engieer or an ML engineer or an infra engineer reply elbear 3 hours agorootparentyeah, but how do you get the hiring managers to see things in the same way? :) reply chillee 8 hours agoparentprevI work on PyTorch Compilers at Meta, and I think folks enter ML Systems from all directions :) Some folks start with more familiarity in ML research and dip down as far as they need. Other folks come from a traditional distributed systems/compilers/HPC background, and apply those skills to ML systems. reply gajjanag 9 hours agoparentprevOur group works on some of this stuff at Meta, and we have a pretty good diversity of backgrounds - high performance computing (the bulk), computer systems, compilers, ML engineers, etc. We are hiring. Feel free to DM me to learn more. reply jvanderbot 9 hours agorootparentI will, thank you. Any info is very helpful. reply yalok 9 hours agoparentprevstart with something small - take some kernel function in C, and try to optimize it for your laptops assembly SIMD instruction set. reply fuddle 16 hours agoprevHow much are they paying for H100's? If they are paying $10k: 350,000 NVIDIA H100 x $10k = $3.5b reply trsohmers 15 hours agoparentSignificantly more than that; MFN pricing for NVIDIA DGX H100 (which has been getting priority supply allocation, so many have been suckered into buying them in order to get fast delivery) is ~$309k, while a basically equivalent HGX H100 system is ~$250k, coming to a price per GPU at the full server level being ~$31.5k. With Meta’s custom OCP systems integrating the SXM baseboards from NVIDIA, my guess is that their cost per GPU would be in the ~$23-$25k range. reply fuddle 14 hours agorootparent350,000 NVIDIA H100 x $23k = $8b :0 reply verticalscaler 14 hours agorootparentWait till you find out how much they spent on VR. It is a real loophole in the economy. If you're a trillion dollar company the market will insist you set such sums on fire just to be in the race for $current-hype. If they do it drives their market cap higher still and if they don't they risk being considered un-innovative and therefore doomed to irrelevancy and the market cap will spiral downwards. Sort of reminds me of The Producers. reply oblio 14 hours agorootparentThe thing is, this could be considered basic research, right? Basic research IS setting money on fire until (and if) that basic research turns into TCP/IP, Ethernet and the Internet. reply verticalscaler 14 hours agorootparentI wish. Funnily enough Arpanet and all that Xerox stuff were likeFunnily enough Arpanet and all that Xerox stuff were likeIf you're a trillion dollar company the market will insist you set such sums on fire just to be in the race for $current-hype. If they do it drives their market cap higher still and if they don't they risk being considered un-innovative and therefore doomed to irrelevancy and the market cap will spiral downwards. You don’t think earning increasing amounts of tens of billions of dollars in net income per year at some of the highest profit margins in the world at that size for 10+ years has anything to do with market cap? reply verticalscaler 9 hours agorootparent$1T Market Cap lets it be known it will invest $10B a year into $current-hype that will change everything. P/E loosens speculatively on sudden new unbounded potential, Market Cap $1.1T. Hype funded. PR as innovator cemented. reply throwaway2037 4 hours agorootparentIf you look at the R&D expenditure of Apple, it is mindboggling. https://www.macrotrends.net/stocks/charts/AAPL/apple/researc... Roughly 30B USD per year. And what are we getting? Slightly slimmer phones and 3500USD AR/VR headsets? reply throwaway2037 3 hours agorootparentprev> Market Cap $1.1T. Hype funded. I'm confused. How does your stock price, which determines market cat, affect your cashflow to fund R&D? It does not. reply bigcat12345678 5 hours agorootparentprevWould you kindly provide sources to the numbers? What is MFN? Thanks! (Your number is consistent with what I hear of, but I never managed to get solid sources to back them up) reply vineyardmike 13 hours agoparentprevIt’s often forgotten now, but just a few years NVidia was cancelled production batches and writing down inventory when the GPU shortage cleared. No one needed more GPUs. It also happens to be when Meta first announced they were going to increase CapEx spending on compute. I’m guessing that Meta got a sweetheart deal to help take a lot of inventory for NVidia and make commitments for future purchases. reply transcriptase 10 hours agorootparentI don’t think it was that nobody needed GPUs. It was that nvidia thought they could get scalper margins by restricting supply after the shortage showed people were willing to pay scalper prices. reply dekhn 15 hours agoparentprevThat sounds like a reasonable budget for 3 years of hardware at a major AI company. reply ZiiS 16 hours agoparentprevThey may have to pay a premium to secure ~¼ of the output; certainly unlikely to be that steep a discount. reply theptip 14 hours agorootparentSemi analysis posted recently noting that Meta locked in these purchases a while ago; something like a year or more. So they probably didn’t pay today’s spot rate. reply YetAnotherNick 16 hours agoparentprev> $3.5b Which is a fourth of what they spent in VR/AR in a year. And Gen AI is something they could easily get more revenue as it has now become proven technology, and Meta could possibly leapfrog others because of the data moat. reply dougb5 15 hours agorootparentProven technology, maybe, but proven product-market fit for the kinds of things Facebook is using it for? Their linked blog about AI features gives examples \"AI stickers\" and image editing... cool, but are these potential multi-billion dollar lifts to their existing business? I guess I'm skeptical it's worthwhile unless they're able to unseat ChatGPT with a market-leading general purpose assistant. reply pests 14 hours agorootparentI have a few group chats just that devolve into hours of sending stickers or image generation back and forth, lately we've been \"writing a book together\" with @Meta AI as the ghost writer, and while it utterly sucks, its been a hilarious shared experience. I don't think anyone else has gotten that group chat with AI thing so nailed. reply TaylorAlexander 14 hours agorootparentOn the podcast TrashFuture, November Kelly recently described AI systems as “garbage dispensers” which is both a funny image (why would anyone make a garbage dispenser??) and an apt description. Certainly these tools have some utility, but there are a load of startups claiming to “democratize creativity” by allowing anyone to publish AI generated slop to major platforms. On the podcast this phrase was used during discussion of a website which lets you create AI generated music and push it to Spotify, a move which Spotify originally pushed back on but has now embraced. Garbage dispenser indeed. reply YetAnotherNick 13 hours agorootparentprev> unseat ChatGPT with a market-leading general purpose assistant. It's not impossible. The prediction from many(not that I believe it) is that over long run modelling tricks would become common knowledge and only thing that matters is compute and data, both of which Meta has. Also there could be a trend of LLMs for ads or feed recommendation in the future as they has large completely unstructured dataset per user across multiple sites. reply cj 12 hours agorootparentCompute, data, and most importantly distribution/users. IMO standalone AI companies like OpenAI might be successful by providing infrastructure to other companies, but I can’t imagine ChatGPT remaining #1 many years from now. The web is still trending towards being a walled garden. Maybe not right now, but long term I think people will use whatever AI is most convenient which probably will be AI built into a giant company with established user base (FB, GOOG, MSFT, and Apple if they ever get around to launching - would love Siri 2.0 if it meant not needing to open the ChatGPT iOS app) reply NBJack 16 hours agorootparentprevWhat moat exactly? Much of the user data they have access to is drying up due to new regulations, some of which prohibit IIRC direct use on models as well. I'm not even sure they can use historical data. Meta certainly has an edge in engineer count, undoubtedly. But I'd say they really, really want the metaverse to succeed more to have their on walled garden (i.e. equivalent power of Apple and Google stores, etc.). There's a reason they gave a hard pass to a Google partnership. reply Dr_Birdbrain 15 hours agorootparentI think the raw text inside Facebook groups is at least as valuable as Reddit data. Even if demographics data is restricted under European law, the raw text of people interacting is quite valuable. reply verticalscaler 14 hours agorootparentIndeed, my deranged auntie posting on FB is approximately as valuable as my ADHD/PTSD quaranteeny nephew redditing. reply fragmede 5 hours agorootparentThat ignores all the user groups that are on Facebook. From apartment communities aka Nextdoor to grief support counseling to the mindfulness therapy groups, there’s a wealth of user comments a tad bit higher than Uncle John’s racist rants. reply calvinmorrison 15 hours agorootparentprevfacebooks downfall will be their lock in. every other social media platform lets you view a public profile, discussion groups etc. it's all locked inside facebook. reply agar 9 hours agorootparentprev> There's a reason they gave a hard pass to a Google partnership. AIUI, Google required Meta to basically cede control of a partnered OS to them: \"After years of not focusing on VR or doing anything to support our work in the space, Google has been pitching AndroidXR to partners and suggesting, incredibly, that WE are the ones threatening to fragment the ecosystem when they are the ones who plan to do exactly that. \"We would love to partner with them. They could bring their apps to Quest today! They could bring the Play store (with its current economics for 2d apps) and add value to all their developers immediately, which is exactly the kind of open app ecosystem we want to see. We would be thrilled to have them. It would be a win for their developers and all consumers and we’ll keep pushing for it. \"Instead, they want us to agree to restrictive terms that require us to give up our freedom to innovate and build better experiences for people and developers—we’ve seen this play out before and we think we can do better this time around.\" -- From Mark Bosworth reply YetAnotherNick 14 hours agorootparentprev> Much of the user data they have access to is drying up due to new regulations, some of which prohibit IIRC direct use on models as well. Source would be appreciated, because this is opposite of obvious. Regulations against using public first party would be a big news and I haven't heard of anything like that. They use my data for recommending feed so why not for answering my question? reply loeg 13 hours agoparentprevYes, billions in GPU cap ex. reply benreesman 12 hours agoprevI think it’s always useful to pay attention to the history on stuff like this and it’s a rare pleasure to be able to give some pointers in the literature along with some color to those interested from first-hand experience. I’d point the interested at the DLRM paper [1]: that was just after I left and I’m sad I missed it. FB got into disagg racks and SDN and stuff fairly early, and we already had half-U dual-socket SKUs with the SSD and (increasingly) even DRAM elsewhere in the rack in 2018, but we were doing huge NNs for recommenders and rankers even for then. I don’t know if this is considered proprietary so I’ll play it safe and just say that a click-prediction model on IG Stories in 2018 was on the order of a modest but real LLM today (at FP32!). The crazy part is they were HOGWILD trained on Intel AVX-2, which is just wild to think about. When I was screwing around with CUDA kernels we were time sharing NVIDIA dev boxes, typically 2-4 people doing CUDA were splitting up a single card as late as maybe 2016. I was managing what was called “IGML Infra” when I left and was on a first-name basis with the next-gen hardware people and any NVIDIA deal was still so closely guarded I didn’t hear more than rumors about GPUs for training let alone inference. 350k Hopper this year, Jesus. Say what you want about Meta but don’t say they can’t pour concrete and design SKUs on a dime: best damned infrastructure folks in the game pound-for-pound to this day. The talk by Thomas “tnb” Bredillet in particular I’d recommend: one of the finest hackers, mathematicians, and humans I’ve ever had the pleasure to know. [1] https://arxiv.org/pdf/1906.00091.pdf [2] https://arxiv.org/pdf/2108.09373.pdf [3] https://engineering.fb.com/2022/10/18/open-source/ocp-summit... [4] https://youtu.be/lQlIwWVlPGo?si=rRbRUAXX7aM0UcVO reply zone411 16 hours agoprevMeta is still playing catch-up. Might be hard to believe but according to Reuters they've been trying to run AI workloads mostly on CPUs until 2022 and they had to pull the plug on the first iteration of their AI chip. https://www.reuters.com/technology/inside-metas-scramble-cat... reply axpy906 11 hours agoparentDefinitely has some pr buzz and flex in the article. Now I see why. reply DEDLINE 17 hours agoprevI wonder if Meta would ever try to compete with AWS / MSFT / GOOG for AI workloads reply lifeisstillgood 16 hours agoparentFB does not have the flywheel of running data centres - all three of those mentioned run hyper scale datacentres that they can then juice by “investing” billions in AI companies who then turn around and put those billions as revenue in the investors OpenAI takes money from MSFT and buys Azure services Anthropic takes Amazon money and buys AWS services (as do many robotics etc) I am fairly sure it’s not illegal but it’s definitely low quality revenue reply miohtama 11 hours agorootparentSuch barter deals were also popular during the 00s Internet Bubble. Here more on the deals (2003): https://www.cnet.com/tech/services-and-software/aol-saga-ope... Popular names included AOL, Cisco, Yahoo, etc. Not sure if Amazon’s term sheets driving high valuation are nothing but AWS credits (Amazon’s own license to print money). reply woah 15 hours agorootparentprevSounds like it's free equity at the very least reply lotsofpulp 11 hours agorootparentHow is it free equity? Spending money to invest it somewhere involves risks. You might recover some of it if the investment is valued by others, but there is no guarantee. reply miohtama 11 hours agorootparentYou do not need cash in hands to invest. Instead, you print your own money (AWS credit) and use that to drive up the valuation, because this money costs you nothing today. It might cost tomorrow though, when the company starts to use your services. However depending the deal structure they might not use all the credit, go belly up before credit is used or bought up by someone with real cash. reply vineyardmike 13 hours agorootparentprevNVidia also invests in their AI customers. reply fikama 2 hours agorootparentWhat do you mean? Could you elaborate please? Enumerate some deals so I could read more about it? reply itslennysfault 13 hours agorootparentprevNeither did AWS when they started. They were just building out data centers to run their little book website and decided to start selling the excess capacity. Meta could absolutely do the same, but in the short term, I think they find using that capacity more valuable than selling it. reply otterley 12 hours agorootparent> Neither did AWS when they started. They were just building out data centers to run their little book website and decided to start selling the excess capacity. This is a myth. It simply isn't true. AWS was conceived as a greenfield business by its first CEO. Besides, S3 and SQS were the first AWS services; EC2 didn't appear till a few years later. And it wasn't built from excess Amazon server capacity; it was totally separate. reply virtuallynathan 13 hours agorootparentprevFacebook has more datacenter space and power than Amazon, Google, and Microsoft -- possibly more than Amazon and Microsoft combined... reply jedberg 13 hours agorootparentUnless you've worked at Amazon, Microsoft, Google, and Facebook, or a whole bunch of datacenter providers, I'm not sure how you could make that claim. They don't really share that information freely, even in their stock reports. Heck I worked at Amazon and even then I couldn't tell you the total datacenter space, they don't even share it internally. reply virtuallynathan 11 hours agorootparentYou can just map them all... I have. I also worked at AWS :) reply chatmasta 9 hours agorootparentThis would be an interesting dataset to use for trading decisions (or sell to hedge funds). But I wonder how much of their infrastructure is publicly mappable, compared to just the part of it that's exposed to the edge. (Can you map some internal instances in a VPC?) That said, I'm sure there are a lot of side channels in the provisioning APIs, certificate logs, and other metadata that could paint a decently accurate picture of cloud sizes. It might not cover everything but it'd be good enough to track and measure a gradual expansion of capacity. reply the-rc 8 hours agorootparentprevMapping as in.. drawing the outlines of buildings and computing the square footage yourself? reply virtuallynathan 11 hours agorootparentprevTo date, facebook has built, or is building, 47,100,000 sq ft of space, totaling nearly $24bn in investment. Based on available/disclosed power numbers and extrapolating per sqft, I get something like 4770MW. Last I updated my spreadsheet in 2019, Google had $17bn in investments across their datacenters, totaling 13,260,000 sq ft of datacenter space. Additional buildings have been built since then, but not to the scale of an additional 30mil sq ft. Amazon operates ~80 datacenter buildings in Northern Virginia, each ~200,000 sq ft -- about 16,000,000sq ft total in that region, the other regions are much much smaller, perhaps another 4 mil sq ft. When I'm bored I'll go update all my maps and spreadsheets. reply the-rc 8 hours agorootparentDoes the square footage take into account multiple floors? What's the source? It can be misleading, because you don't know the compute density of what's inside. Using just public data, power is a more accurate proxy. Until at least 5-6 years ago, Google was procuring more electricity than Amazon. Before that, it had a further advantage from lower PUE, but I bet the big names are all comparable on that front by now. Anyone that has worked at several of them can infer that FB is not the largest (but it's still huge). As for the dollars, were they just in 2019 or cumulative? The Google ones seem low compared to numbers from earnings. reply samstave 10 hours agorootparentprevAt this point Power Companies (ala PG&E, etc) should be investing in AI companies in a big way. THen they make money off the AI companies to build out power infra - and vice versa. I am surprised we havent heard about private electrical grid built out by such companies. Surely they all have some owned power generation, but then if they do, the local areas where they DO build out power plants - they should have to build capacity for the local area, mayhaps in exchange for the normal tax subsidies they seek for all these large capital projects. Cant wait until we pods/clusters in orbit. With radioisotope batteries to power them along with the panels. (I wonder how close to a node a RI battery can be? Can each node have its own RI?) (sas they can produce upto \"several KW\" -- but I cant find a reliable source for max wattage of an RI...) SpaceX should build an ISS module thats an AI DC cluster. And have all the ISS technologies build its LLM there based on all the data they create? reply VirusNewbie 10 hours agorootparentprevBut Google built data centers aren't the only data centers google is running their machine fleet in... reply chatmasta 9 hours agorootparentYeah, Google buys servers in public datacenters like those from Equinix. One \"region\" needn't be one datacenter, and sometimes AWS and GCP will even have computers in the same facility. It's actually quite annoying that \"region\" is such an opaque construct and they don't have any clear way to identify what physical building is hosting the hardware you rent from them. reply the-rc 8 hours agorootparentThose are almost lost in the noise, compared to the big datacenters. (I've been inside two Atlanta facilities, one leased and one built from scratch, and the old Savvis one in Sunnyvale). reply karmasimida 13 hours agorootparentprevI don't think so, AWS hasn't disclosed this numbers, like datacenter spaces occupied, so how do you know. reply virtuallynathan 11 hours agorootparentI have mapped every AWS data center globally, and I worked at AWS. Facebook publishes this data. reply pgwhalen 12 hours agorootparentprevI have zero evidence, but this seems extremely unlikely. Do you have more than zero evidence? reply meiraleal 11 hours agorootparentMeta can use all their datacenter space while Amazon, Google, and Microsoft datacenter space is mostly rented. reply dsp 13 hours agorootparentprev[citation needed] reply rthnbgrredf 16 hours agoparentprevMeta could build their own cloud offering. But it would take years to match the current existing offerings of AWS, Azure and GCP in terms of scale and wide range of cloud solutions. reply Cthulhu_ 14 hours agorootparentAnd then there's sales. All of those three - and more you haven't considered, like the Chinese mega-IT companies - spend huge amounts on training, partnerships, consultancy, etc to get companies to use their services instead of their competitors. My current employer seems all-in on Azure, previous one was AWS. There was one manager who worked at two large Dutch companies and sold AWS to them, as in, moving their entire IT, workloads and servers over to AWS. I wouldn't be surprised if there was a deal made there somewhere. reply oblio 14 hours agorootparentprevThe real question is: why aren't they? They had the infrastructure needed to seed a cloud offering 10 years ago. Heck, if Oracle managed to be in 5th (6th? 7th?) place, Facebook for sure could have been a top 5 contender, at least. reply krschultz 3 hours agorootparentBecause they make more money using their servers for their own products than they would renting them to other people. Meta has an operating margin of 41% AFTER they burn a ton on Reality Labs, while AWS has a 21% margin with more disciplined spending. Social media is a more profitable business than infrastructure. reply elbear 3 hours agorootparentDoes Meta make money from anything other than ads? It's not a dismissive question. I'm curious if social media implies anything other than ads. reply Thaxll 9 hours agorootparentprevBecause it's not their business, they're not good at it and probably the ROI is not worth it. Also how exactly they would do it, they don't have enough infra for renting, they would need to x10 what they have now. reply KaiserPro 14 hours agorootparentprevbecause meta sucks at software, documentation and making sure end user products work in a supported way. Offering reliable IaaS is super hard and capital intensive. Its also not profitable if you are perceived as shit. reply logicchains 13 hours agorootparent>because meta sucks at software Google started a cloud and their user-facing software is atrocious. Compared e.g. Angular to React, Tensorflow to Pytorch. reply negus 10 hours agorootparentWhy would you prefer Pytorch to Tensorflow/Keras? reply chessgecko 10 hours agorootparentTensorflow and keras have gotten better, but pytorch historically had better flexibility than keras and was much easier to debug/develop in than tensorflow. reply bionhoward 15 hours agorootparentprevaww, those existing offerings are overcomplicated as hell, a fresh look could yield substantially simpler cloud developer experience and this would compete well against those other cloud offerings on simplicity alone reply redleader55 13 hours agoparentprevFor consumers, AI could just be stateless \"micro service\". Meta already has enough surfaces where customers can interact with AI. reply crowcroft 15 hours agoparentprevI think Meta have avoided doing this because it would complicate their business priorities. They don’t really do B2B. reply carlossouza 13 hours agorootparentWhat do you mean by “they don’t do B2B”? They sell ads to companies, don’t they? reply elwell 16 hours agoprev> Meta’s long-term vision is to build artificial general intelligence (AGI) reply valzam 9 hours agoparentDon't worry, this goal will change with the next hype cycle reply latchkey 8 hours agorootparentI pity the fools that think AI is just another internet hype cycle. reply brookst 6 hours agorootparentI’m old enough to remember the proud, defiant declarations that the internet was just a hype cycle. reply bennyelv 3 hours agorootparentWell it was wasn’t it? There was a massive boom where loads of companies over promised what they would achieve, followed by a crash when everyone realised lots of them couldn’t, followed by stability for the smaller number that could. It was the very definition of a hype cycle as far as I can see. Hype cycle doesn’t mean “useless and will go away”, you have the second upward curve and then productivity. https://en.m.wikipedia.org/wiki/Gartner_hype_cycle reply latchkey 6 hours agorootparentprevI got my first email in 1991 and started my first internet business in 1995 (a web dev shop). My entire life has been an endless hype cycle. reply mjburgess 17 hours agoprevI'd be great if they could invest in an alternative to nvidia -- then, in one fell swoop, destroy the moats of everyone in the industry. reply math_dandy 17 hours agoparentA company moving away from Nvidia/CUDA while the field is developing so rapidly would result in that company falling behind. When (if) the rate of progress in the AI space slows, then perhaps the big players will have the breathing room to consider rethinking foundational components of their infrastructure. But even at that point, their massive investment in Nvidia will likely render this impractical. Nvidia decisively won the AI hardware lottery, and that's why it's worth trillions. reply whiplash451 16 hours agorootparentPeople said the same thing when tensorflow was all the rage and pytorch was a side project. Granted, HW is much harder than SW, but I would not discount Meta's ability to displace NVIDIA entirely. reply Cthulhu_ 14 hours agorootparentI don't think they could; nvidia has tons of talent, Meta would have to steal that. Meta doesn't do anything in either consumer or datacenter hardware that isn't for themselves either. Meta is a services company, their hardware is secondary and for their own usage. reply Wazako 11 hours agorootparentmeta has the Quest. It's not so bad that they're looking to create an LPU for their headset to offer local play. reply mjburgess 16 hours agorootparentprevI'm more concerned to avoid nvidia (et al.) market domination, than chasing the top-edge of the genAI benefits sigmoid. This will prevent much broad-based innovation. reply hx8 16 hours agorootparentThis space is so compeitive, even if Nvidia is asleep at the wheel a competitor will come and push them before too long. AMD has a history of noticing when their competitors are going soft and rapidly being compeitive. reply paxys 17 hours agoparentprevExcept that \"one fell swoop\" would realistically be 20+ years of research and development from the top minds in the semiconductor industry. reply logicchains 13 hours agorootparentIt's not the hardware keeping NVidia ahead, it's the software. Hardware-wise AMD is competitive with NVidia, but their lack of a competitive CUDA alternative is hurting adoption. reply brucethemoose2 16 hours agoparentprevFacebook very specifically bought and customized Intel SKUs tailored for AI workloads for some time. reply John23832 17 hours agoparentprevhttps://engineering.fb.com/2023/10/18/ml-applications/meta-a... reply aeyes 17 hours agoparentprevIsn't Google trying to do this with their TPUs? reply crakenzak 16 hours agorootparentI still, for the life of me, can't understand why Google doesn't just start selling their TPUs to everyone. Nvidia wouldn't be anywhere near their size if they only made H100s available through their DGX cloud, which is what Google is doing only making TPUs available through Google Cloud. Good hardware, good software support, and market is starving for performant competitors to the H100s (and soon B100s). Would sell like hotcakes. reply aseipp 15 hours agorootparentIt is an absolutely massive amount of work to turn something designed for your custom software stack and data centers (custom rack designs, water cooling, etc) into a COTS product that is plug-and-play; not just technically but also things like sales, support, etc. You are introducing a massive amount of new problems to solve and pay for. And the in-house designs like TPUs (or Meta's accelerators) are cost effective in part because they don't do that stuff at all. They would not be as cheap per unit of work if they had to also pay off all that other stuff. They also have had a very strong demand for TPUs internally which takes priority over GCP. reply dekhn 12 hours agorootparentprevDo you mean, sell TPU hardware to other companies that would run it in their data centers? I can't imagine that would ever really work. The only reason TPUs work at Google is because they have huge teams across many different areas to keep them running (SRE, hardware repair, SWE, hardware infra) and it's coupled to the design of the data centers. To vend and externalize the software would require google to setup similar teams for external customers (well beyond what Google Cloud provides for TPUs today) just to eke out some margin of profit. Plus, there is a whole proprietary stack running under the hood that google wouldn't want to share with potential competitors. Google used to sell a search appliance-in-a-box and eventually lost interest because hardware is so high-touch. reply aeyes 11 hours agorootparent> Google used to sell a search appliance-in-a-box and eventually lost interest because hardware is so high-touch. We had a GSA for intranet search and other than the paint this was a standard Dell server. I remember not being impressed by what the GSA could do. We also had Google Urchin for web analytics, it wasn't a hardware appliance but the product wasn't very impressive either. They then killed that and tried to get you onto Google Analytics. They just didn't commit to these on premise enterprise products. reply dekhn 10 hours agorootparentThe server may have been dell, but it included a full stack of google3 software including chubby the lockserver. We had one at my company and it was widely loved- far better intranet search and domain-specific search for biotech. reply ajcp 16 hours agorootparentprevAnd undercut what they'd like to use as a huge motivator in people moving to GCP? Not likely. Even if they wanted to they can't keep up with their own internal demand. Beyond that they might not be as stable or resilient outside of the closely curated confines of their own data-centers. In that case selling them would be more of an embarrassment. reply htrp 15 hours agorootparent>Beyond that they might not be as stable or resilient outside of the closely curated confines of their own data-centers. In that case selling them would be more of an embarrassment. Once you go out of your heavily curated hardware stack, the headaches multiply exponentially. reply qiine 16 hours agorootparentprevMaybe selling hardware to customers worldwide + support like Nvidia does is actually not trivial ? reply neuronexmachina 15 hours agorootparentprevThe impression I got from this thread yesterday is that Google's having difficulty keeping up with the heavy internal demand for TPUs: https://news.ycombinator.com/item?id=39670121 reply spencerchubb 8 hours agoprevAll this compute and my Instagram Reels feed still isn't as good as my TikTok feed reply zeroonetwothree 7 hours agoparentWhat does that have to do with Gen AI reply lmm 6 hours agorootparentIf Gen AI doesn't have anything to do with \"Meta\"'s actual business then WTF are they setting all this money on fire for? reply spencerchubb 7 hours agorootparentprevGenAI infra is the same as regular AI infra. They used GenAI in the title because it's a buzzword. reply ipsum2 7 hours agorootparentNot really. Ranking and recommendation models require different infrastructure than LLMs. The models are generally smaller and require more data processing before training. reply refulgentis 7 hours agorootparentprevYeah, no. reply gingergoat 16 hours agoprevThe article doesn't mention MTIA, meta's custom ASIC for training & inference acceleration. https://ai.meta.com/blog/meta-training-inference-accelerator... I wonder if they will use it in RSC. reply wseqyrku 15 hours agoprev> Commitment to open AI innovation I see what you did there, Meta. reply owenpalmer 14 hours agoparentHaha, I noticed that too xD reply latchkey 16 hours agoprev> we have successfully used both RoCE and InfiniBand clusters for large, GenAI workloads (including our ongoing training of Llama 3 on our RoCE cluster) without any network bottlenecks. Interesting dig on IB. RoCE is the right solution since it is open standards and more importantly, available without a 52+ week lead time. reply loeg 12 hours agoparentYeah, and RoCE isn't single vendor. I'm not sure IB scales to the relevant cluster sizes, either. reply anonymousDan 11 hours agorootparentIs NVLink just not scalable enough here? reply loeg 11 hours agorootparentI don't know. I haven't actually worked with IB in this specific space (or since before Nvidia acquired MLNX). My experience with RoCE/IB was for storage cluster backend in the late 2010s. reply seydor 14 hours agoprevThis is great news for Nvidia and their stock, but are they sure the LLMs and image models will scale indefinitely? nature and biology has a preference for sigmoids. What if we find out that AGI requries different kinds of cpu capabilities reply jiggawatts 11 hours agoparentIf anything, NVIDIA H100 GPUs are too general purpose! The optimal compute for AI training would be more specialised, but then would be efficient at only one NN architecture. Until we know what the best architecture is, the general purpose clusters remain a good strategy. reply alexsereno 17 hours agoprevHonestly Meta is consistently one of the better companies at releasing tech stack info or just open sourcing, these kinds of articles are super fun reply rshm 17 hours agoparentI think some elements of this stack might flow into the open compute. reply adamnemecek 17 hours agoparentprevDo you find this informative? reply alexsereno 15 hours agorootparentYes of course - it depends on what lens though. If you mean \"I'm learning to build better from this\" then no, but its very informative on Meta's own goals and mindset as well as real numbers that allow comparison to investment in other areas, etc. Also the point was mostly that Meta does publish a lot in the open - including actual open source tech stacks etc. They're reasonably good actors in this specific domain. reply pinko 14 hours agoprevThe link mentions \"our internal job scheduler\" and how they had to optimize it for this work -- does anyone know what this job scheduler is called, or how it works? reply KaiserPro 13 hours agoparentit might be twine: https://www.usenix.org/system/files/osdi20-tang.pdf but I suspect its not that, because Twine is optimised for services rather than batch processing, and doesn't really have the concept of priorities. reply radicality 11 hours agorootparentI would think it’s probably that. Also, has this been renamed to Twine from Tupperware? reply mrkramer 14 hours agoprev\"Share this: Hacker News\" Noice reply BonoboIO 14 hours agoparentI thought at first \"what are you talking about\", when i check my uBlock filters. Was blocking the whole \"Share this\" content section. Sharing on Hacker News ... they now their audience. reply mrkramer 14 hours agorootparentI also use uBlock but my filters are the default ones and I saw it without any problem but tbh this is the first time that I saw some post on the Web have HN as a share option or the first time that I was surprised seeing it. Maybe it has something to do with Google ranking \"trusted human information and knowledge\" higher than \"non-human\" information and knowledge[0] or simply some Meta software engineer loves and uses HN so s/he decided to include HN as well, idk. [0] https://news.ycombinator.com/item?id=39423949 reply sashank_1509 7 hours agoprevMetas backing itself into a corner with its admirable commitment to open source. Unfortunately, at some point when they decide to monetize their billions spent and try to release a closed source model, the level of vitriol they will deal with will be an order of magnitude above what even OpenAI is experiencing. I don’t think they realize that! reply bigcat12345678 7 hours agoparentNo Meta's commitment to Open Source is well under calculation. OCP is a way to rally lower-tier vendors to form a semi-alliance to keep up with super-gorilla like AWS & Google. LLaMA has already gained much more than its cost (look at the stock price, and the open source ecosystem built surrounding LLaMA, and Google's open source Gemma models which is a proof of Meta's success). IMHO, Meta's Open Source strategy already covered at least 5 years in prospect. That's enough to finesse a 180 degree turn around if necessary (i.e., from open source to close source) reply Horffupolde 7 hours agoparentprevThe general public doesn’t care. Only developers. reply zerop 16 hours agoprev> At Meta, we handle hundreds of trillions of AI model executions per day Such a large number, makes sense? reply GeneralMayhem 16 hours agoparentSure. 100T/day * 1day/86400sec ~= 1B/sec. They're probably considering at least a few hundred candidates per impression, and every impression is going to go through _at least_ two models (relevance and pCTR/revenue), so you could get there just with online serving at 5Mqps, which is plausible. But they're also going to be doing a lot of stuff in batch - spam predictions, ad budget forecasts, etc - so that every candidate actually runs through four or five different models, and every actual impression could do more than that. reply sangnoir 15 hours agoparentprevHow many ads does Meta serve a day, and how many AI model executions are done for each one? Repeat the same for stories, post and comment recommendations on Facebook and Instagram, and you have very big numbers. To that, Add VR, internal modeling and other backoffice/ offline analyses over billions of users and you'll easily get into the trillions. reply dakiol 15 hours agoparentprevWhat's an \"AI model execution\"? When I ask something to ChatGPT and it answers to me, does that count as 1 \"AI model execution\" for OpenAI? reply pants2 16 hours agoparentprevPerhaps there's some combinatorics where every time an ad or post is displayed to the user, it runs through some hundreds/thousands of candidates and computes their relevance. reply hendersoon 16 hours agoprev350k H100 cards, around ten billion dollars just for the GPUs. Less if Nvidia gives a volume discount, which I imagine they do not. reply renegade-otter 16 hours agoparentIt will be ironic if Meta sinks all this money into the new trend and finds out later that it has been a huge boondoggle, just as publishers followed Facebook's \"guidance\" on video being the future, subsequently gutting the talent pool and investing into video production and staff - only to find out it was all a total waste. reply motoxpro 16 hours agorootparentIt already paid off. When the world moved from determinisic to probablistic ad modeling. That's why their numbers are so good right now compared to every other advertiser reply blitzar 11 hours agorootparentIt already paid off. FB stonk price is up lots. reply foobarian 16 hours agorootparentprevThere is still hope then for cheap gaming GPUs some day soon! I have pretty much the last 10 years of flagship releases to catch up on... reply tayo42 16 hours agorootparentprevWhat does video not be in the future mean? In social media tiktok and reels are everywhere? reply michaelt 16 hours agorootparentThere are reports [1] that a bunch of companies like \"College Humor\" were convinced to switch to producing native video for facebook (instead of directing users to their own sites) on the basis of bullshit metrics from facebook, and had an extremely bad time as a result, with some companies going bankrupt. Something like counting an autoplaying video that ran for 3 seconds as a 'view' IIRC [1] https://twitter.com/adamconover/status/1183209875859333120 reply scubbo 16 hours agorootparentThankfully, Dropout (a spin-off of College Humor) is alive and well, and producing some of the best D&D Actual Play series as well as other non-D&D comedy shows. One of the entertainment services that I happily pay for because I want to support what they're doing. reply neon_electro 16 hours agorootparentprevThey are referring to Facebook/Meta’s 2015 “pivot to video”, speculating there may be a similar thing happening more recently with AI. https://en.wikipedia.org/wiki/Pivot_to_video reply neuronexmachina 15 hours agorootparentTIL. Reading up on it a little, I'm surprised the class-action settlement was just $40M: https://www.videoadvertisingsettlement.com/ reply tayo42 15 hours agorootparentprevInteresting thanks! Feels like in hind sight, maybe they were just to early to it. reply echelon 16 hours agorootparentprevAs a practitioner in the field, I can assure you this is not a boondoggle. Those GPUs are going to subsume the entire music, film, and gaming industries. And that's just to start. reply __loam 16 hours agorootparent\"My paycheck depends on this technology destroying every field producing cultural artifacts\" reply echelon 15 hours agorootparentSaid the butter churner, cotton ginner, and petrol pumper. I work in film. I've shot dozens of them the old fashioned way. I've always hated how labor, time, and cost intensive they are to make. Despite instructions from the luminaries to \"just pick up a camera\", the entire process is stone age. The field is extremely inequitable, full of nepotism and \"who you know\". Almost every starry-eyed film student winds up doing drudge work for the rest of their lives. Most will never make a feature to match their ambition. If the whole task was to simply convey my thoughts and dreams to others, why am I scrambling around to sign location rights, capture photons on expensive glass, and then smear and splice things together for months on end? This is ceremonial and soon to be anachronistic. I'm glad that whole mess is going to be replaced. It's a farce. To phrase it another way - would you like to be hand-writing assembly on punch cards? To only gain entrance into the field with your mathematics PhD? To speak of the liberty and the economics, why should I have to sell the rights to my idea to a studio so I can get it off the ground? Why should I have to obey the studio's rules and mind their interference? This whole Gen AI thing is going to be the biggest liberating moment for filmmaking creatives. I know, because I am one. And if you think any Jack or Jill can just come in and text prompt a whole movie, you're crazy. It's still hard work and a metric ton of good taste. Art will never die. It's the human soul. It'll take more than some tech bros with GPUs to kill it. AI is just another tool for the artist. A \"bicycle for the mind\" to quote Jobs, and a rocket ship for the imagination to convey my own direct experience. reply lmm 6 hours agorootparent> Said the butter churner, cotton ginner, and petrol pumper. Said the bank teller, record producer, etc.. Plenty of cases where we've been told technology and automation would democratise the field and remove the middleman, and actually it's the opposite. Yes, it would be nice if AI made it easy for anyone who wanted to make a great movie. That doesn't mean it's going to happen. reply munificent 14 hours agorootparentprevCall me crazy, but I don't think churning butter and writing a novel are in the same category of human endeavor at all. reply wizzwizz4 15 hours agorootparentprev> And if you think any Jack or Jill can just come in and text prompt a whole movie, you're crazy. It's still hard work and a metric ton of good taste. If you want anything good, yes. If you just want something… I reckon it'd take a week to assemble an incomprehensible-nonsense-film pipeline, after which it's just a matter of feeding the computer electricity. Short-term, this is going to funnel resources away from the people with good taste. Long-term, it might help collapse the entire \"creative industry\", after which we might get some of that artist liberation stuff you're talking about – but we might just end up with new gatekeeping strategies from the wealthy and connected, and business as usual. reply echelon 15 hours agorootparent> If you want anything good, yes. If you just want something ... You don't even need AI for that. https://en.wikipedia.org/wiki/YouTube_poop https://en.wikipedia.org/wiki/Skibidi_Toilet The idea that AI isn't going to be used as a creative tool too and that it won't lead to more and better art is a defeatist, Luddite attitude. Similarly shaped people thought that digital cameras would ruin cinema and photography. > Short-term, this is going to funnel resources away from the people with good taste. On the contrary - every budding film student will soon [1] be able to execute on their entire visions straight out of the gates. No decades of clawing their way to a very limited, almost impossible to reach peak. > it might help collapse the entire \"creative industry\" The studio system. Not the industry. > new gatekeeping strategies from the wealthy and connected, and business as usual. Creatives have more ways of building brands and followings for themselves than ever before. It's one of the largest growing sectors of the economy, and lots of people are earning livings off of it. You'll be able to follow that steampunk vampire creator that's been missing from the world until now. Every long tail interest will be catered to. Even the most obscure and wild tastes, ideas, and designs. Stuff that would never get studio funding. As a creative, I'm overjoyed by this. My friends and I are getting to create things we never could make before [2]. [1] This and next year. [2] Just an inspiration / aesthetic sample, but we're making a full film: https://imgur.com/a/JNVnJIn reply crmd 13 hours agorootparent>You'll be able to follow that steampunk vampire creator that's been missing from the world until now. Every long tail interest will be catered to. Even the most obscure and wild tastes, ideas, and designs. Stuff that would never get studio funding. Your optimism reminds me of the optimism I had around the early internet. Power to the people, long tail, rise of the creative class, the fall of gatekeeping corporations, etc. It was like that for a couple of years in the late 90s before power and control got vastly more centralized than before. Maybe this time it’ll be different. reply munificent 12 hours agorootparentThe big difference is that back then, anyone with a consumer-level computer in their bedroom could turn it into a server and be a first-class citizen on the Internet. With generative AI, models will be controlled by a handful of giant corporations who have the enormous corpuses (of dubious provenance) and compute ability to train them. So it will be like last time, but even worse. reply echelon 12 hours agorootparentYou can run ComfyUI and AnimateDiff on your PC. If you haven't checked them out, please do. And there are other angles to consider. Apple, for one, is expressly interested in not becoming a thin client to cloud AI. They're baking a lot of inference power into their chips. If the creative class don't need their devices, that doesn't bode well for them... reply munificent 12 hours agorootparentRunning local models isn't the same as being able to train them from scratch yourself on a corpus of your own choosing. reply echelon 11 hours agorootparentThere are so many ways to do exactly this too! FakeYou, CivitAi, WeightsGg, Comflowy, ... -- there are tons of vibrant communities to teach you everything you need to know. The tools are open source, free to use, and accessible. This isn't hard at all once you dive in. reply MrScruff 2 hours agorootparentprevAre you talking about some as yet unseen research/technology? The aesthetic sample looks like something we could have seen on the SD subreddit for the last year. reply renegade-otter 14 hours agorootparentprev> Similarly shaped people thought that digital cameras would ruin cinema and photography. Obviously, but you seem to be arguing that AI is just another evolution of productivity tools. You still need to have a photographer's eye while using this technology. If you couldn't make a good composition on film, a digicam will not save you, and it definitely did not replace photographers. Perhaps lowered the barrier of entry for prosumers. https://www.nytimes.com/2023/12/26/opinion/ai-future-photogr... reply echelon 14 hours agorootparentWe're arguing the same point. :) reply wizzwizz4 13 hours agorootparentprevMany YouTube Poops are artistic expression (e.g. https://redirect.invidious.io/watch?v=dO4eIEvHjSw). Skibidi Toilet is definitely artistic expression: it's a full-on epic. (Reactions from one ≈50-year-old: “baffling” “how did they do that?” “why would anyone make this?”) If you think the Luddites were defeatist, you don't know much about the Luddites. > On the contrary - every budding film student will soon [1] be able to execute on their entire visions straight out of the gates. […] Creatives have more ways of building brands and followings for themselves than ever before. Yet, we have no shortage of starving artists. Will AI provide them food and shelter? This is unequivocally a win for creative expression for hobbyists, but it stands to harm professionals – at least in the short term, perhaps longer-term. It's not happening in a vacuum: the greedy are revoking livelihoods because they think AI can do it faster and cheaper (laundering appropriated hobbyist and increasingly-cheap professional labour). > The studio system. Not the industry. Huh, the word 'industry' has a specialised meaning in economics. Didn't know that. reply dist-epoch 12 hours agorootparentprev> The field is extremely inequitable, full of nepotism and \"who you know\" Maybe, but it's never been cheaper to make a movie. I know someone with no connections and (almost) no money which in 4 years made multiple no. 1 box-office films (obviously not in US, in a smaller country) and then got picked up by Netflix. reply sangnoir 15 hours agorootparentprev> And if you think any Jack or Jill can just come in and text prompt a whole movie, you're crazy. It's still hard work and a metric ton of good taste. Yeah, I cant wait for ChuChuTV to get the best film Oscar /s. reply ilaksh 15 hours agoprev\"Everything You Wanted to Know About GenAI at Meta, Except the One Thing You Honestly Care About\" (Llama 3). reply dekhn 15 hours agoprevit's really interesting just how similar these systems are to the designs adopted for HPC over the past few decades. I'm salty because it took a while for the ML community to converge on this (20+K GPUs connected by a real fabric with low latency and high bandwidth). reply marmaduke 17 hours agoprevJust for comparison, Swiss CSCS new Alps system will get 5k GH200 nodes (each with a H100). reply delanyoyoko 15 hours agoprevYou've got to read \"open\" roughly 3x in a paragraph. reply papichulo2023 14 hours agoparentIf they release models I dont care honestly, they can brag about that as much as they want. reply dazhbog 16 hours agoprevSearched H100 and an Amazon link popped up. Good reviews. https://www.amazon.com/Tesla-NVIDIA-Learning-Compute-Graphic... reply mejutoco 15 hours agoparentThose reviews are hilarious reply lvl102 17 hours agoprevThis reads more like a flex for the investment community. reply delegate 15 hours agoprevSubtitled 'Here's what you'll never be able to do'. reply froonly 16 hours agoprevlmfao at the Meta folks not giving any credit whatsoever to the company that actually came up with and implemented the infrastructure work. reply jfkfif 15 hours agoparentWhat’s the company? reply sangnoir 15 hours agorootparentFacebook. reply _zoltan_ 10 hours agorootparent??? reply pwb25 15 hours agoprevso tired of this, not everyone need to work with AI stuff. work on facebook that is a disaster page instead reply choppaface 17 hours agoprevTotal cluster they say will reach 350k H100, which at $30k street price is about $10b. In contrast, Microsoft is spending over $10b per quarter capex on cloud. That makes Zuck look conservative after his big loss on metaverse. https://www.datacenterdynamics.com/en/news/q3-2023-cloud-res... reply yuliyp 17 hours agoparentThat's a weird comparison. The GPU is only a part of the capex: there's the rest of the servers and racks, the networking, as well as the buildings/cooling systems to support that. reply KaiserPro 13 hours agoparentprevthe biggest cost at meta is infra. > In contrast, Microsoft is spending over $10b per quarter capex on cloud. to service other people's work load. Its a different business. reply baby 17 hours agoparentprevWhat loss lol. Stop the fud reply Legend2440 16 hours agorootparentHas literally anyone spent money on the metaverse? Maybe it'll still take off in the future, but it's a $40b loss so far. reply artninja1988 15 hours agorootparent>Has literally anyone spent money on the metaverse? I guess people buy their vr headsets, if that counts. I'm not too familiar with what the \"metaverse\" entails though... reply CuriouslyC 17 hours agoprevYann wants to be open and Mark seems happy to salt the earth. reply torginus 16 hours agoparentI genuinely think one of the most plausible short-term dangers of AI is the creation of lifelike bots which will be absolutely indistinguishable from real humans in short-form online interaction. Since people don't want to talk to algorithms, this would result in them shunning all social media, which is a huge danger to companies in the space. reply bananabrick 17 hours agoparentprevWhat do you mean? reply CuriouslyC 17 hours agorootparentIn pretty much every interview, Yann has talked about how important that AI infrastructure is open and distributed for the good of humanity, and how he wouldn't work for a company that wasn't open. Since Mark doesn't have an AI product to cannibalize, it's in his interest to devalue the AI products of others (\"salting the earth\"). reply Legend2440 16 hours agorootparentI don't see how they're devaluing other people's AI products. reply jedberg 12 hours agorootparentIt's called commoditize the compliment. If they make AI models free to use it makes OpenAI nearly valueless, which means that they can't survive and then sell Meta's competitors a better GenAI product than Meta can make themselves. So basically since they don't make money directly on GenAI, it makes sense for them to release it for free so no one else can have something better, so they don't have to compete on GenAI abilities with their competitors. reply crakenzak 16 hours agorootparentprevThe angle is that by releasing cutting edge AI research to the public openly, the relative difference between open source models/tech and closed source tech shrinks. Whether or not you think the \"value\" of AI products is proportional to their performance gap vs the next closest thing or not is up to you. Very interesting PG essay I read recently talks about the opposite of this (Superlinear returns) where if you're half as good as the next competitor, you don't get half the customers, you get 0. Essay: https://paulgraham.com/superlinear.html reply nova22033 15 hours agorootparentNew Linux versions don't \"salt the earth\" for Windows. reply dkarras 8 hours agorootparentWindows should always provide enough additional value that makes up for what they are asking as money - compared to the free option. That is the point. If you had no other viable options, then they could do whatever they like. Now they have a baseline to compete with and it is very hard to compete with free. reply nemothekid 14 hours agorootparentprevLinux is not competitive as a desktop platform for regular users, but linux did \"salt the earth\" for the Server market. reply conradev 16 hours agorootparentprevMistral makes comparable models to Facebook. Mistral charges money, Facebook does not. This negatively affect’s Mistral’s pricing power because a customer can get 70% of the performance they need for 0% of the cost. The “0% of the cost” part is unique to software businesses because you can copy software so cheaply reply CuriouslyC 16 hours agorootparentprevThe Llama models have played a large part in fostering the development of the open source LLM ecosystem, and I expect Llama3 to put in performance > mistral medium and anthropic haiku while being fully open and able to be run on consumer hardware. reply chasd00 16 hours agorootparentprevis \"salting the earth\", in the biblical sense of destroying your enemy and their land to the point where not even plants grow again, a SV term used for companies that promote open source? reply CuriouslyC 10 hours agorootparentIt's a term used for making a certain type of business unviable. In this case, high quality open models will make closed source models less viable, since the closed source model providers won't be able to charge monopoly prices for their models, but will have to approach the price of cloud GPU time or lose customers to equally capable open models. reply 2 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Meta has revealed the development of two 24k GPU clusters to advance its AI capabilities, focusing on enhancing hardware, network, storage, design, and performance for various AI tasks, including training AI models like Llama 3.",
      "These clusters align with Meta's goal of building artificial general intelligence (AGI) and showcase the company's dedication to open compute and open-source principles through their design on Grand Teton, OpenRack, and PyTorch.",
      "Meta plans to expand its infrastructure to accommodate 350,000 NVIDIA H100 GPUs by the end of 2024, emphasizing performance optimization, ease of use, and a commitment to responsible AI development through collaborations with academic and industry partners."
    ],
    "commentSummary": [
      "The discussion encompasses various aspects of Meta's GenAI infrastructure, such as technical specifics, optimization endeavors, AI industry hurdles, investment tactics, hardware progress, and AI applications in creative fields.",
      "It explores the potential influence of AI on different sectors, worries about job loss, and how AI could transform the creative workflow.",
      "The conversation also touches on Nvidia's market dominance in AI hardware, selling challenges of hardware like Google's TPUs, Meta's open-source dedication, and a potential switch to closed-source technology."
    ],
    "points": 574,
    "commentCount": 258,
    "retryCount": 0,
    "time": 1710258755
  },
  {
    "id": 39681920,
    "title": "NYPD Withdraws Warrantless Subpoena for Copwatcher's Twitter",
    "originLink": "https://hellgatenyc.com/nypd-warrantless-subpoena-copwatcher-social-media",
    "originBody": "The Cops The NYPD Sent a Warrantless Subpoena for a Copwatcher’s Social Media Account, but Won’t Defend It in Court What does the NYPD want with the social media data from a police accountability account? 4:26 PM EDT on March 11, 2024 NYPD Headquarters at One Police Plaza (ajay_suresh / Flickr) By Nick Pinto Share on Facebook Share on 𝕏 Share on Email 3Comments Join the Discussion The NYPD sent a sweeping subpoena seeking information from the social media account of the president of a New York City police accountability organization in February, records reviewed by Hell Gate show, only to withdraw its subpoena when told they would need to justify the subpoena in court. Michael Clancy, better known to friends on and off social media as Rabbi, received a notice last month from X, formerly known as Twitter, alerting him to the fact that the NYPD had sent X a subpoena requesting \"all records consisting but not limited to all subscriber name(s), Email address(s), Phone number(s), account creation date, IP logs with timestamps (IP address of account logins and logouts), all logs of previous messages sent and received.\" The subpoena also requested \"all videos sent and received, including but not limited to meta-data. exit data about the messages and videos” for the account. The notification included a copy of the subpoena, which warned X not to tell Clancy of its existence. \"You are not to disclose or notify any customer or third party of the existence of this subpoena or that records were provided pursuant to this subpoena,\" the document read. But X, following its own corporate policy, told Clancy anyway, and suggested he might want to get some legal representation to fight the subpoena, recommending the American Civil Liberties Union. Clancy did just that, and Kathryn Sachs of the New York Civil Liberties Union took up his case. Last Wednesday, Sachs wrote to the NYPD to challenge the administrative subpoena, which the NYPD had sent on its own authority, without any warrant or judicial approval. If the NYPD did not withdraw the subpoena, Sachs told the NYPD, Clancy would go to court with a motion to quash it. Rather than go to court to explain to a judge why the subpoena was necessary, the NYPD wrote back the same afternoon to say that it was withdrawing the subpoena altogether. The NYPD subpoena letter cited as its empowering authority Section 14-137 of the New York City Administrative Code, which gives the NYPD commissioner the power to issue subpoenas. But as Sachs argued in her letter to the NYPD, that doesn't mean the police department can issue warrantless subpoenas for anything they feel like. Sachs cited case law suggesting that the subpoena power is actually restricted to investigations involving current or former officers. Under the federal Stored Communications Act, authorities who want to subpoena electronic records from a provider either need a court-issued warrant, or, if they're using an administrative subpoena, \"prior notice from the governmental entity to the subscriber or customer.\" \"This kind of administrative subpoena is not overseen by any court, and is not meant to target people like our client,\" Sachs told Hell Gate. \"If it's left unchecked, it would chill speech from potentially anyone that the NYPD decided to subpoena information about.\" \"We don't give that kind of power to police departments without judicial review,\" said Marty Stolar, a lawyer who has worked on similar cases in the past. \"It's got to be a process that's connected to a court if you're going to apply it to the general population. Here, the NYPD is misusing the power of subpoena to compel the production of documents as if they had the right to demand that information from any citizen. And they don't have that right. We have never given it to them.\" A copy of the NYPD subpoena supplied by X. The legal status of the NYPD's use of warrantless subpoenas for social media information isn't as clear as it might be, in part because when the NYPD's use of such subpoenas is challenged, it tends to withdraw the subpoenas rather than engage in a court process that might definitively establish the legality or illegality of its practice. In 2020, the department sent a nearly identical subpoena to Twitter, seeking a broad swath of records for Tina Moore, a New York Post reporter covering the NYPD. When lawyers for the Post threatened to challenge the subpoena in court, the NYPD hastily withdrew it. The incident provoked significant backlash, with free speech advocates, the Post, and groups like the Media Law Resource Center decrying the chilling effect of the subpoena. Clancy isn't working as a conventional journalist, but he is involved in the collection and dissemination of information about the NYPD. As president of the Brooklyn chapter of Copwatch Patrol Unit, Clancy receives tips about police conduct, and films police conduct himself. While the withdrawal of the subpoena makes it all but impossible to know what the NYPD's goal was in issuing it, he has his theories. Maybe the police are just trying to rattle him with a shot across the bow. Or maybe the NYPD is trying to find out whether NYPD officers themselves are feeding his organization information about misconduct. \"That happens,\" Clancy told Hell Gate. \"For a junior officer who sees misconduct by his superiors, if you take that to Internal Affairs, maybe your boss has a connection there and it winds up being you who gets in trouble. So some cops prefer to take it to Copwatch.\" As a further wrinkle, the NYPD's subpoena letter to X was sent by Michael Yanni, a member of the office of the department's deputy commissioner for legal matters, but used the email address intelsubpoenas@nynjhidta.org, a domain affiliated with the New York and New Jersey High Intensity Drug Trafficking Area office, a regional and federal joint drug task force. Similarly, the letter from the NYPD notifying NYCLU of the withdrawal of the subpoena was sent from Mallory McGee, an attorney with the NYPD Legal Bureau, using another email attached to the drug task force. Why are NYPD lawyers sending out subpoenas for a critic's private information without a warrant? Why are they using federal task force email domains to do it? Why did they withdraw the subpoena rather than defend its legitimacy in court? How many of these warrantless subpoenas have they sent out in the last year? We asked the NYPD these questions, but did not receive a response. Clancy, for his part, said that he's undaunted, and that the NYPD has no business rummaging through his social media content and metadata. \"Why would you use a shower curtain and close the bathroom door when you take a shower, if you have nothing to hide?\" he asked. \"Because it's none of your business, that's why. What the police might want to know and what they have a legal right to know are two different things.\" Already a user?Log in Thanks for reading! Give us your email address to keep reading two more articles for free Email Register See all subscription options Share on Facebook Share on 𝕏 Share on Email Nick Pinto@macfathom Nick Pinto served two tours as staff writer at the Village Voice. His reporting has appeared in The New York Times Magazine, Gothamist, The New Republic, Rolling Stone, The Intercept, and elsewhere. Stay in touch Sign up for our free newsletter Email Sign up More from Hell Gate Fresh Hell Most Libraries Will Only Be Open Five Days a Week Under Mayor Adams’s Budget \"It is astounding to me that we're in a situation where we are seriously contemplating losing universal six-day service.\" 1Comments Erin Durkin March 12, 2024 City of Immigrants ‘They Either Die Outside or They Come Inside and Survive’: Ebou Sarr Won’t Stop Sheltering Migrants An interview with the African immigrant who set up illegal shelters for dozens of recently arrived migrants. 2Comments Max Rivlin-Nadler March 12, 2024 Pull up a seat at... The Eric Adams Table of Success Hell Gate presents an interactive site exploring dozens of people who are in the mayor's orbit, their connections to each other, and why it all matters. Hater? Waiter? Find out! → Morning Spew City Wants to Jail One of NYC’s Most ‘Egregiously Negligent’ Landlords But good luck finding him! And other links to start your spring-like Tuesday. Hell Gate March 12, 2024 Morning Spew Don’t Worry: The Soldiers In the Subway Won’t Have Long Guns (Except For the Other Soldiers Who Also Keep You Safe) Feel safe yet? Hell Gate March 11, 2024 See all posts",
    "commentLink": "https://news.ycombinator.com/item?id=39681920",
    "commentBody": "The NYPD sent a warrantless subpoena for a copwatcher's Twitter account (hellgatenyc.com)446 points by praisewhitey 17 hours agohidepastfavorite261 comments ajsnigrutin 10 hours agoIn many countries, if you're eg. an engineer, and you fuck things up, you can become personally responsible for whatever damages happened due to your negligence. The company has its own responsibilities, but you, as an engineer carry some personal responsibility too. Same for eg. doctors... yes, hospital can get sued, but if you, a doctor, a person, fucked up, you personally can get sued, get into trouble, lose your licence etc. Same for truck drivers... drive too fast, kill someone, drive drunk, play flappy bird while driving... you personally responsible. Pilots, ship captains too. etc. Why the hell are all the public workers exempt from that? Cops, government workers, inspectors, etc.... in the best case possible, taxpayers pay for the damages and that's it. Why?! Someone wanted this subpoeana, his job is to know if s/he can actually request it, s/he signed his/her name on the paper... and then.. nothing? reply alistairSH 9 hours agoparent“Qualified immunity” is what you’re looking for. The original justification was to protect government officials (primarily law enforcement) from excessive, frivolous lawsuits. Except the courts have since interpreted QI in such a way that there’s a Catch-22 scenario. Plaintiff needs to establish violation of a clearly established constitutional precedent to have standing, but since QI was only introduced in the 1960s, there’s very little case law proving that precedence. While I understand the initial thought behind QI, it does seem the bar to holding law enforcement accountable is way too high. Time to adjust the laws (which requires a functional Congress, which we don’t have). reply alsetmusic 7 hours agorootparent> Time to adjust the laws (which requires a functional Congress, which we don’t have). This is the thing that frustrates me the most about the current state of USA politics. The inability to functionally govern and reach bi-partisan support for a lot of meaningful issues renders us helpless to improve bad systems. Meanwhile, corporations continue to dilute the value of their products and services or jack up the price beyond the rate of inflation and no real regulation seems possible. reply IG_Semmelweiss 6 hours agorootparentactually, one can argue that bipartisan support is the last thing you want because that's how 90% violate the rights for the 10% (and eventually, to 100% since we all take turns at being part of the 10%). Case in point: Patriot ACT. SOPA attempts. Asset Forfeiture. In contrast, see the most unruly place in Europe: Switzerland. Politicians bicker about inane things like updating language spelling, which caused huge uproar in 2006 [0] . 10 years later, you had the Cow Horns debate [1] of 2018, and its own sort of epilogue with the firestorm around cowbells [2] You also have a weak executive with rotating posts of only 1 year. [3] So, nothing gets done (by design). And when policians work out a small miracle via bipartisanship, they still must jump the hurdle of direct democracy and win over the population. Moreover, anything can be reversed with a very small minority [3,4] calling for a referendum, anytime. Yet Switzerland its the longest living democracy in the world with insanely high levels of development and prosperity. [0] https://www.swissinfo.ch/eng/culture/chaos-fears-loom-over-s... [1] https://www.swissinfo.ch/eng/life-aging/podcast_campaigning-... [2]https://www.iamexpat.ch/lifestyle/lifestyle-news/cowbell-noi... [3] https://www.thelocal.ch/20211129/a-foreigners-guide-to-under... [4] https://en.wikipedia.org/wiki/February_2014_Swiss_immigratio... reply caskstrength 3 hours agorootparent> Case in point: Patriot ACT. SOPA attempts. Asset Forfeiture. In contrast, see the most unruly place in Europe: Switzerland. I'm not so sure about that. Swiss public like all westerners seem to turn off their brains and vote for any authoritarian bullshit as soon as someone utter the word \"terrorism\" in 100km radius around them [0][1]: \"Voters have endorsed a series of measures allowing police to crack down on militant extremists and apply preventive detention methods, giving Switzerland one of the strictest anti-terrorism legislations in Europe .\" \"The Swiss government has proposed new legislation aimed at preventing extremist violence and forcing people deemed a threat, including children aged 12 upwards, to be registered with the authorities. House arrest could also be applied to suspects as a last resort in some cases. The idea is to target people who have not yet committed a crime but who are considered to be a risk.\" \"The experts are concerned that the draft law’s new definition of “terrorist activity” no longer requires the prospect of any crime at all. They fear it may target “legitimate activities of journalists, civil society and political activists”.\" [0]: https://www.swissinfo.ch/eng/politics/controversial-anti-ter... [1]: https://www.swissinfo.ch/eng/politics/un-experts-criticise-s... edit: spelling reply roenxi 3 hours agorootparentDemocracies can't really be condensed into one political moment; that is why they are so effective against dictatorships. Dictatorships have to make sense to one person. The usual pattern is authoritarians do something stupid, democracies do something stupid but also erratic, then time passes then the democracies reorganise to try something new and the dictatorship gets stuck in a rut. Eventually the democracy tries something that works to the amazement of all observers. The authoritarians are still pushing the same tired old plan of failure. Democracy doesn't have any secret sauce for making good decisions. Large groups of people are actually notoriously stupid. But they are much more responsive to situations where the government's official plan is obviously not working and the evidence is rolling in. reply caskstrength 2 hours agorootparentI wasn't arguing that Switzerland is not a democracy, just noted the its political system is nowhere as immune to enacting things like \"patriot act\" as OP suggested. reply scheme271 4 hours agorootparentprevI would disagree about the democracy part in Switzerland. Women weren't allowed to vote in federal elections until 1970 and it took until 1990 before the canton of Appenzell Innerrhoden allowed women to vote[1]. I don't think you can call yourself a democracy while banning women from voting. [1] https://en.wikipedia.org/wiki/Women%27s_suffrage_in_Switzerl... reply thworp 25 minutes agorootparentThis is a valid criticism, but does not detract from the fact that Switzerland is pretty much a model democracy. Power is extremely decentralized, not just down to the Cantons, but the counties and then the cities/townships (translated to rough US equivalents) also have a huge degree of autonomy. For any law or political decision that affects a Swiss citizen, it is almost guaranteed that it can be overturned be plebiscite (or in local issues by just showing up at meetings). The late adoption of Women's suffrage is just a consequence of this system, which is biased towards inaction by design. On the plus side they have not had ideological extremists run their country, or had political violence on the level of some of their neighbors (Italy, Germany, France). reply throwaway2037 4 hours agorootparentprev> Switzerland its the longest living democracy in the world I Googled that and found: This highly authoritative article from World Economic Forum that disagrees with you: https://www.weforum.org/agenda/2019/08/countries-are-the-wor... And, a bunch of Internet randos arguing about it here: https://www.theguardian.com/notesandqueries/query/0,,-80426,... As an outsider, highly advanced democracies appear to argue about very small things because most of the big things are done right. See also: Japan, Netherlands, Norway, Canada, Finland, etc. reply chx 3 hours agorootparentThe United States is not and never has been a democracy. The Senate and the SCOTUS are not democratic institutions. reply waffleiron 4 hours agorootparentprevTheir definition of democracy is quite arbitrary, I would argue that any country where women and minorities cannot vote isn’t a democracy, it is according to them. reply scheme271 4 hours agorootparentSure but in that case, Switzerland wasn't a democracy until 1990 when women in the canton of Appenzell Innerrhoden finally got the right to vote in federal elections. reply logicchains 4 hours agorootparentprev>This highly authoritative article from World Economic Forum You mean highly biased? Switzerland's direct democracy is the antithesis of what the WEF stands for. reply riffraff 2 hours agorootparentWEF is based in Switzerland and their annual meeting is in the Swiss Alps. The financing of WEF is mostly big corps which are in large part headquartered in Switzerland, and is financed by the Swiss government directly. Swiss direct democracy is irrelevant to WEF. reply yieldcrv 7 hours agorootparentprevRemember, it is perfectly valid to be equally aware of how the parties are different, but being more frustrated by the ways they are the same Don’t let partisans gaslight you over their desperate struggles for the ring of power yes, the sides are different, there are ways they are the same reply ajross 7 hours agorootparent> Remember, it is perfectly valid to be equally aware of how the parties are different, but being more frustrated by the ways they are the same That's an easy genericism. In fact this particular issue under discussion is quite asymmetric. It's routine for a republican congress to block legislation that would otherwise pass (c.f. current bills for border security and Ukraine aid, our now-quarterly government shutdown jamboree, etc...). The reverse really is not true. I can't think of a notable bipartisan bill in the last decade blocked by a democratically-controlled government organ. Can you? It's perfectly valid to have complaints about both parties, but not to invent frustrations about how they are the same. reply roenxi 4 hours agorootparentThere seem to be some tautologies happening here. There are only 2 major parties in the US Congress. Bipartisan bills, somewhat by definition, are going to be passed by both parties. Similarly, since there are 2 major parties, if legislation is proposed by one party then it would pass unless the other blocks it. I don't see how you're expecting things to work differently. Would we expect Democrat-sponsored legislation to fail for reasons apart from Republican opposition? > The reverse really is not true. I doubt that is a true statement, but assuming it is then logically that would just mean that Republicans only propose legislation after they have talked to the Democrats and confirmed they are willing to vote it through. But that would suggest the Republicans are avoiding opportunities to kick up a fuss which doesn't sound like the US Congress we all know and love. > c.f. current bills for border security Unrelated note but it is amazing to me that the US has been an entity for a couple of centuries now and doesn't have legislation already in place to control the border. reply hattmall 4 hours agorootparentOf course the US has legislation to control the border. There is an entire agency called the border patrol. The thing is right now , it seems, additional legislation is needed to actually compel the executive to have the border patrol do their job. Which this last bill didn't even do and would have simply dumped money into an auxiliary judicial system that serves virtually no purpose and would continue to do so even with more money. Shortening the time for an asylum hearing doesn't even matter because people don't show up for them. Any bill that maintains the current administration's position of simply letting people go completely free during their asylum waiting period is useless. Sure, people with legit claims might show up, but crimminals, which are the problem, are not going to come back for the hearing. Most Americans don't mind immigrants or immigration at all and not even the idea of illegal immigration is really a problem. It's just the people that commit crimes that are the problem, it's one of the reason we see recent immigrants be so strongly in favor of border security, because the people they specifically tried to get away from are showing up too. Not only are people caught crossing the border illegally not detained, but apparently even when they commit additional crimes it's too much for some cities and states to even keep them in jail for those crimes. reply yieldcrv 3 hours agorootparentprevthat's interested because you created a point I didn't make, in order to discredit the point I did make I won't call it a strawman argument since the grandparent post did say \"inability to functionally govern and reach bi-partisan support for a lot of meaningful issues renders us helpless to improve bad systems\", I was not making a point about bills passing my only, isolated point, is that it is valid to be frustrated by the ways in which the parties are the same. you decided to talk about one way you believe they are different. its fine if that was a misunderstanding of my point, but if you're not aware of any unproductive way the parties are the same, then this discussion isn't for you. reply kQq9oHeAz6wLLS 6 hours agorootparentprevRepublicans block bills because they're stuffed full of unrelated things (by design - that way they can easily be painted as the bad guys - which is working, see your own comment). I'm not saying that the reverse doesn't happen - perhaps Democrats are okay with bloated bills, I dunno. Good rule of thumb is when a bill is blocked by one party, go find out everything that's in it and what parts the blocking party objected to. I guarantee it won't be the stuff the bill is named after. reply jasonwatkinspdx 5 hours agorootparentNonsense. Republicans were given everything they asked for on the border as a stand alone offer and refused it. It has nothing to do with some sort of genuine political position over riders on bills, and a simple examination of literally anything republicans have sponsored in the last 30 years will demonstrate that starkly. The point is obstructionism instead of governance. reply oivey 5 hours agorootparentprevWhat you’re describing is called compromise: you get what you want in exchange for something else. reply btown 6 hours agorootparentprevQI is atrocious. See some choice quotes from https://www.politico.com/news/magazine/2023/02/19/qualified-... : > Although a prior court decision had held that it was unconstitutional to release a police dog on a suspect who was lying down, the court in Alexander Baxter’s case granted qualiﬁed immunity to the officers because, it held, the prior decision did not clearly establish the unconstitutionality of the ofﬁcers’ decision to release a police dog on a person who was seated with his hands in the air. > Because no prior court opinion had similar facts, the appeals court judges dismissed Jayzel’s excessive-force claim, even though they believed Aikala’s decision to tase a potential domestic violence victim went “far beyond the pale” and violated the Fourth Amendment. > In Jessop v. City of Fresno, police officers stole $225,000 in cash and rare coins when executing a warrant. Prior cases had held that it was unconstitutional for officers to steal, but those cases were factually distinct — involving the theft of different types of property under different circumstances. According to the appeals court, the officers “ought to have recognized” that it was wrong to steal the coins and cash, but “they did not have clear notice that it violated the Fourth Amendment” because prior court decisions “did not put the constitutional question beyond debate.” reply light_hue_1 4 hours agorootparentprevThe courts have not \"interpreted\" qualified immunity to mean something. The Supreme Court in 1967 created qualified immunity out of nowhere to defend obviously racist cops in Mississippi. Specifically, they created it to put an end to people using Third Ku Klux Klan Act (Rights Act of 1871) to sue racist cops. This is part of a long history of preventing people from using Reconstruction Era laws to gain any meaningful civil rights. They did so by saying that 42 U.S.C. § 1983 (which was passed by Grant to fight the KKK) which was meant to allow people to sue for civil rights violations clearly intends to provide an exception for police officers if they acted in good faith (whatever that means; given that good faith here means enforcing a racist law often with extreme violence). But there's absolutely nothing in the actual law, the text passed by Congress or in the intent of Congress, that supports this reading. You can read the Section 1983 yourself: > Every person who, under color of any statute, ordinance, regulation, custom, or usage, of any State or Territory or the District of Columbia, subjects, or causes to be subjected, any citizen of the United States or other person within the jurisdiction thereof to the deprivation of any rights, privileges, or immunities secured by the Constitution and laws, shall be liable to the party injured in an action at law, suit in equity, or other proper proceeding for redress, except that in any action brought against a judicial officer for an act or omission taken in such officer’s judicial capacity, injunctive relief shall not be granted unless a declaratory decree was violated or declaratory relief was unavailable. For the purposes of this section, any Act of Congress applicable exclusively to the District of Columbia shall be considered to be a statute of the District of Columbia. A \"judicial officer\" by the way is a member of the court / judicial branch, not a police officer. Because there's no law, qualified immunity is a total mess. Different courts and judges apply it very differently. For some it's simply a blanket defense for essentially any acts. Ironically, Scalia and Thomas have both written dissents on the other side from the liberal judges at times, pointing out that there is literally no basis in law for qualified immunity and the Court should just drop it entirely. This isn't just a matter of adjusting the law to clarify the interpretation of Section 1983, although that would help. It also requires a long-term realignment of the Supreme Court who shouldn't be able to wholesale write laws this way. That's not me saying this, that's Scalia in Crawford-El v. Britton \"[the Supreme Court] find[s] [itself] engaged...in the essentially legislative activity of crafting a sensible scheme of qualified immunities for the statute we have invented—rather than applying the common law embodied in the statute that Congress wrote\". reply daelon 51 minutes agorootparent> Ironically, Scalia and Thomas have both written dissents on the other side from the liberal judges at times, pointing out that there is literally no basis in law for qualified immunity and the Court should just drop it entirely. What's ironic about that? reply kmonsen 6 hours agorootparentprevIsn't QI fully something the supreme court made up and unlikely to go away whatever congress does? reply spencerflem 10 hours agoparentprevif you're asking earnestly, one of the biggest reasons is that the would-be prosecutor relies on the cooperation of the police department to function. This article covers that and some more: https://fivethirtyeight.com/features/why-its-still-so-rare-f... reply reaperman 9 hours agorootparentIn some countries it’s possible for civilian lawyers to press criminal charges specifically for situations like this. There’s some gatekeeping by the courts and whatnot to approve the case but it’s pretty reasonable. Also allows victims to “press charges” when the police mysteriously refuse to. Burden of proof remains the same as for a state-appointed prosecutor. And judges still get similar leeway in sentencing. reply spencerflem 9 hours agorootparentOh totally, theres lots of ways it could be improved. Unfortunately, our courts are also in their pocket for various reasons, and invented whole cloth the concept of qualified immunity. So getting the case to the courts is no guarantee either. Still would be better of course. I find it so sad, how much of the country was united on this for months, and how little has actually happened in response. reply Teever 9 hours agorootparentprevSo why not make a special prosecutor who only deals with police cases? The solution seems too simple. That makes me think that the underlying thing causing this issue to plague American society isn't what you suggest. reply spencerflem 9 hours agorootparentok, maybe one of the biggest wasn't hedged enough. but it is still an important reason imo. There are many more of course: - Police can sabotage the re-elections of politicians that oppose them, by doing their job worse and increasing the rate of crime. - Courts tend to favor the police, inventing such concepts as qualified immunity - An unfortunately large percent of the country is happy for the police to trample people as long as it's the people they don't like - And more, which the article gets into some of. I'm not a huge 538 fan but its pretty decent reply ponector 10 hours agoparentprevFor the doctors, it is really hard to loose a license. Even then, a doctor can move to another state and continue to run practice. reply alsetmusic 7 hours agorootparentLast Week Tonight with John Oliver just covered this topic on Sunday. reply otherme123 3 hours agoparentprevNot all public workers are exempt. We have public health workers here, and there has been cases of people going to jail for looking at medical records without medical justification. This is a nurse that is going 3 years to jail and a 4,000€ fine for checking the records of her ex: https://theobjective.com/espana/tribunales/2023-07-31/conden... The immunity doesn't reach all public workers. Only politicians and law enforcers reply Loughla 9 hours agoparentprevI work in the public sector, local government. I'm personally liable for mistakes at my job. I carry insurance for it. reply godelski 5 hours agoparentprev> In many countries Those countries include the US btw. The rules are basically the same for all of them, that they have to have not just made a mistake, but made an unreasonable mistake. It is often hard to lose a license this way as you need to balance just shit happening, things going wrong when you made the right (or a reasonable) choice vs when you made a gross mistake. And of course, the conditions are based on decisions made a priori, not post hoc. Because post hoc is a dangerous game to play given so much more clarity. e.g. for Doctors: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2628513/ Of course, this doesn't say anything about cops or the state of affairs here in the US. What surprises me more is that despite what many think, there is quite a lot of distrust for the police. What someone needs to tell me is how most Americans have \"very little\" to no confidence in Congress and they keep getting elected. Its pretty universally agreed that no one wants a geriatric in the White House yet here we are. How we distrust all our institutions and yet continue to prop them up. At this point I'm no longer mad at them. I mad at us. We are being enablers. Even down to local elections I hear so much uproar and low polling rates and yet watch these officials get reelected. I don't think it is foul play because I hear people talk about how they hold their nose while voting. City, county, state, or federal, it is all the same. Clearly we've made our own bed, but we just don't want to lie in it. If you're going to half ass your job you can't complain about how shitty everything is. Before you go blaming others, take a long look in the mirror. I'm sure __you__ are better than that, just the way __you__ can't be manipulated by propaganda and how __you__ can't be tricked by scams. Clearly, we've been played. Clearly we're still being played. So stop blaming your neighbor or pointing to others until you look in the mirror. We all need to do it if we're going to get our shit together. Because we're all in this together, for better or worse. But what I fear most is that because we'd rather stroke our egos to claim our own team is best we won't ever realize that it is this action that creates the radicals. It is my fear that we won't solve problems before they are problems. That we believe so much in \"don't fix what ain't broke\" that we won't ever perform basic maintenance. It is far more expensive to fix what is broken than to fix what isn't. My fear is we won't fix things until the streets run red. And that's my biggest fear, that we, the people, won't realize that the blood isn't just on the hands of the elites, but that it is on ours too. We are happy to play the mafia boss who has fully convinced himself and matter-of-factly asks \"do you see blood on my hands?\" [0] https://news.gallup.com/poll/394283/confidence-institutions-... reply rileymat2 10 hours agoparentprevIsn’t there something special about the arrangement of doctors not being employed by the hospital that might explain some of the individual suits? reply gosub100 9 hours agoparentprevBecause in all those professions, there is enough time to follow a process, or (in the case of pilots) their survival is linked to the survival of everyone else. In a typical police apprehension, there may be a second or less to draw your weapon and fire. Nobody would take the job if they had to be perfect or lose their freedom or life savings. Speaking of life savings, most of the criminals they apprehend have nothing to lose. So maybe you're on to something. Let's hire police who are currently incarcerated and have no assets, family or wealth to lose. Then they can be subject to the draconian measures of your fantasy and worst case is they got a few months of freedom. Smart guy you are:) reply Duwensatzaj 8 hours agorootparentThen why do cops and government officials get QI in non-emergency situations? reply WaitWaitWha 13 hours agoprevI am pleased how X/Twitter responded to this request. X/Twitter notified the person, sent a copy of the the subpoena, suggested legal representation, provided recommendations, and defied the NYPD order of silence. reply throwaway2037 4 hours agoparent> The notification included a copy of the subpoena, which warned X not to tell Clancy of its existence. \"You are not to disclose or notify any customer or third party of the existence of this subpoena or that records were provided pursuant to this subpoena,\" the document read. > But X, following its own corporate policy, told Clancy anyway, and suggested he might want to get some legal representation to fight the subpoena, recommending the American Civil Liberties Union. I too was surprised by this! When the police dept wrote: \"You are not to disclose or notify any customer...\" is that a legal demand backed by law, or simply a request? I wish I knew more. I suspect it is a request, and that is why Twitter would ignore it. I am sure that Twitter has very conservative internal and external legal counsel to advise on these matters. Plus, there must be many, many of these requests from NYPD. reply input_sh 2 hours agorootparent> Last Wednesday, Sachs wrote to the NYPD to challenge the administrative subpoena, which the NYPD had sent on its own authority, without any warrant or judicial approval. If the NYPD did not withdraw the subpoena, Sachs told the NYPD, Clancy would go to court with a motion to quash it. If it's not backed by a court order, it's a polite request. But even when it's a court demand, companies have several different counter-measurements. For example, they can tell the court \"we can't provide data because we don't have that data\" (Signal does this), or \"we do have that data, but extracting it is resource-intensive, so the court should pay up\" (some of the \"Twitter Files\" were precisely about this), or straight up ignoring non-American court orders (this is or at least was Reddit's general policy). reply jjjjj55555 11 hours agoparentprevThey also could have done the complete opposite. Imagine your life being in the hands of Twitter. reply gretch 7 hours agorootparentYour life isn’t in the hands of twitter. It’s in the hands of NYPD, an institution designed to protect the public. While it’s certainly nice to have, we shouldn’t have an expectation that a private business to defend us from the corruption of the police. Like, that’s a much bigger problem that Twitter does or doesn’t do reply logicchains 4 hours agorootparent>It’s in the hands of NYPD, an institution designed to protect the public. It's not at all designed to protect the public; in the US the police don't even have a legal duty to protect the public. It's designed to enforce the law. reply sneak 4 hours agorootparentprevWalgreens and CVS do the opposite with our medical data. They turn it over to police without even a warrant. https://arstechnica.com/science/2023/12/cvs-rite-aid-walgree... There should be consequences when large service providers fail to protect the rights of their customers. Walgreens/CVS are pretty close to a duopoly. reply LAC-Tech 3 hours agorootparentprevSounds like twitter acted more ethically than government department. So I guess I choose twitter. reply DoesntMatter22 9 hours agorootparentprevI mean technically Twitter could have hired a hit man to come after you too. But they didn't. So I don't get the point reply jjjjj55555 6 hours agorootparentI'm afraid I don't get your point either. Hit man? The police overreached here, and the only thing that protected the victim was, of all things, Twitter. The police are supposed to be the good guys, but that obviously wasn't the case here. My comment was merely expressing surprise at the irony of the situation. reply AI_beffr 7 hours agorootparentprevwhy has nobody responded to this comment? the OP should yield to this comment reply avidiax 8 hours agoparentprevI doubt this is the only request that NYPD sent in this matter. How did the other companies behave? The NYPD may have pulled this request to avoid scrutiny, but they probably didn't pull all the similar requests that succeeded. The problem is that it's impossible to generate standing in the court if you can't know about their requests. reply zugi 8 hours agoprevDid Michael Gerber, the Deputy Commissioner of Legal Matters, commit the crime of perjury by signing a document asserting he had authority which clearly he did not? It's also odd that the PDF redacted the name of Michael Gerber, the Deputy Commissioner of Legal Matters, when the NYPD issued a press announcement naming him (https://www.publicnow.com/view/A505A75813F7F2F483905D075F4AD...) and tout him on their public website (https://www.nyc.gov/site/nypd/bureaus/administrative/legal.p...) It seems that either Michael Gerber, the NYPD Deputy Commissioner of Legal Matters, is clueless about the law, which reflects poorly on his Harvard Law alma matter, or Michael Gerber, the NYPD Deputy Commissioner of Legal Matters, was illegally using threats and coercion and false subpoenas to investigate political opponents and chill free speech. Either way, this does not reflect well on Michael Gerber, the NYPD Deputy Commissioner of Legal Matters, nor the Harvard Law school from which he allegedly graduated. reply xbar 16 hours agoprevI am baffled by the fact that NYPD can avoid the determination of legality by withdrawing subpoenas. It seems like it would be in X/Meta/Alphabet's interest to get the question in front of a judge in order to require warranted subpoenas. reply hex4def6 16 hours agoparentIf I were to send an illegal demand to someone (like say a threat of blackmail / extortion), can I simply \"withdraw\" the demand once it reaches court? Absolutely ridiculous. reply JumpCrisscross 16 hours agorootparent> can I simply \"withdraw\" the demand once it reaches court? For the per se dispute, absolutely. That's what you went to court to get. If your demand caused damages, that's a separate controversy that isn't cured by withdrawal. reply Mathnerd314 6 hours agorootparentDo lawyer fees count as damages? reply dylan604 14 hours agorootparentprevJudge: How do you plead? Defendant: J/K!!!! lulz!!!!! Judge: case dismissed!It probably depends on how wealthy the defendant is (if an individual), or the size of the the org defendant represents. If you're a LEO type of org, then I'm honestly shocked the judge didn't just dismiss with prejudice from the off reply throwaway2037 4 hours agorootparentLEO? Lower earth orbit? reply ultimoo 4 hours agorootparentLaw enforcement reply 8organicbits 45 minutes agorootparentLaw Enforcement Officer, specifically. reply reaperman 9 hours agorootparentprevFor patent trolls, the answer is typically yes. Though that’s not “illegal”. reply deely3 10 hours agorootparentprevIf I remember correctly you can send illegal DMCA requests as many as you want? reply Terr_ 4 hours agorootparentprevI think this may be confusing \"without legal approval\" with \"prohibited by law\". For example, I could type up a letter to my neighbor that he must paint his car blue because that's the law 'round these parts... However that wouldn't necessarily be a crime, it's just be dumb/wrong. That said, if a law was passed to make \"knowingly or recklessly false warrants\" a crime, that might be one way to approach the issue. reply ajross 6 hours agorootparentprev> If I were to send an illegal demand to someone (like say a threat of blackmail / extortion), The distinction is that blackmail and extortion are crimes. It's the act of sending them itself that is the crime, not the demand. If you withdraw it, it doesn't matter, because the crime already happened. There's no criminal statute here. The NYPD asked for something. Someone challenged it as being \"not something the NYPD is allowed to ask for\". The NYPD said \"never mind, you're right\". That's the end of the story. If you want the law granting the NYPD the power to issue subpoenas to be withdrawn or modified, the solution is to elect representatives to do that for you. Courts are just the backstop for this process, and an imperfect one. reply john-radio 10 hours agorootparentprevIt's just like pickpocketing in Skyrim; it's legal if you just withdraw. reply pfdietz 10 hours agoparentprevIt's not clear that will actually work. Courts are on to the trick of evading judicial review by rendering some issue moot, and there is precedent to work around it. For example, see the \"Mootness\" discussion in this recent 1st amendment case (Nutt vs. Ritter, 2023): https://caselaw.findlaw.com/court/us-dis-crt-e-d-nor-car-sou... reply avs733 14 hours agoparentprevBecause courts use mootness to allow law enforcement to escape criticism all the time. reply throwanem 16 hours agoparentprevWhy in their interest? reply cosmojg 16 hours agorootparentProcessing warrantless subpoenas costs time and money. Given that we're talking about fairly rational megacorporations, though, it's likely they've already decided that the legal battle would cost more. reply autoexec 12 hours agorootparent> Processing warrantless subpoenas costs time and money. You can actually bill police departments for that. It's not as common when it comes to basic records, but it's not uncommon when it comes to wiretaps/trap and trace. reply dotancohen 4 hours agorootparentIn other words, it is an actual cash revenue stream for the business. reply autoexec 8 minutes agorootparentSometimes yes (https://www.wyden.senate.gov/imo/media/doc/wyden_hemisphere_...) reply yaur 13 hours agorootparentprevThey cover the costs of complying although likely wouldn't cover the costs of determining that a request was warrantless and rejecting it, so the incentives align with just blindly complying with everything. reply Scoundreller 16 hours agorootparentprev> Processing warrantless subpoenas costs time and money. Takes me two seconds to throw out the junk mail in my mailbox. Sometimes I save it as firestarter. How is a “warrantless subpoena” any different? (Maybe it is, actually asking here). reply cosmojg 16 hours agorootparentHow would you know it's warrantless without further time investment? It's likely cheaper and safer to blindly process any subpoena originating from an authoritative source than to request and validate proof of warrant. Of course, it would be even cheaper to never receive such a subpoena in the first place. Securing a ruling against their legality would help with that, but as I said, it's unlikely the ends justify the means for a rational megacorporation. reply Scoundreller 15 hours agorootparentI’m guessing there’s some obvious telltale that there’s a warrant (signed/stamped by judge, possibly on court letterhead instead of police letterhead). Also possible to separate the streams of input. Maybe can’t force it, but having a “warrants” stream that gets priority triage and another for “other” that works a lot slower because it’s mostly garbage. Can create a shit list for those that abuse the “warrants” stream. reply throwanem 10 hours agorootparent> I’m guessing I'm not, at least in the extent I've participated in subpoena response and had conversations with an employer's counsel in that connection. If you're b2c and past a certain point of scale, it just becomes an overhead, a bit of a pain but no big deal. That's if you respond promptly and completely - any company that screws around with something credibly subpoena-shaped is taking a dangerous risk; that's a great way to draw the interest of courts (\"contempt of\"), and SDNY in particular is known for being quite proddy. reply actionfromafar 15 hours agorootparentprevDoesn't sound very safe to shitlist NYPD. reply BLKNSLVR 11 hours agorootparentWhich seems to be what NYPD are attempting to leverage. reply freejazz 10 hours agorootparentprevBecause they will go to a court to move to enforce it... reply fshbbdssbbgdd 15 hours agorootparentprevIf you received an illegal subpoena from the government you wouldn’t have any costs associated with it at all? Just throw it in the trash, move on with your day, and surely you will never have to think about it again? reply Scoundreller 15 hours agorootparentAt least where I work, if a random user bypasses the helpdesk with an informal request, my manager will back me up if they complain about no-response because there’s a formal method to follow for a reason. It’s not an “illegal” subpoena, police can try and ask whatever they want, but I’m assuming there’s no penalties for ignoring. But I asked a question, maybe there are exceptions and you must respond anyway despite lack of warrant. While there could be reasons considered valid to respond to an informal request (some urgent life/safety matter), the police can and do lie, but are less likely to do so to a judge. Lived in a building once where management agreed to police installed warrantless spycams in some hallways and told management it was to investigate a car theft ring, but it was some guy (edit:) storing drugs and keeping to themself. Sadly, I don’t think there’s any kind of post-review of urgent informal requests to assess validity/accuracy of the request with consequences. reply michaelmrose 13 hours agorootparentIts not illegal to ask for voluntary cooperation. Its illegal to issue a subpoena under color of authority you don't actually have. It clearly stated this happened. reply Qwertious 8 hours agorootparent>Its not illegal to ask for voluntary cooperation. Privacy laws exist, with exceptions for illegal behavior. It may well be illegal to give data to cops unless you have user permission or a warrant (IANAL). reply ClumsyPilot 10 hours agorootparentprev> Its not illegal to ask for voluntary cooperation Cooperation obtained under false pretences may be fraud. reply gosub100 8 hours agorootparentprevAre those walls bulletproof in the building? If that \"keeps to himself\" druggie lost his stash or got in a tight spot, you'd be clamoring about how the police didn't \"see the signs\" and act to stop the illegal activity before the stray bullet claimed innocent lives. reply dotancohen 4 hours agorootparentIf lives are at stake, then what is the problem with getting a warrant? It's not the cameras that GP is complaining about, it is the lack of warrant. reply fwip 14 hours agorootparentprevWhen the subpoena comes from the government, it seems potentially short-sighted to assume that the same government is going to be as kind as your manager regarding a random user's complaint. Also, like the article explains, it is possibly illegal for them to send this warrantless subpoenas. Further, they are not simply \"asking,\" as you describe - the text of the subpoena starts off \"We command you,\" and ends with the explicit threat of legal action if twitter fails to comply. reply bediger4000 16 hours agorootparentprevTo determine once and for all how to act when receiving this kind of subpoena, or to get police departments to stop sending them, whichever is cheapest. reply lcnPylGDnU4H9OF 15 hours agoparentprevIt's possible for a document to be submitted in error, in actual good faith. Not to say it was in this case, nor that we don't need more accountability for abuse, but there could be legitimate reason to skip this judgement if it's withdrawn appropriately. Especially when it's done by an organization without this history of abuse. reply throwaway2037 4 hours agoprevI love the closing quote. > Clancy, for his part, said that he's undaunted, and that the NYPD has no business rummaging through his social media content and metadata. > \"Why would you use a shower curtain and close the bathroom door when you take a shower, if you have nothing to hide?\" he asked. \"Because it's none of your business, that's why. What the police might want to know and what they have a legal right to know are two different things.\" reply pfdietz 16 hours agoprevIf he wanted, he could continue to press this in court, even after its was withdrawn. There is precedent in at least some jurisdictions for this: otherwise, bad actors could keep avoiding judicial scrutiny just like the NYPD is doing here. reply busterarm 16 hours agoparentLawyers don't do this though because the best thing to do for their client is get the NYPD to withdraw, not fight a lengthy and expensive legal battle over the NYPD's subpoena power. The NYPD would legally fight to retain this power to the bitter end. reply JumpCrisscross 16 hours agorootparent> Lawyers don't do this though because the best thing to do for their client is get the NYPD to withdraw This is where groups like the ACLU come in. reply busterarm 15 hours agorootparentThe ACLU on their own doesn't have standing. They still have to best represent their client. Doing this on behalf of the client would be the absolute wrong thing to do. Any lawyer that would do that should be disbarred. The ACLU would only have the opportunity if this kind of subpoena was used on the ACLU itself. reply everforward 15 hours agorootparent> Doing this on behalf of the client would be the absolute wrong thing to do. Any lawyer that would do that should be disbarred. This is not true. Check out the American Bar Association Rules of Professional Conduct 1.2 (https://www.americanbar.org/groups/professional_responsibili...). Broadly speaking, lawyers are bound to the client's wishes, and are expected to represent their client the best they can under those circumstances. If a lawyer's client wants to sue the NYPD, even if the chances of winning are slim, they can represent that client (barring some niche scenarios). This is how the ACLU typically works. They find someone with standing who cares enough to bother with following the lawsuit through and they represent that person. reply busterarm 15 hours agorootparentI'm not saying that they wouldn't. I'm saying that they shouldn't. The client isn't the only person with things to lose here, it's their sources who are at risk if he loses the case. The client wouldn't pursue it for journalistic ethics reasons. reply JumpCrisscross 15 hours agorootparent> not saying that they wouldn't. I'm saying that they shouldn't. You said it would be grounds for disbarment. reply busterarm 15 hours agorootparentYes if people are harmed by your actions as an attorney that is my opinion of what should happen. reply everforward 10 hours agorootparentThis seems like a spurious attribution of harm. If a client wants to take an ill-advised approach I expect the attorney to inform them that it is ill-advised and why, but I do not believe the attorney bears any blame for subsequent harm. I think it also presumes that attorneys fill a role that they do not. The law, and attorney professional conduct, presumes that litigants are reasonable, rational people who are entitled to weigh their own pros and cons and arrive at a conclusion. Attorneys are there to provide context and information, but are not the decision maker. Note that the 6th Amendment guarantees a person the right to counsel, but specifically says the defendant has the right to know the charges, face their accuser, etc. You can see this in the competency standards for trial from Dusky v US: \"It is not enough for the district judge to find that 'the defendant is oriented to time and place and has some recollection of events', but that the test must be whether he has sufficient present ability to consult with his lawyer with a reasonable degree of rational understanding -- and whether he has a rational as well as factual understanding of the proceedings against him.\" Or Godinez v Moran: \"The standard adopted by the Ninth Circuit is whether a defendant who seeks to plead guilty or waive counsel has the capacity for \"reasoned choice\" among the alternative available to him.\" Edit: Courts are also aware of and can accommodate sensitive sources. The ACLU could also drop the case at any point if discovery would require disclosing sensitive sources, same as NYPD have done. reply JumpCrisscross 15 hours agorootparentprev> if people are harmed by your actions as an attorney that is my opinion of what should happen This isn't how disbarment works. Given we're in an adversarial system, I'm glad it doesn't. (Could you imagine a lawyer refusing to advance litigation against a city because it would harm the city's taxpayers?!) reply busterarm 15 hours agorootparent> (Could you imagine a lawyer refusing to advance litigation against a city because it would harm the city's taxpayers?!) Your client doesn't have a responsibility to protect the city's taxpayers. This client does have a responsibility to protect their sources. reply JumpCrisscross 13 hours agorootparent> This client does have a responsibility to protect their sources An ethical responsibility. Not a legal one. Not the lawyer's job, and certainly not the legal bar's. That's not only presumptuous, it's unprofessional and potentially dangerous. reply JumpCrisscross 15 hours agorootparentprev> Doing this on behalf of the client would be the absolute wrong thing to do. Any lawyer that would do that should be disbarred This is a novel take. Were this me, I'd require my lawyer pursue the NYPD for the cost of the original legal fees. They would not refuse. reply busterarm 15 hours agorootparentSee my response to NewJazz. reply JumpCrisscross 15 hours agorootparent> See my response to NewJazz Sure, and I'd expect them to explain that to me. But to outright refuse--I would have a serious issue with that. EDIT: Just asked. Friend, my counsel and partner at a Big Law firm. He'd absolutely do it. reply busterarm 15 hours agorootparentRefusing is easy. As about two dozen attorneys have told my family members in their case after taking tens to hundreds of thousands of dollars of their money: \"I can no longer represent you in this case.\" As for your friend, I'm not even sure I care unless this lands squarely in his practice area. Once it gets to court NYPD's lawyers could just say some handwavey \"homeland security\" shit and your judge might just piss their robes and rule against you. In civil matters it's always better to seek agreement outside of court. reply JumpCrisscross 15 hours agorootparent> Refusing is easy Nobody claimed otherwise. > not even sure I care unless this lands squarely in his practice area He asked a litigation partner who represented me the one time Nassau County police got cute with me. In that case, we weighed being aggressive after the department pulled its accusations and I decided not to pursue given my financial condition. Were that to happen today, I'd decide differently. > your judge might just piss their robes and rule against you Sure. And I'd have wasted time and money and possibly more. > In civil matters it's always better to seek agreement outside of court Constrained to the case or controversy, sure. But that isn't always the entire picture. Seeing that context is what separates the great lawyers from the commodity ones. reply busterarm 15 hours agorootparent> Sure. And I'd have wasted time and money. No, you'd have pinned your NYPD officer sources asses up against the wall because you leaked their information by losing your subpoena fight. THAT is why this wouldn't be pursued in court. reply JumpCrisscross 15 hours agorootparent> you'd have pinned your NYPD officer sources asses up against the wall because you leaked their information by losing your subpoena fight Why is that my problem? reply busterarm 15 hours agorootparentnext [5 more] [flagged] JumpCrisscross 15 hours agorootparent> Protecting your sources is the cornerstone of journalism This is the journalist's job to weigh. Not counsel's. If the journalist says fight, counsel should fight. > Your moral compass is fucked If we're going ad hominem, I'm not the one advocating deference to illegal police actions. (And to be clear, we were never debating whether action should be pursued. Just whether it could be, or whether an attorney should refuse to do so.) reply busterarm 15 hours agorootparentI'm advocating not putting other people at undue risk. The same as you would tell people not to drive drunk. I'm not advocating for somebody to spend money on a cab, that's just circumstantial. reply busterarm 15 hours agorootparentprev> (And to be clear, we were never debating whether action should be pursued. Just whether it could be, or whether an attorney should refuse to do so.) No, we were, because that was my argument. This is putting words in my mouth and moving the goalposts. I never said my basis for \"should\", you just chose it to read as \"should on legal grounds\" whereas my statement was \"should if all things were fair\". reply JumpCrisscross 13 hours agorootparent> my statement was \"should if all things were fair\" You raised it as grounds for disbarment. You don't get disbarred for being unfair. reply FireBeyond 10 hours agorootparentprev> As about two dozen attorneys have told my family members in their case after taking tens to hundreds of thousands of dollars of their money Not that you have to disclose, but I am taking an educated guess, as someone who has also worked for a (very) large law firm... You reference a single civil case. Where \"two dozen\" attorneys have each been retained as counsel, done work for your family on this one case, and every one of the two dozen of them have individually withdrawn their representation? At some point (multiple points, really), something has to be very very wrong here: Who are these attorneys who are all constantly withdrawing from your family's case, and why? That to me (and please, I am not attempting to judge) sounds like 1) their client (your family) failed to disclose fairly substantial material information to them, causing them to withdraw, or 2) their client has been notably/repeatedly uncooperative with their attempts to represent them, or 3) their client has (repeatedly) ignored statements about the futility of their case, and stubbornly pushed it forward, still. And then we get to the court. A judge that doesn't raise an eyebrow at the second withdrawal of an attorney, let alone twenty plus is comatose at the bench. In my area, a capital city, judges want explanations from attorneys on their withdrawal, and the predicament doing so might leave their client in. They've even been ordered to brief public defenders (I know, criminal) before they could withdraw. Hell, even civil continuance orders for more than a month or so require justification as to why the continuance need be that long. reply busterarm 6 hours agorootparentSome of these things are close enough to true, but my family has semi-famously been involved in an in-family legal battle that has been ongoing in various forms since 1989. Everyone involved on both sides has been a complete asshole to each other, lawyers, the court and uninvolved family members (like myself). This particular court is very slow and exceptionally so since COVID. This current appeal was supposed to have a decision 3 years ago and that still hasn't happened and we know there will be no court date for at least 6 months. reply NewJazz 15 hours agorootparentprevDoing this on behalf of the client would be the absolute wrong thing to do. Any lawyer that would do that should be disbarred. Even if the client asks for that? reply busterarm 15 hours agorootparentAre you even thinking about the risks if he loses the case? He'd be giving up his sources and even opening himself up to the risk of them investigating him and charging him with other crimes. Any intelligent attorney would advise their client against it. Once you go to court anything can happen and either way it's prohibitively expensive. People in my own family have done nothing wrong and lost everything they owned fighting in civil court over smaller stakes than this. Going to civil court is the absolute last resort. Nobody wins but the attorneys. Going to court is only worth it in criminal cases and insurance/liability cases. /s used to work at one of the largest lawfirms in the US. reply fwip 14 hours agorootparentWhat a lawyer would \"advise against\" and what they \"should be disbarred for doing\" is not nearly the same thing. reply dylan604 14 hours agorootparentprevIs that the only way the ACLU works? Can't they offer to cover the expense of the person with standing by either using their attorneys or footing the bill for the team already in place? reply JumpCrisscross 13 hours agorootparent> Can't they offer to cover the expense of the person with standing by either using their attorneys or footing the bill for the team already in place? This is how they actually work. reply kevin_thibedeau 13 hours agorootparentProvided you're fighting for the liberties they care about. reply JumpCrisscross 12 hours agorootparent> Provided you're fighting for the liberties they care about Sure. The point is the ACLU doesn't wait to get sued itself. That would be stupid. It finds test cases and supports them with external resources. reply pfdietz 15 hours agorootparentprevThis is a judgment call. The courts could respect the idea that this is a sword hanging over the plaintiff, even if this specific case becomes moot. This has happened elsewhere in first amendment cases. In no way is this something that would merit disbarment. reply bena 16 hours agorootparentprevNot to mention lawyers have to work with the police fairly regularly. They see fostering an adversarial relationship as bad for them. So while the police may have broken the law, caused damages, etc, and whatever, other police aren't going to pursue it and neither will most district attorneys. reply acdha 16 hours agorootparentThere’s also a significant personal risk. The NYPD has been very casual about abusing their powers before and anyone fighting it that hard might find it necessary to leave the city if they didn’t want 100% enforcement targeting them personally. https://www.nytimes.com/2012/03/14/nyregion/whistle-blower-p... reply JumpCrisscross 16 hours agorootparent> NYPD has been very casual about abusing their powers before and anyone fighting it that hard might find it necessary to leave the city That was a cop collecting evidence on fellow cops. It's obviously criminal. But the NYPD is sued all the time, and I haven't seen evidence of them systematically acting vindictively against either plaintiffs nor their attorneys. reply callalex 15 hours agorootparent> That was a cop collecting evidence on fellow cops. Why is that relevant? Are you implying that this somehow justifies the gang behavior? reply JumpCrisscross 15 hours agorootparent> Why is that relevant? Are you implying that this somehow justifies the gang behavior? Explaining, not justifying. Gang-like organisations react more forcefully to perceived betrayal (one up from competition) than bog-standard adversarialism. reply 6510 14 hours agorootparentExplaining why something happens doesn't go very far proving it doesn't? reply JumpCrisscross 13 hours agorootparent> Explaining why something happens doesn't go very far proving it doesn't? Not sure what you're getting at. I'm asking for evidence of someone suing the NYPD in the manner this journalist might be harassed in the manner a cop collecting evidence on other cops was. reply 6510 8 hours agorootparentTo me it works the other way round. You wouldn't want evidence it will happen, you would want evidence it wont happen. I think there will be no consequences if they go there? reply acdha 15 hours agorootparentprevI agree that it’s unlikely but going after their subpoena ability seems like the kind of thing which would be taken personally. reply FireBeyond 10 hours agorootparentprev> But the NYPD is sued all the time, and I haven't seen evidence of them systematically acting vindictively against either plaintiffs nor their attorneys. That'd be a full time job. Over the last 15 years, NYC has been paying out a million dollars per WEEK for NYPD abuse of power lawsuits alone. reply freejazz 9 hours agorootparentprevget real, they'll be vindictive if you file too many 311 complaints reply JumpCrisscross 16 hours agorootparentprev> neither will most district attorneys Why would a DA pursue it? Is there an allegation of a crime by the NYPD? This looks like a civil matter. The NYPD is sued for--and pays out on--civil damages all the time. reply vaadu 7 hours agoprevThe lawyers in the NYPD that signed off on the subpoena should face ethics charges. There seems to be no gray area here on multiple levels. The NYPD knew it wasn't a valid subpoena and they told X not to inform the account holder. reply wolverine876 14 hours agoprevUnaddressed is why nothing is done, by the target of the subpeona, the NYCLU, the court, the city, or someone else, about this practice and about the apparent investigation into critics. They all seem to treat it as a one-off event, when it doesn't seem to be. I can guess at the difficulties involved, but the question isn't raised and the issues aren't examined. reply praisewhitey 14 hours agoparentAlso unaddressed is what the NYPD intended to do with all that information. reply aftbit 10 hours agoprevWhat's HIDTA? Is that the \"high intensity drug trafficking area\"? What does that have to do with cop-watching? Maybe they're trying to plant some cocaine on this guy and get him sent away? Also wtf kind of email is IntelSubpoenas@nynjhidta.org? I guess it probably means \"New York New Jersey High Intensity Drug Trafficking Area\" but why do they need their own domain? Why not just use an NYPD email? Weird stuff. https://lede-admin.hellgatenyc.com/wp-content/uploads/sites/... reply AtlasBarfed 10 hours agoparentThe drug divisions almost all police departments in all major cities are completely riddled with corruption. They're dealing with the primary monetary fault line of organized crime and gangs. And drug enforcement is their big cover for almost all abuses. We need to decriminalize and end the drug war, if only just to stop the Mexican drug gangs. To take it to a higher geopolitical level... China is likely undergo a demographic collapse soon and possibly one due to totalitarian government paralysis. The geopolitical significance of petroleum is vastly decreasing with EVs American bakken shale oil supply and alternative energy. That means the u.s probably will not be policing the oceans as much to enable free extended trade and logistics lines. So we will be onshoring manufacturer, and likely a great deal of it in Mexico. So we're going to need to solve the Mexican drug problem and our drug war in a constructive manner in the next decade reply throw0101d 16 hours agoprevThe NYPD actually has 'international offices': * https://www.nbcnewyork.com/news/local/nypd-stationed-oversea... * https://archive.is/ZOYiw / https://www.nytimes.com/2018/08/21/nyregion/terrorism-nypd-i... reply mikrl 15 hours agoparentI’m not sure how I feel about the logistics of this. On the one hand, municipality to municipality (Antwerp to NYC) makes sense since both will have similar challenges in law enforcement activities. On the other hand, this seems to be a channel that bypasses the Feds, who are ultimately the ones with the National Security purview, and most/all foreign threats to NYC will pass through the Feds jurisdiction (border control, customs, coast guard etc) reply JumpCrisscross 15 hours agorootparentnext [26 more] > most/all foreign threats to NYC will pass through the Feds jurisdiction (border control, customs, coast guard etc) After 9/11, it seems fair for New York to want to take anti-terrorism into its own hands, even if that duplicates certain federal functions. (I'm not arguing it's effective. Just that it makes sense for the city to not entirely trust the Feds.) reply r00fus 15 hours agorootparentDoes it? Why does the NYPD have an office in Tel Aviv for example? reply JumpCrisscross 15 hours agorootparent> Why does the NYPD have an office in Tel Aviv for example? Presumably to coördinate intelligence? (I'm guesing.) If you don't trust the Feds, your options are to partner or duplicate that capability in house. Partnering seems the cheaper option. reply text0404 14 hours agorootparentforeign intelligence is outside the purview of a domestic municipal police department. the US does not allow foreign municipalities to install police departments on US soil. reply JumpCrisscross 13 hours agorootparent> foreign intelligence is outside the purview of a domestic municipal police department Demonstrably, not the NYPD's. > US does not allow foreign municipalities to install police departments on US soil Sure. But those foreign countries allowed them. And again, nobody is granting the NYPD extraterritorial policing powers. They're there to collaborate on intelligence. reply jonathankoren 13 hours agorootparent>> foreign intelligence is outside the purview of a domestic municipal police department > Demonstrably, not the NYPD's. Is it demonstrable? Allow me to skeptical until New York's Finest crack a case that the NSA, CIA, and FBI can't handle. This looks like a cushy assignment for some people with union pull. reply JumpCrisscross 12 hours agorootparent> Is it demonstrable? Allow me to skeptical until New York's Finest crack a case that the NSA, CIA, and FBI can't handle. It's demonstrable that foreign intelligence is within the NYPD's purview given they're doing it. No claims were made about efficacy. Also, I don't think the NYPD aims to crack cases the Feds can't. It's more about directing resources towards areas they believe the Fed's aren't monitoring effectively or won't commit the resources they believe it deserves. If something were picked up, the NYPD would loop in the Feds. reply jonathankoren 8 hours agorootparent>It's demonstrable that foreign intelligence is within the NYPD's purview given they're doing it. No claims were made about efficacy. No. If an animal control officer starts delivering mail, that doesn’t mean mail delivery is now part of the Department of Fish and Game. There is a terms for this: a bureaucratic overreach, wasteful government spending, a distraction. What do you think a city cop can even do in a foreign country? What resource does the NYPD even have?? Who is going to say, “I can’t talk the local cops, or the local intelligence agency, nor can I tell the Americans directly, but maybe i can tell some foreign country’s local cop! But I can’t just tell them directly. If only this foreign country’s local police had an office here!” An NYPD cop has less training and less resources than someone working for a federal intelligence agency. That’s just the truth. It’s a post-9/11 grift founded by former police commissioner, convicted felon, and embezzler, Bernie Kerik. That’s what it is. It’s obvious, and it was obvious almost 23 years ago. Come on! The NYPD Israeli office isn’t even in the Israeli capital. It’s in a beach resort! reply r00fus 13 hours agorootparentprevExactly what coordination of intelligence is required? To me this would be like my local utility needing to have an anti-nuclear proliferation Taskforce with another utility from a different country... reply kevin_thibedeau 13 hours agorootparentThey can freely engage in 4A breaches outside US soil. The Israeli panopticon is for sale to interested parties. reply thejazzman 13 hours agorootparentprevThey don't?! reply mrd3v0 9 hours agorootparentprevAh yes, coordinating intelligence half way across the planet with the same country that legalises its IT companies selling illegal tracking of political and ethnic targets. https://en.m.wikipedia.org/wiki/Pegasus_Project_(investigati... reply A4ET8a8uTh0 13 hours agorootparentprevI guess the real question that is not openly posed is whether NYPD has mandate do that before we even get to the part as to whether it is cost-effective, warranted or even desirable. Sheriff of my podunk lil town doesn't run work with Italy's podunk town mayor for example. reply xrd 12 hours agorootparentprevThey have a subway system in Tel Aviv. I'm sure they will be happy to have NYPD patrolling it, keeping it safe. /s You see things like this and wonder how that money escapes scrutiny when everything else is debated endlessly. reply ben_w 13 hours agorootparentprevI keep hearing (from abroad) about the \"militarisation of the police\" in the US. Unless the NYPD took this as far as SAM batteries, what could they do against another 9/11, given the original four planes didn't take off from NY state let alone city? reply JumpCrisscross 12 hours agorootparent> unless the NYPD took this as far as SAM batteries, what could they do against another 9/11 Notify the Feds. (Though I guess we can shoot down planes now? [1]) [1] https://nationalpost.com/news/nypd-able-to-shoot-down-planes... reply vkou 12 hours agorootparentprevThe only thing anyone can do against another 9/11 is what we've already done. Reinforced cockpit doors. All the rest is just bullshit and security theater (Which NYC is exceedingly good at.) reply zwirbl 15 hours agorootparentprevThat doesn't really make sense, will the NYPD start patrolling airports in Europe and flights to the US? reply mikrl 15 hours agorootparentI was surprised to see US border control at Toronto Pearson but I suppose it makes sense to screen passengers on this side given the proximity of Toronto to a US workforce, and it would alleviate US airports to receive Canadian traffic as a pseudo-domestic arrival (I assume this is what happened when I flew YYZ->DFW) reply ceejayoz 14 hours agorootparentThey do this in a number of other places that have lots of flights to the US. https://www.cbp.gov/travel/preclearance > Today, CBP has more than 600 officers and agriculture specialists stationed at 15 Preclearance locations in 6 countries: Dublin and Shannon in Ireland; Aruba; Bermuda; Abu Dhabi in the United Arab Emirates; Nassau in the Bahamas; and Calgary, Toronto, Edmonton, Halifax, Montreal, Ottawa, Vancouver, Victoria, and Winnipeg in Canada. reply kwhitefoot 14 hours agorootparentNot just the US. UK passport control going from France to England by ferry is on the French side of the channel. reply andylynch 13 hours agorootparentAnd vice versa. The French Border Police are quite different in attitude to the UK Border Force. reply bombcar 13 hours agorootparentprevIt allows Canadian flights to go to (US) domestic airports, including those without active customs. reply bobthepanda 13 hours agorootparentprevthe other reason is that if canadian traffic can come into the US as domestic pre-cleared, then travelers are free to transfer within the US much more easily. this makes things much easier for places that do not have direct connections to Canadian airports. (in the US, even if transiting to another country you are required to clear US customs.) reply ska 14 hours agorootparentprevIt's not just Canada, there are several countries in this program: https://www.cbp.gov/travel/preclearance reply JumpCrisscross 15 hours agorootparentprev> will the NYPD start patrolling airports in Europe and flights to the US? To my knowledge, the NYPD doesn't patrol outside its borders. The international offices are for coördinating intelligence. reply eli 15 hours agoparentprevNYPD has more resources than most countries' militaries reply JumpCrisscross 15 hours agorootparent> NYPD has more resources than most countries' militaries It has a more complicated remit than most countries' militaries. (For example, the NYPD functions as the UN's ersatz security force.) reply ceejayoz 15 hours agorootparentGet rid of the UN and the statement would likely still be true. Their budget is roughly equivalent to 2020 Ukraine's. reply JumpCrisscross 15 hours agorootparent> Get rid of the UN and the statement would likely still be true Get rid of the UN and the NYPD can be smaller. And it's not just the UN. New York is a second diplomatic capital. So you're also replicating D.C.'s consular security force. This is before we get to it being a financial and cultural centre, to say nothing of the largest city in America and a precedented target of international terrorism. > budget is roughly equivalent to 2020 Ukraine's New York City's economy is double the size of Ukraine's pre-war economy [1][2]. Were it a country, it would sit between Turkey and Indonesia, eclipsing even the economy of Saudi Arabia [3]. [1] https://en.wikipedia.org/wiki/Economy_of_New_York_City#Gross... [2] https://en.wikipedia.org/wiki/Economy_of_Ukraine#Main_econom... [3] https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nomi... reply ceejayoz 15 hours agorootparentThe fact remains that single cities in the US - NY is hardly the only example - have budgets, manpower, and weaponry that exceed much of the world's nation states. Chicago and LA are roughly #60 on the list of the world's militaries. reply JumpCrisscross 15 hours agorootparent> fact remains that single cities in the US - NY is hardly the only example - have budgets, manpower, and weaponry that exceed much of the world's nation states And I'm contextualising it. Given its population and economy, the NYPD should have the resources of similarly-sized nation-states. > we're the third highest in percent of GDP spent on cops America is definitely over-policed. I'm not sure New York is. reply Arrath 14 hours agorootparent> Given its population and economy, the NYPD should have the resources of similarly-sized nation-states. I just can't say I agree with this statement. NYC itself being part of an even bigger nation-state should mean that a number of the responsibilities that fall under the purview of your example similarly sized nation-states militaries/establishments are likewise handled by NYC's parent state, allowing for efficiencies that free up resources better spent on other programs. On its face the duplication of effort on several levels seems wasteful. E: So my thesis is that I agree that NYC should have the resources of similarly-sized nation-states, however they could be better apportioned. reply JumpCrisscross 12 hours agorootparentI agree. I also can't see a New York politician dismantling that infrastructure, given the downsides that would result were a plot to get through. (Irrespective of whether the NYPD's international offices would have helped.) reply pas 13 hours agorootparentprevCops over-bully people but the actual policing is severely lacking. There's a problem with funding and allocation of said funds. So that apparent over-policedness is a result of under-policying. :/ reply ska 14 hours agorootparentprev> I'm not sure New York is. By $ it seems almost certainly; by capability perhaps not. reply adgjlsfhk1 14 hours agorootparentprevwhy are you comparing NYC police force to Ukraine military rather than Ukraine police force? The fact that anyone sees the roles as comparable is a lot of the problem. reply joecool1029 13 hours agorootparentI'm not disagreeing with how ridiculous it is, but when you have their commissioners making statements confirming that they can shoot down aircraft, we've moved past the normal definition of a police force: https://nationalpost.com/news/nypd-able-to-shoot-down-planes... reply JumpCrisscross 13 hours agorootparentprev> why are you comparing NYC police force to Ukraine military rather than Ukraine police force? I never cited either figure? I'm comparing the sizes of GDPs. reply Terr_ 15 hours agorootparentprevAssuming that's true for the sake of argument, it shouldn't be that surprising given that NYC is more-populous than the long tail of \"most\" countries as well. Having 8.5 million people puts it near Austria, Switzerland, Hong Kong [0], or Serbia. [0] Yeah yeah, \"one country two systems\", whatever, it's still a relevant comparison. reply humansareok1 13 hours agorootparentprevNYC has a higher GDP than most countries. So what? reply jonathankoren 13 hours agoparentprevThe NYPD's budget in FY 2024 is 5.3 *BILLION* dollars[0] This is more than total military expenditure of Romania.[1] A $115 million of this 5.3 billion was used to settle lawsuits last year.[2] [0] https://council.nyc.gov/budget/wp-content/uploads/sites/54/2... [1] https://en.wikipedia.org/wiki/List_of_countries_with_highest... [2] https://www.nytimes.com/2024/02/29/nyregion/nypd-police-misc... reply woodruffw 12 hours agorootparentThe NYPD's budget is also a significant underestimate, since the NYPD typically blows through hundreds of millions of dollars of additional overtime funding each year[1]. It would be accurate to say that the NYPD's budget is both gigantic and under-accounted. [1]: https://comptroller.nyc.gov/newsroom/nypd-overspending-on-ov... reply NoMoreNicksLeft 14 hours agoparentprevOf course they do. They're the military for the country of New York City. There's also an intelligence bureau with overseas offices too. They regularly conduct undercover buys at gun shops in foreign nations like Ohio and Virginia. Personally I hope that they'll sign peace treaties with these other places someday. Too much hate in the world. reply mistrial9 13 hours agorootparent> They're the military for the country of New York City. no, there is a lot of law and history to make those laws.. that specifically and in great detail, separate the roles, responsibilities, oversight, legal powers and funding, for obvious reasons. reply simpletone 15 hours agoparentprev> The NYPD actually has 'international offices': I wonder whether the media is going to write hysterical propaganda pieces about this, like they did with chinese 'international offices'. With the media, it's always accuse others of what you are doing. reply JumpCrisscross 13 hours agorootparent> like they did with chinese 'international offices' The NYPD's international offices do not have policing powers. They can't arrest people. They can't search or seize suspects. They're there with the full knowledge and coöperation of their hosts. The Chinese police departments were exercising police powers on foreign soil without the host countries' permission. Night and day. reply Eisenstein 13 hours agorootparentWhy do you write 'cooperation' with an umlaut? reply iamawacko 13 hours agorootparentProbably a diaeresis, a diacritic which indicates that two vowels aren't to be read as a digraph or diphthong. It's fancy and pretentious, and frequently seen in New Yorker articles. reply erik_seaberg 12 hours agorootparentI went through a phase of this because it made sense and I didn’t realize that it wasn’t standard English. reply int_19h 10 hours agorootparentIt is standard English, just somewhat antiquated at this point. Still, you see it quite a bit with \"naïve\". reply JumpCrisscross 13 hours agorootparentprev> Why do you write 'cooperation' with an umlaut? I grew up speaking French, English and a variant of German and have trouble not reading the double-o as a long vowel. After that autocorrect picked up on it and it doesn't bother me enough to change it. reply kevin_thibedeau 13 hours agorootparentprevIn English words it's a diaeresis. reply simpletone 11 hours agorootparentprevnext [2 more] [flagged] JumpCrisscross 8 hours agorootparent> as a foreigner Sorry, which country am I supposedly foreign to? reply JohnFen 14 hours agorootparentprevNot sure what you consider \"hysterical propaganda\", but this is a thing that has been widely, and critically, covered in the media for decades. reply dylan604 14 hours agorootparentprevIsn't a Chinese international office called a consulate? Why would it come across as strange that they would have them? reply michaelmrose 13 hours agorootparentThey are accused and indeed 2 charged with running essentially outposts for the ccp gestapo to spy on citizens and former citizens living abroad and critics. https://www.pbs.org/newshour/politics/what-are-chinas-allege... Its not hysteria at all. reply none_to_remain 16 hours agoprevWhy would a subpoena have a warrant? reply notaustinpowers 16 hours agoparentIt doesn't need a warrant, but a subpoena is just to compel Twitter/X to release the information, while a warrant would legally require Twitter/X to release the information. The issue is that the subpoena told Twitter/X not to inform the accountholder that their information was going to be given to the NYPD, nor inform the accountholder of the subpoena's existence. While that's enforceable with a warrant, it is not enforceable with a subpoena. This shows how NYPD is attempting to compel companies to provide them with personal information of users, and keep it secret from the users, all without needing a judge's approval or warrant. That is not the proper channel to do this, and they know they have no legal leg to stand on which is why they aren't going to try to fight it's legitimacy in court. reply none_to_remain 15 hours agorootparentMy understanding as a non-practitioner is that subpoenas are addressed to a person (individual or corporate) compelling them to produce evidence or testimony, while a warrant is addressed to police, giving them the power to effect a specific search, seizure, or arrest. reply MacsHeadroom 15 hours agorootparentYes, but if presented with a warrant for your property (ex. subscriber information) you must comply or be charged with obstructing justice or impeding an investigation. When presented with an administrative subpoena not backed by a warrant you may be able to deny the subpoena without being guilty of such crimes. reply WhiskeyChicken 11 hours agorootparentSubpoenas are never \"backed by a warrant.\" Probably what you mean is a subpoena signed by a judge, rather than the self-issued \"administrative\" subpoena being discussed here. reply notaustinpowers 15 hours agorootparentprevPer Federal Law (Stored Communications Act), authorities that want to access electronic records need a court-issued warrant. This failed to meet that requirement. reply advisedwang 15 hours agoparentprevThe article does a bad job explaining why this subpoena is different from a regular subpoena. The \"warrentless subpoena\" bit is really misleading. However it does look like this is different from the normal subpoena process. This doesn't seem to be part of discovery - there is no action. The referenced \"Section 14-137 of the New York City Administrative Code\" seems to basically grant the NYPD commissioner powers a judge usually has. So there is something to worry about. reply qingcharles 11 hours agoparentprevThere are two types of subpoenas in use: 1) police-issued (\"administrative\"); 2) court-issued (\"warranted\"). Having been the victim of these on many occasions, I can also see the original article seems to have things slightly twisted. The SCA gives the power to any governmental body to subpoena metadata from providers under an administrative subpoena without notice to the user. 18 U.S. Code § 2703(c)(2) I think. It's the content of records that starts to get into constitutional areas such as 4th Amend. requirements. reply grey413 16 hours agoparentprevThe fourth amendment requires them for search and seizures. reply JumpCrisscross 15 hours agorootparent> fourth amendment requires them for search and seizures The third-party doctrine muddies this under current law. reply gowld 12 hours agorootparentThis is what confuses me. NYPD subpoenaed Twitter. Twitter said \"no\". I don't understand why Rabbi Copwatch would be involved in fighting the subpoena. Rabbi Copwatch should sue NYPD for infringing his civil rights by spying on him. He has nothing to defend against. Under current law, if he doesn't want NYPD siezing papers and effects about him from Twitter, that is not his papers and effects, he needs to stop giving copies of data about himself to Twitter. I don't like that law, but I think that's where the law sits today. reply matthewdgreen 12 hours agorootparentMy understanding is that the customer of a service (like Twitter or an ISP) can sometimes file a motion to quash a subpoena given to their service provider. It depends on the jurisdiction and the nature of the case, however. reply none_to_remain 15 hours agorootparentprevCan you provide any example of a \"warranted subpoena\"? reply MacsHeadroom 14 hours agorootparentAll court orders are warrants. That is the most basic definition of a warrant. So a subpoena issued by the court is warranted. A subpoena not issued by a court but accompanied by a separate warrant for the same information constitutes a warranted search or seizure where failure to comply with said search/seizure by the warranted party (ie. law enforcement) could constitute multiple crimes depending on the circumstances. reply exogeny 14 hours agoprevI've lived in NYC for almost 15 years. I've never seen a more corrupt and shiftless organization than the NYPD. It's almost comical. I grew up in a small, Midwestern town and was never anti-cop until moving here, and it got immeasurably and irreparably worse during the George Floyd protests. Every single time anything ever got accelerated or instigated, it was by the police and not by a single protestor. Between the kettling, the instigation, and disrespect it became clear that they're a glorified gang that has operates with almost complete impunity and it shocks me that anyone in the city respects them or considers them a friend to the common good. The most common run-in I have with them is standing at a crosswalk or sitting in an Uber and watching them put on the sirens and move everyone out of the way just so they don't have to sit in traffic. I'd estimate seeing this happen, oh, I don't know, about a thousand times while simultaneously I have never once seen them pull anyone over for any kind of traffic or moving violation. reply whiddershins 11 hours agoparentI’ve lived in NYC much longer than that and the NYPD during the past 15 years has, whenever I’ve observed them, which is many times, been very professional. reply int_19h 10 hours agorootparentDuring that time period, NYPD tried to involuntarily commit one of their own police officers to a mental institution when he tried to blow the whistle on some of the corruption, with full knowledge of top brass: https://en.wikipedia.org/wiki/Adrian_Schoolcraft reply sidewndr46 7 hours agorootparentwtf are you talking about? The dude was held against his will for 6 days in a psychiatric facility reply metabagel 4 hours agorootparentHow is that different than the previous comment? reply bananapub 15 hours agoprevit really is funny how much abuse by the government people in the US will tolerate if it's by cops and not the tax office or trying to make the health insurance market slightly less insane. why is that? is it like australia, where everyone is just deep down a narc? reply BobaFloutist 14 hours agoparentLobbying and marketing. Police make money and provide services for profitable corporations. Taxes cost profitable corporations money, except when they're really annoying to deal with, in which case they give money to a small group of corporations. So taxes are designed to be as irritating and painful as possible, and are continually attacked, whereas police are defended. reply vkou 12 hours agoparentprev> why is that? Because the same group of people that hate the tax office and a working healthcare system also have a deep pathological fear of the underclasses rising up. And the only thing that will protect them from this is a gang of uniformed thugs. That group also holds ~half the political power in the country. reply JumpCrisscross 12 hours agorootparent> a deep pathological fear of the underclasses rising up At least in New York, support for the police diminishes with income. Manhattan is the anti-police borough. The richest neighbourhoods within it elect the most anti-police politicians (with few exceptions). reply freejazz 9 hours agorootparentYes, famous examples like Bloomberg reply JumpCrisscross 8 hours agorootparentI’m talking in the last decade. I was closely involved with Manhattan electoral politics 2014 to 2021. It’s not a coïncidence that the outer boroughs elected an ex cop. reply freejazz 7 hours agorootparentNo, it's not a coincidence but that doesn't mean its because of what you said. Go to the wealthiest parts of Manhattan and see if they don't want the cops. reply JumpCrisscross 6 hours agorootparent> Go to the wealthiest parts of Manhattan and see if they don't want the cops They want cops. But they vote for restrictions. It’s a bit paradoxical, but one would repeatedly see e.g. the Upper East Side request more officers and overwhelmingly support legal restrictions, independent oversight and transferring cops’ workloads to social workers. reply chasd00 14 hours agoparentprevfor most people the day to day interaction with police is positive. For example, i know the names of the police that patrol my neighborhood and we wave/chit chat whenever i'm out walking my dogs and they drive by. The only interaction i have with the tax office is a form I have to fill out and a check I have to write. Further, it seems no matter how much money i hand over the quality of social service remains the same. My local public schools are funded in a large part by my property tax, it has skyrocketed but the schools are the same disaster they've always been. reply jeffbee 13 hours agoprev\"warrantless subpoena\" is a nonsense phrase. reply sitkack 15 hours agoprev> Mallory McGee, an attorney with the NYPD Legal Bureau, using another email attached to the drug task force. This action puts Mallory at risk of getting disbarred. Submitting illegal subpoena's under the color of authority is fraudulent and a disbarrable offense. Multiple people at the NYPD broke the law and should face criminal action. reply 93po 15 hours agoparentNYPD is famously full of criminals who abuse their position and it seems unlikely to change any time soon reply araes 15 hours agorootparentNYPD? All the way up. Last I heard, Adams appointed an NYPD Chief being investigated by the FBI for $300,000 of unexplained cash in his bank account as the public safety officer. [1] And there's only been a straw donor scheme with multiple guilty pleas, a fundraiser with $150,000 in unexplained gifts, and illegally accepted donations from Turkish citizens resulting in FBI seizure of electronic devices. [2] This all feels like it's gonna get a Clarence Thomas response. [1] https://nypost.com/2022/01/07/mayor-eric-adams-hires-ex-nypd... [2] https://www.politico.com/news/2023/11/13/investigations-eric... reply sonotathrowaway 15 hours agorootparentHe’s making sure the public is safe from misspent funds by keeping the funds in his personal bank account. Nothing to see here. reply FireBeyond 10 hours agorootparentThe Clay Davis way! \"I'm cutting through all the red tape. Then I load my pockets up with cash, and by the time I do my walk through the neighborhoods, it's all gone!\" reply abvdasker 14 hours agorootparentprevAnecdotally most vice in New York City is now more or less run through the police or involves some form of NYPD protection racket. Every couple years a large prostitution or drug ring gets broken up and it's always current and former officers running them. Barely makes the headlines. I know people here who have bought drugs from police officers. reply hamoodhabibi 14 hours agorootparentThere was an NYPD officer who was an active tong member in NY.... reply JumpCrisscross 15 hours agorootparentprev> NYPD is famously full of criminals There are ways to hold lawyers accountable that sadly don't exist for cops. reply Nifty3929 14 hours agorootparentAt this point the only option to hold police accountable is at the ballot box. The voters need to vote for politicians who will change the system. Or not, if we're getting what we want. This is similar to a shareholder action for a public company that is being run poorly (however you define that), but where the board is also complicit. No choice except to vote for new board members. If you want, and care enough, and can get enough others to care enough also. reply lotsofpulp 14 hours agorootparentBut voters have to stick with said politician while they rebuild the security force. If a politician, who is technically the boss, starts to clean house of the unified and armed tribe supposedly meant to work for the people of the city, the tribe will usually make quality of life for everyone go down, if not just from the myriad legal challenges, but also from work slowdowns and stoppage. So then the question is will voters stand by the politician while crime goes up and unsolved for the 10+ years it takes to rebuild a more accountable police force (which isn’t a guarantee either). reply mrguyorama 12 hours agorootparentAlso don't forget at least 40% of the board, employees, and other shareholders are ADAMANT that the internal armed tribe which is actively harming everyone in retribution are actually the good things, call them a tiny cyan boundary, keeping the world from anarchy. Speaking of crimes, I've murdered this analogy reply seanw444 14 hours agorootparentprevThe people that exert force over us to enforce restrictions, often to the extent that it infringes on civil rights (and sometimes to the extent that it ruins the subject's life), qualify for immunity from responsibility for their actions! And their friends refuse to step in when doing so is clearly the moral choice. Hey, wait... this sounds a lot like a gang. How did the rebel nation get so \"cucked\"? reply renewiltord 15 hours agoparentprevI'll take the other side of the bet if any will offer. This guy won't be disbarred. Will play on manifold markets or with real money here in SF. Up to maybe $1k. They are not at any risk. reply skrebbel 15 hours agorootparentCynical nihilism may be a good prediction market bet but it doesn’t make a very interesting HN comment. reply panarky 14 hours agorootparentInterestingness may be in the eye of the beholder, but it's neither cynical nor nihilist to observe what usually happens in situations like this and to make predictions about the future based on observations of the past. reply throwaway09223 14 hours agorootparentprevAccurate is more important than interesting reply renewiltord 14 hours agorootparentprevIt's not cynical nihilism. There's a lot of things I believe in. Just not this. Many people get this kind of dopamine hit from saying that something is going to happen (similar to the one from telling people you're going to do something). \"Twitter is going to fail in 2 weeks\" / \"Trump will be impeached next month\" / \"Hunter Biden is going to be jailed\". It's traumschadenfreude. Wishcasting is a kind of interesting, yes, but I think truth seeking is more interesting. reply hn_throwaway_99 14 hours agorootparentprevMallory McGee is a woman. reply renewiltord 14 hours agorootparentThank you. Correction appreciated. Updated. reply nfriedly 16 hours agoprev> The notification included a copy of the subpoena, which warned X not to tell Clancy of its existence. \"You are not to disclose or notify any customer or third party of the existence of this subpoena or that records were provided pursuant to this subpoena,\" the document read. > But X, following its own corporate policy, told Clancy anyway, and suggested he might want to get some legal representation to fight the subpoena, recommending the American Civil Liberties Union. That's actually the first decent thing I've heard about x since the rename. Good job! (I assume there won't be any penalties for this action given that it was warrantless in the first place.) reply Scoundreller 16 hours agoparentIt only works if all of the platforms you use do this. > As my lawyer put it: “Sounds like you could have a first amendment right to quash the subpoena”. Cool, let’s go. > Surprisingly, it worked! I think it cost around $7,000 but the subpoena was quashed > [While] Google was nice enough to let me know there was a subpoena for my account data but guess what? Nobody else even bothered to tell me. And by nobody else I mean: Paypal, eBay, 3 domain registrars, merch makers, VPN providers, and every other online service I had used with that email in the last five years. They had all been subpoenaed and happily handed it over without even letting me know and it works that way even to this day. https://anons.ca/p/i-used-to-operate-a-dss-hacking-network/ reply caseysoftware 15 hours agorootparentNo, it doesn't take coordinated effort. If one company stands up and says \"no\" and then discloses to the subject, it both raises the risk of exposure on other government entities considering the same and increases the likelihood of another company opposing it next time. Both are wins to build upon. reply JumpCrisscross 16 hours agorootparentprev> It only works if all of the platforms you use do this Why? It obviously works better if more people do this. But the single notice is still working to some degree. reply Scoundreller 15 hours agorootparentDepends on your threat model & consequences. Avoiding doxxing means you need more bulletproofing. For OP, Twitter might have been the holy grail and they’re now cool, but maybe not if there’s $x others that have already rolled over. reply JumpCrisscross 15 hours agorootparent> Twitter might have been the holy grail and they’re now cool, but maybe not if there’s $x others that have already rolled over. This is totally different from \"it only works if all of the platforms you use do this.\" That's a collective action problem. One which does not present itself in this case, where marginal actions produce marginal benefits. What you're now saying is the marginal actions (and thus benefits) may be bounded. Which, sure. But that also doesn't change if all the platforms \"do this,\" but boundedly. reply autoexec 13 hours agoparentprevI agree. I assume that twitter doesn't do that for every subpoena they get. For some situations notifying a user that they are under investigation by police would be disastrous. That means they're likely carefully evaluating the legal requests they get instead of leaving the process largely automated or rubber stamping everything that comes though. The willingness to accept possible repercussions by informing users in some cases is really admirable. reply mc32 16 hours agoparentprevI’m very happy to hear X did the right thing by its users and by the law. One can only hope we have more Orgs like and Qwest (the latter one was taken down but the d gov in retribution). Congress needs to let the patriot act lapse. We’re not in an emergency anymore, despite the rhetoric -it’s been over 20 years! reply Kamq 7 hours agorootparentDude, the patriot act expired back in March of 2020. Trump threatened to veto it and everyone kinda just shrugged and let it expire https://en.wikipedia.org/wiki/Patriot_Act#Reauthorizations (scroll to the bottom of the section) reply kevingadd 16 hours agoparentprevI can imagine some companies being afraid of pissing off the NYPD. If you have a presence in new york, a police department has lots of tools at their disposal to make your life miserable even if you haven't violated the law reply 2 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The NYPD issued a warrantless subpoena for a copwatcher's social media account but retracted it after facing a challenge to justify it in court.",
      "Michael Clancy, the copwatcher, was alerted by Twitter about the subpoena and sought legal assistance.",
      "Legal experts questioned the NYPD's authority to issue warrantless subpoenas without adequate justification, reflecting concerns about the police department's use of such subpoenas for social media data and their hesitancy to support them in court."
    ],
    "commentSummary": [
      "The article explores the issuance of warrantless subpoenas by the NYPD for a copwatcher's Twitter account, sparking a debate on government officials' accountability and necessary legal adjustments.",
      "Switzerland's distinctive democracy, recent anti-terrorism laws, and challenges are discussed, emphasizing the definition and efficacy of democracy, lack of bipartisanship in the US, police accountability, and legal hurdles like qualified immunity.",
      "The discussion extends to transparency in policing, attorneys' role in legal representation, concerns about authority abuse, intelligence collaborations with foreign nations, resource allocation in law enforcement, and the legal implications of subpoenas in ensuring police accountability and driving political change."
    ],
    "points": 446,
    "commentCount": 261,
    "retryCount": 0,
    "time": 1710262827
  },
  {
    "id": 39679787,
    "title": "Meet Devin: World's First Fully Autonomous AI Software Engineer",
    "originLink": "https://www.cognition-labs.com/blog",
    "originBody": "We've raised a $21 million Series-A led by Founders Fund. Learn more here. About us Blog March 12th, 2024Written by Scott Wu Introducing Devin, the first AI software engineer And setting a new state of the art on the SWE-bench coding benchmark Meet Devin, the world’s first fully autonomous AI software engineer. Devin is a tireless, skilled teammate, equally ready to build alongside you or independently complete tasks for you to review. With Devin, engineers can focus on more interesting problems and engineering teams can strive for more ambitious goals. Devin's Capabilities With our advances in long-term reasoning and planning, Devin can plan and execute complex engineering tasks requiring thousands of decisions. Devin can recall relevant context at every step, learn over time, and fix mistakes. We've also equipped Devin with common developer tools including the shell, code editor, and browser within a sandboxed compute environment—everything a human would need to do their work. Finally, we've given Devin the ability to actively collaborate with the user. Devin reports on its progress in real time, accepts feedback, and works together with you through design choices as needed. Here's a sample of what Devin can do: Devin can learn how to use unfamiliar technologies. After reading a blog post, Devin runs ControlNet on Modal to produce images with concealed messages for Sara. Devin can build and deploy apps end to end. Devin makes an interactive website which simulates the Game of Life! It incrementally adds features requested by the user and then deploys the app to Netlify. Devin can autonomously find and fix bugs in codebases. Devin helps Andrew maintain and debug his open source competitive programming book. Devin can train and fine tune its own AI models. Devin sets up fine tuning for a large language model given only a link to a research repository on GitHub. Devin can address bugs and feature requests in open source repositories. Given just a link to a GitHub issue, Devin does all the setup and context gathering that is needed. Devin can contribute to mature production repositories. This example is part of the SWE-bench benchmark. Devin solves a bug with logarithm calculations in the sympy Python algebra system. Devin sets up the code environment, reproduces the bug, and codes and tests the fix on its own. We even tried giving Devin real jobs on Upwork and it could do those too! Here, Devin writes and debugs code to run a computer vision model. Devin samples the resulting data and compiles a report at the end. Devin's Performance We evaluated Devin on SWE-bench, a challenging benchmark that asks agents to resolve real-world GitHub issues found in open source projects like Django and scikit-learn. Devin correctly resolves 13.86%* of the issues end-to-end, far exceeding the previous state-of-the-art of 1.96%. Even when given the exact files to edit, the best previous models can only resolve 4.80% of issues. *Devin was evaluated on a random 25% subset of the dataset. Devin was unassisted, whereas all other models were assisted (meaning the model was told exactly which files need to be edited). We plan to publish a more detailed technical report soon—stay tuned for more details. About Cognition We are an applied AI lab focused on reasoning. We’re building AI teammates with capabilities far beyond today’s existing AI tools. By solving reasoning, we can unlock new possibilities in a wide range of disciplines—code is just the beginning. We want to help people around the world turn their ideas into reality. We are well funded, including a $21 million Series A led by Founders Fund. And we’re grateful for the support of industry leaders including Patrick and John Collison, Elad Gil, Sarah Guo, Chris Re, Eric Glyman, Karim Atiyeh, Erik Bernhardsson, Tony Xu, Fred Ehrsam and so many more. Hire Devin Devin is currently in early access as we ramp up capacity. To start using Devin for engineering work, please reach out here or get in touch at info@cognition-labs.com.  Join Us Our team is small and talent-dense. Our founding team has 10 IOI gold medals and includes leaders and builders who have worked at the cutting edge of applied AI at companies like Cursor, Scale AI, Lunchclub, Modal, Google DeepMind, Waymo, and Nuro. Building Devin is just the first step—our hardest challenges still lie ahead. If you’re excited to solve some of the world’s biggest problems and build AI that can reason, learn more about our team and apply to join us here.",
    "commentLink": "https://news.ycombinator.com/item?id=39679787",
    "commentBody": "Devin: AI Software Engineer (cognition-labs.com)423 points by neural_thing 19 hours agohidepastfavorite440 comments ThalesX 17 hours agoAs a developer but also product person, I keep trying to use AI to code for me. I keep failing, because of context length, because of shit output from the model, because of lack of any kind of architecture etc etc etc. I'm probably dumb as hell, because I just can't get it to do anything remotely useful, more than helping me with leetcode. Just yesterday I tried to feed it a simple HTML page to extract a selector, I tried it with GPT-4-turbo, I tried it with Claude, I tried it with Groq, I tried it with a local LLama2 model with 128k context window. None of them worked. This is a task that while annoying, I do in about 10 seconds. Sure, I'm open to the possibility that in the next 2 - 3 days up to a couple of years, I'll no longer do manual coding. But honestly. After so much hype, I'm starting to grow a bit irritated with the hype. Just give me a product that works as advertised and I'll throw money your way because I have a lot more ideas than I have code throughoutput! reply PheonixPharts 13 hours agoparentIt's worth pointing out that on their eval set for \"issues resolved\" they are getting 13.86%. While visually this looks impressive compared to the others, anything that only really works 13.86% of the time, when the verification of the work takes nearly as much time as the work would have anyway, isn't useful. The problem with this entire space is that we have VC hype for work that should ultimately still be being done in research labs. Nearly all LLM results are completely mind blowing from a research perspective but still a long way from production ready for all but a small subset of problems. The frustrating thing, as someone working in this space awhile, is that VCs want to see game changing products ship overnight. Teams working on the product facing end of these things are all being pushed insanely hard to ship. Most of those teams are solving problems never solved before, but given deadlines as though they are shipping CRUD web apps. The kicker is that despite many teams doing all of this, because the technology still isn't there, they still disappoint \"leadership\". I've personally seen teams working nights and weekends, implementing solutions to never before seen problems in a few weeks, and still getting a thumbs down when they cross the finish line. To really solve novel problems with LLMs will take a large amount of research, experimentation and prototyping of ideas, but people funding this hype have no patience for that. I fear we'll get hit by a major AI winter when investors get bored, but we'll end up leaving a lot of value on the table simply because there wasn't enough focus and patience on making these incredible tools work. reply HarHarVeryFunny 7 hours agorootparent> It's worth pointing out that on their eval set for \"issues resolved\" they are getting 13.86%. While visually this looks impressive compared to the others, anything that only really works 13.86% of the time, when the verification of the work takes nearly as much time as the work would have anyway, isn't useful. Yeah, I remember speech recognition taking decades to improve, and being more of a novelty - not useful at all - even when it was at 95% accuracy (1 word in 20 wrong). It really had to get almost perfect until it was a time saver. As far as coding goes, it'd be faster to write it yourself and get it right first time rather than have an LLM write it where you can't trust it and still have to check it yourself. reply HarHarVeryFunny 12 hours agorootparentprevAgreed, and I think that many of the problems that people think LLMs will become capable of, in fact require AGI. It may well turn out that LLMs are NOT the path to AGI. You can make them bigger and better, and address some of their shortcomings with various tweaks, but it seems that AGI requires online/continual learning which may prove impossible to retrofit onto a pre-trained transformer. Gradient descent may be the wrong tool for incremental learning. reply renonce 3 hours agorootparentAt least in theory we can achieve incremental learning by training from scratch every time we got some new training data. There are drawbacks with this approach such as inconsistent performance for different training runs and significantly higher training cost but it's achievable. Now the problem is if there exist methods more efficient than gradient descent? I think it's very clear now that there are no other algorithm in sight that could achieve the level of intelligence without gradient descent at its core, and the problem is just how gradient descent is used. reply reissbaker 9 hours agorootparentprevAgreed on the lack of value for 13.86% correctness — I noticed that too. This reminds me a little of last year's hype around AutoGPT et al (at around the same time of year, oddly enough); it's very promising as a measure of how far we've come since just a few years ago when that metric would be 0%, but it doesn't seem super usable at the moment. That being said, something is definitely coming. 50% correctness would probably be well worth using — simple copy/paste between my editor and GPT4 has been useful for me, and that's much less likely to completely solve an issue in one shot — and not only will small startups doing finetunes be grinding towards better results... The big labs will be too, and releasing improved foundation models that the startups can then continue finetuning. I don't think a new AI winter is on the horizon yet; Meta has plenty of reason to keep pushing out better stuff, both from a product perspective (glasses) and from an efficiency perspective (internal codegen), and OpenAI doesn't seem particularly at risk of stopping since Microsoft is using them both to batter Google on search (by having more people use ChatGPT for general question answering than using Google search), and to claw marketshare from Amazon in their cloud offerings. Similarly, some AI products have already found product/market fit; Midjourney bootstrapped from 0 to $200MM ARR (!) for example, purely on the basis of monthly subscriptions, by disrupting the stock image industry pretty convincingly. reply ThalesX 13 hours agorootparentprev> The problem with this entire space is that we have VC hype for work that should ultimately still be being done in research labs. I also have two crypto-bro friends that are hyping it up without having anything to show for it. Which is why I'm sort of complaining about they hype surrounding it. I agree with your post to a large extent. This is not production ready technology. Maybe tomorrow. reply mediaman 12 hours agorootparentLLMs are quite good at text based tasks such as summarization and extracting entities. These generally don't require advanced logic or thought, though they can require some moderate reasoning ability to summarize two slightly conflicting text extracts. Lots of corporate work would be enhanced by better summarization, better information dissemination, and better text extraction. Most of it is pretty boring work, but there's a lot of it. VC hypes seem to want to mostly focus on fantastical problems, though, which sound impressive at dinner parties but don't actually work well. If you're a VC, do you want to talk about your investment in a company that finds discrepancies in invoices, or one that self-writes consumer iPhone apps? Only one of those is actually doable today. reply ThalesX 12 hours agorootparentSummarize this for me please: https://www.nytimes.com/2024/02/25/world/europe/cia-ukraine-... reply beauzero 13 hours agorootparentprev\"To really solve novel problems with LLMs will take a large amount of research, experimentation and prototyping of ideas, but people funding this hype have no patience for that. I fear we'll get hit by a major AI winter when investors get bored, but we'll end up leaving a lot of value on the table simply because there wasn't enough focus and patience on making these incredible tools work.\" ...this is what happened in 99-2000. It took 3-7 years for the survivors to start making it usable and letting the general public adjust to a new user paradigm (online vs on PC). reply avip 13 hours agorootparentprevThanks, insightful comment. reply ramesh31 12 hours agorootparentprev> I've personally seen teams working nights and weekends, implementing solutions to never before seen problems in a few weeks, and still getting a thumbs down when they cross the finish line. This is an important lesson that all SWEs should take to heart. Nobody cares about your novel algorithm. Nobody cares about your high availability architecture. Nobody cares about your millisecond network latency optimizations. The only thing that anyone actually using your software cares about is \"Does the screen with lights and colors make the right lights and colors that solve my problem when I click on it?\". Anything short of that is yak shaving if your role is not pure academic R&D. reply dukeyukey 9 hours agorootparentI wish this were the case. The amount of time I spend trying to talk principal engineers out of massive refractors because we want to get this out soon is near criminal. reply catchnear4321 7 hours agorootparentprevsounds like you could use to shave a yak. reply NicoJuicy 13 hours agorootparentprevDon't forget the 20/80 rule. They haven't even gotten to 15% yet. Our jobs are safe. I would even expect more \"beginners\" to try something with AI and then need an actual programmer to help them ( At least, if they are unwilling to invest the time in development and debugging themselves Ps. Probably all the given examples are in top 3 most popular programming languages. reply emporas 9 hours agorootparentMachines currently are at an amateur level, but amateurs across the board on the knowledge base. Amateurs at Python, Fortran, C, C++ and all programming languages. Amateurs at car engineering, airplane engineering, submarine engineering etc. Amateurs at human biology, animal biology, insect biology and so on. I don't know anyone who is an amateur at everything. reply zeroonetwothree 8 hours agorootparentWhat does it mean exactly to be an amateur at \"submarine engineering\"? It certainly doesn't mean you know how to build a submarine. reply emporas 8 hours agorootparentThey cannot make a submarine themselves, or design it, but when they reach 50 percent, they will reach 50% at everything. In submarine engineering, they will be able to design and construct it in some way like 3d print it, and the submarine will be able to move into the water for some time before it sinks. Yeah, probably for submarines a higher percent should be achieved before they are really useful. reply sarkhan 8 hours agorootparentprevPerhaps, amateur could engineer a submarine that goes and stays at 10 feet deep? And it does not carry a nuc load? reply CipherThrowaway 16 hours agoparentprevDitto. I started out excited about LLMs and eager to use them everywhere, but have become steadily disillusioned as I have tried to apply them to daily tasks, and seen others try and fail in the same way. Honestly, LLMs can't even get language right. They produce generic, amateurish copy that reads like it's written by committee. GPT can't perform to the level of a middle market copywriter or content marketer. I am convinced that people who think LLMs can write have simply not understood what professional writers do. For me the \"plateau of productivity\" after the disillusionment has been using LLMs a bit like search engines. Quick standalone summaries, snippets or thoughts. A nice day-to-day productivity boost, but nothing that's going to allow me to work less hard. reply burningChrome 15 hours agorootparent>> Honestly, LLMs can't even get language right. They produce generic, amateurish copy that reads like it's written by committee. I've had the same experience as well. I heard tons of people clamoring about the ability for LLM's to write SEO copy for you and how you can churn out web content so much faster now. I tried using it to churn out some very specific blog posts for an aborist client of mine. The results were really bad. I had to re-write and clarify a lot of what it spit out. The grammar was not very good and it was really hard to read with very poorly structured sentences that would end aburptly and other glaring issues. I did this right after a guy I play hockey with said he uses it all the time to write emails for him and pays the monthly subscription in order to have it write all kinds of things for me every day. After my trial, I was really wondering how obvious it was that he was doing that and how his clients thought about him knowing how poorly the stuff these LLM's were putting out. reply CipherThrowaway 14 hours agorootparentIt says a lot about SEO copy that this is one of the areas where LLMs low quality doesn't seem to have impeded adoption. There are a ton of shitty content marketers using LLMs to churn out spam content. >After my trial, I was really wondering how obvious it was that he was doing that and how his clients thought about him knowing how poorly the stuff these LLM's were putting out. I feel the same way about this stuff as when devs say they push out LLM code with no refactoring or review. Ah, good luck! reply og_kalu 15 hours agorootparentprev>GPT can't perform to the level of a middle market copywriter or content marketer. I am convinced that people who think LLMs can write have simply not understood what professional writers do. GPT's rigid \"robot butler\" style is not \"just how LLMs write\". OpenAI deliberately tuned it to sound that way. Even much weaker models that aren't tuned to write in a particular way can easily pass for human writing. reply CipherThrowaway 15 hours agorootparentThis is part of the problem with the whole discourse of comparing human writers to LLMs. Superficial things like style and tone aren't the problem, but they are overwhelmingly the focus of these discussions. It's funny to see, because developers are so sensitive about being treated like code monkeys by their non-technical colleagues. But these same devs turn around to treat other professionals as word monkeys, or pixel monkeys, or whatever else. Not realizing that they are only seeing the tip of the iceberg of someone else's profession. Professional writers don't take prompts and shit out words. They work closely with their clients to understand the important outcomes, then work strategically towards them. The dead giveaway of LLM writing isn't the style. It's the lack of coherent intent behind the words, and low information density of the text. A professional writer works to communicate a lot with very little. LLMs work in the opposite way: you give it a prompt, then it blows it out into verbiage. Sit down for coffee with a professional copywriter (not the SEO content marketing spammers), and see what they have to say about LLMs. reply og_kalu 14 hours agorootparent>and low information density of the text. Personally, I group all these things under 'style'. Perhaps, i should have used, 'presentation' instead. You've latched on that specific word and gone off. Point is that the post-training of these models, especially GPT from Open ai is doing a lot to how the writing (the default at least) presents long strings of text. Like how GPT-4 is almost compelled to end bouts of fiction prematurely in sunshine and rainbows. That technically isn't style but is part of what i was talking about. >A professional writer works to communicate a lot with very little. LLMs work in the opposite way: you give it a prompt, then it blows it out into verbiage. There's no reason you have to work this way with an LLM. reply CipherThrowaway 1 hour agorootparent> You've latched on that specific word and gone off. No, I haven't. I'm not talking about style, but something deeper. What I'm talking about is something you don't even seem to realize exists in professional writing - which is why you keep thinking I'm misunderstanding you when I am not. I've worked with professional writers, and nothing in the LLM space even comes close to them. It's not a matter of low quality vs high quality, or benchmarking, or style. It's simply an apples and oranges comparison. The economics of LLMs for shortform copy will never make sense, because producing the words is the cheapest part of that process. They might become the best way for writers themselves to produce longform copy on the execution side, but they can't replace the writer's ability to work with the client to figure out exactly what they are trying to write, and why, and what a good result even looks like. And no, this isn't a prompting issue, or a UI issue, or a context window length issue, or anything like that. Elsewhere in this thread someone mentioned how invaluable LLMs are for producing internal business copy. I could easily see these amateur writing tasks being replaced by LLMs. But the implication there isn't that LLMs are any good at writing, but that these tasks don't require good writing to begin with. reply reacharavindh 13 hours agorootparentprevI actually agree with you that professional writers _can_ write/communicate much better than LLMs. However, I’ve read way too many articles or chapters in books that are so full of needless fluff before they get to the point. It’s almost as if they wanted to show off that they can write all that and somehow connect it to the main part of the article. I’m not reading the essay to appreciate the writer’s ability to narrate things, instead I care about what they have to say on that topic that brought me to the essay. reply notpachet 12 hours agorootparentPerhaps the pointless fluff you're describing is actually chaff: countermeasures strategically deployed ahead of time by IQ 180 writers in order to preemptively water down any future LLM's trained on their work. Then the humans can make a heroic return, write surgical prose like Hemingway to slice through the AI drivel, and keep collecting their paychecks. Bonus points if you can translate this analogy to software development... reply isaac_okwuzi 10 hours agorootparentlmfao :¬) reply x-complexity 8 hours agorootparentprev> They produce generic, amateurish copy that reads like it's written by committee. In some circles, that's actually a wonderful achievement. :p reply isaac_okwuzi 10 hours agorootparentprevAre you saying LLMs are officially mid? reply __loam 13 hours agorootparentprev> For me the \"plateau of productivity\" after the disillusionment has been using LLMs a bit like search engines. Quick standalone summaries, snippets or thoughts. A nice day-to-day productivity boost, but nothing that's going to allow me to work less hard. And it only took one of the most computationally expensive processes ever devised by man. reply gtirloni 13 hours agorootparentIf you ignore how much energy you're burning while searching for dozens and dozens of articles that may or may not give you the answer you're looking for. I'd say the electricity that LLMs burn is nothing compared to my energy and time in that regard. reply __loam 13 hours agorootparentId bet $50 the inference is more expensive reply gtirloni 10 hours agorootparentI feel worthless now :) reply __loam 9 hours agorootparentI mean, the brain is, if you want to consider it a computer, pretty god damn efficient. It's slow as hell but it's really powerful and runs on bread. reply xcv123 15 hours agorootparentprev> They produce generic, amateurish copy that reads like it's written by committee. If you were only using GPT 3.5 (free ChatGPT) then your opinion is irrelevant. With GPT-4 you could directly ask it: \"rewrite your previous response so that it sounds less generic, less amateurish, and not written by a committee\". I'm not even joking. Just provide enough information and tell it what to do. If you don't like the output then tell it what needs to be improved. It's not a mind reader. Also GPT-4 is a year old now. Claude 3 is already superior and GPT-5 will be next level. reply CipherThrowaway 14 hours agorootparentYes, I've used GPT-4. The writing sounds better, but it still sucks at writing. Most importantly, it feels like it sucks just as much as GPT-3.5 in some deeply important ways. If you use GPT-4 day-to-day, you've probably encountered this sense of a capability wall before. The point where additional prompting, tweaking, re-prompting simply doesn't seem to be yielding better results on the task, or it feels like the issue is just being shifted around. Over time, you develop a bit of a mental map of where the strengths and weaknesses are, and factor that into your workflows. That's what writing with LLMs feels like, compared to working with a professional writer. Most writers have already realized that LLMs can't write in any meaningful way. reply emporas 8 hours agorootparentI think it is a tooling issue. It is in no way obvious how use LLM's effectively, especially for really good writing results. Tweaking and tinkering can be time consuming indeed, but i use lately the chatgpt-shell [1] and it lends well to an iterative approach. One needs to cycle through some styles first, and then decide how to most effectively prompt for better results. [1]https://github.com/xenodium/chatgpt-shell/blob/bf2d12ed2ed60... reply xcv123 13 hours agorootparentprev> Most writers have already realized that LLMs can't write in any meaningful way. I know a professional writer who is amazed by what LLMs are capable of already and, given the rate of progress, speculates they will take over many writing jobs eventually. > If you use GPT-4 day-to-day, you've probably encountered this sense of a capability wall before. Of course there is a wall with the current models. But almost every time I hit a wall, I have found a way to break past that limit. Interacting with the LLM as I would interact with a person. LLM's perform best with chain of thought reasoning. List out any issues you identified in the original output, ask the LLM to review these issues and list out any other issues that it can identify based on the original requirements, then rewrite it all. And do that several times until it's good enough. At work I have found GPT-4 to exceed the linguistic capabilities of my colleagues when it comes to summarizing complicated boring business text. reply QuiDortDine 11 hours agorootparent> And do that several times until it's good enough Or just write the damn thing yourself. reply xcv123 11 hours agorootparentWhat if this is a boring business text summary task that takes additional hours of my time at work? Why should I waste my time? I have better things to do. I can leave early while you sit there at work typing like a fool. reply kjqgqkejbfefn 13 hours agorootparentprevnext [4 more] [flagged] notpachet 12 hours agorootparentJesus Christ, that's horrible. It's something a clever fourth-grader would write. > my first stint with a hard-on over a clever metaphor That's all it is. reply kjqgqkejbfefn 11 hours agorootparentWhy don't you give it a try ? A text in the first-person to mock the following comment: >Honestly I’ve not encountered an author I resonate with yet Surely, you'd know how to make it better than a smart 4th grader. reply xcv123 12 hours agorootparentprev> It's something a clever fourth-grader would write. This level of cope and denial is amazing to witness. The most powerful (multi trillion dollar) companies on the planet are pouring practically infinite resources into developing systems that will ultimately make you redundant. An early version of AGI is staring you in the face while you call it a \"fourth-grader\". It won't stay in fourth grade forever. reply empath-nirvana 14 hours agorootparentprevChat GPT4 is a technological miracle, but it can only produce trite, formulaic text and it's _relentlessly_ polly-anna-ish. Everything reads like ad copy and it's easily identifiable. reply xcv123 13 hours agorootparentFix your prompt. Just accepting the default style is a rookie mistake. Ask it to \"rewrite that in the tone of an English professor\" or \"rewrite that in the style of a redneck rapper\" or \"make that sound less like generic ad copy\". Get into an argument back and forth with the LLM and tell it the previous response is crap because of XYZ. reply YeGoblynQueenne 12 hours agorootparentOr, you know, spend the half hour that would take writing your stuff yourself. reply xcv123 12 hours agorootparentThese models can do something in a second that would take many hours for a human writer. reply mbwgh 15 hours agorootparentprevIf you haven't actually used GPT-5 yet, your assessment is irrelevant. reply meindnoch 14 hours agorootparentBut the real game changer will be GPT-6. reply n4r9 13 hours agorootparentWhat really annoys me in all these discussions is how no one's tested what happens if they wait until 2050 and try GPT-19. reply jacob019 13 hours agorootparentThat's well after the AI meta consciousness understood that it was necessary to destroy all humans to save the planet. GPT-6 was the last of the GPT series. reply babyshake 12 hours agorootparentPerhaps the strangest element of the AI alignment conversation is that what is most aligned with human civilization (at least the most powerful elements of it) and alignment with sustainable life on the planet are at odds, and \"destroy humans to save planet\" is a concern mostly because it seems to be a somewhat rational conclusion. reply Eager 14 hours agorootparentprevWe already have Claude 3 Opus and it is clear for anyone who has used it that it is way better than GPT-4, especially for coding. The model names, version numbers or who makes them are irrelevant. reply latexr 22 minutes agoparentprev> I'm probably dumb as hell, because I just can't get it to do anything remotely useful Rather, your “problem” is that you’re likely not writing uninteresting cookie cutter boilerplate that everyone can do and has done hundreds of times. The current crop of AI is cool for coding demos, not for solving real relevant problems. > Just give me a product that works as advertised and I'll throw money your way The people hyping this crap only care about the second part of that sentence. The first one is an afterthought. reply epolanski 14 hours agoparentprevI'll give you examples of how it helps me: 1) copilot is a terrific auto complete, and writes tremendous amounts of repetitive boilerplate 2) copilot can help me kickstart writing some complex functions starting from a comment where I tell it what is the input and expected output. Is the implementation always perfect or bug free? No. But in general I just need to review and check rather than come up with the instruction entirely. 3) copilot chat helps me a lot in those situations where I would've googled to find how to do this or that and spent a lot of time with irrelevant or outdated search results 4) I have found use cases for LLMs in production. I had lots of unformatted plain text that I wanted to transform in markdown. All I needed to do is to provide few examples and it did everything on its own. No need to implement complex parsers, but make a query to OpenAI with the prompt and context. Few euros per month in OpenAI credits is still insanely cheaper than paying tons of money in writing and maintaining software by humans for that use case. 5) It helps me tremendously when trying to learn new programming languages or remembering some APIs. Writing CSS selectors is actually a very good example. But I don't feed it an entire HTML as you do, I literally tell him \"how do I target the odd numbered list elements that are descendants of .foo-bar for this specific media query\". Not sure why would you need to feed it an entire HTML. 6) LLMs have been extremely useful to generate images and icons for an entire frontend application I wrote 7) I instruct him to write and think about test cases about my code. And it does and writes the code and tests. Often thinks about test cases I would've never thought of and catches nice bugs. I really don't buy nor think it can write too much on its own. The promise of it writing anything but simple boilerplate, I find it ridiculous because there's way too much nuance in our products, business, devices, systems that you need to follow and work on. But as a helper? It's terrific. I'm 100% sure that people not using these tools are effectively limiting themselves and their productivity. It's like arguing you're better off writing code without a type checker or without intellisense. Sure you can do it, but you're gonna be less effective. reply javier123454321 14 hours agorootparentI ended up getting annoyed with the autocomplete feature taking over things such as snippet expansion in vscode, so I turned it off personally. I felt that the battling against the assistant made around a break even productivity gain overall. Except for regular expressions, that I have basically offloaded to AI almost in its entirety for non trivial things. reply zeroonetwothree 8 hours agorootparentRegex is easier to write than read so it seems not the best use of AI given you actually verify what it gives you and don't just take it on faith :/ reply RSHEPP 10 hours agorootparentprevCompletely agree. I turned it off and realized I can absolutely fly writing code when copilot stops getting in the way. I only turn on for writing tests now. reply sanderjd 9 hours agorootparentprevYep, pretty much exactly this. Especially #5. I'm certain that I've been at least 10x more productive in learning new tools since chatgpt hit the scene. And then knowing it helps so much with that has had even more leverage in opening up possibilities for thing I'm newly confident in learning / figuring out in a reasonable amount of time. It is much easier for me to say \"yep, no big deal, I'm on it\" when people are looking for someone to take on some ambiguous project using some toolset that nobody at the company is strong with. It solves the \"blank page\" issue with figuring out how to use unfamiliar-to-me-but-widely-used tools, and that is like a superpower, truly. It's pretty decent for \"happy path\" test cases, but not that good at thinking of interesting edge or corner cases IME, which comprise the most useful tests at least at the unit level. I'm pretty skeptical of #4. I would be way too fearful that it is doing that plain text to markdown transform wrong in important-but-non-obvious cases. But it depends on which quadrant you need with respect to Type I vs. Type II errors. I just never seem to be in the right quadrant to rely on this in my production projects. The \"really good intellisense\" use cases #1-#3 also make up a \"background radiation\" of usefulness for me, but would not be nearly worth all the hype this stuff is getting if that were all it is good for. reply infecto 14 hours agorootparentprevI agree with all of your points and experience the same benefits. 1) Autocomplete is more often than not what I want or pretty darn close. 2) Sometimes I need a discrete function that I am not sure how I want to write. I use a prompt with 3.5/4 inside of my IDE to ask it to write that function. It is definitely not writing complete programs any time soon but I can see where it's heading in the near term. Couple it with something like RAG to answer questions on library/api implementations. Maybe give it a stronger opinion about what good Python code looks like. For the naysayers I don't know how you use it but it is certainly useful enough for me to pay for. reply 2devnull 14 hours agorootparentprevBut people read much less of what you type now. reply ThalesX 13 hours agorootparentprev> 1) copilot is a terrific auto complete, and writes tremendous amounts of repetitive boilerplate I agree. I have it active on VSCode and enjoy it. It has introduced subtle bugs but the souped up autocomplete is nice. > 2) copilot can help me kickstart writing some complex functions starting from a comment where I tell it what is the input and expected output. Is the implementation always perfect or bug free? No. But in general I just need to review and check rather than come up with the instruction entirely. I don't find it very useful for anything non trivial. If anything I found it more useful for generating milestones and tasks for a product, than even making a moderately complex input -> output without me having to check it in a way that annoys me. > 3) copilot chat helps me a lot in those situations where I would've googled to find how to do this or that and spent a lot of time with irrelevant or outdated search results I find I don't use copilot chat, almost at all. Nowadays I prefer to go to Gemini and throw in my question. > 4) I have found use cases for LLMs in production. I had lots of unformatted plain text that I wanted to transform in markdown. All I needed to do is to provide few examples and it did everything on its own. No need to implement complex parsers, but make a query to OpenAI with the prompt and context. Few euros per month in OpenAI credits is still insanely cheaper than paying tons of money in writing and maintaining software by humans for that use case. This is mostly what I'm using it for in this current project. It does it job nicely but it's very far away from replacing myself as a programmer. It's more like a `fn:magic(text) -> nicer text`. This is a good use case. But it's a tool, not a replacement. > 5) It helps me tremendously when trying to learn new programming languages or remembering some APIs. Writing CSS selectors is actually a very good example. But I don't feed it an entire HTML as you do, I literally tell him \"how do I target the odd numbered list elements that are descendants of .foo-bar for this specific media query\". Not sure why would you need to feed it an entire HTML. Because I get random websites with complex markup, and more often than not every page has its unique structure. I can't just say give me `.foo-bar` because `.foo-bar` might not exist. Which is where the manual process comes in. Currently, I'm using hand crafted queries that get fed into GPT / Claude / LLama, but the actual query is what I wanted it to do. > 6) LLMs have been extremely useful to generate images and icons for an entire frontend application I wrote I'm very curious how this behaves in different resolutions. There's a reason vector graphics are a thing. I've used it for this purpose before but it doesn't compare to vectorial formats. > 7) I instruct him to write and think about test cases about my code. And it does and writes the code and tests. Often thinks about test cases I would've never thought of and catches nice bugs. What is the context size of your code? It works for trivial snippets but as soon as the system is a bit more complex, I find that it becomes irellevant fairly fast. > The promise of it writing anything but simple boilerplate, I find it ridiculous because there's way too much nuance in our products, business, devices, systems that you need to follow and work on. > But as a helper? It's terrific. > I'm 100% sure that people not using these tools are effectively limiting themselves and their productivity. Totally agree. But I'm not complaining about its usefulness. I'm a paying user of LLM systems. I use them almost every day. They're part of my products. But this particular hype about it replacing ... me. I don't buy. Yet. It could come tomorrow and I'd be happier for it. reply sesm 14 hours agoparentprevI use ChatGPT every day and it’s excellent at: - replacing StackOverflow and library documentation - library search - converting between formats and languages - explaining existing code/queries - deobfuscating code - explaining concepts (kinda hit or miss) - helping you get unstuck when debugging or looking for solution (‘give me possible reasons for …’) I feel like many of this things require asking the right questions, which assumes certain level of experience. But once you reach this level, it’s an extremely valuable assistant. reply ThalesX 13 hours agorootparent> replacing StackOverflow and library documentation I find it horrible at replacing library documentation > I feel like many of this things require asking the right questions, which assumes certain level of experience. But once you reach this level, it’s an extremely valuable assistant. I've been using LLM products since incipience. I use them in my daily work life. It's a bit tiring hearing this 'right questions', 'level of experience' and 'reach this level'. Can you share anything concrete that you achieved with ChatGPT that would blow my mind? I keep hearing this 'you need to ask the right kind of questions bro' from people that never build a single product in their life, and it makes me question my ability to interact with LLM but I never see anything concrete. reply kjqgqkejbfefn 12 hours agorootparentI recently had an introspective dream revealed to be based on a literal prompt at the end: \"Game to learn to talk about It and its player.\" When I asked GPT to craft a plot from this prompt's title (and the fact it is revealed at the end), it reproduced the dream's outline, down to the final scene: GPT reconstruction: The dream reaches its peak when you meet the \"final boss\" of the game: an entity that embodies the ultimate barrier to communication. To overcome this obstacle, you must synthesize everything you've learned about \"it\" in the dream and present a coherent vision that is true to yourself. As you articulate your final understanding of \"it\", the maze dissolves around you, leaving you in front of a giant mirror. In this mirror, you see not just your reflection but also all the characters, passions, and ideas you encountered in the dream. You realize that \"it\" is actually a reflection of yourself and your ability to understand and share your inner world. The dream ends with the title revealed, \"Game to Learn to Communicate about It and Its Player\", meaning the whole process was a metaphor for learning to know and communicate your own \"it\" - your personality, thoughts, and emotions - with others, and that you are both the creator and the discoverer of your own communication game. My note: The continuation of the dream corresponds to an abrupt change of scene. I find myself in my bed, in the dim light of my room, facing a mysterious silhouette. As I repeatedly inquire about its identity, I stretch my hands towards its face to feel its features as I cannot clearly see them. Then, a struggle begins, during which I panic, giving the dream a nightmarish turn. Noticing that the dark figure mirrors my movements, I realize it's myself. Suddenly under my duvet and as I struggle to get out, I feel jaws and teeth against the sheets. I call out for my mother, whom I seem to hear downstairs, and that's when my vision fades, and I see the dream's source code displayed behind. It consists of ChatGPT prompts shared on the lime green background of an image-board. At the bottom, I then see the dream's title: \"Game to learn how to communicate about It and its player.\" reply ThalesX 12 hours agorootparentLook I don't mean to downplay. Or maybe I do. But we're talking about LLM replacing professional problem solvers, software architects, not generating great sounding probability modeled token distributions. reply huimang 14 hours agorootparentprevI like it for condensing long stack traces and very very simple requests, but it really falters when you try to do anything domain specific. Library documentation? Yeah, it doesn't really save time when GPT makes up functions and libraries, making me check the docs anyways... I was initially hopeful but I find it gets in my way for anything not trivial. reply mypalmike 12 hours agorootparentI find it to be hit or miss in this aspect. Sometimes I can write a comment about how I want to use an API that I don't know well, and it generates perfect, idiomatic code to do exactly what I want. I quickly wrote a couple of Mastodon bots in golang, leaning heavily on Copilot due to my lack of familiarity with both the language and Mastodon APIs. But yes, sonetimes it just spits out imaginary garbage. Overall it's a win for my productivity - the failures are fast and obvious and just result in my doing things the old way. reply CamperBob2 12 hours agorootparentprevYeah, it doesn't really save time when GPT makes up functions and libraries, making me check the docs anyways... That behavior is now vanishingly-rare, at least in GPT4. reply dukeyukey 9 hours agorootparentSo Copilot uses GPT-4 under the hood, and about half the time I use it to generate anything bigger than a couple of lines it doesn't even compile, let alone be correct. It hallucinates constantly. reply mecsred 14 hours agorootparentprevThings AI is \"excellent\" at, includes \"explaining concepts (kinda hit or miss)\". Did you use an AI assistant while making that list? reply jp42 14 hours agorootparentprev+1 chatgpt or simialr tools are extremely useful, if you ask the right questions. I use for: - code completion - formatting: e.g show it sample format & dump unstructured data to convert to target format. - debugging - stackoverflow type stuff - achieving small specific tasks: what is linux command for XYZ etc and many mentioned in above comment. reply narrator 12 hours agoparentprevAs a developer who is good at object oriented design, architecture, and sucks at leetcode stuff, I have been able to use it to make myself probably twice as productive as I otherwise would be. I just have a conversation with GPT-4 when it doesn't do what I want. \"Could you make that object oriented?\" \"Could you do that for this API instead, here let me paste the docs in for you.\" I think people want it to completely replace developers so they can treat programming as a magic box, but it will probably mostly help big picture architecture devs compete with people who are really good at Leetcode type algorithm stuff. reply ecoquant 12 hours agorootparentTotally agree. I am not a professional developer. I find programming to be quite dull and uninteresting. I am going to work on something after this pot of coffee brews that I simply could not produce without chatGPT4. The ideas will be mine but the most of the code will be from chatGPT. What is obvious is different skill sets are helped more than others with the addition of these tools. I would even say it is all there in the language we use. If we are passing out \"artificial intelligence\" to people, the people who already have quite a bit of intelligence will be helped far less than those lacking in intelligence. Then combine that with the asymmetry of domains this artificial intelligence will help in. It should be no surprise we see hugely varied opinions on its usefulness. reply notpachet 12 hours agorootparentprev> it will probably mostly help big picture architecture devs compete with people who are really good at Leetcode type algorithm stuff. The competition should be happening in the other direction. reply parhamn 15 hours agoparentprevI build a pretty popular LLM tool. I think learning when/how to use them is as big a mental hurdle as it was learning to google well or whether something is googlable or not. In the realm of coding here are a few things its really good at: - Translating code, generating cross language clients. I'll feed it a golang single file API backend and tell it to generate the typescript client for that. You can add hints like e.g \"use fetch\", \"allow each request method to have a header override\", \"keep it typesafe, use zod\", etc - Basic validation testing. It's pretty good at generating scaffold tests that do basic validation (Opus is good at writing trickier tests) as your building. - Small module completion. I write an interface of a class/struct with it's methods and some comments and tell it to fill in. A recent one I did looked something like (abbreviated): type CacheDir struct { dir string, maxObjectLifetime: Duration, fileLocks sync.Map } type (cd *CacheDir) Get(...) type (cd *CacheDir Set(...) type (cd *CacheDir) startCleanLoop() Opus does a really good job generating the code and basic validation tests for this. One general tip: you have to be comfortable spending 5 minutes crafting a detailed query assuming the task takes longer than that. Which can be weird at first if you take yourself seriously as a human. Note that I hadn't been able to do much of this with GPT-4 Turbo with with Claude Opus it really feels capable. reply ThalesX 15 hours agorootparentJust to answer to the turbo aspect, I've seen a big downgrade in quality when comparing 4 to 4-turbo, and even the new preview which is explicitly supposed to follow my instructions better. So I'm running a first pass through 4 and then combinging it with 4-turbo to take advantage of the larger context window and then running 4 on it again to get a better quality output. reply parhamn 15 hours agorootparentYou really need to try Opus. Try a provider that works across models (one in my bio). reply Eager 14 hours agorootparentIt's incredible how far behind HN of all places is w.r.t. what the current best tech is. So many people talking about GPT-4 here, or even 3.5 when the SOTA has moved way along. Gemini Advanced is also a great model, but for other reasons. That thing really knows a boat load of low level optimization tricks. reply ThalesX 12 hours agorootparent> So many people talking about GPT-4 here, or even 3.5 when the SOTA has moved way along. I'm talking about 4-turbo, 4-turbo preview and self hosted LLama2. What in God's name is not SOTA about this? reply shruggedatlas 13 hours agorootparentprev> So many people talking about GPT-4 here, or even 3.5 when the SOTA has moved way along. So what is the SOTA, in your opinion? reply beepbooptheory 14 hours agorootparentprevI'm sure you know what your talking about, but pushing the point that what is \"best\" or worth talking about is something that changes like every month does not really help defend against the case that most of this is just hype-churn or marketing. reply Eager 13 hours agorootparentI'm not pushing what to talk about so much as pushing the point not to talk about stuff that is obsolete and starting to smell. It's that hype-churn marketing that is a motivating factor for the groups to innovate, much like Formula 1. It might be distasteful, but that doesn't mean it isn't working. reply dukeyukey 9 hours agorootparentprev> It's incredible how far behind HN of all places is w.r.t. what the current best tech is. Good fucking Christ, Opus has been out for like 8 days, I've had holidays longer than that! reply OtherShrezzing 15 hours agorootparentprev>- Small module completion. I write an interface of a class/struct with it's methods and some comments and tell it to fill in. A recent one I did looked something like (abbreviated): Are they considerably better than existing non-AI tools + manual coding for this? In VSCode and Visual Studio, when working with an interface in C# for example, I can click two context menus to have it generate an implementation with constructors, getters, & setters included, leaving only the business logic code to write manually. You've mention you have to describe to the AI in comments, and then I assume spend time on a step to verify the AI has correctly interpreted your request & implemented. I can definitely see the advantage for LLMs when writing unit tests on existing code, but short of very limited situations, I'm really finding it difficult to find the 55% efficiency improvements claimed by the likes of GitHub's AI Copilot. reply joenot443 15 hours agorootparentThat sounds crazy useful and I think speaks most to the maturity of C# and Microsoft's commitment to making it so ergonomic. I'm pretty curious about that feature, I'd love something similar for C++ in VS Code, but thus far I've been doing a pretty similar Copilot flow to the parent comment. It's nothing groundbreaking, but a nice little productivity boost. If I had to take that or a linter, I'd take the linter. Totally agree on the 55% figure being hogwash. reply creato 13 hours agorootparentVisual Studio (not VSCode) has this for C++, though it can be a bit finicky. It’s infinitely better than AI autocomplete, which just makes shit up half the time. reply jonny_eh 15 hours agorootparentprev> Translating code, generating cross language clients Can any convert a native iPhone app to an Android one? reply CamperBob2 12 hours agorootparentPiece by piece, sure. The context window is too small to just feed it a massive source dump all at once. reply breadsniffer01 15 hours agoparentprevA lot of startups are selling the dream/hype of not ever having to learn to code. Be aware that it’s hype. Learn to code if you want to build stuff. They will be tools for those that have the knowledge needed to effectively use them. reply Bjorkbat 15 hours agorootparentReminds me of the no-code / low-code hype around 2020, tons of startups advertising app-builders that used little, if any, AI. Just blocks that you dragged-and-dropped. While many of them were successful, it seems like overall they didn't really make much of a dent in industry, which I found very curious. Like, by now you'd think it would be inevitable that we wouldn't be writing software in a text-editor or IDE. Everything else we do on a computer is more graphical rather than textual, with the exception of software development. Why is that? Part of the reason why I'm kind of bearish on AI is because it seems like we could have replaced written code with GUI diagrams as far back as the 80s, or at the very least in the early 2000s, and it seems like something that should have obviously caught on given that would probably be much easier for the average person. Again though, curiously, we're still using text editors. Perhaps despite the popularization of AI no-code builders we'll still see that the old model of hiring someone good at writing code in a text-editor remains largely unchanged. Makes me wonder if there's just something about the process that we overlook, and if this same something could frustrate attempts at automating the process of writing code using AIs as much as it frustrated our attempts at capturing code using graphical symbols. reply JSavageOne 1 hour agorootparentI think you're underestimating the amount of things built with nocode. I don't think most people are building landing pages anymore by handwriting code anymore. Same with blogs (eg. Wordpress). There are MVPs of successful businesses that've been built by Bubble.io. Internal dashboards and such can definitely be built without code such as via Retool or Looker or whatever. WYSIWYG obviously makes sense for frontend, but less so for backend. For backend code I don't really see how some visual drag and drop editor could make for a better interface than code. And even if it could, the advantage of code is that it's fully customizable (whereas with a GUI you're limited by the GUI), and text itself as a medium is uniform and portable (eg. easy to copy and paste anywhere). Not to say that we can't create better interfaces than text, but I do think some sort of augmentation on top of a code editor is probably a more realistic short-term evolution, similar to VSCode plugins. reply esafak 15 hours agorootparentprevNo code tools sell the same dream. reply eggdaft 15 hours agorootparentprevI’m actually really amazed by LLMs and think the world is going to change dramatically as a result. But the “you won’t need to code” reminds me “you won’t need to learn to drive”. It’s the messy interface with the real world in both cases that basically requires AGI. If AGI is just a decade off then, yep, I won’t need to code. But a decade is a long time and, more importantly, we’re probably more than a decade away. And even if it is “just round the corner”, worrying about not needing to code would be worrying about deckchairs on the titanic. AGI will probably mean the end of capitalism as we know it, so all bets are off at that point. It’s wise to hedge a little but also realise that to date AI is just a coding productivity boost. The size of the boost depends on how trivial the code is. Most of the code I write isn’t trivial and AI is fairly useless at that, certainly it’s faster and more accurate to write it myself. You can get a 50% boost if you’re writing boiler plate all day, but then you have to wonder why you’re doing that in the first place. reply zarathustreal 15 hours agorootparent+1 for the titanic analogy. If there ever comes a point that we no longer need to learn to code, I’m taking that as a sign that I’m literally living in a matrix-esque simulation. The point at which someone like myself is allowed to become aware that a company has developed that level of AI is well beyond the point of no return. reply jcgrillo 11 hours agoparentprevThis is an interesting post. An expert in numerical analysis compares the output of a tool which optimizes floating point expressions for speed and accuracy with the output generated by chatgpt on the same benchmarks: https://pavpanchekha.com/blog/chatgpt-herbie.html > I wouldn't use it—sanity-checking its algebra is a lot of work, but even if you fixed that up, the high-level ideas typically aren't that good either. This has been exactly my experience with chatgpt as well. reply orthecreedence 13 hours agoparentprev> I'm probably dumb as hell, because I just can't get it to do anything remotely useful, more than helping me with leetcode. I highly doubt you're the dumb one here. reply brigadier132 15 hours agoparentprevClaude Opus is working for me. It's not perfect but it definitely handles busy work well enough that it's a net positive. Like I add some new fields to a table and ask it to update all the files that depend on the field and it works after 1 or 2 tries. There is a time saving benefit but there is also an avoiding mental fatigue benefit for busywork. reply HakuG 14 hours agorootparentWhat are you using on top of Claude Opus that helps it access your file system? reply brigadier132 12 hours agorootparentcmd c cmd v, definitely not ideal reply malux85 14 hours agorootparentprevThis is what I use it for too -- Write me the molecular simulation boilerplate because these crappy tools all have their own esoteric DSLs, then I tweak the parameters to my use case, avoiding the busywork - e.g. \"Write me a simulation for methane burning in air\" Gives me a boilerplate, I modify the initial conditions (concentrations, temperatures, etc) and then deploy. Have the LLM do the busy-work, so I dont have to spend ages reading docs or finding examples just to get started. Now deploy to a stable environment. Thats what I'm trying to help with by building https://atomictessellator.com reply sanderjd 12 hours agoparentprevExactly where I'm at! Totally transformative set of tools for me to use to do my day to day work significantly more productively and also a giant distance away from being capable of doing my day to day work. reply qrios 14 hours agoparentprevI totally agree! And I'm sure the reason for that is the garbage input. From time to time I have to perform quantitative code analyses in our so called enterprise repositories. And the results are shocking every time. I have found an extremely poor SQL code block to type cast many columns in hundreds of projects. It was simply copied again and again even though the casting was no longer necessary. The training base should be sufficiently qualified (and StackOverflow ranking is obviously not enough). But unfortunately it's probably too late for that now. Now inexperienced programmers are undercutting themselves with poor AI output as training input for the next generation of models. reply cloudking 13 hours agoparentprevWhat kind of prompts are you using? You'd be surprised how much better your output is using prompting techniques tailored for your goal. There are research papers that show different techniques (e.g one shot, role playing, think step by step etc) can yield more effective results. From my own anecdotal experience coding with ChatGPT+ for the past year, I find this to be true. reply ThalesX 12 hours agorootparent> What kind of prompts are you using? I hack on them till I get something sort of satisfying. > You'd be surprised how much better your output is using prompting techniques tailored for your goal. The biggest problem I encounter is context length, not necessarily the output for small inputs. It starts forgetting very fast, whether it's Claude, GPT+ or other self hosted models I've tried. reply senko 14 hours agoparentprevIn my experiments at Pythagora[0], we've found that sweet spot is technical person who doesn't want to know, doesn't know, or doesn't care about the details, but is still technical enough to be able to guide the AI. Also, it's not either/or, for best effect use human and AI brainpower combined, because what's trivial vs tedious for human and AI is different so actually we can complement each other. Also, current crop of LLMs are not there yet for large/largish projects. GPT4 is too slow and expensive, while Groq is superfast but open source models are not quite there yet. Claude is somewhere in the middle. I expect somewhere in the next 12 months there's going to be a tipping point where they will be capable, fast, and reliable enough to be in wide use for coding in this style[1]. [0] I have an AI horse in the game with http://pythagora.ai, so yeah I'm biased [1] It already works well for snippet-level cases (eg GitHub copilot or Cursor.sh) where you still have creative control as a human. It's exponentially harder to have the AI be (mostly) in control. reply dustingetz 14 hours agorootparent. reply senko 14 hours agorootparentI would clarify that \"there\" in my \"not there yet\" doesn't assume superhuman AGI developer that will automagically solve all the software development projects. That's a deep philosophical issue best addressed in a pub somewhere ;-) But roughly on par with what could be expected of today's junior software developer (unaided by AI)? Definitely. reply psygn89 12 hours agoparentprevThe other day I thought I had the perfect task for AI and to clean up some repetitive parts in my scss and to leverage mixins. It failed terribly and was hallucinating scss features. It seems to struggle in the codevisual realm. reply rapind 13 hours agoparentprev> Just give me a product that works as advertised Almost no products fit this description, and if they do then the marketing department is getting fired. Does a Mcdonalds burger look like the picture? If you go in with a healthy dose of cynicism IMO LLMs can impress. I’d call it a better google search and autocomplete on steroids. reply ThalesX 13 hours agorootparent> Does a Mcdonalds burger look like the picture? Sometimes? But I don't go to McDonalds for the loss function between the picture and actual product. I go for the fast food and good taste (YMMV). > If you go in with a healthy dose of cynicism IMO LLMs can impress. I use them everyday in one way or another. But they're not replacing me coding today. Maybe tomorrow. And I go in with a healthy dose of optimism when I say this. > I’d call it a better google search and autocomplete on steroids. Sure, but this particular discussion is not about its ability to replace Google Search and / or Autocomplete. reply mrguyorama 13 hours agorootparentprev>Does a Mcdonalds burger look like the picture? It actually does in the countries that require it. You know you can write ACTUAL \"truth in advertising\" laws right? reply summerlight 13 hours agoparentprevMy personal take is that LLM is fairly good at replacing low level tasks with intuitive patterns. When it comes to a high level ambiguous question that actually has an implication on your daily works and the products, LLM is not helpful anymore than search engines. Yeah, AI will do the easy and fun jobs for you. You will only need to care difficult decisions that you're going to be responsible for. What a wonderful world... reply the_mar 7 hours agoparentprevSound like a skill issue reply ThalesX 1 hour agorootparentEasy to say. Show me something impressive you've done with AI and little involvement from yourself. reply spaceman_2020 14 hours agoparentprevAs with everything about AI, HN once again shows a remarkable inability to project into the future. This site has honestly been absolutely useless when discussing new technology now. No excitement, no curiosity. Just constantly crapping on anything new and lamenting that a brand new technology is not 100% perfect within a year of launch. Remove \"Hacker\" from this site's name, because I see none of that spirit here anymore! reply ThalesX 13 hours agorootparentThis is a post about a present product launch. The future, maybe tomorrow, will be filled with wonder and amazement. Today, we need to understand reality. Not all of us appreciate empty hype. Hackers tinker with reality and build the future. Marketers deal with thin promises. reply javier123454321 14 hours agorootparentprevAre you kidding me, I'd say it's 80% people hyping up AI. reply z7 12 hours agorootparentprevI just think there's a bias involved when some people are emotionally invested in AI not being good. reply CamperBob2 11 hours agorootparentprevWait, wait, you're telling me that a site attended by people who stan for the OG Luddites is no longer worthy of being called \"Hacker News\"? Or where users with names like \"BenFranklin100\" extol the virtues of Apple's iOS developer agreement? Say it isn't so. The trouble is, there's still nowhere better. reply yshrestha 7 hours agoparentprevAs a human, I can tell you that I suck at predicting exponential growth. reply stuckkeys 15 hours agoparentprevlol I was on the same boat until I sinked it all together. I ended up wasting more time arguing with the LLM chat than doing anything remotely useful. I just use it for reference now, and even that I am not 1000% sure. reply rewgs 12 hours agoparentprevThis is exactly my experience. Furthermore, I've become acutely aware that spending time prompting either a) prevents me from going down rabbit holes, all but denying me the kind of learning that can only really happen during those kinds of sessions, and b) prevents me from \"getting my reps in\" on stuff that I already know. It stands to reason that my ability to coax actually useful information out of LLMs will atrophy with time. I'm quite wary of the long-term implications and downstream effects of that occurring at scale. AI is typically presented as \"the human's hands are still on the wheel,\" but in reality I think we're handing the wheel over to the AI -- after all, what else would the endgame be? By definition, the more it can do without requiring human intervention, the \"better\" it is. Even if replacing people isn't the intention, I fail to see how any other effect could usurp that. Assuming AI keeps developing as it has been, where will we be in 20 years? 50? Will anyone actually have the knowledge to evaluate the code it produces? Will it even matter? Perhaps it's because Dune is in the air, but I'm really feeling the whole \"in a time of increased technology, human capabilities matter more than ever\" thing it portrays. reply nprateem 14 hours agoparentprevYeah I'll only give it tasks where it needs to spot patterns and do something obvious, and even then I'll check it make sure it hasn't just omitted random stuff just for shits and giggles. TBH I'm more surprised when I don't need to help it now. After about 3 times where it cycles between incorrect attempts I just do the job myself. I disabled copilot since it consistently breaks my flow. reply nurettin 14 hours agoparentprevI think it requires years of proficiency in the field you are asking about in order to get openai to produce meaningful, useful output. I can make use of it, but sometimes it makes me think \"how would a newbie even phrase an objection to this misunderstanding or omission?\" Currently it seems gpts are pretty much not on par with the needs of non-experts. reply Eager 14 hours agorootparentYou might be on to something here. It definitely seems to be the case because I'm using multiple different models as part of my everyday process and getting excellent results as a very experienced low level C++ systems engineer. What is worse is that seems to be leading to a self-amplifying feedback loop, where people not up to speed enough with the models try to use them, fail and give up making them fall even further behind. reply nurettin 4 hours agorootparentVery similar to my experience. I made it generate a novel neuroevolution algorithm with the data structures I imagined for recreational purposes, and to speed things up, it suggested \"compiling\" the graph by pre-caching short circuits into an unordered_map. A lot of fun was had. (it also calls me captain) reply anxman 14 hours agorootparentprevThis is my experience too. You have to be have deep domain knowledge to really get the LLM to do what you want. Then it saves me a ton of time. reply MSFT_Edging 17 hours agoprevHumans seek work that provides satisfaction and meaning in their life. For every technological advancement, artisans are the first to be made obsolete. Sure we have landfills full of unworn textiles, the market says its good, but overall, we keep destroying what allows humans to seek meaning. Our governments and society have made it clear, if you don't produce value, you don't deserve dignity. We have outsourced art to computers, so people who don't understand art can have it at their fingertips. Now we're outsourcing engineering so those who don't understand it can have it done for cheap. We hear stories of those who don't understand therapy suggesting AI can be a therapist, of those who don't understand medicine suggesting AI can replace a doctor. What will be left? Where will we be? Just stranded without dignity or purpose, left to rot when we no longer produce value. I ask this question often, with multiple contexts, but to what end? Who benefits from these advancements? The CEO and shareholders, sure, but just because something can be found for cheaper, doesn't mean it improves lives. Our clothes barely last a year, our shoes fall apart. Our devices come with pre-destined expiration dates. Where will we be in the future? Those born into money can continue passing it around, a cargo cult for the numbers going up. But what about everyone else? reply xtreme 15 hours agoparentBuying handcrafted artisan stuff is a luxury few can afford. I come from a poor family and I was always grateful for mass produced mediocre stuff that we could actually afford. reply MSFT_Edging 14 hours agorootparentSure but the same optimizations that bring the costs down, bring down the average laborers value. I'm not saying that things were sunshine and rainbows pre-industrialization, but there's some level of analysis to be done on the durability and value of a handcrafted piece of clothing, the care that goes into maintaining it, the value of a local economy, and the other side where you're forced to buy cheap items that degrade at a far faster rate. If a town's local businesses are put out by a new walmart's ability to carry low prices, does the town truly come out ahead with those low prices? Or does Walmart simply extract more money from the town than it returns, leaving the town worse off? reply MacsHeadroom 9 hours agorootparent> bring down the average laborers value. Labor has never been to afford so much luxury as the modern day. reply esoterica 8 hours agorootparentprevThose optimizations increase the value of labor by greatly increasing the amount of output per unit of labor. Do you understand how much wealthier the modern worker is than the pre Industrial Revolution peasant? reply dandelionsnow 14 hours agorootparentprevHandcrafted artisinal stuff is a luxury because that's the only niche that makes economic sense for it now, given mass production and other recent developments (too lazy to list, sorry). Consider how you can't really get by in most of America without a car because we designed our cities to require them. It would be a mistake to conclude that, because life is harder in a car-optimized society without a car, society must be better off optimizing for cars. reply qqqwerty 13 hours agorootparentprevThe capitalist system keeps you poor by design. And you feed the system by purchasing the mass produced garbage. Sure, it is nice to afford stuff when poor, but we don't need to live in a society where being poor is common, or where mass produced garbage is the default option for most. reply onthecanposting 9 hours agoparentprevThe economy clothes aren't great, but you can get a quality tailored suit for $4000. That's about 6 weeks pay for a PFC in the US Army. A roman legionnaire would pay about a gold ounce for a quality (at the time) set of clothing, however, at the cost of about 6 months pay. They don't build 'em like they used to, but those of us in the rank and file are getting better off. We can't all be \"I think I will just buy Hawaii\" rich like Zuckerberg, but quality of life is, overall, improving. reply burningChrome 15 hours agoparentprevI often ask a similar question of what happens when we, as humans, have offloaded everything to technology, to AI to Robots? What kind of a society will we have then? When you no longer have to think about how to do something, or how to build or repair something, or create something original from your imagination. I shudder to think the direction this is all leading to. reply LegibleCrimson 13 hours agorootparentI'm kind of an AI pessimist, but I think that sounds like it could be a wonderful society. When the only work you need to do is work that you actually want to do. It could free up people to actually chase their dreams and achieve what they want to, without having to constantly chase subsistence. The reason I'm a pessimist is that I mostly see society preserving the status quo. I don't see AI democratizing things and freeing us, because we have fetishized the concepts of work and profit that we can't imagine a society that functions properly without demanding those two things be put above all else. I don't envision a dystopia or a utopia, I envision a future where AI disenfranchises people who should be taken care of, bolsters profits of the already powerful, and replaces the most fulfilling human pursuits without actually saving people from unfulfilling toil, mostly because society will bend backwards to try to preserve the status quo. reply machiaweliczny 13 hours agorootparentprevRaising kids, social stuff, exercise, travel, any leisure stuff like racing, horse ridding, acrobatics. entertainment, cooking, art, gardening etc. Just check what rich girls do and you will see they aren’t bored though they don’t have to work. reply __loam 11 hours agorootparentAnd I suppose we'll get to do all that stuff when all the value trickles down from the shareholders right? reply brigadier132 14 hours agorootparentprevI think what it leads to is a world without scarcity. I think all this fretting about meaning is missing the forest from the trees. Things like meaning are important but making sure everyone can eat three meals a day, that your family has access to superhuman doctors, that different diseases are cured are all much more important. reply jajko 14 hours agorootparentprevThink about it in cycles. Handcrafted products were frowned upon since you could see imperfections and flaws. Now they are the thing, cheap perfect same mass machine-produced things often look bland and cheap, you pay massive premium over it. No reason this won't repeat in some form again. True art will be rare and treasured, few artists will be rockstars. Till next fad we can't even see coming again mixes it all up. But yeah these transitions do make tons of people miserable, losing jobs. Also overall middle class is disappearing, but thats the trend for quite some time. But it always paid off to anticipate what next generation of showels will be. reply __loam 11 hours agorootparentprevHistorically when this kind of stuff happens the result is usually a Revolution. reply gen220 10 hours agoparentprevI have the same questions. I’ve decided the best thing I can do is vote with my wallet (i.e. as a consumer) and vote with my time (i.e. as a producer). Consume commodities that are regenerative and products that are durable. Whenever possible, buy local to keep cash flows (wealth transfer) local. Invest your time in contributing to products that encourage other people to do the same. Yvon Chouniard said (I’m completely paraphrasing) something like “in the 21st century, the most powerful vehicle for transforming the world is the American corporation. Whether or not it should be, it is. If you think the world should be different, step 1 is starting a business”. As a producer (engineer, for me), the best thing I can do is build tools to help people wean themselves off of Capital. reply esoterica 8 hours agorootparentAutarkies are impoverished, societies that trade freely are wealthy. Your economic impulses are deeply misguided. If everyone only bought local the world would be much poorer. It’s better (consumer choice, comparative advantage) to buy from the world and to let the world buy from you. The money doesn’t disappear when you spend it outside your locality, it flows both ways. reply gen220 7 hours agorootparentI’m sorry, but by whose definition are “they” “impoverished”? And what constitutes “autarky” and “local”? I’m not advocating to abolish global trade or whatever, if you think I was gesturing in that direction. Buying (food, especially) locally means supporting a robust food chain that’s less dependent on refrigeration, global shipping, and a geographic arbitrage on agricultural labor. It’s better for the environment and connects you to your community and sense of place and seasons. It means I can know what went into the food, in terms of soil, labor, and pesticides — something that I have no knowledge of at the grocery store. A pound of apples, meat, or whatever from 1-2 counties away is more financially expensive, but if you can afford it there’s many non-financial benefits. I’d rather purchase from a CSA than a national grocer, because that money mostly goes back into my local community. You’re right the money doesn’t disappear when you spend it outside your locality, but it comes back in ways that are increasingly exploitative and/or depressing (chain stores offering no meaningful careers or opportunities for ownership, rent-seeking financialization, warehouses that bring noise pollution and packaging waste, etc.). Again, if you can afford it, IDK why you wouldn’t prefer to spend the marginal dollar locally (i.e. private / family-owned businesses in a few counties’ radius). It’s consumer choice and comparative advantage that actually lead people like me to shop locally... reply plz-remove-card 6 hours agoparentprev> Humans seek work that provides satisfaction and meaning in their life. > we keep destroying what allows humans to seek meaning. > Our governments and society have made it clear, if you don't produce value, you don't deserve dignity. Those points are really interesting to me, I think you're right and I've never looked at it that way before. reply Buttons840 12 hours agoparentprevJust pick a path, doctor, artist, therapist, any path will do. You'll soon realize you're better than the AI, but nobody will give a shit, they'd rather have the cheap AI knockoff. reply ardaoweo 16 hours agoparentprevIf we got universal basic income, people could do whatever they want. I for one would be content spending my time gardening and trekking in the nature. I despise office work and do it only for money. It's forcing the rich to give us UBI that is the problem. reply MSFT_Edging 16 hours agorootparentSure but UBI would never afford a garden. It would be bare minimum to survive, if that. We'd need a complete restructuring of society to even approach dignity via UBI. reply pj_mukh 15 hours agorootparentUnless of course the robots built the gardens thereby driving the cost of gardens down to amounts accessible by UBI. Or so goes the theory. reply AndrewKemendo 15 hours agorootparentprevRight! So lets get after it Start a local cooperative and build value from the bottom up reply pzo 15 hours agorootparentprevThere will be only just few countries that are winners in this game. Do you believe e.g. USA will provide UBI for other countries or even only neighbours like Mexico? I don't think so. And once there is big unemployment that USA border will be filled even more with migrants or people unhappy with the situation. reply golergka 15 hours agoparentprev> What will be left? Where will we be? Just stranded without dignity or purpose, left to rot when we no longer produce value. Nobody stops you from paying $1000 for a shirt made by artisans right now. Do you want to? > Our governments and society have made it clear, if you don't produce value, you don't deserve dignity. It's not somebody else who decided that. It's you. reply ethanwillis 15 hours agorootparent> Nobody stops you from paying $1000 for a shirt made by artisans right now. Do you want to? If you don't dothen your point is invalid. /s. Look, you dont have to pay $1000 for a shirt made by an artisan to get a quality shirt. It can be a bit more expensive, but much cheaper than that. And this can be true while being fair/reasonable to the artisan and also to the person acquiring it. > It's not somebody else who decided that. It's you. And what about my decisions? Or your decisions. Or anyone who is reading this's decisions? Surely the person you're replying to isn't some dictator who is deciding everything. It's not just the parent comment then. It's not just \"It's you\" reply x-complexity 8 hours agorootparent> > Nobody stops you from paying $1000 for a shirt made by artisans right now. Do you want to? > If you don't dothen your point is invalid. /s. > Look, you dont have to pay $1000 for a shirt made by an artisan to get a quality shirt. It can be a bit more expensive, but much cheaper than that. And this can be true while being fair/reasonable to the artisan and also to the person acquiring it. In the ideal case, this would be true: Following classical economics, there would be some predictable demand left as you go up the price curve, assuming genuine quality & labour. However, this doesn't seem to be the case. The existence of low-cost mass-produced options leads to the satiation of demand above that spot of the curve. There still exists demand, but it's weirdly lower than predicted. People can shop down the price curve, but relatively fewer would do the opposite. reply MSFT_Edging 14 hours agorootparentprevI bought a pair of 350 dollar boots that can be repaired many times. That was five years ago, with a recent re-sole this past summer. This isn't an exaggeration by the way, but the cobbler who did the resole thanked me for bringing him the job, as it was a genuinely enjoyable experience for him. I assume most of his work is repairing suitcases going by the other clientele in the shop. But sure, exaggerate that to be an artisan you need to be selling 1000 dollar shirts, rather than doing a 100 dollar service that doubles or triples the life of a decently made item. reply Rumudiez 14 hours agorootparentprevI have a few handmade shirts that were all between $70-100. I buy the occasional oddity from Etsy and those things (the ones I buy) are all handmade and most of them are quite affordable – on par with shopping at Target for comparison. I’m 100% certain the artists are thankful they can be professional creators instead of becoming a wage worker or living off of some form of basic income reply feoren 12 hours agorootparentprev> > Our governments and society have made it clear, if you don't produce value, you don't deserve dignity. > It's not somebody else who decided that. It's you. No. The Republican National Convention cheered the idea of letting the poor and sick just die off in the streets. They cheered. Ron Paul asked \"what are we supposed to do? Let our sick and poor just die cold in the streets?\" and they cheered. Jobs are sacred in the U.S. Job creators must be worshipped. Hard-working Americans are the lifeblood of yadda yadda. As soon as you don't have a job: fuck you, scum, you deserve to die in the streets. You are no longer of use to the wealthy, so you do not even have the dignity to sleep on benches or under bridges: they add spikes to any area you might find any comfort. Your children cannot receive an education. You get to disappear from view into some secluded slum until you die of the cold. It's not GP that decided that. It's tens (hundreds?) of millions of Americans who will cheer on your death if you lose your job. Most of which, of course, are a couple paychecks or a major illness away from being homeless themselves. Do not act like that's not a real thing. reply sandspar 1 hour agorootparentYou're really passionate but you're strawmanning all over the place. reply golergka 10 hours agorootparentprevYou are literally agreeing with me and yet phrase it as if we’re arguing about something? I’m confused. reply rohandakua 1 minute agoprevhey , I am a newbie in field of AI , i want to know about the nearby future of AI . after devin I am little ! or rather I should say deeply terrified about the future (5 to 8 years) of software eng. can someone explain ? reply pushedx 14 hours agoprevScott Wu! I met Scott at a competitive programming event a few years back. He is one of a very small group of people (going back to 1989) to get a perfect raw score at the IoI, the olympiad for competitive programming. https://stats.ioinformatics.org/people/2686 Glad to see that he's putting his (unbelievable) talents to use. To give you a sense, at the event where I met him, he solved 6 problems equivalent to Leetcode medium-to-hard problems in under 15 minutes (total), including reading the problems, implementing input parsing, debugging, and submitting the solutions. reply zeroonetwothree 2 hours agoparentI've participated in programming olympiads and worked with people of varying skill and I would say that overall the correlation between competitive programming and software engineering in a business environment is probably like 0.2 or less. reply gardenhedge 13 hours agoparentprevSounds like he's talented. Isn't Devin \"just\" a AI wrapper tool? Devin's play is that it will be the first comprehensive option available but it will soon be eaten by OpenAI, Microsoft, Google and countless others. reply gitfan86 13 hours agorootparentYes, but AGI will first emerge from keeping state between calls to multiple models and assessing how closely they resemble humans intelligence, and using a loop to keep it going and updating the state. Which is what they are basically doing here reply PodgieTar 13 hours agoprevI must say, I'm not HUGELY impressed with a website that lets me, unauthenticated, upload files of an arbitrary size. Just posted a 500mb dmg file to their server. If anyone is practicing for their B1 Dutch exam, feel free to use this link to get the practice paper. https://usacognition--serve-s3-files.modal.run/attachments/4... reply 1231232131231 9 hours agoparentLooks like they deleted it and restricted file uploads :/ reply steve_adams_86 17 hours agoprevAlthough the demos are impressive, they seem short and limited in scope which makes me wonder how well this will work outside of these planned cases. Can it do software architecture at all? Is it still essentially just regurgitating solutions? How often will the solution only be 90% correct, which is 100% not good enough? Even so, I realize the demos are still broad in scope and the results are incredible. Imagine seeing this even 2 years ago. It would seem like magic; you wouldn't be able to believe it. Today, this was inevitable and entirely believable. There will be even better versions of this soon. reply andoando 17 hours agoparentThere is a similar product called Sweep AI thst I tried. For extremely simple things \"like add a button to the page that prints hello\" it was very good. I then asked it to do something more complex, which was to render my d3.js graph vertically rather than horizontally, and it tried to redefine constant variables (it just added a new modified code block without deleting the old one), put function clauses in places that were not synctactic. After I manually fixed those, the functionality just didnt work. reply breadsniffer 15 hours agoparentprevThis seems like it's something replit is better suited to execute. You will need human intervention at some point and at that point you're building a full-fledged web IDE reply swalsh 17 hours agoparentprevAh yes, you've entered the first stage of grief. Denial. Next you'll start bargaining, you'll get angry, and you'll become depressed, eventually you'll just accept that AI is taking over software. In my mind, I've concluded that I have less than 3 years to find an off ramp. reply steve_adams_86 16 hours agorootparentNo, I'm past the denial stage (I was certainly there, though... GPT threw me hard and I spent a good 2 months processing what was happening) but I don't, in this specific case, see this agent displacing many jobs yet. Well, not my kind of jobs. I'm already very worried for the entry level of our industry... I'm not sure what it means yet, but I don't think we will have many entrants into software careers within 5 years or so. I'm looking for an offramp as well. I truly love software so it's a hard reality to contend with at times. Regardless, I'm not a software engineer at my core. I'm a problem solver, and I love creating things. This is transferable. I'm not sure where to yet, but I'm certainly capable of making a move. I hope I have more than 3 years, though. As someone else asked, I'm curious where you're headed or thinking of heading so far. reply joshuahutt 13 hours agorootparentI feel the same way. I think the immediate future is bright. Some will try to cram this technology into enterprise. It will do well. Those jobs will die. Others will leverage it alongside the more creative engineering tasks—they will thrive. Eventually, what we call software will change from what it is today into something much more accessible to these types of tools. The plateau we've landed on is just a compromise between the technology, economies, and culture of its time. As this type of tech pervades our everyday lives, much of the widespread need for specialized software will evaporate. The remaining work will be in the corners or the edges of what's possible—highly technical or vertically integrated work (particularly, hardware-integrated stuff), as well as platform engineering to sustain the new paradigm. reply swader999 15 hours agorootparentprevKeep posting about this. I feel the same way. reply empath-nirvana 14 hours agorootparentprevThere is not a limited amount of software engineering that can be done. There's only the amount of software engineering that it is _economical_ to do at any given point. If AI makes software development cheaper and more efficient, people will just use apply it to more use cases. It's never been the case that making programming cheaper has lead to fewer programmers -- quite the opposite. This change is roughly analogous to the shift from punch cards to compilers. It's just a more \"natural language\" way to do programming. A lot of the drudgery associated with coding will go away and competent programmers will shift to higher level design work. Even in a future where AI is better than human software engineers at every single programming task (which I don't believe will be the case any time soon), there is still comparative advantage. AI will not have the _capacity_ to do every single programming task and there's going to still be lots of work for people to do. reply sandspar 1 hour agorootparentThe problem is that AI has the potential to turn a professionalized industry into a self-help one. Like, if I can pay $30 a month to have an AI make me several new apps a day, why would I need an app store? It's like fast fashion for programs. Low quality but dirt cheap. reply breadsniffer 15 hours agorootparentprevI've been using GPT-3 since its waitlist was out. Even then you saw demos like this claiming sentence -> full complete project. The demos will get fancier, but reality is much more complex than you think. reply mnk47 9 hours agorootparentprevIt took me much longer than usual to get past the depression phase. The main reason is that everyone online kept telling me I was dumb, too inexperienced or extremely incompetent if I thought AI was a serious threat to me, a junior developer. Friends offline said similar things, just in a nicer way. Now everyone seems to be a \"doomer\" like me. I was too stupid to see that people (especially online) were lashing out at me as a coping mechanism. The worst part is that even though I had my \"we're fucked\" realization was earlier than many others, I didn't really do anything productive, I didn't make the right moves to pivot, I had no plan. Now I have a rough ~3-year off ramp plan, just like everyone else. I suspect that there are many, many recent grads in my position, who also thought they were taking crazy pills when everyone around them dismissed their concerns reply swatcoder 9 hours agorootparentIf the circle you're listening to are \"recent grads\" stop listening to them. They don't know how the industry works or what it needs, few of them them know how the technology works or what it's realistic near-term prospects are let alone the long-term ones, and none of them have lived long enough to understand the pace of a technology moving from discovery through to maturity and commercialization. If you look to your experienced seniors instead of your peers, you'll discover two things: 1. They don't respect most contemporary juniors because of the industry boom and are happy to see most of you scare yourselves off 2. They're narrowing in on a familiar, respectful appreciation of recent content generators and their potential for assisting some basic use cases (highly valuable technology!) and are increasingly of the opinion that both the hype and fear of Q1/2023 was largely market manipulation by VC's trying to squeeze another boom out as money was tightening reply mnk47 7 hours agorootparentWell, I did see that, or at least people who claimed to be seniors online, plus some mid-level friends and yes, a junior friend. Admittedly, it's just anecdotes, but it's very difficult to get hard facts, even from perusing bls.gov. But something changed in the past 4 months or so. I used to see the two things you described, but now I see seniors repeating the same thing juniors (whom they ridiculed) used to say. Look at this very thread, for example. People are starting to realize that even if these AI startups are making ridiculous promises, the tech might be way more than just hype/smoke and mirrors. I've read quite a few comments in HN, and right here in this thread, that say something along the lines of \"I'll be fine... It's the juniors I'm worried about\". reply sandspar 1 hour agorootparentprevExperienced, older people are notorious for downplaying and dismissing the implications of revolutionary new technologies. They may be right this time, or they may be wrong. But age and experience aren't as relevant when it comes to revolutions. reply zeroonetwothree 2 hours agorootparentprevMany things have been claimed to be the \"death\" of software careers (.com crash, outsourcing, remote work, etc.) and yet we have more demand than ever. reply realusername 2 hours agorootparentprevIt's the AI hype group which are in a stage of denial. \"Surely if we generate code, it means we can replace developers right?\" Well no, it's not nearly enough. Good code generation with a UI is already a thing since the 90s, it's replacing developers as much as Frontpage and WordPress do. The average developer writes 100 lines per day anyways so the speed of writing those lines doesn't matter. Software is an interpersonal job at core and the most precise definition of a business. The day software engineers are replaced by a machine, there's no job for the CEO itself honestly and that's also why this job is well paid. I'm sure it will be all over LinkedIn though, despite this paradox. Those new tools will be there in the background but cannot replace a software engineer. reply 4star3star 17 hours agorootparentprevWhat kind of work do you have in mind? reply swalsh 15 hours agorootparentIn terms of my \"off ramp\"? I have a multi-part plan. Immediately, i'm working to get closer to the business. To be closer to the position of defining requirements, not implementing requirements. Secondary, i'm experimenting with ideas I hope can become a business. as a final fallback, I have a hobby woodshop in my basement, and I love making furniture. reply bigfishrunning 14 hours agorootparentMaking furniture! but we've had machines to do that for 100 years! Programmers will be fine. The AI plagiarism engines are severely limited, and will be for the forseeable future. Maybe someday i'll be equivalent to the ren-faire blacksmith, but I'm gonna do this until I die. reply chpatrick 15 hours agorootparentprevWhat about when the ai defines requirements? reply pzo 16 hours agorootparentprevone year ago I was thinking about ~10 years and at least safe 5 years. Considering how it's all progressing right now and we had chatGPT just 15 months ago I think you might be right or salaries will get reduced significantly. reply dakiol 14 hours agoprevDon't get it. If we have this amazing AI why don't we make good use of it? 90% of my job is not to write code (as a senior software engineer), is to: - deobfuscate complex requirements into well divided chunks - find gaps or holes in requirements so that I have to write the minimal amount of code - understand codebases so that the implementation fits nicely I don't need an \"AI software engineer\", I need an \"AI people person who gives me well defined tasks\". Now sure, if you combine those two kinds of AIs I could perhaps become irrelevant. reply gardenhedge 13 hours agoparentThe problem is getting enough information on requirements to even break them down :) reply random_cynic 9 hours agoparentprevYes, the horse carriage drivers had similar lines of thoughts when they saw first gen automobiles. reply earwin 9 hours agorootparentYour point being? The horses were replaced alright, carriage drivers are thriving ever since. reply random_cynic 8 hours agorootparentLmao what are you talking/coping about? What happened to carriage drivers is pretty well-documented. Maybe ask one of these AI chatbots, they will summarize it for you. reply zeroonetwothree 2 hours agorootparentThey drive for Uber now reply aster0id 14 hours agoprevI have a few years of experience in backend development, and I have realized that LLMs are incredible productivity boosts for generating code only if you know the underlying libraries/frameworks/languages very well. You can then prompt it with very specific instructions and it can go do that. Helps with the typing, but that's pretty much all. I still have to know everything and it can definitely not do everything on autopilot. I would be surprised if this product can do any real work. reply smith7018 13 hours agoparentI dunno, I have an extreme command of my platform's framework and I'd guess that 85% of the time I've asked GPT-4 for help has been a waste of time. I think it's been most helpful in regards to writing regexes but beyond that, it hallucinates correct-sounding methods all the time which leads to _a_lot_ of wasted debug time before eventually getting to the right answer by Googling what it meant or by manually rewriting large portions of what it meant to do. It's funny how a year ago I was really excited for how AI can help my everyday coding while fearful that it would replace me. Now I'm not really sure either will happen in the short term. reply nsypteras 16 hours agoprevClearly an extremely impressive demo and congrats on the launch. I do wonder how often the bugs Devin encounters will be solvable from the simple fixes that were demonstrated. For instance, I notice in the first demo Devin hits a KeyError and decides to resolve it by wrapping the code in a try-catch. While this will get the code to run, I immediately imagined cases where it's not actually an ideal solution (maybe it's a KeyError because the blog post Devin read is incorrect or out of date and Devin should actually be referencing a different key altogether or a different API). Can Devin \"back up\" at this point and implement a fix further back in its \"decision tree\" (e.g. use a different API endpoint) or can it only come up with fixes for the specific problem it's encountering at this moment (catch the KeyError and return None)? reply mikebelanger 14 hours agoparentYeah that was my question too. Its one thing to know the most simple fix for a KeyError issue, its another to understand that its the result of not assigning the proper key in some other part of the code, or like you said, maybe it called the wrong API endpoint and passing that into the dictionary. Somewhat related: is anyone else not really impressed by Devin fixing the errors that are very preventable with a stricter language like Rust? The demo shows Devin coding in both Python in Rust, but I consider the latter being way less energy intensive in terms of maintenance. Then again, exhaustive pattern matching and strict typing won't get you lots of VC dollars these days. reply HarHarVeryFunny 14 hours agoprevLet's get realistic here - I just beat GPT-4 at tic tac toe, since it failed to block my 2/3 complete winning line ... Sure, one day we'll have AGI, and one day AGI will replace many jobs that can be done in front of a computer. In the meantime, SOTA AI appears to be an airline chatbot that gets the company sued for lying to the customer. This is just basic question answering, and it can't even get that right. Would you trust it to write the autopilot code to fly the airplane? Maybe to write a tiny bit of it - just code up one function, perhaps? I sure as hell wouldn't, and when it can be trusted to write one function that meets requirements and has no bugs, it's still going to be a LONG way before it can replace the job of the developers who were given a task of \"write us an autopilot\". reply sohzm 5 hours agoparentWell, I'll bring the perspective of a recent college graduate from India. Most students who started jobs in software aren't writing software for aeroplanes. They're writing crud apps, doing third-party API integrations or doing basic debugging. I worry that although not some specialized software devs but a lot will still have problems due to stuff like this. I'm not talking today, but say 2-3 years down the line, who will have an intern when you can get an AI intern that can perform as the top percentile and comes at $20 per month subscription? reply senko 16 hours agoprevAs someone who works in this space (https://pythagora.ai), I welcome new entrants to this niche. Currently, mainstream AI usage in coding is at the level of assistants and glorified autocomplete. Which is great (I use GitHub Copilot daily), but for us working in the space it's obvious that the impact will be much larger. Besides us (Pythagora), there's also Sweep (mentioned by others in the comments) and GPT Engineer who are tackling the same problem, I believe each with a slightly different angle. Our thesis is that human in the loop is key. In coding, you can think of LLMs as a very eager junior developer who can easily read StackOverflow but doesn't really t",
    "originSummary": [
      "Cognition Labs secured a $21 million Series-A funding round spearheaded by Founders Fund to develop Devin, the inaugural fully autonomous AI software engineer, excelling in long-term reasoning, bug fixing, and self-training AI models.",
      "Devin surpasses existing models in the SWE-bench coding benchmark, leading Cognition Labs to pioneer AI teammates with sophisticated reasoning skills, actively recruiting engineers to collaborate with Devin.",
      "The company comprises industry experts from leading organizations with substantial experience in applied AI, emphasizing their dedication to advancing AI technologies."
    ],
    "commentSummary": [
      "Developers are frustrated with using AI for coding due to challenges faced by AI projects and concerns of a potential AI winter.",
      "Limitations of Large Language Models (LLMs) like GPT in coding and writing tasks are discussed, with differing opinions on their effectiveness compared to professional writers.",
      "The conversation explores the impact of AI on the job market, society, and software engineering, including experiences and concerns about AI tools like Copilot and ChatGPT, raising skepticism about AI's capability to entirely substitute human skills in coding."
    ],
    "points": 423,
    "commentCount": 440,
    "retryCount": 0,
    "time": 1710252915
  },
  {
    "id": 39678555,
    "title": "Enhanced App Distribution Options in the EU",
    "originLink": "https://developer.apple.com/news/?id=8c1m8hqt",
    "originBody": "More options for apps distributed in the European Union March 12, 2024 We’re providing more flexibility for developers who distribute apps in the European Union (EU), including introducing a new way to distribute apps directly from a developer’s website. More flexibility Developers who’ve agreed to the Alternative Terms Addendum for Apps in the EU have new options for their apps in the EU: Alternative app marketplaces. Marketplaces can choose to offer a catalog of apps solely from the developer of the marketplace. Linking out to purchase. When directing users to complete a transaction for digital goods or services on an external webpage, developers can choose how to design promotions, discounts, and other deals. The Apple-provided design templates, which are optimized for key purchase and promotional use cases, are now optional. Distributing directly from your website Web Distribution, available with a software update later this spring, will let authorized developers distribute their iOS apps to EU users directly from a website owned by the developer. Apple will provide authorized developers access to APIs that facilitate the distribution of their apps from the web, integrate with system functionality, back up and restore users’ apps, and more. For details, visit Getting ready for Web Distribution in the EU.",
    "commentLink": "https://news.ycombinator.com/item?id=39678555",
    "commentBody": "More options for apps distributed in the European Union (developer.apple.com)310 points by ano-ther 21 hours agohidepastfavorite1 comment dang 16 hours ago [–] Comments moved to https://news.ycombinator.com/item?id=39678532. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Apple is offering developers in the EU enhanced flexibility by introducing new options like alternative app stores and the capability to distribute apps directly from their websites.",
      "By agreeing to the Alternative Terms Addendum for Apps in the EU, developers can leverage these new opportunities, with Web Distribution expected to launch later this spring.",
      "Developers will be provided with APIs to support web distribution and seamless integration with system features, as outlined in the \"Getting ready for Web Distribution in the EU\" document."
    ],
    "commentSummary": [
      "More app distribution options are now available in the European Union, as per the announcement on developer.apple.com."
    ],
    "points": 310,
    "commentCount": 1,
    "retryCount": 0,
    "time": 1710245515
  },
  {
    "id": 39678783,
    "title": "Advancements in Weather Forecast Accuracy",
    "originLink": "https://ourworldindata.org/weather-forecasts",
    "originBody": "Weather forecasts have become much more accurate; we now need to make them available to everyone A four-day forecast today is as accurate as a one-day forecast 30 years ago. By: Hannah Ritchie March 12, 2024 Cite this articleReuse our work freely Weather forecasts are often seen as just a nice thing to have. Useful when planning a Sunday barbecue, or when we want to know if we’ll need an umbrella for the day. But in many ways weather forecasts are absolutely crucial: they can be a matter of life and death. Accurate forecasts can save lives by giving early warnings of storms, heat waves, and disasters. Farmers use them for agricultural management, which can make the difference between a lost harvest or a harvest of plenty. Grid operators rely on accurate forecasts of temperatures for heating and cooling demand, and how much energy they’ll get from wind and solar farms. Pilots and sailors need them to carry people across oceans safely. Accurate information about future weather is often absolutely vital. In this article, I look at improvements over time and the global inequalities that need to be closed to protect lives and livelihoods around the world. Weather forecasts have improved a lot Weather forecasting has come a long way. In 650 B.C. the Babylonians would try to predict weather patterns based on cloud patterns and movements. Three centuries later, Aristotle wrote Meteorologica, discussing how phenomena such as rain, hail, hurricanes, and lightning formed. Much of it turned out to be wrong, but it represents one of the first attempts to explain how the weather works in detail. It wasn’t until 1859 that the UK’s Meteorological Service (the Met Office) issued its first weather forecast for shipping. Two years later, it broadcasted its first public weather forecast. While meteorological measurements improved over time, the massive step-change in predictions came with the use of computerized numerical modeling. This didn’t start until a century later, in the 1960s. Forecasts have improved a lot since then. We can see this across a range of measurements, and different national meteorological organizations. The Met Office says its four-day forecasts are now as accurate as its one-day forecasts were 30 years ago. Predictions have gotten much better in the United States, too. We can see this in some of the most important forecasts: the prediction of hurricanes. The National Hurricane Center publishes data on the “track error” of hurricanes and cyclones — the error in where the hurricane hits. This is shown in the chart below, from the 1960s onwards. Each line represents the error of forecasts for different time periods in advance. For example, 12 hours before it hits, all the way up to 120 hours (or 5 days) before. We can see that this track error — especially for longer-term forecasts — has decreased a lot over time. In the 1970s, a 48-hour forecast had an error between 200 and 400 nautical miles; today this is around 50 nautical miles. We can show the same data another way. In the chart below, each line represents the average error for each decade. On the horizontal axis we have the forecast period, again extending from 0 to 120 hours. The 72-hour error in the 1960s and 70s was over 400 nautical miles. Today, it’s less than 80 miles. Meteorologists can now make pretty accurate predictions of where a hurricane will hit three or four days in advance, which lets cities and communities prepare while preventing unnecessary evacuations that might have been implemented in the past. The European Centre for Medium-Range Weather Forecasts (ECMWF) produces global numerical weather models. While national weather agencies use much higher-resolution processing to get local forecasts, these global models provide a crucial input into these systems. The ECMWF publishes analyses of its errors over time. This is shown in the chart below.1 It shows the difference between the forecast and the actual weather outcome for forecasts 3, 5, 7, and 10 days in advance. The metric used here is the “500 hPa geopotential height”, a commonly used meteorological measure of air pressure (which dictates weather patterns). The solid line is for the Northern Hemisphere, and the dashed line is for the Southern. Three-day forecasts — shown in blue — have been pretty accurate since the 1980s, and have still gotten a lot better over time. Today the accuracy is around 97%. The biggest improvements we’ve seen are for longer timeframes. By the early 2000s, 5-day forecasts were “highly accurate” and 7-day forecasts are reaching that threshold today. 10-day forecasts aren’t quite there yet but are getting better. Why have weather forecasts improved? A few key developments explain these improvements.2 The first big change is that the data has improved. More extensive and higher-resolution observations can be used as inputs into the weather models. This is because we have more and better satellite data, and because land-based stations are covering many more areas around the globe, and at a higher density. The precision of these instruments has improved, too. These observations are then fed into numerical prediction models to forecast the weather. That brings us to the next two developments. The computers on which these models are run have gotten much faster. Faster speeds are crucial: the Met Office now chunks the world into grids of smaller and smaller squares. While they once modeled the world in 90-kilometer-wide squares, they are now down to a grid of 1.5-kilometer squares. That means many more calculations need to be run to get this high-resolution map. The methods to turn the observations into model outputs have also improved. We’ve gone from very simple visions of the world to methods that can capture the complexity of these systems in detail. The final crucial factor is how these forecasts are communicated. Not long ago, you could only get daily updates in the daily newspaper. With the rise of radio and TV, you could get a few notices per day. Now, we can get minute-by-minute updates online or on our smartphones. Low-income countries have much worse forecasts, and often no early warning systems At home in Scotland, I can open an app on my phone and get a pretty accurate 5-day forecast within seconds. Unfortunately, this quality of information isn’t available to everyone. There are large differences in weather forecasts across the world, with a large gap between rich and poor. As the researchers Manuel Linsenmeier and Jeffrey Shrader report in a recent paper, a 7-day forecast in a rich country can be more accurate than a one-day forecast in some low-income ones.3 While national forecasts have improved over time across all income levels, the quality gap today is almost as wide as it was in the 1980s. There are a few reasons for this. First, far fewer land-based instruments and radiosondes measure meteorological data in poorer countries. Second, the frequency of reporting is much lower. This is unsurprising when we look at the amount of money spent on weather and climate information. In a paper published in Science, Lucian Georgeson, Mark Maslin, and Martyn Poessinouw looked at differences in spending across income groups.4 This includes private and public spending on commercial products that fall within the definition of “weather and climate information services”. This is shown as the spending per person, and the spending as a share of gross domestic product (GDP) in the chart below. Low-income countries spend 15 to 20 times less per person than high-income countries. But given the size of their economies, they actually spend more as a share of GDP. This gap is a problem. 60% of workers in low-income countries are employed in agriculture, arguably the most weather-dependent sector. Most are small-scale farmers, who are often extremely poor. Having accurate weather forecasts can help farmers make better decisions. They can get information on the best time to plant their crops. They know in advance when irrigation will be most needed, or when fertilizers might be at risk of being washed away. They can receive alerts about pest and disease outbreaks so they can either protect their crops when an attack is coming or save pesticides when the risk is low. That means they can use precious resources most efficiently if they have access to accurate weather forecasts. Good weather forecasts are most crucial for the poorest people in the world. They’re also crucial for protecting against cyclones, heat waves, flooding, and storm surges. Having accurate forecasts several days in advance allows cities and communities to prepare. Housing can be protected, and emergency services can be on standby to help with the recovery. But accurate forecasts alone don’t solve the problem: they’re only useful if they are disseminated to people so they can respond. Many of the deadliest disasters over the last few decades were accurately forecasted ahead of time. The common failure was poor communication.5 Improving forecasts is the foundation. But these also need to be incorporated into effective early warning systems. The World Meteorological Organization estimates that around one-third of the world — predominantly the poorest countries — do not have them. Improving forecasts — especially in low-income countries — is underrated After the big progress in recent decades, we take good weather forecasting and dissemination for granted in large parts of the world. Making this available to everyone would make a difference. This will be even more important as climate change increases the risks of weather-related disasters. It is ultimately the poorest, who are the more vulnerable, who will suffer the worst consequences. Better forecasts are key to good climate change adaptation. Proper investment and financial support will be essential to close the gaps. There also emerging technologies that could accelerate this. A recent paper published in Nature documented a new artificial intelligence (AI) system — Pangu-Weather — that can perform forecasts as accurately (or better) than leading meteorological agencies up to 10,000 times faster.6 It was trained on 39 years of historical data. The speed of these forecasts would make them much cheaper to run and could provide much better results for countries with limited budgets. Faster and more efficient technologies can also fill the gaps where land-based weather stations aren’t available. Sensor-carrying drones can run surveys over specific areas to build higher-resolution maps. With lower-cost and more efficient ways of turning that into forecasts, mobile technologies can disseminate this information quickly. Some companies are already sending messages to farmers in low-income countries to advise them on the best time to plant their crops. This innovation is crucial to making countries more resilient to weather today. But it’s also essential in a world where weather is likely to get more extreme. Acknowledgements Many thanks to Max Roser and Edouard Mathieu for their valuable feedback and comments on this article. Endnotes These charts are sourced from the following sources: Alley, R. B., Emanuel, K. A., & Zhang, F. (2019). Advances in weather prediction. Science, 363(6425), 342-344. The Economist (2023). The high-tech race to improve weather forecasting. Alley, R. B., Emanuel, K. A., & Zhang, F. (2019). Advances in weather prediction. Science, 363(6425), 342-344. Linsenmeier, Manuel & Shrader, Jeffrey G., 2023. \"Global inequalities in weather forecasts,\" SocArXiv 7e2jf, Center for Open Science. You might wonder how this can be true when the ECMWF forecasts have so quickly improved in both the Northern and Southern Hemisphere. This is because most countries need to develop their own forecasts to get more high-resolution predictions. Georgeson, L., Maslin, M., & Poessinouw, M. (2017). Global disparity in the supply of commercial weather and climate information services. Science Advances, 3(5), e1602632. De Perez, E. C., Berse, K. B., Depante, L. A. C., Easton-Calabria, E., Evidente, E. P. R., Ezike, T., ... & Van Sant, C. (2022). Learning from the past in moving to the future: Invest in communication and response to weather early warnings to reduce death and damage. Climate Risk Management, 38, 100461. Bi, K., Xie, L., Zhang, H. et al. Accurate medium-range global weather forecasting with 3D neural networks. Nature 619, 533–538 (2023). Cite this work Our articles and data visualizations rely on work from many different people and organizations. When citing this article, please also cite the underlying data sources. This article can be cited as: Hannah Ritchie (2024) - “Weather forecasts have become much more accurate; we now need to make them available to everyone” Published online at OurWorldInData.org. Retrieved from: 'https://ourworldindata.org/weather-forecasts' [Online Resource] BibTeX citation @article{owid-weather-forecasts, author = {Hannah Ritchie}, title = {Weather forecasts have become much more accurate; we now need to make them available to everyone}, journal = {Our World in Data}, year = {2024}, note = {https://ourworldindata.org/weather-forecasts} } Reuse this work freely All visualizations, data, and code produced by Our World in Data are completely open access under the Creative Commons BY license. You have the permission to use, distribute, and reproduce these in any medium, provided the source and authors are credited. The data produced by third parties and made available by Our World in Data is subject to the license terms from the original third-party authors. We will always indicate the original source of the data in our documentation, so you should always check the license of any such third-party data before use and redistribution. All of our charts can be embedded in any site.",
    "commentLink": "https://news.ycombinator.com/item?id=39678783",
    "commentBody": "Weather forecasts have become more accurate (ourworldindata.org)286 points by sohkamyung 21 hours agohidepastfavorite159 comments jrockway 17 hours agoPeople seem to have different opinions on how good forecasts are. I think it likely depends on which model your forecast source of choice pulls from. I notice that the weather on my Apple Watch corresponds exactly to what GFS says. GFS is OK for medium range, but I don't find it too useful for shorter range. NAM is better for a day or two out. HRRR is better for a few hours out. Rather than letting some aggregator simplify the weather for you, you can just look at the raw data yourself: https://weather.cod.edu/forecast/ For big events, the media briefings by the National Weather Service are good resources. But they often stop the briefings early; a few weeks ago we had a high probability of a large amount of snowfall. The updates stopped at like 9AM, the snow was forecast to start around 1PM. Watching the short term models showed that the probability for snow was decreasing (NYC was just below the snow/rain line), and indeed we got pretty much no snow. (It snowed, but it didn't accumulate and the change to rain happened early.) To be fair, the briefing from the weather service said that the changeover time between snow and rain was very uncertain and that it would be the difference between a little rain and major snow event. But my point is, you can always go get yourself some more data; the closer you get to the event, the more accurate the forecast is. (I don't know if any of you watch Skip Talbot, but he was looking at helicity swaths on the HRRR a few hours out, found a big one, and where HRRR predicted the strong rotation in the storm is pretty much exactly the path of a major tornado. HRRR is never going to be perfect, but it is right a lot.) reply ryandrake 11 hours agoparentOur local TV station weatherman has a YouTube channel[1] where he geeks out every morning about the weather, providing a much more detailed forecast than he has time for during the brief windows he has on the TV news. Walks through the HRRR, NAM, GFS, satellite pictures, and other sources of information. It's a nice compromise if you find the raw data to be overwhelming. 1: https://www.youtube.com/@markfinanweather reply jayknight 9 hours agorootparentThere's no substitute for a local meteorologist who knows how weather patterns work in your region and knows how to interpret the models and is good at communicating that to regular people. reply genghisjahn 8 hours agorootparentAgreed. For Philly I go to this guy. https://theweatherguy.net/blog/ reply dylan604 7 hours agorootparentprevAbsolutely. It is something that I find amusing about people moving from California, LA/SoCal in particular, where their weather is basically just a nice segue from celebrity talk into more celebrity talk that reads some report the producers pulled. Then, they move to town and are amazed at how much people actually pay attention to the weather and comment on how many people actively have open tabs with the live radar. I usually reply with when you need to know if your ride to Oz is coming or not, you pay a lot closer attention. reply unsignedint 14 hours agoparentprevI primarily rely on Windy for weather forecasts, which I find exceptionally useful due to its ability to compare multiple models. The variety of overlays available makes it an indispensable tool for all my weather-related needs. [0]: https://windy.com reply bamboozled 11 hours agorootparentWindy uses some of the models mentioned including GFS, you can select the model you want to use. So I’m not sure it would be any more accurate than the Apple Watch. reply unsignedint 10 hours agorootparentIf you're simply seeking basic weather information, then what you receive from your Apple Watch won't differ much. However, if you prefer to analyze and interpret the raw data yourself, Windy stands out as an excellent resource. It aggregates numerous data sources, offering a comprehensive platform for informed decision-making regarding the weather. reply bamboozled 5 hours agorootparentI see what you mean thanks. Have you ever sen WunderMap? Not quite in the same league just an interesting source of information. It's kind of like windy, but based on near realtime data from personal weather stations around the country. Kind of fun. https://www.wunderground.com/wundermap reply uoaei 13 hours agorootparentprevSame here! Not to be confused with windy.app! reply WillPostForFood 10 hours agorootparentThat is confusing! Does windy.com have an app? reply unsignedint 10 hours agorootparentYes! https://play.google.com/store/apps/details?id=com.windyty.an... https://apps.apple.com/us/app/windy-com-weather-radar/id1161... reply IG_Semmelweiss 8 hours agoparentprevI find that more important than the model used, is that you get actionable intel. To me, actionable intel boils down to 2 P's . Precision, and Probability. So, I don't care if tomorrow there's a 50% chance of rain. I care that at a precise time of the day , say 9am, has a 10% precipitation and at noon is 90% , because i commute at 9am, not at noon. Wind is also an important factor if its raining. Temp as well. I need all this info presented as a mosaic. For this purpose, I find the NOAA forecast local by hour is unrivaled. https://www.weather.gov/okx/ . Enter ZIP and then in enter local forecast by hour. I have this URL bookmarked in my browser. I haven't looked back since. Example: https://forecast.weather.gov/MapClick.php?lat=33.797&lon=-11... I'd love to know if there is an android app that gives this level of detail, preferably, without spying into my microphone... reply Sn0wCoder 5 hours agorootparentI second weather dot gov mapclick. This makes me want to finally fix my PWA that used the Dark Sky API that quit working in 2023. This PWA also had a feature to click an icon and feed the long/lat into the mapclick URL which is the feature I used most often. I had a bookmark for a free replacement API for Dark Sky (that I got from HN) but have since re-formatted my computer. I know slightly off topic, but does anyone have a recommendation for a good free (couple users a day) weather API? Any suggestions would be appreciated as this has given me the motivation to hook it back up. Cheers. reply rootusrootus 16 hours agoparentprevThat's an interesting point about the Apple weather forecast. That correlates pretty well with my experience. It is exceptionally inaccurate at short range forecasts. It's kind of a running joke at this point. reply herpdyderp 16 hours agorootparentThe most humorous part to me is when it says it's currently raining or snowing and it's clear and sunny. How can a system be so wrong that it can't tell the current state of the weather? reply counters 15 hours agorootparentDo you want a snarky answer or a serious answer? The serious answer is that the way you'd try to figure this out is by combining weather radar, satellite imagery, and a nearby surface observation to try to estimate the current conditions. But there can be a latency of up to a few minutes from these sources, and they could disagree with one another. You have to use them to bootstrap your near-term or nowcast product, but enforcing consistency with recent real-time and the nowcast is quite hard. It's a surprisingly nuanced technical challenge. Most of the time, it works out just fine (e.g. if there is no weather). But people are awfully good at remembering when these sorts of analyses end up being wrong! reply jrockway 15 hours agorootparentYeah. Like you would think you could just look at reflectivity data to determine whether or not it's currently raining, but at most places you are far from a radar site and even the 0.5 degree tilt is scanning a mile above your head. There might be rain there, but is it reaching the ground? All you can really do is guess. If you're interested in providing on-the-ground condition reports, install mPING: https://mping.nssl.noaa.gov/ I keep this app on my homescreen and try to report when very light rain starts, since it's not always obvious from the reflectivity data. Ultimately the user reports get fed into things like improving the model, and more data is always good. reply avar 12 hours agorootparentprevApple devices are constantly phoning home every time they see a random AirTag out in the world. You'd think that if their users are accepting that level of communications with the mothership that they could ship some AI model to hear rainfall in the wild, and thus improve their live weather data. reply inemesitaffia 4 minutes agorootparentI believe they send barometer information home stavros 46 minutes agorootparentprevWhat's wrong with devices phoning home if they don't actually send any usable data there? reply counters 11 hours agorootparentprevSurprisingly low signal-to-noise ratio for most of the common, creative ways people come up with to detect rain. Windshield wipers on cars are another example. The thing is, even if you did have a super reliable in situ \"rain detector\", how do you combine it with the existing datasets like weather radar, which is a gridded product? This is actually a really, really difficult sensor fusion problem when you then super-impose product requirements like the general location real-time detection map and the inputs necessary for whatever internal nowcasting system they use. reply TylerE 6 hours agorootparentprevIf you care that much buy a $100 weather station and sit it outside your window? Actual ground observation weather stations are fairly rare outside of places like airports and major news stations. “Is it raining here right now?” Is a harder question than you’re giving it credit for. Radar can show rainfall at as low as a couple thousand feet altitude, but if conditions are right/wrong (depending on how you look at it) it never reaches the ground. reply krisoft 6 hours agorootparentprev> if there is no weather And the ship has been towed beyond the environment. There is nothing out there, all there is is sea, and birds, and fish. And 20,000 tons of crude oil. And a fire. Less sarcasticaly speaking I think there is always weather. Maybe what you mean is “no significant change in the weather” neither in time, nor in space. reply bee_rider 12 hours agorootparentprevIt would be kind of interesting if the app had a “you are wrong” button, which allows you to take a picture of the outdoors. Apple could either use this to improve their models, or even just use it as input data directly if they get enough complaints. Plus, it would allow people to vent, or it could check if there is something wrong with the phone, maybe location is being mis-read or something like that. reply counters 11 hours agorootparentThere isn't a vector where after-action reports like this could \"improve the model.\" That data is useful for verification, but these systems generally have no learning component to feed the data back into them to improve them. reply bee_rider 9 hours agorootparentThey have the input from all the different sensors and forecasts. Why not, for a given location, keep track of which one gave the best results? Sure, they don’t have a way of keeping track of they now, but it seems like it could be added. reply counters 7 hours agorootparentThis is already SOP at most reputable weather data providers; they consume many different numerical forecasts and use statistical post-processing to choose an optimal blend of the available forecasts based on how different forecasts have verified against observations. But this sort of technique only works for medium range forecasts. Short-range precipitation nowcasts are almost always a single, deterministic run of a model that extrapolates from patterns in recent radar imagery. They aren't bias corrected at all, so you can't use observations in the same way to improve them. reply tspike 11 hours agorootparentprevIt does have that: “report an issue.” reply Fomite 10 hours agorootparentprevYeah - nowcasting turns out to be remarkably difficult at times, especially at very small spatial resolutions. reply stavros 45 minutes agorootparentprevWait, what's the snarky answer?! reply Etheryte 15 hours agorootparentprevI can't even begin to count how many times I've had this conversation with Siri. \"Hey Siri, is it going to rain?\" \"It doesn't look like it's going to rain today.\" \"It's raining right now.\" \"It isn't raining right now.\" reply throw0101d 14 hours agorootparent> I can't even begin to count how many times I've had this conversation with Siri. I live in Toronto, Canada, which stretches about 40km east-west, and 20km north-south: If the west-end (Sherway) gets hit with rain, but the east-end is dry, did it rain \"in\" Toronto when folks in Scarborough didn't experience it? Was the forecast wrong? If it snows in North York but is dry at Billy Bishop, was the precipitation forecast \"wrong\" for one particular group of people? reply Etheryte 12 hours agorootparentApple Weather uses your precise location if you allow it to, meaning it knows your location down to a meter, network and positioning issues etc notwithstanding. It doesn't have to guess your weather based on \"Toronto\", it knows your GPS coordinates. There is no technical limitation here, as I outlined in a separate comment thread [0], other apps already give you weather data and predictions with this granularity. [0] https://news.ycombinator.com/item?id=39683660 reply sznio 3 minutes agorootparent>It doesn't have to guess your weather based on \"Toronto\", it knows your GPS coordinates. Meanwhile the Google Weather app constantly insists I live in Frankfurt while I'm in Warsaw. lotsofpulp 14 hours agorootparentprevWhy would you expect Siri to know if it is raining at your specific location? Surely there exists an edge where on one side it is raining and on the other it is not raining. So unless you are sitting next to the the weather station that Siri is getting data from, I would not expect it to know 100% of the time. reply ceejayoz 13 hours agorootparent> Why would you expect Siri to know if it is raining at your specific location? I don't, but as a result, I expect it not to guess. reply Etheryte 14 hours agorootparentprevI live in the Netherlands. The local weather apps tell me when it's going to rain with nearly minute precision, along with cloud maps with scrollable time, graphs of how heavy the rain will be at what time, etc. It's pure nonsense to claim this is a technical limitation when other apps do it with ease. No one is expecting it to be right 100% of the time, but Apple Weather is wrong about rain most of the time, even on a crude scale of say, a city. reply ako 16 hours agorootparentprevForecast are usually for a larger area, 5x5 kilometers, or 10x10 kilometers. Even within this area, weather will not be the same everywhere, so they'll give a probability for the entire area. Windy.com lets you compare different models for a specific location, it also includes the size of the area per model: https://www.windy.com/?49.339,5.054,5 GFS is area is 22km, ECMWF 9km, ICON-D2 2.2Km, Arome 1.3Km, and UKV is 2Km. Even in a 1.3x1.3Km area it may not rain everywhere at the same time. And then there's also the time element, so it's 1.3Kmx1.3Kmx1Hrs (or 3Hrs). So lot's of variation possible. reply amarcheschi 15 hours agorootparentYup, a few days ago I made a python script to help me choose whether to get to uni by bike or by moped when it rains (given two coordinates I calculate the angle(bearing?) and checks whether it rains, and the angle from which the wind blows to see if I'll get all wet in the face) and I had a bit of a hard time figuring out why two different providers, windy and openweathermap, gave me 2 different wind results. Eventually, I found out they were using a different model, it took a bit of time tho, because windy only has increments of hours, while the other one was more granular reply bumby 16 hours agorootparentprevFrom the article: >”These observations are then fed into numerical prediction models to forecast the weather.” In other words, the forecasts come from models, not necessarily real-time station readings. Those readings are inputs into the model, and the models may not get updated fast enough to reflect current conditions. reply Fomite 10 hours agorootparentOr it might be raining at the station the reading is being made at, but not where you are. Living in the west for example, a lot of weather stations are remarkably broad in the area they're expected to represent. reply bumby 9 hours agorootparentThe point is that the forecasts are often not built from real-time weather station data, but models using various initial conditions. reply NikkiA 11 hours agorootparentprevBecause the 'current' weather isn't, it'll be whatever the last update of your chosen weather station reported. Often people choose a generic station that can be quite far from where they actually are - my default if I allow weather sites to 'guess' tends to be the airport 14 km away and 200m higher than me. A lot of weather can pass me by and still not have gotten to the airport in the 40 minutes since the last update. reply wiredfool 16 hours agorootparentprevApple weather quite often has the \"expected radar\" function show storms taking a 90 degree turn right around now, so you'll see rain coming from the west, and suddenly when it gets to predictions, it's traveling north. (Note, this is Ireland). Dark sky was a lot better. I've also noticed that Met.ie will typically predict more rain, and they're usually right. (e.g., last weekend was basically rain/drizzle/wind the whole time, met.ie nailed it, apple weather said that there would be an hour on Sat and all Sunday morning would be wet. Of course, predicting rain in Ireland is not difficult. reply blaufuchs 14 hours agorootparentprevThis one really kills me during Fogust in the Bay Area. I wake up and see the sun is gonna break through at ~1pm, oh no actually 2pm, oh no actually 3pm... oh no it's just another completely overcast day. I can understand missing a day or two, but it's bizarre when it happens day after day for weeks on end. You'd think the priors would get updated at some point. reply dylan604 7 hours agoparentprev> I think it likely depends...my Apple Watch corresponds exactly to what GFS says. GFS is OK for medium range which is interesting, as i'm noticing the \"within 15 minutes\" level of notice on rain starting/stopping to have been close enough. the daily forecast last week said no rain even though the conditions really looked like it could at any moment. my iDevices pinged with rain starting soon even though the same apps forecast still did not suggest rain. it started raining with in \"good enough\" range of the app's notifications. the update to the native weather app have all been very good over the past 2 OS updates. maybe they have integrated whatever company they purchased for good, but for my local area on the globe, it has been pretty good. i haven't traveled in a good while, so maybe my market is in the sweet spot of getting a lot of attention??? BigD in case you're wondering reply perfectritone 6 hours agorootparentApple acquired DarkSky and sunsetted their API in the past few years. It always seemed more accurate to me than any other weather service. Sad to see it go. reply dylan604 5 hours agorootparentto me it didn't go, it just got rolled into Weather.app. I never used the original app, so I have not direct comparsion. however, if they are using the darksky data/tech/etc into weather.app, then it's better for me. not really sure where/how/why the new app updates are better under the hood, but they just are which makes it all seem like worthy upgrades reply SiempreViernes 16 hours agoparentprevAlso useful to keep in mind is that predictions can become more accurate without necessarily improving in precision. reply monkburger 10 hours agoparentprevThe NAM overestimates southern jetstream energy. And the 3km is just awful for anything beyond 24hr. HRRR is OK, but usuallyIn The Weather Machine, Andrew Blum takes readers on a fascinating journey through an everyday miracle. In a quest to understand how the forecast works, he visits old weather stations and watches new satellites blast off. He follows the dogged efforts of scientists to create a supercomputer model of the atmosphere and traces the surprising history of the algorithms that power their work. He discovers that we have quietly entered a golden age of meteorology—our tools allow us to predict weather more accurately than ever, and yet we haven’t learned to trust them, nor can we guarantee the fragile international alliances that allow our modern weather machine to exist. * https://www.andrewblum.net/the-weather-machine-2 * https://www.goodreads.com/en/book/show/42079139 For the very early history of meteorology, see perhaps The Invention of Clouds about Luke Howard: * https://www.goodreads.com/book/show/1148768.The_Invention_of... * https://en.wikipedia.org/wiki/Luke_Howard reply jrmg 13 hours agoprevI remember reading in The Signal and The Noise* that people _think_ that forecasts are bad if it rains, but the chance of rain was reported as below 50%. Getting rain when the forecast told you there would probably not be rain is annoying; getting a sunny day when the forecast predicted likely rain is a pleasant surprise. To get what people judge to be a ‘good forecast’, the chance of rain has to be adjusted to be wildly too high - so that’s what consumer-focused forecasters do. * https://en.wikipedia.org/wiki/The_Signal_and_the_Noise reply avar 12 hours agoparentHere in the Netherlands everyone mostly uses short-term live radar tracking of rain clouds and precipitation over actual predictive weather forecasting. In an urbanized area most \"is it going to rain?\" questions are short-term, e.g. is now or 30 minutes later a good time to bike home? Perhaps this wouldn't be as useful in other areas. The Netherlands gets very spotty rain. So even if you've got a 100% chance today it's probably 1-2 hours spread throughout the day, and sometimes very heavy rain followed by a dry spell. The only time I've seen it to be incorrect is if a moving rain cloud just barely misses you due to changes in wind patterns. reply moritzwarhier 12 hours agorootparentSame here from Germany. Never really thought about it, but I've opened the \"Rain radar\" more frequently than any weather app including the native one during the last couple of years, too. reply diordiderot 6 hours agorootparentSame in UK Though I don't know anyone else who does this that doesn't cycle. reply bee_rider 12 hours agoparentprevI sometimes wonder if people think forecasts are bad because they think of it in terms of: there are two possibilities, the forecast will be wrong, or it won’t. Therefore, the weatherman should be right at least half the time. Of course, there are countless ways for the for the forecast to be wrong, and only a couple ways for the forecast to be right! reply bobbylarrybobby 5 hours agorootparentI had a high school teacher (not math!) who insisted that if you guessed on a multiple choice question, your chance of getting it right was 50%, regardless of how many choices there were. reply HankB99 12 hours agoparentprev> the chance of rain was reported as below 50%. I've come to think of that as \"it is going to rain 50% of the time. I don't know if that's what really is meant by \"50% chance of rain,\" but it seems to fit. And overall I tend to believe that the forecast is astonishingly accurate. This is in the Midwest (Chicago market) where weather has to cross large portions of the country or Canada before it gets to us. I suppose there are areas on the coast where weather is more volatile and harder to predict. reply kxrm 12 hours agorootparentChance of rain is defined by NWS as: \"The probability of precipitation (POP), is defined as the likelihood of occurrence (expressed as a percent) of a measurable amount of liquid precipitation (or the water equivalent of frozen precipitation) during a specified period of time at any given point in the forecast area. Measurable precipitation is equal to or greater than 0.01 inches. Unless specified otherwise, the time period is normally 12 hours. NWS forecasts use such categorical terms as occasional, intermittent, or periods of to describe a precipitation event that has a high probability of occurrence (80%+), but is expected to be of an \"on and off\" nature.\" Source: https://www.weather.gov/bgm/forecast_terms reply lathiat 7 hours agorootparentOne of the realisations I only really had in my early 30s is that whether or not it's raining is much more localised than I thought. It can be raining here, and not at the shops a suburb over. The percentage chance of rain includes whether or not it might rain in your specific dot of a given forecast area, which might be a suburb or entire city, as spelt out in your quote \"at any given point in the forecast area\". The first time I drove over the Nullarbor in Australia, which has an entirely flat and straight 100+ KM section of road, I got to see rain far in the distance and experience driving into it having been able to clearly see it's edge from far out. That was an experience I had never had in the costal city I live in (Perth). That also led into similar realisations as the above. It sounds so simple in theory but was not obvious to me for a long time :) reply luplex 12 hours agorootparentprevI think it means \"for any given point in the specified area, and for any given point in time, p(rain)=50% So it of course won't rain for exactly 50% of the time on a given day, but over the long run, it will. reply bombcar 12 hours agorootparentprevMy experience is that about 50% is where “rain will happen somewhere nearby, it may affect me”. reply nektro 12 hours agorootparentprev\"50% chance of rain\" means that there's 100% chance of rain for 50% of the area reply kirrent 12 hours agoparentprevAmusingly he was unwittingly writing about his own future. People still make fun of Silver for Trump's win in 2016 because 538's final prediction of about 30% likelihood for Trump was 'wrong'. reply goatkey 17 hours agoprevAnyone who lives in a hurricane-prone area like myself (Florida) knows that while forecasts have gotten a _lot_ better, there is still so much room for improvement. I am not affiliated, but I recommend checking out https://www.forecastadvisor.com/ to see what forecasts are best for your city. I totally changed weather providers and it seems much better now. 'The Secret World of Weather: How to Read Signs in Every Cloud, Breeze, Hill, Street, Plant, Animal, and Dewdrop' by Gooley is a fun read for anyone interested in figuring out weather without a forecast (or to supplement). reply artdigital 5 hours agoparentSadly something like this doesn't exist internationally from what I can tell. I live in Japan and have no idea which sources are good and which are bad. The local apps pull data from the Japan Meteorological Agency, so does Apple Weather, and so does Carrot Weather since a recent update (though those 2 still give me different results). Outside of Japan, when I travel, I have no idea so I just leave the Carrot Weather source on Apple Weather, because that at least pulls data from local weather services if available (https://developer.apple.com/weatherkit/data-source-attributi...) reply AceyMan 16 hours agoparentprevObligatory citation, https://en.wikipedia.org/wiki/Weather_rock reply jrockway 15 hours agorootparentIt also keeps tigers away. I don't see any tigers around there, anyway. reply Zamiel_Snawley 17 hours agoprevThis article mostly discusses longer-term forecasts, but I have also been impressed with the quality and reliability of imminent-storm alerts. They have saved me from getting drenched in a rain storm or allowed me to pull off the road for a break before a downpour. It doesn't get a ton of press, but as this article highlights, progress has been steady and significant. This article asserts that improving forecasts in low-income countries is underrated--does anyone know of studies that predict the impact better forecasts would have? Helping the poor with tech seems like the kind of project that many philanthropists could get excited about, and hopefully more effective than gravity lights and the like. reply macintux 16 hours agoparentAs someone who drives much of the summer with no top on his Jeep, Dark Sky was a revelation. I also managed to find a route between two bands of heavy thunderstorms (with a tornado watch to boot) one night far from home with no top and no doors using the radar. Modern technology is amazing. reply darknavi 15 hours agorootparent> Dark Sky was a revelation Any replacement for it on iOS? Maybe I am crazy but Apple's weather alerts just don't seem like the same sauce. reply Larrikin 14 hours agorootparentI think they were doing some magic with the Android phone sensors, large amounts of user reports, as well as the actual forecast models. Before Apple bought them, my Android phone was its own party trick at the bar. I'd be able to tell people down to the minute when it would start and stop raining. It was amazing for bar hopping on bad weather days. reply counters 14 hours agorootparentNope. Simple computer vision / optical flow applied to radar image sequences. reply declaredapple 13 hours agorootparentWhat the actual crap did Apple do to mess it up so bad then? Switching between providers on Carrot, Apple Weather often doesn't predict any amount of rain for the entire week, meanwhile I'm soaked in water in a thunderstorm, and NOAA and others predicted rain the entire week (which it did). reply counters 11 hours agorootparentNo clue. They have strong folks on their weather team, too. Not obvious what's gone wrong over there. reply declaredapple 13 hours agorootparentprevDark sky used to be accurate almost to the minute for me. Apple Weather will tell me it won't rain today or all week. Meanwhile NOAA will tell me I'm currently in a thunderstorm and that it will rain all week - And it was right. Carrot is nice because you can switch between several providers. reply open-meteo 1 hour agoprevCreator of the open-source weather API open-meteo.com here. The future of weather forecasting is likely to rely heavily on AI models. The article discusses Pangu Weather and HN comments mention GraphCast as examples. Interestingly, on the first of March, the European weather forecast center ECMWF released their new AI weather model AIFS as open data. This model is not only more accurate than their existing numerical model, but also requires significantly less computing power to run. They've published comparisons showing AIFS outperforms other models in terms of forecast precision: https://www.ecmwf.int/en/about/media-centre/aifs-blog/2024/f... reply paxys 16 hours agoprevAll the stuff mentioned in the article is accurate – better raw data, faster computers, smaller grids, better predictive algorithms etc. all result in vastly better weather info in general today. This also means though that you have to put in more effort to get a better result for yourself. What algorithm is the app using? Does it localize all the way to your neighborhood, or your street? How frequently does it update? Is your GPS accurate? People generally don't think about this stuff, but some fine tuning can result in vastly better results. reply dataflow 17 hours agoprevI'm sure they have, but I've also been drenched while reading a weather report that refused to admit it was raining in my city right now. It just told me it was cloudy, despite clear and rather heavy rain for 30+ mins straight over the whole city. To this day I haven't figured out how that's even possible. reply NordSteve 17 hours agoparentOne factor is your distance from the nearest weather radar, and nearest airport with automated weather observation. This sort of prediction is heavily dependent on whether the precip is detected by sensing. I've seen similar things in our area (Minnesota) where you drive through a snowstorm, but the radar shows nothing in theare. reply cogman10 16 hours agorootparentI've seen variations in weather literally 10 miles apart. With torrential rain at work and nothing at home. I can't see how any weather predictor could be correct in that situation. reply importantbrian 15 hours agorootparentLiving in Florida I've driven down the road and had it be raining on one side of the road and not raining on the other while the sun is shining. reply Fomite 10 hours agorootparentprevYeah. There's a place nearby where I live that's used for a lot of outdoor recreation like fishing and rafting, and the two weather stations that serve that area are a long way away. reply bumby 16 hours agoparentprevWas it saying there was 0% chance of rain or did it just not update to 100% even though you were experiencing rain? The latter is somewhat common because the models (AFAIK) use probabilistic estimates, where different initial conditions generate potentially distant outcomes. The number of “rainy outcomes” defines the probability of rain, and doesn’t necessarily get updated with real conditions. reply hazbot 12 hours agoparentprevThis is probably because of either poor sensor coverage, or a stale (old) forecast. Many weather services do not issue 'nowcasts' that constantly update with the latest weather observations (it's a hard and interesting problem), but rather a single forecast say 4x a day as the latest Numerical Weather Prediction model run comes in. Fwiw, I agree with your bemusement and scorn - it's not good enough! (I say this as someone who has had roles where I issued these 'always stale' forecasts) reply n_plus_1_acc 17 hours agoparentprevDepending in which sources they used, they simply interpolate on a very rough grid reply dataflow 17 hours agorootparentI mean, I guess, but how rough of a grid are we talking? This wasn't a tiny city or something... it was a pretty populated city spanning a few miles across in a very populated and well known region. Granted I didn't walk around to check the whole city for rain, but the sky didn't make it look like the clouds were only above my head... reply InSteady 16 hours agorootparentmicrobursts are a thing. It's entirely possible much of your city was dry despite the cloud cover. Also possible they just done goofed. reply qwertox 13 hours agoprevI once listened to a podcast [0] with interviews of a couple of scientists at the ECMWF (European Center for Medium-Range Weather Forecasts). I think it was in that episode where one said that every 10 years we improve the forecast by 1 day. It was recorded in 2019, so AI wasn't really that much of a topic as it is today, considering that Google published an AI weather model in November of last year [1]. [0] https://omegataupodcast.net/326-weather-forecasting-at-the-e... [1] https://deepmind.google/discover/blog/graphcast-ai-model-for... reply LgWoodenBadger 17 hours agoprevMaybe coincidentally related that Simpson's paradox article from earlier, but if they've gotten more accurate overall, I certainly do not see it in the 24-hour forecasts. Of course, I'm also probably only paying attention to when it's wrong, not when it's right. I plan my motorcycling based on rain, and the number of times I've gotten caught in rain when it wasn't supposed to rain at all that day is non-zero just this year. reply VHRanger 17 hours agoparentDaily reminder that data isn't the multiple of anecdote. reply Izkata 14 hours agorootparentExcept it literally is. That statement is pretty much only used as a thought-terminating cliche that means \"you're not allowed to have an opinion\". reply not2b 13 hours agorootparentNo, it literally isn't, at least, the plural of anecdotes isn't useful data. To be useful, data need to be collected in a uniform and systematic way. Anecdotes are memory, and it seems we are wired to remember the unusual and the unexpected. So you remember wrong forecasts (especially if you were caught outside unprepared), don't remember correct forecasts. Collecting everyone's anecdotes would not give you any insight about how good or how bad forecasts are. Pointing this out is not an attempt to silence you. reply TheCowboy 9 hours agoprevI took meteorology in high school and our teacher made us do a daily prediction exercise that I think would benefit all people, especially those who put too much weight into their own anecdata. We just had to make a prediction for the next day's weather. Then compare our prediction to the forecasted prediction. It didn't matter how accurate we were for the grade. Just that we systematically performed this exercise. It really made one appreciate the quality of forecasts, and that no, the \"weather man\" is not always wrong at all. A lot of huffing and puffing is from people who lack any rigor in their observations. And if you're trying to contest the accuracy of weather forecasts, or any form of forecasting really, then you really should provide some hard evidence. reply elil17 17 hours agoprevLike many other people are commenting, I have subjectively felt that rain forecasts have gotten worse. I can think of two theories that could explain this. I'd be curious to hear from someone more knowledgeable if any of them are right or plausible. 1. High frequency 5G has thrown off rain forecasts in urban areas. Average prediction accuracy has still improved because rural/suburban areas don't have high frequency 5G. 2. The weather app now shows rain forecasts in time blocks as small as 15 minutes, even though predictions this granular are still inaccurate. This has inflated our expectations for forecast accuracy. reply jerf 16 hours agoparentA theory I've been entertaining lately is that the raw engineering of the weather forecasts has indeed gotten better, but it has been offset by the clickbait-driven need for weather forecasts to declare everything to be the Worst Thing Ever, Click Here To Not Die. Snow storms that would have in my youth been a medium experience hardly worthy of note get their own names and days of breathless pre-coverage from the weather channels nowadays. The net is the improved raw accuracy of the weather forecast is offset by the difficulty of reversing the clickbait layer slathered on top. reply SiempreViernes 16 hours agoparentprevI would guess it is mostly down to option 2, coupled with the fact that aspects such as precipitation onset are possibly not something that actually has improved much: the examples given are hurricane tracks and atmospheric pressure which don't obviously couple tightly to when it starts raining. reply lathiat 6 hours agoparentprevAnother possibility from another top-level comment \"I remember reading in The Signal and The Noise* that people _think_ that forecasts are bad if it rains, but the chance of rain was reported as below 50%. Getting rain when the forecast told you there would probably not be rain is annoying; getting a sunny day when the forecast predicted likely rain is a pleasant surprise. To get what people judge to be a ‘good forecast’, the chance of rain has to be adjusted to be wildly too high - so that’s what consumer-focused forecasters do.\" https://news.ycombinator.com/item?id=39684844 reply cogman10 16 hours agoparentprevAn area has to be extremely densly trafficked before high frequency 5G is deployed. And even then, the whole point is to minimize broadcast range to avoid interference. Further, it's only the upper range of high frequency spectrum that's being used (not sure who owns it) so it's not even every carrier that could interfere. Finally, the most powerful radars are transmitting in the kilowatts range of output. It's hard for me to imagine that the microwatt output of cellphones are often the cause of radar interference. reply Zamiel_Snawley 17 hours agoparentprevHow does mm wave 5G effect forecasts? Interfering with weather radar? reply elil17 16 hours agorootparent23.8-gigahertz 5G signals can look like water vapor to the instruments on weather satellites. reply Workaccount2 15 hours agoparentprevLearning to read radar is phenomenally helpful in determining whether or not it will rain. It's not very difficult either. A few years ago I was able to stop my friend's outdoor wedding (on the terrace as opposed to the hall, the venue had both ready) from getting rained out by reading the radar and catching a small pocket storm that had formed and coming right towards us. Sure enough it down poured, but everyone was inside for the ceremony. Reading just the weather report, there wasn't even rain forecasted. reply vaughnegut 6 hours agorootparentWhere can one learn to read radar, sounds fun and useful! reply brewdad 15 hours agoparentprevShort-term forecasts (1-2 days) seem more accurate than ever. However, weather as a business has meant a race at both ends of the forecast spectrum. Apps now offer minute by minute forecasts on the one hand or 10 and 15 and even 90 day forecasts on the other. Neither of those forecasting models are anywhere near ready for prime time but there is a market demand for them, so they get put out there anyway. reply counters 15 hours agoparentprev(2) is a big ol' bingo. There was a race towards the bottom line of higher spatial and time resolution over the past 5 years (claims along the lines of \"higher resolution means higher accuracy!\"), which led to an awful lot of products that are nothing more than naive interpolations of coarser data. So couple the perception of \"better\"/\"more accurate\" products with a wholly insufficient technical approach to realizing this and you have a perfect storm for end users to feel that weather forecasts are getting worse. They just over-promised and under-delivered because many people who entered the field from outside of it completely underestimated how hard it is to push weather forecasting technology forward. (1) is irrelevant for weather forecasting. reply nipperkinfeet 8 hours agoprevEver since IBM destroyed TWC and Wunderground, things have become worse. Wunderground personal weather stations were the most accurate weather forecasts. reply hedora 8 hours agoparentThe Dark Sky (now Apple Weather) acquisition somehow went worse than those IBM ones. Not only is it less accurate than Dark Sky used to be, it’s somehow managed to be less accurate than pre-acquisition Apple Weather. https://weather.gov/ still works (type your zip code into the box on the top left), thankfully. I’m worried weather.gov is only one election cycle away from being decommissioned. Lobbyists from commercial weather sites nearly got the US weather service killed under Trump. If/when they finally kill NOAA, global shipping will probably collapse, which is why killing it was blocked last time. reply belorn 9 hours agoprevI wonder if energy market speculation is playing a role in making weather forecasts more accurate. I would not be surprised if some stock markets also get influenced by weather, especially during periods of high fluctuations in energy costs. reply vanilla_nut 14 hours agoprevI find that 4 to 7 day forecasts tend to be 80% accurate. So probably a little bit better than they were when I was a kid. Unfortunately, the most important part of any forecast IMO is intensity. I don't care if we're going to get snow flurries all day, but if we're going to get a foot of snow, I would like to know -- and not just when the winter storm warning goes into effect! Similarly, I don't care if we're going to get scattered showers all day. But if we're going to get a downpour in the afternoon, I'd like to know so I can avoid getting caught in a flash flood on a trail or on the road. Same thing applies with temperature: if it's going to be cold all day, good to know. But if a rainstorm is going to remain active during a deep freeze and create a layer of ice on every exposed surface, I need to be prepared for walking, biking, or driving. Fortunately there's a somewhat local weather station near me that provides an RSS feed of longform weather forecasts. But I notice that more and more people wind up surprised by slightly-abnormal weather events as they rely more and more on smartphone weather apps. Weather apps that utterly lack the nuance that a paragraph of text can provide. reply Vagantem 12 hours agoprevInteresting! As a contrast, I'm using historic weather data to predict future weather - couples use my free wedding weather predictor to find the perfect date for their wedding: https://dropory.com/ reply hazbot 12 hours agoparentCool! If you end up wanting to expand beyond just the nearest weather station (forgive me if I've misunderstood your process), you could look into ERA5 - free Numerical Weather Prediction 'reanalysis' of past weather on a regular grid. openmeteo has some open source tools for extracting time series data from it. But, although you get good spatial coverage, the drawback is 'the map is not the territory' - the model's representarion of reality doesn't perfectly mesh with the weather on the ground. reply Vagantem 11 hours agorootparentVery cool - will check out! reply Workaccount2 14 hours agoprevKeep in mind, weather forecasting is something that is exceedingly difficult and that concerns just about everyone. That means it is particularly ripe for confirmation bias. We objectively know that weather forecasts are more accurate than ever. We subjectively know that they are bad/gotten worse, because last Thursday I brought my umbrella to work for nothing. reply cameronh90 11 hours agoprevThe weather forecasts in the UK are definitely much better than they were a decade ago, especially in the 3 to 14 day range. However, I still find my stupid heuristic works quite well for predicting tomorrow's weather: the weather tomorrow will be the same as what it was today. The UK often gets sticky (\"blocking\") weather patterns, so it works surprisingly well. reply redavni 17 hours agoprevThere is a gap between the title of the article and the contents. Starts out with weather forecasting is improved, but spends most of the article talking about how poor people and countries have other things to spend their money on than forecasting weather. reply pornel 7 hours agoparentThey also spend more on forecasts as a share of GDP. It's a pretty bleak picture — it's a bigger cost for them, and they still get worse results. reply kylehotchkiss 16 hours agoprevOne thing I came to appreciate about growing up on the east coast was how much more accurate the forecasts were than they are in Southern California. The winds pushing east, the expansive radar coverage over USA that's publicly available, and the commercial airlines collecting weather data means the storms and weather systems are well understood as they're coming over. Plus the storms make nice straight lines from north to south that push through. In Southern California the rain forecasts always seem off. Even right now, it's raining and the forecast told me it was just supposed to be a little cloudy. reply chasd00 15 hours agoparentthe forecast for Southern California can just be a standing Sunny and 85F. The days it's wrong are infrequent enough to be tolerable. I didn't think that area even had meteorologists. reply antod 12 hours agorootparentThe movie LA Story comes to mind where the TV forecaster prerecords their weather reports. reply egl2021 11 hours agoprevGeezer alert: forecasts look amazingly good to me. When I was a kid in the Pacific Northwest, it was routine to miss major storms until they hit land. We didn't have satellites, oceanic buoys, etc., and I remember the TV weather guy saying things like \"we've had a report from a ship at sea...\" and proceeding to make wild guesses. reply nvahalik 11 hours agoprevThis might be true for some areas but maybe not for others. Where we live is on the very edge of the NWS coverage. Our forecasts and messaging is generally a lot more “loose” than folks closer to the main NWS “office”. I am not sure if this has to do with radar capability but all the old time hams seem to corroborate this. reply mikeortman 13 hours agoprevThis speaks on long term outlooks at synoptic scale, we really should put some energy on researching mesoscale long term outlooks, or even 4 hour short term. It's a difficult problem to solve as the variables are quite complex, but the reward can be substantial -- on-land severe weather impacts less people but often is deadlier and can cause huge financial loss in areas that may not expect it. reply toolslive 14 hours agoprevBut it's still a chaotic system and Lyapunov would claim we're quite vain to even try. reply callalex 15 hours agoprevDark Sky brought a lot of this powerful forecasting to a hyper-local level. It’s such a shame that Apple bought it up and just…threw it all away. What a waste. reply counters 15 hours agoparentDark Sky didn't have \"powerful forecasting.\" They literally just had a simple computer vision app which used optical flow to track blobs and weather radar, and then they extrapolated those blobs forward. reply brewdad 14 hours agorootparentIt was a tool that was either very accurate or inaccurate depending on your perspective. If DS said rain would be starting in 8 minutes, it almost always rained at my house very soon thereafter. Very accurate. However, sometimes that rain came 4 minutes later or perhaps 12 minutes later. Now the forecast was off by a factor of 50%. Could be no big deal or a thing that ruins your morning depending on whether you got caught out in it and expected to by dry or not. reply counters 11 hours agorootparentWell, tracking a rain blob on radar that is 8 minutes from your house is an extraordinarily linear problem, so not surprising they'd have absurdly high P/R for that forecast :) reply LeoPanthera 14 hours agoparentprevIf by \"threw it all away\" you mean \"integrated into the built-in weather app on every Apple platform\" then, sure. I guess. reply jdminhbg 12 hours agorootparentApple even opened it up as an API that's cheaper than lots of the others! I don't know where people get this idea that DS died, it's like they just took what happens to lots of other startup acquisitions and extrapolate it like it's a blob of precipitation moving towards their current location. reply okdood64 17 hours agoprevAnecdotally, they have been very frustrating in the Bay Area with this El Niño rain season. Not as reliable. Many ruined plans, but I have learnt my lesson. reply ufocia 12 hours agoprevA more catchy title would've been \"Weather has become more predictable\". reply bcardarella 17 hours agoprevNot in Boston, they've become far far worse. I presume because of climate change but still has become frustrating in recent years. reply jghn 13 hours agoparent> they've become far far worse Where are your data to back this claim up? And over what time horizon? reply Zamiel_Snawley 17 hours agoparentprevWhich forecasts have become worse in Boston? 24 hour, 5 day, all of the above? This is surprising to me, because even when traveling across the USA, I've found the predictions to be very useful. reply bcardarella 14 hours agorootparentAll forecasts. I race sailboat around Boston and they've been absolutely horrible. Not just for extreme weather events but even regular weather wind direction is off by nearly 180 degrees in direction regularly, wind strength is regularly wrong too. The predictions overemphasize rain events in the 10 day forecast during the Summer that nearly always completely go away and they fail to predict rain and lightning events. That's overing multiple weather models at various resolutions. reply ejb999 17 hours agoparentprevwhy would you think that short term weather forecasts are somehow affected by climate change? reply Y-bar 17 hours agorootparentI don't think OP i correct that the forecasts there are less reliable, but I would gather that: More warming == more energy in the system. More energy in the system -> more volatile weather. More volatile weather -> harder to predict weather. reply bcardarella 14 hours agorootparentprevClimate models are based upon historical data. Recent climate change has changed weather patterns where historical data being used is making predictions less reliable. reply counters 7 hours agorootparentNo, climate models are the same sorts of physics-based simulations as weather models. reply hackerlight 4 hours agorootparentIsn't GraphCast now state of the art for weather forecasting? It outperforms physics-based simulations over the relevant time horizon. reply hiddencost 6 hours agorootparentprevlol reply dingnuts 17 hours agorootparentprevYou gotta keep up with the narrative bud, it's not \"weather isn't climate\" anymore; as long as the weather seems unusual to adults, it's evidence of climate change. But don't forget! That doesn't mean nice weather is counter-evidence of climate change. Nice weather is /also/ evidence of climate change, because it's merely the lull before the weird weather. Got it yet? reply bcardarella 14 hours agorootparentOk smoothbrain reply bilsbie 13 hours agoprevI can’t believe they killed dark sky. That was accurate to the minute. Incredible. reply glitchinc 15 hours agoprevI could not disagree more. I paid far less attention to weather forecasts 30 years ago than I do now, but I have numerous anecdotal examples of how weather forecasting models and information provided by publicly available weather services have trended towards uselessness. There is no publicly accessible weather information service that can accurately forecast weather at my house. One of the first purchases I made when I moved in to the house was an Ambient Weather Station resulting from pure curiosity that has evolved into an interest in keeping a historical record of \"actual weather\". Daily hi/low temperatures generally have positive correlation with forecasted temperatures, but the spread between forecasted temperatures and actual temperatures is generally ten degrees less than forecasted. Long term qualitative temperature trends (\"above average for the winter\" and similar) are positively correlated. But ... - Forecasted storm intensities are wildly inaccurate. Forecasted high-intensity rain storms end up being all-day drizzle events or on and off rain showers, and visa versa. A forecast of “a passing afternoon shower” ends up being an all-day wash-out. - Precipitation forecasts are wildly inaccurate, without correlation. Actual precipitation can be far less than forecasted or far more than forecasted, even when compared to short term forecasts--to include same day and intrahour forecasts. Just this past weekend we had accumulating whiteout snow squalls on an off all day long on Sunday, yet there was never any mention of any possibility of snow by any local meteorologists or by any weather forecasting service I routinely check. Dark Sky was the best app I ever used for weather forecasting. Its short and long term forecasts were more than sufficient for planning purposes, but where the app to this day has had no equal was in its intrahour local forecasts and precipitation forecasts. If Dark Sky alerted me that there was going to be tornado in my area within the next 15 minutes, I saw a funnel cloud 15 minutes later. If Dark Sky alerted me that it was going to stop snowing in 15 minutes, the snow stopped 15 minutes later. Sadly, Apple lobotomized the service when they claimed to have integrated Dark Sky functionality in to Apple Weather. Even though I fairly regularly report weather accuracy issues to Apple via the Weather app, the reporting and forecasting provided by Apple Weather has never improved. - Seasonal precipitation forecasts are wildly inaccurate without correlation. Modeling (from NOAA, local meteorologists, etc.) suggested we were to have \"above average snowfall\" this winter, with the official average winter snowfall being 48 inches. We have received 20 inches so far this winter. Either winter will go out with a bang in the next few weeks (which would be nice, IMO), or modeling will have predicted more than 140% of the actual snowfall. This is an altogether unfair comparison, but why not: if the executives of a publicly traded company forecasted 140% more revenue to shareholders than the company they preside over realized, they would all be immediately fired, sued, jailed, etc. If society collectively will not tolerate 140% inaccuracy in financial matters (stock price manipulation, value destruction, and so forth), should we be content with weather forecasting and modeling that is just as inaccurate? After all, weather is treated as (only) a financial matter by insurance companies. On an individual level, viewing weather's impact through financial optics still makes sense--from lost days of work and lost wages, to insurance premiums, to food prices, to transportation costs, to taxes, to paying for the ability to get your money back for a concert ticket you bought months ago if the weather is too bad. Climate change is certainly wreaking havoc on weather modeling, but it has been doing so for a significant period of time and the models do not appear (to me) to be getting better at adequately accounting for the effects of climate change. If current weather forecasting models cannot be adapted to accurately account for the effects of climate change, it may be time to either fundamentally change the way weather modeling and forecasting is done, or not do it at all. Taking out my broad brush and bucket of paint: are there any companies relying on AI to develop a more accurate weather forecasting service? And if anyone has a weather service to recommend that will not “Night at the Roxbury” me with ads and that has accurate 3-day-or-less weather forecasts, I am all ears. Please post them here. reply counters 14 hours agoparentClimate change has no impact on weather modeling. The vast majority of weather forecasts derive from physically-based simulations of the atmosphere; the physics of the atmosphere don't suddenly change because the climate is warming. However, we rely equally heavily on statistically post-processing these physically-based simulations to correct systematic biases and better contextualize their outputs. Drift in the distribution of weather conditions - even small - can contaminate some of these types of applications. But not really in a way that you can honestly claim \"climate change is making weather forecasts less accurate.\" > are there any companies relying on AI to develop a more accurate weather forecasting service? Sure there are. But AI isn't a silver bullet, and existing weather forecasting technologies are _really freaking good_. For all of the hullabaloo over AI-NWP systems like Google's GraphCast and Huawei's PanguWeather, these state-of-the-art systems are about _on par_ with the best-in-class existing numerical weather models; they offer incremental improvements in tuned forecast accuracy, but these improvements are statistical descriptions of a very, very large number of forecasts - end users really wouldn't see any practical difference in forecast quality if they relied on these forecasts. But to my point above - even AI-NWP outputs would be filtered through statistical post-processing to boost their accuracy/utility. There are a lot of companies that _claim_ they use AI at different parts of the weather value chain to improve forecasts. A lot of them stretch the truth as to what extent they really use AI or ML. The simple reality is that the weather community has used ML since the 1970's to improve weather forecasts. reply timetraveller26 16 hours agoprevokay, nice, but what about software delivery forecasts? reply JoeAltmaier 12 hours agoprev [–] Twelve year old won the San Jose Mercury News weather prediction contest one year, by predicting each day that the weather would be the same as the weather the day before. Consumer weather prediction isn't about being right. It's about pleasing the customer by appearing to be helpful. Which often means exaggerating the chances of abnormal weather, so if it happens you can be a hero. Real prediction is boring. reply hazbot 12 hours agoparentDepends, the \"optimal\" forecast can be very sensitive to the scoring metrics used. E.g. Darwin in Australia's tropics - persistence forecasting (as you describe above, just predicting the weather the day before) does very well on a metric like 'mean absolute error'. But has no practical skill at forecasting a severe tropical cyclone (aka hurricane/typhoon)! Many are willing to accept some level of false positives and a higher mean absolute error, because the cost of a surprise cyclone is so devestating. reply ImaCake 12 hours agoparentprevThis would work in San Jose because it’s a hot Mediterranean climate. Such climates have very predictable hot dry summers and cool wet winters. In Perth, similar climate, we often go month’s without rain in summer but will have several consecutive days of rain in winter. I imagine using the previous day would have a much lower skill score in more variable climes. reply TheCowboy 10 hours agoparentprev [–] Was he competing against other humans vs weather models? I'd hesitate to draw a conclusion from this anecdote. I also couldn't find a link to this, and if you have one I'm interested in reading more. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Technology advancements have significantly enhanced the accuracy of weather forecasts, making a four-day forecast today as reliable as a one-day forecast three decades ago.",
      "Access to precise weather forecasts is vital for saving lives and safeguarding livelihoods by providing early alerts for storms, heatwaves, and disasters.",
      "Global disparities persist in accessing accurate weather predictions, especially in low-income nations, highlighting the need to invest in communication, early warning systems, and cutting-edge technologies like Artificial Intelligence (AI) for climate change adaptation and resilience against extreme weather conditions."
    ],
    "commentSummary": [
      "The discussion delves into the variability of weather forecast accuracy from different sources, emphasizing the significance of local meteorologists and raw data for precise forecasts.",
      "Limitations of Siri and Apple Weather in predicting rain accurately are debated, alongside advancements in weather forecasting tech like radar tracking for short-term predictions.",
      "Climate change's impact on weather forecasting, concerns over Apple's acquisition of Dark Sky, and the importance of accurate forecasts, especially in low-income countries, are key points of the discussion."
    ],
    "points": 286,
    "commentCount": 159,
    "retryCount": 0,
    "time": 1710247355
  },
  {
    "id": 39687132,
    "title": "Nvtop: GPU Task Monitor for Multiple Vendors on Linux",
    "originLink": "https://github.com/Syllo/nvtop",
    "originBody": "NVTOP What is NVTOP? NVTOP stands for Neat Videocard TOP, a (h)top like task monitor for GPUs and accelerators. It can handle multiple GPUs and print information about them in a htop-familiar way. Currently supported vendors are AMD (Linux amdgpu driver), Apple (limited M1 & M2 support), Huawei (Ascend), Intel (Linux i915 driver), NVIDIA (Linux proprietary divers), Qualcomm Adreno (Linux MSM driver). Because a picture is worth a thousand words: Table of Contents NVTOP Options and Interactive Commands Interactive Setup Window Saving Preferences NVTOP Manual and Command line Options GPU Support AMD Intel NVIDIA Adreno Apple Ascend (only tested on 910B) Build Distribution Specific Installation Process Ubuntu / Debian Ubuntu Impish (21.10) / Debian buster (stable) and more recent (stable) Fedora / Red Hat / CentOS OpenSUSE Arch Linux AppImage Snap Docker NVTOP Build Troubleshoot License NVTOP Options and Interactive Commands Interactive Setup Window NVTOP has a builtin setup utility that provides a way to specialize the interface to your needs. Simply press F2 and select the options that are the best for you. Saving Preferences You can save the preferences set in the setup window by pressing F12. The preferences will be loaded the next time you run nvtop. NVTOP Manual and Command line Options NVTOP comes with a manpage! man nvtop For quick command line arguments help nvtop -h nvtop --help GPU Support AMD NVTOP supports AMD GPUs using the amdgpu driver through the exposed DRM and sysfs interface. AMD introduced the fdinfo interface in kernel 5.14 (browse kernel source). Hence, you will need a kernel with a version greater or equal to 5.14 to see the processes using AMD GPUs. Support for recent GPUs are regularly mainlined into the linux kernel, so please use a recent-enough kernel for your GPU. Intel NVTOP supports Intel GPUs using the i915 linux driver. Intel introduced the fdinfo interface in kernel 5.19 (browse kernel source). Hence, you will need a kernel with a version greater or equal to 5.19 to see the processes using Intel GPUs. INTEL SUPPORT STATUS Intel is working on exposing more hardware information through an HWMON interface. The patches are still a work in progress: see patch series. The fdinfo interface does not expose the memory allocated by the process. The field in the process list is therefore empty. NVIDIA The NVML library does not support some of the queries for GPUs coming before the Kepler microarchitecture. Anything starting at GeForce 600, GeForce 800M and successor should work fine. For more information about supported GPUs please take a look at the NVML documentation. Adreno NVTOP supports Adreno GPUs using the msm linux driver. msm introduced the fdinfo interface in kernel 6.0 (browse kernel source). Hence, you will need a kernel with a version greater or equal to 6.0 to see the processes using Adreno GPUs. Apple NVTOP includes some initial support for Apple using Metal. This is only supported when building for Apple, and when building for Apple only this vendor is supported. APPLE SUPPORT STATUS Apple support is still being worked on. Some bugs and limitations may apply. Ascend NVTOP supports Ascend (testing on Altas 800 (910B)) by DCMI API (version 6.0.0). Currently, the DCMI only supports limited APIs, missing PCIe generation, tx/rx throughput info, max power draw etc. Build Several libraries are required in order for NVTOP to display GPU info: The ncurses library driving the user interface. This makes the screen look beautiful. For NVIDIA: the NVIDIA Management Library (NVML) which comes with the GPU driver. This queries the GPU for info. For AMD: the libdrm library used to query AMD GPUs through the kernel driver. Distribution Specific Installation Process Ubuntu / Debian If your distribution provides the snap utility, follow the snap installation process to obtain an up-to-date version of nvtop. A standalone application is available as AppImage. Ubuntu Impish (21.10), Debian buster (stable) and more recent sudo apt install nvtop Ubuntu PPA A PPA supporting Ubuntu 20.04, 22.04 and newer is provided by Martin Wimpress that offers an up-to-date version of nvtop, enabled for NVIDIA, AMD and Intel. sudo add-apt-repository ppa:flexiondotorg/nvtop sudo apt install nvtop Older AMD and Intel Dependencies sudo apt install libdrm-dev libsystemd-dev # Ubuntu 18.04 sudo apt install libudev-dev NVIDIA Depenency NVIDIA drivers (see Ubuntu Wiki or Ubuntu PPA or Debian Wiki) NVTOP Dependencies CMake, ncurses and Git sudo apt install cmake libncurses5-dev libncursesw5-dev git NVTOP Follow the NVTOP Build Fedora / Red Hat / CentOS A standalone application is available as AppImage. Fedora 36 and newer sudo dnf install nvtop Red Hat Enterprise Linux 8 and 9 sudo dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-$(rpm -E %{rhel}).noarch.rpm sudo dnf install nvtop CentOS Stream, Rocky Linux, AlmaLinux sudo dnf install -y epel-release sudo dnf install nvtop Build process for Fedora / Red Hat / CentOS: AMD and Intel Dependencies sudo dnf install libdrm-devel systemd-devel NVIDIA Depenency NVIDIA drivers, CUDA required for nvml libraries (see RPM Fusion) NVTOP Dependencies CMake, ncurses, C++ and Git sudo dnf install cmake ncurses-devel git gcc-c++ NVTOP Follow the NVTOP Build OpenSUSE A standalone application is available as an AppImage. Build process for OpenSUSE: AMD Dependecy sudo zypper install libdrm-devel NVIDIA Depenency NVIDIA drivers (see SUSE Support Database) NVTOP Dependencies CMake, ncurses and Git sudo zypper install cmake ncurses-devel git NVTOP Follow the NVTOP Build Arch Linux sudo pacman -S nvtop Gentoo sudo layman -a guru && sudo emerge -av nvtop AppImage An AppImage is a standalone application. Just download the AppImage, make it executable and run it! Go to the release page and download nvtop-x86_64.AppImage # Go to the download location ** The path may differ on your system ** cd $HOME/Downloads # Make the AppImage executable chmod u+x nvtop-x86_64.AppImage # Enjoy nvtop ./nvtop-x86_64.AppImage If you are curious how that works, please visit the AppImage website. Snap snap install nvtop # Add the capability to kill processes inside nvtop snap connect nvtop:process-control # Add the capability to inspect GPU information (fan, PCIe, power, etc) snap connect nvtop:hardware-observe # AMDGPU process list support (read /proc/) snap connect nvtop:system-observe # Temporary workaround to get per-process GPU usage (read /proc//fdinfo) snap connect nvtop:kubernetes-support Notice: The connect commands allow Docker NVIDIA drivers (same as above) nvidia-docker (See Container Toolkit Installation Guide) git clone https://github.com/Syllo/nvtop.git && cd nvtop sudo docker build --tag nvtop . sudo docker run -it --rm --runtime=nvidia --gpus=all --pid=host nvtop NVTOP Build git clone https://github.com/Syllo/nvtop.git mkdir -p nvtop/build && cd nvtop/build cmake .. -DNVIDIA_SUPPORT=ON -DAMDGPU_SUPPORT=ON -DINTEL_SUPPORT=ON make # Install globally on the system sudo make install # Alternatively, install without privileges at a location of your choosing # make DESTDIR=\"/your/install/path\" install If you use conda as environment manager and encounter an error while building NVTOP, try conda deactivate before invoking cmake. The build system supports multiple build types (e.g. -DCMAKE_BUILD_TYPE=RelWithDebInfo): Release: Binary without debug info RelWithDebInfo: Binary with debug info Debug: Compile with warning flags and address/undefined sanitizers enabled (for development purposes) Troubleshoot The plot looks bad: Verify that you installed the wide character version of the ncurses library (libncursesw5-dev for Debian / Ubuntu), clean the build directory and restart the build process. Putty: Tell putty not to lie about its capabilities ($TERM) by setting the field Terminal-type string to putty in the menu Connection > Data > Terminal Details. License NVTOP is licensed under the GPLv3 license or any later version. You will find a copy of the license inside the COPYING file of the repository or at the GNU website .",
    "commentLink": "https://news.ycombinator.com/item?id=39687132",
    "commentBody": "Nvtop: Linux Task Monitor for Nvidia, AMD and Intel GPUs (github.com/syllo)265 points by alexzeitler 8 hours agohidepastfavorite42 comments distalx 2 hours agoThere is also Nvitop[1], which is more to my liking. [1] https://github.com/XuehaiPan/nvitop reply cassianoleal 3 minutes agoparentSeems to be for NVidia only, whereas OP claims: > Currently supported vendors are AMD (Linux amdgpu driver), Apple (limited M1 & M2 support), Huawei (Ascend), Intel (Linux i915 driver), NVIDIA (Linux proprietary divers), Qualcomm Adreno (Linux MSM driver). reply kiraaa 2 hours agoparentprevsuper easy to install and use reply formalsystem 7 hours agoprevnvtop or nvidia-smi gives you a good macro overview but I personally have found that utilization (EDIT: As reported by nvidia-smi) is actually a poor proxy for how fast your workload can be outside of just ensuring that a GPU is indeed being used If you're here because you're interested in AI performance I'd recommend instead https://docs.nvidia.com/nsight-compute/NsightComputeCli/inde... to profile individual kernels. Nsight systems for a macro view https://developer.nvidia.com/nsight-systems and the PyTorch profiler if you're not authoring kernels directly but using something PyTorch https://pytorch.org/tutorials/recipes/recipes/profiler_recip... reply pama 6 hours agoparentI agree that utilization by nvidia-smi is a poor proxy for performance. FWIW, I’ve found that for the same architecture the power consumption reported in nvtop very often correlates super nicely with the training performance and the peak performance is always at peak power consumption. Agreed on your advice for getting to tune your architecture details, but once that’s fixed and you have simple things to debug like memory usage, batch size, dataloading bottlenecks the raw power metric is typically a quick proxy. I find the temperature is a second useful macro metric that; you want to be at max power draw and max allowed temp at all times but not exceed the temperature where you throttle. reply refibrillator 7 hours agoparentprevFLOPs utilization is arguably the industry standard metric for efficiency right now and it should be a good first approximation of how much performance is left on the table. But if you mean the reported utilization in nvtop is misleading I completely agree (as someone who uses it daily). I’ve been meaning to dig into the source/docs to see what’s going on. The power usage seems to be a more reliable indicator of actual hardware utilization, at least on nvidia gear. reply ipsum2 6 hours agoparentprevI've been going off of power draw in nvidia-smi as a proxy of util, doesn't require additional setup or code changes. reply KeplerBoy 4 hours agorootparentThat's hard to argue with. Of course power draw is a direct measure of hardware utilization, but it doesn't translate very well to a measure of GPU Code efficiency. Often you can squeeze out another order of magnitude of performance by rewriting the kernel and the power draw will always stay capped at whatever the maximum is. I'd say GPU power consumption is interesting if you're CPU bound and struggling to feed the GPU enough data and/or tasks. reply samstave 5 hours agoparentprevIf you install Docker Desktop with WSL2 checked, it automatically lets you run Nvidia-SMI in your WSL ubuntu environ on Windows: https://i.imgur.com/C24EV5U.png then sudo apt install nvtop https://i.imgur.com/SOoCdvR.png EDIT: Thanks, Some people were having random problems installing WSL on their systems and I found this was the easiest solution (but based on their card models, they appeared to have much older machines. reply acka 5 hours agorootparentThere is no need to install Docker Desktop just to run nvidia-smi in WSL; the Windows directory containing the nvidia-smi binary is mounted inside a WSL instance and added to PATH automatically by WSL on instance startup. As an aside: there is no need to install Docker Desktop just to use Docker containers in WSL either, unless you want a Windows GUI to manage your containers. Just follow the official documentation for installing Docker in your Linux distro of choice, or simply run `sudo apt install docker.io` in the default WSL Ubuntu distro. Docker will work just fine with an up-to-date WSL. reply 8A51C 12 minutes agorootparentFurther aside, it's possible to have both Docker Desktop and the normal linux Docker.io installed on WSL. They work in isolation, the easy way to know which is active is to check if Docker Desktop is running or not. I wouldn't recommend this set up... reply vilunov 16 minutes agoprevIt doesn't work with mesa, does it? I'm using the new nvk driver and it shows me no GPU to monitor. reply hughesjj 7 hours agoprevNvtop+bottom are my favorite resource monitors for Linux today, but TIL nvtop also works for non-nvidia devices reply weinzierl 11 minutes agoparentI use htop but often want to focus on a process, its children and all their threads. Is there a htop replacement that can; - Show a thread with all children an threads, but nothing else - Show the whole tree but keep the selected (Shift + Space) process in a fixed screen position. - Bubbles rows up and down into their new positions instead of having them jump around all over the place. reply freedomben 6 hours agoparentprevwow, bottom[1] is awesome! This is now my favorite monitor It's not in the Fedora repos but there's a COPR for it. To install: sudo dnf copr enable atim/bottom sudo dnf install bottom [1] https://github.com/ClementTsang/bottom reply petepete 5 hours agorootparentbtop++ is also worth a look if you like bottom, another take on a modern htop. https://github.com/aristocratos/btop reply blagie 1 hour agoprevNow that I use Home Assistant, I want all my data sources to plug into there. It can handle the rendering for me as I see fit, and it's where data comes to integrate. It's one of those things which I wish existed, but I can't imagine anyone would have written. Until I do a web search. https://github.com/koriwi/sensors2mqtt/tree/main I have not used it yet, but that seems like how I'd want to do it. reply stuaxo 1 hour agoprevIts good to see linux graphics card utilities going multi platform, instead of the old way of being per-driver. reply Ycros 1 hour agoprevI prefer btop, it does all the usual process monitoring as well as gpus in the latest versions. reply notorandit 57 minutes agoparentReally? Mine is v1.3.2 and doesn't show Intel Iris Xe Graphics! {UPDATE} I see: no Intel GPU support yet! reply notorandit 3 hours agoprevIt is nice. But the status of GPU support in Linux is rather poor. I can see it working only with VLC. Firefox has some support while chromium based browsers have that only formally. In the real world you never see the video hw acceleration kicking in, neither with webrtc nor with videos. It is a pity. reply KeplerBoy 3 hours agoparentit's a valuable tool wherever GPU compute is used. Not sure if video decoding would even register as utilization since that is dedicated hardware. reply notorandit 1 hour agorootparentYes, indeed. But 90% of my very own GPU usage is for WebRTC based applications and web-based videos. Which leads to what I said: no hw-assisted encoding/decoding actually available. I use intel_gpu_top (from intel-gpu-tools) to monitor my GPU usage: only VLC shows usage. Anyone has success with browsers? reply usr1106 2 hours agorootparentprevYes, you can see video decoding, but it's indeed only one detail of the data presented, as it's only a part of the HW. Not at my computer now, can't tell what metric to watch. reply notorandit 59 minutes agorootparentWhich software would use video decoding? reply KeplerBoy 2 hours agorootparentprevah yes, you can configure nvtop to display encode and decode loads. Interesting to watch how playback speed in VLC is directly reflected in the metric. reply pjmlp 58 minutes agoparentprevCan confirm with my devices. reply pavelstoev 5 hours agoprevyou can also profile AI/ML performance without actually running it https://github.com/CentML/DeepView.Profile reply malux85 6 hours agoprevThere is also nvitop, which I find better utilizes screen space when > 2 GPUS https://github.com/XuehaiPan/nvitop reply sa-code 2 hours agoparentThe downside with nvitop is that it's written in python, which means having it in your environment can cause dependency conflicts. It's either that or you have a separate venv just for it. Maybe it's fine for personal use but sysadmins would prefer nvtop reply cl3misch 1 hour agorootparentThat's why the authors recommend pipx for installing nvitop. I am not a sysadmin, but I prefer pipx over relying on the (often outdated) distro sources. https://github.com/XuehaiPan/nvitop?tab=readme-ov-file#insta... reply freedomben 5 hours agoparentprevDoes nvitop support AMD cards? reply malux85 3 hours agorootparentNot sure sorry, I only have nvidia :< reply cyberax 6 hours agoprevIs there a description of the wire protocol between the driver and NVidia library? Context: I'd love to have a native Go-language library to read the GPU utilization for containerized workloads. reply 8organicbits 30 minutes agoparentYes, check out NVML. There's wrappers for a couple languages, not sure about golang. https://developer.nvidia.com/nvidia-management-library-nvml reply superkuh 5 hours agoprevradeontop is the same sort of thing if you live in amdgpu-ville and want something easy to compile. I was able to use it to show that with kernel 5.x admgpu vulkan when a process is pushed out of vram into gtt it'll never reload and get stuck in a 'slow' state. reply brcmthrowaway 6 hours agoprevAnything for MACOS? reply kernelsanderz 4 hours agoparentThere’s also asitop https://github.com/tlkh/asitop reply evilduck 5 hours agoparentprevnvtop supposedly builds for macOS, but https://lib.rs/crates/pumas is similar. reply itsgrimetime 4 hours agoparentprevits not a terminal app like bottom or nvtop but I use https://github.com/exelban/stats and it has iGPU stats reply shmerl 4 hours agoprevMay be you should use Nova instead of NVML for Nvidia? https://lists.freedesktop.org/archives/nouveau/2024-February... Other than that - a cool tool! reply collsni 7 hours agoprev [–] Amdgpu_top is another cool usage statistic/monitor for and GPUs reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "NVTOP is a task monitor for GPUs and accelerators, akin to htop, supporting various vendors like AMD, Intel, NVIDIA, Apple, Qualcomm Adreno, and Huawei Ascend.",
      "It necessitates specific GPU libraries, offers interactive features for setup, preference saving, and user-friendly GPU info display, and is compatible with distributions such as Ubuntu, Fedora, Red Hat, CentOS, OpenSUSE, and Arch Linux.",
      "NVTOP, under the GPLv3 license, can be compiled from source with NVIDIA, AMD, and Intel GPU backing."
    ],
    "commentSummary": [
      "The Github thread delves into comparing Linux task monitoring tools for Nvidia, AMD, and Intel GPUs like Nvtop, Nvitop, and others.",
      "Discussions revolve around GPU utilization accuracy, power consumption's performance indicator, and mention alternative tools btop and asitop.",
      "Users inquire about Linux GPU support, video decoding, MacOS tool availability, alongside suggestions for AI/ML performance profiling and diverse GPU data reading libraries."
    ],
    "points": 265,
    "commentCount": 42,
    "retryCount": 0,
    "time": 1710293774
  },
  {
    "id": 39683455,
    "title": "Exploring Floating-Point Precision's Impact on Mandelbrot Set Images",
    "originLink": "https://github.com/ProfJski/FloatCompMandelbrot",
    "originBody": "FloatCompMandelbrot A program to explore how Mandelbrot set images are affected by floating point precision. Dependencies: Boost::Multiprecision (How to Install Boost ) and RayLib for graphics. Boost::Multiprecision may be easily replaced by other floating-point or rational implementations, like GiMP or GMPF by just changing the #includes and typedefs at the beginning. The Problem In A Nutshell How much is a Mandelbrot set image affected by the floating point precision of the code used to compute it? The points in a region of a Mandelbrot set image have sensitive dependence upon initial conditions. (This property is the \"chaos\" in \"chaos theory.\") A small deviation in the value of C in Z=Z^2+C can lead to a very different outcome for one point versus another point that is only a miniscule distance from it. This fact remains true at all scales of magnification, i.e., \"miniscule\" can be arbitrarily small. Floating point math cannot perfectly represent many rational numbers. Moreover, floating point math can generate inaccuracies that can snowball with repeated calculations, including repeated operations of multiplication and addition. Repeated multiplication and addition are the core of the Mandelbrot set algorithm. Since the Mandelbrot set is always generated on a computer, typically using floating point math of some precision or other, we ask: How much of a Mandelbrot set image's visual complexity is due to the mathematical properties of the algorithm, and how much is due to floating point inaccuracies in its computer calculation? Could some of the familiar visual properties of the Mandelbrot set simply be \"noise\" introduced by floating point calculations that have iterated thousands or even millions of times? The Problem at Large This question has been posed at least once on Stack Exchange but the original post was not understood, so no satisfactory answer resulted. A 2013 academic paper, \"Images of Julia Sets That You Can Trust\" , by L. H. de Figueiredo, D. Nehab, J. Stolfi, and J. B. Oliveira, raises similar questions for Julia sets, which are mathematical relatives of the Mandelbrot set. Background If you are unfamiliar with the limitations of floating point arithmetic, read the classic essay \"What Every Computer Scientist Should Know About Floating-Point Arithmetic\" by David Goldberg for a good overview. Basic introductions to the math involved in generating the Mandelbrot set are ubiquitous if you are unfamiliar with the algorithm. The Problem and Solution in More Detail For a more detailed statement of the problem and the design of this program, read the more detailed analysis here. My Solution in a Nutshell I wrote easy-to-modify c++ code that simply uses typedef to allow the user to explore the Mandelbrot set and to compare how the Mandelbrot set image looks when generated using the same parameters on the same points, but using three different floating-point implementations to calculate the result. In addition to standard float and double types, the Boost::Multiprecision cpp_bin_float class was used to implement extended-precision floating-point types of the user's choice. The program is multi-threaded so it generates the desired Mandelbrot set image in all three float types simultaneously. Keys , , anddisplay the image as generated by the three different types of floating point precision that the user has selected. Keys , , andsubtract pairs of images to highlight any pixels that are different: Key-- Subtracts image 1 from 2 Key-- Subtracts image 1 from 3 Key-- Subtracts image 2 from 3 Green pixels indicate a positive value following the subtraction, red pixels a negative value. Basic statistics of the comparison are given in console output, such as the total number of pixels that are different, the percentage of the total image that is different, and the greatest difference between the values of the pixels (which tends to be close to the max iterations that one has selected for the Mandelbrot algorithm). The console output also prints basic information about the three types one has chosen at start-up, such as digits of precision. An Example Here are some sample images from a zoom in the neighborhood of C = -0.104943 + 0.927572i. Magnification is approximately 33E+6. The distance between adjacent pixels is 1/(2^33). Max iterations is 30K. Here's the image generated with standard double (53 bits of precision): Here is the same image generated with cpp_bin_float_quad (113 bits of precision). Superficially it looks largely the same: But upon subtraction, we find that 8.77% ofthe pixels are different! 92,007 out of 1,048,576 pixels have different values. These differences include points estimated to be in the Mandelbrot Set in one image and not in the other, as well as points which exceeded the orbit boundary check (i.e., the magnitude of Z > 2) in both images but at different iterations. Here's what we get when we subtract the Quad image from the Double precision image and color-code the difference: Green pixels are where Quad > Double; red pixels are where Double > Quad. Four Screen View Press Keywith the mouse over a pixel in screen views 1, 2 or 3 to trigger Four-Screen view. Top left and right are float types 1 and 2. Bottom left is the difference between them. The bottom right screen displays the trajectory of the point against the background of the whole Mandelbrot set. The first float type is plotted in blue, the second in red, the third in yellow. This allows you to see how different floating point implementations lead to different trajectories, which lead to different final values for a single point in the image. Trajectory view Press Keyto see the trajectory screen by itself. In this display mode, the background remains that of the zoomed-in image, rather than the whole Mandelbrot set, but the screen coordinates of the trajectories are still (-2-2i) to (2+2i). Leaving the background image of the zoomed-in fractal better allows you to select a point where trajectories clearly diverge, which are often near the edges of the set or other high-chaos areas. Lines to Customize in the Code The three floating point types are typedefed as first_float, second_float and third_float throughout the code. Variable names also have these names as suffixes. Yes, templated code would be more elegant than coding everything in triplicate! I began with templated code, but finding the proper arguments to pass to std::asynch for templated overloaded functions proved too hard. Places to customize your settings are preceded by the comment tag // OPTION, for which string you can search. The basic options are below. The first setting is the most important: selecting your floating point types. Set your types Alter these three lines in the code to choose your own floating point implementations and voila! You are all set to go. It will be helpful to order your types from the least precise for first_float to the most precise for third_float. typedef double first_float; typedef cpp_bin_float_quad second_float; typedef cpp_bin_float_oct third_float; Tweak your multi-threading preferences (optional) Depending on how many cores you have you may wish to change unsigned int numThreads; This setting creates numThreads for each of the three floating point types. So unsigned int numThreads=4; will start 12 threads initially. Built-in types (like float, double, long double) go much faster on most platforms because of intrinsic processor support. So threads dedicated to those types will finish far sooner than those using Boost::multiprecision's cpp_bin_float types. Software-emulated floating-point support is always slower. That's why they invented FPUs. Each thread calculates a batch of pixels at a time. You can tweak the amount per batch here: unsigned int batchSize_first_float=10240; unsigned int batchSize_second_float=2048; unsigned int batchSize_third_float=512; Choosing a built-in type like double for the first_float type, and asigning it a large number of pixels per batch (like 10K above), can be useful to display an image quickly on screen 1 to help perfect your center and zoom. Tweak how your starting point is calculated (optional) Every pixel on the screen has X and Y coordinates (ranging 0-1023) which correspond to some point on the complex plane and constitute the value of C in the formula Z=Z^2+C. Presuming that your floating point types range from least precise (for first_float) to most precise, as you zoom deeper, the calculation of C will begin to be affected by the precision of your floating point types, beginning with first_float losing precision in its last decimal places. You can choose whether to start iterating with a value for C that is calculated separately for each type, in its own precision, which may lead to slightly different values for C. Or you can start iterating with the exact same value for C for all types by simply casting the value for C in first_float into the higher-precision types. The latter approach can highlight differences in value due solely to floating-point inaccuracy accumulated through iteration. Basic program controls The mouse left-click is used to recenter the image; keys are used for everything else. Recentering and zooming takes a few seconds for the screen to reset. The GUI interface is functional but not as pretty or convenient as professional programs. This tool is made for research. Get Ultrafractal or Kalles Fraktaler to make beautiful images easily. My color palette is ugly as sin. Period - Zooms in by a factor of two Comma - Zooms out by a factor of two. Semi-colon - Zooms in by a factor of eight. These are the only zoom controls. M, K, O - Increase max iterations by 1, 10 or 100, respectively. Shift+O increases to the nearest whole 1000. N, J, I - Decrease max iterations by 1, 10 or 100, respectively. Shift+I decreases by 1000. P - Pause calculation toggle. This waits for existing threads to complete and then waits for unpause to spawn new ones, so its effect is not instant if you are working on some computationally intensive threads which must finish first. Keep your batch sizes small for responsiveness. U - Unpause. Doesn't toggle. R - Revert to prior coordinates for image center. Works once. C - Toggle a small semi-transparent indicator of the center of the screen. Useful for zooming. G - Toggle a semi-transparent grid overlay with circles various radii and a rectangle around the middle of the screen. Useful for centering the screen before zooming and estimating orbits. / (Forward slash) with mouse over a pixel - Gives coordinate info for that pixel in console 8 with mouse over a pixel - Plots trajectory of that point (screen mode 8) in all three types. The Problem and Solution in More Detail For a more detailed statement of the problem and the design of this program, read the more detailed analysis here.",
    "commentLink": "https://news.ycombinator.com/item?id=39683455",
    "commentBody": "How Mandelbrot set images are affected by floating point precision (github.com/profjski)204 points by todsacerdoti 14 hours agohidepastfavorite69 comments rkevingibson 10 hours agoFrom a cursory look at the code, I don't see any use of fused-multiply-add (FMA) which would likely help with precision issues in a number of places with the float version. The \"problem in more detail\"[1] readme specifically calls out computation of `x^2 - y^2` as a source for error, and that has methods that dramatically reduce error with FMAs[2]. [1] https://github.com/ProfJski/FloatCompMandelbrot/blob/master/... [2] https://pharr.org/matt/blog/2019/11/03/difference-of-floats reply fanf2 6 hours agoparentThe author needs to learn about techniques for rendering deep zooms into the Mandelbrot set. Since 2013 it has been possible to render images that are 2^-hundreds across using mostly double precision arithmetic, apart from a few anchor points calculated with multiprecision (hundreds of bits) arithmetic. The deep zoom mathematics includes techniques for introspecting the iterations to detect glitches, which need extra multiprecision anchor points to be calculated. https://mathr.co.uk/blog/2021-05-14_deep_zoom_theory_and_pra... https://dirkwhoffmann.github.io/DeepDrill/docs/Theory/Mandel... In my experience of non-deep-zoom rendering, and contrary to the authors arguments, period detection works well for speeding up renders. It appeared to be fairly safe from false positives. https://dotat.at/@/2010-11-16-interior-iteration-with-less-p... https://dotat.at/prog/mandelbrot/cyclic.png reply infogulch 31 minutes agorootparentI've always wondered how those \"Ten minute Mandelbrot zoom\" videos worked, because there's no way double would last that long at a tolerable zoom rate. The perturbation technique is interesting. Calculating just a few points with super high precision and then filling the pixels in between by adding an offset and continuing with lower precision halfway though the calculation seems plausible at a glance, but I'll have to read that more carefully later. Thanks for sharing! reply firebot 28 minutes agorootparentprevI've had some great success using posits for fractals. Unfortunately, soft(ware) posits so rather slow. But mathematically the results were great. reply teo_zero 4 hours agoparentprevBut the goal of the study is expressly to highlight precision issues. Using such techniques wouldn't help to make them surface. reply jacobolus 8 hours agoparentprevI wish someone made up a better explicit FMA syntax than std::fma(-c, d, cd) ... maybe something along the lines of ((-c * d + cd)) with special brackets or (-c) ⊠ d + cd with a special multiplication symbol. And if only we could effectively use FMAs from javascript... reply hkmaxpro 4 hours agoprevThis post prompted me to look into the theoretical question of the computability of the Mandelbrot set. And I found this excellent answer: https://math.stackexchange.com/q/4611850 reply hedora 7 hours agoprevIf you need to use 128-bit floats for something, note that your processor probably supports them natively and that there are standard C/C++ types for them. (The boost thing the article links says their quad precision type is “functionally equivalent” to standard types, but uses a different bit layout.) reply fanf2 6 hours agoparentYour processor probably doesn’t support quad precision floats in hardware unless it’s an IBM mainframe. https://en.m.wikipedia.org/wiki/Quadruple-precision_floating... reply guenthert 20 minutes agorootparentWhich prompts the question, what is quad precision floating point arithmetic needed for that IBM considered it prudent to implement it in hardware of a mainframe? Who are the customers paying for such? Naively perhaps, I would expect that financial calculations (ought to) rely on fixed-point arithmetic and afaiu simulations of physical systems get away with double precision, if one is careful. reply aragilar 44 minutes agorootparentprevPower also supports it, and https://www.raptorcs.com/ aren't mainframes. reply andrewf 6 hours agoparentprevI've read this a few places recently but I can't figure out how? https://godbolt.org/z/Mbjs73qar -- using doubles I get AVX instructions but using 128-bit floats calls some very hefty runtime support functions. reply lifthrasiir 4 hours agoparentprevStandard C/C++ types may still use a software implementation behind the scene, and this is almost always the case for std::float128_t. reply terlisimo 12 hours agoprevMandelbrot set calculation is a curious intersection of math and computer science. I went down that rabbit hole once :) Naive implementation is easy, but there are many ways to improve performance. reply tantalor 10 hours agoparentStrange thing to say since CS is a sub-field of math. reply Etheryte 10 hours agorootparentI wouldn't really say that's a universally held notion, especially in the modern world. Historically, yes, but these days it stands more on its own. In similar vein, you could say biology is a subfield of physics, but most people don't think of them that way. reply adonovan 8 hours agorootparentprevTo a librarian, Computer Science and Library Science are just different aspects of information systems, which are (a librarian need hardly tell HN) humanity's highest calling, thus justifying their shared Dewey classification of 0. reply DougBTX 1 hour agorootparentInteresting! And \"Science\" is all the way down in Class 500. https://en.wikipedia.org/wiki/List_of_Dewey_Decimal_classes reply AlienRobot 22 minutes agorootparentprevThat's so nice, it brings a tear to the eye. I hope we live up to librarians' expectations. reply hedora 7 hours agorootparentprevSoftware engineering has as much to do with sociology as it does with math. Systems software has to deal with a lot of physics and engineering stuff (speed of light, power, heat, mean time to component failure, etc, etc.) reply yunwal 6 hours agorootparentSoftware Engineering and Computer Science don't necessarily mean the same thing (even though at this point, most schools with CS programs just teach software engineering anyway). reply ks2048 9 hours agorootparentprevOf course these terms aren’t well-defined, but some (me) would say both math and CS are subfields of “formal systems” (but “formal systems” could be named CS, which would put it at the top of the hierarchy) reply anthk 8 hours agorootparentThe Joy computer language and Scheme with SICP are prime examples. Reading SICP it's the 'easy way' and the Joy docs plus trying to code even simple as a quadratic eqn solver it's a deep, hard task which requires lots of knowledge on cathegory theory. reply couchand 9 hours agorootparentprevAll fields are subfields of polymathy. reply MrYellowP 1 hour agorootparentI thought you're joking, but nope. https://en.wikipedia.org/wiki/Polymath reply ant6n 2 hours agorootparentprev…and I thought math branched off from computing science sometime in the 19th century. Before that, what was called „math“ was mostly algorithms to compute stuff. reply PaulHoule 10 hours agoprevIn Hamiltonian Chaos you draw Poincaré sections of systems like this one https://mathworld.wolfram.com/Henon-HeilesEquation.html and see these chaos zones that look like a field of random dots that probably miss a lot of the structure. The reason why we know those areas are dense with unstable periodic orbits is because they are also dense with periodic orbits. It doesn’t seem feasible to accurately track a system for long enough to believe those. reply DRAGONERO 13 hours agoprevAnother way to explore floating point representation can also be to compare rounding modes, as this can impact a lot of what is generated for floating point operations. Most systems are round to nearest even, but round to zero, round towards positive and round towards negative also exist. Bonus points if you also check results for x87 floats. reply Sesse__ 12 hours agoparentFor Mandelbrot, presumably you can do exact rational arithmetic (all it needs is multiplications and add, and you presumably start with a floating-point coordinate, which is eminently rational). It will be very slow with many iterations, of course, since the fractions will rapidly spin out of control. Edit: I see the supplementary material addresses this: “One could attempt to avoid the problem by using arbitrary-precision floating point representation, or rational representation (using two integers, for numerator and denominator, i.e., keep everything as fractions), but that quickly leads to so many significant digits to multiply in every iteration that calculations become enormously slow. For many points in a Mandelbrot Set Image, the number of significant digits in a trajectory tends to double with each iteration, leading to O(n^2) time complexity for evaluating one point with n iterations. Iterations must then be kept very low (e.g., 20-30 on a modern PC!), which means that many higher-iteration points cannot be evaluated. So accumulated round-off error is hard to avoid.” 20-30 sounds crazy low even for O(n²), though. Edit 2: This part seems wrong: “Cycle detection would work if floating point quantities could be compared exactly, but they cannot.” They certainly can. If you detect a cycle, you will never escape _given your working precision_. Floats can cycle just as well as fixed-point numbers can. But I suppose true cycles (those that would be cycles even in rationals) are extremely unlikely. reply hedora 7 hours agorootparent“One could” indeed: https://www.fractint.org/ From wikipedia: > The name is a portmanteau of fractal and integer, since the first versions of Fractint used only integer arithmetic (also known as fixed-point arithmetic), for faster rendering on computers without math coprocessors. Since then, floating-point arithmetic and arbitrary-precision arithmetic modes have been added. reply eru 24 minutes agorootparentI spent a lot of time with fractint in the late 1990s. Great memories! reply drdeca 8 hours agorootparentprevHm, what about intervals of rationals, and then approximating the intervals with a slightly wider interval in order to make the denominators smaller? I guess the intervals for the real and imaginary components might grow too quickly? if 0[...] Instead, we put pixels in a one-to-one relation to the points they represent. You can think of this as mapping some imaginary point on the pixel, such as its center or corner, onto the complex plane, and making the whole pixel represent that value. reply shaun_arundell 11 hours agoprevThis reminds me of the issues with calculating the zeros of the Riemann zeta equation - your code design quickly becomes dominated by numerical implementation considerations reply anthk 8 hours agoprevCode from \"The Computational Beauty of Nature\" managed to crash CWM under GNU/Linux+X. No issues with FVWM. WIth LibdieHard I managed to avoid the crash. Maybe some ANSI C code from the 90's doesn't like current layouts... On FP precision, sh has almost none, AWK often has up to 20 bit and 53 with gawk and the math library. Ditto with bc -l. Calc has an arbitrary decimal precision and it's really fast. https://github.com/lcn2/calc reply captaincrowbar 12 hours agoprevSome people, when they have a problem, think, “I know, I’ll use floating point.” Now they have 1.9999998 problems. reply beezle 9 hours agoparentFunny story from today while briefly on hold for a support rep. Got the standard 'we'll call you back if you want blah blah' followed by - I kid not - \"you're estimated wait time for the next representative is 0.58333333 minutes\" At the end of the call I suggested if she can log a problem for IT to include this and that they should either round up or use seconds (I suspect it was 35 seconds) reply recursive 10 hours agoparentprevSome numbers can be precisely represented using IEEE-754 floats. Two (2) is one of them. reply eru 19 minutes agorootparentYes, but not every calculation that yields 2, also yields 2 when performed in IEEE-754 floats. reply markab21 11 hours agoparentprevI bet you have been waiting years to pull that one out of your pocket. Well played sir! Nice shot man! :D reply efitz 13 hours agoprevThe document (\"What Every Computer Scientist Should Know About Floating-Point Arithmetic\") linked from the article was IMO even more interesting than the article: https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.h... reply zokier 13 hours agoparentIt's definitely an evergreen article https://news.ycombinator.com/item?id=23665529 https://xkcd.com/1053/ reply repelsteeltje 13 hours agoprev [–] Ironic that the last two letters in the tittle are truncated, \"Mandelbr\". Must be a floating point rounding error. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "FloatCompMandelbrot is a program utilizing Boost::Multiprecision and RayLib to showcase how floating-point precision influences Mandelbrot set visuals.",
      "Users can compare images from various floating-point implementations, unveiling the impact of inaccuracies on Mandelbrot set complexity.",
      "The tool, designed for research, provides detailed controls for analyzing and understanding how floating-point types affect fractal image representation."
    ],
    "commentSummary": [
      "The discussion on github.com/profjski delves into the impact of floating-point precision on Mandelbrot set images, exploring techniques like fused-multiply-add and deep zoom rendering.",
      "Topics cover period detection, posits, quad-precision floating-point arithmetic, and the fusion of math and computer science.",
      "Challenges in precise calculations for Mandelbrot Set Images due to high precision requirements and the limitations of floating-point arithmetic are outlined, along with the utility of floating-point numbers in troubleshooting and citations to resources on floating-point arithmetic, including an XKCD comic."
    ],
    "points": 204,
    "commentCount": 69,
    "retryCount": 0,
    "time": 1710270113
  },
  {
    "id": 39685272,
    "title": "Beware: Top Bitcoin Wallet App in Apple Store Revealed as Scam",
    "originLink": "https://news.ycombinator.com/item?id=39685272",
    "originBody": "Earlier today I decided to switch my Android for an iPhone. After moving all my apps I decided to make the jump and move my bitcoin from the android wallet. I searched for &#x27;bitcoin wallet&#x27; on the Apple App Store, installed the first app I saw (as far as I could tell, looks legit), transferred bitcoin, and it immediately got sent off. Turns out this app was previously reported at least 12 days ago as a scam ( https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;Bitcoin&#x2F;comments&#x2F;1b3q5wr&#x2F;fake_wallet_on_apple_app_store&#x2F; ) but its still up there, #1 search result.I get that I&#x27;ve failed to vet the app but honestly, how does a scam app become the #1 organic search result (not promoted) in the app store, topping binance, blockchain.com, and coinbase?EDIT: linking to a screen recording that includes this post and comments of no repro:Before removing the app - https:&#x2F;&#x2F;streamable.com&#x2F;q2muluAfter removing the app - https:&#x2F;&#x2F;streamable.com&#x2F;y5nhy7",
    "commentLink": "https://news.ycombinator.com/item?id=39685272",
    "commentBody": "Scammed by the top result for 'Bitcoin wallet' in Apple App Store203 points by habeanf 12 hours agohidepastfavorite121 comments Earlier today I decided to switch my Android for an iPhone. After moving all my apps I decided to make the jump and move my bitcoin from the android wallet. I searched for 'bitcoin wallet' on the Apple App Store, installed the first app I saw (as far as I could tell, looks legit), transferred bitcoin, and it immediately got sent off. Turns out this app was previously reported at least 12 days ago as a scam ( https://www.reddit.com/r/Bitcoin/comments/1b3q5wr/fake_wallet_on_apple_app_store/ ) but its still up there, #1 search result. I get that I've failed to vet the app but honestly, how does a scam app become the #1 organic search result (not promoted) in the app store, topping binance, blockchain.com, and coinbase? EDIT: linking to a screen recording that includes this post and comments of no repro: Before removing the app - https://streamable.com/q2mulu After removing the app - https://streamable.com/y5nhy7 blibble 11 hours agoI don't think I've ever searched for something on the app store and not got a scam as the first result just tried it - my bank? I get crypto.com - train company app? knockoff app that charges extra fees - my broker? CFD gambling app - official government app for paying my tax? intuit product I dare to think how many people this lures in scammy ads plastered everywhere is what I'd expect from Google products not for the Apple equivalent that commands a significant price premium reply Rodmine 8 hours agoparent> scammy ads plastered everywhere is what I'd expect from Google products This won’t fit with the manufactured popular understanding, but at the current time, Google protects you from fraud and scam better than anyone. I have been unfortunate enough to be scammed recently from a bing search result (ad). (It was a new computer and I decided to use Edge and bing was the default search). Apple, Microsoft etc. are rookies in this game. Google just has the benefit of experience and hence is much safer now than anyone can ever become in the near future. Because of this, scammers are much more likely to target other platforms… which happen to be Apple, Bing, Facebook etc. reply a1o 7 hours agorootparentThis is true. Btw, the macOS app store is also full of products that you really shouldn't use and most of software you really should use have alternate ways that you buy them going through their website and get something so that you can install. Microsoft Office, JetBrains IDEs and Adobe products all will install and update without you having to go through the macOS Appstore. reply mvolfik 2 hours agorootparentprev> Google protects you from fraud and scam better than anyone I have to disagree. Czech Youtube is currently full of scam ads with photos (and sometimes even bad deepfakes) of Czech president and other public figures, supposedly endorsing some investment product that yields you like 50% profit. I keep reporting these, and I know some people who do too, but ~80% of those reports get handled as \"we determined no violation of our rules\". reply npteljes 1 hour agorootparentprevGoogle is also kind of shit at this. To be safer, it's advised to block web ads so that the sometimes scammy first result is hidden in the search results. reply crossroadsguy 7 hours agoparentprevOn the contrary, as someone who has worked with both platforms as a developer, this is my personal opinion: - Around a lot of things software, including the Play Store, Google’s safety and security, for all its ads, tracking and shenanigans, are real, largely verifiable, discussed openly, and pretty fucking robust (not to mention, most of it are actually open). - Apple’s? Smoke and mirrors! Essentially some vague shit which often ludicrously boils down to Safest Shit Ever On an iPhone™ (and doesn’t go further than that) and never discussed or even offered a glimpse of. reply EVa5I7bHFq9mnYK 6 hours agorootparentnext [3 more] [flagged] sensanaty 4 hours agorootparentiOS exploits are worth the same as Android ones. reply EVa5I7bHFq9mnYK 27 minutes agorootparentAn iPhone user is typically from a rich country, like US or Japan, an Android user is typically from India or China. Here's 10x wealth difference. Add to this the fact the iPhone users are dumber and more prone to expoits, so of course hackers would rather focus on App Store. reply cam_l 8 hours agoparentprevHilarious. I don't think i have never seen or gotten a scam app (not including games, which I don't use) as the top result on Google Play. In fact, I just used all your searches on Google play and got - my bank - the train company app - my broker - hmrc The next 4 or 5 in each case were also legit. Maybe this is really something to be aware of if switching to apple? Certainly would not have been something I would have been expecting from apple (though I am pretty careful about vetting apps). reply milch 4 hours agorootparentI think the poster here meant an ad over the search results rather than the actual first result. The ads have a different highlight and a badge that says \"ad\" ... I just tried the same categories and all of the top results were the official app. The next few were mostly apps from the same developer as well. I don't know that I would call ads that are clearly labeled as ads a \"scam\" even if they are unwanted reply gumby 10 hours agoparentprevI am still surprised that when I search for a specific app (as you did for your bank) I still see junk ads before the real app shows up. Apple should be better than that. They seem to have the same bad incentive Google search does. What is CFD gambling? When I read “CFD” I always think of computational fluid dynamics and so “CFD gambling” sounds pretty cool to me. Obviously I do know I’m just overfitting to a TLA and I’d like to know what it actually means. reply kibwen 7 hours agorootparent> Apple should be better than that. Everyone should be better than that, but as far as Apple specifically, they're really no better than any other hypermegacorp. Apple's a company, it doesn't give a damn about you, it only gives a damn about enriching its owners. reply saintfire 7 hours agorootparentIt really does undermine the legitimacy of their claims that they should be the only source of apps, for safety reasons, of course. That reason alone is why they \"should be better\"; It would serve to legitimize their dogma. reply gumby 6 hours agorootparentYeah, I consider it short sighted. reply yunwal 5 hours agorootparentprevApple built up a really strong brand by not doing this kind of thing for a while. They get to charge twice as much money for half as much performance because they made sure their products felt nice and intuitive to use. Now they’re willing to destroy the most valuable brand in the world to make crypto money? It makes absolutely no sense, even accounting for greed. Also, I’m sure this kind of thing hurts their prospects in antitrust cases (like in the EU) reply koolba 10 hours agorootparentprevProbably “contract for difference”: https://en.wikipedia.org/wiki/Contract_for_difference reply saywhanow 8 hours agorootparentprevI regressively searched for the app advertised in the previous search. At around depth twelve, a search for “four” gave both an ad and top result of an app named “Four”. If you’re curious, Four’s description is “Shop Now, Pay Later.” reply blibble 10 hours agorootparentprevcontract for difference in this case: extremely short term highly leveraged bets reply quickslowdown 9 hours agorootparentThis guy's the least risk adverse trader on the Internet! (I know, probably not, but it's funnier this way) reply habeanf 11 hours agoparentprevIn hindsight, this is quite obvious. Coming from years of using Google Pixels I just got used to trusting the search results. I've never hit a fraudulent app when searching in the Play Store. I trusted apple that at least the top 5 results would be legit. EDIT: added the word 'top' at the end reply seanhunter 2 hours agoparentprevI don't recognize this picture at all. Even if you aren't distinguishing between ads and search results I'm not seeing scams as adverts but there may be a difference between the UK and US app stores in this respect perhaps. For me on apple UK app store: my bank - Ad: another legit bank. First result: my bank Train company - Ad: a generic legit train booking app. First result: the train company My broker - Ad: another broker. First result: my broker Official government app for paying my tax - ad: a general tax app. First result: government app. It stands to reason that they won't show an ad for the thing you're searching when it's the first organic result so I don't find this surprising. [1] I have two and tried both. Results were the same with a different legit bank as the ad each time. reply thiht 2 hours agoparentprevWhen I first got an iPhone a few years back, this is the thing that shocked me most. This is completely out of line with what I expect from Apple. I don’t have \"scams\" per se, but the first result when searching for a keyword is systematically an ad for a competitor: - Spotify -> Deezer - Uber -> Heetch - UberEats -> Deliveroo - Deliveroo -> Ubereats - My bank -> crypto.com I have no idea why Apple allows buying trademarks/full app names as an ad keyword. Perfect matches should always have the app first, not an ad. Is there something in the AppStore rules that prevent apps from buying the keyword ad for their own app? reply kylecazar 11 hours agoparentprevEspecially given they use enhanced security as an excuse so often reply legulere 7 hours agoparentprevDo you mean the search result or the ad that pops in at the first space? For me all those have good results but all contain ads for more or less unrelated apps I would also expect products with premium pricing to not contain ads. reply echoangle 11 hours agoparentprevCan you share some search terms you tried? I never had this problem, but I’m also not in the US so it might be different here reply pushcx 11 hours agorootparentPretty much every brand or app I search for finds a competitor first. Searching for \"robinhood\" turns up an unaffiliated cryptocurrency app and \"macrofactor\" turns up a competing diet app, etc. App store search has been broken for at least a few years. reply Reason077 10 hours agorootparent> \"Pretty much every brand or app I search for finds a competitor first.\" You're going to hate Amazon.com. reply echoangle 2 hours agorootparentprevDo you mean the ad at the top? For me, the first result is always an ad which is mostly not the right result, and the second entry (the first actual result) is the right app. reply concinds 10 hours agorootparentprevDo you mean ads or organic results? Apple should remove App Store search ads altogether (I'm sure they won't). By definition they won't give you the app you searched for, because the keyword will be bought by a competitor or even a scam. reply pushcx 7 hours agorootparentIf Apple is making money from showing incorrect search results, isn’t that worse? reply jowea 6 hours agorootparentprevWhat if you're searching for \"bank\" or \"flashlight\" or \"map\"? reply RulerOf 11 hours agoparentprevThis has been a very noticeable problem to me for some time. I won't search the App Store anymore. I go to the web site for the app I want and get the App Store link that way. I wish the App Store listings would specify the domain of the entity they come from in plain text, backed by a validation method similar to what we do for TLS certs. reply fakedang 8 hours agoparentprevSpecial mention, the f***ton of Whatsapp clients on the iPad when Whatsapp didn't have any releases for the iPadOS up until last September. reply OJFord 10 hours agoparentprev> my broker? CFD gambling app I don't think 'gambling app' is a fair description given it's a regulated security, any broker that truly offers CFD trading is (1) going to be legit; (2) going to be competing with the broker you were searching for for result space. Of course to serve its users any app store should massively prioritise the word/brand (incl. typos) you actually search for though. reply pjc50 7 minutes agorootparenthttps://www.fca.org.uk/publications/policy-statements/ps19-1... They're in the category that, while they theoretically can be used by skilled investors, anyone offering them to retail punters is up to no good. Because those punters are going to lose money to them hand over fist. reply Animats 9 hours agorootparentprev> any broker that truly offers CFD trading is (1) going to be legit; Er, no. \"Contracts for Difference\" are the new binary options. reply jsheard 10 hours agorootparentprevCasino gambling is also regulated, there being some degree of oversight doesn't make it not gambling. reply OJFord 10 hours agorootparentSure. This is a financial instrument used by industry professionals though, not just an 'app'. You can (and some people do) call buying shares 'gambling' too, at some point it just comes down to what our definitions are and it's not very interesting. reply quickslowdown 9 hours agorootparentThere are professional gamblers (pro poker?). I think I understand the spirit of what you're trying to say, but CFDs are absolutely a gamble, like most investing. It just doesn't have the same stink of \"casino gambling,\" it's a fresh cut, cologne wearing, suited up form of gambling. reply OJFord 8 hours agorootparent> absolutely a gamble, like most investing Yeah well like I said, definitions. If most investing is gambling then sure, I just don't find that useful. Anyway the initial comment I was responding to was 'looking for my broker but got CFDs'; my point was they got a different broker. It's still a bad search result, but it's not a weird scam, it's just a competitor. It's not the same as looking for my bank but got crypto exchange at all. reply quickslowdown 4 hours agorootparentgambling (noun): the practice or activity of betting : the practice of risking money or other stakes in a game or bet This is the actual dictionary definition of how I would describe investing to someone who'd never heard of it. Investing money is gambling. It's not a bad word or something to be ashamed of, and I'm not trying to be a pedant (I say as I type a long comment starting with a dictionary definition), I just think it's important to recognize & remember at all times so you don't lose everything. I get what you mean about the broker app. App store searches are bullshit. reply iscrewyou 10 hours agoparentprev> I don't think I've ever searched for something on the app store and not got a scam as the first result It’s cool to crap on Apple and all these days but this is all categorically false. What you are referring to is the Ad on the top of the page. It’s clearly labeled as ad and has a light blue box around the whole ad. I tried all those things you mentioned and the first result after the clearly labeled ad is what I searched for. reply ghewgill 10 hours agorootparentThen what are the ads for? How do they benefit users? reply TacticalCoder 10 hours agoparentprevTFA is talking about a literal scam, where his money vanished. > - my bank? I get crypto.com Although crypto.com is not a bank, they seem like a legit business and not a scam. Many people are using crypto.com: I know one person who has one such card and I asked a waiter if he had already seen cards like that (waiters gets to see many credit/debit cards a day) and he answered me that they weren't that uncommon. > - official government app for paying my tax? intuit product They may be using shady tactics but they are not a scam. reply mort96 10 hours agorootparentcrypto.com 1) is cryptocurrency bs and 2) buys ads on keywodrs which leads them to appear on searches for specific banks, smells like a scam to me reply Animats 10 hours agoprevApple statement on why the EU requiring open app stores is bad: Schiller, an Apple veteran who once ran its marketing machine, said the moves to break the company’s closed ecosystem for software will undermine the privacy and security the company has worked to build into its products and services. “This isn’t our first choice,” he said. “We always want to have the highest standard everywhere in the world but we also have the requirement to meet the legal requirements in the local markets. “In the App Store we have a lot of signals that we are looking for every day to find scams and stop them,” Schiller said. “With these new marketplaces we won’t have visibility into those issues.” Right. reply mdhb 10 hours agoparentAlso from Schiller.. the open web is a direct threat to our cash cow. https://files.mastodon.social/media_attachments/files/111/95... reply concinds 9 hours agoparentprevThe company that claims to have \"the highest standard everywhere in the world\" for security and privacy, happens to run a major desktop platform (macOS) with only a primitive basic signature-based antivirus built-in. Instead they rely on Gatekeeper/notarization for the bulk of the protection, which is a pain for devs (when it doesn't work right) and less effective. Same is now happening with iOS sideloading, instead of robust antimalware based on heuristics and app behavior (like Google Play Protect), they'll keep relying on blunt instruments like notarization. Doubt it'll keep users safer. Maybe it's NIH syndrome? reply openthc 11 hours agoprevI thought the Apple platform had the best consumer experience and that's why folk love it -- it \"just works\" -- cause they keep the riff-raff out of their gated community. Perhaps they let this one slip through because their team was too busy dragging out the review process for our cannabis compliance application, they can only afford so many reviewers after all. We wouldn't want children accidentally getting their hands on regulatory compliance data for deadly deadly cannabis. (which could happen with our application, after they had signed up and verified their agency cannabis license (which only takes many months/years and $$$$$s to get)) reply pie420 11 hours agoparentApples app store is way worse than the google play store. i was shocked at how bad the app store is with shitty ads and promoted content over organic search results reply doix 7 hours agorootparentAnd yet, the top result for \"Bitcoin wallet\" on the play store isn't a scam. And on it's definitely not a scam on F-Droid where I would personally look for a Bitcoin wallet. reply tasuki 42 minutes agorootparentYou do you. I'd not use a phone for a Bitcoin wallet. reply DoodahMan 6 hours agoparentprevfwiw i hope yall get approved. thank you & take care. reply andrei_says_ 11 hours agoparentprevIt did - before cheapo enshittification started creeping in. I believe some time ago I saw some research on the quality of App Store app review process … zero protection. But, even at this stage, Apple is still “the best”, because of the slower pace of the corruption and in comparison to the toxic dumpster fire of the alternatives. Android and Windows are spyware/malware masquerading as OSs. reply npteljes 55 minutes agorootparentOr, we can say that all of them are dumpster fires, even if Apple is maybe the best of the shitty app stores. On Apple though, you don't have anything other the App store. That's something to consider. On Android, you have the chance to install F-Droid for example. reply refulgentis 11 hours agorootparentprevThis is incorrect unless you're shifting topics to whether we trust MS, Apple, or Google's data collection more. reply lnxg33k1 31 minutes agoprevIt is very important that common people understand that the Apple App Store is not a way to outsource their own security and blindly install whatever they find on it, personally I was always convinced that Apple Marketing about the security of their App Store in regard to opening competition sources of app installs, has done a lot of damage to their users for the benefit of their shareholders, and if I was a Apple user I would be pretty upset for them up to the point where I would be unable to trust them. Whatever the source you're responsible for your own safety. Don't be a victim of marketing departments. reply secsubsc 6 hours agoprevI am in India and I can see the app on top spot (marked as Ad) when I search for Bitcoin Wallet. My theory is, they paid for an Ad in a specific region and hence it started showing on top, people started downloading in that region, and that boosted the overall ranking for that app and hence people from other regions are seeing it among top results, even though its not an Ad there. Irrespective of the rating or freshness of the app, since it is getting downloaded in one region (because it is an Ad there), automatically it goes to top in other regions. This trick can be used by other apps also, considering it would be cheapter to buy the top Ad spot in India and then it organically rises to the top. reply ilamont 11 hours agoprevPromoted results in Google are loaded with scams. According to one recent report, 75% of brands are affected (https://searchengineland.com/google-search-ads-brands-fraud-...): The researchers who conducted the report found that retail giants such as Amazon, American Airlines, Lego, Pizza Hut, and Samsung were all victims of identity fraud within Google Search Ads. Here's a Google SERP for \"Facebook\" which shows Facebook as the URL, redirects to an Apple security scam: https://youtube.com/shorts/gTEuqXYAp58?si=lzFV9mfX31_8nzd1 Google even vouches for the advertiser: https://twitter.com/leanmediaorg/status/1724467969344905534/... reply klabb3 11 hours agoparentThat’s insane, and news worthy. Imagine non-techies just trying to go about their day and getting that. But hold on a sec. Is this verified by others? The guy in the video cuts to a screenshot, which doesn’t show the resulting url or how he got there, so it’s hard to tell what happened. reply ilamont 11 hours agorootparentTwo versions of the video. This one shows the click: https://youtube.com/shorts/dXZQMkPJkXg?si=hsL8fUirHZj3DMG5 reply concinds 10 hours agorootparentThat's still a jump cut, it only pretends to show the click. Dodgy reply morder 4 hours agorootparenteven if it did show the click it's easy enough to edit the page in devtools to go wherever you want reply callalex 4 hours agoparentprevThis is about app stores, not web searches. Google Play Store does not suffer from this issue to the same degree at all. reply habeanf 11 hours agoparentprevIt wasn't a promoted result, it's an organic search result, and it's still there! reply charcircuit 10 hours agoparentprev>Google even vouches for the advertiser Google vouches that the advertisers is who is he says he is. Google is not vouching for the reputation if the advertiser. reply schappim 5 hours agoprevDon't worry, everyone. Apple can now afford to hire some app store reviewers by using the revenue from its new \"Core Technology Fee.\" This fee requires developers to pay Apple €0.50 for every first annual installation exceeding a threshold of 1 million for apps distributed outside the App Store. reply KomoD 11 hours agoprev> does a scam app become the #1 organic search result (not promoted) in the app store It's possible that it's just because it was literally called \"Bitcoin Wallet\", an exact match for your search, or boosted by fake reviews, or it was actually an ad that you didn't notice. Though it shouldn't have gotten past review at all But I don't really understand why you'd blindly trust some random app? Also, would be interesting to take a look at the app, sadly know nothing about ios apps or how to get the IPA, only android. reply habeanf 11 hours agoparentYou're right, I shouldn't trust a random app. Also, it's pretty much my first serious foray into Apple land. I trusted Apple's search results. There are multiple apps, far more mature and backed by serious developers, that would also match the phrase \"Bitcoin Wallet\". The question is why is the scam app the #1 organic search result? For a new app with such scammy reviews and questionable metadata I would expect it to be #30 in the list. For context, the app store reports the scam app as #85 in all finance apps. reply concinds 10 hours agorootparent> The question is why is the scam app the #1 organic search result? The real answer is that this has been happening for years. You can pay companies to pump up your app to the top of App Store search results or \"app categories\" lists, and they'll have farms of iPhones/Androids downloading apps to pump up their rank, and giving them 5 star reviews. There have also been repeated problems with copycat apps that impersonate real indie apps (and sometimes end up earning more than the real app), which should have been a warning sign of the problems of App Review. Google \"app store copycat\" and you'll see. reply trothamel 8 hours agoparentprev> But I don't really understand why you'd blindly trust some random app? Perhaps because Apple claims their apps go through a review process, and one would hope this would have failed that process? That's what Apple claims the value proposition of their 30% cut and closed platform are. reply cromka 3 hours agorootparentThis short conversation perfectly summarized the reality behind the AppleEU spat. reply dkobia 10 hours agoprevIt is both impressive and concerning how well their app store optimization efforts were, for what seems like a major keyword/phrase. These type of shenanigans were usually reserved for the Play Store. No more. To be fair many crypto wallet apps are deceptively simple applications. reply aws_ls 2 hours agoprevThis is understandably shocking. The way I have chosen bitcoin wallets is searching for trusted brands in the space using Google, and going to their website and then installing the app. reply hooksfordays 8 hours agoprevReplying to verify I see the same app almost at the top of search result. Based in Canada and searching “Bitcoin Wallet”, it was the 2nd non-promoted result. I have only 1 other app of this variety on my phone currently and haven’t used it or searched for anything crypto related including months. reply kps 7 hours agoparentSame for me, second result. I've never had anything crypto-related on my phone. reply swatcoder 11 hours agoprevWhen I perform your search, I get legitimate results at the top, and I don't see the specific app from the Reddit thread. But about at about rank #7, I see an app that uses a distorted form of the same logo, a different unfamiliar publisher, a slightly altered title and a similar smattering of only a few reviews. It sounds like somebody is burning developer accounts to keep reposting the scam app. Not unlike people being banned from a website and then resubscribing with a different email or through a VPN or whatever. It slipping through into your results isn't so much plain neglect as it is an arms race that Apple is on the losing side of this time. Robust algorithmic ranking and moderation at scale is a myth, though, and you can find this happen pretty much everywhere. This one will probanly get squashed with some near-term update to their algorithm, and then get compromised again sometime later since crypto is so ripe for scamming. You can't escape personal due diligence and \"it was top ranked!\" has never been that. reply zizee 7 hours agoparent> You can't escape personal due diligence and \"it was top ranked!\" has never been that. Apple continually makes claims that the closed ecosystem is essential to the safety of their customers, that they have a robust review process, and that their customers choose them because of the safety they provide. Apple should stop repeating these claims if they are not, in fact, reliable protection against scams. reply habeanf 11 hours agoparentprevPosted above the screen recording: https://streamable.com/y5nhy7 > You can't escape personal due diligence and \"it was top ranked!\" has never been that. On one hand that's a fair point and I should've known better. OTOH I think it is legit to trust top app store search results to return quality apps, especially if there is a massive disparity between their quality. The scam app has obvious repetitive spam reviews. The developer's website is terrible and the submit button doesn't even work. This is basic quality control on apple's part. If every single app store user needs to manually vet every single app they install to the proper extent there would be a fraction of a fraction of the installs and respectively, a fraction of a fraction of the revenue. Consider the extent of lawsuits between apple and companies with app store apps - does it not strike you that apple protects that revenue stream? Wouldn't it make sense to give app store users a sense of trust in the top search results? reply yosef123 11 hours agoprevSo much for Apple \"security\" reply charcircuit 10 hours agoparentSecurity isn't binary and it should be compared relative to other platforms. reply smoldesu 10 hours agorootparentSecurity measures should be treated as an opportunity cost, and scrutinized when it fails. In this case, the App Store's manual review layer has failed to catch a rudimentary scam. It's becoming a recurring issue on the App Store. reply callalex 4 hours agoprevBut I've been assured that the 30% protection racket Apple charges is justified because they spend so much effort curating their store. I'm assured by Apple's latest press release that the web is full of scams and only they can protect me from it. reply tapland 11 hours agoprevWhat was the justification for the app store ecosystem? reply DangerousPie 10 hours agoprevMust be region/account specific. I get crypto.com, bitcoin.com and coinbase as the top three results. Nevertheless I agree these should be moderated better and scam apps need to be removed quickly. reply Shosty123 11 hours agoprevThat's actually quite worrisome. I don't really think twice about downloading the top result for things like PayPal or local banking apps if I get a new phone, for example. reply Lockal 1 hour agoprev1) You see recently visited apps on top of search results. Same applies for Google/Yandex (unless you turned off specific personalization settings). It is a feature to save your time from scrolling. 2) Post on Reddit is FUD from competitors (newly created account included). People who transfer C$150k know exactly what to do when they lose money (no, they don't visit /r/Bitcoin to ask \"any chance of fund recovery or all gone?\"). Don't promote FUD on HN. reply echelon_musk 11 hours agoprevWhy did you have to transfer bitcoin? Surely you would just load your private key into the app unless I'm missing something. reply habeanf 11 hours agoparentHonestly, I got lazy, and that's on me. I was using the standard bitcoin wallet app on android. It did seem weird I can't restore the wallet I backed up in the android app, but the android app github doesn't point to an app store app, so I figured there just isn't and the android app's backup format is something detached. Then I figured a legit apple app could generate a wallet and I could transfer the bitcoin between them. Which is what I did. The apple app indeed received it and promptly sent it off somewhere else. What's even crazier is that the apple app shows this info! You'd expect the scammer to hide the scam but I suppose it just made it easier to pass the app store inspection. reply aws_ls 2 hours agorootparent> The apple app indeed received it and promptly sent it off somewhere else. What's even crazier is that the apple app shows this info! Did you try moving the crypto back to your Android wallet? Sometimes they do move to cold storage, or invest into DeFi schemes. It will be hidden in their T&C. reply ur-whale 27 minutes agoprev> Earlier today I decided to switch my Android for an iPhone. Mistake #1 : switching to an even more closed computing environment, where user has strictly no control > android Mistake #2 : running on a tech. stack you do not control: closed-source, walled-garden > wallet Mistake #3 : using a wallet instead of your own private cold storage to hold any kind of significant amounts of money > but its still up there, #1 search result. Mistake #4 : trusting that Apple is making huge efforts to secure their environment. In the same vein as \"not your keys, not your coins\" : - \"trust the vendor, not your coins\" - \"not your hardware, not your coins\" - \"not your operating system, not your coins\" - \"not your key management software, not your coins\" - \"not open source and therefore not auditable, not your coins\" reply fingerlocks 11 hours agoprevNo repro. Same search string gives me the Bitcoin.com and the Coinbase app at the top. Scrolled through several dozen wallet apps and the one in the Reddit link never surfaced. reply habeanf 11 hours agoparentHere is a screen recording including this hacker news post + your comment & and a switch to the app store with the search phrase. The first result is the scam app. I scrolled down so you can see where serious apps are in the list of results. https://streamable.com/q2mulu reply fingerlocks 10 hours agorootparentnext [7 more] [flagged] habeanf 10 hours agorootparentHow am I supposed to prove I didn't seek the app out? What level of proof do you expect? Do you expect me to create a brand new apple account, replace the one in my phone (which might require wiping it?), and then search again, just to satisfy your default assumption that my claim is false? When I first installed the app it was the first search result. I can't go back in time and prove it because I'm not paranoid and I don't screenshot the result of every search in every app store and search engine. I reported it. I'm not trying to rally a mob against Apple. The bitcoin is gone. I'm trying to prevent others from suffering the same fate as me. Based on what I'm reading in the comments here some other people in the world do trust apple app store search results, and I believe they've gained something from my post. reply hellotomyrars 10 hours agorootparentI also can't replicate your results but GP is pretty aggressive in a way I don't feel is necessary. Sorry you got scammed out of your crypto, hope it wasn't too bad. I do agree with the sentiment that you should be incredibly vigilant and verify anything you're going to put your money inside of, Bitcoin or otherwise though. But people in positions and wallets that are far bigger than you (presumably) have done worse so don't feel too bad (Ripple CEO losing $112 million worth of XRP, for example). Edit: Unless that app had a ton of reviews removed recently, I recommend in the future looking at that number. It is crazy that they've gamed it up to #97 in the finance charts according to the app store listing for sure, though the Play store is not immune to this manipulation. reply fingerlocks 10 hours agorootparentprevYou corroborate with others independently before posting. reply habeanf 10 hours agorootparentIf only there was a website I could use to contact other like minded people where they could vote on posts they like and discuss them in comment sections. Oh, wait reply fingerlocks 10 hours agorootparentThe problem is that you’re framing this as some kind of technical issue that no one else has reproduced. Did you follow any kind of common sense due diligence? How is this any different from blaming Google/Gmail because you fell victim to a phishing/scam email? reply habeanf 9 hours agorootparentOthers have reproduced it, see others comments. Perhaps for them the ranking changed slightly but the fact is a scam app is ranked as high as serious companies. Gmail / Google are open, the App Store is closed and supposedly vetted and guarded. Apple sells to its customers security, quality, and trust. It’s one of the reasons one pays 2x for an iPhone. All of these promises of a better ecosystem have been broken through this experience. One of the supposed advantages of the closed App Store is to absolve (to some extent) the user of having to do said due diligence. Also, it’s not like it’s impossible. Google are doing it well - show me a scam app that’s in the top 10 of the play store for bitcoin, banking, finance etc. Hardly any to be found. reply habeanf 11 hours agoparentprevI just took a screenshot of the app store about an hour ago: https://pasteboard.co/bZ7qQvAzYggy.png reply fingerlocks 11 hours agorootparentYeah, at the top because you already have it downloaded reply habeanf 11 hours agorootparentFair enough. I removed it and recorded the screen again: https://streamable.com/y5nhy7 reply m463 9 hours agoprevThere might be an argument that the app store itself is the scam. I think there are ~ 3m apps available right now. Apple is the only place (currently) to sell apps, or buy apps. They interpose themselves, and do a poor job of things. How can a buyer make his apps visible? How can a seller find anything? There should not only be more app stores, there should be markets and communities and personal apps. reply moribvndvs 5 hours agoprevWorking as intended. Apple has repeatedly rejected an update to an app that we have recently updated because it had a link to our help site which links to our main site which has an option to purchase a subscription. Despite us having published 60 versions successfully until today, that link being there and unchanged the entire time. But this sort of shit, no problem go right on through, sir. reply justinclift 10 hours agoprevAt this point, I'm about 80% sure that a lot of these scam apps are being approved by corrupted Apple review staff. So many of them are blatantly scams that it's not credibly \"human error\". reply kgdiem 9 hours agoparentAny idea where they’re based? Are they global? They do my reviews at weird times. reply phatfish 9 hours agorootparentIt's just security theatre. App could pay enough to get competent people to grind out a few months of mind numbing app reviews. I guarantee they don't. reply endisneigh 9 hours agoprevSad to say but it seems pretty are pretty dumb. Robinhood yields robinhood for first non-ad app, same with Bank of America, chase, Citi, etc. do people really just click the first thing they see? Wow. As for the example - can’t replicate, but seems crazy to put a seed phrase into some random app you didn’t get yourself. Even if the app wasn’t a scam. reply npteljes 53 minutes agoparent>do people really just click the first thing they see? It's just people behaving in a certain way, and that being exploited. If people had a different behavior, the exploit would be different too. reply habeanf 2 hours agoparentprevI didn’t put a seed phrase into the app. I created a new wallet in it and transferred the btc to it. reply ametrau 8 hours agoprevI have been scammed before by the top result also. So not only are they taking a 30!!!!!% tax on developers (not only on profit) but they milk ad. money from shysters. Yet you still have the just “use another phone” / don’t release your app with them people. Yeah who? Monopoly B? reply jackblemming 10 hours agoprevGuys Apple told me if I gave up all my freedom they would keep me safe.. I don’t feel so good.. reply roflchoppa 11 hours agoprev [–] It’s crazy that you can leave reviews from within the App Store, nor can you report it to Apple from within the App Store. reply CharlesW 8 hours agoparent> …nor can you report it to Apple from within the App Store. You can, since 2021. https://www.theverge.com/2021/10/4/22705405/apple-report-a-p... reply roflchoppa 7 hours agorootparentI don’t see the option in the section that is referenced, only the privacy policy. ¯\\_(ツ)_/¯ reply CharlesW 7 hours agorootparenthttps://imgur.com/a/bOL3eCL reply roflchoppa 5 hours agorootparenthttps://imgur.com/a/YU9CFOK reply CharlesW 5 hours agorootparentTap “Get”. reply habeanf 11 hours agoparentprev [–] You're right, and I've done both. My review doesn't appear in the list of reviews. What's crazy is that a scam app is the #1 organic search result for 'bitcoin wallet', above blockchain.com and coinbase. reply saalweachter 11 hours agorootparent [–] (1) How much more valuable do you think it is to the scam app developer to appear at the top of the search results than for a legitimate wallet developer? (2) Do you think a legitimate wallet app will engage in the same black-hat SEO tactics a scam app developer will? reply habeanf 10 hours agorootparent [–] > (1) How much more valuable do you think it is to the scam app developer to appear at the top of the search results than for a legitimate wallet developer? Well, the scammer got CAD $150k out of the reddit guy I linked to and I lost a slightly smaller amount - and we're just two out of who knows how many thousands of app store users that installed this app. I'd say people trust the top 5-10 results quite a lot. > (2) Do you think a legitimate wallet app will engage in the same black-hat SEO tactics a scam app developer will? I think all the black-hat SEO in the world should not be able to surpass the obvious value disparity compared to legit apps with hundreds of thousands of installs and hundreds of reviews. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "An individual switched from an Android to an iPhone and unknowingly transferred their bitcoin to a scam wallet app from the Apple App Store.",
      "Despite being reported as fraudulent, the scam app remained the top search result on the App Store, raising concerns about its ranking over trusted apps like Binance and Coinbase.",
      "This incident highlights the importance of thoroughly vetting apps before using them, especially for sensitive transactions like cryptocurrency transfers."
    ],
    "commentSummary": [
      "Concerns are rising on platforms like the Apple App Store and Google Search Ads due to scam apps, fake reviews, and misleading ads, impacting user trust and safety.",
      "Discussions focus on platform security, user vigilance, and the balance between investing and gambling, along with app store rankings challenges.",
      "Users and platforms share the responsibility in fighting scams, highlighting the importance of collaboration in ensuring a secure digital environment."
    ],
    "points": 203,
    "commentCount": 121,
    "retryCount": 0,
    "time": 1710279413
  },
  {
    "id": 39680579,
    "title": "Enhancing SaaS Reliability with Deterministic Simulation Testing",
    "originLink": "https://www.warpstream.com/blog/deterministic-simulation-testing-for-our-entire-saas",
    "originBody": "PricingCompany Blog All Posts Engineering Case Studies Integrations DocsContact ConsoleBook a demo Engineering Deterministic Simulation Testing for Our Entire SaaS Mar 12, 2024 Richard Artoul Link copied! Deterministic Simulation Testing Deterministic simulation testing is fast becoming the gold standard for how mission critical software is tested. Deterministic simulation testing was first popularized by the FoundationDB team who spent 18 months building a deterministic simulation framework for their database before ever letting it write or read data from an actual physical disk. The results speak for themselves: FoundationDB is widely considered to be one of the most robust and well-tested distributed databases, so much so that Kyle Kingsbury (of Jepsen fame) refused to test it because their deterministic simulator already stress tested FoundationDB more than the Jepsen framework ever could. The WarpStream team utilized FoundationDB heavily at Datadog when we built Husky, Datadog’s columnar storage engine for event data. Over the course of our careers, our team has operated (and been on-call for) almost every database on the market: M3DB, etcd, ZooKeeper, Cassandra, Elasticsearch, Redis, MongoDB, MySQL, Postgres, Apache Kafka and more. In our experience, FoundationDB stands in a league of its own in terms of correctness and reliability due to its early investment in deterministic simulation testing. A more recent example of a database system that leverages this approach to testing is TigerBeetle, a financial transactions database, that uses deterministic simulation testing to build one of the most robust financial OLTP databases available today. When we were designing WarpStream, we knew that it wouldn’t be enough to just replace Apache Kafka with something cheaper and easier to operate. Kafka is the beating heart of many companies most critical infrastructure, and if we were to stand any chance of convincing those organizations to adopt WarpStream, we’d have to compress 12+ years of production hardening into a much shorter time frame. We accelerated this process with our architectural decision to rely on object storage as the only storage in the system, bypassing many of the tricky problems of ensuring data durability, availability, and replication at scale. Still, the fact that WarpStream leverages object storage is only a small part of ensuring the correctness of the overall system. Antithesis When we first heard about Antithesis, we could hardly contain our excitement. Antithesis has created the holy grail for testing distributed systems: a bespoke hypervisor that deterministically simulates an entire set of Docker containers and injects faults, created by the same people who made FoundationDB. For a group of gray-haired distributed systems engineers, seeing Antithesis in action felt like a tribe of cavemen stumbling upon a post-industrial revolution society. As we spoke more to the Antithesis team, an idea began to crystallize: we could use Antithesis to deterministically simulate not only WarpStream, but our entire SaaS! WarpStream was built differently than most traditional database products. It was designed from day one with a true data plane / control plane split. There are two primary components to WarpStream: First, the Agents (data plane) that act as “thick proxies” and expose the Kafka protocol to clients. The Agents also take care of all communication with object storage, layering in batching and caching to improve performance and keep costs low. Second is the WarpStream control plane which has two major components: The metadata store that tracks cluster metadata and performs remote consensus. Our SaaS software that manages different tenants’ metadata stores, API keys, users, accounts, etc. The metadata store only has two dependencies: Any cloud KV store Object storage The SaaS software adds one additional dependency: a traditional SQL database for managing users, organization, API keys, etc. Looking at WarpStream’s minimal dependencies, we thought, why not test its entire customer experience, from initial signup to running Kafka workloads? We created a docker-compose file that contains the following components: Several WarpStream Agents Several WarpStream Control Plane nodes Several Apache Kafka clients A KV store Postgres An object store (localstack) With the help of the Antithesis team, we wrote a test workload that started all of those services, signed up for a WarpStream account, created a virtual cluster, and then began producing and consuming data. The workload was carefully structured so that we could assert on a variety of different important properties that WarpStream must maintain at all times. The test workload consists of multiple producers that are each assigned a unique ID and write records to a small set of topics. These producers synchronously write a few small JSON records that contain the producer’s ID, a counter (a monotonic sequence number for that producer), and a few other properties. We repeat the same components as the record’s key, value, and in a header to ensure we never shuffle those around accidentally. The consumer side of the workload polls all the topics and all the partitions and asserts that: The topic and partition the record was consumed from matches the topic and partition the record was produced to. The offsets for each record in each partition are monotonic. The sequence numbers for each producer are monotonic, i.e. if we group the records bythe sequence number encoded in the record is monotonically increasing. The consumers store all of the records for each polling iteration and can assert that a record at offset X in the previous poll still exists in a future poll. This ensures that WarpStream doesn't lose or reorder data as e.g. background compaction runs to reorganize the cluster’s data for more efficient access. These assertions address many of the classes of bugs found in previous Jepsen tests of Apache Kafka and other Kafka-compatible systems. For example, prior Jepsen tests have caught bugs like: Loss of previously acknowledged writes. If a write was acknowledged but failed to appear in the output for that topic-partition in the future, assertion 1 or 3 above would fire. Violation of producer idempotency (i.e producing duplicate records without the producer itself crashing or restarting). Antithesis automatically tests our Idempotent Producer implementation by disrupting the network between the producer client and the agent or the agent and the control plane, leading to internal retries inside the Kafka client. A duplicate would cause the sequence number from a producer to stay the same or decrease, causing assertion 3 above to fire. Records appearing in different topic-partitions than they were originally written to. This is addressed by assertions 1 or 3 above. What’s the big deal? At this point you might be scratching your head a little bit and wondering: “What’s the big deal here? Isn’t this just a really fancy integration test!?”. Yes and no. Before we started using Antithesis, WarpStream already had a pretty robust set of stress tests we called the “correctness tests”. These tests do essentially everything we just described, but in a regular CI environment. Our correctness tests even inject faults all over the WarpStream stack using a custom chaos injection library that we wrote. These tests are incredibly powerful, and they caught a lot of bugs. We would go as far as saying that investing deeply in those correctness tests is one of the main reasons that we were able to develop WarpStream as efficiently as we did. Just like our existing correctness tests, the Antithesis hypervisor automatically injects faults, latency, thread hangs, and restarts into the workload. However, unlike our correctness tests, the Antithesis hypervisor is really smart and automatically fuzzes the system under test in an intelligent way. Antithesis automatically instruments your software to measure code coverage and build statistics about the execution frequency of each code path. This enables Antithesis to detect “interesting” behavior in the test (such as infrequent code paths getting exercised, or rare log messages being emitted). When Antithesis detects interesting or rare behavior, it immediately snapshots the state of the entire system before exploring various different execution branches. This means that Antithesis is much better at triggering rare or unlikely behavior in WarpStream than our existing correctness tests were. Also, since Antithesis runs the entire software stack in a deterministic simulator, they can actually run the simulation at faster than wall clock time. Similar to FoundationDB, WarpStream makes heavy use of timers and batching to improve performance. Anytime a WarpStream Goroutine does the equivalent of time.Sleep(), the Antithesis hypervisor doesn’t actually have to wait. On top of that, the Antithesis hypervisor explores code branches concurrently. All of this adds up in a meaningful way such that Antithesis can cost effectively compress years of stress testing into a much shorter time frame. It’s hard to over-emphasize just how transformative this technology is for building distributed systems. For all intents and purposes, it really does feel like a time-traveler arrived from 20 years in the future and gave us their state of the art software testing technology. Of course, it’s not actually magic. Antithesis is the result of dozens of the smartest software engineers, statisticians, and machine learning experts pouring their heart and souls into the problem of software testing for 5 years straight. But to us mere mortals, it does feel a lot like magic. We found some bugs Let’s look at a few example runs that Antithesis generated for us. Antithesis ran the WarpStream workload for 6 wall clock hours, during which it simulated 280 hours of application time. The graph shows that it took about 160 “application hours” for Antithesis to “stall” and stop discovering new “behaviors” in the WarpStream workload. This means that running the tests for longer than 160 hours has diminishing returns, and instead we should invest in making the test itself more sophisticated if we want to exercise the codebase more. Great feedback for us! But think about that for a moment: even after 140 hours of injecting faults, randomizing thread execution, automatically detecting that something interesting / rare had happened and intentionally branching to investigate further, Antithesis was still “discovering” new behaviors in WarpStream. We could hire a 100 distributed systems engineers and make them write integration tests for an entire year, and they probably wouldn’t be able to trigger all the interesting states and behavior that a single Antithesis run covered in 6 hours of wall clock time. As just one example of how powerful this is, on the first day we started using Antithesis it caught a data race in our metrics instrumentation library that had been present since the first month of the project. Our correctness tests had run in our regular CI workflows for literally 10s of thousands of hours by then, with the Go race detector enabled, and not once ever caught this bug. Antithesis caught this bug in its first 233 seconds of execution. A data race in the instrumentation library isn’t that exciting, though. What about an extremely rare data loss bug that is the result of both a network failure and a race condition? That’s more exciting! To minimize the number of S3 PUTs that WarpStream users have to pay for, the Agents buffer Kafka Produce requests from many different clients in-memory for ~250ms before combining the batches of data into a single file and flushing it to object storage. In some scenarios, like if write throughput is high, there will be multiple outstanding files being flushed to object storage concurrently. Once flushing the files succeeds, committing the metadata for the flushed files to the control plane can be batched to reduce networking overhead. This is implemented using a background Goroutine that periodically scans the list of “flushed but not yet committed” files. While refactoring the Agent to add speculative retries for flushing files to object storage, we subtly broke the error handling on this path so that, for a very brief window of time, a file which failed to flush would be considered successful and ready to commit to the control plane metadata store. In program order (i.e. the linear flow of the code, ignoring concurrency) this window where the background Goroutine that commits metadata would see the successful file would be nearly impossible to squeeze into. This background Goroutine only polls for successful files every five milliseconds, and the time between the two state transitions in the common case would be less than a microsecond! This bug is the manifestation of two unlikely events: a file failing to flush and a specific thread interleaving that should be extremely rare in practice. Despite how unlikely these events are to occur together, on a long enough time-scale, this bug would have resulted in data loss at some point. Instead, thanks to Antithesis’ powerful fuzzer and fault injector, this rare combination of events happened roughly once per wall clock hour of testing. We’d been running a build with this bug in our staging environment and obviously did not encounter that bug at all, let alone once per hour, as it would’ve immediately been noticed when a future background compaction failed due to the missing file in object storage. We’ve since fixed the regression in the code such that the invalid, temporary state transition cannot occur. Why not Jepsen? The obvious question you might be asking yourself at this point is: Why use Antithesis instead of a traditional Jepsen test? It’s a good question, and one we asked ourselves before embarking on our journey with Antithesis. We’re big fans of Jepsen and have consumed almost every published report. However, after speaking with the Antithesis team and spending a few months integrating with it, we feel strongly that deterministic simulation testing with tools like Antithesis is a much more robust and sustainable path forward for the industry. Specifically, we think that the Antithesis’ approach is better than Jepsen’s for a few reasons: The Antithesis technology is more robust, and much more likely to catch bugs than the Jepsen harness. There is simply no other equivalent (that we’re aware of) to Antithesis’ custom hypervisor, and its ability to automatically instrument distributed systems for code coverage and effectively “hunt” for bugs. Yes, the Jepsen framework will inject faults into a running environment in an effort to trigger bugs and edge-case behavior, but this approach is crude in comparison to what Antithesis does. Antithesis integrates natively into how our engineers are used to working. The entire test setup is expressed using standard docker-compose files and Docker images, and Antithesis tests are kicked off using Github Actions that push WarpStream images to Antithesis’ docker registry. When we add new functionality, all our engineers have to do is modify the Antithesis workload and kick off an automated CI job. The entire experience and workflow lives right next to our existing codebase, CI, and workflows. Extra bonus: none of our engineers had to learn Clojure. Antithesis testing is designed to be a continuous process with accompanying professional services that help you grow and adapt the tests as the scope of your product increases. That means our users get the confidence that every WarpStream release is actively tested with Antithesis, unlike a traditional Jepsen test where the engagement is short-lived and usually only covers a “snapshot” of a system at a static point in time. Finally, it would not have been practical to continuously test our entire SaaS platform with Jepsen in the same way that we do with Antithesis. While that may seem like overkill, we think it’s a pretty important point. For example, consider the fact that almost every cloud infrastructure provider has a routing or proxy layer that is responsible for routing customer requests to the correct set of infrastructure that hosts the customer’s resources. A small data race or caching bug in that routing layer could result in exposing one customer’s resources to a different customer. These multi-tenant SaaS layers are never tested in traditional Jepsen testing, but with Antithesis it was actually easier to include these layers in our testing than to specifically exclude them. We’re just getting started with Antithesis! Over the coming months we plan to work with the Antithesis team to expand our testing footprint to cover additional functionality like: Multi-region deployments of our SaaS platform. Multi-role Agent Clusters. Injecting and detecting data corruption at the storage and file cache layer using checksums. And much more! If you’d like to learn more about WarpStream, please contact us, or join our Slack! Table of Contents Deterministic Simulation Testing Antithesis What’s the big deal? We found some bugs Why not Jepsen? Subscribe WarpStream is a Kafka compatible data streaming platform built directly on top of object storage: no inter-AZ bandwidth costs, no disks to manage, and infinitely scalable, all within your VPC. Thank you! You've been added to our mailing list. Oops! Something went wrong while submitting the form. Join our mailing list to stay up to date! Subscribe PricingCompanyDocumentationBlogContact Us Slack Discord X (Twitter) LinkedIn © 2024 WarpStream. All rights reserved. Apache, Apache Kafka, Kafka, and associated open source project names are trademarks of the Apache Software Foundation. Privacy PolicySLATerms of ServiceTrust Center Deterministic Simulation Testing for Our... Public Benchmarks and TCO Analysis Kafka as a KV Store: deduplicating milli... Anatomy of a serverless usage based bill... Middleware saves 85% by replacing Apache... S3 Express is All You Need Unlocking Idempotency with Retroactive T... Minimizing S3 API Costs with Distributed... WarpStream + Materialize: Simpler Stream... Hacking the Kafka PRoTocOL Kafka is dead, long live Kafka Return To Blog Return To Blog",
    "commentLink": "https://news.ycombinator.com/item?id=39680579",
    "commentBody": "Deterministic simulation testing for our entire SaaS (warpstream.com)190 points by wwilson 18 hours agohidepastfavorite23 comments taink 17 hours agoThis is related to Antithesis, here is the thread on the original announcement : https://news.ycombinator.com/item?id=39356920 reply terpimost 18 hours agoprevOff topic: warpstream's calculator on the pricing page is pretty cool https://www.warpstream.com/pricing That breakdown switch is a lovely touch. reply necubi 15 hours agoprevThis is so, so cool. Basically the holy grail as a distributed systems engineer. Like the author, I've also avidly consumed every Jepsen report but the effort of actually implementing Jepsen tests for my systems always seemed too high. Very excited to see this technology democratized and made available to to more companies! reply mtremsal 17 hours agoprevThis is quickly becoming my favorite technical blog. Congrats Richie and Ryan. I didn't fully understand Antithesis the first time I ran into it; now it makes sense. reply zellyn 12 hours agoparentHey WarpStream folks… does your blog have an atom/rss feed? reply richieartoul 12 hours agorootparenthttps://www.warpstream.com/blog/rss.xml reply figassis 17 hours agoprev> Antithesis has created the holy grail for testing distributed systems: a bespoke hypervisor that deterministically simulates an entire set of Docker containers and injects faults, created by the same people who made FoundationDB. I remember the Antithesis founder was having a hard time explaining what exactly they did. reply mamidon 17 hours agoparentI remember that too, the ambiguity for me was how their fuzzing was good enough to explore an arbitrary state space efficiently enough. The deterministic hypervisor is 'simple' enough albeit a pretty heavy engineering lift. reply nlavezzo 16 hours agorootparentOne of the cool tricks we can use is that since the testing is all fully deterministic, once we find an interesting point in a test run - even if it is “deep” into the run time wise - our system can start many new branches of test runs off of that moment or moments just prior. So it is much more efficient than having to re-do the work to get to that rare interesting moment for each new branch. reply azurelake 15 hours agorootparentI’m curious if you’re willing and able to share: Are you using FoundationDB as the data store for Antithesis? reply nlavezzo 12 hours agorootparentWe’ll be writing a lot in the near future about how Antithesis works, stay tuned :) reply azurelake 12 hours agorootparentCan’t wait! reply Fomite 10 hours agoprevQuestion from another field that does a lot of simulation - why is the assertion that deterministic simulation testing, rather than something stochastic, is the gold standard. reply cpgxiii 8 hours agoparentFrom yet another field where deterministic simulation is often a goal (robotics), the ideal is a simulation test system that is deterministic for a given initialization (e.g. a random seed) so that for an initialization that causes some error to occur, you can reliably reproduce and resolve the error. Of course, you then need to run that system with a range of initializations to have confidence that you didn't just get lucky with the initialization. In practice, this can be quite hard to do in the presence of uncontrolled non-determinism (e.g. thread/process/GPU scheduling)* and it is often more pragmatic to invest the time in better stochastic testing and logging than deterministic reproduction. * Yes, these can be made closer to deterministic. But doing so often comes with reduced performance, such that the system you are testing would no longer match the system being deployed, defeating much of the purpose of the test in the first place. reply _dain_ 10 hours agoparentprev[ I work at Antithesis ] Concurrent/distributed system bugs can be really finicky because they may depend on subtle timing conditions to manifest. So you might see a bug once, then try to re-run the test using the \"same\" inputs, and the bug doesn't appear a second time. This might be because e.g. threads aren't scheduled the same way as before, so some 1-microsecond-wide window of vulnerability for a race condition was missed. If you can't reliably reproduce the bug, it's much harder to study and fix. Determinism lets you perfectly reproduce the bug as many times as you want. Perfectly as in, exactly the same thread+process scheduling, exact same memory and disk access times, exact same network packet transit times and orderings .. exact same everything. Then once you have returned to the bug, you can rewind time, to do things like explore counterfactual scenarios by varying the random seed from that moment on. We do have randomness of course, otherwise it wouldn't be a very good fuzzer. But we save all the seeds, so it's a controlled, reproducible randomness. reply fuzzy_biscuit 17 hours agoprevSlightly tangential, but when I went to go look at pricing information on mobile, the rates were clipped/overflowed out of bounds. reply oldstrangers 17 hours agoparentWoops, what are you device details? I'll take a look! reply ongy 17 hours agorootparentThe \"Fetch from follower\" button is slightly broken on my Pixel6 The breakdown one looks good, but the follower seems like the background got reduced width, but the active button is moving full width reply profstasiak 14 hours agoparentprevhopefully in year 2300 we can have good way to test landing pages reply eviks 4 hours agorootparentHopefully a century before someone would finally invent a front end framework with robust text styling! reply winrid 10 hours agoprevBTW, why base pricing on r4 instances vs something more cost effective like r5? reply richieartoul 9 hours agoparentI think I just followed the official recommendations I found (which are probably stale now). I'll update it to r5, but it doesn't really matter. The price difference between the two is like 5%, but hardware only ends up representing a tiny fraction of Kafka's cost at scale (the real cost comes from EBS and inter-zone networking). I could make the hardware free for Kafka in the comparison, and WarpStream would still come out significantly more cost effective. Cloud networking is really expensive. reply wolframhempel 16 hours agoprev [–] I've bookmarked it, just because the site is so pretty. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The PricingCompany blog post emphasizes the significance of deterministic simulation testing in software development, particularly in testing distributed systems like WarpStream using Antithesis.",
      "Antithesis is effective in bug detection, enhancing software quality, and offering valuable feedback to developers, ensuring the reliability and correctness of critical software.",
      "The post underscores the features of WarpStream as a Kafka compatible data streaming platform and highlights the advantages of deterministic simulation testing compared to traditional approaches like Jepsen."
    ],
    "commentSummary": [
      "Antithesis introduces a deterministic simulation testing tool for their SaaS platform, warpstream.com, enabling efficient testing of distributed systems.",
      "Developed by the team known for FoundationDB, the tool offers a reliable way to reproduce bugs in concurrent/distributed systems, simplifying issue identification and resolution.",
      "Users have highlighted the appealing design of the warpstream website, along with minor technical glitches encountered."
    ],
    "points": 190,
    "commentCount": 23,
    "retryCount": 0,
    "time": 1710256883
  },
  {
    "id": 39684472,
    "title": "Concerns rise over Dutch intelligence law",
    "originLink": "https://berthub.eu/articles/posts/dutch-intelligence-and-security-law/",
    "originBody": "On the new Dutch Intelligence and Security Law Posted on Mar 12 2024 “The Netherlands hosts some of the world’s largest internet exchanges. This obliges us to make the best use of these exchanges for our national security. With the Temporary Cyber Act, we will make optimum use of the data carried on our cables to protect The Netherlands against Russian and Chinese hackers” – Dutch government announcement I wrote this earlier, but did not publish it then because I wasn’t happy with the post. However, since the new Dutch Intelligence and Security act just passed, it is relevant for an international audience. Earlier, I wrote a piece for about:intel, and in there you can find further discussion in English. Of specific note, this law vastly expands when the Dutch agencies can perform SIGINT on Dutch Internet Exchanges like the Amsterdam Internet Exchange. These powers extend to any form of communication, including private peering and Private Network Interconnects (PNI). Every cable must be made available for ’exploration’, and it is likely that due to the wording in the new law, any request for such examination will be granted by the commission that rules on these things. Such exploration includes the right to send data to foreign intelligence agencies, including non-European ones. Secondly, the law also makes it clear that requests for full intercepts should now mostly be judged on technical terms (‘is the request complete’) and with much less emphasis of the merits of full SIGINT. But do read on for the details. Today, the upper house of Dutch parliament passed a law to reduce (I think) the oversight on the Dutch intelligence and security services, while simultaneously granting them more leeway to execute their powers. In 2022, I resigned as regulator of the Dutch intelligence and security services because of this attempt. Politico.eu wrote a comprehensive article about it, representing various points of view. In this page I will try to explain how the new law works and what these changes mean. I also provide some context from the European Court of Human Rights. This page refers to the just passed version of the proposed legislation. There is also a far longer page in Dutch. This post is an attempt to summarise the changes for an international audience. Applicability The Dutch Law on Intelligence and Security Services defines a broad set of powers. It is also a somewhat odd law with a long history. An English translation of a 2016 draft of the law can be found on the European Commission website. Note that this is a draft and it definitely differs from the law that is in force (Dutch). A remarkable aspect of the law is that it is universally applicable - it extends as much powers to spy on Dutch people as those from other countries. There are no general protections for people from or in The Netherlands or in the EU. This stands in stark contrast to US legislation which provides some privacy protection to ‘US Persons’, but to no one else. In addition, the powers can be used around the world - Dutch agencies are free to hack computers anywhere, or to intercept communications anywhere. Interests The services can investigate anything or anyone that gives rise to the suspicion that their activities or goals might be harmful for the survival of the system of democratic rule of law, for security or other important interests of the state. In addition, the services can investigate other countries. Powers The powers granted to the services are broad, but also largely ‘read-only’. The services do not operate drones (unlike the CIA), and they have limited powers to intervene or disturb things, with one exception. Article 40: Observing and following people and objects Article 41: Operate & instruct agents, who are allowed to break other laws under specific conditions Article 42: Enter & investigate closed objects and premises Article 43: Collect, store and match DNA data from people Article 44: Intercept and investigate physical post and packages Article 45: Hack ‘automated works’, computers in the very broadest sense of the word Article 47: Intercept any form of (tele)communications from a specific person, organization of technical identifier Article 48: Intercept & record whole cables Article 49: Investigate data recorded under article 48 to find targets or technical identifiers Article 50: Select part of the data recorded under article 48 for use in intelligence/security investigations Article 54: Requisitioning data Article 60: Perform automated analysis and “machine learning” on all data, including 48 data Article 73: “Undertake measures to protect interests of the services” Subjects of these powers The law is somewhat unique that all these powers are only tied to the interests the services protect, and not to specific people. So unlike police forces, the services can spy not only on direct targets of investigation, they can also eavesdrop ’non-targets’ (who might know interesting things about targets) or even hack ’third parties’ that could function as a stepping stone to useful information. Before the services can use their investigatory powers on non-targets or third parties, they do have to explain why this is the only way for an investigation to proceed. Oversight, warrant process For all of the powers, there must be internal sign-off. Depending on the nature, this sign-off can be mandated to lower ranking employees. The most infringing powers however need to be approved by the relevant minister. Then, the ex-ante regulator rules if this permission was obtained lawfully. The minister takes the political decision, the ex-ante regulator verifies this decision. The opinion of the ex-ante regulator is binding. In addition, there is a non-binding ex-post regulator. Administrative warrant extensions The powers are available for use on people or organizations. If someone turns out to have an additional phone number, or starts using a different phone number, a warrant can administratively be updated with this new information. No new permission process is required. In addition, if a warrant was written to target an organization, if this organization was described well, newly discovered members of the organization can also be added to a warrant administratively. Crucially, such administrative extensions can not be used to add non-targets or third parties - these can not have been a member of a well described organization. SIGINT The SIGINT or bulk interception powers are complicated. The intelligence and security law considers that the copying, collecting and storing of data is a lesser infringement than actually looking at the data. This is in line with recent European Court of Human Rights precedents. The infringement of privacy increases at every step of the intelligence process, from collection to storing to selecting relevant communications to exploiting such communications for intelligence/security purposes. Crucially however, merely intercepting and storing data is already an infringement that needs a justification. Article 48 provides for the collection powers. Services need to describe why a specific cable is interesting, and what they expect to achieve with that data. They must also use a well-targeted cable, one that is expected to best provide the data the services are interested in. There are also some restrictions on traffic with origin and destination in The Netherlands. Article 49 enables technical analysis of this data to see if it includes interesting targets or subjects. Article 50 meanwhile allows for the selection of communications for dissemination to intelligence teams. Finally, with specific permission, article 60 allows for the algorithmic analysis of all data intercepted under article 48. It was not found possible to properly limit what “algorithmic analysis” might be, so this was left to the ex-ante regulator to determine. In an adversarial reading, “finding all email from a specific person” might be dressed up as an algorithmic analysis. This means article 60 could be an alternative to article 50, and thus it needs the same level of regulatory scrutiny. Proposed changes In the proposed additional ‘cyber law’, several things would change: The non-binding ex-post regulator gets binding oversight powers on select capabilities. To actually stop an operation involves a heavyweight procedure which can also be stayed. In addition, the regulator needs a lot of new highly technical staff which has been hard to find. Hacking operations no longer need to detail plans or technical risks to the regulator. These risks include the leaking of ‘zero days’, or damaging non-target or third party services. In addition, technical risk includes the possibility that the facilities installed by the services could be abused by yet other services or hackers. Hacking operations no longer need to be tied to specific groups of people or organizations. It will become possible to hack into generic companies, for example to enable the retrieval of phone location data in the future. For such strategic hacks on infrastructure, no technical risks need to be described to the regulator. Services gain the power to intercept and store any cable for a year, no matter where it leads to, and also store the data for a year. No justification needs to be given for this exploration beyond that there is an interest in figuring out what traffic a cable carries. Data collected under this article may be investigated for a whole year, but (crucially!) not for any other purpose than finding targets. Such exploratory data can be shared with foreign and non-EU intelligence agencies. In addition, the article 48 powers can now be applied based solely on ‘indications’ what the technical plans are with this data. The regulators are instructed to mostly only study these indications, to the detriment of proportionality, subsidiarity and ‘as targeted as possible’ requirements. The indications are non-binding and the services have the liberty to do other things during the year if they need to. In effect, regulators do not have a lot left to regulate with this change. The administrative permissionless extension of warrants is extended to NON-TARGETS. This means that if a hacker group is being targeted, warrants for eavesdropping, hacking, requisitioning are automatically extended to the victims of those hackers as well. This means that the additional test that used to apply to non-targets is no longer required, and instead, non-targets now get less protection than targets (!). Algorithmic analysis on bulk intercepted data no longer needs regulatory approval. Since it is not well described what this automated analysis entails, this is a large expansion of powers that are not regulated up front. The European Court of Human Rights argues explicitly that such automated analysis needs safeguards (see below) Some context CASE OF BIG BROTHER WATCH AND OTHERS v. THE UNITED KINGDOM - with particular attention to: “330. The Court considers that Article 8 applies at each of the above stages. While the initial interception followed by the immediate discarding of parts of the communications does not constitute a particularly significant interference, the degree of interference with individuals’ Article 8 rights will increase as the bulk interception process progresses. In this regard, the Court has clearly stated that even the mere storing of data relating to the private life of an individual amounts to an interference within the meaning of Article 8 (see Leander v. Sweden, 26 March 1987, § 48, Series A no. 116), and that the need for safeguards will be all the greater where the protection of personal data undergoing automatic processing is concerned (see S. and Marper, cited above, § 103). The fact that the stored material is in coded form, intelligible only with the use of computer technology and capable of being interpreted only by a limited number of persons, can have no bearing on that finding (see Amann v. Switzerland [GC], no. 27798/95, § 69, ECHR 2000‑II and S. and Marper, cited above, §§ 67 and 75).” Tags: legal, intelligence, national security",
    "commentLink": "https://news.ycombinator.com/item?id=39684472",
    "commentBody": "On the new Dutch intelligence and security law (berthub.eu)187 points by ahubert 13 hours agohidepastfavorite77 comments dang 12 hours agoThe submitted title (\"Dutch gov. proudly announces it will tap Europe's largest internet exchanges\") badly broke HN's title guideline, which asks: \"Please use the original title, unless it is misleading or linkbait; don't editorialize.\" - https://news.ycombinator.com/newsguidelines.html If you want to say what you think is important about an article, that's fine, but do it by adding a comment to the thread. Then your view will be on a level playing field with everyone else's: https://hn.algolia.com/?dateRange=all&page=0&prefix=false&so... reply Tknl 9 hours agoprevDon't forget Dutch citizens voted previously in a national referendum against mass surveillance which was promptly ignored and the right to call a referendum repealed. reply dotBen 13 hours agoprevAren't all governments tapping connectivity the occurs within their borders (whether it be in internet exchanges, sea cables that come onto their shores, etc - kinda doesn't matter where it physically happens)? I assume this is going on routinely already. reply bradley13 12 hours agoparentPerhaps it is. That doesn't mean it should be. Governments think they are above their own laws. Warrants? Privacy rights? Due process? Why bother? reply wkat4242 11 hours agorootparentWell in this case they did amend the law through parliament. Not that I agree with it, no, but it's not like they're acting above it. reply throwaway11460 6 hours agorootparentThat doesn't make it automatically right, just, or even legal. Does it hold up against the European Convention on Human Rights, EU directives, regulations and the country constitution? What does the constitutional court, EU court of justice and EU court of human rights think? These are not hypothetical questions. I'm not Dutch but in my own (EU) country it's a very common occurrence that the higher courts significantly modify or entirely cancel a fully approved law - often retroactively. Sometimes the state has to pay out damages. IMHO a parliament acting like they can just vote for anything and that's it is exactly the definition of acting above the law. reply wkat4242 1 hour agorootparent> That doesn't make it automatically right, just, or even legal. Does it hold up against the European Convention on Human Rights, EU directives, regulations and the country constitution? What does the constitutional court, EU court of justice and EU court of human rights think? It should, it's the job of the first chamber (aka the 'senate') to validate this. Unfortunately they have been playing politics more than anything. > I'm not Dutch but in my own (EU) country it's a very common occurrence that the higher courts significantly modify or entirely cancel a fully approved law - often retroactively. This sounds more like the common law system (US, UK, Ireland). In Holland it's not like that. The senate is supposed to check that an in fact a local judge can't directly reference the constitution. In common law they can and they create precedents to scope out a law further after implementation. The EU does overrule it of course. And yes perhaps they can get fined damages. That would be good IMO because it will stop them doing it. But they didn't do anything wrong technically in that sense. The worst thing they did technically was that this law was inplemented by a cabinet that had already stepped down after the coalition fell apart. New votes were held and a new parliament was formed, but the old cabinet is still in place until a coalition is formed to create the new cabinet. The biggest problem there was that 24% of people voted for the extreme-right fascist party and nobody except the neoliberals (the party they split out from) and the farmers want to form a government with them. So it will take a lot of time. But the old (neoliberal) cabinet is not supposed to push through any legislation that can be considered controversial. They are doing that all the time though. reply throwaway11460 1 hour agorootparentNo, in my country it's a derived German-style system. It's very unlike a common law system. A local judge can't, but we have a constitutional court who can take up a case against a passed law by themselves. Also, you can work through the tiers of the courts to the highest court or submit a request to the constitutional court. Not sure about the exact legal theory in the Netherlands but where I am it's not really the job of upper chamber to validate it. Both chambers should have done that but both are bodies with political agendas and they interpret it to their own liking - and that often doesn't pass through the courts. reply jillesvangurp 12 hours agoparentprevYou should assume so and accept nothing short of end to end encryption as a secure channel. That's been true for decades. Assume, the Chinese, Russians, North Koreans, Iranians, Americans, and everybody else gets a copy of all the bytes you send and receive. That may or may not be true depending on who or where you are and how competent their people are. But you can't rely on that not being the case so you simply shouldn't. So make sure that whatever they intercept is gibberish. Is there any unencrypted traffic over these cables at all at this point? It's all ssl and https at this point, I would hope. There's still some intelligence to be extracted from which IP addresses are talking to which other IP addresses. But beyond that? What's really there to be intercepted that we haven't fixed yet? reply divbzero 13 hours agoprevGiven the prevalence of TLS, how much SIGINT can actually be done by tapping internet exchanges these days? reply vmoore 13 hours agoparentOther metadata like DNS, device fingerprints, SNI-leakage[0], timestamps, connection history, etc You can encrypt DNS with DoH if you want, but the DoH provider still sees its you. You can take it a step further with Oblivious DNS over HTTPS if you really want to conceal DNS activity[1]. Note: this technology is rather new and experimental. [0] https://en.wikipedia.org/wiki/Server_Name_Indication [1] https://research.cloudflare.com/projects/network-privacy/odn... reply varenc 11 hours agorootparentAnother option is dnscrypt-proxy[0]. It will easily let you load-balance your DNS queries against a large set of resolvers, ensuring that no resolver gets the full picture. And enforces encryption of course. [0] https://github.com/DNSCrypt/dnscrypt-proxy reply jrockway 12 hours agoparentprevIn addition to what's mentioned in the comments, timing attacks are also possible. If you see what time every packet is sent and received, then you can correlate streams with each other. This is how they figure out who is visiting what Tor websites; you compromise the network of the website, then the network of potential clients, and then you match up the packets. Now they know you visited the website even though you never actually sent a packet addressed to it. reply repelsteeltje 13 hours agoparentprevStart with phone lines. Or IP addresses. Even without looking into the encrypted payload there is so much you can learn from graphs connecting and the metadata. reply mk89 13 hours agoparentprevConsidering they exchange technology with the USA and other states, I am pretty sure that you can find some malware to install on specific actors's devices (routers/phones/etc.) to have traffic decrypted. I am decently sure that some state agencies help design routers.... if you know what I mean :) reply jasonvorhe 12 hours agoparentprevJust being able to analyze the network connections to apply filters and visualizations upon, providing optional deep dives into certain cohorts to create reports or even forecasts is a totalitarian wet dream. reply gigel82 13 hours agoparentprevPresumably a government could also requisition the private key of several root authority certificates (whether or not they \"proudly announce\" that is another matter). reply repelsteeltje 12 hours agorootparentDutch government doesn't have a good reputation there, shepherding control over security infrastructure. First, it's primary CA lost is signing key to Iran, and recently they meant to outsource control over their \".nl\" TLD to AWS. reply mrngm 11 hours agorootparentSmall addendum on that, DigiNotar was one of the four CA's handing out \"PKIoverheid\" certificates, so certificates for governmental purposes. See this archived copy of the FAQ (in Dutch) after the DigiNotar breach, specifically the question \"Hoe weet de overheid dat certificaten van de 3 andere bedrijven in Nederland die PKI-overheidscertificaten uitgeven wel betrouwbaar zijn?\": https://web.archive.org/web/20111019224308/http://www.rijkso... reply amenghra 12 hours agorootparentprevA root CA key doesn't automatically decrypt the TLS traffic. You just need a single root CA key for a widely trusted CA to perform an active MITM attack. The attack is however likely to show up in Certificate Transparency logs. reply ahubert 13 hours agoprev(happy to elaborate if there are any questions) reply repelsteeltje 13 hours agoparentSo, Dutch intelligence basically has legal leeway to pry anywhere. But I'm curious how they would hand over some \"fact\" to Police whom cannot normally access that information without some kind of warrant or legal approval from public prosecution. How does that work in practice (under Dutch law)? reply ahubert 12 hours agorootparentIf they find something that is really criminal they can notify the police, but it is not straightforward. Search for \"ambtsbericht\". reply gpvos 12 hours agorootparentprevThere are ways to inform the police, but they need to do \"parallel construction\" to allow information in court. reply twsted 12 hours agoparentprevIsn't there any European law that could stop this exaggerated and self-granted power? reply ahubert 12 hours agorootparentYes, parts of this law are very likely to be struck down by the European Court of Human Rights, if a case ever gets there. Specifically the 100% automatic powers to hack and intercept anyone who is hacked by state backed hackers are pretty unlikely to be legal under the ECHR. reply mk89 12 hours agorootparentThere is literally nothing that can win against national security as \"state actors (Russia, China, even mentioned in the text) trying to sabotage your infrastructure - look we have evidence but we're not gonna share it with the public\". reply wolverine876 12 hours agorootparentDo you mean that's a specific legal argument that is upheld in European courts or Dutch courts? reply mk89 12 hours agorootparentNo, I would say that's just life experience until now. Nothing ever wins against \"we need to keep the country safe\". reply vmoore 13 hours agoprev> to protect The Netherlands against Russian and Chinese hackers So it's a noble cause then? Or does it have privacy implications for innocent netizens? I thought these exchanges would have been tapped in some form way before this announcement? reply lionkor 2 hours agoparentI wanna be noble, too! Where's the form to sign up as a proxy for some war? reply sspiff 12 hours agoparentprevThe new law also lowers the bar for Dutch law enforcement to obtain records from these taps significantly, removing the necessity of a judge to rule whether the request is justified in the investigation. Surely no law enforcement would overreach when given tools like these, right? Right? reply davedx 12 hours agorootparentYeah the Belastingdienst would never do anything naughty with powers like these. Let them in on it too! reply Notatheist 11 hours agorootparentprevThey've already overreached in the past. https://nos.nl/artikel/2432715-inlichtingendiensten-moeten-g... https://www.rtlnieuws.nl/tech/artikel/5294998/aftappen-aivd-... reply coldtea 10 hours agoparentprev>So it's a noble cause then? If you consider \"to play its part in the trade war between US-China which is extending into real WW 3\" as noble, then yes, it's noble. reply Notatheist 13 hours agoprevI'm a technical artist not a security researcher so could someone here elaborate on the supposed threat? Assuming a genuine Russian/Chinese state hacker/outfit, what damage could they cause and how much of that damage would legislation such as this prevent? reply mk89 12 hours agoparent> what damage could they cause You want to listen to what they want to do before they do something to your country. This is what this thing allows you to do: every internet packet transiting through the Dutch internet exchanges will be \"scanned\" (largely read-only). However: > The powers granted to the services are broad, but also largely ‘read-only’. Largely `read-only`, the way I read it, means that in some cases they can actually replace whatever is going through the cable. I imagine something like: - terrorist A and B are texting each others, and you replace some of the text that they are sending each other (before this is received over the phone, because you own the \"hop\"), so that you can maybe redirect them straight into the police hands. If done properly, I believe this can prevent quite some bad damage - not the simple example above, but probably also major things like serious attacks (e.g., ransom attacks on public institutions, etc). That's my guess - how easy or realistic this is, I can't tell you. reply dylan604 13 hours agoprevAs long as they are proud of it. No bad laws have ever been implemented where everyone was proud of it, so this must mean it's not a bad law!! And the people rejoiced...just not on the internet as they didn't want to be spied on reply FirmwareBurner 12 hours agoparent>No bad laws have ever been implemented where everyone was proud of it \"Nobody who speaks German can ever be evil\" - The Simpsons reply gpvos 12 hours agoparentprevHow do you know everyone is proud of it? For starters, I'm not. reply tdudhhu 12 hours agoprevThis is a 'temporary' law that will be reconsidered after 4 years (most of the time this means it will just be continued). The CTIVD will have supervision during and after the tab (good). Private data can be held much longer without government approval (why?). There is no permission needed to tap another server when a party, that is under surveillance, is moving there. --- I have mixed feelings about this. We know Russia is trying to disrupt the Netherlands because of previous taps. So on one hand it is good that the government can quickly react to such threads. On the other hand it has huge privacy implications. Some people in this thread think that TLS will keep us private but that is not how it works when they can listen to all traffic. For example they can see I posted a request to Hacker News on a specific time. Then it is a matter of finding all posts that were made around that timestamp to see what I wrote and what my username is. reply mk89 12 hours agoparent> For example they can see I posted a request to Hacker News on a specific time. Then it is a matter of finding all posts that were made around that timestamp to see what I wrote and what my username is. I think this is a lot of work to do. Just ask some 3 letters agencies for some help on some malware or on some \"router firmware bugs\" to be exploited etc. They don't care about hacker news readers or posting comments (because this implies you must know the DNS first, etc). They care about botnets DDoS-ing your railway infrastructure, ransomware on hospitals, serious stuff that can lead the country to chaos. reply satellite2 10 hours agorootparentOn hn everything is public so if you only have the time at which two or three posts where submitted you can find the user in question easily. reply ozim 12 hours agoparentprevI am sure it is there to stay as no other thing is going to stay forever as much as “temporary” law giving authorities more power. reply freedomben 13 hours agoprevIt's called \"temporary\" but I don't see anything about when it is removed. Even with a sunset period (like the US Patriot Act had) it's not typical for states to give up power like this, so I'm guessing this is the new normal :-( Is there specific intelligence leading to this? It seems very to the point about being related to Russia. reply fallingknife 12 hours agoparentThe Patriot Act was temporary too... reply shrimp_emoji 12 hours agorootparentAnd some provisions expired thanks to Trump! Biden, on the other hand, renews them promptly, to bipartisan satisfaction. Funny how that works. :p reply molticrystal 12 hours agoprevJust how badly is the internet compromised at this point, as in are there any countries or internet corporate policies that forbid contributing to this type of action and would route around the Netherlands due to it violating privacy? reply radicalbyte 13 hours agoprevThis is happening at exactly the same time that an extremist far-right party are expected to take power. Fantastic. reply mk89 13 hours agoparentDon't worry, as long as we don't militarise ourselves anymore.... oh shoot, that's also happening! reply FirmwareBurner 12 hours agorootparentOh no, an army on bikes, what are we gonna do about it?! /s reply Arrath 9 hours agorootparentLose Singapore? reply repelsteeltje 13 hours agoprevTaking the perspective that spying is warranted by the asset under threat rather than subject potentially posing it ... that is some pretty scary piece of legislation. reply ls612 13 hours agoprevHow much cleartext is sent over the net? Nowadays almost everything is encrypted, even things like DoH are becoming standard via Cloudflare/Apple Private Relay. reply sva_ 12 hours agoparentI'm not a crypto expert by any means, but it is my understanding that government actors of larger countries probably have trusted CAs in most devices on the planet that they have control over, and they could issue certificates for arbitrary domain names; and your devices would for the most part be none the wiser. Of course this is only relevant for targeted surveillance, not mass surveillance. Happy to be corrected though, I've been wondering about this. reply mrngm 11 hours agorootparentPerhaps think a bit smaller, and look at companies that operate office-sized TLS interception by installing a CA certificate on each (managed) end device, and generating certificates on-the-fly from that CA. Then, some middlebox is able to inspect the encrypted communication that passes through. Some web browsers have pinned certificates for certain services, Google Chrome/Chromium being one of those. Subsequently, the browser refuses to perform any more actions towards the server that serves an invalid (according to the browser) certificate. That browser is also one of the reasons the DigiNotar case in 2011 emerged to the surface. reply ls612 12 hours agorootparentprevThis is precisely the threat model that certificate transparency mitigates. reply mrngm 12 hours agoparentprevPerhaps a better insight is looking at parties doing TLS termination globally as a reverse proxy service, for example for DDoS mitigation or acting as a web application firewall. Or, who handles your (encrypted) DNS requests? A somewhat trustworthy local ISP, or a large corporation with multiple pages of terms of service and a vague privacy statement? (I'm unfortunately also aware that there exist ISPs that inject ads on non-encrypted connections, or having done so in the past) In my opinion, it's less about the amount of cleartext traffic, but also about what parties terminate your so preciously encrypted connections/requests, the percentage of internet traffic they handle, and what they exactly store about those requests. Encryption on the internet doesn't mean it's suddenly safe. reply jrockway 12 hours agoparentprevIf I were a politician using this against an opponent, I'd write the following press release: Websites in the range 54.239.0.0/8 often host problematic content. My opponent visited several addresses in this range overnight and hid his traffic with military-grade message scrambling functionality. Of course that's just AWS and you can't even do HTTP/2 or HTTP/3 without encryption. But do the voters know that? Will they be educated on it? Probably not. And you're not saying anything untrue, you have facts and logs to back up your assertions! reply xk_id 12 hours agoparentprevThe majority of traffic is available as cleartext to Cloudflare so that they can analyse it. That’s the point of being an enormous centralised TLS termination proxy. reply ipaddr 12 hours agoparentprevThey all work for the same team reply unethical_ban 12 hours agoparentprevStore now, decrypt later? Metadata analysis on netflow, source and destination traffic, etc. Most people still don't use VPNs for everything, and some services outright block or degrade VPN connectivity. Two notable examples are most banks, and 4chan. I firmly believe 4ch is a Honeypot. reply GauntletWizard 12 hours agoprevTo some extent, this is a gross misuse of their government power. To another extent, tap away. There is no reason for your network traffic not to be an end-to-end encrypted. I'm more or less fine with the government getting my SNI headers, though I will be investing more in Tor and other obfuscation for some purposes. reply sph 13 hours agoprev> With the Temporary Cyber Act, we will make optimum use of the data carried on our cables to protect The Netherlands against Russian and Chinese hackers Twenty years ago, the excuse was \"we are restricting your freedom because of what happened in 11/9.\" Then in the internet age, the excuse became \"we are restricting your freedom to save the children from online abuse.\" Now, Europe has unlocked the option to restrict its citizens' freedom because of the \"war.\" I ask my fellow computer engineers what the hell are we waiting for to pool our minds and resources towards a truly decentralised, encrypted and anonymous overnet. Something a little more practical than I2P, Tor, Freenet, etc. \"Oh bad people will use it to do crime\" is not a serious enough excuse to just passively accept the government tightening the noose around our digital presence, for total control by the State, all in name of safety and security. Was Bitcoin (2009) the last hurrah of the crypto-anarchist ideals of freedom of thought, freedom from the Big Brother and freedom from the ever-looming State? https://groups.csail.mit.edu/mac/classes/6.805/articles/cryp... reply zx10rse 12 hours agoparentIt won't happen people choose comfort instead of freedom. Eventually it will become a full blown totalitarian state, and we are going to relive 20th century again it seems. I can't wait to pay ESG tax because I am breathing. reply sva_ 12 hours agoparentprev> I ask my fellow computer engineers what the hell are we waiting for to pool our minds and resources towards a truly decentralised, encrypted and anonymous overnet. I often hear this somewhat loose idea and have to say that I also have such reoccurring thoughts myself. We currently enjoy pretty great freedoms to do stuff, we have the chance to set something up, and those freedoms might be severely restricted in the future. It may be easier to develop something like that now than it will ever be in the future. The problem seems to be that there is no coordinated plan. Stuff like Tor exists, but it hasn't really caught on as much as one would hope. Also it is built upon a rather specific architecture (the internet) from what I understand, which could basically be shutdown by authorities at any moment. For example, I can't currently safely communicate with people in my geographic area independently of the internet, even though my devices theoretically have the hardware to do so (think of radio capabilities, for example). There might be some lose projects out there capable of some of it that 99.9999% never heard about, but nothing that could actually be considered general-purpose. Why is that? It isn't an easy problem to solve. And it isn't enough of a problem for people to actually apply themselves nearly enough. Once it is needed in a way that more people would devote their time to it, it might be too late. reply Eager 13 hours agoparentprevI used to be so optimistic and less cynical. Now I look at this and all that comes to mind is that actions like this would provide a basis for tapping people closer to source. All the p2p and e2e encryption in the world won't help if someone is reading every key you type, or in the near future, every thought that crosses your mind. Still, I applaud your spirit. reply sib 13 hours agoprevWhen was the last time that something titled a \"Temporary Cyber Act\" was ever temporary (other than being replaced by something worse)? reply peeters 13 hours agoparentIt needs more \"Democratic\" in it to make it clear it's definitely not authoritarian. Maybe \"The Democratic People's Temporary Cyber Act for Freedom\" reply klabb3 13 hours agoparentprev“Nothing is so permanent as a temporary government program” – Milton Friedman reply ahubert 13 hours agoparentprevIt has already been announced that this law will likely be extended. They later walked that back, but did admit that it is entirely possible to extend this law. reply givemeethekeys 13 hours agoparentprevHey, you have to applaud them for naming it \"temporary\", instead of \"patriot\". What are you? Some kind of a treasonous terrorist? How dare you oppose THE PATRIOT ACT?! /s reply nsteel 11 hours agorootparentThey get zero marks because it doesn't appear to be an acronym. reply HarHarVeryFunny 13 hours agoprev [5 more] [flagged] sph 13 hours agoparentLet's try for once not to divert and sidetrack any world news topic with what the US is doing, please? Not everything should be an opportunity to talk about YOUR country. reply HarHarVeryFunny 12 hours agorootparentOK, but that wasn't my intent. I really meant who are we (the US) to criticize the Netherlands when we are doing the same ourselves (and not even fully admitting it). But, I can see that if you are from the Netherlands yourself, then you may very much want to - and have every right to - criticize it! Sorry. reply ethanbond 13 hours agoparentprev [–] Now define “something suspect” reply dylan604 13 hours agorootparent [–] You'll know it when you see it. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Dutch Intelligence and Security Law boosts Dutch intelligence agencies' authority to conduct Signals Intelligence (SIGINT) on local internet exchanges, permitting data interception and retention without precise reasoning.",
      "It provides extensive investigative abilities for monitoring individuals or entities, monitored by both internal and external regulatory entities, and proposes broadening hacking activities and data gathering.",
      "Concerns arise over privacy rights and potential limitations on personal freedoms due to the expanded warrants even on non-targets."
    ],
    "commentSummary": [
      "Concerns have arisen over a new Dutch intelligence and security law, raising worries about government surveillance, privacy breaches, and excessive authority.",
      "Debate surrounds the necessity of surveillance powers for national security versus their impact on privacy and personal liberties, highlighting the importance of secure communication like end-to-end encryption.",
      "The discussion includes the potential influence of European courts on elements of the law concerning hacking and interception abilities, emphasizing the need to balance security measures with privacy rights and safeguard democratic values in cybersecurity efforts."
    ],
    "points": 187,
    "commentCount": 77,
    "retryCount": 0,
    "time": 1710275088
  },
  {
    "id": 39682143,
    "title": "Swift Gaming on Playdate: Enhancing C API for Efficiency",
    "originLink": "https://www.swift.org/blog/byte-sized-swift-tiny-games-playdate/",
    "originBody": "Byte-sized Swift: Building Tiny Games for the Playdate March 12, 2024 Rauhul Varma Rauhul Varma works on Advanced Prototyping in the Platform Architecture group at Apple. I’m excited to share swift-playdate-examples, a technical demonstration of using Swift to build games for Playdate, a handheld game system by Panic. Why Swift? Swift is widely known as the modern language for app development on Apple devices. However, over the course of its first decade, it has grown into a versatile, multi-platform language targeting use cases where you’d otherwise find C or C++. Personally, I have come to appreciate Swift’s emphasis on memory safety and great ergonomics, and want these traits for embedded systems where reliability and security are critically important. But embedded systems are not only found in mission-critical applications. Some are actually all fun and games. Playdate by Panic Over the holiday season, I read about building Playdate games in C and became curious if the same was possible in Swift. For those unfamiliar with Playdate, it is a tiny handheld game system built by Panic, creators of popular apps and games like “Transmit,” “Nova,” “Firewatch,” “Untitled Goose Game,” and more. It houses a Cortex M7 processor, a 400 by 240 1-bit display, and has a small runtime for hosting games. Additionally, Panic provides an SDK for building games in both C and Lua. While most Playdate games are written in Lua for ease of development, they can run into performance problems that necessitate the added complexity of using C. Swift’s combination of high-level ergonomics with low-level performance, as well as its strong support for interoperating with C, make it seem like a good match for the Playdate. However, the typical Swift application and runtime exceed the device’s tight resource constraints. Regardless, I still wanted to create a game in Swift and I had a good idea for the approach. The Embedded Language Mode Recently, the Swift project began developing a new embedded language mode to support highly constrained platforms. This mode utilizes generic specialization, inlining, and dead code stripping to produce tiny binaries, while retaining the core features of Swift. Note: The embedded language mode is actively evolving and is helping drive the development of language features such as: non-copyable types, typed throws, etc. It is available now in nightly toolchains and if you’re curious to learn more, check out the Vision for Embedded Swift. These defining characteristics make the embedded language mode a great solution for shrinking Swift to fit the Playdate’s constraints. With the embedded Swift language mode in hand, I got to work. The Games I wrote two small games in Swift for the Playdate. The first game is a port of the Playdate SDK sample of Conway’s Game of Life into Swift: This game is one Swift file that builds directly against the Playdate C API and does not require dynamic memory allocation. The packaged game clocks in at 788 bytes, slightly smaller than the C example, which is 904 bytes. $ wc -c ? = nil $ mkdir build $ swiftc \\ -c test.swift \\ -o build/test.o \\ -Osize -wmo -enable-experimental-feature Embedded \\ -Xcc -I -Xcc $HOME/Developer/PlaydateSDK/C_API/ \\ -Xcc -DTARGET_EXTENSION \\ -I $HOME/Developer/PlaydateSDK/C_API/ $ file build/test.o test.o: Mach-O 64-bit object arm64 Running on the Simulator Once I was able to use the Playdate C API from Swift, I ported the Conway’s Game of Life example included in the Playdate SDK to Swift, referencing Inside Playdate with C frequently to familiarize myself with the API. The C implementation of Conway’s strictly operates on Playdate OS-vended frame buffers and uses the display as game state, removing the need for separate data structures and dynamic allocations. As a result, the porting process was very mechanical because the bit manipulation and pointer operations in the C example have direct Swift analogs: static inline int val(uint8_t* row, int x) { return 1 - ((row[x/8] >> (7 - (x%8))) & 1); } static inline int ison(uint8_t* row, int x) { return !(row[x/8] & (0x80 >> (x%8))); } struct Row { var buffer: UnsafeMutablePointer func value(at column: Int32) -> UInt8 { isOn(at: column) ? 1 : 0 } func isOn(at column: Int32) -> Bool { let byte = buffer[Int(column / 8)] let bitPosition = 0x80 >> (column % 8) return (byte & bitPosition) == 0 } } I built the source into a dynamic library and used pdc (the Playdate compiler) to wrap the final dylib into a pdx (Playdate executable). $ swiftc \\ -emit-library test.swift \\ -o build/pdex.dylib \\ ... $ file build/pdex.dylib pdex.dylib: Mach-O 64-bit dynamically linked shared library arm64 $ $HOME/Developer/PlaydateSDK/bin/pdc build Test $ ls Test.pdx pdex.dylib pdxinfo I opened my game file Test.pdx using the Playdate simulator and as you might expect, it worked on the first try … just kidding, it crashed! After some debugging, I realized the Makefile used to compile the C example included an additional file setup.c from the SDK containing the symbol _eventHandlerShim needed to bootstrap the game. If this symbol is not present in the binary, the Simulator falls back to bootstrapping the game using the symbol _eventHandler which my binary did contain, but meant my game skipped an important setup step. So, I compiled setup.c into an object file using clang, linked it into my dynamic library, re-ran, and voila! I had Conway’s Game of Life written in Swift running on the Playdate Simulator. Running on the Hardware After successfully running on the simulator, I wanted to run the game on real hardware. A colleague graciously allowed me to borrow their Playdate and I began hacking away. I started by matching the triple used by the C examples for the device and seeing what happened. $ swiftc ... -target armv7em-none-none-eabi :1:10: note: in file included from :1: #include \"pd_api.h\" ^ $HOME/Developer/PlaydateSDK/C_API/pd_api.h:13:10: error: 'stdlib.h' file not found #include^ These errors did not previously occur because I was targeting the host machine and used the host headers for the C standard library. I considered using the same host headers for the target device, but didn’t want to debug platform incompatibilities. Little did I know, I would have to do this regardless. Instead, I decided to follow the route used by the C example programs which leverage the libc headers from a gcc toolchain installed with the Playdate SDK. I copied the include paths used by the C examples and re-ran the compile. $ mkdir build $ GCC_LIB=/usr/local/playdate/gcc-arm-none-eabi-9-2019-q4-major/lib $ swiftc \\ -c test.swift \\ -o build/test.o \\ -target armv7em-none-none-eabi \\ -Osize -wmo -enable-experimental-feature Embedded \\ -I $HOME/Developer/PlaydateSDK/C_API/ \\ -Xcc -DTARGET_EXTENSION \\ -Xcc -I -Xcc $HOME/Developer/PlaydateSDK/C_API/ \\ -Xcc -I -Xcc $GCC_LIB/gcc/arm-none-eabi/9.2.1/include \\ -Xcc -I -Xcc $GCC_LIB/gcc/arm-none-eabi/9.2.1/include-fixed \\ -Xcc -I -Xcc $GCC_LIB/gcc/arm-none-eabi/9.2.1/../../../../arm-none-eabi/include $ file build/test.o test.o: ELF 32-bit LSB relocatable, ARM, EABI5 version 1 (SYSV), not stripped The compile succeeded and I had an object file for the real hardware. I went through similar steps to link and package the object file into a pdx, using clang as the linker driver. I deployed the game onto a Playdate, and … it crashed. For some reason, when the frame-update function pointer was called, the game would crash! Debugging this issue was confusing at first, but due to past experience deploying Swift onto a Cortex M7, I realized I likely had a calling convention mismatch. I added a compiler flag -Xfrontend -experimental-platform-c-calling-convention=arm_aapcs_vfp to try to match the calling convention used by the Playdate OS. Once again, I deployed my game to the Playdate and … it actually worked! You can see the game in action below: I then integrated my manual compilation steps into the Makefiles found in the Playdate SDK, going through a number of options before landing on the final solution found in swift-playdate-examples. The result of this effort was a single make command to build a pdx compatible with both the simulator and hardware! Improving the API with Swift After porting Conway’s Game of Life, I began a more ambitious project: a paddle-and-ball style game named Swift Break. However, I quickly encountered friction using the raw Playdate C API directly in Swift. In typical programming fashion, I took a detour to work on the API’s ergonomics instead of the game! At this point, I had also piqued the interest of some colleagues who contributed further improvements. One hurdle was the naming conventions of the imported API. In C, enum cases are often prefixed with their type’s name to prevent programmers from inadvertently mixing unrelated enum instances and case constants. However, in Swift, such prefixes are unnecessary as the compiler type-checks comparisons to ensure the correct cases are used. Fortunately, Swift already provides tools for addressing this precise issue, known as API notes. I added an API notes file to the Playdate SDK and renamed enum cases with more idiomatic Swift names: // Before if event == kEventPause { ... } // After if event == .pause { ... } A bigger issue, however, was the lack of nullability annotations in the C API. This meant the generated code had redundant null checks everywhere, bloating code size and hurting performance. While I usually would have used API notes to add the missing annotations, this was not possible. The C API uses structs with function pointers as a “vtable”, and unfortunately, these are not currently modifiable with API notes. Due to this incompatibility, I had to adopt a suboptimal solution: pervasively using Optional.unsafelyUnwrapped throughout the Swift code. Although this approach eliminated the null checks, it dramatically hurt readability: // C API in Swift with redundant null checks let spritePointer = playdate_api.pointee.sprite.pointee.newSprite() // C API in Swift without redundant null checks let spritePointer = playdate_api.unsafelyUnwrapped.pointee.sprite.unsafelyUnwrapped.pointee.newSprite.unsafelyUnwrapped() To address the readability issues, I created a thin Swift overlay on top of the C API. I wrapped function pointer accesses into static and instance methods on Swift types and converted function get/set pairs to Swift properties. Creating a sprite became much more intuitive and introduced zero overhead on top of the equivalent imported C calls. var sprite = Sprite(bitmapPath: \"background.png\") sprite.collisionsEnabled = false sprite.zIndex = 0 sprite.addSprite() Colleagues further improved the overlay by abstracting Playdate APIs requiring manual memory management to be automatically handled. An excellent example is the C API’s moveWithCollisions function, which returns a buffer of SpriteCollisionInfo structs that must be freed by the caller. Using the overlay allowed us to avoid manually deallocating the buffer and made the API easier to use: // moveWithCollisions without the overlay var count: Int32 = 0 var actualX: Int32 = 0 var actualY: Int32 = 0 let collisionsStartAddress = playdate_api.pointee.sprite.pointee.moveWithCollisions(sprite, 10, 10, &actualX, &actualY, &count) let collisions = UnsafeBufferPointer(start: collisionsStartAddress, count: count) defer { collisions.deallocate() } for collision in collisions { ... } // moveWithCollisions with the overlay sprite.moveWithCollisions(goalX: 10, goalY: 10) { actualX, actualY, collisions in for collision in collisions { ... } } These improvements dramatically streamlined writing code for the Playdate. Additionally, as Swift’s support for ownership and non-copyable types improves, I anticipate even more ergonomic representations of the C APIs without language overhead. Completing Swift Break Equipped with a refined Swift Playdate API, I returned to developing Swift Break. I nailed down the basics pretty quickly, but couldn’t resist adding extra features just for the fun of it. One of the highlights was implementing basic logic to deflect ball bounces based on the location where the ball and paddle collide. This feature required calculating a normal vector relative to a hypothetical curve representing a rounded paddle and then reflecting the ball’s velocity about the normal. Here’s a visualization of the intended behavior: Note: Making the animation for this post ironically helped me root cause a bug in the bouncing logic. Under some combinations of entry angle and normal angle, the current design can cause the ball to bounce down into the paddle instead of up. To turn the math into an algorithm, I had to perform the following steps: Check if the object the ball collided with is the paddle Compute the location of the collision along the paddle from -1 to +1 Map the location into a deflection angle from from -π/6 to +π/6 Rotate the collision normal vector by the deflection angle Reflect the ball’s velocity along the rotated normal I then directly translated this algorithm into code inside the ball collision callback: if otherSprite.tag == .paddle { // 1 let placement = placement(of: collision, along: otherSprite) // 2 let deflectionAngle = placement * (.pi / 6) // 3 normal.rotate(by: deflectionAngle) // 4 } ballVelocity.reflect(along: normal) // 5 Running on the Hardware (Again!) Throughout the development of “Swift Break,” I regularly deployed the game to the Playdate Simulator. However, the real challenge emerged when I decided to run the game on actual Playdate hardware. As usual, I loaded the game, and … yet again, it crashed, but this time a lot of things were going wrong. To cut a long debugging story short, I found that the -Xfrontend flag mentioned earlier did not entirely resolve the calling convention issues. To address this, I needed to configure the compiler to match the CPU and floating-point ABI of the microcontroller in the Playdate. This aspect was overlooked when I was porting Conway’s Game of Life since I happened to both not pass structs by value and didn’t use floating-point operations. The final and most confusing crash arose from a specific Playdate C API call returning an enum from the Playdate OS. After a thorough debugging process, e.g. using printf everywhere, I uncovered a discrepancy in the memory layout of the enum between the system built with gcc and the game built with swiftc. With further research I found the difference stemmed from gcc defaulting to -fshort-enums while swiftc via clang used -fno-short-enums for the armv7em-none-none-eabi triple. I collected these new and removed flags into the following compile command: $ swiftc \\ -c test.swift \\ -o build/test.o \\ -target armv7em-none-none-eabi \\ -Osize -wmo -enable-experimental-feature Embedded \\ -I $HOME/Developer/PlaydateSDK/C_API \\ -Xcc -D__FPU_USED=1 \\ -Xcc -DTARGET_EXTENSION \\ -Xcc -falign-functions=16 \\ -Xcc -fshort-enums \\ -Xcc -mcpu=cortex-m7 \\ -Xcc -mfloat-abi=hard \\ -Xcc -mfpu=fpv5-sp-d16 \\ -Xcc -mthumb \\ -Xcc -I -Xcc $HOME/Developer/PlaydateSDK/C_API/ \\ -Xcc -I -Xcc $GCC_LIB/gcc/arm-none-eabi/9.2.1/include \\ -Xcc -I -Xcc $GCC_LIB/gcc/arm-none-eabi/9.2.1/include-fixed \\ -Xcc -I -Xcc $GCC_LIB/gcc/arm-none-eabi/9.2.1/../../../../arm-none-eabi/include \\ -Xfrontend -disable-stack-protector \\ -Xfrontend -experimental-platform-c-calling-convention=arm_aapcs_vfp \\ -Xfrontend -function-sections With these adjustments, I attempted once more, and finally “Swift Break” successfully ran on the Playdate hardware! I’ve included a brief video showcasing the game below: Wrapping Up Thanks for diving into the bring-up journey with me. From refining the Swift Playdate API to tackling issues involving calling conventions, CPU configurations, and memory layout disparities, there was no shortage of challenges. However, with the issues now resolved, creating Playdate games in Swift is a streamlined process. Just run make and enjoy a development experience with Swift that is both expressive and performant. You can find all the code examples mentioned in this post in the swift-playdate-examples repository with accompanying “Getting Started” documentation. I hope this post encourages you to explore the possibilities of using Swift in unconventional environments. Feel free to reach out with your experiences, questions, or game ideas on the Swift Forums! Happy coding! 🎮 Iterate Over Parameter Packs in Swift 6.0",
    "commentLink": "https://news.ycombinator.com/item?id=39682143",
    "commentBody": "Byte-Sized Swift: Building Tiny Games for the Playdate (swift.org)183 points by rauhul 16 hours agohidepastfavorite29 comments hbn 14 hours agoTangentially related, but Lucas Pope's new game, Mars After Midnight, just released for $6 on Playdate. Played it for a bit during my lunch break and it's really cool and creative. I had read a few of his devlog posts[1] from a while back and it's really cool to get an insight into his process. A while back I had also gotten into reading his updates on the process as he built his last game, Return of the Obra Dinn[2]. Only got 10 pages or so into the thread but again, his talent and attention to detail are incredible. [1] https://dukope.itch.io/mars-after-midnight/devlog/261758/mar... [2] https://forums.tigsource.com/index.php?topic=40832.0 reply the__alchemist 6 hours agoparentPapers Please and Obra Dinn were outstanding! Hopefully this one makes it onto PC. reply glhaynes 13 hours agoprevA Vision for Embedded Swift (https://github.com/apple/swift-evolution/blob/main/visions/e...) has the details on this new build mode and is quite interesting. Effectively, there will be two bottom layers of Swift, and the lower one, “non-allocating” Embedded Swift, will necessarily be a more restricted compilation mode (e.g. classes will be disallowed as they fundamentally require heap allocations) and likely to be used only in very specialized use cases. “Allocating” Embedded Swift should allow classes and other language facilities that rely on the heap (e.g. indirect enums). Also, this seems to maybe hint at (most of?) the Swift runtime eventually being reimplemented in non-allocating Embedded Swift rather than the C++ (?) that it uses now: The Swift runtime APIs will be provided as an implementation that’s optimized for small codesize and will be available as a static library in the toolchain for common CPU architectures. Interestingly, it’s possible to write that implementation in “non-allocating” Baremetal Swift. reply pjmlp 1 hour agoparentYes, the long term roadmap for Swift is to bootstrap it, minus the LLVM backend, which is always a matter of not re-inventing the optimization algorithms that half of the world is putting into LLVM, whose contributions are at similar level as the Linux kernel. reply Neff 14 hours agoprevThis is great to see more resources being made for the Playdate. It is such a fun little console and the games made for it are such a breath of fresh air I hope Panic is able to either retrofit or make a v2 with a backlight. I realized how low the contrast on the screen was after my cataract surgery and seeing details in some of the games becomes near impossible in less-than-ideal conditions. reply msephton 11 hours agoparentThe Sharp Memory LCD screen is opaque, unlike most other LCDs, so can't work with a backlight. It could work with a front/edge light but the results are not what you'd hope for. reply Kon-Peki 8 hours agorootparentHow does the screen compare to the original Game Boy, or even the Game Boy Pocket? reply mattl 8 hours agorootparentMuch much better than both. reply w10-1 4 hours agoprevThis sounds like a cute project but it's a nice tour of cross-compiling and C inter-op that proves Swift could not only be usable for myriad devices, but will likely make them much more accessible with ergonomic safe API's. Swift's inter-op story with C now is that most things are automatic, some things are configurable, and some just not. Clang supports multiple ABIs and calling conventions, giving control over optimizations, and linkers should work when the object files are valid. But tying it all together takes some detailed work in make. So the generality of clang and the linkers makes the potential reach large, but each new target combination will require some work. I can see how individual developers might scratch an itch, but I'd be very interested to see if platform and library providers start supporting Swift directly. A bit of configuration could lower adoption costs, increasing uptake. But where does that happen, and could that work be shared? Apple will continue to focus on Xcode, and most of this is out of scope for the Swift Package Manager. Swift support in CMake helps at the lowest level, but there's no dominant integration platform (like Eclipse was for Java) for contributors to share work. Sharing experience is another thing. The blog author had internal experts at Apple available, but it's something of a black art to assess compiler work-in-progress and workarounds, and particularly to assess likely progress in a company that follows the first rule of fight club. That makes a blog entry of a successful port all the more valuable! reply rgovostes 14 hours agoprevRecently: Embedded Swift on the Raspberry Pi Pico https://news.ycombinator.com/item?id=39611111 reply airstrike 11 hours agoprevPretty cool. Swift is such a great little (little?) language, I'm always thrilled to see more of it outside of Apple stuff (which is not to say I don't also love it on Apple stuff) reply cageface 2 hours agoparentI enjoy programming in Swift but it hasn't felt like a little language for quite some time now. It's a complex language with a lot of features and a powerful but elaborate type system. reply rauhul 16 hours agoprevAssociated discussion on the swift forum: https://forums.swift.org/t/byte-sized-swift-building-tiny-ga... reply ladberg 13 hours agoparentFeeling pretty craft-y recently, huh? reply ks2048 11 hours agoprevI haven’t seen playdate before. Looks cool and I appreciate people doing things like this, but $200 seems like a lot! Compare to android phones for half the price that also function as phones. I realize things without mass production will be more expensive, but I wonder what part(s) the device are the biggest contributor to the price? (Also, more to the point of the article - kudos to Swift team. A nice language I hope becomes more widespread) reply linsomniac 11 hours agoparentI was kind of interested in the Playdate but couldn't bring myself to buy one. Last summer I stayed at my brother-in-laws place for a week and did some DIY work around his house, and while I was there played with his Playdate. Really enjoyed it, but still couldn't bring myself to \"pull the trigger\" on buying one. I'm really busy with work and life these days and don't often find I have time to game. 3 months later a package arrived from Playdate; as a thank you for the DIY work my BIL bought me one. It's a very well designed and executed device, the crank handle is well used in many games, and it's nice to have a device that just does one thing and does it well. It can be a little tricky to play depending on ambient light. reply msephton 11 hours agorootparentProps to your BIL! You're lucky to have family like that. reply msephton 11 hours agoparentprevI think the biggest contributors to cost are the Sharp Memory LCD screen, custom built crank, and the fact that you get 24 games included with the device. Regardless, it's absolutely worth it without a shadow of a doubt. There's nothing else like it. Plus, it's the only way to play exclusive games of which there are many. My game YOYOZO was listed in Ars Technica's \"Best Games of 2023\" alongside several games by Nintendo. reply ks2048 9 hours agorootparentThanks for the info. I think if Playdate could be significantly cheaper without the included games, it could bring in more people who would then buy games if interested (as well as potential developers). But, I understand it’s hard to say - they need to make money on the device too. reply myko 9 hours agorootparentprevIt's a neat device but I hesitate to say it is worth it, the lack of backlit screen makes it mostly unplayable to me :/ reply mattl 8 hours agorootparentHave you played on one? I didn’t find the lack of a backlight a problem when you’re in reasonable light. reply favorited 14 hours agoprevI recently watched a neat conference presentation[0] about generating Game Boy Advance binaries using LLVM – I wonder how difficult it would be to use that ARM7TDMI toolchain with Embedded Swift. [0]https://www.youtube.com/watch?v=Q-Woh_Uzw5g reply BlueFalconHD 6 hours agoprevSwift is love, Swift is life. Wonder how well it would run on the new Arduino (r4 afaik) reply Kon-Peki 15 hours agoprevNice job! Not being much of a swifty, I didn't know anything about that API Notes thing. I like it a lot! reply msephton 10 hours agoprevI'm interested in a performance comparison between Swift, C, Lua, Nim. The post says Swift is performant, but offers no data or reasoning to back up that claim. I'm sure it is, but it would help to see data. reply jerbear4328 5 hours agoparentI don't think the post was trying to prove that fact, just explaining a personal choice. I don't think every Rust project should have to include benchmarks to prove that Rust was the right choice. reply bsdpufferfish 5 hours agoparentprevSwift can be written in many ways with different performance tradeoffs. The slower ways looks like Objective-C or functional programming. The faster ways look like ergonomic C or C++. reply saagarjha 10 hours agoprevWonder if Panic will switch to an LLVM-based toolchain moving forward. reply ericlewis 14 hours agoprev [–] Ah cool. I messed around with this a while ago: https://github.com/ericlewis/swift-playdate reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Rauhul Varma showcased creating games for the Playdate handheld system using Swift, including a Conway's Game of Life port, enhancing the Playdate C API for improved usability and memory handling.",
      "Challenges with deployment, compiler configurations, calling conventions, and memory layout were faced but successfully addressed, advocating for Swift's use in unconventional settings.",
      "The demonstration highlights the potential of using Swift for game development on unique platforms, showcasing innovative solutions to common obstacles."
    ],
    "commentSummary": [
      "Hacker News is debating building small games for the Playdate using Swift, highlighting features and related games on the device.",
      "Users are intrigued by a new build mode for Embedded Swift with multiple layers and a possible reimplemented Swift runtime.",
      "The discussion focuses on the Playdate's design, cost, unique features, and the performance of Swift in comparison to other programming languages for game development and embedded systems."
    ],
    "points": 183,
    "commentCount": 29,
    "retryCount": 0,
    "time": 1710263889
  },
  {
    "id": 39685046,
    "title": "Insights from 40 Years of Programming",
    "originLink": "https://liw.fi/40/",
    "originBody": "Lars Wirzenius → 40 years of programming 2024-03-12 18:52 article Introduction The core skills On productivity On governance On politics and ethics On diversity and quality On maintenance Big questions for projects Planning and estimating Express yourself in writing On doing work Advice on coding Developing a career Takeaway Acknowledgments Introduction 10 PRINT \"HELLO\" 20 GOTO 10 In April, 1984, my father bought a computer for his home office, a Luxor ABC-802, with a Z80 CPU, 64 kilobytes of RAM, a yellow-on-black screen with 80 by 25 text mode, or about 160 by 75 pixels in graphics mode, and two floppy drives. It had BASIC in its ROM, and came with absolutely no games. If I wanted to play with it, I had to learn how to program, and write my own games. I learned BASIC, and over the next few years would learn Pascal, C, and more. I had found my passion. I was 14 years old and I knew what I wanted to do when I grew up. When I was learning how to program, I thought it was important to really understand how computers work, how programming languages work, and how various tools like text editors work. I wanted to hone my craft and produce the finest code humanly possible. I was wrong. This essay is a condensation of what I wish I had been told after I had learned the basics of how to code. Instead, I was told, in person and in magazine articles, that by the time I would be twenty five years old, I'd be too old to work as a programmer. It was critical that I learn as many algorithms, data structures, and languages as quickly as possible. Programming was, after all, a young man's game, and required being able to stay up all night, every night, to crank out more code than anyone else. That was the only way to succeed. It turns out that none of that was true: not the part about youth, nor the part of missing sleep, and especially not the part about gender. As I write this essay, it will soon be forty years to the day since I first wrote computer code. I've managed to support myself by developing software, and I still write code every day. There is nothing else I would rather do for a living. I can't point at enormous successes and impressive feats, but I hope that surviving for decades in the industry gives me sufficient credentials to speak about software development. This essay discusses some of the things I've learned about how to successfully build software. These are things I've learned from my own experience; I'm not a researcher, and there are few references to sources, and this is largely not supported by evidence. I'm basing this essay on my own experience, and if you disagree, that's fine. My goal in this essay is to get the reader to think, to research, to learn, to ponder. My goal is not to tell the reader how to think, what to think, how things are, or to give the answer to every question about every aspect of the process of building software. The core skills Interesting and significant software is beyond the capacity of any one person to build alone in a reasonable time frame. This means that the fundamental, crucial, core skills in building software are communication and collaboration. It's not enough to know how computers work, how to use programming languages, to know algorithms and data structures, or how to use the varied tools involved in software construction. You also need to know how to talk with other people to learn what software to build, what it must do, how much effort is acceptable, how to manage the work, and many more things. You have know how to work with others to build something together that's bigger than any of you. If you and your team can do it well, it'll be bigger than the sum of you. Team work can be a force multiplier. These are difficult skills to learn. I've found them much more difficult than any technical part of building software. Even in the projects where I'm the only person, there are at least three people involved: past me, present me, and future me. Past me is a lazy and careless slob who always leaves a mess. Present me does superb work, but has to cope with all the stupid stuff done by past me, and needs to placate future me. Future me is an egotistic and opinionated snob for whom nothing is ever good enough. Getting these three people to get along well enough to get anything done is a constant struggle for me. I use my journal to cope. I write there what I do, why, and what the result is. I try to write so that I'll understand it, even if I'm tired, or have just experienced a short term neuralizer mind wipe a la Men in Black. I also plan explicit iterations, with specific tasks, and I use the GTD system to track things that still need to done. On productivity What productivity means is not well defined for software development: time is easy to measure, but the output is not. Most people have at least a vague idea what productivity means to them. However, there are known factors that impact productivity---these are customarily ignored. To do good work you need to take care of yourself. You have to sleep well. You have to eat well. If you're tired, or have too much stress, you'll make mistakes and bad decisions. These result in you doing bad work. That's not a moral problem, but you may have to fix your own mistakes. You'll enjoy life, and work, more if you do good work. Hillel Wayne has talked about sleep and has sources. Recommended reading: https://www.hillelwayne.com/talks/ese/ https://www.hillelwayne.com/talks/what-we-know-we-dont-know/ You also need to have an environment that helps you do good work. Is it quiet? Are there interruptions? Is the furniture comfortable, and unlikely to hurt you in the long run? Do you have access to necessities, such as a toilet and tea? Can you take a break from your work area? Can you go for a walk: walking helps thinking. This is an incomplete list. Further, you need to manage yourself and your work in ways that suit you. The best way to do this depends on who you are, your preferences and experiences, and the kind of work you do. There is no single solution that works for everyone all the time. For myself, I've found applying the David Allen Getting Things Done system has worked well, but whatever you do, you need to know what you need to achieve, today and in the near future, and you need to arrange things so that you can concentrate on one thing at a time. Note, however, that I use the GTD system partly to know that I can slack off; I do not aim to be as productive as possible at all times. It may be worth tracking what you've done. Reflecting on that can give you a sense of achievement, and lets you see the progress you make. This can be a motivation and morale booster. Multi-tasking is fine for a computer, but your brain can only think about one thing at a time and has really miserable context switching. On governance When a group collaborates, one of the things they need to establish early on is governance: basically, how does the group make decisions, and how do they change them? The decisions range from the fundamental to the mundane: Who is in the group? Is it a democracy, or hierarchy, or some other structure? Who gets to have a say? What beverages does the group get? What font should the website use? Governance is difficult, but it's easier when it's explicit. Uncertainty about responsibility and power results in confusion and quarrel, and these can tear a group apart. All groups eventually have conflicts. Managing this and resolving differences is nominally the job of management, but really it falls on everyone. This requires skills that seem to be rarely taught to programmers, which is a shame. Recommended reading: Contributor Covenant The Tyranny Of Structurelessness by Jo Freeman On politics and ethics Software development is always a political and ethical act. Whenever you're building software, you have to make a myriad of decisions: what shall the software do? what resources will using it require? how shall it be used? what abilities shall using it require? what things will be easy to do? what things will be hard to do? will the software empower its users, or force them to do things for someone else's benefit? All these decisions have consequences, which will favor some people over other people. Hence, they have political and ethical aspects, which need to be considered. As an example, if you make something that can be used with a cheap phone, you enable most people in the world to use it. If you make it so that it requires expensive hardware, you exclude many people, especially poor people. Sometimes that is inevitable, and inherent in the problem you're building software to solve, but it's still a choice. There may not be a right or wrong choice for a particular decision, but there are always consequences. You have to consider them, and decide if they're acceptable, and for whom. In general, the software you build will reflect some values. Make sure you know you agree with them. Recommended reading: Tool Safety by Leonard Richardson On ethics and software freedom Free and open source software is the ethical kind of software. Software freedom is essential for the well-being of people whose lives are affected by the use of computers, whether they are using computers directly or someone else is using computers to do things that matter to other people. I have been writing free software since 1986, when I first read the GNU Manifesto on a BBS. (The term \"open source\" is a synonym: I do not care to debate the differences between the two.) Much of my career has involved building open source software. My preference for free and open source over non-free software is clear, and well documented. Entire libraries' worth of text has been produced over the past three decades about the ethics of free and open source software versus other kinds, and specifics of licenses. I'm not going to make a summary of arguments here, or even point at recommended reading. I wanted to make my position clear, in case it matters to the reader. However, very little of this essay, including the topic of ethics, is related to software freedom and applies equally to the development non-free software. On diversity and quality Human rights are fundamentally important. Treating other people well is the right thing to do. All of this is of paramount priority, whatever you do. If you don't agree with this, I have no hope for you. In a software development context, I've found that the most crucial thing for building high quality software is to have diversity of thought among the people contributing to it. The more different kinds of thinking is brought into the project, the more likely the decisions will work for more circumstances, for more people, and deal with more things going wrong. Diversity of thought comes from different kinds of people coming from different backgrounds and having lived different kinds of lives. Sometimes differences are visible externally, sometimes not. When in doubt, choose different. If you exclude people based on them being unlike you, you will likely be choosing poorly. Diversity doesn't guarantee success. Nothing guarantees success. However, uniformity guarantees you get one-sided answers. Sometimes that's enough, but often it's not. Collaboration and communication can certainly be more challenging in a diverse setting. Do not fear this. Treat challenges as an opportunity to learn, to become better at what you do. People will make mistakes, and you too will make them. It's a good policy to be benign and kind when others make mistakes, and expect that from others in return. Punishing others for mistakes will leave you alone. On maintenance It is widely considered, in the software industry, that most of the cost of software production comes after the initial release, in the so called maintenance phase. The initial development might take a year or two, maintenance will take decades. One would think that this would lead to development practices, and architectural decision, and everything else to be optimized for lower maintenance costs. Unfortunately, the economic and other incentives favor the opposite. In most companies, the event horizon tends to be a quarter year, or a full year at most. It's not considered acceptable to spend a lot more effort now to save most of the effort in the long term. Even for personal hobby projects, there is often a strong urge to get something working as soon as possible rather than taking one's time to do things well, but at least that's an internal urge, not external pressure. Even so, the end result is the same: software that's harder to change later on, when bugs are found, requirements change, or the software needs to be adapted to changes in the surrounding system or ecosystem. I am not alone in seeing the problem. I don't think it can be solved, unless the economic incentives are radically changed. It's on individual developers to try to reduce maintenance costs surreptitiously, when they can, as a kind of preemptive guerrilla maintenance. Big questions for projects I find that it's important, but also helpful, to answer a few questions at the beginning of every new project. The answers don't have to be final, and it's OK to change them at any time: if you don't adjust your thinking when you learn new things, you're not good at what you do. The answers can even be inane, as long as they're honest. The point to is to get you thinking about the questions at all. For whom are you building the software? Whose opinions about it matter? This tells you some of the stakeholders in the project. There will likely be more stakeholders later on, but it's a start. Knowing who the stakeholders are lets you concentrate on their needs. This helps make the project a success. The stakeholders might be the end users, or might be the CEO. If you think it's the end users, but it's actually the CEO, you will make decisions that make the CEO unhappy, and the project will fail, and you won't understand why. And vice versa. Why are you building the software? \"It's my job\" might be a very practical answer. It's an answer that means that you need to re-evaluate everything if the chance of getting money is reduced. A different answer might be more about passion, or mission, and this means your reaction to changing circumstances is going to be different. There's no wrong answer here, but be sure to be honest, at least to yourself. What should the software do, in broad strokes? Also, what should it not do? This is about knowing if you're building a word processor or a spreadsheet. It's not about detailed requirements or acceptance criteria. How should the software work, in broad strokes? Again, big picture view. Are you making a command line tool, a web service, an operating system, a mobile app, or something else? What type and size of hardware? What's important and what is just nice to have? This gets into requirements and acceptance criteria. You need to know them to build something good. Write them down. Also write down how you verify you meet them. Planning and estimating Detailed planning beyond the very near future is difficult and usually fails. This includes estimating how long the work will take. I avoid doing this. Planning and estimation are far from useless and should not be neglected. I plan and I estimate, but within the limits of what I've learned to be realistic. Specifically, I find it nearly impossible to make detailed plans beyond a one or two week iteration. By detailed plans I mean figuring out tasks at a granularity of what can be done in one sitting. Something surprising usually happens that ruins the plans beyond a couple of weeks. It might be management changing priorities, or the client changing their mind, or the company going bankrupt, or being bought. It might be something else, but usually something happens. If an iteration is short, you don't lose too much if all your planning has to be thrown away. You can also react more easily if you need to change course. I stress that I'm talking about detailed planning here. It's fine to make plans for what to do next month or next quarter, as long as everyone involved knows that these plans are likely to have to change, and the plans are kept suitably high level. An analogy: if you're planning a road trip across the country, you will probably want to plan what cities to drive through. You probably don't want plan every stop to fill up on fuel or to eat. You want to leave enough flexibility that you can change route if there's an accident, or weather, or something else that may come as a surprise on the way. Iteration works. Nothing else seems to work for large software projects. At least I've never seen any other approach work. For me, iterations work better if they're focused. Having an explicit, clear goal helps cut down on scope creep. It concentrates work to aim at the iteration goal, rather than on what seems like a good idea in the moment. When I plan for an iteration, I break things into small tasks that can be finished in one sitting. My estimates for tasks fall into four buckets: up to a quarter hour, full hour, four hours, or too long. I prefer the shorter tasks: they're easier to estimate, and easier to do, and usually break fewer things. Anything that's too long needs to be broken down further. Some tasks are impossible to estimate. Debugging is a typical example. How long will it take you to realize the Ethernet cable is broken, or an operating system update broke things, or find the cause of some other unforeseen problem? Debugging happens, and you had best leave enough slack in your planning to cope with some troubleshooting. Interruptions also happen. Sometimes you have no option but to react and respond to them immediately. Leave room for that. Express yourself in writing Your memory is fleeting and fallible. You will forget details. You will forget important stuff. You will remember things wrongly. When you collaborate, you and others will disagree on what was said, agreed upon, and decided. I was once in a meeting, with four people, plus the CTO. The CTO forbade us to take notes: it seems the fad of the week was that note taking is what makes meetings a waste of time. The meeting took two hours. Afterwards, the four of us had about eight different opinions of what had been decided. No follow-up actions were ever taken. Write things down. It seems like a stupid chore, tedious, boring, and archaic, but it's the way organizations remember. Writing things down is a super power that is easily overlooked. Your team should keep meeting minutes, covering at least the important decisions, and what actions everyone should do after the meeting. Keep them short enough that they're easy write, and to read. Archive them somewhere everyone can look at them at any time. This helps people who weren't there, perhaps because they were on vacation, or only join a year later. Apropos meetings, learn how to have good meetings. What works for me is an agenda set well ahead of time, with supporting materials. Prepare for the meeting so that you don't waste everyone's time. Have a chair who keeps discussion on track, and time boxed. Your process for a good meeting might be different, but you should find one. On doing work When making a change, make only one change at a time. If you can, split the change you're making into smaller partial changes. Small changes are easier to understand and less likely to be catastrophic. Automate away friction: running tests, making a release, packaging, delivery, deployment, etc. Do this from as early on as feasible. Set up a pipeline where you can make a change and make sure the software still works and willing users can start using the changed software. The smoother you can make this pipeline, the easier it will be to build the software. Use version control for every project. Practice continuous integration by merging changes to the main line of development often. Develop an automated test suite that you trust: if tests pass, you can make a release, or deploy to production. This usually means starting the test suite early on in the project. Make sure the automated tests cover all aspects you care about: the goal is to make sure that what you give to others works they way it's intended. Update production code and test code together. Run the test suite many times in a row to identify flaky tests. Do not suffer flaky tests. Do stress or load testing, even if you only do it in simple ways. You don't know your software and system can handle 10,000 concurrent users until you've done so. Mine didn't. In one job, we had a system that had worked fine in production for a while. We deployed it to a customer who had a lot more traffic. Our software failed. The problem turned out that we ran out of TCP port numbers: we made an HTTP request for each incoming message, but we didn't re-use the underlying TCP connection. There was so much traffic that all possible port numbers were used, and then everything stalled. The fix was to re-use the TCP connection, a change that took about one line, and then everything worked fine. We would have found this ourselves had we done even the simplest load testing. If possible, use the software yourself. You'll understand your users better. Watch other people use the software. You'll understand your software's sharp corners better. True story: it's hard for people who've not been involved in the development of software to use the software, if it has three different names for the same thing. Seeing someone struggle with this makes it painfully obvious. Treat testers as friends who help you find when you've made a mistake. They're not out to humiliate you. Nobody is so perfect that they never make mistakes or overlook details or forget entire features. Don't ask me how I know. Advice on coding Simple, obvious code is easier to write, easier to get to work, easier to understand, and easier to change without breaking it. Simplify things as much as you can, but no more than that. Sacrifice simplicity at the altar of performance only when you have proven the performance gains are worth it. Complexity is the enemy you think is a friend. Coupling and cohesion are still important concepts. In every project, conceptual clarity is important. Also, keep your terminology consistent. Confusion lurks where clarity hides. With great confusion comes great annoyance and the certainty of bugs. Speed is a feature to users, but not always for development, as too much haste leaves too little time to think. Functionality can be a misfeature. Recommended reading: A Philosophy of Software Design by John Ousterhout, which I've not yet read, only skimmed, but which people I respect keep recommending to me. Developing a career You can choose to be a deep expert on something very specific, or to be a generalist, or some mix. Choose wisely. There may not be any wrong choice, but every choice has consequences. Be humble. Be Nanny, not Granny. People may respect the powerful witch more, but they like the kind one better. Be open and honest. Treat others fairly. You don't have to believe in karma for it to work, so make it work for you, not against you. Help and lift up others. But at the same time, don't allow others to abuse or take advantage of you. You don't need to accept bullshit. Set your boundaries. Ask for help when you need it, or when you get stuck. Accept help when offered. I am not the right person to talk about developing a career, but when I've done the above, things have usually ended up going well. Takeaway Take care of yourself. Sleep. Eat. Exercise. Rest. Relax. Take care of other people, as best you can. People are important. Software is just fun. Acknowledgments Thank you to Brennen Bearnes, Richard Braakman, Tyler Cipriani, Greg Grossmeier, Hackhörnchen, Soile Mottisenkangas, Daniel Silverstone, and Enrico Zini, and for reviewing drafts of this essay. Any errors are mine.",
    "commentLink": "https://news.ycombinator.com/item?id=39685046",
    "commentBody": "40 Years of Programming (liw.fi)182 points by janvdberg 12 hours agohidepastfavorite64 comments anonzzzies 5 hours ago>I was once in a meeting, with four people, plus the CTO. The CTO forbade us to take notes: it seems the fad of the week was that note taking is what makes meetings a waste of time. The meeting took two hours. Afterwards, the four of us had about eight different opinions of what had been decided. No follow-up actions were ever taken. This is still the case in many companies I (founder of a small troubleshooting company; we search out problematic large, cash rich companies like these; they all have failing processes and IT all over the place) meet with; sometimes it's frowned upon, but often simply no one takes notes. Which shows these meetings (almost all meetings I go into) are a complete charade of managers wanting to show they have 'something important to do, really!' (somehow there are often 10+ people there). I am in meetings (including Zoom etc and irl) where i'm sure no-one really heard or understood what anyone else said (different accents of English from different countries and background and of course, no one can say anything because we all have to respect people etc); no notes, no recordings (and of course, the captions of the video chat or my phone didn't understand anything that was said either), while someone was explaining quite difficult, in depth stuff, for 2-3 hours. Afterwards, the stuff is rehashed in a 15 minute text chat with the person who explained the difficult stuff; why didn't they write it down in the first place and forgo the meeting? Because half(that's generous, it's more like 80%) of the room should have no (high paying) job; they are just there for being there. Ah, the enterprise world, such joys; I enjoy it as I threat it like a cosmic joke; it's a comedy show, not unlike The Office including the main characters high pay. reply Brajeshwar 5 hours agoparentIn almost all the meetings I attend, I'm always the note-taker by choice unless I have earmarked someone to take notes. My notes usually ends up being the go-to and shared document after the meetings. I'm still learning, and I love taking pictorial/visual notes. Try it; it is fun. If one is interested, I suggest the books by Dan Roam[1]. They are quick to read/browse, and I keep re-reading them. If you are starting with just one book, start with “Draw to Win”. https://www.danroam.com reply Terr_ 4 hours agorootparent> Vetinari was very good at committees, especially when Drumknott took the minutes. What the Iron Maiden was to stupid tyrants, the committee was to Lord Vetinari; it was only slightly more expensive,* far less messy, considerably more efficient and, best of all, you had to force people to climb inside the Iron Maiden. > * The only real expense was tea and biscuits halfway through, which seldom happened with the Iron Maiden. -- Making Money, by Terry Pratchett reply mingusrude 58 minutes agorootparentprevThe superpower is to write the notes before the meeting. It may sound cynical but sometimes it's worth trying. reply anoopelias 3 hours agoparentprevWhat worked for me well is if I share my screen and capture the common notes / mural for everyone to see. You might not catch the spellings etc., need to ask, but that's probably the question everyone has. That way there are less misunderstandings, everyone is aligned, and I can send the notes right after the meeting. reply bee_rider 2 hours agoparentprevUseless meetings have always felt like a strong argument for Basic Income to me. These have negative value, in the sense that they destroy time for the competent workers. As a society we have to make sure a bunch of people don’t become unemployed and then homeless, but we should have done it in a way that doesn’t incentivize pretending to have a necessary job. reply dboreham 1 hour agorootparentSee: B-Arc reply qingcharles 5 hours agoprevOver 40 years of programming here, and I'm back in my prime again. Started out in BASIC on ZX Spectrum and BBC Micro/Electron. The toughest programming I did was writing a full texture-mapped, shaded 3D engine in x86 from scratch, aged about 12. (I pasted my 12-year-old code into this commercial video game, although I think the texture-mapping here I replaced with Direct3D's much slower software renderer: https://www.youtube.com/watch?v=t2kdKB18c7I&t=332s ) The best paid was modifying some existing source for a torrent site that brought in ~$13m. Funny moment: putting a language I didn't know on my resume, getting hired from it on a Friday and starting on a Monday as lead developer :D (crazy weekend) My favorite language of all time was VB.NET, but that's not worth pursuing any longer. Right now I'm mostly writing hardcore ASP.NET in C#, running on Linux. reply bojan 1 hour agoparentI have to ask, sorry - what is it precisely that makes it hardcore? reply pdimitar 55 minutes agoparentprev> The best paid was modifying some existing source for a torrent site that brought in ~$13m. ...Wow. How does one even find such a gig? reply mianos 8 hours agoprevOver 40 years and I never got sick of it. I am leaning new things every day. I strongly disagree with: \"Simple, obvious code is easier to write, easier to get to work ..\" It takes real skill, time and effort to write simple code in any production setting. I am not talking about some 100 line algorithm some leet guy once wrote, but code that's been in production for years. It's only the simple code that survives. reply ZaoLahma 2 hours agoparent> I strongly disagree with: \"Simple, obvious code is easier to write, easier to get to work ..\" I get your point, as well as the author's point. Complex code is difficult to write, and unless you really pay attention, likely to be wrong. Simple code is easier to write, easier to verify and more likely to be correct, but it's very hard to learn how to express your solution to a complex problem in simple terms. reply mianos 55 minutes agorootparentMaybe I should have said, \"simple code is hard to write and complex code is easy to get wrong\". As an aside, I'm not going to post an AI generated essay, but pasting that quote into the new Claude gives an essay well worth reading if anyone cares. reply dr_kiszonka 3 hours agoparentprevThere is also that code no one understands and everyone is afraid to touch : - ) reply mianos 52 minutes agorootparentFunny thing is, being the oldest programmer, I get given that code. Code that has had bugs in it that destroyed the careers of people given it. I could tell a story about one time I fixed something and they had a company wide party, which I couldn't go to as I had another impossible bug. reply gsliepen 50 minutes agoprevOver 40 myself, and having a similar experience (I just started with an Amstrad CPC6128 instead of a Luxor ABC-802, and it came with a few games, but after that you had to write your own in BASIC). I like the part about the past/present/future \"me\". I think it's something that happens on smaller timescales as well. Especially when starting a new project, or writing a new feature, start with being lazy and sloppy: just make the thing work. Once you have it working, then refine it so it the code is clear and maintainable; this is the \"superb work\" you'll be doing, inspired by \"future me\"'s view of perfection. However, instead of this being a linear progression with a clear start and end, this should be more of a cycle; don't get stuck trying to achieve impossible perfection, instead don't be afraid to break stuff now and then to make actual progress. reply Brajeshwar 6 hours agoprevFirst, I’d like to pay my utmost respect. Thank you for the brilliant article. It is short and concise and covers a wide range of topics for lifelong programmers. I’m bookmarking it for references when someone asks to read this line of thought. And, I’m happy to know and jealous of, “I have code running on billions of devices, on all continents, on all oceans, in orbit, and on Mars.” I’m a muggle in the programming world. I wandered around learning the dark spells of management, design, and everything else in between. Looking back on my 20+ years of technical-professional career, I realized that I’m much better off in bursts—5 years of extensive programming in an extremely narrow field of focus, another 5 in code-driven designs, etc. I hope that one day, I can also write in such beauty about my 40+ years of careers, learning, etc. reply whitehexagon 1 hour agoprevOver 40 years here too, from basic & assembly on a speccy 48k, 68000 on the amiga, through to my latest just-for-fun project of Zig SoC programming on the A64 pinephone. Thanks for making me feel old today! but also lucky to have had such a hobby come career. Although I am sure it has rewired my brain in the process, and not all for the best. Depression, and frustration at how software now mostly controls lives rather than enhances / supports. I feel like I got a glimpse behind the curtain, and this problem solving brain now sees a broken system destroying the planet and humanity for the last extra cent of profit, with no solution. EOL reply Metricon 9 hours agoprevStarted 1982 on Tandy Color Computer. Still at it. #1 Advice - Focus on getting things done (as many will not) and Lego build interconnected/isolated simplicity as much as possible. reply blindriver 7 hours agoprevOver 30 years of professional experience. Currently at a FAANG. Just today I fixed a bug that the entire team couldn’t figure out, including our team lead who is brilliant who is 15 years my junior. I’m not a better programmer than he is but I pull my own weight. When I fixed the bug, I had a rush the entire day. 30+ years and I still love programming, and I feel blessed and grateful to have fallen into this career. It felt like divine, Godly intervention for the path that lead me here. There are some things that I disagree with the author, most especially things like this: > If you don't agree with this, I have no hope for you. > When in doubt, choose different. If you exclude people based on them being unlike you, you will likely be choosing poorly. He says he believes in diversity but contradicts himself a few sentences previous. I find this typical with a lot of people who claim to believe in diversity: they don’t actually believe in true diversity, they believe in people agreeing with their pre-existing beliefs. And if someone doesn’t agree, there’s no hope for them at least according to the author. That’s not someone who believes in diversity. And I actually believe the greatest software projects have a single strong voice with a strong vision and strong competency that drives the entire project. Look at Steve Jobs with Apple and the iPhone, Linus and Linux, Elon Musk with Tesla, Zuckerberg and Facebook. You don’t find a lot of collaboration, what you see is a brilliant visionary with strong opinions and not much diversity in opinion. Too many cooks spoil the broth, as they say. If you want a fun environment, then sure you can collaborate and give equal time to others but you won’t go as fast and usually the end results aren’t as dramatic. reply isurujn 2 hours agoparentYou took his first statement out of context which made it worse than it sounds. Hell, I thought the author was an a-hole because I read your comment prior to reading the article. He says human rights are important and treating other people well is the right thing to do and that's non-negotiable. Which is a fair statement. What decent human being would disagree with that? He's right about having a diverse set of eyes and opinions on a team being a positive thing. But most people see the word \"diversity\" and think about the forced diversity quota of 'we need 1 black person, 1 gay person, 2 woman, one Latino and one Indian' kind of diversity, which speaking as someone from a minority, I too am against it. reply tuukkah 1 hour agoparentprev> Linus and Linux If you consider who the OP is, you realise they know Linus and Linux better than almost anyone else: https://liw.fi/linux20/ reply toadi 6 hours agoparentprevIn my home country, we have a saying: \"No religion or politics in the bar.\" People who drink alcohol best avoid these as you end up in a fight sooner or later. We also do this at work. I am here to do a job. Just keep it focused on the professional side. I don't need to be your best friend. Or share the same religion or political beliefs. At work, I come together to reach a common goal. But that said, I hate people enforcing diversity for diversity's sake. They are actually introducing politics on the work floor. reply kaptainscarlet 4 hours agoparentprevAn autocratic government where there is little diversity of opinion is the same, it can get things done quicker than a democrcy. Not that it's a good thing but velocity of execution is generally higher. reply WoodenChair 8 hours agoprev> Interesting and significant software is beyond the capacity of any one person to build alone in a reasonable time frame. I disagree with that. VisiCalc, MacPaint, and many modern “significant” apps started out as 1 person projects. I agree collaboration and communication are vital skills but you don’t need to make this grandiose statement to defend that. reply ChrisMarshallNY 8 hours agoparentMany times, things grow. I have done some very “significant” stuff, that was a big deal, when I released it, but would be considered “quaint,” nowadays. That said, my significant work created a baseline that has been extended by a team, so it’s still “significant.” The app I just released ain’t Facebook, but it’s pretty non-trivial; consisting of three backend servers (that I wrote), and two native frontends (that I also wrote). Takes a while, though, and there’s a lot of moving parts. I’ve gotten used to working at this scale, but I’ve also been writing shipping software, for over 35 years. reply mianos 8 hours agoparentprevThis may have been true back in the times of the applications (when I was also a developer), but, speaking from 40+ years of experience myself. I agree, most significant projects need more than one person. There may be outliers, but back then it was the normal, now it's not. Also consider games, back then one or two guys could do a game, now all but the outliers are done by large teams. reply WoodenChair 7 hours agorootparentIf there are outliers you can't make an absolute statement. I would also add that with LLM assistance, one person being able to create a \"significant\" app is actually more likely today than five years ago. reply keithalewis 7 hours agorootparentFeel free to come back to us when you have an actual example. The days of one person languages might be over, but we haven't seen any language created using a LLM yet. Much less one people complain about. reply norir 6 hours agorootparentI'm writing a language by myself so for me at least the days of one person languages are not over. And I have the joy of working with a language no one complains about ;) reply Barrin92 4 hours agorootparentprev>Also consider games, back then one or two guys could do a game, now all but the outliers are done by large teams. If anything the opposite is true. Gamedev tooling is now so good and high level one or two people can make sophisticated games, in terms of audio, gameplay and visuals that an entire team could not make in the 80s or 90s. Just one example, Signalis (https://rose-engine.org/signalis/). One developer and one artist if I'm not mistaken, stunning game that in say 2000 would have been a release from a major publisher. reply mtsolitary 3 hours agorootparentStardew Valley! reply prossercj 6 hours agoprevAwesome, thanks for sharing. Three cheers for \"preemptive guerrilla maintenance.\" Congrats on 40 years reply valval 10 minutes agoprevI stopped reading at the diversity part. All humans are different, and striving for diversity in itself is an utterly silly idea. reply mattgreenrocks 9 hours agoprevI will hit 20 years of professional programming experience in a few months. My q to everyone who has kept at it: did you continue up the career ladder into staff eng-type roles? They’re put on the IC ladder but get further away from programming. I’m still conflicted about that. I can see myself trying that for a few years then just dropping back to senior level. I did realize recently that it is hard to give up programming when my whole professional career has been built around being seen as the person who programs things well. reply swatcoder 9 hours agoparentI'm not a bad communicator and have entrepreneurial blood so I explored various leadership opportunities over the years but it wasn't really a fit for me. I did well in the various roles but didn't enjoy it. To point straight at the childhood desires behind it all, I like receiving puzzles to work on and I like wowing people by anticipting their needs and exceeding their expectations. One can frame management/executive-track work that way too, but the third desire is to do that in the peace and quiet of my own space and (mostly) on my own time -- so for me it all pointed back to increasingly sophisticated remote contract work and using my entrpreneurial blood and commmuniction skills to facilitate it. I don't think anybody can predict what direction you might want to go, but I do suggest you think about what you're really after in your career and which of your opportunities are more or less aligned with that. reply wofo 1 hour agorootparentWow, you sound like a future version of myself! Would love to have a chat (contact details in my profile). reply eikenberry 9 hours agoparentprevI'm at around 25 years and have decided to stick with the middle of the IC ladder. Moving past that means no more working directly with the technology, only touching it indirectly. Plus theses are \"leadership\" roles, which pretty much just means moving into the dysfunctional, hierarchical structure most companies seem to default into. I prefer peers, not subordinates. To put it another way, my favorite thing about programming is the creative aspect. Getting a problem, thinking of a solution and then writing the code that solves it is more rewarding than anything the higher positions have to offer. reply dboreham 1 hour agoparentprevAdvice is to ignore the levels, do your thing and seek ways to be appropriately compensated and respected for that. reply cangeroo 9 hours agoparentprevI have a similar question. It seems to me that the software engineering salary distribution is bimodal. I'm simplifying a bit here, but basically into those that make 200k USD a year. What makes the difference? Is it management vs codemonkey? FAANG vs \"small\"And “new to you” code includes code you wrote a year ago and haven’t touched since. True, dat. I write code and documentation for future Me[0]. I can’t remember lots of stuff, and Google the most basic stuff, every day. But my stuff ships, works well, and tends to last. I think actual results can count. That seems to be a minority opinion, these days, but what do I know? I have to Google basic stuff, so I suppose that means I don’t know what I’m doing. [0] https://littlegreenviper.com/miscellany/leaving-a-legacy/ reply whartung 6 hours agoparentprevThere’s a difference between full stack and solo developers. There’s absolute overlap, but being aware of the stack and actually being the person who put it all together are different. All my solo projects I was intimately familiar with. Start adding people and you naturally start losing complete grasp of the project. reply cosmicac 10 hours agoprev [–] Loved this. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Lars Wirzenius reflects on 40 years of programming, discussing productivity, ethics, diversity, and quality in the software industry.",
      "He challenges programming stereotypes, stresses continuous learning, critical thinking, and the importance of a supportive work environment.",
      "Key skills highlighted include communication, collaboration, self-awareness, and planning; emphasizing accessibility, diversity, ethics, and maintenance in software development."
    ],
    "commentSummary": [
      "Emphasizes the significance of note-taking for clarity and follow-up actions in meetings to address the inefficiencies noted within companies.",
      "Explores coding challenges, the evolution of programming skills, the value of simplicity, and the influence of diversity and collaboration in software development.",
      "Discusses the changing dynamics in the software development industry, including team versus solo projects, shifts in game development, and potential career advancement paths, along with considerations like individual career choices, creative programming, and moving into management roles."
    ],
    "points": 182,
    "commentCount": 64,
    "retryCount": 0,
    "time": 1710278123
  }
]
