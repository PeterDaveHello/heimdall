[
  {
    "id": 39629044,
    "title": "Eloquent JavaScript 4th Edition: A Comprehensive Guide to JavaScript and Programming",
    "originLink": "https://eloquentjavascript.net/",
    "originBody": "Eloquent JavaScript 4th edition (2024) This is a book about JavaScript, programming, and the wonders of the digital. You can read it online here, or buy your own paperback copy (3rd edition). Written by Marijn Haverbeke. Licensed under a Creative Commons attribution-noncommercial license. All code in this book may also be considered licensed under an MIT license. Illustrations by various artists: Cover by Péchane Sumi-e. Chapter illustrations by Madalina Tantareanu. Pixel art in Chapters 7 and 16 by Antonio Perdomo Pastor. Regular expression diagrams in Chapter 9 generated with regexper.com by Jeff Avallone. Village photograph in Chapter 11 by Fabrice Creuzot. Game concept for Chapter 16 by Thomas Palef. Contents Introduction (Part 1: Language) Values, Types, and Operators Program Structure Functions Data Structures: Objects and Arrays Higher-order Functions The Secret Life of Objects Project: A Robot Bugs and Errors Regular Expressions Modules Asynchronous Programming Project: A Programming Language (Part 2: Browser) JavaScript and the Browser The Document Object Model Handling Events Project: A Platform Game Drawing on Canvas HTTP and Forms Project: A Pixel Art Editor (Part 3: Node) Node.js Project: Skill-Sharing Website A paper version of Eloquent JavaScript, including an additional chapter, is being brought out by No Starch Press. Other pages Code sandbox and exercise solutions Errata for the paper book This book as a single PDF file (& small version for mobile) This book as an EPUB file This book as a MOBI (Kindle) file The first edition of the book The second edition of the book The third edition of the book Translations Third Edition عَرَبِيّ (Arabic) فارسی (Persian) Español (Spanish, partial) Second Edition عَرَبِيّ (Arabic) Български (Bulgarian) Português (Portuguese) Русский (Russian)",
    "commentLink": "https://news.ycombinator.com/item?id=39629044",
    "commentBody": "Eloquent JavaScript 4th edition (2024) (eloquentjavascript.net)922 points by vajdagabor 20 hours agohidepastfavorite174 comments samtho 18 hours agoThis is, in my opinion, the book to use to learn JavaScript at more than a surface level. The only other materials I recommend as much (but for a different level of learner) are the “You don’t know JavaScript” in-depth book series. In 2015, I was consulting for a distance learning program, administered by a major California University, that wanted to replace their current textbook (one of those “Head First” O’Reilly books) with something that had a bit more meat but was very approachable. I immediately recommended this and it was fawned over by both the advisors and instructors. It was also the cheapest option they had in the running (even excluding the fact it can be read for free) as it was competing against traditional text books. One year later, students were polled on it and it was met with a lot of positivity as well. reply Sn0wCoder 18 hours agoparentOne hundred percent agree on both this book and the “You don’t know JavaScript”. Both are free (search Kyle Simpson GitHub). I ended up buying the YDKJ first edition paperbacks but see the second edition of all of them are done or a work in progress. EJs is the one I tell my students to start with as IMO is more of a programming book that just happens to us JS. I can tell the ones that read it (it’s optional) and the ones that do not. I do a few random chapters every year and learn something that I either missed or forgot I knew every time. I am mostly using TS these days but also enjoy vanilla JS for side projects and prototypes. Note the YDKJ books can come off as very ‘only my opinion is the right one’ kind of like JS the good parts but just look past that and absorb the content, what you do with it after that is up to you regardless of the author’s opinion. reply shanusmagnus 7 hours agorootparent> Note the YDKJ books can come off as very ‘only my opinion is the right one’ kind of like JS the good parts I got the same feeling and it's very off-putting. KS seems like a dick. It's ironic that so much of his dickishness seems to be reacting against what he takes as Doug Crockford's dickishness. Ah the irony. reply akmittal 3 hours agorootparentWent to JSConf India, specially for Kyle Simpson talk. His whole talk was just how his company can be a game changer for web development. Just so disappointing reply synergy20 17 hours agorootparentprevYDKJ 2nd edition after first two parts the rest seems less actively worked on? hope the 2nd revision will be done soon. reply brailsafe 12 hours agorootparentI think Kyle Simpson is out of work and looking for something full-time and also started some other company, so probably not his main priority atm. reply mtalantikite 17 hours agoparentprevI remember reading the first edition back in maybe 2011 when I decided I should sit down and actually learn JavaScript beyond the superficial grasp I had of it from working on web things (mainly via Rails at the time). What a great book, I learned so much from it at the time. For years after, whenever anyone told me they wanted to learn programming, I told them to pick up this book as a first introduction to it. Plus, it was available for free on his website. I've gotten rid of lots of old programming books over the years, but I've held on to my first edition copy of Eloquent Javascript. Lots of thanks to Marijn for writing this! reply iwsk 10 hours agorootparentI don't think this is a good book for someone who wants to learn programming for the first time. reply redbell 14 hours agoparentprevI second JavaScriptInfo as another great resource to learn JS: https://javascript.info/ reply neiesc 13 hours agorootparentI agree reply kabes 16 hours agoparentprev+1 for \"You don't know js\", it's a must read for any js programmer IMO. I haven't read eloquent JS though, you say it's a different level of learner. Can you expand a bit? Is Eloquent for after \"you don't know js\" or vice-versa? Edit: nevermind, reading the TOC of eloquent JS gave me a good enough idea reply deprecative 14 hours agorootparentI get so jealous that people can absorb information via books as an adult. I can read the same chapter a hundred times and nothing sinks in. I know this is off topic but do you, or anyone passing, have a system or tips for how y'all do this? I've got so many programming books but they only collect dust after I read through them without benefit. reply anthomtb 13 hours agorootparentAre you reading programming books the same way you would read a fiction book? If so, stop doing that. Programming or any technical learning is a hands on experience. Take notes and apply techniques, using pen-and-paper or the keyboard, as they are presented. If you really have to just \"read\" a technical book, IME a less-is-more approach works best. 5-10 minutes at a time, not even a chapter at once. Maybe a few paragraphs if its something really information dense. Funny aside, I find my morning \"business\" is the perfect time to this sort of reading using the Kindle app on my phone. reply berkes 13 hours agorootparentI read about fifty books a year for a decade now. Only about four of them per year, are programming books. They are indeed to be read very differently. Here's how I read them, ymmv. - never in bed. never as audiobook. But sitting. At a table or desk. - no distractions. At most \"focus music\". - read a chapter through. Then read it again and do all excercises (on a computer without wifi) - make copious notes, highlight quotes, summarize. Most important for me is to write down why I made that note. - a time (years sometimes) later, do it again. E.g. when having worked on the concepts from a book in real prod projects. - at most an hour. I have ADHD and my mind often flies everywhere suddenly; time to give up and grab a beer or coffee. Very few of these work as ebook (Kobo) for me. The formatting of code is poor and diagrams unreadable. Prefer paper or PDF (but read on a computer or tablet without network). reply anthomtb 11 hours agorootparentI agree with all your bulleted points. Especially the \"at most an hour\". Maybe its age or outside responsibilities, but mental fatigue is a real thing for me, and spending more time than that on technically-challenging materially brings rapidly diminishing returns. > Very few of these work as ebook (Kobo) for me. The formatting of code is poor and diagrams unreadable. Prefer paper or PDF (but read on a computer or tablet without network). My ancient (2011) Kindle is borderline useless for technical material but the iPhone app renders diagrams and equations acceptably well (my experience, of course). The small form factor of the phone is helpful too - more desk or table space for notebooks. Mind you, I make heavy use of app limits and downtime so its not the distracting experience of typical smartphone use. reply rapind 2 hours agorootparentprev> do all excercises (on a computer without wifi) And make sure you are typing the code, and not just copy pasta. Writing it out helps most people process it. reply vadman 11 hours agorootparentprev> Most important for me is to write down why I made that note. My ears perked at this. Great advice! reply skydhash 12 hours agorootparentprevI don’t mind distractions when learning via books (I wouldn’t be able to finish my degree otherwise), but the other advices still apply. Reading a chapter without doing the exercices is like listening to a lecture without taking notes. You may understand a few things (or everything), but you’ll find that doing practice, drawing diagrams, or summarizing it lead to a deeper understanding. More often than not, you have to dedicate a few days or weeks depending on how dense it is. You find yourself rereading a page from a previous chapter or consulting another book. You don’t have to read it end to end unless you view it as taking a course. reply noufalibrahim 4 hours agorootparentprevYup. The key to learning is doing. You need exercises at the end of every chapter if you want to learn. Math books, programming books etc. And they need to be challenging. That's what helps retention and understanding. reply ornornor 5 hours agorootparentprev> I find my morning \"business\" is the perfect time to this sort of reading using the Kindle app on my phone. Beware of hemorrhoids later in life if you’re spending too much time sitting on the throne. reply riidom 13 hours agorootparentprevWhat I like to do, when not under time pressure, is to read such a book cover-to-cover, as a first pass. I get an idea which parts will be difficult for me, and which are probably not. In this stage, I don't sweat the parts I don't get, I just note for later, that, if these turn out to be important, will require some care, patience and time. I also build some sort of \"map\" in my mind about the elements that exist, to get an overview. After doing so, I feel a lot more confident to tackle the topic of the book \"properly\", doing exercises, etc. reply notapenny 13 hours agorootparentprevTake notes as you go or by chapter. If a chapter has a summary at the end, read that first before going through the chapter. If there are code examples, write them out and play around with it. Get the important bits out that way. Also, realistically you probably won't remember most of what you read. I suck at that as well, but you do build up a lot of peripheral knowledge. You may not remember how to do that one thing, but maybe you do remember that it exists, or that it was in a particular book. Just that type of knowledge has worked well for me. reply nestes 14 hours agorootparentprevNot the original poster, but I learn best reading textbooks cover-to-cover. A couple things that help me: 1) I buy physical textbooks and absolutely destroy them with notes in the margins, highlighting, etc. It helps me to interact with the material instead of letting it wash over me, which means I'm both thinking about it more in-depth and as side effect I'm less bored. Otherwise I'll fall asleep and won't learn anything. 2) I accept that I'm not going to remember the entire book. A lot of books are most useful as references anyway. But if you ever find yourself going, \"Oh that's really handy to know,\" then you can make a special note of it or even put it into flashcards. I've been using Anki. The trick is to recognize what is actually worth doing this for. 3) If something is especially worth knowing, (see point 2), see if you can either do problems from the book or try out the concept in some way if there are no problems available. If you're reading something just because you feel like you should, you won't get anything out of it (or a least I don't). reply richrichie 7 hours agorootparentI was a bit like you, read everything cover to cover, until i read Natsume Soseki’s Kusamakura, in which he says: “Reading a book from the beginning to the end is like insisting on marrying every woman (or man) you fall in love with.” And that opened my eyes and now i no longer feel in debt to the book or the author :) reply dr_kiszonka 14 hours agorootparentprev1. Take notes. If you are reading an ebook or a webpage, handwritten notes may be better because you will not be tempted to copy and paste. 2. If feasible for you, consider getting a neuropsychological evaluation to rule out any learning disabilities like adult attention deficit disorder. reply IlliOnato 6 hours agorootparentprevWhen reading a book, I don't do anything special, but I frequently stop and think about what I've just read. It's not something I do by a command, either: it's just that a good book engages my attention, and then I kind of \"chew\" on it. Unlike others here, I never take notes, and rarely do suggested exercises. But I read and think through examples; and as to exercises, I do think of \"how I would approach it\" and \"what is that the author wants me to learn from this exercise\". reply anta40 5 hours agorootparentprevThat's how I learnt programming (self study) as a high schooler more than 2 decades ago: by studying books (affordable fast internet connection wasn't a thing at that time). Start from chapter 1, study the concepts and play with the code examples a few times till you understand. Usually there are exercises at the end of each chapter. Try to work on those. reply sn9 10 hours agorootparentprevSpaced-repetition and active recall are underutilized tools that can increase your retention of material you deem worth remembering. Here's an example of how to use it to learn quantum mechanics, but you can imagine how it would translate to technical books for software developers: https://www.youtube.com/watch?v=OFuu4pesKf0 reply irrational 12 hours agorootparentprevEveryone has different learning styles. I retain maybe 90% of what I read, but only 10% of what I hear. So videos and any audio are the worst formats for me to learn anything. The first step is figuring out what your learning style is. reply matt_j 9 hours agorootparentThis is debunked to some extent. Veritasium made a good video about it. https://www.youtube.com/watch?v=rhgwIhB58PA Personally I find a multi-pronged approach is necessary to really learn anything. Read it. Read it again. Visual guides are helpful. Work some examples. Make some mistakes, debug them, find the corner cases, write tests. Eventually when I've poked around the material for a while it starts to bed in. Three months later it's forgotten, but when I learn it the next time I move a lot faster! reply IlliOnato 6 hours agorootparentFor me the essential part of comprehending new information is my own thinking on what I'm getting. When reading a book, I stop frequently to think, it's quite natural for me. Often go back a few paragraphs or pages, re-read them with the new understanding, think again, go ahead... All of this is possible with video and audio in principle, but much less natural and much less convenient; also video somehow \"hypnotize\" me and I don't feel the urge to think about what I see and hear at the moment; perhaps only afterwards if at all. I have a feeling \"oh I get it\", but not much remains afterwards. So I absolutely prefer text to audio or video when learning. reply Sn0wCoder 11 hours agorootparentprevNote sure if you have gone very far into the Eloquent JS book but if you do it online it’s interactive. You read a little bit and then write code directly on the page. You then run it to see if it works or go back and read what you missed to correct it. Don’t get me wrong you cannot do the ‘projects’ and other parts are meant to run local, but for learning something quick the online version is all you need. Read, program, read, program….. reply samtho 13 hours agorootparentprevDisclaimer: this is my own, metaphor-filled hypothesis. It’s easy to fully internalize generalizations that someone has presented to you, usually because it’s wrapped up in a way that is mostly compatible with how you see the world. Because someone has done the work of distilling this information, you are not able to share in most of the intellectual benefit. It’s the process of having a question, discovering an answer, internalization of the concept, and synthesizing a summary that brings you closer to understanding. It feels like a series of “lightbulb moments” when we consume this content, but it is often shallow and fleeting because the genesis of the idea was not your own. Airport/self-help books are a good example of this mental candy that makes us feel good when reading it but, unless you are able to fully internalize it, are just empty mental calories. Comprehension and understanding are derived from amassing knowledge a little bit at a time as your mind is ready to advance your understanding. In other words, you really have to be “primed” for knowledge found in books. This is the reason that information you obtain after an adequately-informed struggle stays so salient: your mental model had all the scaffolding around a concept but there was a clear “void” where this answer fits, and if you find that fits super cleanly, it’s extra satisfying. When consuming information-dense material, you end up with this knowledge back-pressure where it’s floating around your short term memory but won’t be committed to long term memory because there is just no place for it yet. When recalling information in order to create new content with it, and assuming you are challenging yourself, you will end up in situations where there is information starvation (the opposite of back-pressure) where you must either find answers (to fit in your mental model) or find a workaround (ignore the gap). Some people can just read books and create efficient mental models. Others (myself included) have a limit of how much they can learn in one sitting because our brains need to be fully ready to accept this information. The last piece of this is the dopamine response cycle that is a positive feedback system (positive as in “more creates more”). Dopamine makes us feel good by rewarding behavior that evolution deemed necessary for species survival (over simplification). Dopamine indirectly triggers long term memory retention because we need to remember why we felt good in that moment so we can replicate it. This used to be, “this berry tastes good, I should remember where this bush was and what the berry looked like.” In the modern context, it ultimately delivers motivation to us because we have adapted to using it to learn new information that is less relevant to survival. Achieving goals and solving problems at hand causes a huge amount of dopamine release. The problem is that we’ve found ways to hijack these systems and short-circuit this feedback cycle so everything had to become way more stimulating. This got long, but my advice is to work on a project of your choosing that you are intrinsically motivated to do. Begin work on that project and read some of the book that relates to the problem at hand. Repeat this cycle of reading and working, trying to incorporate patterns and concepts you find while reading into your project. Make little goals for yourself every day before your start, and really hold yourself accountable in finishing them. reply idk1 2 hours agoparentprevIf I put \"You don’t know JavaScript\" into Amazon I get approx 10 books at £20 each, is there a specific one / author I should look for, of even better and ISBN. Thanks! reply m4tthumphrey 12 hours agoparentprevThank you for mentioning YDKJS. I hadn’t heard of it before and am in the beginning of my first proper JS project for about 10 years; man has the JS world changed. I have already skimmed the first few chapters and am already understanding it all very concisely. I will be reading this properly via GitHub for sure. reply pwb25 17 hours agoparentprevyes, also javascript the good parts is really good(hehe) because it's more theoretical and there is not html or web stuff in it just the language reply TehShrike 17 hours agorootparentHow recently have you read it? I thought it was a worthwhile read when I first read it 12 years ago, but I picked it up to skim a couple years ago and was struck with how much of it was irrelevant for modern JS due to changes in the language spec. reply vanderZwan 17 hours agorootparentIn that sense an annotated \"this is what JS used to be when this book first came out\" version could be of historical interest. reply pwb25 17 hours agorootparentprevLast time was 2017 I think, don't know if there is a big difference I use both new and old things reply memonkey 17 hours agoparentprevIf you want a video series that accomplishes a lot of the content in these books it's Will Sentance's Javascript: The Hard Parts. reply willsentance 15 hours agorootparentCheers Josh :) reply wes-k 17 hours agoprevFor those that don't know the author, Marijn Haverbeke is the creator of CodeMirror (code editor) and later ProseMirror (text editor). https://codemirror.net/ https://prosemirror.net/ reply jamager 44 minutes agoparentProsemirror user, it is fantastic. Didn't know Marijn was so prolific, all with free/open source projects. Wonder how can make a living... reply felideon 7 hours agoparentprevI'll always know him as the author of Postmodern, an impressive CL library for PostgreSQL: https://marijnhaverbeke.nl/postmodern/ reply NlightNFotis 17 hours agoprevFancy seeing this here, some days after finishing the third version :) I'm also glad to see the asynchronous programming chapter significantly reworked - it was materially weaker than the rest of the book because of some weird analogies involving crows and their nests that didn't seem to make any sort of sense to me. The third edition also gave me the impression that it was a reasonable book to learn JS and the DOM (and a sprinkle of Node.js, for good measure), but that it was a book aimed primarily at experienced people who were transitioning to JS and the web - not beginners (despite the book's efforts at claiming suitability for beginner programmers). reply Jerrrry 16 hours agoparent>book because of some weird analogies involving crows and their nests that didn't seem to make any sort of sense to me. I am glad I am not the only one. I believe he over-abstracted it to it's own detriment. I went to purchase a paperback earlier this week. Now I will wait for this one to hit print. reply aydoubleyou 15 hours agoprevI don't consider myself a good programmer. I struggled throughout my youth to grasp even the basics. This book pointed me in the right direction. Can't recommend it enough. reply tracker1 2 hours agoprevLike that this now includes a chapter on Node. I think it would be nice to see a follow-up book in a similar style that covers a bit more with Node, Deno and Bun. I really like the Deno approach so far. I prefer TS mostly these days as well as the esm stroke modules. I think node just made usage harder in their approach. I understand why, I still disagree on the solution. reply svat 18 hours agoprevI love this book, even since its first edition. It's very clear even on elementary stuff, e.g. see the section on bindings/variables: https://eloquentjavascript.net/02_program_structure.html#h-l... — avoids the pitfall of thinking of variables as “boxes”. I was trying to find what's new in the 4th edition, and following links from the author's website https://marijnhaverbeke.nl/ found this on Mastodon (https://mastodon.social/@marijn/112020092273623390): > The FOURTH EDITION of Eloquent JavaScript is now online, adjusted to the realities of 2024 and generally touched up. reply arrowleaf 18 hours agoparentThe book is actually maintained on GitHub too, https://github.com/marijnh/Eloquent-JavaScript. When I was just starting out I read this book and noticed a small mistake, I was really proud that one of my first contributions to any open source project was a PR to this repo. reply svat 16 hours agorootparentThanks, that gives a way of seeing the diff between the third edition and the current (last commit 45 minutes ago): https://github.com/marijnh/Eloquent-JavaScript/compare/f8f00... reply DenverSWE 18 hours agoparentprevLooks as if there will still be a paperback version released in the future as well for anyone that prefers that format. I don't work in JS at all professionally but this book has intrigued me for a while at this point after seeing it recommended so often. I think I'll pick up a copy once the paperback is released. reply ricardobeat 18 hours agoparentprevIt's a great explanation, but I've never heard the term 'binding' used to describe variables. It's usually reserved to function binding or bridge APIs like the DOM. The tricky thing is that \"boxes\" are the right abstraction for primitive values. After that you need to explain how references work, and that's pretty much the same 'tentacle' concept. This method spares the reader one step, but might cause confusion once they face problems that require that understanding. reply svat 16 hours agorootparent\"Binding\" is used thousands of times in the language standard, including for variables: https://tc39.es/ecma262/#sec-variable-statement -- that's my point, that this book is precise while being approachable to beginners. And I dispute the claim that \"boxes\" are the right abstraction for anything in JS. (Boxes may work for primitive values, but nothing further.) Directly seeing names as bindings (\"tentacles\") not only skips the incorrect \"boxes\" step, but also causes no confusion or problems whatsoever at any point. (If you have an example, I'd be curious to see it.) (There are some differences in the language between primitive values and Object, but none of them are particularly helped treating variables as boxes AFAICT.) reply ricardobeat 8 hours agorootparentI meant in the context of writing code - you’ll never hear anyone refer to their “bindings”. It breaks down with the simple `let a = 22; let b = a` example where the tentacle/binding metaphor can lead to wrong intuition of why a change to the value of a is not reflected in b. reply svat 4 hours agorootparentIn what way does it break down? Maybe if one is thinking of boxes, going by \"a change to the value of a\"? The book says: > When a binding points at a value, that does not mean it is tied to that value forever. The = operator can be used at any time on existing bindings to disconnect them from their current value and have them point to a new one In this case: let a = 22; The binding a points to the value 22. let b = a; The binding b points to the same value that a points to, namely 22. a = 23; The binding a now points to a different value 23. The binding b is not affected. I've linked it a few times in this thread, but see the equivalent article for Python, which explains it clearly: https://nedbatchelder.com/text/names1.html (It cannot break down because it's how the language works.) reply throwanem 7 hours agorootparentprevYou're struggling due to a conflation of two concepts. Binding refers to associating a name with a value. Assignment is a case of binding, but not the only one; two other examples are positional arguments in a function signature, and ESM imports. A binding can be mutable (let assignment, function arguments) or immutable (const assignment, ESM import). Value mutability is orthogonal. You can mutably bind a primitive value as \"let a = 22\" and then mutate the binding via reassignment, but you can't mutate the value itself; you can immutably bind a reference value as \"const b = {}\" and mutate the referenced object via property access, but you can't mutate the binding. I refer to bindings all the time, where it's useful to be clear in meaning. I also make sure to introduce the concept to mentees who aren't already familiar with it, and by all reports thus far it's proven as valuable an abstraction for them as it did for me when I first learned of it. reply NlightNFotis 17 hours agorootparentprev\"binding\" is a PLT term, denoting the association between a name and a value. It's a higher level concept than the variable - a mutable binding is what people usually refer to as a variable, and an immutable binding is the correct term for what people refer to an \"immutable variable\" (an oxymoron, if you think about it). reply skitter 17 hours agorootparentImmutable variable isn't a oxymoron. It can still vary between instantiations. If you have (a)=>{const b = a}, b can have different values even though it can't be reassigned. reply NlightNFotis 17 hours agorootparentIn the case of the code that you have cited, these are all different elaborations of the binding at the invocation of the lambda, due to the interplay between activation records and scope rules. It's not really an \"immutable variable\" - it's a local binding getting bound to different values on each scope entry. EDIT: By the way, the `b` binding in your code can be modified. Did you mean `const b = a;` ? reply skitter 15 hours agorootparent> it's a local binding getting bound to different values on each scope entry. It is, I just wanted to point out that the term \"immutable variable\" is sensible. I think a good way to put it is that b is a variable, and when the statement runs a value is bound to b. So the value bound to b varies yet b can be immutable, in contrast to a constant which is a binding to always the same value. > Did you mean `const b = a;` ? Fixed, thanks :) reply jameshart 18 hours agorootparentprev1) the most common binding used in well written modern JavaScript is a const binding. It is by definition not a variable. 2) the ‘binding/tentacle’ metaphor works just fine for primitives and the ‘boxes’ model adds more complexity. reply michaelcampbell 18 hours agorootparentprev\"binding\" is pretty standard terminology in some oldtimey languages; lisp, ML, etc. reply kazinator 16 hours agorootparentBash documentation says that \"set -u\" traps variables that are \"unset\". The actual diagnostic itself is better educated: $ set -u $ asdasdf $ echo $asdasdf bash: asdasdf: unbound variable reply pier25 18 hours agorootparentprev\"binding\" seems like a more casual term for memory pointer. I guess if people are just getting started with programming it make sense to simplify things a bit. reply kazinator 16 hours agorootparentIt's not a simplification. It is abstraction. Binding doesn't imply a particular implementation. A variable binding can disappear entirely. If you bind var x = 42 and never use it, the variable need not exist. If it doesn't exist, then there is no pointer. If you do use it, constant propagation/folding can again make the variable disappear. If the variable is never assigned, all uses of it can be replaced with 2. Variables that care captured by lexical closures can be treated differently from ones that are not. Variables captured by closures but not shared among different closures, or not mutated, can be treated differently from mutated variables shared among closures. The abstraction of binding is more complicated than \"every variable is a memory pointer\" because it allows more possibilities, which can coexist. reply svat 16 hours agorootparentprevMy point is that it's not a simplification, it's precisely how the language works: bindings between values and names (JavaScript has no separate notion of memory pointer; everything is a \"pointer\"). (Similarly for Python: https://nedbatchelder.com/text/names1.html) Describing variables in this way gives readers the correct understanding, and the analogy of tentacles is no harder than that of boxes. Such things are what I most appreciated, that the author manages to be approachable without sacrificing accuracy. reply pier25 7 hours agorootparentIf the explanation doesn't mention memory positions then it is a simplification. reply svat 4 hours agorootparentJavaScript does not provide a way for the programmer to access memory positions (and e.g. does not guarantee that the the language runtime will maintain unvarying memory positions), so it doesn't make sense to talk about memory positions: there's nothing to be gained at that level of abstraction. (Unless you'd call it a \"simplification\" to not talk of electrons and transistors too, in which case fine, yes it's a simplification in that sense.) reply MikeTheGreat 18 hours agoparentprevI'm curious - could you expand on why it's a pitfall to think of variables as boxes? reply svat 17 hours agorootparentBecause that's not how variables work in JavaScript/Python etc (though it may be fine for say C++ in the case of value types and copy constructors). For example: I'm typing on phone so for a quick example: let a = []; let b = a; a.push(1); and consider the value of b now (or the fact that we could write \"const\" above because the binding is constant, even though we mutate the value). Or see the equivalent for Python by Ned Batchelder, which has more examples and elaboration: https://nedbatchelder.com/text/names1.html reply MikeTheGreat 16 hours agorootparentI knew that in Python all variables are really references to objects (even when we're using a number) - is JavaScript the same way? Also, does anyone have a link/reference to the place in the spec where it specifies this? I briefly skimmed through parts of [1] but couldn't find anything that says that JavaScript treats numbers this way. [1] https://tc39.es/ecma262/multipage/#sec-intro reply svat 15 hours agorootparentAs I understand it, JavaScript does have a distinction between the primitive types and Object, but this does not show up in any significant way to the programmer / in any way relevant to how to view variables. To put it differently, the \"boxes\" view only applies to primitive values, while the \"tentacles\" view applies to all values, so the former is unnecessary and need not even be considered/taught. reply felipefar 17 hours agoprevPart of the force of this book comes from its explanation of fundamentals of computing, and how it relates to javascript. Another part is due to how interesting are the projects that it proposes that the reader build. I don't even like programming in javascript but was drawn to read the book. reply ibobev 17 hours agoprevI'm currently going through a hard copy of the book's third edition. But I'm wondering whether the description of the language in the book is detailed enough. Could you share some opinions on whether it will be good to go through some other JavaScript books after it? I'm considering going through \"JavaScript: The Definitive Guide\"[1] or \"The Modern JavaScript Tutorial\"[2] after it. [1] https://www.amazon.com/JavaScript-Definitive-Most-Used-Progr... [2] https://javascript.info/ reply XeO3 16 hours agoparent\"JavaScript: The Definitive Guide\" does go deeper, with thorough examples in all the topics mentioned in the index. It also provides examples of static types using Flow instead of TypeScript. reply jmkni 17 hours agoprevChapter 11 on async is particularly good, I still get confused by async/promises sometimes in Javascript - https://eloquentjavascript.net/11_async.html reply veggieWHITES 18 hours agoprev>The field of programming is young and still developing rapidly, and it is varied enough to have room for wildly different approaches. This was an interesting line of thought to digest. He's right, of course. Programming is probably still in it's infancy. Studying and learning about programming however, can make you believe it's some ancient art mastered by the giants of our recent past, the perfection of which is never to be surpassed again. reply gassiss 9 hours agoprevSeconding the sentiment on this thread, I used this book to learn JS 5 years ago, and it's awesome. I've never seen another resource as good. YDKJS is more of an advanced treatment. If you're a beginner it feels academic, while Eloquent JS is very practical and approachable. reply pojzon 2 hours agoprevTo learn js I recommend „WAT” tutorial. Nothing changes in last few years. We still go in with what we incorrectly assumed at the start. reply jgord 4 hours agoprevLooks pretty solid. CH 5 : \"Higher Order Functions\" seems a nice segway into one of my favorite, lesser known js util libraries Ramda.js reply cradle 3 hours agoprevWhere is the physical copy of the 4th edition? I can't find it. reply jslakro 9 hours agoprevFirst jquery, then React, now this book reaches a new version. Good times for JS reply greenie_beans 18 hours agoprevthis book taught me javascript! great book, highly recommend it. reply begueradj 18 hours agoparentI agree. Have you also read \"JavaScript: The Good Parts\" ? reply davidroetzel 18 hours agorootparent\"JavaScript: The Good Parts\" was a great and a very important book back in the day. I am sure it inspired many of the improvements JavaScript has seen since then. But as it has seen these improvements, and as they were many indeed, I am not sure the book is still as relevant as it once was. Or to put it differently: There are many more good parts to JS these days compared to when the book was released :) reply greenie_beans 18 hours agorootparentprevnope, looks like a deep cut and will fill in some knowledge gaps i have. thanks for the rec. another good one for my learning was \"secrets of the javascript ninja\" reply regus 15 hours agorootparentSecrets of the Javascript Ninja is my favorite JS book by far. reply radicalriddler 11 hours agoprevFrustrating that the online version uses weird format for it's code blocks, which leads reader view (Mozilla Readability) to not put code in atag or atag and leaves it as plain text. reply jimhefferon 11 hours agoprevDoes the author ever talk about why he is publishing a free version along with a more traditional paper version? reply ravenstine 16 hours agoprevI don't mean to throw shade on the whole book, but I don't think the section on errors takes things in the right direction. A distinction should be made between errors and exceptions. In JavaScript and many languages, we conflate the two and use exception handling as logic flow control. In my experience, this can end up being a headache and encourage unnecessarily weird structuring of code. Look at this example from the page on errors: --- function getAccount() { let accountName = prompt(\"Enter an account name\"); if (!Object.hasOwn(accounts, accountName)) { throw new Error(`No such account: ${accountName}`); } return accountName; } --- The possibility that a user will enter a account name that doesn't exist is not an exception, but we are treating it like one in this case. In order to handle this exception when getAccount is called, we have to wrap it or some higher level scope in a try-block and then regex-match the error message if we want to handle it differently from other errors. You might be saying \"it's just an example\", but there's plenty of production code in the wild that is written this way. Maybe this could be improved by subclassing Error, but now you're having to manage a bunch of clutter and using object-oriented features as a way to reliably determine what kind of exception you're dealing with. I find this pattern to be preferable: --- const ACCOUNT_NOT_FOUND_ERROR_CODE = 1; function getAccount() { let accountName = prompt(\"Enter an account name\"); if (!Object.hasOwn(accounts, accountName)) { return { accountName: null, error: { code: ACCOUNT_NOT_FOUND_ERROR_CODE, message: `No such account: ${accountName}`, } }; } return { accountName, error: null }; } --- Then we can call the function like this: --- const { accountName, error } = getAccount(); if (error) { if (error.code === ACCOUNT_NOT_FOUND_ERROR_CODE) { errorModalService.show(error.message); } } else { // do something with the account name } --- No doubt, you may still want to catch exceptions at a higher level scope, but at the nice thing here is that exceptions (almost) always represent actual unexpected conditions that aren't being handled properly while return values with error codes represent expected conditions and can be handled like any other logic in your code. It also reduces any ambiguity of how an error should be handled but without subclassing. An error can even contain more information than just a message if you want it to. Also, if you really want to ignore an error for some reason, then you can just pretend that the error doesn't exist. No need to use a try-catch where the catch-block is a no-op. I wish we'd encourage this sort of pattern, but maybe that's one of many pipe dreams of mine. reply svachalek 15 hours agoparentIn other languages like Java this can work because of checked exceptions (can work, I know there are a lot of mediocre devs who don't know what to do with checked exceptions but that's another issue). But in JS this is a terrible way to deal with any problem that can be handled. reply brailsafe 12 hours agoparentprevI'd agree, but incidentally I've also been extracting string labels into some higher level, such as a constant or message creator function that looks up the message by it's code in some dictionary. It helps with readability, particularly in modern view libraries, to just have all your labels in one place and not have to scan the JSX or HTML for the string literals, and likewise all modifications will produce more concise diffs, testing could be easier if you're doing string matching, localization is more manageable with fewer scattered external dependencies on translation hooks or w/e reply Exoristos 14 hours agoparentprev1. Exceptions can have codes, too, you know, and often do. 2. In the example, the exception is meant to bubble up all the way to the consumer. Different consumers can then handle as they see fit, even just displaying the exception message to the user. reply ravenstine 13 hours agorootparentYes, exceptions can have codes. As far as I have experienced, their error instances don't do anything better than a plain object, and a non-exception error object can more clearly take a wide variety of shapes. It also limits potential confusion between errors or exceptions raised by your code and that of some dependency. It is true that exceptions \"bubble\", and plenty of developers take advantage of that behavior. In my opinion, it is not helpful for errors that are expected to happen as part of normal application behavior. Bubbling of errors creates a sense of logical indirection. Errors as return values communicate that likely nothing \" wrong\" actually happened. Good communication through code reduces the amount of time developers spend using the wrong assumptions when debugging. There is also no reason why errors in a return value can't be returned by the caller. When you learn to love objects/hashes as return values, you can get the same benefits of bubbling but with a more clear path to follow when getting a picture of where a value is coming from and why. In the case of actual exceptions, like accessing a property on undefined, an error being thrown is appropriate because, like you say, it can be bubbled up. The nice thing about reserving exceptions for this sort of thing is you might get away with a single try-catch at the highest scope and have a single straight forward error modal for when an unexpected problem occurs. Then you can otherwise implement errors in your code without the possibility that they will escape through bubbling and trigger behavior that is not immediately obvious when following the path of execution. This is not to say that the tradition of using errors and try-catch for normal application control flow is inherently bad. Its just another tool. I do subjectively believe they are counter productive when used that way, and encourage others to try something like my approach. I think we would benefit by making it a standard practice. reply rofrol 11 hours agoparentprevlike Result in Rust reply ravenstine 10 hours agorootparentHaven't even looked at Rust code until just now, but yes, that looks like a more formalized version of this idea. Interesting! reply prudentpomelo 14 hours agoprevDefinitely one of my favorites. It helped me level up when I was learning Javascript but also helped me understand how to be a better programmer. reply richrichie 2 hours agoprev> Every now and then, someone comes up with a new way to circumvent the limitations of a browser and do something harmful, ranging from leaking minor private information to taking over the whole machine that the browser runs on. The browser developers respond by fixing the hole, and all is well again—until the next problem is discovered, and hopefully publicized, rather than secretly exploited by some government agency or criminal organisation. This is from the chapter on HTML and JS (emphasis mine). It is funny to see how govt agencies and criminal organisations are mentioned in the same breath. How did we end up here? reply slow_typist 13 hours agoprevHow does Eloquent JavaScript compare with Horstmann‘s JavaScript for the Impatient? reply justanotheratom 15 hours agoprevI wonder if such high quality free books are already fed to LLMs during training? reply 0xCMP 9 hours agoparentThey're open source and easily accessible, so I would practically guarantee some/most of them are using it and other open source books. Given so many are just on Github and very well known/linked-to I doubt they even put any special effort into getting these specific resources. reply synergy20 17 hours agoprevGreat book, looking for some info about main changes from 3rd edition but did not find it. reply looshch 9 hours agoprevdoes anyone know where can i get the list of changes compared with the previous edition? reply lioeters 9 hours agoparentFrom a comment above thread, here's the diff of current state against the 3rd edition: https://github.com/marijnh/Eloquent-JavaScript/compare/f8f00... reply degun 11 hours agoprevThis is what really taught me JavaScript. reply tithos 18 hours agoprevhttps://arc.net/boost/3098D5E3-F164-478B-9586-077889192460 The 'Eloquent JavaScript 4th edition (2024)' in Dark Mode reply ginkgotree 17 hours agoprevThe best JavaScript book out there, IMHO reply lazysalmon35 15 hours agoprevWhen is the print book coming ? reply panphora 7 hours agoparent> Paper book will take a while longer. According to the author, Marijn Haverbeke. https://mastodon.social/@marijn/112020092273623390 reply justanotheratom 16 hours agoprevWhat if one is using TypeScript? reply brink 16 hours agoparentThe rules still apply. reply TheRealPomax 14 hours agoprevThe one odd thing in it is that it still claims SVG markup needs a namespace. Which it doesn't, SVG became part of the HTML5 spec and emphatically should _not_ use namespaces when used as \"just another element\" inside an HTML document. (even though you do need a namespace when creating svg elements through createElementNS, both \"of course\" and \"unfortunately\", and of course you need namespaces if you're creating an actual stand-alone SVG document) reply coding-saints 13 hours agoprevGreat book reply mixmastamyk 16 hours agoprevI have Zakas' Professional JavaScript for Web Developers and ECMAS 6 update. Was very happy with them (easy to read) but both are getting long in the tooth and the author seems to have lost interest in updates. How does this one compare as an all-in-one? Being up to date is a win, but I'm wondering about the quality of the writing. reply msoad 19 hours agoprevThis is the book that helped me really understand JavaScript. Really great content and I can't thank Marjin enough for putting this out there for free. Also how this person is so productive? I really would love to read on how he manages his time and priorities. reply z3t4 18 hours agoprevSomeone asked about integer support in JavaScript. JS now supports BigInt! >2**57 144115188075855870 >2n**57n 144115188075855872n reply williamstein 17 hours agoparentAnd in some JavaScript engines (eg V8) bigint multiplication is asymptotically fast, unlike say the default CPython. It’s a very pleasant surprise if you happen to be a number theorist (say). reply csjh 7 hours agorootparentHow do you mean? CPython uses karatsuba's for large numbers which should be asymptotically fast https://github.com/python/cpython/blob/d864b0094f9875c5613cb... reply ethagnawl 18 hours agoparentprevThat's news to me! https://developer.mozilla.org/en-US/docs/Web/JavaScript/Refe... reply bovermyer 17 hours agoparentprevOh thank god. I've been having to do some hacky things to represent numbers on an astronomical scale for my star system/planet generators. reply alyandon 18 hours agoparentprevYes, I was asking about that and I'm very happy to see this exists now. reply llmzero 18 hours agoprevWhat's new in this edition? reply llimllib 17 hours agoparentyou can see the differences on github: https://github.com/marijnh/Eloquent-JavaScript/compare/3rd_e... From a quick browse: - # for private properties - ESM imports in node - hasOwnProperty -> hasOwn (TIL: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Refe...) - Math.pow -> ** - coverage of `function*` generators (https://eloquentjavascript.net/11_async.html#h-o+cFzGGhnz) reply bewuethr 16 hours agoparentprevHere's a diff without the first commit that changed all linebreaks: https://github.com/marijnh/Eloquent-JavaScript/compare/d8290... reply begueradj 18 hours agoparentprevIt should be mentioned in the \"Introduction\" but it seems your question is not covered there. reply e12e 17 hours agorootparentCould've used better commit messages, but: https://github.com/marijnh/Eloquent-JavaScript/compare/3rd_e... reply dcre 19 hours agoprevThis is my favorite book about JS, and I always recommend it to people. It occurs to me for the first time that the lack of TypeScript might be a problem, because if I’m making recommendations to someone learning, I am definitely going to recommend they write TS instead of JS. On the other hand it may actually be helpful to learn the concepts in this book without the additional syntax overhead of type annotations, plus the more webby content doesn’t really have much to do with types anyway. reply OmarShehata 19 hours agoparentI very much concur that it is better NOT to introduce TS when explaining JS fundamentals. I've seen smart engineers with a C++ background get tripped up on and very confused working with TS, because it's not clear to them what concepts are \"language fundamentals\" and what concepts are \"the TS transpiler\". (Like expecting that just because you declared something as type X, that it guarantees at runtime it will always be type X, but it won't. You may get data from an API that _says_ it returns type X but the contents don't match. That can be valid code that compiles and has weird runtime behavior) reply samtho 18 hours agorootparent> I've seen smart engineers with a C++ background get tripped up on and very confused working with TS, because it's not clear to them what concepts are \"language fundamentals\" and what concepts are \"the TS transpiler\". This is exactly why I never recommend TypeScript to new developers. A similar problem (that used to be worse) with people learning JavaScript is the lack of separation between JavaScript and the DOM + browser APIs. 10+ years ago, people have told me how much they hated JavaScript and when probed about it further, would admit that it’s actually the DOM or new/inconsistent browser APIs that have caused issued. JS has a number of its own flaws and quirks, yes, but there are two fundamental issues that make it harder to approach as a new learner (as opposed to, to say, Python) are how tightly coupled it has historically been to it’s primarily application case and how high or low level is this language? reply DarkNova6 18 hours agorootparentprevWhat helped me understand the Runtime behaviour of TS is to understand that TS doesn't actually have strong typing. If it was named \"LintScript\", its name would be much closer to the truth. reply epolanski 18 hours agorootparentThis statement is quite questionable starting from the fact that there is no definition for what strong typing is. Care to provide what you mean? Strict TS won't compile pretty much any type-unsafe operation. It's not perfect, the standard libraries are a bit too unsafe type wise, exceptions are untyped and need Either/Result-like data types to be handled but it's an extremely powerful language if you know it and it's ecosystem. Most people though don't even bother reading the docs and can't tell you what a mapped type is or what is a disjointed union, etc. reply DarkNova6 17 hours agorootparentIf you are in a completely self-defined world, then yes you can trust the compiler. But I once built a react component which was called from library code and it simply did not adhere to the method signature (this was for a material UI table extension). As soon as you have third party code calling your methods all bets are off. It could be JS calling your methods, or there simply is a hidden cast somewhere. This is the moment that you realize that type annotations really are just a compile-time fiction, much like Python. At least in my definition, this is weak typing as the variable is physically capable of changing to any type at runtime, despite its type annotations. reply Shog9 16 hours agorootparentTBF, this can also be true in C++, C#, probably most languages that can interoperate with other systems not written 100% in the same language + same runtime. After a while, you just get used to not trusting anything at the boundary - types are for your convenience and your internal logic, nothing more. reply DarkNova6 14 hours agorootparentYou are talking about something entirely different. Having weird interop is one thing. But having your internal state compromised because the language does not perform runtime checks is something entirely different. In case of C++ this would lead to desastrous memory corruption. If all data is dynamic you can't have a safe program as data on the stack must be monomorphic or you corrupt nearby memory. Naturally, you can defend against hostile input via excessive defensive programming (asserting against nulls, asserting against wrong types etc.). Or you simply use a strong static typed language. reply Shog9 10 hours agorootparentYes, that's why I used C++ as an example. It's very easy to create a scenario where the memory layout your logic (and your type-checker) assume exists just... doesn't. Core dumps from the field with \"impossible\" values, vtables missing entries, etc. reply steve_rambo 18 hours agorootparentprevDo you mean runtime type checking? Coming from compiled type safe languages, it was difficult to wrap my head around full type erasure too. Your name would have definitely helped. reply preommr 17 hours agorootparentprev> I've seen smart engineers with a C++ background get tripped up on and very confused working with TS, The idea that someone is good at another language so they'll automatically be good at another is a common misconception. In fact, they're likely to be worse because they're less likely to spend time trying to learn things from the ground up and less likely to write idiomatic code. It's especially bad with js/ts because of it's popularity (so lots of new programmers that complain about how NaN doesn't equal itself), and because it's the defacto web language so lots of people are forced to use it as a secondary language that don't want to spend time learning it. reply lost_womble 18 hours agorootparentprevAgree. I wish tools like zod/valibot were first class in typescript. reply epolanski 18 hours agorootparentAlso effect schema if you're looking for something much more powerful and in effect-land. reply myvoiceismypass 18 hours agorootparentprevIt is (or should be) common practice to parse/validate any external data you depend on. You should be doing this for Javascript too. I find that the library https://zod.dev/ is quite helpful for this reply diggan 18 hours agoparentprev> I am definitely going to recommend they write TS instead of JS Why is that? If you want them to learn JS, teach/recommend them to learn JS? Compile-to-JavaScript languages come and go, but JavaScript has remained. First learning vanilla JavaScript makes sense, and then add TS on top if you really have to. At the very least they'll be prepared for when TypeScript goes out of favor. reply wk_end 18 hours agorootparentTypeScript isn't really a \"compile-to-JavaScript\" language in the same way that, say, Rescript is. Modulo a few corner cases (e.g. enums) which are now considered anti-features, TypeScript has the exact same runtime semantics as JavaScript - you can (almost) convert TypeScript to JavaScript just by stripping away the type annotations (and, if various ECMAScript proposals go through, you won't even need to do even that). This is both a curse and the secret behind its success - it's arguably not a separate language, but instead an annotation layer on top of the existing language for encoding and checking your static reasoning and assumptions. reply diggan 18 hours agorootparentI guess by that same definition, Coffeescript isn't a compile-to-JavaScript language either, as it has the same semantics as vanilla JavaScript? I think that's besides the point. My point was more that people who are supposed to learn JavaScript, should do so by learning vanilla JavaScript first, then they can move on to learning whatever is currently hyped by the zeitgeist (which happens to be TypeScript currently). reply wk_end 14 hours agorootparentNo, CoffeeScript doesn't have the same semantics as vanilla JavaScript; not in any meaningful sense I can think of, certainly not in the way that TypeScript does. Most CoffeeScript will simply syntax error if you feed it into a JS interpreter (and vice versa), and there's no trivial syntactic transform to get around that (i.e. it's not just a new surface syntax over JavaScript). Various features in CoffeeScript have no equivalent in JS, so new code needs to be synthesized for them; even fundamental features that are shared by the two languages are different - for just one example, this [0] article shows that how a function is compiled in CoffeeScript is non-local - the compiler is tracking variable scopes to get around the fact that CoffeeScript and JavaScript have different scoping semantics. With TypeScript, the \"compilation\" process is (almost): parse(typeScriptCode) -> treeMap(removeTypeAnnotation) -> write That's it! TypeScript code is syntactically valid JavaScript code, just with added type annotations (and, as mentioned, soon that'll still count as syntactically valid JavaScript code); JavaScript code is syntactically valid TypeScript code without any type annotations - which doesn't make it syntactically invalid! Indeed, if you turn down the strictness settings on tsc to permit untyped code, the compiler doesn't even complain about it. [0] https://donatstudios.com/CoffeeScript-Madness reply svachalek 15 hours agorootparentprevCoffeeScript has quite a lot of code generation features, while TypeScript largely just removes code in order to transpile to JavaScript. The only exception I can think of is enums, and I suspect they wouldn't have put those in if they had it all to do over again. reply Solvency 18 hours agorootparentprevWhich further exposes the irony of casual hipsters who say things to me like \"Thank God for Typescript, it's like a whole other language than JS\". For example, a designer, who writes a quick one off plug-in for Figma in Typescript without realizing 99.9% could have been just written/pasted into the JS file (which is ultimately what Figma runs). reply wk_end 14 hours agorootparentTo be honest, in the appropriate context I will contradict myself and say that that you should think of TypeScript as a separate language. There's a lot of dynamic things you can do in JavaScript that will interact poorly with attempts to statically reason about your code with TypeScript; even if it's the approach you'd take in JS, when you add typing you should recalibrate how you approach the problem - that, to me, is an argument for thinking of it as separate language. ...but that's mostly just what I say to JavaScript devs, to be polite - to not tell them that their JavaScript code is bad, too. Code that's difficult to reason about statically is strictly worse, in my opinion, than code that's easier to reason about statically. And in that sense, TypeScript is just guiding you to write better JavaScript, which maybe undercuts its claim to being a language of its own. reply epolanski 18 hours agorootparentprevBecause TypeScript is the de facto standard way of writing JavaScript for most of the industry and it has killed all of the other compile-to-js languages. The chances of it going out of favour are very slim, there's a giant ecosystem built on it and the language is very well loved by devs (should be second only to Rust). Microsoft has 50 people on payroll working only on TS. Any competitor needs a gargantuan investment. reply diggan 18 hours agorootparent> Because TypeScript is the de facto standard way of writing JavaScript for most of the industry It really isn't, but I guess that's really context specific. What country and sector are you talking about? For US-SaaS, what you're saying is probably true, but there is a whole world outside of that, and JavaScript is with 99% certainty much more wildly used than TypeScript. > and it has killed all of the other compile-to-js languages. Also it hasn't. I've been writing ClojureScript for the last 5 years, almost exclusively. And while the community is small, I wouldn't say it's \"dead\" or been killed. There are a bunch of compile-to-JS languages that haven't \"been killed\", besides ClojureScript. But it serves basically the opposite niche compared to TypeScript. > The chances of it going out of favour are very slim Same has been said about everything, always, and it's always not true. Winds change, and surely so shall the winds of TypeScript. Not being able to see that it's possible, will put you at disadvantage. reply gr4vityWall 18 hours agorootparentprevTechnically Haxe is still a thing, though its typical use case these days is almost 100% game development (including the JS target). reply whiterknight 18 hours agorootparentprevAnd JavaScript has a bigger ecosystem and more entrenchment, not to mention an international standard. reply epolanski 18 hours agorootparentNot sure how's that relevant. Any valid JavaScript is valid TypeScript and you're gonna write and read TS anyway in the industry. They aren't in competition, but coming back to the first comment it sounds reasonable to start directly with TS for many users. reply whiterknight 18 hours agorootparentYour experience is not the industry. JS is widely used, almost certainly more than TS. reply epolanski 14 hours agorootparentMy experience may not, but the annual JS dev survey points that TS is the main way to write JS from years. reply DarkNova6 18 hours agorootparentprevand not to mention terrible. reply sanitycheck 17 hours agorootparentprevI use TS, I like it. I'm not sure you're 100% right about it overtaking JS for most of the industry. I wouldn't recommend it to beginners until they've learned JS because it's a lot of stuff to learn on top of JS to achieve basically the same outcomes (with fewer bugs). Chapter 5 in the Eloquent JavaScript book gets to higher order functions, which in TS means Generics. Nobody needs to learn Generics in Chapter 5 of their programming journey. You also don't really appreciate how useful TS is until you've battled at least one project in plain JS. (Arguably you can go a long way without HoF too, but perhaps not if you're hoping to understand other people's code.) reply mirekrusin 18 hours agorootparentprevFlow has about a dozen of important features that typescript lacks (while being 10x smaller in terms of LoC). reply dcre 17 hours agorootparentFlow is basically a dead project outside of Facebook. https://npmtrends.com/@babel/preset-flow-vs-flow-bin-vs-type... reply diggan 16 hours agorootparentNot that I think download metrics is the best metric to decide that, but even with that, flow-bin has almost half a million of downloads per day. That's far away from dead, at least in my world. And I'm guessing that doesn't count anything from Facebook as they most likely run their own registries. reply dcre 16 hours agorootparentflow-bin has substantially fewer downloads than coffeescript and both are declining. That is a dead project. https://npmtrends.com/coffeescript-vs-flow-bin reply mirekrusin 12 hours agorootparentIt's not as popular as typescript but not dead, it's consistently active for a decade [0]. Compare it with typescript [1] contributions if you want. [0] https://github.com/facebook/flow/graphs/contributors [1] https://github.com/microsoft/TypeScript/graphs/contributors reply dcre 16 hours agorootparentprevI don't want them to \"learn JS.\" Most likely the person in question is trying to write web applications, and I want them to learn what is most useful for that. After writing web applications for years without TypeScript and then for years with it, I cannot imagine going back. I can make complex refactors with very little worry. Thanks to type inference, most of the TypeScript code I write is indistinguishable from JavaScript, so the idea that learning TS takes away from learning JS is ridiculous. When I suggested that the extra syntax might be distracting at first, I meant for like... a week. reply rglover 18 hours agoparentprevIt's better if they learn the core language and then graduate to TypeScript (if at all, as there's active discussion of adding type annotations to the core language [1]). An anecdote: I was sold a similar story on CoffeeScript back in the day, stressed myself out learning it, only to discard it a couple years later. TS won't have an identical fate as its shepherded by Microsoft, but eventually it will go the way of the dodo (whereas core JS will still be humming along). [1] https://youtu.be/SdV9Xy0E4CM?feature=shared&t=380 reply cdme 17 hours agorootparentThis was the approach one of my first mentors recommended and it's served me well. Learn the core language and its features and then add tools/frameworks as needed. Lately it feels like we're teaching new devs popular frameworks and completely ignoring the fundamentals. reply linhns 16 hours agorootparentYep. But business requirements dictate what happens and these frameworks are the fastest way to achieve business goals, which sucks as when the frameworks lose their appeal, those devs are gonna struggle to transfer their skills to something else. reply irrational 18 hours agoparentprevIt is useful to know vanilla JS. Right now I am learning an integration platform that uses JS for its scripting language, but you cannot use TS. reply synergy20 12 hours agorootparentts-node with ts can do script the same as node with js? surprised why TS can not be used as it's 'safer' for scripting. reply alyandon 18 hours agoprevnext [5 more] [flagged] wk_end 18 hours agoparentThere's BigInt [0]. Probably less useful for this specific case, but possibly useful as well, are typed integer arrays [1]. [0] https://developer.mozilla.org/en-US/docs/Web/JavaScript/Refe... [1] https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guid... reply alyandon 18 hours agorootparentAh, very cool. Thanks. reply throwaway632 18 hours agoparentprev> handling integer math correctly JavaScript doesn't have integers. Everything is a float. https://developer.mozilla.org/en-US/docs/Web/JavaScript/Refe... Although it has BigInt now. reply alyandon 18 hours agorootparentYes, I'm aware that internally it is using floating point representation and that is one of the main reasons I avoid using it for anything math related. However, I am now aware that BigInt() exists so thanks for the link. reply illegalsmile 15 hours agoprevtest reply _obviously 14 hours agoprev [–] Sounds like a joke, an oxymoron. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"Eloquent JavaScript 4th edition\" is a book by Marijn Haverbeke covering JavaScript, programming, and digital concepts, available online or in paperback under a Creative Commons license.",
      "The book delves into language, browser, Node.js, with illustrations by diverse artists, offering resources like code sandbox, exercise solutions, and multiple book versions.",
      "It has been published in various editions and translated into different languages, making it a valuable resource for learners interested in JavaScript and the digital world."
    ],
    "commentSummary": [
      "Eloquent JavaScript 4th edition is praised for its accessibility and affordability, making it a top choice for diving deeper into JavaScript.",
      "'You Don't Know JavaScript' by Kyle Simpson is recommended despite mixed reviews on his writing tone, offering valuable insights for learners.",
      "Tips for effective learning from programming books are shared, emphasizing note-taking, time management, and active engagement, alongside discussions on variable binding in JavaScript and the debate between TypeScript and JavaScript in web development."
    ],
    "points": 922,
    "commentCount": 174,
    "retryCount": 0,
    "time": 1709819536
  },
  {
    "id": 39631139,
    "title": "Sweden Joins NATO for Collective Defense",
    "originLink": "https://www.government.se/press-releases/2024/03/sweden-is-a-nato-member/",
    "originBody": "Press release from Prime Minister's Office, Ministry for Foreign Affairs Sweden is a NATO member Published 07 March 2024 Just before 17.30 CET, Sweden’s instrument of accession was deposited with the United States Government, as depositary of the North Atlantic Treaty. Therefore, Sweden is now a member of NATO. NATO is an intergovernmental organisation with both a political and a military dimension. As a member, Sweden becomes part of NATO’s collective defence. Sweden is NATO’s 32nd member country. Press contact Hanna Strömberg Press Secretary to Prime Minister Ulf Kristersson Phone (switchboard) +46 8 405 10 00 Mobile +46 76 128 61 69 email to Hanna Strömberg Anna Erhardt Press Secretary to Minister for Foreign Affairs Tobias Billström Phone (switchboard) +46 8 405 10 00 Mobile +46 76 130 85 34 email to Anna Erhardt Share Facebook X LinkedIn Email The page is marked with the following categories Press release Tobias Billström Ulf Kristersson Ministry for Foreign Affairs Prime Minister's Office Foreign and security policy NATO Related Prime Minister Ulf Kristersson to address the nation Published 07 March 2024 · Press release, Webcast from Ulf Kristersson, Prime Minister's Office Deposition of Sweden’s instrument of accession Published 07 March 2024 · Press release, Webcast from Ulf Kristersson, Prime Minister's Office Foreign and security policy focus on Sweden’s NATO membership, the neighbourhood and support to Ukraine Published 14 February 2024 · Press release, Webcast from Tobias Billström, Ministry for Foreign Affairs Statement of Foreign Policy 2024 Published 14 February 2024 · Speech from Tobias Billström, Ministry for Foreign Affairs",
    "commentLink": "https://news.ycombinator.com/item?id=39631139",
    "commentBody": "Sweden Is a NATO Member (government.se)487 points by belter 17 hours agohidepastfavorite560 comments u320 15 hours agoOne thing that isn't mentioned a lot in connection to this: Sweden and Denmark has a VERY violent history. Hundreds of years of wars. It is said to be the worst conflict in all of human history. Now we are allied and committed to defend each other. That's a huge thing for peace. reply MasterYoda 4 hours agoparent\"It is said to be the worst conflict in all of human history.\" No it is not, that is incorrect. I really dont know where you get that from. And I dont agree that there have been a VERY violent history. Yes there have been wars, and all war are voilent, but not anything special for that era. Between 1200-1800 there was 15 wars. From 1-10 years and the majority of them lasted just 1 or 2 years. And fun facts: Of the 15 wars Denmark started 11 of them and Sweden won 11 of them. Sug på den danskjävlar ;) Sweden is a peaceful coutry and that contry that have lived longest in the world without war, over 200 years. All nordic countries are very similar in so many ways and our language are almost the same so we can understand, \"almost\" each (except the Danish people (rest of the nordic countries understand what I mean :), maybe that why there have been conflicts ;) We, the nordic countries see ourself more like a family, like siblings that love each outer but also love to tease each other :D reply MasterYoda 1 hour agorootparentI have to correct my self, the war length was not between 1-10 years, it was between 1-12 when I checked again. And about \"Hundreds of years of wars.\" I counted for fun how many years in total Sweden and Danmark was in war with each other between 1207 when the first war started to 1814 when the last one ended, and it was around 54 years in total, over a period of 4 centuries. (54 years are dependent how you count, if a war started 1207 and ended 1208 it could have been both 1 and 2 years long, so I counted 1,5 for all wars. So at best it was 47 years or at worst 64 with a mean time of 3,6 year per war). reply baxtr 14 hours agoparentprevI have many Serbian friends going for holidays in Croatia. Unthinkable 30 years ago. So maybe, in 50 years from now, Russia will join NATO, too? reply swat535 6 hours agorootparentIndeed, let's not forget our incredible human capacity for forgiveness. Two major peace treaties I pray to see every day is Palestine and Israel and for the Russians and Ukrainians to be brothers. Sometimes I wonder what a unified world look like, for example what would it look for Palestine and Israel becoming a single nation? For South Korea and North Korea to merge again in brother hood? For China and Taiwan ? etc. Like how boring would the news be in complete peace? what we wound be doing instead if we weren't busy killing each other? Just some questions I ponder sometimes.. reply empiricus 47 minutes agorootparentprevThe current NATO expansion happens only because those countries are scared of Russia. Also, by definition, NATO exists to fight Russia. reply przyszlem 13 minutes agorootparentThis is not true, NATO is a defensive pact. Only russian propaganda says differently reply AlGrothendieck 14 hours agorootparentprevWhile Secretary General, Ismay is also credited as having been the first person to say that the purpose of NATO was \"to keep the Soviet Union out, the Americans in, and the Germans down,\" a saying that has since become a common way to describe the dynamics of NATO reply red-iron-pine 12 hours agorootparentthe irony is that NATO is pushing Germany to aggressively and quickly re-arm if NATO wants America in then they need to do something about an angry orange political candidate... reply shrimp_emoji 9 hours agorootparentGermany will be dead in 50 years by demographic suicide anyway. (Like China, conveniently.) reply rainworld 11 hours agorootparentprev>aggressively and quickly re-arm On top of that, there's inflation and the value-added tax, which mean that, once all the extra costs have been covered, only about €50 to €70 billion will be left over to spend on actual hardware. \"The longer you have this money sit around somewhere, the longer factors like inflation and interest payments have to eat away at this pile,\" Loss said.[0] There has been some criticism from European allies, and within Germany, that so many big orders have been placed in the United States. Depriving local industry. And of what use is this anyway. Why on earth would Germany need ballistic missile defense etc. >quickly Two years in, the entire West still cannot outproduce Russia. Let alone Russia plus friends. For the most part, dependable large orders—necessary for expansion—aren’t coming. Overall, deindustrialization is only accelerating. Italy just ordered 132 L2s for 2027–2037. Pathetic timeframe. These tanks will be obsolete by then. >NATO wants America in There is no NATO without the US. Something could conceivably carry the same name but it would not be the same thing. NATO is and has always been an instrument of American control over Europe. >angry orange political candidate Remember when Trump ordered a reduction of the occupation force in Germany (35k then)? Well, Serious People got Very Nervous and the military stalled the order. In fact, the generals were in a state of barely concealed mutiny then. And now there’s 50k. So, rest assured: Germany will be Kept Down for the foreseeable future. [0]: https://www.dw.com/en/what-happened-to-the-german-militarys-... reply xpl 14 hours agorootparentprevRussia had in fact expressed desire to join NATO in Putin's early days as a president. Apparently he wanted to fast-track, so it might have hurt his ego somehow, when he was told that Russia is no special. https://web.archive.org/web/20240228155601/https://www.thegu... George Robertson, a former Labour defence secretary who led Nato between 1999 and 2003, said Putin made it clear at their first meeting that he wanted Russia to be part of western Europe. “They wanted to be part of that secure, stable prosperous west that Russia was out of at the time,” he said. The Labour peer recalled an early meeting with Putin, who became Russian president in 2000. “Putin said: ‘When are you going to invite us to join Nato?’ And [Robertson] said: ‘Well, we don’t invite people to join Nato, they apply to join Nato.’ And he said: ‘Well, we’re not standing in line with a lot of countries that don’t matter.’” reply hnarn 12 hours agorootparentYou don’t need to be an expert on Russian foreign policy to conclude from nothing but that final quote that Russia (rather, Putin) never had any serious desire to join Nato in the first place. Putins views on Russia may have been more widely broadcasted recently because of Carlsons interview, but don’t be fooled; they were no different 24 years ago. reply partiallypro 9 hours agorootparentprevPutin had no intention to join NATO, and knew he wouldn't get in. The only reason he would want in is to do the exact same thing that Russia does on the UN security council as a permanent member, which is to stifle and blockade. There are some current NATO members that hold things up, largely Turkey. Coincidentally a large number of smuggled items that get around sanctions to get to Russia...go through Turkey. reply ekkodur 2 hours agorootparentHave you checked direct or inderect export/imports of Greece with Russia? You'll be ashamed to blame Turkiye. If you don't know Greece is also a NATO member. reply xpl 9 hours agorootparentprevYes, that is a plausible explanation. On the other hand, I have witnessed how the Putin's rhetoric made a U-turn over those dozens of years — from \"Fukuyama-level\" liberal and pro-Western (early Putin), to \"geopolitical realist\" and pretty much anti-Western (as of today). I don't know how much this reflects his actual worldview transformation (if there was any), but I have reasons to believe that there is no smoke without fire. reply partiallypro 4 hours agorootparentI highly encourage you to read some well researched books about Putin, his world view hasn't changed all that much. This is a former KGB man. He has held, for a while, that the fall of the USSR was one of the biggest disasters in history. His rise in power was stemmed by a terrorist attack which he possibly had a hand in to orchestrate his rise and the eventual invasion of Chechnya. reply hnarn 14 hours agorootparentprev> So maybe, in 50 years from now, Russia will join NATO, too? While not technically impossible, practically this is very hard to see. Unless Russia becomes a market liberal, well functioning democracy, it will not happen -- and what are the odds of that? Russia had the chance after the collapse of the Soviet Union, instead it devolved into a plutocracy and what is essentially a one-party state in anything but name. Regardless of what some people might think, in no small part due to many people seeing Russia as the spiritual successor to the SU combined with its vast geographical size, Russia is not a superpower, and will not become one in our lifetime. Yes, they have nukes. So does the UK, and the UK economy is 20% larger than Russia despite is essentially being an island off Europe. How about France? They have nukes too, and their economy is 30% larger. Russia is a failed state at the tail end of a century long brain drain, crippled by corruption and authoritarian rule, but none of these things are the most deciding factor in why they will never be part of NATO; the primary reason is that Russia quite simply has an empire complex. What do you get if you combine economical stagnation and a dead empire inferiority complex? You get Hitler, or in this case Putin, and I very much doubt any of us will see a \"rehabilitation\" of the Russian people like we saw in Germany in our lifetime. reply Sammi 10 hours agorootparentRussia will need to break up into smaller pieces before it is palatable for nato to absorb. Too geographically big right now. USA cannot tolerate anyone else in nato being big in any way that rivals them. reply partiallypro 9 hours agorootparentGeographical size means nothing and isn't a challenge to the US, Russia's economy isn't that important to the global economy. The Russian economy is smaller than Canada's and Canada is not only a NATO member but also geographically large. reply baxtr 13 hours agorootparentprevInteresting take. What do you think we will see in Europe the next 5-15 years? reply quickthrowman 11 hours agorootparentprev> So maybe, in 50 years from now, Russia will join NATO, too? Probably not. Russian history for the last several hundred years is a series of kings, emperors, and dictators. Massacres and purges. The entire peasantry was only freed from slavery 150 years ago. Being ruled by hereditary monarchs and dictators for so long has fundamentally changed Russian culture to be vastly different from the West. IMO it would take a radical culture shift for Russia to join the West. reply valarauko 11 hours agorootparentIsn't that true of, say, western Europe too, perhaps leading Russia by 50 years or so? reply andrewjl 6 hours agorootparentWestern Europe was not too different at the start of the medieval period but then evolved in a different direction. The King's Two Bodies by Ernst Kantorowicz is a good read on this. reply tim333 11 hours agorootparentprevThere's been quite a trend in the long term to democracy and the like if with blips. Like in this graph of countries that are democacies, 200 years ago 2.5%, fifty years ago 30%, now 61%. https://ourworldindata.org/grapher/countries-democracies-non... I get the impression Russians would like a more modern system but have a job shifting Putin. But he won't be around for ever. reply gitaarik 1 hour agoparentprevDon't all countries have a very violent history? (: reply gerikson 11 hours agoparentprev> It is said to be the worst conflict in all of human history. By who? Also: https://en.wikipedia.org/wiki/Scandinavism reply nikolay 14 hours agoparentprevnext [6 more] [flagged] luuurker 10 hours agorootparentGood luck to those launching Sarmats too as they'll have to stop the Tridents, Minutemen, M51, etc. It's important to remember that while they don't make weekly threats, there are three NATO countries with land/sea based nuclear weapons. Realistically, there's no defending from nuclear weapons. That's why MAD, while crazy, works. reply nikolay 10 hours agorootparentIt doesn't have to be even Sarmat; it could be Poseidon - options are plentiful. I doubt anyone would have the balls to respond to a nuclear attack. The issue is who's crazier to do the first one, and if one truly believes Russia and Putin are crazy evil, then one should act respectively! So far, I don't see intelligent behavior from the West - I see arrogance, which always has a high cost! reply luuurker 9 hours agorootparentYou expect everyone to bend over because Putin/Russia decided to go down the \"I'm a mad man, give me what I want or I'll nuke you\" route... that's just not how the world works. I hope you don't use this tactic yourself, because you'll end up with a broken nose (or worse) if you do it to the wrong guy. There is certainly arrogance from the west, but what do you call what Russia is doing? They literally make threats to nuclear powers as if they're untouchable. The fact you don't see this should tell you that you're biased here. Sarmat and Posidon are \"doomsday\" weapons. Use them against a small country like the UK or France and they have little to lose. Use them against the US and you're likely to be wiped out. As you've said, options are plentiful, so we should all be very careful and do our best to avoid war (all = includes Russia). reply nikolay 9 hours agorootparentNo, Putin is not a madman - he was cornered. These are the facts! The West is slowly but surely bringing us at least WWIII, if not extinction! The West is always greedy and salivates over Russia's colossal territory and resources! And always gets hurt! This time, it could hurt the entire civilization, though! reply luuurker 8 hours agorootparentIs this the \"she made me do it\" excuse wife beaters use, but for international relations? Three facts (not opinions) for you: - The revolution in Ukraine, which often is blamed by Russians and tankies on the \"west\", happened after the then president Yanukovych decided to abandon the European Union–Ukraine Association Agreement and join Russia's Eurasian Economic Union instead when most of Ukraine was getting more pro-European Union. - Russia started a war with Ukraine in 2014 when they invaded Crimea, breaking international law and the the Budapest Memorandum they signed. Everything - from Azovs to war crimes - came after. Russia then decided to expand that war in 2022, after being warned privately and publicly that the west would help Ukraine if they did so. - Ukraine has been trying to join NATO since the early 2000's. NATO's open door policy is there from the start and will continue to be, but Ukraine didn't join back then and [opinion ahead, based on real numbers] wouldn't be accepted in 2014 either because they were too corrupt, bloated and incompetent. Putin invaded, not because he was cornered, but because he saw an opportunity and took it. And let's be honest, it worked. A few hundred unmarked soldiers and people with guns (eg: Igor Girkin) transporting politicians around to vote to join Russia... there were some protests, some sanctions, but nothing serious. He tried to do the same again in 2022, but it didn't work. I don't know why people like you try to come up to excuses when even Putin himself from time to time drops the BS and just says he did it because he could. Want to support Russia? Fine, but cut the bullshit. You like seeing them attacking and stealing land from a neighbour, you just lack the balls to admit it. My eyes rolled when you mentioned Russia's resources. We have the money to buy their resources. Europe depended on their gas and, to a lesser extent, their oil. It was fine, everyone - including Russia - was getting richer, we had strong economies, we used their gas and they used our planes and technology. No reason at all to start a war. What do we need their land for? Who was calling for an invasion/annexation/controlling of Russian land or a war with Russia (a nuclear power!)? Most countries in Europe don't even want to spend 2% of their GDP now (post invasion)... but they want a conflict with Russia? Again, you only blame one side for that WW3 that is surely about to happen (lol). Russia, which as you've said is not governed by a madman (or a bunch of amateurs that are easily played), decided to start a war in Europe. Can't you see that they have so much power and that they could stop it all? They could also stop making threats like the ones that mention that Sarmat you talked about, but they don't. And where's the \"we'll leave Ukraine if we they don't join NATO\" proposal? Anyway, while it \"takes two to tango\", you can only blame one side. And that's the problem with your position. reply renegade-otter 16 hours agoprevBeing a NATO member gives a nation more security than nuclear weapons, effectively. Nuclear states can and HAVE gone to wars - but no one wants to mess with the devastating conventional power of NATO. After the recent gutting, Russia has a few dozen SU-34s that can fly. NATO has thousands of planes. reply dkjaudyeqooe 15 hours agoparent> Russia has a few dozen SU-34s that can fly And Ukraine is about to receive their F-16s reply __loam 15 hours agoparentprevIt also just got a lot more Gripens. reply rickydroll 11 hours agorootparentit did?? can you point me to articles on this? reply Oath 2 hours agorootparentI think they are referring to Sweden adding Gripens to NATO, not that Russia is receiving any. reply riku_iki 11 hours agoparentprev> NATO has thousands of planes. You mean US has. And someone like Trump may decide to not engage in conflict as part of NATO. reply renegade-otter 11 hours agorootparentThat's a completely different thread - all the more reason to expand NATO before this happens. Europe needs to learn to swim on its own. reply valval 12 hours agoparentprevOh yeah, Russia will lose any moment now :D reply wickedsickeune 12 hours agorootparentIt's because of the crippling sanctions! reply bilekas 16 hours agoprevThis is great news and NATO benefits hugely from this. I have to hand it to putin for bringing further solidarity across Europe and NATO. reply pandemic_region 16 hours agoparentDon't forget he also raised awareness for Europe's dependency on Russian fossil fuels and accelerated there move to green energy across the world. What a guy! reply Log_out_ 15 hours agorootparentIt's KGB, they are the MBAs of the east. If they would capture something innovative in the wild, they would shoot the team and put the thing in a \"palace of innovation\" in mocowardia. reply psunavy03 15 hours agorootparent> It's KGB, they are the MBAs of the east. Of all the takes I've heard in my life, that is certainly one of them. reply hmottestad 16 hours agorootparentprevThere’s also minerals and grain. reply firebat45 15 hours agorootparentprevnext [10 more] He's almost as good as the guy that killed Hitler. Really doing the world a favor. reply unnouinceput 15 hours agorootparentIsn't, at least officially, Hitler the guy that killed Hitler? Or that was the joke and it flew over my head? reply bloopernova 15 hours agorootparentThat is indeed the joke. Hopefully you don't get downvoted for asking a question. reply lostmsu 15 hours agorootparentIt was a very well veiled one too. If not for your comment I would not have realized it. I suspect people downvoting it didn't get it either. reply bloopernova 14 hours agorootparentSome folks forget that not everyone is \"terminally online\" and won't get every reference and idiom. reply WJW 13 hours agorootparentThis is just something you might learn during history lessons in school, no online-ness is needed. reply bumby 13 hours agorootparentprevI think it would be much better if people focused downvoting on comments that go against HN guidelines rather than questions or comments they disagree with. reply cko 15 hours agorootparentprevI don't mean to kill the joke further but I think Hitler was already a dead man walking before Hitler killed Hitler, so I think his enemies (Allied Forces plus Russians) deserve most of the credit. reply enterprise_cog 15 hours agorootparentThe Soviet Union was part of the allied forces. reply cko 15 hours agorootparentprevI don't mean to kill the joke but I think Hitler was already dead before Hitler killed Hitler, so I think his enemies (Allied Forces plus Russians) deserve most of the credit. reply krippe 15 hours agoparentprevIt's crazy how there wasn't really ANY desire to join NATO before Russia showed, once again, that you can not trust them. The full scale of Ukraine really was the straw that broke the camel's back. reply bertil 15 hours agorootparent> ANY desire to join NATO Sweden has been using Nato standards and running occasional exercises with Nato for a while now. They didn’t want to be officially part of it because of their unique perspective on War and Peace (see Olof Palme, sending Blue helmets in Cyprus), but there wasn’t a lack of desire to join. I’d compare it to Switzerland and the EU: the de-facto alliance is obviously beneficial, but principles have kept things separated on paper. Finland, that’s more complicated: unlike Baltic countries and the Kaliningrad exclave, they were not in the Soviet Union. That meant a lot of pressure to remain neutral, translated until last year into “Finlandization”: a refusal to take either side. That pressure ended with the Fall of the Berlin Wall, but Finland (like Sweden) saw no reason to change their official neutral position. When Russia started to mess with Estonia, the need to ally with Nato, in particular on cyber-defense questions, became a lot more present for everyone nearby. I suspect that Finland wanted to be ready, adopt Nato standards, training, methods, etc., and pick the right moment to join officially. Like the Baltic trio, the Russian presence looms high in the East, and I’d be surprised if there were not regular overtures and unofficial promises of support. The USA and Canada care a lot about the Arctic, and it’s not hard to count the allies there. So, I don’t think it was a major shift—like Italy changing sides at the end of WW2. It’s more a gradual rapprochement, matching Putin’s increasingly concerning policies, that hit a very good reason to accelerate. The process has been mostly political and official. Neither Sweden nor Finland had to change guns, tactics, or radio signals. reply the_why_of_y 14 hours agorootparentFinland realized that Finlandization does not work any more, by observing what happened to another country that tried to Finlandize for the last 3 decades: Ukraine. reply bertil 13 hours agorootparentI believe they realized that a while ago, but you are definitely right that they have been looking at Ukraine and taking detailed notes. reply tga_d 14 hours agorootparentprevThey presumably meant in terms of popular opinion, which was always going to be de facto necessary to join (even if in the end it happened without a referendum). See the list of polls enumerated here: https://en.wikipedia.org/wiki/Sweden%E2%80%93NATO_relations#... reply bertil 13 hours agorootparentThe Swiss position on joining the EU follows a similar trajectory: the decision is negative, but that’s not because they think joining the alliance is a bad thing or that relations are strife, but because they thought, or think, that making things official would betray principles. Maybe compare it to a couple who live together but aren’t married and are opposed to it because one of them sees it as an encroachment of religion. reply bilekas 14 hours agorootparentprevHonestly I can understand the previous sentiment, joining would have come across as an act against Putin as that's how he always frames NATO. So keeping the status quo was fine for everyone. But then he showed he doesn't respect the status quo. Just my opinion of course. reply factorialboy 16 hours agoparentprevDon't forget the contribution and sacrifice of Ukraine reply s_dev 16 hours agorootparentOr the blackmailing by Hungary and Turkey. reply vajdagabor 14 hours agorootparentBlackmailing by Orban and his party, not Hungary. reply xdennis 14 hours agorootparentFair enough, but he's been in power for 18 years (1998-2002, 2010-present). He couldn't do that without widespread and lasting support. reply vajdagabor 13 hours agorootparentNo, people supported them, but that is because democracy was hacked. Hungary is not a democracy anymore (it's a hibrid regime[1]). The biggest issue is that the majority of the media is controlled by the government. Also they own jurisdiction and have been gradually rewritten the constitution. Most people who support this regime do that because they believe the propaganda. Many people I know have been bitterly trying to tell their family members that they are watching / listening propaganda (unsuccessfully, for years). Most of Orban's supporters don't know much about politics, they just want to live their lives, so they believe whatever is on TV, radio, online media, posters, etc. For many it is very hard to see what is true and what is lie. But there are many, many people here who don't like this and want a change. The country is in a state where positive change towards democracy is really hard at the moment, many of us still want to believe it is possible. By the way, we could see this madness around the world in the past years: Brexit, Trump, Bolsonaro… many people can be led by their nose. Not just in Hungary. I really whish if people would learn from Hungary's mistakes, and don't let the same thing to happen in their countries. [1]: https://en.wikipedia.org/wiki/Hybrid_regime reply t0bia_s 13 hours agorootparentOr maybe people just agree with this politics. Clearly, you and your source of information don't. reply vajdagabor 12 hours agorootparentDo you mean, some people just consciously agree with being used and lied to and their taxes being stolen so that the leaders can be richer and more powerful while schooling, health care and economy is in decay? I don't think so. reply sobkas 12 hours agorootparent> Do you mean, some people just consciously agree with being used and lied to and their taxes being stolen so that the leaders can be richer and more powerful while schooling, health care and economy is in decay? I don't think so. Many people actually do that, because in their minds alternative is \"left\" that will do the same but with added bonus of supporting LGBT, fight against climate change, unrestricted immigration and such. And getting traditional true patriot of country X rich is preferred to fattening this dirty commies traitors on the \"left\". reply reducesuffering 13 hours agorootparentprevThat's not an indicator of support, look at Lukashenko, dictator of Belarus. reply berner 14 hours agorootparentprevA country that is not in a civil war that has majority support owns the crimes it runs on. reply the_why_of_y 13 hours agorootparentWhen Fidesz won their first election in 2010, they changed voting laws and even the constitution; now elections in Hungary are heavily gerrymandered and Fidesz routinely wins super-majorities with less than 50% of the votes. https://www.journalofdemocracy.org/articles/how-viktor-orban... Nonetheless, in 2014, five center-left parties formed the Unity Alliance. One center-left party (LMP) refused to join, splitting the center-left opposition vote. This cut in half the number of constituencies that the opposition would have won that year, allowing Fidesz to capture 91 percent of the constituencies with just 45 percent of the vote. Still plagued by infighting, the opposition remained fragmented in 2018, even as it gained strength in Budapest. With 49 percent of the vote in 2018, Orbán won 86 percent of the constituencies, losing in Budapest but winning almost everywhere else. The 2014 and 2018 results showed that only a unified opposition that spanned the political spectrum could defeat Orbán’s system. reply jajko 16 hours agorootparentprevIts funny with Hungary, Orban is such an incompetent ruler that currently Hungary is paying way above market prices for gas from Russia. So much for being friends with benefits with russia. You can see how it all is a series of really not that smart moves for Hungary for a long time, borderline treason. And one point generally - please lets stop calling whats happening in Ukraine in any other way than War. putin's war - its a perfect name I'll keep repeating till it sticks around, or I'll die trying. Its relatively personal to me, my home country (former Czechoslovakia) was basically enslaved by russian cough cough soviet forces for decades, people shot or electrocuted when trying to escape (around 500 recorded officially), tens of thousands murdered in other indirect ways (gulags or uranium mines with no ticket back, or just beaten to death in some cold dungeon). I see basically 0 change from that russia to modern one and how it values things like human life, freedom etc. It looked briefly better, much better, but those times are over for good and russia is firmly back at cold/not so cold war with whole western world. Currently trying to subvert quite a few places in Africa. I hope western 3-letter agencies are few steps ahead. reply ProjectArcturis 16 hours agorootparentThis comment really illustrates the hollowness of the \"NATO expanded too aggressively\" claim. NATO didn't force any of these ex-Soviet countries to join. They ran to join NATO as soon as they could. They'd experienced living with the Russian boot on their neck, and they were eager to join a collective security organization to prevent it from ever happening again. reply yencabulator 14 hours agorootparentI have a different perspective. If NATO hadn't expanded to Turkey, Sweden and Finland would have had a much easier time not being blackmailed. NATO expanding to essentially-dictatorship countries was too eager. And yes, I understand Turkey's geographical position giving it power over sea routes, and why that was desirable to NATO. But choosing to include a fickle ruler in a unanimous-decisions-only organization is just asking for trouble. (The Baltics wanted in on NATO, and it's good that they got in. They're largely decently run small countries in a tough spot, not world stage bullies.) reply tga_d 13 hours agorootparentTo be clear, Turkey has been undergoing pretty decisive democratic backsliding since the mid 2000s. It was added to NATO at the same time as Greece, and at that time, Turkey was the more democratic of the two countries (see, e.g., the electoral democracy index for 1952 on https://v-dem.net/data_analysis/CountryGraph/ ). edit: note if you're trying to find Turkey by searching on that site, it uses the endonym Türkiye reply orwin 12 hours agorootparentTürkiye is probably the first western country to not secure its demographic dividends, with maybe Greece (hard to say because the EU mess things up with free movement of people and stuff). It did not fail hopefully, but the infrastructure gains are small compared to even ex-USSR countries. I fear the same is happening in slow motion in India (we'll see in 15 years i guess). reply WJW 13 hours agorootparentprevIn 1951, when Turkey joined NATO, the country was not actually all that dictator-like. reply RajT88 16 hours agorootparentprev> This comment really illustrates the hollowness of the \"NATO expanded too aggressively\" claim. In the US, only wingnuts and \"useful idiots\" (really the same group) are repeating this talking point. reply tim333 14 hours agorootparentMearshiemer got quite a favourable hearing on HN https://news.ycombinator.com/item?id=30559136 reply RajT88 11 hours agorootparentGood lord. I was not aware of this guy before, but his position seems to be akin to suggesting a woman deserved to be raped because of how she was dressed. Even people pretty forgiving of his essays acknowledge he's habitually at odds with basic facts and history. Not a wingnut, per se, but on the same spectrum of people divorced from reality. It's too bad as well, because US foreign policy is deserving of more scrutiny than it gets, and it gets a fair amount. reply whatshisface 15 hours agorootparentprevI really never understood how this statement and \"NATO expanded to aggressively\" were seen as contradictory. Putin's government wants to put the boot on Russia's neighbors, and NATO was in the process of taking that option away from them, and that's why they committed to war. They're not lying about their motivations, they are phrasing them in head-of-state speak. The same goes for denazification, which is thinly veiled code for intervening in who's allowed to govern Ukraine. reply nyolfen 15 hours agorootparentprevnext [6 more] [flagged] dkjaudyeqooe 15 hours agorootparentThis a standard Russian talking point. It's a red herring and a fairly weak justification for Russian aggression in Ukraine. The US can make demands and so can anyone. In the case of Cuba they were met by the Soviets willingly who withdrew their missiles and made a deal with the US. Russia objects to NATO membership because it makes bullying and invasion of those with NATO membership impractical. It doesn't threaten Russia but rather it weakens Russia's hand and that is its main complaint. It didn't have a basis for a deal with NATO or the US. Russia has had nukes adjacent to NATO countries for a long time. There is no moral justification for invasion unless you've already been invaded by that party. reply libertine 15 hours agorootparentprevLet me do something similar to you what you did with this comment: allowing Russia to develop nukes shouldn't have been tolerated, the morally correct course of action was to prevent it. reply wnoise 15 hours agorootparentprevYou're assuming a symmetry that doesn't exist. reply ProjectArcturis 15 hours agorootparentprevI think you skipped several steps in this argument. Can you explain more please? reply tcmart14 15 hours agorootparentI think the argument is, if its okay for eastern European nations to run to NATO (and we defend it as self determination), it should be fine when countries decided to run the opposite way (toward the USSR in this case). I will be honest, I agree.I believe Ukraine has the right to self determine their own relationships on the international stage, and I also believe Cuba does too. Although to me, the biggest fuck is, we didn't have like Cuba, we just needed a relationship sufficient enough for Cuba to side with us instead of the USSR. And I think that was a mistake. We do this Saudi Arabia, we should not be on as good of terms with Saudi Arabia as we are. But the reality is, if we are not, some other heavily influential country on the international stage will. reply vajdagabor 15 hours agorootparentprevOrban and his team don't work for Hungary. They work for themselves (for a small group around Orban's family), trying to take out as much money and power from every opportunity as they can. This might explain purchasing the gas on higher price (and a huge amount of other controversial deals) and preferring partnerships with corrupt governments and politicians. It's all about business and power. There are good meticulously researched articles about their businesses here (one of the few remaining independent, reliable sources in Hungary): https://www.direkt36.hu/en/ reply hermitcrab 14 hours agorootparentprevThe 'Empire' podcast ran a series about Russia: https://podcasts.apple.com/gb/podcast/empire/id1639561921 It is clear from listening to the podcast that Russia has always been nightmare to live in (unless you perhaps belonged to a tiny elite) or have as a neighbour. Serfdom wasn't abolished until 1861. reply usrusr 15 hours agorootparentprevPutin's war isn't a useful term though, because there are so many of them. Regarding change from \"that Russia\" to the modern one, I'm afraid there has been change, to the worse. The Soviet incarnation of the empire was at least nominally bound to the progressive ideas of socialism (which is completely orthogonal to wether those ideas are workable or not), whereas the current incarnation is openly worshipping the fascist ideas of strength and domination, and the struggle to get there. When Russians claim that they don't understand why someone would willingly ally with others who don't prove worthy by actively coercing them, chances are it's genuine, they really don't understand. Sometimes I wonder if their language even has a word for friendship based on equality that is separate from an asymmetric allegiance based on status gradient. Perhaps all the non-gradient terminology was gobbled up by socialist ideology and now the very idea of peers is out, except where seeing it through the socialist lense still fits? reply reducesuffering 13 hours agorootparentFor more understanding of this difference in Russian mindset, see this thread from Kamil Galeev, a Russian independent researcher: https://twitter.com/kamilkazani/status/1761855753290191129 reply throwrus344 14 hours agorootparentprevI humbly bow before the Ukraine. You are the shield that now protects the EU, and USA, Taiwan, and many others. The winter is coming, and we are ever so slowly awakening. Ukraine is giving us (or at least the collective west) the best deal we've possibly ever given - dealing a mighty blow, hopefully the final one, to what used to be Soviet Union and its imperial ambitions. Without a single western soldier having to enter the battlefield. Indeed Ukraine may be the first ally in the long history of regimes that US has supported in their struggles since the end of WW2 that has a will and a skill to take the weapons of war that were developed to battle the commies in the first place and put them now into their intended use in killing russian invaders[nsfw] and destroying their machines of war[losses]. Now, if Ukraine were to fall, especially because US should decide that it is not in it's best interests to support them, this has a high likelihood to result in a world where Europeans now have to alone face the wounded and angry bear living next to them, and starting to arm themselves to the teeth. Historically nothing good has come from Europeans arming up, but this will also limit European countries ability, and willingness to support an ally that was, in other parts of the world. With US influence waning, this will lead to increasingly difficult situations in the middle east and Africa. Soon the willingness of US to support its other allies in Asia would be put to test in Korea. Failing to fight the good fight there, but this time with American skin in the game, then the Taiwan is likely to fall to China like a domino piece, putting an end to pax americana, and post world war 2 world order. And at least in the west, to one of the most peaceful periods in [modern] history. If instead we will help Ukraine deal a decisive and lethal blow to a country that has terrorized its neighbors since the times immemorial, this will send a strong message to every dictator wanna-be, that the rules based world order is the one where we the free people of this planet choose to live in. In democracy, with all its flaws. [nsfw] https://www.reddit.com/r/UkraineWarVideoReport/comments/1b8f... [losses] https://lookerstudio.google.com/reporting/dfbcec47-7b01-400e... reply pimlottc 14 hours agorootparent> I humbly bow before the Ukraine. Note that “Ukraine” is now preferred to “the Ukraine”; the latter term was used in Soviet times to diminish its autonomy by implying it was just a region of the USSR. reply euroderf 13 hours agorootparent> “the Ukraine”; the latter term was used in Soviet times to diminish its autonomy by implying it was just a region of the USSR. Source ? reply pimlottc 12 hours agorootparenthttps://theconversation.com/its-ukraine-not-the-ukraine-here... https://en.wikipedia.org/wiki/Name_of_Ukraine#English_defini... https://old.reddit.com/r/AskHistorians/comments/tg00js/ukrai... reply mellutussa 16 hours agorootparentprevSlava Ukraini! reply Mountain_Skies 16 hours agoparentprevHow does NATO benefit? reply ethbr1 16 hours agorootparentSweden has a pretty advanced and self-sufficient defense industry for its size. Examples: https://en.m.wikipedia.org/wiki/Saab_JAS_39_Gripen https://en.m.wikipedia.org/wiki/Stridsvagn_103 https://en.m.wikipedia.org/wiki/Ground_Launched_Small_Diamet... PS: Not sure why parent is getting down voted for asking a neutral question. Curiosity! reply johnchristopher 15 hours agorootparent> PS: Not sure why parent is getting down voted for asking a neutral question. Curiosity! Phrasing of that question is a classic bait for flame wars. reply ethbr1 15 hours agorootparentIt's a simple interrogative sentence. At some point {subject} {verb} is just {subject} {verb}. And given the abnormally terse formulation, I'd expect GP was explicitly trying to decolor their interrogative. reply rashthedude 16 hours agorootparentprevNot a diverse group of readers, as one would suggest. reply Paradigma11 1 hour agorootparentprevSweden and Finland came as a pack so lets talk about both of them. Benefits: It completely neutralizes the Russian Baltic Fleet. The only railway line supporting Murmansk is less than 200km beyond the finnish border, which means the Nordic Fleet is also compromised. Potential disadvantages of additional members are added political instability as can be seen with Hungary and Turkey. There is little chance of this with both countries. Basically you dont defend members, you defend borders and adding Finland and Sweden to the pact makes for a far easier and better developed front. The Finns have been building bunkers and training their population for the next Russian invasion since WW2. Having Finland as a member strengthens the position of the Baltics, who are threatened by the Suwalki corridor. Basically it strengthens and stabilizes the northern border to Russia/Belarus. reply TulliusCicero 16 hours agorootparentprevMore territory coverage, especially coastal areas of the Baltic Sea. With Finland in there too, it's a NATO lake for sure. Of course, Sweden was already a member of the EU and a NATO partner, so it's not a huge difference in practice. reply fennecbutt 15 hours agorootparentprevSweden is a country that makes their own military related stuff: https://youtu.be/d8x8ITwd4Vg?si=ye6-_fe7EJMuqdIg Archer can deploy fire and retreat so quickly, can also fire multiple rounds and have them land on the same target at the same time. As opposed to countries that do not design and manufacture such things themselves. reply saalweachter 12 hours agorootparentprevNATO is two things. A defense pact, in case shit hits the fan. A deterrence, to make war too difficult to undertake. Adding additional countries around the edge of NATO does two things for the countries currently in the pact, even if they aren't economic or military powerhouses. First, those countries are less likely to be attacked, and not having your neighbors be embroiled in war is good. Germany is much happier if there is no land war going on next door in Poland, bombs occasionally falling on the wrong side of the border, civilian refugees looking to them for safety. Second, adding someone else to the pact puts someone else on the front lines to test the defense provided by the pact, if shit hits the fan. Sure, if Russia wanted to, they could try to bypass Poland on the way to Germany, but practically speaking with Poland in NATO, Germany will get to see how NATO responds to an invasion of Poland, rather than finding out how they respond to an invasion of Germany. Poland, likewise, would be much happier seeing how NATO responded to the invasion of Ukraine-the-NATO-member, rather than watching the invasion of Ukraine and wondering how NATO will respond to the invasion of Poland if Ukraine falls. reply kzrdude 16 hours agorootparentprevIt's easier to defend Denmark, Norway and Finland if Sweden is not in the way. With this, Sweden is more likely to become completely \"open\" to NATO operations in any conflict that involves defending those three NATO members. reply The_Colonel 15 hours agorootparentI think it has a more significant impact on the Baltics which have been an Achilles heel of NATO. reply toyg 16 hours agorootparentprevBy having to jump through fewer hoops to get the intelligence they already got before, and likely further restricting the field of operation for Russian forces in that area. reply scoofy 15 hours agorootparentprevSweden joining ends the problematic area of the Suwalki gap as a pinch point between Kaliningrad and Belarus. It also means NATO controls the Baltic Sea completely. reply charles_f 16 hours agorootparentprevGuaranteed access to Ikea furniture in times of war. reply solardev 16 hours agorootparentAnd delicious meatballs for the troops reply bluGill 15 hours agorootparentIt isn't hard to make better meatballs than what Ikea sells. For that matter if you think Ikea furniture is good you have no idea what good really is. reply fennecbutt 15 hours agorootparentGood for the price Mr. Disingenuous reply guappa 16 hours agorootparentprevSweden and denmark control access to the baltic sea. reply euroderf 13 hours agorootparentI've always wondered what is the reality of passage of Russian nuke subs thru the Skagerrak. Tidal flows. Hull detection. Passage protocols. reply RecycledEle 16 hours agorootparentprevA bigger alliance is harder to defeat in war. Also, a bigger alliance requires each member to contribute fewer troops to manage a common defense. Sweden is a fairly wealthy country that can contribute the required amount to NATO. Sweden's ports are beneficial in a potential Arctic conflict. Finally, Sweden does not bring any new potential conflicts/enemies with it. reply eduction 15 hours agorootparent>Sweden's ports are beneficial in a potential Arctic conflict How? Norway (long a member) has Arctic ports, Sweden does not touch the Arctic. reply red-iron-pine 12 hours agorootparentIndeed. Plus Finland is already in the club, and they're def. on the arctic. reply eduction 7 hours agorootparentThey are not. Not since the 1940s. https://en.m.wikipedia.org/wiki/Moscow_Armistice reply klipt 16 hours agorootparentprevThings are looking up for NATO! Except the part where Trump said he'd pull the US out of NATO if elected. reply joecool1029 15 hours agorootparent> Except the part where Trump said he'd pull the US out of NATO if elected. Congress blocked his ability to do that without their approval: https://www.washingtonpost.com/national-security/2023/12/16/... EDIT: since other commenters don't know, yes it was signed into law, Sec 1250A of the 2024 NDAA: >The President shall not suspend, terminate, denounce, or withdraw the United States from the North Atlantic Treaty, done at Washington, DC, April 4, 1949, except by and with the advice and consent of the Senate, provided that two-thirds of the Senators present concur, or pursuant to an Act of Congress. reply Epa095 14 hours agorootparentIf the commander in chief says that Russia can do 'whatever the hell they want' with NATO countries, it doesn't really matter if the US is still officially in NATO. reply dragonwriter 15 hours agorootparentprevThey made it illegal for him to do so, but Trump has made it clear that he does not view himself as accountable to the law, especially for any acts while President, so... reply dguest 15 hours agorootparentprevI'm too lazy to look this up. Can he do that? reply adastra22 15 hours agorootparentMaybe. Countries can leave the treaty. Congress and the Senate are trying to make it impossible for the president to unilaterally do this, but I don't know if that bill has passed yet. It's a bipartisan effort believe it or not, because even the most politic Republicans are not stupid. I expect it should pass before January 2025. But who knows, maybe he can sabotage the alliance in other ways, or find some other way out. reply ttymck 16 hours agorootparentprevI understand what you say is true in theory, but does this mean the US will deploy fewer troops in NATO theaters, in practice? reply MaxPengwing 16 hours agorootparentNo, it has already increased, but not due to Sweden and Finland joining. the increase was a direct response to Russia invading Ukraine. https://www.uso.org/stories/3518-one-year-later-how-the-uso-... https://www.nato.int/cps/en/natohq/topics_136388.htm https://commonslibrary.parliament.uk/research-briefings/cbp-... But in same time European allies has also increased their NATO soldiers. reply ttymck 14 hours agorootparentYes, I am asking: all else equal, would more NATO members reduce the number of US soldiers deployed. reply Paradigma11 1 hour agorootparentThere is no all else equal here. If your new member is the Democratic Republic of Congo the number would go up considerably. With Sweden/Finland it might very well go down since the border is far more defensible. But the US has a global footprint and those troops in Europe are nearer to potential conflicts in the middle east and northern Afrika, so it might make logistical sense to park them there anyway. reply asveikau 15 hours agorootparentprevIMO it's still not certain NATO will hold together if Putin decides to attack the Baltic states or Poland. Many NATO countries will have sizeable factions of their electorate saying to not get involved. The fact that Ukraine hasn't been easy for him makes it less likely that Putin will attempt that, but it's clearly been on his mind. reply ardaoweo 15 hours agorootparentYep, Putin will absolutely try to grab new land whenever he thinks he can get away with it, no matter how many young Russian men die in the process. To him it's just a game to fulfill his fantasies of being a great conquering czar, and for that reason credibility of NATO's article 5 is vital. Even the good relations between EU and Russia pre-2014 were just theater on Russia's part. Here in Finland during those years shady Russian businessmen kept buying properties that made no financial sense, but were located close to critical infrastructure or military locations. They have never acted in good faith. reply euroderf 13 hours agorootparent> shady Russian businessmen kept buying properties that made no financial sense, but were located close to critical infrastructure or military locations. AFAICT this topic has been mostly avoided by the Finnish media. I guess everyone just kinda trusts that the government is on top of the situation. reply dragonwriter 15 hours agorootparentprev> IMO it's still not certain NATO will hold together if Putin decides to attack the Baltic states or Poland. There is a reason forces of core NATO states farther from the Eastern flank are deployed to Poland and the Baltic states; it is practically impossible for Russia to attack them without attacking the forces of core NATO states, not just in a \"legally, under Article 5, we must treat this as an attack\" way, but in a \"Russian troops are killing troops of those states\" way. reply The_Colonel 15 hours agorootparentprevI think Putin would not invade Poland, it's just as strong or even stronger than Ukraine, with less corruption, better economy and defense treaties. Baltics have always been the biggest risk. They are very small population-wise and can be \"easily\" cut off (Suwalki gap). But the addition of Sweden and Finland to the alliance will significantly improve the defense posture (airfields, maritime logistics). reply Paradigma11 29 minutes agorootparentI think if he is successful in Ukraine, Transnistra/Moldowa are the logical next step. Then he most likely would use his stooges like Orban and radicalized Russian minorities to create problems and wait for an opportunity. reply lumb63 16 hours agorootparentprevBut also, a bigger alliance creates a larger amount of territory to defend. For example if the US were to leave NATO, they miss out on all the benefits that European NATO members provide, but also would not need to defend Europe, which is where any war involving NATO members is likely to happen. It seems to me that the US gains very little from being in NATO. reply OkayPhysicist 15 hours agorootparentNATO is the greatest power projection project the in the history of the world, and is in no small part why the US has achieved hegemonic status. The ability for the US to wage wars on the opposite side of the globe without major logistical issues is greatly aided by NATO bases that are simultaneously in very friendly territory, and much closer to the action. NATO membership also means NATO equipment, which the US's military-industrial complex disproportionate benefits from, but also serves as lock-in: those extremely expensive aircraft are basically scrap without the appropriate service contracts and part availability, meaning any military that aligns itself with the US's tech is far less likely to be able to wage wars we don't approve of. NATO also brings stability: nuclear red-line borders are unlikely to be invaded, reducing the chances of conflicts that are bad for business. A peaceful world is a profitable world, and those profits disproportionately go to the US. NATO also provides soft power projection: NATO membership is a huge boon to its members, which grants the US leverage politically to encourage member states to adopt pro-US policies. reply hollerith 15 hours agorootparentprevThe US benefitted from NATO because if the Soviets has overrun Western Europe, there would have been what was essentially a single country (or a country and its satellites which it dominates militarily) stretching from the North Atlantic to the North Pacific, a country that probably would have become wealthier than the US and consequently eventually stronger militarily (if it had the political will to do so, which it probably would have). It was probably worth the expense for the US to have tried hard to prevent the formation of such a wealthier peer, just as it was worth the expense to prevent Germany from uniting most of Europe under its system during WWII (even ignoring the moral reasons for getting involved): being the wealthiest country in the system with the strongest military makes it less likely the country's civilians will get hurt or killed (by e.g. an invasion or a naval blockade). In other words, there has been a strong streak of national self-interest (correctly calculated IMHO -- at least until the end of the Warsaw Pact and the Soviet Union) in the US's contributions to NATO. reply ipython 15 hours agorootparentprevThat’s silly to say considering the one and only time article five was invoked was by the USA after 9/11. reply adgjlsfhk1 15 hours agorootparentprevthe US believes Russia invading more of Europe is not in their interest. I'm not sure why that is hard to understand. reply red-iron-pine 12 hours agorootparentfeedin the trolls mon ami, they be shillbots. they be putting out literal russian, like on RT, talking points. reply realusername 16 hours agorootparentprev> It seems to me that the US gains very little from being in NATO. NATO is the only reason why the EU buys so much US weapons in the first place, the benefit to the US is enormous. reply rad_gruchalski 11 hours agorootparentprevWhere are you going to sell your Pepsi Cola to? reply simion314 15 hours agorootparentprev>it seems to me that the US gains very little from being in NATO. So far Europeans died in USA conflicts around the world , how many conflicts did Europe start and USA had to get involved so far ? From me a Romanian, feels very shitty that our soldiers died in Iraq and Afghanistan for America but now if we will need help Trumpists will not help back. reply dralley 14 hours agorootparent>So far Europeans died in USA conflicts around the world , how many conflicts did Europe start and USA had to get involved so far ? Setting aside for a moment the rather large conflict that ended in 1945 - if you're in your late 20s or older, both Libya and Kosovo happened within your lifetime and meet your criteria. reply simion314 14 hours agorootparent>Setting aside for a moment the rather large conflict that ended in 1945 - if you're in your late 20s or older, both Libya and Kosovo happened within your lifetime and meet your criteria. 1 That was not an Article 5 thing, 2 how many USA soldiers gave their life in Kosovoa? 3 1945/ww2 I think USA was attacked by Japan, americans did not enter the world to protect Europe, they were forced in the war so they had no choice then to fight their enemies wherever they are. reply dralley 14 hours agorootparent>3 1945/ww2 I think USA was attacked by Japan, americans did not enter the world to protect Europe, they were forced in the war so they had no choice then to fight their enemies wherever they are. The American strategy in WWII was \"Germany First\" despite it having been the Japanese that attacked us. reply simion314 14 hours agorootparentAnd that strategy was somehow altruistic ? Explain? And explain why waiting to defend \"Europe\" Anyway we talking about NATO, USA used Art5 and Europeans died for USA, but now Trumpists complain that we are not doing enough and USA will not return the favor reply mulmen 12 hours agorootparentThis history on this is well documented. The US was helping the allies before entering the war by providing weapons and resources. Before being attacked the US was isolationist (like “Trumpists”). It took getting punched in the mouth to muster the resolve to join the war. The attack on Pearl Harbor brought the UK to war with Japan and solidified allied resolve. https://en.m.wikipedia.org/wiki/Declarations_of_war_during_W... reply mulmen 12 hours agorootparentprev> how many USA soldiers gave their life in Kosovoa? Based on a quick wiki scan looks like two. This accounts for all NATO casualties. No Romanians were killed in Kosovo. A combined total of 30 Romanians were killed in Iraq and Afghanistan. reply Hamuko 16 hours agorootparentprevSweden has a pretty solid defense industry. Gripens and NLAWs for example are Swedish production. reply FirmwareBurner 16 hours agorootparentCorrection: NLAWs are only Swedish designed(SAAB defense) but are production of Thales(a French company) manufactured in Belfast, Northern Ireland(UK), using warheads made by a subsidiary of SAAB in Switzerland. Yes, I'm fun at parties. reply red-iron-pine 12 hours agorootparentThe UK was heavily involved in the NLAW development too, IIRC. Not just made in NI but the UK provided a lot of the initial funding along with SAAB. and boy howdy do those NLAWs work. reply fennecbutt 15 hours agorootparentprevHe he, my response to the sorts of people that use the \"I bet you're fun at parties\" jibe is usually \"we go to different parties\". reply nextos 16 hours agorootparentprevIndeed, for a small country in terms of population, they have a really impressive defense industry. Also a very interesting distributed defense doctrine: https://en.wikipedia.org/wiki/Bas_90 reply dehrmann 16 hours agorootparentprevWe have a permanent aircraft carrier in the Baltic. reply Findecanor 15 hours agorootparentDid you mean the strategically located island of Gotland, which has been called an \"unsinkable aircraft carrier\"? reply PeterStuer 15 hours agorootparentprevnext [3 more] [flagged] ardaoweo 15 hours agorootparentYes, everyone that opposes Russia's imperialist tendencies wants war. If everyone just willingly got annexed by Russia there would be peace. reply TulliusCicero 15 hours agorootparentprevAh yes, it's NATO that wants war, it's certainly not NATO countries being reasonably concerned about a certain neighbor that keeps invading neighboring democracies. I'm sure without being in NATO or the EU, Estonia and Latvia would be left completely alone and unharmed by Russia, just like Ukraine! They certainly wouldn't have been a smaller and easier target. reply AlbertCory 16 hours agorootparentprevnext [2 more] [flagged] kazinator 16 hours agorootparentYou, you can confidently state that at that time, they had at least one train. reply willcipriano 16 hours agorootparentprevThe cold war was the largest project undertaken by any civilization in human history in terms of spending. With Sweden on board with NATO, defence contractors and other NATO adjacent public private partnerships can be assured that they won't interfere with that flow of funds. reply ethbr1 16 hours agorootparentWe did get the Internet, space programs, satellite-based sensing, phased array radar, MAD theory, and some other stuff out of the spending. reply guappa 16 hours agorootparentAnd it costed us 100x more than if we just put the money into science directly! YAY! reply ethbr1 14 hours agorootparent:) Politics decides funding allocation in a democracy. And it's a lot easier to justify military spending than it is research. Same reason NASA is the way it is, but we still have a space program. reply euroderf 12 hours agorootparentsee: Report from Iron Mountain on the Possibility & Desirability of Peace. reply richrichie 16 hours agoparentprevnext [4 more] [flagged] ProjectArcturis 16 hours agorootparentDidn't realize it was NATO troops committing genocide in the Balkans and invading Georgia and Ukraine. reply richrichie 10 hours agorootparentIt is NATO that is fighting in Ukraine. It is they who have been active in Ukraine since 2008. Without NATO’s active involvement so many Ukrainians would not be dead. Canada joining Warsaw pact with the prospect of Russian missiles and troops stationed there and KGB organising a coup to remove their PM to install a puppet will elicit a similar (or perhaps worse given the track record) response from the US. And yes NATO tried to pull the same Ukraine stunt in South Ossetia. https://www.theguardian.com/world/2008/sep/12/putin.georgia reply ProjectArcturis 9 hours agorootparentWithout NATO's involvement, far more Ukrainians would be dead, their children kidnapped, their possessions stolen by Putin's thugs. If the KGB did achieve a coup in Canada, we probably would invade to put it down. The differences are 1) Euromaidan was a popular uprising, not a coup, and 2) the whole thing would have been over in a week, instead of bogging down into a stalemate and humiliating our army. Your link is just quoting Putin complaining that Bush wouldn't help him with his coup in Georgia. reply AlGrothendieck 15 hours agoparentprevnext [6 more] [flagged] dralley 14 hours agorootparentnext [6 more] [flagged] AlGrothendieck 14 hours agorootparentnext [6 more] [flagged] dralley 14 hours agorootparent>Remember, the year before invasion all what Russian diplomacy wanted - it is to give guaranties that Ukraine won't join NATO? \"The year before the invasion\" - 2013, right? That's the year you're referring to? The year in which he got a 30 year lease on Sevastopol, thereby keeping Ukraine out of NATO for decades? Because Yanukovich was still in power, and doing everything to appease him? Why is it that the medal Shoigu got for the Crimea takeover commemorates a date a week prior to Maidan, and two weeks prior to the \"official\" start of the invasion? Why is it that despite the \"diplomacy\" they never even attempted to abide by the Minsk agreements (and if you feel like disputing this, refresh yourself on article 10 and the timeline of events at Donetsk Airport)? reply AlGrothendieck 13 hours agorootparentnext [2 more] [flagged] dralley 12 hours agorootparentDo you or do you not agree that \"the invasion\" started in 2014, not in 2022. This is relevant. You said \"before the invasion, all Putin wanted was diplomatic agreements to not join NATO\". But that's obviously not true, was it? Because before 2022, he had already annexed Crimea. Before 2022, he had already been delivering massive supplies of weapons and \"separatist volunteers\" to Donetsk and Luhansk. Before 2022, he had already published \"On the Historical Unity of Russia and Ukraine\". Before 2022, there was already zero chance of Ukraine being accepted into NATO. Those are not mere diplomatic tools, that is active aggression and denying their right to exist as a nation at an ideological level. How do you negotiate with \"you are Russians, your identity is fake, your language is fake, your land is Russian, prepare to be absorbed\". reply xpl 14 hours agorootparentprevWatch the recent Tucker Carlson's interview with Putin. There is a literally 2 hours long historical lecture from Putin, explaining in great detail why (to his opinion) Ukraine doesn't and shouldn't exist as a sovereign state and \"in fact\" never was, since the beginning of the history, LMAO. This is what really concerns him, if you listen. In his reality, Ukraine is a Russian province, not an independent state. He goes as far as justifying Hitler's attack on Poland (\"they made him to attack by not voluntarily giving away their territory, it was Poland's fault\") — sic. You see where he goes? So it seems that NATO didn't bother him very much, it is just a red herring or \"diplomatic speak\" to express his true concerns. The real issue is Ukraine's sovereignty / independence. Joining NATO is just an act of that sovereignty, that couldn't be tolerated. reply rixrax 13 hours agorootparent>> He goes as far as justifying Hitler's attack on Poland And why wouldn't he. We should not forget that it was russians and nazi's that started the world war 2[0][1]. It's just that the rusians got lucky, and Germany bore all the guilt. Since then, russians of course have been actively trying to rewrite history. My grandfather fought russia and later its ideology in battles across the world. I did forget for a while, but his words now shine brightly in front of me: NEVER trust a russian. [0] https://www.annefrank.org/en/timeline/60/germany-and-the-sov... [1] https://en.wikipedia.org/wiki/Molotov–Ribbentrop_Pact_negoti... reply xpl 12 hours agorootparentAnd just recently Putin said that Belgium \"owes its existence to Russia\": https://meduza.io/en/feature/2024/03/07/in-his-latest-pseudo... Rewriting history is pretty much real — there is a head of Ministry of Culture in Russia that openly states that \"our history books for students should serve the national interests\" (and he is a co-author of those books), implying that the truth can be manipulated to indoctrinate young people. > NEVER trust a russian P.S. I am myself Russian — not saying you should trust me... (just kidding). Its just not every Russian out there likes what Russia does. reply sschueller 16 hours agoparentprevnext [12 more] [flagged] empath-nirvana 16 hours agorootparent> Why isn't there a NATO base in the US where German military etc. \"hang out\"? The reason there is a large US presence in Germany is that Germany lost WWII. You can make all kinds of other justifications for it like \"Germany invited the US\", but it all comes back to the Allies created the current German government, and all of them had troops continuously stationed in Germany from WWII until well after the cold war ended. France and the UK have withdrawn most of theirs, the US is the one that still has a sizable presence and the reason is that Germany is afraid of Russia and wants them to stay. reply miroljub 16 hours agorootparent> ...wants them to stay. This may be true now, by a small margin, but until two years ago, the big majority of Germans wanted US troops to go away. I can't even count how many times I saw or heard \"Ami go home!\" reply tw04 16 hours agorootparentprevNo? Just because Sweden is a member of NATO doesn't meant they have to just let the US do whatever they want. IF Sweden so chooses to allow US troops to be stationed there, it will be for their own benefit. reply u320 15 hours agorootparentSweden hosts US troops already. There was no reason to wait for Orban and Erdogan for that. reply dragonwriter 15 hours agorootparentprev> Why isn't there a NATO base in the US where German military etc. \"hang out\"? The same reason that within NATO, German troops are forward deployed to Lithuania and not vice versa; it doesn't contribute to defensive strategy to backward-deploy forces from the countries closer to the large conventional threat to the ones farther from it. reply Rebelgecko 15 hours agorootparentprevThe German Air Force's USA/Canada command has a couple thousand people in the US (granted, the US military has more people than that in Germany) reply u320 15 hours agorootparentprev> Why isn't there a NATO base in the US where German military etc. \"hang out\"? Because Russia is in Europe, not in the US. reply petschge 16 hours agorootparentprevThere is the GAFFTC (German Air Force Flying Training Center) at Holloman AFB? reply KingOfCoders 16 hours agorootparentprevMost (all?) of German jet fighter training happens partially in the US. reply nemo44x 16 hours agorootparentprevnext [3 more] [flagged] ThisIsMyAltAcct 15 hours agorootparent> Germany is a vassal of the United States Oh lord this again reply u320 15 hours agorootparentprev> Germany is a vassal of the United States No. reply Ygg2 15 hours agoparentprevYeah. We should thank people that put him there. Checks notes - The United States. Huh?! Wait. Putin got voted person of the year 2007? reply The_Colonel 14 hours agorootparentBefore war in Georgia, Putin had a pretty good image in the West. Back then Putin was nowhere near bad as in the recent years. Power corrupts, long time rulers get crazier with time... reply AlGrothendieck 14 hours agorootparentnext [3 more] [flagged] The_Colonel 12 hours agorootparentHello, dear account dedicating all their two posts to repeating Russian talking points. Georgia was indeed dumb enough to let themselves get provoked to fire the first rounds. It's very strange though that the peace loving defensive-only Russian forces ended up permanently occupying even more Georgian territory than before the war. That's the Russian history, Russia never attacks, it only defends itself, by advancing and annexing enemy territory. reply lern_too_spel 12 hours agorootparentprevThat part of the report has been widely panned. Russia had brokered a ceasefire with separatists in that region in 1992, then placed its own troops there to act as peacekeepers. Putin's gaze lands on Georgia, and he moves more troops border, the separatists blow up a police car two days after joint American-Georgian military exercises, Georgia's military responds, and within hours, Russian tanks are rolling into Georgia. The EU report was clear that Russia's response was out of line. > The report found no evidence to support Russian allegations that Georgia was carrying out genocide against the South Ossetian population. > But it said there were \"serious indications\" of ethnic cleaning against ethnic Georgians in South Ossetia and found Russian forces \"would not or could not\" stop atrocities by armed groups in areas they controlled. reply georgespencer 15 hours agorootparentprev> We should thank people that put him there. Checks notes - The United States. Huh?! There's nothing inherently wrong with an administration exercising soft power to help a favoured politician in another country – even if they sometimes turn out to be dictators – but in this instance I think it's disingenuous to suggest that the United States \"put [Putin into power]\", assuming you're referring to the United States' inaction after the FSB apartment bombings? > Wait. Putin got voted person of the year 2007? Time's Person of the Year Award is specifically scoped to not be an endorsement or celebration of the winner. Think of it more as a measure of outsized impact on the world in a given year. reply chasd00 14 hours agorootparent> Think of it more as a measure of outsized impact on the world in a given year. I was Time's Person of the Year in 2006, all i did was get in arguments with people i didn't even know. reply Ygg2 14 hours agorootparentprev> administration exercising soft power to help a favoured politician in another country – even if they sometimes turn out to be dictators You know. It would be fine if it was a few times thing. But US has a long history of putting dictators into power, calling them allies, then turning against them and invading/killing them. Which only seems to benefit the military and cause chaos everywhere else. > That the United States \"put [Putin into power]\" assuming you're referring to the United States' inaction after the FSB apartment bombings? No. The economic shock therapy. reply Macha 14 hours agorootparentprev> Wait. Putin got voted person of the year 2007? Other notable winners include Hitler, Stalin (twice!), Khrushchev and Trump. I don't think it's an indication that the west likes them. Even in recent times, I'm not sure it's even an indication that time magazine likes them (see Trump) reply cactusplant7374 16 hours agoparentprevThe exercise in team work has not been going great so far. It doesn't look like their is a path to victory in Ukraine and Europe is hesitant to commit any troops. There are some rumblings from Macron but most of Europe would prefer to send just enough weapons so Putin can't move forward and Ukraine can't push them out. reply SAI_Peregrinus 16 hours agorootparentMost of Europe is in NATO. NATO nations committing troops would likely drag all of NATO into war with Russia, increasing the chance of a nuclear war. That's not something NATO wants. reply troupo 16 hours agorootparent1. It's really doubtful Russia will commit to nuclear war. 2. If it does, it will do that regardless of whether or not NATO enters the war or not. Russia has signalled it has no intentions of stopping its war of conquest reply bumby 16 hours agorootparentI'd be curious what details you're drawing on to make those conclusions. Regarding #1, do you think this is the case if Ukraine, for example, gains enough traction to attack Russian border cities as a means of preventing a Russian regrouping and counter-attack? Or is the word \"commit\" doing a lot of heavy lifting here? Regarding #2, I've heard two scenarios that would counter this. If Russia wins in Ukraine, they likely have an interest in further expansion. If they think NATO isn't really as committed as they claim, a nuclear exchange into someplace like Poland would prove that, as well as giving the US a plausible way to back out of NATO commitments. That's a huge win for Russia. The previous statement about Ukrainian success provides the other example. Both cases are conditional on NATO activities. reply troupo 14 hours agorootparent> Regarding #1 Too much in Russia depends on the West. I'm not even talking about its industrial capacity which can't even produce military equipment without foreign components. Their children study and live in Europe and the US. Their families live in Europe. Their business interests are in Europe. I really doubt any of them will risk a nuclear war. > Regarding #2 They've been quite vocal about this for a long time: they will continue war until stopped. At least until they claim all/most of the former USSR territories. Some of those territories (the Baltic states) are in NATO. reply cactusplant7374 15 hours agorootparentprev> if Ukraine, for example, gains enough traction to attack Russian border cities That has already happened. https://apnews.com/article/russia-ukraine-war-drones-a9fc4dd... reply gfodor 16 hours agorootparentprevTaken together what you wrote here is a convenient framework for all blame of any possible nuclear exchange to be entirely disconnected from NATO. reply epistasis 16 hours agorootparentDescribing it as \"convenient\" does not make also make it any less true or accurate, which are the true metrics a framework should be evaluated by. We can't disregard frameworks just because we don't want one side to benefit, we must evaluate frameworks on whether they represent reality. reply troupo 16 hours agorootparentprevThere's exactly one country threatening its nukes, conducting the largest war in Europe since WWII and showing no willingness to stop. So, the question is: what do you do? Sit back and let it take whatever it wants? reply watwut 15 hours agorootparentprevIf Russia starts nuclear exchanges yes they will be solely responsible. They are being aggressor. Also, they signaled wish to expand beyond Ukraine multiple times last year. reply Horffupolde 16 hours agorootparentprevSo paradoxically, by NATO increasing in members, its non-MAD strength decreases. reply notahacker 16 hours agorootparentNot really, because Sweden wasn't going to send troops to Ukraine as a NATO proxy without a defence pact anyway. reply KingOfCoders 16 hours agorootparentprevIt has been going better than the last 30 years. Military cooperation in Europe broke down with the last Reforger of '93. It is great that Europe cooperates more, the view of the US public on NATO can be seen in any US TV series or Hollywood film - it does not exist. reply dylan604 16 hours agorootparent> the view of the US public on NATO can be seen in any US TV series or Hollywood film - it does not exist. I wouldn't say it doesn't exist. To me, it's always more of that \"we'll let you guys muck about until it's totally obvious you're not going to fix it, then we'll ride in to save the day\" attitude. Not saying that's accurate, but that's how it's portrayed by Hollywood reply KingOfCoders 16 hours agorootparentWhat Hollywood film that involves the US military does include NATO militaries? Rewatching SG-1, and of course it's US only. reply dylan604 15 hours agorootparentOther than WWII films, I can't think of any Hollywood that actively promotes NATO in anything other than a joke. Even the more common cop shows, once an investigation goes international and INTERPOL has to be brought it, it's always a big sigh as if \"oh boy, here's where the wheels fall off the bus\" as the member agencies that make up INTERPOL are definitely looked down upon. The flip side of this is watching European shows, and they all feel like US law enforcement is just a bunch of gung ho gun toting cowboys. Neither view is entirely accurate, nor are they inaccurate as they are just stereotypes reply chasd00 15 hours agorootparentprevI don't think it was Hollywood per se but The Day After did. reply orwin 12 hours agorootparentprevI've heard that a us tv show about marines or whatever used to include European, Mexican and south American special forces as allies regularly, to add realism in conflict they're not alone in. reply unmole 16 hours agorootparentprev> Not saying that's accurate It's fairly accurate reply bryanlarsen 16 hours agorootparentLess accurate for a NATO that contains Sweden & Finland, though. There's no doubt those 2 have and will pull their weight. Speaking as a Canadian, whose country doesn't. reply dylan604 14 hours agorootparentThere's a difference between member states pulling their weight and the org itself pulling itself together to behave as a coherent entity. If Turkey (or any member) decides to veto or drags its feet prevent any action at all from occurring, it doesn't matter. In the mean time, the aggressor is taking advantage and ransacking its way through Europe. The whole time, the individual members are waiting for the Yanks to get off their arses to unilaterally come to action. Then they can later point at how the Yanks are always doing things unilaterally and turn them into the whipping boy. reply Paradigma11 9 minutes agorootparentThere is no veto power on article 5. It is up to the member states to organize their response or not, but they cant prevent other states to do so. unmole 16 hours agorootparentprev> will pull their weight How much do they weigh in comparison to the 800lb gorilla that is the United States? reply bryanlarsen 16 hours agorootparentThe non-US members of NATO combined are larger economically, demographically and geographically than the US is. If they all pulled their weight NATO would have (slightly) more contribution from non-US countries than from the US. reply unmole 3 hours agorootparent> If they all pulled their weight This hypothetical doesn't advance the original claim. Sweden might punch well above its weight. But that hardly matters in absolute terms. reply dylan604 15 hours agorootparentprevThe non-US members all have different views that must reach a consensus before any action which is precisely where the machine grinds to a halt. Which means a consensus is rarely reached. Contrasting that to the US which can bring all of its might with the whims of one leader and possible brow beating of congress to agree. In this way, NATO/UN is interchangeable from the US point of view. reply rainworld 15 hours agorootparentprev>unironically derives his understanding of the world from his consumption of Hollywood slop Granted, a widespread problem, but, in my experience, nowhere as bad as in contemporary Germany. Why is that? reply The_Colonel 14 hours agorootparentprevDepends what you call \"victory\". The western priority is preserving sovereign Ukraine, even if it has to concede some land. There is a path to achieve such goal. reply NicoJuicy 16 hours agorootparentprevSweden and Finland are in NATO. Leverage from turkey and Hungary is decreasing, if you haven't seen it. Democracy may be slow, but it's still the best system. Additionally, now support for Ukraine can increase and it should go faster. God, I really hope it goes a lot faster... reply dragonwriter 16 hours agorootparentBecause NATO (for most key decisions) works by full consensus, adding new members (a process which involved Turkey and Hungary using their leverage for concessions, especially Turkey) does not substantially reduce the leverage of existing members (this is also why Russia's bid to move to the front of the line and be admitted ahead of other Easter European applicants, without a readiness process--the real root, not the fact of expansion into Eastern Europe, of Putin's resentment against NATO--was rejected.) reply pphysch 16 hours agorootparentprevnext [7 more] [flagged] rnk 16 hours agorootparentCan you explain what project is unwinding? It looks to me like Europe is still coming together to resist the danger of Russian invasion. For Nuland, it looks like she had a long career, she had many good roles, she championed protecting Ukraine and there was only one other position for to be promoted to and they promoted someone else. So that seems like a perfect time to leave after a successful career. reply pphysch 15 hours agorootparentIn Ukraine, Zelensky purged his military leadership amid significant battlefield defeats including the dysfunctional retreat from the stronghold of Avdiivka. The situation is very unstable and the worst time for influential players like Nuland to retreat from power. It suggests a collapse of the entire project. reply Paradigma11 4 minutes agorootparentOh boy, by that account Russia should be in rubbles a year ago. Putin even had to kill off his caterer turned warlord, who staged a mutiny and marched with 5k mercenaries on Moscow and the rest of the country just stood by and watched the show. NicoJuicy 14 hours agorootparentprevRetreating gives more benefits to Ukraine. If you think anything is defeated, you should look at the facts ( eg. Airplanes downed in the last weeks). Which seems to be more significant to me while f16's are on the way. Retreating is not defeating :) reply u320 15 hours agorootparentprevVictoria Nuland is a red flag. Conspiracy theorists are obsessed with her for some reason. Whenever she is brought up, I stop listening. She's not that important. reply pphysch 15 hours agorootparentnext [2 more] [flagged] NicoJuicy 13 hours agorootparentConspiracy theories see things when things aren't there. She had a political career for 30 years and she claimed to retire. Did you consider that she perhaps just wants to retire? NATO has all the new ( and strong) members that wanted to join. Seems like a good time like any other to me, additionally she's 63. Passed the smell test for me. reply dralley 16 hours agoprevHere's hoping this means they can send Gripen, finally. reply whereismyacc 14 hours agoparentTo ukraine? I thought the idea was that Ukrainian pilots would be trained on f16s instead because the stockpiles of those are many times bigger? reply dralley 11 hours agorootparentThe F-16 training and delivery is already public, but it has been hinted at for a while that Ukrainians were being trained on Gripen also and that once Sweden was in NATO they'd be able to send some jets. reply whereismyacc 10 hours agorootparentOh interesting, I guess I just haven't been keeping up. I'm no military dude, but from what little I've read that it is hard to coordinate between different kinds of fighter jets in the same air force, so it would have been preferable to get more f16s. reply ImHereToVote 16 hours agoparentprevSure. The US Military Industrial Complex loves to share. reply dralley 16 hours agorootparentWhat does Gripen have to do with the US military industrial complex? The US isn't the ones sending F-16s anyway. reply sspiff 16 hours agorootparentThe parent comment is referring to the US pressuring all European NATO members to abandon plans to acquire Saab or Dassault jets in favour of its own F35. reply dralley 16 hours ago [flagged]rootparentAh, so mythology. Most of the European nations with F-35 currently either joined the program from the beginning decades ago, or want to participate in nuclear deterrent and have no other option (unless France decides to share their nukes, but that hasn't happened yet, and you can bet they'll want you to buy Rafale for the privilege if they do), or want stealth capabilities which nobody else can offer right now, or want greater compatibility with the US munitions stockpile which is vastly larger than what Europe has available... etc. Jets take a long time to develop, the time for Europe to get serious about \"strategic autonomy\" in that respect was 2 decades ago. That didn't happen, so now the F-35 is the only option if they want the capability to penetrate Russian ground-based air defenses and consistently beat the Su-57 (cough all 8 of them, but it's possible they scale up eventually). reply ianburrell 15 hours agorootparentprevThe F-35 is better and cheaper than Gripen or Rafale. Everyone who can buy F-35 is doing so. There were countries that switched when allowed to buy F-35. The Gripen's problem is that it is expensive for a light fighter. The other problem is that uses an American engine and US can control export. There aren't many countries that can't buy F-35 but can buy Gripen. Brazil is the big one. The French Rafale is having more success because it isn't export limited. It is better than Gripen and same price. reply u320 15 hours agorootparentSweden's problem is that we insist on only selling to the \"good guys\" (for domestic policy reasons), but everyone who is considered a good guy (e.g. Norway) is already in the F-35 program. So we sell to no-so-good-but-not-terrible guys like Brazil, South Africa, Hungary and Thailand. reply aubanel 15 hours agorootparentprevThe Rafale is cheaper than the F35, by a wide margin. reply dralley 15 hours agorootparentIt is not. A brand new F-35A costs a bit over 70 million. A new Rafale costs a bit over 100 million. In terms of operating costs, yes the Rafale is cheaper, mostly because it's not stealth. The fact that France is spending their money in France helps make up the difference - for France. But if you aren't France, the F-35 is a good deal. reply riku_iki 10 hours agorootparentprevGripen is much cheaper to operate at least per official numbers. reply dataflow 16 hours agoprevDoes being in NATO obligate you to respond militarily if another member is attacked (as is commonly believed)? I failed to find this clause in the text last time I looked. Edit: for everyone telling me to read article 5: I already have, hence this question. It says each member \"will assist [...] by taking [...] such action as it deems necessary\". That very much doesn't appear to obligate a military response, or any response at all. To put it another way, the treaty really doesn't seem to mean much, so far as I can tell? Countries could already help each other out (or not) anyway... no? reply SamBam 16 hours agoparentDid you actually take a look at the text of the treaty? It's very short and readable. Article 5 is what you're looking for. https://www.nato.int/cps/en/natolive/official_texts_17120.ht... reply dataflow 16 hours agorootparentYes? It says \"will assist\" and \"such action as it deems necessary\". That doesn't obligate them to provide a military response. What makes you believe otherwise? reply thatwasunusual 16 hours agorootparentBecause no member wants to be seen as unreliable, risking their own security in the future, so everyone will in practice assist in any way they can. reply dataflow 15 hours agorootparentOr they might calculate WWIII isn't worth it? Imagine Russia attacks Latvia. Would the US really risk getting into direct conflict with Russia over it? reply sph 15 hours agorootparentWould the US really risk its name and face ignoring their biggest and most powerful military alliance? Honour and keeping your word are very important in geopolitics, especially among countries that have been allied for almost a century, and you won't remain the top dog very long if you avoid your duties at the first difficulty. So yes, if Russia attacks Latvia, you better believe the US is gonna send everything they have against Russia. That's the whole point of NATO. reply dataflow 13 hours agorootparentWould you really go to war over another country at the risk of you and your country getting nuked? reply pas 13 hours agorootparentYes.. ? Russia already and continuously threatening other countries, the US included reply dataflow 13 hours agorootparentCan you convince the US population of that so that they'd be willing to risk their lives in a nuclear war? reply bradlys 13 hours agorootparentThe US population doesn't need to be convinced. Only the stakeholders of the MIC need convincing that their NW will go up. reply xdennis 14 hours agorootparentprevI'm pro NATO, but I don't think the support of the US is guaranteed. The US has always been an unreliable partner: it depends on who's in power. If the republican war hawks or moderate Democrats are in power they would support Latvia, but libertarians and far leftists would say \"it's not our problem\". reply sph 11 hours agorootparentThere are no libertarians or far left politicians in the Oval Office, now or in the near future. I cannot speak to how the US would behave in 50 years, but being true to its word it is mandatory for the US to project itself as the \"world leader\". The day the Oval Office is able to ignore its allies, it is the day the US is no longer a global leader. That's simply the cost to pay to be seen as the leader of the free world. The fact that some US politicians openly want to go down the route of reneging its allies speaks volumes about how long the US empire will last. reply chasd00 15 hours agorootparentprevIf the US didn't then NATO is effectively toothless, if the US did then Russia would be destroyed. I'm sure lots of officials in Moscow have spent a lot of time thinking about it. reply dataflow 15 hours agorootparent> if the US did then Russia would be destroyed So would a lot of the US. I'm not sure Americans would want to die over this. reply dkjaudyeqooe 15 hours agorootparentprevThe risk is not getting involved. NATO is first and foremost a deterrent, destroying the credibility of that deterrent would destroy the rationale for NATO. To say nothing of the fact that it'd just be an invite to Russia to invade more countries. The seeds of WWII were sown by the appeasement of Hitler, which merely emboldened him. reply csallen 15 hours agorootparentprevYes. reply thatwasunusual 13 hours agorootparentprevCapitalist US would never let such a big market go east without a fight. reply alexisbear 16 hours agoparentprevIt doesn't obligate members to respond militarily, it obligates NATO members to consider an attack against a member as an attack against them. Retaliation and any other action needs to be discussed by NATO members and can also be vetoed. A lot of people seem to think that if article 5 is brandished, war will result, it absolutely depends on what the council will agree upon. reply dragonwriter 14 hours agorootparentWhile Article 5 is important, its not in a \"technical legal mandate\" way, and there is a lot more to the alliance than Article 5, there's integrated military command, training, defense strategy, forward deployments to threatened countries, etc. And there is Article 4 collective regional security, which has ultimately resulted in more NATO combat operations than Article 5, which has been invoked exactly once, even though the former has even less explicit obligation than Article 5. reply hyggetrold 16 hours agoparentprevYes - Article 5 - the absolutely key piece of NATO. reply jedberg 16 hours agoparentprevArticle 5 https://www.nato.int/cps/en/natohq/topics_110496.htm reply empath-nirvana 16 hours agoparentprevOnly if they ask. If you've got it handled, you don't need to invoke article 5. reply dataflow 16 hours agorootparentEven if they ask, it seems to me you get to decide how you want to get involved (if at all). reply evanb 12 hours agoparentprevLawfare had an interesting discussion recently: https://www.lawfaremedia.org/article/the-lawfare-podcast-how... reply mig39 15 hours agoparentprevNATO invoked Article 5 after 9/11, BTW. reply thebeardisred 16 hours agoparentprevThanks for asking this question. In reading the links to Article 5 I discovered that Ireland is not a member of NATO in an attempt to have a Swiss type of military neutrality. reply 291 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Sweden joined NATO as the 32nd member on March 7, 2024, by submitting its instrument of accession to the United States Government.",
      "Being part of NATO's collective defense, Sweden's membership will have implications on its foreign and security policies.",
      "The Prime Minister's Office and the Ministry for Foreign Affairs of Sweden have issued various statements and press releases addressing Sweden's NATO membership."
    ],
    "commentSummary": [
      "The discussion delves into various topics concerning NATO, Russia, Ukraine, Sweden, Finland, Hungary, Turkey, and the USA, focusing on historical conflicts, NATO's global security role, Russian aggression concerns, Ukraine's NATO membership potential, and the promotion of democratic values.",
      "Leaders like Putin, Orban, and Erdogan's influence, NATO's expansion implications, military capabilities, defense industry, potential conflict scenarios, and the US's role in NATO are key points of the conversation.",
      "Emphasizing the complexities of international alliances, the threat of Russian expansion, and the necessity for unity and cooperation to uphold peace and stability."
    ],
    "points": 487,
    "commentCount": 560,
    "retryCount": 0,
    "time": 1709829821
  },
  {
    "id": 39636470,
    "title": "Bypass Copy-Paste Restrictions with Chrome Extension 2.0",
    "originLink": "https://github.com/aaronraimist/DontFuckWithPaste",
    "originBody": "Don't fuck with copy and paste Background It annoys me to no end when a web application prevents me from being able to paste content into an input field, or copy it out. If I paste an incorrect email address, that's my own damn fault. I use tools like 1Password to remember all kinds of things for me, and it's actually more error prone for me to type out all the characters than it is for me to copy from 1Password and paste into a text box. Solution This is a dead simple Google Chrome extension that removes copy and paste blocking. Usage The easiest way to add a site to the blacklist is to click on the extension icon , then optionally edit the auto-generated pattern, lastly click \"Save\": After that, the extension icon should now be blue, meaning the extension is active for your current tab: Version 2 Upgrade Version 2 is a major update to the extension. It makes it much easier to ensure the extension is only running on sites that are bad actors with copy & paste events and it also provides visibility into the active/inactive state of the extension for each tab. In order to provide the smoothest experience as possible, the extension needs to know when you change active tabs. In order for the extension to know about that event, it needs the tabs permission, which Chrome describes as \"can read and change all your data on websites you visit.\" That description is very scary, and is certainly not what this extension is doing. Being an open-sourced project, you can always read all the code to see how this extension works, and what it's [not] doing with your data. To read more about the version 2 upgrade, see: the wiki page.",
    "commentLink": "https://news.ycombinator.com/item?id=39636470",
    "commentBody": "Don't fuck with paste (github.com/aaronraimist)449 points by zettabomb 9 hours agohidepastfavorite223 comments koenraad 1 hour agoBy disabling user input the application security actually gets worse. Users that can’t copy e.g. passwords will use less complex passwords to overcome the trouble of typing in their initially good passwords. But also user experience is degrading when applications enforce complex input and users generate that input like a chad as they should. But now they cannot paste… reply globular-toast 1 hour agoparentI'll add to that systems that require particular characters to be used, like \"must use capital, number and special character\". I prefer to generate longer passwords but using only regular characters because I find it easier to type on the occasions I do have to do that. Even worse, there are some that restrict what kind of special character you can use. So even when I've generated one I still have to edit to remove one particular character. Would it really be that difficult to display password strength and say things like \"use more characters, e.g. you could use four words\". reply _0ffh 1 hour agorootparent> Even worse, there are some that restrict what kind of special character you can use Even worse, there are some that restrict special characters, but don't tell you which! Now you've got to go trial and error to find out which of the special characters in your password is not acceptable to that precious §(\"/$& website! reply piyush_soni 52 minutes agorootparentEven worse, when on top of all these they add an arbitrary length requirement: It can't be less than 8 letters OR more than 12. :| reply cdr 0 minutes agorootparentEven worse, some will simply arbitrarily silently truncate the password. But not everywhere! The sign up page might silently truncate and then the actual login page might not. tracker1 20 minutes agorootparentprevThat one irks me too... When I built an auth/rbac app previously I did make Max length configurable and it would display a night if set. I set a hidden hard limit to 1k only to reduce attack surfaces that would only display and error if exceeded. Default was a min-length of 15 as the only requirement with the default hint of \"try using a short sentence\" I also had optional use of zxcvbn and haveibeenpwned checks during new passphrase creation. I really wanted to open source the application but couldn't get approval to do so. It was a pretty nice little simple auth application that issued RSA signed JWT to configured applications. It was interested into a few internal apps as well as for clients that didn't have something like azure ad, okta, etc. where we wrote bridge apps for auth. If I had my configuration doc, I'd probably recreate it exactly, but with a Rust backend with HTMX instead of C#+react. The date store used SQLite as a KV store, with simple methods for access that allowed an exception later for the values. Also wrote support for PostgreSQL and MS-SQL so they could be used where available. Spent a lot of time on same defaults, hashing and encryption along with required configuration options for a few clients. Aside: more devs really need to better understand public/private key generation and usage... Like not using the same keys for different environments. reply rmetzler 12 minutes agorootparentprevDon’t worry, all of this is necessary because the passwords are stored in plain text in the database. reply oneeyedpigeon 45 minutes agorootparentprev\"look, just tell me what you want my password to goddam be, and I'll go with that!\" reply ssl-3 58 minutes agorootparentprevObligatory: https://xkcd.com/936/ (Those of us who know, already know. I'd like to say that we all know here. But if a reader does not recognize \"correct horse battery staple\", then you're obliged to click the above link -- you're one of today's lucky 10,000![1]) 1: https://xkcd.com/1053/ reply tracker1 19 minutes agorootparentYeah, haveibeenpwned is a great resource. More sites really need to integrate this kind of check. reply pooper 28 minutes agorootparentprevI have multiple Google Accounts. One of them, I want to remember the password. The others, eh. I just want to copy paste. Doubly so for practically anything else. I wish they'd just let me copy paste. I have developed a maybe irrational fear of space in strings such as passwords and paths. It always scares me when people use spaces in either case. reply tracker1 17 minutes agorootparentFor passwords I have to actually remember and type in (os login, password mgr evs) I expressly use a short sentence, often with spacing and punctuation. Sometimes an intentionally misspelled word. reply jessriedel 8 hours agoprev> In order to provide the smoothest experience as possible, the extension needs to know when you change active tabs. In order for the extension to know about that event, it needs the tabs permission, which Chrome describes as \"can read and change all your data on websites you visit.\" That description is very scary, and is certainly not what this extension is doing. Being an open-sourced project, you can always read all the code to see how this extension works, and what it's [not] doing with your data. The problem is that even if I read the code, or more likely chose to trust that someone has, it's not guaranteed to remain true for future updates. The author's scruples may weaken with time, or they might sell the extension, etc. (I think Chrome's extensions auto-update, but even if they didn't I'd still have to remember that this extension is one that I can't assume it's safe to update.) reply nextlevelwizard 3 hours agoparentThe thing is there is no alternative way to do this. I have written some extensions my self and often you cant do anything without having full read and write access to every page. For example I have an extension that lets you right click an image and rotate it by -90/+90/180 degrees. All I want is for the browser to hit me up when there is a `` tag, but that is not an option. Either I have to white list every page separately in the code or ask the user to white list every single page or just ask for full read and write permissions for every single web page the user visits. reply hahn-kev 1 minute agorootparentFor the lay person being able to access any image on any page is pretty much the same thing as being able to access all pages. reply Nifty3929 4 hours agoparentprevThis is a bit cynical isn't it, when the author is clearly being as transparent as possible about what they need and why, which is due to factors outside their control. Of course you're right in a technical sense. They could do whatever they want later. But still let's celebrate and attitude like this rather than criticizing it. reply Beldin 3 hours agorootparentThis has been used as an attack vector in the past: spot reasonably popular plugin; make author an offer; inject whatever tracking/other malwate stuff new owners want (typically after a delay). So now we'd have to trust the author to do thorough vetting of a potential buyer and also not sell if vetting is inconclusive. And this against an adversary aiming to cheat their way past vetting. Might be a cynical take, but it is not one without reason. As a sibling comment points out, this is due to the permission model. This doesn't let the author entirely of the hook though: the permissions model created the situation, the author chose a particular path. The consequences may not have been foreseen by either, but they do exist and affect users. reply tracker1 13 minutes agorootparentIn this case, you can build and self host on Dev mode... It's a pain but doable. reply bryanrasmussen 2 hours agorootparentprev>the permissions model created the situation, the author chose a particular path. perhaps the most reasonable or even only possible path if they wanted their plugin to be able to do what they wanted it to do, which was to keep sites and from messing with your copy and paste functionality - in other words to prevent minor maliciousness. on edit: sure, to provide the smoothest behavior, but really if it wasn't smooth people would be irritated and not want to use it. I know if I was implementing for myself I would want it to be smooth. I understand the whole \"bad things can be done\" perspective, but here for some reason I fall under a \"trust but verify\" perspective instead. reply dotancohen 4 hours agorootparentprevSounds to me like GP is complaining about Chrome's permission model, not this particular extension. reply Nifty3929 3 hours agorootparentThat isn't my interpretation having just reread it, but if that poster comes back to clarify otherwise I'll edit my post accordingly. reply beacon294 3 hours agorootparentprevNo, it's well documented. Popular Chrome plugins, mainly free ones, historically have been sold. reply shakna 3 hours agorootparentprevIt's not cynical - see what happened to ublock. That kind of mess has happened, and will continue to happen, and should be a factor in what you choose to trust. reply josefx 2 hours agorootparentWasn't the worst that happened with it that the guy who took over uBlock tried to take credit for it and asked for donations? Not like he could get away with anything outright illegal when everyone knew he was running the project. reply efilife 3 hours agorootparentprevWhat happened to ublock? Are you talking about uBlock origin? reply eslaught 2 hours agorootparentThe Wiki article has a brief summary of the history, but basically the original author wanted to transfer responsibility for the user-facing maintenance to someone else, who started seeking donations and (I believe) taking payment for \"acceptable ads\" and the like. https://en.wikipedia.org/wiki/UBlock_Origin#uBlock reply Denote6737 1 hour agorootparentprevIt was uBlock that was bought by AdBlock. uBlock origin is a different project and wasn't part of the sale. it is not accepting payment for ads. reply foofie 3 hours agorootparentprev> This is a bit cynical isn't it (...) No, it's called security. Let's put it this way: there have been FLOSS projects whose maintainers intentionally pushed compromised code to unsuspecting end users. See for example the colors attack. What leads you to believe that good intentions are enough? reply bee_rider 2 hours agorootparentprevIt would be more transparent to be candid about the limitation of what they can provide. It isn’t the developer’s fault that the ecosystem is dumb, but they could just note the limitation. reply tsimionescu 2 hours agorootparentSo you're saying they shouldn't add the feature rather than asking for the permission? reply lowbloodsugar 3 hours agorootparentprevBut WHY do they need that permission? They dont need it to implement the paste behavior. Looks super sus to me. reply ambigious7777 2 hours agorootparentThe extension needs to re-enable paste, which means it needs to possibly inject some JS into the page. reply junar 8 hours agoparentprevNot sure why OP linked to a fork instead of the original. But the original has a bookmarklet version if you would prefer an alternative. https://github.com/jswanner/DontF-WithPaste?tab=readme-ov-fi... reply zettabomb 8 hours agorootparentThis one is the version linked by the Firefox addon [0]. Honestly can't tell if one or the other is better but I like having it automatically enabled. Considering it hasn't been updated for years (but still works) I'm not particularly worried. [0] https://addons.mozilla.org/en-US/firefox/addon/don-t-fuck-wi... reply mrd3v0 35 minutes agoparentprevYou mean the permissions system is broken and most extensions do suffer from the same issue? Nah mate, we at Google, (bless them Mozilla crooks giving us control over their extensions), don't care about actual issues, we only update extensions to make money and limit user freedom. reply varenc 8 hours agoparentprevI get around that by downloading the extension source and then using Chrome extension developer mode to “load unpacked extension”. Then I’m confident the extension won’t change on me. (But for this extension I don’t give it all site permissions anyway. I just enable on site by site basis) reply quickslowdown 8 hours agorootparentThat's terrible for security, but great for convenience :) reply varenc 5 hours agorootparentCan you explain what you mean by this more? reply lukan 3 hours agorootparentProbably because \"no automatic updates means bad\"? Which might be true in general, but maybe not here. Depends how complex the source is. reply Hackbraten 2 hours agoparentprevThat's exactly why I use my system package manager to install and update browser extensions. And whenever the package repository is missing a browser extension I need, I contribute the package and take responsibility for its ongoing vetting and maintenance. reply pimlottc 8 hours agoparentprevIt is also not at all clear to me why it \"needs to know when you change active tabs\". reply Vorh 5 hours agorootparentI just read through the 65LOC source, and it's because it swaps out an active or inactive extension icon based on your active tab. https://github.com/aaronraimist/DontFuckWithPaste/blob/8cb68... reply Leszek 3 hours agorootparentYou should have read a few more lines of that source - it also sends an \"active\" message to the tab, which is what adds and removes the copy/cut/paste event handlers. reply jessriedel 5 hours agorootparentprevHuh. That seems not super important to me. Presumably he could make a option/version where the icon didn't change? reply kortilla 3 hours agorootparentYeah, seems like a lame excuse to permissions grab crazy privilege. reply ipaddr 8 hours agorootparentprevYou need to detect and stop sound. You swap out active memory. reply pimlottc 7 hours agorootparentWhat does any of that have to do with making sure input fields are pasteable? reply jdthedisciple 2 hours agoparentprevthen u can simply clone the repo and locally load the extension ... bye bye auto-updates reply pants2 8 hours agoparentprevDoes Chrome have a \"Developer Tools\" feature for extensions, so you can dive in to the code and network requests? reply styxfrix 8 hours agorootparentYes https://developer.chrome.com/docs/extensions/get-started/tut... reply vault 1 hour agoprevAnyone one else noticed OP got 399 upvotes for sharing a fork with no significant upgrades compared to the original repo? reply delegate 39 minutes agoparentOh yes, 3 files changed compared to parent and the changes are gitignore and updated URLs to the forked repo. reply mrunkel 20 minutes agoparentprevWell, this is for firefox, and the other is for Chrome, so maybe that's a significant upgrade? reply MezzoDelCammin 33 minutes agoparentprevyep. Quite the WTF reply tracker1 34 minutes agoprevThis really irks me to no end with password managers in particular. I like to generate 30 character random passwords and sites that limit input or block paste in the verify field are particularly annoying. Similar for login and 2FA entry fields. There's no good reason for it. I've often gone an extra bit to ensure password managers work well. I wish more sites would do the same. reply dkjaudyeqooe 8 hours agoprevTo work around this I usually drag and drop text pasted into the URL field or somewhere, on my Mac at least. Can I just say though that disabling paste, apparently in the name of security, is the dumbest shit I have ever encountered, right in front of ultra short timeouts everywhere. If only I could meet the people who make these decisions in person... reply S201 7 hours agoparent> right in front of ultra short timeouts everywhere > If only I could meet the people who make these decisions in person... For what it's worth, I was once forced to implement a half hour auto-logout on a website that could hardly be considered as containing sensitive data because an external pentest firm flagged the lack of a short timeout as an issue. The only way we could show clients a passing pentest was to comply with all of the findings. We all knew it was stupid but management gave us no choice but to implement it. reply BLKNSLVR 3 hours agorootparentYou must have had your shit pretty tight for the pen-tester to have to scrape that from the bottom of the barrel. reply vidarh 3 hours agorootparentSometimes they will just be excessive because nobody applies any kind of critical thinking and/or because they favour looking like they find a lot over any kind of precision. I once had a site where they insisted on disabling ping responses for the website, citing it as a serious security concern. Because surely nobody would otherwise know that the very public website was there. I replied with listing a number of websites of security focused organisations whose websites responded to ping, including assorted security services, military, and the pentesting company's own website. (I didn't object to them querying what actually responded to the ICMP requests - none of them made it past the firewall, which is what replied and revealed nothing of our internal infra - I objected to them ignoring that answer and still insisting it revealed things it demonstrably didn't, and that lack of understanding was consistent through their report) reply Nifty3929 4 hours agorootparentprev\"management gave us no choice\" - Would you have done differently? \"The only way we could show clients a passing pentest...\" reply cnity 8 minutes agorootparentPush back on the pentest firm and explain reasoning, rather than bubbling pointless requirements to the engineers. reply BLKNSLVR 3 hours agoparentprevEven MS Remote Desktop doesn't allow it. Why do they think password managers exist? reply themoonisachees 1 hour agorootparentMstsc doesn't allow it because the login screen for windows doesn't have copy-paste. It's not that it has been disabled, it's that it was never programmed to have something in the clipboard before logging in. Still, they probably could load the thing first easily, but it's Microsoft we're talking about. reply autoexec 8 hours agoprevYou shouldn't need to trust an addon for this, it's something you should be able to set in the browser. In firefox you can toggle dom.event.clipboardevents.enabled reply amethyst 8 hours agoparentI wish I could selectively disable only the \"paste\" events, because it's extremely useful to have \"click to copy this value\" type of buttons in our various work tools, and I miss the ability to do that every time I try turning off clipboard events to deal with bad actors. reply themoonisachees 1 hour agorootparentThe solution to this is to treat your clipboard as public in the long term. Don't keep sensitive data in it for longer than you need it. KeePass does this and it's great. reply arboles 18 minutes agorootparentEven when your password is erased from the clipboard after 10 seconds, that's enough for any of the tabs open in your browser to steal it. reply coremoff 34 minutes agoparentprevThis used to break google docs copy/paste - haven't tried for a while though, maybe that's fixed reply gruez 7 hours agoparentprevIME this breaks paste functionality in some web apps (eg. certain terminal emulators or text editors) reply arp242 7 hours agorootparentThe worst is when it breaks web apps in really confusing, weird, and broken ways. Slack, for example. Pasting becomes a complete clusterfuck. Things paste in the wrong location, incompletely, etc. I have no idea how they manage to fuck up \"paste in a text box\"... Facebook Messenger also broke last time I used it where the tab would start using 100% CPU, but it's been a few years since I last used it, so don't know if that's still the case. Anyway, I really wish I could do this per website. I have it disabled because GitHub started doing weird and annoying shit when I copy/paste stuff from comments and I absolutely hate it. But ... then it breaks Slack :-/ reply alpaca128 1 hour agorootparent> I have no idea how they manage to fuck up \"paste in a text box\"... For over 5 years Enter has been broken in YouTube's comment text fields. It inserts a new line but often won't move the cursor. Last year for a while they changed the text to black in the dark theme and it was impossible to write comments because the text fields simply never showed up. reply Cyberdog 7 hours agorootparentprevWho’s using terminal emulators and text editors in their browser? Actually, don’t answer that. I’m afraid of the answer. reply gruez 6 hours agorootparent>text editors google docs, WYSIWYG editors built into any number of webapps >terminal emulators ssh/serial consoles on whatever your hosting provider is. Sure, sometimes there's a command line tool to do the same on your OS's terminal emulator, but if it's for a task that you're doing once every few months (eg. recovering a bricked server), clicking a button on a website and getting a shell is just more convenient. reply lukan 3 hours agorootparentprevWikipedia does. Every site with a CMS. Google docs. Here to write in the comments you use a simple text editor. Chrome dev tools can also be used to change the code directly. Quite convenient to have the same dev tool behave and look the same on all the different plattforms. Also, everything ChromeOS related. Was that so scary? reply alpaca128 1 hour agorootparentprevNever heard of Jupyter Notebook? And services like AWS also have editors and terminals, just like countless other sites. reply fragmede 49 minutes agorootparentIf you're a cli jockey and you haven't tried using bash together with Jupyter, you've gotta give it a shot. reply rand0mx1 7 hours agoparentprevOr you can hold shift button while right clicking to force open menu. reply seqizz 19 minutes agoprevAlternative for linux users, which \"types\" the thing on clipboard to the window you select: alias pasteplease='xclip -selection clipboard -outtr \\ \\\\rxdotool selectwindow windowfocus type --clearmodifiers --delay 25 --window %@ --file reply pupppet 9 hours agoprevRight up there with hijacking Ctrl-F. reply LM358 0 minutes agoparentI don't think anyone here can imagine my bewilderment, confusion, and ultimately anger, the day I discovered that in the web interface for Outlook, Ctrl+V is the default shortcut for... send email. I would very much like to know what went through their heads when they decided on that. On further thought, maybe I don't. reply dylan604 8 hours agoparentprevThere's a lot of keyboard shortcuts that mean one thing in the browser but something totally different in another application. Now that it is common for many of these other applications to now be a web app, these keyboard short cuts are possible to start colliding. Take GoogDocs as an example. Do you want the browser's find or the app's find if you hit ctrl-f in a Doc/Sheet/etc? The vast majority of the users want the app's. Reading a news site, most people would probably expect ctrl-f for the browser's search. Just pointing out that hard rules will always have exceptions. Except for the TFA's point of copy/paste. Stop manipulating my clipboard with bullshit marketing/tracking bullshit!!!!!!! reply 8338550bff96 8 hours agorootparentThen the vast majority of users are wrong. The correct answer is for it be the browser's find. Maybe apps could bind their find/search to ctrl-s since it is incorrect for browsers to bind this to save-page anyway. reply dylan604 8 hours agorootparentwhy is all of the sudden ctrl-s wrong by the browser?? you make no sense here. you've never needed to save a web page? i guess i'm showing my age, while i don't use it daily, it has been a valuable feature for many reasons before. Edit: >Then the vast majority of users are wrong. I strongly disagree, and people unwilling to be flexible ruins the experience as those people tend to be the minority reply pinkmuffinere 8 hours agorootparentYou’re absolutely correct though, the parent comment seems to think there are absolute right and wrong answers for UI. I think that’s just not true, a good UI is one that works for your customers. reply dotancohen 3 hours agorootparentOf course there are absolute right and wrong answers for UI. Accessibility, minimum text contrast and font size, minimum size for clickable items. Keyboard shortcuts may not be (or may be) one of them, but in fact consistency across applications _is_ considered a hallmark of good UI and every Human Interface Guideline I've ever read, including open source ones such as that from KDE, specify such. reply vidarh 3 hours agorootparentYou're being obtuse. Even most of those you list will not reasonably have a fixed, absolute value that is right for all users, all applications, and all situations, and assuming they do is the cause of a lot of awful UI limitations. (Your user will never need characters to render as single pixels? Try again - sooner or later someone will decide to abuse your spreadsheet as a raytracer and be annoyed they can't make cells single pixel) And a feeling of consistentency often requires exceptions for specific cases such as the example of \"find\" where few users want to specifically find what happens to be in the browsers idea of what the document currently contains, but what it logically contains in their model of what it should contain. Consistency means that in an app that dynamically updates a scrollable region, for example, it should still find things in the currently not part of the browser document bits, and so shouldn't use the browsers find in those cases. Some users might want a shortcut that always does the browsers own find, and there generally ought to be ways to override the app, but consistently acting how the user will want is rarely compatible with absolute rules. reply pinkmuffinere 3 hours agorootparentprevI agree with almost all of the specific examples you give, but I think I agree _because_ those UI decisions work better for customers, and not because they are absolute right and wrong. I think I can illustrate this with some examples: 1. Consider a keyboard without an f-key, eg Arabic. If the user is using an Arabic keyboard, what should bring up the browsers 'find' functionality. Of course ctrl-f won't cut it. Perhaps it should be ctrl-[first letter of 'find' in Arabic]? Or perhaps ctrl-[the letter in the same position as f on qwerty keyboard]? It makes sense to follow convention if one is already established for Arabic, but then what about languages that are new to the web? 2. Consider a phone-tree, which is a sort of UI. For this UI, the 'absolute right answers' of minimum text contrast, font size, keyboard shortcuts, etc, make no sense, but there are surely other ways to make the UI work well for customers. In both these scenarios, I feel the 'right' choice is to pick the UI that is best for users. I think there isn't a-priori a right answer, and users habits change over time and across cultures, so it's not necessarily an easy choice. reply masto 8 hours agorootparentprevThey're trolling. reply lesuorac 8 hours agorootparentprevThe browser's control-f won't find you text draw onto a canvas element so those users really aren't wrong ... reply Andrex 6 hours agorootparentIt's also trivial to use Ctrl+F on such pages if you so choose by clicking into the URL bar and then doing the keyboard combo. (Or just make two clicks in the browser menu.) I can see valid use cases for customizing Ctrl+F. reply paradox460 4 hours agorootparentDiscourse apps bind the first hit of Ctrl/cmd-f to the app's search feature, and then the second passes through and hits the browsers. Seems to be the right way to do it reply grishka 2 hours agoparentprevI just don't get it why browsers allow websites to override their own hotkeys. I'm sure it even required extra code to be written to work correctly. Linear hijacks Cmd+F for example, very helpfully providing some terrible thing instead of my browser's built-in search that works the same everywhere. (it's the same Linear that thinks you can't not want wysiwyg markdown editing) reply nsinreal 2 hours agorootparentWell, for Ctrl+F there is sometimes a reason. Many websites uses technique called virtualization of lists. That boosts performance, but standard Ctrl+F doesn't works anymore properly reply BasieP2 1 hour agorootparentI know of 2 websites that do this. 1. Confluence It's super annoying and takes up a lot of screen space 2. Nexus It simply kills it. You can use ctrl-f but it simply will not find text right in front of you.. Really i see no valid case reply PennRobotics 1 hour agorootparentDOS emulator and Vim emulator, and that's almost all I can imagine. Maybe games that would use the control key as an additional input---but in the browser??? reply rasz 1 hour agorootparentprevgithub code editor is a big one reply makeitdouble 8 hours agoparentprevThere are semi-legitimate cases where this is warranted. For instance when looking at a Notion database, standard Ctrl-F is almost useless, and document search needs to go through the notion API to return results, sometimes even related to the entries that are displayed on screen. I say \"semi-legitimate\" because I actually wish they'd map to a different shortcut, but can see the case for user wanted the remapping. This of course stems from earlier decisions to have that document handling style in the first place. IMHO it becomes a complex debate when on line between an online application and a webpage. reply AA-BA-94-2A-56 8 hours agorootparentShouldn’t CMD+F be reserved to searching the current document/context? Something like CMD+K should be used for a more global search. reply easton 7 hours agorootparentYou cant use those keys (Super+) in the browser AFAIK. The operating system expects to use them for keyboard shortcuts. (Guessing on macOS Cmd+V is actually triggering a clipboard event in JS, the site can’t actually see that you pressed Cmd+V) reply strbean 8 hours agoparentprevRecently learned that if you Ctrl-F again after the highjacking, it brings up the browser search box. Discovered this thanks to a site (don't remember which) that included a tooltip about this fact in their hijacked search box. I was curious if it would work on Redocly search, which has no such tooltip, and it did. I'm not positive if this works universally, or is just an undocumented feature of Redocly's interface and won't work in places the developers didn't make specific accommodations for it. Env: Chrome + OSX or Windows. reply AA-BA-94-2A-56 8 hours agoparentprevStripe’s API documentation does this and it gives me the shits, because it seizes up my M2 MacBook Pro for several seconds. I can’t believe that it’s 2024, and I can’t simply grep some documentation. reply dotancohen 3 hours agorootparentThe Vimperator/Tridactyl (Firefox VI shortcuts extension) search / is not hijacked on the Stripe API documention. reply bovine3dom 46 minutes agorootparentFWIW, the / search isn't part of Tridactyl but we do inject some code that frees up / from most websites so Firefox can use it. It's possible to write your own user script to do it (you just need to add a keypress event handler that does preventDefault() and maybe stopPropagation()) with no need for Tridactyl :) reply tom_ 8 hours agoparentprevAnd overriding Ctrl+K without even being so good as to give way when you type it a second time. Assholes. reply oneeyedpigeon 1 hour agorootparentWe're talking about you, Slack. (At least I can now remember which app is the one that breaks Cmd-K, but it's still annoying that I have to think that little bit longer to recall that info. every single time I press Cmd-K anywhere) reply oneeyedpigeon 1 hour agoparentprev'Find in page' is now so broken on modern websites that the keyboard shortcut is the last of our problems. reply PennRobotics 54 minutes agorootparentBitwarden. \"Find in page\" will only show a result if it is visible on the page (even though the scrollbar indicates the full vault has loaded, and even after scrolling down to the desired result and then back up). They have a \"Search vault\" field that works fine, so it's not a major inconvenience, but the first few times I've Ctrl+F'd a newly added site and gotten \"Phrase not found\" when I know I added credentials? That's a mild anxiety I'd rather not have. I could be convinced there's a security-related reason for this---in fact, I never really thought about it until now---but then I'd assume anyone able to get access to your vault can use Selenium and fill in the \"Search vault\" input field. reply dugite-code 8 hours agoparentprevHell just hijacking any standard browser controls is infuriating when it catches you out when you're just not paying complete attention. Edit: Apparently Firefox has the `permissions.default.shortcuts` config option UNKNOWN: Services.perms.UNKNOWN_ACTION [0] ALLOW: Services.perms.ALLOW_ACTION [1] BLOCK: Services.perms.DENY_ACTION [2] PROMPT: Services.perms.PROMPT_ACTION [3] And in the site information panel you can disable the Override keyboard shortcuts permission on a per-site basis. Neat, doesn't solve the paste override issue though. Source: https://support.mozilla.org/en-US/questions/1241294#answer-1... reply crtasm 7 hours agorootparentA huge thanks for making me aware of this. permissions.default.shortcuts firmly set to 2. reply lukeholder 8 hours agoparentprevStripe docs do that and it annoys me to no end. They let you use the native search if you press ctrl+f a second time but since there is a delay it causes chaos. reply int_19h 7 hours agoparentprevThis is the biggest reason why I hate Discourse. reply notamy 8 hours agoparentprevCtrl-G and F3 often work to bypass that ime reply dugite-code 8 hours agorootparentI honestly didn't know about Ctrl-G. You my have significantly changed my life! reply dylan604 8 hours agorootparentisn't ctrl-g common for \"find next\" with shift-ctrl-g \"find previous\"? maybe i live too much in my IDE/text editors? reply dugite-code 8 hours agorootparentAlmost certainly, but It's one of of these \"I can't believe I never knew this, it's so obvious!\" things. reply dylan604 8 hours agorootparentIn that case, I'd highly recommend browsing through the drop down menus for any of your apps. It is very common* for the keyboard shortcut to be listed, and very frustrating when it is not. This is my primary source for finding these shortcuts for a new app. * maybe it's a Mac thing??? reply kayodelycaon 8 hours agorootparentprevOn macOS cmd+g is a standard shortcut most apps implement. reply dylan604 8 hours agorootparentyeah, i substituted to ctrl from my normal cmd to not confuse people reply Andrex 6 hours agorootparentprevIn my head Ctrl+G is a shortcut for \"Ctrl+F with the last result I searched for.\" reply gardnr 8 hours agoparentprevJeff Atwood has an opinion: https://meta.discourse.org/t/options-to-disable-hijack-of-cm... reply freediver 8 hours agoprevThis was one of those things that frustrated me so much that we ended building this natively into Orion browser (Tools menu -> Allow Copy & Paste). [1] One of the joys of building your own browser. [1] https://kagi.com/orion reply idonotknowwhy 7 hours agoparentCheers for making Orion. I don't know how you guys managed to support Firefox and Chrome extensions (on iOS) but it's amazing and made moving from Android so much easier! reply freediver 7 hours agorootparentJust the sheer determination to build the best browser in the world :) reply igetspam 8 hours agoparentprevBrave has a \"force paste\" that I use now instead of Chrome and the linked plugin. I assume the motivation was the same. (What a*hole thinks blocking paste is reasonable??) Good on you for solving this too. It's a nonsense bit of functionality. reply serial_dev 8 hours agorootparentIt's always incapable product owners and business people who don't understand security but think they do. reply quadhome 7 hours agoparentprevWhy allow pages to disable copy & paste at all? reply musicale 6 hours agorootparentIt's kind of a misfeature, but the non-evil idea was probably to provide hooks for customizing copy and paste (or other standard command functionality) in beneficial ways, for example seamlessly copying and pasting custom data formats between web apps, or between web and desktop apps. It is a law of the web that any potentially beneficial browser feature will immediately be (mis)used in an abusive, user-hostile manner. reply Spivak 6 hours agorootparentprevIt's not about disabling it, it's about intercepting it by telling the browser that you're directly handling paste events and then doing nothing. The extensions just forces the browser default handler. reply panja 4 hours agoparentprevSadly, I am not in that ecosystem :( reply cute_boi 7 hours agoparentprevThe problem with orion browser is it is not opensource. reply ghostpepper 8 hours agoparentprevHow would you rate the security posture of Orion compared to Chrome? reply torstenvl 7 hours agorootparentWell, there are apparently whole classes of JavaScript malware that Orion blocks but Google doesn't... reply ghostpepper 5 hours agorootparentThis is exactly what I was asking, not sure why my post was downvoted reply freediver 8 hours agorootparentprevAlong what axis? reply ghostpepper 7 hours agorootparentSize of security team? Mean time to patch actively exploited CVEs? Availability of source? Etc reply freediver 7 hours agorootparentSame as Safari in that regard, albeit with a much smaller team (we inherit upstream patches from the WebKit team and publish them regularly, sometimes even before Safari like in the case of patching iLeakage vulnerability). reply morder 8 hours agoprevThis[1] alternative bookmarklet was posted here a while back. [1]: https://bookmarkl.ink/ashtonmeuser/6e3869d8e468e016f22a4b4de... reply dugite-code 8 hours agoparentBookmarklets are seriously undervalued. This is a simple and more importantly readable fix for the issue. reply al_borland 7 hours agorootparentI really wish bookmarklets caught on more. They can provide a lot of the value of extensions, without running all the time and bogging down the browser (or tracking the user around the web). The lack of persistent tracking is probably what led companies like Amazon to abandon them. reply evgpbfhnr 6 hours agorootparentprevI wish firefox would let the wonderbar '*' search feature work with bookmarklets... As it stands I have a few I'll never use because they're 4+ clicks away with no typeable shortcut. reply ringer 1 hour agorootparentYou can define a keyword for this, eg. ctrl+l - `ks` (kill sticky) - enter. It has some backwards because you still can't search by name and you have to remember the keyword and there is no auto-complete, but once muscle memory gets used to it, it works pretty well. I use keywords for bang searches (!keyword search term) and bookmarklets too. - https://support.mozilla.org/en-US/kb/bookmarks-firefox#w_how... reply reticulan 8 hours agorootparentprevi don't think they're undervalued compared to userscripts (with a dedicated extension for managing them). reply NewJazz 7 hours agorootparentWonder if home-manager does user scripts. reply MaxikCZ 1 hour agoprevIs there a way I can tell my browser to always force draging over text to secelt that text, regardless if its a hyperlink? reply flyflyFenix 1 hour agoparentHold alt and then select text with left-click drag. It works most of the time. Enjoy :) reply out-of-ideas 1 hour agorootparentkey words: most of the time like when you get trolled by Atlassian's crap tier website where the wiki/jira-pages do not allow selecting some text blocks (and makes it even harder to tell somebody to browser-page-search for a particular string) reply visarga 28 minutes agoprevMy peeve with copy&paste is that it often fails to copy, not to paste. I sometimes need to copy three times until I succeed a paste. Does anyone see this problem? reply lolinder 8 hours agoprevFor something simple like this that doesn't really need to be on all the time I've started leaning back towards bookmarklets over extensions. The code is usually simple enough to actually audit, it only runs when you click the bookmarklet, and it doesn't update underneath you without warning. A few months back someone shared several bookmarklets that they use, one of which was a simple one that disables all clipboard events on the open tab: https://news.ycombinator.com/item?id=38014653 reply eviks 7 hours agoparentBut then you need to click Also you can get the extension loaded locally, and it will never update reply lolinder 4 hours agorootparentI run into one of these broken-clipboard situations once every few months, I can afford to spend an extra click in order to not have an extension active on every website I ever visit. reply eviks 3 hours agorootparentIt \"ensure the extension is only running on sites that are bad actors with copy & paste events a\", so what exactly can you not afford? reply quitit 54 minutes agoprevFor mac/iOS users on Safari, Firefox or Chrome: StopTheMadness is also a handy utility that tames these and other website bad behaviours. https://underpassapp.com/StopTheMadness/ reply js2 8 hours agoprevFor Safari you can get Don't Fuck with Paste (and lots of other Don't Fuck with X options) with the Stop The Madness extension. https://underpassapp.com/StopTheMadness/ reply lancesells 8 hours agoparentI was going to mention this but glad I found it. Great extension for Safari. One of the greatest things about this is you can change the options per website. So if you use something like Notion you can use their shortcuts just for their website and the default browser ones for everything else. reply jiveturkey 3 hours agoparentprevty! I use both safari and chrome (and dabble in FF), and in my safari usage I haven't actually come across sites that disable paste. I kind of assumed this was built-in to safari, but I may just be getting lucky. But the best feature of StopTheMadness is that it defeats the google link tracking! I've so missed that feature of whatever chrome extension I was using. reply afandian 2 hours agoprevHas anyone else noticed that copy / paste on Mac OS / Firefox has become unreliable some time in the last 5 years? I don’t know if it’s Firefox or Mac OS but I’ve started finding it copied the wrong thing or ignored a copy command. reply yungporko 2 hours agoparentpretty sure it's macOS. i don't use firefox but id say roughly 4/10 times i try to paste something i just copied, the clipboard is empty and i have to do the whole thing again. reply bluish29 1 hour agoprevFor me this is annoying and I really hate websites hijacking paste. But it is more annoying on the phone specially when creating passwords with password manager. Does anyone know something that will work for safari? reply blkhawk 37 minutes agoprevThis is why I build an external password thing that emulates a keyboard. Nothing some stupid braindead \"security\" implementation can do to prevent that from working. It does TOTP too. I had fun with hooking it up to my phone via BT for proximity unlocking. Part of its secret lives on the phone. I wouldn't say it very secure since i didn't bother with implement a proper asnc encryption for the bt part. And its still vulnerable to a variety of evil maid attacks. My threat model doesn't involve that to ever matter. It should be practically fully resistant to remote full-take attacks. reply pjerem 2 hours agoprevFucking with past is gruesome. Breaking back button is infuriating. Yesterday I stumbled upon something even worse : breaking CTRL+F to replace it by a custom search. I know Google Docs / Drive does this but I tolerate it because classical ctrl+f doesn’t even make sense on their apps but here it was on a forum. I knew what a searched was on the displayed page but no, they forced me to search on the entire hundreds pages of the topic. [/rant] Sorry. reply iansinnott 8 hours agoprev100% in agreement with regaining paste. Another workaround is to create a macro that will \"type\" the clipboard contents, simulating typing it out by hand. On macOS you can do this via Keyboard Maestro [1]. Create a macro with the action \"Insert text by typing\" and for the text to insert use `%PastClipboard%0%`. Yes, very niche, but I'm sure some HN users already use Keyboard Maestro. [1]: https://www.keyboardmaestro.com/main/ reply BeefWellington 7 hours agoprevFirefox has some about:config settings for this: * dom.event.clipboardevents.enabled * dom.allow_cut_copy And also, the ability to force a paste as plaintext or force right-clicks to behave properly by using shift. reply Fnoord 1 hour agoprevHmm, if it JS, don't allow JS by default? uMatrix for example works in that regard. reply gnyman 3 hours agoprevAnother app/extension which fixes this and a lot of other things the aptly named StopTheMadness https://underpassapp.com/StopTheMadness/ (No relation other than being a happy user) reply 2d8a875f-39a2-4 3 hours agoprevShould be more like \"don't fuck with anything\". reply seiferteric 8 hours agoprevAlso annoying, not labeling your inputs right so auto-fill doesn't work. reply mr-ron 6 hours agoprevAny time im in this situation i just go to the Menu > Edit > Paste. Usually any overrides are preventing CTRL > V reply Am4TIfIsER0ppos 9 minutes agoparentA menu? We have no need for such outdated shit in CURRENT YEAR. - Sincerely every browser maker since 2010 reply reilly3000 3 hours agoprevBlame ACH. It’s a putrid protocol that is far easier to get wrong than right. Once that daily file is pushed it’s a dozen people’s time to hope to fix a failed transaction, and there is no guarantee that the money will stay where it wasn’t meant to go. It’s one of those things the leaves you feeling “this needs to be better” yet it moves more money than almost any other means. All the upfront friction on users is to get it right should be some indication of what a catastrophe it is to fix. reply cubancigar11 3 hours agoparentUpi fixes it, but has cultural resistance in the west. reply 0xfaded 6 hours agoprevOmg I was being interviewed on coderpad today and something was eating my ctrl-c. So frustrating reply eviks 7 hours agoprevAt a more general level you can use a keyboard remapping app like Keyboard Maestro on a Mac and Autohotkey on Windows to insert text by simulated typing, though it has the downside of requiring a different shortcut reply phyzome 7 hours agoprevA couple alternative ways to paste, at least in my current Linux environment: - Paste the text somewhere else, then drag it onto the text field - Highlight the text elsewhere, and middle-click on the text field I've only ever found one site that blocks both of those too. reply kylecordes 8 hours agoprevI use this similar but even older tool, which still works. This one is Mac-specific but works across most/all apps, i.e. not a browser plugin. https://github.com/EugeneDae/Force-Paste reply cyber-nic 3 hours agoprevExtension for this might be a little overkill. Copy whatever you want to \"paste\" in the URL bar. Then select that and drag it into the input that has paste prevention. reply nico 8 hours agoprevLove this Copy and pasting is such an essential part of everyday computer usage Also can relate a bit as a developer, recently been struggling trying to get scrolling paste capture on a remote terminal with ncurses (wide lines, long texts, utf8 characters, can all be tricky) reply drey08 7 hours agoprevIn a similar vain some of the AWS console pages make it hard to select text and use ctrl^c to copy it. It often deselects the text as soon as you hit ctrl. It is extremely irritating. reply assimpleaspossi 2 hours agoprevDo we really have to put up with vulgarity even on HN? It's bad enough to see it in the title--cause someone will say that's what the title is--but, as can be seen, it breeds unnecessary vulgarity throughout this thread. To see the irony of it all, I'll get fucking downvoted for fucking complaining about this fucking shit and maybe get really fucked up by getting fucking banned. Do you get my fucking point? reply mtlmtlmtlmtl 44 minutes agoparentI don't understand people who give a shit about vulgarity. And more importantly, people that expect other people to adjust to them. They're just fucking words. Lots of little things about the way some people write or speak may annoy me, like their dialect or some word/phrase they use too much, but the adult thing to do is to fucking deal with it and keep it to yourself. It's your problem, not everyone else's. reply jjgreen 25 minutes agoparentprevYou kiss your mother with that mouth? reply geraldhh 1 hour agoparentprevvulgarity in the tile makes sense, in your comment not so much. if you don't get the point you may gtfo or make new account reply a1o 9 hours agoprevLast commit is 4yr ago but I remember using this in the past reply SubiculumCode 8 hours agoprevThere is one thing I hate more. Its having my cursor moved to another application as I am typing out a password in a field. I've literally had to change a password after it suddenly got sent out in a google search when I wasn't paying attention. reply eviks 6 hours agoparentStealing focus should be a misdemeanor! unfortunately it's a pervasive UI flaw reply megous 9 hours agoprevUsed to be simple to workaround this with basic web APIs (el.value = 'whateverIwant'), until this newfangled web framework from Facebook broke the web. reply alisonatwork 8 hours agoparentThis. One especially infuriating trend appears to be not even using the HTML input box at all, the JavaScript just intercepts the keydown event which then (deliberately?) triggers a slow calculation before eventually rendering a star in the text-field-that-actually-isn't, breaking even \"auto type\" solutions. So you sit there like an idiot every time you log in, with your password manager open and password unmasked, copying each character, laboriously, one-by-one. I can't fathom how this sort of thing gets by management, who are presumably told it increases security, but then apparently never actually try to use the end product. Even worse is when the suggestion comes to use an app instead of the website, and then the app comes with its own dedicated keyboard that isn't your own phone's keyboard, because \"security\" and then they scramble the position of all the letters and numbers, making it even slower. reply benmanns 8 hours agorootparentAhh 00s/10s era ING Direct and TreasuryDirect. I think TreasuryDirect finally dropped the virtual keyboard in the last couple years. reply swayvil 9 hours agoprevYa, fucking with paste is a pet peeve of mine. What's a good argument for fucking with paste? reply freedomben 8 hours agoparentThe only one I've ever heard is \"security.\" Sometimes people try to \"yeah it's because for security we don't want some tool to paste credentials or clickjacking mumble something malware hacking clipboard spyware javascript browser\" but most of the time the truth is \"our sec people need to show controls for hardening the authentication process so they came up with that and we just did it because it wasn't worth the fight\" I've also heard \"being able to copy/paste defeats the purpose of having a 'confirm email' field.\" I reject that but it's at least somewhat logical. reply ryandrake 8 hours agorootparentToo much gets justified with vague, handwavey \"because security\" excuses. We can't do this because... uh... because security, yea, that's it! It should be \"Specific vulnerability or GTFO.\" reply styxfrix 7 hours agorootparentYeah. NordVPN is an example of this handwaviness. They refuse to delete my account to which I can successfully log into and which I've never used because they claim that as \"a security-oriented company we do not take account change or deletion requests lightly, hence we request payment information to confirm ownership of the account, as we would not want anyone but the rightful owner making such changes.\" Problem being, I've never paid NordVPN, and when I told them as much and asked what security problem could result from them deleting an unused account, they didn't respond. reply zettabomb 8 hours agorootparentprevI consider it a reduction in security because it makes entering proper random passwords more difficult - either I have to paste it somewhere else first (and leave it in plaintext on my screen) OR I have to use a more memorable/shorter password. When I can paste passwords, they can be as long as possible and never are actually visible in any way. reply freedomben 8 hours agorootparentCompletely agree. There was a particular US government-run website that I had to use that disabled pasting, and required obscenely long passwords (like 15 character minimum, at least two letters/numbers/symbols/capitals/etc), and forced rotation every 60 days, and aggressively blocked \"keyboard patterns\" and once a password had been used, it couldn't be used again forever. Given I only had to log in about once every 90 days, I literally had to change my password every time. I've never been more enraged at a product in my life. My passwords for that site may have looked good out of context, but in reality I just figured out \"keyboard patterns\" that it wouldn't detect and used those, and kept the password in plaintext where I could read and type it. It was the biggest security anti-pattern that I could possibly think of. reply Analemma_ 8 hours agorootparentThey finally fixed it after years of griping (I assume because the skyrocketing interest rates meant a large number of people suddenly began using it), but for a long time the official website for buying US government bonds wouldn't let you use your keyboard at all to enter your password, you had to click on an on-screen keyboard Java applet. For \"security\". Fortunately most password manager tools could break through it and paste into the password field anyway, but what a fuckup that was. (And the site still sucks- you can't use the back button at all, for example-- but it sucks infinitesimally less now) reply MBCook 8 hours agorootparentprevThat’s exactly what I’ve heard. In a discussion about this recently at my job they wanted to add this to some login related forms. I pointed out this was obnoxious and I’ve heard in the past copy & paste can be important for accessibility though I can’t be sure first hand. Security BS was the reason I was given. The irony is our IT department tells us to use a specific password manager and copy & paste usernames and complex passwords because the app doesn’t have browser integration. I assume “for security”. reply bogota 8 hours agorootparentprevA classic “save the children” argument of security. I know you aren’t making it but i have heard people argue this too many times over the years. reply chrisfosterelli 8 hours agoparentprevWhen you have a confirmation field, like signups that require you to type your email and then verify your email, blocking paste stops the user from typing a typo and then copy-pasting the typo into the confirmation field. Another use case is when you want the user to type in the name of the project or resource before they delete it, sites sometimes block copy and pasting the name to avoid having users get into that habit and make a mistake. Personally I dislike the UX, I think it's too user hostile to 99% of users at the benefit of 1% that are making a mistake that's obviously their own fault, but it does cut down on support time I assume. reply robenkleene 8 hours agorootparent> When you have a confirmation field, like signups that require you to type your email and then verify your email, blocking paste stops the user from typing a typo and then copy-pasting the typo into the confirmation field. This argument is indefensible. Browsers have had autocomplete for over a decade, Contact book applications that store email addresses have existed over a decade, password managers that also handle email addresses over a decade. reply chrisfosterelli 8 hours agorootparentTo be clear, I'm not saying I agree with it. But this isn't a feature for users that have password managers. It's a feature for the staff that handle dozens of calls per week from users complaining they never got the signup email when they had given the wrong email. There's better solutions to that problem though, and like I said I'd be the first to advocate for one. But I can see how it's an easy tool to reach for. reply ssnri 8 hours agorootparentprevThat just takes the argument from bad to worse. “How will I condescend to my users without breaking their browser functionality?” Zero times have I been saved by this feature reply RheingoldRiver 8 hours agoparentprevOn the github \"delete repository\" screen or other extremely destructive actions, it stops you from muscle-memorying the confirmation and then accidentally deleting the wrong thing. Since you are actually forced, like really forced, to type out what you expect to happen, you won't delete my-important-work-repo when you meant to delete my-temp-test-repo. reply eviks 6 hours agorootparentYou're forced to copy the name manually, which is just mindless typing, so doesn't force thinking, for that you'd need to provide some meaningful info about the repo to make it click that it's not a temp, or better yet make this actions undoable for some time reply wrs 8 hours agoparentprevMostly it seems to be a countermeasure against mistyping your email/password and then copy/pasting that bad value into the “confirm email/password” box. Nowadays it’s a (hopefully accidental) countermeasure against password managers! reply mattmaroon 8 hours agoprevBeen using this for awhile. It works. reply est 2 hours agoprevI think browsers need to implement an optionalfeature. Stop js shit after the page loads complete. Only enable js when user interacts with a button or something. reply DinaCoder99 8 hours agoprevI've found you can also just disable javascript, paste, and reenable javascript again. reply Solvency 8 hours agoprevHow about an extension that STOPS websites from NOT letting me right-click. When a site forceS me to go through the stupid Chrome menu to laboriously drill down and find Developer Tools manually I want to throw it out the window. reply kayodelycaon 8 hours agoparentThere’s a stackoverflow question with some bookmarklets. https://stackoverflow.com/questions/21335136/how-to-re-enabl... reply moralestapia 8 hours agoprevThanks for this! Also related, Who t.f. thought that * Ctrl-V should be \"paste with format\" * Meta-Shift-Whatever-Ctrl-V should be \"paste without format\" I've never EVER had the need to copy some text and paste it with a different font face, color and ffs background color ... Who is the genius UX expert that decided on this abhorrent behavior? reply maple3142 2 hours agoparentPaste with format is actually usefull sometimes. For example, you can copy from vscode and paste into Microsoft Word, then the resulting code will be property formatted and highlighted. reply fiddlerwoaroof 8 hours agoparentprevI don’t think it’s as simple as this: it’s annoying in the context of text editing, but when you’re dealing with spreadsheets or file managers or copying from an spreadsheet into a document or an image into a document, the rich object behavior makes a lot of sense. I vaguely remember that really old versions of windows had a “live paste” feature where if you copied a range from a spreadsheet and pasted into a word document, the pasted object would update when you changed the spreadsheet reply kayodelycaon 8 hours agoparentprevIt’s beyond aggravating. I use a clipboard manager (PasteBot on macOS) set to paste plain text by default. I can pop open the history and do command+enter to paste the rich text version. reply Solvency 8 hours agoparentprevI guarantee it's the guy who invented rich text fields and he just big dicked the fledgling UX team and because Big Development is incredibly paternal these crappy UX patterns never go away. reply datavirtue 8 hours agoprevI was paying my real estate taxes recently and the payment processor chosen by the county wouldn't let me paste anything. It was fucking bananas. I had to type in bank account numbers and routing numbers. None of the form filling worked. I was paranoid the whole time about entering the wrong data, and I wanted to punch someone in the throat for that bullshit. reply theflyingelvis 9 hours agoprev [–] Seems like a cool idea but perhaps a slightly less offensive name would be good. reply zettabomb 8 hours agoparentIMO the very act of disabling paste is far more offensive but I get it. Not my project to change unfortunately. reply jfoutz 8 hours agoparentprev [–] seems like an unmaintained fork of the less provocative, https://github.com/jswanner/DontF-WithPaste reply theflyingelvis 8 hours agorootparent [–] I mean it doesn’t really offend me but it would be difficult to say recommend it to my mom. I guess it’s just me. reply kstrauser 8 hours agorootparentIt’s not just you. You summarized my own feelings quite nicely: it doesn’t bother me a bit, but I wouldn’t recommend it outside informal circles. reply freedomben 8 hours agorootparentprevAgreed. It doesn't offend me personally, but it's considered NSFW language in most workplaces. I would never name a project like that reply arp242 7 hours agorootparent> NSFW language in most workplaces It would have been fine literally everywhere I've worked. I'm not saying there aren't places where this can be an issue. Partly this seems to depend on the region – in the US, in particular, it seems to be a big no-no, which I find odd considering how fucking often fuck is in fucking American media – the fuck is up with that? But outside of that? It seems to be mostly a non-issue. reply freedomben 4 hours agorootparentProbably because the US does have a pretty wide diversity, particularly when it comes to religion. In some areas you'll find a lot of Mormons (who are greatly offended by \"fuck\"), and in some Christians (where some denonimations don't even like the word \"crap\" let alone \"fuck\"), in some Catholics who generally don't seem to care (I've heard some Catholic priests let loose some pretty blue streaks lol), and all kinds of others. Then there are people who aren't offended, but don't want to talk that way in front of their kids and don't allow their kids to talk that way. In a workplace in the US (where you can't discriminate based on religion as it's a protected class), it's a very bad idea to use that kind of language unless you know everyone around and you know that they don't mind. Even then, be aware of who might be nearby enough to hear. Getting an HR complaint against you for dropping an f-bomb is embarrasing and not very fun. reply wtallis 8 hours agorootparentprev [–] If I'm in a situation where I need to recommend a browser extension to my parents to workaround UI dark patterns, I think we'd all be reassured by an extension name that makes it clear the author has the right attitude toward those dark patterns. Let's not pretend there's anything polite about user-hostile UIs. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The text addresses the frustration of encountering copy and paste restrictions in web applications.",
      "A Google Chrome extension has been introduced to bypass copy and paste blocking, with a recent version 2 update offering enhanced control over the supported sites and increased transparency regarding data handling.",
      "This release aims to empower users with more options to overcome copy and paste limitations on various websites efficiently."
    ],
    "commentSummary": [
      "GitHub discussions delve into password security, user experience, and user input constraints, encompassing topics like application security, Chrome extension permissions, browser search, copy/paste hurdles, keyboard shortcuts, and frustrations with website security features.",
      "Users explore leveraging browser tools and extensions to enhance usability and bypass security controls.",
      "The conversation also touches on the influence of language and cultural diversity on online interactions."
    ],
    "points": 449,
    "commentCount": 221,
    "retryCount": 0,
    "time": 1709858967
  },
  {
    "id": 39637487,
    "title": "Exploring Exponential Smoothing for Smooth UI Animations",
    "originLink": "https://lisyarus.github.io/blog/programming/2023/02/21/exponential-smoothing.html",
    "originBody": "My favourite animation trick: exponential smoothing Feb 21, 2023 There’s a certain simple animation thing that I’ve been using almost since I’ve ever started doing anything related to graphics. I use it for rotating & moving the camera, for moving figures in a turn-based game, for moving UI elements, for smoothing volume changes in my audio lib, everywhere! So I decided I’ll write about it. The trick itself is nothing new, - in fact, you’ve probably already heard about or even used it, - but I’ll also show it in some examples and explain how it works mathematically :) Toggle buttons Speaking of UI, say you’re making some UI component, maybe a toggle button. Something like this (click it!): This simply computes the position of the switch as a function of its state: position.x = turned_on ? max_x : min_x; This works perfectly, but feels a bit lifeless. Adding some animation to it would be cool! Animations are not just a fancy visual thing, they help the user understand what’s going on. Instead of teleporting the toggle indicator to its new position, let’s move it smoothly: The downside is that we need to run some updating animation now: position.x += (turned_on ? 1 : -1) * speed * dt; position.x = clamp(position.x, min_x, max_x); However, this still looks a bit clumsy due to having a constant speed (i.e. the position is a linear function of time). Let’s add some easing function on top of that, like the classic cubic 3t^2-2t^3: or a square root sqrt(t): The difference between these may be hard to see, so let’s slow down the animation by a factor of 8: Linear: Cubic: Square root: This time, instead of just updating the switch position, we have to keep track of some extra animation state: t += (turned_on ? 1 : -1) * speed * dt; t = clamp(t, 0, 1); ease = (3 * t * t - 2 * t * t * t); position.x = lerp(min_x, max_x, ease); Here, I’m using the fact that smoothstep is symmetric in the following sense: 1 - f(t) = f(1 - t), meaning the forward and backward animations can use the same code. With sqrt things are a bit different: we have to explicitly use a different easing function depending on the animation’s direction: ease = turned_on ? sqrt(t) : 1 - sqrt(1 - t); Whichever looks best is arguably a matter of taste, but of all these sqrt is my favourite: the switch starts moving really fast (this is because sqrt has infinite derivative at zero), but then slows down nicely as it reaches the destination (the cubic one is my second favourite, though). The downside of this version is that we need quite a lot of bookkeeping even in the simplest possible case of a two-state toggle button (later in the article I’ll show how this becomes a nightmare in more complicated scenarios). Another downside is that it has a discontinuity: it jumps suddenly if the user clicks on it the middle of animation (try it!). Thankfully there’s a similar version which uses the minimal possible state and doesn’t have the “jumping” problem: I call it exponential smoothing (for reasons that will become clear later). I’ve also heard it being called approach, and I’m certain it has it’s own name in every engine. Here it is slowed down 8x and compared to sqrt: Square root: Exponential: Here’s the code for the exponential version: target = (state.value ? max_x : min_x); position.x += (target - position.x) * (1 - exp(- dt * speed)); Intuitively, on each frame we nudge the current position towards its target position (which is determind by the on/off state). However, the amount of nudging (1 - exp(- dt * speed)) looks really weird, doesn’t it? Before we see where it comes from, let’s have a look at some more complicated animations. Camera movement Say we have some kind of map, and a camera scrolling/moving around. Yes I’ve made a whole procedural map generator & renderer just for this example, and I have zero regrets. Again, this begs us to add some animation. Let’s interpolate it with constant speed: Here’s the code: position.x += sign(target.x - position.x) * speed * dt; position.y += sign(target.y - position.y) * speed * dt; See this jittering after the animation completes? That’s because target.x - position.x keeps alternating between being positive and negative. Instead of sign(delta) we need some function that clamps the delta: float update(float & value, float target, float max_delta) { float delta = target - value; delta = min(delta, max_delta); delta = max(delta, -max_delta); value += delta; } update(position.x, target.x, speed * dt); update(position.y, target.y, speed * dt); Quite a mouthful for such a simple thing! And here’s the result: Much better, although it’s still a bit clumsy and also weird if we move the camera faster than the animation completes. We could, as before, add some easing function, like the cubic one: although this time it gets really complicated: we have to maintain a queue of requested movement events, and animate them one by one (otherwise I have no idea how to slap the easing function here). This still looks a bit weird when moving the camera fast enough. We could just ignore user’s input while the animation is active, but this is a deadly sin as it is infuriatingly frustrating from the user’s perspective. The perfect solution? Why, exponential smoothing of course! The code barely changes compared to the toggle button example: position.x += (target.x - position.x) * (1.0 - exp(- speed * dt)); position.y += (target.y - position.y) * (1.0 - exp(- speed * dt)); and here’s how it looks like: Pretty nice, if you ask me! Notice how it speeds up naturally if you click fast enough. Under the hood Ok, so what’s up with this 1 - exp(- speed * dt), what on Earth is that? Let’s start with a simplified version: we have some animation, it has a current position and the new position target which it must move towards with some speed. To make the movement faster when the difference between position and target is large, we make the speed proportional to this difference: position += (target - position) * speed * dt; Notice how it doesn’t require maintaining any state other than the current and the target position! (speed is usually a constant.) It even doesn’t need to keep track of time that elapsed since the start of the animation, and it adjusts automatically if the target suddenly changes. Now this already works perfectly in many situations, but there’s a small catch. Here’s the toggle button again, with the above udpate code: See the jittering? That’s because I’ve set the speed value so high that speed * dt became larger than 1! Specifically, I used speed = 220 and dt = 1 / 125. To understand what’s happening, it is useful to rewrite the code above using lerp: position = lerp(position, target, speed * dt); You can check that this is ultimately the same formula. We can clearly see what’s going on: the formula interpolates between the current value and the target value. The closer the interpolation parameter speed * dt to zero, the slower the interpolation. The closer it is to one, the faster the movement. Now, what happens when speed * dt is larger than 1 is that the interpolation overshoots! The only reason it still works is that speed * dt is less than 2, so that the absolute delta between position and target still decreases with time. Here’s an example with speed * dt = 248 / 1252: The last one doesn’t do anything useful at all. To solve this, we could simply clamp the value by 1: position = lerp(position, target, min(1, speed * dt)); However, this doesn’t seem like the right thing to do in all scenarios. Consider why speed * dt might actually happen to be so large? One reason is that your speed value is too large because you want a really quick animation. However, as we’ve seen with the above toggle buttons, this is actually way too quick for any reasonable user – the actual animation is impossible to notice. So, out speed value is usually not that high. The other reason is that dt is too large. Maybe because your code runs too slow, and your framerate is dropping. Maybe because the user moved to a different tab/window and your code was sleeping, and now it got woken up with a dt of many seconds. When applying such a dt to something like physics, you certainly want to clamp it, or subdivide into several updates, etc. With animations, however, wouldn’t it be cool if everything worked perfectly even in this case? Even if your physics might lag, at least the camera & buttons would still work nicely – as a user, I would really appreciate such care. Differential equations (oh no) Ok, we want to solve the problem, but how? Here’s the two-step recipe: Realize that what we’re doing is numerically solving a certain differential equation Solve the equation symbolically and use the result directly Time-dependent update that works for small dt but breaks for large dt is pretty typical for numerical solvers of differential equations. What equation does position += (target - position) * speed * dt solve? Whenever you see A += B * dt, this corresponds to an equation In our case, the equation is I will die if I keep typing these formulas with all words spelled out, so let’s make a few variable changes: call x = position, a = target, and c = speed: Solving this needs just a few tricks: Btw, a similar exponent appears in e.g. volumetric rendering for pretty similar reasons. It’s not important to understand exactly where this all comes from. The point is that if we believe that position += (target - position) * speed * dt is the right formula for small dt, then the formula position += (target - position) * (1 - exp(- speed * dt)) is the right formula to use for any dt. This is further supported by expanding the latter equation in terms of Taylor series for the exponent: exp(x) ~ 1 + x, so that 1 - exp(- speed * dt) ~ 1 - (1 - speed * dt) = speed * dt, i.e. we get exactly the former equation. The cool thing is that it doesn’t care about old values: if you have your previous value and you know how much time has passed between the previous and the current iteration, you can compute the new value. (This is a direct consequence of being a first-order differential equation.) So, the TL;DR is that position += (target - position) * (1 - exp(- speed * dt)) is the right formula that works for any speed and dt. Even if the product speed * dt is too large, exp(- speed * dt) handles it nicely, since exp of a large negative number is just something close to zero, so 1 - exp will be close to one. We can, as before, rewrite this using lerp: position = lerp(position, target, 1 - exp(- speed * dt)) or even position = lerp(target, position, exp(- speed * dt)). There are many ways to rewrite this equtaion. Choosing the speed Usually, we think of animation in terms of its duration. Like, the toggle button should move to the new place in 0.125 seconds (the actual value used in the examples in the beginning of the post), after that it stops moving. With this exponential formula, however, the animation technically takes infinite time to complete! exp(- speed * time) gets smaller with time, but it never equals zero, so that position technically never equals target (provided they were different to start with). However, in practice we have a ton of limitations. If position is floating-point, it quickly reaches the precision limit, and it becomes equal to target in practice. If it is, say, the camera position, the user probably won’t notice that the animation is still going since the delta target - position gets ridiculously small even before it hits floating-point precision limits. So, what does the speed parameter mean, exactly? It means the following: 1 / speed is the time in which position becomes closer to target by a factor of e = 2.71828... exactly. Do whatever you want with this information. I usually set the speed to something in the range 5..50. For a linear/cubic animation of a certain speed, I usually set the exponential version speed to be 2 * speed, this feels about right (again, this is what was used in the examples above). Exponential smoothing If you google “exponential smoothing” (or “exponential moving average”), you might find the wiki article on something completely unrelated which, nevertheless, features some pretty similar formulas. It actually is the discrete analogue of what we were talking about in this post! Suppose that our dt is always the same; also suppose that target changes as often as every iteration. Then, indexing the values with the iteration number, we compute something like position[i] = (target[i] - position[i - 1]) * factor, where factor = 1 - exp(- speed * dt). In this case, one typically sets factor directly to some value between 0 and 1 instead of deriving it from other values (although the aforementioned wiki article does explain what this factor actually means). People use it in signal processing for the same reasons I do for animations: it doesn’t require maintaining previous values or any other obscure state, just the current averaged value. They also use it in digital audio, where you typically have a fixed dt of 1 / freq the inverse sampling frequency (e.g. 1/44100 or 1/48000). Last paragraph title I had the idea of this post for many months, glad to finally get it done :) As usual, watch my devlogs: and thanks for reading!",
    "commentLink": "https://news.ycombinator.com/item?id=39637487",
    "commentBody": "My favourite animation trick: exponential smoothing (lisyarus.github.io)370 points by atan2 6 hours agohidepastfavorite167 comments eviks 5 hours ago> Animations are not just a fancy visual thing, they help the user understand what’s going on. Instead of teleporting the toggle indicator to its new position, let’s move it smoothly Unfortunately it is, just a fancy thing that too often makes it worse or just focuses on the wrong thing. Like in this case, what exactly is \"going on\" that requires this visual delay? It looks worse vs the original instant response, why does the user need to care about intermediate movement of this binary toggle? Then this is the fanciness that detracts from solving the major flaw of these sliders: you can't easily tell whether it's off or on reply chefandy 5 hours agoparent> Like in this case, what exactly is \"going on\" that requires this visual delay? Nothing. It's a technical demonstration. The delay is also overemphasized to make the difference more apparent. > The this is the fanciness that detracts from solving the major flaw of these sliders: you can't easily tell whether it's off or on That's because they don't represent anything. Toggles don't always represent on or off, either-- they could represent a binary choice between any number of things. > Unfortunately it is, just a fancy thing that too often makes it worse or just focuses on the wrong thing. > It looks worse vs the original instant response, why does the user need to care about intermediate movement of this binary toggle? Putting aside the functionality that animations can add, because others have addressed it, even \"feel\" does matter to most users. Many people would choose soft-close cabinets over regular spring loaded hinges even if noise wasn't a concern. You'd never see them in a commercial kitchen, though-- people just move too fast and that extra bit just gets in the way. It would be rather absurd for someone in a professional kitchen to assume home cooks have the same requirements and use cases for cabinets that line cooks do. Similarly, people who spend most of their days doing technical things using technical tools made by and for technical people interpret interfaces differently than most people. To developers, UIs are a little part of an application to expose the functionality to users, and the less distance between the application and the user, the better. But more often than not, that approach yields results far more useful to someone with a working mental model of how software operates under the hood. To most people,the UI is the application, and the application is an appliance. Having smooth interactions that 'feel' nice to use are much more desirable. As someone with a good amount of professional experience both as a developer and a designer, I'm consistently surprised by how quick so many developers are to assume their personal usage habits, osmosis-gained knowledge from projects, and folk wisdom about design trumps the expertise of seasoned credentialed professionals in the field. The reason most end-user-facing FOSS projects (that don't have foundation-funded UI teams like Blender or Mozilla) are used almost exclusively by other developers is because non-technical users usually find the experience unpalatable at best, and unusable at worst. reply cardanome 25 minutes agorootparent> Similarly, people who spend most of their days doing technical things using technical tools made by and for technical people interpret interfaces differently than most people. Most \"non-technical\" people use computers, especially desktops, in a professional setting. Navigating those UIs is part of their jobs and they will spent hours upon hours doing so. Pretty animations may make a good first impressions but they get old and annoying very fast. It is a huge mistake in UI design to only optimize for smooth on-boarding of new users and not for long term productivity. > The reason most end-user-facing FOSS projects (that don't have foundation-funded UI teams like Blender or Mozilla Blender has great UI because they are dog-fooding their own software through blender studio. Meaning because they are the users and develop UI based on their own needs, contradicting your theory. Firefox UI has gotten significantly WORSE over time and the browser is becoming irrelevant based on market share. Now that is not all the fault of the UI but it is still worth noting that the height of firefox popularity was when it had a very power-user friendly UI that all that fancy \"UI experts\" would hate. Maybe they should replace the \"foundation-funded\" UI team. As for commercial software, modern UI is so bad that many \"non-technical\" users actively hate using computers and feel absolutely powerless when errors occur. Modern UI is actively user-hostile. reply bongodongobob 3 hours agorootparentprevI think parents reaction might be due the amount of shit software with pretty UIs. In the last 10 years it really feels like software has leaned towards form over function. I personally miss, simple, snappy Windows NT type UIs that were a megabyte and didn't contain a gig of custom icons or hit the GPU to like fade a window or something. I think most people would trade opacity and fancy buttons for MS Teams to be able to load in under a minute. reply faceplanted 2 minutes agorootparentMicrosoft teams doesn't load slowly because of the animations, you can turn all of them off if you mess around with the settings and your windows registry enough. It loads slowly because MS have a huge incentive to keep adding features and almost no incentive to optimise it because the software is almost always purchased by large institutions, half of whom have vendor lock in already, and imposed on the people who actually use it day to day, who have no power to change it. reply johnnyanmac 1 hour agorootparentprev>I think most people would trade opacity and fancy buttons for MS Teams to be able to load in under a minute. It's a paradox. If you don't make it look fancy, smooth, and appealing, you lose out to those that do. Even if you have a better product, that doesn't mean you can sell it on merits of being more technically efficient. Especially to more casual consumer or to business heads to make the purchases. besides, the minute loading times aren't because it's caching all those animations and assets. That's a different division with different problems. Probably a mix of security checks, inefficient networking, and _maybe_ some choice in framework like Electron as a distant 3-5th (not that Teams uses Election). reply anon373839 2 hours agorootparentprevThat's a fictitious trade, though. Opacity and fancy buttons are computationally very cheap, and they aren't the reason for Teams's poor performance. reply wizzwizz4 16 minutes agorootparentIf Microsoft Teams were made in Windows Forms, it would not have the fancy look it has now, and it would be a lot faster. \"The reason\" is complicated. reply dz08dl 4 hours agorootparentprev>As someone with a good amount of professional experience both as a developer and a designer, I'm consistently surprised by how quick so many developers are to assume their personal usage habits, osmosis-gained knowledge from projects, and folk wisdom about design trumps the expertise of seasoned credentialed professionals in the field. Exactly! Too many people think that their personal preference is the best. Sometimes, the best solution is to have plain HTML where the user chooses their favorite font and size, like in the early Netscape era. However, that's not always the case. reply anon373839 2 hours agorootparentprev> I'm consistently surprised by how quick so many developers are to assume their personal usage habits To be fair, developers coming from math and CS backgrounds may not have had any HCI/UX/design training. For those who have, \"You are not the user\" [1] was drilled in as a foundational concept. [1] https://www.nngroup.com/articles/false-consensus/ reply vidarh 1 hour agorootparentMost frontend developers also don't have much - if any - HCI/UX/design training. My experience is that a lot of people who should have had that instead see design as \"above\" that in a way that is worse by far by showing active disdain for user feedback. Often, in fact, by dismissing input from a user perspective from developers with a \"oh, but you're not typical users\". Which is on one hand usually fair, but on the other hand dismissing that they also are users, and often there is no such thing as a \"typical\" user. If one segment of your users reports something as a problem, you need to at least try to understand why and whether it is possible to reconcile with what your other users want, or whether in fact your other users also would agree but just haven't been vocal about it. reply sabellito 1 hour agorootparentprevYour point has truth in it of course, but the main theme gp was pointing out is the hubris of people, often in our field, to think that they know better than someone specialised on a given field. reply injidup 47 minutes agorootparentprevBinary toggles are everywhere in web UX. They are almost always used where a simple dropdown menu with WORDS indicating the chosen state would be better. However designers hate that because then they can't write fancy animations on creamy toggle widgets. A dropdown with the words enabled/disabled indicating the state will always be better than a toggle with some subtle animation and colour signalling the state. reply eviks 4 hours agorootparentprev> It's a technical demonstration So? The justification is still substantive. What would change in a real slider that turns real setting on/off? > delay is also overemphasized That's only true for those slowed down by a factor of 8. Otherwise it's not, I see the exact same issue in real-world sliders on web sites and in apps. > Toggles don't always represent on or off, either But they mostly do, add they do in this case (\"turned_on\"). The other use cases would have different issues like the choice of colors (green is for on) Then you overly general description forgets to prove that slower and more confusing control (which some generic non-tech users try to actually slide by holding a mouse button and moving the mouse instead of clicking) is actually preferable because it \"feels\" nice to them Also, these > seasoned credentialed professionals in the field. are the same people who put these garbage sliders in the OS settings menus (as well as maintaining many other common decades-old UI bugs), so color me unimpressed by their credentialed osmosis degrading user experience at scale (poor FOSS designs notwithstanding) reply bobbylarrybobby 4 hours agoparentprevComputer UIs need animations for the same reason they need shadows, gradients, and momentum: because these are aspects of the physical world that we already know how to interpert, so can be used \"for free\" to aid our understanding of the UI. Not using animations would likely be more taxing on our visual processing because nothing in the world moves discontinuously, so that would be a new thing to learn when using a computer. The primary reasons not to use an animation should be technical: you can't have it run in a short enough time and still look good, not drop frames, conserve battery life, etc. reply wolpoli 1 hour agorootparent> Computer UIs need animations for the same reason they need shadows, gradients, and momentum: The thing about design principles is that the designer picks the one that justifies their decision, which is more like fashion than anything. For example: Windows 8 swept away shadow and semi-transparent material in the name of flat design, making it resemble Windows 2.0. Windows 10 brought back shadow. Windows 11 brought back Semi-transparent material and also rounded corners buttons from Windows 2.0. Gradients have been banished and missing for a decade, but I am waiting for a come-back. > because these are aspects of the physical world that we already know how to interpert, so can be used \"for free\" to aid our understanding of the UI. The above could also apply to more than just shadows, gradients, and momentum. Skeuomorphic icons help users understand functions but we have shape & outlines resembling hieroglyphs. I doubt we are getting skeuomorphic icons back ever because font icons are just too awesome for designers. Most real world buttons are elevated off the surface to let us know it is pressable, but digital buttons now just have a white or colored pill shape around it. I can't recall the last time I saw a pill shaped button in the real world. reply ulrikrasmussen 3 hours agorootparentprevGot any research to back up that claim? I always go to the developer settings and turn off all animations on my Android phones, and it makes a monumental difference in usability, exclusively for the better. I did it on my moms phone as well, and she instantly felt it got faster, so this is not just because I am a technical person. Some UI designers think animations are good, but they are not. They are a clutch that designers who don't know how to design good UIs rely on to indicate what is going on. We have never had faster computers than we do now, but we are collectively spending thousands of hours a day waiting for pointless animations to finish. reply alpaca128 1 hour agorootparent> Some UI designers think animations are good, but they are not Imho animations are not a problem in cases where they do not affect latency. For example a visual click feedback to tell the user that it was registered & is being processed makes sense, especially for situations where the task may take a while. For example Mac OS' \"eject drive\" button would benefit from this, it has zero visual click feedback while ejecting the drive can take a whole minute for some reason. Yet at the same time Apple simply refuses to remove the 1 second long workspace switch animation even if blocked in the accessibility settings, they only swap the swiping animation with a fading animation. The problem is that often designers simply don't put any thought into it other than \"make it smooth and look nice\". In cases where the animation affects latency between user input and reaction to it there is no animation speed that's fast enough. It will always feel like wading through a swamp. reply piyush_soni 59 minutes agorootparentprevI'd always done on my previous (slower) Android phones, but it's NOT because they add to the usability, just that they slow down the experience significantly when the GPU etc. are not capable enough. On my latest Pixel, I don't do that anymore and just enjoy the animations because they don't slow down my daily interactions anymore. So your point does not actually convey that Animations are useless. They just have a toll on performance, yes. They always will. (I work in an industry where we spend a lot of time perfecting 3D geometry animations, because our users prefer it). reply arcastroe 1 hour agorootparentprevThank you for this. I didnt know I wanted disabled animations until I tried it just now. reply geysersam 2 hours agorootparentprevWow just did what you suggested and indeed it feels much faster. I was sceptical at first but this is actually better. reply coldtea 3 hours agorootparentprev>Computer UIs need animations for the same reason they need shadows, gradients, and momentum: because these are aspects of the physical world that we already know how to interpert, so can be used \"for free\" to aid our understanding of the UI More like \"for the same reason a fish needs a bicycle\". >Not using animations would likely be more taxing on our visual processing And yet, Windows 2000 era UI was far more intuitive, consistent, and self-discoverable than today's animated, shadowed, gradient-filled, blurry transulcent, momentum-having flat crap. reply eru 3 hours agorootparentIsn't shadowed and flat sort of the opposite of each other? reply coldtea 3 hours agorootparentAmazingly, not in 2024. They put shadows on otherwise flat layers - for gratuitous effect, but also to get back some separation after they've flattenned everything. reply wolpoli 36 minutes agorootparentThe shadow is around flat piece of paper of zero height, levitating off the background, casting shadow 360 degrees all around. No physical lighting set up could produce this effect. reply eviks 2 hours agorootparentprevThis appealing fallacy is part of the reason, but way too broad in general (and is also not reflected in the flat design land of today, but even before that almost nothing was really skeumorphically close to reality), so let's try to apply it to this specific case: you usually click on a slider, not drag it. So in this case, there is nothing \"physical\" on the user's intuition side that corresponds to the slider sliding. But that's exactly my question - what's the point of emphasizing the sliding? And taken your physical connection into account, the UI element should become a button And if you expect users to slide, then the slider should reflect the speed of your mouse like a physical control commongly would, not that of some random algorithm, no? reply jotaen 1 hour agorootparent> what's the point of emphasizing the sliding? Because the “knob” changes position. Like in the real world, objects can’t disappear in one place, and then instantly appear in another. Animating a UI state change in terms of a move transition can help people to understand how both end-states are logically connected to each other. The animation resembles the mechanics of the slider knob being a “perpetual entity” that can move between two positions, rather than the slider control as a whole being some sort of static icon that is instantly swapped out for another static icon once I click on it. Whether or not animations are necessary might still be arguable. I suppose it depends on the complexity of the UI control, on personal preference, and certainly also on the general level of technical familiarity of the end-user. reply alpaca128 58 minutes agorootparentI don't know about you but I probably wouldn't notice if my light switch suddenly moved to the opposite position instantly instead of in 50ms or something. And animations wouldn't be quite as annoying if they actually were that fast. reply jotaen 38 minutes agorootparentYou might not consciously observe how your light switch transitions to the opposite position, but I’d argue that most people still have an immediate and intuitive understanding of how both end-states of a light switch are related to them applying physical force onto the switch. The toggle transition is supposed to resemble this notion, and aims to allow the user to build up an analogous mental model of cause and effect. To me, this sounds like a valid and worthwhile idea in general. That animations are not always appropriate or fitting, or that they sometimes are laggy and sluggish, is a different story. reply dietr1ch 4 hours agorootparentprevWell, some constructs need animation, but others can work out just fine without animation. When typing, I want the characters to show right away, and instead of using a switch, I could use a checkmark that I wouldn't mind getting checked too fast assuming that the context isn't full of checkmarks. I think that we have tools in the UX toolbox that are less reliant on animations than others, but yeah, if you can get animations to be neat, yet fast it's great unless battery life is also a big concern. TBH, animations only began to be acceptable to me once I got a high refresh rate screen and a fast computer, before that I knew they were speed bumps for my old hardware and maybe hated them a bit more than they deserved. reply dgellow 4 hours agorootparentprevTry to disable/reduce animations in your OS via accessibility settings, you will see that you really don’t lose much. reply jalfresi 2 hours agorootparentprevIm not sure I agree with this, and its quite a common argument for animations in UIs. The fundamental weakness in this argument is that animations are not interactive. They have a beginning, middle and end. Any attempt to interact with the UI during this sequence would disrupt the animation. This is why animation makes UIs feel slow; clicking the widgets requires a period of time to transition from state A to state B via animation. The irony is that this is used to indicate a transition, to emulate what happens “in the real world sliders dont teleport to the next state”, but this often always misses the fact that I didnt slide the slider to its new state: i clicked a mouse button. That is an immediate state transition. So all animations like the above do is slow the feedback of the state transition in a misguided attempt to emulate a real world slider when i never interacted with the UI in any way that resembles a slider. reply pupileater 1 hour agorootparentThis is why we design animations in immediate actions to be below 100ms, any more than that and we start to feel \"computer's reaction is slow maybe it's doing some calculations in the background?\" reply croes 4 hours agorootparentprevThen a toogle button is a bad example. It's a replacement for checkbox and adds ambivalence that the checkbox doesn't have. Can you tell me which position of the toggle means on? Purple or green. reply chii 4 hours agorootparentthe color choice is the problem, not the toggle. If you had red or green, which one is on? reply smaccona 3 hours agorootparentAm I wrong in thinking that checkboxes/tickboxes/whatever-one-calls-them are ubiquitously understandable across cultures from both an interpretation and interaction perspective? If so, it wouldn’t be the first time I’ve made this assumption/mistake so I’d be happy to hear about it! If not (and I am right in thinking these are globally understood), then there’s a clear advantage of the checkbox over the switch in that it doesn’t depend on any color recognition to convey the current state. This is a huge win - I have definitely encountered UI controls where the current state was not at all apparent. I happen to use an iPhone, and haven’t personally had any issues interpreting the “switch” state nor the checkbox, but what if you’re color blind or from a culture where the color doesn’t necessarily mean what you think it does? Edit: clarify intent reply jalfresi 1 hour agorootparentCheckboxes also have a fundamental dependency on the label to assist them with their affordance: a label for a checkbox should almost always be in the form of a yes/no question: “are you hungry? [ ]” The “checked” box is an affirmative, empty is a negative. A checkbox without a label is useless because it has no context. But I constantly see checkboxs without the question label. Think back to all those control panels and settings windows youve seen where the label for a checkbox is something like “animations [ ]”. Does that mean they are on by default? Does checking the checkbox switch them on or off? Now compare with this “animations? [ ]” checking the checkbox has now become an answer to a question reply daniel_iversen 4 hours agorootparentprevIf you were colorblind that could be harder to answer (plus, it seems like a lot of designs have color \"themes\"). Checkbox is probably best. reply function_seven 3 hours agorootparentprevDepends on what it’s controlling. The tune of my car? Red = performance, green = eco-mode. The valve on a pipe? Red means “stop” (off) and green means “go” (on). What about a heater? Does red mean hot? Or stopped? Sibling comments cover colorblind users, but those concerns also apply to most e-ink users as well. reply idiotsecant 3 hours agorootparentIn fact red and green are generally not used for flow in a pipe because they have specific meaning for electrical equipment - green denotes an open circuit and red an energized one. This can be extremely confusing with something like an MOV so convention us to avoid red and green on valve position, pipe fill, etc. reply krzyk 2 hours agorootparentprevNot color choice, but existence of color. Some people have trouble distinguishing between red and green. Black and white is easy - checkbox is understandable quicker than a toggle. reply slimsag 4 hours agorootparentprevI am profusely colorblind, and it's infuriating when people only use colors to indicate something. Please for the love of god give me a checkbox over a red/green slider toggle. Shapes, please, shapes. reply bongodongobob 3 hours agorootparentI'm colorblind too but I don't pretend that it's a big deal, because it's not even a daily occurrence where I'm completely stumped by color and am blocked from doing something. Yeah, charts and graphs can be annoying sometimes, but I don't expect 90% of the world to bend around me because my sight is sub par. I loathe colorblind people who act like it's a disability. It's a slight inconvenience sometimes. reply slimsag 2 hours agorootparent300 million people with some form of color blindness, and for a minority of those it's much worse than others. It's great your vision doesn't affect you most days, I hope you live a happy life. For me, it's pretty annoying having to screenshot applications every few days, to be able to drag it into a color picker to be able to see what is actually on the screen. It's pretty annoying being in a Zoom call, in a tense customer call trying to debug something, and then having to screenshot a Grafana dashboard and drag it into a photo editor on the fly to understand what a customer is telling me - or have to divert the already tense conversation to 'sorry can you tell me what I'm looking at, my vision is fucked?' It's pretty annoying always being that inconvenience-to-others person asking for a link to the google slides, because I can't tell what is being shown on that pie chart. And I get it, I'm in a minority of minorities here. And it's not like 'wow colorblindness has ruined my life now I can't cook or clean myself!' - so I understand where you are coming from with 'it's not a real disability' But like, I'm not asking for you to go and construct a concrete pathway up a mountain here. I'm just asking that maybe when you are quickly adding 'a slider that indicates on/off using nothing other than color' to your shitty web app, that maybe you could just go with a checkbox instead? Or keep the slider and all the colors, but just add `on` and `off` labels to it too? Or add a hover text that says it's 'enabled'? Or at least steal Apple's slider colors instead of coming up with your own low-contrast ones? And yeah, I get it - we live in a world of 'omg wow I got glasses for christmas and now I can finally see color through my tears again!!!' social media garbage content. I just want to be able to know what a button is going to do when I click it. reply eru 3 hours agorootparentprevAdding extra differentiators also helps people with other kinds of vision problems. The benefits of accessibility often go beyond the initial target audience. reply krzyk 2 hours agorootparentprevBut, making UIs tuned for colorblind people make it easier also for others - colors should be distinguishable easily. reply waveBidder 4 hours agorootparentprevlets ask someone who's colorblind reply bongodongobob 3 hours agorootparentRed green colorblind checking in. In the case of a toggle, left is off, right is on. Up is on, down is off. Red tends to have less chroma, so dark is off, light is on if there aren't any other cues. reply vidarh 1 hour agorootparentWhile that seems to be more common by a fairly large factor, in a quick search of images of toggle buttons there were at least two examples of left being on already in the top 10 for me. I also found a few that had both variants in the same image, and a few examples where from the image alone I couldn't tell if my life depended on it whether they are in the left or right position, and what that means because there's no shading or geometrical hint as to which part is meant to be the actual toggle. I wish you were right and that this was consistent, and when I read it I did that image search because I hoped you were and that I somehow just hadn't realist. But sadly it's not consistent. That said, the same image search also demonstrated people managing to mess up the equivalent of a checkbox too (by changing the label, so it's not clear if it needs to be \"ticked\" to disable the state currently indicated by the label) so it's clear the problem isn't just checkbox vs. toggle. reply King-Aaron 3 hours agorootparentprevI have a different definition of what a \"need\" is. reply indigane 5 hours agoparentprevI'm working on a Git GUI for myself, in which one of my top priorities is making it more understandable than Git's own UI. I usually dislike animations. I used to disable transitions whereever possible, as they made the UIs feel sluggish. And I still do. But working on the project made me for the first time really appreciate in practice how much amimations help with understanding. I paid a high price in complexity to add transitions whenever the state of the visual graph changes, and suddenly it was really obvious what was happening. Commits and whole branches were sliding into their new position, and I would go \"a-ha! I see\" instead of \"wtf just happened, what am I looking at now?\". EDIT: Fair warning, it's heavily WIP https://gitlab.com/indigane/visual-git reply zbrozek 5 hours agoparentprevI miss old school checkboxes for UI toggles. They're crisp and clear and don't entice bored people to animate them. reply max_k 4 hours agorootparentMe too, agree with you and GP. And I can't keep wondering why this opinion is so unpopular. Today's UIs are bloated with unnecessary animations (which adds latency). But worse than animations is that UIs are horribly inconsistent; took me a while to figure out those toggles should be clicked, not dragged; or: what is even clickable, how do I scroll, ...? I could go on forever, and probably so can you. Why do only old nerds complain about this, when today's UIs are so \"easy\" that every toddler can use the smartphone? Are we just living in the past, getting old; are we the problem, why is our opinion unpopular? reply josephg 3 hours agorootparent> why is our opinion unpopular? I think other people do feel a vague sense of anxiety using modern software from not quite knowing what all the interaction patterns are. When you click that hamburger menu on the website, what will it do, exactly? But most people from outside the software world just blame themselves for “not being very good with computers”. The problem is that it’s not fashionable any more amongst designers to use built in controls. Everyone wants to think of themselves like Apple, and build their own beautiful design language. Even if it’s just for their own website or app. And it sort of makes sense given modern apps end up needing to be built for the web, iphone, iPad, Android, and the desktop. It makes sense to tie all of those pieces of software together with a cohesive visual language and style. reply zbrozek 3 hours agorootparentprevI'm old now, and I won't dismiss that the new stuff is aesthetically nice. It's also not that hard to use. But I just don't like the visual polish more than I like the clarity and responsiveness of the old UI elements. I just don't care if my computing experience is beautiful. I care if it's snappy, productive, and reliable. reply max_k 2 hours agorootparent> I just don't like the visual polish more than I like the clarity and responsiveness of the old UI elements. Agree, that sums my opinion well. And much software doesn't even look like they preferred eye candy over clarity; it rather looks like they forgot about clarity completely. reply tetha 2 hours agorootparentprevI mostly find the way the opinion presented in this thread by many people exhausting. You're doing it there too: You're throwing every bad point of every bad UI you ever encountered into a bucket and throw all of that at this article by concluding \"Animations in UI are terrible and just bloat everywhere\". That's very close to a strawman. I have worked and AB-tested in UIs for games and such dealing with just that and I would much rather say: Bad UIs are bad, yes. And animations don't help bad UI not being bad. But if you have a good, understandable UI, adding animations smartly - without impeding the user and in subtle fashion - on top of that UI... that can increase the overall aesthetics of the UI a lot and make the UI much more pleasing to use. reply max_k 2 hours agorootparentI agree with that; animations can be OK, but when I have a configuration setting, I usually disable them because input latency drives me crazy. My post replied to \"checkboxes vs UI toggles\", and replying to that aspect was my main point. That's slightly off-topic, of course. It has to do with animations only because checkboxes wouldn't really benefit from animation, whereas toggles are an obscure visual representation for the same control, and adding animation is a feeble attempt to make it somewhat less obscure, even though it doesn't even try to address the main problem: what does toggle \"left\" and \"right\" really mean? I believe checkbox not benefting from animation is a good thing: it's so clear and obvious that you don't need to animate it. reply tetha 2 hours agorootparent> what does toggle \"left\" and \"right\" really mean? Nothing, because that's not the point of the article. It's weird to me that this is such a big point here. In an actual UI, you will have labels or indicators telling you what the toggle means and what the options are - \"Safety door unlatched\" vs \"Control motors engaged\". That's a toggle between two choices and having it a toggle like that would be safer than checkboxes. Otherwise your checkbox without labels is equally bad UX because what does \"on\" and \"off\" mean for an unlabeled checkbox? I could give enough examples from work how vaguely labeled checkboxes like \"remote authentication\" are terrible UX. reply max_k 2 hours agorootparent> that's not the point of the article. Of course not. The post you're replying to explicitly said the discussion was off-topic. That means it's not discussing the point of the article. > In an actual UI, you will have labels or indicators telling you what the toggle means and what the options are If it only were that way, we wouldn't complain. But it's not. > checkbox without labels is equally bad UX Of course, I agree. But nobody asked for that. Your post ignores the things that were said and replies to things that were not said. reply josephg 4 hours agorootparentprevI’m quietly obsessed with the idea of rebuilding a lot of modern user interfaces just using classic controls that we’ve had since the 90s. Checkboxes, radio buttons, labels and text input elements. I don’t know if it would be as good as what we have now, but there would be something so relaxing about all of my applications looking like they’re part of the same world. reply max_k 2 hours agorootparentBack in the days, you had a UI toolkit, and everybody would use those native controls; they looked and felt the same in all application, and you had a central place where you could customize the look. Now every application/website has customized controls for everything; everything looks and works differently. (And don't even get me started with websites implementing their own scrollbars with JavaScript. Uh!) Custom list control: do Home/End buttons work? How to select multiple items, does Shift-Cursorkeys work? Does Ctrl-Click work? Of course not. Custom text control: does Ctrl-Left/Right for word jumping work? Does Ctrl-Up/Down for paragraph jumping work? Can I select everything with Ctrl-A or does it select the whole website? Can I select everything from cursor until the end with Ctrl-Shift-End work? Does Copy/Paste work at all? (I have never figured out why Copy/Paste in Teams simply doesn't work. Apparently I'm the only one with this problem.) Custom dropdown control: does Alt-Down work? Can I scroll the list with the usual keys? If (web) developers would just use standard controls, everything would work the same, and they wouldn't have to reimplement all the basic things from scratch (or not at all). Web devs could write forms that work without megabytes of JavaScript. Hamburger menus. Those horrible things didn't need to exist even in old times with small monitors and 640x480 (or less) - but now they exist everywhere on my 32\" 4K monitor for no reason. reply srg0 56 minutes agorootparentprevI beg to differ. Quick search shows that people animate checkboxes too: https://stackoverflow.com/questions/44876144/how-to-style-th... reply anon373839 1 hour agorootparentprevCheckboxes and switches serve different functions and shouldn't be used interchangeably: https://www.nngroup.com/articles/toggle-switch-guidelines/ reply Animats 5 hours agoparentprev> the major flaw of these sliders: you can't easily tell whether it's off or on That's the whole point. That's why the primary use of sliders on desktop is opt-out of something no user really wants. This is often combined with scroll windows with some sliders offscreen and invisible scroll bars. reply saretup 5 hours agoparentprev> It looks worse vs the original instant response I think that’s an unpopular opinion. The instant response looks janky, exponential smoothing looks well polished as long as it’s not too slow. reply lenkite 3 hours agorootparentDisagree - the instant response was alert and immediately observable and didn't take valuable time away from a limited mortal life. Its like saying a web-page that loads and displays instantly is \"janky\" versus a web-page with a busy symbol that makes you pause and wait. If you had 20 of these toggles to check, you would prefer your \"janky\" version anytime. reply starspangled 4 hours agorootparentprevIt didn't improve it for me. I don't want a switch animation that looks (like the designer thought) a physical switch might move. I want a widget to indicate and change some option. It's not a great type of widget for that anyway, as others said. But either way the animation adds nothing except time the designer could have used to think about a better interface. The snappy \"instant\" and simple UIs for Windows 95/2000 era were the best, IMO. It might take a bit to get used to non-animated updates it if you have never experienced it before and seem a bit janky, but it doesn't take long and at least for me feels much nicer. Quarter of a century of \"UX experts\" and useless animations and transparency and complicating things has resulted in an objectively worse experience. It's slow, clunky, inconsistent, constantly changing, and doesn't even look good. The technical parts of the drawing, scaling, font rendering, color management, etc are much better of course, but not the overall experience. UI design was of huge importance to the industry (Apple and Microsoft) when GUIs were first coming in, many people were still getting into the world of computers, and many of those who were using them were not comfortable with them. Improving the experience could be worth a fortune in new market. That's why usability, intuitiveness, and consistency were priorities and it was taken seriously as a discipline, there was research, and changes were made with (at least in part) quantitative data and study. That field of UI design has basically died in the industry now. At least, it is much smaller and less impactful than it was. Most of it seems to have shifted to getting people to look at and interact with things that they otherwise would not have. I guess shiny new things would help with that, maybe that explains the baffling direction things have gone in. EDIT: BTW., don't take this as criticism of the linked post. Back when I did a bit of graphics programming I loved tinkering around to make things visually pleasing and behave in interesting ways. The post is fun and interesting, and presented in a way that itself is a nice experience and must have taken a lot of work. And I don't think the author was passing it off as an end to end guideline for a user interface design. And I do accept that a lot of people do like more visually interesting and diverse interfaces. They're wrong, of course, but entitled to their opinion (/s) reply arghwhat 2 hours agoparentprevThe animation clarifies the change. A user, heck even I could be confused by an instantaneous touch toggle. Why? Because my finger covers it during the press, so I would not see the change happen - if my mindset was \"change that setting\", I might rapidly tap it again, thinking the press didn't register. Such confusion does not occur with a (fast) animated transition. The map example is much more important. With animation, your eyes and brain recognizes a single image sliding around, giving you automatic positional awareness. Without, you eyes and brain recognize a slideshow of independent images, and makes recognizing and navigating the map a conscious effort. By no means an impossible effort, but an entirely unnecessary one detrimental to the user experience. At the same time, navigation UIs that react as navigation-less updates also tend to have the double-whammy of bad UX in the form of blocking and discarding user input during updates. Bad UIs are not bad because animations are bad, they're bad because they were made poorly. reply y42 1 hour agorootparent>> The animation clarifies the change Isn't that a problem that UI designes (sorry for generalisation) created by themself? Let's stick to the toggle element. It replaces the classic checkbox. A \"checked checkbox\" leaves no doubt. A toggle box does. reply klausa 1 hour agorootparent>A \"checked checkbox\" leaves no doubt. It's absolutely not that simple. I actually think that the toggle design is _very_ clever, explicitly because it sidesteps a lot of cultural baggage that comes with existing symbols, and creates its own, fairly unambiguous one. Do you use \"checkmark\" to indicate that a choice is selected? In some cultures that indicates \"wrong\" answer. https://en.wikipedia.org/wiki/Check_mark Sure, just use \"x\" then, right? In _different_ cultures, that one indicates \"no, I do not agree\". https://en.wikipedia.org/wiki/X_mark reply vidarh 52 minutes agorootparentThis is a problem if using \"tri-state\" checkboxes. Don't. They're confusing. It is not a realistic problem for two-state checkboxes. I'm from nordic country, and yes, I too have seen the \"checkmark\" to indicate mistake on school work as listed on the Wikipedia page and attributed to Sweden and Finland. I've however also seen that on my son's school work in the UK. And I've seen it used for correct. And for just \"I've checked this\". It is not culturally consistent, and usually appears to be more indicative of a personal marking style. And almost only in that kind of context. Outside of that context, a checkmark is still mostly used as a positive confirmation across the Nordic countries too. You may want to make sure that the wording of the label does not give room for thinking you can tick it to answer \"no\", but that equally applies for a checkmark. To make sure I wasn't overlooking some difference between Norway (where I grew up) and Sweden, I just checked aftonbladet.se - one of Swedens largest newspaper, and their signup page for example uses checkmarks to positively indicate the presence of features in their Swedish ad copy. On a form it certainly is consistently meant to indicate \"selected\", just as an \"x\" is, in those countries as well unless the form explicitly says otherwise or indicates you can use both. The only confusion arises if both are valid states because you've used a tri-state checkbox. So don't. They're confusing everywhere. Maybe it's not universal, but the use of checkmarks outside boxes to indicate \"wrong\" in some contexts is entirely irrelevant to the interpretation of checkmarks in boxes on a form where the options are not-marked vs marked. Sure, if you are designing a UI where you are displaying corrections to something, don't display a checkmark - probably irrespective of whether you include a box - without some additional indicator of whether it means right or wrong. reply pazimzadeh 1 hour agorootparentprevfew things in the world are perceived as instantaneous, except maybe lightning. until recently people were not used to interacting with instantaneous objects. even a checkbox, in its original incarnation with pen and paper, would take a second to be filled. as long as the animation is interruptible, and the underlying change happens quickly (whatever the toggle is controlling) then animating the transition is usually better. I really like that their animation is interruptible. reply adrianmonk 4 hours agoparentprevHere's one practical benefit: the UI should make it obvious that your click did successfully register, and an animation does that because it's eye-catching. But the other side of the coin is that, because it is eye-catching, animation can be distracting or annoying. Adding animation to your UI is like adding salt to your cooking: it's going to be pretty bland without it, but you can also ruin things by going overboard. reply chefandy 4 hours agorootparentMost people that complain about these things don't even realize how much of it is integrated into their daily lives and workflows because it's too natural to notice. You don't need experience to see when someone's done a shitty job, though; just like any other tool, inexperienced people often see poor outcomes as an indictment of a technique, tool, or approach when the practitioner is to blame. reply alpaca128 30 minutes agorootparentI think that's a bit of an exaggeration. Yes, animations are useful sometimes, but in at least 90% of cases they make the experience worse. If animations were so important and integrated everywhere then why didn't I notice any change when enabling \"reduce animations\" on all my devices, other than everything being suddenly faster? > inexperienced people often see poor outcomes as an indictment of a technique, tool, or approach when the practitioner is to blame. Or they simply realized that so many practitioners are to blame that the distinction barely matters anymore. reply eviks 4 hours agorootparentprevGreat UI goal, though what's catching is the change in visual state and colors, which also happens without extra animation, or you can add an extra highlight of the changed item to emphasize the change if state, so this doesn't address the specific issue reply jkaptur 3 hours agoparentprevI think the map example is better. Without the animation, you have to figure out visually how far the map moved. reply oneeyedpigeon 1 hour agoparentprevContrary to what some of the replies imply (that a smooth animation is always better than a discrete one), I have a solid example where animation definitely harms the UX. One of the language apps (either Hemingway or grammarly) had this issue stepping through search results. As you moved, it would change the colour of the matching text, but slowly and subtly, via animation. That makes it much, much harder to actually spot where, in a wall of text, the match actually is. An instant change is far more visible. reply dgellow 4 hours agoparentprev“Accessibility > Reduce animations” is the magic trick to make modern UIs feel snappier and less distracting. I wish web UIs would respect it. reply morsch 3 hours agorootparentI was wondering if they even could. The answer is yes: https://developer.mozilla.org/en-US/docs/Web/CSS/@media/pref... My first thought was that the OS accessibility settings must not be available to websites without a permission toggle because the ad industry is just going to weaponize it and use it against people. reply greyhat 3 hours agorootparentprevI use this on iOS and it still leaves a lot of animations that could be sped up. reply ernstgnzlz 1 hour agoparentprev> Unfortunately it is, just a fancy thing that too often makes it worse or just focuses on the wrong thing. For UI elements that try to replicate the physical world, such as toggles, switches, even some buttons, I find it that the animation makes the UI easier for the brain to understand. reply ustad 3 hours agoparentprevRemember that time when the ios calculator was messing up with simple arithmetic due to the animations? reply gleenn 5 hours agoparentprevWhile in small doses it's a bit nice to affirm to the user they didn't blink while the state changed, it adds significant complexity. Nearly every one of the animations in that website flicker and the map rendering actually has a bug where it doesn't stop jittering back and forth very visibly indefinitely. Definitely makes me not want to implement stuff like this. reply recursive 5 hours agorootparentYou were so close to reading the part where it explains why that's happening and how to fix it. reply BriggyDwiggs42 4 hours agorootparentprevThe jittering was an anti demo i think reply ramesh31 4 hours agoparentprevAnimation should enhance and enrich the UI, not make it \"look fancy\". Apple gets this perfectly. The squeeze animation on iOS's volume bar is a great example. Providing feedback to the user in response to an action, in a manner that is expected and makes sense, can massively increase UX. reply eviks 2 hours agorootparentApple is missing a few bites from its perfection: They use these sliders, realize they could be ambiguous, so add an accessibility setting for the on/off labels. Then they have these fancy animations where holding a slider even elongates the button \"providing feedback\", but this is just wasted since your finger is covering the button, and fingers have no eyes! But still, it's a slider, so you'd expect that if you slide your finger over it, it will move. Not so fast, this only works if your started sliding from the slider, not from a side outside it (which would still work in the \"real\" world.). But if you start at the slider, then there is no point in sliding, there is simply not enough width for that! Also if slider's button is to the left and you swipe from right to left, the sliders moves right! Take that, immediate fedback in the opposite direction, massive UX boost in a manner that's unexpected and doesn't make sense! (because it's not a real slider, just a check box of a different form) reply youssefabdelm 4 hours agorootparentprevExactly, to the OP of parent comment, this talk is a great overview of exactly this point: https://developer.apple.com/videos/play/wwdc2018/803 reply baxuz 55 minutes agoprevThis is actually a really good approach and a good proof of concept for an animation / easing technique. Reminds me a lot of Flickity: https://metafizzy.co/blog/initial-demos/ https://metafizzy.co/blog/math-time-resting-position/ https://metafizzy.co/blog/particle-to-slider/ https://metafizzy.co/blog/flickity-begins/ Especially this demo: https://codepen.io/desandro/pen/myXdej This technique isn't just useful for switches. Nor will you use aelement for a switch. Nor will you have 20 parallel requestAnimationFrame loops running on the entire site. Or intentionally broken elements. The site also doesn't have optimizations where the rendering stops once the delta is too small — or probably dozens more of small tweaks that could make this production ready. The comments here show that people either haven't read the article and are making assumptions, or can't see the forest for the trees. Or are just simply so biased and cynical that they need to share their (unprofessional) opinion in order to appear smart. Since when has HN turned into Reddit? reply verisimi 45 minutes agoparentThe cynicism doesn't seem that fair in this case. Re HN becoming like Reddit.. Perhaps there is a generic 'cynical feeling' in the air with technology, with feelings of distrust about the intentions of governments and corporations over their use of the profound technology that is appearing, and how this is being constructed with the intention to constrain and manage the individual, rather than enabling greater freedom. reply cmplxconjugate 17 minutes agorootparentI think maybe more that the users of this website, which is designed in a way that is intensely functional compared to other similar sites, just prefer less “useless” design features. reply Animats 5 hours agoprevYou never get there with exponential smoothing. It takes infinite time to asymptotically approach the goal. Also, that page uses 100% of the CPU in Firefox. reply pixelesque 5 hours agoparentThe bottom half of the page doesn't even show in FF on MacOS for me... just white... Text selection also doesn't seem to work... is it all Canvas maybe? reply raybb 1 hour agorootparentPerhaps it's worth filing a bug report if this works on other browsers but not FF. reply blt 4 hours agorootparentprevAlso FF on MacOS, and same issue. Seems like a hang reply Izkata 4 hours agorootparentFirefox on Android, too. reply TomK32 3 hours agorootparentand Firefox on Linux. reply unwind 2 hours agorootparentSeconded (Firefox on Linux), it worked fine with the toggles but once it came to the map-based examples it stopped. Never saw a map scroll, and continuing down the page started to blank out the (assumed to be) maps, so I closed the tab. Very nice-looking page, but perhaps a smidgen over-engineered for what it wanted to show? :) reply xandrius 3 hours agoparentprevExample of form over function. If the site doesn't even load, does it matter what function you use for the toggle in the settings page? reply buserror 3 hours agoprevAll of this work for buttons that toggle on a mouse/finger DOWN. How irritating that there is no intermediate state on the mouse DOWN and the toggle on the mouse/finger UP. Because it allows you to /cancel/ the action.. click, 'oops don't want that' drag mouse/finger out and release. No action taken. User is in control. Amazing. reply coldtea 4 hours agoprev>However, this still looks a bit clumsy due to having a constant speed Does it? Looks much better to me, and preferred to waiting for a animation to be given more time to speedup while looking \"smooth\" (they seldom seem to be given the same time as instant or linear animations, do they? For some reason we have to be forced to enjoy them in half time). In fact I'd take the \"instant toggle\" in a heartbeat too. Real life toggles don't \"exponentially smooth\" from one side to the other either. reply modeless 35 minutes agoparentThe only problem with instant transitions is that sometimes you may not notice what changed. I would like to see a UI built around instant transitions but with subtle animations that happen after to emphasize the change. Like the new content shines, or does a tiny bit of grow/shrink, but not in a way that prevents interaction. There's nothing I hate more than waiting for long transition animations (looking at you, macOS fullscreen button) reply globular-toast 1 hour agoparentprevI hate animations like that because they are intentionally slowing down the UI. I already clicked it, why are you spending several milliseconds to update your state? Animations, or anything else really, should only be used to improve things. Ask what the problem is, not what cool solution I should implement. It's ok to go to town if the page is artistic, like your blog, but if I'm filling out a form? Get your art out of here. reply useless999 1 hour agoprevAuthor writes about useful easing concept, happens to use UI element purely for illustrative purposes, HN comment section turns into a 100+ comment jihad over animations in GUIs. Nothing substantial is discussed. reply jdiff 49 minutes agoparentSo many people allergic to design and equating bad design with all design. Studies? Experts? What could they know? These guys have opinions on modern software and designers to throw under the bus for them. reply ggm 6 hours agoprevI continue to marvel at how often simple non linear tricks add some joy to interactions online. Or, in the case of colour perception are fundamental to understanding why two colours may not be perceptibly different enough to some people. The odd thing is that humans do not always understand acceleration. Don't run away from a fire uphill, in a belief it travels as fast (semi constant) across level ground: fire accelerates up hills. Kids rapidly learn the rate of movement across ground for a thrown ball but not always just how fast it will be moving under gravity when it smacks into your hands. reply saltyoutburst 4 hours agoprevIt's fascinating that easing, which is what this article mostly boils down to, is something that each new generation seems to need to (re)discover on their own. I remember being fascinated by Yugo Nakamura's experimental websites back in the late 90s because they were some of the first that I'd seen that had an organic feel due to liberal use of easing. https://www.youtube.com/watch?v=NLt7Gwnt3WY reply hellweaver666 1 hour agoprevI appreciate the effort that goes into animation like this, but I always have to wonder, especially in the context of the web if it's worth all of the additional code and development time for such a minor detail. Do our users even notice? reply Klaster_1 42 minutes agoparentDuring my day job as a frontend developer, I have to cope with this quite often. When you raise a point with the manager that we could be fixing real bugs instead re-doing a non-essential, very subjective thing the designer came up with, they usually reply that they trust the designer as a professional and they know better - the developers don't have a choice. reply bee_rider 4 hours agoprevFor some reason this makes me want a toggle that: * Moves slowly to about 75% of the way there while you hold the touch/click * Snaps the rest of the way on release I’m not sure what this could mean in terms of UX though. Maybe the setting is actually applied or saved at the end. Or it could be part of an “are you sure?” dialogue. The setting is applied as you hold, but you can hit escape to undo before it snaps into place. reply aimor 4 hours agoparenthttps://jsfiddle.net/u1vybhqg/ input:checked + .slider:active:before { transform: translateX(8px); transition: 1s; } input:not(:checked) + .slider:active:before { transform: translateX(18px); transition: 1s; } reply PennRobotics 1 hour agorootparentThis, but set closer to 200 or 100 ms (and for me, swapping the translate 8 and 18 so that the switch barely moves on mouseDown, because that's damn close to a real switch's behavior). This seems like it could be a good compromise to all of the people complaining about lag and others who understand that the visual system is powerful and movement is a huge part of that. Also, it's relatively simple as far as CSS animations go! reply bartread 2 hours agoprevI like this although I'm going to stick my neck out and say the author is objectively wrong that sqrt is better than cubic for a toggle switch, and that cubic is in fact the better choice for this situation simply because of the way real life toggle switches generally work. Think about the breaker switches on the electrical consumer unit on your house, or the kind of switches you often find on analog synths or other audio gear, generally stuff that's going for a particular aesthetic. I have a small Hughes & Kettner guitar amp that has two of these switches that are very satisfying to use[0]. When you use these kinds of switches in real life there's some initial resistance, and then they suddenly snap into the new position because of the way they're spring loaded. This is more closely modelled by the cubic function than it is by either the sqrt or the exponential smoothing function. Other than that absolute nitpick, I really enjoyed the piece. It well illustrates how animation can enhance a user experience when done well (with, for example, an appropriate easing function) or alternatively detract from it and be quite jarring if not implemented thoughtfully (thinking particularly of the linear interpolation examples). [0] This does somewhat depend on the type of toggle switch, although even the variety that you find on something like a Minimoog (this isn't some sort of gear flex, by the way - I don't own one of these!) exhibit this \"resist then snap to the new position\" behaviour that makes them so enjoyable to flick. reply asimpletune 1 hour agoparentIntuitively I too preferred the feel of the cubic function. A mechanical toggle will have some inertia, and I guess that’s what we’re seeing with the cubic approach. reply memalign 5 hours agoprevThe resource I come back to often for animation curves: https://easings.net/ reply paipa 24 minutes agoparentNo example from the ultimate, infinitely differentiable tanh(tan(x))-like family? How disappointing. reply huhtenberg 2 hours agoprevSome of example animations are buggy - https://i.imgur.com/hQyh05s.gif https://i.imgur.com/So6KsMt.gif reply mawax 2 hours agoparentThe author addresses this: \"See this jittering after the animation completes?\" reply degun 2 hours agoparentprevSame on my browser and system: Safari on iOS. reply sodimel 2 hours agoprevEmotional design (https://en.wikipedia.org/wiki/Emotional_Design) as it's finest form, there's a lot to tell behind a tiny little animation. reply ulrikrasmussen 3 hours agoprevJust go with the instant toggle instead of wasting time making the UI worse by making users wait for pointless animations. reply d--b 1 hour agoprevI wonder if there is some cultural preference for this. Looking at the map example, I find the cubic smoothing way “calmer”. The exponential one feels a bit hectic and I am wondering if it’s an American thing. It’s got a “wheez” quality that is annoying to me, but that reminds me of hyper cartoons like powerpuff girls. It’s still all very subtle… reply ano-ther 48 minutes agoprevIt’s funny how emotional discussions about UI choices are. I liked the article and its demonstrations because I learnt something. Personally, I switch off animations whenever possible, preferring snappy action over flashy effects. But they have their use, for example in the rubber band effect which would be quite jarring without motion smoothing. reply m_st 53 minutes agoprevIn relation to the formulas near the end of the article, I find it tragic that even in 2024 they are still PNG images. reply frfl 5 hours agoprevReally great write up. The demos seems to work fine in Chrome, but on Firefox they freeze up and cause they page to stop rendering completely as you scroll. reply begueradj 3 hours agoprevInteresting that you got the idea to dive into animating a switch :) Maybe you could help the user to better distinguish the ON/OFF states by also colorizing the whole switch element, not only that \"disc\" (as it is done in Vuetify.js, for example, using the \"color\" prop: https://vuetifyjs.com/en/components/switches/ reply cl3misch 4 hours agoprevInstead of the wall of text and equations, it would have been helpful to plot the easing functions. reply mzs 3 hours agoprevAnimations like this give me a mildly nauseous feeling like a strobe light at an unsettling rate. The instant toggle does not. reply mati365 3 hours agoprevYour site crashes whole Firefox on Linux. reply ben-schaaf 3 hours agoparentWorks just fine here; also Firefox on Linux. reply t43562 2 hours agoprevI think this was fun but I really hate those switch things - I find it confusing to know which position is on and which is off. Which colour is the \"on\" colour? A checkbox might be ugly but I do understand it at a glance. reply whitehexagon 1 hour agoprevhmm, could be my old FF (or hidpi) but many of the examples just dont work. Jittery scrolling on the landscape, and circles that start teleporting back and forth on the sliders, and one that vanishes to the left. To me that just highlights another reason to keep it simple... checkbox? reply nico 6 hours agoprevGreat write up, love how much detail it goes into, and the interactive examples really help illustrate the points reply timvisee 2 hours agoprevI don't like this kind of smoothing at all. It feels slow to me. Please just toggle without animations instead. And yes, I know these examples have been slowed down. reply tadfisher 5 hours agoprevA generalized version of this is Hooke's Law, also known as the spring function. It works well for UI animation because it maintains velocity as the target x changes, instead of stopping/restarting like an eased tween would. reply aappleby 4 hours agoprevI've used this for decades. The sweet spot for ui components seems to be moving 90% of the way to the target every 80 milliseconds. reply ustad 3 hours agoparentReminds me of that famous paradox. reply ramesh31 4 hours agoparentprevi.e ease-in reply gelatocar 4 hours agorootparentthat's ease-out reply nextaccountic 5 hours agoprevCan this be done in pure CSS? reply rabuse 4 hours agoparentCan probably try playing around with cubic-bezier, but not too sure. reply lausbub 1 hour agorootparentSomething like this: https://easings.net/#easeOutExpo reply DevoAKA 4 hours agoparentprev`transition-timing-function: ease-in-out` This reminds me of Medium spending months to change the underline beneath a descender: https://medium.design/crafting-link-underlines-on-medium-7c0... reply skrebbel 3 hours agorootparentThis isn’t like ease-in-out at all. It resembles ease-out a little. reply leetrout 5 hours agoprevSee also: Juice https://garden.bradwoods.io/notes/design/juice https://www.andy.works/words/the-most-satisfying-checkbox reply Hnrobert42 6 hours agoprevIs it just me, or does a little square flash around the toggle. I am on FF Focus. It totally derails the exploration of the smoothing variants. reply grugagag 5 hours agoparentSimilar experience in Safari on IOS reply mediumsmart 5 hours agorootparentMe too. Orion on iOS. So the border radius has a light leak? reply shahar2k 5 hours agoprevit's funny, the thing that got me to understand calculus initially in class was seeing the graphs, somehow my brain immediatly \"got\" the difference between acceleration and movement and the shapes of those formulas... these days I think it might be a type of kinesthetic synesthesia! helps immensely since I work in animation and always had an easy time figuring out just the right math for procedural animation :) reply seemaze 5 hours agoprevI noticed halfway through this article I was just clicking away furiously. Is this a clever ruse by $MEGA_INPUT_DEV_MFG to wear out my mouse clicker quicker..? reply sleepybrett 6 hours agopreveverything old is new agin, dozens of this exact article in the flash era. reply spicybright 5 hours agoparentWe lost a lot of collective wisdom when flash died IMO reply omeze 4 hours agoprevThis is very cool, the camera over world example really highlights the difference reply meehai 1 hour agoprevLinear is perfect. reply exe34 2 hours agoprevHave you tried using a tick box? reply ramesh31 4 hours agoprevNever animate position. One of the number one reasons for jank on the web. reply demondemidi 5 hours agoprevA lot of these don’t seem to work on my iPhone and Firefox. reply iainmerrick 5 hours agoparentTouch input in JS is a bit of a mess. You have to explicitly disable a lot of “helpful” default behaviour to make it work well, like auto-zooming on double tap or selecting on long press. reply GauntletWizard 5 hours agoprevPID Controllers for animations reply simon_kun 4 hours agoprevI love blogs like this. reminds me of 2005. reply jdiff 41 minutes agoparentIn a lot of ways we're just now reclaiming the good parts of the flash-dominated web. Fair bit of bad collectively as we're relearning, but things are trending in an interesting direction. reply qiller 5 hours agoprevStandard easing has one advantage being frame rate independent. Spring functions need some extra care to prevent things from... exploding. reply Traubenfuchs 2 hours agoprevAn I the only one upset this is done by (excessive) JS usage? It would be my deepest displeasure to bring a button toggle animation to production that requires more than CSS. Quick example I found: https://hudecz.medium.com/how-to-create-a-pure-css-toggle-bu... Easing/Smoothing is possible with transition-timing-function. https://www.w3schools.com/cssref/css3_pr_transition-timing-f.... reply tiborsaas 1 hour agoparentButtons are just examples because it's easy to understand. The point of the article is the math. Of course CSS could do it with less JS, but this would have been a totally different post. reply tnvmadhav 5 hours agoprev [–] thanks for sharing. this must've taken a lot of experimenting. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The text delves into utilizing exponential smoothing for generating smooth animations, especially in UI elements such as toggle buttons, explaining its mathematical concept and comparing it to other easing functions.",
      "It provides practical examples of implementing exponential smoothing and touches on challenges like animation jittering and floating-point precision.",
      "Exponential smoothing emerges as a valuable technique for creating seamless and lifelike animations, offering a smooth user experience in UI design."
    ],
    "commentSummary": [
      "The article delves into the role of animations in user interfaces, stressing the significance of user perception and interaction.",
      "A debate exists on the usefulness of animations, with proponents believing they improve user experience and detractors finding them disruptive.",
      "Key elements in UI design such as consistency, clarity, and accessibility are highlighted, urging developers to balance technical efficiency and user input for visually pleasing and functional interfaces."
    ],
    "points": 371,
    "commentCount": 168,
    "retryCount": 0,
    "time": 1709868426
  },
  {
    "id": 39630985,
    "title": "Beware: Potential Data Exposure from Private Links",
    "originLink": "https://vin01.github.io/piptagole/security-tools/soar/urlscan/hybrid-analysis/data-leaks/urlscan.io/cloudflare-radar%22/2024/03/07/url-database-leaks-private-urls.html",
    "originBody": "You can not simply publicly access private secure links, can you? Mar 7, 2024 turns out, you can even search for them with powerful search engines! Summary Popular malware/url analysis tools like urlscan.io, Hybrid Analysis and Cloudflare radar url scanner store a large number of links for intelligence gathering and sharing. It is however not so widely known that these services also store a large amount of private and sensitive links, thanks to: Sensitive links mistakenly submitted by users for scanning unaware that these are public information Misconfigured scanners and extensions submitting private links scanned from emails as public data So what are all these links you refer to? Files shared using cloud storage tools (e.g. Dropbox, iCLoud, Sync, Egnyte, Ionos Hidrive, AWS S3) Cloud connected NAS tools (e.g. Western Digital Mycloud) Corporate communication (e.g. Slido, Zoom, Onedrive, Airtable) Password reset links, Oauth sign-in links All these have one thing in common, the way they are so widely used allows anyone to access their services using a single private link containing random identifiers to ensure security of the links. Sometimes, they can be protected further using a password or passphrase, in those cases just having access to the links does not result in data exposure. Some screenshots I grabbed from urlscan.io before they were filtered out after I reached out to them (they were quick to respond): A lot of these submissions came from falconsandbox as shown in tags by urlscan.io, so I broadened my analysis to include Hybrid Analysis (owned by Crowdstrike) as well. Another new tool with potential to become more widely used and already containing some private links as public data is Cloudflare Radar. Some broad categories of sensitive content I came across: Private files including tax documents, invoices, photos, business communications Shared secrets using onetimesecret Smart home device recordings Meeting recordings stored in the cloud Who is responsible? Now that is a tough one to answer. From terms and conditions of use from Hybrid Analysis: Hybrid Analysis analyses, publishes, and shares Submitted Content from users as part of providing a cybersecurity community resource and is not responsible for the content or information which may incidentally appear in such submissions or be included in automatically-generated reports. From urlscan.io: You specifically acknowledge that urlscan shall not be liable for any user content or conduct. You are responsible for all content posted and activity that occurs under your account. As such, there does not seem to be any mechanism in place to review the existing content and flag/remove potentially sensitive links. Implementing it in an automated fashion might also not be trivial. As a security researcher, it is also hard to figure out the source of these links. I came across this wonderful analysis by Positive Security who focused on urlscan.io and used canary tokens to detect potential automated sources (security tools scanning emails for potentially malicious oinks), and also reached out via email to users. I was able to validate this behavior using canary links as well. We are Threat hunters! All your links are belong to us! urlscan Pro allows access for paid users/companies to a broader category of scans, including not just Public but also Unlisted scans. Unlisted means that the scan will not be visible on the public page or search results, but will be visible to customers of the urlscan Pro platform. We only admit customers to urlscan Pro which are either vetted security researchers or reputable corporations. Source Cortex-Analyzers from TheHive is an example I would like to outline. It explicitly uses public:on configuration for scans in urlscan.io analyzer, making the links appear as unlisted even if an account’s visibility in urlscan is set to Private. These can then easily be accessed by urlscan pro users and platforms based on that information. https://github.com/TheHive-Project/Cortex-Analyzers/blob/master/analyzers/Urlscan.io/urlscan.py#L28 I would expect much more sensitive information to be prone to leaks in this manner, although the data is not public but only visible to urlscan pro users. I hope they vet the users carefully. Counts for scans in each category from urlscan.io for last 24 hours: 398563 Public 328147 Unlisted 955432 Private I used canary tokens to establish: A link submitted to urlscan.io as unlisted, was accessed 12 times within 1 hour of submission A link submitted to hybrid-analysis.com via the API (not through the browser with explicit warning of them being public content), was accessed 10 times within 1 hour of submission Some IP addresses accessed both unique links submitted to these services simulataneously and use source IP anonymization services. a list of these IP addresses is here How to get sensitive links removed? Urlscan and Hybrid Analysis allow flagging the links to get them removed. https://urlscan.io/docs/faq/ https://www.hybrid-analysis.com/knowledge-base/removing-uploaded-sensitive-files For Hybrid Analysis, it is a bit more complex. Quoting from their knowledge base link above: All files submitted to the public Sandbox at https://www.hybrid-analysis.com/ will be searchable and available to the world. Even if the checkbox “Do not share my sample with the community” is checked, the screenshots and actual report will still be made available. The “do not share” portion only applies to the actual input sample. Conclusion This does leave me with mixed feelings. I am quite sure that this problem is here to stay. Perhaps a default of “keep the scans private” would work best, but would defeat the purpose of most of threat intelligence and analysis sharing practices in security community. Be mindful of scan visibility while using these services. Meanwhile, bounty hunters are using this already to report leaked data to companies directly ;) Hell, one of my submissions to a notable payment processing company even turned out to be a “Duplicate”, so I am definitely not the first one to notice this in the wild. Disclaimer If you choose to access some of these links/files from url databases, please be wary of actual malicious files and links. Some of these are just phishing attempts and may contain actual malware. Please use a sandbox environment. Helpful links urlscan.io’s SOAR spot: Chatty security tools leaking private data (2022) urlscan.io Search API Reference Falcon Sandbox Public API Cloudflare Radar URL Scanner",
    "commentLink": "https://news.ycombinator.com/item?id=39630985",
    "commentBody": "You cannot simply publicly access private secure links, can you? (vin01.github.io)345 points by vin10 17 hours agohidepastfavorite182 comments internetter 17 hours agoThe fundamental issue is that links without any form of access control are presumed private, simply because there is no public index of the available identifiers. Just last month, a story with a premise of discovering AWS account ids via buckets[0] did quite well on HN. The consensus established in the comments is that if you are relying on your account identifier being private as some form of security by obscurity, you are doing it wrong. The same concept applies here. This isn’t a novel security issue, this is just another method of dorking. [0]: https://news.ycombinator.com/item?id=39512896 reply ta1243 17 hours agoparentThe problem is links leak. In theory a 256 hex-character link (so 1024 bits) is near infinitely more secure than a 32 character username and 32 character password, as to guess it https://site.com/[256chars] As there's 2^1024 combinations. You'd never brute force it vs https://site,com/[32chars] with a password of [32chars] As there's 2^256 combinations. Again you can't brute force it, but it's more likely than the 2^1024 combinations. Imagine it's https://site,com/[32chars][32chars] instead. But while guessing the former is harder than the latter, URLs leak a lot, far more than passwords. reply internetter 17 hours agorootparentDorking is the technique of using public search engine indexes to uncover information that is presumed to be private. It has been used to uncover webcams, credit card numbers, confidential documents, and even spies. The problem is the website administers who are encoding authentication tokens into URL state, not the naive crawlers that find them. reply thayne 6 hours agorootparentThat isn't an inherent problem with having a secret in the url. The problem is the url was leaked somewhere where it could get indexed. And sometimes it isn't practical to require a POST request or a cookie. And the risk of a url leaking can be greatly mitigated if the url is only valid for a short period of time. reply ta1243 2 minutes agorootparent> That isn't an inherent problem with having a secret in the url. The problem is the url was leaked somewhere where it could get indexed. Technically you're right -- after all sending an authentication as a separate header doesn't make any difference. GET /endpoint/?Auth=token or GET /endpoint Auth: token Sends the same data over the wire. However software treats URLs differently to headers. They sit in browser histories, server logs, get parsed by MITM firewalls, mined by browser extensions, etc using https://user:pass@site.com/endpoint or https://auth:token@site.com/endpoint Would be better than https://site.com/endpoint/user/pass or https://site.com/endpoint/?auth=token As the former is less likely to be stored, either on the client or on the server. I don't do front end (or backend authentication -- I just rely on x509 client certs or oidc and the web server passes the validated username) layer8 14 hours agorootparentprevI wonder if there would be a way to tag such URLs in a machine-recognizable, but not text-searchable way. (E.g. take every fifth byte in the URL from after the authority part, and have those bytes be a particular form of hash of the remaining bytes.) Meaning that crawlers and tools in TFA would have a standardized way to recognize when a URL is meant to be private, and thus could filter them out from public searches. Of course, being recognizable in that way may add new risks. reply internetter 14 hours agorootparentWe already have a solution to this. It’s called not including authentication information within URLs Even if search engines knew to include it, would every insecure place a user put a link know it? Bad actors with their own indexes certainly wouldn’t care reply layer8 13 hours agorootparentHow do you implement password-reset links otherwise? I mean, those should be short-lived, but still. reply andersa 13 hours agorootparentYou could send the user a code that they must copy paste onto the page rather than sending them a link. reply vmfunction 9 hours agorootparentHopefully using POST not GET. The GET links get logged in the HTTP server most of time. Just another great way to store your 'security credential' in plain text. Logs gets zipped and archive. Good luck with any security measure. reply andersa 8 hours agorootparentI mean of course the idea was to put it in a form that is sent using POST, but even then, it's a single-use reset code so once it shows in the log it's worthless. reply fullspectrumdev 7 hours agorootparentThis makes a large assumption about application logic that is often incorrect. t. security auditor/researcher. reply rapind 3 hours agorootparentIt certainly does. Security usually comes at the cost of convenience and can incur confusion. In this example, where best practice may be to use one time tokens, you will end up with users who click on the secure link again (from their email) in the future to access the secure site and they’ll be frustrated when they have to go through the secure link generation dance again. Of course you can mitigate this with sessions / cookies, but that is also a security compromise and not device portable. It’s easy to say that these are minor uxp concerns, but enforcing a high level of security may have a significant user cost depending on your demographic. I have a demographic that skews older and non technical and they are pretty loud when they complain about this stuff… meanwhile they are also more likely to reuse passwords and forward emails with secure links in them! reply conductr 44 minutes agorootparentSome people will always find something to complain about. I feel like it’s completely reasonable to give a “sorry this link was only valid for 5 minutes and is now expired, request a new code here” message. State it in the email that originally contained the link and state it again on the page when they click it afterwards. This is incredibly common practice and very unlikely to be the first time someone has seen this workflow. If they want to complain further, direct them to a password manager and remind them there’s probably one built into their browser already reply hnlmorg 10 hours agorootparentprevAs you said, short lived codes. And the codes don’t contain any PII. So even if the link does get indexed, it’s meaningless and useless. reply dmurray 13 hours agorootparentprevAlso, it would allow bad actors to just opt out of malware scans - the main vector whereby these insecure URLs were leaked. reply fullspectrumdev 7 hours agorootparentSo there was an interesting vector a while back where some email firewalls would reliably click on any link sent to them that was abused by spammers. Spammers would sign up for services that required a click on a link using blabla@domainusingsuchservice The services bots to check phishing would reliably click on the link, rendering the account creation valid. One particularly exploitable vendor for getting such links clicked was one that shares the name with a predatory fish that also has a song about it :) reply rkagerer 4 hours agorootparentSharkGate? Why coy about naming them? reply reaperman 3 hours agorootparentBarracuda. And for plausible deniability so they don’t have as much of a chance of catching a libel suit. Not sure how necessary or effective that is, but I do understand the motivation. reply tsimionescu 11 hours agorootparentprevActually, there are cases where this is more or less unavoidable. For example, if you want a web socket server that is accessible from a browser, you need authentication, and can't rely on cookies, the only option is to encode the Auth information in the URL (since browsers don't allow custom headers in the initial HTTP request for negotiating a web socket). reply zer00eyz 11 hours agorootparentAuthentication: Identify yourself Authorization: Can you use this service. Access Control/Tokenization: How long can this service be used for. I swipe my badge on the card reader. The lock unlocks. Should we leave a handy door stopper or 2x4 there, so you can just leave it propped open? Or should we have tokens that expire in a reasonable time frame.. say a block of ice (in our door metaphor) so it disappears at some point in future? Nonce tokens have been a well understood pattern for a long time... Its not that these things are unavoidable its that security isnt first principal, or easy to embed due to issues of design. reply bigiain 10 hours agorootparent> Or should we have tokens that expire in a reasonable time frame. And that are single-use. (Your password reset \"magic link\" should expire quickly, but needs a long enough window to allow for slow mail transport. But once it's used the first time, it should be revoked so it cannot be used again even inside that timeout window.) reply skissane 11 hours agorootparentprev> the only option is to encode the Auth information in the URL (since browsers don't allow custom headers in the initial HTTP request for negotiating a web socket). Put a timestamp in the token and sign it with a private key, so that the token expires after a defined time period. If the URL is only valid for the next five minutes, the odds that the URL will leak and be exploited in that five minute window is very low reply bigiain 10 hours agorootparentprevYeah - that's just red-flagging \"interesting\" urls to people running greyhat and blackhat crawlers. reply loa_in_ 14 hours agorootparentprevWe already have robots.txt in theory. reply layer8 13 hours agorootparentI didn’t think robots.txt would be applicable to URLs being copied around, but actually it might be, good point. Though again, collecting that robots.txt information could make it easier to search for such URLs. reply shkkmo 16 hours agorootparentprevIt can be OK to put authentication tokens in urls, but those tokens need to (at a bare minimum) have short expirations. reply knome 15 hours agorootparent>It can be OK to put authentication tokens in urls When would this ever be necessary? URL session tokens have been a bad idea ever since they first appeared. The only things even near to auth tokens I can reasonably see stuffed into a URL are password reset and email confirmation tokens sent to email for one time short expiration use. Outside of that, I don't see any reason for it. reply albert_e 15 hours agorootparent\"presigned\" URLs[1] are a pretty standard and recommended way of providing users access to upload/download content to Amazon S3 buckets without needing other forms of authentication like IAM credential pair, or STS token, etc Web Applications do utilize this pattern very frequently But as noted i previous comment these do have short expiry times (configurable) so that there is no permanent or long-term risk on the lines of the OP article [1]: https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-... reply vin10 14 hours agorootparentYou are right about short expiry times but another catch here is that if pre-signed URLs are being leaked in an automated fashion, these services also keep the downloaded content from these URLs around. I found various such examples where links no longer work, but PDFs downloaded from pre-signed URLs were still stored by scanning services. From https://urlscan.io/blog/2022/07/11/urlscan-pro-product-updat... > In the process of scanning websites, urlscan.io will sometimes encounter file downloads triggered by the website. If we are able to successfully download the file, we will store it, hash it and make it available for downloading by our customers. reply couchand 9 hours agorootparentIndeed, the only valid operation with the magic URL is exchanging the URL-based token with something else (your PDF, a session token, etc.) and then expiring the URL, so by the time the scanner gets around to it the original URL is invalid. reply ta1243 1 minute agorootparentThat seems ripe for race condition class problems. knome 11 hours agorootparentprevInteresting. I haven't built on s3, and if I did my first instinct would probably have been to gate things through a website. Thanks for sharing your knowledge in that area. reply dylanowen 15 hours agorootparentprevThey're useful for images when you can't use cookies and want the client to easily be able to embed them. reply FrustratedMonky 12 hours agorootparentprev\"public search engine indexes\" Then it should be the search engine at fault. If you leave your house unlocked is one thing. If there is a company trying everyone's doors, then posting a sign in the yard \"this house is unlocked\", has to account for something. reply lmm 11 hours agorootparentA plain URL is an open door not a closed one. Most websites are public and expected to be public. reply FrustratedMonky 11 hours agorootparentIsn't that the point of the post? There are URL's that are out there 'as-if' public, but really should be private. And some people argue they should be treated as private, even if it is just a plain URL and public. reply lmm 10 hours agorootparentYou can't blame the search engine for indexing plain URLs. Listing a closed-but-unlocked door is a bad analogy. reply noahtallen 11 hours agorootparentprevYou can easily rate-limit an authentication attempt, to make brute-forcing account access practically impossible, even for a relatively insecure passwords. How would you do that for the URLs? 5 requests to site.com/[256chars] which all 404 block your IP because you don't have a real link? I guess the security is relying on the fact that only a very a small percentage of the total possible links would be used? Though the likelihood of randomly guessing a link is the same as the % of addressable links used. reply ummonk 10 hours agorootparentI don’t think you realize how exponentially large the possible combinations of 256 characters would be. In fact it doesn’t need to be anywhere near 256 characters. 64 hexadecimal characters would suffice. reply 4death4 16 hours agorootparentprevPasswords are always private. Links are only sometimes private. reply QuinnyPig 16 hours agorootparentYup. There’s a reason putting credentials into url parameters is considered dangerous. reply bachmeier 16 hours agorootparentprevWell-chosen passwords stored properly are always private. Passwords also tend to have much longer lifetimes than links. reply ablob 11 hours agorootparentprevWhich alphabet did you take as a basis to reach 2^256 combinations? reply hiddencost 13 hours agorootparentprevNo. In theory they are both totally insecure. reply Y_Y 14 hours agorootparentprev> site-comma-com Did you do that just to upset me? reply masom 15 hours agorootparentprevYou won't find a specific link, but at some point if you generate millions of urls the 1024 bits will start to return values pretty quick through bruteforce. The one link won't be found quickly, but a bunch of links will. You just need to fetch all possibilities and you'll get data. reply duskwuff 15 hours agorootparent> You won't find a specific link, but at some point if you generate millions of urls the 1024 bits will start to return values pretty quick through bruteforce. Not even close. 1024 bits is a really, really big address space. For the sake of argument and round numbers, let's say that there are 4.2 billion (2^32) valid URLs. That means that one out of every 2^992 randomly generated URLs is valid. Even if you guessed billions of URLs every second, the expected time to come up with a valid one (~2^960 seconds) is still many orders of magnitude greater than the age of the universe (~2^59 seconds). reply charleslmunger 15 hours agorootparentprevI'm not sure your math checks out. With 1024 bits of entropy and, say, 1 trillion valid links, your chances of any one link being valid are 1/2^984 So test a million links - your probability of finding a real one is (1-1/2^984)^1000000. That's around 1/10^291 chance of hitting a valid URL with a million tries. Even if you avoid ever checking the same URL twice it will still take you an impractical amount of time. reply mbrumlow 14 hours agorootparentAll this is fine and dandy until your link shows up in a log at /logs. reply ummonk 10 hours agorootparentThe same can almost as easily happen with user-submitted passwords. reply blueflow 15 hours agorootparentprev1024 bits seems a bit too much for the birthday problem to be a thing. I looked at [1] to do the calculation but (2^1024)! is a number too large for any of my tools. If someone has a math shortcut to test this idea properly... [1] https://en.wikipedia.org/wiki/Birthday_problem#Calculating_t... reply Dylan16807 3 hours agorootparentThis isn't the birthday problem. That would be the chance of two random links overlapping. The birthday problem scales with n^2, while trying to guess links scales with m * n, number of guesses multiplied by number of links. (Well, before you apply the logistic taper to it. So you wanted an approximation? There you go. Until you get the chance of a hit to be quite high, it's basically equal to guesses * valid links / 2^1024.) reply ta1243 11 minutes agorootparentThe chance is less than guessing a random 128 bit username and random 128 bit password. And then guessing a completely different username and password on the very next go. You'd get far more return on investment breaking bitcoin wallets. 2^1024 is 10^308 Lets say there are 12 billion links per person, and 8 billion people. That's 100 billion billion, or 10^20 links. 10^20 / 10^308 is zero. Lets say you can test 10 trillion links a second, and started when the big bang happened, you'll have tested 10^30 links so far. The number of links you'll have found so far is zero. reply saagarjha 15 hours agorootparentprevStirling’s approximation? reply eknkc 14 hours agorootparentprevWe call 128 bit random data “universally” unique ids. 1024 bits won’t ever get close to returning any random hits. reply bo1024 16 hours agoparentprevThere's probably details I'm missing, but I think the fundamental issue is that \"private\" messages between people are presumed private, but actually the platforms we use to send messages do read those messages and access links in them. (I mean messages in a very broad sense, including emails, DMs, pasted links in docs, etc.) reply internetter 16 hours agorootparentURL scanners are not scanning links contained within platforms that require access control. They haven't guessed your password, and to my knowledge no communications platform is feeding all links behind authentication into one of these public URL scanning databases. As the article acknowledged in the beginning, these links are either exposed as the result of deliberate user action, or misconfigured extensions (that, I might add, are suffering from this exact same misconception). If the actual websites are configured to not use the URL as the authentication state, all this would be avoided reply tobyjsullivan 16 hours agorootparentThe suggestion (in both the article and the parent) is that the platforms themselves are submitting URLs. For example, if I send a link in Discord[0] DM, it might show the recipient a message like “warning: this link is malicious”. How does it know that? It submitted the url to one of these services without your explicit consent. [0] Discord is a hypothetical example. I don’t know if they have this feature. But an increasing number of platforms do. reply internetter 16 hours agorootparentWhere in the article does it suggest this? The two bullet points at the very top of TFA is what I cited to discredit this notion, I read it again and still haven't found anything suggesting the communication platforms are submitting this themselves. reply bombcar 15 hours agorootparentFalcon Sandbox is explicitly mentioned - which is a middleware that can be installed on various communication platforms (usually enterprise): https://www.crowdstrike.com/products/threat-intelligence/fal... Microsoft has \"safe links\": https://learn.microsoft.com/en-us/microsoft-365/security/off... - Chrome has its own thing, but there are also tons of additional hand-rolled similar features. My main annoyance is when they kill a one-time use URL. reply anonymousDan 12 hours agorootparentDo you know if safe links is guilty of the issue in the OP? reply bombcar 11 hours agorootparentI suspect not because Microsoft is using their own internal system. However, it likely exposes the content internally to Microsoft. They do 100% break Salesforce password reset links, which is a major PITA. reply tobyjsullivan 12 hours agorootparentprevI thought I read it in the article but I may have unconsciously extrapolated from and/or misread this part: “I came across this wonderful analysis by Positive Security[0] who focused on urlscan.io and used canary tokens to detect potential automated sources (security tools scanning emails for potentially malicious [links])” I don’t see any mention of messaging platforms generally. It only mentions email and does not suggest who might be operating the tooling (vendor or end users). So I seem to have miscredited that idea. [0] https://positive.security/blog/urlscan-data-leaks reply nightpool 13 hours agorootparentprevThe article says \"Misconfigured scanners\". Many, many enterprise communication tools have such a scanner, and if your IT team is using the free plan of whatever url scan tool they signed up for, it's a good bet that these links may end up being public. reply mikepurvis 16 hours agoparentprevBit of a tangent, but I was recently advised by a consultant that pushing private Nix closures to a publicly-accessible S3 bucket was fine since each NAR file has a giant hash in the name. I didn't feel comfortable with it so we ended up going a different route, but I've continued to think about that since how different is it really to have the \"secret\" be in the URL vs in a token you submit as part of the request for the URL? And I think for me it comes down to the fact that the tokens can be issued on a per-customer basis, and access logs can be monitored to watch for suspicious behaviour and revoke accordingly. Also, as others have mentioned, there's just a different mindset around how much it matters that the list of names of files be kept a secret. On the scale of things Amazon might randomly screw up, accidentally listing the filenames sitting in your public bucket sounds pretty low on the priority list since 99% of their users wouldn't care. reply johnmaguire 16 hours agorootparent> how different is it really to have the \"secret\" be in the URL vs in a token you submit as part of the request for the URL? I'm not sure I grok this. Do you mean, for example, sending a token in the POST body, or as a cookie / other header? One disadvantage to having a secret in the URL, versus in a header or body, is that it can appear in web service logs, unless you use a URI fragment. Even then, the URL is visible to the user, and will live in their history and URL bar - from which they may copy and paste it elsewhere. reply mikepurvis 11 hours agorootparentIn this case it's package archives, so they're never accessed from a browser, only from the Nix daemon for binary substitution [1]: https://nixos.wiki/wiki/Binary_Cache reply cxr 6 hours agorootparentprev> I've continued to think about that since how different is it really to have the \"secret\" be in the URL vs in a token you submit as part of the request for the URL Extremely different. The former depends on the existence of a contract about URL privacy (not to mention third parties actually adhering to it) when no such contract exists. Any design for an auth/auth mechanism that depends on private links is inherently broken. The very phrase \"private link\" is an oxymoron. > I am not sure why you think that having an obscure URI format will somehow give you a secure call (whatever that means). Identifiers are public information.reply nmadden 15 hours agorootparentprevI wrote about putting secrets in URLs a few years ago: https://neilmadden.blog/2019/01/16/can-you-ever-safely-inclu... reply Sn0wCoder 11 hours agorootparentQuestion in the Waterken-Key flow with token in the URL fragment the URL looks like HTTPS www.example.com/APP/#mhbqcmmva5ja3 – but in the diagram its hitting example.com/API/#mhbqcmmva5ja3 Is this a type-o OR are we mapping APP to API with the proxy so the user thinks they are going to the APP with their Key. Or does the browser do us for us automatically when it sees app in the URL and then stores the key in window.location.hash. I am confused and might just find the answer on Google but since you appear to be the author maybe you can answer the question here. reply nmadden 2 hours agorootparentOops, that’s a typo. reply bachmeier 16 hours agoparentprev> The fundamental issue is that links without any form of access control are presumed private, simply because there is no public index of the available identifiers. Is there a difference between a private link containing a password and a link taking you to a site where you input the password? Bitwarden Send gives a link that you can hand out to others. It has # followed by a long random string. I'd like to know if there are security issues, because I use it regularly. At least with the link, I can kill it, and I can automatically have it die after a few days. Passwords generally don't work that way. reply PeterisP 14 hours agorootparentYes, the difference is in what all our tools and infrastructure presume to be more or less sensitive. Sending a GET request to a site for the password-input screen and POST'ing the password will get very different treatement than sending the same amount of \"authorization bits\" in the URL; in the first case, your browser won't store the secret in the history, the webserver and reverse proxy won't include it in their logs, various tools won't consider it appropriate to cache, etc, etc. Our software infrastructure is built on an assumption that URLs aren't really sensitive, not like form content, and so they get far more sloppy treatment in many places. If the secret URL is short-lived or preferably single-use-only (as e.g. many password reset links) then that's not an issue, but if you want to keep something secret long-term, then using it in an URL means it's very likely to get placed in various places which don't really try to keep things secret. reply koolba 16 hours agorootparentprevIf there’s a live redirect at least there’s the option to revoke the access if the otherwise public link is leaked. I think that’s what sites like DocuSign do with their public links. You can always regenerate it and have it resent to the intended recipients email, but it expires after some fixed period of time to prevent it from being public forever. reply 7952 14 hours agorootparentprevThere is a difference in that people intuitively know that entering passwords gives access. Also, it may be different legally as the user could reasonably be expected to know that they are not supposed to access something. reply bachmeier 9 hours agorootparent> There is a difference in that people intuitively know that entering passwords gives access. This is a valid argument. However, I'd say that there are two standard practices with links that are a big advantage: giving them a short life, and generating extremely hard to guess URLs. I was a Lastpass customer before their security problems came out. I had many passwords that I made years ago but don't use the service any longer. I moved more into the URL camp at that time. Who knows how many passwords I made 15 or 20 years ago that today are no longer secure. reply XorNot 11 hours agoparentprevWorked for a company which ran into an S3 bucket naming collision when working with a client - turns out that both sides decided hyphenated-company-name was a good S3 bucket name (my company lost that race obviously). One of those little informative pieces where everytime I do AWS now all the bucket names are usually named -. If it's really meant to be private then you encrypt the project-name too and provide a script to list buckets with \"friendly\" names. There's always a weird tradeoff with hosted services where technically the perfect thing (totally random identifiers) is too likely to mostly be an operational burden compared to the imperfect thing (descriptive names). reply cj 10 hours agorootparentWhat would encrypting the project name accomplish? Typically if you’re trying to secure a S3 bucket you’ll do that via bucket settings. Many years ago you had to jump through hoops to get things private, but these days there’s a big easy button to make a bucket inaccessible publicly. reply XorNot 6 hours agorootparentThe point is that in some cases the name of the project might itself be considered sensitive in some way, so preventing people testing bucket names by trying to create them helps prevent it, but doesn't completely lock you out of being able to associate the bucket back to its internal name, and allows the names to be deterministic internally - i.e. someone spinning up a test environment is still getting everything marked appropriately, deterministically, and uniquely. reply fddrdplktrew 11 hours agoparentprevlegend. reply r2b2 15 hours agoprevTo create private shareable links, store the private part in the hash of the URL. The hash is not transmitted in DNS queries or HTTP requests. Ex. When links.com?token= is visited, that link will be transmitted and potentially saved (search parameters included) by intermediaries like Cloud Flare. Ex. When links.com# is visited, the hash portion will not leave the browser. Note: It's often nice to work with data in the hash portion by encoding it as a URL Safe Base64 string. (aka. JS Object ↔ JSON String ↔ URL Safe Base 64 String). reply jmholla 13 hours agoparent> Ex. When links.com?token= is visited, that link will be transmitted and potentially saved (search parameters included) by intermediaries like Cloud Flare. Note: When over HTTPS, the parameter string (and path) is encrypted so the intermediaries in question need to be able to decrypt your traffic to read that secret. Everything else is right. Just wanted to provide some nuance. reply r2b2 12 hours agorootparentGood to point out. This distinction is especially important to keep in mind when thinking about when and/or who terminates TLS/SSL for your service, and any relevant threat models the service might have for the portion of the HTTP request after terminattion. reply mschuster91 11 hours agorootparentprevCloudflare, Akamai, AWS Cloudfront are all legitimate intermediaries. reply phyzome 12 hours agoparentprevHuge qualifier: Even otherwise benign Javascript running on that page can pass the fragment anywhere on the internet. Putting stuff in the fragment helps, but it's not perfect. And I don't just mean this in an ideal sense -- I've actually seen private tokens leak from the fragment this way multiple times. reply andix 13 hours agoparentprevIs there a feature of DNS I'm unaware of, that queries more than just the domain part? https://example.com?token= should only lead to a DNS query with \"example.com\". reply erikerikson 12 hours agorootparentThe problem isn't DNS in GP. DNS will happily supply the IP address for a CDN. The HTTP[S] request will thereafter be sent by the caller to the CDN (in the case of CloudFlare, Akamai, etc.) where it will be handled and potentially logged before the result is retrieved from the cache or the configured origin (i.e. backing server). reply andix 10 hours agorootparentThis sounds like a big security flaw in the system that uses access links. Secrets should not be logged (in most cases). When opening a Dropbox/GoogleDocs/OneDrive link, I expect the application not to route them through potentially unsafe CDNs. reply r2b2 12 hours agorootparentprevCorrect, DNS only queries the hostname portion of the URL. Maybe my attempt to be thorough – by making note of DNS along side HTTP since it's part of the browser ↔ network ↔ server request diagram – was too thorough. reply klabb3 14 hours agoparentprevThanks, finally some thoughts about how to solve the issue. In particular, email based login/account reset is the main important use case I can think of. Do bots that follow links in emails (for whatever reason) execute JS? Is there a risk they activate the thing with a JS induced POST? reply r2b2 13 hours agorootparentTo somewhat mitigate the link-loading bot issue, the link can land on a \"confirm sign in\" page with a button the user must click to trigger the POST request that completes authentication. Another way to mitigate this issue is to store a secret in the browser that initiated the link-request (Ex. local storage). However, this can easily break in situations like private mode, where a new tab/window is opened without access to the same session storage. An alternative to the in-browser-secret, is doing a browser fingerprint match. If the browser that opens the link doesn't match the fingerprint of the browser that requested the link, then fail authentication. This also has pitfalls. Unfortunately, if your threat model requires blocking bots that click too, your likely stuck adding some semblance of a second factor (pin/password, bio metric, hardware key, etc.). In any case, when using link-only authentication, best to at least put sensitive user operations (payments, PII, etc.) behind a second factor at the time of operation. reply klabb3 11 hours agorootparent> a button the user must click Makes sense. No action until the user clicks something on the page. One extra step but better than having “helpful bots” wreak havoc. > to store a secret in the browser […] is doing a browser fingerprint match I get the idea but I really dislike this. Assuming the user will use the same device or browser is an anti-pattern that causes problems with people especially while crossing the mobile-desktop boundary. Generally any web functionality shouldn’t be browser dependent. Especially hidden state like that.. reply r2b2 4 hours agorootparentI agree, better to use an additional factor than fingerprinting. reply 369548684892826 13 hours agorootparentprevYes, I've seen this bot JS problem, it does happen. reply loginatnine 12 hours agoparentprevIt's called a fragment FYI! reply shiomiru 11 hours agorootparentHowever, window.location calls it \"hash\". (Also, the query string is \"search\". I wonder why Netscape named them this way...) reply nightpool 12 hours agoparentprevThe secret is still stored in the browser's history DB in this case, which may be unencrypted (I believe it is for Chrome on Windows last I checked). The cookie DB on the other hand I think is always encrypted using the OS's TPM so it's harder for malicious programs to crack reply r2b2 10 hours agorootparentYes, adding max-use counts and expiration dates to links can mitigate against some browser-history snooping. However, if your browser history is compromised you probably have an even bigger problem... reply eterm 14 hours agoparentprevIf it doesn't leave the browser, how would the server know to serve the private content? reply jadengeller 14 hours agorootparentClient web app makes POST request. It leaves browser, but not in URL reply Gigachad 7 hours agorootparentThat only works if you can run JavaScript. If you want to download a file with curl for example it fails. reply rpigab 15 hours agoprevLinks that are not part of a fast redirect loop will be copied and pasted to be shared because that's what URLs are for, they're universal, they facilitate access to a resource available on a protocol. Access control on anything that is not short-lived must be done outside of the url. When you share links on any channel that is not e2ee, the first agent to access that url is not the person you're sending it to, it is the channel's service, it can be legitimate like Bitwarden looking for favicons to enhance UX, or malicious like FB Messenger crawler that wants to know more about what you are sharing in private messages. Tools like these scanners won't get better UX, because if you explicitly tell users that the scans are public, some of them will think twice about using the service, and this is bad for business, wether they're using it for free or paying a pro license. reply QuercusMax 16 hours agoprevI've always been a bit suspicious of infinite-use \"private\" links. It's just security thru obscurity. At least when you share a Google doc or something there's an option that explicitly says \"anyone with the URL can access this\". Any systems I've built that need this type of thing have used Signed URLs with a short lifetime - usually only a few minutes. And the URLs are generally an implementation detail that's not directly shown to the user (although they can probably see them in the browser debug view). reply empath-nirvana 16 hours agoparentThere's functionally no difference between a private link and a link protected by a username and password or an api key, as long as the key space is large enough. reply rfoo 16 hours agorootparentMost of developers are aware that username or password are PII and if they log it they are likely to get fired. Meanwhile our HTTP servers happily log every URI it received in access logs. Oh, and if you ever send a link in non E2EE messenger it's likely their server generated the link preview for you. reply nkrisc 16 hours agorootparentprevThere’s a big difference. The latter requires information not contained in the URL to access the information. reply deathanatos 14 hours agorootparent> Here's the URL to the thing: https://example.com/a/url?secret=hunter2 This is indexable by search engines. > Here's the URL to the thing: https://example.com/a/url and the password is \"hunter2\". This is indexable by search engines. Yes, the latter is marginally harder, but you're still leaning on security through obscurity, here. The number of times I have had \"we need to securely transmit this data!\" end with exactly or something equivalent to emailing an encrypted ZIP with the password in the body of the email (or sometimes, some other insecure channel…) … reply pests 13 hours agorootparentRight, but you settled on the answer as well. You must communicate the password via a different medium, which is impossible with links. reply nkrisc 14 hours agorootparentprevSure if you’re comparing worst case of one to best case of the other it’s functionally similar, but if the password is strong and handled properly then they are not functionally similar at all. reply ironmagma 15 hours agorootparentprevThat's not a fundamental difference but a difference of convention. A lot of us have been in the convention long enough that it seems like a fundamental. reply kriops 16 hours agorootparentprevThere is a big difference in how the browser treats the information, depending on how you provide it. Secrets in URLs leak more easily. reply vel0city 16 hours agorootparentprevThere's one big functional difference. People don't normally have their username and password or API key directly in the URL. Example 1: Alice wants Bob to see CoolDocument. Alice generates a URL that has the snowflake in the URL and gives it to Bob. Eve manages to see the chat, and can now access the document. Example 2: Alice wants Bob to see CoolDocument. Alice clicks \"Share with Bob\" in the app, grabs the URL to the document with no authentication encoded within and sends it to Bob. Bob clicks the link, is prompted to login, Bob sees the document. Eve manages to see the chat, follows the link, but is unable to login and thus cannot see the document. Later, Alice wants to revoke Bob's access to the document. Lots of platforms don't offer great tools to revoke individual generated share URLs, so it can be challenging to revoke Bob's access without potentially cutting off other people's access in Example 1, as that link might have been shared with multiple people. In example 2, Alice just removes Bob's access to the doucment and now his login doesn't have permissions to see it. Granted, better link management tools could sovle this, but it often seems like these snowflake systems don't really expose a lot of control over multiple share links. reply Dylan16807 3 hours agorootparentExample 2 sounds like a pretty big pain if I can't go directly from Bob's chat account to his document account. Which is the case the vast majority of the time. reply OtherShrezzing 14 hours agorootparentprevThere's at least one critical functional difference: The URL stays in the browser's history after it's been visited. reply ses1984 16 hours agorootparentprevYou can’t revoke an individual user’s access to a hard to guess link. reply colecut 16 hours agorootparentYou can if it's one link per user reply ses1984 16 hours agorootparentTrue but if you’re generating one link per user, at what point do you lift up your head and wonder if it wouldn’t be easier to just use authentication? reply jddj 14 hours agorootparentThe friction that semi-private links remove is that the recipient doesn't need an account for your service. Any tradeoffs should be viewed in that context. reply anonymousDan 12 hours agorootparentI like how google docs does it. You can specify the email of a user allowed to access the link (doesn't need to be gmail). When they click it they will be told to check for a validation email containing a link to the actual document. reply vel0city 16 hours agorootparentprevLots of platforms I've used with these public share links don't really support multiple share links, and if they do the management of it is pretty miserable. Clicking share multiple times just gives the same link. reply LaGrange 16 hours agorootparentprevI mean, there's a functional difference if your email client will try to protect you by submitting the URL to a public database. Which is incredible and mind-boggling, but also apparently the world we live in. reply voiper1 14 hours agoparentprev>At least when you share a Google doc or something there's an option that explicitly says \"anyone with the URL can access this\". Unfortunately, it's based on the document ID, so you can't re-enable access with a new URL. reply nightpool 12 hours agorootparentNot true, as you may have heard they closed this loophole in 2021 by adding a \"resource key\" (that can be rotated) to every shared URL: https://9to5google.com/2021/07/28/google-drive-security-upda.... reply Terr_ 13 hours agoprevA workaround for this \"email-based authentication\" problem (without going to a full \"make an account with a password\" step) is to use temporary one-time codes, so that it doesn't matter if the URL gets accidentally shared. 1. User visits \"private\" link (Or even a public link where they re-enter their e-mail.) 2. Site e-mails user again with time-limited single-use code. 3. User enters temporary code to confirm ownership of e-mail. 4. Flow proceeds (e.g. with HTTP cookies/session data) with reasonable certainty that the e-mail account owner is involved. reply scblock 17 hours agoprevWhen it comes to the internet if something like this is not protected by anything more than a random string in a URL then they aren't really private. Same story with all the internet connected web cams you can find if you go looking. I thought we knew this already. Why doesn't the \"Who is responsible\" section even mention this? reply AnotherGoodName 16 hours agoparentSuch links are very useful in an 'it's OK to have security match the use case' type of way. You don't need maximum security for everything. You just want a barrier to widespread sharing in some cases. As an example i hit 'create link share' on a photo in my photo gallery and send someone the link to that photo. I don't want them to have to enter a password. I want the link to show the photo. It's ok for the link to do this. One of the examples they have here is exactly that and it's fine for that use case. In terms of privacy fears the end user could re-share a screenshot at that point anyway even if there was a login. The security matches the use case. The user now has a link to a photo, they could reshare but i trust they won't intentionally do this. The big issue here isn't the links imho. It's the security analysis tools scanning all links a user received via email and making them available to other users in that community. That's more re-sharing than i intended when i sent someone a photo. reply nonrandomstring 15 hours agorootparent> Such links are very useful in an 'it's OK to have security match the use case' I think you give the most sensible summary. It's about \"appropriate and proportional\" security for the ease of use trade-off. > the user now has a link to a photo, they could reshare but i trust they won't intentionally do this. Time limits are something missing from most applications to create ephemeral links. Ideally you'd want to choose from something like 1 hour, 12 hours, 24 hours, 72 hours... Just resend if they miss the message and it expires. A good trick is to set a cron job on your VPS to clear /www/tmp/ at midnight every other day. > The big issue here isn't the links imho. It's the security analysis tools scanning all links a user received via email You have to consider anything sent to a recipient of Gmail, Microsoft, Apple - any of the commercial providers - to be immediately compromised. If sending between private domains on unencrypted email then it's immediately compromised by your friendly local intelligence agency. If using PGP or am E2E chat app, assume it _will_ be compromised at the end point eventually, so use an ephemeral link. reply marcosdumay 14 hours agorootparentprevThe situation is greatly improved if you make the link short-lived and if you put the non-public data in a region of the URL that expects non-public data, like in the password, as in \"https://anonymous:32_chars_hash@myphotolibrary.example.com/u...\". reply amanda99 16 hours agoprevOff topic: but that links to cloudflare radar which apparently mines data from 1.1.1.1. I was under the impression that 1.1.1.1 did not use user data for any purposes? reply kube-system 16 hours agoparentCF doesn't sell it or use it for marketing, but the entire way they even got the addresses was because APNIC wanted to study the garbage traffic to 1.1.1.1. reply amanda99 14 hours agorootparent> CF doesn't sell it or use it for marketing Any source for this? Do you work there? I checked their docs and they say they don't \"mine user data\", so I wouldn't trust anything they say, at least outside legal documents. reply kube-system 14 hours agorootparenthttps://1.1.1.1/dns/ > We will never sell your data or use it to target ads. https://developers.cloudflare.com/1.1.1.1/privacy/public-dns... > Cloudflare will not sell or share Public Resolver users’ personal data with third parties or use personal data from the Public Resolver to target any user with advertisements. There's a lot of transparency on that page in particular, down to the lists of the fields in the logs. reply autoexec 9 hours agorootparentprev> so I wouldn't trust anything they say, at least outside legal documents I wouldn't trust them even if it were in legal docs. Companies have a long history of being perfectly fine with breaking the law when doing so is profitable, especially when they're likely to get little more than a slap on the wrist when caught, and the odds of being caught in the first place are slim. reply ttymck 16 hours agoprevZoom meeting links often have the password appended as a query parameter. Is this link a \"private secure\" link? Is the link without the password \"private secure\"? reply bombcar 15 hours agoparentIf the password is randomized for each meeting, the URL link is not so bad, as the meeting will be dead and gone by the time the URL appears elsewhere. But in reality, nobody actually cares and just wants a \"click to join\" that doesn't require fumbling around - but the previous \"just use the meeting ID\" was too easily guessed. reply runeb 13 hours agorootparentUnless its a recurring meeting reply dav43 8 hours agoprevA classic one that has a business built on this is pidgeonhole - literally private links for events with people hosting internal company events and users posing private sometimes confidential information. And even banks sign on to these platforms! reply victorbjorklund 16 hours agoprevCan someone smarter explain to me what is different between? 1) domain.com/login user: John password: 5 char random password 2) domain.com/12 char random url If we assume both either have the same bruteforce/rate limiting protection (or none at all). Why is 1 more safe than 2? reply koliber 16 hours agoparentFrom the information theory angle, there is no difference. In practice, there is. There is a difference between something-you-have secrets and something-you-know secrets. A UrL is something you have. It can be taken from you if you leave it somewhere accessible. Passwords are something-you-know and if managed well can not be taken (except for the lead pipe attack). There is also something-you-are, which includes retina and fingerprint scans. reply rkangel 16 hours agoparentprevThis article is the exact reason why. (1) Requires some out-of-band information to authenticate. Information that people are used to keeping safe. On the other hand the URLs in (2) are handled as URLs. URLs are often logged, recorded, shared, passed around. E.g. your work firewall logging the username and password you used to log into a service would obviously be bad, but logging URLs you've accessed would probably seems fine. [the latter case is just an example - the E2E guarantees of TLS mean that neither should be accessible] reply amanda99 16 hours agoparentprevTwo things: 1. \"Password\" is a magic word that makes people less likely to just paste it into anything. 2. Username + passwords are two separate pieces of information that are not normally copy-pasted at the same time or have a canonical way of being stored next to each other. reply victorbjorklund 16 hours agorootparent1) Make sense. 2) Not sure about that. If someone shares their password with someone else they probably share both the username/email and the password reply amanda99 16 hours agorootparentYes, people share usernames and passwords, but there's no single canonical string, like \"username=amanda99&password=hithere\". For example most of the time when I share user/pass combos, they are in separate messages on Signal. You type them into two different boxes, so you normally copy the username, then the password in separate actions. reply nightpool 12 hours agorootparentI mean, for HTTP Basic there literally is a single canonical string, and it's not uncommon to see people send you links like https://user:somepasswordhere@example.com. I think the arguments other commenters have made about logging, browser history storage, etc are more convincing reply wetpaste 15 hours agoparentprevIn the context of this article, it is that security scanning software that companies/users are using seem to be indexing some of the 12-char links out of emails which ends up in some cases on public scan. Additionally, if domain.com/12-char-password is requested without https, even if there is a redirect, that initial request went over the wire unencrypted and therefore could be MITM, whereas with a login page, there are more ways to guarantee that the password submit would only ever happen over https. reply ApolloFortyNine 12 hours agoparentprevI researched this a while ago when I was curious if you could put auth tokens as query params. One of the major issues is that many logging applications will log the full url somewhere, so now your logging 'passwords'. reply laurels-marts 11 hours agorootparentYou can definitely pass JWT as a query param (and often are in embedded scenarios) and no its not the same as logging passwords unless you literally place the password in the payload (which would be stupid). reply jarofgreen 15 hours agoparentprevAs well as what the others have said, various bits of software make the assumption that 1) may be private and to be careful with it and 2) isn't. eg Your web browser will automatically save any URLs to it's history for any user of the computer to see but will ask first before saving passwords. eg Any web proxies your traffic goes through or other software that's looking like virus scanners will probably log URLs but probably won't log form contents (yes HTTPS makes this one more complicated but still). reply munk-a 16 hours agoparentprevAssuming that 5 char password is done in a reasonable way then that data is not part of the publicly visible portion of the request that anyone along the chain of the communication can trivially eavesdrop. In a lot of cases that password even existing (even if there's no significant data there) will transform a request from a cacheable request into an uncacheable request so intermediate servers won't keep a copy of the response in case anyone else wants the document (there are other ways to do this but this will also force it to be the case). reply kube-system 16 hours agoparentprevThe difference is that people (and software that people write) often treat URLs differently than a password field. 12 characters might take X amount of time to brute force, but if you already have the 12 characters, that time drops to zero. reply hawski 13 hours agoparentprevYou can easily make a regex to filter out URLs. There is no universal regex (other than maybe costly LLM) to match the URL, the username and the password. reply sbr464 16 hours agoprevAll media/photos you upload to a private airtable.com app are public links. No authentication required if you know the url. reply andix 10 hours agoparentThere is a dilemma for web developers with images loaded from CDNs or APIs. Regulartags can't set an Authorization header with a token for the request, like you can do with fetch() for API requests. The only possibility is adding a token to the URL or by using cookie authentication. Cookie auth only works if the CDN is on the same domain, even a subdomain can be problematic in many cases. reply internetter 16 hours agoparentprevThis is actually fairly common for apps using CDNs – not just airtable. I agree it's potentially problematic reply blue_green_maps 15 hours agorootparentYes, this is the case for images uploaded through GitHub comments, I think. reply eddythompson80 10 hours agorootparentThat's not true. There is a JWT token in the url with about 5 minute expiration window. reply snthd 11 hours agoprev\"private secure links\" are indistinguishable from any other link. With HTTP auth links you know the password is a password, so these tools would know which part to hide from public display: > https://username:password@example.com/page reply jeroenhd 2 hours agoparentI think it's quite funny that the URL spec has a section dedicated to authentication, only for web devs to invent ways to pass authentication data in any way but using the built-in security mechanism. I know there are valid reasons (the \"are you sure you want to log in as usernam on example.com?\" prompt for example) but this is just one of the many ways web dev has built hacks upon hacks where implementing standards would've sufficed. See also: S3 vs WebDAV. reply boxed 16 hours agoprevOutlook.com leaks links to bing. At work it's a constant attack surface that I have to block by looking at the user agent string. Thankfully they are honest in the user agent! reply andix 13 hours agoprevA while ago I started to only send password protected links via email. Just with the plaintext password inside the email. This might seem absurd and unsafe on the first glance, but those kind of attacks it can safely prevent. Adding an expiration time is also a good idea, even if it is as long as a few months. reply kgeist 12 hours agoprevTried it with the local alternative to Google Disk. Oh my... Immediately found lots of private data, including photos of credit cars (with security codes), scans of IDs, passports... How do you report a site? reply godelski 15 hours agoprevThere's a clear UX problem here. If you submit a scan it doesn't tell you it is public. There can be a helpful fix: make clear that the scan is public! When submitting a scan it isn't clear, as the article shows. But you have the opportunity to also tell the user that it is public during the scan, which takes time. You also have the opportunity to tell them AFTER the scan is done. There should be a clear button to delist. urlscan.io does a bit better but the language is not quite clear that it means the scan is visible to the public. And the colors just blend in. If something isn't catching to your eye, it might as well be treated as invisible. If there is a way to easily misinterpret language, it will always be misinterpreted. if you have to scroll to find something, it'll never be found. reply heipei 13 hours agoparentThanks for your feedback. We show the Submit button on our front page as \"Public Scan\" to indicate that the scan results will be public. Once the scan has finished it will also contain the same colored banner that says \"Public Scan\". On each scan result page there is a \"Report\" button which will immediately de-list the scan result without any interaction from our side. If you have any ideas on how to make the experience more explicit I would be happy to hear it! reply godelski 13 hours agorootparentI understand, but that is not clear enough. \"Public scan\" can easily be misinterpreted. Honestly, when I looked at it, I didn't know what it meant. Just looked like idk maybe a mistranslation or something? Is it a scan for the public? Is the scanning done in public? Are the results public? Who knows. Remember that I'm not tech literate and didn't make the project. I'd suggest having two buttons, \"public scan\" \"private scan\". That would contextualize the public scan to clarify and when you are scanning is publicly __listed__. And different colors. I think red for \"public\" would actually be the better choice. Some information could be displayed while scanning. Idk put something like \"did you know, using the public scan makes the link visible to others? This helps security researchers. You can delist it by clicking ____\" or something like that and do the inverse. It should stand out. There's plenty of time while the scan happens. > On each scan result page there is a \"Report\" button which will immediately de-list the scan result without any interaction from our side. \"Report\" is not clear. That makes me think I want to report a problem. Also I think there is a problem with the color scheme. The pallet is nice but at least for myself, it all kinda blends in. Nothing pops. Which can be nice at times, but we want to draw the user to certain things, right? I actually didn't see the report button at first. I actually looked around, scrolled, and then even felt embarrassed when I did find it because it is in an \"obvious\" spot. One that I even looked at! (so extra embarrassing lol) I think this is exactly one of those problems where when you build a tool everything seems obvious and taken care of. You clearly thought about these issues (far better than most!) but when we put things out into public, we need to see how they get used and where our assumptions miss the mark. I do want to say thank you for making this. I am criticizing not to put you down or dismiss any of the work you've done. You've made a great tool that helps a lot of people. You should feel proud for that! I am criticizing because I want to help make the tool the best tool it can be. Of course these are my opinions. My suggestion would be to look at other opinions as well and see if there are common themes. Godelski isn't right, they're just one of many voices that you have to parse. Keep up the good work :) reply heipei 13 hours agorootparentThanks, that is great feedback and we'll try to improve how the scan visibility is shown and what it actually means. The suggestion of adding a text to the loading page is a great idea, and the feedback about the colors on the result page is totally valid. I'm the last person who wants to see private data accidentally leak into the public domain. However experience has shown that combating the massive amounts of fraud and malicious activity on the web nowadays requires many eyes that are able to access that data and actually do something about it. That is the reason we have these public scans in the first place. reply godelski 12 hours agorootparentAnd thank you for being receptive and listening! I hope my thoughts and others can help make your tools better. I really appreciate that people like you are out there trying to defend our data and privacy. I know it is such a difficult problem to solve and you got a lot of work ahead of you. But appreciation is often not said enough and left implied. So I want to make it explicit: Thank you. (and I'll say this interaction is the best advertisement you could make, at least to me haha) reply vin10 13 hours agorootparentprevThis is a very well formulated suggestion. Nicely written! reply getcrunk 10 hours agoprevWhat’s wrong with using signed urls and encrypting the object with a unique per user key. It’s adds some cpu time but if it’s encrypted it’s encrypted. * this obviously assumes the objects have a 1-1 mapping with users reply qudat 15 hours agoprevOver at pico.sh we are experimenting with an entirely new type of private link by leveraging ssh local forward tunnels: https://pgs.sh/ We are just getting started but so far we are loving the ergonomics. reply figers 12 hours agoprevWe have done one time use query string codes at the end of a URL sent to a user email address or as a text message to allow for this... reply BobbyTables2 5 hours agoprevWhat happened to REST design principles? A GET isn’t supposed to modify server state. That is reserved for POST, PUT, PATCH… reply overstay8930 16 hours agoprevBreaking news: Security by obscurity isn't actually security reply panic 13 hours agoparent“Security by obscurity” means using custom, unvetted cryptographic algorithms that you believe others won’t be able to attack because they’re custom (and therefore obscure). Having a key you are supposed to keep hidden isn’t security by obscurity. reply makapuf 15 hours agoparentprevWell, I like my password/ssh private key to be kept in obscurity. reply fiddlerwoaroof 15 hours agorootparentYeah, I’ve always hated this saying because all security involves something that is kept secret, or “obscure”. Also, obscurity is a valid element of a defense in depth strategy reply koito17 14 hours agorootparentTo play devil's advocate, people discourage \"security by obscurity\" but not \"security with obscurity\". That is to say, secrets or \"obscurity\" as part of a layer in your overall security model isn't what gets contested, it's solely relying on obscure information staying obscure that gets contested. e.g. configuring an sshd accepting password auth and unlimited retries to listen on a non-22 port is \"security by obscurity\". configuring an sshd to disallow root logins, disallow password authentication, only accept connections from a subset of \"trustworthy\" IP addresses, and listen on a non-22 port, is \"security with obscurity\" reply maxcoder4 11 hours agorootparentprevThe idea behind \"security thorough obscurity\" is that even if the adversary knows everything about your setup *except the secret keys*, you should be secure. Security through obscurity is any method of protection other than the secret key, like for example: * serving ssh on a random high port * using a custom secret encryption algorithm * hosting an unauthenticated service on a secret subdomain in hope nobody will find out * or with a long directory name Some security thorough obscurity is OK (for example high ports or port knocking help buy time when protecting from a zeroday on the service). It's just that relying only on the security thorough obscurity is bad. In this case, I wouldn't call URLs with embedded key security through obscurity, just a poor key management. reply fiddlerwoaroof 9 hours agorootparentBut, this is just relying on the obscurity of the key: all security comes down to some form of secret knowledge. It’s just better to use a space that’s hard to enumerate than a low-cardinality space: if we had 1024 bits of port numbers, picking a random port would be as hard to crack as a 1024 bit encryption key. reply overstay8930 12 hours agorootparentprevIf you use an HSM you wouldn’t have to worry about that either reply zzz999 13 hours agoprevYou can if you use E2EE and not CAs reply rvba 11 hours agoprev [–] Reminds me how some would search for bitcoin wallets via google and kazaa. On a side note, can someome remind me what was the name of the file, I think I have some tiny fraction of a bicoin on an old computer reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Malware/url analysis platforms such as urlscan.io, Hybrid Analysis, and Cloudflare Radar store numerous private links submitted by users, including cloud storage files and password reset links.",
      "Concerns arise as these links can be publicly accessed via search engines, potentially compromising data privacy and security.",
      "Users face challenges in preventing leaks and safeguarding sensitive information, despite some platforms offering the option to flag and remove such links. Exercise caution when accessing these links to mitigate phishing and malware risks."
    ],
    "commentSummary": [
      "The debate centers on the security hazards of publicly sharing private secure links, highlighting the risks of leaked links and using dorking to expose confidential data.",
      "Suggestions include using authentication tokens in URLs, balancing security with user convenience, and implementing security measures like shorter-lived URLs or authentication headers.",
      "Emphasis is placed on the significance of authentication, authorization, access control, and secure practices when sharing sensitive information via URLs or passwords."
    ],
    "points": 345,
    "commentCount": 182,
    "retryCount": 0,
    "time": 1709828987
  },
  {
    "id": 39628842,
    "title": "Google Ex-Engineer Faces Jail for AI Theft",
    "originLink": "https://apnews.com/article/china-google-justice-department-63156ade1e564d15d92adbef91e9c5da",
    "originBody": "FILE - Items are displayed in the Google Store at the Google Visitor Experience in Mountain View, Calif., Oct. 11, 2023. The Justice Department says a former software engineer at Google has been charged with stealing artificial intelligence technology from the company while secretly working with two companies based in China. Linwei Ding was arrested in Newark, California., on four counts of federal trade secret theft.(AP Photo/Eric Risberg, File) Read More By ERIC TUCKER Updated 3:06 AM UTC, March 7, 2024 Share Share Copy Link copied Email Facebook X Reddit LinkedIn Pinterest Flipboard Print WASHINGTON (AP) — A former software engineer at Google has been charged with stealing artificial intelligence trade secrets from the company while secretly working with two companies based in China, the Justice Department said Wednesday. Linwei Ding, a Chinese national, was arrested in Newark, California, on four counts of federal trade secret theft, each punishable by up to 10 years in prison. The case against Ding, 38, was announced at an American Bar Association conference in San Francisco by Attorney General Merrick Garland, who along with other law enforcement leaders has repeatedly warned about the threat of Chinese economic espionage and about the national security concerns posed by advancements in artificial intelligence and other developing technologies. “Today’s charges are the latest illustration of the lengths affiliates of companies based in the People’s Republic of China are willing to go to steal American innovation,” FBI Director Christopher Wray said in a statement. “The theft of innovative technology and trade secrets from American companies can cost jobs and have devastating economic and national security consequences.” MORE U.S. COVERAGE Alabama governor signs legislation protecting IVF providers from legal liability into law ‘Rust’ armorer’s trial gives Alec Baldwin’s team a window into how his own trial could unfold Fewer fish and more algae? Scientists seek to understand impacts of historic lack of Great Lakes ice Google said it had determined that the employee had stolen “numerous documents” and referred the matter to law enforcement. “We have strict safeguards to prevent the theft of our confidential commercial information and trade secrets,” Google spokesman Jose Castaneda said in a statement. “After an investigation, we found that this employee stole numerous documents, and we quickly referred the case to law enforcement. We are grateful to the FBI for helping protect our information and will continue cooperating with them closely.” A lawyer listed as Ding’s defense attorney had no comment Wednesday evening. Artificial intelligence is the main battleground for competitors in the field of high technology, and the question of who dominates can have major commercial and security implications. Justice Department leaders in recent weeks have been sounding alarms about how foreign adversaries could harness AI technologies to negatively affect the United States. Deputy Attorney General Lisa Monaco said in a speech last month that the administration’s multi-agency Disruptive Technology Strike Force would place AI at the top of its enforcement priority list, and Wray told a conference last week that AI and other emerging technologies had made it easier for adversaries to try to interfere with the American political process. Garland echoed those concerns at the San Francisco event, saying Wednesday that, “As with all evolving technologies, (AI) has pluses and minuses, advantages and disadvantages, great promise and the risk of great harm.” The indictment unsealed Wednesday in the Northern District of California alleges that Ding, who was hired by Google in 2019 and had access to confidential information about the company’s supercomputing data centers, began uploading hundreds of files into a personal Google Cloud account two years ago. Within weeks of the theft starting, prosecutors say, Ding was offered the position of chief technology officer at an early-stage technology company in China that touted its use of AI technology and that offered him a monthly salary of about $14,800, plus an annual bonus and company stock. The indictment says Ding traveled to China and participated in investor meetings at the company and sought to raise capital for it. He also separately founded and served as chief executive of a China-based startup company that aspired to train “large AI models powered by supercomputing chips,” the indictment said. Prosecutors say Ding did not disclose either affiliation to Google, which described him Wednesday as a junior employee. He resigned from Google last Dec. 26. Three days later, Google officials learned that he had presented as CEO of one of the Chinese companies at an investor conference in Beijing. Officials also reviewed surveillance footage showing that another employee had scanned Ding’s access badge at the Google building in the U.S. where he worked to make it look like Ding was there during times when he was actually in China, the indictment says. Google suspended Ding’s network access and locked his laptop, and discovered his unauthorized uploads while searching his network activity history. The FBI in January served a search warrant at Ding’s home and seized his electronic devices, and later executed an additional warrant for the contents of his personal accounts containing more than 500 unique files of confidential information that authorities say he stole from Google. ERIC TUCKER Tucker covers national security in Washington for The Associated Press, with a focus on the FBI and Justice Department and the special counsel cases against former President Donald Trump. twitter mailto",
    "commentLink": "https://news.ycombinator.com/item?id=39628842",
    "commentBody": "Ex-Google engineer charged with stealing trade secrets (apnews.com)323 points by kiwicopple 20 hours agohidepastfavorite399 comments hnburnsy 9 hours agoHere is the indictment since AP News can't be bothered to link to it... https://www.justice.gov/opa/media/1341356/dl?inline reply _cs2017_ 8 hours agoparentInteresting. The guy copied source code via copy paste... And it seems, also regular Google documents. He was already caught uploading secret stuff, quit Google and bought a ticket to go to China, and was arrested only because he delayed the trip by a few weeks which was enough time for Google to discover more violations and contact the FBI. Google didn't contact the FBI until they learned of older violations. Which begs the question: what triggered that: the fact that the earlier documents were more secret, more numerous, or the fact that the guy lied about destroying all previously downloaded data? reply HeyLaughingBoy 7 hours agorootparentReminds me of the time I quit my job at a large corp and my boss advised me to not do any large downloads in the days just before I left because \"IT has tools to scan for that and they might think you're trying to take secrets with you.\" Guess this guy didn't get the memo. reply hilux 1 hour agorootparentGood boss! reply whiplash451 1 hour agorootparentI don't want to discount his kindness, but I guess he was also trying to prevent some mess for himself. Being the direct manager in these situations is a hell of a ride no matter what your involvement is. reply throw393200 4 hours agoparentprevThere have been a lot of falsely accused \"spies\" recently like Xiaoxing Xi, Sherry Chen, and Anming Hu. All charges were eventually dropped. I wonder what will happen with this case. reply tracker1 2 hours agorootparentWithout knowing the details, it's hard to say. Nation state actors doing code and design exfiltration definitely happens. And it definitely happens with China. I wouldn't be surprised if every major country didn't do the same. The US has let production capabilities slip away, while China has become very competent at cloning and even creating designs at this point. Russia is now making ordinance at something like 40x US capability. Meanwhile the US is focused on open borders and a trans friendly military. The most important issues to survival are lost to political issues and chaos. Not to mention the articles about aircraft issues with our govt chosen monopoly supplier. reply timthelion 1 hour agorootparentSo in your oppinion the US, despite being not at war, should make as much munitions as a country that is very much at war? reply hilux 1 hour agorootparentprevWi Tu Lo. That one was totally bogus. reply everydecade 3 hours agorootparentprevWen Ho Lee reply billy99k 18 hours agoprevThis doesn't surprise me. I knew someone that intentionally graduated with a specific major, so they could get a job in that industry and send trade secrets/IP back to China. The purpose was to create a competing company. It didn't work out for them that well. They couldn't last more than 6 months at any one company and I think eventually gave up and went back home. reply baka367 7 hours agoparentRepositories are rarely worth much. Sure, some algorithms there might save you some time, but its often the design and the data where the money lies (what this guy focused on). Clone google's repo and you'll likely struggle forever to get anything of substance running on a rando vm/docker/etc. not to mention about spinning the entire stack with interconnected services, certificates, shitty code, and layers upon layers of hacking that can only be resolved by relying on the tribal knowledge on whomever built the darn thing. Compared to that - detailed design docs, a team of motivated Chinese dudes/ettes with some monetary support from the local party, and you can have a close-enough copy running natively on the Alibaba cloud in a few months. reply xyzzyz 5 hours agorootparentSource code repo is like a very extremely detailed doc. You might not be able to actually easily run it due to all of the dependencies etc, but with couple of weeks of reading, you should be able to tease back out the high level design. reply avidiax 5 hours agorootparentI've done enough code archaeology to say that looking at the code to understand the design is a good way to understand that the two halves of the bridge didn't mate up, but there was a deadline, so... The design from a design doc can be replicated at almost any company. The actual code is specific to the company and their exact stack. The company's business position is similarly hard to duplicate. You can understand a company's current capital, customers and money flows. Your new company has to either outcompete for those same flows or create or capture alternative flows, and do this with different capital. Having, say, the entire source code for FedEx doesn't make it easy to launch a competitor. It's practically irrelevant compared to the network of capital investments, corporate goodwill and contracts, etc. reply fragmede 4 hours agorootparentprevA copy of Google3 would take an outsider eons to replicate Borg for any of it to run on. reply paulddraper 5 hours agorootparentprevThere's probably some deep science AI-type stuff. Or maybe useful for security exploits. reply krisoft 9 hours agoparentprev> It didn't work out for them that well. They couldn't last more than 6 months at any one company I don’t understand what you are saying here. How many months does one need to stay to hover up the trade secrets / IP? In software engineering you get access to the repos on day one, but even in other industries I guess what you don’t have access to after 6 months you won’t have access to realistically ever. > eventually gave up and went back home But according to what you said that was their plan all along. So in what sense did it not “work out for them”? reply alsetmusic 8 hours agorootparentI have a strong distrust for authority, but even I would report espionage and IP theft of this sort. Downloading a movie doesn't bother me. Running a site for others to download movies doesn't bother me. But being a snake to go defraud a company to steal the hard work of others so your own illicit company can turn a profit off said labor by others irks me. Do your own R&D. Did you ever consider doing raising a red flag? If so, why did or didn't you? reply twentythree 6 hours agorootparent> Running a site for others to download movies doesn't bother me. > being a snake to go defraud a company to steal the hard work of others so your own illicit company can turn a profit off said labor by others irks me. I'm sorry, I really don't understand this. What part of the second statement doesn't apply to the first? reply nowandlater 6 hours agorootparentYou give it away for free instead. reply shotnothing 27 minutes agorootparentad revenue reply xattt 7 hours agorootparentprevIf anything is going to unite two groups of people who are “naturally” on opposite sides of a political spectrum, it’s going to be stopping treasonous activities. I airquote naturally because it’s obvious that foreign interference is at play, based on the laissez-faire attitude towards this sort of thing by some groups. reply outworlder 9 hours agorootparentprev> In software engineering you get access to the repos on day one Some repositories needed to do your work, sure. Not necessarily all, and the more interesting work may not be available to just anyone who joins. If it's a company like Google, you may not even end up at the group you interviewed for. reply ohyes 7 hours agorootparentI think the subtext here is that the “spy” in question was not the sharpest tool in the shed. You need some level of intelligence and knowledge to know what is worth stealing and what to do with it. Getting a major with the sole purpose of industrial espionage and then telling people about it indicates a lot about the person in question. reply krisoft 9 hours agorootparentprevYeah, yeah. I’m not saying it is easy business. What I am saying is that “bouncing around many companies in a quick succession and then leaving for their home country” is exactly the pattern one would exhibit with that plan. If one would want to show that their plan didn’t work out then one would be talking about other things. For example that they only got junior jobs with no access to the code/secrets, or that they were only hired in fields outside of their interest, etc etc. reply roland35 9 hours agorootparentprevI would be very careful doing that at Google. Even if just about anything is accessible, I imagine most access is logged. If you are downloading everything not related to your job it could raise some alarms! reply htrp 9 hours agorootparentDidn't stop Anthony Levandowski reply kirubakaran 9 hours agorootparentPresidential pardon is the one weird trick that employers hate, when you steal IP and get caught reply al_borland 9 hours agorootparentprevShort of downloading literally everything and sending it back to a team, it's possible he didn't know enough after 6 months (while also trying to maintain his actual job) to get anything of value. I've been at my company for almost 20 years. I have a lot of access, but if I was told, \"go find some trade secrets.\" LOL, not a chance. The haystack is far too big and I don't even know what I'm looking for. Someone who has been at the company 6 months barely knows where the bathrooms are. reply krisoft 8 hours agorootparent> I've been at my company for almost 20 years. I have a lot of access, but if I was told, \"go find some trade secrets.\" LOL, not a chance. Because it is not your intention to do so. Think about how one can live a whole life locking and unlocking locks without ever accidentally lock-picking one. Yet they can be picked, and often quite easily if that is your goal. If you are serious about it you don’t just bumble around randomly until a trade secret hits you on the head. You can ask yourself: what can that company do nobody else can? You can even ask this question before joining a company and thus selecting the right target and the right position to get access to it. reply dsq 3 hours agorootparentprevIf you are an agent of a rival company or govt there may alreay be a \"best practices\" rulebook for stealing IP, a set of established procedures. reply rightbyte 8 hours agorootparentprevMy prior employer was really worried about source code leaks. I was more like, giving the direct competitor the code would more be like industrial sabotage for their sake. What could they possibly do with it. They would waste fte years dechiffering it instead of doing something useful. But nah, rather keep your own engineers in the dark about secret plans and road maps. I worked out quite well though, since the engineers did their thing withoit knowing what the higher ups wanted. reply fragmede 7 hours agorootparentprevIf you're going to steal secrets, do it slowly, don't pull a Levandowski and copy everything in a noticable way so that security gets alerted, at which point it may take you forever to exfiltrate data. reply DinaCoder99 7 hours agorootparentprev> I don’t understand what you are saying here. How many months does one need to stay to hover up the trade secrets / IP? Presumably the more valuable the IP, the harder it is to access. reply atonse 18 hours agoparentprevHmmm that seems like a clear cut case to report to the FBI. Yeah, assuming that they were walking around telling people about it. reply kjkjadksj 18 hours agorootparentThe FBI gets more credible reports than it has the labor to investigate. Not to mention in this example no crime even yet occurred. reply atonse 17 hours agorootparentI agree with the spirit of your statement that no crime has occurred. But this isn't a case where someone just expressed a vague interest in a related topic of national security, but their specific intent to steal secrets and give them to an adversary. And then go ahead and interview at certain companies with that intent. This would be like someone specifically (not vaguely) stating their intent to commit a violent crime and then spend months preparing for it. Yeah, law enforcement, please definitely follow up on that one. reply snotrockets 17 hours agorootparentTrade secrets aren’t national security. reply jandrewrogers 17 hours agorootparentThey definitely can be. In the US there are many different ways in which they can overlap as a matter of law. There are myriad frameworks similar to ITAR that place a national security interest on trade secrets or block public disclosure e.g. patents (which effectively turns them into trade secrets). Your average web dev probably isn’t familiar but navigating this is a routine consideration in deep tech. reply snotrockets 14 hours agorootparentReal, and quasi-real national security projects require more stringent background checks than the ones unnecessarily used in most \"average web dev\" [sic] recruitment processes, and some come with citizenship requirements. I know, because that's one of the reasons I don't work on such projects. ofc, like in any security-related field, many are LARPing instead of practicing, and that's a different issue. reply jandrewrogers 11 hours agorootparentIt is more nuanced than this. A startup is virtually never a \"national security project\" even if they end up involved in an actual national security project. The kinds of background checks startups do are the same as any other company in any industry. It has nothing to do with national security. There are many things that can factor into a citizenship constraint depending on the type of business. A \"real\" national security background check requires support and sponsorship from a national government, and governments don't provide that casually to anyone that asks. If a startup finds themselves with national security customers, there is no requirement for the startup to go full-on Secret Squirrel but governments will calibrate their trust in the startup by how seriously the startup takes security and how diligent they are when vetting employees. It does not involve everyone getting a security clearance, which would not be possible anyway if the startup works with multiple national governments. I find the opposite situation is more common in practice: startups that find themselves in the national security space are often naive about what constitutes a baseline level of security, vetting their employees, and the pervasiveness and character of espionage programs. It is important to recognize that national security considerations are starting to affect startups that never go anywhere near national security customers due to escalating concerns and increased rigor around software supply chains. You may not have an interest in national security but national security may take an interest in you. This has ramifications for many software business models. reply zihotki 17 hours agorootparentprevThey are indeed separate concepts but they may be both true. ASML can be a good example reply MarkSweep 13 hours agorootparentprevThat’s not what the department of commerce thinks. Just giving information to a foreign national can be considered “deemed export” and get your company in trouble. https://www.bis.doc.gov/index.php/policy-guidance/deemed-exp... reply JumpCrisscross 18 hours agorootparentprev> in this example no crime even yet occurred Interviewing for a job with the prior stated intent of pilfering their IP is fraudulent. reply snotrockets 17 hours agorootparentLet the employer file civil case then. reply JumpCrisscross 17 hours agorootparent> Let the employer file civil case then The IP theft is a private concern. The national security implications are public. What OP describes seems worth criminal investigation. reply powersnail 8 hours agorootparentprevIs it? I mean obviously if the said person did pilfer, or attempted to pilfer, it would be illegal. But is there any law against interviewing for a job, while having a prior statement of intending to pilfer? Or in a more general sense, interviewing for a position while previously saying that they intend to breach the contract? I'd imagine that there could only be ground for a lawsuit if 1) a contract has been signed, and 2) the stated activity has at least been attempted. reply mupuff1234 18 hours agoparentprevDid that person just go around disclosing their plan? reply alephnerd 18 hours agorootparentHaving seen something like this happen once, what probably happened was the person OP is referring to was trying to get IP in order to start their own private sector startup, and probably get some seed funding from a regional government (eg. Beijing and Hangzhou did this in the 2000s to jumpstart their tech industry) It's similar to the Israeli program in the 90s (who's name I'm blanking out on EDIT: Yozma I before it was privatized) because just like China in the 2000s-early 2010s, there wasn't a notable private sector VC industry yet. reply maayank 18 hours agorootparent> It's similar to the Israeli program in the 90s (who's name I'm blanking out on EDIT: Yozma) because just like China in the 2000s-early 2010s, there wasn't a notable private sector VC industry yet. Quite an allegation... any reference to them sponsoring/encouraging stealing IP or am I misreading and you simply meant it's a government sponsored startup accelerator program? reply alephnerd 18 hours agorootparentIt's not really that damaging. Israel never recognized American software or pharmaceutical patents, and most countries do some form of Industrial Espionage (France is fairly notable in the space as well [4]). The wildest cases tended to be back in the 1990s, when Israel was trying to build a domestic armament industry, notably by stealing American IP and selling it to the Chinese [0][1][2][3] (most modern Chinese weapons systems today are based on that IP transfer in the 1990s). This largely ended by the mid-late 2000s when the Israeli tech industry was much more established, and Ehud Barak (edit: Olmert - mixed up his surname and the Barak middle scandal) getting arrested on corruption charges, heralding the end of Israel's Wild West days in the tech industry. Also, Tiannammen Era sanctions from the 1990s forced Israel defense companies to pivot to India, which doesn't allow vendors to sell SKUs to India which Pakistan and China have access to, and would leverage French and Israeli SKUs based on American designs. I highly recommend reading this GAO report from the 90s [3] [0] - https://www.jstor.org/stable/2538128 [1] - https://www.nytimes.com/1993/10/12/world/israel-selling-chin... [2] - https://www.jstor.org/stable/1149008 [3] - https://www.gao.gov/assets/t-osi-92-6.pdf [4] - https://www.politico.com/story/2014/05/france-intellectual-p... reply maayank 17 hours agorootparentDid you mean Ehud Olmert? I don’t believe Ehud Barak was ever arrested. Also, not to nitpick, but would appreciate publicly accessible articles… from the abstracts I can only assume these are summaries made in the 90s of pre-90s shenanigans EDIT: saw now the edits with 3-4, will look at when I have time (thanks!) reply alephnerd 17 hours agorootparent> Ehud Olmert Yep. Brainfarted and merged Olmert and the Barak missles corruption case > summaries made in the 90s of pre-90s shenanigans Hence why I wrote \"the Israeli program in the 90s\". It's significantly less egregious nowadays (imo de facto non-existent due to how integrated the Israeli innovation system is with the American system now and how simplified FDI is in Israel compared to the 80s-90s) > appreciate publicly accessible articles Internet based news wasn't really a thing until the post-Netscape era. All you're stuck with are archives of print news or government articles, especially because this kind of behavior largely ended by the 2000s. > EDIT: saw now the edits with 3-4, will look at when I have time (thanks!) No problem! And like I mentioned before, most countries do this in some form to help domestic champions (eg. India and Pharma IP, France and Defense IP, socialist era Israel and Defense IP, 1970s-80s Japan and electronics IP, China and Defense+Software IP). If a country allows almost 100% FDI, there's no reason for industrial espionage in that specific sector because foreign champions become integrated with domestic ones. Hence why Israeli and Indian companies don't steal hardware designs anymore because most Americans companies have design centers there that are closely integrated with domestic champions. reply greatpatton 16 hours agorootparentprevFunny that you mention France when the USA is #1 in the world for corporate spying. Having been involved in western Europe for deal where US competitor were given \"advantage\", USA spying was always number one concern over all other countries (and this is how counter spying agencies brief companies) as it had more direct economic damage and is more difficult to identify than Chinese spying. Few examples just for Airbus every few years you get report of US spying: * https://www.dw.com/en/airbus-fires-16-over-suspected-german-... * https://edition.cnn.com/video/news/2015/05/01/airbus-spying.... reply alephnerd 16 hours agorootparentThe American government will spy, but will not explicitly spy to provide IP directly to a private company like Boeing or Lockheed, as this enters felony level corruption territory due to the Procurement Integrity Act, Federal Acquisition Streamlining Act, and the Federal Acquisition Regulation. The main difference is DGSE would explicitly attempt to steal American IP and then provide it to Thales or Dassault. reply greatpatton 13 hours agorootparentThey may not provide direct R&D details but they will provide direct information about offers price, negotiation status etc. This is part of the Snowden leaks that people seems to have completely forgotten. https://wikileaks.org/nsa-france/spyorder/#spyorder2 reply alephnerd 13 hours agorootparentIANAL but Competitive Intel around pricing and SKUs isn't IP except in certain cases. If they were, just about every single private sector company globally would be guilty of IP infringement, let alone Public-Private Partnerships like the ones I mentioned. reply ramblenode 9 hours agorootparentprevIntelligence agencies often have their own interpretation of the law, which coincidentally allows them to do what they want. And if you don't like that, you can sue them in the special intelligence court where the evidence cannot be revealed, the proceedings are secret, and the judges are very unbiased. reply reaperman 18 hours agorootparentprevI think if someone actually had government handlers asking them to do this, most of those people wouldn't blab about it to their school chums. But there's a subset of people with grandiose delusions / general behavior problems who feel a compulsion to tell everyone about their grand plans/machinations to become rich and powerful. reply alephnerd 17 hours agorootparent> someone actually had government handlers It most likely wasn't a Handler/MSS type espionage. It was most likely trying to grab IP to found a domestic competitor, and raise a Seed round from local government accelerators like those Beijing and Hangzhou have. reply djtango 18 hours agorootparentprevReminds me of that scene from Silicon Valley where Jian Yang has a bunch of new startup ideas on his whiteboard reply popcalc 9 hours agorootparenthttps://www.youtube.com/watch?v=Km5XQxRrQvw reply advisedwang 5 hours agoparentprevDid they tell you that, did you hear it second hand, or figure it out yourself? reply riku_iki 10 hours agoparentprev> get a job in that industry and send trade secrets so, is there a clear line between: steal trade secret, and applied learned experience in new company the way everyone does? reply paulddraper 5 hours agorootparent> so, is there a clear line between: steal trade secret, and applied learned experience in new company the way everyone does? There may be some grey, but copying information in writing is pretty clearly over the line. reply pfannkuchen 10 hours agorootparentprevIf they are intentionally finding information that is outside the scope of their own role and then exporting the information itself as opposed to actually learning it then that would be clearly stealing trade secrets. Of course there are some lesser actions that would be in a gray area. reply dylan604 9 hours agorootparent> intentionally finding information that is outside the scope of their own role some call that a positive initiative. cross training between departments or some such corp speak is used so people can \"fill in\" or just have a better understanding of the other departments so you can possibly work better with each other or come up with novel solutions for someone else. companies that silo everyone off and prevent open discussion between groups are horrible places to work. ask Oppenheimer. reply pfannkuchen 8 hours agorootparent> intentionally finding information that is outside the scope of their own role AND THEN exporting the information itself reply lmm 9 hours agorootparentprevNo. Ultimately courts have to make judgements. reply xnx 19 hours agoprevGoogle has a fair bit of experience with employees stealing trade secrets: https://en.wikipedia.org/wiki/Anthony_Levandowski#Criminal_c... reply breakingcups 19 hours agoparentIncredible that he got pardoned. reply therobot24 19 hours agorootparentnext [6 more] it's because he could afford it reply barbazoo 17 hours agorootparentAssuming you’re talking about money, how does one pay for a pardon? reply blackhawkC17 16 hours agorootparentHe didn’t pay. But Peter Thiel and Palmer Luckey (two big Republican donors) recommended him to the Trump team for a pardon, and Trump obliged. reply barbazoo 16 hours agorootparentOk, however, I still don't see \"it's because he could afford it\". reply dylan604 9 hours agorootparentIf you can't afford it, you're not hanging out with the likes of Thiel or Lucky. Just having money grants you access to certain circles even if you're not having to spend the money for that access, or it allows you to buy tickets to events with those people. reply paulddraper 5 hours agorootparentprevNot all affordances are cash. reply sdwr 19 hours agorootparentprevnext [4 more] [flagged] jasonvorhe 19 hours agorootparentDo you have a source for that? Sounds far fetched. (Not a Trump fan.) reply monocasa 19 hours agorootparenthttps://www.nbcnews.com/politics/politics-news/giuliani-accu... reply anonuser123456 5 hours agorootparentThat’s not a very credible source. reply bigcat12345678 19 hours agoparentprevIt's doubly awkward that this guy apparently doesn't know or underestimate Google's determination to trace employee' access. reply flakiness 13 hours agorootparentThat determination got stronger after this incident. reply ttul 18 hours agoprevIf any of you get this far down, one thing that caught my eye is that Google said they had analyzed this guy’s network traffic after locking his laptop, confirming various things. If you work at a large company like Google, every packet on their employee access network is recorded and indexed for forensic purposes. This is not something Google would talk about publicly, but it’s standard practice in any company that is a serious target for sophisticated cyberespionage to spend a great deal on specialized equipment that can log all network traffic at scale. reply orochimaaru 7 hours agoparentThis is standard practice in all big companies. Everything is tracked and recorded. If you want to say something to a colleague that you don’t want management to know - use your personal phone and talk at a coffee shop or bar in person. reply kshacker 4 hours agorootparentAsking for a friend :) On VPN at home, using my work laptop, I happen to browse some non-decent content, more than once, maybe routinely. Is that all tracked or do VPNs have routing to use VPN only for company network, and leave NSFW be handled by my ISP? Or even if is going via ISP and not company network, are the companies usually able to track what all sites I visit Not a Google employee BTW, but work for a company that I am reasonably sure does monitor their network. reply toast0 3 hours agorootparentDepends on the VPN config if it's everything, or just company resources. Split tunnelling seems uncommon from my experience with corp VPNs... But if you're on work equipment, they likely have corp spyware looking at all your browsing even if you're not on vpn. Shop for fans or whatever you need to do on personal equipment. reply resolutebat 5 hours agorootparentprevIt's indeed safe to assume everything is tracked and recorded and can be found if they bother to look for it, but a random line manager is not going to have access to any of it. reply alephnerd 18 hours agoparentprevIt's SOP in all companies, not just those facing sophisticated threat actors - there's a reason EDRs like Crowdstrike and SentinelOne are massive players now. reply bongodongobob 6 hours agorootparentAs someone who has worked at companies, it sure as fuck is not. Unless you are a very valuable company or you make money with data/software, ain't nobody got time for that. reply fomine3 2 hours agorootparentYeah, install EDR for satisfaction and forget. reply opello 6 hours agorootparentprevIt absolutely has to be more nuanced than \"there's exabytes of pcaps somewhere\" because cloning repositories, pushing branches, backups, these things would basically end up being nasty amplification attacks against the ability to store this data. And block dedupe can work for some storage loads, but it's not solving this problem, especially when that git clone came over ssh or https. Data from employee devices all being captured and stored? That seems plausible. All data on the corporate network? Less so to my naive mind. I'd love to hear exactly how that works and what kind of retention exists for it. What seems far more likely is that there's a rules engine that can see all the traffic and makes a decision about if it trips an event to be logged or looks strange enough to be captured (along with some amount of surrounding context, if possible). reply bongodongobob 4 hours agorootparentYeah you plug holes so you don't have to audit everything. Disable USB ports, alert on large file transfers, audit file access and device logins, no access to local network shares off-site, etc. That's probably good enough for 99% of the world. Capturing all network traffic is absurd and I doubt that's even a thing. You'd need a department the size of the existing company to be able to manage and do anything meaningful with it. Maybe if you had a super secure jump box you could consider monitoring all the traffic on that, but there are much much easier ways to audit behavior than network traffic. Monitor the devices instead. reply deelowe 9 hours agorootparentprevI've never been exposed to that side of things but always wondered do certain levels datamine this information? For example, do they get reports on user activity during the day... A pareto of employee activity perhaps by userid? I mean, why wouldn't they? reply flextheruler 7 hours agorootparentI’ve done this sort of work and my anecdotal experience was it is mostly used to flag blacklisted activities from occurring on the computer spanning things like porn and gambling sites to administrative privileges, but also to modify what level of access these computers had for interacting with different infrastructure between silos. You could use the data to identify activity levels or behavior patterns of the people using the device but it would cost a ton more money and a larger team to do that plus the other responsibilities we had simultaneously. My experience is also not with employee owned devices so in my mind there’s nothing wrong with doing it’s agreed to and is imperative to their function as an employees especially with HIPPA concerned. I think there was some BYOD stuff that was starting at one point and we had to run an emulator on their personal devices so the programs we run to collect logs were sandboxed from their regular phones. reply Nerada 4 hours agorootparentprevEssentially you hook up all your log sources to a User and Entity Behaviour Analytics (UEBA) platform, it comes up with a model of \"normal\" behaviour, and flags users for investigation when they start acting outside of those norms (or things you want to explicitly flag on). No data egress for 6 months, then 20GBs of outbound traffic? Someone's getting notified to take a look and see what that was and where you sent it. You only authenticate against one host on the network, and suddenly you're hitting thousands of hosts? Someone's getting notified to investigate, &c. reply leetcrew 8 hours agorootparentprevwhy would they? if we're talking about sophisticated espionage, that's more of a job for infosec. if we're talking about AFK time, being secretive defeats the purpose. reply NooneAtAll3 9 hours agorootparentprevwhat does EDR mean? reply rrdharan 9 hours agorootparentEndpoint Detection and Response reply ein0p 6 hours agoprevThe guy was allegedly stealing all that using Google Drive. I find such moronic behavior really hard to believe. Literally, there’s no illusion of privacy at Google while using company hardware, let alone company services. This has become quite clear after the Levandowsky fiasco - some of the things disclosed there were surprisingly invasive far in excess of what you’d normally expect reply hnburnsy 5 hours agoparentHe was white washing the documents via Apple Notes and it worked initially. From the indictment I posted here... >In total, DING uploaded more than 500 unique files containing Google Confidential Information, including the trade secrets alleged in Counts One through Four. DING exfiltrated these files by copying data from the Google source files into the Apple Notes application on his Google-issued MacBook laptop. DING then converted the Apple Notes into PDF files and uploaded them from the Google network into DING Account 1. This method helped DING evade immediate detection. reply ein0p 3 hours agorootparent>> This method helped DING evade immediate detection Evidently not. It just shows that the guy is not a foreign intelligence operative - a professional could easily operate there for years undetected with fairly basic opsec. That said, aside from things like hardware designs and perhaps certain model weights, I struggle to think of anything at Google that anyone would want that’s not already on GitHub. reply resolutebat 5 hours agoparentprevThe guy wasn't exactly subtle about things: > Officials also reviewed surveillance footage showing that another employee had scanned Ding’s access badge at the Google building in the U.S. where he worked to make it look like Ding was there during times when he was actually in China, the indictment says. reply martin1975 7 hours agoprevJust hire Chinese engineers who were born/raised here. It would meet the DEI quota and decrease the chance of industrial espionage. No guarantees, but I think this would help. reply kajecounterhack 3 hours agoparent…so you mean hire Americans. Also, pretty sure there are no DEI quotas for Asian men. reply hilux 1 hour agoparentprevChinese and Indian men do not count towards any DEI quota in tech!! reply jeffbee 19 hours agoprevThe opsec of the people who eventually get indicted is always terrible. If you wanted to exfiltrate source code or docs, why the heck would you use the victim's own cloud storage product? You would just point a camera at your display and scroll through the desired materials, or use HDMI capture, or something along those lines. reply diggan 19 hours agoparentSurvivorship bias in action? The only ones we hear about are the ones who are sloppy enough to get caught. The people who know how to not get caught, doesn't get caught so we never hear about them. reply flextheruler 7 hours agorootparentDefinitely. We’re certainly not living in a world where we catch more of these people than we don’t. reply trollerator23 5 hours agorootparentprevExactly. We only know about the terrible ones. reply everfrustrated 17 hours agoparentprevPossibly they thought it would appear as their business-as-usual Google related traffic flows. Rather than say, some tor IP address which would stick out. reply paulddraper 5 hours agoparentprev> The opsec of the people who eventually get indicted is always terrible By definition.... Yes reply mr_toad 8 hours agoparentprevPen and paper can work too. reply greatgib 2 hours agoprevImagine leaving a Google position and risking a 10 years sentence for the reward of a \"monthly salary of about $14,800\" job. reply thescriptkiddie 6 hours agoprevHow can you \"steal\" a trade secret? Isn't the whole idea that you forgo any legal protection of the secret so that you don't have to disclose its nature, as you would have to with a patent? reply anonuser123456 5 hours agoparentNo. Trade secrets are intellectual property that have legal protection. https://www.law.cornell.edu/uscode/text/18/1832 What a trade secret lacks in protection is the monopoly granted by a patent. reply alexnewman 5 hours agoprevDisinfo is how you keep secrets. reply feverzsj 19 hours agoprevSo he was already CEO/CTO of 2 China companies while still working in google. And these information are publicly available right after he registering them. Seems a management disaster of google. reply Thorrez 19 hours agoparentYou expect Google to scan the database of companies in every country continuously to see if employees are executives of them? How would this handle different people who have the same name? Disclosure: I work at Google. reply ActionHank 19 hours agorootparentSo I've worked at a few places, none nearly as fancy as Google. Not a single one would have had files being uploaded to personal cloud storage from a work device go unnoticed. That was the red flag, at that point they should've been monitoring actively. reply zettabomb 19 hours agorootparentprevNot for nothing but plenty of other companies do pretty much just this, for example in defense. Surely Google of all companies should be able to do a simple search like that on a regular basis. reply redkoala 19 hours agorootparentIn highly regulated national security impacting industries like defense, that makes sense. Google has not developed that rigor yet, although it's becoming obvious that their business has high national security implications now. reply zettabomb 19 hours agorootparentI don't think Google has ever had rigor, in anything except possible things which directly affect uptime. It seems to be a systematic problem - look at their history with chat apps for example. Great for hackers - both ones working for Google and ones working for other governments, apparently. reply leoh 5 hours agorootparentNot sure why this was downvoted but there is a lot of evidence to support this statement, despite the way Google is perceived reply htrp 9 hours agorootparentprevGoogle already apparently logs every network packet on the internal network (including DPI), so I imagine scanning corporate registrations can't be that much worse. reply renegade-otter 18 hours agorootparentprevThe Google hiring process can take months. They have time to haze people with Leetcode but no time to do a good vet of a person who may be a high risk security threat. reply eganist 19 hours agorootparentprev> You expect Google to scan the database of companies in every country continuously to see if employees are executives of them? How would this handle different people who have the same name? > Disclosure: I work at Google. It's Google. Not a mom and pop shop, not a startup, not even a large bank. It's a massive conglomerate who's entire business model revolves around data. So yes. And same-name conflicts can be handled case by case. reply Thorrez 19 hours agorootparent>And same-name conflicts can be handled case by case. How? Several times I've had to contact someone within Google whose name I know, but when I go to look up the person's email, there are multiple employees with that name. This is just within Google. Think of within an entire country. reply seanmcdirmid 17 hours agorootparentprevAnd how are they going to access Chinese databases that they are not allowed to access? It's Google, not the CIA. I wouldn't be surprised if all of that information was covered under China's broad state secrets law. reply siva7 18 hours agorootparentprevI'm not aware of any corp doing this and why should they? There are as many valid reasons registering a company without affecting your employment. reply hilux 1 hour agorootparentWhen you apply to Google, they ask what other employment you have, IP you own, etc. Many companies do some variation of this, but I believe Google is one of the most restrictive on its employees. reply edandersen 9 hours agorootparentprevBig 4 do this routinely to check for conflict of interest as a result of audit regs. reply paulddraper 5 hours agorootparentprev> And same-name conflicts can be handled case by case. ...unless they're Chinese. reply unsupp0rted 19 hours agorootparentprev> You expect Google to scan the database of companies in every country continuously to see if employees are executives of them? Um, yes? That’s among the least invasive and cheapest due diligence they could do. reply ithkuil 19 hours agorootparentPerhaps just perhaps the task is a bit harder than what you make it sound like Instances of people sharing the same name are far more common in china than elsewhere. For example there are more than 30 thousand people called \"Wang Wei\". The fact is complicated by the fact that the writing systems are different and transliteration errors are commonplace. reply seanmcdirmid 17 hours agorootparentTo add to that, I don't think Google (or any American company) would ask for foreign ID numbers. Your SSN can be used for a background check in the USA, but not in China. reply unsupp0rted 18 hours agorootparentprevHow many people named Wang Wei in any given year become the officers of companies? Google could even automate this with an email, opting into which would be a requirement for any senior employee handling the kind of information the US government cares about. \"A person sharing your name has registered a company in China, as of 2024-03-07. To affirm that you are not related to this person, please click this link. If you were this person, please reply to this email for next steps.\" Edit: obviously, criminals don't mark the \"yes I'm a criminal box\" on forms. That's not the purpose it's there to serve. reply fooker 18 hours agorootparentIf you are guilty of a much more serious crime, saying you're not related to this person or ignoring the email won't add much to your guilt. reply yellow_lead 18 hours agorootparentprev\"Yes, I am not this person\" phew reply theGnuMe 14 hours agorootparentprevI think it is quite difficult to find out the officers of Chinese companies. There was the big wall street stock scandal a few years ago with respect to Chinese listings on US exchanges. reply fooker 18 hours agorootparentprevEvery problem is easy and cheap until you think about how to do it. reply spywaregorilla 19 hours agorootparentprevThey could try Googling it reply xienze 19 hours agorootparentprevPresumably for such a high profile position a simple um, Google of the person, checking LinkedIn, or a standard background check would reveal this. reply shagie 19 hours agorootparent> Within weeks of the theft starting, prosecutors say, Ding was offered the position of chief technology officer at an early-stage technology company in China that touted its use of AI technology and that offered him a monthly salary of about $14,800, plus an annual bonus and company stock. The indictment says Ding traveled to China and participated in investor meetings at the company and sought to raise capital for it. > He also separately founded and served as chief executive of a China-based startup company that aspired to train “large AI models powered by supercomputing chips,” the indictment said. These events happened after the person was hired. This would suggest performing background checks with some frequency - presumably at least once a month - in order to catch the events promptly. reply shagie 16 hours agorootparent> Prosecutors say Ding did not disclose either affiliation to Google, which described him Wednesday as a junior employee. ... and not just of high profile or senior developers, but all of the junior developers too. reply 2devnull 19 hours agorootparentprev“showing that another employee had scanned Ding’s access badge at the Google building in the U.S. where he worked to make it look like Ding was there during times when he was actually in China” Google can’t secure itself. That’s been true for years. It’s an enterprise held together by monopoly power, lobbying and low interest rates. reply ilickpoolalgae 18 hours agoparentprevI used to work at a Chinese tech company. I hear that it's pretty common to use aliases, instead of your real name, due to anti-compete clauses when you switch between companies. Even if a company had the ability to do background checks, like you mentioned, it'd be pretty hard to automate if the practice is commonplace. reply tppiotrowski 8 hours agoparentprevAre you prohibited from owning a company or acting as the CEO of a company while employed by Google? reply resolutebat 5 hours agorootparentYou're supposed to declare anything that's a potential conflict of interest, and Google is large enough to have a lot of interests. So if you're moonlighting as an Uber driver, Google's probably cool with it (at least if Waymo is not in your hood); if you're moonlighting trying to build the next Uber for X, Google probably would not be. reply jajko 19 hours agoparentprev> What is Article 7 of the Chinese Intelligence law? > Article seven says in part that “All organizations and citizens shall support, assist, and cooperate with national intelligence efforts in accordance with law, and shall protect national intelligence work secrets they are aware of. And that's just a nice wording to make it official, russians don't have anything similar yet they keep bribing small and big people all around the world to often perform literal treason of their home country, and quite a few do so for petty sums. Your Chinese relatives can also be just sentenced ie for made up drug trafficking to execution and subsequent organ harvest if say sending them to 're-education' camp won't convince you. Anybody having Chinese citizenship and any position of power or access to secret stuff should be treated as potential threat and evaluated continuously. Or just not hired. If they are actually serious about such a work they should give up their nationality, if they can't then they are risky. Its a serious stuff by no means, but this is how China plays so literally everybody around the globe has to adjust or suffer subsequent consequences. reply physPop 19 hours agorootparentThey can't \"give up their nationality\". Chinese government's position is that once Chinese always Chinese, and emmigrating doesn't affect that. They will still come after your family on mainland. Or use their \"local police\" forces stationed in most western countries to harass you in your new location. reply 2devnull 19 hours agorootparent“ use their \"local police\" forces stationed in most western countries to harass” Totally under appreciated point. It’s not “over there” anymore, the CCP have a strong and growing presence in the Bay Area now. Penetration into the FBI will take longer than google or local law enforcement but it is inevitable. reply alephnerd 18 hours agorootparentprev> They can't \"give up their nationality\" You can, but it's a fucking pain in the ass, and when Zero COVID kicked in, the Chinese Embassies and Consulates stopped processing anything. reply seanmcdirmid 17 hours agorootparentIts a formality they can ignore though. At least they have recent precedence with that Swedish bookseller who was abducted in Thailand a decade or so back. reply alephnerd 17 hours agorootparentI mean, you historically could ignore it, but it's changed since the anti-corruption purge began in 2016. Imo there's no reason to poke that bear anymore - a lot of bad practices that were common 10 years ago are not tolerated anymore (though sadly, a lot of good practices have also started getting cracked down, like domestic criticism) Edit: you're talking about Gui Minhai. Ok yea that's fair. reply seanmcdirmid 17 hours agorootparentI would place money on China in 2024 being worse at rule of law, not better, than in 2016. They granted defacto citizenship to that snowboarder, for example, even though there is no way she qualified under the text of its own law (China doesn't allow for dual citizenship...unless convenient). That was 2022. It has been downhill since Xi took charge, but yet, he was able to use accusations of corruption to purge his competition. The things that have improved are mostly public order (like prostitution being much less visible than it was). reply alephnerd 17 hours agorootparentI agree with ya! A lot of the crackdown was performative, but silver lining is that at least some bastards got punished as they deserve (albeit by equally reprehensible bastards). Sort of a broken clock is right twice kinda situation. reply thatfrenchguy 17 hours agorootparentprevDo you not know any first generation Chinese Americans to say such inaccurate statements? This is incredibly inaccurate, naturalized citizens are treated as foreigners by the PRC. reply resolutebat 5 hours agorootparentOnly when it's convenient to the PRC to do so. When it isn't, they're Chinese: https://en.wikipedia.org/wiki/Gui_Minhai Who, after being kidnapped, conveniently and totes voluntarily applied to have his Chinese citizenship reinstated. reply ithkuil 19 hours agorootparentprevGiving up citizenship doesn't solve the problem of retaliation against family reply simpletone 8 hours agorootparentprev> Your Chinese relatives can also be just sentenced ie for made up drug trafficking to execution and subsequent organ harvest if say sending them to 're-education' camp won't convince you. Oh god, this nonsense again. Why would they have to target their relatives when the 'ccp' has their police all over world? > Anybody having Chinese citizenship and any position of power or access to secret stuff should be treated as potential threat and evaluated continuously. Sure. But everyone should be treated as potential threat. What's with eastern europeans like you spreading so much garbage propaganda online? Here, reddit, youtube, etc. And it's the same bullshit over and over. reply Waterluvian 19 hours agoparentprevNot agreeing or disagreeing, but what’s the remedy? To regularly scour sources for information on tens of thousands of employees and parse actionable meaning from the data? Maybe someone can make a horrible start-up that does this as a service. reply nonethewiser 19 hours agorootparent> Not agreeing or disagreeing, but what’s the remedy? To regularly scour sources for information on tens of thousands of employees and parse actionable meaning from the data? Maybe someone can make a horrible start-up that does this as a service. If a colleague new about it and reported it then that should lead to action 100% of the time. The question is if that happened or not. reply catchnear4321 19 hours agorootparentprevmaking vast amounts of scraped data accessible, almost like search. you’re right, the big names probably can’t handle that on their own. reply yellow_lead 18 hours agoprevI find it funny how Google is presenting this. These statements don't really mesh well. > “We have strict safeguards to prevent the theft of our confidential commercial information and trade secrets,” Google spokesman Jose Castaneda said in a statement. > Ding [..] began uploading hundreds of files into a personal Google Cloud account two years ago. > He resigned from Google last Dec. 26. Three days later, Google officials learned that he had presented as CEO of one of the Chinese companies at an investor conference in Beijing. reply vasco 17 hours agoparent> We have strict safeguards to prevent the theft of our confidential commercial information and trade secrets This is just something companies have to say to keep their certifications / audits valid and not get sued by shareholders. In the end any system is leakable if workers really want to. reply agitator 8 hours agorootparentIt's also to be defensible in court. If an opposing party can make the valid argument that \"They leave the doors wide open and scatter IP willy-nilly, why wouldn't the IP get leaked?\" it makes it harder to argue \"Person X stole information when it was obvious that there was an expectation of secrecy\" reply ironyman 19 hours agoprevGood time to post this article again: https://www.lesswrong.com/posts/z4MDDwwnWKnv2ZzdK/the-agi-ra... China understands there is a real risk of the US gaining an absolute advantage in A[G]I development. It shouldn't surprise anyone that they will use all kinds of 'greyzone methods' to bridge this gap. reply Dr_Birdbrain 19 hours agoparentSkimming through it I was confused. - No homegrown semiconductor industry: isn’t the recent hand-wringing over the new Huawei chips proof of the opposite? - No interest in training LLMs? Is that true? I thought Baidu was already on it? In fact at every major AI conferences, Chinese R&D groups like Baidu and Ant group are major participants (and sponsors). I am talking about conferences like NeurIPS and AAAI, which both happened in the past few months. EDIT: the comments of that article are also confused by that article, lol. Is there a joke that is going over our collective heads? reply Twirrim 19 hours agorootparentThere's also a lot of research papers in AI coming out of Chinese universities. reply rayval 17 hours agorootparentAlso I have seen some papers on Arxiv from FAANG companies that have a half-dozen or more co-authors, and almost all of those authors have Chinese names. reply CuriouslyC 19 hours agorootparentprevThey're major participants and sponsors because they're definitely behind, and they're trying to rectify that. reply nextworddev 19 hours agoparentprevThis article should decouple 1) capacity to develop AGI versus 2) desire. reply lfmunoz4 10 hours agoprevWonder if any Americans go to Chinese tech companies to steal secrets or if there is just nothing there to steal. reply hnfong 5 hours agoparentThe CIA can basically tap into any network they want. Why take a big risk grooming an intern to join Huawei when you can just get what you want with the tap of a button? reply mr_toad 8 hours agoparentprevHow many Americans go to work in China at all? reply xeonmc 10 hours agoparentprevThe secrets are which secrets had been stolen. reply tivert 9 hours agorootparent> The secrets are which secrets had been stolen. The Chinese probably have secrets worth stealing about solar panel production. They've pretty much driven everyone else out of business. reply ejb999 9 hours agorootparentIt’s hard for others to compete with slave labor. Gives the Chinese quite a leg up on pricing power. https://www.nytimes.com/2021/06/24/business/economy/china-fo... https://www.washingtonpost.com/business/2021/08/27/customs-d... https://www.bbc.com/news/world-asia-china-57124636 reply curt15 7 hours agoparentprevWouldn't Americans find life behind the GFW pretty suffocating? reply leoh 5 hours agorootparentVPNs reply Legend2440 10 hours agoparentprevI would be frankly disappointed if our government and tech companies are not doing the same. reply miohtama 19 hours agoprevLinkedIn profile screenshot: https://twitter.com/pmarca/status/1765533899873288642?t=yUGy... reply NicoJuicy 18 hours agoparentWelp. Infra secrets for deep mind too and access. reply axpy906 18 hours agoprevThe title is “ Ex-Google engineer charged with stealing AI trade secrets while working with Chinese companies” reply wg0 18 hours agoprevWhat exactly sterling secrets looks like? Suppose I work on a video streaming service. Spent 8 years. Now I know in and out of it. The ffmpeg the queues the buckets the meta data and what not. Someone hires me. I build a steaming service. But this time I'm much more polished and faster. Is this stealing too? reply noslenwerdna 15 hours agoparentDefinitely downloading and uploading company documents to people outside the company counts as stealing... reply pradn 4 hours agoparentprevNo, that would be your government or employer preventing you from using your specialized skills to earn a living. In theory, a competitor can hire you for your skills - but your new employer will make it clear they are \"only hiring you for your skills, not your proprietary knowledge\". I've see that clause even as a junior employee. reply bredren 10 hours agoparentprevAs a sibling comment mentions, it depends. If you want to learn more, there is a thing called clean-room development which is a process used to reduce the legal risk of copyright and intellectual property violations. reply lupire 17 hours agoparentprevThere is not exact definition. Real world is messy. reply prepend 9 hours agoprevIt’s interesting to me that google doesn’t do a security clearance review on its engineers. I’ve had a security clearance in the past and there’s no way a foreign national passes. I got questioned significantly about a family member who was a citizen of another country. I wonder if it’s just a matter of time. reply shagie 9 hours agoparent> Within weeks of the theft starting, prosecutors say, Ding was offered the position of chief technology officer at an early-stage technology company in China that touted its use of AI technology and that offered him a monthly salary of about $14,800, plus an annual bonus and company stock. The indictment says Ding traveled to China and participated in investor meetings at the company and sought to raise capital for it. > He also separately founded and served as chief executive of a China-based startup company that aspired to train “large AI models powered by supercomputing chips,” the indictment said. > Prosecutors say Ding did not disclose either affiliation to Google, which described him Wednesday as a junior employee. --- They likely did... he did these things after joining Google as a junior employee. https://www.eeoc.gov/national-origin-discrimination Discriminating against a national origin is illegal as it is a protected class. Unless the material is classified under ITAR ( https://en.wikipedia.org/wiki/International_Traffic_in_Arms_... ) there is no reason to do a security clearance review of a junior developer with a valid work visa. reply mvdtnz 9 hours agoparentprevIs it your contention that Google should never hire anyone born outside of USA or without USA citizenship? Do you realise how much of their workforce that would preclude? reply calculatte 5 hours agorootparentHow many software engineers were laid off in the US over the past year? Shortage of talent is not the issue. reply c2occnw 9 hours agorootparentprevPresumably it would only be required to work on extremely sensitive projects with national security implications (no idea if that applies in this case). reply jpk2f2 9 hours agorootparentprevOf course not. However there should certainly be a risk assessment with regards to citizens of foreign countries known to steal IP or otherwise perform hostile actions. reply 2OEH8eoCRo0 9 hours agorootparentprev> without USA citizenship For jobs in the US, yes. reply tristor 17 hours agoprevnext [36 more] [flagged] dang 6 hours agoparentThis crosses into a slur and you can't do that on HN. We ban accounts that do, so please don't do it again. We've already had to warn you about this once: https://news.ycombinator.com/item?id=37580266. https://news.ycombinator.com/newsguidelines.html reply archagon 11 hours agoparentprev> It's the stated policy of the Chinese government that /all/ Chinese nationals are responsible to participate in, aid, and abet Chinese intelligence efforts > If you hire Chinese nationals in your tech company, you can be assured they are committing espionage One does not follow the other. Prejudice. I have a dual citizenship, and my other government says all sorts of things about my supposed allegiances that I vehemently disagree with. I am not my governments’ property. reply mrd3v0 11 hours agorootparent> I am not my governments’ property. That quickly changes when one has loved ones or assets inside the Fascist country. This has been done to many dual citizenship nationals. Anyone originally from a country controlled by a tyrannical system should most definitely seize all connections to it otherwise they are opening themselves up to being a liability such as what OP is saying. And this is coming from one of those people. reply 20after4 10 hours agorootparentprev> I am not my governments’ property. Your government might have a different opinion on the subject. reply falcor84 10 hours agorootparentThe British once believed that pretty much the entire world population were their subjects (as have many other empires), but we didn't let that stop us. reply akira2501 10 hours agorootparentprev> Prejudice Yes. What other form of insurance could I use here? > I have a dual citizenship, [...] I am not my governments’ property. No one is the property of their government but you have openly decided to split your allegiances. Anyways, you can only have this dual citizenship because both of these governments have decided to allow it, and that could change tomorrow. reply Mr-Frog 10 hours agorootparent> you have openly decided to split your allegiances Many people are born in the USA inherited dual citizenship. I personally know American-born Iranian and Russian citizens that would like to renounce their citizenship (since it negatively impacts their job opportunities in cleared engineering work) but are not able to do to current global political situations. reply archagon 9 hours agorootparentprevI retain my second citizenship in hope of a brighter future, not out of any sense of duty to my country. reply tristor 9 hours agorootparentWhile I applaud your optimism, your hope for a brighter future is not a scalable solution for securing trade secrets at tech companies. reply archagon 9 hours agorootparentIf your company elects not to hire people on account of them being Chinese, they will rightfully get the pants sued off of them. reply tristor 9 hours agorootparent> on account of them being Chinese Ethnically, yes, this would be immoral. On the basis of nationality, it's absolutely not immoral. In fact, in the US, it's /required by law/ to discriminate in this way if your company works on technology related to national security. It is illegal to discriminate on the basis of national origin, someone's birthplace, but not on the basis of nationality, their citizenship, in the case of national security. Only US citizens, including those who are naturalized, may work on technology related to national security within the US. What is prudent is another matter besides that which is legal. That which is moral is another matter besides that which is legal. I've made my opinion known, you're welcome to form your own. reply blackoil 17 hours agoparentprev> It's the stated policy of the Chinese government stated as in? Can you share some source. reply 2OEH8eoCRo0 17 hours agorootparenthttps://www.fbi.gov/investigate/counterintelligence/the-chin... https://en.wikipedia.org/wiki/National_Intelligence_Law_of_t... reply blackoil 15 hours agorootparent> All organizations and citizens shall support, assist, and cooperate with national intelligence efforts in accordance with law, and shall protect national intelligence work secrets they are aware of. That's as vague as it can be. Does any country not have similar laws? reply josephh 14 hours agorootparentWhich countries have laws that require their citizens to \"support, assist, and cooperate with national intelligence efforts\"? reply tivert 9 hours agorootparentprev>> All organizations and citizens shall support, assist, and cooperate with national intelligence efforts in accordance with law, and shall protect national intelligence work secrets they are aware of. > That's as vague as it can be. Remember: China doesn't really have \"rule of law.\" I understand vague laws are pretty par for the course in Chinese law. It makes it easier for the \"law\" to be bent by officials to achieve their goals. > Does any country not have similar laws? I doubt the US does. The CIA can't press you into becoming an agent. reply EasyMark 13 hours agorootparentprevI've never heard of that as a requirement for US citizens. I mean the government can't compel you to be a spy (industrial or espionage) last I checked. I'd be happy to be corrected. I mean you can totally volunteer for it though, of course. reply riku_iki 10 hours agorootparentnothing prevents government from creating such law or executive order if necessary. reply nindalf 10 hours agorootparentNothing except the Constitution. Seems like an open and shut 1A case. reply riku_iki 9 hours agorootparentHow 1A can protect from this? It is essentially draft. reply nindalf 32 minutes agorootparentThe government forcing you to reveal information you know is compelled speech. That’s a 1A violation, feels like. reply ClumsyPilot 17 hours agoparentprev> If you hire Chinese nationals in your tech company, you can be assured they are committing espionage Liberal principles dropped at the first sign of trouble? reply tristor 16 hours agorootparentWhat Liberal principle requires you to employ and inform your enemies who are actively engaging in espionage? I didn't say anything about Chinese /ethnicity/, only nationality. There are millions of ethnically Chinese people around the world who are not Chinese nationals and are not bound by the same legal framework. reply ClumsyPilot 16 hours agorootparent> employ and inform your enemies This is literally presumption of guilt based on where someone was born. It's the literal definition of prejudice. Very illiberal. Mind you, I am not claiming that you have to be liberal. I just want honesty and consistency. If we all oppose China because they threaten liberty, then we must protect liberty. But if you oppose China just because it's a pissing contest between superpowers, that's fine but then maybe it's not my problem. reply tristor 15 hours agorootparent> This is literally presumption of guilt based on where someone was born. Not where someone is born, but whom they owe allegiance to. If you do not discriminate based on someone's allegiances, then on what basis would you expect it to be appropriate to discriminate? The idea that all discrimination is immoral or unethical is not only suspect, it's clearly foolhardy. China is an enemy to everyone who is not China, they aren't even subtle about this. China only cares about China, and will sabotage or interrupt deals and commit espionage in order to get ahead. Anyone who owes allegiance to China is not a friend of the West, full stop. The West and the entities operating in it owe no benefit of the doubt or benefit at all towards anyone who has allegiance to China. China holds no such ridiculous notion as that they shouldn't discriminate based on allegiance, they make it clear you are either theirs or you are unwelcome. Note, I am also /not/ saying that China is necessarily doing anything immoral within their own framework here, I am simply observing that they are not, and have not been, any friend to the West, and they are clearly at this point our enemy. Just because they are our enemy does not mean that their behaviors aren't rational from their perspective or point of view and don't align with their own moral systems. We don't owe them any regard or benefit of the doubt however, as they are our enemy, and it's just as much not immoral for us to uphold ourselves against our enemies. > It's the literal definition of prejudice. Very illiberal. You and I have different definitions of prejudice and what it means to be Liberal. My definition of prejudice matches what's in the dictionary: noun 1. The act or state of holding unreasonable preconceived judgments or convictions. 2. An adverse judgment or opinion formed unfairly or without knowledge of the facts. 3. Irrational suspicion or hatred of a particular social group, such as a race or the adherents of a religion. Acting to protect yourself from an enemy and its nationals is not unreasonable, based on any unfairness, based on lack of knowledge of the facts, nor is it irrational, therefore the preconditions of prejudice aren't present. When you have a clear, rational, open basis for discrimination, it's not prejudicial, it's simply prudent. reply Cookingboy 10 hours agorootparentYour entire argument is based off the propagandized idea that \"China is our mortal enemy\". Should we cut off all tourism/business travel between China and the Wests? Cease all commercial activities? Declare war? Wait we should ask China to close the 9000 Starbucks in their country first. Your opinion, while fairly popular amongst propagandized Americans, is not supported by hard facts.. And your argument of \"all Chinese nationals could be spies means all Chinese nationals are working as spies\" is absolutely illogical. reply tristor 9 hours agorootparentI don't think China is our mortal enemy or that we should formally declare war. Nowhere in my comments do I make any of the spurious statements you're trying to attribute to me. However, it is clear that China /is/ our enemy, \"mortal\" though they may not be (currently). China exists for China, and I personally hold no animus towards China for that, but I am also not so much of a fool that I would willingly employ and inform my enemy of my secrets, regardless of whether I hold no ill will for their rational self-interest as a state. It is possible, and logically consistent, to both recognize someone is your enemy and to have respect for their position and no ill regard for their existence. It is possible, and logically consistent, to have respect for someone's position and yet identify the foolishness of providing them employment, succor, or otherwise helping them to achieve their aims, aims that are to your own detriment. Just as China is primarily acting out of rational self-interest, so too should the West. The West is far too willing to cut off its own toes just to signal some virtue which has no bearing or relation to reality. Reality always wins, and I'm merely recognizing reality. And what is peace, if not being willing to agree to disagree? I disagree with China's aims, but agree that their aims are beneficial to China. I am not obligated to assist them in their aims, nor does refusing to do so obligate me to go to war with them. Nor does it obligate anyone else in the West to assist them or to battle them, we can merely refuse to help those who are openly our enemies and let them go their own way. reply ClumsyPilot 8 hours agorootparent> it is clear that China /is/ our enemy. I personally hold no animus towards China...inform my enemy...recognize someone is your enemy... Firstly, if you are so obsessed with finding enemies everywhere, then you do hold animus. Second, you are cutting off your nose to spite your face. The US missed out on 5G technology because a foreign researcher wasn't granted a green card and sold his research to Chinese companies instead. Way to go, undermining the very foundation of your success. I think you need to realise that, outside of the west, for example in India, US is not trusted a whole lot more than China is. The allegations of threat to national security against Huawei seem suspiciously well timed, they appeared just as Huawei came out with 5G tech. To many nations this looks like economic warfare, and resorting to sanctions simply to secure markets. You need to accept the fact that China is here to stay, and third nations are going to enjoy playing China and US off against each other, and see who can offer them better deal. reply hughesjj 10 hours agorootparentprevBeing born someone does not force you to have an undying allegiance to that place, what are you on about? reply renewiltord 10 hours agorootparentIt's amusing that every alternative comment is the guy expressly saying he's talking about nationality held and who they hold allegiance to, and every response is a misunderstanding. This is normal stuff. You can even lose your US dual citizenship if you serve in another nation's armed forces. I actually pasted the sequence into ChatGPT and it seems to comprehend him easier than people responding to him. It is always interesting when the Turing test is failed because the bot is too smart and capable of comprehension. reply tristor 10 hours agorootparentprevI don't think your response is in good faith, since you are responding to a comment where I explicitly say that I have made no reference to where someone is born, only to their allegiance. Do you not understand what the term \"national\" means? A significant number of US nationals were not born here, being born here is not a prerequisite to be a national. While the same is largely not true for China, there are many people born in China who are no longer Chinese nationals because they have renounced their citizenship as part of immigrating and naturalizing elsewhere. The statement I made is not about ethnicity or birthplace, it's about allegiance. /Every/ citizen of China is required by Chinese law to participate in espionage either directly or indirectly as requested, and they are requested, the evidence is insurmountable. reply ClumsyPilot 8 hours agorootparent> I don't think your response is in good faith I don't think yours's is, you pretend to draw a difference where no practical distinction exists. For 95% of people worldwide, birthplace IS nationality - you get 1 by birth, and only have 1. You can't renounce it, that would make you illegal alien. If you put up a corporate email \"specifically Chinese citizens will get no promotions this year\" you will have immediate lawsuits on your hands, it is discriminatory. Instead you should own up to to the fact that your position is illiberal, and provide serious evidence that the threat is so great, that liberal principles must be abandoned. Of course that is much tougher and less palatable argument to make. reply toss1 10 hours agorootparentprevYou say \"it's not my problem\" But you do not live there, you live in the West. Just because the problem is diffuse and distant for you now, does not mean it is not your problem. If we all fail to treat it as the problem, it will become concentrated and local here also. (but sure, right now, it's more convenient to ignore it and party) reply ClumsyPilot 8 hours agorootparentIt seems awfully convenient that when western corporations were cutting jobs and outsourcing to China we were told to take one for the team capitalism. Same concerns about human rights in China existed. And now when cheaper Chinese cars/5G/etc threaten profits, we are again told to take one for the team. I don’t think we get to party, looks a bit like a racket reply 2OEH8eoCRo0 17 hours agorootparentprevhttps://www.fbi.gov/investigate/counterintelligence/the-chin... > Talent plans can sometimes foster legitimate sharing and collaboration as part of an appropriate business arrangement or research exchange, but this is not the norm. > Instead, talent plans usually involve undisclosed and illegal transfers of information, technology, or intellectual property that are one-way and detrimental to U.S. institutions. reply ok123456 19 hours agoprevnext [6 more] [flagged] woooooo 19 hours agoparentIt's cut and dried industrial espionage. Yes, we do it too, but this guy was caught red handed. reply ok123456 19 hours agorootparentWhy, then, have a DOJ spokesman do a press conference in the middle of an ABA function? reply woooooo 17 hours agorootparentBecause politics. Doesn't change the simple facts of this case. reply benopal64 19 hours agoparentprevnext [3 more] [flagged] edgyquant 19 hours agorootparentnext [3 more] [flagged] ok123456 18 hours agorootparentnext [3 more] [flagged] azinman2 18 hours agorootparentnext [3 more] [flagged] ok123456 17 hours ago [flagged]rootparentnext [2 more] You somehow missed all the jobs and communities destroyed in your analysis. reply azinman2 16 hours agorootparentI don’t disagree, but I think the thought was they would/should transition up the ladder, and that American labor was too expensive to be globally competitive. The later point may just be too true to go back in time. The world evolves; you can’t stay static in time. reply lupire 19 hours agoprevnext [3 more] [flagged] HarHarVeryFunny 18 hours agoparentJustice dept and FBI love any chance for photo ops and press releases patting themselves on the back. I'm not sure there's too much ace detective work to be proud of here - seems Google heard he'd presented as a CEO of a Chinese firm days after he quit, and must have given the FBI a call. It makes a good story, but the current crop of LLM-based AI's don't seem to require much more than a bit of prior job experience to build. The most successful recentish AI/LLM startups such as Anthropic, Mistral and Reka.ai all have ex. Google, OpenAI and Meta employees as founders, and that prior experience appears plently enough to hit the ground running. reply ParetoOptimal 18 hours agoparentprev> > The case against Ding, 38, was announced at an American Bar Association conference in San Francisco by Attorney General Merrick Garland, Yeah, this seems very suspicious. reply boringuser2 19 hours agoprevnext [19 more] [flagged] neom 19 hours agoparentYah, so there is a problem here in that many of these \"homeland nationalistic\" folks who send stuff back to \"their governments\" are in many instances, citizens of the country they are betraying. Since these two[1][2] incidents, I wondered deeply for a while, what is going to happen here as it seems the CCP is getting more and more brazen. I don't believe it would be legal for \"enhanced background checks\" on someone in the hiring process because they are Chinese American, and it opens an awful door regardless. I don't really know the solution, but I've thought a lot about the problem. (And yes, of course there is that 5 eyes/other \"Western\" powers are almost certainly doing similar stuff) [1] https://japantoday.com/category/world/2-us-navy-sailors-arre... [2] https://www.justice.gov/opa/pr/harvard-university-professor-... reply delfinom 18 hours agorootparent>I don't believe it would be legal for \"enhanced background checks\" on someone in the hiring process because they are Chinese American, It absolutely would be legal if AI was recategorized as ITAR. In 2022, dual citizenship was cracked down on in ITAR regulations and dual(or more) citizen individuals must now be deeply vetted for contact with ITAR Section 126.1 countries which include China. reply neom 18 hours agorootparentI am by no means an expert at all, however my very basic understanding is that ITAR is primarily concerned with important/export controls? Even if AI was added but the specific staffing was to a project was totally unrelated to something that would be imported or exported (this guy was an infra guy it looks like?), would that apply? reply freedomben 18 hours agorootparentI'm no expert either, but from living through the painful days of ITAR and encryption, it only matters that some technology/information can be used for military purpose for it to be restricted. For years we couldn't distribute the code for AES above 128 due to ITAR, even though it was widely and easily available all over the internet. reply neom 18 hours agorootparentIf you'd care to opine, would you then there agree with delfinom that ITAR in AI could be a solution to this? From what you just described, it made me wonder if it would impose undo burden on, and so hamper, the emergence of the next generation of NN/ML tools (\"AI\"). reply freedomben 16 hours agorootparentThere's considerable nuance involved in the question, which I haven't done due diligence on, but with that as a disclaimer... Thinking out loud, it does seem like it could help, although it does place a significant (at times) burden on people affected, and in the case that I worked with it made no practical difference whatsoever at preventing the use of the technology by US adversaries. So yes I think it might could help, but my inclination is away from it as I think it would ultimately be ineffective yet quite burdensome on US companies. Possibly to the point of backfiring, leading to a country like China that is (so far at least) much more \"open\" in regards to AI/ML. Do you have any thoughts? reply neom 15 hours agorootparentI have such limited knowledge of this stuff it's all just gut, but I'd be aligned with you when pushed to comment. The best solution is probably no solution, it's going to happen and that's that. reply programmarchy 19 hours agorootparentprevAt the end of the day, do we care about Google trade secrets or US national security? reply JumpCrisscross 19 hours agoparentprev> Hiring a Chinese national and giving them access to trade secrets To be fair, this would have been prohibited if we subjected AI to ITAR restrictions. That said, do we know Ding was a Chinese national? reply lupire 19 hours agorootparentIf US prevented Chinese people for working on US AI, US would be in a far worse position in AI. Half of AI contributors in US are Chinese. reply JumpCrisscross 18 hours agorootparent> Chinese people If you mean people of Chinese national origin, yes. If you mean Chinese nationals, I’m unconvinced. reply xienze 18 hours agorootparentprevWhy do they bother spying on us if they’re so far ahead then? reply ziddoap 18 hours agorootparentBecause the other half of contributors aren't Chinese? reply xienze 18 hours agorootparentBut what value are they providing to the Chinese? The original assertion was that American AI is only where it is because of the contributions of Chinese nationals. I.e. they’re leading the way. Why even contribute to our efforts and potentially improve our standing? reply ziddoap 16 hours agorootparent\"Leading the way\" doesn't mean that other contributors have no value whatsoever. Leaders can still learn from new/other perspectives, and non-leaders can still have good ideas. It is obviously valuable to have quick/inside access to new and promising perspectives. And sometimes those ideas are going to come from the other 50% of contributors. reply onlyrealcuzzo 18 hours agorootparentprevProbably because most of China's top talent leaves. reply nextworddev 19 hours agorootparentprevYes according to the article reply boringuser2 19 hours agorootparentprevIf he's not a Chinese national, that makes the discussion much more likely to get me censured when I suggest the obvious corrective remediation. reply 139 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Former Google software engineer Linwei Ding faces charges for stealing AI trade secrets from Google while collaborating with Chinese companies, punishable by up to 10 years in prison.",
      "Google detected the theft, prompting law enforcement involvement, resulting in Ding's arrest and the confiscation of electronic devices holding stolen data.",
      "The Justice Department highlights worries about national security due to foreign entities potentially exploiting AI tech for nefarious activities."
    ],
    "commentSummary": [
      "The focus is on trade secrets and intellectual property theft, especially by Chinese nationals in the tech field, discussing espionage cases, motivations, and legal consequences.",
      "Concerns are raised about Chinese companies' influence, the Chinese Communist Party, and the delicate balance among global economic relations, espionage, and national security.",
      "The dialogue also touches on dual citizenship, biases, and the intricate dynamics of international relations within AI development and espionage efforts."
    ],
    "points": 323,
    "commentCount": 399,
    "retryCount": 0,
    "time": 1709818426
  },
  {
    "id": 39631516,
    "title": "The Pile: 800GB Open-Source Language Modeling Dataset (2020)",
    "originLink": "https://pile.eleuther.ai/",
    "originBody": "The Pile An 800GB Dataset of Diverse Text for Language Modeling What is the Pile? The Pile is a 825 GiB diverse, open source language modelling data set that consists of 22 smaller, high-quality datasets combined together. Pile Paper (arXiv) Download The Pile is hosted by the Eye. Download Pile The format of the Pile is jsonlines data compressed using zstandard. Have a model that uses or evaluates on the Pile? Let us know! Why is the Pile a good training set? Recent work has shown that especially for large models, diversity in data sources improves general cross-domain knowledge of the model, as well as downstream generalization capability. In our evaluations, not only do models trained on the Pile show moderate improvements in traditional language modeling benchmarks, they also show significant improvements on Pile BPB. Why is the Pile a good benchmark? To score well on Pile BPB (bits per byte), a model must be able to understand many disparate domains including books, github repositories, webpages, chat logs, and medical, physics, math, computer science, and philosophy papers. Pile BPB is a measure of world knowledge and reasoning ability in these domains, making it a robust benchmark of general, cross-domain text modeling ability for large language models. Citing If you use the Pile or any of the components, please cite us! @article{pile, title={The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling}, author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor}, journal={arXiv preprint arXiv:2101.00027}, year={2020} } Leaderboard * indicates potential test-set overlap. Zero-shot indicates that not all of the components of the Pile were present in the training data. Rank Model Test BPB 1. Jan 1.2021 GPT-3 (Zero-Shot)* OpenAI 0.7177 2. Jan 1.2021 GPT-2 (Zero-Shot)* OpenAI 1.2253 Evaluation code EleutherAI 2021",
    "commentLink": "https://news.ycombinator.com/item?id=39631516",
    "commentBody": "The Pile is a 825 GiB diverse, open-source language modelling data set (2020) (eleuther.ai)319 points by bilsbie 16 hours agohidepastfavorite214 comments Ninjinka 15 hours agoI raised a concern about the inclusion of books3 in the Pile back in 2020, and this is what the head of Eleuther (Stella Biderman) told me: \"So here’s the big picture. There are three sets of datasets: 1. Data exists out there in the world. It has been collected into datasets and posted online. I’ll call this raw data. 2. We take that data, clean it, and process it for language modeling. I’ll call this per-set data. 3. We combine those per-set data into one massive dataset, the Pile. This is heavily processed, including weighing the components. We created 2 and 3 and put them online. We put 2 online so that people can reweigh and remix the data if they wish, but we expect most people to just download 3 and use it out of the box. Access to 3 will be provided in several forms, including HuggingFace and from our website. 2 and 3 are not copyright violations, even if the data is copyrighted, because they fall under fair use (at least in the US). The Pile contains code that turns 1 into 2 and code that turns 2 into 3. When you download Maroon 5 from a website, you are creating a dataset corresponding to 2. That can be copyright violation depending on what you do with it, but our use is not a copyright violation.\" reply otterley 11 hours agoparent> 2 and 3 are not copyright violations, even if the data is copyrighted, because they fall under fair use (at least in the US). This cannot be known until it is litigated. Fair Use is not something you can unilaterally declare and have it be so, just like you can't be like Michael Scott in the Office shouting \"I declare bankruptcy!\" OpenAI is currently defending itself against the New York Times for this very reason. There's a multi-factor test that courts weigh the facts against in making a determination as to whether a prima facie copyright violation would be protected under a Fair Use defense: Factor 1: The Purpose and Character of the Use Factor 2: The Nature of the Copyrighted Work Factor 3: The Amount or Substantiality of the Portion Used Factor 4: The Effect of the Use on the Potential Market for or Value of the Work See https://copyright.columbia.edu/basics/fair-use.html for a pretty good overview of what the analysis entails. reply wiremine 11 hours agorootparent> This cannot be known until it is litigated. Fair Use is not something you can unilaterally declare and have it be so. Correct, but what isn't clear here is their rationale for why they think they're covered by fair use. Does anybody have that information? I'm not saying their interpretation is correct, but seems to be germane to this discussion. The parent comment seems to assume none of this has been litigated yet, which might also be true. Or not. reply nickpsecurity 7 hours agorootparentThey're hoping that $10 billion will buy them the kinds of lawyers Oracle had when they basically rewrote copyright law on API's. The AI companies hope to do this in a way that makes everything they're doing either legal or too muddy for lawsuits. reply ryukoposting 11 hours agorootparentprevThanks. This is really informative, and really important information given the growing relevance of IP law in everyone's daily life. Part of me wonders if these four factors will ever become part of core curriculum for civics classes. By no means am I an expert in copyright law, but factor 3 seems like very bad news if you're OpenAI. reply fluoridation 11 hours agorootparentprevWhether something is fair use or not is not determined by a court, but by the definition of what fair use is. A court interprets that definition and the situation, and if their interpretation matches yours you may have a ruling in your favor. But saying \"this is fair use\" is no more incorrect than saying \"this is red\". You're interpreting your perception and putting that interpretation into words. reply dkarras 10 hours agorootparent>But saying \"this is fair use\" is no more incorrect than saying \"this is red\" No, they are different. \"fair use\" is a legal term. It is not like saying \"I use it like this, I think it is fair!\", the term \"fair use\" literally is a legal term that means a particular thing in the court of law. https://en.wikipedia.org/wiki/Fair_use reply otterley 11 hours agorootparentprev> But saying \"this is fair use\" is no more incorrect than saying \"this is red\". When a court determines that it isn't, you can continue to argue it as much as you like (to deaf ears), and yet you're still liable to the copyright holder. Whether it's \"incorrect\" or not is then irrelevant. Let's not argue semantics here. reply layer8 14 hours agoparentprevI don’t understand how this can be true if set 2 contains a complete copyrighted work (say, a book) that the copyright owner hasn’t approved for such distribution. Unless I misunderstand and the “process[ing] for language modeling” is an entirely irreversible process. reply michaelt 13 hours agorootparent> Unless I misunderstand and the “process[ing] for language modeling” is an entirely irreversible process. In the case of The Pile, \"processing for language modelling\" means \"converting epub and pdf into plain text, maybe deduplicating, maybe removing some sorts of detectably malformed files\" So not a particularly lossy conversion. reply layer8 13 hours agorootparentI see, thanks. Yes, in that case, I don’t see how this can possibly not constitute copyright infringement. reply bayindirh 12 hours agorootparentIt's generally tucked under Fair Use doctrine because \"It's for the science\", until it doesn't (looking at commercial AI non-profits). Then \"they're doing something amazing, they don't need permission, and the cat is already out of the bag, and similar musings\". Seriously, it's both copyright infringement, and unethical. This is why I don't use any of the popular AI tools, or even AI add-ons in Evernote, Notion, etc. They all link back to the usual suspects. reply CobrastanJorji 12 hours agorootparent> \"They're doing something amazing, they don't need permission, and the cat is already out of the bag\" Ah, the Uber theory of law. Works surprisingly well for some reason. reply bayindirh 12 hours agorootparentProbably due to Murphy's Golden Law of Golden Laws: Who has the gold makes the laws. reply Grimblewald 12 hours agorootparentprevThe question then becomes do these concerns remain even for AI that cannot reproduce origional works? Does what does that mean for us? When we read things, or interact with Amy information for that matter, it changes us and how we do things. If you consume art it will forever influence art you produce yourself. Are these copyright infringements also? I can see the problem where direct and faithful replication is possible but where it isn't is there still a problem? Or is the automatable aspect, the scale at which it can occur, that is the problem? reply bayindirh 11 hours agorootparentThe difference is what you mix, and the amounts of things you mix. As a human you mix many more inputs, plus your emotions, plus the things you consume to create it. Moreover, what you can consume and how perfectly you can consume is limited by our innate limits. An AI system consumes something perfectly, then ingrains it into its weights perfectly, and becomes capable of imitating the same thing perfectly. Plus, ther are no other internal or external factors which affect these \"generation\" over time. Hence, it mixes and reproduces based on what it consumed, solely. I might get inspired by people, and add my own values to it, iterate over it and diverge from what I'm inspired from ultimately to create my own style. AI doesn't work like that. Also, if I do the same amount of inspiration with the same precision and accuracy, I'll be neck deep in accusations and lawsuits (for the right reasons). As a result, just because we fail to ask the right questions to reproduce the training data verbatim or almost verbatim doesn't mean that the information is not there. At the end, a neural network is a compression algorithm which encodes data in terms of weights. Given the correct input, you can regenerate the training data as is. Unless you have special abilities, you can't read 500 books an hour, remember them perfectly, and generate derivative works by mashing all of them together. If I do and try to sell a novel, I'll be ridiculed no end. If I write a Ph.D. the same way and try to defend it, I'll be banned from academia for three lifetimes at least. For more elaboration on the subject, see [0]. [0]: https://news.ycombinator.com/item?id=39188463 reply miki123211 11 hours agorootparentThe myth that AI models store all its training data in its weights verbatim is as widespread as it is false. In fact, if this were the case, deep neural networks would be considered far better compression algorithms than anything we have on the market right now, by literal orders of magnitude. If you divide Stable Diffusion's file size by the number of images used to train it, you get something like 1.2 bits per image, and it is physically impossible to get this kind of a compression ratio. The actual problem with AI is that it sometimes plagiarizes random fragments of the work it is trained on, even if it is not the user's intend, and we currently don't really know how to fully prevent this. reply bayindirh 11 hours agorootparentIt still doesn't change the fact that the inclusion of commercial works is copyright infringement though. Same for code generating models trained on Open Source and Free Software. Tons of licenses violated, from strong copyleft to source available models and reproduced (almost) verbatim with comments intact. Some researcher's codebase is almost completely reproducible without any licensing information just by hinting the function names. Maybe for the image compression it's borderline impossible for now due to network size, but for text and code, generation of training data almost verbatim is very possible and straightforward. Also in image generation models, style transfer is the bigger problem, because it completely eliminates the artist who uses/created the style in the first place. \"You pioneered this, and we fine tuned this model with your images, and we can do your work for free, without you, have a nice day\". However, the artist's life expenses doesn't disappear when they're transferred to an image generation model. This is also unethical. reply miki123211 3 hours agorootparentStyle transfer is perfectly legal AFAIK, asking artists to do a drawing of X in the style of Y was already a thing. Style is not copyrightable. reply bayindirh 2 hours agorootparentI didn't mean to say style transfer is illegal, it's not, but doing it en-masse is unethical. Just because you can doesn’t mean you should. That's what I'm trying to say. reply rasz 1 hour agorootparentprevIts not always that simple https://ethicsunwrapped.utexas.edu/case-study/blurred-lines-... reply numpad0 10 hours agorootparentprev> you get something like 1.2 bits per image, and it is physically impossible to get this kind of a compression ratio. IIRC it was like 1.4 bytes before adding in random initial and prompt. And Amiga Four-byte Burger is 4 bytes long. reply bayindirh 10 hours agorootparentFor reference, this [0] is Amiga Four Byte Burger, and it looks impossibly good. [0]: https://bytecellar.com/2023/04/24/lost-amiga-four-byte-burge... reply ribosometronome 11 hours agorootparentprevWhether or not I'm influenced by media isn't super relevant to whether or not I pirated that media. Before even arriving at the question of whether or not the resulting models are infringing, it's clear the training data is. reply __loam 11 hours agorootparentprevThe \"humans do it too\" argument is totally irrelevant to this because humans have special and specific privileges under the law that computers don't. The problem is that a lot of data was copied into training sets and used for commercial purposes without permission. reply layer8 12 hours agorootparentprevI’m talking about distributing the corpus, which by itself is not bound to any particular usage. reply bayindirh 12 hours agorootparentIt's again copyright infringement. If I share a copyrighted ebook by accident, any and every cloud provider will ban my account with no warning and recourse. Open science repositories would take down the \"dataset\" immediately (or at least limit its access) if a copyright holder brings the matter to the eyes of the admins. reply CharlesW 14 hours agorootparentprevEven if the model encoding is not lossless/reversible, it's probably not true. A good place to start when thinking about fair use is the \"four factors\" that the U.S. legal system will consider. https://fairuse.stanford.edu/overview/fair-use/four-factors/ reply kmeisthax 13 hours agorootparentGoogle keeps coming up with new ways to statistically infer training set data from models. So it's not entirely lossless. At the very least, models that have been trained on a particular work are unusually good at compressing[0] those works, relative to other valid text. In terms of fair use, one of the larger factors is the 'market substitution' factor, which basically means \"does this use compete with otherwise licensed uses that people would ordinarily pay for?\" AI absolutely does compete with human artists for the same market. In fact, it's winning handily[1], because you don't have to pay human artists. AI art models absolutely shouldn't be trained on anything with copyright on it. The other factors don't fare much better. Nature of the original work will differ based on the plaintiff, but the purpose and character of the AI's use of that work is very much commercial. And the amount and substantiality of the use is complete and total. I don't see AI being fair use - at least, not in every one of the many, many training lawsuits currently ongoing against OpenAI and Stability. [0] Starting with any body of text, an LLM, and an empty context window, compute the next-token probabilities and take the highest one. If it matches the source text, output a 1 bit. If it doesn't, output 0 followed by the ID of the correct next token. Add the correct token to the context window and repeat until the text has been fully compressed. This produces a list of perplexities (wrong words) for the given text which can be used to guide the LLM to output the original work. [1] Hey, remember when both WotC (biggest art commissioner on the planet) and Wacom (hardware vendor that sells art tools and payment terminals[2]) both got caught using AI art after making very loud and public pledges to not do that? They both wound up buying stock photography on marketplaces that are absolutely flooded with AI trash. [2] All the credit card readers in Japan are built by Wacom, which is really funny as an artist reply layer8 13 hours agorootparentprevSummary books for example are legal, so there is some threshold of compression where things are fine. reply knodi123 13 hours agorootparentare you referring to things like CliffNotes ? reply layer8 13 hours agorootparentI’m referring to the “Summary of ” booklets you can find on Amazon. Also services like Blinkist. reply knodi123 9 hours agorootparentHuh. Never heard of those. The internet sure has a lot of dark little nooks and crannies. reply nickpsecurity 7 hours agorootparentprevOn top of that, add patent and trademark law which also ban things A.I.'s might generate from training data. In the case of patents, they can't reproduce the invention even if it's an independent creation. From there, the damages go up if they did it on purpose. That some are trained on patent filings and research papers about patented inventions is just asking to be in a patent suit eventually. And that's legitimate inventions I'm talking about. Just wait until the patent trolls figure out how to get the A.I.'s to divulge patent violations to sue the A.I. suppliers. reply IshKebab 14 hours agorootparentprevYeah I agree. If 2 contains complete copyright works (e.g. all of Harry Potter) then \"we're just using it for AI training!\" stands approximately zero chance of passing the fair use test. Their assertion that it does is just wishful thinking. reply HenryBemis 13 hours agorootparentI'm playing stupid now: I believe that if I ask the LLM to \"display Harry Potter Book 1\" and it does, word-by-word, then you're 100% right, it's copyright infringement. But, if I ask the LLM to \"give me an analysis of the \"Professor Severus Snape's\" character and it gives me one, then I don't see the problem. So in that sense I understand the response that \"they don't violate copyright\" by studying the material. Again, I don't pretend to be a lawyer, and not every law has to follow my logic. reply swatcoder 12 hours agorootparentThat's a different discussion. This isn't about the output for content generators or about the abstract numeric weights that they operate over. That's more complex and a largely open question. But this is literally about indiscriminately distributing copyrighted works in a large, convenient archive while arguing that it's okay because you normalized the formatting a bit and because you suspect that some people might find \"fair use\" value in it. reply mistrial9 13 hours agorootparentprevsaid with confidence, however Silverman et al Judge explicitly rejected what you just asserted AFAIK reply papercrane 13 hours agorootparentYou've got it backwards. The judge in Silverman et al dismissed the claims asserting that OpenAIs output is copyright infringement. The claims for copyright infringement in the training data are still going forward, that will directly test whether it is \"fair use\" or not. From the ruling: > Assuming the truth of Plaintiffs’ allegations - that Defendants used Plaintiffs’ copyrighted works to train their language models for commercial profit - the Court concludes that Defendants’ conduct may constitute an unfair practice.6 Therefore, this portion of the UCL claim may proceed. https://caselaw.findlaw.com/court/us-dis-crt-n-d-cal/1158180... reply mistrial9 12 hours agorootparentaha - much appreciated reply whimsicalism 13 hours agorootparentprevno, she didn’t reject the claim that training on copyrighted work is infringement, merely that the outputs are not infringing simply by bearing similarity to the texts reply doctorpangloss 14 hours agorootparentprevnext [19 more] [flagged] Cheer2171 13 hours agorootparentBy that logic: Best to operate under the presumption that Uber's contractor-based employment model is legal, because we wouldn't be talking about any of this gig economy stuff if it weren't for that. You can argue that the law should change, but SV has this absurd idea that it somehow can't be illegal if it is a part of a successful business model. reply idiotsecant 13 hours agorootparent>Best to operate under the presumption that Uber's contractor-based employment model is legal. Yes, that's how the law works. If something is in a grey area you can do it until the government tells you not to. reply Cheer2171 10 hours agorootparentIt wasn't a grey area. The definition of who is an independent contractor vs an employee is pretty clear in US and especially California law. The only grey area is Uber just saying that all of their workers should be considered independent contractors because everything was through an app. Courts didn't buy the \"it's OK because of an app\" argument, so Uber and Lyft bankrolled a citizen initiative in California to amend the constitution (Prop 22) to declare that workers for app-based ride-sharing and delivery platforms are independent contractors and not employees. So for years, they operated illegally, gaining market share subsidized by VC funds and wage theft of driver's benefits, then changed the law to legalize what they had been doing from the start. reply wrs 13 hours agorootparentprevA grey area by definition is where you aren’t sure whether the government has told you not to, so that’s not very useful guidance. reply sangnoir 13 hours agorootparentprevThis is the reason why Silicon Valley is losing its reality-distortion field image of disruptive innovators changing the world and being seen as the exploitative assholes on par with - or exceeding the cocaine-addled \"greed is good\" finance bros from the 80s. The attitude and attendant results from sloppiness are just begging for legislative clamp-downs similar to what happened with finance. You can only profitably arbitrage gaps in the law for so long before someone goes too far and there's a scandal or two, and public opinion turns against you, followed by the business-end of legislative attention. reply CamperBob2 12 hours agorootparentIt's hard to call out people for being \"exploitative assholes\" for disrupting the legacy taxi cartels. The \"exploitative assholes\" at Uber appeared on the scene because the market demanded something better. As for the government, they are theoretically supposed to be responsive to the people, and the people have spoken loudly and unequivocally in favor of the \"exploitative assholes.\" reply sangnoir 12 hours agorootparent> It's hard to call out people for being \"exploitative assholes\" for disrupting the legacy taxi cartels. It really isn't, if you don't fall for the false dilemma: taxi cartels and Uber both exploit their drivers and passengers in different ways. > ...the people have spoken loudly and unequivocally in favor of the \"exploitative assholes.\" Public support is turning against tech slowly, and then rapidly. We will deserve it, going by the comments and attitudes I frequently see on HN. While I'm grateful for the high tech salaries, the prospect of wealth attracted a lot of people whose aims start and end with getting rich by any means, and will ruin it for the rest of us. reply layer8 14 hours agorootparentprevYour last sentence is a non-sequitur. reply wcrossbow 13 hours agorootparentAbsolutely. This is very much like the answer you would get from a politician. It's proof by word count. Q: Why do you think X is Y? A:... therefore Y. reply sva_ 12 hours agorootparent> proof by word count Sorry for off-topic, but I love that expression. reply doctorpangloss 13 hours agorootparentprevAll of this conversation aside, anyone who is saying \"fair use\" concedes that their use more or less would be explicitly unauthorized, but because it advances \"some\" \"goals\" in this country, it should be allowed anyway. It is a necessarily adversarial stance but it belies that the author the Pile sincerely wants to advance human progress in an idiosyncratic way. I think this is what pisses off the commenters here, they want to take sides, but like, who cares. > I don’t understand how this can be true if set 2 contains a complete copyrighted work (say, a book) that the copyright owner hasn’t approved for such distribution. Everything you're saying could be true, but also meaningless. Meaningless in what sense? In my personal opinion, here are some meaningful things: - Economic meaning: Is the Pile making anyone suffer, in an intellectually honest sense? No. Is the way it is being used making anyone suffer? No. Would anyone use the Pile if they had to pay for it? No. Would some tiny amount of money paid for it matter? No, not to anyone. The Pile is but one of many things that are going on in this pareto-inefficient world that we gain little economically by fucking with. - Meaning in the sense of human progress. Is the Pile sincerely, substantively helping advance creative and scientific progress? Yes. Can the progress it promotes coexist peacefully, in this current status quo, with selling books on Amazon, which can also advance progress? Yes. Is there any evidence that this non-economic progress, such as sharing important stories or points of view meaningfully through text content, is harmed by the Pile? No. The following ways of looking at this issue are meaningless: - Psychological meaning is meaningless. Feelings that are more strongly felt by more favored parties, like beloved authors versus impetuous programmers, are not more valid. Stakeholders like authors merely exercising their collective bargaining power over something that doesn't actually make them worse off is meaningless too. Social media drama also doesn't matter. These are just opinions. I don't go out there and say I'm a lawyer, and I'm not running for Congress, and neither are you. It's crazy to me, because when you look critically at what dog you personally have in this race, like of course you want the status quo where The Pile exists, and you and the only agitators in all of this - authors' guild authors - gain nothing from nearly every dead-on-arrival framework like \"Compensation, Credit and Consent\" or whatever, BESIDES meaningless psychological satisfaction of winning social media arguments and flexing the favorability of authors over programmers. Imagine if we ran the whole country this way! It's radioactive. So this isn't a non-sequiter. Supporting the status quo, in this particular case, is the succinct way of saying everything here: that you can be right in ways that don't at all matter, which is okay, but you should have the insight or maybe if you are a programmer or an author or actually you are running for Congress, you have the duty to understand this stuff. reply mkipper 13 hours agorootparent> It's crazy to me, because when you look critically at what dog you personally have in this race, like of course you want the status quo where The Pile exists, and you and the only agitators in all of this - authors' guild authors - gain nothing from nearly every dead-on-arrival framework like \"Compensation, Credit and Consent\" or whatever, BESIDES meaningless psychological satisfaction of winning social media arguments and flexing the favorability of authors over programmers. Do you really expect anyone to engage with you in good faith if you feel this way? Copyright law isn't perfect, but most of the great creative works from the last few centuries wouldn't exist if their authors didn't have legal protections allowing them to monetize their work. Redistributing existing media without worrying about copyright might be great in the short term, but the next century of art will probably look pretty barren if that becomes the status quo. Maybe we'll reach a point where the benefits of training AI outweigh the benefits of all human creative output. But you don't need to be a stooge of some author's guild to not feel like we're there today. reply kmeisthax 12 hours agorootparentprev> and you and the only agitators in all of this - authors' guild authors - gain nothing from nearly every dead-on-arrival framework like \"Compensation, Credit and Consent\" or whatever, BESIDES meaningless psychological satisfaction of winning social media arguments and flexing the favorability of authors over programmers The first agitators were Free Software developers, namely the Software Freedom Conservatory and Hector Martin, who pointed out that GitHub's proprietary cloud hosted code completion LLM was trained on shittons of GPL code and regurgitated a lot of it[0]. The only reason why book authors, visual artists, and so on are suing AI companies is because the Free Software people - who otherwise oppose software copyrightability - wrote the rhetorical framework that everyone else is using. [0] My favorite example is that commenting \"evil floating point level bit hack\" produces the entire Quake lighting function. Or at least it did until Microsoft specifically filtered that phrase out from code completion. reply layer8 13 hours agorootparentprevWe are talking about the distribution of that corpus of text. Whether someone uses it for ML training or not is immaterial to the question of whether distributing it is legal. reply doctorpangloss 13 hours agorootparentI am saying that you can be right that it is illegal, but nobody cares except for meaningless reasons. reply layer8 13 hours agorootparentI don’t see how you can say it is meaningless if it gives anyone who downloads the pile access to countless copyrighted books. Unless you believe that copyright in general is meaningless, in which case we have nothing further to discuss. reply quatrefoil 13 hours agorootparentprevYou make a lot of categorical assertions here, but I don't think they're really all that watertight. If any individual \"economically harmed in an intellectually honest sense\" if PG&E overcharges every subscriber five cents on every bill? No, and a class action lawsuit against that practice doesn't make anyone better beyond \"meaningless psychological satisfaction\". But it's still something we do as a society, for good reasons, right? Here, you have a number of commercial entities that are poised to make a ton of money off individually small but entirely non-consensual contributions of other people. You also have a \"data laundering\" industry where the datasets of webpages, books, and images are published by notionally non-profit entities that are in one way or another bankrolled by the commercial players, keeping their hands clean. Framing the debate as being about \"human progress\" is sort of goofy in that world. We're not talking about academics tinkering with wacky ideas. We're talking about an industry that takes your work without asking and monetizes it. Yes, the sum of it is greater than the component pieces, but that doesn't mean you get to do whatever you want. reply doctorpangloss 13 hours agorootparent> If any individual \"economically harmed in an intellectually honest sense\" if PG&E overcharges every subscriber five cents on every bill? No. Nobody cares. > But it's still something we do as a society, for good reasons, right? I really don't think whatever you're talking about matters. I don't think anyone cares about class action lawsuits over pennies except assholes. But I enjoy that you are reading these comments. I also think you are ascribing way too much meaning to the legal process. Process and procedure matter to lawyers. There are places where you can never get relief from the law, and there is no process and procedure, but oftentimes, people will concede that they are governed by rules and laws. In the US the starkest example was probably the BP oil spill, where they had video of oil coming out of the thing, there was nothing to investigate, and in 15m a meeting between political and business executives concluded that the fine would be large but not ruin BP. The next 2 years of process for that disaster were meaningless under the same framework of meaning I gave you. It provided psychological satisfaction to all sorts of people, maybe especially to the people who had to feel good about taking a big check from BP for destroying the ecosystem except by fishing instead of by extracting oil. But it didn't matter, they could have also not done 2 years of process and nothing would change. > Here, you have a number of commercial entities that are poised to make a ton of money off individually small but entirely non-consensual contributions of other people. The Pile is doing the exact opposite, and unauthorized but nonetheless legal use of copyrighted material for LLM training is the only way you will see a world where non-commercial entities, such as authors, can use the technology for free, in whatever way they choose it to be aligned, under whatever rules. The DMCA already made movie piracy cost $100,000 a pop more than 2 decades ago, and yet here we are. Who. Cares. Did any of it work? You can find lots of bad guys in movie piracy, and extremely few good guys, and it turns out none of it matters. Surely if we can tolerate them we can tolerate the Pile. > We're not talking about academics tinkering with wacky ideas. You might mean that they specifically are not graduate students or professors at Universities, sure, but they are all Ivory tower, bone dry Ivy League or adjacent academics. Some of the people at the very top may have dropped out of Stanford or Harvard or whatever, and some of them may have some nuanced or fringe beliefs about specific journeys through learning for specific people, but they are very much the kind of person who would be called \"academic,\" as an insult, by a layperson. This is just to say that you are immediately reaching for the good versus bad people appeals. Like it's the same shit, it's the same energy as saying the authors are right and the weirdos are wrong. You have some favorable idea in your head of nice humanities PhDs, and yes, they are nice, and this disfavorable view of mean compsci bachelors, but does it matter? In a real sense they are cut from a similar cloth, but in the imaginary setup in your head they're different. > We're talking about an industry that takes your work without asking and monetizes it. This is the same appeal. Authors Guild is the industry. Writers Guild is the industry. You are mixing up your antagonists and protagonists. You need to get an independent opinion, colorfully we could call it a \"contrarian\" one, about who all these people really are. And then you'll see, man, it's not so black and white, and I wonder why people are arguing about this shit that does not matter. If you are a guilded writer, I will tell you now, you are wasting your time worrying about this stuff, because monetizing via traditional publishing or screenplays has been bad forever, and the Pile changes nothing. reply dougb5 12 hours agoparentprevI don't know what the right answer is to the copyright questions, but I hope that in 2024 we'll have a better attitude about the human labor that went into these models than \"Data exists out there in the world\" and the passive-voice \"It has been collected into datasets\" reply clooper 11 hours agorootparentHave you heard of data unions? reply dougb5 10 hours agorootparentI have not! What's your preferred background reading on this? reply clooper 10 hours agorootparenthttps://www.thedataunion.org/ reply whimsicalism 13 hours agoparentprevscraping libgen and downloading copyrighted content and redistributing it isn’t illegal? call me skeptical, seeding a torrent of movies that you downloaded from elsewhere on the internet isn’t “fair use” and the pile isn’t just code for transforming data, it is the redistributed data itself by this logic i could legally run a libgen mirror reply artninja1988 15 hours agoparentprevHopefully that is correct. The pile has been very valuable for open model work. It's a really high quality dataset reply tycho-newman 11 hours agoparentprevFair use is a defense to infringement. Do not start your copyright argument by admitting you infringed. reply MacsHeadroom 6 hours agorootparentFair use is an exception to infringement. Use which is fair is non-infringing. For example, Google books containing a searchable copy of every book ever written is fair use, as is Google's cache containing every news article and web page. reply nickpsecurity 12 hours agoparentprevThey’re distributing copyrighted works without the authors permission, using them in ways that compete with the author, many make money off AI’s, and the AI’s reproduce some verbatim. These datasets seem to fail most tests (\"four factors\") in copyright law. Even laypeople I’ve explained LLM’s to think the AI companies are ripping others’ work off. For those concerned, I have an article that covers legalities, each dataset (including The Pile), legal issues with them, alternatives that are legal, and a copyright amendment that balances all sides. http://gethisword.com/tech/exploringai/ Looking back at my proposal, I think we need at least three rules passed immediately in at least one country: 1. All copyrighted works can, if a person has legal access, be used for training AI systems. Any terms restricting copyrighted works from use in training, charging more for that, restricting downloads for it, etc are illegal. Every act of publishing can benefit both a human mind and AI training equally. 2. People can copy and transform for their own use any work they have access to only for AI training. This might include reverse engineering for extraction, multiple copies in different formats, and so on. They can do whatever is needed to get it into the AI system. Other uses or abuse of this data is subject to existing law. 3. Any work published online for free and with public access can be copied, shared, processed, and bundled for AI training. That’s regardless of its terms. Note: In No. 2 and No. 3, the resulting AI’s copyright will be determined by existing law about AI’s and mixing copyrighted works. Or no copyright if that’s the law. 4. If AI outputs are copywritten, their status will be the same as if the user published it themselves while relying on prior works. AI training sets will also be public to determine this. With those rules, we can share works like those in The Pile, still pay creators that want to be paid, be less likely to just steal existing work, and infringement in outputs is still illegal. What do you all think of that? reply chinathrow 13 hours agoparentprevNicely stated copyright violations. Has noone filed suit yet? reply SEGyges 13 hours agorootparentHuckabee v Bloomberg, Meta, et al reply 3abiton 11 hours agoparentprevInteresting take on the copyright law. reply Fornax96 8 minutes agoprevIt seems the Pile is currently inaccessible. I would be willing to mirror it on Pixeldrain if I can get my hands on it. reply swatcoder 15 hours agoprevWhere do I find the license reproductions and credits/attributions for the content being distributed in this data set? Is it all in there? Are all inclusions compliant? Can I know? I'm open to the argument that generators built with models that consumed copyrighted data may evade copyright obligations on their output, but surely the data sets themselves are bound by any copyright on their content? reply jsheard 15 hours agoparentThis dataset includes \"books3\", which is a comprehensive dump of Bibliotik, a torrent tracker dedicated to pirated ebooks. Throw a dart at a wall filled with every notable author/publisher ever and whoever you hit probably owns some of this data. Apparently you can just do whatever as long as you say it's for AI research, go post Blu-ray rips online, it's fine provided you have a .ai domain :^) reply oldgradstudent 14 hours agorootparentIt also contains an archive of opensubtitles, which is also not very open source. reply refulgentis 14 hours agorootparentThe subtitles aren't open? If you meant transcribing dialogue from a TV show is violating copyright, I'm not so sure, it's relatively common to quote dialogue for varied purposes, ex. TV critics Definitely understand if you're saying the whole dialogue for a TV show is copyrighted, but I'm curious about the opensubtitles part, used to work in that area. reply layer8 14 hours agorootparentQuoting excerpts is different from transcribing an entire work, which is unambiguously copyright infringement. (Otherwise you would find the “book” version of any and all TV shows on Amazon.) The subtitles in question are generally translations, which likewise fall under copyright, being a derived work. reply refulgentis 13 hours agorootparentYeah, I was just curious about the opensubtitles site because I used to work in that field (subtitles) and wasn't sure if there were some new pirate sites that were monetizing subs. n.b. not being argumentative, please don't read it that way, I apologize if it comes off that way: Not every derived work is a copyright violation, that's why subs and dubs don't get kicked around, you can quote dialogue in an article, etc.[^1] Answering if it applies to AI is playing out in court currently with ex. NYT v. OpenAI[^2] and Sarah Silverman et al v. OpenAI[^3] and v. Meta.[^4] [^1] \"Copyright doesn't protect against all use of the work or use of derivative works. There are a few exceptions that fall under what's commonly known as the fair use doctrine:\" (https://www.legalzoom.com/articles/what-are-derivative-works...) [^2] https://www.nytimes.com/2023/12/27/business/media/new-york-t... [^3] https://www.theverge.com/2024/2/13/24072131/sarah-silverman-... [^4] https://www.hollywoodreporter.com/business/business-news/sar... reply PavleMiha 14 hours agorootparentprevQuoting is very different from posting the full contents of something. I can quote a book but I can’t reproduce it in its entirety. reply refulgentis 13 hours agorootparentRight, you can't reproduce a book. W/r/t subs and dubs, fair use has applied historically. reply fsckboy 15 hours agorootparentprev> Throw a dart at a wall filled with every notable author/publisher ever copyrights do expire, and any books older than Mickey Mouse are public domain, so it's not every notable author ever reply jsheard 15 hours agorootparentTechnically true, narrow that down to merely \"every notable living author and a subset of dead ones\" then. Bram Stokers bones will be relieved to hear that their work isn't being misappropriated. reply gosub100 14 hours agorootparentprevnot the domain per se, but the high-powered law firms at your fingertips. Copyright law is much easier to enforce against working-class parents of 12-year-olds than SV elites. reply pk-protect-ai 14 hours agorootparentprevI wish it had included the books3, but it doesn't anymore. I wish it was possible to download that 36GB books3.tar in the wild these days. Herewith, I promise to use this dataset according to the \"fair use\" only... reply SekstiNi 13 hours agorootparent> I wish it was possible to download that 36GB books3.tar in the wild these days. There... is a torrent. reply pk-protect-ai 12 hours agorootparentI know. But here where I am, using torrent means participate in distribution of the content and that is where I'll get huge bill for illegally sharing this file. reply MacsHeadroom 5 hours agorootparentUse a debrid provider or seedbox to download the torrent. They torrent it for you and then you direct download from them. Should cost $10 or less. reply __loam 15 hours agoparentprevThey stole it because they think building their toys is more important than everyone else's rights to the product of their own labor. reply idle_zealot 14 hours agorootparentI congratulate all of the authors whose work is included in this dataset on contributing their knowledge, skills, and perspective to humanity's various endeavors, both creative and technical. I hope that the fruits of their labors are returned to them, rather than being selfishly hoarded by the few with the resources necessary to produce those fruits, be they publishers, middlemen, or big tech. Which is all to say that information shouldn't be hoarded and guarded. If it can produce something more than the sum of its parts we should use it to do so. The result of that should, on the same grounds, not be hoarded and guarded, doubly so being based on the work of others. reply nonrandomstring 13 hours agorootparent> I congratulate all of the authors whose work is included in this dataset on contributing their knowledge, skills, and perspective to humanity's various endeavours Thank you. You know in some ways it's an honour and a privilege to live in such times of progress. The very act of publishing is to \"let go\", and hope that your words and ideas contribute to something bigger and beyond your life. I never believed much in \"intellectual property\" as it's all stuff that flows through us. > I hope that the fruits of their labours are returned to them They rarely are, because knowledge and creativity are not greatly valued in our time. But authors, artists and scientists go into that with eyes wide open these days. The rewards come in other ways, as the more you give and put into life the more you get out. > rather than being selfishly hoarded by the few with the resources necessary to produce those fruits This is not what we fear. Hoard away. We will simply take back what is ours, whenever we desire it. The hoarders will never win against what they call \"piracy\", because they have no moral right. In the long run, they are on the wrong side of history. Far worse, and more likely is that the creative and technical works of generations of artists and scientists are going to be turned to exactly the opposite of what they would want. They will be used to harm and disempower humans, divide society instead of heal it, and even make the pursuits of art, science and knowledge irrelevant. We cannot take back our words, or our formulas, or our paintings or our songs. But we can take back tech. reply gosub100 14 hours agorootparentprevIt will produce \"something more\" for the already-wealthy who control the technology. For instance, LLMs will eliminate the need for some customer service jobs, increasing the profit margin for the existing executives and shareholders, while eliminating entry-level jobs from the job market. reply idle_zealot 14 hours agorootparentCall me an idealist but I don't think humans should be spending their time on jobs a computer can do. The solution to wealth disparity cannot include \"invent menial untalented high-paying labor for people to do\". reply __loam 12 hours agorootparentYeah why should humans do bothersome labor like...creating literature? reply nickpsecurity 7 hours agorootparentprevThat sounds nice except that it didn't happen. They scraped this off many sites whose authors published the material in ways that wouldn't legally allow that. They often publish for a mix of self interest and public benefit. The more you look, the more you find that much altruism is actually self-interest at work. Some have specific terms, too. Let's look at examples in The Pile. It might be something that's part of their job, requires citations since they value credit, optionally bans commercial use, and maybe has a patent. Arxiv papers are a mix of that. Many on StackOverflow and Hacker News want attribution with some asserting copyright in their comments. Film producers usually want the subtitles to accompany sales of their movies. For FreeLaw, the material is public by law about people who might have never even wanted to testify or be remembered. FreeLaw itself was seeking donations for its service with an additional request to protect the names of people in the dataset. StackOverflow's license explicitly bans copying their data without permission with many individual users also wanting self-promotion to happen side by side with their answers. So, many things that are in The Pile are works people published hoping to gain some benefit in return. They often had terms that banned their reproduction in ways that prevented them from getting that benefit. They were just public for humans to read and learn from. Some allowed sharing but just wanted credit. The A.I. users of these works ignore all of that by taking what they made conditionally available without meeting the conditions that benefit the authors. Whereas, if you got the same content on Hacker News or Arxiv, the authors might benefit from it. Even a pirate would benefit them more than A.I. companies because the users would often at least know the author or source site. So, the fruits of their labor were taken, not given, by those who are the least beneficial to them. I will note that some people do publish truly free content that has no strings attached. Mostly public domain or CC-0. Those are exceptions that might fit your description. reply idle_zealot 5 hours agorootparentI specifically did not say that they had contributed willingly. They have contributed nonetheless. reply johndough 14 hours agorootparentprevI doubt that anyone is going to download and search through over 800 TB just to find a badly formatted copy of some book that could be found much quicker on different websites with better formatting. Authors are losing fractional cents here at most. reply __loam 12 hours agorootparentThe penalty is up to $150k per violation. reply gosub100 14 hours agorootparentprevso just like Office Space? (paraphrasing) \"We steal a fraction of a cent from each transaction, who do we hurt? Nobody. We just put the remainder into our account!\" Sorry that's not how damages are calculated in the US tort system. reply johndough 14 hours agorootparentI do not know how damages are calculated in the US tort system. What do they say about the books3 dataset? I also think that the case is different here, since in your example, there is a specific amount of money being stolen, while in the books3 case, there is an unspecified amount of money not being made by the authors. reply SEGyges 13 hours agorootparentI am pretty sure if the authors were trying to license their works for this purpose we would just not use them at all; it is difficult to see under what circumstances they would stand to profit from this other than by suing people after the fact over it. reply doug_durham 12 hours agorootparentI think you could argue that authors could profit from their works being cited in an LLM response. It could drive sales of their works much like citations do on the web. The counter argument is that and LLM could give you the Clif Notes version of the work and thus taking away a portion of sales. reply SEGyges 12 hours agorootparentIn a world where the options were to 1) pay the author, 2) implement guaranteed citation of the author any time the model gave an answer that was directly derivative, with an option to not do so if the summary was sufficiently vague, or 3) ignore the author's book completely as training data we would all choose 3). reply __loam 12 hours agorootparentAnd the authors would probably be very happy that you did. reply pk-protect-ai 14 hours agorootparentprevthey have stole nothing, they make no profit from it as well. reply idle_zealot 14 hours agorootparentOh, so there aren't AI companies charging for access to private models? reply pk-protect-ai 12 hours agorootparentWho are they? Why do you mix up the guys who prepared the data with the other guys who used this data and making money from a vague memory of that data? reply arthurcolle 15 hours agoprevI can't believe people would do this, just share and republish copyrighted works over the internet. I'm in shock and in disbelief. Anyways... Is RedPajama 30T and The Pile \"all you need\" ? ;) reply doctorpangloss 14 hours agoparentIt’s enough for pre training to later tackle specific NLP tasks. To get something interesting you would have to generate an instruct dataset from it. It would have to cover a diverse range of tasks. The completions themselves do not make LLMs manifest knowledge and reasoning, a large and diverse instruct dataset does. reply artninja1988 15 hours agoparentprevThere is currently a project going on to create the pile v2 which has only permissively licensed data, because of all the bickering about copyright. reply idle_zealot 14 hours agorootparentSo a bunch of extra work to create a downgrade? I'm sure that's going to be very popular. reply arthurcolle 14 hours agorootparentThe training data distribution is the only thing that matters, not the actual content reply observationist 12 hours agorootparentUnless you want something like style from a range of authors, knowledge of a fictional universe or storyline, or other domain specific data or style characteristics. A blanket removal of copyrighted data would make a bot sterile, boring, unrelatable, and ignorant of culture and common memes. We have amazing AI technology. Let's lean into it and see where it goes. reply arthurcolle 9 hours agorootparentI agree, hypothetically, if I were ever to have an opinion on the matter. Just playing to the audience and potential audiences if this ever gets read into evidence ;) Haha... just kidding... unless.. ? reply __loam 11 hours agorootparentprevBy the violating the copyright hundreds of authors. reply chasd00 13 hours agorootparentprevif the pile contains the code to go from step 1 to step 2 and then to 3 then couldn't you just remove the parts you don't want from the raw dataset and re-run the code? reply jeffrallen 14 hours agorootparentprev> because authors prefer to be paid for their labor FTFY. reply evilduck 14 hours agorootparentI asked an AI tool to create a cheery poem about ringworms infecting kids from the 1600s and it created something that's never existed before. Which author gets paid for this labor they performed? reply ben_w 14 hours agorootparentprevNaturally, but I wonder what writers are going to do when the AI trained purely on suitably licensed content is still good enough to make most redundant. (The authors in best seller's lists may well be immune for a bit longer than others writers, as they're necessarily the top 0.1% of writers, but not forever: nay-sayers claimed that AI could never beat humans at chess or go because the games required special human insight). reply jfvinueza 13 hours agorootparentDunno. Writing fiction myself; asked AI to read it aloud. Narrative paragraphs worked fine: a clear, if a bit deadpan, slightly tone-deaf delivery. But dialogue was horrendous: it didn't understand emotional reactions and connotations at all. More so than cringey and robotic, it felt soulless. And the distance from \"something that makes sense\" to \"something that feels human\" felt unsormountable. Yes. Many novels will be written with LLMs in the coming years. They might even touch us. But this little Text-to-Speech experiment felt like an evidence that this technology has a void at its core: it doesn't have access, like a human does, to a gargantuan emotional spectrum, which allows us to understand all sorts of subtleties between what is being said, and why, and what does it actually mean, and why does it affect us (or, hell, how should the next line be read in this context, because it has no context, it doesn't feel). reply ben_w 12 hours agorootparentI'm also writing a novel, and using text to speech to hear how it sounds. One of the ones built into Mac OS. And I'd agree with your assessment, I value the synthesiser for bringing my attention to things my eyes gloss over, such as unnecessary repetition and typos which are still correctly spelled words (a common one for me is lose/loose). But: AI was seen as \"decades\" away from beating humans at go, even 6 months before it did. I don't know how far we are from them writing award winning novels (awards we care about, it doesn't count if it's an award for best AI), though my gut feeling is we need another breakthrough as significant as the transformer model… but even then, that's only a 1σ feeling. reply mejutoco 13 hours agorootparentprev> The authors in best seller's lists may well be immune for a bit longer than others writers, as they're necessarily the top 0.1% of writers The top best selling. Only one of many possible reasons for that might be the quality. reply ben_w 12 hours agorootparentQuality is subjective, therefore I think it is reasonable to say the best are those most able to profit rather than, e.g. winners of the Nobel Prize in Literature, or the list of books people most pretend to have read. reply wizzwizz4 13 hours agorootparentprevOnce upon a time, nay-sayers said that nobody could travel to the moon, regardless of what vehicle they used. They were wrong. Once upon a time, nay-sayers said that nobody could transmute lead into gold using alchemical equipment. They were right. Nay-sayers who said that no possible algorithm could beat humans at chess and go? They were wrong. Nay-sayers who say that these algorithms cannot write better books than humans? Well… reply ben_w 12 hours agorootparent> Once upon a time, nay-sayers said that nobody could transmute lead into gold using alchemical equipment. They were right. Now I'm wondering if, with modern knowledge, you could build a 0.5 MeV heavy ion accelerator with only the things available to a medieval alchemist. I'm thinking probably yes? Triboelectics can get the right voltage. But how good does the vacuum need to be? > Nay-sayers who say that these algorithms cannot write better books than humans? They may be right or wrong in the specific, but I think they're asking the wrong question, too specific. reply SEGyges 13 hours agorootparentprevBy \"these algorithms\", do you mean the ones that currently exist, or the ones that will exist next month, next year, or in 2034? reply wizzwizz4 12 hours agorootparentWe're not developing new algorithms all that quickly. My point is that one shouldn't dismiss criticism out-of-hand, just because some critics of some other thing turned out to be wrong: for this point to be valid, I don't need to be making criticism. On an unrelated note… Personally, I'd be referring to the family of algorithms that purely take as input a context window and provide as output a prediction of the next token likelihood. (Plus or minus iteration, to generate strings of text.) Pejoratively, one might call these \"fancy Markov chains\", though as with most pejoratives, that's overly reductive. All the approaches we're seeing marketed heavily are just fancy Markov chains. I expect every \"new algorithm\" for the next 5 years at least to be a fancy Markov chain, because that's what I expect to get funding. (I do expect that some people will be working on other approaches, but only for amateurish reasons.) reply SEGyges 12 hours agorootparentThese are fancy Markov chains in the sense that humans are just chemicals and computers just do math. Technically true, but not even \"overly reductive\"; it is just wrong if it is used to imply that, e.g., humans just swirl around in beakers or the most complex thing you can do with computers is trigonometry. You can make anything sound unimpressive if you describe it sufficiently poorly. And: So many different variations are published every month. There are a good number of people in serious research trying approaches that don't use cross entropy loss (ie, strictly next-token prediction). I don't know what the trajectory of the technology is over the next ten years, but I am positive no one else does either and anyone who thinks they do is wrong. reply onion2k 13 hours agorootparentprevIf the data is available online for the pile, surely it's also publicly available to ordinary people in a way that means authors aren't getting any money. reply sangnoir 13 hours agorootparentWhat sort of defense is this? \"Your honor, after someone broke in, they left the door open. Since the door was unlocked anyone could have committed the crime I'm accused of.\" reply zettabomb 14 hours agorootparentprevThis is pretty reductive - \"FTFY\" is rarely the witty response you think it is. reply zellyn 16 hours agoprevIs the \"books3\" dataset mentioned in the Pile paper the one that authors are suing over? The one that includes a whole bunch of popular and copyrighted material? reply taylorfinley 16 hours agoparentYes, from the linked paper: \"Books3 is a dataset of books derived from a copy of the contents of the Bibliotik private tracker made available by Shawn Presser (Presser, 2020). Bibliotik consists of a mix of fiction and nonfiction books and is almost an order of magnitude larger than our next largest book dataset (BookCorpus2). We included Bibliotik because books are invaluable for long-range context modeling research and coherent storytelling\" reply pimlottc 13 hours agorootparentThis is the most ridiculous legal hand wave I’ve ever seen. “They’re not books, man, they’re a dataset!” reply Balladeer 16 hours agoparentprevI believe it is. See https://www.wired.com/story/battle-over-books3/ reply DiggyJohnson 15 hours agoparentprevDo they claim that none of their data came from copyrighted sources / is copyrighted? reply seanhunter 15 hours agorootparentThe claim (which I don't personally agree with, but I'm trying to represent here in good faith) is that although the data is copyright, training models constitutes \"fair use\" under US copyright law and therefore you're entitled to use copyright material for this. Fair to say that whether or not this is correct is pretty important to all the outstanding court cases on this matter. reply jdiff 15 hours agorootparentThat seems to fall apart quickly. Even if training could be considered fair use, surely just distributing the raw masses of copyrighted works can't be under any reasonable definition. Otherwise, why did TBP, KAT, and MegaUpload shut down if you could defeat copyright with sheer numbers? reply seanhunter 15 hours agorootparentIndeed. Also in the US, whether or not something is fair use involves a four factor test[1] and two of the factors are the amount and substantiality of what's taken and the effect on any market. In this case, the amount is \"everything\" and the effect on the market is potentially very large for authors/publishers. [1] https://fairuse.stanford.edu/overview/fair-use/four-factors/ reply fsckboy 14 hours agorootparent>two of the factors are the amount and substantiality of what's taken and the effect on any market books.google.com has been allowed to copy all the books they can lay their hands on, so long as they don't regurgitate them in full, so it's not really the taking, but any subsequent reproductions. And the effect on the market is insubstantial if the alternative wasn't going to be the equivalent sales. reply ascorbic 11 hours agorootparentYou can download the whole dataset, so they're certainly able to regurgitate them in full. reply PeterisP 14 hours agorootparentprevOne thing that we did with distributing certain copyright-protected textual material was to scramble them at the paragraph level. If you take every paragraph in the Harry Potter saga and sort the paragraphs in alphabetical order, it's just as good for training short-context-window models, but not a \"harm to the market\" leading to a lost sale for anyone who wants to read the books. reply justinclift 14 hours agorootparentprevSince when has TBP shut down? reply gosub100 14 hours agorootparentI think they are referring to the many times the domain name has been seized, and shut down temporarily. reply RecycledEle 14 hours agorootparentprevSome of the founders were convicted of crimes but the database and code are out there. reply YeGoblynQueenne 12 hours agorootparentprevMegaupload et all went against the entertainment industry in a time when that industry had the money to pay the lawyers to convince the judges what the law means. In the present moment on the other hand, it is the entities in the AI industry (e.g. MS) that have the money and can hire the lawyers to convince the judges. Realistically speaking, it's very likely that things will swing the way of AI companies, which will benefit, albeit indirectly, these guys, even though by themselves they're too small to push their agenda, they're just bit players. reply retrac 15 hours agorootparentprevI think there is actually a good argument that an AI model is transformative, and that training a model is therefore not infringing of the copyright. (An analogy: if you rolled dice to select words randomly from the Lord of the Rings and rearranged them into a poem, it's not infringing the Lord of the Rings even if in a sense, every word was taken from that book.) But you still have to get your hands on the copyrighted data legally. It might be legal to scan every book an institution owns, and train off it, so long as those scans are not distributed. But it is probably not legal to scrape copyrighted content off torrents - creating the copy to train with is infringing, even if the model's final product maybe isn't. reply seanhunter 15 hours agorootparentYes agreed, and transformative use itself also has limitations. You don't have carte blanche to use something just because you think it's transformative, for example the Lynn Goldsmith vs Andy Warhol Foundation case over the \"Orange Prince\" work. https://copyrightalliance.org/warhol-decision-reins-transfor... reply fsckboy 15 hours agorootparentprevwhile there is a good argument that AI produces transformative outputs, it's refuted when the models are shown to regurgitate literal text, which they have. Then it just starts to look like a neural memorization agent, compressed storage algorithm, etc. reply 7moritz7 12 hours agorootparentThis very rarely happens, usually when trying hard to get it to regurgitate, and I don't think it has ever happened for anything longer than 2 paragraphs, or at most a short article. Certainly not something like a book or even the whole issue of a newspaper. reply bee_rider 14 hours agorootparentprevDefinitely open to the idea, that couldn’t be the whole argument. I mean, my brain can output some quotes, but I’m not a compressed storage algorithm. Or at least I hope I’m not. reply zettabomb 14 hours agorootparentprevI've seen examples of this, but they're nearly always isolated, rather difficult to obtain, and not in fact exact copies. You need to specifically ask for an exact copy, and then attempt to defeat the safeguards the model has in place to prevent this, and hope that it was \"memorized\" - which for the record is considered to be a flaw in the model as it's a reduction in information density and capability, compared to if that \"memory\" was used for something else. Good models seek to reduce this as much as possible. With the size of the datasets involve (see OP) this feels more like an understandable and reasonable issue to have. reply jsheard 15 hours agorootparentprev\"Open source\" implies that, no? A definition of open source which includes blatantly pirated material on the condition that the people who collated and released the pirated material did so for free is really stretching it past breaking point. By that standard everything on The Pirate Bay is open source. reply numpad0 15 hours agorootparentprevWhy do everyone assume \"open source\" imply legality? (/s) reply qwertox 15 hours agorootparentprevThere's odd stuff in there. I just randomly downloaded a file, https://the-eye.eu/public/Books/ThoseBooks/Puzzles.tar -- 20-Jan-2023 14:54 -- 6M and it pretends to be a jigsaw puzzle, but is actually eISBN 9781594868573 - The South Beach diet cookbook / Arthur Agatston reply mistrial9 16 hours agoparentprevthis list [0] seems like a starting place to look into various legal actions.. not sure how often it is updated e.g. Silverman et al [0] https://originality.ai/blog/openai-chatgpt-lawsuit-list reply bt1a 16 hours agorootparentPouring one out for the future litigators, jurors, and judges who will have to pore over this inextricable web of legal and technical details reply PeterStuer 15 hours agorootparentThey'll just let their ai do it over lunch. reply quatrefoil 13 hours agoprevWhile a lot of attention has been given to books3, another large component of this dataset is the deceptively-named \"OpenWebText2\". What's that? It's a scrape of 15 years' worth of third-party websites that were linked to from upvoted Reddit submissions. I know this includes some of my writing. reply observationist 11 hours agoparentRelevance and impact aside, if you publish something to the internet on a site with no access restriction in place, I don't know how you can keep a straight face while claiming some sort of moral right to the content. It's the equivalent of broadcasting it over radio, or printing and delivering it straight to the doorsteps of millions of random individuals. Methinks you doth protest too much, or something. There are ways of copyrighting data, and establishing ownership of intellectual property. Your tumblr fanfic, youtube comments, or HN discussions are not legitimate copyright avenues. Stuff you post to legally scrapeable websites are fair game for fair use. I can do anything I want in private to any data I collect. I could create an awesome HN LLM on the scraped datasets, and use it privately to my hearts content. I can even set up an API to that LLM that generates content, and, given recent rulings, even if i had all the written copyrighted data in the world, as long as I was making good faith efforts to ensure copyright was being respected and works weren't being recreated verbatim, then I could even use that model commercially. I just couldn't sell it to other people, or distribute it, without entering a different legal regime. I can collect any data I want from public facing websites. That's how the internet works; it's how it was designed. There are authentication mechanisms, network configurations, and a myriad other access control schemes you can implement to prevent public access. If you post to sites without those mechanisms, you're tacitly agreeing to give up any plausible claims of protection against a wide array of fair uses well established by precedent cases at this point. If you don't prevent public access, and you've got a domain name on a server, you're tacitly inviting the world to come download whatever it is you have on your server. This is a social good. This is what we want when we participate in the internet. Insisting on some sort of vague entitlement as to how \"your\" data gets used completely bypasses the fact that anything you consider to be misused in OpenWebText2 fundamentally stems from the fact that you posted the content to a publicly visible website and gave up any say in what happens thereafter. It was scraped fair and square. Don't complain that you didn't know the rules, or that life isn't fair. It's not even clear that terms of service or those little popups on public websites have any legal relevance. If your website is open to the public, then it's fair game. If you post content to a public website, then that content's fair game. reply quatrefoil 10 hours agorootparentIt feels like you're picking apart an argument I didn't make. But I would note that most people don't see this so unambiguously as the position you're defending. To give you an analogy: doxxing is \"fair game\" too if you posted your info online or gave it to others. But it's not exactly cool to do it, right? It's a subversion and abuse of the system we have in place. Finally, here's a fun experiment: decide that terms of service don't matter and start building a product by scrapping Facebook or Google. See how they'd react. Actually, no need for guesswork - they clutched their pearls and threatened legal action more than once before. It's a bit of a \"have your cake and eat it too\" kind of a deal. Their data is precious intellectual property; your stuff is, well, up for grabs. reply observationist 10 hours agorootparentOh, for sure, they get all pearl clutchy when others try to do exactly what they have done, and they get all \"not like that!\" about it. The US is a society run by lawyers, and the big corps have the best lawyers. Maybe we can legislate out of the hole at some point, but it's a pretty grim outlook. Google et al also don't have to have the law on their side, they can simply litigate people and businesses into bankruptcy, regardless of the legal merit of their actions. At any rate - there are ways of staking legitimate claim to content you publish online. Even by doing so, it may not be relevant. Robots.txt is a convention, not a regulation or law. It's respected out of social nicety, not because it's strictly legally required. If you publish your data to a website where it's publicly visible, you are inviting the world to come download your data. When that data leaves your server and goes to live on the downloader's computer, the downloader can do whatever they want with that data. It's not clear that it's legally possible to prevent the use of data in training models unless you require someone to sign a contract to that effect before being allowed to download your data. That would be obnoxious, and I wouldn't bother with your content anymore. Like Instagram, LinkedIn, and Twitter, your site would get a 127.0.0.0 hosts file entry. The US needs a clear, modern update to copyright law that upholds and maximizes individual rights, as well as privacy and property concerns. We shouldn't be playing this game where we pretend a website is somehow an analogy for a page of text scribed with a quill pen and using laws developed to handle issues when quill and parchment were relevant. Let's write some new laws where we regulate what things are, and not play tortuous mental gymnastics to contort and butcher existing laws and precedents to say whatever the most expensive lawyers want. Maybe the social contract allows for people to prevent their conversations from being scraped and used by third parties without explicit consent, even if the conversation is entirely public. I don't like that view, but I see the argument for it. As things stand, though, fair use and public access make things pretty bright and clear, and rulings in various AI cases so far have favored broad fair use interpretations, and are requiring complainants to show specific, particular harms. If/When those harms are shown, then we'll see if any carveouts will be made, or if broad fair use interpretations will be the baseline for content scraping going forward. reply UncleEntity 9 hours agorootparentprev> It's the equivalent of...printing and delivering it straight to the doorsteps of millions of random individuals. Which, incidentally, the New York Times does and they seem to think they have some legal right to the redistribution of their work. Maybe they're right, maybe they're wrong, it's up to the courts to decide. reply 7moritz7 12 hours agoparentprevCare to give me your domain name so I can check all major llms for plagiarism? I have a feeling none of them can produce a sentence from your writings reply quatrefoil 11 hours agorootparentIt takes deliberate effort, but I was actually able to get pieces of my writing out of one of the leading LLMs (not ChatGPT). This is not particularly unique, a number of folks demonstrated the same. reply 7moritz7 10 hours agorootparentHow long were those pieces? reply jwitthuhn 16 hours agoprevIs this still available somewhere? I attempted to download it several months ago and saw the download link 404ing, seems it is still like that. reply TrueDuality 15 hours agoparentMost of the distribution for this is via torrents/magnet links and in person hard drive exchanges. I'd go look at some public trackers if you want a copy and don't know someone that already has it. Do be aware that it does include copyrighted content so distribution is piracy. reply Der_Einzige 15 hours agorootparentAlmost all LLM training datasets include copyrighted content so almost all open source LLM distribution is piracy and almost all API based LLMs, including ChatGPT, are also piracy and copyright laundering. Also, most image-text dataset pairs contain far worse than that. You might want to check out LAION-5B and what stanford researchers have found in there. Technically, anyone who even touched that could in theory be in some serious, serious trouble. I find it quite remarkable that nothing has happened yet. reply visarga 14 hours agorootparent> almost all open source LLM distribution is piracy and almost all API based LLMs, including ChatGPT, are also piracy and copyright laundering That's an amplification of copyright, original expression is protected, but not the ideas themselves, those are free. And don't forget when we actually get to use these models we feed them questions, data, we give corrections - so they are not simply replicating the training set, they learn and do new things with new inputs. In fact if you think deeply about it, it is silly to accuse AI of copyright violation. Copying the actual book or article is much much faster and cheaper, and exact. Why would I pay a LLM provider to generate it for me from the title and starting phrase? If I already have part of the article, do I still need to generate it with AI? it's silly. LLM regurgitation are basically attacks with special key, entrapments. They don't happen in normal use. reply Workaccount2 13 hours agorootparentprevModels are not information archives. The size of the final model is orders of magnitude smaller than the size of the training data. Somehow people are just not able to get this through their heads. Stable diffusion is like 12GB or something and you have people convinced it's a tool that is cutting and pasting copyrighted works from an enormous image archive. reply feoren 11 hours agorootparent> The size of the final model is orders of magnitude smaller than the size of the training data. Good to know I can avoid copyright on a book just by zipping it up! reply 7moritz7 12 hours agorootparentprevStable Diffusion 1.5 is 1.5 to 6 GB depending on the finetune and trained on like 5 billion images reply vineyardmike 15 hours agorootparentprevThe courts (in the US) have not found LLM model weights to be piracy, nor the outputs, but it’s really surprising that LAION was used for so long consider the content you allude to. reply Filligree 15 hours agorootparentLAION is essentially a list of every image on the public internet. It was filtered, of course, but do you really expect perfection? It's impossible to create such a list while evading all such material. reply vineyardmike 15 hours agorootparentThere exists databases of “the hash of problematic photos” (CSAM), so it seems trivial to search your billions of photos against them before training an AI. You can’t catch everything, but this seems like an obvious miss considering the explicitly tried to scrape pornography. These hashes is exactly how researchers later discovered this content, so it’s clearly not hard. reply duskwuff 14 hours agorootparentThe Stanford researchers also found a substantial number of CSAM images in the LAION-5B dataset which were not recognized by PhotoDNA, probably because the images in question were not in wide distribution prior to their inclusion in LAION. Full paper: https://stacks.stanford.edu/file/druid:kh752sm9123/ml_traini... reply SEGyges 13 hours agorootparentprevYou are uploading 5 billion examples of . You cannot filter it manually, of course, because there are five billion of it. Given that it is the year 2024, how hard is it to be positive that a well-resourced team at Stanford in 2029 will not have better methods of identifying and filtering your data, or a better reference dataset to filter it against, than you do presently? It is a pretty hard problem. reply vineyardmike 11 hours agorootparentYou don’t have to do it manually. There is a database of file hashes. And this isn’t just “one engineer”. Companies like StabilityAI, Google, etc have used LAION datasets. If you built a dataset you should expend some resources on automated filtering. Don’t include explicit imagery as an intentional choice if you can’t do basic filtering. reply beeboobaa 15 hours agorootparentprevTurns out you can ignore copyright law if your company has enough money. reply doctorpangloss 14 hours agorootparentprev> I find it quite remarkable that nothing has happened yet. While I don't think it's because you're wrong, per se, it's just that none of this drama really matters. reply littlestymaar 15 hours agorootparentprevIt's only piracy if it's private individual doing it, otherwise it's just “ask for forgiveness not for permission”-type Capitalism. reply gosub100 14 hours agorootparentIt'll be some epic lawsuit like google-v-samsung that will get drawn out for a decade, awarded, and reduced, appealed, etc. where the only winners will be both party's lawyers. reply littlestymaar 11 hours agorootparentIt's gonna be way worse than this: - OpenAI and others will just settle with MPAA, RIAA and the likes for a revenue stream (a single digit billion a year, likely) + some kind of control over what people can and cannot do with the AI + the access to the technology to produce their own content. - artists will see peanuts from the deal, and the big names are going to be able to stop doing any kind of business with artists which are just expenses in their eyes. They will have been replaced by machines that where trained using their art with no compensation whatsoever. IP is already predatory capitalism, AI will definitely be weaponized against the workers by the owners of the means of “production”. reply HanClinto 14 hours agoparentprevIs it kosher to post magnet links here? I'm not sure. magnet:?xt=urn:btih:0d366035664fdf51cfbe9f733953ba325776e667&dn=EleutherAI_ThePile_v1 reply SEGyges 13 hours agorootparentThis is the correct one. reply archon1410 15 hours agoparentprev> The Pile is old news, check out more recent datasets like; https://huggingface.co/datasets/bigcode/the-stack-v2 — https://the-eye.eu/public/AI/pile/readme.txt reply natch 14 hours agorootparentSuper odd message since the stack v2 seems to be exclusively code and The Pile is (mostly?) text. reply spindump8930 14 hours agoparentprevAlso good to note that that the Pile contains lots of curated sources and recent trends have been to take curated data sources and combine them with filtered webcrawls (i.e. commoncrawl with heavy processing). See dolma or the stack v2 (for code models) as others have mentioned. reply DiggyJohnson 15 hours agoprevAwesome name. Reminds me of the \"original\" \"Pile\" from the Manhattan Project. I read about it in \"The Making of the Atomic Bomb\" (1986), but presumably it's featured in the recent movie. reply groby_b 15 hours agoparentNot really. There's an ultra-brief scene where it's mentioned, but that's it, IIRC. The movie... is a bunch of anecdotes strung together to make a ham-handed point at the end. It was a decent movie if you treat it as a fictional story instead of an actual retelling. I'd stick with the book. (And if you specifically care about Fermi, I recommend \"The Last Man Who Knew Everything\" by David Schwartz) reply clooper 11 hours agoprevThe big Hollywood studio pay a lot of money to various cyber security companies to look for pirated content and send cease and desit letters to hosting companies for letting their users distribute copyrighted content. If authors and artists were to join a data union they could do the same thing as studios. If copyrighted law has any real teeth then the data union can send legal requests to whoever is hosting the content and requesting it to be taken down. I'm not a lawyer but I know the studios definitely do this. reply turnsout 16 hours agoprevThe Pile is pretty old—is this an updated version? reply bt1a 16 hours agoparentIt is not. In related news, v2 of the \"stack\" dataset was recently released > 3.28B unique files belonging to 104.2M github repositories were collected by traversing the Software Heritage 2023-09-06 graph dataset. Additional repository-level metadata was collected from GitHub Archive data up to 2023-09-14. The total uncompressed size of all files is 67.53TB. Near-deduplication was implemented in the pre-processing pipeline on top of exact deduplication. V1 vs V2 by Deduped Size Tokens V1: 2.9TB and 200B V2: 32.1TB and 900B I imagine we'll see some fairly powerful open coding models soon. The ones I'm looking at testing are: dolphincoder-starcoder2-15b-iMat.GGUF CodeFuse-DeepSeek-33B-iMat.GGUF OpenCodeInterpreter-DS-33B-iMat.GGUF starcoder2-15b-instruct-iMat.GGUF more info dataset https://huggingface.co/datasets/bigcode/the-stack-v2 gguf quants https://huggingface.co/dranger003 reply bick_nyers 15 hours agorootparentDo you happen to know what the v2 dedup size is when compressed? 32.1TB is quite a bit, but if that compresses down to say 3-6TB, it would be much more manageable. Code has a lot of whitespace, repetition, and structure/predictability, so I imagine it would compress better than average text. reply spindump8930 14 hours agorootparentThose sizes refer to the data before processing and filtering. The actual training size was about 3 TB: The Stack v2 is ten times larger than its predecessor, yielding a raw dataset of 67.5 TB. Through extensive cleaning, filtering, and subsampling of the source code, along with the incorporation of other high-quality code-related datasets, we created a training set of approximately 3TB (900B+ tokens). Source: the paper, Section 10 (https://arxiv.org/pdf/2402.19173.pdf) reply dang 4 hours agoprevRelated: The Pile: An 800GB dataset of diverse text for language modeling (2020) - https://news.ycombinator.com/item?id=36685115 - July 2023 (70 comments) reply __lbracket__ 14 hours agoprevLLM are of use to megacorps. Megacorps assume authors, painters, etc are poor and powerless (which lets face it, they are) we can b** and moan on HN, but megacorps will find ways to use copyrighted works for free. reply willvarfar 13 hours agoprevAre there any simple text editors or wysiwyg that have local LLMs and can tidy up and auto-suggest whole paras of slick verbage as you type? reply kristianp 11 hours agoprevPlease add (2020) to the title. reply mjtechguy 14 hours agoprevWould be interested to see what is in there. Luckily noone has posted the magnet link on Twitter. reply SEGyges 13 hours agoparentThe counterparties on related legal action are sufficiently litigious that it is probably smarter to DM the magnet link. reply _obviously 14 hours agoprevSeems kind of small tbqh. reply beiller 11 hours agoparentIt seems small, until you try to download it. reply joering2 15 hours agoprevThe pile can be downloaded here. 404 Not Found nginx 825 GB is a great candidate for torrent use, whatever was under that broken link better be a torrent magnet. reply intalentive 13 hours agoprevThis is 4 years old. Why the top of HN now? reply brokensegue 16 hours agoprev\"open source\" as in gratis but not as in libre? reply Legend2440 16 hours agoparentmore like, it's a scrape of the entire internet, use it at your own risk reply o11c 14 hours agoparentprevIt should be understood in contrast to most traditional corpora, which are heavily paywalled/restricted ... or else based solely on century-old books. It has long been a major obstacle for linguistics tooling. If the current push of AI companies to get their way (to allow copyright laundering) succeeds, this would almost count as open source by the real definition. If not ... lots of people/companies are committing copyright crimes, some are committing civil infractions, and some may be able to claim fair use. reply Der_Einzige 15 hours agoprevI came so close to getting my Debate document dataset \"DebateSum\"[1] included into this[2] and I am very sad that it wasn't included to this day: [1] https://github.com/Hellisotherpeople/DebateSum [2] https://github.com/EleutherAI/the-pile/issues/56 reply joshuakogut 15 hours agoparent> If you’d like to contribute it, feel free to submit a PR Stella was waiting for you to submit your dataset. Did you? She closed the ticket many months later. reply Der_Einzige 15 hours agorootparentThey did a significant amount of work themselves of taking other peoples datasets and including them without the work of the original author needing to submit the full PR to do it. I was then and to this day remain extremely busy Also this was before most datasets were hosted conveniently on huggingface. It's all tears in the rain now. reply racee 16 hours agoprevi love the Illuminati vibes of The Eye reply fddrdplktrew 14 hours agoprev825gb seems really small reply jMyles 14 hours agoprev [–] So much of this thread is concerned not with the achievement of this data set, but with the (by comparison) silly and outdated spat over how to frame it as \"property\" for the purposes of government intervention (pursuant to which jurisdiction?). The era of intellectual \"property\" is over. Let's be at peace with that and just move on into the next age. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Pile is an 800GB open-source language modeling dataset hosted by The Eye, combining 22 smaller datasets in jsonlines format, enhancing cross-domain knowledge and model generalization.",
      "It serves as a benchmark for evaluating models, with good performance on Pile BPB indicating a grasp of diverse domains, leading to enhancements in language modeling benchmarks.",
      "GPT-3 and GPT-2 have demonstrated strong performance on The Pile dataset, showcasing the dataset's impact on model training and evaluation."
    ],
    "commentSummary": [
      "The debate centers on the legality and ethics of utilizing extensive datasets for training AI models, highlighting copyright infringement, fair use, and effects on creative sectors.",
      "Concerns involve unauthorized access to copyrighted material, legal hurdles, and impacts on authors and artists, alongside arguments on AI-generated content and Text-to-Speech tech limitations.",
      "Discussions also touch upon data acquisition challenges, adherence to copyright regulations, and finding a balance between innovation and safeguarding intellectual property rights."
    ],
    "points": 319,
    "commentCount": 214,
    "retryCount": 0,
    "time": 1709831691
  },
  {
    "id": 39635483,
    "title": "Answer.AI Launches Project to Utilize Multiple GPUs for QLoRA Training",
    "originLink": "https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html",
    "originBody": "Jeremy from Answer.AI here. This is our first project since launching our new R&D lab at the start of this year.It&#x27;s the #1 most requested thing I&#x27;ve been hearing from open source model builders: the ability to use multiple GPUs with QLoRA training. So that&#x27;s why we decided to make it our first project.Huge thanks to Tim Dettmers for helping us get started to this -- and of course for creating QLoRA in the first place!Let me know if you have any questions or thoughts.",
    "commentLink": "https://news.ycombinator.com/item?id=39635483",
    "commentBody": "Fine tune a 70B language model at home (answer.ai)253 points by jph00 5 hours agohidepastfavorite41 comments Jeremy from Answer.AI here. This is our first project since launching our new R&D lab at the start of this year. It's the #1 most requested thing I've been hearing from open source model builders: the ability to use multiple GPUs with QLoRA training. So that's why we decided to make it our first project. Huge thanks to Tim Dettmers for helping us get started to this -- and of course for creating QLoRA in the first place! Let me know if you have any questions or thoughts. jph00 4 hours agoOne thing I forgot to mention in the post which I think is kinda cool: at the NeurIPS Efficiency Challenge this year, where Tim Dettmers and I both did keynotes, every single top-ranked entry used QLoRA! The challenge was to create the most accurate model on a single GPU in 24 hours. I think that is a great example of how important and useful QLoRA is. Maybe we should run a dual-GPU challenge next time not that multi-GPU is working... reply int_19h 1 hour agoprevThis is great, but one thing I really hoped would come sooner is fast training on Metal. As things are, you can get an M1/M2 Ultra (~800 Gb/s memory bandwidth; for comparison, RTX 4090 is ~1050 Gb/s) Mac Studio with 128Gb RAM for ~$3500. For large model inference, this is already way more affordable than stacking GPUs while being \"fast enough\", but training solutions are basically non-existent. I do wonder why; it feels like a low-hanging fruit. reply buildbot 1 hour agoparentCompute limited - an m2 ultra has 27 tflops, a 4090 80+ reply yumraj 1 hour agorootparentSo it should just take longer.. reply AnthonyMouse 16 minutes agorootparentIf you don't care how long it takes you can get an old server with 128GB of RAM for a lot less than $3500. reply erichocean 1 hour agorootparentprevMemory limited - an m2 ultra has >150GiB, a 4090 24GiB reply sqreept 1 hour agoparentprevM1, M2, M3 still have very low number of GPU cores. Apple should release some better hardware to take advantage of their recently released MLX library. reply yalok 3 hours agoprevHave you guys looked at using sparsification? It would probably require true re-training of the foundation model, to go at high sparse ratios (say 90% weights excluded), which could be done once on expensive GPU - but fine tuning such sparse models would require less RAM hopefully. The trick with getting more benefit from sparse approach is to do block sparse (iirc, Tim Dettmers used to work on this as well, a few years ago), but large block size (say 16x16) would require much longer retraining to recover for the lost accuracy… reply jph00 2 hours agoparentYes, sparsification is another useful approach for higher efficiency, although block sparse kernels are pretty complex to work with -- especially when combined with quantization and LoRA! Most of the sparsity papers I've seen use \"structured\" sparsity; i.e removing layers, attention heads, and features. But the upside from this seems somewhat limited so far. reply AhtiK 1 hour agoparentprevHas anyone seen an implementation of 'SpQR: A Sparse-Quantized Representation,' published in June 2023 by Tim Dettmers et al.? https://arxiv.org/abs/2306.03078 reply AhtiK 1 hour agorootparentFound it from https://github.com/Vahe1994/SpQR Was somehow expecting it to be at https://github.com/TimDettmers/bitsandbytes. My bad. reply jamesblonde 2 hours agoprevThis is a fantastic breakthrough for those of us who fine-tune LLMs on limited hardware budgets. I was curious about the choice of FSDP over DeepSpeed. I have been using Axolotl for fine-tuning, and FSDP has been broken there, whilst DeepSpeed is rock solid. Why FSDP over DeepSpeed jph00? reply jph00 2 hours agoparentDeepSpeed has more features than FSDP, but it's much more complex to hack on -- FSDP is written directly in python using calls to the PyTorch library, whereas DeepSpeed is 20% C++ and 10% CUDA (according to the GitHub stats). We've found that FSDP works just as well for our needs, and we appreciated the increased \"hackability\". (Axolotl is terrific BTW. I hadn't heard of problems with it with FSDP before -- I'll see if that's something we can help with.) reply pella 3 hours agoprev> the ability to use multiple GPUs with QLoRA training. Thorough article! Question: What's your opinion on: - How viable will NVIDIA's consumer cards be in the long run? - Besides https://tinygrad.org, what other cost-effective future alternatives could there be? reply bugglebeetle 2 hours agoparentUnsloth (mentioned in the Answer.AI post) is planning multi-GPU support in a future release. reply ricopags 5 hours agoprevThis is such exciting news! Huge thanks to you for your continued work in making sense of AI. I wonder if the recent Bitnet 1.58 paper [the use of ternary bits in lieu of fp/int] might be an advancement that could further reduce the computation required for inference? reply jph00 4 hours agoparentYes, along with the many other <4 bit quant methods recently developed -- there's been a wonderful boom in low-bit quant methods in the last 6 months, and we've got our own ideas for taking them further too. Along with QLoRA/FSDP, we're likely to see big advances in model training this year on consumer hardware. reply delegate 46 minutes agoprevMaybe I've missed it in the article - but how long would a full training run take on 2 consumer GPUs (local or rented) ? Ballpark - hours, days... ? reply itsgrimetime 2 hours agoprevWould be cool to build an “LLM@home” project like folding@home or SETI@home (rip), where tons of folks could donate their GPUs and train something huge and FOSS. I don’t know enough about how these models are trained though. Could it be chunked up and distributed in that way, then stitched/merged back together? reply fho 2 hours agoparenthttps://stablehorde.net/ comes somewhat close. reply keeptrying 3 hours agoprevIf you are gonna be doing stuff like this I’m damn excited for answer.ai! It’ll be the first time we’ll have someone who knows AI create leverage to open source it. Way to go! reply tbenst 2 hours agoprevVery interesting but hard to interpret until the performance numbers / benchmarks are available. I can already fine-tune a 70B language model at home using CPU + RAM, but it would be so slow as to be almost totally impractical (~20x slower than GPU). It would be great to see a comparison to eg 8 x A100 (available for $32/hr on AWS on-demand) and also CPU + RAM. Presumably it’s somewhere in between, but hard to predict where! reply buildbot 4 hours agoprevNice, I tried to use QLoRA+FSDP in the past with litgpt and obviously at that time it did not work. This is very useful! reply artninja1988 5 hours agoprevSo, as I understand it, this is for finetuning a preexisting llm? So not actually training one from scratch. I guess that would be too much to ask for. Nonetheless, cheers to Jeremy and the gang for the work. reply jph00 4 hours agoparentFor now, it's for finetuning. The issue of to what degree it might be possible to train a model from scratch using QLoRA is still an open question. The relora paper showed that it can work in some situations, but attempts to scale it up were unsuccessful. The recent DoRA paper perhaps might allow a \"re-DoRA\" approach to work. If so, that could be combined with quantization to do \"re-QDoRA\"! reply hantusk 1 hour agorootparentDigging into the low rank structure of the gradients, instead of the weights seems like a promising direction for training from scratch with less memory requirements: https://twitter.com/AnimaAnandkumar/status/17656138151468933... reply hantusk 1 hour agorootparentSimo linked some older papers with this same idea: https://twitter.com/cloneofsimo/status/1765796493955674286 reply qsi 3 hours agorootparentprevThe headline and introduction on the linked page say \"You can now train a 70b language model at home. We’re releasing an open source system, based on FSDP and QLoRA, that can train a 70b model on two 24GB GPUs.\" How does \"fine tuning\" differ from \"training?\" Reading the linked article I had assumed I could create my own trained LLM at home with two 24GB GPUs. reply jph00 2 hours agorootparentThe article actually sneaks in a footnote that answers this (https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html#fn1): \"Throughout this article “training” can refer to either pre-training, or fine-tuning\". (Generally, we've told students at fast.ai since 2017 that they should almost never be starting from random weights -- most of the time it's best to start with a pretrained model and fine-tune that, even if it's from a somewhat different domain to the problem you're working on.) reply Tomte 2 minutes agorootparentHave you changed your mind on „The End of Finetuning“ (https://www.latent.space/p/fastai ) or did I simply misunderstand that? Oh, and thanks for quirky stuff like your APL video! keremturgutlu 2 hours agorootparentprevYou most definitely can, the main difference is that only partial ~2% of the parameters get updated during training. Say you start from a model like llama-70B which already knows english and has some world knowledge based on its pretraining dataset. It might not be ideal for drastic domain shifts, such as adapting a model to learn new languages (which might require a new tokenizer and model embeddings) but still might be possible to some extent. reply qsi 2 hours agorootparentThank you for clarifying. I have been wanting to dip my toes into LLMs at home but obviously I have a steep learning curve ahead of me, and would need considerably beefier hardware! reply IanCal 2 hours agorootparentprevYou can take an existing 70B model and train it to do a more specific task. You're teaching it the task but you're relying on a foundation model for the base understanding of the world/words/etc. reply qsi 2 hours agorootparentOK, that makes sense. Thank you! reply buildbot 4 hours agoparentprevLit-GPT is what I have been using to pretrain models at home: https://github.com/Lightning-AI/litgpt Using the openwebtext example, I can train a 700M param model to 2.6 loss in a few days on dual 4090s. Pretty awesome! reply carbocation 3 hours agoprevI wonder whether LoRAs could be useful for U-Net training. Especially thinking of CNN-based U-Net models with pre-trained encoders (but randomly initialized decoders). At least, it seems possible that normal weight updates on the decoder and LoRA training on the encoder could improve efficiency. reply jph00 2 hours agoparentDiffusion unet has an \"extended\" version nowadays that applies to the resnet part as well as the cross-attention: https://github.com/cloneofsimo/lora reply lbj 2 hours agoprevCan't believe they didn't name this Qolor reply g42gregory 3 hours agoprevThis is brilliant. Thank you for doing his! reply m3kw9 3 hours agoprevIf they can continuously train it, it could be better than a large context as this is how a AI OS would need to work when you have constant updates to your files reply padolsey 2 hours agoparentI don’t think you’d be fine-tuning a whole model in such cases. That seems over the top, no? I assume you’d get sufficiently far with big context windows, vector search, RAG. Etc. reply 5 hours agoprev [–] [deleted] GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Answer.AI, led by Jeremy, unveils their first project post-launching their R&D lab, emphasizing the utilization of multiple GPUs for QLoRA training.",
      "The initiative originated from the primary request of open-source model builders, with support from Tim Dettmers to kickstart the project.",
      "Jeremy encourages engagement by welcoming inquiries and feedback on the project."
    ],
    "commentSummary": [
      "Answer.AI's R&D lab initiated a project to optimize a 70 billion language model using multiple GPUs at home with QLoRA training, a highly requested endeavor among open-source model developers.",
      "QLoRA's significance was underscored in the NeurIPS Efficiency Challenge, sparking interest and positive reviews.",
      "The project addresses hardware constraints, sparsification, varied training techniques, and future advancements, striving to enhance accessibility and efficiency in training extensive language models."
    ],
    "points": 253,
    "commentCount": 41,
    "retryCount": 0,
    "time": 1709851129
  },
  {
    "id": 39628285,
    "title": "Futuristic visual programming: Flyde revolutionizes coding",
    "originLink": "https://github.com/flydelabs/flyde",
    "originBody": "Hi HN! I’m Gabriel, and I’m happy to share a project I’ve been working on for the last few years: Flyde, an open-source visual programming language. Check out the interactive examples and online playground on the website: https:&#x2F;&#x2F;www.flyde.dev.In my last role as an engineering manager for a B2B-oriented product, I authored and reviewed many diagrams for backend applications, mostly for integrations between 2 third-party services. Some of these diagrams were elaborate enough that I started dreaming of a way to simply run a diagram as is; I imagined a “run” button on the top-right corner of the screen that would execute the diagram without the need to translate it into code.That led me down a rabbit hole of exploration and experimentation, from tools like Zapier, Pipedream and Make, which are great for automating “backoffice” stuff, and up to NodeRED, NoFlo.js and the great work of J. Paul Morisson on Flow-Based Programming. I failed to find a tool that would answer my needs - a tool that balances a new level of abstraction, manages to stay powerful and flexible, and most importantly, integrates with the existing ecosystem, and doesn’t replace it. I built Flyde as an attempt to answer that need.Flyde is designed to complement and enhance traditional textual coding, not to replace it. It includes a VSCode extension, it seamlessly integrates with existing TypeScript&#x2F;JavaScript code and can run on Node.js and in the browser.I believe that as we delegate more coding tasks to AI, we’ll assume the role of an architect rather than a programmer. This shift will require tools that focus more on orchestration and high-level troubleshooting and less on low-level functionality.I’d love to hear your thoughts and feedback on Flyde’s direction!",
    "commentLink": "https://news.ycombinator.com/item?id=39628285",
    "commentBody": "Flyde – an open-source visual programming language (github.com/flydelabs)245 points by gabigrin 21 hours agohidepastfavorite115 comments Hi HN! I’m Gabriel, and I’m happy to share a project I’ve been working on for the last few years: Flyde, an open-source visual programming language. Check out the interactive examples and online playground on the website: https://www.flyde.dev. In my last role as an engineering manager for a B2B-oriented product, I authored and reviewed many diagrams for backend applications, mostly for integrations between 2 third-party services. Some of these diagrams were elaborate enough that I started dreaming of a way to simply run a diagram as is; I imagined a “run” button on the top-right corner of the screen that would execute the diagram without the need to translate it into code. That led me down a rabbit hole of exploration and experimentation, from tools like Zapier, Pipedream and Make, which are great for automating “backoffice” stuff, and up to NodeRED, NoFlo.js and the great work of J. Paul Morisson on Flow-Based Programming. I failed to find a tool that would answer my needs - a tool that balances a new level of abstraction, manages to stay powerful and flexible, and most importantly, integrates with the existing ecosystem, and doesn’t replace it. I built Flyde as an attempt to answer that need. Flyde is designed to complement and enhance traditional textual coding, not to replace it. It includes a VSCode extension, it seamlessly integrates with existing TypeScript/JavaScript code and can run on Node.js and in the browser. I believe that as we delegate more coding tasks to AI, we’ll assume the role of an architect rather than a programmer. This shift will require tools that focus more on orchestration and high-level troubleshooting and less on low-level functionality. I’d love to hear your thoughts and feedback on Flyde’s direction! pjerem 21 hours agoThat's really great. I know it's polarizing but I truly think that visual programming remains an entirely unexplored area. I'm convinced that the current state of \"text\" programming is totally ineffective and in fact we are the last domain producing things with computers which still insist in being limited by the text files in folders abstraction. It's a shame that in 2024, I still have to search for text in files like it's 1970, guess which file does what based on the dozen of characters of the file name and can't see at a glance which other files are dependencies or uses. I can't \"see\" my entire codebase, zoom in and out, I still have to guess the relation between some line of code and another in another file. My ideal IDE of the future just allows me to see all my codebase like a big fractale. reply thesuperbigfrog 19 hours agoparent>> visual programming remains an entirely unexplored area Visual programming languages have been around since at least the 1970s: https://en.wikipedia.org/wiki/Visual_programming_language Several are used real-world production cases: https://en.wikipedia.org/wiki/LabVIEW https://en.wikipedia.org/wiki/Simulink https://en.wikipedia.org/wiki/Pipeline_Pilot https://en.wikipedia.org/wiki/IBM_Cognos_Analytics While visual programming systems work better for some use cases, they are usually less agile versus text-based systems for several generalized programming tasks: 1) difference comparisons (\"diffs\"), 2) version control, 3) code search, and 4) code input (Most visual systems require a mouse and careful placement of connectors). Visual programming systems tend to excel for domain-specific tasks carried out by non-technical or semi-technical users, but for generalized programming, text-based programming systems are more popular for highly technical software developers. reply throwaway632 18 hours agorootparentAnother problem with visual programming is it forces your program to be planar, or at least nearly planar with some crossovers. In my experience real code can't be represented legibly on a 2D plane. \"Generate code map\" features of IDEs usually produce incomprehensible graphs, when you try them on actual codebases. I never tried CodeSee before it shut down, was it any better? reply k_g_b_ 17 hours agorootparentWhat is code in a text editor if not a legible 2D planar representation of the \"real code\"? The real difference between text and a graph of definition-use edges and nodes is that the text representation does not have any intrinsic information on the real code's structure and does not display definition-use edges - at least by default. Most code editors provide some way (via LSP/etc) to display the edges however - usually as a list of the other end of the edge in a dialog/window/similar separate from the code. Given that some textual representation (e.g. variable names) of source and target is possible, a graph-displaying editor can just \"abbreviate\" an edge (or: make it implicit) by using the other end of the edge. If a graph editor makes all edges implicit by a certain strategy of using names/whitespace/parentheses/etc, the result would be a text representation a text editor displays. The main issue is not in difference of display, but in the mode of editing: a text editor allows much more partial input that violates the structure a graph is expected to have: malformed definitions of programming constructs that are neither a node nor an edge in the graph. A graph editor must either allow such partial input in some way (e.g. enter temporary textual representation, transform into group) or use a different input model that doesn't allow such unrepresentable input, e.g. using mostly mouse, or an editing keyboard language like Vim's objects & verbs. On the other hand a text editor requires you to agree on a formatting style with your colleagues and running autoformatters, looking past useless formatty diffs in code review, fixing syntax errors, dangling else,... reply zem 13 hours agorootparenti think the parent meant \"planar\" in the graph sense, that is, capable of being drawn in a plane with edges not crossing each other. reply LudwigNagasena 17 hours agorootparentprevAnd text forces you to be linear. It’s no coincidence that everything asynchronous is treated as a minefield of potential race conditions and ephemeral bugs. “Real code” allows you to hide the mess behind imports (I think everyone had experience of dealing with accidentally circular imports at least once in their career) and abstractions. But once you put your code as a graph on a 2D plane, you realise what kind of horror you are dealing with. And sometimes even this trick doesn’t work: For example, it’s hard to abstract away the states and transitions of a finite state machine; so while they may look fine on a diagram, they often look like a mess of spaghetti as code. reply vmfunction 10 hours agorootparentprevOr if you do functional Scheme Bricks is perfect visual programming language for scheme/lisp https://github.com/nebogeo/scheme-bricks reply hermitcrab 14 hours agorootparentprevText vs visual programming is a topic that comes round regularly. I've tried to write summary of why both have their place here: https://successfulsoftware.net/2024/01/16/visual-vs-text-bas... It agrees with quite a few of the points you make above. reply spauldo 14 hours agorootparentprevThere are also the three graphical PLC languages standardized by the IEC - Ladder, Function Block Diagram, and Sequenced Function Chart. They're used everywhere - anywhere you see a shiny metal cabinet with conduit running in and out, there's a chance there's a PLC plugging away in there. Ladder dates back to the 70s and I'm willing to bet is the most used graphical language in existence. It looks like the relay diagrams that electricians use. reply int_19h 7 hours agorootparentprevIf you count flowcharts (which one should, IMO), it has been around older than any text programming languages even. reply analog31 10 hours agorootparentprevI used LabVIEW for a couple of years. The big issue for me was ergonomics. Coding was physically laborious, and I went home with splitting eyestrain headaches. It occurred to me at the time that a dataflow programming language could support either text or graphical input, depending on the user's ergonomic preferences. I have the same problem with CAD. reply javcasas 20 hours agoparentprevI think lines of code in a text editor is a terrible way of describing state machines and flow diagrams. And I think flow diagrams are a terrible way of describing many algorithms. Flow diagrams look great for multi-processing pipelines and event suff. I'm interested in this as an addition to current coding practices. reply gabigrin 20 hours agorootparent100%! Cherry-picking what makes sense to \"elevate\" to the visual sphere and having it co-exist with traditional coding is the only way I believe visual programming can be truly useful. reply IshKebab 13 hours agorootparentYeah I completely agree. Simulink works quite well because it's modelling things that are naturally physical networks. DSP/Blender style node graphs also work quite well because you really want an interactive control on every node. In contrast visual systems that try to recreate traditional programming (add, multiply, loops, etc.) seem to be mostly awful. LabVIEW is a complete disaster (probably not helped by its dire graphic design but even so). UE Blueprints are bad. Scratch is not too bad, but only because it's basically building a normal text-based program with lego, rather than nodes and edges. reply catapart 20 hours agoparentprevHard agree. For me, it's the delegation of concerns that visual programming excels at. I don't have to care how you're getting a random point in a sphere. Whatever algorithm you want to use works for me. And since you know which algorithms work better for which scenarios, you can put all of that delegation under the hood. All I have to know about is which node to use. And if something starts breaking, I know that it can't be my problem; it has to be a problem with the node's code and that domain professional can address it. Of course, a good dev might say \"That's just having a really good API\", which is true! But once you have an API so good that you don't need to know any of its internals, you're essentially trying to write a graph using documents, which is kind of silly. High-level servers are a great example (node servers, python servers, etc). There's not any utility in writing an expressJs server with text that isn't satisfied by writing it as a flow chart. Servers, at that level of abstraction, are so simple that all you really need to do is tell which keys go where, when. And that's most simply done by drawing boxes and arrows that all point to each other. Put more starkly: there's no difference between writing `Lib.Physics.GetPointInSphere(param1, param2, param3)` and linking a `GetPointInSphere` node to `param`s 1, 2, and 3. So I think that if you're in a domain that is already at that level of abstraction, visual programming is a fantastic way to go. And if you're not already at that level of abstraction, there's value in getting there (even though it's probably going to require fracturing/modularizing existing concepts). reply ivanjermakov 19 hours agoparentprevWhile I agree that text might not be the best way to represent a program, text files have many convenient properties that make them hard to replace: - Simplicity: every computer and most humans can interptet it without any issues - Diffing: it's relatively easy to tell what changed in a text file between revisions - Editing tools: text editors, formatters, etc. exist and many people know how to use them efficiently Also, modern developer setups allow you to manipulate code as a syntax tree (see tree-sitter text-objects) and intellisense and snipping tools allow you to type much less. reply andoando 12 hours agorootparentThis seems more like a matter of having more tooling because 99.999% of programming has been text based. If we spent the time building tools for visual coding it’s possible we’d be able to do way more. Just thinking about diffing for example, it would be much easier to see which nodes have changes in the whole codebase by just highlighting them red. It’d be easier also to depict something like “the flow for this process changes from this to this” reply gabigrin 2 hours agorootparentAuthor here. Diffing is a great example. The first incarnation of Flyde had naive version control built into it and had such diff features! Diffing from VSCode already makes a bit of sense (see https://imgur.com/a/WXj89tx). Next step is to color it differently, and finally, add a browser extension to render diffs nicely from GitHub/Gitlab's UI as well :) reply papa0101 20 hours agoparentprevI suppose it all depends on personality: some swe's prefer to write, some prefer to draw (I do both, but would hate to only have to deal a 1mil-lines-of-code fractal, but would be (and am) fine with the 1970 approach). The OP's solution then might be a great addition to my toolbox. reply hakanderyal 20 hours agoparentprevThat's the outcome I'm hoping to get from all these VR/AR tech people are working on. I believe we have the hardware to make this work and someone just needs to build the software. reply conartist6 20 hours agoparentprevCouldn't agree more, which is why I spent the last three years building the tech that gets us there -- a universal document object model for code! If you thought you'd be waiting 10 years for this tech, it's probably more like 2 now. reply corytheboyd 11 hours agorootparentI was like WAIT that sounds REALLY familiar :D reply divan 15 hours agoparentprevI attempted to do this [1] and explored topic of why VPL are not a mainstream too, but postponed project for maturation of VR/AR ecosystem as I see them usable only in VR now. This year actually I think to return to work on it. [1] Rethinking Visual Programming with Go - https://divan.dev/posts/visual_programming_go/ reply gabigrin 20 hours agoparentprevThanks! I agree, and while no one knows how development will look in 5-10 years from now, I find it hard to believe it'll be _just_ textual based, and I hope Flyde will help inspire that move. reply esc861 8 hours agoparentprevWhat you are describing sounds a lot like C4: https://c4model.com/ reply tistoon 16 hours agoparentprevI see it as comparing comic book (visual) vs text book (text code). You can express some visual better with comic books, but in the end, I find text books easier to define richer and complex matter. reply rsoto2 7 hours agoparentprevIDK i started using TouchDesigner and while it was cool...had to watch tons of tutorials before i was remotely effective, and once I figured out how to have it call a python script, everything was much more productive/simple. reply d--b 13 hours agoprevI will tell you the same thing I tell everyone making visual graphs like this: PLEASE PLEASE PLEASE PLEASE PLEASE make the nodes snap to a larger grid, and have the grid browsable with the keyboard. I'd rather use Factorio as an interface than an interface where the nodes and edges just float around. Excel is a DAG that is browsable with the keyboard (Ctrl+[ anyone?), and that's proven very usable. reply culopatin 10 hours agoparentAnd I rather have the boxes stay where I put them and make a mess of the lines than have the lines be all clean but every time I add a condition all the boxes reshuffle like ServiceNow does in their visual low code programming stuff reply lominming 13 hours agoprevGreat to see more people thinking about this space. Totally agree on the sentiment that we should be able to visualize/describe and build business logic in a very simple manner. The way I've been thinking about this is something like Legos where we can compose any logic, app, workflow we want visually and that anyone can build and share these blocks. This is almost similar to NPM packages, but for non-developers. My attempt on this is https://openexus.com where the goal is really to create some form of universal plug-and-play building blocks. Your approach is almost very low-level with direct translation to code. My attempt is slightly higher-level (but developers can create as low level as they want). More importantly, the visual diagram build on openexus is a reactive graph (almost like spreadsheet), not a sequential directional flow graph (like node-red, or yahoo pipes). Would love to chat if you are up for it. m at lominming dot com. reply gabigrin 2 hours agoparentOpen Nexus looks great! The UI design is super clean, and the open-ness of your nodes give it true Bret-Victor-ish vibes! I started closer to this at first (but still lower-level) and slowly got \"down to earth\", taking a https://www.dreamsongs.com/WorseIsBetter.html approach. reply westoncb 11 hours agoparentprevThis looks super nice :) love it reply catapart 21 hours agoprev- This looks great! Very robust and I really like the focus on interoperability. - As a general design, I prefer the integrated \"function with parameters\" node type found in systems like Unreal's \"Blueprints\" far better than having to manage the parameters and functions separately from each other. It's more cumbersome to develop, but no less flexible to use, and loads more simplified. A distinction for the code flow from the data referencing is helpful for me, at least. - It doesn't fit my exact needs, because I need something that allows me to expose this functionality to users. Essentially, I need something that does what your playground does, as a library. The interoperability is perfect, though! Allowing users to set up flows and then being able to simply import them rather than having to translate them is a fantastic DX. I guess, at the end of the day, I would still like clean, nicely formatted Javascript (not typescript), so I imagine your library isn't really suited for that, either. But I definitely like the architecture of it! reply gabigrin 20 hours agoparent- Thanks! - Re: \"function with parameters\" you mean so the configuration is exposed on the node itself? It's definitely something I want to consider as an UX improvement. At first, I was very puristic about having all configurations acceptable as dynamic inputs, to ensure Flyde is robust and flexible, but that led to things like the HTTP node having 5 different pins which made it even worse. Now it's a hybrid model - as an author of a node you can choose how to expose it, and many nodes in the Stdlib expose configuration in both ways - Interesting use-case! Flyde's last incarnation was an attempt to offer this to other SaaS products (before realizing it must start OS) to allow their users to do \"Visual scripting\" (like https://luna-park.app/, a project I've stumbled upon) You can check the source code of the playground and try embedding Flyde, should work! reply Herobrine2084 8 hours agorootparentHey, author of https://luna-park.app here! (I was wondering where the spike in visits came from ^^'... Thanks a lot for mentioning me!). Luna Park was indeed a npm package that you could integrate to allow users to build their own logic using visual scripting. In the end, I pivoted, and Luna Park is now a visual scripting wrapper around the Vue.js framework, allowing people to build modern webapps without code. (I also made https://roller-coaster.app to create endpoints using the visual scripting system of Luna Park) In any case, that's super cool to see people building awesome tools like yours in the visual programming space :) ! I love how Flyde show you what node is running, and the way it executes logic is really interesting! reply gabigrin 2 hours agorootparentHaha glad you found this comment! Love your dedication to the amusement park theme, it's rare to see web products take such a \"game design\" approach. reply catapart 20 hours agorootparentprevYeah, that's part of what I'm talking about, for sure. Being able to, say, click a little expand icon to see other pins that aren't usually necessary, is great for discoverability. I'm not a big fan of library maintainers deciding that I can only use certain inputs for an underlying function that I know has more functionality. Node authors should, of course, be able to guide, but I don't appreciate them having the ability to control. More than that, though, it's about the difference between 'data references' and 'execution flow'. It's something that Unreal makes very, VERY obvious. Color coding data types, emphasizing execution flow lines, etc. I can't recommend using it as a guide, enough. Whatever problem I've come up with as 'complicated' for visual scripting, they've got an elegant solution for. Even stuff like being able to 'break' nodes into other nodes, or pins into other pins. So a \"Point\" node might be useful for sending into a function expecting a Point data type, but you might also want to send just that Point's x or y value to something else. In that scenario, you might 'break' the exit pins into a Point reference, an x reference, and a y reference. It would be cumbersome and unruly to always have that available. But the UX to make it available is straightforward and satisfying. And then, all of that aside, I just prefer the visual look of pins integrated into the nodes. The extra lines you get from drawing between the pin and the node is messy and since you can't move them independently, they don't really add anything. Just a lot of extra buffer to prevent a 'crowded' feeling. But at least that's not something unique to Unreal. I don't think I've ever seen a visual scripting system that used pins as separate elements. So maybe it's just saturation bias. But I do like what I like! Anyway, I'll definitely mess around with it and see if it can help with its embedded version. To be completely honest, I think there's too much friction, overall. I would need something to render as plain javascript, rather than a JSON structure or minified JS (or typescript), so there's a translation there. And then to get it graphically how I want it would be complicated due to all the domain-knowledge required for integrating with your app's underlying library (looks like next js? Not sure, but it's not something I want to work with). Not to be too pessimistic or anything. Most libraries work toward providing a complete solution, not towards providing a composable, compsitable, modular set of utilities, which is what I need. I don't hold any contempt for providing what you've provided because it's a hell of a thing and perfect for a lot of use cases, I'm sure! I'm just very pragmatic about my own use-cases, and my development quirks (like not using libraries that obfuscate functionality [next, react, svelte, etc]). reply o-o- 13 hours agorootparentInsightful – commenting to keep a reference as I spent the last four years engineering around this very aspect. I'm trying to wrap my head around the difference between execution models. On one hand we have real-time node systems like Unreal Blueprint, Modo or DaVinci Resolve, which depend heavily on state nodes, i.e. nodes whose sole purpose is to hold and serve information (a mesh, a transformation, an RGB matrix). These nodes act as complementary inputs to nodes in the main execution flow. On the other hand we have Node Red and the likes, whose nodes rely on a single input. At first glance this approch might make sense from a UX perspective – simply connect your components and deploy. However with time I've come to see this as a major limitation to the extent that you're often better off writing code. After building a number of flows you start to notice certain patterns... Since nodes are uncapable of fetching data from other nodes (\"state nodes\" above), it's up to you as a developer to serve the node with the exact data structure it needs. The GUI gives you no hint as to the structure itself, so you're left reading the node's documentation: {payload, action, topic, subject} – different for every node. At the core, this approach is the FBP version of polish notation in the sense that you first have to fetch all the operands before sending them to the operation. So what you're left with is a flow where every core node is surrounded by data shuffling nodes: \"put this into 'action', put that into 'body', take 'payload' and store it away because the next node will overwrite it\". As an effect of this \"serial execution model\", flows often stretch wide from the left to the right where two thirds of nodes are pure data transformations. Somewhere around here the USP of FBP gets literally lost in the flow – the USP being to offload working memory. During building/debugging/revisiting flows, I often know right away what piece is broken. Nevertheless I spend a lot of time double-clicking on function, change, and switch nodes just to find that piece of code. The irony of it all is that I wanted to use FBP to _visualise_ logic. Instead the same paradigm hides it away. So what you've accomplished is transforming code that goes from up to down into boxes that go left to right. The thinking behind it all is that Node Red shouldn't make assumptions about the incoming data, however someone has to make those assumptions, and that someone is you, the developer. It would be trivial to create a node that takes an arbitrary input (a sample message), analyses the message structure and exposes the appropriate output pins, however as long as Node Red persists with single input ports and the \"message-based execution\" it entails, it will take tolls on working memory and lag behind the Unreal model which I too consider state of the art. reply catapart 11 hours agorootparentYeah, this is a really tricky space to delve into because the people who are most familiar with it are working on mathematical models and have very strong opinions about the \"kind\" of graph you're making, or what you're actually doing with the underlying data. Which is all definitely important for a node library maker to know, but the whole purpose of the library is that an implementing dev shouldn't have to know any of that stuff. So you really have to dig through a lot of irrelevant stuff about graph theory and whatnot to get to any relevant programming knowledge on the topic. What's galling to me is that nodes map 1-1 to function calls. I can't even think of a scenario where a program that can be written as functions and objects couldn't be represented as a node graph. I can certainly shudder at the horrific types of graphs most programs would make, but they can all be mapped out. And, on the flip side, every single program I've written in visual scripting could easily be written as functions and objects. So, with that in mind, it's frustrating to go looking for information about node graphs and getting a bunch of muck about why they're 'not good', and why they don't handle certain things well, and why they aren't suited for x or y, when - in my head - I'm literally just asking for a well-formed API, and I'm getting a bunch of pushback about how APIs aren't really a good way to program. -_- But because it's so hard to get good comparative information on visual scripting, I've also wrestled with what makes for good abstractions vs what is too broad. And that's what I think the Node Red stuff is: just too broad. Abstracting away everything breaks the value of the abstraction. So it's always about finding a balance, which is why I appreciate Epic's maintenance of Blueprints. A good API abstracts concepts you don't need to understand behind concepts you already understand. \"GetDistance()\" is a great abstraction because I don't care about the vector math underneath. But \"Login()\" is a terrible abstraction because a library shouldn't have the intimate details of my login process (meaning that it should be abstracted into more modular parts that I can more easily inject my business logic/keys into). So a node, or a library of nodes, or a node library system - they're all only as good as their abstractions. And Node Red is just way too abstracted to be \"good\". And, of course, the irony there is that if it were only a little more flexible, it could even be abstracted, itself, into something more use-able! If you could write a node that DID accept two inputs, you could abstract away all of the setting and swapping and trading. But since even that node could only ever be kick started by a single node, you would have to come up with some kind of hacky scheme to make any kind of utility actually function. So it's the one type of abstraction that they don't allow (their node model) which I find to be fundamental to successful visual scripting languages. Honestly, though, I think there are some fundamental designs that visual scripting languages just need to implement, but I have a very hard time describing those designs with rigor. I would naively say that they need nodes, edges, a variable deck, a node discovery utility, and a good variable property manager. But that's just because that's what I'm used to. I think once someone really serious sits down to formalize these things, we'll have a much richer environment. I just hope they do it soon! reply rcarmo 1 hour agoprevAs someone who routinely uses Node-RED to develop surprisingly long-lasting \"micro services\" and workflows, I really like this, especially if it can be made to generate WASM or, even better, native code in the future (so that I could use the output in other things). reply couchand 20 hours agoprevCongrats on the show HN! It looks like you've put a lot of thought and effort into this, and reasearched a number of alternatives. I'm curious to know more of what you found lacking in the various visual programming languages? You mention a few general things but (other than the integration angle) I'm having a hard time understanding exactly what limits you hit with the other options that caused you to build your own. And to add some context to the above questions, is this primarily your own research or do you anticipate it being used for production systems? reply gabigrin 19 hours agoparentThank you! Flyde's first incarnation was more of a \"Visual serverless\" platform, something like Zapier, but where you could \"see the JSON\" and change any piece you want. So the alternatives at that time were NodeRED, n8n and NoFlo.js. I think NodeRED and n8n are both successful products with their own niche, but I was looking for something more flexible. At the time, I was trying to build something very robust, flexible, and generic, that could also be used to build UIs with (see this cumbersome, but working example https://play.flyde.dev/apps/974a3913-1b3b-4a0a-9ca7-4e2a69d0... ) a lower-level visual language that will match the functional-reactive paradigm I was used to code with, and not something too structured. I wanted it to be able to do most things I can do with code, and cater to application-layer developers. NoFlo.js was the closest to allowing this, but I think that it was too early for the game, and NoFlo took a non-integrative approach. Vladimir Sibirov wrote vastly about that and why he thinks it failed in this great blog post - https://blog.kodigy.com/post/state-of-flow-based-programming... I was happy to see Flyde addressed this Other purely FBP implementations did not put enough emphasis on the visual aspect, which for me was crucial to nail on a holistic level. And for your last question - my goal is for it to be used in production systems, yes. I plan to release a Flyde-based visual API builder soon - https://www.trigg.dev as a more specific use-case, and hope that the fact you can download the Flyde flows and run them wherever you want will help potential users overcome the fear of using a low-code tool. reply ilaksh 12 hours agorootparentI mean, NodeRED can be used to build front ends with Dashboard 2.0 or UI-buikder or custom nodes. Is your project really better than NodeRED? I mean it does have over 4000 community contributed nodes. reply gabigrin 2 hours agorootparentNo, not at all. I'm not trying to build a better NodeRED. Moreover, I can only hope Flyde gets to the popularity of NodeRED one day and believe that while there is some intersection between the projects, each can excel in non-conflicting use cases. reply spiralganglion 12 hours agoprevNice! Have you explored showing live values flowing through the nodes? This seems like a good use of animation. I'll also take a moment to plug my Visual Programming Codex[1], which collects VPLs as though they're butterflies. I'm adding Flyde to the backlog of projects to document, but I'd also suggest looking around there for ideas. There are a ton of valuable new things you can do once you start visualizing programming, especially if you're visualizing the execution behaviour. I'd love to see you push this further. [1] https://github.com/ivanreese/visual-programming-codex reply gabigrin 1 hour agoparentThanks! And yes, definately something I am considering exploring. It's right after auto-layout, and \"zoom to view grouped nodes internals\" (a-la https://xai-primer.com/tool/) And I was planning to submit a PR for your codex soon! I've visited your codex dozens of times in the last couple of years. Big fan :) reply snadal 20 hours agoprevNice work, congrats! I do love visual programming and I use n8n a lot for my side projects. I really like its \"delayed debug\" features, so that I can analyse each step of the flow weeks later than it happened (i.e, I can see why a webhook failed long ago and even replay it step by step). One missing feature that I've been working on is a \"export workflow to code\" feature. This way, once you are finished working on a workflow, you could run it everywhere without the need of installing the full IDE. Again, nice work! reply gabigrin 20 hours agoparentThanks! Flyde is \"just\" a library behind the scenes, so you grab your .flyde file, and can run it with an npm package. For example: ``` import { loadFlow } from \"@flyde/runtime\"; const execute = await loadFlow(\"./celsius-to-fahrenheit.flyde\"); const inputs = { celsius: 0 }; // \"celcius\" is a main input in the flow, therefore it must be provided when executing the flow const { result } = execute(inputs); // execute returns a \"result\" promise, along with a cleanup function that can be used to cancel the execution. const { fahrenheit } = await result; // each output in the flow is a property on the result object console.log(fahrenheit) ``` (taken from https://www.flyde.dev/docs/integrate-flows/) But your comment strengthens my feeling that making this more intuitive and discoverable and is indeed something I should prioritize reply snadal 18 hours agorootparentNice! I definitely will try it :) reply lolpanda 9 hours agoprevI don't think visual programming language can or should replace code in plaintext. I like the idea of visual programming at higher level. It can be very good at communicating data flow, business logic and processes. But each of the building blocks can still be implemented in programming languages today. I feel like AWS Step Function is very close to what I wanted. They also have a very nice builder UI. reply gabigrin 1 hour agoparentAuthor here. Agree 100%. While Flyde allows you to group visual nodes together and create new ones with a \"visual implementation\", I foresee that code-based nodes will still be the majority. reply nagstler 11 hours agoprevThis is interesting, we are building https://github.com/Multiwoven/multiwoven an open-source ReverseETL and workflow orchestration is a big part of our platform, maybe I ca experiment with small use-case like setting up backend tests using flows. reply cjbprime 19 hours agoprevNice! https://github.com/enso-org/enso is related as a modern visual lang. Is Flyde entirely focused on TypeScript? Python would be a logical fit too. reply gabigrin 15 hours agoparentEnso is super cool, been following their work since the time they were called luna :) Flyde is TS to start with, but Python is the next candidate in language support. Eventually, the plan is to rewrite the core/runtime layer of Flyde to a more performant language (Rust/Go) and enable interoperability, so you could potentially even mix and match node implementations. One node could be built with Python, the other with JS, and the flow itself will be agnostic to it. reply s_gourichon 15 hours agoprev> visual programming 25 years ago when I read that Microsoft has a software development environment named \"Visual Studio\" I imagined, well, something with graphs and nodes and flow and... Well something visual, right? Now it's 2024 and VS is still not visual. Of course things are not as simple. Flyde (along with many alternatives mentioned) is visual IMHO. Keep up the good work! reply int_19h 7 hours agoparent\"Visual\" in Visual Studio was a reference to RAD. Basically, visual form and report designers. reply gabigrin 1 hour agoparentprevThank you! reply blowski 20 hours agoprevI like this, it’s lower-level than Scratch, with a focus on doing stuff more than making funny noises and animation. I want to have a play with seeing how you can demonstrate composition. Do you suggest any good templates for this? reply gabigrin 20 hours agoparentThanks, exactly what I aimed for (: Regarding composition, anything you had in mind? I can draft a playground (https://play.flyde.dev/) example for you reply blowski 19 hours agorootparentI teach a code club to 8-10 year olds. They understand functions, and that the main script can call functions, and that functions can call other functions. Conceptually, they struggle with a function being assigned to a variable, passed in as an argument, or being returned from a function. So far, demonstrating to them what's going on here and why this is useful has eluded me. reply brabel 15 hours agorootparent> they struggle with a function being assigned to a variable, passed in as an argument, or being returned from a function. I've seen professional programmers struggle with that. reply gabigrin 16 hours agorootparentprevAuthor here. It's a valid point, I think once they start entering into Discord and want to build a Discord bot, Flyde might come in handy as an educational tool. reply dugmartin 20 hours agorootparentprevbtw, your playground homepage has a date/time error. I see this in the DOM: Modified -12537 seconds ago You may want to use a timestamp instead of comparing local time. reply gabigrin 16 hours agorootparentThanks for reporting! Will check and fix this. reply epgui 20 hours agoprevWhat I love about this sort of tool is that it appears to pair so perfectly with functional programming. Are all expressions constructed in this language referentially transparent? reply gabigrin 19 hours agoparentYes! As a big fan of functional-reactive programming, it was important to me that Flyde builds on that foundation, and doesn't fall into more imperative paradigms. With that being said, Flyde does offer an easy way for node authors to set local and global state, but in a functional way - you can pass your own map as the global state, and do whatever you want with it. It's not properly documented yet, but here's the code - https://github.com/flydelabs/flyde/blob/main/core/src/execut... reply mdaniel 13 hours agoprevFWIW your VSCode Repository linkis 404, it should be https://github.com/flydelabs/flyde-vscode but is https://github.com/flydehq/flyde-vscode reply gabigrin 1 hour agoparentThanks! Fixing reply daltont 19 hours agoprevI was just thinking about something where I could design web based workflows that could use online services to perform transformations on items dragged into a dropped and perhaps the output would be downloadable. Maybe this exists elsewhere, but is is free?. It is still a thought and I haven't dig deep to see what exists, but Flyde seems kind of close. reply gabigrin 15 hours agoparentHey, author here. Did you try tools like Make or Pipedream for this use-case? Curious about what was missing for you in them. With Flyde you can do that, and one of the monetization strategies I have planned for Flyde is Trigg - https://www.trigg.dev, a \"visual serverless\" platform based on Flyde, that can run the work flows for you and expose them as APIs. And for \"ejection\" purposes, one could always download the flow and use the lower-level Flyde to run it self-hosted. reply rajatrocks 19 hours agoprevVery cool, thanks for sharing! I first tried out something like this in the early 1990s at VPL Research - one of the originators of the VR industry. They had a visual programming language called Body Electric for controlling how things behaved in VR. Some details here: https://news.ycombinator.com/item?id=22788773 And more recently, Google showed PromptChainer for wiring together LLM calls and Javascript: https://www.cs.cmu.edu/~sherryw/assets/pubs/2022-promptchain... reply mellutussa 20 hours agoprevWow, impressive! I also have to congratulate you on the landing page. So often I click on something that I think is interesting only to scroll down in confusion before I close the tab. I'm definitely going to check this out. reply gabigrin 16 hours agoparentThanks! this means a lot, as I've been through many iterations with this one. Each affected some other revelation in my journey with Flyde. PS: Here's an older one - https://web.archive.org/web/20230313081811/https://www.flyde... reply pie_flavor 14 hours agoprevThere's a project I keep wanting to start that involves a visual programming language and the JVM. Java supporting JS natively through Nashorn, is Flyde capable of doing this? Others I looked at like Node-RED seemed to have Node.js as a hard requirement, but Flyde supporting the browser environment makes me wonder if it can support this too. reply spankalee 15 hours agoprevThis looks very cool! I have just a couple of wishes on top of this: - I wish the file format were JSON-based instead of YAML-based. Then you could import .flyde files with standard `{type: 'json'}` import attributes and not need fetch or a webpack loader. - I wish the editor were distributed as web components so taht they could be easily embedded into any framework. reply adinb 10 hours agoprevI have yet to find a language/ide with good discoverability. Does the visual metaphor enable the blocks/objects/functions to tell you what they do and how to use them? reply gabigrin 1 hour agoparentAuthor here. Not really. I mean, there are tooltips and icons, but not in the sense that you'll be able to grasp what it does just by taking a look at its structure. reply mynjin 18 hours agoprevEh, I guess slapping some literal, variable, and operation nodes together makes it visual, but I don't think it makes it easier. Contrast with something like Scratch which is useful because it helps prevent typos, clearly presents expected arguments, and creates snap connected chains of logic. Even better, contrast something like Drakon which offers visual abstractions such as skewers, happy paths, silhuettes, common fate, etc. I really like the concept of visual abstractions. Nodes are abstractions but I don't think they are high enough level to improve over text. And I think text will always be awesome even if it is assisted by better ways to animate and visualize logic and systems. Also, can I grep over Flyde? I'd hate to lose that basic ability. Drakon: https://drakonhub.com/en/drakon reply gabigrin 15 hours agoparentThis is something I ponder about a lot. For those proficient in writing textual programs, a tool such as Flyde as-is might provide value by enforcing modules to be stand-alone and well-defined; the premise of https://en.wikipedia.org/wiki/Flow-based_programming as a paradigm that promised value even without using a visual editor, and just by adhering to the concept. But for those who lack the understanding of coding syntax and grammar, a visual tool, even in a not-much-higher level of attraction, could make all the difference. I've personally mentored dozens of entry-level developers many struggled with concurrency and asynchronicity. (callbacks, promises, etc). these are concepts that become a no-brainer using a nodes-and-wires editor. Regarding prepping - fair point. I'm sure it's not what you meant, but here's a grep in a Flyde flow (the second example) - https://imgur.com/a/V9u1ETl reply mynjin 12 hours agorootparentThank you for your reply. I'm usually skeptical of visual paradigms but I'm not trying to be critical. Looking at flow-based programming, it looks like it could help in visualizing and understanding asynchronous systems that wouldn't be so intuitive from a code listing. In that way, I suppose it would force a functional style as well. So maybe good for gluing those parts of one's apps together. I did look at the code examples; attributes and code wrapped in json. Obviously greppable, but then if one expected a learner to grep, version diff, author tests?, linting?, etc. they still must dip into and learn regular dev tools. I don't know if Flyde is supposed to eventually subsume that other functionality or if it is a higher scripting layer used in conjunction, and so must eventually be learned anyway. Or is Flyde just trying to introduce an easier coding path in order to bypass the more superfluous parts of software dev such as tabs vs spaces, editor choice, oop vs functional vs procedural vs whatever. reply o-o- 12 hours agoparentprevInteresting take on \"visual abstraction\" vs \"visual representation\" (if I may add the wording), where Drakon represents the former and Flyde the latter. I'm uncertain to what extent visual _representations_ of real code has anything to offer developers as a target group. I think it's the strive to have FBP represent already human-friendly code that gets in our way of thinking. At the end of the day I want to define and execute logic without having to approach parallelism and asynchronicity as programming concepts. Instead the paradigm should transform such challenges into spatial ones. Instead the paradigm should transform such challenges into spatial ones. (Yes I wrote that twice for effect.) I have yet to see such a system, but Drakon comes close. reply gabigrin 1 hour agorootparentI like the \"visual abstraction\" vs \"visual representation\" distinction, and agree with your point that Flyde falls into the latter. Regarding parallelism and asynchronicity, Flyde manages to answer that need. You simply connect 2 nodes in parallel - for example - https://imgur.com/a/GJewFHd this fetches data from 2 apis, maps them and collects them into a new object. It's low-level for sure, but parallelism and asynchronicity are completely spatial. Do you mean it in a different way? reply smusamashah 18 hours agoprevThis looks great. I have played with litegraph.js before for fun (image generation via api). Creating custom nodes in that one was tiring. - Can we make custom nodes in this (e.g. a node which to display images)? - Can it be used like a client side js library (like litegraph)? or does it have to be used in IDE and its more like an IDE enhancement? Looking back at the page and examples again, is it not the usecase this is for? reply gabigrin 16 hours agoparentThank you! 1. Nodes with custom logic, structure (inputs/outputs etc) and custom configuration editing are definitely possible. But I assume you mean custom nodes as in nodes that render the data passing through them, like in litegraph.js, right? If so, it's not possible, as IMO it's less of a value for \"production\" use-cases/application level layer. But if the community's interest will slide in that direction, I will consider going that route. 2. It can be used as a library, either Node.js (via an npm package) or the browser (using a custom webpack loader) reply luke-stanley 18 hours agoprevDoes it sync bi-directionally from code to flow visual representation and back? Because that seems pretty useful to mass adoption. I notice you said \"Flyde is designed to complement and enhance traditional textual coding, not to replace it.\" Bi-directional sync would help a lot with that idea. reply gabigrin 16 hours agoparentIt's a question I get many times and while the point makes sense, I think that the value of translating a piece of code as-is to Flyde isn't that high. The resulting visual representation will resemble the AST of the transformed code and be less of a value. It's like converting assembly to C and vice-versa. It might be useful, but data will be lost and a 1 to 1 conversion doesn't really exist. On the other hand, you can easily take a single piece of your code, say a business logic of a controller in an MVC-based web service, and transform it into a Flyde flow and call it from the original controller. With that being said, I have no doubt that for Flyde to be truly mass-adopted I have to invest more in easier onboarding and safer ejection. reply penteract 19 hours agoprevFor me (Firefox; Linux; X11) right-clicking on the background when editing a visual flow causes to a menu to appear then disappear almost instantly. Everything else seems to work (clicking run, adding nodes from the menu, right-clicking on nodes and editing them that way). Does anyone else have this problem? reply gabigrin 16 hours agoparentHey, author here. Does this happen in the playground, home page or extension? Will look into it! reply penteract 11 hours agorootparentI observed it in the playground. reply andoando 12 hours agoprevI love this OP. I have so many ideas around this and have spent years thinking about doing something similar (albeit very different). reply gabigrin 1 hour agoparentThanks! reply fcsp 20 hours agoprevLooks interesting! However, the tutorials link on the website footer 404s for me. reply gabigrin 20 hours agoparentOh, snap! Deploying a fix now. Forgot to remove that after a docs rehaul. Tutorials still need to be re-written after some major changes, but perhaps this old blog post can give a sense - https://medium.com/@gabrielgrinberg/visual-programming-in-vs... reply mattdesl 19 hours agoprevLooks great! Is it capable of doing real-time editing? For example hooking it up next to a canvas element and having the nodes change the colors or shader uniforms. reply mellutussa 20 hours agoprevThe playground button on the front page https://www.flyde.dev/ links to localhost:3030 reply gabigrin 20 hours agoparentOh damn! thanks for reporting! Fix going live now Please try a different example (it changes the link based on the example on the right side). reply vmfunction 10 hours agoprevKinda reminded me of Pure Data or MaxsP. reply bearwithme1 20 hours agoprevHow does this compare with Pure Data? reply gabigrin 20 hours agoparentAuthor here. The main difference is the target use case. I'm far from being a Pure Data expert, but it seems more aimed towards music/media generation. Flyde on the other hand is aimed to be used in the application layer of modern web development. reply couchand 20 hours agoparentprevThe model seems quite similar to a subset PD, with only object and message boxes. Objects can have only a single output port. (edit: I was mistaken, they can have multiple ports, though as far as I can tell there's no port-sequencing rules a la PD). Objects don't seem to be addressable: the only messages that can be sent are via the visible connections. The type system seems to be directly leveraging the host JS type system, but the docs seem a bit sparse on that. The documentation on control flow is empty, so it's not clear what the model there is precisely, but it apprears that the entire system is message-based, given the way the Hello, World is written: \"output hello, wait a bit, output world\". The documentation suggests that what's novel about Flyde is the integration with other codebases, but it's JS/TS so it requires that whole enviroment. PureData is embeddable in any C application, so any language with C FFI can integrate. reply otabdeveloper4 19 hours agoprevDid you just reinvent Node-RED? reply gabigrin 19 hours agoparentValid point. Node-RED is a successful and popular tool and has a great ecosystem, especially around automation and IoT. NodeRED is a standalone tool than a \"language\", and although it's possible, using it to integrate with (and from) existing code is not trivial. Flyde on the other hand, takes more of a \"library\" stance. It is agnostic to the code running it and can be easily embedded into other codebases. reply WillAdams 19 hours agoprevAs a visual person (traditionally trained as a graphic artist), I've wanted this sort of thing for a long while, and I've been trying to use it for 3D. Surprisingly, there are multiple specialized tools for this: - https://www.blockscad3d.com --- an adaptation of Google's Blockly to OpenSCAD - https://github.com/derkork/openscad-graph-editor --- wires and nodes, it has the advantage of exposing _all_ of OpenSCAD's commands (the above has a subset) - https://github.com/Tanneguydv/Pythonocc-nodes-for-Ryven --- a module for using PythonOCC in Ryven --- when I finally succeeded, I found the language inscrutable, even when provided w/ quite nice examples (definitely a failing on my part, not that of the tool) - https://github.com/graphscad/graphscad --- it took a long while for the source code for this to be made available, and for a while it had compatibility problems (why was \"cube\" redefined?) --- probably defunct for political reasons, it had some interesting ideas, in particular the ability to have custom icons for modules - https://www.nodebox.net --- if memory serves I got hung up by not easily being able to do 3D, and when doing 2D having precision problems (or maybe that was Processing.org) and I've been using these tools to make various things: https://willadams.gitbook.io/design-into-3d/3d-project (and maybe eventually I'll finish something) The problem I've been running into is there doesn't seem to be an answer to the question: \"What does an algorithm look like?\" I recently had occasion to mention Herman Hesse's _The Glass Bead Game_ (also published as _Magister Ludi_) and I'll bring it up again --- what is a meaningful graphical representation of a program? The Drakon folks argued that there should be one true path but that's not really communicative and I would note that if this was a simple thing it wouldn't be decades since I last saw a physical Flowcharting Template: https://americanhistory.si.edu/collections/object-groups/flo... (and it's pretty rare to even see a well-done electronic drawing of a flowchart since Visio made its splash and vanished into the bowels of Microsoft) The main problem seems to be one of expressiveness not scaling up well, hence: https://blueprintsfromhell.tumblr.com/ https://scriptsofanotherdimension.tumblr.com/ Presumably, one doesn't want to define modules/variables unnecessarily --- but the question becomes where is that dividing line? If you define too many, then you're back to the \"wall of text\" which one was trying to avoid (but wrapped up in nice boxes with some lines or shapes), and if one doesn't use them (well, look at the pretty/awful images in the links above). Ideally, a well-coded visual program would have a pleasing aesthetic appearance which is expressive and communicates flow and function, and I've tried for that at: https://willadams.gitbook.io/design-into-3d/programming (though I wish that there was an easy way to export an SVG version of a program) I believe that what is needed here is some graphical equivalent to Literate Programming: http://literateprogramming.com Is there a nice tutorial for GUI toolkit integration which would allow easily making a graphical application with this? I have an idea I want to try it which might be a good fit. reply CyberDildonics 19 hours agoprevI think approaches like this can be good for people learning programming for the first time, but there are two huge aspects that mean it is unlikely to be used for anything serious. 1. A new language. New languages don't go anywhere 99.9% of the time and when they do it takes a massive undertaking by hundreds of people as well as very careful design over the course of decades. 2. Building visually at the expression level. Writing expressions in any modern language is very compact. In this case what takes up a single line now takes up and entire screen. if(n>1) return fib(n-1) + fib(n-2); Graphs are much better for working at a high level because that's where the locality isn't there are it isn't clear what data is going where. For expressions drawing lines instead of just using the same variable that was used in the previous line doesn't help clarity. reply andoando 12 hours agoparentThis is easily solved by having nodes which serve as a collection of nodes reply CyberDildonics 7 hours agorootparentThat's basically a function, and it might work out if one typical line didn't end up taking up so much space. Expressions still end up more clear and more condensed by such a huge margin it makes nodes look silly in general cases. Houdini is able to make it work but it's all data flow, there are no branches or loops. This gets in to another problem I didn't mention - branches and loops (not to mention class declarations). Feeding a function result into another function argument is great, but that was never difficult for expressions anyway. Once you get into branches and loops you have another problem that is a huge problem for nodes. Ultimately programs don't map to data flow nodes, they are imperative. Certain parts of certain programs will end up mapping very well, but when looking for silver bullets in something that isn't 1:1, you end up hacking it up and forcing a square peg in a round hole to make the dream work. reply speed_spread 20 hours agoprevLooks cool, shades of Prograph, that's a good thing! https://en.m.wikipedia.org/wiki/Prograph reply mypalmike 15 hours agoparentI always wanted to love ProGraph - I played with it back in the pre-OSX Mac days. But I found it's very hard to \"read\" ProGraph code, which it would seem is the thing that visual languages are supposed to enable. It could be that my brain had already been shaped into text based programming for too long to adapt. That said, I did create a visual toy language demo a few years back for the fun of it. It's kinda like a visual Lisp, or maybe upside-down ProGraph: https://github.com/mypalmike/skastic reply gabigrin 20 hours agoparentprevThanks! reply littlestymaar 17 hours agoprevQuick feedback: put a screenshot (or even better: a gig/video) on the readme! reply gabigrin 15 hours agoparentThere should be one :o Perhaps it took time to load on your end? reply littlestymaar 11 hours agorootparentYou're right, I'm on a really slow connection today, and you didn't put any alt text ;). reply gabigrin 1 hour agorootparentCaught me there! Fixing :) reply lolive 14 hours agoprev [–] [off-topic] I used to follow another visual language a while ago. It seems that it is still alive. Tersus Studio: http://www.tersus.com/#Id=178 [may be some good things to take there, as an inspiration] reply gabigrin 1 hour agoparent [–] I'm so immersed in this space that it's rare to find a related project I haven't heard of before. Thanks! Looks interesting. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Flyde is an open-source visual programming language created to enhance traditional coding by enabling the execution of complex diagrams without the need for manual code translation.",
      "It integrates seamlessly with TypeScript/JavaScript code, compatible with Node.js and web browsers, offering a bridge between visual and text-based coding.",
      "Gabriel foresees a future where AI handles coding tasks, emphasizing orchestration and high-level issue resolution, while inviting input on Flyde's development direction."
    ],
    "commentSummary": [
      "Users are discussing Flyde, an open-source visual programming language meant to enhance traditional coding practices.",
      "The conversation delves into the pros and cons of visual programming languages versus text-based coding, contemplating their effectiveness and future evolution.",
      "Users appreciate the distinct features of visual programming tools such as Flyde, Luna Park, and Node Red, recognizing the convenience and flexibility offered by text-based coding."
    ],
    "points": 246,
    "commentCount": 115,
    "retryCount": 0,
    "time": 1709814816
  },
  {
    "id": 39627443,
    "title": "Monitoring Global Energy Systems from Space",
    "originLink": "https://blog.datadesk.eco/p/sky-lapse-in-two-tone",
    "originBody": "Share this post Sky lapse in two tone blog.datadesk.eco Copy link Facebook Email Note Other Discover more from Data Desk Powering the climate movement with refined data on raw materials Subscribe Continue reading Sign in Sky lapse in two tone Using Sentinel-1 radar imagery to make out the shapes of our changing seaborne energy system Sam Leon Mar 7, 2024 Share 2023 in the busy English Channel, where shipping lanes are clearly visible. The European Space Agency’s Sentinel-1 satellite paints a striking picture of our changing energy system at sea. Sentinel-1 uses synthetic-aperture radar, which can detect objects at sea and through clouds, making offshore infrastructure and shipping lanes visible as constellations of sparkling white dots. Our economy at sea has tended to be a blind spot for satellite imagery services, with the eyes in the sky taking much more interest in what is happening on land. The world’s most well-known satellite imagery service — Google Earth — does not serve pictures of the ocean at all, which just appears as a vast mass of fuzzy blue on the platform. Yet the ocean economy is growing faster than the global economy. Most of the international trade in fossil fuels still takes place over the sea — 60% of the world’s oil production transits by sea. It is also a key venue of the energy transition with the rapid expansion of offshore wind and with the seabed, which is also a controversial new frontier for the mining of critical minerals. Inspired by some similar images shared by Tim Wallace in 2020, we’ve created a series of black-and-white composite renderings of some of the points of flux in the global energy system. Created in Google Earth Engine, these pictures are effectively annual timelapse images based on Sentinel-1’s radar, consisting of multiple snapshots taken by the satellite across 2023. Offshore wind farms, shipping lanes and oil and gas infrastructure glimmer like constellations of stars in a sea of black. North Sea offshore wind The build-out of the massive Hornsea offshore wind farms (1 & 2). The North Sea now glitters with the world’s largest offshore wind farms. Often difficult to see clearly with non-commercial satellite imagery, Sentinel-1’s radar imaging picks out the turbines as matrices of fine white dots. Hornsea 2, the world’s biggest offshore wind farm located off the Yorkshire coast, came online in August 2022. Alongside its sister project Hornsea 1, the cluster is made up of 339 windmills and covers an area of 335 square miles, it can provide enough electricity to power 2.4 million homes. Sentinel-1 provides a beautiful picture of its growth. Zooming out from Hornsea, you can see many other constellations off the East Coast of England, especially around the mouth of the Humber. Robin Hawkes has built a beautiful map that lets you visualise the power generated by any of these farms at a given moment. 2023 view of the Humber offshore wind cluster in the North Sea. Some of the other clusters of light are offshore oil and gas rigs — a fundamental but declining part of the North Sea economy. According to the Financial Times, offshore wind turbines now exceed the number of oil platforms globally. Thanks for reading our Substack. Subscribe now to receive new posts. Subscribe Dörtyol and shifting flows Traces of tankers at Dörtyol across 2023 — how many were carrying forbidden Russian oil destined for Europe? Dörtyol is a lesser-known oil port in Turkey and the transit point at the heart of our investigation with POGO and The Washington Post into the potential laundering of Russian oil into European refineries, which supply both US and UK naval bases. Global crude oil, diesel, and gasoline flows have been reconfigured as Europe’s sanctions on Russian oil have bitten since their introduction in 2023. Some Russian hydrocarbon molecules have seemingly continued to flow into Europe but have taken more indirect routes via places like Dörtyol or India’s Jamnagar refinery. Strait of Malacca Strait of Malacca in 2023. One-third of the world’s seaborne crude oil and half of globally traded goods move through the Strait of Malacca. The shipping lanes are intensely crowded, especially around Singapore, one of the world’s great trading hubs, and you can see them clearly in the radar imagery. Our research with Transport & Environment tracked the movement of tankers carrying palm oil from Indonesia to European biofuel refineries that had supposedly phased out the controversial feedstock. Panama Canal Panama Canal with traces of vessels to the South (Pacific) and North (Atlantic). You can make out a snake of white light through the Panama Canal, which connects the Atlantic and Pacific and through which, until recently, 13,000 to 14,000 vessels passed a year. Our friend, Seb over at Energy Flux, has recently written about the droughts affecting the canal preventing the passage of LNG tankers and how these may ultimately lead to fragmentation of the global LNG market and lower LNG prices in Europe as the route to take American LNG to Asia becomes longer and more expensive. Thanks for reading our Substack. Subscribe now to receive new posts. Subscribe Share",
    "commentLink": "https://news.ycombinator.com/item?id=39627443",
    "commentBody": "Sky lapse in two tone (datadesk.eco)245 points by ltrg 23 hours agohidepastfavorite31 comments moxli 19 hours agoThere is a NGO in Germany (Space-Eye) working with satellite images for search and rescue purposes. I know from some people involved there that they are always looking for developers and data scientists. The main page is in German but the call for volunteers (you need to scroll down to \"Volunteers for Space-Eye\") is in English. [1] https://space-eye.org/satelliten-erkundung reply jvanderbot 21 hours agoprevTechnical SAR question: Is the sea surface removed post-hoc, or does the sea not reflect these radar waves in a sufficient amount to show up? I seem to remember NISAR [1] will map land and ice - so presumably water ice shows up. I ask b/c if they are synthetically removing the sea surface at 1-5m resolution that seems really hard given tides, waves, etc. 1. https://en.wikipedia.org/wiki/NISAR_(satellite) reply m2fkxy 21 hours agoparentCalm water surface is a specular scatterer, meaning radar energy will be reflected away, and proportionally so as the incidence angle increases. Images collected at lower incidence (closer to nadir) might feature very bright surface water if it is calm, as more energy is reflected towards the radar. Rough water surface is a diffuse scatterer, and will generally appear brighter than calm waters. reply fithisux 19 hours agorootparentAny good source for SAR image formation? From antennas to image processing? reply m2fkxy 19 hours agorootparentI am just a half-educated layman when it comes to SAR, which has a very heavy electrical engineering heritage. I hear that one very good technical resource is this book written by Iain Woodhouse [1], but I would be lucky to understand a tenth of it myself. There is also a very good SAR vulgarisation book written by Tom Ager [2]. [1] https://www.routledge.com/Introduction-to-Microwave-Remote-S... [2] https://www.amazon.com/Essentials-SAR-Conceptual-Remarkable-... reply fithisux 1 hour agorootparentthank you very much reply enriquto 21 hours agoparentprevThe liquid water absorbs most of the radar wave (at least for the C band, as is the case of sentinel-1). Thus, in the images it appears natively \"black\". When there are a lot of waves, the surface of the water forms some spurious reflectors that appear as a light texture in the surface, but the signal is definitely less powerful than metal/concrete reflectors of buildings and ships. Notice that in the \"Humber\" image of TFA you can appreciate some texture in the water. The contrast in this image has been exaggerated a lot, which saturates most of the land. reply m2fkxy 21 hours agorootparentIt actually does not absorb radar energy, rather it reflects it away (specular reflection). That's why rougher sea surface appears brighter on SAR as it turns from a specular to a diffuse scatterer. Furthermore, in some specific conditions (low incidence angle, closer to nadir), calm water can appear much brighter than other land surfaces. reply enriquto 19 hours agorootparentthanks for the clarification! I'm just used to looking to the images without thinking too much about the water. I've never seen \"close to nadir\" radar images... wouldn't the ground fold over itself? reply m2fkxy 19 hours agorootparentNadir-SAR would look very confusing, full of ambiguities, and probably unexploitable since the ranging part of RADAR would not be able to distinguish the left returns from the right returns -- that's why SAR is side-looking. even with a strictly side-looking geometry, images collected closer to nadir (steep/low incidence) start exhibiting some artifacts such as nadir returns [1]. [1] https://www.researchgate.net/figure/Example-of-nadir-echo-in... reply notahacker 21 hours agoparentprevThey're composite images, so there's definitely some post-processing going on. The sea surface will show up darker, but SAR is sufficiently sensitive to to detect oil slicks. reply 3dsnano 20 hours agoprevcool way to see the formation of wind farms from above... hornsea seems to follow a pattern then it gets all organic looking on the left hand side. i wonder why? my absolute favorite of the wind farms shown is sheringham, a true ocean rhomboid. i got curious and found the website for the wind farm [1], and here's an aerial view [2] [1] https://sheringhamshoal.co.uk/about/overview.php [2] https://sheringhamshoal.co.uk/about/benefits-of-offshore-win... reply sparsely 21 hours agoprevI wonder at what density of wind farms they begin to interfere with each other's generating capacity. Presumably within one farm the turbines are sufficiently spaced that the reduction in power is minimal, but how large can an individual one get? Or is the amount of energy they take out of the wind negligable? reply gmane 21 hours agoparentYou pretty much answered your own question: compared to the size of the coast, the amount of energy these wind farms capture is negligible. You might see a reduction of power if you built a dense farm up and down an entire coast, but even then, the ocean is big compared to these farms. Edit to add: the Hornsea wind farm featured in this article is 2.5 GW and about 400 sq miles. [0]. The total energy capacity of existing generation assets is on the order of 7,500 GW [1]. Let's double that, so 15,000 GW, which would be about 2,400,000 sq miles. There's 1,015,756 linear miles of coast [2]. We know Hornsea is roughly square, so a 20 mile deep set of turbines doesn't interfere with each other, so that gives us ~20,000,000 sq miles of usable coast for wind, and again, if we double the existing electrical generation for the earth, we'd cover 2,400,000 sq miles. (obviously not all of that is usable, but we're talking orders of magnitude here) There's really no conceivable situation where we'd build enough wind farms to interfere with each other. [0] https://en.wikipedia.org/wiki/Hornsea_Wind_Farm [1] https://www.statista.com/statistics/267358/world-installed-p... [2] https://en.wikipedia.org/wiki/Ocean reply helsinkiandrew 21 hours agoparentprevA maximum of 60% of the winds power can be extracted from wind (Bet'z law [1]) and modern turbines are only capable of catching 80% of that. There's lots of research in optimal spacing. I read that a rule of thumb was 4-5 diameter widths between turbines at right angles to prevailing wind and then 7 diameter widths between rows facing the wind [1] https://en.wikipedia.org/wiki/Betz%27s_law reply The_Colonel 21 hours agoparentprevWikipedia has a short paragraph on the topic with some references for more details: https://en.wikipedia.org/wiki/Wind_farm#Turbine_spacing reply DavidPeiffer 21 hours agoparentprevI don't have the answer handy, but I do know there is a ton of research that goes into wind farm optimization. Location of the towers, cost to build, etc. I am curious how much small elevation differences impact optimal positioning of turbines, as that is a non-issue with offshore wind. The turbines in land-based wind farms are not in a nice grid like the offshore wind farm. https://openinframap.org/#9.68/41.6112/-92.4971 The excerpt from an abstract below mentions the wake turbulence can cause other turbines to shut down. \"...The distance between the turbines is among other things dependent on the recovery of wind energy behind the neighboring turbines and the increased wind load. Models for the mean wind speed and turbulence intensity in wind turbine parks are considered with emphasis on modeling the spatial correlation. Representative limit state equations for structural failure of wind turbine towers are formulated. The probability of failure is determined taking into account that wind turbines are parked for wind speeds larger than 25 m/s resulting in reduced wind loads. An illustrative example is presented where illustrative models for the spatial correlation is taken into account\" https://www.tandfonline.com/doi/full/10.1080/102866006011566... reply algo_trader 20 hours agoprevOff topic: are there datasets for solar incidence over the oceans ? There are plenty of accurate maps/widgets for ground based PV, but ocean data seems much rarer reply Cthulhu_ 20 hours agoparentThat's because ocean PV is... unrealistic; the wear and tear on these things is immense. I am guessing windmills are affected less, with the blades being made out of fiberglass and most of the construction being above the waves. Source: I made it up, I'm a software engineer not a maritime power expert. reply jpm_sd 19 hours agorootparentI worked on a \"floating solar\" project at Google and you're completely correct. Also, waterproofing is a nightmare, and the panels get dirty (salt, algae, bird shit) and stop producing power efficiently. reply electricships 15 hours agorootparenthow much higher is the capex for deep sea floating structure ? (i guess stuff like o&m, grid connection and yield are really project dependent) edit: solar duck just announced 150mw combined pv wind project reply cmos 15 hours agorootparentprevI work in Oceanography.. The solar panels on individual buoys also get cleaned by rain, and can work ok vertically because of reflections. We have special diode bypass panels made so if 10% is covered in poop (SeaLion poop in our case) only 10% of the power is reduced. Our biggest problem with small wind turbines on buoys is fishingpeople throwing line into them. But yea, solar for power generation at sea is not realistic.. and you are stealing it from the tiny animals that make up our carbon sink. https://twilightzone.whoi.edu/explore-the-otz/value-of-the-o... reply hardlianotion 21 hours agoprev [–] I had no idea shipping lanes were so densely populated. EDIT - thanks all. I'll read it next time ... reply mysterypie 20 hours agoparentHere's a site that does live tracking of ships: https://www.marinetraffic.com/en/ais/home/centerx:1.7/center... \"Since December 2004, the International Maritime Organisation (IMO) requires all passenger and commercial vessels over 299 Gross Tonnage that travel internationally to carry an AIS transponder that include a GPS receiver which collects the vessel's position and movement details.\" Counting across the longest segment of ships at the narrowest point of the English Channel in the feature article I see about 18 ships. In the live image from marinetraffic I see about 8. So even though the feature article is a composite image, the English Channel is indeed pretty crowded. reply hardlianotion 20 hours agorootparentYes, you can still see the shipping lanes very clearly. reply buzzm 19 hours agorootparentprevOutstanding link; thanks for sharing. reply d1sxeyes 21 hours agoparentprevThese are composites, although it's not clear exactly how many images are composed to build the final view. reply rob74 21 hours agoparentprevTo quote the article: > Inspired by some similar images shared by Tim Wallace in 2020, we’ve created a series of black-and-white composite renderings of some of the points of flux in the global energy system. Created in Google Earth Engine, these pictures are effectively annual timelapse images based on Sentinel-1’s radar, consisting of multiple snapshots taken by the satellite across 2023. Not sure if the wind farm images are also composites (wouldn't make a lot of sense), but those of shipping lanes definitely are... reply sokoloff 21 hours agoparentprevThose are composite renders, integrating over time, not a snapshot. > Created in Google Earth Engine, these pictures are effectively annual timelapse images based on Sentinel-1’s radar, consisting of multiple snapshots taken by the satellite across 2023. reply cricalix 21 hours agoparentprevYou can look at Marine Traffic to get a better sense of now versus the composites in the article. reply louthy 20 hours agoparentprev [–] The English channel is apparently busiest shipping lane in the world, so it’s not surprising that it looks so busy on these images. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The European Space Agency's Sentinel-1 satellite utilizes radar imagery to track shifts in the global energy sector at sea, such as changes in offshore wind farms, shipping routes, and offshore oil and gas infrastructure.",
      "The article highlights how Europe's sanctions on Russian oil are reshaping global crude oil movements, alongside the impact of droughts on the Panama Canal and its repercussions on the worldwide LNG market."
    ],
    "commentSummary": [
      "The focus is on satellite images from Space-Eye, a German NGO engaged in search and rescue operations, discussing SAR imaging and offshore energy generation.",
      "Technical inquiries about SAR imaging, wind farm layouts, and shipping lane density in the English Channel are central to the conversation.",
      "The dialogue encompasses topics like SAR image creation and the efficient placement of wind turbines within wind farms."
    ],
    "points": 245,
    "commentCount": 31,
    "retryCount": 0,
    "time": 1709808519
  },
  {
    "id": 39630457,
    "title": "The Evolution of UNIX: A Story of Berkeley Software.",
    "originLink": "https://www.abortretry.fail/p/the-berkley-software-distribution",
    "originBody": "Share this post The Berkeley Software Distribution www.abortretry.fail Copy link Facebook Email Note Other Discover more from Abort Retry Fail In Abort Retry Fail, I am attempting to chronicle the history of the computing industry. This is a goal that will likely not be completed within my lifetime. Subscribe Continue reading Sign in The Berkeley Software Distribution UNIX is always litigious Feb 5, 2024 14 Share this post The Berkeley Software Distribution www.abortretry.fail Copy link Facebook Email Note Other 4 Share The first public presentation of UNIX was made at the Symposium on Operating Systems Principles at the IBM Research Center in Yorktown Heights in October of 1973. Dennis Ritchie is quoted as saying it was beautiful day, and Ken Thompson layered his own memories with a thick coating of modesty: The audience was several hundred. I was pretty nervous. The response was the normal, polite applause. I don't recall any questions. The IBM Research Center in Yorktown Heights, image from IBM In contrast to Thompson stating that he didn’t recall any questions… the two were immediately asked for copies of the operating system, and this wasn’t a simple matter for AT&T. The American Telephone and Telegraph company had been established as a legal, nation-wide monopoly in the USA via the Kingsbury Commitment in late 1913. This position was further cemented during World War I when the United States’ federal government nationalized the phone system. Following the end of the war, the phone system went back into the hands of AT&T and the company achieved some rather remarkable regulatory capture with the Willis Graham Act of 1921 and the Communications Act of 1934. This complicated legal history presented a very serious question to AT&T’s legal department when people began asking for UNIX: were computer operating systems part of the common carrier services of the phone company and therefore required to be distributed? If they were not, then the company needn’t distribute UNIX at all, but if they were indeed, then it was only a matter of time before the FCC would force AT&T to distribute UNIX. In the end, the decision was made to distribute UNIX to universities and research centers at the cost of the media plus shipping. Somehow, quite magically, this resulted in a nice round number of $150.00 (or around $927 in 2024) for Katholieke Universiteit in Nijmegen, The Netherlands in December of 1974. There are some rather important points within the license that Katholieke was granted. Licensees were granted source code for the operating system as computer systems of the day weren’t standardized in any meaningful way. The license then granted free use and modification within the university, but disallowed any spread outside. Specifically, the license mentioned that employees and students had access. UNIX license from Western Electric to Katholieke 438KB ∙ PDF file Download UNIX license from Western Electric to Katholieke Download At this point in the computer industry, user groups were somewhat common. IBM had SHARE, and that had inspired similar groups around DEC, Burroughs, Rand, and so on. It was therefore somewhat natural that a group would form around UNIX. Thus, Mel Ferentz and Lou Katz organized a meeting of UNIX users in New York on the 15th of May in 1974. Around twenty people were in attendance, and by this time there were just over thirty UNIX installations outside of AT&T and its subsidiaries. This user group grew to become USENIX over time. Following the user group’s formation, a mailing list started. From the first list on the 30th of July in 1975, we have the following organizations listed as installation/user sites: AT&T, Brooklyn College, Carleton College, Case Western Reserve University, The Children's Museum, City University of New York, Columbia University, Duke Medical Center, East Brunswick High School, Harvard University, Hebrew University of Jerusalem, Heriot-Watt University, Johns Hopkins University, Knox College, Naval Postgraduate School, Oregon Museum of Science, Polytechnic University of NY, Princeton University, Rand Corporation, St. Olaf College, Stanford University, The Spence School, University Catholique de Louvain, University of Alberta, University of California (Berkeley), University of Manitoba, University of North Carolina, University of Saskatchewan, University of Texas (Dallas), University of Toronto, University of Utah, University of Waterloo, and the University of Wisconsin. As we know from the license granted to Katholieke, there were more UNIX user sites than this, but they weren’t members of the user group (or at least not at first). Dennis Ritchie (standing), Ken Thompson at the teletype, PDP-11 1972 In this early time period, UNIX only ran on the PDP-11, but that changed in 1976. The first port was at Princeton where UNIX was ported to the IBM 360 in late 1976. From the history I can find, this wasn’t a fully functional port until later in the following year. The second port was to the Interdata 7/32 at Wollongong University by Richard Miller on the 4th of February in 1977. This was an interesting one as the UNIX kernel was running on top of the Interdata operating system at first. The Wollongong port became a stand-alone and fully functional port on the 28th of April in 1977. The next target was the Interdata 8/32 in 1977 which was undertaken by Ritchie and Steve Johnson (author of yacc, lint, pcc). But, porting efforts took off only after John Lions at the University of New South Whales wrote a commentary on the UNIX sources and distributed them as a book, Code and Commentary, for teaching students about operating systems. Western Electric tried to stop dissemination, but this was apparently impossible. Likewise, modifications of UNIX began circulating following the release of Lions’ book, and a culture we would recognize today as “open source” began to develop helped in no small part by AT&T’s policies regarding UNIX that essentially stated UNIX would have no advertising, no support, no bug fixes, and payment in advance. To color the time period more thoroughly, Bob Kahn and Vint Cerf published the first description of TCP/IP in 1974 and by January of 1976 there were sixty three hosts on ARPAnet, and UNIX while being used globally would run only on hardware that cost over $9000 (around $48000 in 2024). Professor Robert (Bob) Fabry was at the Symposium on Operating Systems Principles where UNIX had first been announced and he was very excited. Returning to UC Berkeley where he was then employed, he assembled a group to purchase a PDP-11/45. As this was a large purchase, he coordinated the departments of computer science, math, and statistics. With the machine purchased, Fabry then ordered a tape of UNIX from Thompson. The actual installation of UNIX was first undertaken by Keith Standiford in January of 1974. In 1973/1974, it was somewhat routine for Thompson himself to be involved in nearly every UNIX installation for a licensee. The folks at Berkeley seemed to be interested in doing everything themselves, but things didn’t go well. Eventually, Standiford reached out to Thompson, and Thompson would connect to the University’s 11/45 over a three hundred baud acoustic coupler to remotely debug crash dumps from New Jersey. I personally like to imagine that it was a Novation CAT 300, but I haven’t been able to find a model number of the modem, and I haven’t found any reference to what system Thompson was actually using. Plus, the Novation CAT 300 wouldn’t be released for several years. Following the purchase of the PDP-11/45, the departments involved began having issues with scheduling time on the machine. Berkeley bought several more computers. One of these was a PDP-11/70, and its arrival coincided with the arrival of Thompson as a visiting professor. Thompson, Bob Kridle, and Jeff Schriebman then setup V7 UNIX on the 11/70. Shortly after the installation was completed, two graduate students, Chuck Haley and William Nelson (Bill) Joy, arrived on campus. They were intrigued by the computer system, and they began hacking on Thompson’s Pascal compiler. Within a few weeks (from what I have been able to find), the teletypes attached to the 11/70 were replaced with ADM-3 screen terminals. ADM-3, image by Chris Jacobs, CC BY-SA 3.0 For Bill Joy, using ed or em on a screen terminal wasn’t really sufficient. He took a detour from hacking on Pascal, and he created the ex editor. Together with Pascal, the V7 UNIX at Berkeley was notably better than other UNIX systems of the time. In early 1978, Bill Joy began offering the Berkely Software Distribution. The first copy we know to make it out of Berkeley was to Tom Ferrin at UCSF on the 9th of March in 1978. The license was signed on the 13th, the media was an 800 bpi tape, and on the tape was the “Unix Pascal system” and the “Ex text editor.” Credits were made to W.N. Joy, S.L. Graham, C.B. Haley, K. Thompson for Pascal, and to W.N. Joy for Ex. Bill Joy BSD (now referred to as 1BSD) shipped around thirty copies in the first half of 1978. Somewhere around June, Pascal had been further improved, the C shell had been written, vi had been written (by Joy), and termcap had been written (by Joy). These new tools comprised the bulk of the Second Berkely Software Distribution or 2BSD. Joy was the man running the show, and he’d answer the phone, create the tapes, incorporate feedback, package, and mail the software. Seventy five copies of 2BSD were sent out. The DEC VAX was first introduced in 1977. It was a 32 bit ISA with virtual memory. In the first half of 1978, Professor Richard Fateman at Berkeley was looking for a machine with a lot of memory for a project of his. The VAX-11/780 met his requirements and his budget, and he got some folks together to purchase it with a bit of help from the National Science Foundation. The VAX did have one disadvantage in the minds of those at UC Berkeley; the VAX ran VMS. Luckily, UNIX had already been ported to VAX as UNIX/32V (V7 UNIX variant) by John Reiser and Tom London at Bell Labs. This port, however, didn’t take advantage of the main VAX feature, virtual memory, which limited the available memory to 1MB. This had to be fixed. For Fateman, virtual memory was a requirement, and UNIX was a requirement. He then contacted Professor Domenico Ferrari about getting virtual memory support in UNIX. One of the graduate students working with Ferrari, Özalp Babaoğlu, then set about this task. Along the way, Babaoğlu reached out to Joy for assistance. Joy helped integrate Babaoğlu’s memory system into 32V and helped him debug the resulting UNIX variant. The new version of UNIX was good. Joy knew that the 32 bit VAX running UNIX would render the 16 bit PDP-11 obsolete, and he started porting 2BSD to 32V. Peter Kessler and Marshall Kirk McKusick worked on porting Pascal, while Joy handled ex, vi, C shell, and many other BSD utilities. The group had completed their work by the end of 1979, and Joy shipped 3BSD in December. At this point, it’s important to delineate the family tree. 1BSD and 2BSD were improvements to UNIX version 6 while 3BSD was an improvement to 32V which was itself of a port and modification of UNIX version 7. During the creation of 3BSD, 2BSD had continued to see additions, fixes, and releases. One important distinction to be made is that the filename for the kernel in 3BSD became vmunix for virtual memory UNIX. Computer hardware and software were highly varied in 1979 and most often nothing was compatible with anything else. For the Defense Advanced Research Projects Agency, this was an issue. They felt that the best they could hope for was unity at the operating system level for the growing network of networks, ARPAnet, and to this end they chose to standardize on UNIX due to its proven portability resulting from having been written in C. This same system would also be used by DARPA for work in the VLSI Project. In the autumn of 1979, Fabry reached out to DARPA offering 3BSD as the solution to their problem. Initially, this wasn’t well received, but the success of 3BSD that December changed opinions. Fabry secured an eighteen month contract with DARPA that began in April of 1980. This contract stipulated that Berkeley’s new Computer Systems Research Group would add the features to 3BSD needed by DARPA which was left rather open. In a technical report on the matter, the government doesn’t actually list anything too specific. ARPA Standard UNIX Report 182KB ∙ PDF file Download ARPA Standard UNIX Report Download Fabry hired Laura Tong to be the project administrator, and Fabry then set about finding a tech lead. On an evening in early March, Joy rang Fabry at his home and expressed his desire to lead UNIX development, and Fabry agreed. Unlike earlier BSD releases (and on-going releases of 2BSD) a more robust system of distribution needed to be in place to handle a higher number of orders. Tong setup just such a system, and had Fabry coordinate with Bob Guffy at AT&T as well as the university’s lawyers to find license terms that would satisfy all parties. The system that Joy and the software team would create was 4BSD which was available in October of 1980. This release brought job control to the C shell initially developed by Jim Kulp and integrated by Joy, delivermail (predecessor of sendmail), job control signals that worked more reliably (so if you sent SIGHUP the job would actually hangup), the curses library, the control-z suspend/resume functionality we know today, a filesystem that supported block sizes up to 1K, and greater hardware support. Notably, 4BSD supported the VAX-11/750. Like prior BSD releases, 4BSD included the Pascal compiler and that saw still more improvements, and it included the Franz Lisp system by Richard Fateman. 4BSD saw nine months as the system de jour, and about one hundred fifty copies were shipped. Licenses were per institution and not per machine, and estimates are that the distribution was running on about five hundred computers. 4BSD drew criticism from David Kashtan at Stanford Research Institute. He’d run some benchmarks on VMS and BSD and he claimed that VMS was the clear winner. This didn’t sit well with Joy. He went and made a series of performance improvements to BSD (mostly in vmunix), and a few weeks later released a paper rebutting Kashtan’s claims and showing that BSD was every bit the match to VMS. The improved kernel was coupled with Kevin Robert Elz’s auto-configuration and a system featuring them was released as 4.1BSD in June of 1981, and 4.1BSD for VAX became 2.8BSD for the PDP-11.. This version was current for two years and shipped four hundred copies. This version was good enough to win CSRG another two years of funding from DARPA, and the CS department at Berkeley also saw funding. Interestingly, the reason for a point release was political. The CSRG had wanted to call this release 5BSD, but AT&T blocked that name. AT&T were releasing System V UNIX at the time, and they felt that a Berkeley release also named “five” would confuse customers. Point releases of 4.?BSD were then the agreed upon solution. The new funding round came with some more concrete goals. Specifically, DARPA wanted a better filesystem with higher throughput, support for multi-gigabyte adress spaces, better IPC, and integrated networking. Marshall Kirk McKusick outlines how these decisions were made: To assist in defining the new system, Duane Adams, Berkeley's contract monitor at DARPA, formed a group known as the \"steering committee\" to help guide the design work and ensure that the research community's needs were addressed. This committee met twice a year between April 1981 and June 1983. It included Bob Fabry, Bill Joy, and Sam Leffler of the University of California at Berkeley; Alan Nemeth and Rob Gurwitz of Bolt, Beranek, and Newman; Dennis Ritchie of Bell Laboratories; Keith Lantz of Stanford University; Rick Rashid of Carnegie-Mellon University; Bert Halstead of the Massachusetts Institute of Technology; Dan Lynch of The Information Sciences Institute; Duane Adams and Bob Baker of DARPA; and Jerry Popek of the University of California at Los Angeles. Beginning in 1984, these meetings were supplanted by workshops that were expanded to include many more people. What was developed largely by McKusick, Joy, Sam Leffler, and Rob Gurwitz is astounding. These developments were iterated in releases 4.1a, 4.1b, and 4.1c but ultimately culminated in 4.2 which was released in August of 1983. The two largest innovations were the Berkeley Sockets API and the Berkeley Fast File System. The sockets interface allowed multiple different network protocols to be used at any time and exposed them as files. The Berkeley FFS allowed blocksizes from 128 bytes to larger than 4096 bytes if needed. Thus, with Berkeley’s FFS, a system operator could optimize either for disk use or for disk performance. 4.2BSD included a full TCP/IP stack and NFS support as well. Starting in the early iterated releases, small tools like rcp, rsh, rlogin, and rwho appeared to demonstrate capabilities. These were intended to be short lived, but those familiar with UNIX will certainly recognize them. 4.2BSD also brought disk quota facilities, a better install process, better documentation, and some new filesystem related system calls. Events less visible to users also occurred between 1982 and 1983. Joy left the project in late spring of 1982 for Sun Microsystems, but still spent a little time that summer working on IPC and reorganizing the UNIX kernel sources to isolate machine dependent code. Once at Sun, contributions to BSD continued with Sun sending their modifications for running BSD on the Motorola 68000 back to Berkeley. Leffler then took over Joy’s previous duties. Joy’s work, however, would make BSD the single most easily ported operating system for about the next twenty years. Pauline Schwartz was hired to take over distribution duties in April of 1983. In June of 1983, Fabry went on sabbatical and Ferrari and Susan Graham took over running the CSRG. In 1984, Leffler went to work for Lucasfilm and Mike Karels took over as the UNIX dev lead. He’d previously worked on the 2.?BSD series for the PDP-11. Later that year McKusick joined CSRG full-time. 4.2BSD gained over a thousand site licenses in a year and a half, and most UNIX system vendors actually shipped 4.2BSD (or a system based on it) rather than AT&T’s System V UNIX. Likewise, many varieties of corporate UNIX would be based upon 4.1c or 4.2BSD such as SunOS and DEC Ultrix. An important point at this time is that BSD’s codebase was largely divergent from that of AT&T while still mostly maintaining compatibility. Additionally, AT&T had started shipping UNIX without source, and those customers who bought AT&T’s commercial UNIX would often then separately obtain BSD sources if they wanted to modify some aspect of the system. For those using microcomputers, Microsoft’s (and SCO’s) XENIX was the standard. Over on the PDP-11, a mixture of 4.1a and 4.1c formed 2.9BSD, but 4.2BSD never showed up. In 1976, comic artist Phil Foglio drew the first versions of Beastie as T-shirt art for Mike O’Brien as payment for unlocking a safe. T-shirt art of BSD by Phil Foglio Much of this is largely a pun on services being called daemons, and UNIX making heavy use of pipes, with /dev/null being a bit bucket. The daemon, Beastie, gained lasting association with BSD via a drawing by John Lasseter of Lucasfilm being used on the cover of the Unix System Manger’s Manual that was published in 1984 by USENIX for 4.2BSD. Cover of the manual, image from jacobelder.com While 4.2BSD was successful and mostly well received, it did get some complaints. The majority of the complaints were centered on performance. The team then spent two years improving performance, refining the networking stack, and they felt that they were ready to announce an impending release at USENIX in June of 1985. This didn’t go well. The fine folks of Bolt, Beranek, and Newman (part of the steering commitee) noted that 4.2BSD had shipped without the final version of the their networking code, and was instead using a heavily modified version of their initial prototype. After some bickering back and forth, DARPA provided both network stacks to Mike Muuss (author of ping) of the Ballistics Research Laboratory for testing. Berkeley’s code was better. 4.3BSD was released in June of 1986. Keith Bostic joined CSRG in 1986 with the condition that he be allowed to continue and to complete a prior project. He was working to port 4.3BSD to the PDP-11. This meant that a 250K minimal system on VAX would be made to fit in the 64K address space of the PDP-11… no one thought this would work. No one. The man was clearly mad, but despite what madness may have lurked within him, Bostic made it work using a set of overlays and auxiliary processor states, and this formed the 2.10BSD release. He also began attending USENIX and he’d announce the progress of the removal of all AT&T code from BSD which started in 1986 at thirty five percent AT&T license free, and the announcement was met with widespread cheers and applause. This became important due to price increases from AT&T. As of the 24th of February in 1984, the price for a commercial license of UNIX System V Release 2 with source for a single CPU stood at $43000 (about $126000 in 2024), and each additional CPU was a further $16000 (nearly $47000 in 2024). For educational institutions, the price was lower at $800 (or $2300 in 2024) and an additional $400 for each CPU. AT&T UNIX prices 1984 186KB ∙ PDF file Download AT&T UNIX prices 1984 Download Getting well into the 1980s, it became rather obvious to CSRG members that VAX wouldn’t go on forever. Computer Consoles Inc had opened a development center in Irvine and there they developed a proprietary minicomputer called the Power 6/32 code named Tahoe. This new platform was aimed to compete directly with VAX. In need of an operating system, CCI turned to CSRG. CCI provided the team several machines and the team set about porting 4.3BSD to the 6/32. This became 4.3BSD-Tahoe in June of 1988. Importantly, they were able to push forward Joy’s work of separating machine dependent code from the rest of the system, and 4.3BSD-Tahoe introduced an OSI network protocol stack, improved the kernel’s virtual memory system, and introduced more efficient TCP/IP support. Unfortunately, 4.3BSD-Tahoe was short lived as CCI changed their company’s focus, and Sperry and Burroughs released rebranded minicomputers based on the Power 6/32 platform. Where this effort did live on, however, was in a merger of 4.3BSD-Tahoe and 2.10BSD into 2.11BSD for the PDP-11. The Tahoe effort also meant that much of the BSD codebase had been rewritten furthering the aim of removing AT&T licensed code. BSD was released in source format only. Any prospective user would be required to compile his or her system from source entirely. Given this, any user would first need to acquire an AT&T source license before he/she would be able to make use of BSD. As noted previously, UNIX license fees were ridiculous. Yet, the TCP/IP stack in 4.3BSD was unique to Berkeley. Several software developers requested that BSD’s networking code be offered separately from UNIX, and this was done in June of 1989 as Networking Release 1. Pricing for a tape from Berkeley was $400 (around $983 in 2024), but the license terms allowed for free modification, free redistribution, and free application to any use case provided that the copyright notices on Berkeley’s code remain in place, and that products incorporating the code mention in documentation that code from the University of California and its contributors was included. It was also available via anonymous FTP shortly following the initial release. This newsgroup post was made on the 7th of December in 1988, but other documentation states public availability was November: Path: utzoo!utgpu!watmath!clyde!att!osu-cis!tut.cis.ohio-state.edu!mailrus! ames!pasteur!ucbvax!OKEEFFE.BERKELEY.EDU!bostic From: bos...@OKEEFFE.BERKELEY.EDU (Keith Bostic) Newsgroups: comp.bugs.4bsd.ucb-fixes Subject: V1.73 (BSD Networking Software, Release #1) Message-ID:Date: 7 Dec 88 01:54:54 GMT Sender: dae...@ucbvax.BERKELEY.EDU Organization: University of California at Berkeley Lines: 374 Approved: ucb-fi...@okeeffe.berkeley.edu We are happy to announce the availability of the first release of the BSD networking software. It consists of the standard user level applications, (along with their manual pages and some related documentation) and some kernel and C library support. It should be noted that this software has only been tested for compilation and operation on 4.3BSD and 4.3BSD-tahoe. A complete list of files is attached to this message. The TCP and IP code is approximately the same as that recently made available via the ARPANET and Usenet. Several new algorithms are used in TCP, in particular Van Jacobson's slow start and dynamic window size selection algorithms and Phil Karn's modification to the roundtrip timing algorithm. These changes increase throughput and reduce congestion and retransmission. Several fixes were made in the handling of IP options and other gateway support. This software suite is copyright The Regents of the University of California and may be freely redistributed. No previous license, either AT&T or Berkeley is required. The release costs $400.00 US. To request an order form, please contact our distribution office by phone at 415-642-7780, or by email at bsd-d...@ucbarpa.berkeley.edu or uunet!ucbarpa!bsd-dist, or by U.S. Mail at:CSRG, Computer Science DivisionUniversity of CaliforniaBerkeley, CA 94720 Mike Karels Kirk McKusick Rather than including the file list, I will simply note that this release was very complete. It included arp, ftp clients and servers (yes, plural on both), route, telnet, dns tools, ifconfig, inetd, pieces of the BSD libc, sendmail, syslog, ping, and uucp. While the networking release was being made, 4.3BSD development continued. A new virtual memory system was implemented from MACH at Carnegie-Melon with the porting and merging work done by Mike Hibler. The interface for that VM system was, however, a purely Berkeley design adhering to the architecture descriptions found in 4.2BSD. The NFS system was upgraded to be Sun-compatible via the use of Rick Macklem’s work at the University of Geulph. This became 4.3BSD-Reno. The original BSD license in Reno: Copyright (c) 1987 Regents of the University of California. All rights reserved. Redistribution and use in source and binary forms are permitted provided that this notice is preserved and that due credit is given to the University of California at Berkeley. The name of the University may not be used to endorse or promote products derived from this software without specific written prior permission. This software is provided ``as is'' without express or implied warranty. The license of Tahoe read: Copyright (c) 1987 Regents of the University of California. All rights reserved. Redistribution and use in source and binary forms are permitted provided that the above copyright notice and this paragraph are duplicated in all such forms and that any documentation, advertising materials, and other materials related to such distribution and use acknowledge that the software was developed by the University of California, Berkeley. The name of the University may not be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED ``AS IS'' AND WITHOUT ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, WITHOUT LIMITATION, THE IMPLIED WARRANTIES OF MERCHANTIBILITY AND FITNESS FOR A PARTICULAR PURPOSE. Keith Bostic was still as avid as ever to rid BSD of AT&T code and free it from the heavy costs that came with that code. At a meeting of CSRG, he mentioned the popularity of the networking release and he proposed an expanded release that would include more than networking. Discussion went on for a while, but eventually McKusick and Karels took up the kernel work with Bostic handling the utilities and C library. Obviously, this would be a seriously large undertaking for a single individual, and Bostic figured that others would be willing to help. Bostic then encouraged people to coordinate over the budding Internet with people across the globe submitting contributions. He also encouraged people to contribute at USENIX. In a little over a year, most of the utilities and libraries had been rewritten with major contributions coming from Bill Jolitz, Donn Seeley, Trent Hein, Vadim Antonov, Mike Karels, Igor Belchinsky, Pace Willisson, Jeff Polk, and Tony Sanders. Karels and McKusick hadn’t actually expected Bostic to succeed, but with the work completed, Bostic walked into their office with his head held high and inquired as to their progress on the kernel. The two then went off to work on it file by file, removing everything originally included in the 32V release, but they were short by six files. Rather than getting a new license and name created by the university’s lawyers, the group reused what they had and this became Network Release 2 announced on the 3rd of July in 1991. Path: gmdzi!unido!fauern!ira.uka.de!sol.ctr.columbia.edu!spool.mu.edu! caen!zaphod.mps.ohio-state.edu!cis.ohio-state.edu!ucbvax!OKEEFFE.BERKELEY.EDU!bostic From: bos...@OKEEFFE.BERKELEY.EDU (Keith Bostic) Newsgroups: comp.bugs.4bsd.ucb-fixes Subject: V1.95 (BSD Networking Software, Release #2) Message-ID:Date: 3 Jul 91 23:14:59 GMT Sender: dae...@ucbvax.BERKELEY.EDU Organization: University of California at Berkeley Lines: 36 Approved: ucb-fi...@okeeffe.berkeley.edu We are happy to announce the availability of the second release of the BSD networking software. The distribution includes approximately 75% of the utilities distributed as part of 4.3BSD-Reno and the C library (along with manual pages and some related documentation), and much of the kernel. We wish to *strongly* emphasize, however, that significant portions of the kernel are missing and that no binary support is supplied for any architecture. Please note also that this software has only been tested for compilation and operation on 4.3BSD-Reno. This release is intended for system developers and others who wish to preview or experiment with the most recent Berkeley system. It may also be useful as an update to earlier BSD or BSD-derived systems, although substantial work will be required to integrate portions of this release into older systems. This distribution is *not* intended to be used on production systems, nor is it intended for sites without the expertise to find and fix problems that are encountered. This software suite is Copyright (C) 1991 The Regents of the University of California and may be freely redistributed without further charge. No previous license, either from AT&T or Berkeley is required. The release costs $850.00 US on 6250 BPI 9-track magnetic tape or 8mm Exabyte cassette or $950.00 US on 1600 BPI 9-track magnetic tape. The distribution is approximately 90Mb in size. To request an order form, please contact our distribution office by phone at 415-642-7780, or by sending email to bsd-d...@ucbarpa.berkeley.edu or uunet!ucbarpa!bsd-dist, or by U.S. Mail at:CSRG, Computer Science DivisionUniversity of CaliforniaBerkeley, CA 94720 Mike Karels Kirk McKusick Keith Bostic Keith Sklower Marc Teitelbaum Less than a year later, Bill Jolitz replaced the missing kernel pieces and compiled everything for the PC-compatible Intel 80386 platform, and he released this new distribution as 386/BSD. Only a few short months later, a group of 386/BSD users formed NetBSD to continue enhancements and releases of Jolitz’s system. This occurred because Jolitz still had a full-time job, and the number of bug fixes, enhancements, and additions coming into his project were beyond his capabilities to manage alone. NetBSD has and continues to emphasize porting NetBSD to absolutely everything, including but not limited to ARM, MIPS, i386, AMD64, SPARC, PowerPC, Motorola 68K, SH3, HPPA, Itanium, RISC-V, and VAX. If your toaster has a chip in it, NetBSD will probably run on it. NetBSD has also engaged in some research and experimentation that has led to many security and system hardening technologies, an amazing package management system, kernel level hardware virtualization, and some surprising levels of backward compatibility. As is the BSD tradition, NetBSD has also merged in changes from systems like Solaris with ZFS. A toaster at the Linux World Expo in 2005, running NetBSD While NET2 was being worked on and released, the last version of 2.11BSD was also being prepared and released, but this time by USENIX. On the 14th of March in 1991, Steven M. Schultz posted the following to comp.sys.dec.micro: Second Distribution of Berkeley PDP-11 Software for UNIX Release 2.11 (Revised January 1991) The USENIX Association is pleased to announce the distribution of a new release of the \"Second Berkeley Software Distribution\" (2.11BSD). This release will be handled by USENIX, and is available to all V7, System III, System V, and 2.9BSD licensees. The Association will continue to maintain the non-profit price of $200. The release will consist of two 2400 ft. 1600 bpi tapes or one TK50 tape cartridge (approximately 80M) and approximately 100 pages of documentation. If you have questions about the distribution of the release, or require 800 bpi tapes, please contact USENIX. USENIX's address and phone number is as follows: 2.11BSD USENIX Association 2560 Ninth St. Suite 215 Berkeley, CA 94710 +1-415-528-8649 USENIX may also be contacted by electronic mail at: {ucbvax,decvax}!usenix!office If you have technical questions about the release, please contact Steven M. Schultz at: s...@wlv.imsd.contel.com wlbr!wlv!sms This release is in celebration of the 20th anniversary of the PDP-11! Work has been ongoing since the release of 2.10.1BSD in January 1989. This release incorporates all fixes and changes posted to the USENET newsgroup comp.bugs.2bsd since 2.10.1BSD was released. Present in this release are several more missing pieces from the 4.3BSD distribution: 1) the kernel logger (/dev/klog) 2) the namei cache and argument encapsulation calling sequence 3) readv(2)/writev(2) as system calls rather thane mulation/compatibility routines 4) shadow password file implementation (the May 1989 4.3BSD update) 5) a TMSCP (TK50/TU81) driver with standalone support (boot-block and standalone driver) 6) Pronet and LH/DH IMP networking support 7) the portable ascii archive file format (ar, ranlib) 8) the Unibus Mapping Register (UMR) handling of the network was rewritten to avoid allocating excessive UMRs. 9) the necessary mods to the IP portion of the networking were made to allow traceroute (which is present in 2.11BSD) to run 10) long filenames in the file system This last addition is the reason a coldstart kit is necessary. The 4.3BSD on-disk directory structure has been ported (along with the utilities that know about on-disk directories via the raw filesystem: fsck, ncheck, icheck, dcheck, etc.) and is not compatible with previous versions of UNIX for the PDP-11. A limited amount of filesystem backward compatibility with earlier versions of 2BSD (2.9BSD, 2.10BSD and 2.10.1BSD) is present in a version of dump(8) which can read old filesystems. The disk partition sizes have not changed from 2.10.1BSD (the urge to standardize the haphazard partition sizes was suppressed in the interest of backwards compatibility). The restor(8) utility automatically converts old dump tapes to the new format on input. The constant MAXNAMLEN is now 63 instead of 14. While it is possible the limit could be higher, with MAXPATHLEN at 256 a MAXNAMLEN of 63 was judged sufficient. MANY other fixes and changes have also been made, see the \"Changes To The Kernel\" document which describes the changes made to both the kernel and the application programs. Steven M. Schultz Contel Federal Systems 31717 La Tienda Drive Westlake Village CA 91359 s...@wlv.imsd.contel.com wlbr!wlv!sms A few months after the NetBSD group formed, the FreeBSD group formed. They chose to target the PC architecture specifically and to make their system a bit easier to use. In contrast to NetBSD which was solely a product offered over the internet, the FreeBSD group offered their product on CD-ROM. FreeBSD did expand to ARM, PowerPC, and MIPS, and like NetBSD they did continue the CSRG spirit of research and experimentation with things like bhyve virtualization and jails. Additionally, they have continued the legacy of borrowing the best bits of other systems like dtrace, and ZFS, but also via their immense collection of software in ports. Richard L. Adams Jr (Rick) was the founder of one of the earliest internet service providers (if not the first), UUNET, after first creating SLIP (TCP/IP over serial). He also maintained the most popular usenet transport in the early 1980s, B News. After the release of 386/BSD, he got together with Keith Bostic, Marshall Kirk McKusick, Mike Karels, and Bill Jolitz and the group founded Berkeley Software Design, Inc otherwise known as BSDi. Their first product was BSD/386 based off of Networking Release 2 and it first released in January of 1992. Their system retailed for $995 (about $2160 in 2024). The target market for BSDi was the nascent internet infrastructure market. In their advertising they touted their “99% discount over System V” and advised interested parties to contact 1-800-its-unix. Of course, this didn’t sit well with Unix System Labs (AT&T subsidiary). Shortly after BSDi began sales, they received a cease and desist from USL with a particular request to stop using the phone number that included “it’s UNIX,” as the ownership of the UNIX trademark was firmly in USL’s hands. BSDi complied changing their advertisements, the number, and explaining that BSD wasn’t precisely UNIX. This, however, wasn’t quite enough. USL brought a lawsuit against BSDi seeking an injunction against the sale of BSD/386. As part of the suit, USL claimed that BSDi’s product contained USL code and trade secrets, and that further sales of BSDi’s product would irreparably harm USL. BSDi then claimed that they shouldn’t be held liable for any code contained in Berkeley’s original source offerings, but that they were completely willing to discuss the six added, BSDi-original files. BSDi’s argument won and USL was required to restate their complaint or have the case dismissed. USL then filed suit against both BSDi and the University of California with roughly the same complaints but this time seeking an injunction against the sale and distribution of both Networking Release 2 and BSD/386. The employees of both CSRG and BSDi were deposed. In December of 1992, US District Judge Debevoise in New Jersey took the arguments for injunction under advisement. Six weeks later, he dismissed all but two of the complaints and suggested that the matter should be taken to a state court before being heard in a federal court. The University listened and filed a lawsuit against USL the following Monday in California. The suit by the University of California claimed that USL had failed to provide credit to the University for BSD code contained within System V as required by their prior licensing agreement. The University of California wasn’t asking for financial compensation but rather that USL reprint all of their System V documentation with proper credit and attribution for BSD code, that USL notify licensees of their mistake, and that USL run advertisements in print news publications informing the public of USL’s mistake. On the 21st of December in 1992, Novell announced that it would be acquiring Unix System Laboratories including the UNIX copyright, trademarks, and licensing contracts. The LA Times stated that this transaction was completed with an exchange of stock wherein all of the shares of USL would be traded for twelve million three hundred thousand shares of Novell, and after which USL would be a wholly owned subsidiary of Novell. Discussions around a settlement of legal proceedings began in the summer of 1993, but were not resolved until January of 1994. The result was that three files were removed from Networking Release 2, minor changes were made to a few other files, and USL copyrights were added to about seventy files though the license of those files would continue to be the BSD license. On the part of USL, they agreed that with those terms met, the company would not bring litigation against any company, group, or user who used or distributed the resulting open source BSD system. That resulting system was 4.4BSD-Lite and 4.4BSD-Encumbered in March of 1994. 4.4BSD-Encumbered included the prior USL code and required a USL UNIX license. 4.4BSD-Lite was fully free and open source software. This version was merged into FreeBSD 2.0, NetBSD 1.0, and a newly named BSD/OS which was previously BSD/386. For CSRG, the revenue received from 4.4BSD allowed the group to continue as a part-time effort integrating bug fixes and enhancements made locally and from the BSD community. As focus shifted into the other distributions like BSD/OS, NetBSD, and FreeBSD the rate of submissions to Berkeley slowed. The final release from the University was 4.4BSD-Lite Release 2 in June of 1995. This version was merged into FreeBSD 3.0, Rhapsody (and later Darwin), NetBSD 1.3, OpenBSD 2.3, and BSD/OS 3.0. By 1995, BSDi’s BSD/OS was the most common operating system used in the datacenters of the early internet. This is a market that BSDi had intentionally targeted, and this would prove to be a sword of Damocles. The strand of hair holding the sword broke as the Dot Com Bubble began bursting and BSDi merged with Walnut Creek which itself was later sold to Wind River Systems. Today, BSD is among the most common operating systems on Earth despite people not recognizing it as such. Modern macOS descends from 4.4BSD, as do NetBSD, OpenBSD, FreeBSD, Dragonfly BSD, and a few others. In the past, various versions of BSD formed the basis of SunOS, DYNIX, NeXTSTEP, Ultrix, and Tru64. Various editions of BSD were merged into AT&T UNIX, and this means that modern commercial UNIX systems like AIX and HP-UX are also running some BSD code. BSD has been used in many commercial products owning to its permissive licensing, and some of the most notable of those are the Sony Playstation 3, 4, and Vita, the Netflix Open Connect Appliance, Juniper routers, Isilon IQ clustered storage systems, Dell Compellent storage systems, and the Weather Channel’s IntelliStar forecast computer. I now have readers from many of the companies whose history I cover, and many of you were present for time periods I cover. A few of you are mentioned by name in my articles. All corrections to the record are welcome; feel free to leave a comment. Subscribe to Abort Retry Fail By Bradford Morgan White In Abort Retry Fail, I am attempting to chronicle the history of the computing industry. This is a goal that will likely not be completed within my lifetime. Subscribe Error 14 Share this post The Berkeley Software Distribution www.abortretry.fail Copy link Facebook Email Note Other 4 Share",
    "commentLink": "https://news.ycombinator.com/item?id=39630457",
    "commentBody": "The Berkeley Software Distribution (abortretry.fail)179 points by rbanffy 17 hours agohidepastfavorite59 comments Animats 10 hours agoFord Aerospace was one of the first commercial sites of BSD Unix. The licensing was complicated. We had to buy Unix 32V from AT&T first. That transaction got on the path for major corporate documents. AT&T and Ford Motor had a cross-licensing agreement. Eventually, I got a no-cost license agreement embossed with the corporate seals of both the Ford Motor Company and the American Telephone and Telegraph Corporation. Made a copy and taped it onto a VAX. Then I drove up to Berkeley from Palo Alto and Bill Joy gave me a BSD tape. BSD didn't have networking at that point. We bought 3COM's UNET.[1] That was TCP/IP, written by Greg Shaw. $7,300 for a first CPU. $4,300 for each additional CPU. It didn't use \"sockets\"; you opened a connection by opening a pseudo-device. UNET itself was in user space, talking to the other end of those pseudo-devices. Once we got that going, we had it on VAX machines, some PDP-11 machines, and some Zilog Z8000 machines. (The Zilog Z8000 was roughly similar to a PDP-11) All of which, along with some other weird machines including a Symbolics LISP machine, eventually interoperated. We had some of Dave Mills' Fuzzballs as routers [2], and a long-haul link to another Ford location that connected to the ARPANET. Links included 10Mb/s Ethernet, a DEC device called a DMC that used triaxial coax cables, and serial lines running SLIP. A dedicated 9600 baud serial synchronous line to Detroit was a big expense. My work in congestion control came from making all this play well together. These early TCP/IP implementations did not play well with others. Network interoperability is assumed now, but it was a new, strange idea back then, in an era when each major computer maker had their own networking protocols. UNET as delivered was intended to talk only to other UNET nodes. I had to write UDP and ICMP, and do a major rewrite on TCP. When BSD got networking, it was initially intended to talk only over Ethernet, to other BSD implementations. When 4.3BSD came out, it would only talk to some other implementations during alternate 4 hour intervals. I had to fix the sequence number arithmetic, which wrapped incorrectly. And finally, it all worked. For a few years, it was said of the TCP/IP Internet that it took \"too many PhDs per packet.\" One day, on the Stanford campus, I saw a big guy with a tool belt carrying an Ethernet bridge (a sizable box in those days) under his arm, and thought, this is finally a working technology. [1] https://archive.org/details/bitsavers_3Com3ComUN_1019199/pag... [2] https://eecs.engin.umich.edu/stories/remembering-alum-david-... reply tingletech 17 hours agoprev> The first copy we know to make it out of Berkeley was to Tom Ferrin at UCSF on the 9th of March in 1978. The license was signed on the 13th, the media was an 800 bpi tape, and on the tape was the “Unix Pascal system” and the “Ex text editor.” UCSF and UCB (as well as all the UC campuses and enterprises) are technically the same legal entity, incorporated by article 9 section 9 of the California constitution. Strange that they signed a license, when I worked for the regents we called them MOUs if it was a \"contract\" with another part of ourselves. When I started working for UCSD and found out we were licensing BSDi for some boxes I was sort of confused, like why don't we get that for free? reply dekhn 11 hours agoparentTom was my manager for a while when I was a programmer at UCSF (~20 years after this event). He ran the Computer Graphics Lab which had a bunch of SGIs and DEC Alpha servers to do molecular modelling and I was 'the first linux guy in the group' (they asked me why I used linux when bsd existed). I remember him saying that whenever he licensed a commercial UNIX he insisted the contract include a section giving him access to the source code for the kernel (\"because you never know when you will have to recompile the kernel or debug a kernel-space problem\"). He's also known for presenting at USENIX on how to enable a hardware-disabled (but physically present) instruction on a cheaper version of the PDP/11. You'd cut a trace in the microcode and add a jumper wire, and presto! you had a fast floating point instruction that was previously disabled. reply flyinghamster 5 hours agoparentprevI always thought there was quite a contrast between UCB giving away BSD versus UCSD making its p-System a commercial product. In the short term, it was probably good for UCSD, but in the long term, the p-System is all but forgotten while BSD trucks along. reply pjmlp 2 hours agorootparentWe get to have its influence on Smalltalk, JVM, MSIL, DEX, and all those that think WASM is the first of its kind. reply alberth 14 hours agoparentprev> MOUs MOU = Memo of Understanding? (Which are typically non-binding but formalized acknowledgement of intent) reply tingletech 14 hours agorootparentyes, we were told we could not contract with ourselves, so we had to do memorandums of understanding when we had revenue or other agreements between units. We also had a policy to follow state law (which does not apply to UC unless UC is specifically mentioned in the law). reply actionfromafar 13 hours agorootparentIs UC federal land or something?! reply fredoralive 13 hours agorootparentAs noted above, the University of California is given special status by a section of California constitution, so it has special status through that. It's presumably clauses like that fact it's governed by the Regents \"...subject only to such legislative control as may be necessary to insure the security of its funds and compliance with the terms of the endowments of the university...\" that mean it isn't normally affected by new laws. (https://leginfo.legislature.ca.gov/faces/codes_displaySectio...) reply ihigxigcogc 10 hours agorootparentAll that said, parts of UC are indeed on federal land. Berkeley Lab, for instance. reply Cheer2171 9 hours agorootparentBerkeley Lab / LBNL is on land owned by the UC, but all the buildings are owned by the feds. Livermore / LLNL is fully owned by the feds. reply retrac 16 hours agoprevThere's an alternate universe, almost identical to our own, where BSD 4.4 was released slightly earlier, its legal status was not ambiguous, it got ported to the 386 in '90, Linus never bothered to write Linux, and every machine from smartphone to server runs some version of BSD. reply microtherion 14 hours agoparentIn fact, one of the two major smartphone platforms IS running some version of BSD. reply dboreham 13 hours agorootparentNot really. See: https://en.wikipedia.org/wiki/XNU and https://github.com/apple-oss-distributions/xnu reply microtherion 13 hours agorootparentYeah, I know, it's a somewhat complicated story. But even the kernel has, as the Wikipedia article confirms, considerable BSD heritage, and the userland is even more BSD flavored. I would think that the vast majority of Mac / iOS programmers has never made a direct Mach system call in their code. At the very least, you can say that OS X is more BSD than Linux is GNU. reply pjmlp 13 hours agorootparentThe vast majority also doesn't do POSIX stuff, rather Objective-C and Swift framework calls. The UNIX compatibility on NeXTSTEP, and follow up culture on OS X, was that BSD layer was to bring UNIX stuff into the system and get those DoD contracts, not much so for the software born on the platform. Even NeXTSTEP drivers were Objective-C. reply bch 13 hours agorootparentprevWow! Amazed to see Apple is still committing to the repo (albeit a huge code dump, latest of which was ~5 months ago)! reply ManuelKiessling 12 hours agorootparentWell, they don’t really have a choice, do they? My uninformed understanding is: Darwin is Open Source, so when they improve/extend/modify it for their own needs (Darwin still is the foundation for macOS, iOS etc.), they must make those changes available. reply yjftsjthsd-h 11 hours agorootparentNo; open source encompasses copyleft, which broadly requires derivative works to be under the same or similar license, and permissive licenses, which generally don't care and are happy to let you incorporate the code in differently licensed derivatives up to and including producing completely proprietary programs using permissively licensed open source code. Darwin largely uses permissive license code, in particular from the BSDs, which means that it's under very little obligation to share. (IANAL and this is somewhat oversimplified, but that's the gist) reply numpad0 7 hours agorootparentprevActually, you don't have to. For GPL software you must provide source code for the binary, and can't put it under NDA, that is many fails to comply with. But you don't have to upload the files onto Internet or anything. This is about BSD so it's even less than that, you only have to display correct attributions. You can refuse to distribute binary and/or source code, or your private copy thereof, entirely, with FOSS. You don't have to send your fork to non-users randomly asking for it, even with AGPLv3. It's just a totally fine and also easy conformance that is slightly beneficial to all. reply Alupis 10 hours agorootparentprevHow exactly would someone prove they improved/extended/modified the source? These sort of copy-left licenses rely an awful lot on good faith, and that does not contend for human nature. Even if someone could prove it - good luck legally compelling Apple to do anything. reply samatman 8 hours agorootparent> How exactly would someone prove they improved/extended/modified the source? Disassembly would work. > These sort of copy-left licenses rely an awful lot on good faith, and that does not contend for human nature. This is not a copyleft license. Also, copyleft licenses rely on lawsuit, more than good faith. > Even if someone could prove it - good luck legally compelling Apple to do anything. If Apple gets sued, they do have to show up in court. If they lose, they must pay damages. If they're injuncted, and do not comply with the injunction, the courts can take any number of actions against them in response. I don't sit on Apple's board, but I can't imagine them preferring an arbitrarily-large fine over releasing GPL-licensed code which Apple modified without complying with the license. reply wbl 11 hours agorootparentprevNot the kernel: that's BSD licensed reply lproven 16 hours agoparentprevIt may have come closer than you realise. « My first choice was to take the BSD 4.4-Lite release and make a kernel. I knew the code, I knew how to do it. It is now perfectly obvious to me that this would have succeeded splendidly and the world would be a very different place today. RMS wanted to work together with people from Berkeley on such an effort. Some of them were interested, but some seem to have been deliberately dragging their feet: and the reason now seems to be that they had the goal of spinning off BSDI. A GNU based on 4.4-Lite would undercut BSDI. So RMS said to himself, \"Mach is a working kernel, 4.4-Lite is only partial, we will go with Mach.\" It was a decision which I strongly opposed. But ultimately it was not my decision to make, and I made the best go I could at working with Mach and doing something new from that standpoint. This was all way before Linux; we're talking 1991 or so. » http://www.groklaw.net/article.php?story=20050727225542530 reply jeff_vader 16 hours agoparentprevAnd we're all happy CVS users. reply yjftsjthsd-h 15 hours agorootparentMy understanding is that git and mercurial were created independently around the same time, so in a world where Torvalds didn't create git we'd probably all use hg. Although, it isn't super hard to imagine him still working in system software and eventually creating git under almost the same circumstances, just that it would be used on a BSD rather than Linux. reply walteweiss 12 hours agorootparentI studied CS in the university and there was that guy who was a computer geek (out of very little among my fellow students), but actually was just your average gamer with close to zero knowledge of anything computer beyond double-clicking the shortcut to CS / Half-Life / WarCraft / Dota / whatever else was popular back then. I met him 15 years later. We had some casual chat about this and that. And at some point he said— Well, I have to give you a bit more of the context. He was a junior front-end (React, ofc) developer at that point. Yes, 15 years later! He just arrived into the profession due to his wild incompetence. Although he sold himself as a senior to the company! Still, he was very incompetent, and I had those wild eyes of wondering how on Earth some company would even hire such a guy, not to say giving him a senior role! He was talking about this front-end thing. That there’s that other OS, Ubuntu, and he thinks he’ll try it instead of Windows. As drumroll for some reason (for some reason!) this git thing works better on Ubuntu than it works on Windows. My first try of Linux was like 15 to 20 years before the talk, so I was like spilling my coffee on him with laughing. And here is your comment, casually pretending everyone on HN is aware of that history of git. And that it’s that git guy is somehow related to this Ubuntu thing, you know. reply jmcqk6 16 hours agorootparentprevI'm not an expert, but I'm pretty sure there is no universe where that is true. reply drewg123 15 hours agorootparentWe'd all be happy perforce users :) (For a long time, before git and subversion, even though the official freebsd repo was in CVS, a lot of development happened in perforce) reply bch 13 hours agorootparent> a lot of development happened in perforce Interesting - that I didn’t know. Based on my personal p4 experience, I’m sorry anybody else had to use it. That said - nobody has mentioned that git (and hg) was preceded and directly inspired by Larry McVoys BitKeeper[0]. Indeed, I’ve heard rumor that both “git” and “mercurial” were named after people’s perceptions of others personalities during the Linux/BitKeeper breakup/debacle. [0] https://en.wikipedia.org/wiki/BitKeeper reply drewg123 11 hours agorootparentIt was a lot better than the alternatives at the time, which were CVS and SCCS.. :) reply bhasi 12 hours agorootparentprevStill is, in some hardware companies. reply arp242 16 hours agoparentprevThat BSD would most likely be similar to what Linux is today. There's this idea that this alternative universe would have one of the BSD variants as they exist today be dominant, but colour me skeptical on that. RedHat would have been a \"BSD company\", systemd would have been a \"BSD thing\", Hans Reiser would still have written ReiserFS, but for BSD, Steve Balmer would have ranted that \"BSD is cancer\", etc. There would be some technical differences of course, but I suspect many of them would be relatively small and not all that interesting. So basically it would have been \"Linux, but with fewer Penguins and more Satanism\". reply wbl 11 hours agorootparentNo there would have been some big differences. The first is that threads would have a kernel representation unlike in Linux where the kernel just knows about processes. This mistake caries over to tools like strace that grab one thread by default because they don't know the others. Secondly with integrated user space and kernel packaging and distribution would not be a necessity. That would change a lot of things. Microsoft had taken the TCP implementation from BSD for Windows as shown in the copyright notice. The relationship would be different. We would not have alternate libcs. Containerization would be built around slightly more heavyweight jails which look like real systems as more software would be assuming that environment. With systemd there is a real need, but I would hope the solution is better. reply arp242 10 hours agorootparentYou're assuming that BSD systems would have evolved the same way over the last 35 years. I don't think that's a given. People like to tinker and would have created alternative ${anything}s for BSD systems. And even with a bundled userspace and kernel you still have distros; e.g. PC-BSD did the whole \"fat packages\" thing 20 years ago, and there have been tons of \"distros\" based on various BSD systems. Jails weren't created until around the late 90s (1998 or 1999 IIRC), and who knows what that would have looked like if (Free)BSD had massively taken off in the same way Linux did, and who knows if \"FreeBSD cgroups\" wouldn't be a thing in addition to jails? Or: all the people that worked on those Linux things wouldn't have worked in a completely different field: they would have worked on BSD, presumably with (roughly) the same interests, and (roughly) the same opinions. Of course it wouldn't be exactly the same, but roughly, from a high level? It would probably be very similar. reply Gud 2 hours agorootparentFrom a high enough level, FreeBSD and GNU/Linux already are very similar? reply yjftsjthsd-h 15 hours agorootparentprevThe key difference would be if they kept permissive licensing or if there would be copyleft BSD derivatives. For example, without GPL forcing the kernel to stay open source, Android phones would be a lot harder to modify reply arp242 13 hours agorootparentMost Android phones are already so locked down and partly proprietary that it's a right pain to meaningfully modify anything without jumping through tons of hoops. In short, I don't think it would have made much of a difference. \"GPL vs BSD\" is an old debate that's been done a million times, and I never seen any convincing argument one way or the other. Plus there's long been copyleft (GPL, CDDL) parts to many BSD systems, both in userspace and kernel. reply kmeisthax 15 hours agorootparentprevTo be clear, the only GPL source in Android is the kernel. Google took great pains[0] to avoid shipping copyleft code. This is why they were able to make most of Honeycomb proprietary[1]. They can't withhold source on the kernel, but most of what makes Android Android isn't part of Linux and thus the GPL does not apply[2]. From https://source.android.com/docs/setup/contribute/licenses: > For userspace (nonkernel) software, Google prefer Apache 2.0 (and similar licenses such as BSD and MIT) over other licenses such as the GNU Lesser General Public License (LGPL). Android is already permissively licensed and that's half of why Google can legally lock things down[3] so easily. The other half being that Google requires a CLA from outside contributors, which is just plain silly for a permissively licensed project. Then again, GNU did the same thing at one point. In the alternate world where BSD was unambiguously legal and RMS was the Ted Nelson[4] of Free Software, I don't think they would have independently invented copyleft. A lot of the BSD people had very reactionary opinions to the concept, and while some of it might be motivated by their personal disdain for RMS, it's hard to separate RMS and copyleft. Given all that, a legally unambiguous BSD almost certainly would have wound up being eaten by Microsoft rather than AT&T and the UNIX Wars. All the anger from early 2000s Microsoft about Linux was solely because the GPL prevented them from just writing a better version of Linux or GNU that other people couldn't have. Microsoft was very good at taking open standards and extending them with the goal of making their implementation replace the standard. The GPL copyleft was the perfect thorn in the foot of a Microsoft that liked to cheat the norms of standards bodies. [0] At least on early versions of Android, this went as far as changing all the system paths. GNU and Android can technically coexist under the same Android/Linux kernel. [1] Even the FOSS release of Ice Cream Sandwich deliberately omitted all the Honeycomb-related tags to frustrate attempts to use Honeycomb under the FOSS terms of the later AOSP release. [2] Linux is commonly stated to be GPLv2, but it additionally has an exception stating that user-mode code never trips the GPL copyleft, as a \"just in case\" sort of thing. This is the reason why Nvidia is allowed to have kernel modules, and also the reason why Linus explicitly refused GPLv3. The v3 anti-TiVo clause forbids you from shipping an application on the same consumer electronics system as GPL software, if the application refuses to run if the GPL software is modified. This is perfectly reasonable for GNU but flies in the face of the whole \"user mode APIs are not copylefted\" thing. [3] Android ain't done 'till Termux won't run. [4] https://en.wikipedia.org/wiki/Project_Xanadu reply samatman 8 hours agorootparent> Given all that, a legally unambiguous BSD almost certainly would have wound up being eaten by Microsoft rather than AT&T and the UNIX Wars. All the anger from early 2000s Microsoft about Linux was solely because the GPL prevented them from just writing a better version of Linux or GNU that other people couldn't have. Microsoft was very good at taking open standards and extending them with the goal of making their implementation replace the standard. The GPL copyleft was the perfect thorn in the foot of a Microsoft that liked to cheat the norms of standards bodies. This is a strange counterfactual to propose, given that Microsoft at one time sold the most installed Unix on the market: Xenix. Which it later abandoned. Given that they were licensing System 7 from AT&T, why would a tweak to BSD licensing result in them \"eating\" it? reply pjmlp 2 hours agorootparentSometimes I wonder how much they repent themselves of having sold Xenix, and later on not holding on to a better POSIX support on NT, having to ship a Linux VM in the end. I for one only bothered with Linux initially after having learned UNIX on Xenix at the technical school, and using DG/UX on university campus, because POSIX support on Windows NT was so flanky, and later on it was more practical to deal with dual boot than SUA. reply actionfromafar 13 hours agorootparentprevI feel a bit of history is slipping away. Linux forced a lot of companies and individuals to keep stuff together in a way that uniquely unified industry against Windows and custom embedded software stacks. Without the Linux kernel we’d have a bunch of competing proprietary forks bickering at every turn. You could have forgotten video drivers from major vendors like NVidia. Even with Linux it was an anomaly. reply pjmlp 2 hours agorootparentInstead of UNIX wars, we now have Linux wars. reply bluGill 12 hours agorootparentprevHuh? We have many competing forks - debian, redhat, ubutntu, arch... Sure there is mostly on kernel, but there is a lot more to an OS than a kernel. Linux didn't force anyone to keep stuff together. It just is to your advantage to work together most of the time. Anyone who didn't work together soon paid a price as something you want couldn't easily be brought to you. reply actionfromafar 10 hours agorootparentI may not make my point clearly - but I have seen from the inside how companies reluctantly use the Linux kernel because it's the industry standard and the board support package comes with Linux. If BSDs had won, there would have been very little incentive to upstream patches or provide source code to customers, but a lot of incentive to pitch the vendors proprietary additions as something special. Every company selling a solution would have been tempted to close off the source and not give anything back. Back then, it was a knee-jerk reaction to keep as much as possible in-house. Everything begins with hardware support. The standard distributions also all used glibc and GNU utilities which is also GPL. For the longest time all standard distributions were based on either RedHat or Debian. I think Linux shaped the industry in very special ways which would not have come naturally in a non-GPL world. Things today are by-and-large extremely open, with github etc being almost a force of nature, but my hunch is that this outcome was not a given which would have just have happened in some kind of manifest-destiny way. I think the world could easily have tipped into a much more proprieraty path without GNU and Linux. reply arp242 9 hours agorootparentIt seems to me the incentive is mostly that it costs tons of effort to maintain these things in-house, and that it doesn't actually hurt the business to share (most) things. Does copyleft help? Perhaps, but that's not clear to me. There are tons of successful non-copyleft projects so copyleft certainly doesn't seem to be a requirement, and look at how many companies are violating the GPL and basically just keep doing it. While GPL suits do happen (just last week in France), in general they're exceedingly rare, and the risk consists of a bit of negative publicity among a small number of people. All of this is of course an old discussion, and to be fair I don't think anyone can be sure of anything here which is why people have been discussing this for 30 years. Basically we'd have to construct an alternative universe to be sure. reply freeremote 11 hours agorootparentprevI was a BSD admin for 4 years back in the early 00s after running Sun Solaris DNS servers for a well known domain registrar. Ran Web servers, and later firewalls using FreeBSD. While I really like BSD for a server, I've had terrible times with it on laptops. I've tried them all: Free, Net, Open, Ghost, and others that are no longer being developed. I've standardized on Debian over the years because it just works. I buy refurb Lenovo laptops from eBay for about $120. I would like to see BSD grow and become more popular on the desktop. For now, Debian is just super stable and good enough. reply hcfman 17 hours agoprevBeautiful bit of history. I remember quite a lot of it being old. Nice work by the author. Without people taking the time to write things like this up history gets lost. Years ago I build one of the first local search engines for the Netherlands called Search.NL. I am indebted to the writer of the only page that refers to this history any more here: https://www.eindhovenfotos.nl/1/ilse.nl.html#news1-y8 reply drewg123 15 hours agoprevKirk McKusick gives a great ~45min talk called the history of BSD at conferences. There are essentially 3 versions (one about the lawsuit, one about the TCP wars with BBN, and one about the early history), and he asks the audience to vote on which version they want to hear. Search for Kirk McKusick history of BSD on YouTube, or come to a BSD conference to hear it. And I'd recommend coming to a conference, just so you can talk to Kirk. He's one of the most down to earth, friendly and just generally nice luminaries I've ever talked to in person. reply microtherion 13 hours agoparentYes, I have fond memories of the one or two times I heard him as well. One bit I remember from his presentation was his parable of BSD as the building of a road, with Bill Joy (IIRC) hacking a path through the jungle with a machete, someone else driving a bulldozer along that path, somebody pouring asphalt, somebody adding streetlights, and finally somebody (I seem to recall it was Sam Leffler) painting the streetlights. reply steve1977 13 hours agoparentprevI’ve actually bought a recording of these talks something like 20 years ago. It was either on DVD or maybe even VHS. reply drewg123 12 hours agorootparentOh! I forgot he still sells those. Its now offered as a downloadable video file as well. See https://www.mckusick.com/history/videoorderform.html reply TomMasz 12 hours agoprevI recall the brouhaha that erupted around Sun's move from a BSD foundation to System V. I've always felt it was a downgrade. reply korginator 4 hours agoparentI used to work with the Sun 3's, SparcStations and other Sun hardware at the time when they were transitioning from the BSD flavoured SunOS to Solaris. There was confusion at many levels including branding. The SysV flavour was initially called SunOS 5, not Solaris. Later they decided to use the Solaris name for later SunOS releases and drop the SunOS name altogether. There was some confusion about the Solaris 1.x vs 2.x naming, since Solaris 1.x was really the BSD flavoured SunOS, while Solaris 2.x was a completely different beast, being all SVR4. What first struck me when we moved from SunOS to Solaris was just how clean and simple SunOS was to manage, from the directory structure to the /etc configurations to device file names. Yes, there were different ways in Solaris to address the disk block device (physical, logical, etc.) but it always made me cringe that something I'd addressed as '/dev/sd0a' now became something like '/devices/iommu@0,10000000/sbus@0,10001000/espdma@5,8400000/esp@5,8800000/sd@0,0:a', or at best '/dev/dsk/c0t0d0s0'. The /etc/ filesystem hierarchy also got a lot more complicated. I felt the SVR4 Solaris was far less elegant than SunOS. Guess it was also because of the relative maturity of SunOS, but I felt things just worked as expected and the OS itself was generally snappier than SVR4/Solaris on the same Sparc hardware. Eventually we had no choice but to move to SVR4/Solaris but I still have fond memories of a simpler time and a cleaner SunOS. reply freeremote 10 hours agoparentprevI was a Solaris admin in those days. No one was impressed with the changes. I still, even after all these years, recall the BSD command differences. For whatever reason, I still sometimes enter a random BSDism command on the Linux command line and then say, \"Ooops, wrong command. I cannot believe I did that.\" Especially since I've run Debian stable for the better part of 20 years on and off with attempts to like other distros like Fedora or Arch. Still working in IT closing in on 60 years of age. Still love being a sysadmin. Still love writing shell scripts and automating things. reply pjmlp 2 hours agoprevAnd this is why UNIX took over the server world, and C widespread everywhere, had it been a commercial product sold by AT&T at a similar price tag as VMS, MVS and others, history would have been quite different than the one portrayed on the article. reply williamDafoe 7 hours agoprevThe sword of damacles explanation of BSDi's ending is utter bullshit in this article. BSDi knew they were in direct competition with RedHat in the late 1990s. The President, Rob Kolstad, ran a very lean operation with only about 20 employees. Red Hat managed to IPO and was heavily capitalized. Some BSDi employees and one in particular, argued for an IPO and the particular guy from MIT guaranteed that with an IPO everyone would be rich in one year. The palace revolt worked, the MIT guy took over, Rob Kolstad was deposed, and the MIT guy bankrupted the company in just 1 year (record time). The pieces were sold off to Wind River, which was later absorbed into VxWorks, which finally cancelled development a few years later, and that was the end of BSD/OS or BSDi. reply brlcad 3 hours agoprevI have the sources to \"JHU UNIX\" which were a direct fork of one of the earliest BSD's, though I don't recall hearing which. They were recovered off data tapes (and are still in an arcane raw binary format) from the late 70's / very early 80's. reply Taniwha 8 hours agoprev [–] One minor niggle here: NFS was released by Sun in '89 (used internally before that), couldn't have been released in 4.2 in '84 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article delves into the history and progression of the UNIX operating system, emphasizing its distribution to academic institutions and research centers by AT&T in the 1970s.",
      "Key personalities like Bill Joy and Ken Thompson are highlighted, along with the UNIX advancements at the University of California, Berkeley, and the legal hurdles and impact BSD had on contemporary systems such as macOS and FreeBSD.",
      "It references \"Abort Retry Fail\" by Bradford Morgan White as a valuable resource for documenting the computing industry's history."
    ],
    "commentSummary": [
      "The article examines the early commercial application of BSD Unix at Ford Aerospace, emphasizing the complexities of licensing and TCP/IP advancement.",
      "It addresses the challenges of achieving network compatibility, the evolution of TCP/IP tech, and legal ramifications for companies such as Apple.",
      "The post also covers version control systems, disparities in licensing like GPL vs. BSD, the influence of Richard Stallman, and industry shifts driven by Linux and Microsoft."
    ],
    "points": 179,
    "commentCount": 59,
    "retryCount": 0,
    "time": 1709826382
  },
  {
    "id": 39633172,
    "title": "How Computers Calculate Sine Functions",
    "originLink": "https://androidcalculator.com/how-do-calculators-compute-sine/",
    "originBody": "Toggle navigation Home Reviews Contact Us Manual Getting Started FAQ Blog How do calculators compute sine? Posted by vegesm on March 6, 2024Featured Introduction Sine, one of the fundamental trigonometric functions, plays a crucial role in various fields, including mathematics, physics, engineering, and computer science. Its calculation is not trivial, especially when it comes to implementing it in electronic calculators, where efficiency and accuracy are paramount. In previous entries of the series, we looked into how calculators solve equations and how they calculate square roots. In this blog post, we’ll delve into the intricate process of calculating the sine function, starting from simple approximations to more sophisticated methods. How sine is calculated To begin, let’s inspect the plot of the sine function: It is immediately obvious, that the function is periodic, and has a strong symmetry of the interval between 0 and 𝜋 / 2 : In other words, it is enough to calculate the function on the interval [ 0 ; 𝜋 / 2 ] . Then, we can just use flipping and negation to get the final value. To calculate sin ⁡ ( 𝑥 ) within the reduced interval, one option is to use the well-known Taylor series approximation: sin ⁡ ( 𝑥 ) = 𝑥 − 𝑥 3 3 ! + 𝑥 5 5 ! − … Visualized: The sine function and its Taylor series up to the ninth degree While this method is simple, we need to calculate very high exponentials, and the approximation errors can get quite large around 𝜋 / 2 . For example, with a nine-degree approximation, the result at 𝜋 / 2 would be 1.00000354258428. This is an error of 3e-6, which is quite bad, since most calculations are done to 15 digits precision. In other words, that’s a loss of precision around 10 digits! How sine is really calculated While the method presented in the previous paragraph is quite bad, it does serve as a blueprint for better methods. Essentially, every implementation of sine uses the following three steps: Reduction: Using some algebraic tricks, reduce 𝑥 to a small number 𝑟 . Approximation: Calculate the value of sin ⁡ ( 𝑟 ) using an approximation method, such as the Taylor series. Reconstruction: Calculate the final value of sin ⁡ ( 𝑥 ) based on sin ⁡ ( 𝑟 ) . There are many ways to approach this problem. In the following , I present what Intel uses in their processors, based on their paper. They start with the formula 𝑟 = 𝑥 − 𝑁 𝜋 16 . Here, 𝑁 is the integer chosen such that𝑟is minimized. In other words, we approximate 𝑥 by 𝑁 ⋅ 𝜋 16 , and 𝑟 is the approximation error. How can we use this? Using the identities of the sum of arguments: sin ⁡ ( 𝑥 ) = sin ⁡ ( 𝑟 + 𝑁 𝜋 16 ) = sin ⁡ ( 𝑁 𝜋 16 ) cos ⁡ ( 𝑟 ) + cos ⁡ ( 𝑁 𝜋 16 ) sin ⁡ ( 𝑟 ) = = sin ⁡ ( 𝑁 32 2 𝜋 ) cos ⁡ ( 𝑟 ) + cos ⁡ ( 𝑁 32 2 𝜋 ) sin ⁡ ( 𝑟 ) We have to calculate sin ⁡ ( 𝑟 ) and cos ⁡ ( 𝑟 ) – this is the approximation step, more on this later. For now, assume we know both sin ⁡ ( 𝑟 ) and cos ⁡ ( 𝑟 ) . We still have to find the sine and cosine of 𝑁 32 2 𝜋 . Notice, that both sin and cos are periodic by 2 𝜋 , so we only really have to calculate 𝑁 for 𝑁 = 0 , 1 , 2 , … , 31 . These are 32 values in total, we can easily precompute them. During the calculation of the final value, we just have to look up them up from a list, which is quite efficient. Only one piece of the puzzle remains: how to calculate sin ⁡ ( 𝑟 ) ? In the paper, Intel does not publish the exact polynomial but they do mention that they use minimax approximation. This approximation finds a polynomial that minimizes the maximum error over an interval: max 0 ≤ 𝑟 < 𝜋 16𝑝 ( 𝑟 ) − sin ⁡ ( 𝑟 ), where 𝑝 is the approximating polynomial. One way to calculate 𝑝 is Remez’s algorithm. The results might look like this: 𝑥 − 0.166667 𝑥 3 + 0.00833 𝑥 5 − 0.00019 𝑥 7 + 2.6019 ⋅ 10 − 6 𝑥 9 . The maximum error of this polynomial is 4.1 ⋅ 10 − 9 , which is a thousand times better than the original Taylor-approximation! Conclusion In conclusion, calculating sine in computers involves a combination of reduction, approximation, and reconstruction steps. From simple reduction and Taylor series to more precise methods like minimax approximation, computers employ various techniques to compute sine efficiently while maintaining acceptable levels of accuracy. Understanding these methods sheds light on the underlying mathematics that power computational tools and simulations in numerous fields. Facebook Twitter Recent Posts How do calculators compute sine? How do calculators solve equations? How do calculators compute square roots? Units Android CoordinatorLayout insetEdge © 2024 Algeo Calculator",
    "commentLink": "https://news.ycombinator.com/item?id=39633172",
    "commentBody": "How do computers calculate sine? (androidcalculator.com)178 points by vegesm 15 hours agohidepastfavorite140 comments staplung 9 hours agoI recently learned how Doom was ported to the SNES. It's quite impressive. The SNES hardware was nowhere near fast enough to do all the trig calculations needed for the game but cartridge based games had a trick up their sleeve: they could include actual hardware inside the cart that the game code could make use of. It was more expensive but if you expected to sell a boatload of copies, it could be worth it. However, even using extra hardware wasn't enough in this case. So they pre-calculated lookup tables for sine, cosine, tangent etc. for every angle at the necessary precision. They were helped by the fact that the game resolution in this case was fairly low. If you're interested, you can peruse the C code that was used to generate the tables. Here's the file for sine/cosine: https://github.com/RandalLinden/DOOM-FX/blob/master/source/m... reply pillusmany 7 hours agoparentGames targetting pre-Pentium PCs also used precomputed trig tables. Pentium was fast enough that it didn't matter as much. Just a few years later it was slower to read a trig precomputed table. reply BD103 6 hours agorootparentYup, I remember watching a video about how the RAM bus is the bottleneck when running Super Mario 64 on the N64. The original implementation used trig lookup tables, but the person optimized it by instead using Taylor series (I think) and some negation / shifting. reply Polycryptus 6 hours agorootparentFor anyone curious, the video: https://youtu.be/xFKFoGiGlXQ reply xarope 7 hours agorootparentprevin other words, for those of us who remember, they used the equivalent of a slide rule reply tdudhhu 1 hour agoparentprevOnce I tested lookup tables for a path tracer (ray tracer). It is interesting that you can get very decent results even with low quality tables. Of course there will be artifacts but due to the randomness of a path tracer this is not always very noticeable. reply ImHereToVote 1 hour agorootparentI always wonder when hearing about these old optimizations why they aren't used in contemporary code. Wouldn't you want to squeeze every bit of performance even on modern hardware? reply tdudhhu 28 minutes agorootparentArtifacts are ugly. So why force it on modern hardware when GPUs are extremely fast? For reference: I was doing a path tracer in PHP :) so yeah, that renders like ancient hardware. (The browser requested different buckets of an image. A PHP script then rendered and returned that bucket. So it was a kind of multi-threading but still very slow.) reply csande17 41 minutes agorootparentprevThe \"processor-memory performance gap\" is a big reason why lookup tables aren't as clear of a win on modern hardware as they were on the SNES. If it takes two CPU cycles to read from RAM, a lookup table will basically always be faster than doing the math at runtime. If it takes fifty cycles (because, while your RAM may be faster, your CPU is a lot faster), and your processor has more advanced hardware that can do more math per cycle, maybe just doing the math is faster. reply lambdas 36 minutes agorootparentprevA lot get antiquated by instruction additions, like the infamous inverse square root reply lkschubert8 1 hour agorootparentprevDoing so costs time/wage dollars. reply mads 4 hours agoparentprevBack in the 80's, when I made demos on the C64, we also used pre-calculated sines. I remember going to the library to get a book with the values. reply hakuseki 2 hours agoparentprev> However, even using extra hardware wasn't enough in this case. So they pre-calculated lookup tables for sine, cosine, tangent etc. for every angle at the necessary precision. Is this really the order of events? I imagine the pre-calculated route is what you'd try first, and only go for extra hardware if that failed somehow. reply shzhdbi09gv8ioi 1 hour agorootparentLookup tables were commonplace in 80s-90s graphics programming. It should have been used before any consideration of custom hardware solutions. reply Dwedit 7 hours agoparentprevIt turns out that SNES DOOM missed out on a big optimization that people figured out later on. If you use the SNES's Mosaic feature combined with scrolling tricks, you can nearly double the fill rate. Rather than outputting two pixels, you let the SNES's mosaic hardware do the pixel doubling. reply inkyoto 4 hours agoparentprev> […] pre-calculated lookup tables […] The approach is older than that. I remember my grandfather's engineering books from 1950's – nearly each of them had a large addendum with the said pre-calculated sine, cosine, tangent and logarithm lookup tables. And there was at least one book that only had such tables and no other information. That is how engineers used to calculate before the advent of computers. reply aoanla 1 hour agorootparentThe classic of this field of books is Abramowitz and Stegun's \"Handbook of Mathematical Functions\" - although the two listed names are merely those of the compilation editors, as the calculations of the numerous tables of values (and sheets of mathematical identities) required hundreds of human computers operating for years. Ironically, on publication in 1964 it was just in time to see the dawn of the electronic computer age that would supplant it. reply warpech 14 hours agoprevThis made me realize that trigonometric functions are not deterministic across different CPU architectures, OS, and programming languages (floating point precision aside). E.g. I would assume that Math.sin(x) returns the same thing in NodeJS on Windows and Mac/M1, but it turns out it is necessarily so. https://stackoverflow.com/questions/74074312/standard-math-f... reply retrac 12 hours agoparentRounding transcendentals correctly has unknown time and space complexity. [1] Sort of brushes up against the halting problem. With limited precision, the upper limit becomes calculable but it's rather large - packages that offer correct rounding on 64-bit floating point use potentially hundreds of bytes to deal with a single floating point value. Dedicated circuitry to implement it fast would be big and complicated even by today's standards. https://en.wikipedia.org/wiki/Rounding#Table-maker's_dilemma reply lifthrasiir 9 hours agorootparentTrue in general, almost false for binary64. Important univariate functions have been thoroughly mapped for known hard-to-round cases, which resulted in the fact that we only need at most triple-double format to do the correct rounding. (Bivariate functions like `pow` are much harder, and not yet fully mapped as of this writing.) As a result we now have a mathematical library that almost ensures correct rounding [1], and a further optimization is currently in progress. [1] https://core-math.gitlabpages.inria.fr/ reply throwawaymaths 8 hours agorootparentprevunknown time and space complexity True in general but for the basic datatypes sent through hardware regusters your processor architecture has fixed precision. So the time and space complexity is O(1) reply ImHereToVote 1 hour agorootparentprevWhy should just use analog circuit for this sort of thing where the exact precision doesn't matter. reply Arelius 8 hours agoparentprevYeah, but you'd be surprised at how frequently they appear to be the same. I once worked on a HTML5 game that that relied on deterministic simulation for networking. And it wasn't untill pretty late in development that a build of Chrome shipped on some platform that finally triggered a desync in our extensive cross-platform test suite. We implemented a deterministic approximation, and moved on. But I learned something important about trig functions that day. reply cogman10 14 hours agoparentprevWell, what's fun is that (AFAIK) trigonometric functions tend not to be implemented in the newer floating point instructions, such as AVX or SSE. So while what you say is true about the x87 implementation of those functions, for anything targeting a machine built in the last 20 years it's likely the code will run consistently regardless the architecture (barring architecture floating point bugs, which aren't terribly uncommon in the less significant bits and when overclocking comes into play). x86 compilers won't use x87 instructions when SSE2 and later are available. x87 is just a really weird and funky instruction set that's best left in the gutter of history. reply bnprks 13 hours agorootparentSadly even SSE vs. AVX is enough to often give different results, as SSE doesn't have support for fused multiply-add instructions which allow calculation of a*b + c with guaranteed correct rounding. Even though this should allow CPUs from 2013 and later to all use FMA, gcc/clang don't enable AVX by default for the x86-64 targets. And even if they did, results are only guaranteed identical if implementations have chosen the exact same polynomial approximation method and no compiler optimizations alter the instruction sequence. Unfortunately, floating point results will probably continue to differ across platforms for the foreseeable future. reply cogman10 13 hours agorootparentThat's a bit of a different problem IMO. Barring someone doing a \"check if AVX is available\" check inside their code, binaries are generally compiled targeting either SSE or AVX and not both. You can reasonably expect that the same binary thrown against multiple architectures will have the same output. This, of course, doesn't apply if we are talking about a JIT. All bets are off if you are talking about javascript or the JVM. That is to say, you can expect that a C++ binary blob from the Ubuntu repo is going to get the same numbers regardless the machine since they generally will target fairly old architectures. reply zokier 12 hours agorootparent> Barring someone doing a \"check if AVX is available\" check inside their code Afaik that is exactly what glibc does internally reply gpderetta 13 hours agorootparentprevGCC won't use FMA without fast-math though. Even when AVX is otherwise enabled. reply zokier 13 hours agorootparentSure it will: > -ffp-contract=fast enables floating-point expression contraction such as forming of fused multiply-add operations if the target has native support for them > The default is -ffp-contract=off for C in a standards compliant mode (-std=c11 or similar), -ffp-contract=fast otherwise. https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html#ind... reply gpderetta 12 hours agorootparentOh, wow, forgot about fp-contract. It says it is off in C by default, what about C++? reply zokier 12 hours agorootparentRead closer, it defaults to fast, not off reply planede 13 minutes agorootparentI would have expected to be a bug in the documentation? Why would they turn FMA off for standard compliant C mode, but not for standard compliant C++ mode? But the documentation does appear to be correct: https://godbolt.org/z/3bvP136oc Crazy. reply enriquto 13 hours agorootparentprev> x87 is just a really weird and funky instruction set that's best left in the gutter of history hmmm, can you use the long doubles in sse or avx? They are glorious, and as far as I see from playing with godbolt, they still require dirtying your hands with the x87 stack. reply cogman10 13 hours agorootparentThe 80bit float? Not as far as I'm aware. However, it's fairly trivial to represent a 127bit float with 2 64bit floats. And with the nature of AVX/SSE, you don't really take much of a performance hit for doing that as you are often operating on both parts of the double with the same instruction. reply enriquto 12 hours agorootparentDo you know if there's language support for that? Are there obscure gcc options that make \"long double\" be quadruple precision floats? reply zokier 12 hours agorootparentYou can just use standard C _Float128 type https://gcc.gnu.org/onlinedocs/gcc/Floating-Types.html reply cogman10 12 hours agorootparentprevWhich language? For C++, there's this: https://en.cppreference.com/w/cpp/types/floating-point reply fulafel 2 hours agorootparentprevWhat about GPU ISAs? reply bee_rider 13 hours agorootparentprevThey do, however, have some intrinsics for trig functions in AVX in their compilers. Not as good as having an instruction of course. reply paulddraper 7 hours agoparentprev> deterministic I would use the word \"consistent.\" Non-determinism implies randomness. reply TylerE 14 hours agoparentprevSafer to assume that floats are never deterministic. reply jacobolus 13 hours agorootparentFloats follow a clear specification which determines precisely how basic arithmetic should work. They should work the same on all popular modern platforms. (Whether specific software libraries are the same is a separate question.) reply zokier 13 hours agorootparentBut transcendentals like sine are not part of the strictly defined basic arithmetic; they are intentionally defined with relaxed behavior. reply jacobolus 12 hours agorootparentIf you implement sine in software using the same sequence of basic arithmetic instructions, the result should be the same across platforms. If you make two different implementations using different arithmetic, then of course you can't rely on them being the same. reply zokier 12 hours agorootparentPoint being that IEEE 754 defines two sets of operations, the required operations (section 5) that should be produce correctly rounded results to the last digit, and recommend operations (section 9) with relaxed requirements. And sine belongs to the latter section, so IEEE 754 does not mandate reproducible results for sine. reply jacobolus 11 hours agorootparentMy understanding is that most software always uses some software implementation of sine, rather than calling a hardware instruction. Which is definitely what you should do if you care about getting the exact same results across platforms. reply dzaima 9 hours agorootparentSoftware implementations can and do differ (even dynamically) based on the hardware though - e.g. glibc's sin(x) function, what C code will end up using (if not other languages relying on the C stdlib), uses FMA instructions on my CPU, and thus the exact same binary on the exact same OS with the exact same glibc should behave differently on a very old CPU without FMA where it should have a different implementation (as generally things using FMA cannot be exactly ported to hardware without it without a gigantic drop in performance which'd be extremely unacceptable). reply gpderetta 13 hours agorootparentprevEven there many standard libraries provide very good precision at least within sane domain. reply TylerE 13 hours agorootparentYeah, that’s kind of my point. 99% consistent isn’t. reply saagarjha 13 hours agorootparentprevThis is not always a safe assumption (in certain scenarios floating point results being nondeterministic has the possibility to introduce bugs and security issues) and is also a kind of sad way to look at the world. The response to \"I don't understand how this works\" should not be to adopt an incorrect viewpoint, but to know the limitations of your understanding. reply otabdeveloper4 1 hour agorootparentIrrational numbers cannot have an exact representation in digital bits. (Computers use rational numbers with modular arithmetic under the hood.) reply TylerE 12 hours agorootparentprevIt’s not that I don’t understand, it’s that I do. Floats are inherently lossy representations. Yes, this means the more operations you perform on a float input, the fuzzier the value is.You ignore that harsh reality at your peril. If you find engineering rigor sad, I don’t know what to tell you. reply saagarjha 12 hours agorootparent\"Floats are not deterministic\" is not engineering rigor, it's just wrong. They are specified precisely by IEEE-754 in how they must behave and which operations are allowed to produce which results. reply TylerE 12 hours agorootparentIEEE 754 conforming floats conform to IEEE-754. If they actually conform. Low end devices with shitty software implementations often get the hard edge cases wrong. reply saagarjha 12 hours agorootparentYes and when that happens it is important to know what went wrong rather than just handwaving \"oh it's that float stuff again I can't trust it\". reply jacobolus 12 hours agorootparentprev> the more operations you perform on a float input, the fuzzier the value is No, any float always precisely represents a specific number. The issue is that only a finite number of numbers are representable. Some algorithms are poorly conditioned and when implemented using floating point arithmetic will lead to a result that is different than what you would get in idealized real number arithmetic. That doesn't make any floating point value \"fuzzy\". reply aidenn0 3 hours agorootparent> No, any float always precisely represents a specific number. The issue is that only a finite number of numbers are representable. A float always precisely represents a specific number, but that number is not always precisely the equal to the algebraic result of the operations performed (even when ignoring transcendental and irrational functions). This should be obvious since there is no limit to rational numbers, but finite floating point numbers. If you design your algorithms very carefully, you can end up with the ratio of the output of your algorithm to the ratio of the algebraic result close to unity over a wide domain of inputs. reply aardvark179 13 hours agorootparentprevFloats are well defined, and it is perfectly possible to reason about how algorithms based on them should behave. Few languages specify the accuracy of things like trig functions, so relying on them can be tricky, and JavaScript is particularly bad in that respect. reply throwway120385 10 hours agorootparentprevIt depends. If you're constrained to one chip and one platform you can characterize or you can estimate the characteristics of a float that matter in your application. In some applications like embedded that's actually totally fine, and modern embedded chips can often do floating point as fast or faster than they can emulate fixed point to work around floating point's drawbacks. On one project I worked on they originally wrote everything fixed point out of fear that floating point would introduce some deleterious effect. But in the end they rewrote parts of the project using floating point to no ill effect and great performance improvement. And there were features of the product that they had to strike because the rewrite needed to support them couldn't touch certain sensitive areas of the code that had been tested extensively in the 2 or 3 years of development. It would have been much better to evaluate the assumption that floats are bad early on in the project and make the decision based on real information. The heuristic they were applying ended up costing part of the product that was strategically important. reply Jyaif 10 hours agorootparent> constrained to one chip and one platform and constrained to one compiler at a precise version, and one set of compiler options reply mhh__ 10 hours agorootparentprevThey're always deterministic in some sense (and as long as your OS respects the rounding mode after a context switch properly). This might sound pedantic but it determines how we think about floats — the behaviour is specified quite exactly. reply kens 9 hours agorootparentCuriously, early Intel 386 processors had a bug where 32-bit multiplies were genuinely nondeterministic: some answers would depend on the voltage, frequency, temperature, and manufacturing conditions. The problem was essentially analog, a layout issue, where the signal didn't always have enough margin. (This is unrelated to the famous Pentium FDIV bug.) Until Intel got the problem fixed, they would stamp bad chips with \"16 BIT S/W ONLY\", while good chips were labeled \"ΣΣ\". reply TylerE 10 hours agorootparentprevWhat I mean is that the same code running on different hardware/os may not always give the same answer. It’ll be close, but you can’t always expect bit for bit identical. reply ducttapecrown 10 hours agorootparentprevThey're always deterministic, just as long as physics is. reply charlieyu1 5 hours agorootparentprevBut why? We all know 0.1+0.2 won’t give 0.3 with floats but at least we should expect deterministic result for same numbers and same operations and same order, no? reply contravariant 10 hours agorootparentprevI don't think that's safe at all. Catastrophic cancellation would be quite a lot less catastrophic if rounding errors were random but accurate on average. reply aardvark179 13 hours agoparentprevSomewhat annoyingly the ascribe standard only specifies that various math functions return an approximation but does not set any bounds on that approximation. So for many functions you could just return NaN and still be compliant. reply layer8 13 hours agorootparentIsn’t NaN the one value that can’t possibly count as an approximation, because it’s not a number and unordered? ;) reply aardvark179 13 hours agorootparentYou might think so, but if it’s not specified in the standard… reply layer8 13 hours agorootparentI was submitting an interpretation of the standard. reply zardo 13 hours agorootparentprevIt is the worst possible approximation though. reply adgjlsfhk1 14 hours agoparentprevsome languages (e.g. Julia) provide their own math library do that you get the same results across across operating systems. reply quickthrower2 9 hours agoparentprevSo you can use sin(x) for various x to tell what you are running on. Maybe even in the browser? reply lifthrasiir 9 hours agorootparentV8 and SpiderMonkey have converged to the same underlying library (fdlibm), partly for the interoperability, so you generally can't. reply Tade0 38 minutes agoprevI was truly amazed when my high school computer science teacher expanded the sine function into a Taylor series for us to implement in class. Couldn't wrap my head around the concept until the topic was revisited in college, but idea was there and helped me understand the tradeoffs brought by \"fast math\" libraries I was using during high school. reply jxy 13 hours agoprevIt's much clearer if you read one of the source code of the libm. Plan 9: https://9p.io/sources/plan9/sys/src/libc/port/sin.c Freebsd: https://cgit.freebsd.org/src/tree/lib/msun/src/k_sin.c reply planede 6 minutes agoparentAh, I always referred to the musl implmenetation, but I just now realized that they copied the Freebsd one. reply lifthrasiir 9 hours agoparentprevFreeBSD code is missing the range reduction step (it's named a \"kernel\" for the reason): https://cgit.freebsd.org/src/tree/lib/msun/src/e_rem_pio2.c reply toolslive 10 hours agoprevAfter reducing the interval, you don't want to use the Taylor series as you're building an approximation that's really good in 0 but not so good moving away from 0. It's better to use an interpolating polynomial (Chebychev comes to mind) over the whole target interval. reply paulpauper 10 hours agoparentThere are many ways to do this. It's not a difficult problem unless memory is constrained. reply eska 14 hours agoprevYou might also find this video interesting: \"Finding the BEST sine function for Nintendo 64\" https://www.youtube.com/watch?v=xFKFoGiGlXQ reply microtherion 14 hours agoprevP.J. Plauger's _The Standard C Library_ provides an implementation for all functions in the (then) C standard: https://www.amazon.com/Standard-Library-P-J-Plauger/dp/01383... reply tails4e 14 hours agoprevI've seen the third order Taylor series used, but with the coefficients calculated at various offsets for a quarter wave. So you lookuo where you are in the quarter wave, then look up the 3 or 4 cofficients. This keeps the error somewhat bounded as the size of X is a small so the series does not diverge too much. reply eh_why_not 9 hours agoprevAnyone experienced with the Remez algorithm mentioned at the end of the article? The degree-9 polynomial, said to be a thousand times better than the original Taylor approximation in maximum error, also appears to be very close to the Taylor series in the first place. Rounding the Taylor coefficients to 6 digits after the decimal: 1/3! = 0.166667 1/5! = 0.008333 1/7! = 0.000198 1/9! = 0.000027(56) The first 2 are exact, the third is 5 digits only (so 0.000190), and the fourth is more different starting from the 6th digit (0.000026019). The delta in the 9-th order is expected if you were to truncate the Taylor series starting from the 11th order to infinity (+ x^11 / 11! - x^13/13! ...). reply stephencanon 8 hours agoparenthttps://en.wikipedia.org/wiki/Remez_algorithm It’s a very simple iterative algorithm, essentially the dumbest thing that could possibly work (like most good algorithms). It fails to converge for functions that have poles nearby unless you have a very good initial guess (the Chebyshev or Carathéodory-Fejér approximants are ~always good starting points and easily computed). In practice you want to optimize a weighted L-inf norm rather than absolute, because floating-point errors are measured in a relative norm. reply dboreham 14 hours agoprevThis was my first use of open source, around 1978. I wondered how calculators and computers did trig functions, and was also using Unix V7. We had a large disk and kept the source on line. So I was able to find this: https://www.tuhs.org/cgi-bin/utree.pl?file=V7/usr/src/libm/s... and from there this book: https://www.biblio.com/book/computer-approximations-john-f-h... reply azhenley 13 hours agoprevI blogged my adventure of implementing cosine from scratch and how others have done it: https://austinhenley.com/blog/cosine.html reply sema4hacker 5 hours agoprevI remember seeing the source code for a version of SpaceWar! running on an Adage Graphics Terminal (a one's complement machine) around 1970 that used a precomputed sine table. I wonder what the first program was that ever used a precomputed trig table. reply duped 14 hours agoprev1 - cos^2(x), obviously reply nh23423fefe 14 hours agoparentinstead, you could just double negate to optimize away the square root -(-(sin(x)) reply ChainOfFools 12 hours agoparentprevclear, but too verbose. 1/csc is what you want. or for style points just -cos' reply mettamage 7 hours agoprevWhat’s the best way to calculate it by hand? I’m brushing up my math basics (I graduated CS while dodging the math requirements) and it frustrates me that in trig I need to remember values at all. The values such as sqrt(2)/2 make sense but how hard is it to calculate sin(5 degrees) by hand? reply HenryPrickett 7 hours agoparentUse a Taylor series with a four function calculator. 0 is a decent approximation of sin near 0. x is a better one. x - x^3/6 is an even better one. x - x^3/6 + x^5/120 ... Note that x here is in radians rather than degrees so convert (degrees * pi/180) first. Repeat until you're satisfied with how many stable digits you get reply empath-nirvana 6 hours agoparentprevhttps://www.youtube.com/watch?v=3d6DsjIBzJ4 reply simonblack 8 hours agoprevThe same way you can eat an elephant: one byte at a time. Any calculating job can be undertaken by a proper Turing machine. You just have to keep in mind the old triangle of usage: Cost, capability and speed. If a human can calculate a sine, so can any full-blown computer. reply t-3 14 hours agoprevLink doesn't appear to be valid, but aren't these usually precalculated and stored in a lookup table? reply vardump 14 hours agoprevCORDIC is how it's usually done in hardware (and FPGAs). https://en.wikipedia.org/wiki/CORDIC reply perihelions 14 hours agoparentCORDIC is pretty obsolete, AFAIK. Its advantage is that its hardware requirements are absolutely tiny: two (?) accumulator registers, and hardware adders and shift-ers—I think that's all. No multiplication needed, in particular. Very convenient if you're building things from discrete transistors, like the some of those earlier scientific calculators! (Also has a nice property, apparently, that CORDIC-like routines exist for a bunch of special functions and they're very similar to each other. Does anyone have a good resource for learning the details of those algorithms? They sound elegant). reply pclmulqdq 14 hours agorootparentCORDIC still is used in tiny microcontrollers (smaller than Cortex-M0) and in FPGAs when you are very resource-constrained. Restricting the domain and using Chebyshev/Remez is the way to go pretty much everywhere. reply idatum 4 hours agorootparentprevI just recently came across CORDIC in a fun read I just finished, 'Empire of the Sum', by Keith Houston. It's a history of calculators, and there's a chapter on the HP-35 which used CORDIC. The author goes into some details how it was used by that constrained device. There are a lot of references given in the book including more details on CORDIC. reply rcxdude 10 hours agorootparentprevCORDIC doesn't make sense if multiplies have a similar cost to adds. If you have cheap multiplications then a successive approximation is faster. If multiplications are expensive relative to adds then CORDIC can still generally win. Basically this is only the case nowadays if you are doing ASICs or FPGAs. reply derf_ 6 hours agorootparentprevI have used CORDIC to compute fixed-point log/exp with 64 bits of precision, mostly because I wanted to avoid having to implement (portable) 64x64->128 bit integer multiplication in C. It is probably still slower than doing that, but the code was really simple, and very accurate. reply dilyevsky 10 hours agorootparentprevI think still used out of necessity when hw floating point not available (like fpgas) reply f1shy 13 hours agorootparentprevMultiplication is pretty much needed in cordic! And is far from obsolete! It works perfectly fine, and dont have any of the problems said in the article. reply kevin_thibedeau 10 hours agorootparentCORDIC doesn't use multipliers. That's the whole appeal for low performance hardware since it's all shifts and adds. It can still be useful on more capable platforms when you want sin and cos in one operation since there is no extra cost. reply perihelions 13 hours agorootparentprevIt uses multiplication by powers of two, which is a floating-point bit shift. reply adgjlsfhk1 14 hours agoparentprevno it's not. cordic has awful convergence of 1 bit per iteration. pretty much everyone uses power series. reply f1shy 13 hours agorootparentThat is 64 iterations for a double, that is nothing! reply adgjlsfhk1 10 hours agorootparent53, but that's still a lot more than the 5th degree polynomial that you need. reply rcxdude 10 hours agorootparentyeah, but 52 adds can be a lot cheaper than a few multiplies, if you're making them out of shift registers and logic gates (or LUT). in a CPU or GPU, who cares, moving around the data is 100x more expensive than the ALU operation. reply Const-me 9 hours agorootparent> in a CPU or GPU, who cares, moving around the data is 100x more expensive than the ALU operation Moving data is indeed expensive, but there’s another reason to not care. Modern CPUs take same time to add or multiply floats. For example, the computer I’m using, with AMD Zen3 CPU cores, takes 3 cycles to add or multiply numbers, which applies to both 32- and 64-bit flavors of floats. See addps, mulps, addpd, mulpd SSE instructions in that table: https://www.uops.info/table.html reply adgjlsfhk1 9 hours agorootparentprev> moving around the data is 100x more expensive than the ALU operation. This is exactly the problem with CORDIC. 52 dependent adds requires moving data from a register to the ALU and back 52 times. reply rcxdude 14 minutes agorootparentIt's the problem with CORDIC in that context, yes! reply ajross 7 hours agorootparentprevActually pretty much everyone implements double precision sin/cos using the same (IIRC) pair of 6th order polynomials. The same SunPro code exists unchnaged in essentially every C library everywehre. It's just a fitted curve, no fancy series definition beyond what appears in the output coefficients. One for the \"mostly linear\" segment where the line crosses the origin and another for the \"mostly parabolic\" peak of the curve. reply bigbillheck 13 hours agoparentprevWhy would you ever use CORDIC if you had any other option? reply vardump 13 hours agorootparentIt's great for hardware implementations, because it's simple and you get good/excellent accuracy. I wouldn't be surprised if that's still how modern x86-64 CPUs compute sin, cos, etc. That said, last time I had to do that in software, I used Taylor series. Might not have been an optimal solution. EDIT: AMD's Zen 4 takes 50-200 cycles (latency) to compute sine. I think that strongly suggests AMD uses CORDIC. https://www.agner.org/optimize/instruction_tables.pdf page 130. Same for Intel, Tiger Lake (Intel gen 11) has 60-120 cycles of latency. Page 353. I'd guess usually ~50 cycles for Zen 4 (and ~60 for Intel) for float32, float64/float80 datatype. Denormals might also cost more cycles. reply bigbillheck 12 hours agorootparentThey switched away from CORDIC at one point: https://www.intel.com/content/www/us/en/developer/articles/t... (there doesn't seem to actually be a linked article there, just the summary) reply vardump 11 hours agorootparentPretty weird Intel's sine computation latency hasn't changed all that much over the years. Latencies have been pretty similar for 20 years. EDIT: That's a paper for a software library, not the CPU's internal implementation. Which is probably still done with CORDIC. reply bigbillheck 11 hours agorootparent> EDIT: That's a paper for a software library, not the CPU's internal implementation. Unless you're seeing something I'm not, it's talking about x87, which hasn't been anything other than 'internal' since they stopped selling the 80486sx. reply vardump 10 hours agorootparentAh you're right. Anyways I wonder why it's still so slow. 60-120 cycles sure looks like a CORDIC implementation, but perhaps not. reply deepthaw 13 hours agoprevIgnorant question: Given the ridiculous number of transistors and so on we can use in CPUs and GPUs nowadays how feasible is a relatively huge trig lookup table burned into rom? reply aidenn0 3 hours agoparentA lookup table (for any function) that covered all values between 0 and 1 in single precision, would be ~4GB; there are approximately 1B values between 0 and 1, and the result of each value is 4 bytes. Such a table for double-precision would be much, much larger. reply zokier 13 hours agoparentprevThere is huge number of 64bit floats and huge portion of those are between 0..pi/2. reply bigbillheck 13 hours agoparentprevSeems like a terrible idea on latency grounds alone. reply perihelions 14 hours agoprevIs this still current? The paper has a publication year of 1999. reply convolvatron 14 hours agoprevhttp://steve.hollasch.net/cgindex/math/inccos.html is a great technique if you need a fast integer approximation for some some arbitrary sampling interval (i.e. motor control) reply phkahler 9 hours agoparentI've been doing FoC motor control for a long time and I've settled on a neat little fixed point approximation for sin/cos. I haven't been able to find the blog I got the idea from. It's accurate to 9 bits, but is very symmetric and hits 1,0,-1 exactly. It's also smooth which usually makes it better than lookup tables. reply Solvency 9 hours agoprevWhy isn't this just done with an industry standard lookup table these days? reply demondemidi 10 hours agoprevI thought modern CPUs since the late 1980's used a lookup table for trig/transcendental functions. Is the LUT just an expansion of the polynomial? I never really understood how FPUs worked... reply Someone 8 hours agoparentThat would take way too much room. A full lookup table would have 2^64 entries of 64 bits each, at 2^70 bits of ROM. For comparison: - Apple’s M2 Ultra has about 134 billion transistors. That’s about 2^38. - Avogadro’s number is about 2^79. Reducing the argument to a small range around zero decreases that a lot, but not enough by a far stretch. There are 2^52 doubles in [0.5, 1.0), 2^52 more in [0.25, 0.5], 2^52 more in [0.125, 0.25], etc. so you’d still easily need 2^52 entries or 2^58 bits (likely way, way more) reply demondemidi 8 hours agorootparentThey DO use a lookup table because that’s what the FDIV but came from: https://en.m.wikipedia.org/wiki/Pentium_FDIV_bug “It is implemented using a programmable logic array with 2,048 cells, of which 1,066 cells should have been populated with one of five values: −2, −1, 0, +1, +2.” Not sure what you’re trying to demonstrate, they wouldn’t store every single float!! I hope don’t program. ;) reply aidenn0 3 hours agoparentprevTFA says they use a 32 entry LUT, then do some math on the result. reply lifthrasiir 8 hours agoparentprevOlder CPUs generally have used CORDIC (which does use LUT but that's only a part of the algorithm) due to its simplicity and compactness, while later CPUs with extensive microcode support would do the same thing as software implementations. reply amelius 14 hours agoprevAnd arcsin? reply paulpauper 10 hours agoparentharder due to convergence issues reply paulpauper 14 hours agoprev [–] sine is easy because the series is globally convergent and fast converging reply zgs 7 hours agoparentIt would also be extremely inaccurate. The x^n numerators grow very quickly and digits get lost because unlimited precision isn't available. Likewise, the n! denominators also grow rapidly. Then the series is alternating which means cancellation is happening for every added term. If you don't believe me try for x=10. reply planede 1 minute agorootparentI expect that it's much less of an issue if you add the terms in reverse order. reply ot 14 hours agoparentprev [–] Did you read the article? It is specifically about how the series looks simple, but the error is actually very bad if you do things naively. reply paulpauper 14 hours agorootparent [–] That still makes it easier compared to computing constants in which the series are not globally convergent, like inverse trig functions. Obviously, you would have to break it apart to speed convergence. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The blog post explains how calculators calculate the sine function through reduction, approximation, and reconstruction steps.",
      "It contrasts the less precise Taylor series approach with the more accurate minimax approximation methods employed by Intel processors.",
      "Understanding these techniques offers a glimpse into the math computations supporting computer simulations and computational tools."
    ],
    "commentSummary": [
      "The discussion explores the use of lookup tables and trigonometric functions in graphics programming, especially on older systems like the SNES, focusing on optimization techniques and challenges in rounding transcendental numbers.",
      "It addresses differences in floating-point calculations across CPU architectures and the efficiency of algorithms like CORDIC for trigonometric computations.",
      "The conversation also touches on the precision and limitations of floating-point numbers, the impact of FMA instructions, and the potential use of lookup tables in modern CPUs and GPUs for mathematical calculations."
    ],
    "points": 178,
    "commentCount": 139,
    "retryCount": 0,
    "time": 1709837955
  },
  {
    "id": 39629213,
    "title": "Area 51 (2005) Source Code Unearthed in Garage Sale",
    "originLink": "https://github.com/ProjectDreamland/area51",
    "originBody": "Area 51 (2005) Source Code Release Welcome to the unofficial release of the Area 51 (2005) video game source code! This project aims to resurrect and preserve a piece of early 2000s video game history for enthusiasts, historians, and developers alike. Below is a brief overview of the source code details, its origin, and guidance on how the community can help bring this game into the modern era. Historical Overview of Area 51 Initial Release: Area 51 was originally released on April 25, 2005, for PC, PS2, and Xbox. It has become a memorable cult classic. Air Force Sponsorship: In a unique turn of events, the game was sponsored by the United States Air Force and released as freeware for PC. Abandonware Status: Despite the game's initial success and the novel sponsorship, it eventually fell into obscurity, becoming abandonware. The game's support and distribution were discontinued, leaving it in a state where it was difficult for fans to access or play on modern systems. Source Code Snapshot Details Snapshot Date: The source code is a snapshot from 2005-03-31 10:40:19, just before the game's official release. Discovery: It was found at a garage sale of a former THQ developer. Contents: This release includes the source code for the Entropy engine, game logic, and targets for PC, PS2, Xbox, and an early version for GameCube. Additionally, debug symbols for various platforms are available in the release. Assets are not included, but those can be recovered from the retail game files. How to Contribute The main goal is to get the source code into a buildable state on modern systems. Whether you're interested in game development, historical preservation, or simply a fan of Area 51, here's how you can help: Building the Project: Assistance is needed to compile and run the game on contemporary hardware. If you have experience with game development, legacy systems, or cross-platform development, your expertise is invaluable. Documentation and Research: Insights into the original development environment, including compilers, libraries, and tools, would greatly assist the restoration effort. Debugging and Porting: Once buildable, the project will require debugging and potentially porting to newer platforms to reach a wider audience. Getting Started To contribute, please fork the repository, make your changes, and submit a pull request with your updates. For discussion, collaboration, and support, join our community on platforms like Discord or participate in GitHub Discussions for this project. Releases Here",
    "commentLink": "https://news.ycombinator.com/item?id=39629213",
    "commentBody": "Source Code for Area 51 (2005) by Midway Studios Austin Found at Garage Sale (github.com/projectdreamland)176 points by andrew_rfc 19 hours agohidepastfavorite63 comments bsimpson 16 hours agoAs you may have gathered from the list of home systems, this is not the light gun arcade shooter you remember from childhood. It appears to be a 2005 addition to that franchise. reply Arrath 15 hours agoparentAnd it is, actually, a pretty decent game! Like the little-known console sequels to Far Cry, you later get infected with the gunk you're dealing with and it opens up the game a bit by adding extra abilities. reply ablation 3 hours agorootparentYou mean the Crysis series? With no doubt tens of millions of sales[1] by this point in time and AAA marketing budgets, I’m not so sure they’re little-known. Apologies if you’re referring to something else of course. 1: https://www.vgchartz.com/game/226218/crysis/ reply sunnybeetroot 14 hours agorootparentprevAgreed, I remember getting it in an Xbox demo disk and playing the same level over and over again. reply cosmojg 6 hours agorootparentprevJust like the better-known sequels to Jak and Daxter! reply LarsDu88 17 hours agoprevWow they built a whole game engine and editor just for this game, and they got David Duchovny and Marilyn Manson to voice act in it! And now it lives in obscurity reply myfavoritetings 16 hours agoparentFWIW, most games before early 2000s built all their tooling from scratch as there wasn't off the shelf engines to use. Unreal engine came out in 98 and Source in 2004 reply LarsDu88 15 hours agorootparentJedi Knight 2 came out in 2002 and the original Call of Duty came out in 2003, both running heavily modified versions of John Carmack's Quake III engine reply bpye 7 hours agorootparentRelated - https://en.m.wikipedia.org/wiki/File:Quake_-_family_tree.svg reply modeless 5 hours agorootparentprevValve originally started in 1996 by licensing the Quake engine for Half-Life. reply Terr_ 9 hours agorootparentprevThat reminds me of how Torque [0] debuted in 2001 using Tribes 2. [0] https://en.wikipedia.org/wiki/Torque_(game_engine) reply jamesu 9 hours agorootparentTorque was pretty instrumental in kick-starting my career. Glad it's still being maintained to this day (despite it arguably getting steamrolled by unity and mismanagement). reply treflop 15 hours agorootparentprevWasn’t Quake 1 reasonably off the shelf for its time? Released in ‘96, it did have a map editor and a number of games were built with it. Source itself being built from Quake 1… reply TonyTrapp 14 hours agorootparentidTech 2 was specifically built for Quake 1, and only later it was licensed to other developers as well. So it was not an off-the shelf solution id could simply take for building Quake. It was tailored for that game. reply BikiniPrince 12 hours agorootparentThey specifically built Quake to sell the engine. They knew it would be difficult to actually sell an engine without a successful title. reply iamtheworstdev 7 hours agorootparentQuake 3 was probably the first time the id really built a game to demonstrate an engine for the sake of engine sales. And then again with Rage. reply franzb 11 hours agorootparentprevInteresting, I’ve never heard about that. Do you have a source? reply mike_d 10 hours agorootparentI don't think they specifically understood the idea of a \"game engine\" as the core product at the time. But there are plenty of references if you Google a bit that Quake was designed for modders due to the popularity of DOOM mods - so developer experience was absolutely taken into account from the start. They had already done licensing deals for the DOOM engine at that point, including the greatest game of all time \"Chex Quest.\" https://en.wikipedia.org/wiki/Chex_Quest reply aoanla 1 hour agorootparentYeah, this is why Quake's logic for a lot of game things - monsters, weapons, moving platforms - is written in a byte-code interpreted language (QuakeC). The idea was to separate it from the engine code so modders could easily make new games without needing access to the full engine source. (And QuakeC was supposed to be simpler as a language than C, which it... is, but at the cost of weird compromises like a single number type (float) which is also used to store bitfields by directly manipulating power of two values. Which works, of course, until your power of 2 is big enough to force the precision to drop below 1...) reply busterarm 6 hours agorootparentprevKeep in mind that they had developed a good relationship with Raven Software and made a good chunk of money off of their use of idTech 1 for Heretic. reply Unfrozen0688 12 hours agorootparentprevWon't really work on consoles like PS1 and PS2. reply CalRobert 15 hours agorootparentprevI think some of the early FPS engines were re-used, with Rise of the Triad using the Wolfenstein 3d one. reply TonyTrapp 14 hours agorootparentEngines that come to mind: XnGine (Daggerfall, Terminator, ...), Dark Engine (Thief, System Shock 2), Build (Duke 3D, Blood...). Yes, they existed before the 2000s, but the difference to today is that there were many engines being reused for a handful of games at most. Today it's few engines running most games. reply CalRobert 4 minutes agorootparentI wanted to mention the odd adventure game Normality as using the Doom engine, but I can't find a source reply holoduke 14 hours agorootparentprevBattlefield, gta, alan wake, ms flightsim, cyberpunk, hogsward (unreal) all top games with different engines. Agree that unreal engine has many games, but plenty of alternatives reply TonyTrapp 14 hours agorootparentThat's a pretty small selection of well-known AAA games. Those few examples really don't change the general skew towards using 3rd party engines these days vs. few games doing something like that in the 90s. And in fact, most of these engines have also been reused between games (with heavy modifications of course - e.g. Remedy's Northlight engine has been evolving since Alan Wake 1). reply holoduke 13 hours agorootparentBuild engine duke nukem also used in many other games like blood etc. Same for quake engine. Even doom engine was used in games like hexen. Doom was also an evolution of the wolfenstein engine. Quake 1 to later quake engines all evolutions and used in a lot of other games. These are all 3d engines. On the nes, snes and sega machines the same platform engine was reused in 1000s of games. Same for sound engines, physics engines etc. My point is. I dont think there is a lot of difference. Innovation still happening today. Not everything is Unreal. reply Arrath 12 hours agorootparentprevHonestly it seems like it always has: there are a handful of dev houses using their own engine for a spread of games (e.g. EA with Frostbite, Ubi with Anvil, Rockstar with RAGE, Bungie with whatever they call the Halo/Destiny engine these days), then UE or Unity are out there mass licensed for a whole bunch of stuff, then the few less widely licensed engines like Source. reply lmm 10 hours agorootparentThat's not how it always has been. Licensing engines was virtually unknown 30 years ago (and when it happened at all it was within a very narrow range for making games in the same genre, more like asset swaps and level packs than outright new games), and new and exciting in the '00s. reply Arrath 10 hours agorootparentYeah sorry I meant from, like, 2000-ish and on. reply lostlogin 9 hours agorootparentprevYou’ve reminded me of Return to Castle Wolfenstein. It was very limited in terms of maps and possible multiplayer only. It was fantastic. reply Keyframe 11 hours agorootparentprevid Tech engines, Renderware and a few other big ones were for sure available at the time and used. Earlier, during 90's was another situation however. reply MisterTea 11 hours agorootparentprevid sold a few Doom and a bunch of Quake 1/2/3 licenses back in the day. Off the top of my head: Heretic and Hexen used the Doom engine, their sequels used the Quake and Quake 2 engines respectively. Strife was an FPS RPG that was Doom based. Half-Life started out as a HEAVILY modified Quake engine and rumor has it that there is still a bit of Quake code in Source. Duke Nukem Forever started out on Quake before moving to Unreal. reply rasz 2 hours agorootparentI heard an interview about Doom port to hmm 3DO? and it cost them one time payment of $50K cash money to outright buy Doom assets + game engine license. reply partiallypro 9 hours agorootparentprevCall of Duty and Medal of Honor both used id Tech 3 (aka the Quake 3 engine), as did some of the Star Wars games, etc. Unfortunately, The Source Engine came out around the time of id Tech 4, and it really took the reins, with Unreal Engine hitting a stride thereafter. https://en.wikipedia.org/wiki/Id_Tech_3#Games_using_a_propri... reply partiallypro 9 hours agorootparentprevThe Quake 3 engine was incredibly popular in the early 2000s. reply Unfrozen0688 12 hours agoparentprevA lot of games did this. Like each Final Fantasy game on the PS2 is a new engine. Each Resident Evil. There was Renderware I guess to share liek Unreal. But that's it. Not like these new game devs. I can really tell these new games are so unoptimized. I mean look at this. Is this progress?? Not really. And 2024 has just waaaaay worse performance. https://www.youtube.com/watch?v=E5fr1R8kZzI reply xandrius 41 minutes agorootparentAnd that's because most people don't really care about performance, just that it works. The most important thing is that something exists not that it's at its peak performance. reply Unfrozen0688 21 minutes agorootparentThats sad. Anyone can feel a difference in fps. MOST people DO care. Show them side by side. So yea. Shit devs. I feel we are going backwards in quality. Like wtf happened. Look at the video. Just shit. A game from 2015 and 2024 looks the same. reply withinrafael 13 hours agoprevAndrew initially had a rough disassembly-only dump of the retail Area 51 on GitHub in a (quickly abandoned) attempt to remaster it. He must have gotten lucky here, or maybe more likely a previous developer tossed him a (very nice) bone. reply MisterTea 11 hours agoparent> or maybe more likely a previous developer tossed him a (very nice) bone. This is what I am thinking. It seems a long shot the source code was simply discovered at a garage sale by chance. Too much coincidence that a. the THQ dev still had the code just kicking around and b. a computer savvy person happened upon this garage sale, buys said artifact and discovers the code. Stars must have been aligned. I think the THQ dev likely knew the \"buyer\" or someone put them in touch and the cover story is \"oops, I accidentally sold the source at a garage sale\" in case lawyers get involved. reply westhanover 8 hours agorootparentI like this thinking. It could trigger the discovery of a bunch of cool stuff at garage sales. reply accrual 15 hours agoprev> It was found at a garage sale of a former THQ developer. This would be interesting to know more about. Was it on a CD, or maybe left on the disk of an old PC? I wonder how much old source code is hanging out in developer's attics and basements around the world, before git/online repos were a thing. reply ryandrake 13 hours agoparentThere's probably a lot lying around. Back in the early 2000s, or maybe late 1990s, I was a big fan of a PC game that ended up being a total commercial failure for the studio and publisher. It was so bad the studio went under, and the game was left with many bugs unpatched. On a lark, I (and a few other fans) got together an reached out via E-mail to the publisher and offered to take a stab at the code and fix some bugs for free. To our shock they sent us the source code on CDROM. We never actually got very far (the source code was a colossal mess) but decades ago, wild shit like this probably happened more than you'd think. I have no idea where those CDs ended up, probably tossed them at some point. reply burningChrome 10 hours agorootparentThe early aughts were like the Wild West in game development. I remember almost taking a job with a smaller agency trying to build their own game engine and similar game to GTA but with a sci-fi twist. Same thing. Company went under for a myriad of reasons. I should really write a blog post about it, some if was just so fantastical to believe. In the end, the devs scattered in all directions but my buddy took all the source code with him and tried to get a workable version going and release it on an independent label he started just for this game and project. I ran into him a few years back and asked him about it and he said he still had the CD's laying around in his basement in storage. He said every few months, he would try and take a stab at it, but it was just too much for one person to handle and too many years had passed were it just wasn't worth it any more. It would be interesting to see how many people had similar stories. reply kloch 17 hours agoprevIs the source for the original 1995 version available? At a retro arcade it is the one game you can never play because there is always a line (harder to get a spot than Tron). I remember playing it on pc around 1997 also. reply rolph 15 hours agoparenthttps://www.retrostic.com/roms/mame/area-51-41277 http://adb.arcadeitalia.net/dettaglio_mame.php?game_name=are... reply joezydeco 11 hours agoparentprevThe funny part is that the original arcade title, cabinet and kit, was probably the best selling Atari Jaguar title ever published. The arcade hardware was CoJag, a version of Jaguar modified to run with a hard drive and have JAMMA-compatible signal output. https://www.system16.com/hardware.php?id=778 reply johnbellone 16 hours agoparentprevThis is the 2005 game released on Xbox and PS2. reply jstarfish 14 hours agoparentprevHeh. That was one of the few games I could pump $20 into, hit start for both players and get most of the way through akimbo-style. I tried it again later with House of the Dead 3. It didn't work as well. Harder to dual-wield shotguns. reply mynameisnoone 1 hour agoprevNeeds a write-up about when, where, and how the media was found. reply mattw2121 15 hours agoprevSo many questions...did the developer realize he was putting it up for sale? Did he just have a bin full of a bunch of old CDRs? How did the person rummaging even know what they came across? Presumably the buyer then took this to the developer to purchase. Did the developer know realize what he was selling? reply asveikau 15 hours agoprevA find like this will get more interesting in the future when the artifacts are from the era of git. Imagine you find a single developer's git clone, and suddenly you have the full source history of the project. reply fabiensanglard 15 hours agoprevHopefully someone will write a source code review to explain how this all worked! reply jim90 16 hours agoprevWonder how long until takedown notice (considering all the confidential files in that repo..!) reply edifus 16 hours agoparentIt's been up for three years now.. reply toast0 16 hours agoparentprevProbably a while, ownership is likely unclear, given the number of corporate transactions Midway has been through. reply bsimpson 16 hours agorootparentI thought Midway became WB Games when the Viacom family fucked up their financing. reply gamacodre 12 hours agorootparentGeez, these companies just keep getting passed around and around. I was working for Time Warner Interactive (aka Tengen, the consumer arm of Atari Games) when Midway bought them in '96. reply ocdtrekkie 7 hours agorootparentprevThe bigger question is if the company who legally owns the right to the code realizes it's their IP, and then, of course, if they care. Not every game publisher has a Nintendo-level concern for their archaic titles, and some publishing houses may own hundreds of franchises technically they have zero intent to ever touch again and may not even have bothered to keep track are theirs. reply MaxBarraclough 13 hours agoprevUploading the source code to GitHub no doubt constitutes copyright infringement. I imagine GitHub will take it down if they're asked to, it seems rather clear-cut. edit I see another comment pointing out this has been up for around 3 years now. reply skiman10 16 hours agoprev [–] Every time someone finds \"lost media\", I always wonder how much media is lost to time because the wrong people went to the garage sale and did not know how rare a find was and it gets thrown away at the end of the sale. reply daniel_reetz 13 hours agoparent [–] Something I've seen personally is that family calls a cleaner/buyer in to sweep the home of a deceased relative and all is lost. In short, there is no rummage sale. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The unofficial release of the Area 51 (2005) video game source code is aimed at preserving this nostalgic cult classic from the early 2000s.",
      "The source code snapshot from March 2005 contains the Entropy engine, game logic, and cross-platform targets, seeking contributors for compiling, debugging, and potentially porting the game to current systems.",
      "Those interested can participate by forking the repository and engaging with the community on platforms like Discord and GitHub Discussions to assist in the restoration project."
    ],
    "commentSummary": [
      "The source code for the 2005 game \"Area 51\" by Midway Studios was discovered at a garage sale and shared on Github, with David Duchovny and Marilyn Manson as voice actors.",
      "The discussion delves into comparisons with other game engines, the reuse of engines in early FPS games, optimization and performance concerns in newer games, and the ongoing innovation in game development.",
      "Finding old source code like this reflects the earlier era of game development, with discussions on copyright issues and the value of discovering lost media."
    ],
    "points": 176,
    "commentCount": 63,
    "retryCount": 0,
    "time": 1709820473
  }
]
