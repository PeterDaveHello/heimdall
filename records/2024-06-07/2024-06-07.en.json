[
  {
    "id": 40607442,
    "title": "Professionals Advised to Cancel Adobe Over Trust Concerns with NDAs",
    "originLink": "https://twitter.com/Stretchedwiener/status/1798153619285708909",
    "originBody": "Here it is. If you are a professional, if you are under NDA with your clients, if you are a creative, a lawyer, a doctor or anyone who works with proprietary files - it is time to cancel Adobe, delete all the apps and programs. Adobe can not be trusted. pic.twitter.com/LFnBbDKWLC— Wetterschneider (@Stretchedwiener) June 5, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=40607442",
    "commentBody": "Cancel Adobe if you are a creative under NDA with your clients (twitter.com/stretchedwiener)529 points by wanderingmind 7 hours agohidepastfavorite308 comments tartaruz 7 hours agoTo avoid a cancellation fee with Adobe, follow these steps: 1. Change Your Current Plan: First, switch your existing Adobe plan to a different one. This is because Adobe offers a 14-day cancellation period for any new plan. 2. Cancel the New Plan: Wait until the next day, then cancel this new plan. Due to the 14-day cancellation policy, you should be able to do this without incurring any fees. Explanation: Adobe's policy allows you to cancel a plan within 14 days of purchase without a penalty. By switching to a new plan and then cancelling it within this window, you effectively bypass any cancellation fees that would apply to your original plan. reply dmd 5 hours agoparentI've done this with hotel stays. No fee to change a reservation, but large penalty to cancel with less than 48 hours notice. So you change your reservation for a week later, then cancel it. reply ASalazarMX 1 hour agorootparentThis sounds like that story of the elder withdrawing $20 at the bank: Elder: I want to withdraw $20 from my account. Teller: You can withdraw $500 minimum here, or use the ATM for small change. Elder: I'll withdraw $500 then. Teller: Here you go. Anything else? Elder: I want to deposit $480 in my account. reply Gud 3 hours agorootparentprevSmart move and thanks for this! reply winternett 2 hours agoparentprevAdobe is the new comcast... Cancellation fees? There is also no security in using any other tool, especially with this predatory precedent, and especially if you're designing in a web browser. My best option is to use a copy of CS6 on a computer I keep offline, really all the content aware and other features issued since CS6 aren't that necessary for me, and by going old school, it sets my artistry apart from everyone else. It's long overdue for people to resist and reject the price/service manipulation fetish model that is exhibited by many major software companies. These companies shouldn't be saddling consumers with higher and higher prices and monthly subscriptions, and cancellation fees... Prices should be going down over time, not up as products mature. There is no longer an \"older software\" sale bin at the computer software store for people that don't have volumes of wealth to spend, and that's pretty crazy to think about... And then on the flip side arbitrarily and illegally sunsetting older software whenever they wish. These companies are not even respecting the EULAs they've issued prior, and scraping older promises off the Internet over time. Fraudulent exploitative software buy/update/service schemes, even in long-trusted companies, are becoming rampant, it's likely profit desperation, but also highly predatory in all the wrong ways. reply ozzcer 4 hours agoparentprevIf they change the terms and conditions surely you're entitled to cancel your plan without a fee? reply talkin 1 hour agorootparentSure. We all know that’d right. Everybody at Adobe knows too. We also all know that they have the best dark patterns and just-not-too-sketchy business practices. Everybody at Adobe knows too. Good luck with the fight. reply whatevaa 3 hours agorootparentprevEven if you don't right now, you should be able to. reply rpastuszak 4 hours agoparentprevThe alternative I tried (successfully): 1. Switch your payment method to PayPal. 2. Cancel the subscription using the Paypal UI (That was a couple of years ago so I’m not sure if this works) reply skilled 7 hours agoprevAdobe response: https://blog.adobe.com/en/publish/2024/06/06/clarification-a... https://news.ycombinator.com/item?id=40606768 Previous: Photoshop ToS grants Adobe access to user projects for 'content moderation' - https://news.ycombinator.com/item?id=40591860 - (86 comments) reply j16sdiz 7 hours agoparentTheir response is basically \"we won't abuse it\". Even if we trust them, this response is not good enough for any project under NDA. Unless I can, somehow, put Adobe under the NDA, it's not helping. Never mention most of us don't actually trust them to be not abusing. reply qarl 6 hours agorootparentAnyone remember when Instagram changed their TOS and then said \"trust us\"? And now they're training AI on it. Nobody's falling for it, Adobe. reply makomk 5 hours agorootparentprevAll of the stuff that they're saying in that response about how they won't abuse it is stuff that's already in the TOS. If you don't trust them despite that, then you should not be using their products regardless of what the TOS says. reply berniedurfee 2 hours agorootparentprevEvery time I read corpo-speak like the voice in my head is Joe Isuzu. reply macNchz 6 hours agoparentprevRight out of the gate the phrasing “pushed a routine re-acceptance of those terms” reads with a hint of user contempt, like “just click Accept damnit, it’s normal that there are updates, you’re not supposed to read them”. Ultimately what I’m getting from the terms directly and their response is that they want to be able to review things that go through their cloud, including with humans manually looking at it, but have given themselves a broader general license to review anything you open or create with their software, with a “trust us” message on the side saying it’s just for specific purposes. Additionally since all software these days seems to want you to save everything to the cloud, you’re likely to be prompted many times to save files there, where they may be subject to manual review even under the “trust us” purposes. reply chii 6 hours agorootparentit is in their interest to make the permissions in the TOS they grant themselves as broad as possible, since it's free to them (at least, they assume people don't care). It's good that this is getting attention, because the change supposedly already have been put in since Feb 2024! It's only recently that the UI exposed it with the accept screen! I say that the laws should be changed such that TOS cannot actually be changed from the time the contract was signed, unless a new commercial contract be signed for the new TOS. reply estebarb 6 hours agoparentprevHow it works for people doing handling material that obviously violates ToS? It would be expected for police investigators, war crime prosecutors, etc? Imagine that a crime ends up impune because adobe decides to delete part of the proofs! reply 42lux 2 hours agorootparentThere is a special version of Photoshop like the ltsc versions of windows but they are not as easily accessible. They come without a subscription though. reply berniedurfee 2 hours agoparentprevThe use weasel terms like “such as” and “may include” are great big red flags. “Your vehicle purchase may include items such as a steering wheel and brakes.” reply lelandfe 7 hours agoparentprev> [Human or automated review] [a]ccess is needed for Adobe applications and services to perform the functions they are designed and used for (such as opening and editing files for the user or creating thumbnails or a preview for sharing). You dummies, it's needed! They have to access our data, it's needed! N.b. this is one of the worse examples of corporate writing I've seen in recent memory. I feel like this entire thing is gaslighting the reader and using corpo jargon to make the thing easier to swallow. reply lelandfe 3 hours agorootparentSelf-replying since the sibling is getting buried. What they say: > Access is needed for Adobe applications and services to perform the functions they are designed and used for (such as opening and editing files for the user or creating thumbnails or a preview for sharing). What you read: > A tautology: in order to open a file on your computer for use in an Adobe app, that app will need to open a file (\"access\") on your computer. What they actually mean: > Adobe can review (\"access\") any data you put into their apps and services. For instance, if you open a file in Photoshop, Adobe can have a human or an automated process review that file. This behavior is needed. A huge part of this is the awful business-y passive tone they are using to disguise intent: \"access is needed\" - access by whom, and to what? Needed why? reply ryandrake 2 hours agorootparentI think one of the fundamental problems here is that the line has become blurred between \"Company can access user's data\" and \"Company's application can access user's data.\" In the past, this was pretty clear. The application is running on my computer, unconnected to the outside world, running on my behalf, in order to do what I want it to do. When I ran Borland's C++ compiler on DOS, I would have never even considered \"Borland\" having access to my data. These days, with every device connected to the Internet, companies are deliberately blurring the line between the application and the company that made it. If my iPhone accesses my location, is Apple accessing my location? Who knows? Some people think no, some people think yes. If my Windows PC takes a screenshot of my desktop, is Microsoft accessing the contents of my desktop? No clue. They say they aren't, but all they can offer for proof is \"Trust me, bro.\" I'm so tired of software that is so hopelessly intertwined and tethered to the software's vendor. When I buy a software from someone, I buy it in order to use it, by itself, with no \"help\" from the vendor. I don't want a relationship with my software vendor. I just want to use the software alone. reply jf22 7 hours agorootparentprevCan you describe how Adobe would generate a thumbnail preview without accessing your content? reply lelandfe 6 hours agorootparentThe corpo jargon has gotten to you! \"Access\" doesn't mean \"the app accesses a file on your computer.\" It means the company has access to your data. You would be forgiven for thinking this, as I assume it was their intent that you be confused. The phrase refers to 2.2 \"Our Access to Your Content\" - https://www.adobe.com/legal/terms.html Anyway, I have utmost faith that an app can open and edit files without calling home. reply tsukikage 6 hours agorootparentprevCan you explain how showing my content to a human employed by an Adobe subsidiary helps with thumbnail preview generation? reply HWR_14 7 hours agorootparentprevLocally, on my machine. reply hnbad 7 hours agorootparentprevWell, considering how GitHub's ToS was defended as \"but they need it to host your code\" and was what ended up as their justification for feeding all of your code into Copilot, I don't think that argument aged well. They could specify a minimal set of permissions they need from you to offer their service but they deliberately worded it as ambiguously as possible to allow them to change their mind later. The question isn't what they need it for, the question is what they can justify later with the language they use now. reply ImPostingOnHN 2 hours agorootparentprevThe quote is, > Access is needed for Adobe applications and services to perform the functions they are designed and used for. ...which boils down to, 'we need access to your stuff to do what we want'... which is not very reassuring. The thumbnail example was just 1 example. Another example would be an adobe service designed for training AI on your content. As long as the service designed for training AI on your content only accesses and uses your data for training AI on your content, it would be in compliance with these terms. reply mavhc 7 hours agoparentprevnext [3 more] [flagged] wtfwhateven 7 hours agorootparentYou should read the ToS everyone is talking about before claiming it's a \"trivial matter\" (it isn't) reply mavhc 6 hours agorootparentWhat TOS doesn't say \"we need to ability to show you your own images\" in stupid legal terms that sounds like they're going to steal all your stuff? And the law insists that all cloud data is scanned for CSAM reply fidotron 7 hours agoprevConsidering the news of Nintendo tracing leaks to Google contractors viewing their private YouTube this seems like a total mess waiting to happen. For those that have not worked near big advertising launches for physical products these NDAs are taken very seriously. The simple reason is leaks profoundly impact the value of inventory. reply PawgerZ 5 hours agoparentNintendo/Google news: https://www.404media.co/google-leak-reveals-thousands-of-pri... Unblocked: https://archive.is/RXEjf reply alanning 4 hours agorootparentGoogle’s efforts there seem laudable. They have an internal db for tracking issues that employees identify, resolve them promptly (according to the notes in the db and their response to the article authors), and generally seem to be taking the issues reported seriously. I have more trust for google after reading that, which is not what I expected reply vmfunction 7 hours agoparentprevI will never every trust real content to be on Cloud of any of these big corp, or any software going towards an online like photoshop is going. USB is way better than online for privacy and security. reply qwertox 5 hours agoprevI have an Android app which I developed just for me. I use it to record bike rides, see weather forecasts, see who called on the landline and the like. It's not published nor uploaded to Google Play or elsewhere, because it is just for me. When I upgraded it today to a new version, my Pixel phone told me that I have to upload the app to Google, so that they can security-check it. Previously there was an option where you could select something like \"don't check\". It's somehow the same as with this Adobe issue. Google now has a copy of my app even though I just installed my app on my phone. If now a Google employee installs the app on a device to \"check\" it, he will be able to see my power consumption, see who called, listen to voice messages, and so on. reply jhanschoo 5 hours agoparentAre you referring to this: https://support.google.com/android/answer/2812853 If so, do the directions mentioned there help? The situation you are describing is not morally the same. (edit: striking through this part ~~Adobe's desire for their customers' content is to use it to develop AI tools that Adobe can then resell to its customers.~~) The security checkup you are talking about is intended to combat a real problem that unsophisticated smartphone users are victim to. Adobe's request to access your content is not based out of intending to safeguard unsophisticated users. reply schlipity 4 hours agorootparentImagine if Microsoft had to sign off on every single program you write for your own windows box in the name of privacy and safety. It certainly does feel like we're moving in that direction, really. reply nicce 4 hours agorootparentMicrosoft already does something closely similar with Defender. For years. All your files are uploaded to Microsoft cloud automatically, if the file checksum is not in the Defenders database. It is even very difficult to disable. It always gets back on after a while, unless you do some serious PowerShell magic. https://medium.com/sensorfu/how-my-application-ran-away-and-... reply Kerbonut 3 hours agoparentprev> he will be able to see my power consumption, see who called, listen to voice messages, and so on So is your API key baked into the app? Implement a login screen or a field for the API key and then who cares if Google has a copy of your app, they can't access your data. reply qwertox 2 hours agorootparentThere is no API key. The URLs are not public, not guessable, and traffic runs over HTTPS. Any attempt at accessing a wrongly guessed URL bans the IP address. I have no intention to rewrite this stuff. You need to have the app to get to know the URLs. reply jpc0 2 hours agorootparentRandomly generated url (even by a bad random generator like a human) is equivalent to an API key. Your exact same argument you made about the url can be made about a baked in API key. Implement auth, do extremely basic auth if you have to using http basic auth over https and check against a hardcoded value on the server, it's not secure by any stretch of the imagination but is better than giving \"api keys\" to any intermediary that can do a string dump on your APK... reply kohbo 5 hours agoparentprevI have multiple personal apps that I've side loaded on my Pixel Fold. This hasn't come up for me. Are you sure there isn't more to this story? reply qwertox 5 hours agorootparentAbsolutely sure. It's this \"Google Play Protect\"-feature. I have multiple apps, and they used to trigger randomly these scan requests on upgrades for maybe two years, but with today's install it appears that they have changed how they deal with this, where you no longer have a choice. Maybe it's because it has extensive permissions wich I need in order to get access to my data usage, but that's not the point. Then again, it also happens with apps which don't require many permissions. reply gchadwick 7 hours agoprevThe devil really is in the details here, a tweet further down the thread quotes the new terms of use: https://x.com/Stretchedwiener/status/1798390688830402802 > Solely for the purposes of operating or improving the Services and Software, you grant us a non-exclusive, worldwide, royalty-free, sublicensable, license, to use, reproduce, publicly display, display, distribute, modify, create derivate works based on, publicly perform, and translate the content (typed from the image, may have some mistakes!) Which to be seems astounding! Especially for professional software such as photoshop where many many of its users will be working on behalf of clients and this clause would likely breach agreements with those clients and for other users making content for themselves they understandably don't want to allow Adobe to 'publicly display' their work. reply BiteCode_dev 7 hours agoparentYou can only grant something you have the rights to, so basically, they are asking you to do something you can't. I don't have the rights of my client's product. I want to see how it plays in court. reply kuschku 7 hours agorootparentIf I'm signing a contract to sell you something I don't have, e.g., the Brooklyn Bridge, then I'm liable. If you sign a contract to license something to Adobe that you don't have the rights to, you're now at fault. Either get a good liability insurance, or avoid using Adobe. reply HWR_14 7 hours agorootparent> Either get a good liability insurance, or avoid using Adobe Or get the client to direct you to use Adobe even with this clause. Easiest solution is to get the client and Adobe to agree to what rights each of them get. reply bell-cot 6 hours agorootparentTHIS. And the moment that the client expresses reservations about this strategy (which your approach to the subject was trying to elicit), you start talking about the alternatives to continued use of Adobe. reply bustling-noose 7 hours agorootparentprevYou granting someone rights to material you don’t have makes you liable twice right? If your friend lent you his car and you lent it to someone else, you become liable twice not zero times. reply jeroenhd 7 hours agorootparentprevYou don't have the right to upload your client's product, yet you use software that uploads your client's product. You're correct that the two conflict, but I think the courts would have a problem with you for using a product that breaches the employment contract you signed rather than rule that Adobe is at fault here. reply imglorp 6 hours agorootparentprevEveryone should edit some copyrighted works in private. If Adobe goes on to publish them publicly, they will be seen in court pronto. Poison the well? reply dathinab 5 hours agorootparentprevand holding your data hostage until you do agree reply tzs 3 hours agoparentprevIt's been that way for the Service as far back as Archive.org has archived Adobe's general terms of service which was 2015-05-31. Here was the TOS then: > 3.2 Licenses to Your Content in Order to Operate the Services. We require certain licenses from you to your content to operate and enable the Services. When you upload content to the Services, you grant us a non-exclusive, worldwide, royalty-free, sub-licensable, and transferrable license to use, reproduce, publicly display, distribute, modify (so as to better showcase your content, for example), publicly perform, and translate the content as needed in response to user driven actions (such as when you choose to store privately or share your content with others). This license is only for the purpose of operating or improving the Services. They changed from \"Services\" to \"Services and Software\" in June of 2018: > 4.3 Licenses to Your Content in Order to Operate the Services and Software. We require certain licenses from you to your Content in order to operate and enable the Services and Software. When you upload Content to the Services and Software, you grant us a nonexclusive, worldwide, royalty-free, sublicensable, and transferable license to use, reproduce, publicly display, distribute, modify (so as to better showcase your Content, for example), publicly perform, and translate the Content as needed in response to user driven actions (such as when you choose to privately store or share your Content with others). This license is only for the purpose of operating or improving the Services and Software. reply belter 6 hours agoparentprevIt seems that the business world has traversed four distinct eras, even though various companies appear to be in different stages of each: The Era of Volume Sales: Customer was just a number, and the primary goal was to maximize profits. Sears and so on... Required Relationships but Poor Service: Businesses recognized the need for customer relationships but failed to deliver effective customer service. Like in monopolistic AT&T, where customer service was notoriously bad... Stellar Customer Service At Least As An Idea: Businesses began focusing on exceeding customer expectations with exceptional service. Early Amazon, Zappos and so on New Era of Customer Exploitation: Companies move to a stage where they exploit their customers by overstepping privacy provisions or signed agreements, driven by the desire to maximize ad revenue or leverage data for AI. Facebook...Adobe...examples so many there is even no need to add more... reply red_trumpet 7 hours agoparentprevMakes me wonder if section 4.2 (the part you quote) was part of the update or if it has been there for before? Adobe's official communication [1,2] does not seem to indicate 4.2 changed with this update? [1] The popup in OP only mentions sections 2.2, 4.1, 5.2 and 14.1 [2] https://blog.adobe.com/en/publish/2024/06/06/clarification-a... reply Buttons840 6 hours agorootparentA lot of these services have draconian terms, but nobody reads them. It's kind of funny that an innocent change in one part of the agreement might prompt people to read the whole agreement and realize there are outrageous parts that have been there forever. One agreement I do read carefully every time is employment contracts, and it always makes me extremely angry, but what am I going to do? My family needs to eat. I sometimes complain about certain terms to co-workers and they almost always comment that their contract didn't have those terms, but I bet it does and they just didn't read it. reply MissTake 4 hours agorootparentbut I bet it does and they just didn't read it Actually, in my case, it’s true. I had three terms removed from my contract that I didn’t like - initialed by both me and our CFO/Legal Counsel, and further annotated at the end of the contract before we both signed. She told me at the time that I was the first person to ask for this in the 15 years she’d been there. Way back in the 1990s in the UK I refused to sign a contract that had terms I saw as being illegal (pertaining to holiday/PTO). The three directors finally agreed to remove the terms (after wasting 2 months on it) the same day I turned in my notice to start a new job paying £6K more without dumb requirements in the contract! reply lotsofpulp 5 hours agorootparentprev> but what am I going to do? While employed, looking for a different buyer of your labor at preferable terms. During the negotiation process, cross out anything you disagree with. reply szundi 7 hours agoparentprevI'm really interested wether this will blow up in their faces or not, they have some power over their subjects - oh sorry - users. reply temporallobe 6 hours agoparentprevIANAL, but it seems to me that this is meaningless, vague, non-enforceable legal dribble, probably designed as some kind of catch-all for internal purposes (indeed, it even says it’s for “operating or improving the Services or Software”). Obviously Adobe would still be subject to copyright infringement laws if they were to ever actually use a client’s work for profit. This TOS does not exempt a corporation from an entire class of laws designed to protect copyright holders. reply qarl 6 hours agorootparentYes, that's exactly what Instagram argued when they changed their TOS, too. And now they're training AI on your images. Never trust a corporation when it says \"trust us\". reply Buttons840 6 hours agorootparentprevIANAL. See the part about \"grant license\". This is all about copyright. Legalese seems to be a \"choose your own adventure\"--or rather, \"choose your own permutation\"--that results in many simple statements. One of those simple statements would be: \"For the purposes of improving the software, you grant us license to create derivative works based on your content.\" reply temporallobe 4 hours agorootparentI get what you’re saying, but consider that I could take someone’s work of art and create infinite derivatives of it, which is not illegal, unless I try to claim them as my own and sell them. reply thombat 4 hours agorootparentAt least in the USA it seems you need permission for derivative works regardless of your intent in making them: https://www.law.cornell.edu/wex/derivative_work reply marcosdumay 6 hours agoparentprevWell, if you want to call that a \"detail\", then yes. A corporation taking possession of everything you create does not feel like a \"detail\" to me. reply tossandthrow 6 hours agoparentprevSeems like this is a language change for their Ai work. reply knorker 7 hours agoparentprevAlmost all the instances of people getting upset by language like that it's because they refuse to understand that it's required for you the customer being able to share the photo with others. Why sublicense? Well, because the way they create the service may involve a third party CDN. In this case though, the limiting part where this license will ONLY be used to do what you the customer intended (e.g. share or publish the photo on their platform) is missing. And that's suspicious. reply dathinab 5 hours agorootparentbut it didn't matter weather they have a reasonable use case for it it's a gross overreach they could abuse at any point in time without any additional consent from you one which can force you to grant them permissions you legally are not allowed to do weather they had no intention to to abuse it really is irrelevant as long as they didn't legally constrain themselves further in ways they don't do furthermore given Adobes post actions assuming they have no intention to abuse it and will not start having such intentions in the future is IMHO highly foolish with a large degree of worldly innocence reply tsukikage 7 hours agoparentprev...so, AI training. reply miniBill 6 hours agoprevThe main issue with \"Cancel Adobe\" is that it's very naïve to assume that a nontrivial percentage of people using it can realistically switch to something else. reply crazygringo 5 hours agoparentSeriously. It's losing many weeks of your life to retrain in alternatives to Premiere, Illustrator, Photoshop, After Effects. If not months. Not to mention it's often simply not a choice at all, because you have to collaborate with people who only work in those file formats. reply berniedurfee 2 hours agorootparentNot to mention there realistically aren’t any alternatives. For what it’s worth, their products are unfortunately still best in class in most categories. reply jerf 5 hours agoparentprevIf you limit your actions in life to only \"the things that cause me zero inconvenience right now\", you will be played like a fiddle your entire life. Literally everyone who wants to control in any way for any reason knows this one simple trick, and I don't just mean the big things, but the little things like marketing tricks, cell phone contracts, etc. If you get used to the need to occasionally take a short-term hit for a long-term win, it gets easier over time. It's a good thing to practice, in all senses of the term. If everyone actually did switch, the options would become better through the increased resources the alternatives would get. reply nuclearsugar 4 hours agoparentprevPlus there really is no replacement for After Effects. Also I have 14 years of prior projects that I sometimes need to reopen, so canceling just isn't an option. reply dathinab 5 hours agoparentprevyes but also by today there are a lot of good alternatives many more and much more mature ones then in the past reply bufferoverflow 3 hours agorootparentThere's no alternative to Photoshop. And if you're going to say GIMP or Krita, you've never seriously worked with Photoshop. reply berniedurfee 2 hours agorootparentSame with Lightroom. I still use v4 and it’s still better than anything else available today. I try all the alternatives annually and nothing has as comprehensive and complete a toolset. reply spacebacon 7 hours agoprevI have only a sneaking suspicion they wrote this in specifically for training ai models with files stored on adobe cloud. reply surfingdino 5 hours agoparentIt's for this already present feature that they want to expand to other products and that's why they need even more content to repurpose https://www.adobe.com/uk/products/photoshop/generative-fill.... You are literally paying them to steal your work and the content of your clients. reply flycaliguy 4 hours agorootparentHere is my sort of contrarian take looking at this as a freelance designer who pays the bills with Adobe tools. Im happy to prosper from the work of other commercial artists while feeding the pool. I’ve used a combination of Bing Image Generator and Adobe’s tools to streamline a bunch of work flows and a lot of the pain points in my day have been smoothed over and made delightful. For me these AI tools are like an ergonomic upgrade, literally saving my eye sight. Regarding sensitive files, I don’t have any hardcore NDA clients but I guess I’d wait for their legal team to care and carry on. Isn’t Windows itself compromised this much? reply surfingdino 4 hours agorootparentIt's not for you or Adobe to assume the right to use other people's property without explicit permission. Adobe forces users to give it a blanket permission and profit from the properties that aren't even theirs. We might dispense with the rule of law at this point. reply montjoy 6 hours agoparentprevThat was my first thought too. I looked at the Firefly product and it seemed to say they don’t use your personal data. They have other “AI” products through like GenStudio that’s “coming soon” and I don’t see any “ethical ai” section in its description. I bet this is a “test the waters” EULA and we’ll see them pull it back in the next few days if not hours. If they do pull it back I wonder if they get to keep all of the content of the people who clicked yes within the time window of the onerous EULA? reply daemin 6 hours agoparentprevNot only for training their models on any content opened by Adobe products, but it would also cover any content generated by their models, since the models would implicitly have a licence to use the content they were trained with. reply Kelteseth 7 hours agoprevAnd you will get screwed a second time, because you have to pay the cancellation fee. > Should you cancel after 14 days, you'll be charged a lump sum amount of 50% of your remaining contract obligation and your service will continue until the end of that month's billing period. reply yellow_lead 7 hours agoparentYou can get around this by switching your subscription (free of charge), then canceling. reply dathinab 5 hours agoparentprevI think your theoretically have the right for a free out of bad cancellation if a provider changes their TOS in a way where it's reasonable to argue it affects the usability of the product for you. At least in the EU. reply jerf 5 hours agorootparentCommon law countries in general as well. You're not supposed to be able to rewrite a contract arbitrarily, nor can you simply write into a contract that you can rewrite it at will. In common law, that theoretically makes it not a contract in the first place. For that reason, if you raise the right kind of fuss and raise it to their legal team, the legal team will almost certainly let you out for free rather than run the risk of getting their contract invalidated to any degree in a court of law. I speak here generally in such countries, not just for Adobe. However, this is theory. The rule of law is generally declining in the West. Your mileage may vary in any specific attempt. reply immibis 7 hours agoparentprevLawyer up! They unilaterally changed the contract, that probably invalidates that clause in any sane country (so, not USA). They might even have to pay you the cancellation fee, or more. reply bluGill 7 hours agorootparentIt may even in the us, but a lawyer will cost you more than paying the fee. Though there is a class action in the us if enough people cancle - the lawyers get the cancle fee and you get $.25 reply gradientsrneat 4 hours agorootparentAdobe's fine print contains a Binding Arbitration Agreement and Class Action Waiver, which would prevent class action in the U.S. if you didn't follow the instructions to opt out. Similar clauses, some without opt-outs, have proliferated widely throughout the tech industry. Most notably, Windows added them to most of their products shortly before the release of Windows 8. Interestingly, GitHub does not have these clauses yet. reply ghufran_syed 7 hours agorootparentprevyou can almost certainly do a chargeback on your credit card, then they have to come after you, and their legal costs will hugely outweigh any payment they are seeking reply bluGill 6 hours agorootparentCheck your credit card terms of service carefully. Cards do have consumer protection, but generally it isn't about this type of situation. If you owe the fee then it is fraud to do a chargeback and you can get into legal trouble (though odds are as the other poster said: they will just send you to collections and it will hit your credit report). I agree that this type of change to contract terms is not acceptable, but unfortunately you need a lawyer to fight it. reply ceejayoz 7 hours agorootparentprevNo, they’ll just sell the debt for peanuts to a collection agency, who will ruin your credit. reply immibis 2 hours agorootparentprevIn some countries, the loser pays for both lawyers (and there are rules about how much money that can be). reply belter 7 hours agorootparentprev> Lawyer up! Did their previous contract said they could unilaterally change the contract? :-) reply hajile 6 hours agorootparentThat kind of clause won't stand up in court and they know it. reply belter 5 hours agorootparenthttps://www.adobe.com/legal/terms.html \"1.5 Updates to Terms... ...We may make changes to the Terms from time to time, and if we do, we will notify you by revising the date at the top of the Terms and, in some cases, we may provide you with additional notice. Adobe will not make changes that have the effect of imposing additional fees or charges without providing additional notice. Any such changes will not apply to any dispute between you and Adobe arising prior to the date on which we posted the revised Terms incorporating such changes, or when the Terms otherwise become effective. You should look at the Terms regularly...\" \"...Unless otherwise noted, the amended Terms will be effective immediately, and your continued use of our Services and Software confirm your acceptance of the changes. If you do not agree to the amended Terms, you must stop using our Services and Software and, if applicable, cancel your subscription...\" reply hajile 5 hours agorootparentYou can add anything you want to a contract, but that doesn't mean the courts will uphold it. reply belter 5 hours agorootparentHow did the contract made it through Adobe Legal Dept? reply philistine 4 hours agorootparentI’m trying to free your mind, Neo. But I can only show you the door. You’re the one that has to walk through it. reply belter 4 hours agorootparentOk ...give me the pill... reply immibis 2 hours agorootparentprevIt doesn't have to be court-enforceable. It just has to be scary enough that the customer will self-enforce. Usually the worst thing that happens if you put something invalid in a contract is that it's as if you didn't write it. Which is fine because that's what you would have done anyway. reply immibis 6 hours agorootparentprevIt doesn't matter, because in sane countries, even when such clauses aren't outright illegal, you still have the right to refuse the change and cancel the contract without penalty. I am not sure exactly which countries are sane. There's all kinds of law about what you can and can't put in a contract. You can't just write whatever you want and then hold people to it. Why do you think they always have a clause that says that if part of the contract is found to be invalid, the rest still applies? reply freedomben 6 hours agoprevFor anybody switching from Photoshop to GIMP, there are tons of different resources to help. There are books[1], video courses[2] (especially on Udemy), blogs, and just about anything else you can imagine. GIMP isn't a full replacement, but I do know several people who have made the switch. I bought \"The Book of GIMP: A Complete Guide to Nearly Everything\" and I didn't personally like it, but many people do. [1]: https://www.gimp.org/books/ [2]: https://www.udemy.com/courses/search/?src=ukw&q=gimp reply misswaterfairy 4 hours agoparentAnd if you need a commercial alternative, for professional shops that need/require it, Affinity Photo is very good. Serif only offers perpetual, per major version licensing (i.e. no subscription) and offers a 'universal' licence if you use multiple platforms (Windows, Mac, iOS). https://affinity.serif.com/en-gb/photo/ reply weberer 5 hours agoparentprevPeople coming from Photoshop would probably prefer Krita, which is also FOSS. reply freedomben 5 hours agorootparentYes thanks, that's a great point. I haven't had good luck with Krita personally, but I also haven't put hardly any time into it. I've heard very good things about it so I really need to give it some dedicated time. reply bbatha 4 hours agorootparentprevThey serve different workloads. Other than GIMP Affinity is also a great alternative, not FOSS, but buy it for life with an incredible price that includes feature updates. reply bluesounddirect 7 hours agoprevHow is gimp now ? https://www.gimp.org/ While I am not a professional artist, gimp has served me well in this role for many years . reply Lariscus 7 hours agoparentI replaced Gimp with Krita. It is often said that Krita is made for digital painting rather than photo editing but I haven't missed any features so far. reply Aerroon 7 hours agorootparentFrom my experience Krita didn't feel much better than GIMP. Stuff like the eraser being hidden under brushes is just weird compared to my previous experience. reply hellcow 7 hours agorootparentIf you're doing digital painting, it's probably just what you're used to. I disliked Krita until I spent a few weeks learning it. Once I found the \"Krita way\" of doing things, it was actually a much better experience than Photoshop offered for digital painting. I'd recommend finding a YouTube video where an artist takes you through how they use Krita to see some of the workflow and most common shortcuts. reply raverbashing 7 hours agorootparentprevKrita is made for artists, not as a demo of a widget library (I'm half serious) reply Aissen 4 hours agorootparentIt's the other way around. The widget library was built for Gimp, then took a life of its own. Note that Gimp still has not released a version ported to GTK 3 (but the port is done since last year, in the 3.0 branch), released 13 years ago. GTK 4 has been released 4 years ago. reply acomjean 7 hours agoparentprevI started using GIMP again. I used color adjustment and creating images using 5-7 layers with layer masks and not normal blending. It worked well. I always find the program not quite intuitive initially but once I get it, it’s not bad. For sorting and basic photography edits I use Dark table. Still getting used to it. I showed a couple of those GIMP created pieces and people seemed to like them. My theory is people look at the final image and don’t care much how you got there. reply oniony 6 hours agoparentprevI'm a casual user, and GIMP is totally usable and powerful, but some of the UI/UX choices are a little bizarre. For example: * Open a document, draw a selection box to copy a piece out and then close this document: it asks you if you want to save because the selection is actually part of the document. (I understand that if you had a complicated selection you might want to preserve it, but if it's a simple rectangle I think the default should probably be to assume you don't want it.) * Pasting an image into a document: it does this weird thing where it doesn't create a layer automatically and you have to decide each time whether you want a layer or to collapse the weird pseudo-layer thing down onto the existing layer. I'd rather it just created a layer. (Maybe there's a setting for this, I haven't looked too hard.) * I find I end up fighting with the multipurpose resize/skew/rotate tool. I can't remember exactly why, but it's a little unintuitive. reply pawelmurias 7 hours agoparentprevGimp is horrible. It's existance propably has a negative value on society by discouraging kids from pursuing art when they try it. reply dathinab 5 hours agorootparentI wouldn't agree with the first two parts of your sentence. But definitely with the last part. reply bravetraveler 7 hours agoparentprevJust a disclaimer: I haven't ever edited for anything serious. I'd be fine with an open 'paint' from Windows That said, I haven't noticed much change with GIMP over the years. It's... fine. The timeless one. Other/newer editors are around though. I hear good things about Krita reply stephen_g 6 hours agoparentprevI wanted to like Gimp, but I bought Pixelmator Pro on my Mac because I just couldn’t… Hard to describe, it’s just clunky if you know the professional tools well. But plenty of people do use it, so your mileage may vary. I hope the rough edges are polished down over time, but not at the moment for me. reply jddj 7 hours agoparentprevGive me a few days/weeks each time to get used to the at times perfectly backwards UI and it's great reply telesilla 7 hours agoparentprevI'm happy with Photopea, I'm a casual user of photpshop and this is a fine enough alternative for what I need to do (no extra plugins etc). reply keiferski 7 hours agoparentprevGimp really ought to rebrand myself. I mean come on, who wants to use a program with that name? Even IMP is a hundred times better and lends itself to some impish branding assets. reply dathinab 5 hours agorootparentfor non native English speakers this often isn't an issue as the English word gimp is rarely used, so there is a good chance to more associate it with the software then the meaning behind the word. reply keiferski 5 hours agorootparentIt’s a terrible name, period. A substandard piece of software already has enough issues with marketing itself. Naming it with a word that means “cripple”, no matter how often it is used, is just such a poor decision that it’s not really surprising that no one cares about the app. reply dathinab 5 hours agoparentprevit really depends on what your needs are, it can be anything from okay but inconvenient to switch (different UX) to not competitive at reply zbrozek 7 hours agoparentprevIt's powerful and the UI is atrocious. I'm an occasional user and there's nothing I can do in under ten minutes of futzing or seeking tutorials. reply 4ggr0 6 hours agorootparentExact reason why I love Paint.NET[0] Has all the features I need and you can install plugins, and I think that it's very intuitive. Used it to refine DALL-E pictures for my hobby-project. I work on a MacBook and Paint.NET is windows-only. There's a Windows VM on my MacBook with one single reason for its existence, I'll let you guess what that is. [0]https://getpaint.net/ reply Archelaos 7 hours agorootparentprevThis is also my experience as an occasional user. What I cannot explain is that even after more than two decades, no better open source alternative with a comparable range of functions has emerged. (Krita comes close, though.) reply pessimizer 2 hours agorootparentprev> I'm an occasional user and there's nothing I can do in under ten minutes of futzing or seeking tutorials. This is true of occasional users of Photoshop, too. If you don't know how to use Photoshop, its UI is an atrocity. reply Gigachad 7 hours agorootparentprevI’ve encountered plenty of things you just flat out can not do in gimp. The latest one is you can’t highlight text. Something incredibly basic that other programs have had since the 90s. reply lewiscollard 5 hours agorootparenthttps://imgz.org/i8CEMAJq.png Create a transparent layer. Paint on it in a colour of your choice. Set layer mode to \"darken only\" or \"lighten only\" depending on your background. \"Difference\" is fun too. I've been using GIMP for longer than I care to remember and I am painfully aware of what is not great in its UI, which is something that seems to have gotten _worse_ over time. This isn't one of them; to paraphrase, that's just knowing how to use basic features that GIMP and other programs have had since the 1990s. reply Gigachad 4 hours agorootparentThat’s not what I’m talking about. I wanted the same effect as you get on MS Word when you highlight text. Creating a coloured background behind the text. It’s a pretty common effect on posters and similar things. The best you can do in gimp is fill the entire text box background, but that doesn’t look good at all on multi line text where you want it to fit each line properly. reply wizzwizz4 7 hours agorootparentprevYou can highlight text in GIMP: (1) Create new layer from visible. (2) Select the text using the text tool. (3) Set zoom to 100%. (4) Screenshot. (5) Paste screenshot. (6) Floating layer to new layer. (7) Align screenshot layer with the new-from-visible layer. (8) Difference layer mode. (9) If the selection did not fit on screen, duplicate the new-from-visible layer. (10) Merge down. (11) Select the selection box with the fuzzy select tool. (You may need to delete the regions of the merged layer that represent the GIMP UI to do this.) (12) Select → Remove Holes. (13) Invert selection. (14) Delete. (15) Invert selection. (16) Delete. (17) Fill (with a block colour of your choice). (18) Hide the layer, then goto step 2 until all of the selection is accounted for. (19) Merge all the block colour layers together. (20) Re-order the block colour layer under the text layer. (21) Reduce opacity to taste. See! It's theoretically possible to assign the desired pixel values using GIMP, therefore GIMP is perfect and has no problems at all. In fact, you can automate this with a very simple combination of AutoHotKey and Script-Fu (passing control data from Script-Fu to AutoHotKey using PixelGetColor), which is practically as good as having it built-in. (More seriously: you can probably do gimp-vectors-new-from-text-layer, segment it into glyphs, take only the glyphs within a selection, split those into lines, find bounding boxes for those lines, and fill them with the current foreground colour, but there appears to be no way to query the current text selection from Script-Fu, so you'd have to use this with the Lasso selection tool or something.) reply ghufran_syed 7 hours agorootparentprevso maybe gimp + a gpt model? reply das_keyboard 7 hours agorootparentThis might be a pretty big idea. Since GIMP is highly scriptable afaik it should be possible to have an AI do all the work. But as always... you would need a lot of training data. reply gavinhoward 5 hours agoprevI've said it before [1], and I will say it again: This is why the industry needs to professionalize, with a code of ethics and regulations so that we can tell our bosses no when they tell us to do this stuff. https://gavinhoward.com/2023/11/how-to-fund-foss-save-it-fro... And our ethics must include safeguarding customer data from everyone, including ourselves. And this is why: https://gavinhoward.com/2023/11/your-loved-ones-are-prisoner... [1]: https://news.ycombinator.com/item?id=40603025 reply giraffe_lady 59 minutes agoparentTraditional engineers have a code of ethics and it prohibits them from misleading their clients, requires they take certain precautions to prevent unexpected harms. But it does not prevent them from making things intended to harm. They may still design bombs and warplanes, may make guns and bullets for their clients to sell, with no constraints on the final use to which they are applied. An engineering code of ethics would require us to avoid inadvertent data leaks, but would not constrain us from developing spyware, or programs intended to steal the work of users and profit from it. For that we need a moral position from which to say that these things are wrong, and courage to refuse to do them. We will not find precedent for this in traditional engineering, which has always refused to make this stand. reply keiferski 7 hours agoprevIt’s unclear to me if this policy applies only to files that you’re hosting with their cloud service - in which case I think (?) the scanning of those files is no different from any other cloud file hosting service, which I assume also scan files to make sure you aren’t hosting something egregiously illegal. Of course, this problem doesn’t exist at all if you just sell offline desktop software, but alas, the monthly SAAS business model has eaten everything else. reply WillAdams 7 hours agoparentWell, there is Serif's suite: https://affinity.serif.com/en-us/designer/ (There's also a Photo and page layout app) or the open-source stuff: - https://krita.org/en/ - https://inkscape.org/ - https://www.scribus.net/ reply ptero 7 hours agoparentprevIALAL, but the content from the Adobe clarification posted earlier tells it applies to all files created with their software, regardless of whether you host them in the cloud or on your own computer. reply throw46365 6 hours agorootparentPart of this, I think, is the bleed-in of cloud services even to work stored entirely offline: their AI-enhanced tools send content out to the cloud. For example you likely cannot implement content-aware fill in the cloud without sending the content to the cloud. reply hskalin 6 hours agorootparentprevSoftware like these should have no right to demand internet access. Subscriptions for using software that resides fully on your computer is such a huge scam reply davidmurdoch 5 hours agorootparentPhotoshop runs it's AI features \"in the cloud\", not locally. reply keiferski 7 hours agorootparentprevIf that’s the case, then yeah this is a serious issue worth the outrage. reply sunaookami 6 hours agoparentprevIt's just for stuff you store in the cloud (obviously) reply srameshc 7 hours agoprevSide question: how good are Affinity Designer & Photo as a replacement for Adobe tools ? reply gyanreyer 7 hours agoparentI really really like Affinity. I'm not a pro but they offer all of the tools I would possibly need in a much better designed package compared to open source alternatives like gimp. Adobe has more bells and whistles like AI, but I don't need that. And I'm very happy to support their no-subscription model, it's very refreshing. reply fidotron 7 hours agoparentprevNot close. They’re utterly inconsistent and glitchy which makes using them in a hurry a total nightmare. The corel stuff was better. Adobe have successfully built a moat around particularly Photoshop that no one seems likely to get close to crossing. reply Daub 6 hours agorootparentCounter-opinion.... I use both the Adobe suite and afinity. I love affinity for the interoperability of it's apps (which has to be experiencd to belive) and it's comprehensive support of RAW. At home I use mostly Affinity, at work it's Adobe all the way. Photoshop is unique for the many professions it supports: web designers, graphic designers, photographers, digital painters etc. If you are looking for a PS replacement, first ask what you are going to use it for. Other apps may be the more natural choice. God is in the detail. I would hesitate to use Affinity photo for digital painting simply because there is no slick way to change the flow rate of brushes. Recent changes have improved this, but not to the point where it is comfortable to use. reply depingus 5 hours agorootparentKrita is excellent for digital painting. https://krita.org reply coldcode 6 hours agorootparentprevI used Affinity Photo in my generative art; I also own Photoshop and use it for a few things, but it's rare. AP is faster at many things that matter to me, the UI is much cleaner, and it suits me just fine despite occasional issues (mostly memory related, their memory management sucks). Many of Photoshop's AI \"features\" are useless to me since they can only do tiny images (basically the usual 1MP that AI can manage). PS is a giant pile of features, but the UI is often a giant pile of complexity because of it. reply misswaterfairy 4 hours agorootparentprevI use Affinity Photo and Designer professionally and at home. Affinity Photo is not Photoshop. That said, it's a great image editor in and of itself. Making direct comparisons I feel is unfair, yes it is glitchy and can, and does, have inconsistent UI behaviour though is only occasional in my experience. But for US$70 per major version, perpetual (no subscription), for the small team behind it, compared to the Adobe behemoth behind Photoshop, it's pretty damn good. As of writing (2024-06-08), Serif's entire suite is 50% off at the moment, which makes Affinity Photo US$35 perpetual, for (in my opinion) 80% of Photoshop's capabilities, when Photoshop itself is US$22.99 per month (individual subscription). Designer blows Inkscape out of the water. Yes, I know I'm comparing a commercial package to an open source package, though for professionals who don't have time to troubleshoot bugs in open source software, let alone time to write their own patches, Inkscape sadly doesn't cut it here. For what its worth, I am a huge advocate of open source software, though I'm also a professional that needs my tools to work now, not in six months when someone gets around to helping/assisting/writing a patch. Downsides for Designer though, it does lack some features, like image tracing, though it's not a deal breaker for US$70 perpetual, for a user interface that's intuitive (sorry Inkscape), and that spits out SVG images that don't cause problems in other consuming applications, or SVGs that fail to display correctly. Coincidentally I do use Inkscape for image tracing though migrate over to Designer after that. As much as it sucks, the only way Serif will get close to knocking Photoshop, or Illustrator, or InDesign, off the 'industry-standard' pedestal is if we all support them by kicking in a few dollars for a licence, even the cheaper iOS tools, which are more-or-less on-par with their desktop counterparts. Likewise for open-source software: they'd easily start winning over people with a better UI design (again, in my opinion, most FOSS software in this space feels unpolished) and perhaps commercial sponsorship like Blender has been able to partner. Before version 3.0, Blender's UI sucked and drove a lot of people away. It's now easy and intuitive, if overwhelming with the number of features it has, attracting a lot of newbies and kids wanting to learn 3D. Blender (there are some really big names here!) - https://fund.blender.org/ Inkscape - https://inkscape.org/*sponsors/ GIMP (Web Archive as GIMP's website is having cache issues right now) - https://web.archive.org/web/20240603082716/https://www.gimp.... (I have no financial interest in Serif at all, apart from owning licences for their software.) reply kranke155 7 hours agorootparentprevNot just Photoshop. There is literally nothing but After Effects in the 2D Motion Design world. reply depingus 5 hours agorootparentHow about DaVinci Resolve from BlackMagic? https://www.blackmagicdesign.com/products/davinciresolve reply fidotron 6 hours agorootparentprevBlender will get there eventually, by which I mean in 10 years, but it will. They have the clear foundations going in place today. reply misswaterfairy 3 hours agorootparentprevWhat about Natron? https://natrongithub.github.io/ reply spaniard89277 6 hours agorootparentprevCavalry? That's what my GF uses. reply kranke155 6 hours agorootparentExtremely rare. I've used it but no agency in London would allow you to work with it, they need copies of the files for later. AE is just the standard. reply spaniard89277 6 hours agoparentprevMy GF uses Affinity tools and Cavalry for her freelance work. She told me she's happy with it. She made the change like 6 or more months ago. reply gizajob 7 hours agoparentprevTotally awesome. reply nefrix 7 hours agoprevI work a lot in Premiere / After Effects / Photoshop. For me it would be incredible difficult and time consuming to change my workflow. Wouldn’t be impossible, but I would need to spend lot of time learning new tools until I would get to the same work speed. Plus it would be much more expensive. Am I concerned regarding this claim? Not really. I think the chances of being hacked are higher then Adobe doing something nasty with any of the projects they might have access to. reply bustling-noose 7 hours agoparentHacked ? They are saying they can access your content (I’m assuming cloud though won’t be surprised if they also have remote access enabled by default) can be accessed by them anytime they want. Yes adobe can access the next pixar film being made when they desire to take a peek. reply nefrix 7 hours agorootparentI am sure when you work on projects for Pixar, they are more strict regarding this kind of things, but I am not in that position, so why to worry? reply albrewer 4 hours agorootparentFirst they came for the socialists, and I did not speak out—because I was not a socialist. Then they came for the trade unionists, and I did not speak out—because I was not a trade unionist. Then they came for the Jews, and I did not speak out—because I was not a Jew. Then they came for me—and there was no one left to speak for me. —Martin Niemöller reply baq 7 hours agoparentprevconsider the chances of you getting sued by your clients that you have signed an NDA with. the only realistic option is to seek an amendment to the agreement. reply janosdebugs 6 hours agorootparentNDAs are not the only problem. Adobe Acrobat is used to work on a whole host of documents that may be straight up illegal to share. reply bustling-noose 7 hours agorootparentprevThe agreement now should include such scenarios. But then again a good lawyer probably already knows incidences and probably has rock solid agreements that cover this. reply lambdaone 7 hours agoprevAt this point, if you are in the graphic design area, Adobe basically own you. There is really nothing that will substitute for Photoshop without a massive drop in productivity. GIMP is better that it was, but not yet good enough to replace Photoshop in the same way that Blender replaces Maya etc; perhaps this will drive support for GIMP to get better, but I'm not optimistic. reply nacs 7 hours agoparentEvery time someone brings up alternatives, I always recommend Krita. User friendly, cross platform, OSS. Far better than Gimp, especially in the UI side. reply j16sdiz 7 hours agorootparentIf krita work for you, that's great. However, Photoshop do lots more than painting and photo editing. If you use the vector, content filling or plug-in features on Photoshop, krita won't help you. reply nacs 5 hours agorootparentKrita has vector support too. In fact you can mix and match vector and raster layers. It also has plugins that do content filling like this (and they do it offline): https://github.com/Acly/krita-ai-diffusion But nothing is going to be a 100% replacement, I'm merely offering alternatives. reply gizajob 7 hours agoparentprevAffinity make a pretty decent clone of the main Adobe apps. Although they just got bought by Canva so may be at the shark-jumping point. reply masswerk 5 hours agorootparentThey just shipped an update with support for variable fonts. So, I guess, it's still much alive. reply spaniard89277 6 hours agoparentprevAs I said before, my GF works in design and she's been using Affinity and Cavalry for a while. She seems happy about it. reply dawnerd 6 hours agoparentprevHonestly a lot of creatives have managed to move away from Adobe. It's why they tried to buy Figma. And they'll try to buy probably every other alternative they can if it keeps you in their system. reply davidmurdoch 5 hours agoprevSounds like great motivation for someone to write some software that auto generates junk images, or even better, terrifying and horrific images, that then get uploaded to Adobe, in order to taint their data. reply troyvit 4 hours agoprevThe title of this piece recommends cancelling Adobe. If anybody has, what are they cancelling to? Our organization uses their audio tools among other things, and they're used to the polish and convenience they get from Adobe products. reply jpc0 1 hour agoparent> Our organization uses their audio tools among other things, and they're used to the polish and convenience they get from Adobe products. As a professional in audio with over a decade of experience... Adobe has ** audio tools. There are a lot of other tools that are significantly better, commercial ones, much older than Adobe's... Now for what Adobe is actually good at, which is not video nor audio, I'm not sure there is an alternative. I've heard things about Affinity but cannot tell you since all the people I know in graphics use Photoshop, illustrator, lightroom. reply tambourine_man 7 hours agoprevCanceling Adobe is like unplugging from the grid. Sure, it's perhaps possible depending on your particular reality, but it's terribly inconvenient. reply spaniard89277 6 hours agoparentI only know about this world through my gf, who uses Affinity tools and Cavalry. What's the biggest friction? reply tambourine_man 6 hours agorootparentBasic, essencial, decades-old features lacking. reply carlosjobim 6 hours agoparentprevIs it? I switched from to Affinity some time ago, after having used Photoshop for decades. So far everything has been fine, and it can still edit .psd files. reply ho_schi 7 hours agoprevI wonder why people suck things like Cloud, Co-Pilot or Adobe, WhatsApp or Windows? Let me guess. It seemed once convenient. This is the root of all evil. Don’t use it because it was pre-installed (Windows). Don’t keep using it because it was good before the Cloud (Photoshop). Don’t use it because others force you (WhatsApp). Literally all quick and convenient decisions are something we will regret. If you don’t like Gimp, Blender, Krita or Linux you need help to improve it. Look at the dumpster fire Azure. Nobodies data shall be stored there: https://www.zerodayinitiative.com/advisories/ZDI-24-581/ If the Chinese government isn’t right now reading your E-Mails somebody else does. Literally everything in Azure or any other Cloud must be considered compromised. And between the security issues the rise the prices. reply sofixa 7 hours agoparent> Don’t use it because others force you (WhatsApp). That's probably the worst example you could provide. The only utility of WhatsApp is because others use it. If you want to communicate with those others, that's your one and only option. You might have luck convincing some of your social contacts to move to a different chat tool, maybe, but there's around zero chance you'll be able to convince all of them. reply 4ggr0 6 hours agorootparentI was able to convince all of my friends and family to use Signal so that they can chat with me, the rest just uses SMS, Telegram, Threema or something else to communicate with me...haven't had WhatsApp since 2021. reply jasonvorhe 6 hours agoprevAdobe knows exactly how many people are dependent on their subscription products in order to make a living. reply coldcode 6 hours agoprevThis article has dropped off of HN while I was making a comment. reply nicce 5 hours agoparentWonder what would be the reason for that? It is clearly highly interesting one and not total disinformation. reply yodon 5 hours agorootparentIt disappeared because people disagree on whether to upvote or downvote comments, or other similar signals of disagreement. When the HN algorithm sees signals like comments receiving large numbers of upvotes and downvotes simultaneously, or long strings of rapid back-and-forth tit-for-tat comments between posters, or etc, it interprets those signals as \"this post is controversial.\" HN explicitly and intentionally downranks algorithmically identified controversial topics in order to reduce the odds commenters behave badly (which is disproportionately more likely in controversial topics). The HN algorithm is intentional about a primary goal of driving well behaved discussions, even knowing that means important controversial topics getting downranked. The fact that the disagreement here is likely centered around whether gimp is good or bad is irrelevant to the algorithm. It doesn't want people to get in the habit of behaving badly on HN, regardless of the topic. reply qarl 4 hours agorootparentSo an entity can censor HN by simply spamming the comments with disagreement. Cool. reply yodon 3 hours agorootparentOr just flag it, which is much easier and quicker and hardly news that the feature is there reply qarl 1 hour agorootparentCool. reply scotty79 53 minutes agoprevHow's that even legal. There should be laws about this. reply lebuffon 6 hours agoprevSounds like they want grist for the AI mill without paying for it. reply iroddis 7 hours agoprevI don’t use Adobe, but this feels like it’s missing some context around when / how the private data is accessed. Is it for AI? To satisfy law enforcement requests? For compression or some technical need? To make the most charitable case for Adobe, this could just be clarifying that there is no encryption at rest. It’s not clear if “may” means that the user is giving them permission when they agree to the T&C, or if there are other circumstances where they “may” be required to access the data. Uncharitable cases would be for AI training, life-ruining tech like automated scanning for illegal content that gets automatically forwarded to authorities, etc. reply t0bia_s 6 hours agoprevYou cannot even work offline. Only solution is cracked version to working with Adobe on field without internet connection. reply ale42 7 hours agoprevTo what do these conditions apply? To any content edited within Adobe software, or to content stored in Adobe cloud? reply ergonaught 7 hours agoparentIt's from their general terms. It does not clearly restrict itself to items stored on their servers. See... 2.2 Our Access to Your Content. 4.1 Content. “Content” means any text, information, communication, or material, such as audio files, video files, electronic documents, or images, that you upload, import into, embed for use by, or create using the Services and Software. 4.2 Licenses to Your Content. Solely for the purposes of operating or improving the Services and Software, you grant us a non-exclusive, worldwide, royalty-free sublicensable, license, to use, reproduce, publicly display, distribute, modify, create derivative works based on, publicly perform, and translate the Content. ...and so forth https://www.adobe.com/legal/terms.html They're unlikely to actually want to go out of business or spark major revisions to law, so they'll presumably \"clarify\". (As they've attempted to do I guess.) reply kome 5 hours agoprevStallman is right. reply gizajob 2 hours agoparentMetaphysically and morally, yeah. Getting the job done for things like photoshop replacements, no. reply wslh 6 hours agoprevI cannot believe the costly legal and compliance departments approve all of this. It seems more stupidity and not right due diligence, it is something that should concern stockholders as well. reply qarl 4 hours agoparentI once had a startup that did image processing in the cloud. Our lawyers recommended changing the TOS to allow us to use the imagery. I strenuously disagreed, and tried to get them to explain their rationale for such a change. They never really provided a convincing answer. (Literally, they argued that the company may someday accidentally use unlicensed imagery, and we want to protect ourselves from that liability. I suggested that the better policy should simply be more careful with our customer's data.) reply marcosdumay 5 hours agoparentprevHow do they approve the Windows's \"we can upload any data from your computer at any time\" terms? Corporation legal departments have been neglecting their jobs for a while now. reply wslh 4 hours agorootparentI understand that Microsoft is not doing it right now but it was an statement? reply marcosdumay 1 hour agorootparentIt's at their privacy policy for Win8 and 10. At the context, it's natural to assume they meant to talk about error reporting, but this is not stated anywhere. And it's been there for a while, and they never clarified this point... reply KyleOneill 7 hours agoprevAlready did and now learning Plasticity a good free alternative software. reply mrkeen 7 hours agoprevDidn't we as an industry collectively shrug at closed source code (which could be doing anything with our client's data). Then we shrugged again when we took all our client's data off-prem and onto the cloud. Then GDPR hit some of us, so we just sorta declared the cloud-providers to be GDPR-compliant and called it a day. What's gonna be different about this (justifiable) outrage? reply pjc50 7 hours agoprevNot really a choice. 99% of people in this position are going to hit Accept, because they have no choice, and their clients aren't going to complain, because they want work done in Adobe format and have no real alternative. This is going to continue until either there's a big copyright slapfight (you can't just copy Disney stuff and expect not to get sued), HIPAA, or DPA intervention. reply berniedurfee 2 hours agoprevWould it be legal to just “clone” Photoshop or Lightroom as a FOSS project? I mean create a nearly exact clone. Everything down to the layout, tools, menus, options, etc. Who would Adobe sue? Could they make it illegal to distribute? reply anArbitraryOne 7 hours agoprevI hate myself for paying for Adobe's bullshit but lightroom is so useful. reply adzm 6 hours agoparentit's okay, I think the dark lord will understand. there really is no comparison among other creative suites and tools. reply Hizonner 6 hours agoprevYou never should have been disclosing to Adobe anyway, even if they did pinkie promise not to look at it. reply smrtinsert 6 hours agoprevThe AI generation of software seems to be turning into the largest erosion of privacy ever. It's all fun and games until a malevolent political party decides it wants access to your Windows and Photoshop history. reply nalekberov 6 hours agoprevCan’t you just block all traffic to Adobe products with tools like Little Snitch? reply HumblyTossed 2 hours agoprevVote with your wallets creators. You either accept or find another software. reply vitiral 6 hours agoprevI guess it's time to amend contracts so that Big Tech Bro has rights to everything, no matter what reply Waterluvian 6 hours agoprevSo I’m guessing this isn’t even a moral issue anymore. If you have a legal obligation not to share the image data, you must not use Adobe software. reply surfingdino 6 hours agoparentSo you cannot do your job. I see AI getting rid of a lot of jobs already... reply Waterluvian 6 hours agorootparentI saw this problem in my domain and I imagine it happens everywhere: We were taught exclusively with ArcGIS in undergrad and then so many peers can’t even navigate QGIS because it lacks some window dressing. Schools are where these companies capture the industries. reply surfingdino 6 hours agorootparentThis is worse. Adobe is jeopardising legal status of creatives working on clients' assets. Adobe feels free to take whatever it wants, because it needs other people's content to offer \"generative fills\" in Photoshop. It's not their content and they have no legal leg to stand on, so they create T&Cs that incriminate their users should the work of one photographer show up in somebody else's photos... like nobody asked for this shit and yet they want their own users to pay the price of the mad chase to not be punished by the Wall St for not doing AI. reply panphora 7 hours agoprevIf you're planning to cancel, here's how to avoid the 50% cancellation fee: 1. Go to cancel your subscription. You will see a screen with the cancellation fee. Continue. 2. They will offer you a new plan to avoid the cancellation fee. Choose the cheapest one to switch to a new plan. 3. Here's the loophole: You can cancel any plan for free within two weeks. Cancel the new plan within this period and get your money back for the first month of the new plan. You've avoided the fee. Source: https://www.reddit.com/r/VideoEditing/comments/k9kh6v/how_to... reply dannyw 7 hours agoparentI don't understand how it's legal for a company to materially change their terms of service, but not allow you to cancel out of the contract if you disagree. reply Akronymus 6 hours agorootparentIn europe, afair, whenever a contract is changed to be worse for a party, that party has the right to cancel it right then and there without additional fees. reply olex 6 hours agorootparentIt's definitely the case in Germany for all insurances, electricity and other utilities, etc - they have long cancellation periods, but if the contract changes to your detriment (usually any sort of price increase), you're allowed to cancel immediately. It's even required that they remind you of that right in the same notification as the price increase. reply qwertox 6 hours agorootparent> they have long cancellation periods This used to be. Now you only have to fulfill your first (long) period, and afterwards you can cancel any month. If they change the terms during the first period, as you mention, you can cancel immediately. reply switch007 6 hours agorootparentprevThere has to be some individual country nuance there. EU legislation for B2B is very limited for example (article is about B2B) reply perihelions 6 hours agorootparentprevIn Europe companies are obligated to intercept and scan images anyway, so you can't escape Adobe's new spyware terms by switching vendors; they'll all have the same ToS provision. (i.e. \"for content review\") reply ttyyzz 6 hours agorootparentAre you talking about possible future regulations that have not yet come into force? Are you aware that many drafts are very unrealistic and will never come into force? reply tjoff 6 hours agorootparentprevNo. reply tossandthrow 6 hours agorootparentprevnext [4 more] [flagged] perihelions 6 hours agorootparentCan I ask you to ask things more civilly please? HN is not a classless insult place like Reddit. The EU regulation I'm referring to is this one, that's been trending on HN for many months now: https://news.ycombinator.com/item?id=40560391 (\"EU: Users who refuse scanning to be prevented from sharing photos and links (patrick-breyer.de)\"; 4 days ago, 313 comments) reply tossandthrow 6 hours agorootparent> Can I ask you to ask things more civilly please? HN is not a classless insult place like Reddit. Please, I completely responded in your tone. Also, there was no personal attack, I merely called out that what you wrote was not correct. Yes, trending. First, you don't refer to that, Secondly, as my sibling say, this is not in effect. All in all, your comment was indeed ignorant and did not even make a note to support why you would say that. I would also ask you to please keep that commentary on reddit. Edit: seems like I am not the only one who thinks you comment is indeed classless given the other answers and the very gray text color. And lastly, please don't abuse the flagging system. reply malermeister 6 hours agorootparentprevThis is just a draft though, right? Not actual legislation. So while still worrying, I think your post was inaccurate - it may be the case in the future, if things go poorly. But it isn't the case right now. reply guizzy 6 hours agorootparentprevTerms of service are full of terms that aren't legal in many jurisdiction, they're written brodadly and assuming that any ambiguity is in favor of the company so that they have the best possible position to argue from if anything has to be decided in court or arbitration. reply borski 6 hours agorootparentprevThe same is true in the US. The catch, of course, is that you’d have to sue if they don’t let you out, and most people don’t have the time, will, or means to do that. reply robryk 6 hours agorootparentWhy isn't this the model case for small claims courts? reply teeray 6 hours agorootparentYep, so after a few years of litigation and hours of time filling out forms on your state’s awful judicial portal (hopefully correctly!) you might stand a chance at a judgement. Then you can spend more time going back to court when they don’t pay, talking about time payment plans, etc. Maybe they’ll pay some of it, maybe not. Then they move to another state and you need to get your judgement domesticated there (with an in-person visit). Also, remember to keep renewing that judgement or it goes bye-bye! Remember that the goal of the defendant is to exhaust the plaintiff, and cause them to spend more time than the judgement is worth. This is how the small claims system works. reply tzs 5 hours agorootparent> Yep, so after a few years of litigation and hours of time filling out forms on your state’s awful judicial portal (hopefully correctly!) you might stand a chance at a judgement Do you have any examples of states where it takes hours of filling out forms for a small claims case? The ones I've seen have been pretty simple. California asks for plaintiff contact information, defendant contact information, how much money plaintiff is asking for and why the plaintiff believes they are owed that, the date that happened and how they calculated the amount, whether or not you've asked the defendant to pay you, a multiple choice question on which geographical aspect of the case takes place within the territory covered by the particular courthouse where you are filing, the zip code of the place that aspect of the case took place if you know it, whether or not the case is about an attorney-client fee dispute, if you are suing a public entity, if you've files more than 12 other small claims with the last 12 months in California, and if you claim is for more than $2500 and if is if you understand that you cannot file more than two small claims above $2500 in a single year. California's is one of the more involved ones that I've seen. Washington is pretty just much name the parties, how much you think you are owed, what it is owed for (checklist with categories like faulty workmanship, merchandise, rent, property damage, and a line to write in something else), and explanation of the reason for the claim. Texas' is even smaller than Washington's. reply borski 6 hours agorootparentprevIt is. And someone will certainly make it happen. But most people don’t even know where their local small claims courts are, much less how to make a case, and lawyers cost money, and that is where most people end the conversation. Yes, you can represent yourself and win, but it’s a lot of time, effort, and money (because time is money). Someone will do this, and it will be great precedent. But most people will simply accept it because they don’t have the time or means. reply meowster 6 hours agorootparentprevSounds like Class Action material then. How often does Adobe change their TOS? reply hajile 6 hours agorootparentAvoid class action and take them to small claims court or simply backcharge them and let your credit card handle it. Either way, they lose a lot more than a class action that will take years and get you nothing. reply tomrod 7 hours agorootparentprevI'm not sure it is, but then I am highly averse to monopolies. reply asmor 7 hours agorootparentprevCertainly isn't legal in many places, or at least entitles you to \"special cancellation\". reply EForEndeavour 7 hours agoparentprevImagine how much revenue Adobe must make from cancellation fees from people who don't discover this loophole. Quietly charging people to leave a subscription while a perfectly working (if ridiculous) workaround exists is a perfect example of an unethical business practice that exploits the incomplete information of customers. I don't know where I'm going with this. Just had to reflect on how we all agree to denominate value into these fake but useful units of dollars, only for transactions like cancellation fees to be so far removed from the generation of actual, tangible _value_ that it all starts to feel like beta-testing a computer game. reply daemin 6 hours agorootparentFrom what I read somewhere it's because the cheaper plan where you pay by the month is actually an annual plan where you pay in installments. So technically speaking you're not paying a monthly subscription, you're paying off a debt that you took on when you got the annual subscription. reply blcknight 7 hours agoparentprevI wish I knew this when I accidentally let my photoshop trial continue and had to pay to cancel. What a horrible business practice, I really wish the worst for them. reply op00to 6 hours agoparentprevThis is how I avoid hotel cancellation fees. Simply call and ask the reservation to be moved forward outside of the cancellation penalty window, and then cancel. reply martinbaun 6 hours agoparentprevwow there's a 50% cancellation fee? It's getting more and more ridiculously. Good way to step outside this though. reply hansoolo 7 hours agoparentprevAh damn, why did I never come up with this?! Thanks for posting! reply Simon_ORourke 6 hours agoparentprevWatch them swiftly close this loophole with a new end user agreement hastily pushed out after hours! reply portaouflop 6 hours agoparentprevHow the hell is a cancellation fee legal reply baxuz 6 hours agoparentprevOr simply issue a chargeback, or change your card to a virtual one without any funds on it. reply woodylondon 6 hours agorootparentWhen I tried this a couple of years ago with them - they sent debt collectors for the remaining balance (I am in the UK). reply hajile 6 hours agorootparentIn this case, you tell them to either settle for $0 or you sue for breach of contract. reply op00to 4 hours agorootparentWhy would a debt collector who bought the \"debt\" for pennies on the dollar care about a contract issue with someone who isn't them? reply 53 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A tweet by Wetterschneider advises professionals, particularly those under NDAs (Non-Disclosure Agreements) or dealing with proprietary files, to cancel Adobe services and delete its applications.",
      "The tweet claims that Adobe cannot be trusted, though it does not provide specific reasons or evidence for this assertion."
    ],
    "commentSummary": [
      "Forum users criticize Adobe's cancellation fees and extensive data access under new Terms of Service, sharing tactics to avoid fees and expressing privacy concerns.",
      "Comparisons to Google and Microsoft highlight a trend of vague and overreaching policies, with discussions on alternatives like GIMP, Krita, and Affinity, despite challenges in switching due to Adobe's dominance.",
      "Legal implications of Adobe's terms, including potential conflicts with client agreements and enforceability of contract clauses, are debated, emphasizing the ethical issues and the need for better open-source solutions."
    ],
    "points": 529,
    "commentCount": 308,
    "retryCount": 0,
    "time": 1717758722
  },
  {
    "id": 40605532,
    "title": "Ice Ice: Open Source Menu Bar Manager for macOS with Customization Features",
    "originLink": "https://github.com/jordanbaird/Ice",
    "originBody": "Ice Ice is a powerful menu bar management tool. While its primary function is hiding and showing menu bar items, it aims to cover a wide variety of additional features to make it one of the most versatile menu bar tools available. Note Ice is currently in active development. Some features have not yet been implemented. Download the latest release here and see the roadmap below for upcoming features. Usage Simply Command + drag your menu bar items to rearrange them. Install Manual Installation Download the \"Ice.zip\" file from the latest release and move the unzipped app into your Applications folder. Homebrew If you use Homebrew, you can install Ice using the following command: brew install jordanbaird-ice Features/Roadmap Menu bar item management Hide menu bar items \"Always Hidden\" menu bar section Show hidden items when hovering over the menu bar Show hidden items when an empty area in the menu bar is clicked Show hidden items by scrolling or swiping in the menu bar Automatically rehide Hide application menus when they overlap with shown menu bar items Drag and drop interface to arrange individual items Search menu bar items Display hidden items in a separate bar (e.g. for MacBooks with the notch) Custom spacing between items Profiles for menu bar layout Menu bar appearance Menu bar tint (solid and gradient) Menu bar shadow Menu bar border Custom menu bar shapes (rounded and/or split) Hotkeys Toggle individual menu bar sections Toggle application menus Show/hide section divider icons Temporarily show individual menu bar items Enable/disable auto rehide Other Launch at login Automatic updates Menu bar widgets Why the name? Because your menu bar becomes like ice, allowing your menu bar items to slide away 🧊🧊🧊 Why not support earlier operating systems? There are a number of system APIs that Ice uses that are only available starting with macOS 14. Gallery Choose your own icon to show in the menu bar Customize the menu bar's appearance License Ice is available under the MIT license.",
    "commentLink": "https://news.ycombinator.com/item?id=40605532",
    "commentBody": "Ice - open source bar manager for macOS (github.com/jordanbaird)518 points by saikatsg 13 hours agohidepastfavorite147 comments kstrauser 12 hours agoNot the submitter, but I installed this a couple days ago to replace Bartender. For those not following along at home, the Bartender author just sold it to some sketchy app mill. It’s an app that legitimately requires screen recording permissions to work. I trusted the old author’s reputation. I have zero trust in the own owner. Ice isn’t as good yet. It’s not nearly so configurable. For example, you can either hide all icons or none. You can’t decide to show some of them all the time. That’s a bummer. I’ll still happily accept that trade off to replace Bartender, which I’d previously purchased several versions of. It’s dead to me. Great job so far, Ice. I hope you keep improving! reply scottoreilly 6 hours agoparent> For example, you can either hide all icons or none. I thought so at first too, but it’s just a confusing UI. The documentation says: > Simply Command + drag your menu bar items to rearrange them. What isn’t said is that this dragging also allows you to keep some of the items permanently showing. Anything you drag to the right of that double arrow icon will stay there. reply oefrha 6 hours agorootparentGood thing it’s been added to README now, I was similarly stumped two days ago and only found out from the issue tracker: https://news.ycombinator.com/item?id=40580582 The instructions really should be in the app, though. reply kstrauser 4 hours agorootparentprevI swear I tried that. Guess not! In that case, it covers 95% of what I used in Bartender. reply latexr 6 hours agoparentprev> I trusted the old author’s reputation. I have zero trust in the own owner. As of now, I have zero trust in the old author too. The lack of communication is the fault of both parties. Ben Surtees has caused serious damage to their own reputation. Any new software released under that name will have a hard time gaining trust and traction. reply CodeWriter23 3 hours agorootparentProbably one of the terms of sale. But choosing to receive even more money above protecting the privacy of those who previously paid you money and trusted you is pretty dastardly IMO. reply latexr 2 hours agorootparentI would say that if it was one of the terms of the sale (we don’t know), that’s even worse. If a buyer makes that request, the seller should be even more conscious of “who the heck am I selling to?” reply carlosjobim 2 hours agorootparentprevAnd who are you to slander a named person and publicly question his reputation, for selling his own business that he created to somebody else? Your behaviour is not proportional to what has happened. It is an icon utility. reply latexr 1 hour agorootparent> And who are you A paying customer of Bartender. > to slander It is not slander to say you distrust someone. > a named person Ben’s name was already public. Bartender was published under “Surtees Studios”, after all. Ben has also appeared in at least one podcast. Trusting Bartender meant trusting Ben, the developer, the person. > for selling his own business that he created That’s not what’s in question. No one is blaming Ben for selling Bartender, the complaint is that it was done in secret. And it would’ve continue to be a secret were it not for a feature in MacUpdater that someone noticed. > It is an icon utility. No, it is not. It is an app that requests Screen Recording, Accessibility, and Location permissions to do its job. Thus it is an app which requires a high degree of trust. That the new owners added an analytics framework to, and who knows what else. This case is awfully similar to when a developer sells a browser extension to someone shady because the profit is in the user-base. reply kstrauser 1 hour agorootparentprev...that has screen recording privileges on its users' systems. It's not shady to sell a business. It's not shady to have screen recording privileges because that's legitimately needed to provide the functionality. It is shady to sell that product to some unknown group without notifying your customers. I trust(ed) Ben Surtees to run his software on my machine with enhanced privileges. I'm not thrilled that someone else controls it now, and that I'd probably never have known that if I didn't follow tech news closely. reply callalex 1 hour agorootparentprevWhich part of the statement is slander? reply draugadrotten 11 hours agoparentprevFrom the blog: I understand that the transition hasn’t been entirely smooth. Recently, there was a change in the signing certificate for the app, and unfortunately, this change wasn’t communicated properly to you, our loyal users. https://www.macbartender.com/b5blog/A-New-Chapter-for-Barten... previous discussion on the hidden sales: https://news.ycombinator.com/item?id=40584606 reply web3-is-a-scam 6 hours agoparentprevI didn’t know Bartender was sold. I thought it was fishy to need screen recording permissions but I assumed it was because of “security” features of the new macOS/m series chip/whatever so I just let it go because I trusted the Bartender dev. Uninstalled. What a shame. reply hbn 4 hours agorootparentIt does need screen recording permissions because trying to do what Bartender does on a closed down system like macOS is inevitably going to require a hack. The issue is we trusted the dev because we knew who he was and he'd have an angry mob after him if he was caught doing something sketchy. Now that it's sold to an unknown third party, there's no one to hold accountable for this software we're trusting with screen recording permissions on our machines. reply jmuguy 5 hours agorootparentprevIce (and other menu bar managers) require the same permissions - your assumption was correct and Bartender has needed those permissions for a while. reply c-hendricks 3 hours agorootparentHidden Bar doesn't, I'm guessing the screen recording permission is needed for showing on hover. reply ziml77 6 hours agorootparentprevBartender did need screen recording permissions for certain features to work under the more locked down security model in macOS 11+. That happened well before Bartender was sold. reply nicolas_t 11 hours agoparentprevArgh, I didn't realize that the app was sold and I paid for the upgrade recently :( If I had known I wouldn't have. I wonder if this could be a case for a chargeback, since the owner silently sold the app reply butchlugrod 3 hours agoparentprevReally satisfied with Ice. I've been a long-time Bartender user, but have thought that version 5 is a little slow and buggy. Ice does what I need it to do (hide/show icons) and it does it faster than Bartender. Not as many features, but my own needs are pretty simple. reply jnsie 3 hours agoparentprevThanks for flagging the sale of Bartender. As a long time user I had no idea and am saddened to hear the new. I'll be switching to Ice or another open source tool immediately. reply txdv 10 hours agoparentprevI just bought it a month ago for max support for a ridiculous price like 50eu :( reply lycopodiopsida 9 hours agorootparentAfter many years of updates I've also bought the mega supporter license this year, which I deeply regret - not because of the money lost but because it seems that author has misused our trust. reply omlitt 12 hours agoparentprevIt’s missing a lot, especially for a MacBook with a notch. I went with ibar until ice or any other app gets more features that bartender had. -Difficult to configure unless you don’t have a lot of apps in the top bar. -No way to scroll through the list. -Does not offer a way to show the extra icons in a bar that displays below. -No search functionality. This is something I used frequently. -Folders which were recently added to Bartender was a nice option. I do hope the developer is planning on adding more features. reply atmosx 10 hours agoparentprevThanks for the heads up. I use bartender, I will replace it with this app. reply kevsim 11 hours agoparentprevWhy do these apps require screen recording permissions to work? Just curious. reply swah 7 hours agorootparentTo run a bunch of \"hacks\" that make it work better than the alternatives, I guess: https://www.macbartender.com/Bartender5/PermissionInfo/ reply eviks 11 hours agorootparentprevTo get information about other apps' icons reply yig 1 hour agorootparentDozer [0] doesn't require any special permissions. I guess this is needed by Bartender because it creates a little pop-out menu that shows the icons somewhere else? [0] https://github.com/aonez/Dozer/releases/tag/v4.2.1-silicon reply SpaghettiCthulu 7 hours agorootparentprevIf true, that's a wild permissions model reply theshrike79 2 hours agorootparentYou wouldn't want your mouse driver, for example, being able to see _every_ piece of software in your menu bar by default without extra permissions? The case for \"I need to see what apps are in the menu bar\" is so rare there's no setting for it -> \"screen recording\" is the closest one. reply nativeit 3 hours agorootparentprevAmong many. Apple’s security model has a lot of ups and downs like this. I gather any app that needs access to anything on the screen outside of itself requires “screen recording” permission. I know all of the Remote Desktop apps, as well as color sampling functions in browsers, Adobe’s apps, and all of the designer tools with similar functionality require it. reply vmladenov 10 hours agoparentprevThanks for sharing this note, I completely missed this development. Sad to see what has become of this app. reply nobodywasishere 12 hours agoparentprevYou can always show some icons by moving them to the right of the open/close icon reply omlitt 12 hours agorootparentThis only works when you don’t have many icons. You can’t see ones that are hidden behind the notch. If you can, I missed this. reply nobodywasishere 12 hours agorootparentAh yeah that's a good point. Don't know if this covers that use-case yet reply frob 6 hours agoprevFor about 4 years now, I've used Dozer [0] to hide and organize menu items. It's been mature and stable the whole time. It had many of the features still on the development roadmap for Ice. You can drag-and-drop icons, hide and reveal with clicks or keyboards, and have two levels of hidability. [0] https://github.com/Mortennn/Dozer reply BChass 4 hours agoparentSame here. Only if the original author didn't abandoned the project. reply yig 1 hour agorootparentThere is an Apple Silicon build under Releases. It's work fine. It doesn't ask for extra permissions. reply lyall 12 hours agoprevI use Hidden Bar (https://github.com/dwarvesf/hidden) for this. It works well enough despite not being updated in a while, though there is a bit of jank. Just tried out Ice and it seems to be a nice, lower jank replacement! reply emsixteen 7 hours agoparentI use Hidden Bar too, it's done the job for me, but if there's a reason to hop to another I'm all ears. reply luuurker 1 hour agorootparentIf it works, then there's no reason to change. I stopped using Bartender (because of the recent news) and noticed that Hidden Bar was last updated in Jan 2022 and Dozer in 2019. Neither have a way (or plan to have something) to handle the notch. Ice still lacks some features, but the developer is working on it and eventually it will be able to work around the notch (if you have too many icons, they'll be hidden by the notch... not sure what Apple was thinking). That's why I picked Ice. reply healsdata 2 hours agorootparentprevIt's a minor nit, but I like that Ice doesn't force me to have extra icons in the bar. Unless I missed a setting, Hidden Bar required having \" and \"defaults -currentHost write -globalDomain NSStatusItemSpacing -int \". Needs restart/logout to apply. Personally I set both to 6. Works well if you have a static number of icons. reply tomphoolery 12 hours agoprevThis is awesome for folks who don't use their own custom status bar! Personally, I use SketchyBar and Yabai for window management. SketchyBar is shown almost all the time, but the status bar is set to hide unless I have my cursor at the top of my monitor. I like this because it's a good combo of having my \"status bar items\" all compacted on the right, with the useful information I want on the status bar at all times...such as battery level, current weather, volume, the date/time, as well as the focused window. For those who don't want to install a custom status bar, this seems like it would solve similar problems of the status bar getting way too big with all these programs running in the background. reply arvinsim 12 hours agoprevGlad that there are now open source options after Bartender was acquired. reply bdcravens 4 hours agoprevI see many people mentioning other alternatives to Bartender as well, so I'll point out again that if you already have a subscription to Parallels, their companion toolbox app (a suite of utilities) includes a similar app reply jingsam 12 hours agoprevHidden Bar is good enough https://github.com/dwarvesf/hidden reply TMWNN 12 hours agoparentI could never get Hidden Bar to work correctly. I think it's because my MacBook has a notch. iBar (free at the App Store) so far does what I need. reply sontek 5 hours agorootparentYou need https://apps.apple.com/us/app/say-no-to-notch/id1639306886?m... reply luuurker 1 hour agorootparentThat's an okay workaround, but you lose part of your screen and create a thick top bezel. As a laptop user, I want to use all available space, so what I do is to use https://topnotch.app/ (or a dark wallpaper) which makes the background of the bar black and until now I was using Bartender, which would display the hidden icons on a second bar under the original one when we clicked the icon. I barely noticed the notch. reply FinnKuhn 10 hours agorootparentprevMy MacBook Pro also has a notch and it is working fine so the notch isn't the problem reply TMWNN 9 hours agorootparentI'm sure I'm missing something obvious. I ran Hidden Bar for months and the hidden icons only became visible left of the notch after clicking > maybe twice, briefly, and I could never figure out what I did differently. iBar worked immediately. reply FinnKuhn 8 hours agorootparentHm, I don't have enough icons for them to be left of the notch so that might be why I haven't noticed any issues ;) reply benyafai 6 hours agoprevIf anyone is looking for alternatives, I've been using Vanilla for a while and have ben happy with it. https://matthewpalmer.net/vanilla/ reply luuurker 1 hour agoparentSeems to be a nice option, but the way it handles the notch... I think the second bar that Bartender has and Ice will eventually have[0] works better. For now, I'll support Ice because it's open source. I'll donate more than the $10 Vanilla Pro costs just to have an open source alternative. It's nice to have alternatives though. [0] https://github.com/jordanbaird/Ice/issues/1 reply robotmachine 2 hours agoparentprevSeconded. reply residualmind 10 hours agoprevAw, I thought it was for managing a bar. Supplies, shifts, etc.. reply SoftTalker 3 hours agoparentHaha I thought the same. reply mulhoon 12 hours agoprevSeems like one of those apps ripe for Sherlocking by Apple. Which would also solve screen recording privacy issues. reply egypturnash 3 hours agoparentApple's sort of already sherlocked this. The \"Control Center\" that consolidates a bunch of menu bar widgets into one big iOS-style dropdown full of controls solves the same problem of \"I have more crap in my menu bar than I have room for, especially now that my laptop has this stupid notch\". Except it's a private interface so absolutely zero of the programs that want to put stuff in your menu bar can offer up buttons in there. reply KronisLV 12 hours agoprevI got a MacBook for a freelance development project (also needed to build an iOS app) and I have to say that the OS itself feels pretty good. Very much a walled garden and normally perhaps outside of my budget (and the button order on the keyboard feels odd), but in general it's pretty smooth so far! Nice to see various customization options, BetterDisplay was also pretty useful when dealing with an external monitor (sadly I never figured out why there's no proper anti-aliasing on the external monitor). reply mastercheif 5 hours agoparentApple removed sub-pixel text rendering support in macOS 3-4 years ago. They wanted to merge their GPU driver stack across iOS and macOS when they moved to Apple Silicon. This has come with some benefits, like the lightning fast speed that Macs can enable external monitors. reply nottorp 2 hours agoparentprevYou're saying about what I said when I first got paid to do some mac software :) I bought a crappy used mac, used it to make the money, then thought it was immoral to not get a new laptop (it wasn't a big project but it paid like 3x the laptop, and i was moonlighting it so it was all bonus money). 15 years later i have more macs than x86 boxes in my home office... reply foxandmouse 2 hours agoprevIt is just me or has this been happening more and more with mac apps. The unarchiver, Transmition, uTorrent.. reply monocularvision 6 minutes agoparentDo you mean Transmission the open source BitTorrent app for macOS? What’s the problem with it? reply rovr138 1 hour agoparentprev>more and more It's been like 18 years since uTorrent... and that was Windows only, unless I'm crazy. reply alpb 1 hour agoprevIt should be a crime that Apple hasn't seen the popularity of tools like Bartender, which is major privacy violation to begin with, and developed their own feature for this. reply benreesman 11 hours agoprevThis observation is tangential at best, so don’t upvote it. But HN remains simply unparalleled in finding cool software that makes computers more fun day in, day out. My internal jury is out on whether chatbots ranked through an elaborate mechanism (Instruct) via what amounts to what I wish I had never built on P(click) feed ranking. But the Rust Terminal Renaissance? Nix? Flox? TailScale? Linear and Graphite and the golang community with things like fzf? That shit makes my life better and more fun daily. And HN is where to hear about it first short of being really, really good at GitHub. reply memset 7 hours agoprevAlso xbar: https://github.com/matryer/xbar reply xmdx 8 hours agoprevI had to install one of these tools recently because of the notch on newer macbooks, they hide icons that overflow and you never notice it. For a while I thought my apps were erroring and not opening properly. You have to manage it yourself with one of these tools otherwise they are lost to the void in my experience. reply jsomedon 12 hours agoprevI particularly like the explanation on naming it ice: > Because your menu bar becomes like ice, allowing your menu bar items to slide away reply walthamstow 12 hours agoparentSounds like GPT to me reply saghul 10 hours agoprevInteresting! Is there a way to center the content of the bar on the screen? The split is very cumbersome to use on an ultrawide display and I'd be happy if there was a way to put the content on the middle 3rd. reply hbn 4 hours agoparentIf it did it would have to be only for external displays because the laptops all have a notch there reply gregoriol 10 hours agoprevI still use the old Bartender 4, works perfectly on macOS 14.5 even with a notch If this tool gets the additional bar feature and some more customisations, it might be a good alternative reply deagle50 4 hours agoprevCan this shrink the gap between icons like bartender? reply ninjin-carh 12 hours agoprevI would love to try this but hesitant as I couldn't find a privacy policy. Can it be added or maybe I just couldn't find this? reply skrebbel 12 hours agoparentIt’s open source. A privacy policy implies that the author is willing to make themselves liable to broken promises. Most OSS authors work for free and do not want to put their ass on the line like that. The license says “you figure it out”: > THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. “Without warranty of any kind” also excludes any warranty wrt your privacy. reply drcongo 6 hours agoprevI switched to this yesterday and much prefer it already. Bartender 5 was extremely flaky for me, this just does what it's supposed to. reply geniium 10 hours agoprevThat looks very promising. Thanks for doing this. reply anileated 12 hours agoprevI am not the target audience for macOS menu bar managers like Ice. You can drag-reorder icons in pure macOS without any third-party software, and the additional options seem unnecessary. reply omnimus 12 hours agoparentWhy would anyone write comment like this? Like does it add anything to the discussion? Who cares that you dont use menubar apps so you dont need Bartender. reply freehorse 11 hours agorootparentBecause some people may just need this so it is good for them to know, in conjunction to this https://www.reddit.com/r/MacOS/comments/16lpfg5/hidden_prefe... > to change the spacing in the menubar and I found out it changes two preferences: ``` defaults -currentHost write -globalDomain NSStatusItemSpacing -int X defaults -currentHost write -globalDomain NSStatusItemSelectionPadding -int Y ``` > which can be reverted with ``` defaults -currentHost delete -globalDomain NSStatusItemSpacing defaults -currentHost delete -globalDomain NSStatusItemSelectionPadding ``` I learnt about these in a similar topic here. I do not need anything more from a bar app really, so I do not need to install an additional app for sth that I can just handle through a couple of preferences changes in the terminal. For people who do need more functionality, they can get one of these tools. reply anileated 3 hours agorootparentprevI opened the README and Usage section only showed how to reorder items, nothing else. This app is utterly unnecessary for that purpose. I thought, perhaps other macOS users are unaware of that, and my comment would help. reply Angostura 9 hours agoprevThe Usage section hopefully rather under-sells it: 'Simply Command + drag your menu bar items to rearrange them.' ... err yes, that's how MacOS works without add-ons reply russelg 8 hours agoparentJudging by the last thread about Bartender, when people were discussing alternatives, some people did not even know that was how you can rearrange your menu icons. So I can see why it's worth mentioning. Also worth mentioning that this behaviour is completely undiscoverable, just like most power user interactions on macOS. reply ars 12 hours agoprevI know some people like it, but I find the top menu bar on a mac extremely non ergonomic. The argument goes that it's an easy spot to hit: Just move your mouse to the top. But they forget that in order to use the menu bar you have to activate the app! In other OS's, if you want to pick an item from an inactive app, just click - it will activate the app, and pick the item from the menu bar at the same time. Not so with a mac. If you click on an active area of an inactive window on a mac, the click is eaten, and you have to click a second time to actually do what you are trying to do. Same with the menu bar, first you have to activate the app, then you can click a second time on the menu bar. I do not like it. reply jorvi 12 hours agoparentPeople like the menu bar because of the global menu - no more hunting and pecking for a print or save option, with every application using disparate or bespoke UI kits having those aforementioned options be different places both visually, physically and logically. MacBooks are absurdly overpowered as a layman’s computer, but the global menu means they only need to learn the OS once. reply jwells89 3 hours agorootparentIt also means that app devs can’t just eschew menus altogether in favor of junk drawer hamburger menus, like is so common on Windows/Linux. Even Electron apps that don’t have menubars under Windows/Linux usually do under macOS because it’d be silly to not populate the menubar when it’s always there anyway. The omnipresence of the menubar also makes it an excellent hook point for UI enhancement utilities (e.g. a keyboard-driven HUD app) and automation, since it provides a universal interface that covers most of an app’s functions, which is truly rare these days. reply eviks 10 hours agorootparentprevHow do you \"learn it once\" if the dev can put \"print\" option anywhere using UI kit? reply tracker1 4 hours agorootparentApple publishes UI guidance in terms of how/where common features are meant to be. If you drift from this, you will likely be told about it. Unlike Windows, where guidance has shifted over time, and even then, many would just ignore UI/UX standards altogether. Linux, I find to be in the middle.. applications meant for a given DE will usually come together relatively well, others less so. reply piva00 10 hours agorootparentprevI haven't found any apps on macOS the past 15 years where the \"print\" option wasn't where it's expected to be, would like an example just for the fun of it. reply eviks 9 hours agorootparentIt doesn't matter, this is not a property of the OS or the menubar location, just an unenforced convention that you can adhere to on Windows as well reply quesera 7 hours agorootparentIt matters. The menu location (and the keyboard shortcut) for Print has been in the Apple Human Interface Guidelines since the 1980s, and all Mac apps adhere to this convention. You might be able find an exception -- there is no approval process for HIG in Mac apps. But you will not find an exception (v1.0.1 or later) in an app that has any meaningful user population, or is of any meaningful quality. Ignoring HIG without good justification is a strong garbage signal. It was a surefire indicator of a crap Windows port back in the 1990s, but I don't think that's even a thing any more. reply eviks 7 hours agorootparentWindows also had ancient conventions, so what? > and all Mac apps adhere to the convention, or are laughed out of existence. so not all Mac apps adhere > there is no approval process indeed > Ignoring HIG without good justification Oh, but there is a very easy justification - most of these defaults are poorly (unergonomically) engineered based on design ignorance from the 80s reply quesera 7 hours agorootparentThe Mac app ecosystem considers HIG failures to be serious bugs, and they do not survive. So yes, for all meaningful definitions, \"all Mac apps adhere\". You do not appear to know what you're talking about here, and I don't know why you're bothering to argue. Show me an example of an even semi-popular Mac app which uses a different convention for Print, and we might have a conversation. Your excerpted quotes are misleading, but I'll respond to one point: > most of these defaults are poorly (unergonomically) engineered based on design ignorance from the 80s You just invented that problem. Show me a more ergonomic design for Print. A large part of design/usability success is consistency and predictability. reply eviks 4 hours agorootparent> The Mac app ecosystem considers HIG failures to be serious bugs, and they do not survive Only in your fantasy world of perfect compliance. In real life Apple itself often doesn't comply. Like their new settings app can't be resized despite the fact that guidelines guideline \"Let people resize app windows when possible\" > I don't know why you're bothering to argue Because you're so obviously wrong in ignoring the obvious it's hard to resist > Show me an example of an even semi-popular Mac app which uses a different convention for Print, and we might have a conversation. Show me the part where anything in my argument depends on specifically Print being out of place in some app? Though you've manged to have a conversation despite simply ignoring it and making up your own point > You just invented that problem. Show me a more ergonomic design for Print. A large part of design/usability success is consistency and predictability. Why would I show you anything when you're so biased as to have ignored my lived experience of struggling with this persistent awfullness of unergonomic default keybinds that often can't be changed conveniently or at all? reply lycopodiopsida 7 hours agorootparentprevI have yet to see how the defaults of windows are more ergonomic. I am also not aware that humans have significantly changed as a species since the 80s, so all the the ergonomic considerations are the same. Besides, it completely misses the point - ergonomics are often about convenience and knowing what to expect. A zoo of frameworks and UI paradigms is a most terrible outcome with any kind of HIG. Microsoft is not able to keep a consistent UI paradigm for their own software - what a shining beacon of chaos they are. reply eviks 4 hours agorootparentI have yet to see how I stated that Windows defaults are more ergonomic. And your limited awarenes ignores the fact that ergonomic considerations were not properly taken into account in the design in the 80s just like now there are still plenty of ergonomic issues reply jorvi 7 hours agorootparentprev> just an unenforced convention that you can adhere to on Windows as well And yet on Windows I often have to do a Where’s Waldo for certain options or even submenus. On macOS it has been crystal clear where to find these options for almost 25 years if we count from Mac OS X, and you can trace some the lineage back all the way to Mac OS 1 which was launched 40 years (!) ago. The thing that has finally started to chip away at this steely adherence to UI conventions is nothing other than poorly made Electron applications, which ironically Linux can actually get nice menu options for via Dbus trickery :) reply piva00 8 hours agorootparentprevGiven that the convention seems more often followed on macOS can't it be a property of the environment of macOS developers usually following the conventions better than on Windows? Even if not an inherent property of the system it's an emergent one, something causes it if it's not restricted by the system itself but is still a property that apps developed for macOS are much more homogeneous in their menus UI implementation than the others. reply eviks 8 hours agorootparentIf you shroud this basic misunderstanding in so much emergent mystery, sure, but otherwise no, and the real reason is simple enough - that's just one of the defaults in some \"UI kits\", which macOS devs can also use, including \"disparate or bespoke\" ones, and could've been just as consistent without it being positioned at the single top spot reply piva00 7 hours agorootparentCan we agree that it could be a 2nd or 3rd order effect of a design choice such as putting the menu system as part of the system's UI nudging developers to follow conventions instead of inventing their own menu UI because it resides inside a window which they can control completely, hence giving the thought of \"I can do the menu however I want\" a bit more probable? It's hard to analyse and quantify this ergonomics objectively so I'm just spitballing a potential way that the menus in Mac apps follow much more the convention than Windows apps. I think Apple is also much better at keeping their UIs consistent than what the Windows team is, could also be another nudge to developers to follow them. And no, I'm not saying they're perfect, I hate all their UIs like the App Store, new System Settings (it's frankly stupid, also in responsiveness), Apple Music, etc. Design choices are nudges, Apple's ecosystem has nudged developers much better into keeping some consistency across the system, it's much more jarring on macOS when an app doesn't follow conventions than on Windows. reply eviks 4 hours agorootparentSure we can agree on nudges and indirect effects, it just wasn't the original statement I've argued with reply draven 12 hours agoparentprevTrue. On the other hand I like that the menu is on top on the screen so I can just throw the pointer up (I use a trackball) and it will end up on the menu, no precise targeting required. reply sevg 12 hours agoparentprev100% agree. Not having those menu buttons visible until you focus means an extra click (and you can't even see what the menu headings are for that app until you click, so it also delays your thought process). Ok so you get more screen space I guess, but IMO it's not worth the trade off. reply allenbrunson 12 hours agoparentprevhaving one menu bar at the top of the screen made sense when macs had nine-inch screens. it saved some screen real estate, which was at a premium at the time. but the bigger the screens get, the less sense this makes. apple should have switched to menu bars in each app when they introduced mac os x. we were all forced to make so many changes at the time, one more wouldn’t have been a big deal. reply xlii 11 hours agoprevOfftopic, but am I the only person surprised how badly Apple stalled when it comes to window management and UX on MacOS? I feel that today, as a power user/professional, MacOS wouldn't be usable without applications like Bartender, external window manager such as Rectangle/Divvy etc. I feel strong attraction coming from yabai, but disabling SIP is too much for me. Personally I'm reaching the point where I'd migrate to Linux ONLY for the user experience (I live in Emacs most of the time anyway, and there I could get some minimal WM on top of it, I guess). reply nlkl 9 hours agoparentI use Mac for work, Linux/Gnome for personal use, and have used Windows up to and including 11 both personally and professionally in the past (although a lot less recently). In terms of the desktop environment, I feel Mac is severely lacking behind. I know there are very mixed opinions on Gnome, but to me the Gnome UX feels extremely well thought out in comparison. The apple ecosystem might be very well integrated, but in terms of the macOS UX it feels the exact opposite. Why can’t I close an app from mission control? Why can’t I launch apps from mission control, but need to e.g. open launchpad first? In Gnome the activities view unities all of those experiences in a way that is seamless and just clicks for me, but in macOS everything seems to be separate apps/features that don’t play together at all (out of the box at least). Add to that all the other small frustrations that you need to address with third party tools, it is - for me at least - a very unproductive out-of-the-box experience. Of course this is subjective, and might be partially an issue of (my lack of) skill/experience. reply tuyiown 8 hours agorootparentNot to disregard your remarks, I suspect they are valid, but they come from habits of uses of others systems, and there's also the good practice to avoid introducing too much ways to do the same thing, because it's dramatically augments your chances that it is known by all (in the end), works well and as expected. Hence launchpad is only here to shortcut finding your apps in the finder or the dock, mission control just an increment over spaces. Those are not replacement for dock. So quitting app, aside the app shortcut and menu, goes through the dock, either by dock app menu or ⌘+Q on the ⌘+tab app switcher. reply lycopodiopsida 10 hours agoparentprevWindow management is fine, what is missing are half-split/full-screen shortcuts. There are many apps which can set them up, but a built-in possibility would fix most issues, since only a very small subset of users cares about tiling wm even on linux. Though, there was a new project the other day on HN, where SIP can remain on: https://github.com/nikitabobko/AeroSpace I, for my part, have configured window placement shortcuts in Hammerspoon (also possible with KM, BTT and a what feels like a million of other apps) and am completely happy. I mostly look at the maximized terminal anyway, the splits do happen in neovim and tmux. reply philistine 6 hours agorootparentThere are shortcuts to tile windows. Hold the green indicator in an app’s window. reply atombender 7 hours agoparentprevI don't personally care about advanced window management like tiling windows. But I do use Spaces constantly to divide my screen into \"work\" and \"personal\", and Apple has introduced a huge, annoying bug macOS Sonoma that is not being fixed. Since Sonoma, windows regularly are stuck on top of other windows. This has been known since the betas in 2023 [1] and persists till this day. It typically happens during restoring after a reboot, but it can happen at basically any time when a window is created a space that isn't the first one. The workaround is to drag the window from the second space into the first, release the mouse button, and then drag the window back into the second space. This appears to reset whatever the internal state keeps track of window order. [1] https://www.reddit.com/r/MacOS/comments/16bb19g/sonoma_finde... reply d3ckard 11 hours agoparentprevTotally in the opposite camp, in the sense I think it’s preferable to customize your experience using 3rd party apps and it should be encouraged model by vendor. reply Vuska 10 hours agorootparentSadly Apple picked neither option. 3rd party apps cannot meaningfully interact with Spaces for example, even basic things like moving windows between spaces or adjusting the animation speed so it's not nauseating on ultrawides. reply zimpenfish 8 hours agorootparent> even basic things like moving windows between spaces I don't really use spaces but I've got \"Displays have seperate spaces\" turned on and Rectangle Pro has \"Next Display\" and \"Previous Display\" which moves the foreground app to, unsurprisingly, the next and previous display (which seems to be a space). Are you after something like \"move this window to space Y\" rather than just \"next space\"? reply user3939382 9 hours agorootparentprevThere is a way to switch spaces instantly. If anyone actually cares I’ll dig up my solution. reply drdo 8 hours agorootparentI care! Does your solution involve disabling SIP? reply user3939382 2 hours agorootparentThis utility allows it https://totalspaces.binaryage.com/ You hit a keyboard shortcut, you get to the new space with 0 animation/transition. It can work with SIP on but only on macOS older than 10.14. They mention here this is due to some changes they make to the Dock https://totalspaces.binaryage.com/sip-details I'm not sure if the only part I care about (instant Spaces switching) would continue to work without that / if you turn SIP back on. I haven't tried. reply d3ckard 10 hours agorootparentprevAgreed, just yesterday I was researching APIs in this area. Still, it’s a different criticism. reply imiric 10 hours agoparentprevI'm not sure why you're surprised. The macOS GUI has never been designed with power users in mind. If you wanted to enhance your experience as a power user, you always had to resort to 3rd party software. By using Apple products you accept to use them as they were designed to be used, and any customization of the experience is a luxury that could be taken away at any point. Apple knows best how you should use your computers, after all... Linux is on the other side of that spectrum, but then you lose the benefit of a tightly integrated and curated ecosystem. If you're willing to give that up, and don't mind tinkering and frequently dealing with jank, then Linux might be for you. reply bartvk 7 hours agoparentprevStage Manager was introduced in Ventura, which is definitely a form of window management. I don't think you can call that \"badly stalled\"? reply thiht 6 hours agorootparentDoes anyone actually use Stage Manager though? Probably a bias on my part but I don't know anyone who use it ever reply hbn 4 hours agorootparentStage Manager is confusing as hell. It was designed for iPads in an attempt to give them some semblance of window management but even there it makes no sense. They tried to reinvent window management to be more \"simple\" but in doing so they made something WAY more complicated to the point where even computer-savvy folks can't figure out how to do basic tasks. https://youtu.be/sFJuAjyWOc4?t=500 reply veidr 2 hours agorootparentprevI don't, but was astonished to find out the majority of my team at work uses it, and has been for a while. reply kstrauser 4 hours agorootparentprevI do. I made myself try it for a week a while back For Science and ended up kinda liking it, at least enough not to bother turning it off again. Yesterday my coworker asked me what neat window manager I’m using and it took me a moment because I’d forgotten about it. reply creesch 11 hours agoparentprevI totally agree. But it also seems like a lot of people don't. Frustratingly enough when discussions like that come up a lot of people respond with a \"MacOS just works differently, you just need to get used to it\" discussion killer. Yeah, I know some design principles are different. That doesn't always mean they are better or couldn't be better. In general I feel like UI/UX on MacOS for the most part has stagnated in the past decade or so. Where there has been evolution, it mostly has been things (poorly) ported over from iOS (system preferences for example). This in contrast to other desktop platforms that have kept evolving and experimenting with other ways of working. Granted, not always improving things but often making things much better. Window management on windows 11 for example is just plain awesome imho. Certainly on wider screens or screens in portrait it is extremely powerful. Everything I am working on MacOS I feel very limited in that area. Even with tools like Rectangle it still doesn't work as fluid as it does on windows. reply jwells89 3 hours agorootparentI think this sort of thing is more subjective and dependent on what the user is used to than most are willing to admit. I find the vaunted Aero Snap feature incredibly annoying for example because it’s so “noisy” with its proposed-snap animations and how easily it’s accidentally triggered (especially when multiple displays are involved) — the way the third party app Moom does it with a popover that appears on green traffic light hover or key shortcut is vastly preferable to me. It’s true that Mac desktop evolution has somewhat stagnated and could use some movement, but that movement shouldn’t necessarily be toward the Win9X desktop paradigm. reply the_other 10 hours agorootparentprev> But it also seems like a lot of people don't. n=1... I'm one who doesn't care. I didn't like to have a lot of system tray icons when I used Windows, and I don't like having a lot of them in macOS. I turn off the ones I rarely use (when I can). However, I also feel Apple have reduced the usability of the menu bar icons in recent iterations. Their clickable area/spacing has enlarged, which is counter to having small icons in a focused space. The Clock and Control Centre can't be moved around. I didn't use Control Centre at all until I discovered some setting that can only be toggled. Since I toggled it, I've forgotten what it was and haven't touched Control Centre since. I'd turn it off if I could. Maybe I need Bartender after all. reply pasc1878 9 hours agorootparentThe menu bar has now to have more icons than in earlier OSX as to run services like dropbox, 1password etc they now need to be in the menu bar whilst previously they were not needed there - now I suspect this might be the app writers fault as well as Apple's but it is an annoyance. reply veidr 2 hours agoparentprevLinux pulled ahead of macOS for desktop computing like 3 years ago. But, that was about 3 years after desktop computing became this niche thing that only GenX/Boomers still do... (T_T) reply replete 9 hours agoparentprevNot really a big deal IMO, MacOS has loads of great software available to improve workflow. BetterSnapTool, BetterTouchTool, MOS, Dropzone, Slidepad, Raycast, TotalFinder (visor feature), iTerm2 (dropdown shell) etc. BetterSnapTool is probably what you are looking for - for normal monitors the keyboard shortcuts for window management (modifier + arrows) are great, and for giant monitors the custom snap zones are great. My favourite feature is the ability to move/resize windows by holding down ctrl/shift while hovering over them. These kinds of things are a big reason I stick with Mac, great ergonomics and automation. Seems a better situation than the alternatives? You need a defensive mindset with Windows with their behaviours over the last 5 years reply suneater92 11 hours agoparentprevThis extends across the system - Safari is great for battery life and hit or miss for performance, but it has an infuriating bug where it zooms out every webpage in Split View. There have been so many Stack Exchange and forum posts since 2013 on this issue and no action. Other WebKit browsers like Orion don’t do the same thing; it’s not a browser engine issue, so the only recourse is through Apple’s stupid feedback process that spits out autogenerated answers to everything. Windows is no better, but if Adobe, Affinity and a few medical anatomy apps opened the doors to something similar to Proton… jeez. reply leokennis 10 hours agoparentprevFully agree. For work I use Windows 11 and even without any third party tools, its ability to easily group and align windows using either the keyboard or mouse is vastly superior to what macOS has. The only thing that macOS is still best at is Exposé. The smoothness, animations, superior Apple touchpads to make the gesture and then ths ease of picking the right window are second to none. reply art0rz 9 hours agorootparent> Exposé KDE 6's new Overview effect works really well https://kde.org/announcements/megarelease/6/ reply lunar_rover 9 hours agorootparentNeither is even close to how smooth Gnome's overview is. And macOS still renders only a single desktop for some reason. reply hbn 3 hours agorootparentprevWindows' multiple desktops implementation is half-baked and janky too. I was excited when they added that in Windows 10 but didn't end up using it because it's too awkward, slow, and limited. reply jprd 2 hours agoprev [–] It took me a seriously long amount of time to realize this wasn't software for MANAGING AN ACTUAL BAR/PUB. Must be frustrating as a Mac user to be locked into paying lots of money to have a desktop OS that works how you want reply kstrauser 1 hour agoparentThis is an article about a free, open source tool. Running FOSS tools to tweak my desktop isn't frustrating. reply hbn 1 hour agoparentprev [–] It turns out Safari isn't for backpacking expeditions in Tanzania either reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"Ice Ice\" is a macOS menu bar management tool that allows users to hide, show, and rearrange menu bar items using Command + drag.",
      "It offers features such as custom spacing and menu bar appearance customization, with future updates planned to add more functionalities.",
      "The tool requires macOS 14 or later, can be installed manually or via Homebrew, and is available under the MIT license."
    ],
    "commentSummary": [
      "The discussion focuses on macOS menu bar management tools, including Bartender, Ice, Dozer, and Hidden Bar, and user experiences with these applications.",
      "Concerns are raised about Bartender's recent acquisition by a less trusted company, leading to potential privacy and transparency issues.",
      "Users compare macOS's usability and window management with other operating systems like Windows and Linux, debating the need for third-party tools to enhance productivity and the consistency of macOS's user interface."
    ],
    "points": 518,
    "commentCount": 147,
    "retryCount": 0,
    "time": 1717737118
  },
  {
    "id": 40610435,
    "title": "Microsoft Makes Controversial Recall Feature Opt-In After Security Concerns",
    "originLink": "https://www.wired.com/story/microsoft-recall-off-default-security-concerns/",
    "originBody": "ANDY GREENBERG SECURITYJUN 7, 2024 12:11 PM Microsoft Will Switch Off Recall by Default After Security Backlash After weeks of withering criticism and exposed security flaws, Microsoft has vastly scaled back its ambitions for Recall, its AI-enabled silent recording feature, and added new privacy features. PHOTOGRAPH: DAVID RAMOS/GETTY IMAGES When Microsoft named its new Windows feature Recall, the company intended the word to refer to a kind of perfect, AI-enabled memory for your device. Today, the other, unintended definition of “recall”—a company's admission that a product is too dangerous or defective to be left on the market in its current form—seems more appropriate. On Friday, Microsoft announced that it would be making multiple dramatic changes to its rollout of its Recall feature, making it an opt-in feature in the Copilot+ compatible versions of Windows where it had previously been turned on by default, and introducing new security measures designed to better keep data encrypted and require authentication to access Recall's stored data. “We are updating the set-up experience of Copilot+ PCs to give people a clearer choice to opt-in to saving snapshots using Recall,” reads a blog post from Pavan Davuluri, Microsoft's corporate vice president for Windows and devices. “If you don’t proactively choose to turn it on, it will be off by default.” The changes come amid a mounting barrage of criticism from the security and privacy community, which has described Recall—which silently stores a screenshot of the user's activity every five seconds as fodder for AI analysis—as a gift to hackers: essentially unrequested, preinstalled spyware built into new Windows computers. In the preview versions of Recall, that screenshot data, complete with the user's every bank login, password, and porn site visit would have been indefinitely collected on the user's machine by default. And though that highly sensitive data is stored locally on the user's machine and not uploaded to the cloud, cybersecurity experts have warned that it all remains accessible to any hacker who so much as gains a temporary foothold on a user's Recall-enabled device, giving them a long-term panopticon view of the victim's digital life. \"It makes your security very fragile,” as Dave Aitel, a former NSA hacker and founder of security firm Immunity, described it—more charitably than some others—to WIRED earlier this week. “Anyone who penetrates your computer for even a second can get your whole history. Which is not something people want.” In addition to making Recall an opt-in feature, Microsoft’s Davuluri also writes that the company will make changes to better safeguard the data Recall collects and more closely police who can turn it on, requiring that users prove their identity via its Microsoft Hello authentication function any time they either enable Recall or access its data, which can require a PIN or biometric check of the user’s face or thumbprint. Davuluri says Recall’s data will remain encrypted in storage until the user authenticates. All of that is a “great improvement,” says Jake Williams, another former NSA hacker who now serves as VP of R&D at the cybersecurity consultancy Hunter Strategy, where he says he's been asked by some of the firm's clients to test Recall's security before they add Microsoft devices that use it to their networks. But Williams still sees serious risks in Recall, even in its latest form. MOST POPULAR SECURITY The Snowflake Attack May Be Turning Into One of the Largest Data Breaches Ever BY MATT BURGESS SECURITY This Hacker Tool Extracts All the Data Collected by Windows’ New Recall AI BY MATT BURGESS CULTURE Ikea Will Pay You Real Money to Work in Its Virtual Roblox Store BY MEGAN FAROKHMANESH BACKCHANNEL Inside the Biggest FBI Sting Operation in History BY JOSEPH COX Many users will turn on Recall, he points out, partly due to Microsoft’s high-profile marketing of the feature. And when they do, they’ll still face plenty of unresolved privacy problems, from domestic abusers that often demand partners give up their PINs to subpoenas or lawsuits that compel them to turn over their historical data. “Satya Nadella has been out there talking about how this is a game changer and the solution to all problems,” Williams says, referring to Microsoft's CEO. “If customers turn it on, there’s still a huge threat of legal discovery. I can’t imagine a corporate legal team that’s ready to accept the risk of all of a user’s actions being turned over in discovery.” For Microsoft, the Recall rollback comes in the midst of an embarrassing string of cybersecurity incidents and breaches—including a leak of terabytes of its customers' data and a shocking penetration of government email accounts enabled by a cascading series of Microsoft security slipups—that have grown so problematic as to become a sticking point given its uniquely close relationship with the US government. Those scandals have escalated to the degree that Microsoft's Nadella issued a memo just last month declaring that Microsoft would make security its first priority in any business decision. “If you’re faced with the trade-off between security and another priority, your answer is clear: Do security,” Nadella's memo read (emphasis his). “In some cases, this will mean prioritizing security above other things we do, such as releasing new features or providing ongoing support for legacy systems.” By all appearances, Microsoft's rollout of Recall—even after today's announcement—displays the opposite approach, and one that seems more in line with business as usual in Redmond: Announce a feature, get pummeled for its glaring security failures, then belatedly scramble to control the damage.",
    "commentLink": "https://news.ycombinator.com/item?id=40610435",
    "commentBody": "Microsoft Will Switch Off Recall by Default After Security Backlash (wired.com)226 points by georgehill 2 hours agohidepastfavorite188 comments pixelpoet 3 minutes agoWhat pisses me off is that they are just going to keep pulling shit like this, and it's up to everyone to push back ferociously every damn time. It's up there with \"not now\" instead of \"get rekt and never ask me again\" choices in terms of user-antagonism. I'm aware that other OSes exist, but I happen to hate Windows least on the whole :/ reply herf 1 hour agoprevIt's interesting to compare this to the Chrome/Safari/Edge browsing history, which is stored in an unencrypted SQLite database, and tracks what you do for the last 90 days. It's just a bit less visual, Incognito/Private modes work, and some users clear it more often. But a whole lot of the surveillance attacks people imagine about Recall apply just the same to the browser. I think it's the \"little brother\" casual attacks that are so well enabled by Recall - it makes it faster, easier, and way more visual. reply byteknight 53 minutes agoparentThis is a horrible comparison. Browsing history doesnt show the contents of the page. It doesnt show you what you were doing on that page. It doesn't reveal anything other than you went there and maybe how long. reply GordonS 59 minutes agoparentprevYour browsing history is unlikely to contain personal information, secrets, porn images etc. And if you use Chrome, they get your full browsing history by default. I get your point, but Microsoft's Recall can capture anything onscreen - emails, personal info, porn, passwords and the like. And it feels, bizarrely for 2024, that little thought has gone into privacy or security. reply nomel 53 minutes agorootparent> that little thought has gone into privacy or security. I think the thought is proportional to the amount of thought a non-tech customer will put into it. Nobody seems to care about or understands privacy these days. Everyone knows they're being tracked everywhere they go physically and on the web. People use their real names, address, etc for every junk service they sign up for, without seeing any reason not to. If you tell people that their TV is tracking and taking screenshots of what they watch [1], they say \"yeah, Netflix knows too\". It's literally, \"how it's always been\" for any non tech person under 30. [1] https://themarkup.org/privacy/2023/12/12/your-smart-tv-knows... reply haswell 42 minutes agorootparent> I think the thought is proportional to the amount of thought a non-tech customer will put into it. Part of me wonders if this is the consequence of how accessible tech has become, and the prevalence of increasingly non-technical product managers. I'm a former PM, and I'm not here to denigrate the PM role, but the fact that a product like Recall got shipped says a lot about the makeup of the product org that shipped it. While I get that younger people tend to see privacy differently, I'd argue this isn't really a privacy issue, it's a security conversation, albeit with obvious privacy implications. Leaking what apps I use or what sites I visit is mostly a privacy issue. Leaking what I type into the boxes on those sites is a security issue. If the end result of leaking this info is the attacker can pwn all of my bank accounts, we're solidly into security territory. The fact that this got shipped means that multiple levels of leadership either didn't think about the consequences or didn't care about the consequences. I hope it's the former, because that means they can learn from the backlash and hopefully recalibrate. Microsoft is in a position of power that IMO requires a significant duty of care and responsibility to their customers, and lapses like this need to be judged through that lens, i.e. it is their entire business to make sure features like this are safe. reply xattt 17 minutes agorootparent> The fact that this got shipped means that multiple levels of leadership either didn't think about the consequences or didn't care about the consequences. I hope it's the former, because that means they can learn from the backlash and hopefully recalibrate. There was probably from lower decks, where they are closer to reality. However, people are scared for their jobs in this economy and likely didn’t take it farther. reply sumtechguy 27 minutes agorootparentprev> Everyone knows they're being tracked everywhere they go physically and on the web That sounds good to some people. But if I mentioned it to most people in my family they would probably be rather weirded out by it. They probably also would have no idea of the scope of the size of it and how it is being used against them. reply skydhash 25 minutes agorootparentprevDo you listen to music only with earbuds? Do you cover your face when going outside? Do you transform your voice for each person you’re talking to? Are you buying only with cash that you handled with gloves? Privacy is not a binary concept. There are actions and information that some people are ok being public, and there are some they prefer to remain private. What is not OK is spying and exploitation. I should know what data you’re collecting and preferably specify which I’m ok with. I also should know what is intended for and preferably for most of it to be anonymized. Most people expect reasonable privacy policies from companies and they believe that there’s some regulation in place. reply SketchySeaBeast 25 minutes agorootparentprevI think they actually did consider that - that's why they emphasized it was all on device. They thought about it, they just didn't think about how little we would trust that promise. reply axus 57 minutes agorootparentprevIt's analogous to phone call metadata vs. the contents of the phone calls. reply ragnese 35 minutes agorootparentprev> And it feels, bizarrely for 2024, that little thought has gone into privacy or security. No, no. They thought about the privacy and security aspect. They decided that it's better for their bottom line if Windows users don't have privacy from the mother ship. Really, they already decided that way back when Windows Vista first came out and periodically asked Microsoft HQ if you should continue being allowed to use your computer. reply biftek 1 hour agoparentprevYour browser history doesn't contain screen recordings of what you do on websites reply juancn 58 minutes agoparentprevThe ickier parts are on the unintended capture side, like enabling \"show password\" on a site doesn't affect browser history but Recall may capture it in the clear. Or from history you may see that you accessed a site, but not what you did on it (what comments you typed for example). reply torstenvl 58 minutes agoparentprevThey're quite obviously very different, as browser history doesn't tend to include things like financial details or information subject to an NDA. reply ydnaclementine 1 hour agoparentprevDoes Recall run entirely locally? I don't think your browser history gets sent out reply toyg 1 hour agorootparentI expect it does, if you're using Chrome outside of Incognito Mode. Iirc, there is an opt-out about \"web history\" on the google account - which then disables some other things so that it annoys enough people into keeping it on. reply juancn 1 hour agorootparentprevIt does, that's why it needs an NPU to run. reply neilv 51 minutes agoparentprevOne difference is that Web browser history has been there 30 years, since before most people at the time had even touched a Web browser. At the time, it wasn't very thinkable that someone would have the audacity to take and abuse that information. It dates from when Internet people overall were more savvy about privacy than users overall today are, but it was also when the Internet was closer to a trustworthy environment, and before Wall Street sociopath types took over the tech and the culture. Lots of kinds of abuse that today are routine and almost universal, for even startup tech companies, (e.g., embedding third-party trackers into Web site, and getting even worse from there), I think would've gotten them ostracized, and outraged demands for criminal charges. During the dotcom gold rush, there was such a flood of totally new, posturing people, and so much money being thrown wildly at everything, that any remaining outrage was lost in the noise. And now virtually no one knows any different. But if you're trying to push some new abuse today, I think ordinary people are starting to have some awareness of what vicious sociopathic buttholes tech companies have become, and so acceptance might not be a slam-dunk. reply russdpale 1 hour agoparentprevno it isnt the same, you may know I went to my health care provider's website, maybe even to make an appointment depending on the url, but with recall, everything that is on the page will be stored, not just the url. It's totally different. So the message I sent my healthcare provider that is discussing some of my most sensitive medical issues will be available to read and a record is kept of it... not just the url. Do you not see the difference? reply herf 1 hour agorootparentYes, but one product cycle and there's metadata (like a background texture) that tells the OCR to skip this page. Or ask your local LLM if the user is talking about medical conditions? If you like the feature at all you can make these things work. reply entropicdrifter 41 minutes agorootparent\"If you like the feature at all you can make these things work.\" It's not on the individual users to take steps to preserve their basic human dignity. It's not Microsoft to not take that dignity away by default as was their plan before this fiasco predictably blew up in their faces just like the Xbox One always-online Kinect requirement before it. reply nyrikki 1 hour agoparentprevTheir is a very different scope at the OS level. Most of us know that the public Internet is based on surveillance capitalism, no matter if we hate it or are just complacent or ignorant. OS wide is far more problematic and of low value to the user. reply EGreg 1 hour agoparentprevBrowsing history doesn't contain what's displayed on the page, and what you input into the input boxes, or POST requests. It's sorta like telephone metadata. On the other hand, I am always freaked out by Chrome extensions that \"can read and change your data on all websites\". Can't they have more granular permissions? You gotta have a lot of trust for those extensions LMAO. They can read your bank passwords, probably!! And if they are ever sold... reply herf 1 hour agorootparentExactly - knowing the content of each webpage is pretty easy if you're \"big brother\" surveilling millions of people, even more so if you have a Chrome extension to help. It's \"little brother\" that benefits a lot here: bosses, spouses, parents, etc., who otherwise wouldn't click on 1000 links in your history. reply red_admiral 52 minutes agorootparentprevI have an extension like that called uBlock. If that ever gets compromised or sold, I will have much bigger problems ... reply ls612 1 hour agorootparentprevTo be fair for me the extensions that get that are uBO, Privacy Badger, and Tampermonkey. I trust gorhill and the EFF to not fuck me over on my data, and Tampermonkey kinda needs those sorts of permissions to work. My password manager has read access to every website but I'm already trusting it with all of my passwords so... reply Analemma_ 1 hour agoparentprevYeah, I think this entire debate is uninformed hysteria and manufactured outrage. \"If an attacker has administrator access, they can see everything you have done on your computer!\". OK? That has literally always been the case? \"Attacker is root\" is game over and always has been. The original writeup from DoublePulsar tried to justify that Recall is somehow different from other such scenarios, but I found it totally unconvincing. I think it's the right move to have it off by default, but I'm just not convinced by the outrage here. reply Tool_of_Society 1 hour agorootparentBrowser history doesn't show my passwords, everything I typed out and did on the machine. In comparison browser history is nothing. reply sanktanglia 49 minutes agorootparentYou can get cookies/tokens from chrome databases so its the equivalent to passwords in alot of cases reply Analemma_ 1 hour agorootparentprevYou’re missing the point. An attacker can only see the passwords in your Recall database if they have root, but if they have root there are (and always have been) a thousand other ways they can get your passwords. There is no new attack vector being introduced by Recall. reply Benedicht 52 minutes agorootparentIf an attacker got root with recall they might not need to wait the user to type their password and risk detection. The information they want to know might be already in the recall database. reply morder 46 minutes agorootparentprevOne difference is that you can get root access after the fact and get however much prior data Recall recorded vs only going forward. reply fh9302 38 minutes agorootparentprevIt is possible to access to Recall database without admin access. https://x.com/GossiTheDog/status/1798832390070276500 reply shermantanktop 29 minutes agorootparentprevIn a typical bigcorp environment, laptops are loaded with silently installed spyware. Certainly equivalent to taking a screenshot every second or an always-on keylogger. The horse is out of the barn for many people during work hours. But in the OS and on by default is a different story! reply amusingimpala75 36 minutes agorootparentprevExcept that before today you didn’t even need admin for access to the database, any process that is allowed to read things could access the Recall database. reply mostlysimilar 1 hour agorootparentprevRecall FEELS like being watched. Your browser history does not. reply listenallyall 37 minutes agorootparentTo be clear, I am not in favor of Recall or dismissing its intrusiveness. However, the correct comparison is not just \"browser history\". Google is also tracking your search history, passwords (built-in password manager), location history (Google Maps), ad clicks, and more. All-in, it's a LOT of data. reply andrewmutz 1 hour agoparentprevIf there's AI involved, everyone's panic level skyrockets. No one retweets \"Attacker gaining root access reveals all user information\", but instead \"Attacker gaining root access reveals all user information collected by AI program\" will go viral for sure. reply workfromspace 43 minutes agoprevIt's sad that Microsoft (or any big company) wouldn't take a step back from such privacy intrusive or anti-user behavior unless there's a public backlash. Can't we just have a peaceful life without wasting time on constantly following and analyzing every single move from these companies? reply grugagag 38 minutes agoparentMicrosoft will go ahead with Recall, will temporarily make it opt-in. Eventually, when weather is good they’ll default it to opt-out. If new backlash ensues they’ll PR that it was a a bug and turn it off only to bundle it later with something that can’t be turned off. At this point MS is a toxic company that you’re better off, as a user, to steer away from. reply ragnese 23 minutes agoparentprev> Can't we just have a peaceful life without wasting time on constantly following and analyzing every single move from these companies? Not if you're using Microsoft products, no. People continue to get irritated when \"we\" do this, but here I go: you should be running Linux exclusively on your personal computers. You should also stop buying \"smart\" shit. reply chinathrow 42 minutes agoparentprevHave you not seen Windows 11 lately? I have, and I am still happy to be on Linux as my daily driver for over 20 years now. reply bee_rider 28 minutes agorootparentI almost want to start using Windows as a daily driver just so I can leave again. reply nerdjon 1 hour agoprevWill have to wait and see if the extra security measures actually improve anything or not. However regarding it being opt out… what would prevent a virus from just enabling it on a bunch of machines silently. Sure it would be caught but the damage done and most won’t be bothered to go in and disable it after. Or Microsoft just decides they need to really market the hell out of AI and it gets turned on my default anyways. reply paxys 1 hour agoparentWhat would prevent a virus from directly stealing the data it wants without going through this feature? reply ndiddy 1 hour agorootparentWithout Recall, an attacker needs to get a program to stay resident in memory to log keystrokes, screen contents, etc. for an extended period of time without getting detected. With Recall, they can get the same end effect by exfiltrating the Recall database file whenever it's convenient (i.e. an infected version of a text editor could send it while pretending to check for updates). This significantly lowers the barrier to entry for getting a victim's data, while also making it much easier to avoid detection. reply drexlspivey 48 minutes agorootparent> Without Recall, an attacker needs to get a program to stay resident in memory to log keystrokes, screen contents, etc Or it could just steal your cookies which are out there in the open. reply haswell 24 minutes agorootparentCookies are of relatively low value compared to a database of everything the user has typed and seen. reply djmips 1 hour agorootparentprevJust like in biology a virus can be simpler if it can co-opt existing machinery. reply buildbot 54 minutes agorootparentI agree, the ability to take screenshots is unsafe and should be removed. A virus is just a PRT SCRN away from stealing everything! (/s) reply ragnese 28 minutes agoparentprev> Or Microsoft just decides they need to really market the hell out of AI and it gets turned on my default anyways. This is what will happen. And when you turn it off again, it'll be turned back on by the next update. Enjoy. reply strictnein 1 hour agoparentprev> what would prevent a virus from just enabling it If that occurs, the malware won't have access to months or years of data to sift through. reply cybrox 1 hour agoparentprevIt will be re-enabled accidentally by an update anyways. reply dv_dt 1 hour agorootparentOr by intent - it seems I was reading about an early proof of concept attack that turned Recall on and hid a systray indicator that it was on. reply ragnese 28 minutes agorootparentprev\"accidentally\" reply Rinzler89 1 hour agorootparentprevPlease stop with these kinds of made up fantasy scenarios. There's no such thing as \"accidental enablement\" for stuff like this, as if it's a switch every employee at Microsoft has access to, and one of them one day can end up flipping by accident with their elbow and it ends up in production without anyone else noticing. Either they decide to intentionally enable it or not. There are no accidents , when stuff like this needs to go through a committee of people for approval before it makes it into production. reply meowster 53 minutes agorootparentI think OP forgot the quotes around \"accidentally\". You're right it won't be a true accident; it will be intentional and just called an \"accident\". reply Tool_of_Society 1 hour agorootparentprevYet despite all that I've witnessed accidents still make it in production... reply i_s 1 hour agorootparentprevI'm not sure the use of 'accidentally' was sincere. But I like this choice of words in your post in your first version: > unmercenary assumptions reply malshe 1 hour agoprevOn LinkedIn someone in my network pointed out that, apart from the security and privacy disaster, the name Recall was a bad choice because of negative events like product recall. reply leprials 1 hour agoparentThey should take note and recall Recall. reply permo-w 54 minutes agorootparentthis is one of the first things mentioned in the article reply bee_rider 30 minutes agoparentprevIt would actually be a fantastic name if this were a real concern. Imagine, a well-known feature to mask any searches of a product recall. The only problem with this theory is that computer QA is so incredibly shit that the concept of a recall more or less doesn’t exist in the first place. reply jonny_eh 1 hour agoparentprev\"Total Recall\", aka \"We Can Remember It For You Wholesale\" reply dylan604 1 hour agorootparent\"Total Recall\" in quotes makes me think you're trying to get your ass back to Mars and that you're trying to remember something because you had your memories wiped. It makes me think of nothing about a friendly service being offered forcefully upon you from your friendly and malevolent OS provider. reply jonny_eh 1 hour agorootparentIt's a story about false memories, and how that can change your identity. Regardless, it's the first thing I thought of when I heard about the feature. reply organsnyder 1 hour agoprevI still don't understand how this got this far. Enabling this in any corporate setting would be a compliance nightmare. reply Rinzler89 1 hour agoparentCorporate is never on the bleeding edge of Windows feature updates. They bring security updates first, but feature updates are at least one generation behind, maybe more waiting for Microsoft to fix bugs and doing their own regression testing, plus they get to choose wich features employees receive or are enabled by default via group policy. In other worlds, recall was never making it into any corporation anyway. reply 3qmtacr674qac 1 hour agoparentprevWith Chat Control[1] coming up in EU, it would be awfully convenient to have the technological capability readily available to deliver a solution. Once you have the Recall capabilities, it doesn't take much to start collecting and searching the data. [1]: https://www.patrick-breyer.de/en/posts/chat-control/ reply rchaud 41 minutes agoparentprevCorporate clients get whatever they want. I am certain that their Windows 10 support won't be pulled in Oct 2025 as MS has threatened for everyone else. And when they migrate to Win11, it will almost certainly be a separate OS image free of the garbage bloatware and ads that the consumer devices are plagued with. reply Terretta 37 minutes agoparentprevThe corporate settings that care already do this to the employee screens ... Compliance doesn't say \"company can't watch employee\" -- in many cases it mandates surveillance. This just lets the employee leverage that too. reply organsnyder 33 minutes agorootparentDepends on the compliance. If this monitoring sucks up any personal data (I don't mean employees' data here—personal data owned by anyone) there are erasure and data subject access requirements, for instance. reply karaterobot 28 minutes agorootparentprevSecurity compliance generally does not require a third-party company, unaffiliated with the corporation, to be sent a copy of everything shown on a user's screen. reply LegitShady 1 hour agoparentprevI bet there are a trillion companies and governments who want to know what all of their employees are doing every second of the workday. compliance won't stop them from trying. reply surfingdino 9 minutes agoprevOnly to be enabled by default by the IT department of your mistrusting employer. Microsoft better remove Recall altogether if they want to avoid costly lawsuits. reply SavageBeast 50 minutes agoprevIn all the MS Recall drama, I've yet to hear or read one single person utter something to the effect of \"Wow - great!!! - I've been waiting for something like this for years! This will solve at least one of the major issues I face regularly!\". In fact, it seems to me the only people that really want this feature are the ones trying to push it down everyones collective throat. Why is MS pushing something so hard when nobody asked for it? reply rurp 1 minute agoparentThis applies to most AI features that have been released recently. It feels like almost every business that wants to think of itself as a tech company has been desperate to throw out as many new features as possible that they can slap an AI label onto. Most of those features are garbage and make the product worse, either because they don't address an actual problem or because they are implemented poorly. But of course improving the product is at best a secondary concern, chasing the hype is far more important, both for the company itself and the individuals building this stuff. reply rvense 13 minutes agoparentprevFor tech savvy people, it's a bewildering feature. Why would you want some weird unpredictable AI thing when you've already got filesystem search, browser bookmarks, the neatly categorized PDF collection, and my Zettelkästen/2GB Org.mode doc/Joplin notes? But for non-technical people, of course, computers are already unpredictable. They routinely (appear to) misplace files and overwrite them with previous versions, and if the URL falls out of autocomplete the site might as well not exist. For people who google to find the Facebook login page, this would simply be how computers should work. You tell it to give you the thing and it gives you the thing. How that happens is immaterial. reply Terretta 39 minutes agoparentprevOn the contrary, executives at the office have been coming to me about various such tools for months now. It really picked up last fall. Microsoft was last to the party. reply haswell 16 minutes agoparentprevIf I knew that the data could be absolutely kept safe and private to me, I’d love a feature like this. Keeping track of my work over time would be so much easier. The natural next step is to have a local model trained on everything I’ve ever done, and for all of my computing tasks to be contextual to that history. I could see this transforming how we use computers. But I wouldn’t go anywhere near Recall. I suspect Microsoft is pushing this so hard because they want to do what I just described, and they want to start collecting the data necessary to enable it ASAP. I can easily see a future capability that people might love that they wouldn’t have even known to ask for. But the way they’re willing out Recall is certainly not a good foundation. reply ragnese 29 minutes agoparentprev> In all the MS Recall drama, I've yet to hear or read one single person utter something to the effect of \"Wow - great!!! - I've been waiting for something like this for years! This will solve at least one of the major issues I face regularly!\". There were definitely some comments in a previous HN post about it that attempted defend it and to paint everyone else as overreacting. Several of them even said that they thought it would be useful for something they might hypothetically like to remember or search for... I don't really remember, because the whole thing is crazy to me and I think it's crazy for any tech-savvy person to be running Windows in 2024. > Why is MS pushing something so hard when nobody asked for it? I assume this is a rhetorical question, but just in case it isn't: this is not a feature/product for Windows USERS. This is a feature to help train/test MS's AI stuff- YOU are the product, not the customer. reply delecti 33 minutes agoparentprevSecurity nightmare aside, it seems like it would be handy all the time. Surely everyone has had trouble finding a website or document or email again, days or weeks later? reply ragnese 27 minutes agorootparentDocuments and emails are probably easier to find via old-school text searching, though. reply roywiggins 24 minutes agorootparentprevMost or indeed all of that doesn't need screen-scraping though. reply barbazoo 35 minutes agoparentprev> when nobody asked for it It's easy to say if you aren't one to benefit from this, but that doesn't mean no one will or that no one asked for it. reply rchaud 43 minutes agoparentprevI've heard Microsoft wants to do away with on-device Windows entirely for consumer devices, and go with a \"dumb client\" form factor that is always connected to a remote Windows server. I'm not sure who at the org is pushing for this as it would essentially hand the PC games market to SteamOS. I suppose they saw how well it's worked for enterprise customers that essentially already use a Windows VM through Citrix or some other provider, and think this would solve the virus/malware problem once and for all. reply chucke1992 20 minutes agoparentprevHas the new Copilot devices even launched? Because I don't think that aside journos anybody else has even tried to play with the Recall yet. reply WorkerBee28474 44 minutes agoparentprevI use the search inside Windows all the time. To me, this seems like a 2% improved version of that. Probably useful, mostly mundane, something I would use but not get excited about. I assume they would push it for the same reason they would push any other mildly-useful feature improvement. reply LordKeren 44 minutes agoparentprevRewind.ai is the Mac version of this and many is the same talking points apply. However, it’s a third party tool, and as such isn’t enabled by default. I think most, if not all, of the overwhelmingly negative feedback is tied to this being enabled by default, and shipped by default reply mikehearn 35 minutes agoparentprevI can be that guy. I use Rewind for Mac, which is almost identical to Recall in functionality. I love it, and I've used it frequently to find things that otherwise would have been lost forever. Most recently I used it to refresh my memory on a particularly convoluted way to authenticate with a third-party oauth system (it involved using an online oauth debugger and curl commands). I had gone through the process once successfully weeks ago, but by the time I had to do it again I'd forgotten every detail. Rather than have to go through the process of figuring it out again, I went back to my successful attempt, watched it, and basically retraced my steps. Rewind probably saved me an hour or two. My take on Recall is that, like with almost everything, it's a trade-off of security for convenience. I find it valuable enough that I'm willing to make the trade-off, but others might not. reply Daedren 39 minutes agoparentprevAs long as stockholders think it'll be good, that's what matters. Perceived value is easier to create than real value. reply chrisjj 39 minutes agoparentprev> Why is MS pushing something so hard when nobody asked for it? Here's the thing. When no-one asks for it, hard push is the only way to sell it. :) reply pjmlp 33 minutes agoparentprevJust go to Windows Central, and you will get a couple of editors shouting exactly that. reply vondur 31 minutes agoparentprevIt could be handy if the data was stored locally and was managed by the users. reply pdntspa 38 minutes agoparentprevHonestly this whole thing reeks of some sort of data grab dressed up as an \"innovative\" new feature. They probably wanted a bunch of new training material for their AI projects, and this is what they came up with. reply wing-_-nuts 42 minutes agoparentprevWhen this was announced I actually saw a post by someone who used a similar tool for time tracking in OS X and they claimed it was really helpful. To be frank, I would not mind having this feature on linux provided it was entirely local, and encrypted. reply kylehotchkiss 23 minutes agoprevRecall certainly validates China's government decision to try to get rid of Windows on government computers (https://www.marketwatch.com/story/china-reportedly-seeks-to-...). Of course recall wouldn't have been enabled on those, but the company providing the OS has made it clear they're willing to make such a sloppy attempt to AI all the things reply danielcampos93 1 hour agoprevThis seems to be a feature that execs wanted, and people find creepy, and no one has the gumption to push back on the exec request. reply dylan604 1 hour agoparentHow can you have the number of employees they do and not have a single non-sychophant employee? reply fingerlocks 32 minutes agorootparentCompany-wide internal push to shoehorn AI into every product and service. All recognition and rewards are given to the sychophants, no matter how ludicrous their proposals. Even Principal and Senior developers are dragged into meetings with senior leadership to provide suggestions on how AI can be used in their microcosm. Whether it should be used is completely out of the question. It’s a complete circus right now. Plenty of us just ignoring it and opting-out but it might reflect on our bonuses. reply pluc 7 minutes agorootparentprevBecause you get fired when bringing dissenting opinions reply riscy 47 minutes agorootparentprevThe layer of management reporting to leadership are yes-men. reply foxandmouse 50 minutes agoprevDo we know anything about Linux support for Snapdragon X.. Personally, I don't trust Qualcomm with Linux support. Their WiFi adapters don't work properly with Linux. Their mobile SoC that supposedly have mainline support only have the CPU part working, but GPU, modem, Bluetooth, etc. won't. Also, wasn't their history of closed source drivers and their short support timeline was the reason Android devices only ever got 2 years of updates only a few years back? reply skc 11 minutes agoprevThe first big mis-step of the Nadella era. Will be interesting to hear what he has to say when he's inevitably asked to comment in his next public appearance. reply karaterobot 25 minutes agoprevWould love to know if any product research was done on this at all, or if it was a mandate from someone high up in Microsoft. I cannot imagine they'd go very long talking to potential users without hearing the exact same fears they seem to be surprised about today. reply chrisjj 44 minutes agoprev> requiring that users prove their identity via its Microsoft Hello authentication function any time they either enable Recall or access its data, So now I need MS permission to read my own data stored on my own machine? Insane. reply _zoltan_ 10 minutes agoprevI think Recall is really cool and it's a shame that it's disabled. reply o283j5o238ju 6 minutes agoparent... then you can turn it on for yourself. Unless you think it's a shame it's disabled for other people? Why would you be concerned about that I wonder? reply hedgehog 46 minutes agoprevIt's interesting that for years Safari stored page screenshots in its history to allow a \"coverflow\" view and there wasn't broad concern. reply OtherShrezzing 6 minutes agoparentI think the main difference there (apart from the feature being deprecated over a decade ago) is that Coverflow stored a single thumbnail, from which you couldn't derive much information - it's metadata alongside your browsing history, but not much more than that. Meanwhile Recall takes a stream of high-quality images, from which a full reconstruction of your entire computer-use activity over the last 90 days can be reconstructed in high fidelity and searched through. From a security point of view, the threat models are a world apart. reply Foobar8568 12 minutes agoprevI am done with Windows, I really love .net, SQL Server, WSL, but I have been burnt on so many of their tools, features etc, Windows 11 was the last straw (task bar unmovable? Are you kidding me? ), and Recall will be the never look back for my personal computing. reply mackrevinack 53 minutes agoprevmake it a separate program that people can install if they want to. if its really that great then people will download it reply LordKeren 39 minutes agoparentWe will never see Microsoft ship a major product like this and not have it bundled in to a windows update. (Rather than specific install) After their success with installing Teams, Microsoft has seen that the regulators will not proactively stop this kind of thing anymore reply ffhhj 49 minutes agoparentprevAnd how are they going to convince people to be surveilled voluntarily? This needs to be behind a switch they can silently enable in some update, ofcourse. reply ratg13 33 minutes agorootparentThis isn't new technology. Apple has had \"Rewind\" for some time, which is basically the same thing, and it's widely used. The major difference is that it's a 3rd party software, not bundled with the OS, and you would have to intentionally go out and buy it and install it. Microsoft has just taken it for granted that everyone would want this and then forced it on everyone. reply nerdix 1 hour agoprevI only have a windows partition for games. I would occasionally use it for other stuff because it's sometimes inconvenient to switch back and forth. After recall, I'm only using it for gaming and nothing else. reply pipes 1 hour agoparentI'm surprised by how good proton is at running windows games on steamdeck. Because of this and nonsense like recall and the adverts in windows I'm considering just getting rid of windows all together, I'll just run mint Linux probably. reply ryukoposting 1 hour agorootparentI run Ubuntu on nearly all of my machines, but I build it up manually from the Ubuntu Server installation to reduce bloat. If anyone was going to have problems with Proton on an Ubuntu machine, it's me. Yet, every game I've tried works fine. Everything from Among Us to Metro Exodus runs great. Some games require a little fiddling, sure, but I've never had an issue that couldn't be resolved using some copy-pasting from ProtonDB. As you may have surmised from the way I set up my machines, I may have a higher tolerance for fiddling than most folks. YMMV. reply Novosell 1 hour agorootparentprevCan't play League, TFT or Valorant on Linux though sonce they started enforcing Vanguard for League as well. reply st3ve445678 42 minutes agoprevIt could still just be switched on and used to spy on an unknowing spouse for example... its just so creepy. Who asked for this feature?? No one did. reply mprime1 36 minutes agoparentThe AI training team asked for this feature (I’m being a bit provocative and assume today it stores locally only but a future TOS change will secretly and “anonymously” upload your data ‘for training purposes’ —- that’s what everyone else is doing these days) reply st3ve445678 14 minutes agorootparentThe same thought did cross my mind... would not surprise me. reply k8sToGo 39 minutes agoparentprevIn theory you could have always just installed a screen recorder to record your spouse even before this. reply jrepinc 13 minutes agoprevEven if it shows being turned off you can't be sure it really is. And yeah they have a tendency to secretly turn malicious features on with little updates. One would really be naive to believe them after their past bad behaviour. It is just another step in slowly boiling the frog to death. Maybe it will be off by default only for as long as people get used to it and normalise it and then, next step turn it on again, more quietly of course. reply postepowanieadm 43 minutes agoprevRecall got recalled(ba dum tss). reply chx 21 minutes agoprevThis is nothing. An abusive spouse will easily switch it to on. It's very likely Windows will downright push you to do so anyways. How does Microsoft intend to mitigate that harm? Because AirTags worked out just fine: > AirTags have been a tool for stalkers and domestic abusers since Apple launched them in 2021. Police records show that this is a problem, and the legal system has failed women who were targeted by stalkers using AirTags. There have been several instances where AirTag stalking has turned violent, and in at least two cases, resulted in the tracker murdering their target. https://www.404media.co/email/ce4cec4d-51c3-4101-b2b4-2c9a64... How many women will beaten and murdered because of Recall? Why is it that Microsoft reacts to software security concerns but not to the concerns of women? reply skazazes 0 minutes agoparentKnowing you could turn on recall to spy in this way implies an individual with the technical know how to grab a freeware keylogger anyways. Similarly with airtags, you have been able to buy cheaper cellular based GPS trackers for years prior to airtags existing. In their airtag case, those GPS tags also do not alert the individual that there is a beacon following their person, and as such most likely go unnoticed and under reported. reply bigstrat2003 9 minutes agoparentprevThis is sheer moral panic. Of course tools can be misused by bad people, but that doesn't make it the tool's fault (\"how many women will be beaten and murdered because of Recall\"). It is the fault of the person misusing the tool to do bad things. reply gigel82 45 minutes agoprevGood progress, but to take it just over the trust threshold for me, I'd like it to be a component that you can add/remove (like Hyper-V or IIS); removing literally uninstalls the associated services, applications, DLL registrations, scheduled tasks, etc. reply lowbloodsugar 12 minutes agoprev>If you’re faced with the trade-off between security and another priority, your answer is clear: Do security,” Nadella's memo read Just insane that this wasn’t already the rule. reply rolph 1 hour agoprevwithout seeing an actual data file created by recall, i would expect it to quickly become large. if so, i would not keep it on a system drive, when you can store it externally, to be plugged in when the owner feels they actually need recall data, and left physically out of band when its wise to do so reply dylan604 1 hour agoparenthow would the recall data get expanded if it is not plugged in all the time? hopefully you can see while it's not designed that way reply rolph 36 minutes agorootparentideally it wouldnt be expanded, the whole point is to have definite denial of recording at any time, or a cut off period, such as archiving the system portion after 12, or 24 hours. this saves system storage space, and preserves data for the owner should they need what they were doing 6months ago. reply notaustinpowers 1 hour agoprevArchive: https://archive.ph/xlh7n reply jug 33 minutes agoprevI hope it can be uninstalled altogether. Actually I wish it was a Microsoft Store app. I mean, I don't want that codepath dormant in my OS for malware to enable via a Windows Registry value or whatever. No, not a screenlogger please. reply deafpolygon 1 hour agoprevWhat exactly is a good usecase for Copilot Pro (I'm assuming Recall will be powered by that in some form)? I'm on the free trial and I'm not finding it to be any more useful than the free version, and pretty similar to ChatGPT. It can't really do anything. Can someone smarter than I chime in on this? reply wkat4242 1 hour agoparentIt'll be the other way around I expect. Recall will provide more context to CoPilot. It's not really about looking back at your own activity in case you forgot. But the AI will use it to learn about your habits, wants and hates, interests, people you deal with, usual schedule etc. An assistant is after all much more effective if it knows you through and through. The one problem is: I don't want Microsoft to be that assistant and know all that about me. Even if \"it's all local\". They still control what gets done with that info and can change it at any time. reply AlexandrB 1 hour agoprevAll I can say is LOL. Off by default for Windows 11 24H2, on by default in Windows 11 25H2, impossible to disable in Windows 11 26H2 (except in enterprise versions of course). Microsoft's history with respecting the user's wishes speaks for itself. reply bonton89 1 hour agoparentNot to mention all the dark pattern lying nag dialogs that will trick you into turning it on, or just wear you down. reply the_snooze 1 hour agorootparentI saw a yellow dot alert next to the restart/shutdown button on my Windows machine the other day. Those historically indicate a request to restart to apply critical updates. But no, it was a message recommending I sign into a Microsoft account. That was the last straw for me when it comes to Windows BS---designs that only serve Microsoft, and disrespects all the other times I've said no to their crap. I switched everything over to Linux the next day. reply ASalazarMX 1 hour agoparentprevGiven their eagerness, I'd guess: > on by default in Windows 11 25H1, impossible to disable in Windows 11 25H2 reply lordofgibbons 1 hour agoprevDoesn't Microsoft have a long history (and present) where they just enable privacy invasive \"features\" after a windows update even though the user has disabled or removed the \"feature\"? reply ceejayoz 1 hour agoparentTwitter used to do this all the time; they'd make the notification email options more granular and opt you in to the three new options that used to make up the one option you already unchecked. reply giancarlostoro 55 minutes agoparentprevYeah, which is why I'm over on Linux now. reply resource_waste 1 hour agoparentprevYes. Windows is soo low quality. It feels cheap. It feels like you are at a car dealership. Fedora, feels like you are at some futuristic office that has buttons that do multiple steps. I was literally angry last year that it took me so long to learn about up-to-date linux. Canonical's marketing of debian-family linux gave Desktop Linux a bad name. reply 7thaccount 1 hour agoparentprevYeah. You tell everyone you learned your lesson and then just go back and do it anyway a year later. reply lawlessone 1 hour agorootparentIt must be the year for all this. Bethesda are basically trying again to make paid mods stick with their Fallout 4 update. Softwar never changes. reply terrut 1 hour agoprevI've been a Windows user since 3.1, but this was the straw for me. They have always provided an OS that just worked for my home needs, even with the creeping privacy invasions in the last update. I've been dual booting for a while and last weekend I went full Linux at home. My day job revolves around being truly good at solving Windows issues, and I will happily continue doing that, but at home I'm still just liking for something that \"just works\" I hope I'm part of a trend, and that 2024 is the year of the.... reply jug 30 minutes agoparentYes, it's a really tough thing to manage this whole Recall thing philosophically and it makes me concerned about this OS. Even if MS is backtracking somewhat, they have shown their cards now and how they prioritize positioning themselves as an AI company above even rudimentary privacy. It's hard to just regain trust as if nothing happened. I'm considering Linux with a Windows VM for Visual Studio. I've had my Linux detours in the past and it honestly works pretty well for me. I personally enjoy Fedora with Gnome which I think strikes a good balance between stability, security, and freshness. But if being stable and worryfree is of top importance (like where you are \"unpaid tech support\", haha), why not just go Debian. :) reply lawlessone 1 hour agoparentprevAny Recs? i've just gotten a Kubuntu image. I am thinking if i dual boot that and SteamOS i should have everything i want covered. reply tapoxi 1 hour agorootparentNo reason to use SteamOS, it's just immutable Arch with an A/B partition scheme. Modern SteamOS is designed specifically for the Steam Deck and they only ship it as a recovery image for the Deck. You can install Steam on whatever distribution you want, I use the Flatpak, and just enable Proton in the compatibility settings. reply exitb 41 minutes agorootparentAnd if someone’s after that console-like functionality, ChimeraOS is the right choice in this area. It behaves like SteamOS, but is more compatible with PC hardware. reply arusahni 1 hour agoprevLooking forward to the update that accidentally re-enables it. reply creativeSlumber 1 hour agoparentThis. I won't be surprised at all if i' silently enabled in a future update that has nothing to do with it. reply vinyl7 1 hour agoprevThen they'll enable it by default once people forget reply wongarsu 1 hour agoparentPeople will opt-in to it during setup the same way people opt-in to logging in via a Microsoft account instead of a local account. reply cybrox 1 hour agorootparentLocal accounts are almost impossible to set up for the normal user in win11 reply wongarsu 1 hour agorootparentThat's the hidden joke. In early Win10 it used to be a simple dark-pattern screen with a prominent button \"use a Microsoft account\" and a text link in the corner \"use a local account\". Then they made it increasingly ridiculous with subsequent updates until the current point where you need a tutorial on how to even make the option visible. reply nerdjon 1 hour agorootparentprevIt is pretty easy now if you use Rufus to create your installation usb. It will prompt you (and select by default) to disable the need for an online account. I installed the Pro version and then just said I was setting it up for work or school, chose domain and then I set it up just fine as a local account. I don't know for sure how much of this is rufas or the pro version. But I just installed Windows 11 within the last hour. reply g15jv2dp 1 hour agorootparent> normal user > use Rufus to create your installation usb Pick one. \"Normal\" users don't use specialized software to create installation media. They boot the laptop with the OS already installed and go on from there. reply nerdjon 1 hour agorootparentI mostly agree, but installing Windows is not as daunting of a task as it used to be. It is also not uncommon for 'normal' gamers to use a custom built PC which would require installing Windows. Maybe normal is the wrong word, but it would be a pretty quick and easy to understand guide to do this. reply jachee 1 hour agorootparentNormal gamers aren’t representative of normal users on a whole. Gamers are just a tiny fraction of the overall user base. Normal users buy cheap-ass laptops with their manufacturers’ opinionated Windows installation, including boatloads of bloatware. And they don’t ever change any of the defaults. reply hobo_in_library 1 hour agorootparentprev> if you use Rufus to create your installation usb. You've already scared away all the normal users reply ngneer 1 hour agorootparentprevNormal user, agreed. You can find tutorials online, though, for those of us who still remember that the PC was something the user used to own. reply aeurielesn 1 hour agoparentprevDoubting they'll even disable it at all. reply nerdjon 1 hour agorootparentIt’s one thing to be critical of the feature. But this is a pretty cut and dry announcement. There isn’t any ambiguity they could stand behind if they are lying. I would fully expect it will be disabled by default (for now) reply bonton89 1 hour agorootparentThey'll just say it is a bug when it is turned on. reply LegitShady 1 hour agoparentprev\"game pass is only available with Recall enabled!\" \"microsoft office features y and j require Recall! please click here to enable it\" etc etc reply autoexec 1 hour agoprevLawyers, law enforcement, and three letter agencies everywhere are going to be extremely disappointed by this development. reply russdpale 1 hour agoparentand abusive partners/stalkers. reply webwielder2 1 hour agoprevnext [3 more] [flagged] iaabtpbtpnn 1 hour agoparentBut Satya got the stock price so high! reply satvikpendem 1 hour agorootparentMy 401k is doing great! The reality is that most people who aren't tech people don't care about the changes Microsoft has done, and shareholders especially don't care. Clearly what Satya is doing is working. reply resource_waste 1 hour agoprevOh man this is totally going to affect: >My workplace It wont affect me personally, because I dont use crappy operating systems on my personal time. Microsoft products are just an efficiency loss, I still bill the same. I literally get everything done faster on Fedora, no linux prayer needed anymore. Its just better. reply leprials 1 hour agoprevHopefully any debloat tools will remove it quickly. Can't wait until Microsoft force pushes this spyware to the masses. reply aikinai 1 hour agoprevI switched to Macs in 2006 and haven’t felt like Windows’ grass is greener once since then. Until today. Maybe it shouldn’t be on by default, but this looks amazing. reply nativeit 37 minutes agoprevMicrosoft keeps attempting to violate HIPAA on my clients’ behalf. Before this, they turned on OneDrive backups via updates, and began moving sensitive documents onto their servers without prior authorization or consent. I documented the incident, because I honestly wasn’t sure whether or not a lawsuit would result from it. I notified Microsoft, but never got a response. reply bongodongobob 28 minutes agoparentIf your clients are storing sensitive PII on their desktop or my documents folders, they're already likely way the fuck out of compliance. Nice FUD though. reply modeless 1 hour agoprev [–] It is puzzling to me that so many people seem to think this concept has no value. To me the concept is obviously good and something I have wanted for a long time. Of course the security of the implementation is important and I agree with some of the criticism there. But I see a lot of people arguing that the feature is worthless, or that it doesn't make sense at the OS level, or that Microsoft specifically should not be allowed to add it to Windows, and I have to strongly disagree. reply A4ET8a8uTh0 1 hour agoparentHmm. I think I can respond here. No one is really saying this feature has no value. For a user, there is value to being able to get to a previous point in time. That feature, however, is clearly not very well designed and implemented if it took days for it to be cracked on the internet for everyone to see. If I could trust that it STAYS local, maybe I would be less paranoid. But this is MS we are talking about. Personally, I am glad this thing was created. It may be finally make people hesitate over the evolution of PCs. reply sseagull 50 minutes agorootparent> clearly not very well designed and implemented if it took days for it to be cracked on the internet for everyone to see I really don't understand this line of thinking. What was cracked? That the database is readable, unencrypted? How could it be encrypted and usable at the same time? > If I could trust that it STAYS local This I agree with. While it's local now, not trusting MS is a valid belief, given their past behavior. If they feel sending some of the info to the cloud could get them $$$, then they will do it. Although I feel regulators might be pretty quick on this one... reply A4ET8a8uTh0 31 minutes agorootparent<< I really don't understand this line of thinking. What was cracked? That the database is readable, unencrypted? How could it be encrypted and usable at the same time? I am admittedly mildly confused by this response. Do online portals typically use unencrypted passwords? Do they let data flow unecrypted? Are those portals somehow unusable? Could you elaborate a little bit? It is possible I am misunderstanding your point. reply sseagull 19 minutes agorootparentI have only been somewhat paying attention, but there were lots of stories about someone \"cracking\" the implementation of Recall and getting access to the locally-stored database. The criticism is that it is easily accessible, but it's hard for me to imagine it any other way and have it still be useful. It's still encrypted at rest, but must be unencrypted for data to be written to it. There is plenty to criticize about Microsoft, but that one seems manufactured. As far as I know, the database is local, and Recall does not use the cloud at all. That also means that you can't view the history from one computer on another. But I agree that trust that it will stay that way is not particularly wise. reply A4ET8a8uTh0 9 minutes agorootparent<< \"cracking\" the implementation of Recal I think you have a point there. Would you accept reverse engineering[1] as a more accurate term instead of cracking? << I have only been somewhat paying attention We are in the same boat. I saw the thing pop in my feeds in the past weeks. I skimmed it, thought it was a bad idea, but since I don't have a PC that would be affected, mostly ignored it. I think I only pay more attention today, because it is the weekend and somehow my testing is not ready for me.. [1]https://en.wikipedia.org/wiki/Reverse_engineering [2]https://www.wired.com/story/microsoft-windows-recall-privile... reply Tool_of_Society 1 hour agorootparentprevIndeed since this is MS you can guarantee this is just a another step in them expanding their ability to monitor your habits for further monetization. reply mschuster91 1 hour agoparentprevThe concept itself has value, but the ethical and legal concerns are severe, not to mention the issue of Recall also capturing sensitive stuff like passwords. Microsoft, Google, Apple - everyone is scared shitless of some AI startup kicking their nutsacks, and is launching products that should have gone through extensive ethics discussions beforehand in a matter of weeks. reply russdpale 1 hour agorootparentpasswords are the last of it, think about women inquiring about abortions in states where they aren't legal. Or people trying to get away from an abusive partner, on and on it goes. reply LegitShady 1 hour agoparentprev [–] the concept is valuable but so ripe for abuse that even it existing at all is a threat to everyone's privacy. I have been a windows user basically my whole life. 3 years ago I got an ipad pro (2018, 12.9\") for drawing and I hate the operating system. 7 months ago I got a steam deck and its fine for games but doing anything in the OS is confusing and annoying. Microsoft announced recall and suddenly I'm using a spare computer to test linux distros, and I suck at everything to do with linux and I'm doing it anyways. It's too dangerous, to much an invasion of privacy, and too easily enabled completely outside of my control. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Microsoft has changed its Recall feature, which records user activity for AI analysis, to an opt-in setting after backlash over security concerns.",
      "Critics had labeled Recall as potential spyware, citing risks of unauthorized access to sensitive data; now, user authentication via PIN or biometrics is required to enable or access Recall data, which will remain encrypted.",
      "Despite these changes, privacy risks remain, especially in legal contexts, and this move follows a series of security incidents at Microsoft, with CEO Satya Nadella emphasizing security as a top priority."
    ],
    "commentSummary": [
      "Microsoft is disabling its Recall feature by default due to significant privacy and security concerns raised by users.",
      "Critics argue that Recall captures sensitive information, such as emails and passwords, likening it to invasive browser history tracking, reflecting broader issues of diminishing privacy awareness.",
      "The discussion highlights the need for high privacy standards and responsible data handling by companies, with users expressing distrust towards tech companies' data practices and considering alternatives like Linux due to privacy issues."
    ],
    "points": 229,
    "commentCount": 189,
    "retryCount": 0,
    "time": 1717778848
  },
  {
    "id": 40608269,
    "title": "How GPT-4o Encodes Images: A Deep Dive into Tokenization and Embedding Strategies",
    "originLink": "https://www.oranlooney.com/post/gpt-cnn/",
    "originBody": "A Picture is Worth 170 Tokens: How Does GPT-4o Encode Images? by Oran Looney June 5, 2024 Machine Learning LLM CNN Embeddings Number of Feature Dimensions Embedding Images Strategy 1: Raw Pixels Strategy 2: CNN Experimental Validation Pyramid Strategy Optical Character Recognition Conclusion Postscript: Alpha Channel Shenanigans Here’s a fact: GPT-4o charges 170 tokens to process each 512x512 tile used in high-res mode. At ~0.75 tokens/word, this suggests a picture is worth about 227 words—only a factor of four off from the traditional saying. (There’s also an 85 tokens charge for a low-res ‘master thumbnail’ of each picture and higher resolution images are broken into many such 512x512 tiles, but let’s just focus on a single high-res tile.) OK, but why 170? It’s an oddly specific number, isn’t it? OpenAI uses round numbers like “\\$20” or “\\$0.50” in their pricing, or powers of 2 and 3 for their internal dimensions. Why choose a numbers like 170 in this instance? Numbers that are just dropped into a codebase without explanation are called “magic numbers” in programming, and 170 is a pretty glaring magic number. And why are image costs even being converted to token counts anyway? If it were just for billing purposes, wouldn’t it be less confusing to simply list the cost per tile? What if OpenAI chose 170, not as part of some arcane pricing strategy, but simply because it’s literally true? What if image tiles are in fact represented as 170 consecutive embedding vectors? And if so, how? Embeddings The first thing to recall about the transformer model is that it operates on vectors, not discrete tokens. The inputs have to be vectors, or the dot product similarity at the heart of the transformer wouldn’t make any sense. The whole concept of tokens is a pre-processing step: text is converted to tokens and tokens are converted to embedding vectors by an embedding model before they even hit the first layer of the transformer model. For example, Llama 3 uses 4,096 feature dimensions internally. Consider the sentence, “My very educated mother just served us nine pizzas.” It gets converted into 10 integer tokens (counting the period) by BPE, then those are each converted into 4,096-dimensional vectors by an embedding model, resulting in a 10x4096 matrix. That’s the “real” input into a transformer model. But there’s no law that says that these vectors must come from a text embedding model. It’s a strategy that works well for text data, but if we have data in a different format that we want to feed into a transformer then we can simply use a different embedding strategy. We know that OpenAI has been thinking along these lines because in 2021 they released the CLIP embedding model. CLIP embeds both text and images into the same semantic vector space, allowing you to use cosine similarity to find images related to text strings, or images which are semantically similar to other images. You can try the demo on hugging face to get a feel for how it works: However, CLIP embeds the entire image as a single vector, not 170 of them. GPT-4o must be using a different, more advanced strategy internally to represent images (and likewise video, voice, and other kinds of data; that’s why it’s “omnimodal.”) Let’s see if we can’t deduce what that strategy might be for image data in particular. Number of Feature Dimensions Let’s start by guesstimating the number of dimensions used internally by GPT-4o to represent embedding vectors. We can’t know the real number for certain because it’s proprietary, but we can make some reasonable assumptions. OpenAI seems to likes powers of 2, sometimes with a single factor of 3 mixed in. For example, they used 1,536 for ada-002 embeddings or 3,072 for text-embedding-3-large. GPT-3 is known to use 12,288 dimensions throughout. It’s probable that GPT-4o either kept or increased that parameter. It doesn’t seem likely that the number of embeddings would have gone down from GPT-3 to GPT-4o, but it’s possible. Releases like GPT-4 Turbo were actually faster and cheaper than earlier version, and a reduction in embedding dimension may have been part of that if the developers had benchmarks showing that the smaller size was just as good in terms of quality. “Interest rates may go up, they may go down, or they may stay the same. I’m sorry, but I really can’t be any more vague than that.” —Alan Greenspan Given all that, it’s likely that the number of feature dimensions used inside of GPT-4o is one of these: Dimension Prime Factors $1{,}536$ $3 \\cdot 2^9$ $2{,}048$ $2^{11}$ $3{,}072$ $3 \\cdot 2^{10}$ $4{,}096$ $2^{12}$ $12{,}228$ $3 \\cdot 2^{12}$ $16{,}384$ $2^{14}$ $24{,}576$ $3 \\cdot 2^{13}$ For the sake of argument, I’ll assume that GPT-4o is using 12,228 for the dimension of its embedding vectors. It doesn’t really matter if we’re off by a factor of 2 or 4; the same arguments will work. Embedding Images Image tiles are square, so are likely represented by a square grid of tokens. 170 is very close to $13 \\times 13$. The extra token could a single embedding vector which encodes a kind of gestalt impression of the entire image, exactly as CLIP does (and similar to their strategy of using an 85 token “master thumbnail” for each image.) So, the question is, how do we go from 512x512x3 to 13x13x12228? Strategy 1: Raw Pixels Here’s an extremely simple way to stuff an image into a vector space: Divide the 512x512 image into a 8x8 grid of “mini-tiles.” Each mini-tile is 64x64x3; flatten it a vector of dimension 12,228. Each mini-tile is a single embedding vector. The entire image tile is represented as 64 consecutive embedding vectors. There are two problems with this approach: 64 ≠ 170, and it’s extremely stupid. By “extremely stupid” I mean that it doesn’t make any sense to embed using raw RGB values and then just cross your fingers and hope the transformer will sort it out. Transformers aren’t really designed to handle the spatial structure of 2D images, especially not when it’s embedded in such a braindead way as this. To see why, imagine the image is shifted a few pixel to the left. The dot product between the embedding vectors of the original and shifted images would immediately drop close to zero. The same would happen if we resize the image. Ideally we’d want a model that was robust to these kinds of transforms—we’d like it to have translational and scale invariance, to use the technical jargon. Strategy 2: CNN Luckily, there already exists a model with those characteristics, with over a decade-long track record of successfully handling image data: the Convolutional Neural Network. (Here, I’m using the term to describe the broad family of deep learning models which use convolution layers somewhere inside them.) Just to get a sense of what the options are, let’s take look at a classic CNN architecture introduced in 2012, AlexNet: The basic building blocks are: Convolution Layer. These scan over an image in $k \\times k$ sized blocks, training a small neural network. Max Pool Layer. These also look at $k \\times k$ block, but simply take the maximum value from each. You should spot two key trends as we move into the deeper layers of the network: the height and width get smaller, while the number of “channels” (sometimes called “filters”) gets larger. That means we’re incrementally digesting many low-level features into fewer high level concepts until, at the very end, AlexNet has turned the entire image into a single categorical concept representing something like a “cat” or “dog.” CNNs are essentially funnels that squeeze the lemons of raw pixels into the lemonade of semantic vectors. If you’re following my somewhat strained analogy, you should see how a CNN can turn an image into a single embedding vector. To see how (and why) a CNN can turn an image into many embedding vectors, let’s take a look at a slightly newer (circa 2018) CNN architecture, one that’s a little closer in spirit to what we’ll need for GPT-4o. It’s called YOLO, short for “You Only Look Once.” Here, the notation “xN” means that the entire block is repeated N times. YOLOv3 is 10 times as deep as AlexNet but is still very similar in some regards. It has a somewhat more modern design: stride 2 convolutional layers instead of max pooling layers to reduce dimensionality, residual layers to preserve good gradients in very deep networks, etc. But the key difference is that it doesn’t reduce the image to a single flat vector, but stops at 13x13. There are no fully connected layers after that; the output of YOLOv3 is in fact 169 different vectors, laid out in a 13x13 grid, each of dimension 1,024, and each representing the class (and some bounding box data we’ll ignore) of the object found in or near a particular cell of the grid. This means that YOLO doesn’t see just one object in the image—it can see many in a single pass. That’s why it’s said to “only look once.” These examples give us a rough sense of what GPT-4o’s (hypothetical) image embedding CNN might be shaped like. All we have to do now is play a little game of connect the dots: how do we go from 512x512x3 to 13x13x12228 using standard CNN layers? The moves in this game are the standard building blocks we’ve seen in the above CNN architectures. We can choose the layer types and play around with hyperparameters like kernel size, stride length, padding strategy, etc. Note that we ignore things like residual layers, repeated blocks, batch/layer normalization, or 1x1 convolutional layers as these don’t affect the overall tensor size. The goal is to suggest a workable CNN architecture that connects the known input size (512x512 images with 3 RGB color channels) to the assumed output shape (13x13 embedding vectors with 12,228 dimensions each.) I tried several different variations, but most of these required special cases on one or more layers to “fit.” Until I found this one, which steps down elegantly with no special cases at all: It very neat, isn’t it? It’s almost identical to AlexNet, and it steps down from from 512 to 13 in five identical repeating blocks, while simultaneously quadrupling the number of channels with each block to hit 12,228 on the bottom layer. Unfortunately, it also feels a little outdated due to the 5x5 kernels and max pool layers. AlexNet was a breakthrough in 2012 but I would be suprised if OpenAI was using something similar in 2024. Here’s an alternative that almost worked (got to 12x12 instead of 13x13) while staying closer to the more modern YOLO design: While impossible to prove, these speculative designs demonstrate that there are plausible CNN architectures that could represent an image as a $k \\times k$ grid of embedding vectors. Experimental Validation Does GPT-4o really see a 13x13 grid of embedding vectors? I invented a task, loosely inspired by Zener cards, to test this. The task is to identify the color and shape for every symbol on a grid in an image. A simple program generates test grids that look like this: I then used this prompt to obtain comparison data: \"\"\"Divide this image into a {k}x{k} grid and describe the shape and color of each cell. Report your results as a JSON array of arrays (no objects or row labels) and use all lowercase.\"\"\" If the 13x13 hypothesis is correct, GPT-4o should do well on this task up to about 13 and then performance should degrade. The CNN isn’t literally dividing the image up into a grid so it might start to degrade before then, and might struggle more with even grid sizes than odd because the cells won’t be aligned. However, that is not in fact what happens. Instead, performance is perfect for 5x5 grids and smaller (e.g. 2x2, 3x3, 4x4) but begins to degrade rapidly after that. By the time we reach 13x13 it was no better than chance. For example, here is a 7x7 grid: For which GPT-4o returned this: [ [\"blue diamond\", \"purple triangle\", \"red plus\", \"purple circle\", \"orange square\", \"purple circle\", \"blue circle\"], [\"blue diamond\", \"red circle\", \"green plus\", \"red square\", \"orange diamond\", \"blue triangle\", \"purple plus\"], [\"blue diamond\", \"orange triangle\", \"red square\", \"orange square\", \"blue plus\", \"purple circle\", \"blue circle\"], [\"green diamond\", \"blue circle\", \"orange circle\", \"green circle\", \"purple diamond\", \"green triangle\", \"orange diamond\"], [\"purple square\", \"purple circle\", \"green plus\", \"green diamond\", \"green circle\", \"blue triangle\", \"purple triangle\"], [\"red plus\", \"red triangle\", \"purple circle\", \"blue triangle\", \"orange triangle\", \"red diamond\", \"orange diamond\"], [\"orange plus\", \"blue diamond\", \"green triangle\", \"green plus\", \"green triangle\", \"purple diamond\", \"purple square\"] ] It got 38⁄49 correct—an accuracy of 76%. The exact pattern of hits and misses looks like this (yellow is correct, purple incorrect): Performance continues to degrade as the grid size increases and by the time we get to the 13x13 grid: The results are no better than chance: Does that mean I was wrong about 169 tokens representing a 13x13 grid? Yes. Yes it does. My disappointment is immeasurable and my day is ruined. “The great tragedy of science: the slaying of a beautiful hypothesis by an ugly fact.” —Thomas Huxley But the 5x5 grid results are suggestive. GPT-4o really can keep track of 25 distinct objects and their absolute positions within in an image. Maybe the basic concept is right; I just got the dimension wrong. It would be easy to tack on another couple of layers to our CNN to get down to 5x5 instead of 13x13: How could we structure the output to reach 170 tokens if we assume we only use 5x5 grids and smaller? Pyramid Strategy One way to get close to both 85 and 170 is to assume that we encode the image in a series of increasingly granular levels, like a pyramid. We start with one embedding vector to capture a gestalt impression of the whole image, add a 3x3 to capture left/middle/right and top/middle/bottom, then adding a 5x5, 7x7 etc. This strategy gets us very close to 85 tokens for the ‘master thumbnail’ if we stop at 7x7: $1^2 + 3^2 + 5^2 + 7^2 = 1 + 9 + 25 + 49 = 84$ And very close to 170 if we add one final 9x9 grid: $1^2 + 3^2 + 5^2 + 7^2 + 9^2 = 1 + 9 + 25 + 49 + 81 = 165$ If we throw in an ad hoc 2x2 grid for the 512x512 tile and assume one specialfor each, we can get a perfect match: $1 + 1^2 + 3^2 + 5^2 + 7^2 = 1 + 1 + 9 + 25 + 49 = 85$ $1 + 1^2 + 2^2 + 3^2 + 5^2 + 7^2 + 9^2 = 1 + 1 + 4 + 9 + 25 + 49 + 81 = 170$ This scheme lacks any sort of delimiters for the start and end of a row, but I think that could be handled with positional encoding similar to the way RoPE is used to encode position information for text tokens, but in 2D. The above takes only odd grid sizes and goes well beyond 5x5; given that the Zener grid performance starts to fall off after 5x5 this does not entirely concord with the evidence. As an alternative, we could try taking all the grids (even and odd) up to 5x5: This approach gives us 55 tokens: $1^2 + 2^2 + 3^2 + 4^2 + 5^2 = 55$ If we assume 3 tokens per mini-tile and a delimiter token between each, we can get to 170: $3 \\times (1^2 + 2^2 + 3^2 + 4^2 + 5^2) + 5 = 170$ This isn’t fully satisfactory on numerological grounds but does jive well with the empirical results. The pyramid strategy has a lot of intuitive appeal—it feels like an almost “obvious” way to encode spatial information at different zoom levels - and may explain why it does so well with the 5x5 grid and below and so poorly on 6x6 and above. It’s maddening that every hypothesis seems to come tantalizingly close to explaining everything but the numbers never quite seem to work out neatly… Still, these pyramid strategies are the best I’ve been able to come up with. Optical Character Recognition The one thing that none of the above hypotheses explain is how GPT-4o is doing OCR. YOLO and CLIP can’t natively do OCR, and the strategies suggested above seem like they would struggle for the same reasons. I mean, if it can’t read off 36 symbols in a neat 6x6 grid from an image, it certainly can’t read off a several hundred text characters flawlessly. State-of-the-art OCR engines like Tesseract do a great deal of work to find bounding boxes and strips of characters, and then run specialized character recognition models along those strips, one character or word at a time. They aren’t just big CNNs. And yet GPT-4o patently can do high-quality OCR: it can transcribe long blocks of text, read handwritten text, or text which has been shifted, rotated, projected, or partially occluded. I have a simple theory to explain that: I think OpenAI is running Tesseract (or their own in-house OCR) and feeding the identified text into the transformer alongside the image data. I mean, that’s what I would do. This would explain why the early versions were so easily confused by text hidden in images: from its POV, that text was part of the prompt. (This is fixed now; GPT-4o is good at ignoring malicious prompts hidden inside images.) However, this does not explain why there’s no charge per token for the text found in an image. Interestingly enough, it’s actually more efficient to send text as images: A 512x512 image with a small but readable font can easily fit 400-500 tokens worth of text, yet you’re only charged for 170 input tokens plus the 85 for the ‘master thumbnail’ for a grand total of 255 tokens—far less than the number of words on the image. This theory explains why there is additional latency when processing images. The CNN would be essentially instantaneous, but 3rd-party OCR would add additional time. By the way, (and I’m not saying this proves anything) but the Python environment used by the OpenAI code interpreter has PyTesseract installed. You can literally just ask it to run PyTesseract on any image you’ve uploaded to get a second opinion. Conclusion Well, we’ve made a lot of speculative hay out of what is essentially only one morsel of hard fact: that OpenAI used the magic number 170. However, there does seem to be a complete plausible approach—very much in line with other CNN architectures such as YOLO—for mapping from image tiles to embedding vectors. As such, I don’t think 170 tokens is just an approximation used to bill for roughly the amount of compute it takes to process an image. And I don’t think they’re concatenating layers to join image and text data the way some other multi-modal models do. No, I think GPT-4o is literally representing 512x512 images as 170 embedding vectors, using an CNN architecture that’s a mixture of CLIP and YOLO to embed the image directly into the transformer’s semantic vector space. When I started this article, I was entirely convinced that I had cracked it entirely, that I was going to find that the 170 tokens were for a 13x13 grid and one additional “gestalt impression” token. That got blown out of the water when performance on the Zener task started to degrade after 5x5—whatever they’re doing internally, it seems to be a lot smaller than 13x13. Still the analogy to YOLO is compelling, and the performance on the 5x5 Zener task all but confirms that they’re doing some kind of grid. This theory has a lot of predictive power in other areas as well: it explains how GPT-4o is able to handle multiple images, and tasks like comparing two images, for example. It explains how it’s able to see multiple objects in the same image, but gets overwhelmed when there are too many objects in a busy scene. It explains why GPT-4o seems extremely vague about the absolute and relative positions of separate objects within the scene, and why it can’t count objects accurately in images: when an object spans two adjacent grid cells the same classes are activated in both so it’s not sure if it’s one object or two. Ironically, the one thing this theory can’t cleanly explain is the question which motivated this article in the first place: why 170 tokens in particular? The pyramid theory (1x1 + 2x2 + 3x3 + 4x4 + 5x5) was the best I was able to come up with, and it’s not particularly neat. I’d love to hear from anyone who has a theory that fits a little better (or even actual knowledge, assuming it doesn’t run afoul of an NDA!) Postscript: Alpha Channel Shenanigans One other thing I noticed while working on this project is that GPT-4o ignores the alpha channel, resulting in somewhat counter-intuitive behavior. When I say, “ignores”, I don’t mean that it gets rid of transparency by compositing it onto some default background, the way an image editor might when converting PNG to JPG. No, I mean it literally just grabs the RGB channels and ignores the alpha channel. We can illustrate this with four carefully prepared images. For convenience, I’ve used HTML and CSS to display these images on top of a checkerboard pattern—the images themselves have flat, transparent backgrounds. However, half have transparent black backgrounds, and half have transparent white backgrounds. What do I mean by “transparent black” or “transparent white?” Well, when we represent an RGBA color with four bytes, the RGB bytes are still there even when alpha is 100%. Thus, (0, 0, 0, 255) and (255, 255, 255, 255) are in some sense different colors, even though there’s no situation where a correct renderer would display them differently since they’re both 100% transparent. Let’s ask GPT-4o what it “sees” on these four images: Black Text on Transparent Black Background GPT-4o Reads: “” Black Text on Transparent White Background GPT-4o Reads: “ENORMOUS” White Text on Transparent Black Background GPT-4o Reads: “SCINTILLA” White Text on Transparent White Background GPT-4o Reads: “” What’s going on here? The pattern that emerges is that GPT-4o can read the text if and only if the text color is different than the “color” of the transparent background. This tells us that GPT-4o disregards the alpha channel and only looks at the RGB channels. To it, transparent black is black, transparent white is white. We can see this even more clearly if we mess with an image to preserve the three RGB channels while setting the alpha channel to 100%. Here’s a little Pillow function to do that: from PIL import Image def set_alpha(image, output_path, alpha_value): # copy the image and ensure it's RGBA image = image.convert(\"RGBA\") # set the alpha channel of every pixel to the given value pixels = image.getdata() new_pixels = [(r, g, b, alpha_value) for r, g, b, a in pixels] image.putdata(new_pixels) return image I used that to make the two images below; they have identical RGB data, and only differ in the alpha channel: Alpha Channel = 255 Alpha Channel = 0 GPT-4o has no trouble seeing the hidden platypus: You can try downloading the hidden_platypus.png image and dropping it into ChatGPT yourself; it will correctly describe it. You may also note the image is 39.3 KB, the same size as platypus.png even though PNG compression should have made it much smaller if it was really a perfectly blank, transparent image. Or you can use the above function to set the alpha channel back to 255, recovering the original image. I’m not sure if this is bug but it’s certainly surprising behavior; in fact, it feels like something a malicious user could use to smuggle information past humans and directly to GPT-4o. However, GPT-4o is much better at detecting and ignoring malicious prompts hidden in images than GPT-4v was: (You can find other examples of GPT-4o successfully detecting and ignoring malicious prompts hidden in images in my gallery of GPT-4o test images generated by my image_tagger utility.) So, even if it is a bug, it’s not obvious it can be exploited. Still, it would be less surprising in GPT-4o “saw” the same thing that a human would in a browser.",
    "commentLink": "https://news.ycombinator.com/item?id=40608269",
    "commentBody": "How Does GPT-4o Encode Images? (oranlooney.com)218 points by olooney 6 hours agohidepastfavorite78 comments ComputerGuru 2 hours agoWe desperately need a modern open source replacement for tesseract built on current SoTA ML tech. It is insane that we are resorting to using LLMs — which aside from being the wrong tool and far too overpowered for the job also are prone to hallucinations, have insanely expensive training and inference costs, etc — for this purpose because the “best” non-LLM solution is so bad it can’t even correctly ocr monospaced hi-res scans of ascii text with sufficient accuracy. reply rvnx 2 hours agoparentOne good self-hosted OCR is PaddleOCR, https://github.com/PaddlePaddle/PaddleOCR Beats everything else, truly international and multi-lingual, including Chinese (as it is made in China) reply paul-tharun 1 hour agorootparentIt is insanely fast compared alternatives and has really high accuracy even on new tasks without any training. Their PaddleLayout models are also miles ahead compared to LayoutParser or TableTransformers in both inference speed and output quality reply rfoo 1 hour agoparentprevThere's certainly smaller and even better models for OCR. But the whole \"point\" of LLM (forget it, it's not AGI) is you don't need to make many specialized models and cursed pipelines anymore, to solve a definitely-in-reach-without-LLM problem your farmer neighbor wants to pay $500 for. Before LLM it's not going to be done as it takes more than $500 engineer hours. Now we just brute force. Sure, more compute, but we get it done! I guess your OCR dream is covered by this. reply orbital-decay 46 minutes agoparentprevA good open source model for handwriting recognition is sorely missing as well. reply nine_k 37 minutes agorootparentOften in humans, too, depending on the badness of the particular handwritten word. reply asadm 2 hours agoparentprevhmmm I haven't tried but does apple's OCR api do better here? ie. is it possible to do it. reply rgovostes 54 minutes agorootparentThe API: https://developer.apple.com/documentation/vision/recognizing... In my experience it works remarkably well for features like scanning documents in Notes and in copying or translating text embedded in images in Safari. It is not open source, but free to use locally. Someone has written a Python wrapper (apple-ocr) around it if you want to use it in other workflows. The model files might be in /System/Library/PrivateFrameworks/TextRecognition.framework if you wanted to port them to other platforms. reply enjoylife 19 minutes agoprev> Interestingly enough, it’s actually more efficient to send text as images: A 512x512 image with a small but readable font can easily fit 400-500 tokens worth of text, yet you’re only charged for 170 input tokens plus the 85 for the ‘master thumbnail’ for a grand total of 255 tokens—far less than the number of words on the image. Sounds like an arbitrage opportunity for all those gpt wrappers. Price your cost per token the same, send over the prompt via image, pocket the difference? reply valine 4 hours agoprevLlava1.6, IntenVL, CogVLM2 can all do OCR with nothing but tiled image embeddings and an LLM. Feeding in OCR results from tesseract improves the reliability of the transcript, especially for long strings of random characters, but it’s not strictly necessary for the model to read the text out of the image. Clip embeddings can absolutely “read” text if the text is large enough. Tiling enables the model to read small text. reply Onawa 4 hours agoparentDo you know of any guides or tutorials to doing this? I tried using the MiniCPM model for this task, but it just OCRed a tiny bit of information then told me that it couldn't extract the rest. reply pwillia7 3 hours agorootparentI bet you could get this working in https://github.com/comfyanonymous/ComfyUI I have done some other LLava stuff in it reply pests 12 minutes agorootparentthanks been trying to remember the name of this project for weeks now reply 3abiton 3 hours agorootparentprevI thought ComfyUI was mainly for SD. I should get into the game again. reply lagniappe 3 hours agorootparentYou can build just about anything with it reply riemannzeta 4 hours agoprevLove this curious and open-minded exploration of how this stuff works. The pyramid strategy loosely tracks with renormalization group theory, which has been formally studied for years as a method of interpreting machine learning models: https://arxiv.org/abs/1410.3831 I love the convergence we're seeing in the use of models from different fields to understand machine learning, fundamental physics, and human consciousness. What a time to be alive. reply simonw 5 hours agoprevSomething I don't get is why OpenAI don't provide clear, comprehensive documentation as to how this actually works, I get that there's competition from other providers now so they have an instinct to keep implementation details secret, but as someone building on their APIs this lack of documentation really holds me back. To make good judgements about how to use this stuff I need to know how it works! I had a hilarious bug a few weeks ago where I loaded in a single image representing multiple pages of a PDF and GPT-4 vision effectively hallucinated the contents of the document when asked to OCR it, presumably because the image was too big and was first resized to a point where the text was illegible: https://simonwillison.net/2024/Apr/17/ai-for-data-journalism... If OpenAI had clear documentation about how their image handling works I could avoid those kinds of problems much more effectively. reply Onawa 5 hours agoparentI was trying to figure out this exact same issue. OCR on a PDF worked great, up until a certain point when it just started hallucinating like crazy. I was working on a whole pipeline to just feed in a PDF one page at a time to try and get around this issue. Otherwise, the OCR works absolutely fantastic compared to all other other tools I've been trying lately. These include OCRmyPDF (Tesseract), SuryaOCR, and some of the models on the Visual LLM Leaderboard. I've also seen some people recommend Paddle OCR, but I find their documentation to be lacking and I haven't got that one working yet to evaluate. reply raybb 4 hours agorootparentSimon wilson recently had a thread going through some of the options here https://x.com/simonw/status/1797526667797442773 reply Onawa 4 hours agorootparentFunny enough, Simon Willison is the op of this comment thread lol. reply infecto 5 hours agorootparentprevFor document text/table extraction, nothing beats the quality from the cloud providers. It can get costly but the accuracy is much higher than what you will find using an openai API. reply ilaksh 21 minutes agoparentprevThere is an effectively infinite number of possibilities of things people could throw at it and they can't know ahead of time whether your use case will work or not. Even if they told you exactly how it worked, you wouldn't know for sure until you tried it. And giving a vague explanation wouldn't help you either. reply infecto 5 hours agoparentprevBut they do document that the images are resized and give you some rough guidelines on how you should be sizing your images. Low resolution is 1024 x 1024 with no tiling and High Resolution starts at 2048 x 2048 which then gets tiled. It could use further documentation but it is enough to know more than one page should never be used via the API. reply alach11 4 hours agorootparentRight. But I still have a lot of questions. How does the model handle when something important overlaps multiple tiles in high-resolution mode? Am I better off doing the tiling myself with some overlap? reply nolok 3 hours agoparentprevThe fact that it's so eager to hallucinate random things that sounds plausible enough if you're not paying attention without warning you or giving any error should make people reconsider using it for \"data journalism\" or similar. If you make your system and it \"works\", then how will you see the one time out of X where it confidently provides you false information that you happily use because it usually work ? reply TeMPOraL 3 hours agorootparent> how will you see the one time out of X where it confidently provides you false information that you happily use because it usually work ? You don’t. You treat it like you would a human worker: set your process to detect or tolerate wrong output. If you can't, don't apply this tool to your work. reply IanCal 3 hours agorootparentThis is true but misses a key fact, that typical llm errors are different to human errors. Not that they're worse or better but just that you need to understand where and when they're more likely to make mistakes and how to manage that. reply rafaelero 4 hours agoprevThey are very likely using VQVAE to create a dictionary of tokens and then just converting images into them with an encoder. reply lisperforlife 3 hours agoparentWhy is this not the top comment? FAIR published their C3MLeon paper about decoder-only autoregressive models that work with both text and image tokens. I believe GPT-4o's vocabulary has room for both image and audio tokens. For audio tokens, they probably trained an RVQ-VAE model like Encodec or Soundstream. reply cs702 4 hours agoprevOne possibility is that mapping images to a token embedding consumes ~170x more compute+space than mapping a token id. Another possibility is that OpenAI is mapping each image to ~170 vectors in an embedding space that is shared with token IDs. If that's the case, the architecture of the image-to-fixed-number-of-tokens model has not been disclosed. It could be a standard CNN, a ViT-like model, an autoencoder, a model that routes a variable number of vectors with RGB data to a fixed number of vectors, or something else that has not yet been ublished. The whole thing is likely trained end-to-end. reply CuriouslyC 46 minutes agoparentAt some point we're going to go from tokens to embeddings for everything. I saw some research on variable length embeddings, I wouldn't be surprised if someone generated a huge embedding space, did some form of PCA on generated embeddings, threw away low eigenvalue vectors, then trained a distilled model that generated variable length embeddings directly from that. reply cs702 5 minutes agorootparent> At some point we're going to go from tokens to embeddings for everything. Yes, I agree. Further down the road, I think we will end up finding interesting connections to the symbolic approaches of GOFAI, given that the \"embedding\" of a token, object, concept, or other entity is basically a kind of symbol that represents that token, object, concept, or entity in some vector space. reply cs702 1 hour agoparentprev*that has not yet been published reply HarHarVeryFunny 3 hours agoprevI don't think a 13x13 tiling (of N channels/features) can be ruled out just because it can't recognize a grid of 13x13 objects. There is presumably a lot of overlap between the receptive fields of the tiles (due to kernel step sizes). A pyramid of overlapped tiling resolutions is of course possible too. reply geor9e 3 hours agoprevNit: the implied premise that this isn't a beautiful and skilled painting https://www.oranlooney.com/post/gpt-cnn_files/malicious_dogs... reply olooney 19 minutes agoparentThe painting is Charlie and Sheba, from the Museum of Bad Art: https://museumofbadart.org/zoo/ I found it while Googling for a test image for the malicious prompt test, where it was used as the lead photo for this blog post: https://www.artsy.net/article/artsy-editorial-bad-art-good There's definitely something eye-catching about it that really makes it stand out from the crowd. reply imranhou 46 minutes agoprevNot to be nit-picky but double checking myself, isn't a token just 0.75 words, so 170 token would be 127 words and not 227? reply simonw 5 hours agoprevThe way this tests GPT-4o performance by feeding in a 7x7 grid of colored shapes and requesting them back as JSON (about half way down the page) is really clever. reply blixt 5 hours agoparentI did something similar when GPT-4V came out, partially with the goal to figure out the input format (I did not get anywhere other than \"magic vectors\"), but also to roughly estimate the amount of data you can get back out of a 512x512 (the low quality option) image. What I found is that you can sometimes get more text out of 85-token image than you can out of 85 tokens of text! That said, I think there will be plenty of edge cases where it actually loses some information, and maybe you could argue that if you remove every other word in the text, it could still restore the text. I never went deeper on this, but I believe there's something clever to be done in the context window with the fact that images are relatively cheap tokens-wise. reply _dark_matter_ 5 hours agorootparentThe author mentions this in the article, that more than 170 tokens of text can be pulled from an image. reply blixt 4 hours agorootparentAh, you're right! My bad! reply iknownothow 4 hours agoprevI'm probably wrong but the author may have have misunderstood input embeddings. Input embeddings are just dictionary lookup tables. The tokenizer generates tokens and for each token you find its embedding from the lookup. The author is speculating about an embedding model but in reality they're speculating about the image-tokenizer. If I'm not wrong the text tokenizer Tiktoken has a dictionary size of 50k. The image tokenizer could have a very large dictionary size or a very small dictionary size. The 170 tokens this image tokenizer generates might actually have repeating tokens! EDIT: PS. What I meant to say was that input embeddings do not come from another trained model. Tokens come from other trained models. The input embedding matrix undergoes back propagation (learning). This is very important. This allows the model to move the embeddings of the tokens together or apart as it sees fit. If you use embeddings from another model as input embeddings, you're basically adding noise. reply iknownothow 2 hours agoparentI've pondered a bit more about it and I was the one who was mistaken. I think the author made great observations. It's just that I don't want to go back to non token thinking. I don't want there to be a 13x13xE final output from the CNN. I really want there to be a visual vocabulary from which tokens are chosen. And I want this visual vocabulary to be fixed/untrainable/deterministic. That'd be very cool. But why only choose 13x13 + 1? :( I'm willing to bet that the author's conclusion of embeddings coming from CNNs is wrong. However, I cannot get the 13x13 + 1 observation out my head though. He's definitely hit on something there. I'm with them that there is very likely a CNN involved. And I'm going to put my bet on the final filters and kernel are the visual vocabulary. And how do you go from 50k convolutional kernels (think tokens) to always 170 chosen tokens for any image? I don't know... reply kolinko 1 hour agoparentprevInput embeddings are taken from a dictionary in case of text tokens, but they don’t need to be - they can be any vector really. reply iknownothow 1 hour agorootparentBut don't input embeddings need to undergo backprop during training? Won't the external-model's embeddings just be noise since they don't share embedding space with the model that is being trained? If the external-model also undergoes training along with the model then I think that might work. reply sva_ 4 hours agoprevGreat article. Perhaps some part of this magic number simply factors in the amount of compute necessary to run the image through the CNN (proportional to compute use per token in the LM). reply joelburget 4 hours agoprevVision transformers should be our default guess as to how GPT-4o works, yet this article never mentions them. reply blixt 5 hours agoprevI went through a similar journey back when GPT-4V came out. Here's an additional puzzle for you: GPT-4V knows the exact pixel dimensions of the image (post-resize since there is a max size for images in the pipeline, besides 512x512), but I'm 99% sure it's not provided as text tokens. How am I so sure? It's easy to get GPT to divulge everything from system prompt to tool details, etc. but I've tried every trick in the book and then some, multiple times over, and there is no way to get it to quote the dimensions as text. The only way to get it to give you the dimensions is to tell it to output a structure that contains width and height and just pick something reasonable, and they will \"randomly\" be the correct values: https://x.com/blixt/status/1722298733470024076 reply llm_trw 4 hours agoparent> It's easy to get GPT to divulge everything from system prompt to tool details, It's easy enough to get it to hallucinate those things. It doesn't actually tell them to you. reply blixt 4 hours agorootparentI'm well aware of that, but there are plenty of ways to induce verbatim quoting from \"hidden\" information, and mostly verify it (through sampling a large number of times in separate runs). Models are improving in truly hiding or ignoring information these days though. As the author of the article states, you'll have a hard time tricking GPT-4o to read text in images as instructions, most likely thanks to this research: https://openai.com/index/the-instruction-hierarchy/ I do feel pretty confident that when the model is happily spitting out its system prompt, and all metadata around the image, but not its pixel dimensions, that probably those dimensions were not provided in any system/assistant/tool message. So maybe part of the image embeddings also encode the pixel dimensions somehow (it would also help the model not think of the image as a squished square for non-1:1 images that have been resized to 512x512). reply dannyw 4 hours agoparentprevPerhaps images aren’t tokens at all… and 170 tokens is just an approximation of the compute cost. reply blixt 4 hours agorootparentI think that would have pretty serious implications for the transformer architecture though. If they're not embedded like text tokens, how would attention, etc work? And a conversation with multiple images back and forth? Not to mention with GPT-4o now having audio support as well. I would assume it does become tokens. reply qarl 4 hours agorootparentprevThey address this question in the article. reply surfingdino 5 hours agoprevOCR is hard https://www.vice.com/en/article/gvy4gb/one-mans-david-and-go... reply yorwba 4 hours agoprevIt would be interesting to see what happens when you slightly shift the grid of objects until they're split across multiple tiles, and how that affects accuracy. reply jmount 4 hours agoprevScanning images is quite the problem in the presence of compression (and now interpolation) https://www.bbc.com/news/technology-23588202 . reply tantalor 5 hours agoprev> CLIP embeds the entire image as a single vector, not 170 of them. Single token? > GPT-4o must be using a different, more advanced strategy internally Why reply freediver 5 hours agoparentThe embeddings do not offer the level of fidelity to recognize fine details on an image, hand writing for example. reply GaggiX 5 hours agoprevAn important aspect that is not considered in the article is that GPT-4o can generate images by itself (even though the feature is not enable to the public) meaning that it's very likely trained on sequential image tokens and the images are quantized using a VQGAN, my guess is that the VQGAN takes 512x512 images and outputs 13x13 tokens (169 image tokens + special token), the VQGAN can be a convolutional network like shown in the article, for a transformer-based VQGAN I cannot think of a configuration with overlapping patches where it would output 13x13 tokens on a 512x512 image (unless they just added a padding of 4 on the entire image and the patches are not overlapping). reply edude03 4 hours agoparentHow do we know it generates the images itself and isn’t passing the text to dalle? It’s supposedly how the current gpt4 model does listen mode (with whisper but same idea) reply GaggiX 3 hours agorootparentGo to the \"Explorations of capabilities\" and explore all the capabilities: https://openai.com/index/hello-gpt-4o/ You cannot have this level of control by prompting Dalle, also GPT-4o isn't using Whisper (older GPT-4s yes). reply rvnx 5 hours agoprevAuthor claims that the most likely is that there is Tesseract running behind ChatGPT-4v/o. There is no way that this is Tesseract. -> Tesseract accuracy is very low, it can barely do OCR on printed documents. reply kherud 4 hours agoparentShouldn't this theory be testable? The response time for an image of the same size should remain constant (assuming a generated response of constant size). You could then try to put an increasing amount of text inside of the image. If this text is fed to the LLM using OCR, the total amount of tokens grows. You should then be able to observe an increase in response time. reply jerrygenser 5 hours agoparentprevEven if tesseract accuracy is low, if the tesseract result in addition to the image is then passed to the LLM, it can result in a much more accurate OCR. For example, GPT4 with some vision capability would be able to fill in the incorrect OCR with the additional word co-occurrence understanding. I've tested this approach with purely text LLM to correct OCR mistakes and it works quite well. Also note that in some newer OCR pipelines that don't involve LLMs, there is a vision component and then a text correcting model that is in some ways similar to some forms of spell check, which can further improve results. reply lyu07282 4 hours agorootparentyou can tell that the OCR fails more in cases without natural language like with code/random characters. OAI seems to claim 4o is a fully end to end multimodal model, but we will never know for sure, we can't trust a single word OpenAI is saying. reply llm_trw 4 hours agoparentprevBecause no one knows how to prep the images. With the right file type and resolution I get under a single character error per 10 pages and it's been that good since the late 00s. reply alach11 1 hour agorootparentWith handwriting? With mixed fonts? Tesseract requires heavy customization and extension to perform reasonably on these workloads. The off-the-shelf options from major cloud providers blow it out of the water. reply yorwba 4 hours agorootparentprevHow do you prep the images? reply freedmand 4 hours agoparentprevAgreed. Tesseract is not able to handle handwriting or text that is distorted well, e.g. colored text over an image background — to the point that it would hurt any downstream LLM trying to make sense of the contents. It won’t even pick out bounding boxes. I doubt they are running an OCR model, but if they actually were it would likely be an in-house one trained with more modern techniques. reply RicoElectrico 4 hours agoparentprevYeah, Tesseract is barely production quality. reply lyu07282 4 hours agorootparentyeah it was SOTA in 2006, 18 years ago reply jascha_eng 3 hours agorootparentOther than proprietary models, what is better than it today? Just asking in case I ever need OCR and don't want to pay the cloud providers for it :D reply lyu07282 3 hours agorootparentcheckout https://github.com/mindee/doctr or https://github.com/VikParuchuri/surya for something practical multimodal llm would of course blow it all out the water, so some llama3-like model is probably SOTA in terms of what you can run yourself. something like https://huggingface.co/blog/idefics2 reply alach11 5 hours agoprevI really hope we see improvements to the resolutions large multimodal models can handle. Right now this patchwork approach leads to lots of unwieldly workarounds in applications. reply eminence32 5 hours agoprevI'm assuming that the tokens used to encode an image are entirely distinct from the tokens used to encode text. Does anyone know if this is actually the case? reply tempusalaria 5 hours agoparentIt’s probable that there is a separate vision encoder which projects the image tiles into the distribution space of the text tokenizer a la CLIP/LLava reply blixt 5 hours agoparentprevI would assume it has a \"mode\" token where it switches between text/image (and now audio), or you'd have to try to maximize the number of reserved tokens between multiple modes. GPT-4o did go from 100K to 200K vocabulary, but as far as I understand all of that vocabulary is in use for text (reducing the token cost for non-English). reply euclideanfire 3 hours agoprev [2 more] [flagged] chpatrick 3 hours agoparent [–] You can download plenty of open source models and try it yourself, OpenAI is just one step ahead. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article \"A Picture is Worth 170 Tokens: How Does GPT-4o Encode Images?\" by Oran Looney explores why GPT-4o charges 170 tokens to process each 512x512 image tile, equating this to about 227 words.",
      "It contrasts GPT-4o's advanced image encoding strategy with CLIP's simpler method and highlights the effectiveness of Convolutional Neural Networks (CNNs) for image data.",
      "The text discusses GPT-4o's performance on grid-based image analysis, proposes a pyramid strategy for improvement, and speculates on its OCR capabilities, suggesting it might use an external engine like Tesseract."
    ],
    "commentSummary": [
      "The discussion emphasizes the need for a modern, open-source OCR (Optical Character Recognition) solution, suggesting alternatives like PaddleOCR for better speed and accuracy compared to outdated tools like Tesseract.",
      "It critiques the inefficiency and high costs of using large language models (LLMs) for OCR tasks and mentions Apple's OCR API as a non-open-source option, highlighting user experiences with various OCR tools and issues with documentation and cloud service costs.",
      "The conversation also delves into advanced topics like image tokenization with models like VQVAE, computational complexity of image embeddings, and future trends in AI, with debates on GPT-4's image processing capabilities and the need for improved OCR accuracy for complex text."
    ],
    "points": 218,
    "commentCount": 78,
    "retryCount": 0,
    "time": 1717764867
  },
  {
    "id": 40608413,
    "title": "σ-GPTs: Dynamic Order in Autoregressive Models Enhances Efficiency",
    "originLink": "https://arxiv.org/abs/2404.09562",
    "originBody": "Computer Science > Machine Learning arXiv:2404.09562 (cs) [Submitted on 15 Apr 2024] Title:σ-GPTs: A New Approach to Autoregressive Models Authors:Arnaud Pannatier, Evann Courdier, François Fleuret View PDF HTML (experimental) Abstract:Autoregressive models, such as the GPT family, use a fixed order, usually left-to-right, to generate sequences. However, this is not a necessity. In this paper, we challenge this assumption and show that by simply adding a positional encoding for the output, this order can be modulated on-the-fly per-sample which offers key advantageous properties. It allows for the sampling of and conditioning on arbitrary subsets of tokens, and it also allows sampling in one shot multiple tokens dynamically according to a rejection strategy, leading to a sub-linear number of model evaluations. We evaluate our method across various domains, including language modeling, path-solving, and aircraft vertical rate prediction, decreasing the number of steps required for generation by an order of magnitude. Subjects: Machine Learning (cs.LG); Artificial Intelligence (cs.AI) Cite as: arXiv:2404.09562 [cs.LG](or arXiv:2404.09562v1 [cs.LG] for this version)https://doi.org/10.48550/arXiv.2404.09562 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Arnaud Pannatier [view email] [v1] Mon, 15 Apr 2024 08:22:47 UTC (1,389 KB) Full-text links: Access Paper: View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.LGnewrecent2024-04 Change to browse by: cs cs.AI References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) IArxiv recommender toggle IArxiv Recommender (What is IArxiv?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=40608413",
    "commentBody": "Σ-GPTs: A New Approach to Autoregressive Models (arxiv.org)174 points by mehulashah 5 hours agohidepastfavorite38 comments cs702 5 hours agoThis looks great. The authors randomly permute (i.e., shuffle) input tokens in training and add two positional encodings to each token: one with the token's position and another with the position of the token to be predicted. Otherwise, the model is a standard autoregressive GPT. The consequences of this seemingly \"simple\" modification are significant: * The authors can prompt the trained model with part of a sequence and then decode the missing tokens, all at once, in parallel, regardless of order -- i.e., the model can in-fill in parallel. * The authors can compute conditional probability densities for every missing token in a sequence, again in parallel, i.e., densities for all missing tokens at once. * The authors propose a rejection-sampling method for generating in-fill tokens, again in parallel. Their method seems to work well in practice. I've added this to my reading list. Thank you for sharing it on HN. reply toxik 1 hour agoparentThis problem formulation has been around for a while, it’s kind of the holy grail of modeling. What is new compared to PixelCNN and related is this position embedding idea. reply WanderPanda 16 minutes agoparentprevWait wasn't BERT all about non-causal masking aka predicting words in the middle?! reply thomashop 3 hours agoparentprevI don't understand how that parallel prediction can work... Let's say I give it as input the sentence: I . . . . . . . . happily. The second word to be predicted depends on the first word. reply cs702 3 hours agorootparentGive the model the tokens \"happily\" and \"I\", and add to each input token its respective position embedding and the position embedding for the token to be predicted. You can do this in parallel for all tokens to be predicted. The model has been trained so it can predict tokens in any position. reply hexomancer 2 hours agorootparentYes, but is there any guarantee that the complete sentence makes sense? reply toxik 2 hours agorootparentThat is indeed an issue. Their sampling method rejects impossible combinations. reply entropicdrifter 2 hours agorootparentprevThat guarantee didn't exist with regular GPT LLMs, did it? It just came about as an emergent property of throwing more and more compute, training data, and training time at the problem reply KRAKRISMOTT 2 hours agorootparentprevIsn't this bag of words all over again? Except with positional hints? reply tripplyons 1 hour agoparentprevThe only difference I see from XLNet is how they use it during inference. reply arnaudpannatier 2 minutes agorootparentHey! I'm Arnaud, first author of the paper. XLNet also shuffles the data during training, but they use a masking mechanism instead of the causal + double positional encoding. The application differs, XLNet is not AFAIK focused on generation (even if it can be used for that) and the burst-sampling idea is new. reply taneq 2 hours agoparentprevWow, if that works that's wild (and also has that \"damn, now you say it it's obvious\" flavour that so many really cool discoveries share...) reply smusamashah 59 minutes agoprevThere is a video on twitter showing it generating text (looks a bit like image diffusion) https://x.com/ArnaudPannatier/status/1799055129829839166 reply lukasb 14 minutes agoparentWeird that they chose an example that ended up somewhat nonsensical. reply szvsw 4 hours agoprevWow, really cool concept! I wonder if this starts to become similar dynamics to what we see in image generation models, where structure/detail emerges in one region of the image and then the surrounding areas start to resolve themselves into place. That kind of behavior seems particularly useful for longer reasoning/logic/planning, where the big ideas might become apparent first, and then the interstitial details and text just naturally fill in… reply byteknight 3 hours agoparentThe process you describe is referred to as diffusion reply szvsw 3 hours agorootparentYep yep I know, but I was trying to suggest something diffusion-like occurring with a language model through a totally separate mechanism that does not rely on the denoising process (at least not literally). reply immibis 2 hours agorootparentprevI'm fairly certain diffusion refers to the overall architecture, not the emergent self-organization process. reply mbil 58 minutes agoprevI wonder if this would help especially for computer code generation, where what is output at a given step may materially depend on what would be written at a later step. reply mbil 0 minutes agoparentAnd, maybe prohibitively slow, perhaps integrate some kind of linting or syntax checking as part of the rejection sampling. I.e. burst sample N potential generated snippets in parallel, reject those that are syntactically invalid. reply bigyikes 4 hours agoprevIs this applying the learnings from vision transformers to language transformers? If I understand correctly, vision models split an image into tiles and append a positional encoding to each so the model can understand the relative position of each tile. I admittedly only read the abstract - a lot of this stuff goes over my head - but it seems like this paper proposes a similar idea, but for 1D instead of 2D? reply seurimas 4 hours agoparentPositional encoding is standard for transformers of all stripes. They introduce a seemingly novel, redundant positional encoding scheme. It's more difficult to train, but seems to enable producing multiple tokens at once (i.e. you could get an answer that is N tokens long in N/x steps instead N steps). reply behnamoh 2 hours agoprevTitle is incorrect: it's σ not Σ. reply modeless 2 hours agoparentΣ is uppercase σ. Maybe this happened automatically? Pretty funny if so. Correct in a Greek context; clearly incorrect in a math context. reply mehulashah 1 hour agorootparentYes, HN automatically did that. reply modeless 1 hour agorootparentFor future reference, it is possible to edit the titles of stories you've submitted. This allows you to correct any errors introduced by HN's title rewriting heuristics at submission time, without waiting for a moderator to do it for you. Just like for comments, though, the edit window is time limited. For comments the window is two hours. I don't know if it's the same for story titles. reply klysm 4 hours agoprevEncoding the sequence like that seems like a really clever workaround for some of the data dependency limitations of GPT. reply optimalsolver 4 hours agoprevYann LeCun would say [0] that it's autoregression itself that's the problem, and ML of this type will never bring us anywhere near AGI. At the very least you can't solve the hallucination problem while still in the autoregression paradigm. [0] https://twitter.com/ylecun/status/1640122342570336267 reply cs702 3 hours agoparentLeCun may or may not be right, but I'm not sure this is relevant to the discussion here. The OP's authors make no claims about how their work might help get us closer to AGI. They simply enable autoregressive LLMs to do new things that were not possible before. reply TheEzEzz 2 hours agoparentprevLeCun is very simply wrong in his argument here. His proof requires that all decoded tokens are conditionally independent, or at least that the chance of a wrong next token is independent. This is not the case. Intuitively, some tokens are harder than others. There may be \"crux\" tokens in an output, after which the remaining tokens are substantially easier. It's also possible to recover from an incorrect token auto-regressively, by outputting tokens like \"actually no...\" reply vessenes 3 hours agoparentprevI think this method might not be amenable to the exponential divergence argument actually. Depending on token sampling methods, this one could look at a proposed generation as a whole and revise it. I’m not sure the current token sampling method they propose does this right now, but I think it’s possible with the information they get out of the probabilities. reply modeless 1 hour agorootparentYes, to me this seems to address LeCun's objection, or at least point the way to something that does. It seems possible to modify this into something that can identify and correct its own mistakes during the sampling process. reply andreasmetsala 3 hours agoparentprevDoes everything have to take us towards AGI? If someone makes a LLM that’s faster (cheaper) to run then that has value. I don’t think we want AGI for most tasks unless the intent is to produce suffering in sentient beings. reply skilled 5 hours agoprev> The main idea is to train the model to generate sequences in a random order, which allow conditional density estimation, infilling and generating sequences by burst using a novel rejection sampling method. > In exploring that idea, we aslo compared to a discrete diffusion baseline, which also allows to generate sequences in burst. We were surprised to see that diffusion models were able to solve path-finding task and we made a short Twitter thread The said thread: https://nitter.poast.org/ArnaudPannatier/status/176286434739... And a showcase here: https://www.idiap.ch/~apannatier/sigma-gpt/ (excerpt taken from here: https://www.idiap.ch/~apannatier/) reply 3abiton 3 hours agoparentI just wonder if such models based on their method would make hallucination even worse. reply arnaudpannatier 7 minutes agorootparentHey, I'm Arnaud, first author of the paper. The answer is a bit mixed. We actually started looking into this because of a repetition problem that appeared in a low-data regime for a sequence generation task. Basically, the left-to-right GPT was stuck repeating the same token once it sampled twice the same in a row during generation. And to mitigate that, we tried to generate the sequence in a random order and it seemed to help and we see less of this repetition issue. We initially thought when we don't have enough data, shuffling would be like data-augmentation and might actually help the model reach better performance. But this is not what we found in the experiments: apparently as learning in any order is a harder task, the model memorise the data more. reply hammock 5 hours agoprevAdd to this some kind of \"autofocus\" for the user to click on the word that is the \"center\" of the prompt and you've really got something reply cwilkes 4 hours agoprev [–] Look at this nonsense that ChatGPT made: >What is called a Cadillac Show? Basically, a pretend automobile under tow to really cash on the good vibes coming off a ditch. Whereas this from o GPT almost sounds like it would come off an F1 “news” site: > Fernando Alonso, upset by Honda's huge contractual and salary costs, was contacted with a windblown rival who has apparently joined Sauber and his team and staying competitive and performing on means with the team, and because my contracts have not changes we have been unable to do anything to push in terms of my tire and support.\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The paper \"σ-GPTs: A New Approach to Autoregressive Models\" introduces a novel method that challenges the traditional fixed left-to-right order in autoregressive models like GPT.",
      "By using positional encoding for outputs, the model can dynamically adjust the generation order per sample, allowing for sampling and conditioning on arbitrary token subsets.",
      "This approach enables efficient multi-token sampling with a rejection strategy, significantly reducing the number of model evaluations and generation steps across various domains."
    ],
    "commentSummary": [
      "The paper \"Σ-GPTs: A New Approach to Autoregressive Models\" presents a novel training method for GPT models involving random permutation of input tokens and dual positional encodings.",
      "This method enables parallel prediction of missing tokens, simultaneous computation of conditional probabilities, and uses a rejection-sampling method for generating in-fill tokens, potentially improving efficiency and capabilities.",
      "While similar to models like XLNet, Σ-GPTs introduce unique elements such as double positional encoding and burst-sampling, with discussions on applications and limitations, including sentence coherence and hallucination issues."
    ],
    "points": 174,
    "commentCount": 38,
    "retryCount": 0,
    "time": 1717765951
  },
  {
    "id": 40601711,
    "title": "HP BIOS Update Renders ProBook 445 and 455 G7 Laptops Inoperable, Users Seek Solutions",
    "originLink": "https://h30434.www3.hp.com/t5/Notebook-Operating-System-and-Recovery/HP-Probook-BIOS-Upgrade-1-17-failed/td-p/9081096/page/3",
    "originBody": "Sign up · Sign in Community Home Welcome First Time Here? Learn How to Post and More Community Blog Product Discussions Notebooks Notebook Video, Display and Touch Business Notebooks Notebook Hardware and Upgrade Questions Notebook Wireless and Networking Notebook Operating System and Recovery Notebook Audio Notebook Boot and Lockup Notebook Software and How To Questions Notebooks Knowledge Base Printers Mobile Printing & Cloud Printing Scanning, Faxing & Copying Printer Setup, Software & Drivers Printing Errors or Lights & Stuck Print Jobs DesignJet, Large Format Printers & Digital Press Sprocket Samsung Printers Knowledge Base Printer Wireless, Networking & Internet Printer Ink Cartridges & Print Quality Printer Paper Jams & Feed Issues Desktops Desktop Software and How To Questions Desktop Hardware and Upgrade Questions Desktop Audio Desktop Operating Systems and Recovery Desktop Video, Display and Touch Desktop Boot and Lockup Sprout by HP Business PCs, Workstations and Point of Sale Systems Desktop Wireless and Networking Desktop Knowledge Base Gaming Gaming Notebooks Gaming Desktops Gaming Accessories Gaming Knowledge Base Apps, Services & Software HP ZCentral Remote Boost and Connect HP Instant Ink Poly Services Poly Software Poly Headsets Accessories Aviation & Specials Bluetooth Headsets & Earbuds Contact Center Headsets DECT Wireless Headsets USB Headsets Poly Phones Accessories UC Platforms Desk and IP Conference Phones ATA / VoIP Adapters USB / Bluetooth Speakerphones PSTN Wireless Phone Solutions Poly Video Conferencing Accessories & Cameras Meeting Room Solutions Webcams Collaboration & Conferencing Platforms Poly International Chinese 中文 Support Resources Product Support Printer support Computer support All Software & Drivers Diagnostics Printer Diagnostic Tools Computer Diagnostic Tools Windows Support Print And Scan Doctor HP PC Hardware Diagnostics Contact Us HP Expert Center Meet the Experts Meet HP Team Expert Portal Sign up / Sign in 2 × Information Windows update impacting certain printer icons and names. Microsoft is working on a solution. Click here to learn more Information Need Windows 11 help? Check documents on compatibility, FAQs, upgrade information and available fixes. Windows 11 Support Center. 2 × Information Windows update impacting certain printer icons and names. Microsoft is working on a solution. Click here to learn more Information Need Windows 11 help? Check documents on compatibility, FAQs, upgrade information and available fixes. Windows 11 Support Center. HP Community Notebooks Notebook Operating System and Recovery HP Probook BIOS Upgrade 1.17 failed HP Probook BIOS Upgrade 1.17 failed Options Mark Topic as New Mark Topic as Read Float this Topic for Current User Bookmark Subscribe Mute Printer Friendly Page Entire Community This Category This Board Knowledge base Users cancel turn on suggested results Showing results for Search instead for Did you mean: Start a conversation Guidelines We have new content about Hotkey issue, Click here to check it out! Create an account on the HP Community to personalize your profile and ask a question Your account also allows you to connect with HP support faster, access a personal dashboard to manage all of your devices in one place, view warranty information, case status and more. Sign in / Create an account Learn more Sign in / Create an account / Learn more Check out our WINDOWS 11 Support Center info about: OPTIMIZATION, KNOWN ISSUES, FAQs, VIDEOS AND MORE. UniteZack 2 0 0 15,348 New member 05-31-2024 07:09 AM HP Recommended Mark as New Bookmark Subscribe Permalink Print Flag Post Thanks, was it just a case of connect onto the chip and flash? Reply Was this reply helpful? Yes No Fr4nk78 1 0 0 15,310 New member 05-31-2024 09:49 AM HP Recommended Mark as New Bookmark Subscribe Permalink Print Flag Post Hi to all. Same issue here! (probook455g7, Italy), after the assistant initiated update of the last week. A sudden change from a perfectly working state into a deadly useless state occurred. After several days of contacts with HP phone assistance, and following multiple suggested interventions, nothing worked. Since the machine is out of warranty, the proposed solution is a rather expensive motherboard replacement. This would certainly solve the issue (until the next official virulent BIOS update). Given the origin of the problem, I believe HP should reconsider and provide genuine assistance to their customers. However, if anyone could kindly provide more details about the flashing operation, location of the chip, connection to the chip, etc., it would be greatly appreciated. Reply Was this reply helpful? Yes No Andreas--- 4 1 0 15,273 New member 06-01-2024 12:27 PM HP Recommended Mark as New Bookmark Subscribe Permalink Print Flag Post i Reply 1 person found this reply helpful Was this reply helpful? Yes No Dioxaz 6 1 0 15,244 Level 1 06-02-2024 01:40 PM - edited 06-04-2024 08:49 AM HP Recommended Mark as New Bookmark Subscribe Permalink Print Flag Post Hello, I wanted to inform the community that I successfully revived one Probook 455 G7 using a CH341A programmer, but it involved me removing the chip from the motherboard. I detailed the process in another thread. Edit: My second attempt was unfortunately not successful, as the second machine won't turn on and will instead make Caps and Num lock LEDs blink 7 times, all that with the charge LED being permanently lit in white all the time. I couldn't find any worthwhile info on the net for the moment regarding that error code. Reply Was this reply helpful? Yes No Wojtek0586 5 0 0 15,227 New member 06-02-2024 05:18 PM HP Recommended Mark as New Bookmark Subscribe Permalink Print Flag Post Hi Dioxaz, thanks for Your work! It can be very useful! I am surprised by the fact that HP has not made an official statement on this topic. These laptops are approaching 3 years old, we have literally a dozen or so days left in which they are covered by the manufacturer's warranty. What about units that are already out of warranty? The official update broke working equipment, who will pay for the repair? The scale of the phenomenon seems to be very large and concerns the entire world. Could this be planned obsolescence of the product? Reply Was this reply helpful? Yes No Animal_JC 1 0 0 15,199 New member 06-02-2024 11:38 PM - edited 06-02-2024 11:38 PM HP Recommended Mark as New Bookmark Subscribe Permalink Print Flag Post I too suffered the BIOS update death last Wednesday 22nd May I took the SSD HDD out and put it in an external enclosure and all my files were there including a folder named SWSetup which has all the previous BIOS bin files so I can roll back if I can get the BIOS chip flashed Going to contact HP today and see what they propose to do about their THEIR software bricking MY 3 year old perfectly functioning laptop Reply Was this reply helpful? Yes No itscout49 4 0 0 15,137 New member 06-03-2024 03:02 AM HP Recommended Mark as New Bookmark Subscribe Permalink Print Flag Post Thank you Dioxaz for your research and detailed description under: https://h30434.www3.hp.com/t5/Notebook-Boot-and-Lockup/Bios-update-crash-problem-HP-ProBook-445-G7/m... At the German Support Website for the HP ProBook 445 G7, HP have replaced the malicious BIOS Patch V1.17 against V1.06.02. I am wondering about this numbering, as the last Patch was V1.16. For me \"unsolder\" the BIOS Chip of a prior fully working system which still has a market value of EUR 400,- is no option, because this will be fully onto my risk. I think with the customer reply of roughly 100 demaged Business Class Notebooks it is clear what is responsible for the \"sudden death\" of so many systems. I now try to escalade this to the German HP Management. Thank you and best regards from Frankfurt Area Reply Was this reply helpful? Yes No tcsenter 73 8 1 15,109 Level 3 06-03-2024 06:39 AM HP Recommended Mark as New Bookmark Subscribe Permalink Print Flag Post Better get screen captures of this thread before HP locks and deletes it. This is NOT the first instance of new BIOS introducing troubles on 455 G7. e.g. https://h30434.www3.hp.com/t5/Notebook-Video-Display-and-Touch/HP-Elitebook-G7-G8-series-issues-with... Reply 1 person found this reply helpful Was this reply helpful? Yes No Dioxaz 6 1 0 15,073 Level 1 06-03-2024 02:19 PM HP Recommended Mark as New Bookmark Subscribe Permalink Print Flag Post @Wojtek0586 wrote: Hi Dioxaz, thanks for Your work! It can be very useful! I am surprised by the fact that HP has not made an official statement on this topic. These laptops are approaching 3 years old, we have literally a dozen or so days left in which they are covered by the manufacturer's warranty. What about units that are already out of warranty? The official update broke working equipment, who will pay for the repair? The scale of the phenomenon seems to be very large and concerns the entire world. Could this be planned obsolescence of the product? It looks like there has been an official statement according to a previous post. But I have no idea of where this was published. Me too can't believe we're the only ones in those threads who ended up with bricked 455 G7s, this incident must have struck potentially millions of machines. However I can't seem to find anything related on the internet for the moment. Fun fact: when this incident struck me a few days ago with two users at work reporting bricked machines due to a bad BIOS update, I thought I was the only one, started to panic and doubt my IT asset management skills. Just think of it, that dreaded BIOS update even escaped the GPO I made preventing driver updates, and we're indeed running a WSUS server with manually approved updates. How could that be? Well, a smile came back to my face when a teammate alerted me that HP deleted the BIOS update from their servers. Then I saw this thread, then another one, and another one... and I finally realised. 🙂 And yep, my initial searches leading me to clueless people advising to use a GPO to prevent BIOS updates. I don't think they have any idea of what they're talking about (especially with that dreaded “Native OS Firmware Update Service” functionality). Speaking of which, that will be our next task: setting that “Native OS Firmware Update Service” to off on all machines we can in order to disable the UEFI Capsule BIOS update for good. Reply Was this reply helpful? Yes No fiwkfcw434 2 0 0 15,037 New member 06-04-2024 12:20 AM - edited 06-04-2024 12:39 AM HP Recommended Mark as New Bookmark Subscribe Permalink Print Flag Post I would suggest everyone to contact HP customer support and create ticket about the bios issue: https://www.hp.com/us-en/contact-hp/contact.html Write notice of defect of the broken bios. In my case (Finnish company customer and laptop manufacturer warranty has expired) they still provided good will onsite service. A new motherboard will be replaced into laptop. It will take some time to wait a new motherboard and technician will be provided onsite to make repairing of the laptop. Hope they act elsewhere the same way. Reply Was this reply helpful? Yes No « Previous 1 2 3 4 Next » Be alert for scammers posting fake support phone numbers and/or email addresses on the community. If you think you have received a fake HP Support message, please report it to us by clicking on \"Flag Post\". † The opinions expressed above are the personal opinions of the authors, not of HP. By using this site, you accept the Terms of Use and Rules of Participation. English Open Menu English Español Português 中文 Didn't find what you were looking for? Ask the community † The opinions expressed above are the personal opinions of the authors, not of HP. By using this site, you accept the Terms of Use andRules of Participation.",
    "commentLink": "https://news.ycombinator.com/item?id=40601711",
    "commentBody": "HP bricks customers laptops with faulty automatic BIOS upgrade (hp.com)161 points by jackpot433 23 hours agohidepastfavorite114 comments busterarm 22 hours agoNot even surprised. 20 years ago when I worked in retail selling computers, we had moderately high end HP laptop at a great sale price with an extra RAM slot that many customers opted to get a stick for. We started getting returns immediately and once we started inspecting the laptops, we saw that the extra DIMM slot didn't even have any lines connecting it on the motherboard. HP tried to insist that they wouldn't credit us for the returned defective computers without us signing an NDA, which we refused but still shipped back all the units from our entire chain of retail stores. This was many thousands of units and they probably lost an assload on the returns. Their products have been garbage for a long, long time. reply jakderrida 20 hours agoparentI'd argue HP has both really good products and some of the most awful products in history. I remember getting one of their first tablets (called TX2000Z) and it was such a hunk of crap. On the flipside, I still have several Z-series workstations that are extraordinarily good and, having dual processor slots, allowed me to beef them up on a budget. reply RIMR 22 hours agoparentprevDell did something similar to me, but way scummier a decade ago with my Dell XPS L702x. I bought it spec'd down with the intent to upgrade it myself with parts I had a better deal for. It was advertised as having two DIMM slots, and I ordered it with only one small stick of RAM because I had a source for two of the largest supported sticks. When I got the Computer, there was only one DIMM slot. The technicians had me take the entire thing apart looking for the second slot under they keyboard, but instead I found a an entire unpopulated part of the board with the words \"DIMM 2\" printed on it, with no slot to put another stick of RAM into. When I figured out that they had lied in their spec sheet, I asked for a replacement motherboard with two DIMM slots. They explained that for computers ordered with less than 2 sticks of RAM, they use a single-slot board. I pointed out that the line \"2x DIMM\" was on my order, and they complained that the only in-stock version of the motherboard that was compatible with 2 sticks of RAM was the version for the 3D version of the laptop which would require the complete replacement of my computer with a better processor and better monitor. After calling them daily for a few weeks, and sending them a letter from a lawyer, they finally shipped me the new laptop. Bought myself a pair of active switching 3D glasses and hand fun with that for about an hour before finally enjoying my laptop like I intended to. reply BenFranklin100 22 hours agoparentprevWe have several HP Spectra X360s at work. People seem pretty happy. reply busterarm 21 hours agorootparentThanks, that guy. Also. https://www.reddit.com/r/Hewlett_Packard/comments/11zykiu/do... reply BenFranklin100 21 hours agorootparentNo need to be a snarky asshole. I’m just relating our experience at work. The link seems to focus on playing video games on the device, not office apps and browsing. reply busterarm 21 hours agorootparentOh nothing about the power bricks...bricking... reply bearjaws 22 hours agoprevSemi-related, I have an HP envy x360 with 4500u (same one as the laptop posted) and I have to say, the way HP applies BIOS updates is reckless at best. The battery had died and when I first plugged it in, it immediately powered up, and started installing a BIOS update. If someone tripped on the power cord, or had a power outage, it would brick the laptop. I guess it downloaded the update the last time it was powered on (months ago). They should put a minimum battery % (could be as low as 10% I imagine), but it seems very likely to me if someone has just plugged in their laptop, there is a strong chance it may get unplugged as well. I only use this laptop to tune my AEM ECU on my car, and it rarely gets used, but I bet I am not the only one who has a HP laptop that isn't used every week. reply lobsterthief 22 hours agoparentI think they need minimum estimated battery time rather than battery %, since old batteries that have very little capacity remaining could still brick with a high % remaining. Or, like most laptops, just require power cable + some level of battery to install something so critical. reply acchow 21 hours agoparentprevShouldn’t there be 2 BIOSes and you only update 1 at a time? I feel like this has been a solved problem for 20 years. reply Technetium 4 hours agorootparentThis is still not something you can take for granted, even on a mid to high range desktop motherboard. Flashback is still something you must pay extra for, and a secondary BIOS is the same. Laptops are even further behind! reply prerok 21 hours agorootparentprevSolved problem for some but not for these end consumer laptops? reply fuzzy2 22 hours agoparentprevI think that's fine, though maybe surprising. Regular PCs don't have redundant power at all and we still update their firmware all the time. reply bearjaws 16 hours agorootparentYou are way more likely to unplug a laptop than a desktop. reply immibis 22 hours agoparentprevThe real real Copy of final_2024_last solution is, of course, to make the BIOS chip twice as big as it needs to be, and flash the other half, then atomically change which is selected. reply crest 2 hours agorootparentThe few cents for larger flash or the one-time cost to write less bloated firmware would cut into their budget for gambling, drugs and hookers. reply Perz1val 20 hours agorootparentprevI'm sure this has a name, Android uses A/B partitions for system updates, everything crucial should... reply fifteen1506 22 hours agorootparentprevI believe that's called shadowbios or similar. I think a few laptops have it. reply Qem 22 hours agoprevThis is news to me. I knew they bricked your printer if a non-HP cartridge was found. Now they brick your computer too? I hope they don't venture in the pacemaker market any soon. Then HP may brick consumers themselves. reply throw46365 22 hours agoprevSomeone in that thread talks about the numbers of devices potentially affected versus the few complaints. I’m not sure “potentially millions” is right; I suspect individual HP models (of which there are many) maybe sell in the tens or at most low hundreds of thousands. But I fear the reason why there are so few complaints is easy: it’s a three-year-old HP laptop model so they are mostly already in the bin. reply jackpot433 22 hours agoprevSorry, accidentally linked to page three, you either find the way to page 1 yourself or use this link: https://h30434.www3.hp.com/t5/Notebook-Operating-System-and-... reply Animats 22 hours agoprevThis is worth filing a Computer Fraud and Abuse case. This clearly \"exceeds authorized access\". HP can argue in court that they are entitled to brick your laptop. That would probably not impress a jury or judge. reply krger 22 hours agoparent>This is worth filing a Computer Fraud and Abuse case. No. reply TheGamerUncle 22 hours agorootparentcurios honestly, why not and what do you think that actually merits filling a case? I am genuinely interested on hearing the mental gymnastics you do in order to hate consumers reply fckgw 22 hours agorootparentDo you think this was on purpose? Or just a botched update? reply Animats 21 hours agorootparentThe access was intentional. The botch was presumably an error, but that doesn't matter. What matters is the \"authorization\" issue. Was HP authorized to access the computer? Probably. Were they authorized to damage the computer? No. There's room for legal argument. CFAA: intentionally accesses a protected computer without authorization, and as a result of such conduct, causes damage and loss. The point is that fighting this is a lose-lose for HP. Either HP has to argue in court that they have a contractual right to brick your computer, or they have to make you happy enough to drop the case. Visualize the press coverage of HP arguing on the record that they have the right to brick your computer. reply fckgw 20 hours agorootparentWhere are you getting \"HP accessed the computer\"? This was a BIOS update initiated by the end user. reply dcan 15 hours agorootparentIt was rolled out through Windows update, so I'm not confident the end users authorized it. reply fallingsquirrel 22 hours agorootparentprevDo you think \"but it was an accident\" should be an accepted legal route to escape liability for your actions? reply abstractbeliefs 21 hours agorootparentYes - the CFAA as a criminal issue requires \"mens rea\", the intention to commit a crime. Very few laws (such as involuntary manslaughter) have exceptions to this. If you break someone's shit by accident, they can still sue you, and people should probably sue HP, but you can't try to have a prosecutor bring them to a criminal trial under the CFAA. reply TheCoelacanth 21 hours agorootparentprevNo, but it usually makes it civil liability and not criminal liability. For the most part (there are exceptions), you can't accidentally commit a crime. reply fckgw 20 hours agorootparentprevFor the CFAA? Absolutely. reply bdcravens 22 hours agorootparentprevhttps://en.wikipedia.org/wiki/Hanlon%27s_razor reply can16358p 22 hours agoprevFor some reason I could never ever trust HP, their products never seem high quality and given the brand culture (especially about printers and cartgridges) I'm not even remotely surprised. reply dougg3 22 hours agoprevThis same kind of thing happened to a family member several years ago on his Dell Inspiron 3650 minitower. Dell’s automatic updates installed a BIOS update (3.9.0) that bricked the computer. It would just power off and on automatically forever, no way to even access the BIOS. A bunch of other people online ran into the exact same problem. I ended up reflashing the 3.9.0 BIOS to the flash chip and it fixed it [1]. Later on, people discovered that changing an Intel ME-related jumper on the motherboard also fixed it - a much easier fix. It seems crazy to me that manufacturers include BIOS updates as part of automatic updates, especially for out-of-warranty systems. [1] https://www.downtowndougbrown.com/2019/06/fix-dell-inspiron-... reply 0x1ch 22 hours agoprevSeems this has (now is) been the norm. HP recently acquired gaming peripheral company HyperX. I purchased a HyperX Cloud III headset on recommendation. While it hasn't bricked, apparently a month or so prior to now HP bricked all of those headphones for a month with no update. reply Feathercrown 21 hours agoparentI've bought two of those headsets and neither one lasted 2 years reply algaeselect 1 hour agorootparentSame, bought 2 Hyper X Cloud Orbit-S Gaming (one for me and my partner), and they both broke within 2 years. If you look on the Amazon reviews and sort by date you will see everyone is having this problem, they break in the exact same place after about 2 years. reply 0x1ch 21 hours agorootparentprevI just got mine, they seem really nice. Only one issue with software and that was it. Hopefully it continues to be good. reply heartag 4 hours agoprevAlmost relevant: https://github.com/ctag/hp_envy_reviver reply bloomingeek 21 hours agoprevIn the 90s I owned several HP products including pcs, laptops and printers. First the printer ink cartridges were either half full or just trash, then Toshiba simply put out a better laptop. (hated to see the Toshiba laptop go away, they were easy to get into for upgrades.) Now I'm using a HP laptop with an i7 CPU and it's actually a nice machine. I'll see how much I like it when I need to open it up. reply er4hn 22 hours agoprevIf you do this in the cloud, it's DevOps. If you have a faulty automatic BIOS upgrade for on-prem products, it's the EU cyber resilience act. reply averageRoyalty 14 hours agoprevThe comments in this thread make me happy I live in a country with good consumer law. Who cares how long the \"warranty\" is? If you bricked my laptop, you'll fix it or give me a new one, simple as that. reply eisa01 21 hours agoprevI switched jobs to a small startup, and could choose the laptop model for the first time Bought the latest generation HP Elitebook G11 to try something else than Lenovo that had crappy trackpads due to the nibble It has been a disaster in terms of software/BIOS: * The mic cut off my voice when I was speaking all the time * The CPU clocked down to ~300 MHz and got stuck there * External display would not work if I disconnected, came back 30-60 min later, and reconnected. Had to hibernate inbetween * The display would suddenly dim and be impossible to change I think all these issues resolved itself with some firmware update released last week, and today I discovered there's an \"HP Audio Control\" app that by default is set to \"conference mode\" and \"AI noise reduction\" Now I only need to get the built-in 5G modem to work Would have gotten a Microsoft Surface model, but the selection is very slim in Norway reply ultra_nick 22 hours agoprevWhy are hardware companies like this? reply duped 22 hours agoparentThe mainboard on my Framework died, randomly. They replaced it, (almost) no questions asked. Shipped me a replacement, and I was able to replace it myself in fifteen minutes. Speak with your dollars. Even though I was unhappy that the device was broken, my next laptop will probably be a framework. Although I would love if they started selling replacement chassis, mine is bent (that was my own fault). reply MostlyStable 21 hours agorootparentWhich part of your chassis is bent? They sell both top[0] and bottom[1] cover portions, as well as the input cover[2], which I think covers all the chassis [0]https://frame.work/products/top-cover-cnc [1]https://frame.work/products/bottom-cover-kit [2]https://frame.work/products/input-cover-kit?v=FRANHC0001 reply baq 22 hours agoparentprevRace to the bottom. No reputation of quality => can’t raise price => cutting costs beyond the red line => mishaps => no reputation of quality => … Cf Apple who charge an arm and a leg for anything above the base config (which admittedly is tight but still usable) because people trust them to not pull such dumb moves. reply varjag 21 hours agorootparentWhat's funny is that HP consumer business had started out with reputation of quality from their top of the shelf professional electronic products. reply samatman 21 hours agorootparentI would prefer to call this tragic actually. reply can16358p 22 hours agorootparentprevThis. I buy Apple mainly for reliability (not just that but that's an important factor), and HP is polar opposite of reliable in my view as a consumer. reply acchow 21 hours agorootparentprevApple does not charge an arm and a leg. Their prices are quite competitive with similarly performing alternatives For the lower end, you can get a refurbished from various resellers all over the internet. reply duped 18 hours agorootparentUnless you want a reasonably sized disk or enough memory, that is. The charge for disk storage is particularly egregious. reply baq 21 hours agorootparentprevThey really do for extras. The base config is IMHO on the contrary best price/experience ratio available on the market even including the subpar macOS. reply mulmen 22 hours agoparentprevIt's not like software companies are any better. It's just plain ol' incompetency. reply deelowe 22 hours agorootparentI've worked with many engineers from HP. They were NOT incompetent. What they did describe though was a culture of firefighting and micromanagement complaining there was no opportunity to drive systemic improvement. reply mulmen 21 hours agorootparentOk if incompetence is not to blame how do you describe bricking customer laptops with a BIOS update? reply deelowe 2 hours agorootparentLack of adequate testing probably due to rushed schedules, insufficient infrastructure, and perhaps poor release practices because management celebrates firefighting as it's the easiest way to show \"business impact.\" reply richwater 22 hours agoparentprevWithout a vertically integrated environment, it's a race to the bottom. I cannot recommend any non-Apple laptop these days. They are all total shit for run-of-the-mill consumers. If you're tech inclined you can probably wipe and reinstall windows on a Lenovo thinkpad and come out _okay_, _maybe_. reply RajT88 22 hours agorootparentDell business machines are great. I have had nothing but great luck with them. They do not even come with much crapware. Most of what is there is from the base Windows OS. reply acchow 21 hours agorootparentMacs are still on a different level. You can even wipe the entire SSD on a mac and STILL you have the ability to reinstall macOS without putting in any disc or USB stick. It's just always there waiting reply adamomada 21 hours agorootparentThis is almost correct. For the new Apple silicon machines you do need a second working Apple computer and cable in order to do a DFU if you completely wipe the SSD. You can also waltz into an Apple store to have it done if you don’t have access to any other machines. reply robotnikman 21 hours agorootparentprevFramework laptops are great if you need Windows/Linux, even if a bit more expensive reply ThePowerOfFuet 21 hours agorootparentprevFramework. reply INTPenis 22 hours agoprevWhat a nightmare, for all parties involved. As a consumer this is what must not happen, and someone at HP QA must be kicking themselves right now. reply mulmen 22 hours agoparentDoes HP have QA anymore? I haven't seen or heard from a QA person in years. From what I can tell nobody has QA anymore. I think it was a rapture situation. If anyone was worthy of salvation it was QA. reply mattmcknight 21 hours agorootparentNo, they sold Quality Center Enterprise to MicroFocus in 2017. https://en.wikipedia.org/wiki/OpenText_Quality_Center reply ejj28 22 hours agoprevI really liked my old 2013-era HP Elitebook, but honestly every other HP laptop I've come into contact with (and I used to see a lot at the laptop repair shop I worked at) has just sucked. reply sva_ 21 hours agoparentI personally really enjoy my 2023 Elitebook (Ryzen). The repairability was one of the main reasons I chose to buy it. reply sva_ 22 hours agoprevI just saw a new BIOS upgrade for my HP Elitebook 845 G10, but now I'm worried if I should install it. Should really get ahold of a bios chip programmer, since they're pretty cheap. reply malux85 22 hours agoprevThat person in the thread was remarkably calm about their out of warranty laptop being randomly bricked, I would be fuming, that is not OK! reply bsza 22 hours agoparentI feel like the willingness to buy something from HP is already a pretty strong filter as to how much BS one can tolerate. reply munk-a 22 hours agoprevThis is honestly just par for the course for HP. The customers should feel lucky that their machines bricked before lighting their houses on fire. A lower quality mainstream computer manufacturer does not exist. reply nebula8804 22 hours agoparentMan you aren't kidding. It has been going on for a long time now. I work on vintage computers for \"fun\" and recently got a couple of those HP \"bubble\" systems from the 90s. What a pain in the arse to disassemble. They have to make the PC so convoluted to take apart and have multiple parts where a normal PC just has one. (Ex. Front panel is multiple pieces of brittle plastic, extra metal pieces that are overkill and are sharp so you always cut yourself). I have a stack of P4 and Core 2 era machines that just have some random failure and its almost not worth my time anymore to fix them other than curiosity. P4 and core2 are basically worthless but HP P4 and Core2 systems? No one wants that trash. reply neilv 22 hours agorootparent> P4 and core2 are basically worthless but HP P4 and Core2 systems? No one wants that trash. Nobody wants post-Fiorina HP computers. But I recently sold several Core 2 IBM/Lenovo laptops, for significant money, because people love those. Last night, I ordered a part to polish up the final one on hand, to sell for $200+. reply Rinzler89 22 hours agorootparent>But I recently sold several Core 2 IBM/Lenovo laptops, for significant money, because people love those. What do people do with them? Those chips came out in 2006. Loading a modern JS heavy webpage will kneel a Core 2 Duo laptop chip. Maybe it's tinkerers and businesses who buy them, not people. Business who depend on keeping older machinery going in workshops and what not. If you're an average joe in need of a laptop at home for web & stuff, for 200+ USD used you can get much more modern machines for home users that will be better at running modern web content than core 2 duo machines. reply neilv 21 hours agorootparentI was using Core 2 Duo just fine as daily drivers with Linux until a year ago. I upgraded for different reasons. With 8GB RAM, they will run 3 separate Web browsers at once, still using only a minority of the RAM, and are just fine with pretty much any Web site. (Maybe it helped to be using uBlock Origin, which reduces both page load bulk, and also eliminates some very invasive behavioral spying that can be resource-heavy.) One of the ~10 Core 2 ThinkPads I recently I sold went to a business involved with used science lab equipment. The rest of the sales seemed to be for probably personal use. reply Rinzler89 21 hours agorootparent>I was using Core 2 Duo just fine as daily drivers Eh, everyone has a different subjective definition of what \"just fine\" means. In case of an emergency I guess could make do for a little while with one, but I came back once to my old Core 2 Quad system out of nostalgia and while it does works cand can do things, compared to my newish Ryzen laptop the difference when browsing the web and multitasking is absolutely night and day, but the day is on Pluto. Once you get used to a modern fast machine with 8 cores at 5GHz and NVME ssd, I just could never imagine going back to a core 2 duo and spinning rust machine today, Linux or not. The difference is staggering. >With 8GB RAM That definitely makes a difference, but it's a bit disingenuous, since stuff like 8GB RAM or SSDs were not the standard for Core 2 Duo machines though. Most of those machines you'll find in the wild tend to have more like 1-4GB RAM and spinning rust. That's really tough to daily drive. reply neilv 21 hours agorootparentI also use powerful contemporary hardware, so I have a basis for comparison. What software you run, and how you use it, are also big factors. reply Rinzler89 21 hours agorootparent>What software you run, and how you use it, are also big factors. The Web is the same for everyone. So is stuff like browsers and webapps like Teams, G-Suite, Office 365, Youtube, Spotify, Facebook, flight bookign sites, etc. The UX difference between those pages/apps running on a core 2 duo and a modern machine is stratosferic. Even bigger when you have same other stuff in the background like a torrent downloader, dropbox client, messengers, etc. You are free to say it doesn't bother you, but the difference is definitely there and palpable for everyone. Of course, if your core 2 duo laptop is the Ship of Theseus with everything on it maxxed out and only the chip being the old original part, sure I can believe the difference is smaller, but like I said, that's not really representative for most core 2 due systems of their era which definitely show their age today. reply adamomada 21 hours agorootparentprevJust a guess but they may be for the truly paranoid as they are the last from Intel without the ME black-box, and with a lean Linux (e.g. Void, or my new beau Chimera-Linux) probably work better than you’d think. (and Now I’m thinking of digging out my old MacBook Pro 2,2 to see what’s what) reply Rinzler89 21 hours agorootparentI know, I also use lean LInxues, but JS heavy websites and productivity apps will still be slower compared to modern machines, no matter what Linux distro you use, especially if you multitask a lot. Void Linux won't give your machine more GHz or TFLOPs or turn your HDD into a NVME, it'll just reduce the RAM and storage footprint compare to more feature rich distros, but your CPU performance in all tasks will stay the same. reply adamomada 21 hours agorootparentMy old Macbook, has 2x2.2 GHz 64-bit procs and one of the OG Intel SSDs to replace the HDD – given it’s only sata3 but I have a suspicion it’s still good. I think I actually will revive it this weekend just to see for myself, all else fails it has a great big matte screen and excellent keyboard I could use for distraction free writing. reply nebula8804 21 hours agorootparentprevWell thats more of an special exception because people want those nice keyboards. I think there are some projects to replace the PCB of the older T60 and T400 models with modern motherboards. Also whenever I see Fiorina partner with someone in politics that person loses credibility in my eyes. Like they clearly don't care at all about track record. reply dylan604 22 hours agorootparentprev> extra metal pieces that are overkill and are sharp so you always cut yourself This is memory I have as well, but not specific to Dell. It was the least favorite part of building/updating any PC from back then. I don't remember one time of not coming away with some sort of scrape or cut. reply nebula8804 21 hours agorootparentYeah it was common in 90s PCs. Dell had those silly metal \"bars\" you had to remove. Like did anyone care if the case would stay intact if you dropped it? But HP just had so much extra redundant metal that you'd have to touch to disassemble the pc so it was just so much more common(in my opinion). reply sva_ 22 hours agoparentprevTheir business class devices are pretty damn alright. Typing on one right now. Also the 3 year onsite repair included is pretty great. Would never buy a printer or so from them though. reply dessimus 22 hours agoparentprev> A lower quality mainstream computer manufacturer does not exist. Just wait until you heard what ASUS Support does for their customers... reply mistrial9 22 hours agorootparentmany ASUS products have excellent engineering in my experience reply moffkalast 22 hours agoparentprevEh HP will at least repair it for free under warranty, ASUS will actively try to scam you to pay extra for repairs you don't even need. reply JohnMakin 22 hours agoprevWhat the heck is with the url here? looks weird and malicious https://h30434.www3.hp.com/t5/Notebook-Operating-System-and-... reply mardifoufs 19 hours agoparentFor some reason HP has always had those god awful links that look like the average phishing scam link. I think their docs and even the main website used to have a similar URL pattern at some point. reply jackpot433 22 hours agoparentprevSadly their forums are like that reply viraptor 22 hours agorootparentIt's not just forums. So many things, both internal and external at HP are like that for some weird reason (for a very long time now) reply JohnMakin 22 hours agorootparentprevholy cow. reply mjcl 21 hours agorootparentA former HP employee told me that a creating named subdomain required a VP-level signoff, while those \"license plate\" style subdomains could be provisioned by almost anybody via a self-service tool. So if there isn't a real reason for a nice URL, people tend to create license plate subdomains. reply adamomada 21 hours agoparentprevWhy would a malicious URL look malicious though, if it was trying to be malicious? reply purpleblue 22 hours agoprevCarly Fiorina successfully and completely dismantled \"The HP Way\" which was the root of Silicon Valley. After her, HP was nothing but a carcass and filled with opportunistic people that destroyed the company. HP looks at their customers like an enemy. Anyone who buys anything HP is intentionally making their lives miserable and they deserve it for not doing a modicum of research to avoid them. reply Rinzler89 22 hours agoparent>dismantled \"The HP Way\" which was the root of Silicon Valley. The \"real\" HP still lives on as its spinoffs that do \"real\" engineering: HPE servers, Agilent oscilloscopes, Broadcom optical sensors, Philips heartrate monitors. The consumer facing brand of HP (printers, laptops, etc) is bottom of the barrel Chinesium, same as Philips. reply varjag 21 hours agorootparentAgilent stuck out to biotech years ago, scopes are now Keysight business. reply mathattack 22 hours agoprevWhat a tarnished legacy. They used to be the model for Silicon Valley. reply chaostheory 22 hours agoprevI remember not being able to get a newer video card since HP opted not to update my bios. It’s surprising that my outcome was the better one. reply drpossum 22 hours agoprev [–] Now here I thought HP went out of business for being a trash company because that's what a sane universe would have dictated. reply bayindirh 22 hours agoparent [–] HP servers are complete opposite of their laptops. I today retired a 17 year old DL140 which was working fine. reply doubled112 22 hours agorootparentIs their driver and firmware update section still paywalled? reply rcostin2k2 22 hours agorootparentYes, unless you are under warranty or there is a critical patch release. This habit emerged since the 'strategic' merge with Micro Focus. I think they are, in fact, in charge with all software side of HPE. reply mustntmumble 22 hours agorootparentprevYes, and also they are not HP servers, they are HPE. HP split off the server division into HPE Hewlett Packard Enterprise some years ago. I have a very high regard for HPE products, not so much for HP products... reply bayindirh 22 hours agorootparentIt was an \"HP server\" when it arrived 17 years ago :) Split happened in 2015. reply bayindirh 22 hours agorootparentprevIt was not paywalled before. It's new for HPE. I don't know why they started doing it. Probably due to GreenLake (server renting) initiative. reply fifteen1506 20 hours agorootparentprevServer line is not the same company AFAIK. reply bayindirh 20 hours agorootparentThey split in 2015. The server I have written about is acquired in 2006. reply Dalewyn 21 hours agorootparentprevTo be clear, HPE (HP Enterprise) which sells servers and HP Inc. which sells PCs and printers are polar opposites despite their common origin and nearly identical names. reply bayindirh 21 hours agorootparentYes, but that server is not an HPE. HPE split in 2015. We acquired said server in 2006-2007. reply FpUser 22 hours agorootparentprev [–] I have 10 years old Z840 workstation which I use as a server with quit fans. Built like a tank and works like a charm reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "An online community forum discusses hardware and software topics, including product support for various devices like notebooks and printers.",
      "A recent BIOS update has rendered HP ProBook 445 and 455 G7 laptops inoperable, causing user frustration due to HP's lack of response and concerns over repair costs for out-of-warranty devices.",
      "Users share solutions such as flashing the BIOS chip, contacting HP for repairs, and advise disabling future BIOS updates to prevent similar issues."
    ],
    "commentSummary": [
      "HP is criticized for bricking laptops with faulty BIOS updates, reflecting ongoing quality issues and poor customer service.",
      "Users discussed the risks of BIOS updates, the absence of dual BIOS systems, and potential legal implications under the Computer Fraud and Abuse Act (CFAA).",
      "Comparisons were made with brands like Apple and Framework, which are praised for reliability, while HP's quality is seen as declining, especially post-Carly Fiorina's leadership."
    ],
    "points": 161,
    "commentCount": 114,
    "retryCount": 0,
    "time": 1717704097
  },
  {
    "id": 40610621,
    "title": "OpenSSH Adds New Features to Penalize Repeated Failed Authentication Attempts",
    "originLink": "https://undeadly.org/cgi?action=article;sid=20240607042157",
    "originBody": "Contributed by Peter N. M. Hansteen on 2024-06-06 from the sshd to carry a big stick dept. In a recent commit, Damien Miller (djm@) introduced the new sshd(8) configurations options, PerSourcePenalties and PerSourcePenaltyExemptList, to provide a built in facility in sshd(8) itself to penalize undesirable behavior, and to shield specific clients from penalty, respectively. The commit message reads, List: openbsd-cvs Subject: CVS: cvs.openbsd.org: src From: Damien MillerDate: 2024-06-06 17:15:26 CVSROOT: /cvs Module name: src Changes by: djm@cvs.openbsd.org 2024/06/06 11:15:26 Modified files:usr.bin/ssh : misc.c misc.h monitor.c monitor_wrap.c servconf.c servconf.h srclimit.c srclimit.h sshd-session.c sshd.c sshd_config.5 Log message: Add a facility to sshd(8) to penalise particular problematic client behaviours, controlled by two new sshd_config(5) options: PerSourcePenalties and PerSourcePenaltyExemptList. When PerSourcePenalties are enabled, sshd(8) will monitor the exit status of its child pre-auth session processes. Through the exit status, it can observe situations where the session did not authenticate as expected. These conditions include when the client repeatedly attempted authentication unsucessfully (possibly indicating an attack against one or more accounts, e.g. password guessing), or when client behaviour caused sshd to crash (possibly indicating attempts to exploit sshd). When such a condition is observed, sshd will record a penalty of some duration (e.g. 30 seconds) against the client's address. If this time is above a minimum threshold specified by the PerSourcePenalties, then connections from the client address will be refused (along with any others in the same PerSourceNetBlockSize CIDR range). Repeated offenses by the same client address will accrue greater penalties, up to a configurable maximum. A PerSourcePenaltyExemptList option allows certain address ranges to be exempt from all penalties. We hope these options will make it significantly more difficult for attackers to find accounts with weak/guessable passwords or exploit bugs in sshd(8) itself. PerSourcePenalties is off by default, but we expect to enable it automatically in the near future. much feedback markus@ and others, ok markus@ This new facility comes in addition to the already well known and loved pf.conf state tracking options, and is for now available only in OpenBSD-current, but is almost certainly to be available in the upcoming OpenBSD 7.6 release. At first we were wondering whether these options would be enabled by default before the new release. We did not have to wait long. This subsequent commit settled the issue: List: openbsd-cvs Subject: CVS: cvs.openbsd.org: src From: Damien MillerDate: 2024-06-06 20:25:48 CVSROOT: /cvs Module name: src Changes by: djm@cvs.openbsd.org 2024/06/06 14:25:48 Modified files:usr.bin/ssh : servconf.c Log message: enable PerSourcePenalties by default. ok markus NB. if you run a sshd that accepts connections from behind large NAT blocks, proxies or anything else that aggregates many possible users behind few IP addresses, then this change may cause legitimate traffic to be denied. Please read the PerSourcePenalties, PerSourcePenaltyExemptList and PerSourceNetBlockSize options in sshd_config(5) for how to tune your sshd(8) for your specific circumstances. So now we know: starting with OpenBSD 7.6, PerSourcePenalties will be enabled by default, and admins who do not themselves run PF or other network translation mechanisms will need to keep the consequences of inconsiderate NAT use in mind. Reply Latest Articles Fri, Jun 07 04:21 OpenSSH introduces options to penalize undesirable behavior (0) Thu, Jun 06 18:05 DHCPv6-PD - First steps by florian@ (0) Tue, Jun 04 14:51 Game of Trees 0.100 released (0) 08:50 dhcp6leased(8) imported to -current (4) Mon, Jun 03 13:24 clang option -fret-clean committed (0) Sat, May 25 12:31 clang -fret-clean: cleaning return addresses off stack (by deraadt@) (0) Wed, May 22 04:52 Important message for Apple Silicon OpenBSD/arm64 users (0) Mon, May 20 11:48 KDE 6 landed in OpenBSD-current (0) Fri, May 17 09:24 sshd(8) split into multiple binaries (7) Credits Copyright © 2004-2008 Daniel Hartmeier. All rights reserved. Articles and comments are copyright their respective authors, submission implies license to publish on this web site. Contents of the archive prior to April 2nd 2004 as well as images and HTML templates were copied from the fabulous original deadly.org with Jose's and Jim's kind permission. This journal runs as CGI with httpd(8) on OpenBSD, the source code is BSD licensed. undeadly \\Un*dead\"ly\\, a. Not subject to death; immortal. [Obs.]",
    "commentLink": "https://news.ycombinator.com/item?id=40610621",
    "commentBody": "OpenSSH introduces options to penalize undesirable behavior (undeadly.org)156 points by zdw 1 hour agohidepastfavorite66 comments janosdebugs 36 minutes agoHaving written an SSH server that is used in a few larger places, I find the perspective of enabling these features on a per-address basis by default in the future troubling. First, with IPv4 this will have the potential to increasingly penalize innocent bystanders as CGNs are deployed. Worst case, this will give bad actors the option to lock the original owner out of their own server if they have a botnet host in the same network. With IPv6 on the other hand, it is trivially easy to get a new IP, so the protection method described here will be completely ineffective. From my experiments with several honeypots over a longer period of time, most of these attacks are dumb dictionary attacks. Unless you are using default everything (user, port, password), these attacks don't represent a significant threat and more targeted attacks won't be caught by this. (Please use SSH keys.) I have seen experienced sysadmins create the test user with the password of \"test\" on a live server on port 22 because they were having an \"autopilot moment\". It got hacked within 20 minutes of going online and these mechanisms wouldn't have saved it, the attacker got in on the second or third try. If you want to have a read about unsolved problems around SSH that should be addressed, Tatu Ylonen (the inventor of SSH) has written a paper about it in 2019: https://helda.helsinki.fi/server/api/core/bitstreams/471f0ff... reply TacticalCoder 2 minutes agoparent> First, with IPv4 this will have the potential to increasingly penalize innocent bystanders... Worst case, this will give bad actors the option to lock the original owner out of their own server if they have a botnet host in the same network. So instead of looking, like the author of these new options, for ways to make life for the bad guys harder we do nothing? Your concerned are addressed in TFA: > ... and to shield specific clients from penalty > A PerSourcePenaltyExemptList option allows certain address ranges to be exempt from all penalties. It's easy for the original owner to find the list of all the IP blocks the three or four ISPs he's legitimately be connecting from to that exemption list. I don't buy your argument nor all the variation on the same theme: \"There's a minuscule risk of X, so we absolutely nothing but saying there's nothing to do and we let bad guys roam free!\". There's nothing more depressing than that approach. Kudos to the author of that new functionality: there may be issues, it may not be the panacea, but at least he's trying. reply overstay8930 22 minutes agoparentprev> With IPv6 on the other hand, it is trivially easy to get a new IP, so the protection method described here will be completely ineffective. I’m sure this will be fixed by just telling everyone to disable IPv6, par for the course. reply mananaysiempre 26 minutes agoparentprev> I have seen experienced sysadmins create the test user with the password of \"test\" on a live server on port 22 because they were having an \"autopilot moment\". pam_pwnd[1], testing passwords against the Pwned Passwords database, is a(n unfortunately abandoned but credibly feature complete) thing. (It uses the HTTP service, though, not a local dump.) [1] https://github.com/skx/pam_pwnd reply Latty 26 minutes agoparentprevAnd even with IPv4, botnets are a common attack source, so hitting from many endpoints isn't that hard. I'd say \"well, it might catch the lowest effort attacks\", but when SSH keys exist and solve many more problems in a much better way, it really does feel pointless. Maybe in an era where USB keys weren't so trivial, I'd buy the argument of \"what if I need to access from another machine\", but if you really worry about that, put your (password protected) keys on a USB stick and shove it in your wallet or on your keyring or whatever. (Are there security concerns there? Of course, but no more than typing your password in on some random machine.) reply janosdebugs 19 minutes agorootparentYou can use SSH certificate authorities (not x509) with OpenSSH to authorize a new key without needing to deploy a new key on the server. Also, Yubikeys are useful for this. reply tonyarkles 4 minutes agorootparentJust a warning for people who are planning on doing this: it works amazingly well but if you're using it in a shared environment where you may end up wanting to revoke a key (e.g. terminating an employee) the key revocation problem can be a hassle. In one environment I worked in we solved it by issuing short-term pseudo-ephemeral keys (e.g. someone could get a prod key for an hour) and side-stepped the problem. The problem is that you can issue keys without having to deploy them to a fleet of servers (you sign the user's pubkey using your SSH CA key), but you have no way of revoking them without pushing an updated revocation list to the whole fleet. We did have a few long-term keys that were issued, generally for build machines and dev environments, and had a procedure in place to push CRLs if necessary, but luckily we didn't ever end up in a situation where we had to use it. reply waihtis 9 minutes agoparentprevAgreed. In addition to the problems you mentioned, this could also cause people to drop usage of SSH keys and go with a password instead, since it's now a \"protected\" authentication vector. reply Someone1234 1 hour agoprevThis is great, and helps solve several problems at once. I would like to remind everyone that an internet facing SSH with a password is very unwise. I would argue you need to be able to articulate the justification for it, using keys is actually more convenient and significantly more secure. Aside from initial boot, I cannot think of the last time I used a password for SSH instead of a key even on a LAN. Support for keys is universal and has been for most of my lifespan. reply securam 1 hour agoparentSome might argue SSH certificates are even better: https://smallstep.com/blog/use-ssh-certificates/ reply mianosm 1 hour agorootparentThere's a high bar to set for most organizations. Leveraging certificates is excellent if the supporting and engineering actors are all in accordance with how to manage and train the users and workforce how to use them (think root authorities, and revoking issued certificates from an authority). I've seen a few attempts to leverage certificates, or GPG; and keys nearly always are an 'easier' process with less burden to teach (which smart(er) people at times hate to do). reply upon_drumhead 56 minutes agorootparentSSH Certificates are vastly different then the certificates you are referencing. SSH Certificates are actually just a SSH Key attested by another SSH Key. There's no revocation system in place, nor anything more advanced then \"I trust key x and so any keys signed by X I will trust\" reply magmastonealex 40 minutes agorootparentThere is a revocation system in place (the RevokedKeys directive in the sshd configuration file, which seems to be system-wide rather than configured at the user-level. At least, that’s the only way I’ve used it) I agree with the sentiment though, it is far less extensive than traditional X.509 certificate infrastructure. reply karmarepellent 48 minutes agorootparentprevI am no familiar with SSH certificates either. But if there is no revocation system in place, how can I be sure access from a person can be revoked? At our org we simply distribute SSH public keys via Puppet. So if some leaves, switches teams (without access to our servers) or their key must be renewed, we simply update a line in a config file and call it a day. That way we also have full control over what types of keys are supported and older, broken kex and signature algorithms are disabled. reply hotdogs 20 minutes agorootparentThe certificates have a validity window that sshd also checks. So the CA can sign a certificate for a short window (hours), until the user has to request a new one. reply wkat4242 1 hour agorootparentprevYou can store your regular keys in gpg, it's a nice middle ground especially if you store them on a yubikey with openpgp. Of course OpenSSH also supports fido2 now but it's pretty new and many embedded servers don't support it. So I'm ignoring it for now. I need an openpgp setup for my password manager anyway. reply KAMSPioneer 55 minutes agorootparentI use both PKCS#11 and OpenPGP SSH keys and in my opinion, PKCS#11 is a better user experience if you don't also require PGP functionality. Especially if you're supporting MaxOS clients as you can just use Secretive[0]. As you say, FIDO is even better but comes with limitations on both client and server, which makes life tough. [0] https://github.com/maxgoedjen/secretive reply GordonS 1 hour agorootparentprevI setup a test smallstep instance recently, and it works really well. Setup is... complicated though, and the CLI has a few quirks. reply jppittma 52 minutes agorootparentprevHoly shit. I wondered if this was possible a few weeks ago and couldn't find anything on it. Thanks for the link! reply _JamesA_ 43 minutes agoparentprevThe number of expect scripts I find in production that are used to automate ssh password authentication is ridiculous. reply GordonS 1 hour agoparentprevAnother good option is making SSH only accessible over Tailscale or a VPN. reply Someone1234 54 minutes agorootparentThe two aren't exclusive of one another. We've also witnessed situations, with major companies, wherein an SSH \"leaks\" outside the VPN due to network misconfiguration or misconfiguring interfaces on the server. As I said above, keys are actually more convenient than passwords. Only reason people still use passwords is because they believe keys are difficult to use or manage. reply nativeit 59 minutes agorootparentprevThis, with key pairs, is the best blend of security and convenience. I use ZeroTier and UFW on the server and it’s really very simple and extremely reliable. On the very rare occasion that ZeroTier encounters a problem, or my login fails, I still have IPMI access through Proxmox/VMWare and/or my server provider. reply timw4mail 1 hour agoparentprevAny time you access an SSH connection from a different computer, you basically need the password. reply retrac 7 minutes agorootparentAs others note, you could (and if you can, you should) use public key authentication. But -- if you absolutely must use passwords, use one-time single-use pregenerated passwords. Each one becomes invalid after you use it. Keep a handful on a sheet of paper in your wallet. Obviously not appropriate for all circumstances, but quite useful for some cases. Probably more secure than a reusable password. (Doesn't matter if someone key logs your password at a public machine - won't work if used again.) https://wiki.archlinux.org/title/One_Time_PassWord reply krisoft 1 hour agorootparentprevThis is not true. SSH keys are a viable alternative. reply sseagull 1 hour agorootparentIf I can be charitable, I think they mean a different computer than one you usually use (that doesn’t have the SSH key already in authorized_keys). Spouses computer, etc. reply traceroute66 41 minutes agorootparent> If I can be charitable, I think they mean a different computer than one you usually use If I can be charitable .... What the hell are you doing storing your SSH keys on-disk anyway ? :) Put your keys on a Yubikey, take your keys with you. reply unethical_ban 32 minutes agorootparentRight, much easier than a password! And so easy to backup! I'm not arguing it isn't more secure. The point of this subthread is that SSH keys are not as easy to do ad-hoc as passwords, especially when moving workstations. reply Rucadi 40 minutes agorootparentprevIt's just an usually bigger password. reply LtWorf 1 hour agorootparentprevIf it's in the cloud, you pass the public key when creating the vm. If it's a real machine, ask the data center person to do it. reply idoubtit 47 minutes agoprevI've read the commit message in the post, and read it again, but I did not understand how it would be configured. The penalty system seems complex but only 2 parameters are mentioned. From the documentation, one of these parameters is in fact a group of 8 parameters. I guess the separator is space, so one could write: PerSourcePenalties authfail:1m noauth:5m grace-exceeded:5m min:2m See https://man.openbsd.org/sshd_config.5#PerSourcePenalties Unfortunately, the default values are undocumented. So `PerSourcePenalties yes` (which will be the default value, according to the blog post) will apply some penalties. I did attempt to read the source code, but I'm reluctant to install a CVS client, two decades after dropping that versioning system. reply mananaysiempre 38 minutes agoparentThe OpenBSD project provides a CVSWeb interface[1] and a GitHub mirror[2]. The portable OpenSSH project[2] that most of the world gets their OpenSSH from uses a Git repo[4] that also has a Web interface (at the same address) and a GitHub mirror[5]. Per the code there[6], the default configuration seems to be PerSourcePenalties crash:90 authfail:5 noauth:1 grace-exceeded:20 max:600 min:15 max-sources:65536 overflow:permissive [1] https://cvsweb.openbsd.org/ [2] https://github.com/openbsd/src [3] https://www.openssh.com/portable.html [4] https://anongit.mindrot.org/openssh.git [5] https://github.com/openssh/openssh-portable [6] https://anongit.mindrot.org/openssh.git/tree/servconf.c?id=0... reply jiripospisil 37 minutes agoparentprevhttps://github.com/openssh/openssh-portable/commit/81c1099d2... reply yjftsjthsd-h 1 hour agoprevI've seen MaxAuthTries used for similar reasons, and of course fail2ban, but this seems like a nice improvement and it's built in which is probably a win in this case. reply tomxor 37 minutes agoparentI've used fail2ban in production for many years but eventually removed it due to causing very large iptables leading high memory use and ultimately a source of instability for other services (i.e it turns into a DDoS vulnerability for the whole server). I know the usual advice then is to reduce ban time and just not have permabanning, but that seems to kind of defeat the purpose. I wonder how this SSH feature differs since it's implemented at the SSH level. So long as the SSH and or PAM config requires more than a password (I use hardware keys), the main concern to me is log noise (making it hard to identify targeted security threats) and SSH DDoS. I know tarpits and alternative ports are another way of dealing with that, but when SSH is used for many things having to change the port is kind of annoying. I think I'm probably just going to end up layering it like everyone else and stick everything behind a wireguard gateway, although that concept makes me slightly anxious about that single point of access failure. reply yjftsjthsd-h 31 minutes agorootparent> I've used fail2ban in production for many years but eventually removed it due to causing very large iptables leading high memory use and ultimately a source of instability for other services (i.e it turns into a DDoS vulnerability for the whole server). I know the usual advice then is to reduce ban time and just not have permabanning, but that seems to kind of defeat the purpose. Didn't the advice switch to using ipset a while back, precisely in the name of efficiency? reply tomxor 26 minutes agorootparentInteresting thanks, I hadn't heard of that option. reply lxgr 1 hour agoparentprevIt does seem to be very similar in spirit and implementation: > PerSourceNetBlockSize > Specifies the number of bits of source address that are grouped together for the purposes of applying PerSourceMaxStartups limits. Values for IPv4 and optionally IPv6 may be specified, separated by a colon. The default is 32:128, which means each address is considered individually. Just like fail2ban, this seems like it can be equal parts helpful, a false sense of security, and a giant footgun. For example, allowing n invalid login attempts per time interval and (v4) /24 is not a big problem for botnet-based brute force attacks, while it's very easy for somebody to get unintentionally locked out when connecting from behind a CG-NAT. reply SoftTalker 1 hour agorootparentufw/iptables and other firewalls can also throttle repeated connection attempts, which is almost always fine but could be something you don't want if you have a legitmate need to support many rapid ssh connections from the same source (CM tools, maybe?) reply yjftsjthsd-h 1 hour agorootparent> if you have a legitmate need to support many rapid ssh connections from the same source (CM tools, maybe?) If you're doing that, I strongly suggest using ControlMaster to reuse the connections; it makes security tools like this less grumpy, but it's also a nice performance win. reply tgv 48 minutes agoparentprevI managed one machine with such a mechanism, and I had to remove it, because it basically took all resources. I can't remember which daemon it was, but now the machine is only accessible from a limited set of ip addresses. reply ComodoHacker 1 hour agoprevWill it really help today, when anyone with any serious intent doesn't launch their attacks from one or two standalone hosts, but buys botnet capacity? reply gweinberg 13 minutes agoprevIf a client \"causes\" sshd to crash, isn't that a server error? reply Grimeton 1 hour agoprevpam-script with xt_recent works just fine. Everytime when an authentication fails, you add the ip address to the xt_recent list in /proc and in iptables you just check via --hits and --seconds and then reject the connection attempt the next time. reply pluc 12 minutes agoprevSo fail2ban? reply SoftTalker 1 hour agoprevAlso here, didn't gain much notice for some reason. https://news.ycombinator.com/item?id=40605449 reply juancn 58 minutes agoprevThis looks like an easy DDoS exploit waiting to happen. reply idunnoman1222 1 hour agoprevIt’s just fail2ban, should have been in core years ago reply RockRobotRock 1 hour agoparentDid you forget to submit a patch for it? reply cedws 1 hour agoprev [–] The OpenBSD approach to security always seems to be adding things rather than removing them. For example, this feature is intended to make it harder for attackers to break in by guessing the password. So why not remove password authentication, or enforce a minimum password complexity? Password auth for SSH is almost always a bad idea anyway - good, secure software should nudge people towards using it securely, not give them the option to configure it with a gaping security hole. It's the same with the OpenBSD operating system. There's so much extremely obscure, complex code that attempts to address the same problems we have been dealing with for 30+ years. What if we started removing code and reducing the attack surface instead of trying to patch over them, or we came up with an entirely new approach? A good example of how code should be stripped down like this is WireGuard vs the old VPNs. WireGuard came along with fresh cryptography, took all of the bells, whistles, and knobs away and now provides an in-kernel VPN in a fraction of the LOC of IPsec or OpenVPN. As a result, it can be proven to be significantly more secure, and it's more performant too. reply yjftsjthsd-h 1 hour agoparent> It's the same with the OpenBSD operating system. There's so much extremely obscure, complex code that attempts to address the same problems we have been dealing with for 30+ years. What if we started removing code and reducing the attack surface instead of trying to patch over them, or we came up with an entirely new approach? OpenBSD absolutely removes things: Bluetooth, Linux binary compatibility, and sudo, off the top of my head, with the sudo->doas replacement being exactly what you're asking for. reply adamrt 59 minutes agoparentprevI can't think of any long term, open source project that has removed and ripped out more code than OpenBSD. They are know for doing exactly what you are suggesting. Go ask @tedunangst. It was literally called \"tedu'd\" for ripping out old crusty code. reply SoftTalker 1 hour agoparentprevYou can (and need) to do both. And OpenBSD does. LibreSSL as one example removed a huge amount of dead/spaghetti/obsolete code from OpenSSL. And they are removing old/obsolete features all the time. Do you use OpenBSD? Do you read the release notes? reply cedws 1 hour agorootparentThat's not really good enough though, the distros just enable the build flags that let them do naughty things. The software needs to be opinionated on how to use it securely, not leave it up to the users, because the developers that wrote it probably know best! The code simply needs to not exist. If users want to fork and maintain their own insecure branch, let them. reply akerl_ 56 minutes agorootparentAs the parent comments note, LibreSSL ripped out tons of code. Not \"hidden behind build flags\". Deleted. There's plenty of flaws with any project, but OpenBSD is pretty well known for doing exactly the thing you're claiming they don't do. reply adamrt 56 minutes agorootparentprevOpenBSD is also known for this. They constantly push back against adding configuration knobs or running non standard configurations. Have you used OpenBSD? You're telling them they should be doing something, that is already basically their mission statement. reply cedws 50 minutes agorootparentLooking at OpenSSH tells a different story. It is a massive, overly configurable behemoth. The 'WireGuard of SSH' would be 1% of the LOC. It would not provide password auth, or let you log in as root with password auth, or let you use old insecure ciphers. Maybe OpenBSD itself is better at sticking to these principles than OpenSSH. I haven't used (experimented with) it for ~5 years but read about various updates every so often. reply freedomben 1 hour agoparentprevBy removing password auth from openssh, you're not reducing the complexity, you're just moving it somewhere else. I would argue that you're actually adding significantly more complexity because now users/admins can't just bootstrap by using a default or generated root password on a new machine, creating a user, copying over the public key, and then disabling password auth. Now you have to figure out how to get that key into the image, an image you may or may not control. God help you if you don't have physical access to the machine. Edit: I realized after posting that I was equivocating on \"complexity\" a bit because you're talking about code complexity for openssh itself. I don't disagree with you that openssh itself would be less complex and more secure without password auth features, but I think it would have spillover effect that isn't a net positive when considering the whole picture. reply tedunangst 1 hour agoparentprevThat's a pretty weird summary of openbsd development. reply dd_xplore 1 hour agoparentprevOpenSSH is used in variety of platforms, enforcing secret key will prohibit it's usage in lot of places due to the added complexity. reply wkat4242 59 minutes agorootparentIndeed. And then someone will just fork it and the situation will be messier. reply idunnoman1222 1 hour agoparentprevIf you want password auth, you already have to change a default setting in SSHD and restart it. How exactly is removing that as a option ‘less complex’ for the downstream distros? reply cedws 1 hour agorootparentI don't really understand your question. Removing password auth reduces code complexity and therefore attack surface whilst also preventing users from using the software with a dangerous configuration. Maybe the users don't want that, but tough shit, maybe it's the nudge they need to use SSH keys. reply joshuaissac 39 minutes agorootparentIn practice, this will just result in people and organisations using the last version of OpenSSH that supports password authentication. reply karmarepellent 1 hour agoparentprev [–] Just for info: there are alternative SSH server implementations out there that disable features that are discouraged (e.g. password authentication)[0] Tinyssh is just one I already knew, I suppose you would find more via a proper search. [0] https://tinyssh.org/ reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "On June 6, 2024, Damien Miller introduced two new configuration options for OpenBSD's sshd(8): PerSourcePenalties and PerSourcePenaltyExemptList.",
      "These options aim to penalize undesirable client behaviors, such as repeated failed authentication attempts or actions causing sshd crashes, by temporarily blocking offending IP addresses.",
      "PerSourcePenalties will be enabled by default in the upcoming OpenBSD 7.6 release, and administrators are advised to configure these settings carefully to avoid blocking legitimate traffic, especially when handling connections from behind NATs or proxies."
    ],
    "commentSummary": [
      "The debate focuses on new OpenSSH options designed to penalize undesirable behavior, with concerns about their fairness and effectiveness, especially with IPv6.",
      "There is a consensus that SSH keys are more secure and convenient than passwords, with discussions on SSH certificates, key management, and alternatives like PKCS#11 and VPNs for enhanced security.",
      "The conversation also touches on OpenBSD's philosophy of minimizing code complexity for security, contrasting with OpenSSH's configurability, and the potential impact of removing password authentication."
    ],
    "points": 156,
    "commentCount": 66,
    "retryCount": 0,
    "time": 1717780103
  },
  {
    "id": 40604596,
    "title": "CPU Energy Meter: Monitor Intel CPU Power Consumption with Precision on Linux",
    "originLink": "https://github.com/sosy-lab/cpu-energy-meter",
    "originBody": "CPU Energy Meter CPU Energy Meter is a Linux tool that allows to monitor power consumption of Intel CPUs at fine time granularities (few tens of milliseconds). Power monitoring is available for the following power domains: per package domain (CPU socket) per core domain (all the CPU cores on a package) per uncore domain (uncore components, e.g., integrated graphics on client CPUs) per memory node (memory local to a package, server CPUs only) per platform (all devices in the platform that receive power from integrated power delivery mechanism, e.g., processor cores, SOC, memory, add-on or peripheral devices) To do this, the tool uses a feature of Intel CPUs that is called RAPL (Running Average Power Limit), which is documented in the Intel Software Developers Manual, Volume 3B Chapter 14.9. RAPL is available on CPUs from the generation Sandy Bridge and later. Because CPU Energy Meter uses the maximal possible measurement interval (depending on the hardware this is between a few minutes and an hour), it causes negligible overhead. CPU Energy Meter is a fork of the Intel Power Gadget and developed at the Software Systems Lab of the Ludwig-Maximilians-Universität München (LMU Munich) under the BSD-3-Clause License. Installation For Debian or Ubuntu the easiest way is to install from our PPA: sudo add-apt-repository ppa:sosy-lab/benchmarking sudo apt install cpu-energy-meter Alternatively, you can download our .deb package from GitHub and install it with apt install ./cpu-energy-meter*.deb. Dependencies of CPU Energy Meter are libcap, which is available on most Linux distributions in package libcap (e.g., Fedora) or libcap2 (e.g, Debian and Ubuntu: sudo apt install libcap2), and a Linux kernel with the MSR module (available by default) Alternatively, for running CPU Energy Meter from source (quick and dirty): sudo apt install libcap-dev sudo modprobe msr make sudo ./cpu-energy-meter It is also possible (and recommended) to run CPU Energy Meter without root. To do so, the following needs to be done: Load kernel modules msr and cpuid. Add a group msr. Add a Udev rule that grants access to /dev/cpu/*/msr to group msr (example). Run chgrp msr, chmod 2711, and setcap cap_sys_rawio=ep on the binary (make setup is a shortcut for this). The provided Debian package in our PPA and on GitHub does these steps automatically and lets all users execute CPU Energy Meter. How to use it cpu-energy-meter [-d] [-e sampling_delay_ms] [-r] The tool will continue counting the cumulative energy use of all supported CPUs in the background and will report a key-value list of its measurements when it receives SIGINT (Ctrl+C): +--------------------------------------+CPU-Energy-Meter Socket 0+--------------------------------------+ Duration 2.504502 sec Package 3.769287 Joule Core 0.317749 Joule Uncore 0.010132 Joule DRAM 0.727783 Joule PSYS 29.792603 Joule To get intermediate measurements, send signal USR1 to the process. Optionally, the tool can be executed with parameter -r to print the output as a raw (easily parsable) list: cpu_count=1 duration_seconds=3.241504 cpu0_package_joules=4.971924 cpu0_core_joules=0.461182 cpu0_uncore_joules=0.053406 cpu0_dram_joules=0.953979 cpu0_psys_joules=38.904785 The parameter -d adds debug output. By default, CPU Energy Meter computes the necessary measurement interval automatically, this can be overridden with the parameter -e.",
    "commentLink": "https://news.ycombinator.com/item?id=40604596",
    "commentBody": "CPU Energy Meter: A tool for measuring energy consumption of Intel CPUs (github.com/sosy-lab)151 points by todsacerdoti 17 hours agohidepastfavorite49 comments sirn 14 hours agoSince kernel 3.3 or so, RAPL is also exposed through `/sys/devices/virtual/powercap/intel-rapl/*/energy_uj` in micro-joules (if not, `modprobe intel_rapl`). So if you want to do a quick power measurement, it can be done using just POSIX sh (root required): # in milli-watt (1000 = 1W) because shell arithmetic doesn't do floating point while true; do LAST_MJ=$MJ MJ=$(cat /sys/devices/virtual/powercap/intel-rapl/intel-rapl:0/energy_uj) echo $(((MJ - LAST_MJ) / 1000)) sleep 1 done Despite the powercap name being intel-rapl, the powercap interface is also available on AMD machines. For a more detailed reading on several more metrics about the CPU, I think pcm[1] may be a better tool (it's a successor to the Intel Power Gadget the project was forked from). Though, it only works on Intel CPU. [1]: https://github.com/intel/pcm reply 149765 12 hours agoparentThere is also perf: perf stat -e 'power/energy-pkg/' -I 1000 --interval-count 3 # time counts unit events 1.001064377 11.00 Joules power/energy-pkg/2.002605466 10.98 Joules power/energy-pkg/3.003726824 11.01 Joules power/energy-pkg/ reply lathiat 13 hours agoparentprevAMD have an equivalent in uProf: https://www.amd.com/en/developer/uprof.html Power profiling is listed as supported on all CPUs though a bunch of features (including memory bandwidth, one that I had wanted) are limited to EPYC CPUs and don't exist in Ryzen or Threadripper. reply 3abiton 10 hours agorootparentHow to do so with pn Ryzen? RAPL? reply jeffbee 13 hours agoparentprevAnother easy tool that may already be on your system is \"turbostat\". reply ngneer 6 hours agorootparentHighly recommended, very useful tool. reply Sweepi 5 hours agoprevUnfortunately, the AMD equivalent (introduced in 5.8) has been removed from the Linux Kernel (in 5.13) https://www.kernel.org/doc/html/v5.8/hwmon/amd_energy.html https://www.kernel.org/doc/html/v5.12/hwmon/amd_energy.html https://www.kernel.org/doc/html/v5.13/hwmon/amd_energy.html (404) https://www.phoronix.com/news/Linux-5.13-AMD-Energy-Removed https://www.phoronix.com/news/No-More-AMD-Energy reply sirn 1 hour agoparentThis only applies to hwmon, i.e. `sensors`. You can still read this through powercap/intel-rapl (even on AMD systems). reply aftbit 2 hours agoparentprevErm what? So silly! >Long story short, since last year the AMD Energy sensor information has been limited to root due to the PLATYPUS security vulnerability. HWMON maintainer Guenter Roeck proposed slightly limiting and randomizing the sensor data so it couldn't be used for nefarious purposes but still accurate enough for genuine use-cases and no longer needing to be root-only access. However, AMD engineers didn't like that approach. >With the hardware monitoring subsystem maintainer not wanting the information to be restricted to root-only and AMD not wanting the limiting/randomization approach, Guenter went ahead and removed the driver. So... we're better off without having this system at all than we would be if it were limited to root OR if it were randomized? Sounds like silly kernel politicking to me. \"You don't like my plan? Oh well, I guess I'll take the ball and go home, have fun losers!\" reply gpuhacker 1 hour agoprevIn my opinion, Astron's PMT (Power Measurement Toolkit) is a much more useful tool than this, because it abstracts over Intel, AMD, and Nvidia (including Jetson): https://git.astron.nl/RD/pmt There is also a paper about PMT: https://arxiv.org/pdf/2210.03724 reply robertheadley 58 minutes agoprevWild, I just came across this while doing some research on power consumption. I got a AMD 5950X and a Nvidia 4080 Super and I was conscerned about using too much power on my 750 Watt power supply. lol. This was yesterday. Wild. reply speedgoose 12 hours agoprevAre the energy consumption values reported by Intel CPUs accurate? Measuring energy consumption for cheap is hard, so I wonder whether they are big approximations or they have some magic tricks. reply ngneer 6 hours agoparentYes. Much earlier architectures (e.g., Sandy Bridge) used event counters as a rough approximation for energy consumption. However, these days, we use calibrated current sensors, not approximations. These are rather accurate. And accurate enough to do a side-channel attack, too. If software opts-in for security, we also add a little bit of randomness to the readings, in order to avoid measurements being too data-dependent to where crypto would be broken (PLATYPUS attack), but not enough to affect accuracy for normal use cases. reply lastgeniusua 10 hours agoparentprevThey've been verified to be fairly accurate: https://dl.acm.org/doi/10.1145/2989081.2989088 https://dl.acm.org/doi/10.1145/3177754 reply wiz21c 11 hours agoprevCould help to combine with this ? \"Identifying Compiler Options to Minimise Energy Consumption for Embedded platforms\" https://arxiv.org/pdf/1303.6485 reply gpuhacker 2 hours agoparentFor GPUs: https://arxiv.org/pdf/2211.07260 reply reportgunner 7 hours agoprevWhy measure just the CPU and not the whole machine ? reply dannyw 6 hours agoparentBecause PSUs (sadly) don't have a simple interface that transmits to your OS. reply wongarsu 4 hours agorootparentMany server PSUs can report those values. `ipmitool dcmi power reading` might give good results, but most server vendors have their own tooling that gives more detailed results. If you are into overspending on PSUs you can get similar features in consumer PSUs, for example [1]. But at that point we have completely left the realm of \"simple interface\". If you want to get a whole-system overview the PSU values are obviously better. But one of the biggest power consumer in many servers are the fans, and because they react to temperature sensors they always lag behind what's actually happening. That makes attributing a rise or fall in power tricky at sub-minute timescales. That's where CPU energy use is much more useful, even if it's a less complete picture. 1: https://www.thermaltake.com/toughpower-irgb-plus-850w-gold-t... reply jeffbee 2 hours agorootparent> because they react to temperature sensors they always lag Why don't systems drive the fans based on the VR state instead of lagging temperature readings? reply kube-system 2 hours agorootparentAmbient temps aren't always a constant? Nor is the end-to-end performance of the cooling solution? reply jeffbee 42 minutes agorootparentI'm not suggesting that the fan control should be open-loop with respect to temperature, only that temperature is lagging but voltage and current are leading, so if the CPU is going to ask for a massive step change in power the platform can look at that and precharge the fans. reply kube-system 31 minutes agorootparentI guess you could, but what would you gain? You can't dissipate the heat until it reaches the heatsink anyway. And for bursty workloads you might not even know whether or not you're going to need it. The point of the thermal solution is just to keep the chip from throttling or destroying itself, and direct measurement is plenty quick enough to accomplish that. reply gradschool 4 hours agorootparentprevSome do, but (sadly) only for Windows. https://www.corsair.com/us/en/explorer/diy-builder/power-sup... reply reportgunner 6 hours agorootparentprevWhy do you need to transmit to your OS though ? reply SketchySeaBeast 5 hours agorootparentPresumably for ease of reference and integration. If you don't need your OS to see it, why not just plug your PSU into a watt meter? reply shermantanktop 5 hours agorootparentUse hardware to measure hardware…makes sense. reply wongarsu 4 hours agorootparentEspecially in the current IoT boom where you can get \"smart plugs\" with built-in power meters for $5-$10 a piece. If you get the zigbee version you get easy-ish API access without sending the data off into the cloud. But even the Wifi cloud-enabled versions usually have a semblance of an API. I use lots of them throughout the house without any intention of ever using them to actually toggle power. reply Weryj 5 hours agorootparentprevWell imagine your PSU has all the metrics, now what? You need to get them somewhere for software to use. reply iAm25626 15 hours agoprevcheck out Kepler from CNCF https://sustainable-computing.io/design/architecture/ reply petermcneeley 15 hours agoprevhttps://manpages.debian.org/testing/linux-cpupower/turbostat... reply Gelob 7 hours agoprevIntel PMT could be used to do this also https://github.com/intel/Intel-PMT reply steve1977 7 hours agoprevA tool for monitoring Intel RAPL data would probably be a bit more accurate, as this tool is not really measuring anything. reply jhrmnn 7 hours agoprevIn general, how does CPU utilization correlate with CPU power draw? reply sandworm101 6 hours agoparentMore utilization = more power draw. Generally. reply Marsymars 3 hours agorootparentAnd the more cores you load, the higher the total draw, but the lower the per-core draw. reply formerly_proven 12 hours agoprevAs far as I know RAPL is implemented entirely in the CPU and is an estimate of CPU power using a complex model of CPU state, temperature and such. I don't believe it's an actual power measurement like e.g. SVI telemetry is. reply ngneer 6 hours agoparentThis was true for earlier implementations, but newer ones actually measure power. There is an ADC in there. At least for Intel. Not sure about AMD implementation. reply jeffbee 4 hours agorootparentDoesn't this vary from part to part based on how integrated the VR is in that generation? It seemed to go back and forth in some prior generations, or seemed more accurate with Xeon than with client parts. I still treat it as accurate enough but I have wondered. reply imvetri 11 hours agoprevwouldnt the meter also consume excess energy? reply jeffbee 4 hours agoparentIt's just a coulomb counter you can read from an MSR. But yes monitoring it inevitably consumes some amount of energy. It won't cost anything on a busy system but waking up an idle system to read it will be more noticeable. This is why I no longer use background metrics monitors like atop or netdata. An Intel client CPU can idle below 100mw if you leave it be, but something like netdata will raise that to 5W or worse. reply chickenchase-rd 10 hours agoparentprevSomeone should make a monitor for the monitor reply kube-system 2 hours agorootparentAdd it to the list: RAPL, PSU metering, PDU metering, UPS metering, utility metering, etc... reply aljgz 16 hours agoprevnext [6 more] [flagged] silotis 15 hours agoparent> does not switch to 200MHz for a minute during video calls I had a Dell work laptop that did the same thing. As far as I was able to tell the system had a bug/fault that continuously asserted the CPU's BD PROCHOT line when the integrated webcam was active. I don't think it was an Intel bug, the CPU was just responding to the external signal that (falsely) indicated the system was overheating. reply kelnos 13 hours agorootparentI have a similar problem on my Framework laptop; I suspect it's Intel's fault. I've tailed the system's embedded controller log, and I see when PROCHOT gets triggered, but even while watching the temperatures quickly drop to safe levels, the condition never gets cleared. (It's possible it's a bug in the EC's firmware, but I'm afraid it might be something lower level.) reply krackers 14 hours agorootparentprevI think there's an MSR you can set to disable throttling when BD_PROCHOT is active. reply userbinator 5 hours agorootparentprevThrottleStop is your friend. reply Almondsetat 14 hours agoparentprevWhat does this have to do with the content of the article? reply nottorp 8 hours agoprev [–] What's the point, they already consume more than entry level space heaters... reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "CPU Energy Meter is a Linux tool designed to monitor Intel CPU power consumption using the RAPL feature, available from the Sandy Bridge generation onwards.",
      "Developed by LMU Munich, it tracks power usage across various domains (CPU package, core, uncore, memory, and platform) with negligible overhead and is licensed under BSD-3-Clause.",
      "Installation is simple via a PPA for Debian/Ubuntu or from a .deb package on GitHub, and it can run without root access by setting appropriate permissions."
    ],
    "commentSummary": [
      "The discussion highlights tools and methods for measuring energy consumption of Intel and AMD CPUs, including \"CPU Energy Meter,\" Intel's RAPL interface, `perf`, `turbostat`, and AMD's uProf.",
      "AMD's energy measurement feature was removed from the Linux Kernel (version 5.13) due to security concerns, with alternatives like Astron's Power Measurement Toolkit recommended.",
      "The conversation also addresses the accuracy of Intel's energy measurements, challenges in whole-system power measurement, and issues with CPU throttling due to the PROCHOT condition, offering mitigation suggestions."
    ],
    "points": 152,
    "commentCount": 49,
    "retryCount": 0,
    "time": 1717725561
  },
  {
    "id": 40603625,
    "title": "Why Multi-Threading Can Slow Down Your Code: Insights from AWS Engineer",
    "originLink": "https://brooker.co.za/blog/2014/12/06/random.html",
    "originBody": "Marc's Blog About Me My name is Marc Brooker. I've been writing code, reading code, and living vicariously through computers for as long as I can remember. I like to build things that work. I also dabble in machining, welding, cooking and skiing. I'm currently an engineer at Amazon Web Services (AWS) in Seattle, where I work on databases, serverless, and serverless databases. Before that, I worked on EC2 and EBS. All opinions are my own. Links My Publications and Videos @marcbrooker on Mastodon @MarcJBrooker on Twitter Make Your Program Slower With Threads How much do context switches matter? Years ago, while taking a numerical methods course, I wrote some code to calculate the expected number of shared birthdays in a group. The code is very simple: each attempt constructs a vector of N birthdays, then counts the duplicates. The outer loop runs millions of attempts, and calculates the mean number of shared birthdays across all the samples. It’s little more than a tight loop around a pseudo-random number generator. I was also learning about threading at the time, and decided that I could speed up my program by running it on the lab’s shiny dual-core machine. I knew that communicating between threads was expensive, so I had each of my threads calculate their attempts in parallel, and merge the results right at the end. I was expecting a great speedup. Much to my disappointment, though, the multi-threaded version was slower. Much, much, slower. Much like the birthday paradox runs counter to our intuition about statistics, the behavior of bad multi-threaded programs runs counter to our intuition about computer performance. We’re used to computers being much faster than they used to be, and single-threaded efficiency mattering less than it used to in most cases. Counter to that intuition, the gap between good and bad multithreaded programs has gotten worse over time. To illustrate just how bad it can be, I replicated my program from back then. It’s not much more than a multi-threaded tight loop around random(3). It’s nice and quick single-threaded: running 10 million attempts in under 7 seconds. Going up to two threads makes it a bit faster, down to less than 6 seconds. When we hit three threads (on my four core Haswell E3-1240), it all goes horribly wrong: To figure out what’s wrong, we can turn to Linux’s excellent perf tool. Running the 1-thread and 4-thread versions with perf stat make it obvious that something’s going on. For 1 thread: 3,788,352 L1-dcache-load-misses #0.03% of all L1-dcache hits 43,399,424,441 instructions #1.46 insns per cycle 734 context-switches and for four threads: 4,110,904,396 L1-dcache-load-misses #6.88% of all L1-dcache hits 248,853,610,160 instructions # 0.51 insns per cycle 15,993,647 context-switches Two things are going wrong here. One is that we’re seeing a more L1 cache misses with more threads, but the bigger issue is that we’re seeing a whole lot more context switches. The effect of both of these is visible in the much lower instructions per cycle of the second version. There’s no nice constant for the cost of a context switch, but a good modern estimate is around 3μs. Multiplying 3μs by 16 million context switches gives 48 seconds, which is a good hint that we’re headed in the right direction. So, what’s causing the context switches? Back to perf, this time running perf record on the processes, followed by perf report. First, the top few rows for the single-threaded version: # Overhead Command Shared Object Symbol # ........ ........ .............. ........................ 62.01% birthday libc-2.19.so [.] msort_with_tmp.part.0 11.40% birthday libc-2.19.so [.] __memcpy_sse2 10.19% birthday birthday [.] simulate We’re spending 62% of the time sorting the array, which is used to find the duplicates. That’s about what I would have guessed. What about the version with four threads? # Overhead Command Shared Object Symbol # ........ ........ ............. ............ 46.80% birthday [kernel.kallsyms] [k] _raw_spin_lock 8.86% birthday libc-2.19.so [.] __random 3.42% birthday libc-2.19.so [.] __lll_lock_wait_private 3.23% birthday [kernel.kallsyms] [k] try_to_wake_up 2.95% birthday libc-2.19.so [.] __random_r 2.79% birthday libc-2.19.so [.] msort_with_tmp.part.0 2.10% birthday [kernel.kallsyms] [k] futex_wake 1.46% birthday [kernel.kallsyms] [k] system_call 1.35% birthday [kernel.kallsyms] [k] get_futex_value_locked 1.15% birthday [kernel.kallsyms] [k] futex_wait_setup 1.14% birthday [kernel.kallsyms] [k] futex_wait Well, that’s suspicious. There aren’t any locks in my code, but there are a whole lot of references to locks in the trace. raw_spin_lock is obviously a candidate, and it’s suspicious to see so many futex-related calls. Something’s taking locks, and the fact that random is near the top of the list makes it a likely candidate. Before we dive in there, though, let’s confirm that we’re doing a lot of syscalls: sudo perf stat -e 'syscalls:sys_e*' ./birthday Which spits out a long list of system calls, most (like mmap) with just a handful of hits. There are two huge outliers: 46,889,267 syscalls:sys_enter_futex 46,889,267 syscalls:sys_exit_futex That confirms it, something’s taking a lot of futexes. Knowing whether it’s random or not requires a dive into the glibc source, which nearly instantly reveals something suspicious: /* POSIX.1c requires that there is mutual exclusion for the `rand' and `srand' functions to prevent concurrent calls from modifying common data. */ And, just a little bit further down: __libc_lock_lock (lock); (void) __random_r (&unsafe_state, &retval); __libc_lock_unlock (lock); Getting rid of the locks means getting rid of one of two things: shared state, or the necessity to prevent concurrent modification to that state. It seems like the former is easier: reasoning about a data-race-safe PRNG is tricky. There are a many good ways to get rid of shared state in the PRNG. Linux has one particularly convenient way: the C library exposes a reentrant random number generator called random_r (which is used by random, as you can see from the snippet above). Dropping random_r in place of random has an amazing effect: As expected, the context switches are way down and instructions per cycle is nicely improved: 4,166,540 L1-dcache-load-misses # 0.04% of all L1-dcache hits 40,201,461,769 instructions # 1.43 insns per cycle 572 context-switches I recognize that spinning on a tight loop on random is a contrived example, but it’s not too far away from reality. Many programs that multi-thread for performance end up with library or system calls inside relatively tight loops. Our intuition about these things tends to follow Amdahl’s law. At worst, it’s tempting to think, these things count as a non-parallel portion of code and lower the maximum achievable parallel speedup. In the real world, though, that’s not the case. Multi-threaded programs can, and very often do, run much more slowly than the equivalent single-threaded program. It’s just another thing that makes writing multi-threaded code difficult. Other Posts « Back to the blog index 04 Jun 2024 » Not Just Scale 09 May 2024 » It's always TCP_NODELAY. Every damn time. 25 Apr 2024 » MemoryDB: Speed, Durability, and Composition. Marc Brooker The opinions on this site are my own. They do not necessarily represent those of my employer. marcbrooker@gmail.com RSS Atom This work is licensed under a Creative Commons Attribution 4.0 International License.",
    "commentLink": "https://news.ycombinator.com/item?id=40603625",
    "commentBody": "Make your program slower with threads (2014) (brooker.co.za)142 points by unchar1 19 hours agohidepastfavorite92 comments kelnos 13 hours agoImmediately as I started reading this, my mind said, \"random() has global shared state so of course more threads will make it slower\". I say this here not to attempt to demonstrate my oh so special advanced knowledge, but just to marvel how bad our abstractions are sometimes. Maybe \"bad\" is the wrong word. Leaky, inadequate, inscrutable. \"Give me a random number\" feels like it shouldn't require all this overhead and hidden complexity under the hood. But unless you've read a lot about implementation details, or have been bitten by that exact problem before, how would you know? You wouldn't, of course. reply bheadmaster 7 hours agoparentAs others have already mentioned, the main culprit here is libc and its assumption that it is running in a single-threaded environment. The libc was designed with heavy use of global variables in mind. Modules in C are designed as compilation units with static global variables and exposed functions that operate on them. There's a reason many big projects (and programmers) use their own version of the standard library instead of libc. reply ykonstant 5 hours agorootparentAre there any good, instructive open source examples of custom libc replacements you could link? Especially ones that circumvent the single-threaded assumption of libc; I'd love to use the code to educate myself. reply sn9 2 hours agorootparentmusl is one of the better ones [0]. None other than John Regehr recommends studying it [1]. [0] https://en.wikipedia.org/wiki/Musl [1] https://blog.regehr.org/archives/1393 reply ketralnis 13 hours agoparentprevI love the fancy modern type systems and all of the superpowers that they give you. But this is an area that I feel isn't adequately explored. If you call a function that calls a function that has side effects, or that closes the file descriptor you're using, or could panic, or that needs global shared state, or spins up a thread, or allocates, we don't have a good way to deal with that. There are some partial and underutilised solutions like monadic IO and linear/affine types and, uh, not having panics. I have some (bad) ideas but I think this is a space that's worth playing in. reply hnick 13 hours agorootparentI had the same thought years ago, I think some people have looked into it a little, but nothing popular yet. Imagine if you could look at a function, and know, undeniably - this, and anything it can call, absolutely cannot alter the file system directly or make a network call or spawn a process. Maybe instead of just types, you need to supply capabilities (much like interfaces on classes). Sounds like it could be a real pain. Would make auditing easier though. reply mrkeen 12 hours agorootparent> alter the file system directly or make a network call or spawn a process. Those three cases are excluded in pure functions. reply hnick 10 hours agorootparentI've been waiting for functional programming to have its day for more than 20 years. It just seems fundamentally at odds with how most people want to work. You can make individual functions pure in any language, but do many languages enforce this at a core level? Or we have to behave ourselves and hope nothing we call is changed by another programmer later? reply mrkeen 10 hours agorootparent> I've been waiting for functional programming to have its day for more than 20 years. The tech is there. It sounds more like functional programming has been waiting for you for 20 years. > It just seems fundamentally at odds with how most people want to work. \"Most people\", or \"you\"? If you believe in something, stand up for it. reply hnick 10 hours agorootparentI've used it. I've never had a job using it. Most people haven't. reply 0xDEAFBEAD 12 hours agorootparentprevHow about a spawning a separate process and using seccomp, or having the child process run as a different user perhaps? There are a couple of advantages to doing it through the OS. For one thing, the implementation is shared across programming languages. For another thing, it's potentially more secure against an attacker trying to inject code. I guess the disadvantage of doing it at the OS level is that you might have to write OS-specific code. reply hnick 10 hours agorootparentI think that mostly works (as an aside, last time I looked at this in Windows, we couldn't do something like sudo reliably because there was no secure way to provide the password without user interaction - it seems they've just released sudo for Windows in Feb this year). The OS can support this like how a mobile OS limits the capabilities of apps, which is more secure from a final standpoint. But I was also thinking of the development and refactoring side of things. Guarantees and assurances. Sometimes I know the generic code I wrote is 'OK' but the C# compiler says no - you're not being specific enough, I can't cast that - so I have to do better. A while ago I was trying to track down programs that loaded data from certain network shares, and it was a bit rough because these calls were all very deep, and very occasional. Traces (or blocks) of system calls only work when the code fires, but my task was to find and replace them all, so we could delete the share (for various reasons string searches only got part of the way). If 'network access' was tagged at a high level, I could follow the trail to see what we actually access and report on that. We had a similar issue identifying jobs that called out to external APIs. We left a log running for a while to catch some, but some only run a few times a year and we didn't have that long. Adding a firewall exception later could take days and these jobs have turnaround times. I don't know if this is at all feasible. It's just some thoughts I had back then. reply arghnoname 10 hours agorootparentprevThere have been papers that limit the capabilities of a program within a certain context (i.e., within a function), some of which were implemented at the OS level and enforced with hardware protection (e.g., address space isolation). The difficulty is that doing this sensibly requires new OS abstractions. It's one thing to put one in a research paper, but it's really tough to get this kind of thing into an OS nowadays. Another difficulty is OS level abstractions, with a few exceptions, cannot be used without a system call, which is cheaper than it used to be but much more expensive than just a function call. A third problem is just that the programming language has a lot more semantic information, or at least it _can_ have a lot more semantic information, than can be fluently transmitted to the OS. There are approaches to deal with this (like having a richer OS / user land interface, almost impossible to get into an OS). In general, plugging into and extending the type system of some user land is probably going to be much easier route to take. If the research world worked differently than it does, I'd have loved to continue previous explorations on OS / userland interfaces. reply Yoric 13 hours agorootparentprevAlgebraic effects? reply fullstackchris 11 hours agorootparentprevisnt this the whole point of what rust is trying to accomplish? i havent yet worked with rust but from what i hear they make it very hard (impossible?) to have memory issues and things like deadlocks. sounds great in theory but no idea if this is the reality of using it reply adhamsalama 7 hours agorootparentRust solves race conditions, but not deadlocks. reply felipellrocha 11 hours agorootparentprevIt is, yeah. Which is why when you compile 99/100 times it just works. I don’t think people understand that this is one of Rust’s best features. reply mike_hock 10 hours agoparentprevHow would anyone come to the conclusion that an API that explicitly allows setting a global seed value without mentioning anything about thread-local state, wouldn't contend for some kind of shared resource? reply matheusmoreira 10 hours agoparentprev> \"Give me a random number\" feels like it shouldn't require all this overhead and hidden complexity under the hood. It doesn't. The libc is the real problem. The libc and its global state. Even something as simple as errno creates global state problems just like this one. There are buffers all over the place. Freestanding C is a better language just by virtue of not linking against libc. reply glandium 7 hours agorootparentAt least in glibc, errno is thread local. reply teraflop 18 hours agoprevThis reminds me of the fact that Go 1.20 added a clever optimization: If you call `rand.Seed` to seed the global RNG, it's assumed that you want it to produce a single deterministic sequence across all threads/goroutines, which requires locking. But if you never explicitly seed it, you get a different implementation that uses thread-local RNG state without locking. (In practice, there is no way for a program to distinguish this from a single RNG that was initialized with a truly random seed that you can't recover, so it doesn't break backwards compatibility.) Unfortunately, the C standard prescribes that failing to call `srand` must be equivalent to calling `srand(1)`, so I don't think glibc can get away with this trick. Mentioned here: https://go.dev/blog/randv2 reply codr7 15 hours agoparentExactly my issue with Go. For a language that talks so much about simplicity, it puts a lot of effort into inventing clever wheels, with mixed results. I recently realized that SQL connections are thread safe and automagically pooled, I would have preferred more manual control. Don't get me started on date/time-formats. reply skissane 17 hours agoparentprev> Unfortunately, the C standard prescribes that failing to call `srand` must be equivalent to calling `srand(1)`, so I don't think glibc can get away with this trick. I think it could. Just supply two implementations of rand(): a strictly conforming one without this optimisation, and a less than strictly conforming one with it. The second is implemented in a function with a name like `__gnu_rand_optimized`, and then if you `#define __GNU_RAND_OPTIMIZED` before `#include `, then that header does `#define rand __gnu_rand_optimized`. All the C standard requires is that you provide a mode of operation which strictly conforms to it. A platform is allowed to provide configuration options which cause it to be violated in various ways. Strictly conforming code won't turn on those options and so will work as the standard prescribes. If you choose to turn on such an option, that's a non-portable extension, and how it works is between you and the platform, the C standard isn't involved. reply rerdavies 15 hours agorootparentAdd a new function that modifies behavior of an existing one. Or add a new function that provides behavior different from the existing one. The latter is simpler. And safer. And more flexible. And faster, because you don't have a condition to check. reply vlovich123 17 hours agoparentprevI don’t see why it requires locking. You could easily have a TLS seed that gets initialized from the main thread RNG that’s seeded with Rand.seed. No locking required. It’s not like reading from a single RNG state from multiple thread with locks is any extra deterministic over that approach since you fundamentally would have to synchronize at a higher level to guarantee that the RNG is read deterministically across all thread orderings. reply owl57 16 hours agorootparentIn Go or any other runtime that schedules code implicitly on multiple OS threads, this wouldn't work: application writer can (and should, if they need deterministic results) ensure that reads from RNG are done in some specific order, but which of these reads will run on which thread is still non-deterministic, and so will be the results if the library uses thread-local RNG state. reply yuliyp 17 hours agorootparentprevI think the point is that the library probably should assume that if you called `Rand.seed` and/or `srand` it's because you intended to get a specific sequence of random numbers out of it. reply vlovich123 17 hours agorootparentWhat did I write that suggests you wouldn’t get a deterministic specific sequence equivalent to a lock within the RNG? What I said is if you fail to synchronize the surrounding code’s order of calls into the RNG, then you’re going to have non-determinism anyway even with locks within the RNG. For example: T1: If rng()You cannot have sets of threads fetching the same value from the PRNG - it will totally destroy how random the PRNG is. This is the rule you need to follow, but I don't understand what you're saying about global state. The broken version has global state. Non-broken versions can have global state or not have global state, depending on implementation choice. And that choice is separate from whether they have good performance. reply pclmulqdq 16 hours agorootparentHow do you guarantee that you won't get the same value from a PRNG without global state? A PRNG is a state-updating machine that spits out a value based on that state. reply ayende 16 hours agorootparentThread local state You don't need it global Seed it once per thread from /dev/random and you are done reply pclmulqdq 15 hours agorootparentAs I suggested in the first comment here, yes. I believe the GP believed that you could have one PRNG that did not have global state for its consumers. reply stephc_int13 18 hours agoparentprevThe generator state should either be part of the API or be thread local, but there are probably complex implications regarding the long legacy of libc. reply owlbite 17 hours agoparentprevAlgorithms for producing the same stream of random numbers in serial or parallel have been well known for many years. Perhaps the easiest in concept is \"encrypt the sequence 1, 2, 3, 4, 5, ...\". reply pclmulqdq 17 hours agorootparentThe author of the blog wanted each thread to have different random numbers, not the same stream of numbers. reply matheusmoreira 10 hours agoparentprevIs there a reason why the RNG function would not be able to simply take the random seed as one of its arguments? reply randomonad 18 hours agoparentprevThanks, you answered the question I came here to ask, i.e. why did it need a lock for a random number, what are the tradeoffs of using the non locking version. Makes sense. reply kibwen 18 hours agoprevPersonally I think it's kind of silly that we've normalized the idea of PRNGs-as-global-state. I'm not asking everyone to go Full Monad, but please just consider giving each Thing that needs access to randomness its own independent PRNG. reply wging 17 hours agoparentThe world seems full of APIs that make it easy to avoid global state. Most of my usage of randomness has been through things like rust's rand::thread_rng or Java's ThreadLocalRandom. (In fact I think even java.util.Random uses its own state: the docs call out perf issues but only if you share the same instance across threads.) Honorable (?) mention goes to (client-side) JavaScript - it's harder to have threading issues if you only have a single thread to work with! reply mardifoufs 16 hours agoparentprevI think Linux has recently seen quite a lot of work on speeding up its PRNGs but something similar to what you proposed didn't end up making it into the kernel due to a few issues, and I think at the time it ended with this : \"Linus Torvalds, though, argued that none of this effort was worth it. Speeding up random-number generation, beyond a point, is not the kernel's job, he said; instead, that should be left to the C library.\" https://lwn.net/Articles/919008/ I've not been following the mailing lists so I don't know if that ever changed since then. The dev (Jason Donenfeld) seemed to think there was a way to get it working without the issues that blocked the improvement from getting merged last time though. reply hi-v-rocknroll 16 hours agoparentprevBlaming PRNGs sounds like bikeshedding when it's the hot paths in user-land that often do too much or do it badly without respecting the caches, the branch predictor, and/or compiler or don't diagonalize the problem sufficiently to break the hot section of the problem into independent steps to run concurrently and in parallel as much as possible. Some classes of problems have no known parallel solution, and that's okay and beneficial in some cases. Also, if you feel that strongly, please don't roll your own crypto and use a CSPRNG. reply darby_nine 17 hours agoparentprevHow does being a monad intersect with scope i'm genuinely curious reply lalaithion 17 hours agorootparentMonads allow representing which global values a function reads to or writes to in the type of the function. reply darby_nine 16 hours agorootparentMonads do not imply global values or variables. reply anyfoo 15 hours agorootparentBut the stated paradigm is one thing they allow. See the Reader, Writer, and State monads. reply yazzku 17 hours agorootparentprevThe Random Monad in Haskell allows you to write pseudo-random code without caring where the prng comes from. It's only when the caller runs evalRand/runRand that it must provide the prng. So you can't possibly screw up the random code by sourcing a global prng even if you wanted to; rather, it's the caller that has full control over the prng. https://hackage.haskell.org/package/MonadRandom-0.1.3/docs/C... reply Taniwha 17 hours agoparentprevit's not just random() - it's stdio, and malloc/free/new/delete (though some implementations use per-thread pools with added complexity) reply reverius42 17 hours agorootparentSounds like it’s time to go Full Monad then. reply wumbo 17 hours agoparentprevif you have multiple threads working a queue of like-kinded jobs, you’ve given up the determinism of using any sort of global seed you could pull different set lengths of pseudorandom numbers from the different seeds reply owlbite 17 hours agorootparentThat's what counter-based RNGs are for. reply tombert 18 hours agoprevI feel like it's a rite of passage for every engineer to write a program with the assumption of \"more threads = faster\", only to find their program going considerably slower. reply atrettel 18 hours agoparentSimilarly, vectorizing some code doesn't always speed things up. I'm dealing with this problem at work right now. Sometimes complicated loops are pretty well optimized already! That said, vectorization can often make things easier to read. reply fnordpiglet 18 hours agoparentprevThere was a time that was how you achieved throughput. IPC was janky and unreliable and often slow, and threads offered a cleaner interface for cooperative multitasking on a single runtime. That’s changed as IPC and async improved, and the dangers of threaded led to more safety in threading, leading to slower threading performance. reply mandevil 18 hours agoparentprev“The real problem is that programmers have spent far too much time worrying about efficiency in the wrong places and at the wrong times; premature optimization is the root of all evil (or at least most of it) in programming.”- Donald Knuth, The Art of Computer Programming reply foobiekr 18 hours agorootparentSadly, this no longer really applies. Programmers today do absolutely grotesque things unimaginable to Knuth of that era. One look at any modern pipeline that converts to and from JSON a dozen times, sometimes even inside the same process between modules, would make Knuth recall all of that. Programmers now have done the impossible: written code so consistently bad that there are no hotspots, because the whole codebase is non-performant trash. reply rerdavies 15 hours agorootparentKnuth was right! Profiling would indicate whether your JSON serialization/deserialization is actually a problem. Maybe it is. Maybe it isn't. reply hindsightbias 17 hours agorootparentprevI was going to get a t-shirt printed with “Knuth Was Wrong” We’re going to be in a world of hurt when the newest process node doesn’t save us. reply xiasongh 18 hours agorootparentprevThere's key parts left out of that quote that changes the tone quite a bit. Here is the full one \"Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.\" - Donald Knuth reply Animats 14 hours agorootparentKnuth was writing in an era when scientific programs tended to be dominated by inner loops. There was an input part, a compute part that did some number-crunching, and an output part. Only performance in the compute part mattered. Many programs today have no inner loop. Compilers were the first important programs that didn't. Most interactive programs have an outer loop processing events, rather than an inner compute loop. Note that \"AI\" programs are much more like the scientific problems of Knuth's era. All the compute is in tight loops wrangling matrices. reply tombert 18 hours agorootparentprevWell the last time the thread thing bit me was actually a case where it wasn't premature optimization; I had written the code in such a way that I thought was pretty enough, and we were hitting bottlenecks, so my genius brain thought \"ok I'll make this use 20 threads and it'll go faster\". It did not. reply asa400 17 hours agorootparentPretty sure I’ve made exactly the same mistake. I feel like everyone who ever writes concurrent code learns that lesson at some point. It’s absolutely astonishing how much mileage one can get out of thread-per-core-fed-by-work-queues architectures. reply gbin 16 hours agorootparentprevOh with modern architectures this sentence is so wrong. If you don't think about grooming the hardware in the right way from the get go you will never touch peak performance by a couple orders of magnitudes period. See how games are developed and I tell you they don't just use OOP with virtual interfaces and neat indirections all over the place then think oh it is ok we will optimize after the fact. reply DeathArrow 13 hours agoprevI don't know what can we learn from analyzing a poorly written program aside from what not to do. If the program was properly written there wouldn't be any issues with multithreading. The title of the article gives the false impression that multithreading is slowing programs in the general case. In fact poorly written programs are slow and that is true for both multithreaded and singlethreaded programs. If your program runs slow but you don't need it to run fast, don't bother. If your program runs slow and you need it to run fast, go see what optimizations can be done. reply nlitened 13 hours agoparent> the false impression that multithreading is slowing programs in the general case. In fact poorly written programs are slow From my experience, in the general case programs are poorly written, so yeah, multithreading makes them 1) more complex, more poorly written, and 2) slower in many cases. reply Animats 15 hours agoprevI've had trouble with futex congestion. Worst case was under Wine. Wine has DLL files which emulate various Microsoft libc-level functions. They have their own memory allocator. It has three nested locks, some of them have spinlocks, and at least one is locked while a \"realloc\" call is in progress and a buffer is being recopied. Now try a multi-thread program compute bound program with more threads than CPUs that does Rust vector \"push\" operations. Push causes an array to grow, locks get set, other threads hit locks, threads go into spinlock mode, threads inside spinlocks context switch, lose control, and performance drops by two orders of magnitude. Most CPU time is going into spinlocks. I'm not convinced that user-space spinlocks are a good idea. If you're never compute-bound, they can work, but if you run out of CPU time and a context switch occurs inside a spinlock, it's all downhill from there. Windows itself doesn't do this; it has a different locking strategy. reply dang 16 hours agoprevDiscussed at the time: Make your program slower with threads - https://news.ycombinator.com/item?id=8711162 - Dec 2014 (47 comments) reply overfl0w 9 hours agoprevGood article, also has references to the glib's source. I've never dived deep into the random() system function before. This is from the man pages: \"The random() function should not be used in multithreaded programs where reproducible behavior is required. Use random_r(3) for that purpose.\" Perhaps the notes should be updated to state that the performance of multi-threaded programs is also affected due to the shared global resource? reply wonrax 16 hours agoprevThis is the exact same problem I encountered back when I was taking the Parallel Computing course, except I was an absolute novice and had no idea about debugging or profiling. I remember it took me several days speculating and searching for an answer to this issue on Google, and I finally found an explanation on Stack Overflow, which also suggested using rand_r. It was one of the first instances where I was introduced to solving and debugging a difficult computer programming problem, and it still sticks with me to this day. reply beyonddream 16 hours agoprevThis needs 2014 in the title. reply AndyKelley 16 hours agoprevIf you used zig you wouldn't have this problem, just grab your entropy from std.crypto.random and you have yourself a thread-local RNG, properly seeded, with fork safety. reply dkarl 16 hours agoprev\"There's no semantic reason my program isn't embarrassingly parallel but oh WTF are my libraries/frameworks doing? is it all a lie?\" is apparently the pretty universal first experience trying to speed things up with threads. Hopefully we're working towards a future where single threaded legacies don't routinely stab the unwary in the back. I remember in the early 2000s doubling the performance of a CPU-bound C++ program by using two threads, and my boss being shocked that I was able to do it. He had written it off as a naive idea. I was too young and inexperienced to understand why he was surprised; to me it just seemed like he didn't understand what I was doing. In retrospect, now I feel like maybe he had the right default expectation, and I got lucky that it worked out so easily. reply starik36 17 hours agoprev [–] Many, many, many years ago, my employer needed me to write some code to read 8 COM ports in OS/2. Then based on that, do some operations. So, me, being brand new to OS/2, immediately wanted to try out the new multi-threading features. Without knowing anything about locks and spins or deadlocks or shared state, I plunged into it just using the OS/2 documentation and a programming book I bought. Keep in mind, this was a single CPU machine, probably an Intel 486dx or something like that. I spent the next couple of weeks debugging all sorts of things that should not have been happening. There were crashes, lock up, slow downs, missing variables, etc... that I couldn't resolve. After a while, I gave up and did what the OP did: just put everything in a loop and it solved everything. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Marc Brooker, an AWS engineer, discusses the complexities of multi-threaded programming in a blog post, using a birthday paradox simulation as a case study.",
      "He found that multi-threading made the simulation slower due to context switches and L1 cache misses, identified using Linux's perf tool.",
      "Switching to a reentrant random number generator (`random_r`) improved performance, illustrating that multi-threaded programs can sometimes be less efficient than single-threaded ones."
    ],
    "commentSummary": [
      "The discussion highlights the inefficiencies and complexities of using threads in programming, especially with global shared states like `random()`, and the limitations of standard libraries like libc in multi-threaded environments.",
      "Participants suggest custom libc replacements, better abstractions to manage side effects, and explore the benefits and challenges of functional programming and OS-level security measures.",
      "Emphasis is placed on the importance of deterministic behavior in multi-threaded random number generators (RNGs), the use of thread-local states, and the debate over global versus independent pseudorandom number generators (PRNGs)."
    ],
    "points": 142,
    "commentCount": 92,
    "retryCount": 0,
    "time": 1717716521
  },
  {
    "id": 40608332,
    "title": "Exploring \"Badness 0\": Three Unique Approaches by Tom 7",
    "originLink": "http://tom7.org/bovex/",
    "originBody": "Badness 0 (Three ways) Ways one and two: Read Knuth's Version nand Epsom's Version. They appear in the extremely discerning proceedings of SIGBOVIK 2024. Be warned that due to \"BUG\", these seem only to display properly in Chrome. I am working on fixing \"BUG\" once I get some sleep; I have some leads due to helpful people on the blog. The laundered version in the SIGBOVIK proceedings may work. Way three (recommended) is to sit back and bathe in the 4k, 60Hz flashing lights that are Badness 0 (Apostrophe‛s version) is the newest installment in the Main Sequence: If you want to experience \"BUG\" directly, way four is to immediately head to the source code. I will try to add some notes on compiling and using this after getting some sleep. The source code is available under the GPL (COPYING) or GJPL (JCOPYING). Of course, there are multiple irksome videos on my YouTube channel Suckerpinch! Please leave a comment on my blog or on Mastodon at @tom7! Get all Tom 7 thingos at → [tom7.org]",
    "commentLink": "https://news.ycombinator.com/item?id=40608332",
    "commentBody": "Tom 7: Badness 0 (Three ways) (tom7.org)132 points by cubefox 6 hours agohidepastfavorite22 comments plasticbugs 1 hour agoTom7 is my favorite content creator. Each of his projects feels like a ~master’s thesis~ video dissertation. If you are not familiar with his work, please take some time to watch his other videos. They are all outstanding so I won’t recommend any specific one. Tom7, if you’re out there (here), thank you for the free education and entertainment. You are an inspiration! reply umvi 23 minutes agoprevThis guy is great. My favorite of his videos is the one where he has an NES playing SNES games - https://www.youtube.com/watch?v=ar9WRwCiSr0&t=142s reply cenazoic 1 hour agoprev(Read the paper, didn’t watch the video) I am a 54-year old undergraduate in computer science. I don’t know from Curry-Howard (or Hurry-Coward), but this paper made me giggle with delight and glee. (Knuth invokes this delight in me as well, although I don’t understand most of his writings, yet.) If nothing else, it’s inspired me to implement half-ass easter-egg achievement systems in any future ‘serious’ software I write. reply WJW 45 minutes agoparentThat also struck me as a great way to make software more whimsical. Imagine if you randomly got an \"Achievement unlocked!\" message if you churn through more than 10 GB of data in a single invocation of `grep` for the first time or something. So many possibilities! reply ketralnis 22 minutes agorootparentFor something user facing sure but trying to debug why _sometimes_ my grep job hangs with large data dumps because my popen3 didn't know to consume from stderr because of your whimsicle message would be pretty rage inducing reply shadowgovt 15 minutes agorootparentWhy isn't your shell consuming stderr? Errors happen. reply elbasti 7 minutes agoprevEvery Tom7 video is a work of humbling genius that—just by virtue of knowing about them—makes me feel like I'm in some sort of secret club of people smarter than me. reply tromp 4 hours agoprevWorth watching for the great punchline at the end alone... reply JadeNB 2 hours agoprevDo I win anything for noting, in a video about hyper-detail orientation, that, when the text on screen says \"Is this the most beautiful ____\", the voice-over says \"This is the most beautiful ____\"? https://www.youtube.com/watch?v=Y65FRxE7uMc&t=9m07s (I'm not sure if it's also an error, or part of the joke, or if it just parses in a way that I can't accomplish on my own, that the text before the video reads \"Way three (recommended) is to sit back and bathe in the 4k, 60Hz flashing lights that are Badness 0 (Apostrophe‛s version) is the newest installment in the Main Sequence:\".) reply idle_zealot 1 hour agoparentThere are quite a few errors in the presentation that seem intended to annoy detail-oriented people. One that stood out to me was the use of backslashes in his website URL. reply JadeNB 1 hour agorootparent> There are quite a few errors in the presentation that seem intended to annoy detail-oriented people. One that stood out to me was the use of backslashes in his website URL. Yeah, some of those (like the backslashes) were clearly intentional. It didn't look to me like the \"Is this\" / \"This is\" was, but I guess anything can be put down to being part of the joke. reply pimlottc 4 hours agoprevNothing on this page gives me any idea of what this video is about. reply heleninboodler 41 minutes agoparentYou're getting a lot of glib answers, but in all seriousness, this is one of the amazing things about tom7 videos. It sometimes starts by appearing to be about nonsense or triviality, but as he weaves all these weird stories, they start to come together into observations that are absolutely brilliant and funny and he writes code to demonstrate what he's saying. I highly recommend \"Harder Drive: Hard drives we didn't want or need\" as an intro to his style, his humor, and the absurd lengths he will go to in order to prove a point. It's in three chapters and the middle chapter still blows my mind. reply quasimodem 2 hours agoparentprevYou have taken the first step towards understanding Tom7. reply aidenn0 21 minutes agoparentprevDon't worry, by the end he has fully justified all of the diversions. reply mcpar-land 11 minutes agoparentprevskip to 7:22, he explains what the video is about. reply nimih 2 hours agoparentprevThe video explains the main idea(s) behind the two papers which are linked at the top of the page. reply horacemorace 4 hours agoparentprevYou are then in the correct headspace to experience it. reply enqk 39 minutes agoparentprevTruly great works cannot be summarized reply brokensegue 2 hours agoparentprevIt's about many things reply recursive 2 hours agoparentprevDon't feel bad. It's not for everyone. reply cpach 5 hours agoprev [–] See also https://news.ycombinator.com/item?id=40605970 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"Badness 0\" can be explored through Knuth's and Epsom's versions in the SIGBOVIK 2024 proceedings, best viewed in Chrome due to a display bug.",
      "Apostrophe's version offers a 4k, 60Hz visual experience, appealing to those interested in high-quality visuals.",
      "The source code is available under GPL or GJPL licenses, with compilation notes provided, catering to those interested in the technical aspects."
    ],
    "commentSummary": [
      "Tom7 is a well-regarded content creator known for his intricate and educational videos, often likened to academic dissertations.",
      "Fans appreciate his creativity and depth, with notable projects like the NES playing SNES games being highlighted.",
      "His whimsical approach, including intentional errors to engage viewers, often leads to profound and humorous insights, earning him a dedicated following."
    ],
    "points": 132,
    "commentCount": 22,
    "retryCount": 0,
    "time": 1717765275
  },
  {
    "id": 40602886,
    "title": "How to Keep Using Adblockers on Chrome Despite Manifest V3 Transition",
    "originLink": "https://gist.github.com/velzie/053ffedeaecea1a801a2769ab86ab376",
    "originBody": "How to keep using adblockers on chrome and chromium google's manifest v3 has no analouge to the webRequestBlocking API, which is neccesary for (effective) adblockers to work starting in chrome version 127, the transition to mv3 will start cutting off the use of mv2 extensions alltogether this will inevitably piss of enterprises when their extensions don't work, so the ExtensionManifestV2Availability key was added and will presumably stay forever after enterprises complain enough You can use this as a regular user, which will let you keep your mv2 extensions even after they're supposed to stop working Linux In a terminal, run: sudo mkdir -p /etc/opt/chrome/policies/managed /etc/opt/chromium/policies/managed echo '{ \"ExtensionManifestV2Availability\": 2 }'sudo tee /etc/opt/chrome/policies/managed/policy.json /etc/opt/chromium/policies/managed/policy.json ChromeOS enable developer mode upon rebooting, go into vt2 (shift+ctrl+right arrow function key) log in as root type in /usr/libexec/debugd/helpers/dev_features_rootfs_verification && reboot upon rebooting, go into vt2 again and log in as root run the commands from the linux section Windows Open regedit, and create Software\\Policies\\Google\\Chrome\\ExtensionManifestV2Availability as a dword set to 0x00000002 MacOS In a terminal, run: defaults write com.google.Chrome ExtensionManifestV2Availability 2 (note that i haven't tested this for mac. please let me know if it doesn't work)",
    "commentLink": "https://news.ycombinator.com/item?id=40602886",
    "commentBody": "PSA: How to keep using adblockers on Chrome and Chromium (gist.github.com)124 points by coolelectronics 20 hours agohidepastfavorite90 comments martin293 20 hours agoThis solution will work for exactly a year. That's how long google gives enterprises to migrate to manifest v3. Source: https://developer.chrome.com/docs/extensions/develop/migrate... reply plorkyeran 16 hours agoparentI suspect it'll actually work much longer and just become more obnoxious to enable in a year. A complicated workaround that placates power users so they stay on Chrome but that most users won't bother with is perfect. reply justahuman74 20 hours agoprevThis seems like it's too hard to tell friends/family. Is the go-to now just 'install firefox'? reply coolelectronics 20 hours agoparenti would always tell them to install firefox. this is mostly for developers and tech oriented people who need to keep chrome around reply stavros 20 hours agorootparentIf I need to keep Chrome around for development, I'm not using extensions on it, and I'm only visiting the sites I'm developing. reply ASalazarMX 2 hours agorootparentIf you use things like Apple Business Manager and you're not using a Mac, it only works on Chrome. reply jsheard 20 hours agoparentprevIf Google leaves a boolean switch in Chromium to keep V2 enabled then I assume most if not all of the third party Chromium-derived browsers will just flip it to true by default. That's easy for them to do, the hard part is if Google strips out V2 altogether and leaves the downstream browsers to patch it back in. reply londons_explore 20 hours agorootparentEven when it's stripped out entirely, it will be easy to add back in. The changes (that adblockers care about) are just a few if statements changing the conditions under which synchronous inspection of web requests are allowed. Manifest V3 still allows them. But only in limited circumstances which are unsuitable for ad blocking. reply davesmylie 20 hours agorootparentprevThere will be a switch available ... but only for a period of time before the functionality is completely removed reply metadat 20 hours agorootparentAnd apparently, as per this post, only available to folks who are savvy with the command-line. reply pieter_mj 18 hours agorootparentNo, use a chromium derived browser, like https://github.com/win32ss/supermium. Or use firefox, it will always support adblockers. reply metadat 17 hours agorootparentNo kidding, thanks but I do use Firefox. As for Chrome, the writing is on the wall. It's not so shiny any longer. reply VancouverMan 20 hours agoparentprevAfter the XUL removal debacle a number of years ago, I can't trust Firefox to offer a suitably flexible and capable extension system over the long run. While some people will claim that those changes were necessary, the impact was still very negative for the extension developers and users who were affected at the time. The numerous other user-hostile decisions made by Firefox's developers certainly don't help repair the trust that was lost then. reply metadat 19 hours agorootparentWere the poor decisions made by Mozilla engineers or by Mozilla Corporate Executives? Mozilla has never paid engineers particularly well; in the past engineers joined mostly out of a duty bound to philosophical alignment. reply add-sub-mul-div 19 hours agorootparentprevIf this is about trust, Google is by far less trustworthy. Firefox is a flawed option vs. a shitty non-starter option. reply pennybanks 16 hours agorootparentwhy do you think that? mozilla is not without its controversies and google basically owns them reply add-sub-mul-div 3 hours agorootparentThis guy is complaining about \"XUL removal\". Google is the linchpin and progenitor of surveillance capitalism. reply hgs3 20 hours agoparentprev> This seems like it's too hard to tell friends/family. It's an opportunity for someone to write an \"adblock fixer\" app for the non-technical market. reply stavros 20 hours agorootparentI don't understand why people want to teach others how to cope with an abusive relationship, instead of telling them to leave. reply shrimp_emoji 19 hours agoparentprevI consider my non-technical friends/family technically dead. They're lost souls you can't help, like Elizabeth Swan looking down at her father's ghost on the boat. We can't help. Freedom and privacy are luxuries only available to those nerdy enough to use Linux. The rest are prey/prisoners/peasants to the technofeudal overlords. reply causality0 20 hours agoparentprevYou can't just send them a .reg file and tell them to double click on it and hit ok? reply fifteen1506 20 hours agoparentprevJust tell them to use Edge. It's the worse you can do to Google. reply paxys 20 hours agorootparentIt's also the worst you can do to your friends and family. reply irrational 20 hours agorootparentprevWhy edge instead of Firefox? reply pcf 20 hours agorootparentOn macOS it actually uses less resources, and you can use Chrome extensions. reply ASalazarMX 2 hours agorootparentOn Mac? Just tell them to use Safari, why install twice the same browser? reply rwaksmunski 20 hours agorootparentprevSearch defaults to Bing? reply gravescale 20 hours agoprevFinally some news that makes me think the Firefox has a glimmer of hope left. reply quickthrowman 20 hours agoparentSame here, I’ll be switching to Firefox today after hearing this news. reply gravescale 15 hours agorootparentWhat has been holding you back so far? It seems strange to me that so many normally quite pro-privacy, pro-FOSS, Google-sceptical people are still using Chrome to this point. reply chx 20 hours agoparentprevEvery app there are gets worse and worse -- aka Doctorow's enshittification So people will just take the sudden deluge of ads in stride and move on. Chances of this moving people to Firefox is slim. reply wruza 19 hours agorootparentAlmost every adblock user will move to firefox. They didn’t install it out of boredom, cause that’s non-trivial. I mean it is trivial, but the world fights over defaults now and there are people who only use defaults and complain when they change. So yeah, they will move. Download an app, import userdata, reinstall extensions and blog about this hacker-news-level journey. reply chx 18 hours agorootparent> reinstall extensions I've done this journey. This step is exceptionally difficult, sometimes impossible. https://github.com/exile-center/better-trading And it's even more difficult because I can't find a link to the original store page from an addon within firefox so I can't easily share my findings. https://i.imgur.com/zh2M9YA.png I do not see a link. Perhaps fixing that should have been a priority for Mozilla instead of a bullshit alt generator but it's AI so of course it's got priority. Instead of doing everything in their power to grow their market share ... spreadfirefox reboot when. reply nazgulsenpai 20 hours agoprevFirst recommendation would be a pihole running anywhere you can run it, but if you don't want to or can't do that, you can use Steven Black's ad list to create a hosts file to DNS sink ad/bad networks locally: https://github.com/StevenBlack/hosts reply jsheard 20 hours agoparentIsn't DNS-level blocking strictly less capable than even the nerfed Manifest V3 filtering API? V3 can still block at a more granular level than nuking entire hostnames AFAIK, even if it's not as granular as V2. reply cj 20 hours agorootparentSpecifically, the ability to block specific URL patterns in addition to just hostnames. DNS blocking gets you 90% of the way there, though. (And the only way to block your smart fridge from phoning home) reply nazgulsenpai 19 hours agorootparentprevYes, I didn't intend to suggest it a replacement for a strong ad block but if v3 neuters it for those who don't want to or can't swap browsers. Also a big benefit of DNS level blocking is it can block telemetry other unwanted network traffic (unless connection by IP or some other wizardry), for instance how often my TV attempts to phone home. reply stavros 20 hours agoparentprevFirst recommendation would be Firefox. reply krackers 20 hours agoprev>key was added and will presumably stay forever Nope, it will be removed after 1 year. There is a chance they delay it depending on how much the enterprises complain, but so long as all their big clients are migrated I doubt they care about the long-tail. reply londons_explore 20 hours agoparentThere's a good chance that whoever is driving this change within Google gets promoted/retires in less than a year, and then it gets left in limbo forever like so many other TODO():'s in the code... reply ipsum2 19 hours agoprevToo bad the HN Flamewar detector removed this post from the homepage. The content is interesting, the comments are pointless bickering over browsers reply sigma5 20 hours agoprevI switched to firefox one year ago. Works fine! reply unpopularopp 20 hours agoprevHow do you make sites that are only whitelist certain browsers to work under Firefox? Is there an app, extensions etc anything? For some handful of sites I have to keep a Chrome install around reply JoshTriplett 19 hours agoparent1) User agent switcher. 2) Report those sites to https://webcompat.com/, and/or to Mozilla (who have an evangelism team to reach out to those sites and get them to stop doing that). reply actinium226 20 hours agoparentprevI just keep a Chrome install around and when a website seems like it's misbehaving I just fire it up in Chrome. It's a bunch of baggage to have around, but it's useful for other stuff. Like when you hit your monthly limit of free articles you can just fire it up in Chrome and now you've doubled your monthly free limit! reply page_fault 20 hours agoparentprevhttps://addons.mozilla.org/en-US/firefox/addon/user-agent-st... reply martin293 20 hours agorootparentSometimes this might not work, in that case try a different extension or different user agent strings. I had to go through a few before I found one that worked. reply drcongo 6 hours agoprevSerious question, why is anyone still using Chrome? It's so user-hostile and basically spyware at this point, it boggles my mind that anyone would intentionally install spyware on their computer. reply freehorse 19 hours agoprevHow many people are gonna do all this to get an adblocker working? How long is this workaround gonna be allowed by google? What excuses remain for sustaining the chromium monopoly that allows this shitshow, and for using chrome and chromium derived browsers instead of firefox? reply jay_kyburz 20 hours agoprevQuestion: Is it possible to run ad blocking at the OS level rather than in the browser? Requests to ad servers just never leave your PC? traffic from ad servers just never arrives at the browser? reply II2II 19 hours agoparentA common approach is to mess around with name resolution. Many operating systems have a hosts file that can be modified. You can do DNS on your own computer. Piholes are a variation on this where people usually use a separate machine to handle DNS requests for their entire network. If you cannot change the DNS for your computer/device, some people use a VPN. I believe this is how things are currently handled on Android. This approach is less flexible than the filtering you can get from a web browser. On the other hand, it can be used to filter DNS requests from all software. With something like a Pihole, you can configure the Pihole and (maybe) your router, and it will work for all devices on your network. reply Krssst 19 hours agoparentprevOn Android there is AdGuard which runs a VPN locally to block ads. It can also parse SSL traffic if one installs an SSL certificate but I don't like the idea very much. In the end I just use it as a light adblock for unencrypted traffic when I don't use Firefox. reply stvltvs 19 hours agoparentprevYou can use the hosts file to block ads. https://winhelp2002.mvps.org/hosts.htm reply efilife 19 hours agoparentprevLook up technitium dns reply Teknomancer 16 hours agoprevBrave. reply tumsfestival 5 hours agoparentImagine still using Brave in plain 2024. reply efangs 20 hours agopreveveryone should just have a pihole reply skrause 20 hours agoparentPi-hole and other DNS-based ad blockers are a lot worse than even Manifest v3 ad blockers, so they aren't any alternative. reply actinium226 20 hours agoprevOr just use Firefox. reply cranberryturkey 20 hours agoprevfirefox is still the best browser. reply coolelectronics 20 hours agoparenti do wish! firefox has web features missing that i need on a daily basis, and their developer team seems wholly uninterested on working on them at all reply martin293 20 hours agorootparentWhich features do you mean? reply wruza 19 hours agorootparentI don’t know whether FF lacks these features, but I’m using: - webpage splits - search by image - go to non- url in bg tab - open in new tab in a virtual sub group rather than just to the right or at the end - tabs retain width on close until mouse goes away (helps with closing series of tabs) - bookmarks open in new tab by default - last tab doesn’t close the browser - gestures and toolbar customization That’s in Vivaldi, I probably forgot a bunch of features that feel natural but may have no FF counterpart. Tbh, looking at FF settings, there’s basically none. You can’t miss features that you never had, I guess. reply cranberryturkey 15 hours agorootparentmost people don't use those it would be silly to add them to firefox unless it was an extension. reply ClassyJacket 20 hours agorootparentprevWhat features? reply stavros 20 hours agorootparentprevLike what? reply baxuz 20 hours agoprevUse Adguard reply jiggawatts 20 hours agoprev“I like this pot, but it just keeps getting warmer and warmer. Does anyone know where I can get some ice packs or something?” “No, I don’t want to jump out. Stop telling me that!” reply hehdhdjehehegwv 20 hours agoprevJust stop using Chrome FFS. reply noman-land 20 hours agoprevnext [18 more] [flagged] TheGlav 20 hours agoparentIt's IE all over again. Sometimes you can't. reply viraptor 20 hours agorootparentIf you have those one or two very specific websites that just cannot cope with Firefox, just use Chrome for them. They're likely not the ad-filled pages anyway, but rather some specific companies. We've done this for the IE in the past, we can do it today. It really doesn't take that much time. There's only one thing that will fix this situation long term and it's lowering chrome's market share. Don't stick with it and get continuously abused - no, it won't get better, only worse from now on. reply page_fault 20 hours agorootparentMost of those \"cannot copes\" can be fixed by changing the user agent to that of Chrome. Most sites that refuse to work in other browsers simply don't test on anything but Chrome and don't want the support burden. reply cal85 20 hours agorootparent> Most sites that refuse to work in other browsers simply don't test on anything but Chrome Today yes, but that could start to change if most tech people stop using Chrome, which they will if they can't block ads easily. Plus many average semi-tech-savvy people use ad blockers and will kick up a fuss and switch if their browsing becomes suddenly ad-ridden. If Chrome gets a reputation like AOL or something, then it won't be as acceptable for a website to only support Chrome. reply noman-land 20 hours agorootparentprevI can't really fathom a scenario where you can't install other browsers but you can run a command in Terminal to alter some root permissions. reply hehdhdjehehegwv 20 hours agorootparentprevI don’t see how that is true. There are a ton of chromium based browsers, many with privacy enhancements baked in, and Safari is well supported (iOS monopoly ensures that). I’ll give you Firefox not working well due to Mozilla shitting the bed under Baker who I consider a vandal and a charlatan. As somebody who lives through IE6 the trend of a dominant browser shitting on users is the same - but so many more options to switch now. reply MattGaiser 20 hours agorootparentprevIE was technically garbage and people eagerly wanted to get away from it as it broke sites. Sites are now mostly built and tested on Chrome. Firefox breaks sites. reply yoyohello13 20 hours agorootparentI've been using Firefox exclusively for years. I have no idea what these people who say it breaks sites are talking about. If you absolutely need to use chrome for something just keep chromium around for that specific site. reply Y_Y 20 hours agorootparentprevFirefox is a decent browser. Some sites are coded like shit and then they futz around until it works in whatever Chrome your cousin used for development. reply viraptor 20 hours agorootparentprevThere's some very specific sites that are broken in non-chrome browsers. Writing this message took you more time than you'd spend starting chrome specifically for them for the next few months. reply ChrisGranger 20 hours agorootparentprevFor sites that break when using Firefox, you can report them here: https://webcompat.com/ reply loloquwowndueo 20 hours agorootparentprevFirefox doesn’t break sites. reply yreg 20 hours agorootparentprevAll browsers break sites. In the past few months we've had our stuff broken due to bugs in Safari, Chromium and even Firefox. But the situation is nothing like when IE was alive. Webdevelopment is much more pleasant now. If I had to pick a black sheep now, I would say it's Safari, simply because the updates are tied in with OS updates. That's a shame. reply noman-land 14 hours agorootparentSafari is the one, nowadays, that seems to always be the outlier. reply freehorse 19 hours agorootparentprevRemoving ad blocking is what breaks the internet for me. Firefox is working 99.9% fine except 0.1% of site which I usually do not miss or in the worst case open in another browser. This is an insignificant nuance compared to the constant distraction of not using an ad blocker. reply noman-land 20 hours agorootparentprevThis is FUD. Firefox doesn't break any sites. There is functionally no difference between sites rendered in Firefox and sites rendered in Chrome. reply wizzwizz4 20 hours agorootparentprevI see you managed to avoid encountering the Great ActiveX Catastrophe. Microsoft successfully managed to lock thousands of services behind proprietary extensions that only IE supported. Also, IE wasn't technically garbage. There were a few, specific things wrong with it. Its main issue was that it implemented web features idiosyncratically, and Microsoft didn't document it, so you had to learn all of IE's \"quirks\" through trial and error: but apart from being undocumented, few of IE's idiosyncrasies were actually bugs. Its box model, for example, was arguably superior to the official W3C recommendation. reply scotthn 20 hours agoprevnext [4 more] [flagged] actinium226 20 hours agoparentBut Brave is based on Chrome, so won't it be affected the same way? And same for Arc? reply johtso 20 hours agorootparentArc are planning on building a built in adblocker https://x.com/joshm/status/1728926780600508716 reply rglullis 20 hours agorootparentprevNo. Brave will keep supporting Manifest v2 extensions. reply nunez 19 hours agoprev [–] Or maybe stop supporting Google's quest to monopolize the web and use (and test against!) LITERALLY ANY OTHER BROWSER reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Users can continue using adblockers on Chrome and Chromium by enabling the ExtensionManifestV2Availability key, despite Google's shift to Manifest V3, which lacks the webRequestBlocking API.",
      "Instructions for enabling this key vary by operating system: Linux and ChromeOS require terminal commands, Windows uses regedit, and MacOS involves a terminal command (untested).",
      "This workaround utilizes enterprise policies to maintain the functionality of adblockers even after official support for Manifest V2 extensions ends."
    ],
    "commentSummary": [
      "A GitHub post addresses maintaining adblocker functionality on Chrome and Chromium despite Google's shift to Manifest V3, which limits adblockers, suggesting temporary solutions for developers and tech-savvy users.",
      "Users debate switching to Firefox, which still supports adblockers, despite distrust due to past changes, and discuss alternatives like Edge, Safari, Pi-hole, and DNS-level blocking.",
      "Concerns are raised about the sustainability of workarounds, the dominance of Chromium-based browsers, and the impact of Google's changes on ad blockers, with mentions of support from browsers like Brave and Arc."
    ],
    "points": 124,
    "commentCount": 90,
    "retryCount": 0,
    "time": 1717711466
  },
  {
    "id": 40601776,
    "title": "Boeing's Starliner Successfully Docks at ISS Despite Thruster Issues",
    "originLink": "https://gizmodo.com/watch-boeing-starliner-dock-iss-astronauts-1851521917",
    "originBody": "By Passant Rabie UpdatedYesterday Comments (12) Starliner approaching the ISS on June 6.Screenshot: NASA TV After back-to-back failed launch attempts, a crewed Starliner capsule finally launched on Wednesday to deliver two NASA astronauts to low Earth orbit. Boeing is testing its capsule’s ability to transport crews to the International Space Station (ISS), with Starliner set to dock to the orbital lab on Thursday. Update: June 6, 1:45 p.m. ET: Starliner missed its first docking opportunity at 12:15 p.m. ET due to technical issues, prompting NASA to target another docking window starting at around 1:30 p.m. ET. Five of the spacecraft thrusters failed during its approach, and four were subsequently recovered. As a result, NASA astronaut Butch Wilmore had to assume manual control of the crew capsule for a while before it went back into its autonomous mode. The capsule finally docked with the ISS at 1:34 p.m. ET following a nail-biting journey. Related Content Boeing’s Starliner Faces Further Delays, Now Eyeing April 2024 Launch Boeing's Starliner Spouts Fresh Helium Leaks During Trek to ISS Sonos First Ever Headphones Are Too Expensive For What They Offer CC Share Sonos First Ever Headphones Are Too Expensive For What They Offer Update: June 6, 1:27 p.m. ET: Starliner’s docking has not gone smoothly, the spacecraft developed trouble with its reaction control system thrusters. Update: June 6, 12:40 p.m. ET: NASA has waved off Starliner’s first docking opportunity and is now targeting a backup at 1:33 p.m. ET. Original article follows. Starliner is scheduled to autonomously dock to the forward-facing port of the space station’s Harmony module at approximately 12:15 p.m. on June 6, where it will remain for about a week. The hatch is set to open at 2 p.m. ET to allow the crew to board the ISS. The docking maneuver will be broadcast live on NASA’s website, as well as the space agency’s YouTube channel. The live stream is set to begin at 9:30 a.m. ET, and you can tune in through the feed below. NASA Live: Official Stream of NASA TV Boeing’s Starliner capsule launched atop United Launch Alliance’s Atlas V rocket, carrying NASA astronauts Butch Wilmore and Suni Williams. “Two bold NASA astronauts are well on their way on this historic first test flight of a brand-new spacecraft,” NASA Administrator Bill Nelson said in a statement. “Human spaceflight is a daring task—but that’s why it’s worth doing.” The Crewed Flight Test is part of NASA’s Commercial Crew Program and is meant to transport crew and cargo to and from the International Space Station (ISS) under a $4.3 billion contract with the space agency. NASA’s other commercial partner, SpaceX, has so far launched eight crews to the space station. During Starliner’s first crewed flight, Boeing will monitor a series of automatic spacecraft maneuvers while NASA will monitor space station operations throughout the flight. Meanwhile, the astronaut crew on board will be testing the environmental control system, the displays and control system, and maneuvering the thrusters, as well as other features of the spacecraft. The two astronauts will provide a virtual tour of the Starliner capsule from space, which will be broadcast live on Saturday, June 8 at 8:50 a.m. on NASA TV. It’s been a struggle for Boeing to reach this stage with its Starliner program, which has been marred by technical hiccups and delays over the past few years. Throughout it all, NASA remained dedicated to seeing its crew ride on board the company’s crew capsule. “For many of us, this is a career-defining moment bringing on a new crew transportation capability for our agency and our nation,” Steve Stich, manager, Commercial Crew Program, at NASA’s Johnson Space Center in Houston, said in a statement. “We are going to take it one step at a time, putting Starliner through its paces, and remaining vigilant until Butch and Suni safely touch down back on Earth at the conclusion of this test flight.” For more spaceflight in your life, follow us on X and bookmark Gizmodo’s dedicated Spaceflight page. Show all 12 comments CONTINUE READING",
    "commentLink": "https://news.ycombinator.com/item?id=40601776",
    "commentBody": "Boeing's starliner docks at ISS after five thrusters unexpectedly shut off (gizmodo.com)117 points by heywire 22 hours agohidepastfavorite78 comments perihelions 21 hours agoI'm not certain I counted right, but I believe it was eight failed thrusters in its first uncrewed test in 2020, and later four failed thrusters in the second one, in 2022. (Combining the failure count of the larger orbital maneuvering thrusters with that of the smaller reaction-control systems). - \"The NASA source said eight or more thrusters on the service module failed at one point and that one thruster never fired at all.\" https://arstechnica.com/science/2020/01/nasa-and-boeing-are-... - \" Two of the 20 main thrusters on the spacecraft's service module, used for orbital maneuvering, failed shortly after Starliner separated from its Atlas V rocket. They were not recovered during the flight. Two smaller reaction control system thrusters also failed during the approach to the space station, but they were recovered.\" https://arstechnica.com/science/2022/05/nasas-verdict-on-sta... reply amelius 21 hours agoparentHurray for the software that managed to work even with the failed thrusters. reply verzali 13 hours agorootparentSounds like they think it's actually a software problem rather than a hardware fault. reply hulitu 7 hours agorootparentWill be fixed in the next release. /s reply appplication 5 hours agorootparentV0.0.2 Patch Notes: fixed a bug that occasionally caused it to explode reply jauntywundrkind 17 hours agorootparentprevAn astronaut took over manual control after automatic docking system started popping failures. reply amelius 8 hours agorootparentIn reality a Python programmer at the mission control center modified the scripts and told the astronaut what buttons to press. reply blackhaj7 21 hours agoprev> As a result, NASA astronaut Butch Wilmore had to assume manual control of the crew capsule for a while before it went back into its autonomous mode Is there a video of this? Bet it was nail biting. Kudos they pulled it off given all that went wrong. reply wmf 20 hours agoparentHe trained for hundreds of hours to fly the capsule manually and the docking process is excruciatingly slow so it's probably not that stressful. Even when everything is working they let the astronauts have a little manual control as a treat. reply UberFly 20 hours agorootparentI don't know. I pictured it like the landing scene from airplane and I'm sticking with it. reply bacheaul 19 hours agorootparentprevYou can try it yourself here -> https://iss-sim.spacex.com/ Protips: Watch your instruments and take it super slow! reply tomxor 18 hours agorootparentSeems pretty easy by orientating first and then just translating into position with some patience. Would be good if there were modes where it randomly breaks n thrusters so you are forced to get creative. reply JumpCrisscross 15 hours agorootparent> Would be good if there were modes where it randomly breaks n thrusters so you are forced to get creative Or rates you on your time to approach vs fuel efficiency. reply blackhaj7 17 hours agorootparentprevDon’t doubt they were well prepared but I can’t imagine what it would be like to learn to drive virtually and then do my first test drive on the edge of the earths atmosphere reply jdsalaro 20 hours agorootparentprev> He trained for hundreds of hours to fly the capsule manually and the docking process is excruciatingly slow _COME ON TARS!_ reply legohead 17 hours agoparentprevWhy not try for yourself: https://iss-sim.spacex.com/ reply jakderrida 20 hours agoparentprevYou know you can always rely on a guy named \"Butch Wilmore\" to take control when shit hits the fan. reply HighlandSpring 20 hours agorootparentButch \"Space Cowboy\" Wilmore reply replyifuagree 16 hours agoprevMcDonnell Douglas knows they can continue to drain Boeing of money because the US govt will step in to bail Boeing out when things go really bad. Reminds me of the cynical calculus around power companies here in California. reply JumpCrisscross 15 hours agoparent> the US govt will step in to bail Boeing out when things go really bad Even an optimistic bail-out scenario involves the stock being wiped clean. (For example, what we did with the automakers.) Given the bipartisan frustration with Boeing, I'd fully expect calls for clawbacks that make failure personally painful for Boeing's leadership. reply replyifuagree 11 hours agorootparentOh there will be political theatre on par with the WWF, no doubt. For those people it falls under the heading \"Nice work if you can get it.\" reply JumpCrisscross 20 hours agoprevThese are the same thrusters that are on Orion [1][2]. (They were delivered to Boeing 6 years ago.) [1] https://www.spacefoundation.org/2018/03/15/aerojet-rocketdyn... [2] https://www.compositesworld.com/news/aerojet-rocketdyne-to-p... reply rootusrootus 22 hours agoprevSounds like they worked through it, so docking was successful. I imagine that much like SpaceX in a similar situation, there are lessons learned that will be applied to the next iteration. reply fabian2k 20 hours agoparentThe reports while it was happening suggested that they were almost down to no redundancies for the thrusters. Which would have been bad enough to disallow the capsule to approach the ISS. They managed to get enough thrusters working to still dock, but it seems that this was rather close. So even though it worked in the end, I would assume that this is still a quite serious incident and will require some examination. Also it seems that these problems are not well understood, which is not the kind of problem you want to have on manned spacecraft: > Some notes from the post-docking news conference on Starliner: Engineers found a fourth helium leak after capture. The thruster issues were similar to those observed on OFT-2, and are still not fully understood. https://x.com/SciGuySpace/status/1798827111454827003 reply bandyaboot 21 hours agoparentprevIdeally you want the iterative lesson learning to wrap up before you put the humans inside. reply wannacboatmovie 21 hours agorootparentThat's why they're called test pilots. They're not putting the general public into spacecraft. FWIW, like many previous astronauts, both crew members are US Navy test pilots. They've literally made a career out of solving complex problems and navigating ultra-sticky situations. reply perihelions 21 hours agorootparentThe function of test pilots is to validate things that can only be validated by manual flight testing, like aircraft handling characteristics—not to risk their lives to do QA. reply latentsea 21 hours agorootparentSometimes the tests aren't green. reply fragmede 20 hours agorootparentsometimes in the messy real world, things go wrong even when the tests are green. reply b33j0r 20 hours agorootparentprevHmm. I hear ya, but this isn’t what I want test pilots spending effort on. Test pilots are closer to an integration test in software. You expect the unit tests to pass green before you expect the related integration tests to work. Multiple failures of maneuvering thrusters is technically a mission failure (in the flight plan sense), despite having nothing to do with the parameters of the mission, nor anything to do with adding humans. Maneuverability is the difference between a craft and junk. I wouldn’t find this acceptable in a project I was managing or funding. Just sayin! Still impressed as hell. reply phone8675309 18 hours agorootparentYou're not funding it yet until the government either nationalizes or bails out Boeing reply usefulcat 13 hours agorootparentThe government is paying Boeing, no? reply rootusrootus 21 hours agorootparentprevYeah, that would be ideal. But it seems uncommon. Lots of complexity in a spacecraft. reply ricardobeat 19 hours agoparentprevSpaceX has a little advantage in that their next iteration doesn’t cost a billion dollars… reply colechristensen 22 hours agoparentprevGenerally you actually want 0 \"lessons learned\" in human spaceflight. Test flights especially and unmanned missions have a much higher tolerance for error. A bunch of thrusters failing with people on board could have been catastrophic and is well into the territory of \"enormous problems which never should have happened\". The astronauts may have to be \"rescued\" with a different capsule returning the departing astronauts to earth. reply rootusrootus 21 hours agorootparentI agree, zero would be ideal. Maybe I'm just too accustomed to the iteration we see all the time when SpaceX equipment fails. This is arguably more a traditional space program and less of a 'go fast and break things' process. IIRC, SpaceX also had some sort of glitch when docking with the station, on a crewed mission. Related to the thrusters, too, if memory serves. reply colechristensen 21 hours agorootparentFound this https://www.space.com/spacex-dragon-international-space-stat... Which was a problem, but it was a backup in case of a leak that broke, it had 0 effect and it was a cargo mission not a people mission. Starliner's failure today did have a significant mission effect and had people. SpaceX had a bunch of rocket landing failures on real missions, but that didn't matter because landing was a bonus, basically nobody had done that before and everything was just fine if a rocket blew up trying to land. SpaceX has also done a bunch of test launches with new vehicles, this also doesn't matter if they fail, they're specifically launched to find failures instead of expending tons of engineering effort which is more expensive than blowing up test rockets. Those things are quite separate from \"there's people on board and several thrusters broke requiring unexpected manual pilot intervention\", this was not expecting or allowing for failure by design but a serious anomaly that has to be researched how the problems weren't caught in human certification. reply inglor_cz 21 hours agorootparentSpaceX destroys a lot of hardware, but during almost quarter of a century of operations, they had AFAIK just one work fatality - a worker was blown off his truck in McGregor in 2014. No doubt they were lucky several times, but so were others, and most space institutions have much longer fatality lists, especially when adjusted to total flight volume. reply SrslyJosh 21 hours agorootparentFatalities aren't the whole story. > The 2023 records, newly disclosed by the Occupational Safety and Health Administration, also show that injury rates at some SpaceX facilities grew worse than those the company had reported in 2022. At its manufacturing-and-launch facility in Brownsville, Texas, for instance, SpaceX reported 5.9 injuries per 100 workers, surpassing its rate of 4.8 injuries in 2022 and topping a space industry average of 0.8. That's 7x the industry average. Ref: https://www.usnews.com/news/top-news/articles/2024-04-22/exc... reply bumby 19 hours agorootparentCan you elaborate on what constitutes the “space industry” in this sense? Sometimes the OSHA groupings don’t align as well as one would think. A wet lab testing material compatibility and a rocket test stand can both be “aerospace” but have very different risk profiles. reply mensetmanusman 16 hours agorootparentprevI imagine these numbers can get padded by including people that have no chance of injury because nothing new is being done (ie human exploration has always been risky). reply inglor_cz 20 hours agorootparentprevCould it be a reporting issue? Fatalities certainly aren't the whole story, but they do have one \"advantage\": they cannot be hushed away, at least not in the Western world. (In China or Russia, well...) That is why, in an analogical situation, crime and public safety of individual countries is often compared using murder levels. It is expected that most developed countries cannot really suppress/fudge murder statistics, even though they can do a lot of shenanigans around registration of lesser crime. I can see a similar situation with individual corporations and their diligence when it comes to reporting injuries. reply HPsquared 3 hours agorootparentEven murder statistics aren't foolproof when comparing crime rates across different countries and time periods, because of differences in medical care standards. Someone can be shot several times in the modern West and recover, but 200 years ago or in a poor country (or maybe in a more remote location) the same person would not survive and it would count as a homicide. reply justinclift 20 hours agorootparentprevIt'll be super ironic if they have to come back via SpaceX capsule. :) reply leptons 20 hours agoparentprev>there are lessons learned that will be applied to the next iteration. Maybe that's why there's 20 thrusters instead of 5. reply dark-star 20 hours agoprevHard to believe that NASA sent people to the moon 60 years ago, when they can't even get basic reaction thrusters working after over 10 years of developing and building the Starliner... reply JshWright 19 hours agoparentNASA is just the customer. Starliner is built by Boeing. reply failuser 19 hours agoparentprevBoeing used to mean quality. Now it means cutting corners for the sake of stock price and dividends. reply userbinator 16 hours agorootparent...and the other trendy word that starts with a D. reply saltymug76 15 hours agorootparentCome on, say it for everyone. reply croes 18 hours agoparentprevThere is a reason why it says Boeing's Starliner and not NASA's reply justapassenger 19 hours agoparentprevYou should read more about failures than happened during Apollo program. Apollo 1, Apollo 13 being most famous, but even during Apollo 11 multiple things failed. reply dark-star 11 hours agorootparentI am aware, but they went from Apollo 1 to Apollo 11 on the moon in like 2 years. This is now after what, 10+ years? With billions of cash and 60+ years of additional know-how and technology? reply yencabulator 2 hours agorootparentOn the contrary, there's a knowledge about spaceflight that was lost because the funding dropped so low in between, and the older generation retired without having a steady flow of projects to train the new generation(s) with. Also, with this round of space enthusiasm, everyone is trying to improve on what was done before, to make the cost of ongoing operations finally more sustainable. For example, EVA suits are built more maneuverable and agile, while at the same being more modular, repairable, and no longer custom fitted to each individual. reply borbulon 21 hours agoprevI think given Boeing’s focus on profit over quality and safety lately, this really shouldn’t come as a surprise. reply impulser_ 20 hours agoparentThey should be focused on profit, because they haven't made a profit in almost 5 years now and Boeing is a very important company to the US. If Boeing ends up going bankrupt, the US government will 100% have to bail it out. reply yarg 20 hours agorootparentThe reason that they're losing money is that nobody trusts their planes - because they're badly designed and prone to falling out of the sky. reply rurban 13 hours agorootparentOnly their new McDonell Douglas planes. The old Boeing planes are still fine. Question is, if the space division is the former or the latter. reply yarg 12 hours agorootparentSure, but it's kind of a moot point when the issues have such an impact on consumer confidence. reply thsksbd 20 hours agorootparentprevThat's inevitable. Management knows they're too big to fail. reply notfried 22 hours agoprevWill these leak and thrusters issues affect the return trip? reply bell-cot 22 hours agoparentAbsolutely. Even if zero additional problems crop up while they are at the ISS, the Controllers will need to have fall-back-back-back-and-further-back plans on tap, in case of yet more failures in the Service Modules' propulsion systems. reply cbsks 20 hours agorootparentNot surprisingly, they already have these backup plans. The potential for RCS jet failure was known ahead of time and was determined to not be a flight safety concern. There’s a bunch of info about it here: https://spaceflightnow.com/2024/05/24/nasa-says-boeings-star... reply LegitShady 21 hours agoparentprevIt sounds like they only recovered 4/5 thrusters. reply bumby 21 hours agorootparentTo be clear, I believe there are a total of 28 RCS thrusters, 5 of which had issues reply Fishpants 15 hours agorootparentKSP mojo. reply wkat4242 11 hours agorootparentMoah struts! Uhh, I mean thrusters. reply vlovich123 22 hours agoprev> Starliner’s docking has not gone smoothly, the spacecraft developed trouble with its reaction control system thrusters reply Aaargh20318 21 hours agoprevI wonder how this affects the certification. reply throwup238 21 hours agoparentI bet the actuaries responsible for the astronauts’ life insurance just bumped the premiums up a few tiers. reply SirMaster 5 hours agoprev\"It's not possible\". \"No, it's necessary\". reply hehdhdjehehegwv 20 hours agoprev“If it’s Boeing I ain’t going” reply replyifuagree 16 hours agoparentCame here to post this! reply downrightmike 22 hours ago [flagged]prev [5 more] What a joke: An accurate and all encompassing critique of the vehicle and the company who made it. reply dang 22 hours agoparent\"Please don't post shallow dismissals, especially of other people's work. A good critical comment teaches us something.\" https://news.ycombinator.com/newsguidelines.html reply rootusrootus 21 hours agoparentprev [–] Has SpaceX had any thruster problems when docking with the space station? reply willy_k 20 hours agorootparentThey had an issue with a leaky valve, in a backup system, on a cargo mission. https://arstechnica.com/space/2023/08/valves-are-a-regular-c... reply gpm 20 hours agorootparentprev [–] Crew Dragon C204 docked with the ISS and returned without problem without people. It was suppose to do so again with people, but instead it blew itself to smithereens on the ground due to a thruster problem. Which is to say \"not exactly (at least that I remember), but problems in a similar vein on hardware that did similar things\". reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Boeing's Starliner capsule successfully launched, carrying NASA astronauts Butch Wilmore and Suni Williams to low Earth orbit, after multiple failed attempts.",
      "The mission, part of NASA’s $4.3 billion Commercial Crew Program, aims to test Starliner's capability to transport crews to the International Space Station (ISS).",
      "Despite technical issues with its thrusters, the capsule docked with the ISS, and the astronauts will provide a live virtual tour of the Starliner on June 8."
    ],
    "commentSummary": [
      "Boeing's Starliner successfully docked at the ISS despite experiencing multiple thruster failures, continuing a trend of similar issues from past missions.",
      "Post-docking, engineers discovered a fourth helium leak and unresolved thruster problems, raising concerns about the spacecraft's reliability for future manned missions.",
      "The discussion highlights a comparison between Boeing's traditional approach and SpaceX's iterative, risk-tolerant strategy, noting criticisms of Boeing's quality decline and the impact of funding cuts on spaceflight knowledge and mission safety."
    ],
    "points": 117,
    "commentCount": 78,
    "retryCount": 0,
    "time": 1717704393
  },
  {
    "id": 40607889,
    "title": "E-Paper 7-Color Display Shows Daily Weather Updates",
    "originLink": "https://github.com/blixt/sol-mate-eink",
    "originBody": "A hobby project of mine that was surprisingly easy to put together! I love waking up and seeing the freshly generated image.",
    "commentLink": "https://news.ycombinator.com/item?id=40607889",
    "commentBody": "E-Paper 7-color display showing the current weather (github.com/blixt)109 points by blixt 6 hours agohidepastfavorite62 comments A hobby project of mine that was surprisingly easy to put together! I love waking up and seeing the freshly generated image. butchlugrod 4 hours agoI use the Inkplate 6Color for some similar projects. Built-in ESP32 controller, so no Pi required. Can be fully battery-powered, so if you make intelligent use of the deep-sleep/wake schedule, you can stretch that battery charge for a couple of weeks. My favorite one is a red \"On Air\" sign that hangs on my office door (used for meetings and recordings.) I just trigger a shortcut on my Mac and it automagically updates my status and mutes my devices. They have example open source projects for weather displays, news, Google Calendars... https://soldered.com/product/inkplate-6color-color-e-paper-b... Cheap e-ink projects are super fun and useful. reply localfirst 4 minutes agoparentholy crap! this hits all the checkpoints edit: shocked at the prices! reply blixt 3 hours agoparentprevNice, super clever use of it! Yeah it's super fun and makes me want to put these screens all over my house. reply edent 4 hours agoprevI'm struggling to work out why this uses AI for getting the weather? There are dozens of different free weather APIs which could be called directly. reply blixt 3 hours agoparentIt uses AI for some things you can't really use free APIs for: - Turn a colloquial name for a neighborhood into the exact latitude and longitude (I'm sure there's some API that can kind of do this but I don't know a free one that is as accurate as GPT) - Adding to the previous point, if you say \"Hogwarts\" or \"Tatooine\" it will dutifully give you the weather of those too thanks to AI [1] - Most importantly, write a Dall-E compatible prompt to generate an image with the actual weather conditions (and sun/moon etc) [1]: https://x.com/blixt/status/1724099967361462714 reply itishappy 1 hour agorootparentFrom your first point, I'd expect you to be using a service that provides the weather for a given latitude and longitude. From your second point, I'd have expected that to fail for Hogwarts or Tatooine. Taken together, it sounds like you're blindly trusting the coordinates generated by AI, and will happily generate correct-looking results for any input. I'd call that hallucinating the weather. If I type in \"Springfield\" I'm gonna get results but I'm not gonna know if they're mine. This may be an unreasonable level of concern for a fun app. reply blixt 1 hour agorootparentI do use a service for getting the weather (backend: https://github.com/blixt/sol-mate) Yeah the second part I consider a bonus. Especially because I can use street names or slang for locations that no rigid API supports. > Taken together, it sounds like you're blindly trusting the coordinates generated by AI Actually I guess I do \"blindly\" trust the AI's coordinates more than a traditional location search service. Not sure how much you've tested it yourself, but I haven't gotten a single false latitude/longitude pair from GPT-4 unless the location was truly ambiguous, in which case it would be one of them. But as a European, I've definitely experienced rigid APIs happily giving me the weather for \"Stockholm, NY\" which apparently does exist, but wouldn't have been my first option... But yeah, it is just a fun app, so I wouldn't want to make it more rigid either way! reply itishappy 58 minutes agorootparentHave you stress tested it? \"Springfield\" is a very common name in the US. I'd love to know how it handles \"Sandwich\" as well. Again, probably overboard for a fun app! reply blixt 44 minutes agorootparentThey're fair questions, and yeah it will have to disambiguate in such cases. I tried it now and it picked: - Springfield in Illinois - Sandwich in Massachusetts (the exact coords: https://www.google.com/maps/place/41%C2%B045'31.0%22N+70%C2%...) I think that people underestimate how much non-AI technology fails in these cases. And the recovery path is to retry with more specificity in either case. reply itishappy 4 minutes agorootparent> I think that people underestimate how much non-AI technology fails in these cases. That's an excellent point! Just browsed through your code and I see that you're calculating the positions of the sun and moon. Very cool! gandalfgreybeer 2 hours agorootparentprevOf these, only the first one actually has any real value. And as you've mentioned there are likely less \"expensive\" ways to do so. By expensive, I mean this is essentially using a sledgehammer to crack a nut (especially the Dall-E call). reply blixt 2 hours agorootparentI don't understand how I would get a unique but accurate prompt that makes Dall-E produce an image that besides the weather/lighting conditions also has a randomly picked iconic scene for the location, people in the picture that have the correct clothing and doing reasonable activities, etc. I'm trying to understand the criticism here, so please give the GPT a shot with a few different locations (you can also do neighborhoods or fictional locations) and describe how this is a nut that can be cracked with non-LLM solutions: https://chatgpt.com/g/g-QIydQSFRm-sol-mate reply stetrain 1 hour agorootparentIf your goal was wanting to use AI image generation to create such an image, this makes sense. I think people are just questioning the assumption that such a feature is a desirable or necessary feature of a weather display. I definitely was a bit surprised to open the repo to a weather display and see the need for an OpenAI key without an explanation of which features were AI-powered. But of course this is a personal project, so \"Generate a representative AI image using the location and weather information\" is a perfectly valid, and cool, challenge to set yourself and achieve. reply itishappy 1 hour agorootparentprevGoogle images? You could run a classifying model if you wanted to segment by time of day and weather. reply callalex 1 hour agoparentprevIt’s a very useful line to have on a CV. reply prmoustache 3 hours agoparentprevOr you can just look at the window. reply dylan604 2 hours agorootparentMy favorite is the weather station consisting of a rock sitting in a dish suspended by a chain. If rock is wet, it is raining. If rock is moving it is windy. If you can't see rock, it is night. If rock casts a shadow, it is sunny reply gandalfgreybeer 2 hours agorootparentMore details: https://en.wikipedia.org/wiki/Weather_rock reply dylan604 2 hours agorootparentThanks! I should have known that it would have a wiki entry. Although, I think some of the rules are a bit meh. \"if rock is warm\" you're not meant to need to touch it as it's meant for a quick glance kind of tool. also, as it mentions, the rock is a finely tuned instrument, so thank you for not touching. reply blixt 2 hours agorootparentprevI appreciate that it's just a nice picture that happens to update with the weather, but there's no reason to only use this for your current physical location. It accepts any place in the world (and also fictional places), so you could see a picture of what it looks like where your distant family member or friend is, among many other things. reply gandalfgreybeer 2 hours agorootparentprevWhen I look at the weather app, I don't need to know what the weather is now but what the weather will most likely be later when I'm outside. reply jrussino 1 hour agoprevThanks for posting! I picked up two of these same displays after seeing similar projects here that used them: - \"A WiFi color eInk picture frame\" https://news.ycombinator.com/item?id=35689706 - \"Color E-Ink Display Photo Frame Pranks [Mom]\" https://news.ycombinator.com/item?id=26977747 I was planning to turn them into color photo displays like in those two examples, but this is a neat application too. reply blixt 1 hour agoparentThank you for sharing – I love that the second one also has a microscope closeup of the display while it's changing. Happy hacking, hope to see your projects on here soon! reply kfogel 2 hours agoprevDoes anyone know why projects like this always seem to specify using a particular type of tiny, low-power computer (usually a Raspberry Pi or something similar) to drive the display? I already have plenty of non-tiny computers that run Debian GNU/Linux. Suppose I wanted to run an e-paper display from one of those computers, using this code, just via a normal USB cable. I could do that, right? There's no reason I would have to use a Raspberry Pi or something similar? reply bloggie 1 hour agoparentSmall computers like RPi make it easy to access the low-level peripherals such as SPI, which this small screen uses, and others like GPIOs. If your big-computer has such peripherals available to the OS, you can use them also. Before small computers, you could use the parallel port (and some small program) to talk to your own peripherals via the same low-level signalling. reply Animats 1 hour agorootparentThe other extreme would be nice. Something very low powered that can spend 99% of its time in standby. Then you could run the whole thing on a battery for months. For a weather display, waking up for a few milliseconds per minute should be enough. The 7\" E-Ink display is US$86, which is not too bad. reply kfogel 1 hour agorootparentprevGot it -- I appreciate the explanation. reply joezydeco 1 hour agoparentprevThere's no reason at all. RPis come with lots of bootstrap documentation and code so it's comfortable for someone that's played with Linux to get one running, install some packages, and make it do something. You could do this with a tiny microcontroller if you had the time and knowledge to do it. There's nothing magical about the displays other than strange supply voltages at times. The more common problem is that they don't listen to USB. They take SPI or parallel digital interfaces to set the pixels. So you need some kind of intermediate interface and software to draw the display. Which is why people just slap an RPi into the mix and talk to that over more common protocols. reply kfogel 1 hour agorootparentThank you. My idea was more the opposite: do it with a normal laptop or desktop computer driving the display, rather than a tiny microcontroller. I guess I'm assuming that either the display's USB input supplies enough voltage to run the display, or that the display has a separate power supply -- i.e., that there's nothing magical about a Raspberry Pi that makes it supply special bits or special voltages to these displays that can't be supplied by, say, my desktop computer. reply joezydeco 1 hour agorootparentEdited my response above. The answer is more about the interface that these displays require. reply kfogel 1 hour agorootparentAHHHH, that's the key thing I didn't know (I have a Raspberry Pi sitting in a drawer and have played with it embarrassingly little -- I didn't realize how important having the SPI or other special interface is in this context). Thank you again. reply sethhochberg 1 hour agorootparentWith that said, though, there are also tons of inexpensive ways to output SPI or various other serial protocols from just about any device with a USB port, like your full-sized computer: https://www.adafruit.com/product/2264 The RPis and friends just optimize the workflow - theres nothing particularly magic about they way they implement SPI or GPIO, they just have it out of the box because its such a common way to extend hobby computer boards. reply kfogel 47 minutes agorootparentJust ordered. Thank you :-). reply jrussino 1 hour agorootparentprevMy first thought for a project like this (grab photos/data from the internet, display them on a device) would be a Pi Zero 2 W or a Pi Pico W, for the reasons you stated. I'm not particularly up to date with the tiny microcontroller ecosystem - if I wanted to execute this at lower cost and/or lower power, what would be some better options to consider? reply mikepurvis 4 hours agoprev“Don't leave the same image on the display for too long.” Why is this? Is it something particular about colour e-ink? reply blixt 4 hours agoparentThe recommendation from Waveshare is \"refresh at least once every 24 hours\"[1]. It's probably not such a big deal in reality, but maybe there's some risk of the pixels getting stuck if they are active for too long? [1]: https://www.waveshare.com/wiki/7.3inch_e-Paper_HAT_(F)_Manua... reply mikepurvis 3 hours agorootparentAre they “active” when set? I thought part of the point of e-ink was that it would maintain its image without power applied. reply blixt 3 hours agorootparentNope, it's a completely physical process and state, so you can disconnect it and the image will remain the same. So you could use it as a reusable post card if you wanted, but for $80 maybe it's not such a good idea... I don't really know why they recommend to clear the screen after leaving the same image in for 24 hours, but I think it's mostly a precaution and most likely not a real issue. But for my own sanity, I do clear it at 2am and then it's already drawn a new image at 8am, before I'm out of bed. reply Animats 1 hour agoparentprevBecause it burns in, or because it fades out? reply skrebbel 52 minutes agoprevTotally off topic because this is v cool, but what’s the use case for things showing the current weather? Forecasts I get, but current? Lots of phones and even Windows 11 prominently display this information but who is it for? People who work in windowless offices or something? I mean it’s a bit like a restaurant menu app that only shows you pictures or what you just ordered, right? reply germinalphrase 2 hours agoprevWhat are the power requirements for the build? Could it be run off one or more 18650’s? reply blixt 2 hours agoparentIf using the Raspberry Pi 5 I wouldn't go for a battery-powered solution because it's pretty power hungry. But you can run this screen just fine off of a Raspberry Pi Zero, or ideally a microcontroller if you want basically no power draw while it's sleeping. I was just lazy. reply doakes 2 hours agoprevI never realized just how thin these displays are. Very cool! reply move-on-by 2 hours agoprevThe e-ink review say the display has a dark purple color. How is your experience? reply blixt 2 hours agoparentI asked it to generate the weather for \"The Purple Forest\" (Narnia) and this is what it came up with: https://i.imgur.com/kmKiqbO.jpg You can zoom in on the purples. I would say it looks like a wine purple IRL. reply move-on-by 2 hours agorootparentI’m sorry I was not very clear with my question. The review is negative- saying the ‘white’ is more like a dark purple color. I’m not asking how well it shows purple- but the general quality of the display itself. reply blixt 2 hours agorootparentMy bad, I understand now! I think it's at least as good as a Kindle. I understand why someone who is comparing to bleached paper or OLED/modern LCD screens would say it looks purple, but yeah, I have no complaints. Edited to add: In the picture you can also see the white material at the bottom that is not part of the screen, I would say that is \"true white\". And the picture also has white pixels on the screen. That's the \"dark purple\" the review refers to in that case. reply nathancahill 53 minutes agorootparentYeah, that's pretty purple for white. But still amazing how far this tech has come. reply dh-g 2 hours agorootparentprevWow, these have come a long way. reply slig 4 hours agoprevBeautiful. The prompt in the GH repo is the same you used in your custom GPT? reply blixt 4 hours agoparentSince GPTs only have the one prompt I had to specialize it for a multi-step process with tool use (since it has to call an endpoint and draw an image in one go). Here's the prompt that I use in the GPT: https://gist.github.com/blixt/44bbed23bee7f65b53fd3304781edd... reply slig 4 hours agorootparentThank you very much. reply thih9 4 hours agoparentprevFirst prompt: https://github.com/blixt/sol-mate-eink/blob/1236dbb66b80c838... > Get the weather data for {location}. If the location is not on Earth (fictional or otherwise), pick the place on Earth that is most visually similar to the location. Before you get the weather, reason about which temperature unit is most likely to be used at the location in question. That is the temperature unit you should use when getting the weather. Second: https://github.com/blixt/sol-mate-eink/blob/1236dbb66b80c838... > Here is the weather for the location \"{location}\": > {weather[\"status\"]} > Use Dall-E to generate a beautiful illustration of the location in the style of a wide post card filling up the entire image. Try to describe a scene that is iconic and aesthetically pleasing for the location specified above, but also one that can showcase the weather. (…) reply rlv-dan 4 hours agoprevReally nice color palettes. Some reminds me if MUI on the Amiga. reply MissTake 4 hours agoprevThere a difference in how the display works that’s different to a monochrome one, that requires the caveat of “don’t leave the same image on the display for long”? If memory serves, standard monochrome e-ink displays don’t suffer from burn-in. reply blixt 3 hours agoparentI'm just parroting Waveshare's own recommendations[1], but yeah probably it's not such a big deal. I'll link their docs in the README to help people make a more informed decision. [1]: https://www.waveshare.com/wiki/7.3inch_e-Paper_HAT_(F)_Manua... reply illuminant 6 hours agoprevNice work! Beautiful use of the display. reply NoMoreNicksLeft 3 hours agoprevI suppose I don't understand how colors work on e-paper at all. Can these 7 colors not be dithered? Or even mixed more properly, for something like true color functionality? reply blixt 3 hours agoparentI use Floyd-Steinberg dithering, and if you look at the \"More images\" section you can see it does a reasonable job at representing colors. Also worth nothing that 2 of those 7 colors are black & white, so questionable naming perhaps. I'm pretty sure real production use cases of these screens don't tend to use dithering though, focusing on contrast and readability. reply koolala 3 hours agoprev [–] Does your room have a window? i am trying to understand the earth reply blixt 3 hours agoparent [–] I love windows! And doors. And pictures that update in response to the weather. reply koolala 47 minutes agorootparent [–] Like a news paper that get tossed right through your window onto your desk! I love all those too thank you! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "A GitHub user, blixt, showcased a project using a 7-color e-paper display for weather updates, utilizing AI to convert location names into coordinates and generate weather images.",
      "The discussion included the use of various devices like the Inkplate 6Color and Raspberry Pi for driving e-paper displays, emphasizing power efficiency and ease of use with interfaces like SPI and GPIO.",
      "Participants debated the necessity and effectiveness of AI in disambiguating location names and generating weather visuals, considering the availability of free weather APIs and the practical application of such technology."
    ],
    "points": 109,
    "commentCount": 62,
    "retryCount": 0,
    "time": 1717762230
  },
  {
    "id": 40609126,
    "title": "How I Restored My HP Envy 17 After a Failed BIOS Update and Tech Support Letdown",
    "originLink": "https://github.com/ctag/hp_envy_reviver",
    "originBody": "BIOS Recovery on HP Envy 17\" Not a polished product. These are tools and notes generated while re-programming the BIOS memory chip in an HP Envy-ae110nr laptop. It is my hope that by posting my troubleshooting process here I will help those who own a HP laptop to fix it themselves, and those who don't to save the hassle and avoid buying one in the first place. +--------------------+------------------------------------------------------------+FolderDescription+--------------------+------------------------------------------------------------+extracted_firmwareDumped binaries from SPI chip| host_pcC++ software that interfaces with STM32| imagesImages in readme|recovery_usb_filesFiles created by sp133161.exe and copied off the USB drive| resourcesDatasheets and schematics used| stm32Embedded firmware that interacts with BIOS chip+--------------------+------------------------------------------------------------+ Scenario Several months ago my father's new laptop started having issues with the trackpad sensitivity. He called HP's tech support and was informed that they could not help him troubleshoot the problem at hand until he had updated the BIOS. The tech stepped him through downloading and running the BIOS update tool, and when the computer was rebooted my dad found that HP had indeed upgraded him from a laptop to a brick. For whatever reason the BIOS upgrade failed, and without warning the laptop was left in a near completely inert state. The power button would light up when pressed and the system fan would spin up to maximum. That was it. No backlight, no blinking caps lock, no beeps. After a few attempts at re-doing the BIOS flash, or booting with the special hotkeys, or using a recovery USB drive, the tech gave up and referred dad to the expensive warranty he had purchased along with the laptop. So the computer was packed up and mailed off for repair. Eventually HP got back in touch, and alongside a picture of the laptop's (existing) upgraded RAM was a message stating that the laptop's condition was the consumer's fault for changing the RAM and thus the warranty was void. HP wanted $500 to downgrade the RAM back to stock before they would continue troubleshooting. As we've already established, the laptop's failure was not due to the 3rd party RAM, which had been cranking along perfectly for months. Under the Magnuson-Moss Warranty Act it is illegal for HP to renege on their warranty unless the reason for warranty work is demonstrably due to the consumer's changes. Which is a higher burden of proof than a picture of RAM. See also \"Tie in Sales Provisions\" on ftc.gov. But I am not a lawyer and I sure can't afford one, so Hewlett-Packard will continue to harm consumers with impunity. The laptop was returned un-repaired, and it sat unused for several months. Until this past weekend, when I decided it couldn't get any more bricked and asked to borrow it to continue troubleshooting. Locating the BIOS chip A quick search online led me to schematic-x.blogspot.com which had two PDFs claiming to be schematics for \"HP Envy 17.\" I downloaded one and thumbed through until I found the BIOS on page 34: The IC listed there is a MX25L8005M2C-15G. There isn't one of those on this laptop, but the schematic shows that the BIOS is a SPI device, and there is a MX25L6473F SPI flash chip right next to the CPU: Searching for that second part number brings up some hits that include \"BIOS\" in them. That's promising, and it makes me suspect that this schematic is for a slightly different motherboard also sold as \"Envy 17\". I look at any other likely candidate chips and search their part numbers as well. I'm not sure if this MX25L6473F is the BIOS chip or not, but it seems to be the best bet. Test fixture There's a chance this process would have been much easier if I'd had a Bus Pirate and SOP8 adapter to clip onto the IC while it was still on the motherboard. FYI. Since I don't have a way of clipping onto the IC I decide to remove it from the motherboard rather than try soldering flying wires to the pins while it's attached. From the datasheet I know this part runs at 3 volts, which should be compatible with an STM32F103 dev board I have laying around. I start by folding some jumper wires and soldering them to the 6 pins I need for SPI. Then those wires are brought up to the STM32 dev board. +---------------+---------------+SOP8 BIOS PinSTM32 Pin+---------------+---------------+1PA4| 2PA6| 3Not Connected| 4GND| 5PA7| 6PA5| 7Not Connected| 8VCC+---------------+---------------+ Finally the ST-Link is connected via SWD to the dev board, and a serial (FTDI) adapter is added. Eventually I switched out the FTDI adapter for a virtual CDC serial port on the STM's integrated micro-USB. Extracting the firmware This step wasn't strictly necessary, but I want to gain familiarity with communicating to the SPI device before attempting a write, and I want the ability to read back out new firmware to confirm it was written correctly. The first step was to read the RDID and status registers, saving the contents to a variable and inspecting in my debugger. Along the way I learned that the STM32's chip-select function isn't great, and pivoted to manually toggling the line as a GPIO. Once that was working I added in serial output and expanded the STM32 code to begin transferring all of the flash memory contents on boot. The SPI code was cribbed from this digikey source file. If the STM32 only programs successfully the first time (SWD wasn't configured): https://www.reddit.com/r/embedded/comments/kz5ar4/error_in_initializing_stlink_device_found_on/gjorccv/ SPI reference for STM32: https://www.digikey.com/en/maker/projects/getting-started-with-stm32-how-to-use-spi/09eab3dfe74c4d0391aaaa99b0a8ee17 The first several attempts at dumping the chip contents used either gnu-screen or stty: screen /dev/ttyUSB0 115200 > output.bin (stty raw; cat > output.bin) 083AD.hex Flashing new BIOS My first inclination for flashing the new binary file would be to bundle it with the STM32 firmware. That way no host_pc-serial issues could corrupt the transfer. Unfortunately the STM32 I have is too small to package the whole file, so I had to backtrack to sending the file from the C++ host_pc program. Bundling binary file for C: https://web.archive.org/web/20140313121651/http://www.burtonini.com/blog/computers/ld-blobs-2007-07-13-15-50 Building for the STM32: arm-none-eabi-ld -r -b binary -o 083AD.o 083AD.bin Including that file in STM32cubeIDE: Right click the project -> Properties -> C/C++ Build -> Settings -> Tool Settings tab -> MCU GCC Linker -> Miscellaneous -> Additional object files. Add the path to the arm linked 083AD.o file there. After that approach was blocked by the microcontroller's flash storage space, the object file was re-built with regular ld and included in the host_pc program. After a few failed attempts to write a page-at-a-time to the STM32, I once again fell back on byte-by-byte with ack bytes being sent back. This time the entire file was written and read back out intact. MD5 sums: 746d2097365c29ba0a2e90d04f627b4b 083AD.bin 746d2097365c29ba0a2e90d04f627b4b dump_16_0x1_full_post_write_083AD.bin Conclusion Once I was happy with the successful programming of the chip, it was removed from the test fixture and put back in the laptop. On first boot... Nothing happened. In fact, it took several boots, and possibly removing the hard drive (which we had also tried prior to all of this) before I finally saw the backlight flicker. The system then rebooted on its own, and the HP logo appeared along with a message complaining that the CMOS BIOS settings had been lost. After that everything appeared to work.",
    "commentLink": "https://news.ycombinator.com/item?id=40609126",
    "commentBody": "Recovery after HP tech support bricked my father's laptop (github.com/ctag)108 points by heartag 4 hours agohidepastfavorite50 comments c2h5oh 1 hour agoReport warranty fraud here https://reportfraud.ftc.gov/form/main?TC=RightToRepair more info https://gamersnexus.net/gn-extras-news/gamersnexus-warranty-... reply heartag 1 hour agoparentThank you! reply pugworthy 1 hour agoprevRepeating a reply to a comment here, but I think it needs to be said. People need to be careful they don't overgeneralize too much about HP. Especially if basing opinions on home/small office product lines, which is not all HP does. I'll fully agree that the bad rep due to home/small office products is probably well deserved. But HP has some pretty interesting and successful products such as the Life Sciences dispensers for cells and fluids (see https://www.hp.com/us-en/specialty-printing-solutions/life-s...). Then there are the industrial printing products (https://www.hp.com/us-en/industrial-digital-presses.html) which are rather mind bogglingly fast and capable. Imagine a printer printing to a paper roll 40\" wide at hundreds of feet per second, and the printing is paper statements to be cut and sorted for mailing. reply ChrisArchitect 2 hours agoprevRelated context: HP bricks customers laptops with faulty automatic BIOS upgrade https://news.ycombinator.com/item?id=40601711 reply focusedone 2 hours agoprevNo human on this planet has gone to that much trouble to repair an Envy before. Bravo! I think we've all been frustrated by HP/Dell/Others support. This guy took it to the next level to prove it's not the freaking ram. Nicely done. reply molticrystal 2 hours agoprev> would have been much easier if I'd had a Bus Pirate and SOP8 adapter to clip onto the IC while it was still on the motherboard. >I decide to remove it from the motherboard For those who want to avoid soldering(unavoidable if chip is bad), SOP8 clips are very cheap on ebay. There are many alternatives to the STM32 dev board used in the article, common ones are raspberry pi's such as the zero w(what I tend to use, requires some config changes), but also SPI to USB adapters for PC. Always read the flash several times to increase the likelihood you got an error free read, and also read it after writing as well. Having a working setup able to read and write spi flash is very inexpensive and a handy emergency skill to have. reply h3xk1tt3n 2 hours agoparentI used one of those clips and a raspberry pi to reflash an HP all-in-one that was bricked. I was super proud of it, but then it was destroyed in shipping on its way to the customer. reply Scoundreller 2 hours agoparentprevI miss rs232 and parallel ports that made stuff like this trivially easy with just diodes and resistors. reply casenmgreen 2 hours agoprevLong time ago, I had a similar - but worse - experience with Sony, with a Vaio laptop. Ultra-high end laptop, and I had also paid for and fitted an Intel X25-E, the very first and single-cell ultra high performance SSD. Laptop was 2k euro, drive was another 700 euro. After a year or two, the fan begins to run loud - that's fine, normal event. Laptop is sent back under warranty for fan replacement. Laptop returns with a new motherboard (and as such, the SIM I left in the motherboard was missing). Windows does NOT like having the motherboard changed underneath it, and was royally confused on boot, and no longer worked correctly. I explain this to Support, and that it takes a month for a fresh install of Windows to be fully up to speed. Laptop is sent back again. Support then sends me an invoice : their solution to having fucked up Windows is to remove my 700 euro X25-E drive and charge me 400 euro to install a new standard Sony spinning-platter drive with a fresh install of Windows on it. I gave up with Sony, got my laptop back, reinstalled Windows, and never had anything to do with them again. reply OptionOfT 1 hour agoparentI'm truing to understand here what you were expecting the outcome to be? Your old motherboard back, but fixed? reply nick238 2 hours agoprevTechnically awesome, but policy-wise, sic Tech Jesus from Gamers Nexus (https://www.youtube.com/watch?v=7pMrssIrKcY) and/or Louis Rossmann on HP! reply cricalix 2 hours agoparentLink needs a (2021) I think. Repository says 3 years ago. However, they'd probably still be interested as a pattern of evidence thing. reply heartag 1 hour agorootparentI should have included the year, apologies. reply yelling_cat 2 hours agoprevWhen I was a kid HP products were well-supported and famed for their durability. My dad still uses the HP scientific calculator his father bought in the '70s and my uncle has an old PC hooked up to a '90s LaserJet printer that works perfectly well. Retired engineers I've talked to gush about working there in the HP Way days, they were respected and got to create fantastic products that their customers loved. Now \"HP\" is a curse word to anyone in tech. It's a damn shame. reply heartag 59 minutes agoparentI feel like a lot of companies went that way. I have one of those HP calculators, and it was fantastic. reply Scoundreller 1 hour agoprevThe old school cooked BIOS method (or flash programming of anything if it was the same chip) was to boot a similar and good board to its bios updater, hot swap the bad chip and the good chip, and then execute the updater to write on the bad chip. But life was easier when this stuff was socketed. reply bell-cot 4 hours agoprevSo sad. I still have my (working) HP 11C and HP 15C calculators, from the early 1980's. But HP was still a decent company, back then. reply Rudism 2 hours agoprevIt's too bad the author didn't think to restore the factory RAM before shipping it. My unrelenting cynicism always makes me very careful to hide any evidence of tinkering on the rare occasions when I need to send a tech product off for warranty repairs. reply heartag 56 minutes agoparentThe laptop was purchased with upgraded RAM, so there wasn't any OEM sticks to swap in. That is a good idea if possible though. reply jsiepkes 2 hours agoprev> Under the Magnuson-Moss Warranty Act it is illegal for HP to renege on their warranty unless the reason for warranty work is demonstrably due to the consumer's changes. Aside from what would be morally right to do I doubt HP is legally compelled to repair it. I'm guessing that since this was handled under \"extended warranty\" the rules of your \"minimum legal warranty\" don't apply since those have already expired. So the rules of the agreement of the \"extended warranty\" apply. Which as far as I know is basically just a sort of insurance product with it's own agreement. Guess it's a pretty safe bet the terms contain a clause about not allowing modifications to the product in any form. reply balozi 2 hours agoparentA major vendor did something similar to us about a decade ago. Brand new notebooks bricked after the vendor's tech support directed us to install a \"required\" system update. Vendor claimed that they we're not liable for the damage and pointed to their End User Agreement and Warranty policy. Turns out they had clauses that stated that the vendor was not responsible for damage to devices caused by firmware or BIOS updates. We would later learn that the required system update included a BIOS update responsible for the damage. Its been over a decade since we've even considered a purchase from them again. reply shermantanktop 3 hours agoprevI’ve never had much success once the bricking gets to this point. Nice to see such a detailed write up, though this is probably the exception that proves the rule. reply JoeAltmaier 1 hour agoprevFor a long time HP bought their motherboards as 'a spot-buy on the open market'. Meaning, whatever overseas supplier promised the cheapest price for a given spec. That must have been part of the erosion of the 'HPWay'? reply dirtsoc 2 hours agoprevI got a HP Spectre x360 around 2018 and it was the worst laptop I ever owned. Ever since that experience I steer clear of HP stuff. reply dishsoap 2 hours agoprevThat's very surprising that the image from the firmware update alone is sufficient to boot the machine. On the overwhelming majority of laptops, the update capsules don't contain the NVRAM sections (so no set or default values for any settings) and typically this is enough to prevent the firmware from booting the machine. It's also interesting/impressive how much work the author went through to reinvent the wheel with his own custom microcontroller setup for reading and flashing the EEPROM when there are so many cheap off the shelf solutions available (that can read and program hundreds of times faster than it sounds like his did). reply 42lux 2 hours agoparentThe desoldering did it for me but good for him if it works in the end. It could be done by way less technical people with the proper (cheap) tools though. The article is a bit of a double edged sword because of it. reply heartag 58 minutes agorootparentI agree. The situation was me trying to use materials on-hand, but it makes the writeup less useful. reply croes 2 hours agoprevA fallback BIOS should be standard by now Didn't Gigabyte put that onto their mainboards reply rzzzt 2 hours agoparentYes, around 2009: https://www.techpowerup.com/96967/dualbios-becomes-standard-... reply jonathanlydall 1 hour agorootparentGigabyte has had dual bios motherboards since 1999, but made it standard for all their motherboards in 2009. https://www.gigabyte.com/Press/News/795 As I recall, they initially introduced this feature due to a virus around the time which would intentionally corrupt BIOSes. reply jgrahamc 2 hours agoprevThis is a very cool story. I've messed around with SPI flash stuff both using Pomona clips and also individual per pin clips: https://blog.jgc.org/2023/05/an-unexpected-journey-making-yo... reply heartag 55 minutes agoparentThanks for sharing. I love how detailed that writeup is. And you used the tools that I wasn't smart enough to employ for mine. reply ielillo 3 hours agoprevI think the main lesson is that if you are going to send something for warranty, better to send it as original as possible, otherwise they are going to claim that your modification was the cause of the issue. reply db48x 2 hours agoparentYep. It’s illegal but they’ll keep doing it as long as people don’t take them to court. reply Hizonner 2 hours agorootparentHP is falling behind the state of the art, though. I hear Samsung would have just stolen the thing. reply db48x 2 hours agorootparentThat would save on shipping. reply cmbernard333 3 hours agoprevGreat read. Hopefully others find this useful if they are in a similar situation. HP support is appalling. reply nusl 2 hours agoprevHP is an all-round abomination of a company. reply renewiltord 2 hours agoprevIncredible work. Dual BIOS setups seem like they require very little more PCB space and will cut down on returns. I wonder why manufacturers don't do this. Perhaps the numbers just don't pencil out and it's better to handle the risk of failure. reply widerporst 2 hours agoprevI'm continually amazed by the (lack of) quality of their laptops and the scenario described really reinforced my opinion of HP. At work, I've used a ProBook, a ZBook, and two EliteBooks, all of which had major issues. Sleep mode never worked on any of them (immediately turning back on again with powercfg /lastwake showing no reason), and my current EliteBook frequently shuts off without warning and then won't turn on for five minutes. The ProBook and one EliteBook failed randomly and needed to be replaced. The ZBook's workstation CPU overheated even at light usage, making it unbearably slow. Despite IT saying nothing could (and should) be done, I disassembled it and found it was missing thermal paste, or what little there was had hardened into a brittle, useless mess. Reapplying thermal paste about tripled the Cinebench score. Given all this ridiculousness, I can't imagine how much worse their consumer laptops must be. It's baffling how anyone but the most naive non-tech people still buys from them. reply Tade0 2 hours agoparentMy manager's company-issued HP EliteBook is actually the third device he's had during the last three years, as the previous units failed without warning. Nowadays if I don't see him on the standup I assume it's this problem again. Kind of reminds me of the darkest days of MBPs and their failing keyboards. My work laptop is doing ok so far with only minor annoyances like needing to reconnect peripherals after waking up the device, but that's it. Meanwhile I cooked the screen in my Asus personal machine because it assumed that sleep = 100% CPU. Thermal paste was of course cooked as well, so I had to replace it. All in all I'm glad that Framework expanded into my country recently. It's expensive for what it offers, but half the reason I'll be ordering one is spite. reply jonathantf2 3 hours agoprevHP make nice monitors and that's about it, every other product of theirs is utter trash now - I advise everybody I know to steer clear of any of their stuff. They're on my shit-company list, right at the top with GoDaddy. reply mtsr 3 hours agoparentIt’s a shame, their small form-factor business notebooks were once really good. Swappable batteries, worked well with Linux (apart from the touchpad drivers that were just bad for all brands back then). reply SoftTalker 2 hours agorootparentTheir servers always seemed pretty nice back in the day, though I don't have any experience since the HPE spin-off. reply h3xk1tt3n 2 hours agorootparentWhen I was a tech I hated their servers so much. It was super easy to damage them when taking them apart, and their raid controllers would cause dban to crash. I did liked the Z400 workstation, though, since it could secure erase drives without having to go to sleep to unfreeze the drives. reply jonathantf2 2 hours agorootparentprevHPE stuff is fine imo, networking stuff is decent and the servers still have some quirks from the HP days but other than that are great. reply didip 2 hours agoprevOther lessons one can take from this: * Never buy an HP. * For personal use, one should also treat computers as cattle, not pets. Always automatically backup to at least 2 different locations. reply pugworthy 1 hour agoparentCareful you don't overgeneralize too much. You're probably basing this on home/small office product lines, which is not all HP does. HP has some pretty interesting and successful products such as the Life Sciences dispensers for cells and fluids (see https://www.hp.com/us-en/specialty-printing-solutions/life-s...). Then there are the industrial printing products (https://www.hp.com/us-en/industrial-digital-presses.html) which are rather mind bogglingly fast and capable. Imagine a printer printing to a paper roll 40\" wide at hundreds of feet per second, and the printing is paper statements to be cut and sorted for mailing. reply nolongerthere 3 hours agoprev [–] Impressive that he had the tools to flash the chip but it’s absolutely ridiculous that HP and all companies in general can get away with this sort of nonsense, voiding the warranty should be an easy small claims court appeal. reply user_7832 2 hours agoparent [–] I wonder if there are people who disagree with you, because I can't think of why someone would've flagged your comment. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The document outlines the author's process for reprogramming the BIOS memory chip in an HP Envy 17-ae110nr laptop after a failed BIOS update rendered it non-functional.",
      "The author used tools such as schematics, an STM32 development board, and SPI communication to extract and reflash the BIOS firmware, ultimately restoring the laptop after several attempts.",
      "The document serves as a guide for others facing similar issues and highlights concerns about HP's customer service, especially regarding warranty voids due to third-party components."
    ],
    "commentSummary": [
      "A GitHub user shared their experience recovering a bricked HP laptop, sparking discussions on warranty fraud, HP's product reputation, and BIOS recovery techniques using tools like Bus Pirate and Raspberry Pi.",
      "The conversation highlighted challenges in modern hardware repair, with users sharing similar issues across various brands and frustrations with warranty services, particularly Sony's.",
      "Technical insights on firmware updates, custom hardware solutions, and legal aspects of warranties were discussed, with mixed opinions on HP's reliability and support, emphasizing the importance of data backups and considering computers as replaceable."
    ],
    "points": 108,
    "commentCount": 50,
    "retryCount": 0,
    "time": 1717770785
  }
]
