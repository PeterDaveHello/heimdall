[
  {
    "id": 40267666,
    "title": "Safeguarding Open Source: Defending Against Social Engineering Takeovers",
    "originLink": "https://openssf.org/blog/2024/04/15/open-source-security-openssf-and-openjs-foundations-issue-alert-for-social-engineering-takeovers-of-open-source-projects/",
    "originBody": "XZ Utils cyberattack likely not an isolated incident By Robin Bender Ginn, Executive Director, OpenJS Foundation; and Omkhar Arasaratnam, General Manager, Open Source Security Foundation The recent attempted XZ Utils backdoor (CVE-2024-3094) may not be an isolated incident as evidenced by a similar credible takeover attempt intercepted by the OpenJS Foundation, home to JavaScript projects used by billions of websites worldwide. The Open Source Security (OpenSSF) and OpenJS Foundations are calling all open source maintainers to be alert for social engineering takeover attempts, to recognize the early threat patterns emerging, and to take steps to protect their open source projects. Failed Credible Takeover Attempt The OpenJS Foundation Cross Project Council received a suspicious series of emails with similar messages, bearing different names and overlapping GitHub-associated emails. These emails implored OpenJS to take action to update one of its popular JavaScript projects to “address any critical vulnerabilities,” yet cited no specifics. The email author(s) wanted OpenJS to designate them as a new maintainer of the project despite having little prior involvement. This approach bears strong resemblance to the manner in which “Jia Tan” positioned themselves in the XZ/liblzma backdoor. None of these individuals have been given privileged access to the OpenJS-hosted project. The project has security policies in place, including those outlined by the Foundation’s security working group. The OpenJS team also recognized a similar suspicious pattern in two other popular JavaScript projects not hosted by its Foundation, and immediately flagged the potential security concerns to respective OpenJS leaders, and the Cybersecurity and Infrastructure Security Agency (CISA) within the United States Department of Homeland Security (DHS). Open source projects always welcome contributions from anyone, anywhere, yet granting someone administrative access to the source code as a maintainer requires a higher level of earned trust, and it is not given away as a “quick fix” to any problem. Together with the Linux Foundation, we want to raise awareness of this ongoing threat to all open source maintainers, and offer practical guidance and resources from our broad community of experts in security and open source. Suspicious patterns in social engineering takeovers: Friendly yet aggressive and persistent pursuit of maintainer or their hosted entity (foundation or company) by relatively unknown members of the community. Request to be elevated to maintainer status by new or unknown persons. Endorsement coming from other unknown members of the community who may also be using false identities, also known as “sock puppets.” PRs containing blobs as artifacts. For example, the XZ backdoor was a cleverly crafted file as part of the test suite that wasn’t human readable, as opposed to source code. Intentionally obfuscated or difficult to understand source code. Gradually escalating security issues. For example, the XZ issue started off with a relatively innocuous replacement of safe_fprintf() with fprintf() to see who would notice. Deviation from typical project compile, build, and deployment practices that could allow the insertion of external malicious payloads into blobs, zips, or other binary artifacts. A false sense of urgency, especially if the implied urgency forces a maintainer to reduce the thoroughness of a review or bypass a control. These social engineering attacks are exploiting the sense of duty that maintainers have with their project and community in order to manipulate them. Pay attention to how interactions make you feel. Interactions that create self-doubt, feelings of inadequacy, of not doing enough for the project, etc. might be part of a social engineering attack. Social engineering attacks like the ones we have witnessed with XZ/liblzma were successfully averted by the OpenJS community. These types of attacks are difficult to detect or protect against programmatically as they prey on a violation of trust through social engineering. In the short term, clearly and transparently sharing suspicious activity like those we mentioned above will help other communities stay vigilant. Ensuring our maintainers are well supported is the primary deterrent we have against these social engineering attacks. Steps to help secure your open source project: In addition to these recommendations, there are a number of security best practices that can improve the security properties of our projects. While these recommendations will not thwart a persistent social engineering attack, they may help improve your overall security posture of your project. Consider following industry-standard security best practices such as OpenSSF Guides. Use strong authentication. Enable two-factor authentication (2FA) or Multifactor Authentication (MFA). Use a secure password manager. Preserve your recovery codes in a safe, preferably offline place. Do not reuse credentials/passwords across different services. Have a security policy including a “coordinated disclosure” process for reports. Use best practices for merging new code. Enable branch protections and signed commits. If possible, have a second developer conduct code reviews before merging, even when the PR comes from a maintainer. Enforce readability requirements to ensure new PRs are not obfuscated, and use of opaque binaries is minimized. Limit who has npm publish rights. Know your committers and maintainers, and do a periodic review. Have you seen them in your working group meetings or met them at events, for example? If you run an open source package repository, consider adopting Principles for Package Repository Security. Review “Avoiding social engineering and phishing attacks” from CISA and/or “What is ‘Social Engineering’” from ENISA. Steps for industry and government to help secure critical open source infrastructure: The pressure to sustain a stable and secure open source project creates pressure on maintainers. For example, many projects in the JavaScript ecosystem are maintained by small teams or single developers who are overwhelmed by commercial companies who depend on these community-led projects yet contribute very little back. To solve a problem of this scale, we need vast resources and public/private international coordination. There is already great work underway by the following organizations: Open source foundations: The Linux Foundation family of foundations and other similar organizations like ours can help provide a safety net for open source projects. Maintainers often lack the time, people and expertise in areas such as security. Neutral foundations help support the business, marketing, legal and operations behind hundreds of open source projects that so many rely upon. Our goal is to remove any friction outside of coding to support our maintainers and help their projects grow. As vendor-neutral nonprofits, we are uniquely positioned to offer expertise garnered from multiple stakeholders represented in our organizations. On security, our open source foundations have found that an effective best approach is to provide technical assistance and direct support to open source projects. Alpha-Omega is an associated project of the OpenSSF, funded by Microsoft, Google, and Amazon, funds critical projects and ecosystems. The project aims to build a world where critical open source projects are secure and where security vulnerabilities are found and fixed quickly. The OpenJS Foundation has experienced how funding developers for security has had a proven impact through Alpha-Omega investments in Node.js and jQuery. Sovereign Tech Fund: The Sovereign Tech Fund, financed by the German Federal Ministry for Economic Affairs and Climate Action, is providing the OpenJS Foundation and more open source organizations significant funding to strengthen infrastructure and security. They have built a model with detailed reporting and accountability of resources, yet at the same time, have technical expertise on staff to customize security proposals for the variety of open source projects they fund. It’s encouraging to see the German government taking this initiative to improve the lives of citizens by investing in critical open source infrastructure through the Sovereign Tech Fund. We are advocating for more global public investment in initiatives like the Sovereign Tech Fund to invest in open source global that society depends on, complimentary to private funding. We recommend that public institutions learn from, adapt and coordinate with Germany’s Sovereign Tech Fund to support our interconnected open source projects and shared digital economies. About OpenJS Foundation The OpenJS Foundation is committed to supporting the healthy growth of the JavaScript ecosystem and web technologies by providing a neutral organization to host and sustain projects, as well as collaboratively fund activities for the benefit of the community at large. The OpenJS Foundation is made up of 35 open source JavaScript projects including Appium, Electron, Jest, jQuery, Node.js, and webpack and is supported by corporate and end-user members, including GoDaddy, Google, HeroDevs, IBM, Joyent, Microsoft, and the Sovereign Tech Fund. These members recognize the interconnected nature of the JavaScript ecosystem and the importance of providing a central home for projects which represent significant shared value. About the OpenSSF The Open Source Security Foundation (OpenSSF) is a cross-industry initiative by the Linux Foundation that brings together the industry’s most important open source security initiatives and the individuals and companies that support them. The OpenSSF is committed to collaboration and working both upstream and with existing communities to advance open source security for all. For more information, please visit us at openssf.org. About the Authors Robin Bender Ginn is the Executive Director of the OpenJS Foundation, the neutral home to drive broad adoption and ongoing development of key JavaScript and web technologies. She also serves on the leadership team at the Linux Foundation. Robin has led major initiatives advancing open source technologies, community development, and open standards. Previously, Robin spent more than 10 years at Microsoft where she was at the forefront of the company’s shift to openness. Omkhar Arasaratnam is the General Manager of the Open Source Security Foundation (OpenSSF). He is a veteran cybersecurity and technical risk management executive with more than 25 years of experience leading global organizations. Omkhar began his career as a strong supporter of open source software as a PPC64 maintainer for Gentoo and contributor to the Linux kernel, and that enthusiasm for OSS continues today. Before joining the OpenSSF, he led security and engineering organizations at financial and technology institutions, such as Google, JPMorgan Chase, Credit Suisse, Deutsche Bank, TD Bank Group, and IBM. As a seasoned technology leader, he has revolutionized the effectiveness of secure software engineering, compliance, and cybersecurity controls. He is also an accomplished author and has led contributions to many international standards. Omkhar is also a NYU Cyber Fellow Advisory Council member and a Senior Fellow with the NYU Center for Cybersecurity where he guest lectures Applied Cryptography.",
    "commentLink": "https://news.ycombinator.com/item?id=40267666",
    "commentBody": "Social engineering takeovers of open source projects (openssf.org)636 points by mooreds 14 hours agohidepastfavorite260 comments willvarfar 4 hours agoSo next the attackers playing the long game will just set out to develop the next great everybody-uses-it open-source library, so they control it from inception? Great that we'll finally get state-sponsored open-source development :D reply Nition 57 minutes agoparentA kind of similar thing happened with game key scammers. People will email the devs of hundreds of Steam games pretending to be a popular YouTuber, asking for keys for themselves and usually a few extra \"for a giveaway\". If they get the keys, they'll try to resell them for a profit. At first you'd get emails from like, pewdiepie@outlook.com instead of pewdiepie@gmail.com. But you could usually check the YouTube about page to find the real business email and compare it. So eventually the scammers started creating their own YouTube channels. They'd steal videos from other channels and reupload them, then get bots to add views and subscribers. Now the email matches the one on their channel. One remaining tell tended to be the lack of comments, but it's been a few years since I had a game that was getting those kind of emails, and I wouldn't be surprised if they have good fake video comments these days too. Here are a couple of examples of fake channels I have saved from a few years ago: https://www.youtube.com/channel/UCzOhUFVqJSGk20eB0kFCyOg https://www.youtube.com/channel/UC_TgLJm0paPjmJQTWaHqDhQ reply saagarjha 46 minutes agorootparentRelevant xkcd: https://xkcd.com/810/ reply pants2 4 hours agoparentprevThe ideal state is having the world's superpowers all devoting effort to improving open source libraries, but all catching each others' backdoors and at the end of the day improving security for everyone. reply bee_rider 4 hours agorootparentWeb of trust, but all commits must be signed by at least 3 intelligence agencies from rival countries. reply wiseowise 2 hours agorootparentRussia, China, Iran and NK cock-block development for years, because the MR “doesn’t represent their interests”. reply saagarjha 48 minutes agorootparentSounds a lot like how web standards work! reply transpute 3 hours agorootparentprevAt least 2 rival legal Jurisdictions/Alliances/Spheres At least 2 rival state intelligence Agencies per Sphere At least 2 rival corporations per Sphere TOTAL: 2*(2+2) = 8 Widely used OSS projects are contested spheres of collaboration. reply voiper1 3 hours agorootparentSo the long game is rival countries are secretly collaborating, so we secretly have world peace! reply glodalitazic 3 hours agorootparentImagine all war stops and noone knows it reply jonhohle 4 hours agorootparentprev“Web of distrust” reply Angostura 2 hours agorootparentMutually Assured Commitment reply goodpoint 2 hours agorootparentprevThis has been quietly happening for a good while... reply Contortion 1 hour agorootparentprevMutually Assured Backdoors reply lynx23 1 hour agorootparentprevSounds like asking for World Peace. reply loa_in_ 2 hours agoparentprevIf it's an everybody uses it solution it will eventually be reimplemented as a part of the environment, like the browser or the kernel. The lifetime is limited and linking to a library like that wouldn't mesh very well there. reply transpute 4 hours agoparentprevUS gov has encouraged finance/tech industry to invest in the security of OSS supply chains. OpenSSF members: https://openssf.org/about/members 2021, $10MM, https://openssf.org/press-release/2021/10/13/open-source-sec... > Financial commitments from Premier members include Amazon, Cisco, Dell Technologies, Ericsson, Facebook, Fidelity, GitHub, Google, IBM, Intel, JPMorgan Chase, Microsoft, Morgan Stanley, Oracle, Red Hat, Snyk, and VMware. Additional commitments come from General members Aiven, Anchore, Apiiro, AuriStor, Codethink, Cybertrust Japan, Deepfence, Devgistics, DTCC, GitLab, Goldman Sachs, JFrog, Nutanix, StackHawk, Tencent, TideLift, and Wind River. 2022, $5MM for 10,000 OSS projects, https://openssf.org/press-release/2022/02/01/openssf-announc... > Following a meeting with government and industry leaders at the White House, OpenSSF is excited to announce the Alpha-Omega Project to improve the security posture of open source software (OSS) through direct engagement of software security experts and automated security testing. Microsoft and Google are supporting the Alpha-Omega Project with an initial investment of $5 million.. “Omega” will identify at least 10,000 widely deployed OSS projects where it can apply automated security analysis, scoring, and remediation guidance to their open source maintainer communities. 2022+2023, $4.8MM disbursed to ten (not 10K?) OSS projects, https://openssf.org/blog/2024/02/16/alpha-omega-2023-annual-... & https://openssf.org/blog/2022/12/14/alpha-omega-project-firs... Eclipse $1,150,000 NodeJS $579,000 Rust $920,000 Homebrew $175,000 jQuery $350,000 OpenSSL $127,968 OpenRefactory $50,000 Prossimo (ISRG) $530,000 Python $400,000 Linux Kernel $620,000 reply throwaway2037 3 hours agorootparentWhy did Eclipse org get so much and Apache org none/less? In my experience, the footprint of Apache exceeds anything else in enterprise programming. reply VoidWhisperer 2 hours agorootparentThis also struck me as a bit odd.. even more so when you consider that over recent years, eclipse's general usage over time has decreased reply transpute 1 hour agorootparentAre OpenSFF members using Eclipse sub-projects in the financial services industry? In automotive/embedded, Eclipse hosts the safety-certified OSS ThreadX RTOS (formerly Azure RTOS), which runs on 10B+ devices, https://finance.yahoo.com/news/eclipse-foundation-showcases-... reply gkbrk 1 hour agorootparentprevTons of modern and really critical development happens on Eclipse-based environments. Two examples I can think of off the top of my head are - DBeaver (very widely used to connect to production databases) - STM32Cube IDE (for embedded development in all sorts of devices) reply _fizz_buzz_ 20 minutes agorootparentTI's Code Composer Studio is also eclipse based. reply saagarjha 47 minutes agorootparentprevNSA's got to keep developing Ghidra reply cess11 57 minutes agorootparentprevEclipse manages a distribution of Java and the Jakarta libraries, formerly known as JavaEE/J2EE. Arguably Jakarta is a larger footprint, since pretty much every enterprise-like library or application derives functionality from it. reply winrid 4 hours agorootparentprevNeat that jQuery gets so much. I guess they have a ton of stuff on jQuery still (and probably will forever). reply varispeed 1 hour agoparentprevRelevant: https://www.youtube.com/watch?v=jgYYOUC10aM reply Ygg2 2 hours agoparentprev> So next the attackers playing the long game will just set out to develop the next great everybody-uses-it open-source library This already happened. *cough* React *cough* Ask yourself, why would rogue AI and famous human impersonator Mark (short for Mark Zero Ai) Zuckerberg make an open-source UI library for everyone to use? /tinfoil That said, who knows, maybe it already happened. reply chipdart 4 hours agoparentprevnext [9 more] [flagged] austinjp 1 hour agorootparentIt's a multi-layered satirical joke. The 'absurd' aspect is the flipping of 'taking over a library' versus starting one, which is amusing given that the motivations of state-sponsored bad actors are to poison and/or control the hard work of others. They themselves would end up doing the hard work they've undermined. Further satirical layers are added by the prospect of those bad actors producing 'good' software instead of 'evil', and providing funding that they've historically been reluctant to cough up. Of course, as the old saying goes, explaining a joke is like dissecting a frog: nobody laughs and a frog dies. reply yesseri 3 hours agorootparentprevI think he was making a tounge in cheek point about how we could finally get extremely well-founded open source projects. reply chipdart 3 hours agorootparentI did not referred to the funding remark. It is besides the point and immaterial to the discussion. My point was on the remark that this attack vector is somehow only applicable when projects are starting out. This is false, and insinuating this does a disservice to the community. The attack consists of asking someone for the keys. The projects that are the most vulnerable are those who are already established and have a significant adoption rate but are not actively maintained. We are talking about Colors-like and Faker-like projects. All you need to pull this off is posting one message asking nicely for permissions, post a commit, and make a release. https://fossa.com/blog/npm-packages-colors-faker-corrupted/ reply soks86 3 hours agorootparentThe comment was not sincere and is meant to amuse. If you believe amusement is a disservice to the community then I believe many others disagree. edit: For clarity, the \"since inception\" part is an absurd setup for the, equally absurd, \"well funded open source\" part. reply Teknomancer 2 hours agorootparentTaking over Open Source, just for the LULz. There is an extreme lack of sense of humor around these parts. I lol'd, and thank you for that. reply solumunus 43 minutes agorootparentprevDouble whoosh then I guess. reply willvarfar 3 hours agorootparentprevI edited to clarify that I was meaning future tense. reply z3t4 3 hours agorootparentprevFor example NPM will give ownership to a package/module if you just ask support. reply bsuvc 11 hours agoprevI'm a maintainer (one of many) of an open source project, and this topic has been on my mind a lot lately as I review PRs. I am more suspicious of PRs from new contributors by default now. Of course I keep these suspicions to myself, but besides simply reviewing code for all the regular things, I now ask myself \"what sort of sneaky thing could they be doing that appears benign on the surface?\" reply andix 11 hours agoparentThat's great that you are considering this more now. But the xy story taught us, that every contributor is dangerous, the most dangerous ones are probably the most helpful and most skilled contributors. If someone barely get's a PR accepted, they probably lack the skills to add a sophisticated backdoor. Another thing that was not talked about a lot: There are many ways to compromise existing maintainers. Compromising people is the core competency of intelligence, happens all the time, and most cases probably never come to public knowledge. reply victorbjorklund 10 hours agorootparent> Compromising people is the core competency of intelligence, happens all the time, and most cases probably never come to public knowledge. Yea. It would almost be strange if security service didnt consider the route of getting \"kompromat\" on a developer to make them \"help\" them. reply nyokodo 8 hours agorootparent> consider the route of getting \"kompromat\" on a developer to make them \"help\" them I suppose that’s an option, but it also introduces an additional risk of exposure for your operation as it doesn’t always work and makes it much more complicated to manage even when it does work. reply emodendroket 8 hours agorootparentDoes it matter though? They don’t have to say “I am so and so of the Egyptian intelligence service and would like to blackmail you” reply andix 8 hours agorootparentThey might not even use blackmail, they might just \"help out\" in a difficult financial situation. Some people are in severe debt, have a gambling problem, are addicted to expensive drugs, or might need a lot of money for a sick relative. There are many possibilities. The trick is finding the people that can be compromised. reply somenameforme 5 hours agorootparentI think you're going overboard on what's required. Take anybody who is simultaneously offered a substantial monetary incentive (let's say 4 years of total current/vesting comp), and also threatened with the release of something that we'll say is little more than moderately embarrassing. And this dev is being asked to do something that stands basically 0 risks of consequences/exposure for himself due to plausible deniability. For instance, this is the heartbleed bug: \"memcpy(bp, pl, payload);\". You're copying (horrible naming conventions) payload bytes from pl to bp, without ensuring that the size of pl is >= payload, so an attacker can trivially get random bytes from memory. Somehow nobody caught one of the most blatant overflow vulnerabilities, even though memcpy calls are likely one of the very first places you'd check for this exact issue. Many people think it was intentional because of this, but obviously there's zero evidence, because it's basically impossible for evidence for this to exist. And so accordingly there were also 0 direct consequences, besides being in the spotlight for a few minutes and have a bunch of people ask him how it felt to be responsible for such a huge exploit. \"It was a simple programming mistake\" ad infinitum. So, in this context - who's going to say no? If any group, criminal or national, wanted to corrupt people - I really don't think it'd be hard at all. Mixing the carrot and the stick really changes the dynamics vs a basic blackmail thing where it's exclusively a personal loss (and with no guarantee that the criminal won't come back in 3 months to do it again). To me, the fact we've basically never had anybody come forward claiming they were a victim of such an effort means that no agency (or criminal organization) anywhere has ever tried this, or that it works essentially 100% of the time. reply saagarjha 45 minutes agorootparentThis doesn't look intentional at all, because this is basically like how 90% of memory disclosure bugs look reply somenameforme 12 minutes agorootparentAbsolutely. And that's the point I'm making here. It is essentially impossible to discern between an exploit injected due to malice, and one injected due to incompetence. It reminds one of the CIA's 'simple sabotage field manual' in this regard. [1] Many of the suggestions look basically like a synopses of Dilbert sketches, written about 50 years before Dilbert, because they all happen, completely naturally, at essentially any organization. The manual itself even refers to its suggestions as \"purposeful stupidity.\" You're basically exploiting Hanlon's Razor. [1] - https://www.openculture.com/2015/12/simple-sabotage-field-ma... nyokodo 7 hours agorootparentprev> They might not even use blackmail, they might just \"help out\" If the target knows or suspects what you’re asking them to do is nefarious then you still run the same risks that they talk before your operation is complete. It’s still far less risky to avoid tipping anyone else off and just slip a trusted asset into a project. reply nyokodo 7 hours agorootparentprev> “I am so and so of the Egyptian intelligence service and would like to blackmail you” No, but practically by definition the target has to know they’re being forced to “help” and therefore know someone is targeting the project. Some percentage of the time the target comes clean about whatever compromising information was gathered about them, which then potentially alerts the project to the fact they’re being targeted. When it does work you have to keep their mouth shut long enough for your operation to succeed which might mean they have an unfortunate accident, which introduces more risks, or you have to monitor them for the duration which ties up resources. It’s way simpler just to insert a trusted asset into a project. reply emodendroket 7 hours agorootparentI would guess there are many projects they could target at any given time. reply mewpmewp2 6 hours agorootparentThe more projects they target the more risk of being flagged and preventive measures to be engaged by counter intelligence etc. reply andix 9 hours agorootparentprevThey would be really bad at their job, if they didn't try. reply MichaelZuo 9 hours agorootparentprevThe most secure systems are those that are also resistant to rubber hose cryptography. reply nbk_2000 8 hours agorootparent\"Rubber Hose Cryptography\" comes in the form of a PR. \"Rubber Hose Cryptanalysis\" comes in the back door and waits for you in the dark. reply MichaelZuo 7 hours agorootparentNo, it 'comes in the form of' a rubber hose... reply mavelikara 2 hours agorootparentprev> Another thing that was not talked about a lot: There are many ways to compromise existing maintainers. Also not talked about a lot - there are many ways to compromise existing software engineers who are paid to work on proprietary software systems. reply andix 10 hours agorootparentprevOne follow up to compromising existing maintainers: This makes the creators or long-term good faith maintainers maybe even more \"dangerous\" than new maintainers. reply WanderPanda 9 hours agorootparentAre we facing a Byzantine generals kind of situation now? reply Mtinie 7 hours agorootparentWe have always faced it, it’s just that there's more awareness of the potential issues. reply xpe 5 hours agorootparentprevWho can share a threat model with specific probability estimates on this? FWIW, I’m less interested in the particular estimates (priors) and more interested in the structure. reply lupusreal 1 hour agorootparentprev> If someone barely get's a PR accepted, they probably lack the skills to add a sophisticated backdoor. That's true, but it's also true that a sophisticated and well formed PR is probably genuine too. Hostile PRs are the exception rather than the rule. And if only the high quality PRs are treated with suspicion, then the attackers will tailor their approach to mimic novices. General vigilance is required, but failure is likely because these attacks are so rare that maintainers will grow weary of being paranoid about a threat they've never seen in years of suspicion and let their guard down. reply tschwimmer 10 hours agorootparentprev>If someone barely get's a PR accepted, they probably lack the skills to add a sophisticated backdoor. Unforuntately it's easy to sandbag being dumb. Just because someone submits a PR defining constants for 0-999 does not mean they're actually bad at programming. reply vundercind 6 hours agorootparentHow incredibly wasteful. You can form most useful numbers with just ten singe-digit constants, some casting, and string concatenation. reply andix 10 hours agorootparentprevSure, but being known for submitting bad code is going to make code reviews more thorough, not less. It's drawing additional attention to yourself. reply MenhirMike 7 hours agorootparentprev> defining constants for 0-999 That person might just be an old school Javathree JS projects were targeted in failed attempts. Suspected to be targetted, in a way that seems to have 0% chance of succeeding for almost any project. Which is why nothing happened. reply josephg 6 minutes agorootparent> seems to have 0% chance of succeeding for almost any project. Its obviously more than that given xz was successfully taken over and backdoored. Even a 5% chance of malicious takeover per project would make the situation pretty worrying given how many well funded, motivated government agencies are out there. Mtinie 7 hours agorootparentprevAre people really looking though? Are all open source libraries being run through extensive performance profiling to look for known heuristics? Are they being looked at line by line for aberrations? I don’t have confidence that people are looking for evidence of potential exploitation because of reasons like the ones you bring up. So we’re back to we just don’t know. reply BobbyTables2 6 hours agorootparentIt’s worse than that, and that wouldn’t be enough. A large class of exploitation methods simply have no performance impact. reply arp242 7 hours agorootparentprevMost commonly-used projects are watched by a bunch of people, or diffed on updates. These are not in-depth reviews, but should catch most of it. So yes, people are looking, and have been looking for a long time. The reason Jia Tan could do their thing is because 1) the main meat was in a binary test file, 2) the code to use that seemed relatively harmless at a glance, and 3) people were encouraged to use the .tar.gz files instead of git clone. Also you need to actual get maintainer status, which is not as easy as it sounds. I've been thinking of inserting a \"// THIS LINE IS MALICIOUS, PLEASE REPORT IF YOU SEE IT\" in some of my projects to see how long it would take. I bet it would be pretty fast either after commit or after tagging a release. reply caf 7 hours agorootparentMaybe // this line is an external audit test - a free gift card to the first person to report finding it. reply kjok 7 hours agorootparentprev> I've been thinking of inserting a \"// THIS LINE IS MALICIOUS, PLEASE REPORT IF YOU SEE IT\" in some of my projects to see how long it would take. Tools that use LLMs to review code will catch such projects. reply caf 7 hours agorootparentprevWith hindsight it's not the runtime behaviour of the library that you'd want to test - the weakest point in the chain is where the distributed source .tar.gz can't be regenerated from the project repository. reply josephg 11 minutes agorootparentFor how many projects is that actually checked? I bet barely any. Its especially difficult because most projects aren't built in a reproducible way. You should be able to uncompress and compare a source tarball. But if you get a binary and the source code used to generated that binary, there's no way to tell that they match. Aerbil313 6 hours agorootparentprevNo. If there is strong incentive to compromise, and little to no chance a compromise is being found, it's statistically most likely to assume compromises happen on a regular basis and only rarely are found out. reply andix 8 hours agorootparentprevWe know about the failed attempts, we have no idea about the successful ones, and the ones that are going to be successful in the future. reply arp242 7 hours agorootparentYou can always use this line because you can never prove something doesn't exist. Go find evidence. It's been over a month. reply xpe 5 hours agorootparentYour choice of language in your comments (in this thread, not in general) isn’t bolstering your argument. Why not be curious rather than just dismissive? This seems to be people just talking past each other at this point. There have been a lot of changes in the last ~five years that point in the direction of supply chain security being at greater risk. Evidence comes in many forms. The relevance of evidence depends on what part of the problem you are looking at. Also, it is rational to talk about the probability by which different evidence is likely to be surfaced! I think it is possible you are sensitive to people making such claims for self-interested purposes. Fair? But I don’t think it’s fair to assume that of commenters here. reply devjab 4 hours agorootparentprevYou have evidence of a state-sponsored attack which was only discovered because we got extremely lucky, and you’re not worried? The attack itself is the frankly evidence. It’s sort of like how we expect there to be life on other planets because there is life on earth. reply chipdart 3 hours agorootparentprev> So we've had what, two incidents (xz and eventstream) in how many years? This is specious reasoning. You're only complaining you only heard of two incidents. What you're really pointing out is that this attack vector works reliably well and is reproducible across projects. You're also pointing out that this attack vector will continue to work until something is done to mitigate it. I really do not understand what point you think you are making. reply Invictus0 8 hours agorootparentprevYou're really going to pretend like there have been no socially-engineered cybersecurity attacks in the last 30 years...? And by the way, stabbings happen all the time, at least 3 per day. Stabbings hurt a few people, cybersecurity incidents can hurt millions. reply arp242 7 hours agorootparentThis is about \"social engineering takeovers of open source projects\", not \"socially-engineered cybersecurity attack\", which is much much broader. I've been pretty clued up on open source for the last 20 years, and I don't really recall any other similar incidents other than the two I mentioned. I tried to find other examples a few weeks ago and came up empty-handed. It's certainly not common. So please do post specifics if you know of additional incidents, because from what I can see, it's exceedingly rare. reply xp84 6 hours agorootparentYou seem super confident that there have been zero similar attacks that achieved their goals without detection. By definition, almost anyone who pulled off this kind of thing would try really hard not to burn that backdoor by being super obvious (for instance, using it to deface a website). We literally would not know anything about it, in all likelihood. Therefore I feel like it’s a lot more intellectually honest to say we have no idea if that has happened elsewhere, than it is to confidently proclaim that it certainly has not just because it’s been a month since xz. reply arp242 6 hours agorootparentWhat I'm argueing against is absolutist fear-mongering statements such as \"every contributor is dangerous\". I'm not confident about anything, but anything could happen or have happened all the time. We need to operate on the reality that exists, not the reality that perhaps maybe possibly could perhaps maybe possibly exist. And we certainly shouldn't be treating anyone sending you a patch as a dangerous hostile actors by default. reply transpute 7 hours agorootparentprevThere are CVEs where an empty string performed an authentication bypass. > social engineering The best bugdoors are deniable. reply Der_Einzige 9 hours agorootparentprevnext [4 more] [flagged] jrockway 9 hours agorootparentI don't think the \"armed to the teeth\" theory is correct. If you were right, people wouldn't honk at each other or otherwise involve themselves in any sort of road rage. But people rage at each other all the time, and only very rarely does someone get shot. The reason people aren't walking around stabbing you in the eye with a needle is because there is no reason for them to do that. They gain nothing. They don't desire that it be done. reply emodendroket 8 hours agorootparentprevIf the news articles about Instagram extortion are anything to go by, adding weapons to an extortion situation is more likely to lead to a suicide than the extortionist being dissuaded. reply nineteen999 9 hours agorootparentprevFlipside of that being a highest number of school shootings in the world. reply andy99 11 hours agoparentprevIt's not the new contributors you have to watch, it's the sleeper contributor who has built up a solid reputation and then is \"activated\". At least that's how I understand XZ. reply Uehreka 11 hours agorootparentIt’s both. The fact that one happened recently does not preclude the other. reply runjake 11 hours agoparentprevLooking at some of these cases, each PR on their own doesn’t look suspicious, but it was what they all built up to — in some cases from multiple bad actor contributors that, on the surface, weren’t connected. reply WanderPanda 9 hours agoparentprevWasn't a key thing of the xz attack vector that people where encouraged to download the custom source release instead of the autogenerated Github one? I don't know if that is a pattern but it seems like best practices in the (source) supply-chain could prevent a large class of these attacks. reply leeoniya 7 hours agorootparentyep. same with npm. i publish releases of my OSS libs to npm, but there's no guarantee that what is uploaded is what you see on github. that's a lot of trust you have to put into my opsec, etc. not good. reply supriyo-biswas 4 hours agorootparentprevThat is unfortunately how `the `autotools` ecosystem works; although I guess projects could guide their users to run `autoreconf -i` if working with the source code instead of the release tarballs before doing the usual `./configure && make && make install` step. reply euroderf 4 hours agoparentprevWhy is there not a policy that any PR can be rewritten by a maintainer ? Wherever the PR looks a bit odd, rewrite it so do the same thing a different way. Enough unpredictable change to disrupt finely-tuned subterfuge. reply vincnetas 3 hours agorootparentyou can wait for tree (or x) PR's passing specified unit tests for functionality and then merge a random one. But this is a luxury (effort wise) for any kind of project. reply out-of-ideas 8 hours agoparentprevthe attitude remindes me of maintaining game-servers and looking out for cheaters; once we had a handful of folks looking out for cheaters, it turned the community against itself calling everybody a cheater... i think it is good to be cautious; but overall it's the same cat and mouse game we've seen before. i can only say good luck on not letting it stress you out second guessing other folks actions and intent - and hope we continue writing code for humans to read vs the cryptic, obstrufcated, even \"elegant\" code (not to dive into the skill issue rabbit hole lol) reply SlightlyLeftPad 7 hours agoparentprevI’m in it for a free t-shirt. reply arccy 33 minutes agorootparenthttps://medium.com/pentesternepal/hacking-dutch-government-f... reply Beefin 6 hours agoparentprevwhat do you look for to prevent this? reply rhim 4 hours agoprevI recently had a xz moment where the rust zip crate was taken over by a single person and the original crate was completely replaced. I'm still not sure if this was legit or not: https://github.com/zip-rs/zip-old/issues/446 reply KolmogorovComp 2 hours agoparentThis is not a ‘xz moment’, as a sibling comment said, it is norm in open-source. Someone with more time forked the repo, included the changes that were necessary, build up trust and then this eventually get merged. Now obviously there is no guarantee they will never act up in the future, but this is not different than for the original owner. Trust is a necessity to open-source reliably functionning, because it in parts makes up for the lack of money, and allow to move fast. XZ is the exception. And frankly there is not much to do against it. reply saghm 2 hours agoparentprevHonestly, from reading this it seems like people blew it _way_ out of proportion. Someone forked the project to make updates because the original maintainer seemed to be not doing much, the original maintainer came back to say that they were correct to ask about maintenance because they didn't expect to do any more work due to health issues and then volunteered to transfer the crate to the person who forked on their own, which the person who forked it accepted. It's kind of bizarre to me because I don't really understand what mental model could lead to not taking any action earlier than this if the was things turned out is so upsetting. If they were happy to keep using it exactly as is because no updates were needed, why not just pin the dependency to that version exactly (and republish their own fork if they were worried about old versions being \"yanked\" and not being able to use it for anything new or offline)? If they did expect some form of updates over time, where did they expect them to come from when the existing maintainer felt they were unable to continue given health issues? Any attempt to find some other solution to future ownership would be heavily scrutinized by the exact people who commented on this issue with strong opinions and don't seem to have much empathy for the health issues, which would defeat the entire purpose of trying to take time away from work for their health. I'm surprised that this needs to be said, expecting people to put in extra work to project you from what happens to their projects when they literally can't keep maintaining them due to health issues will never work, and it's also just an awful way to treat people. If you have serious concerns about how situations like this could be exploited by malicious actors, you should be paying much closer attention to the status of your dependencies and taking actions to insulate yourself from potential fallout long before some like this happens. If you've gotten to this point under the assumption that you can just veto any change in ownership that you don't trust, you're already too late. reply posed 2 hours agoparentprevThat does look suspicious. reply greatgib 7 hours agoprevFirst I think that it is wrong to single out this issue on Open Source projects. For example, since the first versions of app stores, when you are an app developer you would receive a lot of messages from random shady dudes ready to buy your application if it had a few users. Also, the xz thing was kind of pretty smart, but it is also a thing in mind of most OSs developers that you can't trust any random contributor and so that you will be very careful of really knowing someone before giving some privilege. Just look how the debian policies were designed or that things like \"pull requests\" were invented for Open Source projects where everyone in a project used to be allowed to push whatever in private ones before. reply lazyasciiart 5 hours agoparentChrome extensions have also been subject to those purchase offers for a long time. reply williamdclt 1 hour agoparentprevThe scope of impact if a mobile app becomes malicious is _immensely_ smaller than if xz becomes malicious. The latter seems to be national security level reply dvh 13 hours agoprevWe've been adding features for 30+ years to open source software, they become so complex only very few people understand them anymore. Recently I looked into jfet level 2 implementation in ngspice, expecting familiar equations, but through series of small changes and maybe some DRY too, the code is almost unrecognizable. When graybeards finally retire, there will be lots of shrugs. reply zer00eyz 12 hours agoparentYour average American could quit working for 5 million dollars. They could live comfortably for the rest of their lives off that money, if well invested (read EFT for sp500) 5 million bucks is change for you average government. \"Amazon made billions on my project and if I turn a blind eye to this I can retire, fuck them...\" Sponsorship, for good or bad makes a lot of decisions simple. reply NonDairyMatt 6 hours agorootparentProbably wouldn't even need 5 Million. According the the PBS article about the 2 US Navy sailors arrested for spying around august of last year one of them was apparently only bribed $10k-15K for the year [1]. I was pretty shocked at first but it made sense that with the financial stress many face in today's market a 10K bribe would go along way and have a high return on investment especially if the potential payoff for a backdoor or zero-day is in the millions [2] [1] https://www.pbs.org/newshour/politics/2-u-s-navy-sailors-arr... [2] https://en.wikipedia.org/wiki/Market_for_zero-day_exploits reply shiroiushi 1 hour agorootparent>2 US Navy sailors arrested for spying around august of last year one of them was apparently only bribed $10k-15K for the year This is exactly why investigations for security clearances focus mostly on a person's financial situation: someone who has a lot of debt and shows a pattern of poor financial management, i.e. someone who'd jump at a chance to make an extra measly $10k, is the kind of person they want to avoid giving a high security clearance to, because of incidents exactly like this. It's a common misconception that clearance holders who sell secrets make a lot of money doing so, and this just isn't the case: it's comparatively small amounts like this. For someone who's deep in debt and desperate, it doesn't take much to buy them. reply BobbyTables2 6 hours agorootparentprevEven politicians are cheap these days. It astonishes me that for less than the price of a nice car, what people seem to be able to do. reply eesmith 2 hours agorootparentAfter Abscam in the late 1970s/early 1980s, one of the quote going expressed surprise not that politicians could be bought, but that they were so cheap. I cannot find that quote now. reply kbenson 2 hours agorootparentprevMoney isn't the motivator I worry about. Money requires people to make rational decisions about what they think they can get away with. Threatening someone's family? Who wouldn't be willing to turn their access to a project into a hack that has some possible theoretical future harm to protect their child? Sure, the circumstances to being able to leverage someone like this are rare, but the population which is susceptible is much larger than those willing to do somethibg similar for just money. reply imachine1980_ 11 hours agorootparentprevnext [2 more] [flagged] SgtBastard 11 hours agorootparentFriend, I read the comment that you’re replying to that if hostile governments wanted to backdoor our software supply-chains, it wouldn’t cost that much to corrupt an open source maintainer. Whereas, if it was the norm for well used OS projects to be sponsored, it’s far less likely to be tempted by relatively small bribes. reply armini 5 hours agoparentprevI've been working on thanks.dev for over two years now & reading this report is disappointing to say the least. Why not spend the time to explain the value XZ Utils created for all the commercial users & what companies can do to better supporting maintainers with hundreds of issues experiencing burnout from their unpaid work? OpenSSF should instead promote FOSS programs like https://frontendmasters.com/blog/how-were-supporting-open-so... & how they help their open source community stay active. Working in open source is a social contract & corporates need to be better citizens if they want to reduce their risk profile. reply bruce511 4 hours agorootparentOn first reading your comment makes a lot of sense, and is certainly logical for maximizing the common good. But unfortunately, companies simply don't work the way you are proposing. The short reason is this \"good citizenship is indistinguishable from corruption. Therefore good company governance leans away from both.\" The somewhat longer answer is that while a \"company\" might have a lot of money, or might make a lot of money leveraging some common good, it is not (usually) one person's money. The bigger the company the harder it gets to actually -spend- the money. There are procurement departments, various sign-offs and so on. First and foremost it helps if there is a tangible (defendable) reason to spend the money. Yes, companies \"give\" money away. Usually under the guise of marketing. It's easy to donate money to the local cancer center. It's harder to explain the marketing value of supporting random open source projects. For tech companies it's -somewhat- easier, but even then it's simpler to donate time rather than money. I've said it a lot lately, but OSS development has to \"commercialize\" if it wants to be commercial. That means first understanding \"what companies pay for\" and designing products to fit that. Or target individuals with excess cash of their own that they're willing to just \"pass along\". reply int_19h 1 hour agorootparent\"What companies pay for\" is anything they cannot get for free. If the value of an OSS project is mostly in its code, then any license that allows it to be used commercially will mean lots of free-riding. reply chiph 13 hours agoprevAnyone who has played Eve Online is familiar with this process. Gain membership, become a valued contributor to the corp, then betray it for profit. reply glenstein 12 hours agoparentAnd one difficulty here I believe is that those intent on social engineering think about it in more sophisticated terms than their targets, which perhaps is obvious. And part of the process can be a kind of performative incredulity at the very suggestion that they are part of a campaign of hostile takeover, even if it's exactly accurate. I suppose you could even have unfortunate circumstances where parts of an open source community are unwitting advocates of being co-opted. And I think you probably see a parallel in state-based information warfare, where part of the objective isn't just to spread misinformation, but to shift cultural norms so that the transmission of misinformation is inherently easier, which can involve sewing distrust in institutions or expertise, or normalizing a gish gallop argumentative style. I'm perhaps stating the obvious here, but I suppose the upshot is that human psychology can be targeted in a programmatic way, and there might need to be something in the way of a normalized infosec-oriented doctrine relating to the stewardship of open source programs as an intentional countermeasure. reply zer00eyz 12 hours agorootparent>> And I think you probably see a parallel in state-based information warfare, where part of the objective isn't just to spread misinformation, but to shift cultural norms so that the transmission of misinformation is inherently easier, which can involve sewing distrust in institutions or expertise, or normalizing a gish gallop argumentative style. TikTok springs to mind when reading this... reply Aerroon 10 hours agorootparentMy thought immediately went to Linus Torvalds. The way he acted was tolerated in the past, but the culture was changed and it was used to force a change onto the project. Same thing with all of those Codes of Conduct that suddenly propped up. reply Thorrez 6 hours agorootparentAre you saying codes of conduct make the transmission of misinformation is inherently easier, e.g by sewing distrust in institutions or expertise, or normalizing a gish gallop argumentative style? Are you saying Linus Torvald's behaviour prevented those problems? reply int_19h 1 hour agorootparentIt has not occurred to me before, but I don't see why the cancel culture surrounding such matters couldn't be used as an attack vector. Basically, target key maintainers who are vulnerable to this (white, male, history of questionable interactions etc) and push until you force them out one way or another. Then when project gets in trouble because of the lack of qualified manpower, pitch your own agent as replacement. For bonus points, make it someone who hits the right buttons wrt \"diversity\". reply johnnyanmac 5 hours agorootparentprevI think it's more that it leads to brain drain. Be it by misinformation, by discouraging anyone who doesn't fit [demographic of leadership], or by simply not giving proper code reviews in the name of politeness. All 3 ways creep up and the code base suffers from lack of care, and/or lack of talent. We can certainly debate how effective old methods were (and yes, I have no doubt Torvald's old behavior turned off many a talent) and how we can improve on them, but in a more general viewpoint we need to remember that many open source code bases are, or started as, volunteers providing their knowledge in their free time. It doesn't take much to make them walk to the next repo. Or not contribute to OS at all. reply dgfitz 11 hours agorootparentprevNot going to quote the whole thing, but yes hard agree. Contrasting this factual opinion with the opinions in the “sell TikTok” hn threads is quite a delta. reply Der_Einzige 9 hours agorootparentprevRe: normalization of gish gallop The speed reading shit they do in competitive debate was in my opinion 100% caused by clandestine elements who wanted to keep the future “revolutionary” intelligentsia class obsessed with ivory tower elitism so that they don’t get too close to doing actually subversive things. I have no other explanation for how otherwise smart people think that speed reading lacanian psychoanalysis is high school is valuable for anything. https://en.m.wikipedia.org/wiki/Spreading_(debate) reply stuartjohnson12 8 hours agorootparentIn Europe, the most popular high school and university debate format is British Parliamentary in which spreading is not popular because you only have 15 minutes to prepare and weakly justified arguments don't require responses. https://youtu.be/XyIK_Cg_8jc?t=327 British culture certainly has plenty of ivory tower elitism, yet has passed by this. I don't think it's a special revolutionary pedagogy, just a different interpretation of how to deal with subjectivity in debate. reply glenstein 7 hours agorootparentprevI agree with you that it's incredibly bizarre, but I associate it with the strange cultural norms that can only crop up in very specific academic environments that have just the right alchemy of academic strangeness, competitiveness, and idiosyncratic historical origin. I'll compare it to something I recently discovered, which is some viral video I recently saw of some sort of pig fare where kids lead pigs out on this walk to show how well the pigs are trained and I think to show off the pigs as models specimens, and the kids do this intentional intense eye contact with judges in order to get the judges to look at them. It seems so strange and abnormal, but it was explained away as just something that's part of the history of the competition and having strategic value for being effective in the competition. I've seen videos of the college debates you're speaking of though, and I've definitely felt that they're badly in need of reforms that either impose a word count or otherwise disincentivize speed reading. The long and short of that is just to say you can explain it without regarding it as some sort of intentional state disinformation program. I would also say I find that especially implausible just because, while I don't love the practice, I don't think it degrades our ability to follow arguments or have information literacy necessarily, and meanwhile modern social media absolutely does seem to instill habits that reinforce short-term attention spans, disjointed thinking, object permanence problems and the like, all of which would dispose people to be more receptive to bite-size arguments that don't have to fit into a comprehensive or coherent worldview. reply omoikane 4 hours agorootparentprev> transmission of misinformation is inherently easier This may already be the case, according to \"Study: On Twitter, false news travels faster than true stories\" (2018): https://mitsloan.mit.edu/ideas-made-to-matter/study-false-ne... reply zmgsabst 6 hours agorootparentprev> And I think you probably see a parallel in state-based information warfare My own research shows it’s the opposite: they destroy trust in institutions and experts by telling the truth when those groups lie to their own citizens. The US did this routinely during the Cold War. More recent examples include: - demographic facts about murder, violence, and police - demographic facts about college enrollment, eg the racism at Harvard - George Floyd’s autopsy report - images of cities burning; reports of the 70+ people murdered - facts about COVID - facts about COVID vaccines - facts about Ukraine’s status on the battlefield - footage of Nazis in Ukraine - facts about Tavistock and WPATH lacking scientific evidence for their recommendations Because institutions and experts have normalized lying to “nudge” the public via narrative manipulation, it has become easy for adversaries to undermine the nation by showing contrary facts. People become more radicalized by demonstrating with evidence a supposed ally has betrayed them, eg, your government lying to you. Once broke, trust in institutions and experts takes generations to repair — or a replacement of those institutions entirely. reply anitil 9 hours agoparentprevIf anyone has a few hours free, I'd recommend this documentary about Eve Online - https://www.youtube.com/watch?v=BCSeISYcoyI reply KolmogorovComp 40 minutes agorootparent6 hours of robotic Siri-like voice speech? Torture. It's a shame because I presume if it's recommended despite that it must be insightful. reply fastball 8 hours agorootparentprevWhen you said \"a few hours\", I was expecting 3, not 6(!) reply bitwize 8 hours agoparentprevRudi Dutschke's long march through the institutions. Marcuse endorsed it -- because it works. reply andix 12 hours agoprevMaybe we need a reporting system for maintainer changes of bigger projects. Some list where they get published and people can keep an eye on it. Those changes of maintainers need to be synced to package distribution sites like npm.js or Debian packages and put in context with versions/releases. In Europe this was introduced for banks after the banking crisis. If a bank does any organizational change, a report is sent out to all member states of the EU right away and any of the 27 national bank agencies can check if they notice something unusual. It might be possible to bribe a few people in your own country, but it’s really hard to bribe all responsible people in 26 other countries. reply wrsh07 6 hours agoparentI'm sure some security researcher is doing this, but we could easily create a visualization of \"who has contributed over time\" and identify transitioning of maintainers automatically just from git. This might be worth doing and contributing to a site like bestofjs or libraries.io (I don't really use that one though!) reply guappa 3 hours agorootparent> who has contributed over time When major security players insist that using GPG is bad, there is no way of knowing if bob@bob.bob is the same account that it was last month or not. reply StrauXX 1 hour agorootparentIt is not the idea of GPG thats bad. In fact, the idea is great! The implementation of GPG however is quite another thing. Ease of use and user experience are really not that great with GPG. It is difficult to use even for developers. Developers are users too amd so on. reply guappa 1 hour agorootparentTry uploading a signed package on pypi. Sign it with sequoia instead of GPG if you like. You'll receive an email asking you to stop uploading signatures. reply arccy 28 minutes agorootparentprevyou can sell/steal keys just as easily as accounts reply guappa 17 minutes agorootparentOk. Can you get my private key? Feel free to respond to this comment with my private GPG key. I think guessing a password and getting lucky is much easier. reply jmakov 1 hour agoparentprevIs it really though? Cum-ex and cumcum appear to still work great. reply triblemaster 10 hours agoparentprevHow about simply paying the maintainers and then getting stuff done like the classical business does. reply omoikane 6 hours agorootparentDid you mean: instead of trying to become a maintainer to a trusted open source project, how about bad actors simply bribe the existing maintainer to do their bidding? There would be no maintainer changes in that scenario. Related, the motivation for trying to gain privileged access to open source projects is to leverage the existing trust associated with that project. A different long game that could be played is to create a new project with the intent on backdooring it a few years down the road, after it has gained sufficient trust. reply __MatrixMan__ 7 hours agorootparentprevWell yes, sounds great, but it doesn't really address the security problem. Now you've just got the bad guys getting two paychecks instead of one and the good guys getting one paycheck instead of zero. reply armini 5 hours agorootparentOne of the biggest risks for companies is securing dormant code, it's perfectly fine for a project to be no longer sexy enough to maintain. Platforms like thanks.dev have already proven how reward & recognition can help promote development in an ecosystem https://www.youtube.com/watch?v=e5FV-AnKPlo&t=1s reply transpute 7 hours agorootparentprev> bad guys getting two paychecks instead of one 1 to 2 paychecks = 100% increase. > good guys getting one paycheck instead of zero 0 to 1 paycheck = infinity increase. With a known baseline of \"good paychecks\", financial analytics can pursue identification of \"bad paychecks\". reply PartiallyTyped 12 hours agoparentprev> Maybe we need a reporting system for maintainer changes of bigger projects. Some list where they get published and people can keep an eye on it. The rust project does it. There's a repo with all [active] members and their permissions on github, etc. These get synchronized and updated every time there's a change. reply guappa 3 hours agorootparentThe major projects aren't on github. reply andix 12 hours agorootparentprevJust for the main project, or for all/most packages on crates.io? reply PartiallyTyped 11 hours agorootparentEvery repository and team under rust-lang on github. reply andix 11 hours agorootparentThat's great, but I think that's not enough. This would need to extend to crates.io, I'm sure there are some packages, that are very commonly used and not part of rust-lang. reply GauntletWizard 12 hours agorootparentprevThis is, in my eyes, one of the most important parts of \"Infrastructure as Code\". You should make the list of who has what permissions a critical artifact, as immutably part of the repo as any other change. reply Devasta 20 minutes agoprevThe fix is easy: license your code as AGPL3. Your project is no longer a useful target as corporations will avoid you, and on top of that you get rid of the demands for free tech support. Win win. reply wiseowise 2 hours agoprevCan we, PLEASE, return back to batteries-included first? While it doesn’t eliminate the threat completely, but I can sleep soundly knowing that I need to vet one party instead of 1000. reply porcoda 2 hours agoparent+1. My team still favors batteries-included style systems, even if it means missing out on the latest thing all the cool kids on the internet are talking about. For languages, this usually means sticking with the standard library that ships with it along with one or two libraries that augment it (e.g., BOOST for C++). It's not like \"batteries included\" eliminates the problem - a bad actor can wander in there and cause trouble. It's just a much more controlled environment that often has a process for contributing. I'm not a fan of the move of languages away from rich standard libraries to the \"Random Interconnected Pile Of Internet Stuff\". Unfortunately people like me are in the minority it seems, and the \"move fast and break stuff\" mentality seems to still dominate the open source world even if that phrase has fallen out of favor - the attitude still seems to exist. reply arccy 26 minutes agorootparentin open source you rarely have the cohesion of a full team to NIH all the stuff that you could pull in libraries for... reply throwaway2037 3 hours agoprev> This approach bears strong resemblance to the manner in which “Jia Tan” positioned themselves in the XZ/liblzma backdoor. No, it doesn't. I stopped after read this. Jia's \"attack\" was near state level actor stuff. A bunch of emails asking/begging for commit access sounds like a 16 year old sending emails from the basement of his parent's house. reply guappa 24 minutes agoparentIt's openssf… All of their posts are basically \"install our github action, get our scorecard!\". Which I personally think is completely useless. If they want open source maintainers to do boring compliance stuff, they can pay them. I won't be doing that for free for sure. reply logrot 2 hours agoparentprev> A bunch of emails asking/begging for commit access This is not what it says. If you're going to argue, argue with the text from the article not a made up reinterpretation. reply kazinator 11 hours agoprev> Enable two-factor authentication (2FA) or Multifactor Authentication (MFA). Not on any third party system, where you're locked out forever if you lose your second factor. Fuck that! Only self-hosted, where you can recover via physical access. (That should actually be the first advice: host the stuff yourself. People lose control of projects due to hosting them on third party services. Be the guy who can pull the power cord out of the wall.) reply em-bee 10 hours agoparentyes and no. i hate 2FA as well, but in the end, even if i loose my access to github i only loose access to my github identity but i don't loose access to my code, so i can live with that. of course in the light of this discussion losing access to my github identity would be part of the problem, so it's a tradeoff. is it more likely that someone will break into my account and abuse my identity if i don't have 2FA or is it more likely that i loose my second factor and have to rebuild my identity. in the latter case someone else could also pretend to be me, but since the xz debacle both of us would face more scrutiny that my hope still is that i would win. will the real eMBee please raise their hand? reply guappa 2 hours agorootparent> if i loose my access to github i only loose access to my github identity but i don't loose access to my code, so i can live with that. That means that you need to fork your own project, and there is no way to communicate it to the users, since the new account could just be someone pretending to be you. If there is a security vulnerability, it would remain unfixed forever. > is it more likely that someone will break into my account and abuse my identity if i don't have 2FA or is it more likely that i loose my second factor and have to rebuild my identity Since phones are very easy to break, and until very recently there was no way to backup google authenticator, I'd say that losing your 2nd factor was the most likely of the two. Now if you say that you backup your 2nd factor seed in your password manager, where your password is… congratulations you're doing over-complicated 1 factor authentication! reply eMBee- 10 hours agorootparentprevnext [6 more] [flagged] em-bee 9 hours agorootparentgee, people can't take a bit of humor to make a point? reply Kiro 4 hours agorootparentStrange reaction. They created an account just for you and the joke. You should be flattered. reply johnnyanmac 5 hours agorootparentprevHN has traditionally been a bit more resistant to humor, even slightly more witty humor. reply sverhagen 9 hours agorootparentprevI thought that was all pretty funny. reply humzashahid98 9 hours agorootparentYou're not alone. It made me laugh too. reply samatman 9 hours agoparentprev> Not on any third party system, where you're locked out forever if you lose your second factor. Every two-factor system I've ever seen is actually two-of-three, with an account recovery code that you save elsewhere. I lost all my two-factor auths when my phone got wrecked, it was annoying to reestablish access to those accounts (and I now use a TOTP client which backs the tokesn up), but it was tedious rather than difficult. reply KronisLV 7 minutes agorootparent> and I now use a TOTP client which backs the tokens up What are some good options for this? I think my ideal solution would export an encrypted file, a bit like KeePass does on the desktop, but I don't know of many mobile apps for that. reply guappa 1 hour agorootparentprevIt was not difficult because you actually had the recovery codes. How many people have them? Also you're supposed to print them. Where? How many people own a printer? If you print them in a shop they can be considered compromised. reply ChrisMarshallNY 13 hours agoprevThis is a great write-up. It's a very serious issue. I don't really know if there is any \"one solution.\" I suspect that each project needs to set its own bar, and that any dependency that falls out of maintenance should be removed as quickly as possible (which was good practice, beforehand, but even more important, now). [EDITED TO ADD] I would also think about \"scoring\" the sensitivity of projects. Things like cryptography and low-level drivers would be highest-rated, while user-space chrome might not be as important. reply transpute 7 hours agoparentScoring framework: https://securityscorecards.dev/ Code: https://github.com/ossf/scorecard April 2024 ranking of OSS projects by criticality, 100MB CSV: https://commondatastorage.googleapis.com/ossf-criticality-sc... reply guappa 21 minutes agorootparentThe scorecard stuff is flawed. You want to decide if something is secure or insecure but without reading the code. It's never going to have any correlation. reply ChrisMarshallNY 55 minutes agorootparentprevThanks! I have a friend that used to work for a company called “SecurityScorecard.” Different beast, though. I think the idea was similar. reply krishadi 1 hour agoprevI hope there is a better way to maintain open source projects without being overly cautious and suspicious of every PR someone makes. Maintaining open source projects is hard, and this is going to slow down development on many projects. And, rightly so, it's better to make a good code base, rather than one that is littered with backdoors. I wonder what could make this situation better for the maintainers of open source projects? reply __loam 58 minutes agoparentPublic funding for security maintenance. reply guappa 23 minutes agorootparentAnd serious taxation of tech companies. reply perlgeek 1 hour agoprevAs a (mostly former) OSS maintainer, I don't want to be suspicious of any contributors. I want welcome contributors as co-maintainers. I don't want to gate-keep specific tasks (such as releasing and updating the website) so that I'm the single point of failure. It's already hard to enough to get people to contribute more than a README typo fix or maybe a single feature that they need themselves, and get them invested into the project as whole. Somebody please create an alternative for keeping projects secure that's not based on suspicion and gatekeeping. reply michelsedgh 12 hours agoprevWell after the XZ attack, I was thinking how common this can be. Good to know that at least im not the only one and others inside the community are wondering about this. I hope someone is smart or lucky enough to find a solution to at least be able to lessen the impact of these attacks. I still wonder how many more of these are there, and my question is because of these attacks, isn’t open source more prone to these compared to closed sourced software? Usually the argument for open source is because everyone can read the code, its less vulnerable but now because everyone can write the code and have big incentives to do malicious stuff, doesn’t it make open source worse? reply 01100011 5 hours agoparentMany open source projects just don't get enough attention for the 'many eyes' benefit of OSS to occur. Many projects are neglected and poorly maintained, with little participation from the users. I don't think OSS is particularly special though. If a state actor threw cash around they could find folks at many big companies to do their bidding. In my experience, commercial software reviews are susceptible to the same sorts of attacks as those listed in the article(\"please review my change ASAP because it needs to go into the next release before the deadline!\"). I don't know what to do about this. You could subject approved submitters to better background checks. You can improve automated threat detection and code analysis. You can switch to safer-by-default languages that make backdoors and malicious behavior more obvious. I wonder if the same issue exists in other engineering fields? Has anyone ever bribed an engineer to make a bridge or a water supply less robust? reply eviks 2 hours agorootparent> Has anyone ever bribed an engineer to make a bridge or a water supply less robust Of course, this happens all the time, check the consequences of any earthquake in any corrupt country for the more visible examples reply 01100011 2 hours agorootparentYeah, for sure. I was mainly thinking about, say, a foreign state actor doing the bribing and not just the usual grift/embezzlement/corruption. reply eviks 1 hour agorootparentAh, misunderstood, you mean something like a sneaky sabotage? Hm, don't recall any, the payoff seems to be too small and unpredictable? But I think there were cases of \"poisoning\" the design of some weapons reply janosdebugs 11 hours agoparentprevThe problem is, we don't know. I've seen PRs that could be curious students, or it could be a first try to see if we are paying attention. It's really easy these days to produce a halfway decent looking PR for someone in their first year of uni and my worry is that an increased volume of low to medium quality contributions will lead to maintainer fatigue. Depending on the project, that may be the point where pressure can be applied to share maintainership. reply fancyfredbot 11 hours agoprevDoesn't have to be a takeover. If I'm a state actor I'll maintain a few projects specifically so I can hide backdoors in them. When/if one gets popular and I decide to backdoor them I'll claim it was a social engineering takeover. reply lnenad 2 hours agoparentExactly, pick something that is expensive right now, make it free and people will use it. reply pcloadletter_ 12 hours agoprevGood warning for the future... but what about the past? Any thoughts on retroactively looking at behavior for existing OS projects? Seems like an impossible amount of work. reply andix 12 hours agoparentMaybe not so impossible. Start with making a list of projects that are everywhere. Inside every Linux distribution, inside every react/angular/vue/etc project, … Then check which companies support those projects with active development, and calculate a rating. Are the companies located inside democracies or are they mostly from china or Russia? It’s probably not that many packages in the end. A few thousand high impact/risk projects probably. reply int0x29 11 hours agorootparentBackdoor attempts won't be that obvious. The xz incident just had a random unaffiliated burner account and nothing of any clear national origin. reply andix 11 hours agorootparentI wanted to make a different point. If for example Google or Red Hat were deeply involved within the xz project, there might have been more people reviewing the code. The evil changes to xz were easy to overlook, but not impossible to notice. Especially the added \"accidential\" semicolon made me think about probabilities. I think in a code review I would notice that with a probability of 10-20%. So if 10 people would've looked at it, there might have been quite a low chance to get away with it. Having some high profile companies involved into an open source project the risk score would drop in my opinion, which would highlight the projects that are completely community maintained, and might be more susceptible. Having such a list might be a security threat by itself though, because attackers would focus on the \"low risk\" projects first. reply galaxyLogic 9 hours agorootparentOne possibility could be a license that requires big companies to dedicate one or more people as maintainers or at least reviewers of a project if they want to get license to use the software. reply andix 8 hours agorootparentGPLv4? I doubt this would be a bigger success story than v3 and v2. Permissive licenses won the war against Copyleft a long time ago. reply BobbyTables2 6 hours agorootparentprevThe list you speak of already exists — it is the package registries of Debian/Ubuntu, RHEL, etc. What about American companies using mainland China developers to drive their (well known) open source projects with crappy code? Who’s to blame? We’re currently smoking at the gas station and things haven’t blown up yet… reply 01100011 5 hours agoparentprevI think the only feasible option, due to the shortage of SWE talent, is a combination of automation, tooling and safer languages. We need better threat analysis systems, and we need to rely more on safe-by-default languages that make program behavior more obvious. reply generationP 8 hours agoprevWhich of the suggested \"Steps to help\" would have helped prevent the xz infiltration? For a single-maintainer project, adding bureaucracy can only make things worse. reply xyst 8 hours agoparentSeems just being aware of it. Doesn’t hurt to have people be more alert. Although that alertness can quickly become fear and anxiety. Which can eventually evolve into paranoia. reply eviks 2 hours agoprevSeems like this would mostly raise suspicion for little gain, wasn't there a more robust solution of signed distributed reviews so you could simply pin the dependency at a state where reviewers you've explicitly added as trusted review code (and similarly after you review you can express that for the others to rely on) Of course, this won't magically increase the time spent on code review, but will at least allow currently internal reviews to be available reply anon24775 1 hour agoprevHostile takeover of FOSS projects leading to nefarious outcome for users is indeed nothing new. Oracle for example has been playing that game for a while (OpenSolaris, Java, MySQL, OpenOffice,...). reply xyst 8 hours agoprevI wonder how the person that was running “xz” is doing. Hope he’s doing okay mentally after being thrown into the spotlight by the shit stain of a person(s) behind the “jia tan” and et al aliases reply piecerough 12 hours agoprevThis is only going to get worse with Large Language Models. Let's imagine a somewhat knowledgeable individual, could craft both emails, messages and even commits with a bunch of prompts. Those will relate deeply to the project. reply smsm42 11 hours agoparentMaybe one day it will happen, but right now LLM-generated persona would likely set off every alarm bell for a lot of people. LLMs have very recognizable style, and it usually falls right into the uncanny valley. reply int_19h 1 hour agorootparentThe \"recognizable style\" that people usually refer to is the default persona that most are exposed. However, the style can be changed very drastically with some fairly simple prompting. reply heavyset_go 8 hours agorootparentprevIt doesn't have to be completely automated, just enough to make the process of juggling multiple personas a bit smoother. reply andix 11 hours agoparentprevI don't think this is going to be a big issue. Those attacks have to be high-profile attacks. If you look at the xz backdoor, there was some top notch engineering behind it. If we ever reach a level of LLMs being able to do that, we don't need any open source contributors any more. We just tell tell the LLM to program an operating system and it will just do it. reply andy99 12 hours agoparentprevDo you have any evidence or real examples to support that? I hear people say similar things but see nothing to suggest LLMs are a particular threat. reply TechDebtDevin 12 hours agorootparentThe real threat of LLMs is their potential to ruin your day if you use them to assist in your work. reply cgh 10 hours agorootparentUsername checks out reply kemotep 11 hours agorootparentprevAre you asking for evidence that LLM’s can be used to write emails and chat messages? reply jesprenj 20 minutes agoprev> XZ Utils cyberattack likely not an isolated incident and yet they haven't provided any other examples. reply smsm42 11 hours agoprevIt's a pity they don't give any details about the \"attempted takeover\" - are they available elsewhere? reply langsoul-com 3 hours agoprevWhat about hostile takeover of existing maintainer accounts? reply wslh 32 minutes agoprevTechnical people undersestimate the power of social engineer because, most generally, we look for extreme problem solving in security incidents and social matters is not the expertise of the field. Wenshould be aware that mane grandiose hackers as Kevin Mitnick knew well this art. I highlight this because it is a weakness that is not solved by any of the artifacts you learn at university. I think scamming, in general, should be taught early on in schools, as well as finances. reply EVa5I7bHFq9mnYK 9 hours agoprevThe guidelines are laugthably naive in the light of current global security environment and level of threats involved. Who said a long standing member of community can't be a threat? Who checks identity of contributors? All I know it's enough to have an email to create a github account. Is FBI checking their credentials? reply brohee 4 hours agoprevIf the \"Jigar Kumar\" kind ends up in a CIA black site mailing lists will be a lot more enjoyable. reply ClassAndBurn 8 hours agoprevThere's an awkward reckoning in open source software about inclusivity and protecting the long-term security of projects coming. Authors from several countries were already suspicious, such as Iran. Anyone from Russia and China or unknown places are all potential risks now. Combined with recent inclusive ideologies, it’s gonna cause hard conversations. There will be a furthering in segmenting the Internet. Why fight contributing to an open source project when you could fork it and contribute with your allies? For true enemies, there’s no risk to licensing or copyright issues. You can merge changes from the original, no problem. China even falls into this as there’s a limited ability for US companies to litigate within the country. People think the Network State is hot, but at the end of the day, the Internet still has borders. reply Thorrez 6 hours agoparentI don't see how blocking contributions from people in Russia etc will help. Malicious actors can simply falsely claim to be American. Is GitHub going to start verifying citizenship? Even if GitHub did that, it likely wouldn't be too hard to fake. reply int_19h 58 minutes agorootparentAnd to be honest, it's not like getting US citizenship for their agent is difficult for a government agency. The same goes for most other countries. Keep in mind that most places allow you to literally buy citizenship through investment. The amount you need for a country like US is prohibitive for the vast majority, but, again, is not really a problem for another government. reply jen20 2 hours agorootparentprev> Is GitHub going to start verifying citizenship? As an American company they must presumably already do this to avoid violating sanctions, and least for anyone giving them money. It’s not a huge stretch to imagine they could also do so for free tier users. reply Palomides 8 hours agoparentprevthis is a wild prediction to make and disturbingly regressive FOSS is one of the most beautiful examples of supranational collaboration, and is in my experience much more integrated than the web at large, in a way that has nothing to do with \"recent inclusive ideologies\" reply mdavidn 8 hours agoparentprevWould those countries not have similar concerns about US maintainers? The larger issue is successful projects with too few active maintainers. reply ilhuadjkv 9 hours agoprevWould it be interesting if Github (and others) had a program where they would verify people using the same regulations the banking industry uses for KYC (know your customer)? Optional step for developers to show they are who they say they are? reply orthecreedence 9 hours agoparentLol bluechecks for programmers. I think a better idea is people actually read the code before merging it. The more people use your project, the more suspicious you should be. reply csomar 3 hours agoparentprevNo. Let's stop having these ideas of a totalitarian world. Instead, find solutions for zero-trust environments. reply wiseowise 2 hours agorootparentHow is that totalitarian? Bureaucratic - yes, totalitarian- hardly. reply heavyset_go 8 hours agoparentprevPeople will just buy and sell verified accounts like what happens on every platform. reply bsima 9 hours agoparentprevNo. This is a terrible idea reply puffybuf 11 hours agoprevIt doesn't even have to be a server like ssh. It could be a client side project engineered to somehow deliver all your ssh keys or bitcoin wallets. There is no reason backdoors couldn't stealthily phone home from client side applications. reply andix 11 hours agoparentThe crazy thing about the xz issue was, that xz is not even a dependency of openssh, but of systemd. And the xz backdoor exploited the systemd integration of openssh. This exploit was invisible to people that tested plain openssh without one of the most common integrations into Linux. reply that_lurker 9 hours agoprevIs there a list of these projects that are 1 to 5ish people maintained, like the mentioned XZ Util. Would be nice to have some sort of monitoring changes in these. reply sim7c00 12 hours agoprevi dont think maintainer changes is even the endgame for this stuff. its hard to get a new person in, but a nation state can likely more trivially attack a current maintainer.. everyone has a button somewhere. the only solution to this is tooling which can flawlessly reason about code changes being malicious or not, being applied to every change in a project. and then still its a lost cause. a lot of issues and vilnerabilities come from how softwarw interoperates witj other software. will you be able to reason about all possible package combinations and how they are secure or not when they come together in certain ways? it should be easier to write systems from scratch, rather than to have to use third party code for everything. computers currently are not condusive to this. they need to be built different, to allow software to be built different. maybe while we are at it we can also make it so computers reduce complexity in peoples lives instead of adding to it. reply transpute 7 hours agoparent> it should be easier to write systems from scratch, rather than to have to use third party code for everything. computers currently are not condusive to this. they need to be built different, to allow software to be built different. Yes. We also need to encourage user scripting of first party library APIs on devices. iOS Shortcuts are a step in the right direction, but they need better tooling to maintain and distribute source-controlled shortcuts. reply Arch-TK 11 hours agoprevI'm not so certain that the `safe_fprintf` to `fprintf` swap was ever itself meant to be malicious. There's some speculation about the addition of strerror but again. reply roschdal 4 hours agoprevThe problem with open source is that it's free software, so no one wants to pay for it. Therefore the developers don't spend enough time maintaining and securing it, and not enough time preventing social engineering takeovers. reply __loam 9 hours agoprevThis is going to become a huge problem with state actors attacking what is basically public infrastructure. We should be publicly funding efforts to protect it. reply doubloon 9 hours agoprevthe September that never ended reply keithalewis 5 hours agoprevLike Microsoft did with GitHub? reply jongjong 8 hours agoprevI've been saying for years, over and over, that we need to focus on simple architecture and improve our coding standards but I keep getting ignored. People keep making software and tools more complex... \"Just use TypeScript\" they say, \"Just use React, with Typescript\" then they end up with literally thousands of unnecessary dependencies. The bad guys are laughing at our collective ignorance and naivety. Now probably the whole industry is compromised. Maybe our entire political system is compromised because of this. People just had to look for projects with minimal code and dependencies. Precisely the opposite of what they did. These clean projects were starved of attention and opportunities and the overengineered projects were brought to the forefront. Now they are easy targets for bad actors. It's so easy to hide vulnerabilities amongst complexity... And we've chosen complexity. reply phendrenad2 6 hours agoparentI suspect that the bad actors aren't merely benefitting from this situation, they're probably actively encouraging it, in GitHub issue discussions, on Reddit, in the comment sections of YouTube videos, here on HN. Maybe I'm paranoid. reply jongjong 6 hours agorootparentI think this is a reasonable assumption for most high exposure projects. There are state actors from all sides who want to inject backdoors into software and they all benefit from complexity. It could indicate that governments are more focused on offense than defense. It seems to be a logical consequence of our short term corporate and political mindset. Offense is the best defense, they say... Ok, but not at scale when your opponents also possess the same weapons as you do; then you both end up worse off. reply wiseowise 2 hours agoparentprevTypescript has zero dependencies. reply lloydatkinson 9 hours agoprevI initially expected this to be about how Vercel/Next is trying to takeover React and morph it into Vercel.js reply mistrial9 9 hours agoprevit appears that this organization is repeatesly agitating for \"threats\" publicity in open source.. what is the motivation? https://news.ycombinator.com/from?site=openssf.org reply Thorrez 6 hours agoparentThey're an open source security foundation. Their entire purpose is to fight those threats. One way to help flight them is to bring awareness to them. reply 10 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The XZ Utils cyberattack attempted a backdoor takeover, raising concerns as similar incidents have been intercepted by the OpenJS Foundation and Open Source Security Foundation.",
      "Social engineering tactics were employed to gain maintainer access to popular JavaScript projects, highlighting the importance of vigilance and security best practices for project maintainers.",
      "Industry and government efforts are crucial in securing open source infrastructure, with the OpenJS Foundation and OpenSSF collaborating to enhance the security of the JavaScript ecosystem through project support and funding."
    ],
    "commentSummary": [
      "The discussion highlights security threats in open source projects like social engineering, malicious code, backdoor attacks, and state actor exploitation, emphasizing transparency and proactive security measures.",
      "Recommendations include enhancing security through collaboration, verifying contributor identities, improving code review processes, and implementing systems for maintainer changes.",
      "Challenges such as project maintenance, dependency management, securing dormant code, and the necessity of public funding for security maintenance are underscored, along with concerns about cultural shifts, false news propagation, and bribery risks in engineering fields."
    ],
    "points": 639,
    "commentCount": 260,
    "retryCount": 0,
    "time": 1714938828
  },
  {
    "id": 40267639,
    "title": "Israel Shuts Down Al Jazeera Offices amid National Security Concerns",
    "originLink": "https://www.theguardian.com/world/article/2024/may/05/israel-shuts-down-local-al-jazeera-offices-in-dark-day-for-the-media",
    "originBody": "2:19 Israel orders a ban of Al Jazeera and suspends broadcasts – video report Israel Israel shuts down local Al Jazeera offices in ‘dark day for the media’ Foreign Press Association decries move under new law based on claim network is a threat to national security Jason Burke in Jerusalem Sun 5 May 2024 12.53 EDT Share Israeli authorities shut down the local offices of Al Jazeera on Sunday, hours after a government vote to use new laws to close the satellite news network’s operations in the country. Critics called the move, which comes as faltering indirect ceasefire negotiations between Israel and Hamas continue, a “dark day for the media” and raised new concerns about the attitude to free speech of Benjamin Netanyahu’s hardline government. Israeli officials said the move was justified because Al Jazeera was a threat to national security. “The incitement channel Al Jazeera will be closed in Israel,” the country’s prime minister posted on social media after the unanimous cabinet vote. A government statement said Israel’s communications minister had signed orders to act immediately to close al Jazeera’s offices in Israel, confiscate broadcast equipment, cut the channel off from cable and satellite companies and block its websites. The network, which is funded by Qatar, has been critical of Israel’s military operation in Gaza, from where it has reported around the clock throughout the seven-month war. Al Jazeera said the accusation that it threatened Israeli security was a “dangerous and ridiculous lie” that put its journalists at risk. “Al Jazeera Media Network strongly condemns and denounces this criminal act that violates human rights and the basic right to access of information,” the company said in a statement. “Al Jazeera affirms its right to continue to provide news and information to its global audiences.” A pre-recorded “final report” listing the restrictions placed on the network by a reporter in Jerusalem was broadcast on the network after the ban came into effect. Al Jazeera has previously accused the Israeli authorities of deliberately targeting several of its journalists, including Samer Abu Daqqa and Hamza Al-Dahdouh, both killed in Gaza during the conflict. Israel has rejected the charge and says it does not target journalists. The office of the UN high commissioner for human rights also criticised the move. “We regret cabinet decision to close Al Jazeera in Israel,” it said on X. “A free & independent media is essential to ensuring transparency & accountability. Now, even more so given tight restrictions on reporting from Gaza. Freedom of expression is a key human right. We urge govt to overturn ban.” Israel’s parliament ratified a law last month that allows for the temporary closure of foreign broadcasters considered a threat to national security. The law allows Netanyahu and his security cabinet to shut Al Jazeera’s offices in Israel for 45 days, a period that can be renewed, so it could stay in force until the end of July or until the end of major military operations in Gaza. While including on-the-ground reporting of the war’s casualties, Al Jazeera’s Arabic-language service often publishes verbatim video statements from Hamas and other militant groups in the region, drawing sharp criticism from Israeli officials. Israel tells Hamas to accept ceasefire terms or risk new onslaught ‘in near future’ Read more A campaign of judicial reform led last year by Netanyahu’s coalition government, the most rightwing in Israel’s history, prompted great opposition and accusations of authoritarianism. Recent crackdowns on protesters against the Gaza war in Israel have also raised new concerns for free speech. The Foreign Press Association, a NGO representing journalists working for international news organisations reporting from Israel, the West Bank and Gaza accused Israel of joining a “dubious club of authoritarian governments”. “This is a dark day for the media. This is a dark day for democracy,” it said in a statement. There was also some political opposition in Israel to the move, or at least its timing. The National Unity party, a centrist member of the ruling coalition, said that coming as ceasefire talks appeared close to failing, it could “sabotage efforts” to free Israeli hostages in Gaza. Qatar established Al Jazeera in 1996 to build influence around the Middle East and further afield. The small Gulf state, where several Hamas political leaders are based, was a key mediator in the talks but has been marginalised in recent weeks, which may have encouraged the Israeli government to act. Israel has barred foreign journalists from entering Gaza to cover the conflict, which was triggered by Hamas attacks into southern Israel on 7 October last year in which 1,200 people, mostly civilians, were killed. Israel’s ensuing offensive has killed more than 34,000 people, mostly women and children. Explore more on these topics Israel Al Jazeera Press freedom Israel-Gaza war Middle East and north Africa TV news Television industry news Share Reuse this content",
    "commentLink": "https://news.ycombinator.com/item?id=40267639",
    "commentBody": "Israel shuts down local Al Jazeera offices (theguardian.com)569 points by jjgreen 14 hours agohidepastfavorite734 comments dang 13 hours agoAll: if you're about to comment in this thread, please review https://news.ycombinator.com/newsguidelines.html and make sure your post is in the intended spirit of the site. If it isn't, please edit it until it is; or simply remember that the internet is usually wrong and refrain from posting. The intended spirit is curious, respectful conversation in which we learn from each other. Yes, that is hard when emotions run strong, but hard != impossible, and it's what the site rules ask: \"Comments should get more thoughtful and substantive, not less, as a topic gets more divisive.\" Animats 13 hours agoprev\"and block its websites.\" So this keeps Israelis from reading Al Jazeera. Now that's new. Israel started Internet censorship in 2017.[1] Initially it was limited to \"terror group websites, online illegal gambling, prostitution services, hard drug sales\". At the time, \"due to warnings from rights groups that the law poses a slippery slope toward additional censorship, the final version of the legislation dictates that rights groups may appeal the decisions.\" Then, in 2021, there was the \"Facebook bill\", authorizing very broad censorship.[2] That does not seem to have passed. It was first proposed in 2016, almost passed in 2018 [3], tried in 2021, and tried again in 2022. It doesn't seem to have passed. But something new happened recently. Wikipedia has a note at Censorship in Israel: \"This article needs to be updated. The reason given is: New ban issued by the knesset on foreign media channels. Please help update this article to reflect recent events or newly available information. (April 2024)\"[4] The Knesset gave the government the authority to ban foreign media on April 1, 2024.[5] This isn't just about preventing outside media from reporting from Israel. It keeps Israelis from viewing media the government doesn't like. Haarez has good coverage.[6] The US White House press secretary issued a weak statement condemning Israel's action, but it was on April 1st and the costumed Easter Bunny overshadowed that statement.[7] [1] https://www.timesofisrael.com/to-tackle-online-crime-israel-... [2] https://www.timesofisrael.com/proposed-censorship-bill-more-... [3] https://www.timesofisrael.com/how-israel-nearly-destroyed-fr... [4] https://en.wikipedia.org/wiki/Censorship_in_Israel [5] https://www.msn.com/en-us/news/world/israels-knesset-approve... [6] https://www.haaretz.com/israel-news/2024-05-05/ty-article/is... [7] https://www.whitehouse.gov/briefing-room/press-briefings/202... reply suddenexample 12 hours agoparent> The US White House press secretary issued a weak statement condemning Israel's action, but it was on April 1st and the costumed Easter Bunny overshadowed that statement.[7] Wow you can't make this stuff up reply throwup238 10 hours agorootparentArmando Iannucci's career has twice now been upstaged by the increasing ridiculousness of reality in politics. One of the reasons he says he stopped making his first series, The Thick of It, was that UK politicians became parodies of themselves. There was nothing left to make fun of. Then the same thing happened to his American incarnation, Veep. reply tialaramex 9 hours agorootparentIannucci's Death of Stalin has the advantage that it's all history - it can't become more ridiculous after he made the movie because it's in the past. But of course the reality is more or less as absurd as the movie, although different in important ways. (Lots of the timing is completely wrong, events are re-arranged, people are shuffled about so that fewer actors play more important roles) but it does have the advantage that yeah, Stalin can't have another even more ridiculous life which makes the movie seem tame, he's dead. reply VelesDude 8 hours agorootparentI really appreciate works like Death of Stalin that are more than happy to move things around in the name of entertainment. It isn't a documentary so it has the freedom to get creative and hit the key notes rather than pure accuracy. They also just said \"No need to have Russian accents, the audience is smart enough to just go with what you have.\" reply benja123 2 hours agoparentprevIsraeli here - I just checked and I can still access the site in both English and Arabic. My guess is you won’t actually see the site being blocked. reply cies 1 hour agorootparentAfter rt.com et al were blocked in the EU they also kept working for a few more week/months before I had to use a mirror/VPN to read the new paper of \"the enemy\". Side note: I though being able to read the news of the enemy was testimony to the moral high ground of a \"modern free and democratic society\". No more moral high ground if you are trying to shape the perception of the public with censorship. reply benja123 1 hour agorootparentI personally don't support blocking Al Jazeera. It's not that I'm a fan of them or any news station operated under a dictatorship without free press. It's clear that Al Jazeera acts as the propaganda wing of the Qatari government, and Qatar, being one of the main sponsors of Hamas, uses it to effectively spread Hamas propaganda. However, the real question is what does blocking them achieve. Does it stop their influence? No. Does it make them harder to watch here? No. Are the people advocating for their blockage even watching it? Likely not. It's mostly done because it's something the government can claim to have achieved for what remains of their base. I'm skeptical that the blocking of the website will actually occur, as it will probably be challenged in court. Regardless, in a democracy, internet censorship is a slippery slope, and for that reason, I am against it in most cases. reply brnt 47 minutes agorootparent> Does it make them harder to watch here? No. Of course it does? Most people are not going to go out of their way for it unless they have specific reasons to do so. What's on/available by default matters. (Ask any UX specialist.) reply benja123 27 minutes agorootparentI should know better, my wife is a UX specialist! You make a good point. What I should have said is that it won’t stop people who really want to watch it. reply raxxorraxor 17 minutes agorootparentprevIt usually is, but more neurotic elements in western governments often tend to not give much on values at all. reply angra_mainyu 12 hours agoparentprevEurope has done the same with Russia. Also, I think a few other Arab countries like Egypt have blocked/banned Al Jazeera. reply mrtksn 11 hours agorootparentThat's correct and IMHO its the right thing to do when shooting begins because when people shoot each other this is no longer a discussion and the press is part of the warfare. Remember all the Russian media and social media accounts claiming that its American hysteria that they will invade Ukraine? They denied and mocked anyone who claimed that they will invade up until the tanks rolled in. Personally, I'm critical of the Israeli government but I think it's in their right to try to control information flow as they are in process of driving people from their homes and mass killing people in retaliation of a terrorist attack that claimed the lives of over thousand innocent people. I really dislike glorification war and pretending that it has rules or honour or something like that. People are taking lives en masse and its more than normal to try to control the information flow when doing it. reply guappa 2 hours agorootparentIt's a bit ironic how we are constantly reminded that china isn't a democracy because they have censorship, while we are free and democratic and don't have censorship (except we do). Italy is passing laws to be able to block websites within 30 minutes, without any oversight from any judge. (more details here, link in italian https://stop-piracy-shield.it/) reply AlecSchueler 35 minutes agorootparentIn Germany they can't publicly gather to protest Israel's actions either, not legally anyway. https://www.aljazeera.com/features/2023/10/26/complete-censo... reply tenlp 43 minutes agorootparentprevIn fact, a series of Western media including the BBC, Reuters, CNN, etc. all have offices in China.... and you can also easily access Western media websites through VPN and other methods which are uncensored reply mrtksn 1 hour agorootparentprevI’m also quite concerned about the Western world adopting the Chinese style of governance but censorship at wartime is very different from censorship at peace time. False information leading to spreading misguided ideas is deadly at wartime because there is no time to address it. I definitely don’t support Italy blocking websites or the US blocking apps. reply cpursley 54 minutes agorootparentWhat about all the misleading news (i.e., positive spin) in the Western world's \"free press\" that lead to many believing that Ukraine had a chance for a counter offensive (against even basic military thinking and logic). Didn't that lead to unnecessary suffering and deaths during war time (vs say an alternative, beefing up defense lines)? That's just a random recent example. Another older one is the the media going nuts over \"WMDs\", leading to an unnecessary US invasion and millions of deaths. My point being is that even free press can spread bad ideas and narratives - in fact, it frequently does. And those ideas should be able to be challenged, at any time, by any source. reply pastage 15 minutes agorootparentI do not understand how supporting Ukraine can lead to more suffering, there is a long history of implerialism causing suffering so there is a rather large opposition against it in the west. What Russia does to Ukraine is pretty gruesome, it is not even considered a country in russian intellectual discourse that is a bad sign. When we talk about the press and politics these kind of cynical tones are not helpfull. The WMD issue was heavily discussed and debated. The same as Gaza is now, it is interesting what issues manage to be changed by public opinion and what that really is, this is not only an issue with the press. People and groups tend to think that their own opinions should matter, that is only true if you managed to get enough people to care about your opinions and even then you might need a referendum. reply mrtksn 50 minutes agorootparentprevwhat a starange statement about the counterattack, as if there was a definite knowledge about the future and the Western propaganda suppressed it. They attempted at taking back their own lands and they failed. The same people claiming to know the future also used to claim that Russia would not attack and later that Ukraine will fall in matter of days. Maybe just stop listening to fortune tellers. reply raxxorraxor 10 minutes agorootparentprevIt isn't and shouldn't be different in war time either. If you allow this exception, you will just always be at war. Censorship for security reasons isn't a new concept. reply guappa 50 minutes agorootparentprevIs italy at war? I must have missed the memo. reply mrtksn 48 minutes agorootparentIt’s not at war, that’s why I don’t support such a thing. reply logicchains 1 hour agorootparentprev>I’m also quite concerned about the Western world adopting the Chinese style of governance but censorship at wartime is very different from censorship at peace time If you allow this then the government will just forever find excuses to be at war. reply mrtksn 53 minutes agorootparentWe live in a broken world, there are no perfect solutions. But I would argue that staying alive comes as a primary priority. reply guappa 49 minutes agorootparentAh yes the \"we must kill them, to stay alive\" narrative. It has never been abused in the whole history of mankind. reply mrtksn 46 minutes agorootparentWhat are you even talking about? Of course those attacked must kill the attackers to stay alive. reply mda 2 hours agorootparentprevOne of them is 100 times more extensive. Ideal is to have none, but lets not pretend that magnitude and severity is not relevant. reply cies 1 hour agorootparentRelevant to the people living in it. Not relevant to the ideals set forth, which our moderns free democracies are based on. This is fundamental, so it's actually worse than when it happens in China. reply lupusreal 1 hour agorootparentprev> china isn't a democracy because they have censorship, What the fuck even is this? That's like saying Pizza Hut isn't a hamburger shop because they don't sell french fries. They're not, but that's not the reason! reply kragen 9 hours agorootparentprev> That's correct and IMHO [censorship is] the right thing to do when shooting begins it's been a common observation since aischylos that the truth is the first casualty of war it's quite different to assert that the truth should be the first casualty of war, which seems to be your position reply mrtksn 2 hours agorootparentI don’t say that the truth should be this or that, all I say is that information flow is part of the warfare and the parties at war will want to control it for their advantage. After all, they are literally killing people. reply dlubarov 4 hours agorootparentprevFew questioned Ukraine's decision to ban RT's operations. This doesn't seem terribly different - Al Jazeera has very deep ties to Hamas, the enemy Israel is at war with, and has arguably been acting as their mouthpiece for propaganda. reply int_19h 3 hours agorootparentIt should be noted that Ukraine has banned far more than just RT. https://en.wikipedia.org/wiki/Freedom_of_the_press_in_Ukrain... reply kragen 3 hours agorootparentprevi did and do. banning your citizens from reading the 'propaganda' of your enemy reduces them to subjects and foments further armed conflict. it's also a power invariably used to avoid accountability for petty corruption such as beria's garden full of young, beautiful women reply kristiandupont 1 hour agorootparentIs the fact that you put \"propaganda\" in quotation marks an indication that you don't consider RT to be that, or that you don't think propaganda even exists? reply AlecSchueler 31 minutes agorootparentI think the point is that one side's propaganda is another side's information. reply kristiandupont 10 minutes agorootparentAs in \"there is no such thing as true or false, only opinion\"? twixfel 2 hours agorootparentprevYes RT was doing wonderful journalistic work in Ukraine and elsewhere... Lol... Poor Ukrainian subjects not getting to read that shite. It was abject shite 15 years ago, let alone today. reply kragen 2 hours agorootparentif it was abject shite nobody wanted to read or watch, they wouldn't bother to ban it reply _djo_ 1 hour agorootparentPropaganda is still dangerous, and a country at war has no obligation to allow the country it is at war with to continue to openly spread it. Banning RT makes sense in that context. I don't see the banning of Al Jazeera in the same light, as it's not like it's a Hamas-run outlet. reply twixfel 44 minutes agorootparentprevA country at war has no moral or legal obligation to publish its enemy’s propaganda. This should be very obvious. And yes RT was always complete shite for gullible morons, though it is rather beside the point in this case actually. The British government had no moral obligation on free speech grounds to allow the spread of nazi propaganda in the United Kingdom during WW2, either. Again, obvious stuff. reply twixfel 2 hours agorootparentprevRT is not truth though. When a country is fighting for its very survival against a genocidal foe, I'm not about to start clutching my pearls that they've banned their genocidal foe's propaganda. Seems reasonable to me. reply Loquebantur 11 hours agorootparentprevI find it downright perverse to call genocide \"retaliatory\" and the act of covering it up \"normal\". reply mrtksn 11 hours agorootparentnext [2 more] [flagged] kelnos 10 hours agorootparentI think the issue is that you are saying things are \"normal\" without addressing whether or not they are right and good. I would probably agree with you that it's normal for a country at war to want to control information flow. But that's absolutely a bad thing! Obviously not all \"information\" is created equal: some of it is high-quality and objective, and some of it is lies (and everything in between). I don't trust pretty much any government entity to be objective about these sorts of bans, though. The ultimate consequence of these sorts of things is a poorly-informed, propagandized public. reply ipaddr 7 hours agorootparentprevTo parse your statement we need to understand what genocide means to you. For most it means the systemic killing of every person of a certain genetic identity. Is that what you believe is happening or something else? If genocide is illegal than covering up would be the only logical move. Therefore it would be normal. reply runarberg 6 hours agorootparentGenocide has a precise definition and has been codified in international law. I believe this internationally recognized definition mirrors what most people mean when they use the term. It does not necessitate the systemic killing of every person of a certain genetic identity. Genocide is outlined in the Genocide convention from 1948[1]. It is short so I’ll give you the whole definition here: > In the present Convention, genocide means any of the following acts committed with intent to destroy, in whole or in part, a national, ethnical, racial or religious group, as such: > (a) Killing members of the group; > (b) Causing serious bodily or mental harm to members of the group; > (c) Deliberately inflicting on the group conditions of life calculated to bring about its physical destruction in whole or in part; > (d) Imposing measures intended to prevent births within the group; > (e) Forcibly transferring children of the group to another group. Genocide is illegal under international humanitarian law, there is no justification admissible for the crime of genocide. It is not normal to cover it up. Israel is currently being investigated by the ICJ for the crime of genocide. Israel has argued that whatever it is doing in Gaza is not genocide. 1: https://en.wikisource.org/wiki/Convention_on_the_Prevention_... reply edanm 3 hours agorootparentI think it's fairly clear that given this definition (which is the same one I always reference), Israel isn't committing a genocide. If you do think Israel is committing a genocide, I think one thing you have to do is demonstrate how what Israel is doing is different from any other war (e.g. war on ISIS, Afghanistan, Iraq as obvious examples). The statute makes the difference pretty clear - it's the intent to kill some (definable) part of the members of the group. This is not the case with Israel given its current actions and lack of actions; it could kill far more people if it decided to, militarily speaking. (I say this not because Israel deserves any \"credit\" for not killing more people, obviously, only to make it clear that the reason more aren't killed isn't because of lack of capability, but because of lack of desire to kill more). Of course, you might disagree with me. If you don't have some kind of way to distinguish between what Israel is doing and what e.g. the US did in Iraq, you can just bite the bullet and say that all wars are genocide. That would be a consistent POV, but that would also effectively render the concept of Genocide meaningless. reply antman 2 hours agorootparentI cannot understand your key points that this is a) not genicide, b) it is simply what the US was doing all these years c) Israel kills acts with self constraint not imposed by others Israel is actively doing most of the points above against an effectively unarmed and blockaded group of people. US was fighting against actual armies whichever their quality. Israel claims “hamas” and kills indiscriminately, there is no footage of “hamas” army with any heavy military equipment, israel actively causes famine, destroys all hospitals, creates mass graves that have victims with hands tied behind their backs. Israel official claim their desire to kill everyone. They simply cannot do it immediately because they are doing it with western financial and military support which would evaporate because you can only do propaganda so much. https://news.un.org/en/story/2024/04/1148876 reply edanm 1 hour agorootparent> Israel is actively doing most of the points above against an effectively unarmed and blockaded group of people. US was fighting against actual armies whichever their quality. You are just factually wrong on many of your points. It's true that Hamas isn't a traditional military with heavy equipment, but they are a 30k strong insurgent group that has had years to plan their defense. They've built tunnel complexes that are said to be larger than the NY Subway and hide in them, coming up to ambush soldiers. If your view of what is happening is that the IDF is going around shooting at civilians, then you're just incorrect about what is actually happening on the ground for the last many months. If you look at videos that Hamas themselves post, you can see them constantly attacking soldiers, collapsing buildings on soldiers, placing munitions on tanks to blow them up, etc. > [Israel] destroys all hospitals, Absolutely not true. Israel hasn't destroyed hospitals, definitely not all of them, despite this being commonly claimed. There was one hospital that saw a week of fighting between Hamas and the IDF. After that week, much of it was destroyed. This btw goes against your point that Hamas is effectively unarmed. But while most other hospitals have seen attacks around them and many have been ordered evacuated, they aren't destroyed. (Some are damaged, to be fair - but hospitals are pretty big, and there's a world of difference between \"some hospitals have been damaged\" and \"Israel has destroyed all hospitals\".) > israel actively causes famine I think Israel has acted horribly around humanitarian aid, yes. This has largely changed recently, thankfully. > creates mass graves that have victims with hands tied behind their backs. This was recently reported and hasn't been investigated. Many things later turn out to not be what was claimed by the Gazan authorities (Hamas) who are playing a disinformation campaign. Israel says this mass grave was made by Palestinians. Neither you nor I know the truth of this. I highly doubt it was Israel, if those people are civilians. If it was Israel, that would most definitely be a war crime as far as I can tell. > Israel official claim their desire to kill everyone. Not true, and I've talked about this in another comment in this thread. > They simply cannot do it immediately because they are doing it with western financial and military support which would evaporate because you can only do propaganda so much. OK. But that's an unfalsifiable statement. You can always say that Israel is \"just about\" to do more. Do you think Israel has more support now or 6 months ago right after the October 7th attacks? I think it has far less support, which was entirely predictable. So why wait so long? What kind of evidence would convince you that Israel doesn't want to engage in genocide, if not doing it when it had more support isn't strong enough evidence? reply NomDePlum 2 hours agorootparentprevThe two wrongs make a right argument. There isn't a single item on that definition that hasn't been reported and evidenced on numerous times by the limited press coverage. To bring the conversation back to the article. The argument that it could kill more is ridiculous. Israel is clearly killing as many as it believes the international community will let it, without becoming a pariah state. Deliberately, and indiscriminate killing or maiming of 5% of a population is not trivial. I find it difficult to classify what is happening as a war. The disparity in power, control and access to military and other means is to disparate. Like Iraq, Afghanistan, etc it's transparent one side is doing it because they can, and without regard for anything but their own satisfaction and revenge. reply int_19h 3 hours agorootparentprevThere were quite a few statements by key people in the current Israeli government that demonstrate clear intent for genocide. As to why they don't massacre every living Palestinian in Gaza if they really want to do so - Israel still depends significantly on external support, most notably from US, but also from European countries. Thus even if intending to commit genocide, they have to do so in a plausibly deniable way. reply edanm 2 hours agorootparent> There were quite a few statements by key people in the current Israeli government that demonstrate clear intent for genocide. There were a few statements, mostly made very early in the war, most of them ambiguous. These are horrible, but fairly similar to most war-time propaganda in most countries. They're also dwarfed by the many, many statements almost all of them made that quite explicitly clarified that that isn't what they want, and that the only goal is to remove Hamas while trying to minimize harm to civilians. Btw, this is less true of ethnic cleansing - there is a minority, but influential, part of the government that is, at the very least, hinting strongly at ethnic cleansing. I find it despicable and am convinced the majority of Israelis would never go along with this, but those statements by those (despicable) \"leaders\" are recent. > As to why they don't massacre every living Palestinian in Gaza if they really want to do so - Israel still depends significantly on external support, most notably from US, but also from European countries. Thus even if intending to commit genocide, they have to do so in a plausibly deniable way. This is an unfalsifiable statement. People have been claiming for most of my life that Israel is either committing genocide, or wants to, and is only held back by foreign powers. A genocide hasn't occurred so far, and I believe very strongly that Israel will never do so. But you can always say \"oh well, they just can't because other people are keeping them in check\". OK - so what kind of evidence would convince you that that's not true? reply boffinAudio 2 hours agorootparentprevIn fact, the US is guilty of genocide as well - it just has a far more effective media control apparatus, which shields its citizens from the outrage they'd experience if they really knew and understood just how responsible they are for such atrocities as, the funding of ISIS, the destruction of Mosul, the destruction of Raqqa, the destruction of Libya, the military support of the genocide of Yemen, and .. on and on. So yeah \"the bigger bully also kills people\" might be an effective thought-blocking argument, but that is only the case because that bully has been effectively thought-blocking any inspection of its war crimes by the people, who ultimately pay for them. Yes, the US should face justice for its war crimes, crimes against humanity, and so on. No, it won't face justice because, instead of frog-marching its war criminals to face justice in The Hague, it has plans to invade The Hague, instead. Those who support Israels massacre of innocent Palestinians need to be very, very careful about the association with bigger bullies. Just because your allies got away with genocide, doesn't mean you will. (See also: Australia) reply xenospn 4 hours agorootparentprevThe US has killed over 400,000 Iraqis since invading Iraq. Do you think that qualifies as genocide? reply klyrs 4 hours agorootparentSpeaking as an American: yes. Bush and Obama are war criminals and I'd say that they belong in Gitmo but I'm more principled than that and we need to shut Gitmo down. reply _djo_ 1 hour agorootparentprevSource? None of the organisations tracking Iraqi civilian deaths that have broken down figures by cause show that number caused by US forces directly. If you're including indirect causes too, such as a rise in sectarian violence, deprivation, and increased criminality then, yes, but that's a different statement. reply pasabagi 2 hours agorootparentprevThat's a built-in failing of the genocide definition: it requires intent, otherwise no doubt the US would have been on the hook as early as the Korean war. Sadly for the Israelis, they have a cabinet of the kind of people who just cannot help themselves from communicating intent. reply boffinAudio 2 hours agorootparentAre you familiar with the fine words of Madeleine Albright and her cohorts in PNAC, who very clearly demonstrated the intent to massacre hundreds of thousands of Iraqi's, mostly children, and then proceeded to do so? Just because the US got away with genocide doesn't mean any other nation should. The American people should be jailing their own war criminals, and then go after those of Russia and Israel and the UK and Ukraine and so on. However, war crimes are good for (American) business. See also: the military support of the genocide of Yemen by a known fascist totalitarian-authoritarian dictatorship. reply hardlianotion 1 hour agorootparentI am not, and I have no idea what PNAC is. I think I am probably representative. Will you explain what you are talking about? reply actionfromafar 26 minutes agorootparentSome explanation here: https://en.wikipedia.org/wiki/Madeleine_Albright#Sanctions_a... raverbashing 3 hours agorootparentprevIt isn't The context of the convention needs to be understood in the general context and especially in the context of a war (though of course they don't \"exist\" only in that context). And wars are awful. (not saying what Israel is doing is correct or even adequate - but generalizing terms helps nobody) reply Aeolun 1 hour agorootparentprevJust because it’s normal doesn’t mean it’s morally correct. If they had nothing to fear from it they wouldn’t do it. Israel fearing anything from Hamas is laughable, so what they really fear is their own citizens. reply mrtksn 58 minutes agorootparentThey have a lot to fear from their citizens knowing that what they hear from their officials is not all the truth. Some time in the future, when someone mentions the Jewish genocide, people will ask if they mean the one happened to them or the one they did. Obviously the current Israeli government will want to control the information flow so they can proceed with their final solution to the Gaza issue. They will also try to control what comes out of Gaza to avoid the consequences of their actions. reply medo-bear 3 hours agorootparentprevIt is at least the biggest insult to your adult citizens to say to them, \"you are not allowed to read this because you are too stupid.\" reply mrtksn 2 hours agorootparentI don’t think it’s that at all, it’s about controlling the narrative for your advantage. You might be lying as well, be doing horrible things and you don’t want the people you control to know that or to think that way. It’s not a noble thing, it’s a war weapon. reply medo-bear 2 hours agorootparentI dont think the EU is at war reply mrtksn 1 hour agorootparentIs taking sides in a war though. That’s why the Russian outlets push talking points. This can affect the public support on war efforts issues, which is fine only before shooting start because when no one is shooting arguments(which can be fair ones or designed to achieve something like withholding weapons deliveries) can be addressed in timely manner. Whatever problems a website or an app is causing can be solved through civil means, but at a war time irreversible damage using deadly weapons can be done before even realizing it. reply actionfromafar 1 hour agorootparentprevSomeone is blowing up military facilities in the EU though. So if at war or not is starting to be open to interpretation. reply medo-bear 1 hour agorootparentWhats open to interpretation? If sabotage = war we would have seen ww3 several decades ago. reply Supermancho 11 minutes agorootparentIt is not clear that the pipeline was an act of war, but acts of war can be that simple. \"WW3\" is a conceptualization of existential fear, rather than a singular event. The world rallying to one of two sides to make war (or neutrality), is highly unlikely. Aerbil313 5 hours agorootparentprev> That's correct and IMHO its the right thing to do I'm in awe. I'm pro-censorship myself as a general principle (though not in this obviously unjustifiable instance of it), but please be coherent. How come the current semi-popular opinion on HN happens to be pro-censorship, when HN community is against censorship in every other situation? reply edanm 4 hours agorootparent> How come the current semi-popular opinion on HN happens to be pro-censorship, when HN community is against censorship in every other situation? The answer to most questions like this is: there are many different users on HN. You might think the \"general consensus\" of HN is one thing, but you're talking to a specific person commenting on a specific article, who has their own views, not to a generic \"HN user\". And note that different threads will have a different makeup of users clicking on it. E.g. I wouldn't necessarily be interested in a random \"censorship\" thread, but this one is about Israel, so (as an Israeli) I am interested. Since this logic probably extends to other users as well, that will give the thread a specific bias, depending on its subject matter and framing. reply mrtksn 4 hours agorootparentprevI’m not pro-censorship at all. I just think media is part of the warfare and it’s normal to try to control it during a war. I am anti war in first place, if you don’t want censorship don’t start a war. reply Supermancho 9 minutes agorootparentAs a complete aside, I enjoy your posts. Cheers. reply guappa 2 hours agorootparentprevBecause when \"israel\" is in the title, some accounts are unshelved to steer the conversation. reply hardlianotion 59 minutes agorootparentprevIf you believe that RT is an organisation that is not interested in the truth, but is set up purely to disrupt and disturb, then a government can reasonably want to prevent its operation. Adding plausible noise to information causes people to have to do much more work to discern between what is true and what is not, time that many people do not have. A reluctance to ban a bad-faith organisation is good: a moral society should thoroughly debate why it might undertake a repressive thing. But you cannot wish away the effects of corrosive and coercive behaviour because the act of banning a such an organisation is repressive. A poor but useful analogy is use of violence in society. Violence is a bad thing, but to absolutely forswear it in all situations is something that very few governments will do, for reasons that seem quite justified to me. reply awuji 40 minutes agorootparentThere are two things wrong with this argument. One, it implies that there isn't a better way to deal with an such a malicious organization. And two, it doesn't acknowledge how such a ban creates an obvious opening for abuse. Holding up the classic Western ideals of Democracy and Freedom is hard because it is much deeper than simply giving people the freedom to access all information so they can form their own opinions. It also requires that these people are educated and trained to be competent critical thinkers and be able to intelligently form their own opinions. It holds its citizens and the government to high standards and will collapse if these standards aren't met. Accordingly, better education and trust in citizens is the better solution, not banning. As for the obvious opening for abuse, it doesn't have to be said that every system will eventually be maximally exploited, and creating this opening for exploitation will eventually be exploited as well. It is just a matter of time... Another poor but useful analogy is fast food. Banning bad media is like consuming fast food. It is quick and easy, and \"satisfies\" the goal within some basic parameters, but it really does more harm than good in the long term. reply hardlianotion 18 minutes agorootparentThe argument in no way implies that there is no other way to deal with malice, only that it is an option. The argument further implies that the decision to censor should not be taken lightly. When censorship is being considered in a democratic society, the decision to do so must be argued and debated. Note that it is perfectly possible to be well-educated and still be taken in by bullshit and false information - it happens all the time. Education is a good, not a nostrum, and durable opponents of truth are also motivated, sophisticated and smart. Democracy is about the means you use to undertake drastic decisions, and in no way rules out the restriction of unseemly behaviour. Maybe explain why my analogy is not useful - I’ll do the same for you. What I initially said did not in any way imply banning should be quick or easy, while “fast food” is not, in itself, bad for you, but a restricted and monotonous diet of anything can well be. reply patall 12 hours agorootparentprevDo you have examples for Europe blocking Russia? Because all I have seen is DNS providers omitting certain sites (i.e RT), but their apps still work (plus URLs when using other DNS). An nothing of that coming from the nation states as all seems to be due to the activities of private companies doing these things. reply jdietrich 11 hours agorootparentRT, Sputnik and related Russian state media outlets are subject to sanctions in the EU, their broadcast licenses have been revoked and their channels have been removed from terrestrial, cable and satellite broadcasts. Their accounts on all major social media platforms are blocked. Their apps are no longer available on the Google or Apple stores. Europe doesn't have a Chinese-style Great Firewall, but EU countries have taken every reasonable step to prevent Russian state media from reaching EU audiences. https://www.politico.eu/article/russia-rt-sputnik-illegal-eu... https://www.ofcom.org.uk/news-centre/2022/ofcom-revokes-rt-b... https://www.washingtonpost.com/technology/2022/03/01/youtube... https://www.reuters.com/technology/exclusive-google-blocks-r... reply medo-bear 3 hours agorootparent> Europe doesn't have a Chinese-style Great Firewall, but EU countries have taken every reasonable step to prevent Russian state media from reaching EU audiences. Because they think the EU audiances are stupid? reply guappa 2 hours agorootparentBecause they don't like anti-government point of views to become too widespread. In Italy the current defence minister is personally earning considerable amount of money by sending weapons to Ukraine, because he's also an owner and manager of companies that make said weapons. Do you think they'd want to favour open discussions? No. In fact besides RT being banned, they want to be able to block any website, giving providers 30 minutes to implement the ban, no appeal and no oversight from a judge (https://stop-piracy-shield.it/) reply liopleurodon 11 hours agorootparentprevIt's part of the EU sanctions, EU ISPs are required to block certain Russian sites. But they didn't specify how, that's left up to the countries to figure out afaik. But as you say, some of the what has been done barely qualifies. Here's my personal experience with this: Germany does exactly what you describe, the bare minimum to say \"we're blocking\" --- DNS omitting certain sites. Spain is doing deep packet inspection, blocking DNS requests that lookup RT, so DNS over HTTPS or through a VPN is a must. Additionally, they're also reading the SNI in TLS requests and blocking that way. If you try accessing RT in pure unencrypted HTTP you're get some fortigate blocking message back. reply Maken 11 hours agorootparentIt is still perfectly possible to access RT from Spain, even using regular ISP DNS servers. reply guappa 2 hours agorootparentprevmullivad has a free to use DNS over https service by the way. reply angra_mainyu 11 hours agorootparentprevrt.com and their twitter are blocked. How are you not aware of this? It's been a few years already. The EU ruled on this. https://paste.pics/4b60ebef97d3b7fbb6aba74637c2e818 https://paste.pics/0cb2ae98754165dad5cf086e75c4cc31 reply Hock88sdx 9 hours agorootparentMany using VPN. For example if you set the VPN servers to HK, RT will display as usual. In general I notice my peers will use anti-west countries based servers for censored western news and the reverse for anti-east. Some do use it so intuitively they might not realized RT or any Russian sites blocked. A lot of time I just assume it is due to network outages. reply distances 2 hours agorootparentI would think most people know about RT blocking because it was widely announced and discussed at the time, and not because they actually tried to access RT. reply jurmous 1 hour agorootparentprevI am in the Netherlands and I am able to visit rt.com without VPN. reply sdk77 8 minutes agorootparentMe too. I didn't even know about this blockade. I'm using T-Mobile fiber internet and rt.com loads just fine. reply AlecSchueler 25 minutes agorootparentprevNetherlands here also and it's blocked for me. reply cassianoleal 12 hours agorootparentprevAt least in the UK, you used to be able to watch RT on broadcast. Now only the Internet version is accessible, and I think some ISPs DNS block them. Granted, a DNS block is easy to circumvent if you understand it, but most users will still be cut off. reply Scoundreller 11 hours agorootparentAs a total outlier, RT was paying the cable/sat systems to be carried in US/Canada, instead of the other way around. https://www.theglobeandmail.com/opinion/rts-purchase-of-cana... https://www.wsj.com/articles/rt-channels-unique-carriage-dea... I wonder if other countries were the same. reply TylerE 11 hours agorootparentThat’s not really an outlier. Common for political networks, religious channels, shopping networks…. Probably more than half the channels on a typical American cable system are paying to be there. Especially the stuff in the basic tiers. reply jimbobthrowawy 6 hours agorootparentprevWas it actually broadcast terrestrially? I remember picking it up from Astra2 (the \"skytv\" satellite) but looking online, it's not transmitted anymore. reply riffraff 6 hours agorootparentprevThe main difference would be that Al Jazeera is from Qatar and not Gaza, this would be the equivalent of the EU banning Armenia because they are friendly with Russia. reply dlubarov 4 hours agorootparentBanning Armenia isn't really an apt comparison since Israel isn't banning all media from Qatar, just one particular news organization which has very deep ties to the enemy they're at war with. reply guappa 2 hours agorootparentThey're only banning the important one :D reply Al-Khwarizmi 3 hours agorootparentprev> Europe has done the same with Russia. Yes. Which I find abhorrent as an European, and has made me realize even more than our countries are much less democratic than we officially paint them as. reply guappa 2 hours agorootparentNono, it's only undemocratic when china does it. When the italian government wants to be able to block any website immediately and with no appeal, calling it an anti-piracy measure, it's completely fine and democratic. reply ClumsyPilot 9 hours agorootparentprev> Europe has done the same with Russia. Not really. Top Russian officials, say Dmitri Medvedev, are still shitposting on twitter: > Macron preparing to go to kyiv? But he's a zoological coward! Most Russian news websites are still up, say vesti.ru And Russia is active participant in a war, while Quatar is not bombing anyone. reply objektif 8 hours agorootparentAlso many european countries are not actively fighting a war. I do not really think it was the right decision to ban RT what are they scared of? reply mynameisnoone 7 hours agoparentprevApartheid South Africa also tried to ban news media. It's a sign of extreme desperation and signals political vulnerability. Hopefully, Israel will get elections soon, and center- or left-leaning coalition government will pursue different foreign and military policies that a significant majority of Israelis prefer over the absurdity of the militarists and militant settlers. https://truthout.org/articles/media-and-the-end-of-apartheid... EDIT: Haaretz is a good and honest news source that has been extremely critical of the current leadership. Also recommended documentary film: https://en.wikipedia.org/wiki/The_Gatekeepers_(film) reply verdverm 13 hours agoprevReporters Without Borders gathers data and produces some interesting graphics. They recently released their World Press Freedom Index https://rsf.org/en https://rsf.org/en/country/israel --- edit: they appear to keep a list of mirrored news sites to circumvent censorship https://github.com/RSF-RWB/collateralfreedom (was hoping they had data available for their index, but have not found it yet) --- edit: the index has a download button in the bar at the top of the map https://rsf.org/en/index It does not provide source data, just the calculated results presented. There is also a methodology link, which points to different pages, depending on the year selected reply Synaesthesia 10 hours agoparentNot only does Israel target journalists, but it targets the families of journalists. Nobody is immune not even the bureau chief of Al Jezeera. reply oefrha 4 hours agorootparentDamn that was a difficult read. https://en.wikipedia.org/wiki/Wael_Al-Dahdouh 2023-10-28: Wife, 15-year-old son, 7-year-old daughter, a grandson and seven other relatives killed in strike on refugee camp. 2023-12-15: Wounded by missile during filming, cameraman fatally wounded and bled to death with ambulances cut off. 2024-01-07: Eldest son, a journalist, killed in air strike. 2024-01-08: Two nephews killed. 2024-01-16: Evacuted to Egypt/Qatar with family. Probably would have been dead by now or with more dead family members if he remained. reply jajko 26 minutes agorootparentAs much as it hurts even writing it, I don't think its a rare story there these days. History will not look kindly at both sides here, they really have no higher moral ground than the other reply loceng 13 hours agoparentprevFTL: \"... while more than 100 journalists were killed in six months in Gaza by the Israel Defence Forces (IDF) ...\" Anyone know where to find what the current accurate count of number of journalists killed in the Gaza bombardment to date? Last I heard it was 170. There are also journalists who lived but their whole family died in the strikes. reply bawolff 12 hours agorootparentDo these numbers distinguish between journalists killed while doing journalism vs journalists killed as collateral damage not in the capacity of a journalist vs combatants who were journalists prior to picking up a gun and joining the war? I feel like its very hard to draw any conclusions from these numbers without distinguishing between those cases (other than of course that war is a tragedy and innocents generally pay the price of war). reply loceng 10 hours agorootparentIsn't it any less worse that 170+ journalists were killed among the 30,000+ civilians? And no, it's unlikely these are journalists who picked up a gun - the IDF is dropping up to 2,000 Lbs bombs, and that's possibly how many are killed; however there have been many reports of journalists homes being targeted in direct strikes, at least one journalist I heard who's wife and children/whole family were killed but he wasn't. It's not surprising in reality if it's 170+ journalists amongst 30,000+ killed. There are videos of IDF soldiers harassing and brutally assaulting journalists, if that's any indication of the rules of engagement they subscribe to; I'm obviously not linking to any here on HN - this topic is already a minefield. What HN is missing is going one layer up, where all of the various points brought up are catalogued-organized - and then allow a second round of discussion, perhaps with a more sophisticated UI/UX for people (from \"both\"/all sides) to contribute things like citations to evidence/data support the claim/statements, etc; and then those cited sources and data could also then be another layer deeper to constrain or contain further contextualized-narrowed conversation to the analysis of those sources. reply xdennis 8 hours agorootparentprevnext [4 more] [flagged] jquery 4 hours agorootparentI did not read the whole thing, I read far enough to realize the methodology was garbage. The article basically says anyone ever associated with Hamas or resisting the IDF is a legitimate military target. Let's try switching that around. Anyone who served in the IDF or spoke positively of a successful attack of Israel against Palestine is a legitimate military target and not a journalist? No, this is obviously nonsense. I'm pretty sure at this point, there's almost nobody left in Gaza that hasn't said a few epithets towards Israel... that doesn't make them military targets. Al Jazeera getting banned tracks with the current Israel government not being a huge fan of journalism. reply vdsk 1 hour agorootparent> Anyone who served in the IDF or spoke positively of a successful attack of Israel against Palestine is a legitimate military target and not a journalist? This very specific point is made almost always by Hamas and co to justify indiscriminate firing of rockets into populated areas. \"No civilians because everyone has done military time or is a reservist, even women\" is a very, very common point. reply indigo945 1 hour agorootparentprevYou conflate average Gazans who \"said a few epithets towards Israel\" with those who \"promoted and celebrated terrorism and the death of innocent civilians\". reply verdverm 13 hours agorootparentprevIt is unlikely that reliable numbers will come out of Gaza with the media blackout and two sides who both want to present information \"favorably\" At least until the war has subsided and independent orgs can gain access. reply no_exit 12 hours agorootparentGaza Health Ministry numbers are a reliable floor, as confirmed by numerous organizations like the US State Department. The real count probably above 100k dead so far. reply verdverm 10 hours agorootparent100k would be well above any estimates I've seen from any source This is the best meta-analysis I've seen: https://www.washingtoninstitute.org/policy-analysis/gaza-fat... Here is another that questions the statistics of the underlying reported data: https://www.telegraph.co.uk/global-health/terror-and-securit... reply TiredOfLife 16 minutes agorootparentprevI think you mean ceiling. reply YZF 10 hours agorootparentprevI don't think this is true. Can you refer to the US State Department official position on this? It's true that in previous conflicts the numbers were in the ballpark but the breakdown between e.g. combatants and civilians was not. This conflict is very different so I don't think we should be extrapolating. https://www.fdd.org/analysis/2024/04/09/hamas-run-gaza-healt... reply weatherlite 3 hours agorootparentprevNo 10 millions at least reply Qem 11 hours agorootparentprevhttps://en.m.wikipedia.org/wiki/List_of_journalists_killed_i... reply amitport 6 hours agorootparenthttps://news.ycombinator.com/item?id=40270344 reply YZF 10 hours agorootparentprevLike anything coming from Gaza I have some doubts about how legit this data is. This list has 96 total in Gaza since the beginning of the war. So not 170. Many of the names use this reference: \"97 journalists and media workers were confirmed dead: 92 Palestinian, 2 Israeli, and 3 Lebanese.\" https://cpj.org/2024/05/journalist-casualties-in-the-israel-... Is media worker always a journalist? The list seems in disagreement with the other list. Also if you work for Hamas or Palestinian Islamic Jihad are you a journalist? Where does that line pass? Are IDF media people journalists or are they soldiers? Either way, even by the most generous interpretation not everyone on that list is a journalist. e.g. a \"a sound engineer working for the Gaza’s Hamas government owned Al-Rai radio and freelancing for other local radio stations\" is not a journalist. I would trust the list a lot more if there was a reference to their work. Are they published journalists? The list says nothing about what media they worked for. Random e.g. on the list uses Al Jazeera as a reference: https://www.aljazeera.com/news/liveblog/2024/1/8/israel-war-... \"Gaza’s Government Media Office has confirmed the deaths of two more Palestinian journalists – Abdullah Baris and Muhammad Abu Dayer\" A journalist should mean \"a person who writes for newspapers, magazines, or news websites or prepares news to be broadcast.\", is there a list that includes what newpapers, magazines, or news websites those people worked for? I drilled down the wikidata reference link from the article. Some of the people have some references to them being journalists. Others don't. EDIT: Btw the list of clarifications in the bottom of the CPJ source is also interesting. \"CPJ has removed two Israeli journalists, Shai Regev and Ayelet Arnin, from this list after their outlets confirmed that the journalists were not on assignment to cover the music festival, nor did they have any opportunity to begin reporting on the attack by Hamas militants that killed them on October 7. CPJ’s global database of killed journalists includes only those who have been killed in connection with their work or where there is still some doubt that their death was work-related.\" I don't see any evidence that the Gazans reported killed were killed in connection with their work. \"After receiving reports that Palestinian journalist and presenter Alaa Taher Al-Hassanat may have survived the attack thought to have killed her, CPJ has removed her name from its casualties list pending further investigation.\" - so people are sometimes reported killed but aren't. \"CPJ has removed a Palestinan man, Mohamed Khaireddine, from this list. Khaireddine was previously identified as a journalist, but his family later clarified that he was neither a journalist nor a media support worker.\" - so we have people reported as journalists by someone ... but aren't. also: \"The list below is CPJ’s most recent and preliminary account of journalist deaths in the war. Our database will not include all of these casualties until we have completed further investigations into the circumstances surrounding them.\" EDIT2: This is a more in-depth analysis of the specific journalists and their affiliations from the Israeli side: https://www.terrorism-info.org.il/app/uploads/2024/02/E_013_... reply bentley 13 hours agoparentprevI find it difficult to tell how their reports translate to objective numbers. For example, the United States’ ranking fell from 45 to 55 in the last year. Here are the reports for those years: https://web.archive.org/web/20230817030548/https://rsf.org/e... https://web.archive.org/web/20240505202537/https://rsf.org/e... As far as I can tell, the only negative differences between these two reports are that a reporter was killed while investigating a murder by the murder suspect (who is now in jail and on trial), and that Biden “has come under criticism for failing to press US partners like Israel and Saudi Arabia on press freedom.” Falling ten places is a significant change (and is called out in the preface to the whole report)—are these two things really enough to justify such a change, or is the ranking sourced from more data not present in the report? Here’s another story about Reporters Without Borders, about the first time I dug into one of their publications. In 2018, I read a report they published listing the six most dangerous countries for journalists: India, Yemen, Mexico, Syria, Afghanistan, and the United States. It described how in Mexico journalists are executed by cartels and organized crime, how journalists in Yemen die in prison due to mistreatment, how in Syria journalists were killed in airstrikes and taken hostage by Islamic militants, how in India Hindu nationalist mobs would run down journalists with trucks… and how in the US, four journalists were murdered by a stalker angry at a 2011 story the newspaper had published (subsequently jailed, tried, and found guilty of mass murder); and two more were killed by a falling tree. Somehow these two cases were enough to warrant the United States being called out with the other five countries. And it made the headlines everywhere, of course, because it was the midst of Donald Trump’s presidency. reply jeswin 1 hour agorootparent> how in India Hindu nationalist mobs would run down journalists with trucks Sorry I might have missed this. I live in India, and I've never heard of nationalist mobs running down journalists (plural?!) with trucks. Do you have any links to share? Google doesn't give me anything. reply burkaman 12 hours agorootparentprevIt's a ranking, so presumably part of the US dropping is due to other countries improving. There is another major negative change noted though - more newspaper closures and huge layoffs at news organizations. It also sounds like the Sociocultural section might be partially based on polling of trust in media, which could have dropped, but I don't know where to look into that more. The 2018 report you're talking about is here: https://rsf.org/sites/default/files/worldwilde_round-up.pdf. The list is not the most dangerous countries for journalists, but the most deadly - a straightforward measure of how many journalists were killed in each country. They publish this every year and the US is usually not on it, but this year someone murdered 4 journalists because of their reporting. I'm not sure how they could make this more objective, and I can't think of any metric that would include murders committed by angry men in cartels or angry men with SUVs in India but not angry men with shotguns in America. Obviously the falling tree is not reflective of the journalistic climate in the country, but if they had been the only two the US would not have been listed. The mass shooting is what put it within the same neighborhood as Mexico and India. Here's the latest one of these, which as usual does not feature America: https://rsf.org/sites/default/files/medias/file/2023/12/Bila.... reply bentley 12 hours agorootparent> It's a ranking, so presumably part of the US dropping is due to other countries improving. Is that the case? Do the other countries’ entries in the report reflect that? > I can't think of any metric that would include murders committed by angry men in cartels or angry men with SUVs in India but not angry men with shotguns in America. One such metric would be whether a country’s justice system arrests and puts the perpetrator on trial, as happened with the American murder and presumably didn’t happen in the case of Mexican cartels or the Indian mob. reply burkaman 11 hours agorootparentYes, the following countries improved their scores between 2023 and 2024 and passed the US (71.22 -> 66.59) in the ranking: Chile (60.09 -> 67.32), Ghana (65.93 -> 67.71), Poland (67.66 -> 69.17), Fiji (59.27 -> 71.23), Armenia (70.61 -> 71.6), Slovenia (70.59 -> 72.6), Mauritania (59.45 -> 74.2), Suriname (70.62 -> 76.11). I agree that whether or not perpetrators are tried is relevant to the country, but I don't agree that it's relevant to this metric or to the dead journalists. They don't come back to life if their murderer is imprisoned, and the arrest rate doesn't have any impact on how dangerous it was to be a journalist that year. If they were to do a forward-looking report on the outlook for journalists in each country in the next 10 years or so, then I do think the effectiveness of the justice system might be relevant. In the Indian cases named in the 2018 report, arrests were made: https://apnews.com/general-news-eb9e0dbcdbab4d93a2d90767c270..., https://www.indiatoday.in/india/story/madhya-pradesh-journal.... The situation in Mexico was a bit more grim, but in at least one of the cases the police did make several arrests: https://cpj.org/data/people/mario-leonel-gomez-sanchez/. reply user982 13 hours agoparentprevI don't know how to interpret the front page saying \"More than 100 journalists killed in six months in Gaza\" directly above a \"real time\" abuse barometer saying that 12 journalists have been killed worldwide in 2024. reply verdverm 13 hours agorootparentDifferent (overlapping) time spans Probably different measuring / classifications at play too. For example, they may be including independent journalists in one set vs only recognized outlets in another. --- edit, they have the following note in the barometer > Journalists are listed only if RSF has established that their death or imprisonment was linked to their journalistic activity. The list does not include journalists who were killed or imprisoned for reasons unrelated to their work or when the link to their work has not yet been confirmed. https://rsf.org/en/barometer?type%5Btue%5D=tue&annee_start=2.... reply NewJazz 13 hours agorootparentprevThere hasn't been six months in 2024 yet, for one. Many of the deaths could have been in Nov and Dec. reply verdverm 13 hours agorootparentThe problem is, if you look at 2023, they only count 50 deaths in the barometer. Something is definitely amiss, see my peer comment reply wslh 12 hours agoparentprevThe world is so focused on Israel that forget the rest. reply ttul 13 hours agoprevNetanyahu's ongoing corruption trial looms large over all of this. If he were to lose power, he would be far more vulnerable to conviction and potential imprisonment. So from this vantage point, the Al Jazeera ban could be seen as an act of desperation - muzzling a high-profile critic as a concession to far-right parties, even at the expense of free press principles, all in service of his own political and personal survival. It paints a troubling picture of a leader whose decision-making is distorted by clinging to power at all costs. Undermining democratic norms to appease extremist coalition partners is a dangerous road that could lead Israel to more illiberal and authoritarian policies, especially toward Palestinians, the Arab media, and domestic dissent. reply stoperaticless 13 hours agoparentI share similar view. Israel response is guided by prime minister’s personal political ambitions, while war is ongoing, the leader has more power and less of a chance to be replaced. reply easyThrowaway 13 hours agoparentprevHe's playing with the same rulebook of Slobodan Milošević - He's trying to make apparent to everybody that if he goes down, his own country will go down with him. Frankly, it feels like the only hope for an end to this conflict is in the hands of the internal Israeli political opposition. I wouldn't be surprised if he's not stopped, we're gonna see the same... \"approach\" used with Palestinian people applied to whatever internal resistance is left. reply int_19h 2 hours agorootparentIIRC something like 80% of Israeli citizens approve of the way Israel is fighting in Gaza, according to the polls. reply easyThrowaway 1 hour agorootparentit would be interesting to know to what extent they're aware of what's actually happening in Gaza. Also, given the vilification we've seen of those expressing sympathy for a two-state solution or the general Palestinian population in the last 30 years, I'd be wary to take at face value any poll without being sure they were done with some sort of guarantee of anonymity. I'm pretty sure you'd get even more one-sided approval results asking the British about the handling of the Ireland border during the troubles, or the Spanish on the repression of the Basque population after Zaragoza. reply int_19h 24 minutes agorootparentI'm pretty sure you're right on that last bit, but I don't think that wouldn't be reflecting the reality in UK or Spain, either. People are generally pretty quick to get into an \"us vs them\" mode when threatened, and, once there, will come up with increasingly ludicrous justifications even for the nastiest stuff. And yes, in Israel right now it's probably somewhat affected by social (and sometimes legal) consequences of dissent... but those very social consequences are in and of themselves indicative of supermajority support. Point being, even if it is, say, 65% rather than 80%, that still means that opposition has no power, and so nothing is really in its hands at the moment. reply refulgentis 13 hours agoparentprevIt was a very strange day yesterday: the whole week coverage had been building up to a meeting in Cairo, Hamas signalled they were going to accept the cease fire. Saturday AM EST, it was reported that Hamas confirmed they were going to accept the deal. By noon Saturday EST, the \"Israel-Hamas War\"...idk what to call it, live blog? collection-of-news headline?...was gone for the first time in months. Israel reporting (not just Haaretz) reported huge, multiple, protests (it was at night there, early afternoon EST) due to Israel rejecting a cease fire. Piecing it together from Twitter natsec people, standard blob, certainly not polarized against israel, Israel didn't even send a delegation to the talks, and the far right Israeli leader said Bibi promised him they wouldn't accept \"a rushed deal\" (i.e. the cease fire), and people were irate. An irate Israeli TV reporter revealed the anonymous \"diplomatic source\" promising no deal Friday night was Bibi himself. The blogs are back up now, with a sort of hurried framing that the talks fell apart because Hamas wanted a permanent cease fire (no mention of any of the above -- I assume that'd complicate it too much for, it needs to be a nice little set piece of Israel vs. Hamas. It's really, really strange watching the coverage the last week, in America, without any attachment to either \"side\". I guess its easier to push the A vs. B framing on a new subject, our college kids, rather than trying to explain how any of this makes any sense at all. reply NewJazz 13 hours agorootparentNetanyahu has also said that he intends to do an operation in Rafah regardless of whether Hamas gives up the remaining hostages. https://text.npr.org/1248276817 reply JumpCrisscross 13 hours agorootparentIsrael's war aims have transparently been about both returning the hostages and removing Hamas from power. reply NewJazz 12 hours agorootparentI am just pointing out that Israel isn't planning on agreeing to a ceasefire anytime soon. They'll probably need to have new elections. reply JumpCrisscross 12 hours agorootparent> Israel isn't planning on agreeing to a ceasefire anytime soon A permanent one, no. But that wasn't ever on the table. A multi-week ceasefire is absolutely still on the table in exchange for hostages. reply arp242 11 hours agorootparent\"We might agree to a cease-fire for a little bit, but I guarantee you will invade you soon anyway\" is not exactly what I'd call good faith language. What's even the point of a cease-fire if you don't at least offer the possibility of something long-term? It's a completely absurd thing to say. This has long been the problem, with the Israeli government never offering any perspective or hope on a long-term solution. reply AdamN 1 hour agorootparentThat seems very good faith and transparent. Bad faith would be saying, 'yeah we are happy with a permanent cease fire', getting all the hostages, and then continuing the assault. Agree that the logic kind of pushes Hamas into keeping the hostages so not sure what Israel is expecting and most likely Israel has just marked the hostages as dead and will do anything to permanently destroy Hamas and show other potential beligerents that the deterrent strategy is concrete - even though that means a death sentence for most of those hostages still alive. reply JumpCrisscross 11 hours agorootparentprev> What's even the point of a cease-fire if you don't at least offer the possibility of something long-term? To get aid to civilians. To let fighters regroup and restock. To open a window for negotiating a permanent ceasefire. I can't think of an example of a permanent cease fire being immediately agreed without surrender or withdrawal. reply arp242 11 hours agorootparent> open a window for negotiating a permanent ceasefire. Israeli government already very clearly and explicitly said that this window doesn't exist; that was my entire point. If he had said \"we MAY still invade\" or anything even slightly more qualified, then sure. But he didn't. Unless something went spectacularly wrong in translation, what I read is that he said in very clear terms that Israel will absolutely invade. Whether that's just empty threads or not is a judgement call, but it's certainly not the language of good faith. reply AdamN 1 hour agorootparentI think what you're getting at - and which is the critical thing that seems to keep getting forgotten is the critical situation of the hostages. Everything from both sides is rotating around the hostages and we are still not yet in a classical war so much as an extreme hostage negotiation with many layers of force including jets, tanks, and infantry driving the negotiations. reply JumpCrisscross 9 hours agorootparentprev> But he didn't. Unless something went spectacularly wrong in translation, what I read is that he said in very clear terms that Israel will absolutely invade. A temporary ceasefire also creates room for the opposition to create pressure to call for elections. That’s difficult to do while a war is ongoing. reply runarberg 11 hours agorootparentprevI think it is much easier to understand the situation if we take Netayahiu at his own words. He does not want any ceasefire, he has said so in the past, and he keeps saying that now. His actions are consistent with the fact that he does not want any ceasefire, as he tries to vandalize any prospects of a ceasefire, even a temporary one, e.g. by wowing to invade Rafah, even if there is a ceasefire. The timing of this ban on Al Jazeera is also consistent with his behavior of trying to vandalize any ceasefire talks. Al Jazeera is a Qatar based media company, and Qatar is also the mediator in the ongoing talks. If Netanyahu wanted these talks to be successful he would not antagonize the mediator this way. No, the Israeli government does not want a ceasefire, neither a permanent one, nor a temporary one. What they want is to make it look like they are making an effort, but only enough to improve the optics. There may be a faction inside the military which actually wants a ceasefire, so perhaps—and hopefully—a ceasefire can be negotiated despite vandalism attempts by the Israeli government, but I’m not hopeful. In the meantime, I do take Netanyahu at his words, that he does not want a ceasefire, and he wants in invade Rafah, to continue the genocide, and to ethnically cleanse Gaza of Palestinians. reply edanm 8 hours agorootparentI agree with most of your comment, but I'll push back on a couple of things: > No, the Israeli government does not want a ceasefire, neither a permanent one, nor a temporary one. All the things you wrote before that are things that Israel explicitly says, that it won't agree to a permanent ceasefire. But this statement is your personal opinion, and I don't think it's necessarily justified. > In the meantime, I do take Netanyahu at his words, that he does not want a ceasefire, and he wants in invade Rafah, to continue the genocide, and to ethnically cleanse Gaza of Palestinians. I think your phrasing on this is misleading, since it implies that Netanyahu's words are that he wants to \"continue a genocide\" and \"ethnically cleanse Gaza of Palestinians\". That is incorrect. Are words are that he will invade Rafah, the \"continue the genocide/ethnic cleansing\" thing are your words, not his. (I highly disagree with using those words, but either way, my point stands.) reply runarberg 8 hours agorootparentYou are right, actually. It is too late for me to edit, but from the second paragraph onward I needed to prefix with “I think”. Rereading it I can see how this is misleading, that was not my intention. reply edanm 3 hours agorootparentThank you, and for what it's worth, I didn't think you were being intentionally misleading (just came out misleading in the phrasing). reply refulgentis 11 hours agorootparentprevIt's impossible to disagree with -- we have to eliminate the terrorist military leadership that perpetrated a massacre. Why would they get a permanent cease fire? To your point, in my varied Israeli media diet, it's well-understood in the entire Israeli press that Bibi went out of his way to torpedo it by saying this simple idea over and over. The only question is whether this show he has a spine (willing to lose an eventual vote just to defeat the terrorists), or that he's weak (willing to lose the hostages to retain his political position) A majority is weary of it because the implementation of the specifics is \"we will work our way through the refugee camp and then ???\" and they feel they've seen that plan before. We can confirm this is from an unbiased perspective by noting that the protests kicked up a notch, and the TV presenter outed the anonymous diplomatic source as him, and then perusing original sourcing as to why. (to share something I learned re: sourcing, Haaretz will be seen as some interlocutors as a left-wing rag doing performances for overseas audiences, Times of Israel is better) reply wslh 8 hours agorootparentprevnext [4 more] [flagged] NewJazz 8 hours agorootparentWhat does that have to do with my comment? BTW October 7th is decidedly not the root of this war, just a dark event in a series of atrocities that have occurred during the last 70+ years. reply wslh 49 minutes agorootparentYou can put the root of the problem whenever you want in the last thousand years. reply Comma2976 2 hours agorootparentprevDo you remember what the word \"root\" means? reply wazoox 1 hour agorootparentprev\"transparently\" or \"allegedly and repeatedly\" ? People have been protesting precisely because absolutely no progress have been made towards freeing the hostages, on the contrary. So obviously \"return of the hostage\" is a simple mantra to which Israeli government pay some lip service. What they actually plan to do with Hamas or Gaza people in general remains to be known, we have all reasons to question every single word coming from them. reply edanm 9 hours agorootparentprev> Israel reporting (not just Haaretz) reported huge, multiple, protests (it was at night there, early afternoon EST) due to Israel rejecting a cease fire. Correcting you on this - the protests were not because Israel \"rejected a cease fire\". For one thing, these are the same protests that happen every Saturday for the last few weeks of the war, continuing the \"tradition\" of protests that happened every Saturday for the ~10 months before the war. For another, Israel didn't \"reject\" the ceasefire deal, Hamas did, or at least that's the way it is being talked about by Israel itself. There are many reasons to think Israel (and specifically Netanyahu) may have tried to tank the deal, but Hamas are the ones that eventually walked away. reply H8crilA 13 hours agoprevI don't feel good about this at all, but please keep in mind that there is still serious independent journalism in Israel. And it's doing very well. For example I can recommend pretty much anything published by Haaretz, or Barak Ravid. We should monitor the health of their domestic media should things start going un-democratic there. After all nothing can replace domestic media, this is painfully clear in the case of Russia. reply magic_hamster 1 hour agoparentIt's worth noting Qatar is the main benefactor of Al Jazeera, while also having funded Hamas for years, and hosting the Hamas leaders in their country. Al Jazeera in English is extremely different from al Jazeera in Arabic where journalism takes a back seat on any item somehow connected to Israel and especially Hamas. Like many of these seemingly weird decisions, there's more to it than \"Israel bad\". reply YZF 13 hours agoparentprevThere are also many other foreign journalists in Israel. Other than Al Jazeera there are no restrictions on foreign media from operating in Israel. Certainly not western foreign media. reply sa501428 12 hours agorootparentThere are indeed restrictions on western foreign media. \"Like all foreign news organizations operating in Israel, CNN’s Jerusalem bureau is subject to the rules of the Israel Defense Forces’s censor, which dictates subjects that are off-limits for news organizations to cover, and censors articles it deems unfit or unsafe to print. ... the military censor recently restricted eight subjects, including security cabinet meetings, information about hostages, and reporting on weapons captured by fighters in Gaza. In order to obtain a press pass in Israel, foreign reporters must sign a document agreeing to abide by the dictates of the censor.\" https://theintercept.com/2024/01/04/cnn-israel-gaza-idf-repo... https://theintercept.com/2023/12/23/israel-military-idf-medi... reply flumpcakes 12 hours agorootparentThis seems reasonable to me? If a western press were outside missile factories saying \"this is the only place our super missiles are built!\" I would expect the department of defence to block that information from being published... reply SauciestGNU 12 hours agorootparentThat's unlawful in the United States, whose values Israel purportedly represents. It's called \"prior restraint\". reply vundercind 11 hours agorootparentIt’s very much legal in wartime, for exactly those sorts of purposes. Though I’m not sure we’ve tested the legality of it in this modern world where nobody actually formally declares war anymore—I don’t think it’s been attempted. (Please don’t flame thinking I’m hardcore in support of a particular side in this war due to this post—you’ve very likely gotten the wrong impression. I’m commenting only on the narrow point that the US in fact can censor, including with prior restraint in certain circumstances, during war.) reply flumpcakes 12 hours agorootparentprevAre you saying the United States would not block something being reported by the media? Because that is certainly false. reply mhuffman 10 hours agorootparentMy understanding is that they put a lot of pressure to block things and sometimes offer quid pro quo and sometimes even implant operatives in certain media positions, but legally they can't just come in and shut it down. reply SauciestGNU 12 hours agorootparentprevThere is not legal mechanism for that to occur unless the writer is a government employee reply feedforward 11 hours agorootparentprevWell the parent said there are no restrictions and there are restrictions. reply trandango 12 hours agorootparentprevThere absolutely are restrictions. No journalists are allowed in Gaza, which is at odds with almost every other conflict in the past hundred years. The stated reason is \"to keep journalists safe\". But journalists have risked their lives in many conflicts to bring the news to people, its their choice to risk their life or not. Unless one were to believe that all journalists biased against israel, there is no reason to restrict all journalists. Why not let in Christiane Amanpour, or many other western trained and western paid journalists? reply flumpcakes 12 hours agorootparentIs this true? I do not think journalists are just allowed to the front lines of any war. The entire Gaza strip seems like one giant front line. There needs to be more journalists reporting but I think just allowing anyone to walk anywhere because they've got 'press' on their jacket is probably just going to end up with dead journalists considering journalists will want to be were the fighting is and will gravitate towards danger. reply Maxious 12 hours agorootparentWar correspondents have been around since at least the French revolution. Article 79 of Additional Protocol I of the Geneva Conventions provides for protection of war correspondents to the level of civilians. reply SauciestGNU 12 hours agorootparentprevIDF also kills journalists for sport (Shireen Abu-Akleh comes to mind). reply flumpcakes 12 hours agorootparentThat seems very reductive to just say that as if it is a fact. 90% of the claims I've seen about the IDF end up being just nonsense. I did pay very close attention to what happened with Shireen Abu-Akleh and I think that was definitely not dealt with in a satisfactory way. reply so_delphi 11 hours agorootparent\"not dealt with in a satisfactory way\" is exactly the justification that IDF has used after many similar circumstances. Let's say they just don't care, since there are no repercussions. reply YZF 11 hours agorootparentWe should also keep in mind the Palestinians refused to allow the IDF to conduct its own forensic investigation. That's partly why the was no definite conclusion from the investigation into the matter. You can't demand that Israel investigate and then not enable it to do so. \"The US State Department subsequently announced on July 4 that tests by independent ballistics experts under U.S. oversight were not conclusive about the gun it was fired from but that US officials have concluded that gunfire from Israeli positions most likely killed Akleh and that there was \"no reason to believe\" her shooting was intentional. US investigators had \"full access\"[138] to both IDF and PA investigations.[139][140][141] The Palestinian Public Prosecutor's Office disputes the US conclusion that the bullet cannot be matched to a gun and maintains its position that the killing was premeditated.[142] On July 5, the US stressed that it did not conduct its own probe, but the conclusion was a \"summation\" of investigations by the Palestinian Authority and Israel.[143]\" - https://en.wikipedia.org/wiki/Shireen_Abu_Akleh#Subsequent_i... reply arp242 11 hours agorootparentprev> Let's say they just don't care, since there are no repercussions. I think that's a fair statement, but also a far cry from \"killing journalists for sport\". These kind of exaggerated claims aren't helpful. reply ictebres 1 hour agorootparent> According to Reporters Without Borders' tally, at least 105 [journalists] have so far [since October 7th] been killed by Israeli airstrikes, rockets and gunfire, including at least 22 in the course of their work. https://reliefweb.int/report/occupied-palestinian-territory/.... reply Jochim 11 hours agorootparentprevFunny how often that seems to happen with the IDF. reply Maxious 12 hours agorootparentprev> Israel’s military can continue barring foreign journalists from accessing the Gaza Strip, the High Court said Monday, citing ongoing security concerns after months in which only Gazans or correspondents accompanied by the army have been able to report from inside the enclave. https://www.timesofisrael.com/high-court-says-israel-can-kee... reply Ecstatify 12 hours agorootparentprevhttps://theconversation.com/how-israel-continues-to-censor-j... reply adhamsalama 12 hours agorootparentprevThey literally kill journalists, including western journalists. reply jl6 13 hours agoprevHow big a share of Palestinian media consumption does Al Jazeera have? As in, do the residents of Gaza treat it as the main news source? The reason for asking is because a poll[0] of Palestinians says “90% believe that Hamas did not commit any atrocities against Israel civilians during its October the 7th offensive. Only one in five Palestinians has seen videos showing atrocities committed by Hamas.” So is it Al Jazeera’s fault that Palestinians have not seen the evidence and seem not to think 10/7 was all that problematic? One assumes that if such deliberate distortion/omission was normal practice at Al Jazeera, Israel would be able to clearly point to it. But the justification for the ban is a pretty vague concern about national security. [0] https://www.pcpsr.org/en/node/969 reply wesselbindt 2 hours agoparent> The reason for asking is because a poll[0] of Palestinians says “90% believe that Hamas did not commit any atrocities against Israel civilians during its October the 7th offensive. Only one in five Palestinians has seen videos showing atrocities committed by Hamas.” I think they have more pressing matters on their mind than getting informed at the moment, such as trying not to starve, finding dead relatives in the rubble of destroyed apartments, and similar things. Moreover, I think the people of Palestine might be a bit biased against Israel. And I don't think that's an information issue, I think it's a completely natural response to Israel killing tens of thousands of civilian, 70% of whom women and children. I think if you ask Ukrainian civilians what they think of Russians you'll hear some falsehoods and unreasonable stuff too. That's a completely natural response, and the Ukrainian media is not to be blamed for that. reply JumpCrisscross 13 hours agoparentprev> do the residents of Gaza treat it as the main news source? Does Gaza have sufficient connectivity for its population to have a real news source? reply runarberg 10 hours agorootparentI think they do, as long as their internet connections are up. At least the Gazans I follow on social media seem to be perfectly aware of the world news. Graffiti tags on refugee tents in Rafah thanking American students for their solidarity seems to support that. I’ve also read in the past that Al Jazeera is a rather popular media outlet among Palestinians in Gaza. > Pan-Arab satellite TVs, especially Qatar's Al-Jazeera, are popular. [1] I know that Shireen Abu Akleh—an Al Jazeera journalist murdered by IDF in 2022—was a superstar among Palestinians, including Palestinans in Gaza. 1: https://www.bbc.com/news/world-middle-east-14631745 reply nailer 10 hours agorootparentprevnext [2 more] [flagged] I_o_IllI__o_I 47 minutes agorootparentMike, kill yourself and the world will be a better place reply akira2501 13 hours agoprevIf you allow \"national security\" to be used as an excuse to \"grant powers\" which ultimately just \"destroy freedom\" then you will end up with leaders who intentionally do a bad job at security in order to access the power that grants them. If your government cannot protect the country from journalists, then you should force them to resign, and call for new elections. reply amitport 6 hours agoparentand if you allow foreign agents to incite for ethnic murder and the destruction and on your country on local cable TV? you think this ends well? reply akira2501 3 hours agorootparentIsn't that just a crime in and of itself? Wouldn't that entitle the government to just arrest and charge that particular person with these crimes? I think that stands a chance of ending well, or at least, justly. reply afavour 13 hours agoprevI’m not sure about “dark day for the media” but it does feel like a dark day for Israel. Once you’ve established that the government can unilaterally ban a voice for reasons of “national security” you’ve essentially given them a free pass. As Americans living post-9/11 will know, “national security” is a deliberately elastic term that can cover anything required in the moment. reply brabel 13 hours agoparentThe EU has banned many Russian and Belarussian news sources since the invasion of Ukraine. The USA seems to not have followed through (as far as I know - as Russian news sites seem to be available). reply NewJazz 13 hours agorootparentIs Qatar a belligerent in the war? Belarus has allowed Russia to use their territory as a point from which to launch both ground assaults and missiles into Ukraine. Hard to say the same about Qatar and Hamas. If Al Jazeera were an Iranian publication the comparison might be more similar. Israeli news reports and analysts say Qatar has sent more than $1 billion to Gaza over the past decade. Qatar sent that aid through fuel to the Gaza Strip's Hamas government, which in turn sold it and paid partial salaries. In the past, the money was sent via suitcases stuffed with cash. Israel allowed these transfers to Hamas. Supporters of Prime Minister Benjamin Netanyahu say the payments his government approved helped keep the status quo in the Gaza Strip and Hamas from escalating attacks on Israel. https://www.npr.org/2023/11/02/1210110109/qatar-israel-gaza-... reply YZF 13 hours agorootparentQatar is not exactly a belligerent but it hosts the Hamas leadership. It has been funding Hamas and other groups. It (partly) funds Al Jazeera. Al Jazeera is considered by some to be its PR/Propaganda arm and has a low standard for factual reporting - https://mediabiasfactcheck.com/al-jazeera/ reply robert_foss 12 hours agorootparentIsreal has funded Hamas directly too. https://www.nytimes.com/2023/12/10/world/middleeast/israel-q... reply lucumo 12 hours agorootparentIs that claim in the part behind the paywall? The furthest the freely accessible part goes is to say Israel \"encouraged\" Qatari payments to Hamas. reply Xeronate 11 hours agorootparentnon paywall version: https://archive.fo/lgtyM from what I can see it never mentioned Israel directly giving money to Hamas. But encouraging payments seems close enough. reply YZF 11 hours agorootparentIsraeli enabled money to go in to pay government salaries to prevent Gaza from descending in chaos. That said I think it's a matter of fact that maintaining Hamas as a counter to the PA was part of strategy of the Netanyahu government. I think pretty much any money going into Gaza should be considered funding Hamas. It either went directly to Hamas or it was taxed or it allowed Hamas not to spent that money. This means Europe and the US also funded Hamas. reply NomDePlum 10 hours agorootparentThat's quite a take. The reason for funding was to create chaos, not to remove it. Why else do you fund a group that destabilises the area. Basically Israel wanted Gaza to be more chaotic as they felt it gave them more control. The decision to pursue power and chaos as opposed to seeking resolution and a path to peace has had obvious consequences. reply wahnfrieden 9 hours agorootparentThe consequence, a violent one, has been incremental land capture reply lucumo 8 hours agorootparentprevThank you for checking. > But encouraging payments seems close enough. Hmmm. I don't think I agree with that. It's such a polarized and emotional debate. It could really benefit from being precise with words. Making everything sound just slightly worse than it is will help rile up the side that feels slighted and it will let the other side just pass the speaker off as a liar. The result is fewer shared facts, more polarization and a more emotional debate. The truth of war is bad enough. It doesn't need to be stretched. reply corinroyal 9 hours agorootparentprevYes, it's real. reply avip 13 hours agorootparentprevIs this some kind of rhetorical question? Qatar is the main funder of Hamas regime. And hosting Hamas leadership. They fund Hamas more than Iran, according to Israeli intelligence (which may be wrong but that’s the source we have) reply JumpCrisscross 13 hours agorootparent> Qatar is the main funder of Hamas regime Iran is Hamas' main backer. Qatar funded Hamas with Israel's consent, so it's not really fair to hold this against Doha. (Their continuing to host Hamas' leadership is fair to criticise.) reply jjgreen 13 hours agorootparentIn July 2017, former CIA director David Petraeus revealed that Qatar has hosted the Hamas leadership at the request of US. https://en.wikipedia.org/wiki/Qatar_and_state-sponsored_terr... reply JumpCrisscross 13 hours agorootparent> Petraeus revealed that Qatar has hosted the Hamas leadership at the request of US Sure. Hence why I qualified my statement with \"continuing.\" Doha hosting Hamas in '17 was fine. Doha hosting them after October 7 is fair to criticise. reply jjgreen 12 hours agorootparentDiscovering that request surprised me, it strikes me as pragmatic and forward thinking; it also suggests that Qatar is rather keen to accede to US requests. Has that US policy changed now? If so I would have expected Qatar to expel. reply JumpCrisscross 12 hours agorootparent> Has that US policy changed now? If so I would have expected Qatar to expel Yes. Hamas was seen by even Israel as better than anarchy. That's why they let Doha fund them. We're now seeing American lawmakers criticising Qatar [1]. That's prompting Dohas to \"re-evaluat[e] its role as mediator in ceasefire talks\" and weigh \"whether to allow Hamas to continue operating [its] political office\" [2][3]. [1] https://www.reuters.com/world/middle-east/qatar-says-gaza-ce... [2] https://www.reuters.com/world/middle-east/qatar-says-gaza-ce... [3] https://www.reuters.com/world/middle-east/qatar-considers-fu... reply forty 11 hours agorootparent> Hamas was seen by even Israel as better than anarchy. Better then anarchy or better than peace? There is some people on both sides of this conflict which are happy to see it radicalized and I think those people all benefits from the other being strong on the other side. reply YZF 11 hours agorootparentBetter than anarchy and better Palestinians divided between Hamas and the PA is fair statement. Most Israelis don't believe any Palestinians have an interest in peace (I don't have a survey handy but I'm sure we can find one) and their actions reflect that belief. But if you can make a reasonable argument how defunding Gaza would result in peace then I'd be interesting in hearing it. All that said, the actions taken by the Israeli right are certainly not helping the possibility of a future peace agreement, but it's not clear whether this specific action belongs in that group. One might argue that a stronger central authority in Gaza means there is a partner for a future agreement and that if Gaza can transition to be a more peaceful place (and it seemed to be heading in that direction) that would also support a future agreement. reply SauciestGNU 12 hours agorootparentprevI wish I could source this but I was reading rumors earlier this weeks that the US is currently in talks with Doha to expel Hamas leadership. reply zarzavat 5 hours agorootparentprevI can point to a laundry list of atrocities committed by US. Should Qatar refuse to host US? Qatar is not a western country, there is no reason to expect it to buy into western exceptionalism. This is not to defend Hamas but simply to point out that the western double standard doesn’t reach much beyond Europe. reply JumpCrisscross 5 hours agorootparent> Should Qatar refuse to host US? America is (a) a country and (b) major trading and security partner. > Qatar is not a western country, there is no reason to expect it to buy into western exceptionalism We are their security guarantor. It’s why Doha is publicly debating shutting down Hamas’ political office. reply NewJazz 12 hours agorootparentprevIndia pays a lot of money to Russia for oil, it doesn't make them a belligerent. China also has close ties, but arguably they've refrained from arming Russia. Are missiles coming out of Qatar? Are they even supplying arms to Hamas, or do they simply fund the civilian portions of the government? reply zeroCalories 11 hours agorootparentBoth India and China produce their own fascistic propaganda supporting Russia. I wouldn't blame the EU for banning the Global Times. reply zztop44 6 hours agorootparentBut isn’t that just an argument in favor of censorship? reply screye 11 hours agorootparentprevQatar plays both sides. They have friendly relations with Hamas, Houthis and Iran. AlJazeera is known to be untrustworthy on matters of the middle east. Just as BBC is untrustworthy on matters of UK international politics and the NYT [1] can't be trusted on US foreign policy matters. AlJazeera, NYT and BBC are weapons of mass propaganda just like Globaltimes or RT. The main (and admittedly stark) difference is how often these weapons are deployed. []1 https://www.theguardian.com/media/2004/may/26/pressandpublis... reply maskil 13 hours agorootparentprevSince when is the entire Europe a party in the war on Ukraine? reply NewJazz 12 hours agorootparentI didn't say that, and the matter is irrelevant to the status of Russia and Belarus and their media outlets. reply taf2 13 hours agorootparentprevWe definitely did block or at least make them less available, as I recall prior to the invasion RT was commonly on when walking into a hotel room or in Youtube recommendation lists. Post invasion in US I never see it in any hotels or recommended on Youtube... was it censored or maybe just wildly boycotted, not sure... but seems appropriate as a response to me reply arp242 13 hours agorootparentRT was outright banned by the EU after they invaded Ukraine: https://en.wikipedia.org/wiki/RT_(TV_network)#Responses – it's also been dropped/banned by most mainstream platforms. I don't think you can compare Al Jazeera and RT, because one has been a firehose of bullshit that has literally advocates invasion of Ukraine, and the other does not. As far as I know, Al Jazeera is banned purely because they've been critical of current Israeli policy. There are some reasonable criticisms of Al Jazeera and things they could have done better, but that applies to every media outlet on the planet. reply jolj 12 hours agorootparentYou'd might want to watch some arabic al jazeera. While Al Jazeera English pushes s the progressive post-colonialist narrative in the United States, Al Jazeera Arabic gears the Middle East for a war by pushing a Muslim Brotherhood idea of a Sharia state, Salafism and Jihad. Both have the same aim, just as Qatar Airways sponsor your flights with oil money so you might fly through Qatar, Al Jazeera pays journalists so they can push Qatar's narratives to Western or Arabic audience. This is highly similar to RT in intent. Looking from Israel standpoint, it's a news outlet that pushes your enemies propaganda arm videos unfiltered and also uses it to radicalize part of your population reply A1kmm 11 hours agorootparentDo you have a link to an article in Arabic where they incite violence? I've spent a while translating various articles on the Al Jazeera Arabic site from Arabic to English with mistral-7b. Everything seemed to be very fact based, and was emphasising things like civilian deaths, which aligns to what I'd consider public interest. The Arabic text does consistently use the term شهيد (martyr) to describe Palestinian civilian casualties in Gaza, which is the closest thing to biased language I found across multiple articles about Israel and Palestine - but I think that is normal in Arabic for describing even non-combatant casualties and not necessarily reflective of bias given Arabic conventions. reply int_19h 1 hour agorootparentIt's normalized enough that even the notoriously secular SDF in Syria uses that term for their own fighters, and not just in Arabic publications, but also in Kurdish ones. reply arp242 12 hours agorootparentprevI don't speak Arabic so I can't really judge that; I'm sure there's tons of stuff I'd find distasteful, but being distasteful or even inflammatory (within some limits of reason) should not be outlawed. All I can do is go by articles such as this, which don't really seem to cite the same \"firehose of bullshit\"-type stuff. Also note that the Israeli government spends tons of money to push Israeli narratives and viewpoints. That's fine, they're allowed to do that, but we can leverage the same \"highly similar to RT in intent\" accusations against them. In the end we should judge actions, not intent. reply jolj 11 hours agorootparentIn most of the world outside of the United States, there are laws that relate to the concept of a \"defensive democracy\". For example the laws that outlaw display of swastikas in Germany are contradictory with freedom of speech but are aimed at denying a democracy being exploited by extreme groups (see ww2). The discussion here is about the actions of Israel versus Al Jazeera, not a possibility of banning Al Jazeera in the United States or may",
    "originSummary": [
      "Israel has banned Al Jazeera and halted its broadcasts, citing national security worries, despite facing backlash for stifling free speech and democratic values.",
      "The closure of Al Jazeera's offices in Israel has sparked criticism globally as ceasefire talks with Hamas stall, with international bodies condemning the move.",
      "The ban, permitted by a fresh law enabling the temporary shutdown of foreign media seen as a security risk, is part of Israel's controversial actions during the Israeli-Hamas conflict, including limiting Gaza access for foreign reporters."
    ],
    "commentSummary": [
      "The conversation addresses various topics, such as media censorship involving outlets like Al Jazeera, journalist roles in conflict zones, and allegations of war crimes and biased reporting influence.",
      "Ethics surrounding government information control and transparency during conflicts are debated, along with Qatar's support for Hamas and the intricacies of peace talks in the region.",
      "The discussion underscores the intricate nature of conflicts like Israel-Hamas and the difficulties of upholding journalistic principles and freedom within these challenging environments."
    ],
    "points": 570,
    "commentCount": 734,
    "retryCount": 0,
    "time": 1714938659
  },
  {
    "id": 40267675,
    "title": "The Vital Role of Bollards in Public Safety",
    "originLink": "https://josh.works/bollards",
    "originBody": "Bollards: Why & What April 2024 tag(s): cities • urban_economics • bollards Reading time: 7 mins Article Table of Contents What are bollards What are not bollards? What does not bollards look like Footnotes, Links What are bollards # The what and the why in a single image: A bollard is: any sort of physical barricade strong enough, shaped in such a way, that if a vehicle tries to overlap with the bollard in location, intentional or not, the vehicle cannot cross. Sometimes they’re built into the physical environment, sometimes not. They can be movable or not. Large and intrusive, or not. source: @worldbollard twitter account In the words of a local city engineer’1, as he was explaining why a bollard placed near where pedestrians congregate to cross a large roadway would be innapropriate: Barriers (bollards, guardrail, etc.) are considered for installation if the result of a vehicle striking the barrier will be less severe than hitting the unshielded object. So a placement like this makes intuitive sense: source: @worldbollard twitter account You’ll never unsee this: https://www.urbanismspeakeasy.com/p/take-another-look-at-where-they-put But public works and transportation departments routinely install guardrails on the outside of a sidewalk. Please, please stop evaluating local transportation administrations as competent. I’ve hung out with these people, gone on walks with them, driven around with them, listened to them get excited about a new pedestrian affordence they’ve installed, and the lack of awareness and closemindedness (which is obviously necessary to sustain a shitty system creaking into it’s 100th year of existance) is stunning. To interact with local transportation administrations requires that you 1) self-abandon, and 2) maintain performative allegience to a pseudo-scientific view of the world. What are not bollards? # It’s tricky to hit the right emotional tone of why bollards matter. Sometimes it seems academic and dry, sometimes its very visceral and raw. Where there are not bollards, there are careening, speeding vehicles, and often enough death and destruction. To park a vehicle, one needs to press the correct pedel. When people are driving unfamilar vehicles, or rushed, or whatever, sometimes the wrong peddle gets pushed. Would you suggest that this small error should result in death of people and the elimination of businesses and buildings? vehicle operator makes error when parking When there are not bollards, in areas where people are promised safety, even if everyone behaves correctly, there are still failures. For example… Once, a widely respected member of a local software development community and their partner (also a widely respected member of the same community) were walking on a sidewalk in California one night a few years ago. Far from them, a speeding car struck another car of course careened through the sidewalk. Both friends were hit by the car. She was killed instantly, he was knocked unconcious, woke up days later to find out the news. The lanugage in the article is full of ‘this was an unavoidable tragedy’, though i think it’s obvious a local city engineer ought to be held criminally liable for their neglect. Because not only was it entirely preventable, it was also statistically inevitable. Not putting bollards where they need to be is like not only not wearing a seatbelt when driving, but arguing that seatbelts should not be available in cars because usually they’re not needed. A bollard is just a seatbelt for someone outside of the vehicle. So… this is the sort of devestation done to a community that everyone would obviously want to prevent. And this exact pattern plays out many times a day. So, when there’s not bollards, often enough, there are cars. Here’s another example - even souless corporate entities understand bollards and appreciate that they have a duty of responsibility to people who are using their spaces: 7-Eleven to pay $91 million to suburban man who lost both legs because they didn’t install bollards at that location. It’s a normal occurrance. Notice down below how often this kind of thing happens. I hope there’s bollards at every 7/11 now: A 57-year-old suburban man who became a double amputee after a car pinned his legs against the front of a Bensenville 7-Eleven will receive a $91 million payout from the convenience store chain In a moment, you’ll see the kinds of vehicle-strikes-building results this refers to. The case was the first in which attorneys had access to some 15 years of reports from 7- Eleven, which identified some 6,253 storefront crashes at 7-Eleven stores across the country, Power said. Data from a previous lawsuit against the company identified another 1,525 crashes between 1991 and 1996. Who in the story do you think uttered the following? It is important to note that this unfortunate accident was caused by a reckless driver who pled guilty, and this store followed all local building codes and ordinances. That was the legal representation of 7/11, but you can also hear the local city manager or city engineer saying ‘it was not my fault either!’. (“followed all local buildiong codes and ordinances”) Maybe it would feel poetic if he was also a car-user, having his life destroyed by a car, but he didn’t even have a car. Carl was a frequent customer of the Bensenville store, and most days would walk a few blocks from an apartment he shared with his three sons to buy his morning coffee… [the morning of this tragic-yet-statistically-inevitable systemic failure] Carl’s ride was running late, and a man pulling into a parking space in front of the store stepped on his car’s accelerator instead of the brake. The car lurched over the curb, across a sidewalk and pinned Carl against the storefront, causing injuries that would require the amputation of both his legs above the knees. Another driver had crashed into the front of the same store 16 months earlier, Power said. What does not bollards look like # In the context of ‘bollards as protecting store-fronts from cars’, sort of akin to a physical insurance policy, here’s what moving vehicles, through stores, can look like. I wonder what losses the involved parties were able to recoup. Presumably insurance would make partial financial repairs, but it would all be at great opportunity cost, hassle, sadness, anger. Even a rock, obtained functionally for free, could have fully prevented this. Again, no one’s fault, but it hurts to see damage accrue. 👉 driver presses wrong peddle when parking outside store I dislike the caption. It says “woman forgets…” which feels like it’s possibly playing into the mysogentistic sentiment that some people are, for reasons of fundamental inferiority, unable to develop a certain skill. It should say: vehicle operator makes error when parking This sort of incident would be perfectly prevented by bollards like this: source: @worldbollard I live in the Cheesman park area in Denver, there’s already plenty of bollards and bollard-passing objects (trees, light poles, boulders) and I’d love for there to be many more, following similar-enough patterns of what is already placed, with some reasonable, obvious iteration. Footnotes, Links # World Bollard Association twitter account Bollards to do cars what cars do to people… 1 Dead, 14 Injured After [vehicle operator presses wrong button] reversed through his own garage, crashed through a condo that was going on the market friday Take another look at where they put the guardrail Bollards are like iceburgs. Some bollards are not placed deep into the ground or very strong, and might deform under a vehicle impact. Some bollards are quite firmly placed. source: @worldbollard This was the city engineer of Loveland, but I’ve had direct, 1:1 conversations with the city engineer in Golden (after parents in a local neighborhood demanded a meeting about the dangerous road that their kids were walking to school alongside. The engineer said ‘due to traffic count data, the road does not qualify for any state-funded improvements’), which is obviously a 🖕🏻 to the kids he was failing. (there are many cheap was to slow traffic without speed bumps, police, camearas, signs, and half million dollar increments of spending, but people committed to bad plans will use their poor imagination as a reason for why something must be done. He gave me permission to run my own road experiments, though, which I did, and it worked great. example 1, example 2, example 3. ↩",
    "commentLink": "https://news.ycombinator.com/item?id=40267675",
    "commentBody": "Bollards: Why and What (josh.works)378 points by mooreds 14 hours agohidepastfavorite287 comments crazygringo 11 hours agoThis article is a bit difficult to read, as it seems to be written with a heavy dose of sarcasm/irony. I genuinely can't tell what the author is arguing for, as it's extremely difficult to tell if he's quoting things because he agrees or disagrees with them. My biggest question is: is the author arguing that there should be spaced bollards along literally every sidewalk in the country/world, and around all edges of every parking lot? If so, it's an interesting idea, but I also can't help but think that would not just be expensive, but also possibly extremely ugly. I'm curious if there are estimates of both installation cost as well as lives saved and other damage to buildings avoided. reply notatoad 10 hours agoparenti'm not entirely sure what the author intended, but what i took from the article is that there should be a general consensus that some sort of physical separation between cars and pedestrians is necessary to protect pedestrians from cars, and failing to build that protection means you're failing to protect pedestrians. it's up to each individual jurisdiction to decide how much they want to protect pedestrians, but when a pedestrian is killed by a car it should be acknowledged that a bollard probably could have prevented that, and not doing the thing that would have protected a pedestrian was a decision that was made for reasons such as \"it's expensive\" or \"it's ugly\". The people or organizations making decisions to not protect pedestrians should be held liable for choosing to endanger pedestrians. too often, the response to a car running into a person or building is to either claim nothing can be done about it, or to blame the driver. no protections from cars is seen as a road designer following best practices, and they've done their job acceptably well. and that should be corrected. reply ndsipa_pomu 54 minutes agorootparentI think it would make a lot of sense to charge insurance companies for the installation of a bollard whenever there's an instance of a driver mounting a sidewalk. reply olalonde 9 hours agorootparentprev> The people or organizations making decisions to not protect pedestrians should be held liable for choosing to endanger pedestrians. Seems a bit extreme. If the incidence of pedestrian accidents is relatively low, it's perfectly reasonable to prioritize aesthetics and cost considerations. reply arp242 5 hours agorootparentThere's a catch-22 here because if a footpath is unsafe people won't walk there. So there will no incidents not because it's safe or because people don't want to walk there, but because it's unsafe. Or to put it in another way: https://i.redd.it/auq600rozlsc1.png – pretty sure that road has very low cyclists and very low cycle accidents. Of course not every road should have cycle lanes and bollards, but in general there's a huge lack of attention to the safety of anything that's a non-car. reply matsemann 3 hours agorootparentI sit on the board of my country's bicycle association, and work on getting more safe cycle roads. On these public hearings for new infra, someone always tries to counter building anything cycling related with \"but there are no cyclists here today, build more car lanes instead\". A common retort is that bridges aren't built where most people swim across the river. It's a chicken and egg problem, and you are absolutely correct in what you address. To use a popular HN quote: build it and they will come. reply orthoxerox 3 hours agorootparentprevYes, we've all heard [0] and probably agreed with the person Mitchell plays, but the cost of bollards is actually really low. I can buy concrete hemispherical bollards for less that $20 a piece. Let's make the total installation cost $50 per bollard. How many bollards does a 7/11 parking need? [0] - https://www.youtube.com/watch?v=fqYyxvM85zU reply olalonde 3 hours agorootparentMany areas in Americas cannot even afford sidewalks... reply pixxel 3 hours agorootparentprevYou haven’t included consultation fees, planning fees, backroom bidding markups, unions, pensions etc. What it costs you to do the job isn’t the reality of it, sadly. reply slow_typist 3 hours agorootparentprevThat is a utilitarian argument, but did you really think it through? If you drive a car, you increase the risk of cyclists and pedestrians to get hurt or killed. Hurting or killing pedestrians also harms the society in several ways. Tax the car sales appropriately to the risk imposed on individuals and the society and you have enough money for bollards. reply olalonde 3 hours agorootparentIt depends on cost. If the tax required to place bollards everywhere possible amounted to 1M$ per car, would that be a reasonable tax? reply PeterSmit 2 hours agorootparentIf the harm to society is 1M$ per car, should we be driving them at all? reply close04 1 hour agorootparentprev> The people or organizations making decisions to not protect pedestrians That statement is both too generic and too specific. It's mainly driven by narrow sentiment, perhaps understandably since we're all pedestrians, especially the \"choosing\" part. \"Endangering\" is very generic. Does a functionality in your software that could be beneficial to or facilitate endangering people but you chose not to disable it fit the assessment? Is E2EE helping criminals endanger people, or protecting honest people? \"Pedestrian\" is too specific, there's nothing exceptional about pedestrians compared to any other mode of transportation so the statement above would need to be extended to \"any decisions that did not protect people\". And then it becomes very generic again. reply bigstrat2003 8 hours agorootparentprevAgreed. I think that it's in fact quite immature to act like we must always optimize for lives saved, no matter the cost and no matter how small the gain. reply AnthonyMouse 7 hours agorootparentThe important thing to remember is that dollars are always lives, but there are finite resources available. If we can save more lives spending the same money on medical research or emissions reductions or housing construction[1] then we should do that instead. [1] Keep in mind that a single new housing unit that reduces the owner's commute by 40 miles/day is good for eliminating more than half a million vehicle miles, in addition to all of its other benefits. reply slow_typist 3 hours agorootparentYour argument only holds where prices reflect the real (internal + external) cost. Otherwise you are bound to market failure (which has already happened to the transportation market). reply AnthonyMouse 1 hour agorootparentThe values are entirely on paper. It's a comparison you make when deciding how to allocate funding. Politicians obviously and frequently don't get the math right (or even do the comparison), but that doesn't affect what they should do if they were making better policy choices, or what voters should ask for if they're doing the numbers. reply komali2 6 hours agorootparentprevThis presumes efficient spending of effort and capital across government, which, especially in the USA, a State comprised of up to nearly a hundred governments depending on where you're standing (federal, with federal agencies; state, with state agencies, county, with county agencies, city, with city agencies, school district, with school district agencies; etc), is not a good presumption. If a local government can get together a million bucks to install some bollards at one or two dangerous intersections, that's a win. That million dollars could never have been spent on a national emission reduction effort. reply AnthonyMouse 1 hour agorootparentIt doesn't presume anything, it's just relative value. The local government by definition can't enact a national program, but it could certainly use the money for e.g. local tax credits for solar panels or electric vehicles or heat pumps. It could provide incentives for local housing construction or a hundred other things. They could even return the money to citizens, who would do something with it, often something good. And if any of those things provide more value than the bollards then that's what they should do instead. reply scott_w 4 hours agorootparentprevSweden operate Vision Zero with exactly this goal and the Netherlands also have a great record here, showing it’s possible if you actually try. reply mikhailfranco 2 hours agorootparentThe Dutch have gone in the other direction in some places: - deliberately mix pedestrians, bicycles and vehicles - remove all traffic signs, traffic lights and markings at intersections https://bigthink.com/the-present/want-less-car-accidents-get... reply PeterSmit 1 hour agorootparentI'm not sure if you're from the Netherlands, but I can assure you it's more nuanced that this. Mixing only works when cars are not dominant, so you need low car volumes and low speed in these areas. Residential areas in cities are an example of this: no through traffic, max 30kmh limit. Most of (new) Dutch road design is designed to give pedestrians and cyclists multiple safe options, while cars have to take the long way round. You can in theory still get basically anywhere with a car if you need, but often (especially in cities) it easier to walk/cycle/take the train/tram/metro. The result is that things can be closer to each other (no parking moat everywhere) so in the end the trip is shorter and safer for everyone, including people choosing to take the car. reply olalonde 4 hours agorootparentprevYet, most of its sidewalks do not have bollards. reply PeterSmit 2 hours agorootparentI would argue the point of the article isn’t “we need more bollards everywhere “, it’s “our regard for pedestrian safety is absurdly low, even cheap tools to increase pedestrian safety (like bollards) are uncommon / controversial\" reply scott_w 3 hours agorootparentprevI’m not arguing for or against bollards, I’m specifically addressing the following claim: > I think that it's in fact quite immature to act like we must always optimize for lives saved, no matter the cost and no matter how small the gain. This is plainly incorrect, as Sweden and the Netherlands demonstrate. reply treflop 10 hours agorootparentprevI’m pretty sure that at the end of the day, it comes down to cost. The author writes as if people who work in this space are not smart. I’m pretty sure everyone realizes bollards saves lives, but are cities going to pay for it? Will constituents support it? Will people be okay either ballooning budgets for transportation works? Especially at the same time when people are asking for money for teachers or some other important issue. Paying for miles of bollards is an easy cut. reply emodendroket 8 hours agorootparentIt’s not just about cost: if you read about the topic you will find many arguments that bollards shouldn’t be placed because they endanger motorists — even though they would make pedestrians safer. The article is challenging the implicit prioritization of motorist safety over pedestrian safety that underlies such a judgment. reply flaminHotSpeedo 6 hours agorootparentAnd that's fair, to an extent, but the author seems to have a vendetta or total lack of empathy towards motorists. You can't just ignore the consequences of vehicles hitting bollards, you have to weigh the likelihood of cars hitting them and the severity of those incidents against the likelihood of cars going past where the bollards would be and the severity of that scenario both when there are or aren't pedestrians that could be struck. I'm not saying the status quo is correct, but I am saying that the author's tone does not strike confidence that they are approaching this from an objective and rational viewpoint that accounts for all the factors, at least in the case of bollards in locations where there's a good chance of high speed collisions with them. reply emodendroket 4 hours agorootparentI didn't get the sense the author is wishing for motorists to die; he's taken the (in my view quite reasonable) stance that the person operating the dangerous machine has a greater responsibility and that pedestrians who are not endangering anyone else shouldn't shoulder the risk for what they do. reply p_l 2 hours agorootparentprevCars are already incorporating features to ensure survival of people inside them in case of hitting a bollard - not explicitly for bollards, but because big trees are the more extreme version of bollards that give even less care to cars. Meanwhile there's often absolute zero empathy to people who are not going to have enhancements available to survive getting hit by a car. reply throwup238 8 hours agorootparentprevI think most people (including TFA author) just don't realize what bollards actually cost to install. They're not simple little poles that can be plopped on top of concrete, they have to actually be built into the foundation. Ironically the @WorldBollard association account TFA links to illustrates it best: https://twitter.com/WorldBollard/status/1384527600639434755 and https://twitter.com/WorldBollard/status/1635595240508735490 That's why the US is full of corrugated steel barriers TFA maligns by association. They use tension cables mounted at the ends to provide the rigidity, requiring just two holes to dig instead of an entire ditch. reply Terr_ 1 hour agorootparent> That's why the US is full of corrugated steel barriers TFA maligns by association. TFA does not malign those barriers, it is against their specific placement on the outer edge of sidewalks, rather than in between the sidewalk and the road. Such placement implies minimizing scratches to the paint of a swerving car is more important than the lives and limbs of the average pedestrian. reply caf 3 hours agorootparentprevThe article includes a construction picture that shows the foundation portion, down at the bottom. reply jjmarr 7 hours agorootparentprevOne of the things about evaluating the cost-effectiveness of a safety feature is that there's implicitly a monetary value assigned to human life, when you know the probability of something saving a life and the amount of money that thing costs. https://www.transportation.gov/office-policy/transportation-... For 2022, the US Department of Transportation benchmarks that at $12.5 million and that's the number used to decide if something is cost-effective. If one is proposing that society spends more on road-safety, that's more or less saying that $12.5 million should be higher. So what should it be? Are we ok with spending $20 million? $50 million? $100 million? Because that's the question we're implicitly answering when we decide if a proposal such as bollards are cost-effective. reply ctidd 3 hours agorootparentThe implicit premise in this argument is that safety is an add-on that you buy or install like an antivirus package. If we designed to encourage less dangerous forms of transportation from the start, there may be cost savings that aren’t surfaced in the “add-on safety” cost calculation. reply loeg 8 hours agorootparentprevThe cost of bollards is pretty low. reply eVeechu7 7 hours agorootparentVery low compared to not bollards. reply notatoad 9 hours agorootparentprev>I’m pretty sure everyone realizes bollards saves lives [...] Will constituents support it? and this is, i think, the whole point. we're not stupid. we all know that bollards save pedestrian lives. for a relatively low cost. and we as a society have just decided nah, we're not gonna do that. it is, as you say \"an easy cut\". and some of us feel it should not be that way. reply xp84 6 hours agorootparentThis is such a shallow take though. If 10,000 cars pass a certain stretch in a day, and 40 pedestrians, and 2 cars veer off the road per month there, chances are zero pedestrians are hurt most years. If you had enough big beefy bollards likely half those cars would have a fatality. You do the math. I don’t think it would be appropriate to do the bollards if it killed 12 people per year just because some people think pedestrians are more righteous. Setting aside entirely the absurdity of lining every street and road with bollards from a cost perspective, just the disruption alone of such a massive, decade-long public works project would no doubt enrage all street users alike. This would be the most unpopular policy move ever. Anyone arguing that it should be done anyway seems to deeply dislike the idea of democracy. Now, the idea that convenience stores and such ought to be strongly encouraged to do bollards is another idea entirely and probably a good one. Also, people should learn to f**king back in. It’s not that hard since backup cameras were invented. That would also eliminate ¾ of these idiots crashing into stores. reply cuu508 3 hours agorootparentA couple extra factors to consider when doing the math for the 10'000 cars and 40 pedestrians example: * if bollards are installed, more pedestrians may start to use the road (because pedestrians now perceive the road as safer) * if bollards are installed, the average car speed may decrease (because motorists consciously or subconsciously weigh in the potential consequences of hitting the bollards. This has been shown to work with tree lines. Not sure about bollards, as they are less visually prominent). reply devman0 6 hours agorootparentprevI think guardrails should also be in this discussion (and indeed the article does address this). Many places have guardrails installed behind the sidewalk instead of in front of the sidewalk. Like if we are going to have guardrails anyway they may as well protect the pedestrian spaces. reply ctidd 9 hours agorootparentprevThe other part of this decision not to protect human-powered mobility (pedestrian, bicycle, wheelchair, etc.) is that we allow or encourage automotive traffic as a constant, and _then_ we choose not to protect people. It’s a two step process where we make an active choice to create danger and then a second choice not to mitigate the danger. reply bobthepanda 6 hours agorootparentprevit would be also equally cheap to just narrow the roads, plant street trees, etc. that slow down cars without necessarily having bollards everywhere at least in the US, the root issue is the same, that society has prioritized the fast movement of cars, and ever bigger cars, and so we're reaping what we sow. reply roenxi 6 hours agorootparentprevThis seems like a will-have-bad-consequences line of thought. If pedestrian/car interactions are unacceptable then the obvious engineering solution is to ban pedestrians and design for cars only. And it isn't as reasonable as it seems to hold the designer liable for statistically inevitable deaths. Everyone dies. Statistically, someone will die in your shop, car park or whatever sooner or later. At some point engineers are allowed to say \"this is rare enough\" and accept a certain level of collateral damage in their designs - if society can't accept this then it can't have engineered designs for a bunch of things. The costs would be impossibly high and we'd probably have to do away with driving as a mode of transport; it is too risky. It is statistically inevitable that someone will kill themselves on the bollards. reply adrianN 5 hours agorootparentEverybody is a pedestrian from their door to their parking space. Banning pedestrians is impossible, life without cars on the other hand has worked for millennia. reply arp242 5 hours agorootparentprev> It is statistically inevitable that someone will kill themselves on the bollards. Yes, maybe someone will walk in to a bollard once every 10 years and die. It's noting compared to the tens or hundreds of thousands of people dying every years from cars (direct accidents, air pollution, microplastic pollution), never mind the environmental impact, city design impact, and many people \"merely\" injured rathter than killed. There is no equivalence here on any level. And the \"obvious\" solution is to ban pedestrians? I don't even... reply roenxi 3 hours agorootparentYou've made an effective argument in favour of banning cars. Is that what you meant to advocate? I'd accept that too. But I don't think that is a mainstream position by any stretch, or what the article is arguing for (if we're banning cars, we don't need as many bollards). reply arp242 3 hours agorootparentCan you only think in black/white extremes? \"Let's have not ALL of the infrastructure 100% centred around cars and build public infrastructure for everyone, including cars, although maybe a bit less than we have today\" is an option. reply roenxi 1 hour agorootparentWell, ok. But that gets us back to the starting point (ie, present state) where some level of collateral damage is acceptable. Which happens to be the current state that is being built to presently and the original article seems to be arguing against. If you want a grey area, we're already in one. How do you want to navigate it? How do you want to work out where the level should be? And why do you feel that is better than the current status quo? We can always say \"do more\", but without deciding what we're optimising too before building the designs it just ends up with a series of knee-jerks every time there is an accident until cars or pedestrians are banned. We need to set a tolerance for accidents, and there needs to be an argument for why it isn't the current level of tolerance that we are displaying. reply arp242 1 hour agorootparent> that gets us back to the starting point (ie, present state) where some level of collateral damage is acceptable. No one claimed that it's not; they just said \"let's have a wee bit more protection, which rarely exists today, because thousands of people are dying needlessly every year\". That's it. You're argueing on your own against things that were never said. I have no interest in continuing this because I no longer believe you're engaging in good faith but are merely trying to pull some \"gotcha\" zinger or whatever. Talking to has all the appearances of being utterly pointless because you seem unable or unwilling to read what's being said. reply roenxi 1 hour agorootparent> let's have a wee bit more protection, which rarely exists today, because thousands of people are dying needlessly every year We add a wee bit more protection. Maybe it cuts the rate by 80%. Why do you think it is acceptable to stop adding protection? We've already added protections like that, the rate has already been cut 80%, and people are still saying it should drop. You're applying a knee jerk algorithm - asking for increases in the controls every time you see something you don't like. That path ends with complete isolation of cars and pedestrians, ie, pedestrians and cars can't occupy anything that would reasonably be seen as the same space. Otherwise you'll keep seeing things you don't like and there will always be more that can be done. There isn't any reason the rate has to be positive. We can ban pedestrians from being anywhere near cars. If you're not happy with this positive rate, what rate do you want and why? Or even how do you want it determined? reply scott_w 4 hours agorootparentprev> The costs would be impossibly high and we'd probably have to do away with driving as a mode of transport Now you’re thinking in portals reply komali2 6 hours agorootparentprev> obvious engineering solution is to ban pedestrians and design for cars only. No, the obvious engineering solution is to ban cars, the worst means of transporting humans ever conceived, and design for pedestrians only. If we want motorized vehicles sharing space anywhere near pedestrians, they should be operated only by highly trained professionals (e.g. taxi drivers with retest licensing requirements, commercial truck drivers, bus drivers, etc), or, by vehicles on rails (subways, trolleys, trains). reply montag 5 hours agorootparentI hope we can soon restrict human-operated cars. reply jeffreygoesto 4 hours agorootparentHow soon? A well proven prediction seems to be \"50 years from now\" still... At least for Level 5. reply littlestymaar 4 hours agorootparentprevGiven how the automated ones are being developed in a “move fast and break thing” fashion by engineers under strong management pressure to deliver ASAP, I'm not sure the alternative is too much of an improvement. If we added mandatory formal methods use (mathematically proving the code's invariants) during development, and gave full criminal liability to the managers in charge of the project when someone is injured/killed, then it probably would, but we clearly aren't there yet. reply Fradow 4 minutes agoparentprevMy understanding is that the author is arguing that: - guardrails should always be between the sidewalk and the road. Not after the sidewalk - in places where statistical data shows collision or where there's a high risk of cars going on the sidewalk, bollards should be installed. A prime example is in parking lots where cars park facing the sidewalk. reply alistairSH 11 hours agoparentprevCan’t speak for the author, but IMO… Everywhere a pedestrian might be? Probably not. But, we can do a MUCH better job building sidewalks and roads to increase safety. Lower speeds (not just posted limits, but road design). Raised sidewalks that are continuous, not the disjointed mess we have in much of the US. At bus stops, schools, and any shopping area where cars are parked directly adjacent to eh store front? Yeah, bollards should be installed. reply chatmasta 11 hours agorootparentThe trouble is that we’re rarely “building sidewalks and roads” in a large empty space. Either there is already a road there, or there’s other immovable constraints like buildings and landmarks. If you’ve got some large empty space, then sure you can build a safe road and sidewalk. But the reality is that’s rarely possible, especially in urban areas that were originally planned in the horse and buggy era. The roads in the UK are narrow, and there’s limited parking space, so people park half on the sidewalk and make the road even narrower. reply alistairSH 10 hours agorootparentI’m not really sure why a large empty space is needed to build a safer road/sidewalk? Just looking out my front door (suburban DC)… the road is posted 35mm but you can “safely” go 50+ because the lanes are wide and relatively straight. But, there are uncontrolled/no-signal entrances to neighborhoods every 1/4 mile or so, so speeds really should befloppy bollards The technical term near me seems to be something like \"delineator posts\" (or just \"orange posts\" after the colouring) and I think that's pretty reasonable. As you say, they don't provide any protection against a car or truck, but they do signal where not to be a bit better. reply jrochkind1 8 hours agorootparentprevIronically, the UK already does quite a bit better than the USA in pedestrian safety, despite having much more history of existing built environment. Or actually, it's perhaps not ironic, it's perhaps because of the limitations of the existing built environment in the UK, which prevented doing what has been done in many parts of the USA -- optimize for car speed over pedestrian safety. reply drozycki 11 hours agorootparentprevWhile vehicles partially on the sidewalk are a nuisance, they do provide a barrier between pedestrians and vehicles, and do have a traffic calming effect by narrowing the travel lane. reply CydeWeys 8 hours agorootparentAnd they're excellent barriers for preventing people in wheelchairs or using walkers to get through at all. reply sleepybrett 10 hours agorootparentprevVehicles partially on a sidewalk probably shouldn't be surprised if they are found with broken headlights, taillights and scratched paint. reply kube-system 7 hours agorootparentIn the places where this practice is common, it is also accepted. reply komali2 6 hours agorootparentNot true in Taiwan, nearly every sidewalk is overflowing with cars and scooters illegally parked. I started closing mirrors and opening windshield wipers on cars that do this to try to get the zeitgeist moving and someone threatened to kill me for it recently. reply kube-system 5 hours agorootparent“Accepted” is a generality. There are individuals with beliefs counter to the norm in every city. reply bobthepanda 6 hours agorootparentprevthe post is talking about the US, where the roads are absolutely massive and dangerous because they are built to Interstate standards and then posted for 40MPH and have left turns everywhere. the road diet is pretty common in the US where roads are extremely wide and plagued by speeding, and where the local political will allows realigning priorities towards safety. https://en.wikipedia.org/wiki/Road_diet reply sleepybrett 10 hours agorootparentprevI'm willing to pay taxes for roads, I'm increasingly not cool with paying taxes for parking. If the space for the road is too small for one lane each way plus parallel parking and ample sidewalk and pedestrian safety. The order of operations for determining what should be build it is : Sidewalks, then if there is room for a road - pedestrian safety, then a single lane road (one way) then a two way road, then we can discuss street parking. reply david-gpu 9 hours agorootparentLet's not forget cycling infrastructure and public transit as well. We should prioritize the means of transportation that are most beneficial to society and most equitable first, then if there is room we can make some allowances for energy inefficient traffic-congesting air/noise polluting motor vehicles. reply aldonius 6 hours agorootparentIf your roadway is designed so that the average driver only feels comfortable going about 30 km/h / 20 mph, you don't really need to have separate cycle lanes because bikes can match car speed. reply TeMPOraL 3 hours agorootparent30 km/h is like 10 km/h too fast for a cyclist to comfortably match, unless we restrict mobility only to people for whom cycling is a lifestyle. reply bsza 10 hours agoparentprevWhat really made cities ugly is when we demolished half of each to make space for cars. A bollard is a weird place to start caring about aesthetics. reply kyleyeats 10 hours agorootparentYou know it was just piles and piles of horse shit everywhere before cars, right? reply CydeWeys 8 hours agorootparentNot everywhere! Horses were banned in Rome (and many other places) for exactly this reason. reply Brendinooo 8 hours agorootparentprevElectric trolleys were a thing as well. And the extent to which horse manure was a problem depended on population size. reply bsza 10 hours agorootparentprevAs someone who has smelled both horse shit and car exhaust on many occasions, I’d choose horse shit any day. It just smells like old wet hay (because that’s what it essentially is). reply bobthepanda 6 hours agorootparentit was very voluminous. in New York alone there were three million pounds of horse feces being produced every day in 1894. https://danszczesny.substack.com/p/the-great-horse-manure-cr... reply TeMPOraL 3 hours agorootparentprevAs someone who also smelled both, but is a natural city dweller, and despite not liking cars all that much - I'd chose car exhaust any day. I mean, a nondescript warm gas that doesn't smell like anything - unless you're inhaling it straight from the tailpipe, or your country is 50 years behind on automotive health standards - versus literally horse shit that just sits there (ugh) and stinks up the whole street in a 50+ meter radius, not to mention being a low-key biohazard (like all shit)? You'd seriously choose the latter? reply bsza 2 hours agorootparentWhere I’m from, exhaust gas creates a phenomenon called smog that can make the air toxic. In environments that were supposed to be designed for humans to live in. One day, as I was walking to the local grocery store while choking in said gases, I heard a guy say this to his kid: “Quick, let’s get into the car because the air’s horrible”. Can you appreciate how surreal and fucked up this is, that people can just pull up somewhere in their rolling couches, pump stinky toxic shit into the air, and then they, the people who do that, get to be protected from it while pedestrians and cyclists have to breathe it all in? So yes, over choking in fumes that will probably give you cancer, I absolutely would choose horse manure that might be a biohazard if you rub your face in it but is otherwise completely harmless. Luckily though, this is all a false dichotomy thanks to the invention of the so-called bicycle. reply Affric 10 hours agoparentprevYour question is phrased with humans as in intruders on space that exists for cars. As long as cars have existed they have have been intruders into human spaces. reply crazygringo 10 hours agorootparentNo it's not phrased that way at all. You seem to be reading into it something I simply didn't say. reply Affric 1 hour agorootparentAround sidewalks. It’s not a deep and personal criticism. I think it’s just a sign of how far our society is gone. reply strken 11 hours agoparentprevA pedestrian safety feature doesn't need to be ugly. Consider trees, or big rocks, or unusually sturdy art installations, or nice wrought iron poles with decorative flourishes. reply rossjudson 4 hours agorootparentI'm a big fan of huge rocks. Very effective. There are a lot of them. Highly entertaining on YouTube. 100% effective at reducing people driving over the edge or corners of property. reply TeMPOraL 3 hours agorootparentprev> or unusually sturdy art installations That would be great! I'm imagining a hypothetical future where the city partners with local artists to produce bollards in disguise, each one unique and one of a kind piece of art. reply p_l 1 hour agorootparentprevThe last ones are, in fact, bollards. Stones can be also a form of bollard. You could also turn bollards into art installation, which goes back to first line ;) reply tempsy 9 hours agoparentprevI'm in Vegas right now, and while I've been here a bunch of times just realizing how protected pedestrians are on the Strip...every sidewalk is basically lined with thick concrete blocks with no spacing and bollards everywhere. Which makes sense...you have thousands of drunk pedestrians and lots of cars on a busy/giant two way street with potential drunk drivers as well. reply drozycki 11 hours agoparentprevI would argue that the status quo is already expensive and ugly. Shouldn't any aesthetic claim be relative to the beauty of the parking lot itself, or of the carnage left by a vehicle after striking a pedestrian? reply waveBidder 10 hours agoparentprevbollards don't have to be Brutalist utilitarian objects. one could, for example, make light poles actually intended to wreck cars that trespass into pedestrian spaces. Target's bollards look decent IMO. reply p_l 1 hour agorootparentFor various reasons light poles tend to be made to be easily bent/broken (in fact, it's also safety related). So I'd argue that to avoid competing objectives/priorities one should not combine bollards and light poles, otherwise one goal or the other will get compromised, quite possibly in opposite way than they should for given location. Essentially, making them separate is a physical infrastructure form of making incorrect states non-representable. reply fargle 7 hours agoparentprevagree, but it's not really sarcasm/irony. it's more derision/snobbery. this isn't about whether one agrees or disagrees with the author's \"more bollards better\" platform, but the entire framing is off-putting. reply nimbius 8 hours agoparentprevthe reason the article is difficult to read is because it is written by an insufferable elitist hipster who evades every opportunity to share his learning experience with the audience and instead treats them like drooling toddlers with expressions like \"Some bollards are not placed deep into the ground or very strong, and might deform under a vehicle impact. Some bollards are quite firmly placed.\" other gems in this article include: - shitting on the city engineer of Loveland, a public servant. - taking a break from bollards to remind the audience about his good feminism. - taking time to webster the definition of bollards and dance around the idea of them, but never once mentions ASTM F3016 vs. ASTM F2656 or other standard test methods for bollards. we stay out of the technical here because youre not being taught, youre being told about bollards by the 21st century equivalent of a fucking victorian. reply deadlocked 9 minutes agorootparentI was a little confounded by the author's point about guardrails often being on the outside of sidewalks. It was only when I copy/pasted the URL for the article that they were quoting that I realised that they both had it arse-about-face and actually meant that guardrails are often on the _inside_ of the sidewalk. The outside of a sidewalk (path in this part of the world) would be the bit that borders the road, surely. reply deadbabe 8 hours agoparentprevIf we want a more cynical take, the economic value of the human lives saved by installing those bollards does not outweigh the cost of installing all those bollards. In some areas like Manhattan, where the average economic value of a life may be higher in some areas, bollards may be a good investment, if for example they save the lives of some high net worth individuals. This does not reflect my personal opinion though so please don’t downvote me. I value all human life highly, except of course rapists and murderers, etc. reply komali2 5 hours agorootparentAn excellent example of why capitalistic measurements of \"value\" are a death-cult method of making decisions. reply TeMPOraL 2 hours agorootparentThat's a nice sentiment and all, but in reality, value of life will always be convertible to dollars if you talk about it and anything else in the same sentence. You are putting a finite dollar value on your own life each time you get out of the bed, or cross the street. reply komali2 1 hour agorootparent> but in reality, value of life will always be convertible to dollars if you talk about it and anything else in the same sentence. > You are putting a finite dollar value on your own life each time you get out of the bed, or cross the street. Can you explain how? I'm extremely confident I've never put a dollar value on my or anyone else's life. To me, human life is immeasurably valuable. reply p_l 1 hour agorootparentprevLife is in many ways always convertible. The death cult aspect of it is assigning value of life based on capitalist net worth. reply woodruffw 13 hours agoprevBollards are fantastic technology: cheap to manufacture, easy to install, and life-saving (both in terms of crashes and also forcing drivers off of curbs, crosswalks, &c.). It's a shame that so many US cities are focused on installing pseudo-bollards and flexible strips of plastic, rather than putting down permanent protections for cyclists and pedestrians. One recent example of this is NYC's Gowanus[1]: they're redeveloping the area for residential use, including bike lanes and daylighting down 4th avenue (historically a high-volume, industrial avenue). But these bike lanes and daylight zones are protected only by plastic bollards, which even a sedan can comfortably park over. [1]: https://www.nyc.gov/assets/planning/download/pdf/plans-studi... reply bombcar 12 hours agoparentFlexible markers (which aren’t even attempting to be bollards, to be clear) are usually a step up from a simple painted line and often recommended by fire departments and other emergency personnel as they can ignore them with their equipment. reply woodruffw 12 hours agorootparentThey're often sold as \"flexible bollards\"[1], so I think it's fair to evaluate them by that title. I don't object to the idea that EMS or other emergency responders might need roadside access. From my experience, many European cities do this admirably by having retractable bollards embedded in the street, or by redesigning streets to have a bollard-free section (e.g. by the fire hydrant, where it's already illegal to park or idle). (There's also the irony of not placing bollards into a street crossing because emergency services might need it, when bollards might prevent the need for many emergency responses.) [1]: https://www.reliance-foundry.com/bollard/flexible-bendable reply rootusrootus 7 hours agorootparentBack many years ago, we were driving down a two lane highway in a good old Air Force blue Dodge van. Loaded with maybe 8 airmen. Down the center of the road on the yellow line were flexible bollards every couple feet. The area was under construction and the lanes were narrow, and the bollards were to keep drivers alert and in their lane, I guess. Anyway. Idly chatting with the driver, I asked 'I wonder how sturdy those are, what happens if someone hits them?' A minute or two later, when there was no oncoming traffic, the driver jerked the wheel and put the van in the center of the road. BAM-BAM-BAM-BAM-BAM at 60 mph. Then back in our lane, glances in the rearview mirror, and calmly announces that they go down and stay down. I nearly crapped myself laughing. What a crazy SOB. Still makes me chuckle at the memory, 30 years later. There is no point to that story, really, although perhaps that modern flexible bollards like the ones you link to claim to stand back up if they get hit. But do they, if the car is doing 60 mph? Hmmm. Lucky for those bollards, I don't drive a big ugly blue Air Force van. And I'm too much of a rule follower. reply caf 3 hours agorootparentAlways consider this classic from @worldbollard: https://twitter.com/WorldBollard/status/1560021356858806272 reply TeMPOraL 2 hours agorootparentYes. Staggering bollards and fake bollards could be an effective cost-saving measure, if for some reason the city finds bollards too expensive to put everywhere. If the drivers know that 10-20% of the bollards are the real deal, they'd steer clear of all of them. reply bombcar 6 hours agorootparentprevThe yellow ones I’ve interacted with that are permanently glued down will come back up most of the time after a 60-80 mph hit. Eventually they break off. reply throwaway290 6 hours agorootparentprevI don't think proper bollards are about making you follow the rules so much as reducing death when you are unable to follow the rules. reply estebank 4 hours agorootparentIt can be for either. These ones for example are unlikely to stop a truck going at high speed, but will stop someone who doesn't want a repair bill from parking. https://maps.app.goo.gl/sYuNiXN4TqAcsHLg9 reply krisoft 11 hours agorootparentprev> when bollards might prevent the need for many emergency responses I doubt that? If a bollard stops a car which would have caused an emergency that is often reason enough for an emergency response in itself. It doesn’t change the number of emergency calls, just changes the form of the emergency. Also the whole argument you are making is silly. A bollard on a street crossing can prevent some kind of emergencies (the kind a runaway vehicle would cause). It absolutely does nothing to prevent other kind of emergencies (like fires caused by faulty wires, or hearth attacks) but might lenghten the response time for those. There would be maybe some form of irony if emergency responses were only required because of runaway cars, but that is far from the case. reply woodruffw 10 hours agorootparent> I doubt that? If a bollard stops a car which would have caused an emergency that is often reason enough for an emergency response in itself. It doesn’t change the number of emergency calls, just changes the form of the emergency. A somewhat common automotive accident in NYC is one where a driver falls asleep or unconscious at the wheel, causing (or nearly causing) a mass casualty event on a sidewalk. These kinds of tragedies can happen at low speeds, since the car rolls forwards silently over the curb and hits pedestrians or cyclists from behind. Bollards would stop this, just like they would stop cyclists from being backed into by trucks in bike lanes, and pedestrians from being sideswept on non-daylit corners, etc. Of course, these are contrived examples. But the larger phenomenon holds: a single driver injured after collision with a bollard requires fewer emergency resources than a driver plus pedestrians injured after collision with a building. I'll point out again: other cities have solutions for this that clearly work without impeding emergency response. Compare London's emergency response times[1] to NYC's[2]. [1]: https://www.london.gov.uk/who-we-are/what-london-assembly-do... [2]: https://www.nyc.gov/site/911reporting/reports/end-to-end-res... reply sdwr 10 hours agorootparentprevThose retractable bollards blew me away when I visited Germany, they looked so sensible and durable. That's a commitment to walk-drive balance reply kube-system 7 hours agorootparentprevIn the US they're often used to divide entire lanes for a long distance. https://shur-tite.com/WebData/images/ca74404b-a476-4826-8e47... They're somewhere in between a line and a fixed bollard. They are more effective at encouraging drivers to voluntarily stay in their lane than a line is, but they still don't do anything to prevent vehicles from crossing in emergency situations, accidents, or people who don't care about the paint on their car. It would be cost prohibitive to replace this usage with retractable bollards because these often extend for long distances. reply Terr_ 11 hours agorootparentprev> Flexible markers (which aren’t even attempting to be bollards, to be clear) are usually a step up from a simple painted line There's a T-intersection near my house which is more of a 30°/150° split, and I'm glad they finally upgraded to those not-quite-barriers: It has reduced the number of people who were ignoring the stop-sign and driving straight through as if it were just a curve in the road, which could easily cause head-on collisions. (The gore-point is also paved, not a raised curb.) Even so, some of the sticks have been lost to attrition now, and I kinda wish they'd get replaced with much heavier ones guaranteed to leave big dents and scratches... reply sleepybrett 10 hours agorootparentprevThey last about three months, then you are back to a painted line. reply ktosobcy 11 hours agoprevIf only cars weren't gigantic, oversized killing buckets... NotJustBikes just posted another video (https://www.youtube.com/watch?v=JRbnBc-97Ps) about the speed limit but touching on the same issue - less speed x less mass = safer environment -> less need for physical barriers (they even removed some street lights). Honestly, there wouldn't be that much need for bollard is majority of cars would be city-car like the one in 4:39 min (https://youtu.be/JRbnBc-97Ps?t=279) reply brikym 9 hours agoparentThis clip shows how ridiculously large vehicles have become. Not only is the mass higher than ever but the front ends have become stupidly large which results in pedestrians being mowed down rather than rolling over the top. It's a symptom of american culture being highly individualistic and selfish. https://www.youtube.com/watch?v=n6tMSEW_EBs reply TylerE 7 hours agorootparentIt’s hardly stupid and it’s not in the spirit of HN to go on such diatribes. Cars today are vastly, vastly safer than even 30 years ago, never mind 80, and crumple zones are a huge part of that. reply p_l 1 hour agorootparentCars got safer thanks to design differences unrelated to size. The Car Obesity Crisis in USA is related at least partially to tricking NHTSA regulations related to mileage (IIRC), which take into account platform size of the car, which in turn drives other design concerns. reply vhcr 5 hours agorootparentprevNot that vast (in the US). https://upload.wikimedia.org/wikipedia/commons/e/ef/1994-_Mo... reply kube-system 7 hours agorootparentprevI agree about the tone, but that RAM 1500 doesn't have a lot of empty space under the hood because it is necessary for a crumple zone. It's the design of the frame rails and passenger safety cage that determines the crumple zone, the empty space has no effect on this. That empty space is there because of packaging requirements for various drivetrain options and because of styling. reply ladams 7 hours agorootparentprevSafer for the passengers, but increasingly dangerous for pedestrians. reply cow_boat 5 hours agorootparentAnd other cars! reply Cthulhu_ 1 hour agorootparentThat's when you get an even bigger car, for safety! That said, two cars hitting each other's large crumple zones is probably safer than a car hitting a bollard or something else unforgiving. reply bobbylarrybobby 6 hours agorootparentprevSafer for who? > It's a symptom of american culture being highly individualistic and selfish. Your comment is a bit on the nose. reply Aurornis 9 hours agoparentprev> Honestly, there wouldn't be that much need for bollard is majority of cars would be city-car like the one in 4:39 min A Smart car starts around 1500 lbs without driver. Something like a Smart fourtwo can be as much as 2300 lbs. No person or bike is going to stand a chance against a vehicle weighing an order of magnitude more, even if they look visually smaller. The idea that Smart car sized vehicles would remove the need for bollards is not realistic at all. You also can’t judge vehicle safety by appearance. There are a lot of lightweight, small, low front end cars that actually have poor pedestrian crash ratings because the low front end takes people out at the knees. The Honda S2000 is a classic example. A lot of the internet anti-car anger likes to idolize things like Smart Cars as solutions to everything, but the reality is that any time you have a vehicle weighing an order of magnitude more than a human capable of traveling at 40mph in a matter of seconds, humans don’t stand a chance against it in an impact. Smart cars are great for parking and fuel efficiency, but the idea that they would automatically solve pedestrian safety issues as well is just fantasy. Marginal improvement? Sure. Solution that removes the need for bollards? Definitely not. reply dghlsakjg 8 hours agorootparentA human isn't going to have much effect on the mass of a smart car, but something like a planter, another car, a curb, a tree, the front of a 7-elevn etc. is going to stand a much better chance of stopping a smart car than it is a 4,900 + lb. F-150 that carries its weight up high. If there is a tree between me and a speeding car, I would much rather it be a Smart car than just about any other car. reply Aurornis 4 hours agorootparent> but something like a planter, another car, a curb, a tree, the front of a 7-elevn etc. is going to stand a much better chance of stopping a smart car than it is a 4,900 + lb. F-150 that carries its weight up high. This is another area where looks can be deceiving. Those large vehicles also have large frontal areas and large crumple zones to absorb impacts. Those small smart cars have small frontal areas and relatively rigid frames because they can’t crumple on impact. It’s not hard to imagine scenarios where a small, narrow smart car would literally slip between obstacles where a larger vehicle would get hung up on them. This is especially true for typical bollard spacing. > If there is a tree between me and a speeding car, I would much rather it be a Smart car than just about any other car. I think you’re overestimating the difference it would make. Like I said above, the smaller area of a smart car makes it less likely to actually catch the tree (by definition) and the relatively rigid frame isn’t doing much to dissipate the energy it’s carrying. Looks can be deceiving. I know everyone wants to believe smart cars are super safe alternatives, but any of these thousand pound vehicles isn’t going to be good to go up against. The differences are more nuanced than your eyes would tell you. reply rootusrootus 7 hours agorootparentprevI don't think I'd feel too safe no matter what. There are good odds that the smaller car is moving faster than the big clumsy pickup, and so the car is likely to have at least as much, and maybe more kinetic energy. Also, bumpers on pickups are actually pretty low. Any normal bollard or concrete planter is going to be pretty effective. No pickup is going to drive over something like that. reply dghlsakjg 7 hours agorootparentWhy are there good odds that a smaller car is traveling faster? Pickup trucks and SUVs don’t noticeably lag behind traffic or travel slowly in my experience. The scenario I was discussing is when there isn’t a bollard but some other barrier specifically designed to stop a vehicle. A higher center of gravity and larger wheels will certainly help get over many obstacles that would otherwise stop a smart car. If you had to bet which car was more likely to be deflected by a curb strike and which would not, I have a hard time believing you would put your money on a truck vs a small car. reply ignormies 8 hours agorootparentprevEven if we assume smartcar--pedestrian collisions are just as dangerous for pedestrians as pickup--pedestrian collisions, a smartcar--smartcar collision is going to be a lot less dangerous for the occupants than a smartcar--pickup or pickup--pickup collision at equal speeds. Not disagreeing with your overall point, but vehicle size and weight still contribute an awful lot to the >40000 vehicle fatalities in the US each year. reply rootusrootus 7 hours agorootparentStatistically, the majority of pedestrian deaths each year occur on high speed roads, with cars doing 45-55 mph. The v^2 part of the equation is going to dominate. We should get average speed down in areas where pedestrians are, and take steps to ensure that pedestrians are nowhere near the places we allow cars to go highway speed. About half of all pedestrian deaths are caused by drunk driving, so that's another relatively low hanging fruit we could aim for if we really had the political will to do so. reply mcmoor 5 hours agorootparentI used to be against speed limit like this, but when I realize it's MPH instead of KPH and starts converting, I realize that the speed is quite extreme from what I'm used to. My motorcycle-addled road already feels quite dangerous if the riders goes to 60 kph ( the non-thinking, car-driving traffic engineers solution to “if we put two bollards on the sides of the trail, there’s enough space for a car to go between them, so we’ll save some money while we make things SAFER.” That doesn't make sense to me. The entire reason the bollard exists is to stop cars from turning into the bike path. It has nothing to do with non-thinking engineers. It has nothing to do with saving money. It's because if there's space for a car to fit between them, then the bollards will not fulfill their one and only purpose. EDIT: Note that I'm not saying that it's a good design. But I am saying it didn't come about for the reasons you're saying. reply rahimnathwani 4 hours agorootparentIt sounds like GP is assuming that a car is wider than the bike lane, so putting bollards on each side of the bike lane would stop cars, without blocking bikes from using the centre of the lane? reply Aeolun 8 hours agorootparentprevRight in the middle of a cycle path is extremely obvious though. You basically have to not be paying attention for an extended period of time if you want to run into them. reply rexf 5 hours agorootparentIt's entirely possible someone is cycling and has reduced vision (sunset, evening time, sun in your eyes, etc.) or isn't 100% focused for a few seconds (while thinking about something else). Putting any physical obstacle in the middle of a path is a very odd and dangerous choice. reply arp242 4 hours agorootparentprevNot if there are others in front of you obscuring your view. Or maybe you need to avoid someone else and you hit it. Tons of reasons you can hit them other than \"not paying attention\". reply Aeolun 1 hour agorootparentWhy are you so close to the person in front of you in the first place? You should be far enough back that you can come to a stop to avoid exactly that issue. Also, why would you need to avoid anything? It’s (from the description) right at the end of the path. reply arp242 57 minutes agorootparentBecause it's busy? Because you're cycling in a group of ten people? The person in front of you avoids the bollard, which suddenly comes in to view and you have 3 seconds to see it and react (without crashing into anything or anyone else). Usually this is okay. Sometimes it's not. This sort of thing happens to pedestrians too. reply persnickety 4 hours agorootparentprevIt only takes about 2 seconds. Ask me how I know. reply Aeolun 1 hour agorootparentTwo seconds is an eternity depending on your speed. If you can’t see two seconds of travel ahead, you better not stop paying attention for that time. reply gpm 11 hours agorootparentprevYeah bollards right in the middle of the cycle path are unfortunately common around here. reply Cthulhu_ 1 hour agoparentprevYeah there's a few \"light\" bollards on bike paths around here, mainly to keep cars off of them, and while they're plenty visible and won't stop a car at speed, they are unforgiving if you were to clip them with your bike. reply _whiteCaps_ 12 hours agoprevThere are old cannons that have been used as bollards: https://westevan.org/bollards/cannonbollards3.htm > The one on the right is a real cannon outside the main gate into the original Chatham Dockyard. It is one of a pair (see the gateway photograph in the gallery below). It had been one of the Royal Navy's biggest smooth-bore muzzle-loading (SBML) guns but when it was no longer fit to be used on a warship it was buried breech-down to protect the brickwork of the gatehouse from damage by carts and other vehicles. The muzzle of this one has been sealed off with a cross-shaped piece of iron. You can also see them as mooring bollards in harbours around the world. reply mlhpdx 11 hours agoparentIndeed. I din't know the term bollard applied to anything other than large mooring cleats. TIL. reply trhway 12 hours agoparentprevBetween our house and the road turn 30ft away was a thick reinforced concrete pole, a bollard of a kind, severely leaning from being regularly hit by tanks - the road was used by tanks driving from/to loading point, and the tanks in the convoy would regularly miss the turn due to the dust raised by the tanks in front of them. reply apwheele 13 hours agoprevBollards are also good ideas to prevent intentional terrorist acts of driving cars into pedestrian areas, https://www.nbcnews.com/slideshow/terrorist-truck-attack-sho.... reply forgotusername6 13 hours agoparentThe automatic bollards in my city, designed for exactly that purpose, have claimed over 200 tailgating cars since their installation. reply titanomachy 12 hours agorootparentHow does that happen? Are they tailgating maintenance vehicles or emergency vehicles who are authorized to access those areas, and then the bollards go up again after they've passed? reply jdietrich 11 hours agorootparentYep. The bollards rise much faster than you'd think, so you're in real trouble if you ignore the no entry signs. https://www.youtube.com/watch?v=i_Cw0QJU8ro&t=32 reply petepete 4 hours agorootparentI just knew it'd be Manchester! reply fsckboy 11 hours agorootparentprev\"tailgating\" at a red light or in a parking lot might mean \"following the car in front of you closely at low speed\", and as such the driver might not realize there is an automatic bollard there. this pleases people because schadenfreude reply dghlsakjg 8 hours agorootparentAll of the automatic bollards I've seen are very clearly signed, and frequently have lights indicating their presence. It is very rare that they are not very obvious. In any case, if you are following another vehicle so closely that you cannot see a hazard in the road in front of you, it is your fault for hitting that hazard (by law in most places). reply Aeolun 8 hours agorootparentprevMost of those places do not have any automatic bollards. Who would place them at a red light? reply nmc 12 hours agoprevBollards are good at preventing the inconsiderate from parking on the sidewalk. For fewer people to be killed by cars, however, you want transportation infrastructure which does not rely on having fast metal boxes in close proximity to pedestrians (or cyclists, wheelchair users, etc). reply janalsncm 10 hours agoparentIn some places I’ve been, the sidewalk and road are separated by mature trees, with bushes between the trees. It enforces safety, adds distance between cars and pedestrians, and adds ample shade. In intersections, pedestrian islands are protected by bollards. In Singapore for example many of the busiest roads even have pedestrian bridges. It’s a major contrast with my experience of California, where any non-automobile transportation method is an extremely hostile experience. Pedestrians and bicyclists are truly second class citizens on the roads unfortunately. reply leoc 12 hours agoprevAbove all I want to see automatic bollards which pop up along the full length of both sides of a pedestrian crossing when the light is green for pedestrians. reply quasarj 11 hours agoprevThis is so poorly written, I can't tell if he's advocating for bollards or not bollards??? reply dullcrisp 11 hours agoparentHe wants the bollards. Post some initial confusion about whether the road is on the inside or the outside of the sidewalk, this wasn’t very hard to follow. reply netaustin 11 hours agoparentprevThe purpose of the article is persuasive but the HN title is ambiguous and reads much more expository. I’m a New Yorker who walks, bikes, and drives, in roughly that order and it was clear to me that the author is pro-bollard. reply tmorgan175 11 hours agoparentprevThat's some writing under the influence if I've ever seen it. A shame, since the underlying argument is interesting. reply tonymet 12 hours agoprevLA started installing plastic bollards on main boulevards like Venice, Olympic. I've had a few buddies injured, one severely, because the well-meaning bollards interfered with organic cycling paths and led to collisions. An organic cycling path is one where either there's no formal cycling path or the painted path is not actually safe for cyclists. Often engineers who install these devices are not regularly cycling on the routes, or even cycling at all. They are not aware of the natural flow of cyclists and how they interact with vehicles. They see a deterministic cause and effect of road markings to road behavior. True road dynamics among cyclists & motorists are non-deterministic. Another terrible example was installation of bollards along popular \"group\" ride routes where hundreds of club cyclists ride before commute times (before dawn). Thankfully we worked with the city to have them removed, but it likely cost $500k+ for the installation & removal. My point is that often well meaning safeguards end up causing harm, and that policy makers don't actually use the systems they are managing. reply hellcow 11 hours agoparentWilshire in Santa Monica also installed these bollards, and I feel less safe as a pedestrian, cyclist, _and_ when driving because of them. We know the solution -- build protected bike lanes, tax cars by weight, and close of some streets to encourage walking through neighborhoods. reply tonymet 8 hours agorootparentVery good point . Another road in Fremont , ca installed plastic bollards but the traffic was 50-65 mph. Also the bollards prevented street cleaning from clearing debris so cyclists were forced to ride with traffic reply philips 13 hours agoprev> The lanugage in the article is full of ‘this was an unavoidable tragedy’, though i think it’s obvious a local city engineer ought to be held criminally liable for their neglect. > Because not only was it entirely preventable, it was also statistically inevitable. Not putting bollards where they need to be is like not only not wearing a seatbelt when driving, but arguing that seatbelts should not be available in cars because usually they’re not needed This is 100% correct. A woman in Portland here was killed when a street racer plowed into a bus stop. The racer lived and the woman died. The racer got 36 months. Totally preventable. https://www.kgw.com/article/news/crime/portland-street-racer... reply dumbo-octopus 13 hours agoparentIn the case in the article, it sounds like the killed person was walking down the middle of a totally ordinary sidewalk, not a bus stop or intersection or storefront or anything. Are you proposing we place bollards on the edges of every sidewalk in existence? reply runeb 13 hours agorootparentLowering the speed limit where there are sidewalks next to cars driving seems to work well in Europe. But that also requires policing of those speed limits so they are not considered mere suggestions by drivers. reply gregmac 11 hours agorootparentEurope has a lot more roads with a lower design speed. Curves, narrow lanes, on-street parking, trees/poles/etc close to the road. These things cause people to drive slower, because it doesn't feel safe to go fast. In North America, roads are usually built in the complete opposite way, with long straight roads and wide lanes, so the design speed is actually quite high -- even if that wasn't the intent. People go fast, because it feels safe to go that speed, but isn't, because there are pedestrians and turns. We then \"fix\" that shit road design by having low speed limits. This video is all I think of when this discussion comes up now: https://www.youtube.com/watch?v=bglWCuCMSWc reply AnthonyMouse 7 hours agorootparentThe problem we keep having is that you have a highway that goes through a town, and it should be a highway. Its purpose is to connect the larger cities on either end of the highway at high speed. And it's perfectly simple to do that, you just make it a limited access road and then the town has other roads with lower speeds for local people. But the local residents don't want that, because they want the traffic from the road to come into the little town and patronize local businesses. So they put the businesses along the main road and put pedestrians where the traffic is, and then complain about the speed limit on the road whose purpose was supposed to be high speed travel. reply Vinnl 12 hours agorootparentprevA proper speed limit is not just a number on a sign. You can add curves, change the surface material, road width, etc. Not much policing required. reply jajko 12 hours agorootparentprevJust put enough speed cameras, they are much cheaper than any human police guys in long run, can watch 24/7 things like red lights, stops, seat belts, using of phones while driving etc. They can be even connected together for those a-holes who slow down in front of them just go enter again lightspeed right after, its not rocket science in 2024 and all required tech is there for decade and a half. Here in Switzerland even foreigners have their cheeks so tight on the roads even sharpened hair wouldn't cross, they behave like angels and traffic is generally well behaved. And when they don't, punishment is heavy and it doesn't matter how many millions you have on your account or whom you know. Have this, and peace comes. Don't have it, fast a-hole drivers doing whatever they want is not your biggest problem anyway. reply hombre_fatal 12 hours agorootparentMeanwhile in Texas, red light cameras cannot be used to catch traffic violations as of 2019: https://guides.sll.texas.gov/recording-laws/red-light-camera... In Houston, bollards and raised pedestrian paths were removed recently (after being installed last year) because drivers kept hitting them. It's not a tech issue. reply bluejekyll 12 hours agorootparentIf people keep hitting the bollards, doesn’t that mean they’re working? reply AnthonyMouse 8 hours agorootparentSuppose you have a misaligned intersection, so a car that drives straight through ends up on the sidewalk. This is a bad design because pedestrians get hit by cars. Suppose you have the same intersection but put bollards on the sidewalk. This is a bad design because drivers hit the bollards and damage their cars. You want a design where cars go through the intersection without hitting anything. reply bluejekyll 5 hours agorootparentNo. You want an intersection that is safe for everyone outside of cars, and bollards help do that. If drivers aren’t capable of negotiating streets with them, then they shouldn’t be driving, or they should be driving a smaller vehicle. The idea that we should be building our streets to make driving easier is exactly how we’ve ended up with so many people being killed by cars every year in the US. Car centric design is a failed experiment of the last 75 years. reply AnthonyMouse 2 hours agorootparent> You want an intersection that is safe for everyone outside of cars Why would you not want an intersection that is safe for everyone, period? > If drivers aren’t capable of negotiating streets with them, then they shouldn’t be driving, or they should be driving a smaller vehicle. The size of the vehicle isn't what causes most collisions. Moreover, there are certain roads that have a disproportionate number of collisions. That implies there is something wrong with the road. Roads should be designed for actual reality rather then ideal hypothetical drivers and conditions. > The idea that we should be building our streets to make driving easier is exactly how we’ve ended up with so many people being killed by cars every year in the US. That is not how we've ended up there. It was quite the opposite. We made driving a necessity by moving people to the suburbs, without making roads safe enough that everybody could do it, and then demanded it of them regardless. It isn't the monkey's fault that the only housing he can afford is 30 miles from his job and he has to take a road full of obstructions to get there. The monkey's behavior is predictable, and we know that what happened last year will happen next year unless we do something different. \"Damage the monkey's car\" is not a solution, it's just the fast track to angry monkeys. reply gpm 12 hours agorootparentprevIt's like most issues, political will is needed to implement solutions, technology gives access to better solutions. reply JadeNB 10 hours agorootparentprev> In Houston, bollards and raised pedestrian paths were removed recently (after being installed last year) because drivers kept hitting them. So the city chooses not to pay the cost of protecting pedestrians, in favor of letting individual pedestrians bear the risk, and cost, of being injured themselves. If ever there were a better example of externalizing costs …. reply bobthepanda 9 hours agorootparentI wonder if there is a wrongful death basis to sue a city into having safe streets. I know in the US disability groups have successfully sued cities due to a lack of curb ramps. reply briHass 12 hours agorootparentprevThese are only useful for otherwise-law-abiding people who go a little too fast. The trend in big cities in the US is to joyride/race with your license plates removed, obscured, or fake, and that's assuming the car isn't stolen (Kia/Hyundai.) reply dghlsakjg 8 hours agorootparentI think there would be constitutional challenges in the US, but in Canada, the police are allowed/required to seize your vehicle roadside for certain offenses (unfair if you are found not to have committed and offense, but I've never heard of that happening). reply janalsncm 10 hours agorootparentprevCameras only catch criminals after the fact. Bollards directly save lives. In the example here, even if the law is a potential deterrent, killing a person was only punished with three years in prison. Bollards work even if the courts don’t. If men were angels, no government would be necessary. Until then, we have bollards. reply piva00 12 hours agorootparentprevOr even better: put speed bumps, narrow lanes, add chokepoints, lots of design features that physically force drivers to slow down instead of speed cameras that don't impede anything for someone wanting to speed. Physical features are much harder to ignore. reply pclmulqdq 12 hours agorootparentprevAnd raising speed limits where appropriate. US speed limits right now are often set at about the right level on urban and suburban roads, but far too low on highways and other roads intended for long-distance travel. This effectively causes people to speed at dangerous levels in the suburbs and cities - it does not slow everyone down everywhere. Edit: The statement \"speed limits are about right\" does not mean \"current travel speeds are about right.\" If you read the rest of the comment, it means that current travel speeds are about 5-10 mph too fast for most roads, but you don't actually need to change any signs if you start making speed limits a credible fact about the actual speed limit of the road. reply esteth 12 hours agorootparentI'm very curious where your data comes from to back up this statement. \"The current level of pedestrian fatalities from motor vehicle collisions is the right level\" just seems wrong to me. reply pclmulqdq 11 hours agorootparentI never said that. Go back and read closely. The obviously-too-low speed limits cause all speed limits to be called into question. Thus, Americans drive about 10 mph over the limit on suburban roads, where lots of fatalities occur, and the opinion that speed limits are too low is very common. Also, significant data exists that shows that the vast majority of fatalities involve a driver that is speeding. reply aspectmin 12 hours agorootparentprevI’m curious. Do you have data to back this up? reply pclmulqdq 10 hours agorootparentDo you have any data to contradict this? The statement you are asking for data about is an opinion, and asking for data to back up an opinion is at best a logical fallacy. However, if you want to know how I got my opinion, I would suggest that you look at NYC, which has almost eliminated pedestrian fatalities by heavily enforcing its 25 MPH speed limit and similar traffic laws. Conversely, most drivers I see in suburban areas drive at least 5 MPH over the speed limit. reply willy_k 11 hours agorootparentprevJust anecdotally, I’ve experienced the same. The speed of traffic on highways is regularly 5-25 mph above the limit, and this mindset does translate to other types of road. reply alistairSH 11 hours agorootparentprevOf course not. “Speeds are correct on non-highways” doesn’t match the level of pedestrian fatalities in the US. He might be 100% correct about the highway speed, though I doubt it, since most highways (interstate/limited access) seem to be 65 or 70, except in urban areas. reply pclmulqdq 11 hours agorootparentIt's a good thing that the pedestrian fatalities you are trying to cite very often happen due to someone speeding (that is a fact that you can corroborate with police data if you would like). If people don't obey a speed limit, you can't cite a consequence of their driving speed to say that the limit is too high. Also, I have exactly as much data as everyone else is bringing to this discussion, including you and the GP comment, who have brought no relevant data either. This is just my opinion. reply alistairSH 10 hours agorootparentYou’re the one who made the contention that suburban/non-highway speeds are just fine, despite high levels of pedestrian/non-car injury/death, not me. And yes, I can absolutely say speed limits are too high, even if people are exceeding them. People drive the speed they feel safe, not the speed we want. So, we should design the roads to ensure people drive the speeds we want. IE, a wide open 4-lane road is going to see speeds above 40mph, even if it’s posted at 20mph. Because it looks/feels safe from within a car. Yet, we keep building wide open 4-lane roads and wondering why everybody speeds and people keep getting run over. reply pclmulqdq 9 hours agorootparentI never said speeds are just fine. I said speed limits are fine, but being flagrantly violated. And yes, I agree that road design plays into this. My experience with the design of many roads suggests that people generally take them far too fast regardless: they cut corners, don't stay fully in their lane, and do lots of other things that indicate they are driving far too fast. reply paulgb 12 hours agorootparentprev> it sounds like the killed person was walking down the middle of a totally ordinary sidewalk, not a bus stop or intersection or storefront or anything Are we talking about the same article? The article says she was at a bus stop. > Ashlee McGill was waiting at a bus stop at Southeast Stark Street and 133rd Avenue reply geraldwhen 13 hours agorootparentprevAnywhere street racing happens, legal or no, probably yes. reply baobabKoodaa 13 hours agorootparentBuilding streets is going to become pretty expensive if we follow that advice. reply bobthepanda 13 hours agorootparentIt is probably much more doable, and less hostile, to traffic calm streets so that people cannot get up to such speeds, and also to reduce the necessity of driving so that there is no car to crash in the first place. reply geraldwhen 11 hours agorootparentThere’s a residential road not too far from me that is legitimately 8 cars wide. The people there continuously wonder why cars are literally drag racing next to houses. That’s why. reply ryanmcbride 12 hours agorootparentprev\"won't someone please think of the money\" reply BeefySwain 13 hours agorootparentprevOnly the sidewalks next to roads. reply bobthepanda 13 hours agorootparentWhat sidewalk isn’t next to a road? It’s in the name: side-walk. reply naikrovek 12 hours agorootparentWhich is why they are so dangerous for pedestrians, even though nothing bad happens most of the time. reply philips 13 hours agorootparentprev> \"They say [the car] hit so hard, it exploded the bench,\" explained Misty Nicholson, McGill's mother. reply spoonjim 12 hours agorootparentprevA good start would be life without parole for the murderer reply zer00eyz 13 hours agoparentprevnext [5 more] [flagged] baobabKoodaa 13 hours agorootparent> Likely a few too many I guess it's fun to be a contrarian, but I didn't expect to see a pro-death contrarian position here. reply bombcar 12 hours agorootparentEveryone’s pro-death and some point, or we cover Everest in escalators. It’s all where the line between allowable risk and unacceptable risk is drawn. reply baobabKoodaa 12 hours agorootparentNo. There's a difference between saying: A) The cost doesn't justify the benefits and B) There's too many humans anyway The comment I responded to said B, not A. That's what I mean by \"pro death\". Viewing it as a positive that we reduce the amount of humans by... random deaths from traffic? reply paulgb 12 hours agorootparentprev> now we want to filter for \"statistically inevitable\" I mean, yes? We’ve been making trade-offs to reduce statistically inevitable deaths for years with some real success stories to speak of. Getting doctors to wash their hands between patients, removing lead from gas, seatbelts, engineering safety factors, checklists in aviation, food safety standards… reply sandworm101 13 hours agoparentprevCorrect. Cars need to be separated from people by barriers. But that goes both ways. Deaths by pedestrians getting into places they shouldn't are very common even absent roads (ie railroad crossings). Some have called for all railroads to be fenced off. But few want to live in a world with fences around every possible dangerous area. When I went to school there was no fence. Now schools are surrounded by so many that they look like prisons. Barriers can go too far. reply hmottestad 13 hours agorootparentIt's an article about bollards and how they stop vehicles from hitting pedestrians. Fences to keep people out of places where they can easily kill themselves is very important, but doesn't have anything to do with the article. A trend I see on Twitter is that someone will bring up an important issue and comments will highlight that it's very important, but what about this other thing that is somewhat related but also unrelated. Not saying that you intended to do that here, but be aware that fences provide no security against cars and that the whole point of bollards is to stop cars from killing pedestrians who are not on the road. reply sandworm101 12 hours agorootparent>> the whole point of bollards is to stop cars from killing pedestrians Except all those bollards that have nothing to do with pedestrians. Many are there to prevent cars deliberately accessing protected areas with absolutely zero thought about stopping a crashing vehicle. The most common use of bollards is to stop vehicles from parking where they shouldn't. Some bollards are even soft so that they can be driven over without damage to either party. https://www.maibach.com/en/soft-bollard.html reply hmottestad 45 minutes agorootparentThis does all seem true and you make a good point. How do you feel about all the bollards that are designed to stop a crashing vehicles from injuring or killing pedestrians? reply estebank 11 hours agorootparentprev> Some bollards are even soft so that they can be driven over without damage to either party. That's not a bollard. I'm assuming you're thinking of flex posts, or how some of us call them, car ticklers. reply hmottestad 36 minutes agorootparentWikipedia does actually bring them up as a type of bollard. They don't really fit in with the origin of bollards though, which were to moor ships to. I think that \"soft bollards\" are made to look like bollards because most people assume that a bollard is a rigid structure and as such treat them in that manner. It's basically just an elaborate traffic cone. A sign is to a \"soft bollard\" as a \"soft bollard\" is to a bollard. reply bombcar 12 hours agorootparentprevPeople in general are pretty good at assigning blame - pedestrian hit by car is usually blamed on the car unless the pedestrian was doing something exceptionally stupid - pedestrian hit by train is usually blamed on the pedestrian. The job of government should be to evaluate and require safety equipment where it makes sense - to protect the innocent and reduce issues. And part of that is recognizing when people are using something regularly “against the law” and fixing the underlying issue, not just make it “more illegal” (for example, people using a railroad bridge to cross a river). reply bobthepanda 13 hours agoprevI don’t know if it’s just me, but it is very hard to parse the title headers that say “What are not bollards.” It’s not a standard way to construct that thought into a headline. reply beAbU 12 hours agoparentAuthor is clearly not a mative speaker. I noticed one or two homophonic errors after a brief scan. But I like the headers, as I interpreted it as \"bollards\" and \"not-bollards\", kinda humoristic and fun. reply llsf 9 hours agoprevFunny, just drove today in SF and saw and it looks like someone when straight into that house again... https://www.google.com/maps/@37.7475584,-122.44409,3a,75y,89... The City or the owners should put some bollards and be done with this once and for all. In the past 15 years they had some many cars crashing into their houses. reply Aurornis 8 hours agoprev> though i think it’s obvious a local city engineer ought to be held criminally liable for their neglect I’m always surprised when armchair observers casually call for criminal prosecution of engineers from other fields. There are obviously some cases where criminal neglect should be pursued, but the frequency with which people demand criminal charges against someone, anyone whenever something happens is getting out of control. I also don’t think these people have considered what would happen when the tables are turned on themselves, such as being involved in something like a security breach or any number of other scenarios where any of us could get caught up in a situation with damages to someone. There are countries where engineers are routinely held liable when anything goes wrong, and the outcome is not good. You get one of two situations: 1. Smart people avoid the job like the plague, knowing that they could end up on the end of a witch hunt trial when anything goes wrong. When the government strives to make an example out of someone, it barely matters if you did your job well or not. Someone must pay, and it might be you. So smart people avoid the job, leaving the role to unqualified people who will take the pay in exchange for the risk. 2. Nothing gets done or approved without incredible amounts of overspending to be extra-extra-extra careful. If you’re possibly criminally liable for damages in a situation and there isn’t much downside for you not approving it, the natural action is to avoid approving anything. Either that or you overengineer it to such an extreme that it’s incredibly expensive and maybe impractical to build, but hey, at least nobody can say you didn’t try. When you look at licensed engineering professions, they tend to be more about ensuring that designs meet agreed upon standards. If the engineer involved was following the standards and actually reviewed the design, it’s unlikely there would be any basis for criminal charges. If we abandon that standard and instead hold engineers to impossibly high standards of evaluating every design for worst case outcomes, then you get the nightmare situation above. But I guess this is the internet and articles like this are more about populist outrage than practical governance. reply AnthonyMouse 8 hours agoparent> When you look at licensed engineering professions, they tend to be more about ensuring that designs meet agreed upon standards. If the engineer involved was following the standards and actually reviewed the design, it’s unlikely there would be any basis for criminal charges. This still leads to bad results because then if ordinary practice leads to bad outcomes or inefficiency, everyone continues on with it because there is no liability for the status quo and potential liability for doing anything different. The thing that usually works is to put liability on companies that cause injuries through negligence, because then the company has the incentive to prevent injuries in the best way it can come up with, without specifying any particular method of doing it. This isn't perfect because then you still have courts deciding what \"negligence\" means and large companies rigidly conforming to whatever the lawyers say, but at least then they're operating in a competitive environment where the ones that waste resources being unnecessarily rigid make less money and the ones that fail to prevent the harm get sued. The problem is this doesn't work for governments because they're not subject to competition. Then they either immunize themselves from getting sued or waste a ridiculous amount of resources over-engineering a way to prevent the harm or fail to prevent it and get sued but it's the taxpayer who pays, and in all cases the harm falls on the public. reply ideasphere 6 hours agoprevStruggled to get past the author using ‘Peddle’ instead of ‘Pedal’ multiple times reply orthoxerox 3 hours agoparentPeddle to the mettle, bay bee. reply delta_p_delta_x 11 hours agoprevThat first image is from this junction[1] in Singapore. https://maps.app.goo.gl/cwAerb4uPN4KD6sR7 reply spencerchubb 11 hours agoprevI've never thought about in my life. Now that I have read about it, I'm probably going to notice them everywhere. It seems like a remarkably simple technology that saves lives. reply tcfhgj 10 hours agoprevOur city uses them to protect the city center from traffic while giving access to delivery vehicles, pedestrians and cyclists reply eecc 10 hours agoprevAmsterdam has many city-themed bollards along the city center streets to delimit the pedestrian part. Although they’re not deeply seated into the ground and will topple if a car hits them (but I guess they’re menacing enough to keep drivers from speeding). reply bombcar 12 hours agoprevAn alternative (often temporarily) to bollards are jersey barriers - https://en.m.wikipedia.org/wiki/Jersey_barrier They’re about $2k per 12 feet and are widely used to protect construction workers on roads. They’re also kind of ugly, to be fair. reply nativeit 10 hours agoprevTangentially related: https://youtu.be/JRbnBc-97Ps?si=EJ8d3SVc7sy9_gc9 Not Just Bikes on speed limits. reply 37 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article emphasizes the critical role of bollards in preventing vehicle collisions and tragic outcomes due to their absence.",
      "Several incidents are cited where bollards could have averted accidents, reducing injuries and fatalities.",
      "The author criticizes local transportation authorities for neglecting to install bollards and stresses the importance of their correct placement for public safety."
    ],
    "commentSummary": [
      "The article delves into the debate about installing bollards for pedestrian safety against vehicle threats, factoring in effectiveness, cost-benefit analysis, and prioritizing pedestrians in city planning.",
      "It examines the influence of road layout, speed regulations, and the involvement of various parties in advancing safer transportation choices.",
      "Emphasizing the necessity of a well-rounded road safety strategy that accommodates the concerns of all individuals using the roads."
    ],
    "points": 378,
    "commentCount": 287,
    "retryCount": 0,
    "time": 1714938886
  },
  {
    "id": 40264042,
    "title": "Exploring Traefik: Proxy Server Advantages Beyond Containers",
    "originLink": "https://j6b72.de/article/why-you-should-take-a-look-at-traefik/",
    "originBody": "j6b72s site Published on: 2024-04-30 Why you should take a look at traefik, even if you don't use containers What traefik is known for Why it’s also viable for non-container usage Common Misconception: no container engine required Common Misconception: also supports config files Their documentation is great! Traefik feels robust and well thought-out Features I really like TLS Passthrough & PROXY protocol Things I miss when using traefik Authentication Blocking of user agents and IP addresses Configuration example /etc/traefik/traefik.yml /etc/traefik/dynamic.yml Traefik got really popular over the last few years in the bubble of home-lab youtubers, that’s when I first heard about it. Traefik is more comparable to HAProxy than to nginx/caddy/apache2 - it forwards requests to services and returns the responses, can even modify headers and other aspects of the request and response, but it can’t serve files. This article states my experience with traefik in an environment without containers. What traefik is known for Traefiks site states their mission to help the microservices world. All these youtubers share that they own some kind of container infrastructure, either docker or kubernetes. Traefik runs as container too, you mount the docker socket into the traefik container and gain the ability to auto-detect other containers that you might want to expose using traefik. You can configure the proxying behavior right on the specific container via labels. Traefik can automatically request a TLS certificate from Let’s Encrypt and makes your service available as soon as it detects the existence of new container. As I don’t use linux containers that much right now, I thought traefik wasn’t for me. But I was wrong. It’s fantastic! Why it’s also viable for non-container usage Common Misconception: no container engine required Traefik doesn’t need to run in a container engine, and your services don’t need to run in a container engine. Traefik is written in Golang and compiles to a single executable file, which you can download from their releases page. I don’t know why, but I get a really good feeling when I encounter software that is written in Golang and compiles to a single binary. It makes it so easy to “deploy” the thing and you get to keep full control. An example systemd service unit is contained in their repository, and that’s, apart from the configuration files, all you need. For security, you should create a user and correctly set the permissions on the your configuration files, though. Common Misconception: also supports config files If you don’t use containers, you can’t use container labels - but I find these labels confusing and hard to read anyway. The good thing: Traefik can also be configured with configuration files. As a rule of thumb, Traefik splits its configuration in two parts - a “static” configuration that contains your certificate provider (e.g. Let’s Encrypt) and entrypoints (the ports traefik listens on) and a “dynamic” configuration that contains your routers, services and middlewares. Traefik listens to file system events and can hot reload the dynamic part. The config file isn’t thaaaat complicated. See my configuration at the bottom of this article. Their documentation is great! It explains all the concepts that Traefik builds upon clearly, has a configuration example for whichever way of configuring your instance you took at the beginning of the relevant pages (which, let’s be real, is the thing we’re searching for most of the time) and their docs covered most of the demands that I had. If you didn’t understand the terms I used earlier (certificate provider, entrypoint, routers, services & middlewares), the documentation will help you in sub-10 minutes. Try it out yourself. The sidebar is your friend. Traefik feels robust and well thought-out Traefik warns you if your configuration doesn’t make sense and I haven’t run into random issues yet. Traefik doesn’t seem to log much by default, but the way your request takes is easy to understand and I was up and running really fast without any frustration. Features I really like TLS Passthrough & PROXY protocol Traefik supports TLS Passthrough and HAProxys PROXY protocol (in and out). TLS passthrough means that you can forward traffic to web services that supply their own TLS certificate (even request it themselves from Let’s Encrypt, through Traefik, which just forwards everything to the service so that can work) without terminating TLS on the proxy. The proxy can’t see what’s being transmitted. The decision which virtual host is selected normally happens via the “Host”-Header - but as that’s in the encrypted body, that’s not possible. TLS has a solution for that problem - the “Server Name Indication” (SNI), and Traefik and many other web servers / proxies use that to make the selection. As an addition, HAProxys PROXY protocol is a more secure way of transmitting the info that gets lost due to the user first reaching the proxy - in the past, you would’ve used the “X-Forwarded-” headers, but I always disliked those, as making them secure isn’t trivial and requires testing, as header handling often times isn’t well documented. Note: the PROXY protocol has to be supported by the target service too - but for apache2 and nginx (and therefore, PHP) that’s the case, and the list of services that support the protocol is growing. Things I miss when using traefik Authentication On NGINX, I use the great Vouch Proxy (also a Golang one-binary program :>) to secure certain services with Azure AD (sorry, Microsoft Entra…) Authentication. (If you know NGINX, you’ll understand how it works just by looking at this: https://github.com/vouch/vouch-proxy?tab=readme-ov-file#installation-and-configuration) Traefik supports something similar to NGINX’s auth using ForwardAuth. Sadly, Vouch Proxy doesn’t work yet for Traefik (open issue). You could roll your own keycloak instance, integrate that with AAD and use that for ForwardAuth. The internet says that works. But it also requires you to keep that keycloak instance secure and up-to-date and set it up in the first place. For bigger projects, that might be viable. Often recommended is traefik-forward-auth. Sadly, that project has had its last update in June of 2020, the developer disappeared from GitHub and the dependencies need updating. There are open pull requests, which will probably never be handled. Not viable for me. I’ve had a bad experience with oauth2-proxy in the past (but to give them credit: also golang and a single executable :>). I don’t want to proxy to a proxy, as things HTTP2/3, timeouts, body size and WebSockets require configuration on all proxies between the user and the service. Feels too error-prone to me. But the Traefik ForwardAuth seems simple enough, so I might write my own simple tool for integrating with AAD. Or maybe someone should fork and audit traefik-forward-auth, and update its dependencies. Blocking of user agents and IP addresses I don’t want my internal services to be archived by archive.org. As robots.txt and similar headers don’t work for disallowing Archive.org, there are only two possibilities to block their Crawler: Blocking the “archive.org_bot” user agent, or blocking their IP range. In Traefik, you can only block user agents or IP addresses via a third-party plugin. I don’t like third-party plugins as I need to keep them in mind when updating, and they can introduce security vulnerabilities. You could block IPs by using the IPAllowList middleware, and just allow everything but the IPs that you want to disallow. You can calculate the IP ranges. That’ll work and isn’t any worse than blocking directly, but doesn’t feel very elegant at all as you can’t see what subnets are exactly blocked just by looking at the ones that are left. Configuration example The following example sets up: entrypoints on :80 and :443 with the http endpoint redirecting to https without exceptions Let’s Encrypt for certificates with the TLS challenge tls passthrough proxying for cloud.xx.xyz (service on another host) with enabled PROXY protocol tls-terminating proxying to git.xx.xyz (service on the local host) redirect middleware from https://xx.xyz/redirmepls to https://google.com header-adding middleware to add a x-robots-header /etc/traefik/traefik.yml providers: file: filename: /etc/traefik/dynamic.yml watch: true entryPoints: https: address: :443 http: address: :80 http: redirections: entryPoint: to: https scheme: https certificatesResolvers: le: acme: email: xx@xx.xyz storage: /etc/traefik/acme.json tlsChallenge: {} # Required as per https://blog.alexanderhopgood.com/traefik/letsencrypt/2020/12/09/traefik-http-challenge.html /etc/traefik/dynamic.yml tcp: routers: nextcloud-router: rule: \"HostSNI(`cloud.xx.xyz`)\" service: nextcloud entrypoints: - https tls: passthrough: true services: nextcloud: loadBalancer: servers: - address: 10.33.1.2:4433 proxyProtocol: version: 2 http: routers: gitea: rule: \"Host(`git.xx.xyz`)\" entrypoints: - https service: gitea middlewares: - noindex tls: certResolver: le xx.xyz: rule: \"Host(`xx.xyz`)\" entrypoints: - https middlewares: - my-redirect tls: certResolver: le service: dummy middlewares: my-redirect: redirectRegex: regex: \"https://xx.xyz/redirmepls\" replacement: \"https://google.com\" noindex: headers: customResponseHeaders: X-Robots-Tag: noindex, nofollow, nosnippet, noarchive services: gitea: loadBalancer: servers: - url: http://127.0.0.1:3000 dummy: loadBalancer: servers: [] hi@j6b72.de // github.com/j6b72",
    "commentLink": "https://news.ycombinator.com/item?id=40264042",
    "commentBody": "Take a look at Traefik, even if you don't use containers (j6b72.de)283 points by q2loyp 22 hours agohidepastfavorite171 comments sph 18 hours agoTraefik is pretty cool, but suffers from the same, terrible problem of Ansible: there is a lot of documentation, and a lot of words written, yet you can never find anything you need. I have used it since v1 and I routinely get lost in their docs, and get immensely frustrated. I have been using Caddy for smaller projects simply because its documentation is not as terrible (though not great by any stretch) Technical writers: documentation by example is good only for newbies skimming through. People familiar with your product need a reference and exhaustive lists, not explanation for different fields spread over 10 tutorial pages. Focus on those that use the product day in and day out, not solely on the \"onboarding\" procedure. This is my pet peeve and the reason why I hate using Ansible so damn much, and Traefik to a lesser extent. reply johanbcn 17 hours agoparent> Technical writers: documentation by example is good only for newbies skimming through. People familiar with your product need a reference and exhaustive lists, not explanation for different fields spread over 10 tutorial pages. Focus on those that use the product day in and day out, not solely on the \"onboarding\" procedure. I agree. We all would benefit by giving more exposure to documentation frameworks such as https://diataxis.fr reply siamese_puff 9 hours agorootparentAlso https://docs.divio.com/documentation-system/ reply yoyojojofosho 13 hours agorootparentprevDiscussed on HN: https://news.ycombinator.com/item?id=33721314 reply yread 1 hour agorootparentprevMSDN also follows these principles reply adolph 16 hours agorootparentprevI'm glad to have clicked through for curiosity's sake. Diátaxis is tremendously interesting. For folks who might recognize the author's name: Daniele Procida: Director of Engineering at Canonical. Creator of Diátaxis and BrachioGraph. Django core developer. Fellow of the Python Software Foundation. reply samuell 31 minutes agoparentprevAnsible is definitely requiring constant lookup in the documentation. I've found a pretty good workflow with using ansible-doc though, with two-three aliases that I use constantly: alias adl='ansible-doc --list' alias adls='ansible-doc --listless -S' alias ad='ansible-doc' Then I'll: 1. Use adls to quickly search (in less with vim bindings) for relevant commands, 2. Check up the docs with `ad `. 3. Almost always immediately jump to the end (`G`) to see the examples, which typically provides a direct answer to what I need. Since authoring Ansible scripts is so dependent on the docs, I think they really should make this process work better out of the box though, providing some interface to do these lookups quicker without a lot of custom aliases. reply plantain 17 hours agoparentprevMy latest gripe in this category - opentelemetry. Thousands of pages. Very little about actually achieving basic common workflows. reply jdub 1 hour agorootparentWhen I started using Honeycomb, I had such a wonderful integration experience with their Beeline SDKs. Then they transitioned to OpenTelemetry – for very good, justifiable, \"good community member\" reasons – and yikes, everything got so much more complicated. We ended up writing our own moral equivalent to the Beeline SDK. (And Honeycomb have followed up since with their own wrappers.) There's so much I love about Open Source, but piles and piles of wildly generic, unopinionated code... ooft. :-) reply silisili 14 hours agorootparentprevSame experience. Otel is one of the wordiest docs I've ever come across that says very little. Further, I found a lot of little bugs that are hard to Google, or when Googling finding open issues that are either known and working on, or no response at all. I ended up just throwing it in the garbage and using direct connectors. I like what Otel is trying to achieve, but it feels extremely opaque and half baked at the moment. reply zaphirplane 1 hour agorootparentYes! Otel, is exactly what I was thinking. It’s for people working on it with access to tribal knowledge. For mortals working to add it to their service it’s not fit for purpose reply jackthejacky 7 hours agorootparentprevOh man, I FEEL this comment. That was one absurdly awful set of documentation, because they not just have a lot of confusingly placed repeat content, they also follow the philosophy of only explaining top level initial conceptual primer for everything, and only explaining the actual main use case of the component 3 navigation pages deep. So a beginner has to jump a BUNCH of pages to get a primer, and an expert has to bookmark the couple actually-useful pages and later give up and just look at github for specific operators/processors when they already know the basic config inside out. reply hinkley 7 hours agorootparentSeveral things I needed were in completely separate documents not linked together. reply tnolet 16 hours agorootparentprevOh boy that hits home. Been deep in the OTEL world the last months and the official docs are very, very undercooked. reply hooverd 7 hours agorootparentThe best OTel docs I've seen have been from observability vendors. The CNCF needs some volunteer technical writers or something. I think a lot of those docs suffer from being written by people who know the spec inside and out. reply jethro_tell 15 hours agoparentprevOne of the problems that the yaml interpreter class of languages, or whatever you'd call them, suffer from is the fact that yaml itself is a language and tends to be more or less undocumented in the interpreter docs. It's sort of assumed that you are going to do extremely simple tasks on very flat data structures. That doesn't tend to be the reality that most of us live in. And to really get the most out of these languages you have to understand an entire unspoken set of rules on how to use yaml. That's never really pointed out in the docs. Additionally, there are docs for the unique settings for each module but as far as using the standard settings, additionally, its rarely clear how to operate on the data that might be returned or combined with anything mildly complex, you are given a dozen 1 stanza examples for each item like a stack of ingredients and then told to bake a cake. I've had this experience with basically every one of the various yaml interpreter systems I've used. After a few 100k lines of yaml I can get things done but the docs are useless other than a listing of settings. reply ornornor 15 hours agorootparentTo illustrate this point, here is how to have a multi line value in yaml: just kidding, it’s so confusing that there is a whole website to help you figure it out: https://yaml-multiline.info/ reply cromka 11 hours agorootparentprevIsn’t it why toml is seemingly increasingly used to replace yaml in projects? reply jethro_tell 6 hours agorootparentI hope not, toml is even worse at complex things and just slightly better at the stuff that isn't confusing. Add a k:v to a mildly complex dict. At this point, I'm pushing into a place where I'm just going to switch to go because its getting to be a mess. reply vundercind 5 hours agorootparentIt’s insanely better at config. It’s about as bad at being a programming language or data structure serialization format, though. reply jethro_tell 5 hours agorootparentBut yaml is fine at config, it sucks at looping, conditionals and data structures, if you aren't fixing that its just another standard we have to learn, so thanks for that reply mardifoufs 6 hours agorootparentprevIn my experience toml is worse at anything complex. It's nice as an .ini replacement but makes even yaml look sane in comparison if you want to use it for very complex or deeply nested stuff. But it wasn't designed to do that anyways reply vundercind 5 hours agorootparentAm I alone in greatly preferring nesting in toml compared with yaml? reply tormeh 7 hours agorootparentprevToml is great for simple use-cases. For complex ones you have the same problem that yaml has: Templating a language with significant whitespace via text substitution is a horrible horrible idea. Somehow this sad state of affairs has become industry standard. reply jethro_tell 6 hours agorootparentIts not even the white space, a food liter or language server can handle that. It's not that, as much ad the fact that the most complex data structure is a list. If you want to get crazy, you can push a dict into a list and operated on it but it gets tough at the second level. And don't get me started on if/else statements. reply arendtio 1 hour agoparentprevI agree that the documentation could be better, but it isn't that bad. I enjoyed all the gophers, and these images really helped me understand the structure. However, I find it amusing that you wish there was a better reference. I think getting to the initial setup is quite hard. Once you have that, extending it is straightforward. reply linsomniac 18 hours agoparentprevDo not agree WRT ansible, been using it for well over 5 years and usually a google search points me right at the correct part of the documentation to answer my question. Ansible, the tool itself, can be a bit obtuse, largely IMHO because of the YAML source language, so some concepts are hard to translate into the tool, but the documentation has never bothered me. As far as \"a lot of words written, can't find what you need\", Fortinet is my poster child there (based on trying to use it a decade ago). Everything I looked up there had 10,20,30 pages of introductory material with the Fortinet stuff spread throughout it. reply sph 17 hours agorootparentAlright, please link me to an exhaustive list of Jinja filters supported by Ansible out of the box. I'll wait. What you are given is https://docs.ansible.com/ansible/latest/playbook_guide/playb... and you need basically to read/scan each example until you find what you need [1]. Do you call that good, especially when these are basically the only way of doing anything a little complex? That's a sure way of killing my flow and productivity in its tracks. I have been through this page in anger a dozen times, and I still have no idea what Ansible filters can or cannot do. Also, using Google to find stuff is \"cheating\". The goal of documentation is to be able to use it as reference; if you need an external tool to find anything in it, that defeats its purpose a bit. When people wrote documentation books, they had to make sure it's usable, legible and efficient. These days apparently that's become lost art. 1: these examples are not even exhaustive, because they don't list all the builtin Jinja filters; chances are that what you need isn't listed on that page, but you should instead refer to https://tedboy.github.io/jinja2/templ14.html reply freedomben 17 hours agorootparentI'm not GP, but I agree with both them and you so thought I'd chime in. You're absolutely right that there are big omissions/holes in the Ansible docs, but I also think that using Google is not \"cheating.\" My ideal of great documentation sounds like exactly what you would agree with: A complete and comprehensive \"book\" (could become a physical printed book, but needn't have to as it should be equally usable with good old-fashioned hyperlinks). It should have a logical flow, introductory sections to describe pre-requisite knowledge/concepts and things that are broadly applicable to the project as a whole. It should have a table of contents, and it should definitely have an index and comprehensive lists/tables of API details such as available field/properties, which options are valid (for enum fields), etc. Your example of Jinja filters supported by Ansible is a great one. I really miss the 90's era here where such manuals were common practice, even for things like PCs. With that ideal described, though, I think it's important to recognize pragmatism and feasibility. Documentation takes time and money to produce. Search tools (including Google) already exist and can provide a valuable addition without spending time/effort on it, so I think they should be used. That said, I agree that it's not a good idea for doc writers to rely on that for things to be found! Table of contents, logical flow, and indexes should absolutely be thought through. If the documentation is just a bunch of random unorganized and uncatalogued pages that can only be found with a search engine, that is really bad and they should feel bad. I think Ansible falls right in the middle there. It undoubtedly has some real glaring omissions/holes in it, but it is also not nearly the worst I've seen as well. I do dread having to go the Ansible docs though, which is an indictment against their quality, and the more I think/write about this the more I agree with you lol. reply linsomniac 16 hours agorootparentprevAs you say, Ansible's filter list does not include the base Jinja2 filters, which I guess is a difference of opinion. I feel that is preferred to reproducing the Jinja2 documentation, especially as the Jinja2 filter list is the first (non-TOC) link on the page. Also going to disagree about \"using Google is cheating\". The purpose of documentation is to help me get stuff done. The Internet is not printed on dead trees, I don't want to read through a TOC or index looking for what I want when I'm searching, I want to use a search engine. I often don't want a reference, I want to quickly find how to do something. I rarely want to read about all the filters, instead I want to find the even/odd filter, or the the default or omit filter. Yes, sometimes I want to brush up on all available filters, but that's rare. reply throwaway984393 17 hours agorootparentprevI like that it forces users to read the docs to find the functionality. Users don't read the docs, and then they wander around the internet looking for a random blog post with a snippet for one problem, and they don't ever really learn how to use the program. Users are a bit like high school students just skimming books for an answer to fill in on a test. They need to be forced to learn. reply freedomben 17 hours agorootparentThis doesn't make a lot of sense in the context of the parents. Did you post this to the wrong parent? To accomplish what you are asking, a project needs actual good documentation. Everyone has agreed that is good. The only real disagreement here is whether Ansible docs have this, and regardless whether they do, they definitely have the example-driven docs that I think you are saying you don't think should exist, so you definitely aren't supporting the Ansible status quo. reply coryrc 6 hours agorootparentThe rest makes more sense if you assume parent post meant to write \"read the code\". reply mholt 18 hours agoparentprevFunny you say that because we don’t have nearly any examples in the Caddy docs. We’re working on improving them later this year. reply sph 17 hours agorootparentExamples are good in docs. But documentation that's only made of examples and tutorials... not so much. Thanks for Caddy btw. Neat little tool. reply lamontcg 16 hours agoparentprev> Technical writers: documentation by example is good only for newbies skimming through. People familiar with your product need a reference and exhaustive lists, not explanation for different fields spread over 10 tutorial pages. Focus on those that use the product day in and day out, not solely on the \"onboarding\" procedure. You really need at least three documentation targets: - onboarding the newbies workflows/tutorials - intermediate \"focus on the important bits\" workflows/tutorials - exhaustive references There might be other useful ones as well, but I never see those three hit at the same time adequately. reply Fire-Dragon-DoL 9 hours agoparentprevI want both, in the same page if possible, for every possible permutation of input arguments. In theory ansible does this, but then it doesn't link to \"you might use it in combination with...\", essentially, it lacks integration of multiple things in the reference docs. but I didn't find ansible docs that bad? Most of the time I search module name and find the reference doc reply igor_varga 16 hours agoparentprevI'm using the Traefik and have the same experience with the documentation. It can be time consuming to configure it properly if you are not a power user. I'm happy with it though, it's a great piece of software. I wonder is there any other product out there with a similar feature set? reply throwfaraway398 18 hours agoparentprevIt's funny because one thing I like about ansible is how easy it is to get the reference doc for any module with `ansible-doc -t module`. I do sometimes struggle to find the right doc when I'm searching for something about ansible core itself, but that doesn't happen too often. reply bshacklett 14 hours agoparentprevThis was exactly my experience. It’s incredibly frustrating to search documentation only to be stuck with examples that are related, but don’t fit one’s exact situation, and don’t explain the underlying behavior. reply hinkley 15 hours agoparentprevSome projects need documentation, some need cookbooks. Sounds like traefik is the latter. Hopefully as an aside (I know very little about traefik so maybe I am talking about them too and don’t know it), it seems like in the time since I abandoned Java they have weaponized that architectural strategy and I have no patience for it. I look at that sort of documentation and my eyes glaze over. Or if they don’t I feel disgust or anger and all three result in my stomping off. Opentelemetry, particularly the stats code (vs the span code) triggered a lot of this in me. It has several sets of documentation that say different things. It took me a long time to figure out how to connect the amorphous dots, and then I didn’t entirely agree with their solution anyway. reply scrubs 7 hours agoparentprevOh man are you on to something!!! One huge, bad side effect of web is the atomization of an overall body of work into 62.9 million links. One pdf please. The book concept works! You know who's docs blow too? Mellanox. I hate their stuff. And to give credit where due: intel does a damn good job. reply cdelsolar 14 hours agoparentprevIf only there were a program that had crawled bazillions of documents, including all of the traefik documentation, examples, and thousands of code files using it, and if only said program were especially designed to answer natural-language queries about said documents. reply lopkeny12ko 17 hours agoparentprevThis take is, at best, disingenuous, and at worst, dangerous. The Traefik maintainers and community contributors (including myself) have collectively invested hundreds of man-hours writing and improving documentation, specifically in response to feedback from users that things are hard, unintuitive, or complex. You are discounting massive amounts of unpaid labor done specifically for people like you. At this point, if you can't find what you're looking for, it's on you. Maybe do a little bit of your own homework instead of throwing your hands up after 2 minutes and crying to the maintainers. reply arp242 15 hours agorootparentI never used Traefik and have no opinion on it one way or the other as such. But if this is the response to some criticism of the documentation – which you can agree or disagree with, then you've done more to turn me of from Traefik than anything anyone here can write. reply halJordan 16 hours agorootparentprevDisagree that this isnt a generic problem. And i'll take the same amount of umbrage at you calling it disingenuous. There are dual needs here. Having to read a story and take in a wholly unrelated workflow just to discover only half of the switches available to the feature im looking up is a problem. And when there isn't just straight documenting of what's been implemented then it is an unreasonable gate to usage which limits customers to only the flows imagined by the technical writer. Which itself breeds this sort of refusal to participate. Either the end user is ungrateful and needs to express that gratitude through silence or there's a smug moderator who's read everything and knows which paragraph of which tutorial has the answer and harangues anyone asking with a link and a \"why didnt you read sentence 5 of paragraph 2 of a tutorial written 2 years and 3 major versions ago?\" reply alex_lav 16 hours agorootparentprevInvesting a lot of time and trying really hard is not the same as adding a lot of value. If your users don't find value in your documentation, saying \"But we spent a lot of time on it!\" doesn't really change anything. And, to be clear, I have no idea if the person you're responding to's criticism is valid. But I also know that your response does not negate their criticism at all. reply lopkeny12ko 16 hours agorootparentHow about submitting a PR to improve the documentation instead of complaining about it? reply Thiez 16 hours agorootparentThis is why OSS looks like a cult at times. People are allowed to criticize your project and complain about it. They have no obligation to become a contributor. \"Submit a PR\" is such a conversation killer. reply barfbagginus 8 hours agorootparentprevA PR needs maintainer approval. And the maintainer I've seen thinks the documents are good enough already. In cases like that, a PR might not be able to solve the problem. Complaining about it reroutes people to better projects, and pushes the project to fix the problem. reply HellzStormer 15 hours agorootparentprevI didn't use try Traefik's documentation, but the complains appear to be somewhat structural. Meaning a PR would need to possibly restructure at least part of the documentation, or add a whole section of documentation of a different type. You can't expect someone not core to a project to just propose to restructure the whole documentation. Which may also mean changing the website. And in any case, such overhaul coming from a \"nobody\" would very likely be rejected as being both too large or incomplete or not desirable. Re-structuring needs to be pushed for by at least one person from the core team. So yeah \"Just submit a PR\" in that context is not an answer, it's an excuse to avoid trying to understand the problem and actually improve the situation. reply joshmanders 15 hours agorootparentprevCustomer: I can't find anything I'm looking for in your store. Store: I spent a lot of time arranging things around in the store, if you can't find what you're looking for you can stop complaining and write signs for us. Customer: Or I can just use your competitor who actually cares about their customers. ¯\\_(ツ)_/¯ reply alex_lav 16 hours agorootparentprevYour adversarialism isn't a good look. Users are allowed to have opinions, this does not mean they have maintain the work themselves. reply deepersprout 17 hours agoparentprevI usually feed all the documentation to an llm and ask the question to the llm that fetches the info with rag. For example: > how do i configure Traefik to proxy traffic to two backend services? Configuring Traefik to proxy traffic to multiple backend services is a common use case. You can achieve this by defining multiple entry points and routers in Traefik's configuration, each pointing to a different backend service. Here's a basic example of how you can configure Traefik to proxy traffic to two backend services: yaml # traefik.yaml # Enable API and Dashboard api: dashboard: true # Define entry points entryPoints: web: address: \":80\" app1: address: \":8081\" app2: address: \":8082\" [...more stuff...] reply freedomben 17 hours agorootparentCan you describe more on your process? Which LLM are you using? Are you doing soething specific to make it us RAG or is that automagic (might be obvious depending on which LLM you are using but)? How do you feed the documentation in? for example, when the documentation has more than one page, how do you get that content in to the LLM? Is it part of the prompt or something you've tuned it on? have to clone the docs site, turn it into plan text and feed that in to the prompt or can you pass a URL and have it crawl ahead of time or something? This is the system I've been dreaming about but haven't had time to dig into yet. I've got ollama and openwebui set up now though, and with OpenAI getting bigger context windows it seems like it might be possible to inject the whole set of docs into the prompt, but I'm not sure how to go about that reply san-fran 17 hours agorootparentprevThis might be an additional way to quickly iterate on the content of the documentation itself. If I were the one writing, an easy test is passing the documentation to a lay person and asking them if they have what they need to perform X by following the documentation. Perhaps having a focused LLM generate the steps could help catch some documentation deficiencies. reply freedomben 17 hours agorootparent> If I were the one writing, an easy test is passing the documentation to a lay person and asking them if they have what they need to perform X by following the documentation. What kind of documentation is this though? Is this how to bake a cake or tie a necktie, or is it how to setup a reverse proxy for the services in your k8s cluster? If it's something a lay-person could do then I think this is a good strategy (though depending on the size/scope of the project/documentaiton it does seem like a pretty big effort to undertake without compensation), but if it's something highly technical like Traefik, I expect a lay-person to not even understand half the words/vocabulary in the documentation, let alone be able to perform X by reading it and following it. reply engine_y 17 hours agoprevWe've been using Traefik in prod for 2 years. While I used NGINX in the past, I decided to migrate to Traefik mainly because of the automatic let's encrypt integration. I am sorry for that decision. Traefik's documentation does not make sense to me or my team. It is finicky and misbehaves without proper logging. As an example - when I want to recreate the certificates - it fails sporadically leaving prod down for an indefinite amount of time. We're moving back to NGINX. reply ap-andersson 2 hours agoparentI have moved to Traefik from NGINX aswell because of the built-in support for DNS challenge and wildcard cert. I myself spent many hours trying to get it working for my domain I use at work. I used the same config I use at home (which works perfectly) but could never get it to actually do anything, even though the setup was identical. Same domain registrar with same API based on the same docker configs etc. Had all logs enabled and still I get no information what so ever about why my certificate could not be created. It simply defaulted back to its generated cert without trying it seemed. After two troubleshooting sessions and several hours of searching and troubleshooting I had to admit defeat and just use my own self-signed cert files. Very frustrating when you get no information about why it doesn't work. Just a silent failure and fallback. Overall that has been my biggest problem with traefik. Its awesome when it works, but when it does not I always seem to have problems troubleshooting and/or finding the information I need in the docs. At work we will start using Traefik in prod towards the end of the year. I hope Traefik and I will become better friends before that :) reply dirkt 1 hour agoparentprev> I decided to migrate to Traefik mainly because of the automatic let's encrypt integration. You probably already know and maybe it didn't work for you, but there's quite a few Docker companion containers that automate let's encrypt certs for an nginx Docker container. reply spyspy 16 hours agoparentprevI’ve always just used go’s built in reverse proxy if I need an API gateway. You can adapt it to meet any specific need, easily find libraries to do common tasks (CORS, rate limiting, retries, etc), and the best part: no configuration language. You just write go. reply jimmyl02 16 hours agorootparentcurious what are the performance characteristics here? I would assume something like Nginx that has been optimized over a longer period of time / a more specific use case would have non-negligible performance benefits at scale? reply spyspy 9 hours agorootparentNot everything needs to be at “scale”. I’ve deployed this pattern over 10k req/sec but it’s all about your SLOs. I’ve (thankfully) never needed to lose sleep over a millisecond or 2 in my line of work. reply 2NGINXSUX 6 hours agoparentprevWe’ve been using Nginx is prod for 3 years. While I used Traefik in the past, I decided to migrate to Nginx mainly because of its scriptability (Traefik plugins suck). I am sorry for that decision. Nginx’s documentation is absolute trash full of non-explanations (far worse than Traefik or Caddy. It’s finicky and misbehaves constantly. Lurking around every corner is a decision from 1995 sticking around in 2024; Nginx can barely function on the modern internet without _significant_ tuning. On top of it, the OpenResty community must be the rudest, most entitled people in the entire internet. Have a question, “YOURE DOING IT WRONG IDIOT” is the response. Of course every terrible decision they’ve made they justify with “BUT THE PERFORMANCE” as that’s the only thing worth considering. We’re moving back to Traefik, or Caddy, both still in POC. reply arush15june 18 hours agoprevI use caddy rather traefik. It's much easier to manage the Caddyfile compared to the traefik YAML config IMO, and we just keep three separate Caddyfiles for local, production and on-prem deployments. There are a plethora of great plugins, we use the coraza WAF plugin for caddy and it works well. reply pricci 18 hours agoparentI moved from Traefik to Caddy with caddy-docker-proxy for my self-hosting setup. All the features I need but *much* simpler. https://github.com/lucaslorentz/caddy-docker-proxy reply preya2k 12 hours agorootparentSame here. I enjoyed Traefik for being able to use docker tags for my reverse proxy configuration. The mechanism is great, however I did not like Traefiks internal config structure. Caddy is much easier for me to understand and matches my (small scale) use cases much better. Using Caddy via Docker labels through caddy-docker-proxy is about as perfect as it gets (for me). reply sureglymop 17 hours agorootparentprevLooks interesting but I don't see the benefits really. Still looks like a lot of labels exactly like with traefik. Why should one switch? reply BrandoElFollito 13 hours agorootparentHaving had used traefik, caddy and now caddy proxy, I like the latter because labels are simple pointers to actual caddy features (reasonably documented). I used to have all my docker compose files in elaborate structures but moved to portainer for simplicity. Together with caddy proxy it rocks (well, there are several things missing but I have hope) reply beestripes 20 hours agoprevWhy traefik over nginx for my modest needs, a couple docker hosts and a few dozen containers. I use https://github.com/NginxProxyManager/nginx-proxy-manager, would traefik provide a benefit on such a small scale? reply aedocw 19 hours agoparentI think https://github.com/caddyserver is the best option here. Automatic handling of SSL certs, it's incredibly lightweight, and has super clear config syntax. reply withinboredom 3 hours agorootparentIf only the caddy ingress were done. I’ve been waiting years for it. reply hoistbypetard 15 hours agorootparentprevThat’s exactly the situation I like Caddy in also. reply johnchristopher 19 hours agoparentprevI like traefik hot reload (among other things). Want to hide a service (the proxied app), a new route (a router in traefik terminology), a middleware (basic auth, https redirection, headers manipulation) ? Just drop the file and it gets automatically picked up, no need to reload traefik or that vhost. Truth is: I don't like nginx syntax and traefik is/was shiny :]. I went in for the LE renewal and containers, I stayed for the configuration style. reply drdaeman 15 hours agorootparentIt’s not that nice in practice. Traefik until 3.0 (which was released just a few days ago) wasn’t been able to reload TLS certificates under some circumstances: https://github.com/traefik/traefik/pull/9993 Built-in ACME support doesn’t work for me, so I still have some `systemctl restart traefik` hacks here and there. reply simonw 19 hours agoparentprevIf what you've got already works then no, I don't think you would see any benefit from switching. The moment you need a feature which Traefik provides that isn't in Nginx is when I would consider the switch. reply treyd 16 hours agorootparentBut what features does Traefik have that nginx doesn't? reply simonw 5 hours agorootparentI believe the biggest are automatic Lets Encrypt certificates and the ability to discover services and route to them based on things like Kubernetes labels. reply navels 15 hours agoparentprevI also use NginxProxyManager (8 hosts) and I'm not seeing any replies to your post that would explain why caddyserver or traefik provide any benefit over NPM. reply blinded 12 hours agoparentprevmetrics with non enterprise nginx are very limited. reply treyd 16 hours agoparentprevYeah I agree with this. Nginx config is easy and you can just set it and forget it. Most of the time you're copypasting from other configs you already have anyways. Automatic LE is kinda a strange selling point when Certbot is available everywhere and supports more scenarios. Traefik's and Caddy's selling points just don't make any sense to me because they don't make anything easier than the alternatives that are already widely supported. reply aaomidi 19 hours agoparentprevTraefik does certificate management for you reply psYchotic 20 hours agoprevI'm considering moving reverse proxying to Traefik for my self-hosted stuff. Unlike the article's author, I'm running containerized workloads with Docker Compose, and currently using Caddy with the excellent caddy-docker-proxy plugin. What that gets me, currently: - Reverse proxying, with Docker labels for configuration. New workloads are picked up automatically (but I do need to attach workloads to Caddy's network bridge). - TLS certificates - Automatic DNS configuration (using yet another plugin, caddy-dynamicdns), so I don't have to worry too much about losing access to my stuff if my ISP decides to hand me a different IP address (which hasn't happened yet) There are a few things I'm currently not entirely happy about my setup: - Any new/restarting workload makes Caddy restart entirely, resulting in loss of access to my stuff (temporarily). Caddy doesn't hand off existing connections to a new instance, unfortunately. - Using wildcard certs isn't as simple as it could/should be. As I don't want every workload to be advertised to the world through certificate transparency logs, I use wildcard certs, and that means I currently can't use simple Caddy file syntax I otherwise would with a cert per hostname. This is something I know is being worked on in Caddy, but still. Anyway, I've used Traefik in k8s environments before, and it's been fairly pleasant, so I think I'll give it a go for my personal stuff too! PS: Don't let this comment discourage you trying Caddy, it's actually really good! reply eropple 19 hours agoparentI use Caddy for single-purpose hosts and the like, but I 100% would throw Traefik at the problems you're describing--and I do, it's my k8s cluster ingest and it runs in my dev environments to enable using `localtest.me` with hostnames. It's worth kicking the tires on. Both are great at different things. reply sureglymop 17 hours agoparentprevI use (rootless) docker compose + traefik. Precisely because for wildcard certs it was really painless. Although I use my own DNS server and use RFC2136 DDNS for the LE DNS challenge. No plugins needed, really. I have basically one ansible playbook to set all this up on a vm including templating out the compose files. Then another playbook that can remove everything from the server again (besides data/mounts). For backups I use restic with a custom script that can back up files, different dbs etc to multiple locations. In the past I deployed k3s but I realized that was too much and too complicated for my self hosted stuff. I just want to deploy things quickly and not have to handle the certs myself. reply Cyykratahk 19 hours agoparentprevI've used caddy-docker-proxy in production and it doesn't cause Caddy to drop connections when loading a new config. I just tested it locally to check and it works fine. reply psYchotic 19 hours agorootparentHmm, I'll have to take a better look at my setup then, because it's a daily occurrence for me. Either I'm \"holding it wrong\" (which is admittedly possible, perhaps even likely given the comments here), or I have a ticket to open soon-ish. reply mynegation 20 hours agoparentprevI have not used Caddy, I use traefik and it discovers docker properties for configuration and TLS certificates with auto update. Not sure about dynamic DNS - I do not use it from Traefik. Adding and removing containers does not need a restart AFAIR. reply remram 20 hours agoparentprevThose are giant limitations. This is the first I hear of any reverse proxy that has to restart and drop connections to update configuration. That is usually the first, most fundamental part of any such server's design. reply mholt 19 hours agorootparentThat is absolutely not the case. Caddy config reloads are graceful and lightweight. I have no idea why this person is stopping their server instead of reloading the config. reply remram 15 hours agorootparentThat makes more sense. Maybe something with the Docker plugin? That or GP messed up. reply IggleSniggle 19 hours agorootparentprevCaddy doesn't have to restart, I think it's related to the specifics of their setup. The simple/easy path that gets a lot of people into caddy has a workflow that's more like, run caddy, job done. The next level is, give caddy super simple configuration file, reload caddy with \"caddy reload --config /etc/caddy/Caddyfile\". After that, you use the REST API to make changes to the server while it is running, which uses a JSON configuration definition instead of a Caddyfile, so it ends up being a jump for users. reply m_sahaf 19 hours agorootparent> After that, you use the REST API to make changes to the server while it is running, which uses a JSON configuration definition instead of a Caddyfile, so it ends up being a jump for users. You can, in fact, use any configuration format with the API as long as Caddy has its adapter compiled-in; you just have to use the correct value in the `Content-Type` header. For instance, you can use Caddyfile format using the `text/caddyfile` value in `Content-Type`. This is documented[0]. [0] https://caddyserver.com/docs/api#post-load reply jasoneckert 18 hours agoprevAnother thing worthy of note is that Traefik is configured by default in K3s. This has allowed K3s to be the quickest way to spin up a K8s cluster for testing, essentially allowing you to treat your cluster like cattle too. Simply add your deployment and associated service using NodePort, and you can access your app without worrying about the ingress controller. I use a shell script to spin up K3s clusters and test apps I specify as a positional parameter on demand (leveraging the ttl.sh ephemeral container registry). The same script tears down the cluster when finished. reply teekert 16 hours agoprevI have used traefik a lot. But I mostly got frustrated with all the docker-compose labels and layers and so many lines just to have a rev proxy. Then I found Caddy. Never looked back. I guess I was never the audience for Traefik. I just need an https enabled rev proxy. Or a basic-auth layer. In Caddy both are just 1 line, very concise, no layers (which I still don’t understand…) reply woopwoop24 1 hour agoprevi had such a hard time learning traefik and transitioning to V2. I do not fall into the standard case, wanting to use traefik for containers, not running on the same host (you cannot have labels annoted as the docs suggest, if the container is on another host) Docs were sparse and also not wanted to use the env vars for the traefik config as well, so took a bit of fumbling and reading and eventually i figured it out, but was almost on the verge of going back to haproxy reply djhworld 20 hours agoprevI've been using traefik for a few years for all my self hosted things. I abandoned the dynamic/discovery/docker labelling functionality though it was just too finicky and annoying to debug. Instead I generate a static config file using a template engine, pretty much all my things are just a combination of host/target/port so very easy to generate the relevant sections - I don't really have any complicated middlewares other than handling TLS. It sounds like the author of the linked post has taken the same route. The config gets generated through an ansible script and then gets copied to the machine where traefik is running - traefik watches the directory where that file is and auto-reloads on changes. It's been working great! reply silverquiet 20 hours agoprevI use Traefik in production (with containers), and my favorite aspect of it is that the configuration is carried via the labels on containers which means I rarely if ever need to make any modifications to the Traefik config itself. I'd say the biggest con is trying to figure out how to pronounce the name - I think it's just regular traffic, but I can't help wanting to call it \"trey-feek\" or something like that. reply arccy 13 minutes agoparentapparently \"traffic\" https://github.com/traefik/traefik/issues/795 reply fidotron 19 hours agoparentprevHeavy +1 on the labels thing. Reduces the scope of things to keep track of massively, even if writing them the first time is slightly harder because of the escaping and verbosity. I think a combination of traefik and docker compose are in the sweet spot for small scale self hosters that haven't reached the point where k8s will pay off. i.e. if you have less servers than a k8s HA control plane would use. reply silverquiet 19 hours agorootparentSmall-scale self hoster would certainly describe my situation (though we do have some of the same infrastructure issues as larger companies). We actually use Swarm which I generally like, but if it was my call we might have looked more at a simplified Kubernetes platform like K3s just because of a safety in numbers aspect. reply arcanemachiner 16 hours agoparentprevI just pronounce it \"traffic\". I'm not playing their damn head games. reply sureglymop 16 hours agoparentprevI would say the biggest con is that, if a container is not existing/running, traefik is not aware of it or its labels. Otherwise you could more easily do cool stuff like maintenance pages, bringing up containers on the first request after inactivity etc. So for me, I have been thinking about creating a plugin that is aware of where I store my compose files and can look at them instead. reply Projectiboga 19 hours agoparentprevae is closest to y, or hi. So Tryfik, is my guess, otherwise is Trayfik. If it's European fik, might be feek. *Just taking a guess here. reply psYchotic 19 hours agorootparent> ae is closest to y, or hi. So Tryfik, is my guess, otherwise is Trayfik. If it's European fik, might be feek. *Just taking a guess here. I wondered how to pronounce Traefik myself, so I started googling, and came across this: https://traefik.io/blog/how-to-pronounce-traefik-d06696a3f02... Tldr: just pronounce as you would \"traffic\". reply tazjin 19 hours agorootparentprevI think its \"träfik\", i.e. \"traffic\" with a German accent. reply dizhn 19 hours agoprevI use caddy wherever I can. That it can already handle automatic certificates is a big plus. Plus it's very easy to congiure. reply amne 3 hours agoparentI tried to get caddy to listen to both ports 80 and 443 in a cluster. I failed miserably. The documentation simply dismisses this as a possible scenario. reply jspdown 14 hours agoparentprevIf you like Caddy for it's ACME capabilities, then you might enjoy Traefik as well. It supports HTTP, TLS ALPN and DNS challenges and can be configured in one line as well. reply dmeijboom 2 hours agoprevI don’t get the appeal of Traefik. If you want an easy to use reverse proxy that works well, pick nginx. Want something simple for self-hosting? Take a look as caddy. For Kubernetes, try out Envoy Gateway. reply ofrzeta 16 hours agoprevIs it any better than HAProxy? HAProxy has served me well for at least a decade and has also been modernized for the cloud age with the runtime API that allows dynamic configuration. reply ljhtlajdfqasd 15 hours agoparentAll of these proxies seemed to have achieved feature parity within the last couple years. Where they seem differ is the licensing, enterprise model, source language, and data plane model (sidecar vs no sidecar). reply notoall 5 hours agoprevFor simple deployments, consider whether you need a reverse proxy at all. I have IPv6 everywhere, with each service getting its own IPv6 address. Each service is managed in inetd-style (via systemd-socket-proxyd ), and so essentially listens directly. For services that need to serve IPv4, I have a reverse proxy on my network edge that demuxes on TLS SNI to the corresponding IPv6 address. The advantage here is never having to deal with complex applications, with their complex and changing configuration. reply notpushkin 5 hours agoparentI'm using a reverse proxy just to terminate TLS. Pretty sure it is possible to do that at a service level, but don't think it's worth the trouble. reply Sincere6066 17 hours agoprevI'll stick with caddy. It's worked for me for years. reply riedel 19 hours agoprevFunnily I spend my weekend making a traefik config file to gitlab pages on a self hosted instance without pages enabled but using the artifact API. No code involved. Had to configure quite some rewriting logic and use three different plug-ins, which are mostly unmaintained. In the end probably something like nginx, Apache or caddy or a bit of code probably would have worked better, because of all the layering of different middleware. But it worked somehow. I guess it shines through still for easy SSL termination of docker and great observability. That is why at least I have been using it for the past years. reply MrOxiMoron 20 hours agoprevI love treafik, we use it with nomad/consul and docker to setup our whole infrastructure. The plugin system is also simple yet powerful and the dynamic configs are great for our customers custom domains, we can quickly see if a domain points to the right IP and put it in to get everything working. And of a domain no longer points to is we get a slack notification and it removes it from traefik so it no longer tries to get SSL certificates for it. reply 1oooqooq 1 hour agoprev> “Server Name Indication” (SNI) into the trash it goes. anyone who support https everywhere and ever slightly tolerates SNI is a fool. reply barbazoo 16 hours agoprevI’d stay away from it. The magical way to set it up via docker compose tags is nice but doesn’t allow for zero downtime deployment at least until recently. Getting true zero downtime deployments only worked with their file provider but that’s a bit archaic these days. reply rglullis 17 hours agoprevFor authentication, I had good luck with authentik as forward proxy. The one thing that bothers me with traefik is that their implementation of ACME does not work if you have some sort of DNS load balancing. I had one setup with three servers responding to the same domain. It seems the first request )to start the ACME dance) would go to one server, and if the second one (with the .well-known address) is sent to a different one, it will just return a 404 and fail the whole thing. Now I either have * to delegate the certificate management to the service itself or add Caddy as a secondary proxy just to get certificate from it. * Of course, someone smarter than me will point me to a better solution and I will be forever grateful. reply jackweirdy 16 hours agoparentIf I am not misunderstanding (sorry if I am) it sounds like you use the http challenge where your cert provider tries to GET your challenge file — if so, could the DNS challenge be better suited? There, you put the challenge in a TXT record value reply rglullis 16 hours agorootparentYou got it, but your solution won't work because of one detail: I can not use the DNS challenge because I am running a managed service provider, and my customers are the ones who own the domain. All I can do is ask them \"please add a CNAME to my gateway\", and I need to figure out everything else on my side. reply arccy 8 minutes agorootparentACME supports Delegated Domains for DNS01: _acme-challenge.customer.com IN CNAME _acme-challenge.your-automated-domain.org. reply francislavoie 10 hours agorootparentprevSounds like you're looking for Caddy's On-Demand TLS, then. No other server or ACME client does this. https://caddyserver.com/docs/automatic-https#on-demand-tls reply jspdown 14 hours agorootparentprevIt might not be suitable for your use case but, have you tried ACME DNS challenge delegation to a different one hosted by yourself? reply cab404 15 hours agoprevSomehow, I find myself using Caddy everywhere I would use Træfik in the past. reply chadsix 19 hours agoprevYou can also use Cloud Seeder [1] which might be easier since it gives each container a dedicated IP.[1] https://github.com/ipv6rslimited/cloudseeder reply wg0 19 hours agoprevSide question - what people use to hide (and make accessible) the internal services such as grafana, prometheus, rabbit mq (the web interface) and such? Should they be public behind such a proxy? (seems odd) Or should they be totally internal and then setup a Wireguard VPN to reach them? reply withinboredom 3 hours agoparentThey are open to the internet but each ingress is using the “external auth” feature of nginx ingress, pointing to our internal login. There’s no vpn or magic ip addresses. Once you’re logged in, you can access whatever you need. reply mrj 16 hours agoparentprevCloudflare tunnels are super convenient and provide lots of auth mechanisms. If you set up a tunnel using cloudflared and proxy the IP through cloudflare, there's nothing even exposed directly to the internet. You can even have different auth requirements for urls (like /admin) or punch holes for stuff like webhooks. I have set up quite a few as kubernetes pods that direct to private hostnames in different namespaces and pretty happy with it for internal apps. reply section_me 19 hours agoparentprevAuth forwarding[1] is normally the route. This allows you to basically zero auth your services. You can also use wireguard or tailscale[2] [1] https://doc.traefik.io/traefik/middlewares/http/forwardauth/ [2] https://doc.traefik.io/traefik/master/https/tailscale/ reply Hrun0 19 hours agoparentprev> what people use to hide (and make accessible) the internal services such as grafana, prometheus, rabbit mq (the web interface) and such? Proxies or VPNs like you mentioned. You usually don't expose things if you don't have to. reply nullify88 16 hours agoparentprevFor the purposes of some of my self hosted stuff, I wanted to see how far I could go without VPN and instead use mutual tls authentication with my stuff exposed to the internet. Client certs are issued by cert manager in my k8s cluster and traefik does my TLS Auth. reply pyr0hu 19 hours agoparentprevWe use tailscale for this exact use case and has been working flawlessly so far. You can even set up ACL lists as a firewall. reply John23832 19 hours agoparentprevFrom the internet? Drop them at the ingress level (if using kubernetes). You could also do some ip filtering. Then use an internal proxy (or internal ip of some kind) to reach them. For proof of concepts, I use cloudflare tunnels which allows you to add ACLs to particular routes. reply waldrews 19 hours agoparentprevServe them on a firewalled port, then: 1) VPN if you need to expose them to multiple trusted users, 2) firewall rules to make them accessible to your IP range, or (probably easiest), 3) access them by ssh tunnel. reply blinded 12 hours agoparentprevzero trust, host firewalls, mtls, ssh tunnels, bastion hosts. reply firesteelrain 13 hours agoprevWe just started running Traefik in production since looking at self managed K8s was just too hard and complicated for what we were trying to do. We have an Ansible Docker compose service (that’s what we call it), that starts up the containers and auto registers the containers with Traefik. It works really well. We are airgapped so can’t use Let’s Encrypt. We inject the certs into our containers via Ansible or Docker Compose. reply methou 19 hours agoprevThe only problem I'm having with it is that it doesn't support unix domain socket[0], in a \"cloud native\" environment you rarely need it but if you are using single node this can be sweet. -- [0]: https://github.com/traefik/traefik/issues/4881 reply meonkeys 18 hours agoparentCould you say more about how a non-network socket would be beneficial? I'm guessing simpler code and lower resource usage, but I'm curious what you're interested in. And by \"single node\", do you mean one server / one user (even if the user is, say, a single API consumer or whatever), or something else? reply btbuilder 19 hours agoprevWhen looking for a reverse proxy that is performant on Windows and Linux around 5 or 6 years ago the options were very limited. Traefik is what we ended up using. I haven’t checked recently but at the time nginx on Windows used select() and envoy was either beta or needed a recent version of the Windows kernel that not all customers were running. We still use it today. reply brainzap 12 hours agoprevIt would be nice if proxies are opinionated about typical URL usecases and offer an easy way to redirect www to non-www or handle path with missing slash. reply vedmed 14 hours agoprevI needed a reverse proxy the other week. OPNSense is my firewall. I tried traefik, but it was too complicated. So I installed caddy, and it was easy as pie. My .02 reply muhehe 18 hours agoprevIn the future our company will migrate to k8s. It looks like it will be openshift, specifically. Do we need this in openshift or is there some \"native\" mechanism baked in? reply verdverm 18 hours agoparentYou'll likely have an ingress controller provided with openshift, which tends to be more batteries included. There are quite a few options: https://kubernetes.io/docs/concepts/services-networking/ingr... reply siva7 19 hours agoprevIt’s nice if you’re running a bare metal server on hetzner or DO but in the age of cloud platforms like aws or azure there is hardly a need for traefik. reply PennRobotics 3 hours agoparentEven on Hetzner, it's not amazing and not a one-click workflow. Load their Photoprism image on a standard server with only IPv6 (as a v4 address costs extra) and certificates will not get generated; logs point to Traefik although the solution is modifying Dockerfiles; thanks Dockerphiles, for insisting your software is the answer to everything server... reply kopadudl 20 hours agoprevWhen my company looked at different proxies for k8s, we ended upon traefik cause we had experience from docker swarm and it has a dashboard. reply iansinnott 20 hours agoprev> Traefik is more comparable to HAProxy than to nginx/caddy/apache2 Aren't caddy and traefik fairly comparable? I've only used them both lightly so I may be missing the core point of each, but I thought of them as very similar. reply candiddevmike 20 hours agoparentTraefik can't serve static files, or interact with CGI providers like PHP. reply justusthane 20 hours agoparentprevThe rest of the sentence you quoted explains that nginx, Caddy, and Apache are all webservers (which can also reverse proxy). Traefik and HAproxy are only reverse proxies and not webservers. reply IggleSniggle 19 hours agorootparentHAProxy can be a web server though, albeit it is not designed to operate this way and thus requires some goofy configuration to make happen. I only know this because it was useful for me while working on a HAProxy extension. reply thinkmassive 20 hours agoparentprevCaddy is primarily a web server like nginx and apache httpd. Traefik and HAproxy are primarily reverse proxies. reply mholt 19 hours agorootparentCaddy is actually used as a reverse proxy more than a static file server. It's equally excellent and proficient as both! Caddy's functionality is comparable to nginx, apache httpd, and haproxy. reply indigodaddy 18 hours agorootparentAnd while we’re at it, it can even forward proxy recentlyish I believe? reply mholt 13 hours agorootparentYeah, Caddy v1 had a forwardproxy plugin that finally got updated for v2: https://github.com/caddyserver/forwardproxy/ reply mkesper 20 hours agoparentprevCaddy is at the same level as nginx/apache. It is able to do everything a web server is expected to (serving web sites, files and proxying services) plus handling LetsEncrypt automatically. It does not, afaik, do dynamic service discovery like traefik nor load balancing of TCP at the protocol layer, like e.g. haproxy. https://caddyserver.com/features reply mholt 19 hours agorootparentCaddy can absolutely do both of those things. - https://caddyserver.com/docs/modules/http.reverse_proxy.upst... - https://github.com/mholt/caddy-l4 reply jiehong 24 minutes agorootparentcaddy-l4 still shows: This app is very capable and flexible, but is still in development. Please expect breaking changes. And it does not seem to be really seeing many new commits recently, so it feels pretty much \"beta\" at this point. reply lmeyerov 18 hours agorootparentprevWe are long-time fans of Caddy, preferring it over traefik + nginx especially for our docker-compose flows.. though it's fair to distinguish 'can' vs 'easy to do' E.g., we can imagine writing or using a plugin to figure out some upcoming fancy sticky session routing logic based on routes/content vs just the user IP, but there are easier and more 'with the grain' solutions than with what Caddy exposes today, afaict (Agreed tho: The reverse proxy module, for more typical cases, is awesome and we have been enjoying for years!) reply francislavoie 10 hours agorootparentSticky sessions are supported: https://caddyserver.com/docs/caddyfile/directives/reverse_pr..., and yes it's pluggable so you could write your own LB policy. Very easy, just copy the code from Caddy's source to write your own plugin. Let us know if you need help. Also yes, Caddy does service discovery if you use https://github.com/lucaslorentz/caddy-docker-proxy, configuration via Docker labels. Or you can use dynamic upstreams (built-in) https://caddyserver.com/docs/caddyfile/directives/reverse_pr... to use A/AAAA or SRV DNS records to load your list of upstreams. reply lmeyerov 1 hour agorootparent`query [key] ` A+++, I missed when this got added, amazing! (this basically solves the sticky shared chatroom websocket problem for us when routing to resources with gravity, in our case, multiuser data science notebooks!) reply baobun 19 hours agorootparentprevJust to add on, haproxy does service discovery too. https://www.haproxy.com/blog/consul-service-discovery-for-ha... reply lakomen 17 hours agoprevTraefik is considerably slower and more resource hungry than nginx. There is nothing more to say. reply cagenut 19 hours agoprevIn a mirror/reverse of the OPs premise - I always wondered why so many of these open source http reverse proxies sprung up in the container era, like what did they offer that varnish or a vmod to varnish wasn't already doing or capable of? somehow varnish almost completely missed the container era, despite seemingly being the exact type of tool a bunch of teams would go on to create. reply Starlevel004 19 hours agoparentDevops guys are mostly incapable of using any service that isn't a) written in Go and b) configured using a YAML-based DSL. reply TNorthover 19 hours agorootparentTraefik's YAML does a particularly bad job at keeping syntax (such as it is) separate from user-defined labels, I feel. Very difficult to just look at a file and see which bits are labels for the sake of it, and which bits are direct instructions to builtin features. reply lmeyerov 18 hours agoparentprevFor Caddy, LetsEncrypt: Free TLS in one line without talking to anyone For Traefik, afaict, something about k8s reply znpy 19 hours agoprev [–] > you mount the docker socket into the traefik container and gain the ability to auto-detect other containers that you might want to expose using traefik. Totally not a security issue. Source: trust me bro. reply xorax 19 hours agoparent [–] https://github.com/traefik/traefik/issues/4174 reply meonkeys 17 hours agorootparent [–] Related: https://doc.traefik.io/traefik/providers/docker/#docker-api-... https://www.reddit.com/r/Traefik/comments/g46lhh/does_bindin... https://github.com/wollomatic/traefik-hardened reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article explores the use of Traefik, a widely used proxy server, beyond container environments, showcasing its ability to automatically handle TLS certificates and proxy services without relying on a container engine.",
      "Traefik can be set up through configuration files, offering features like TLS Passthrough and support for the PROXY protocol, though lacking functionalities such as authentication and IP blocking.",
      "The author emphasizes Traefik's strength, user-friendliness, and high-quality documentation, providing a configuration sample to demonstrate its setup process."
    ],
    "commentSummary": [
      "Users express frustrations with navigating complex documentation for tools like Traefik and Ansible, underscoring the need for more user-friendly and comprehensive guides.",
      "Discussions cover challenges with YAML configuration, issues with Ansible docs, and the significance of well-organized reference documentation.",
      "The conversation delves into experiences with Nginx, Caddy, and Traefik, comparing features, advantages, and limitations for managing SSL certificates and proxying services effectively in a Docker environment."
    ],
    "points": 283,
    "commentCount": 171,
    "retryCount": 0,
    "time": 1714908677
  },
  {
    "id": 40269489,
    "title": "Master Deep Reinforcement Learning Algorithms",
    "originLink": "https://github.com/alessiodm/drl-zh",
    "originBody": "Deep Reinforcement Learning: Zero to Hero! Welcome to the most hands-on reinforcement learning experience! This is a short and practical introductory course on foundational and classic deep reinforcement learning algorithms. By the end of the course, you will have written from scratch algorithms like DQN, SAC, PPO, as well as understood at a high-level the theory behind them. We will be able to train an AI to play Atari games and land on the Moon! Environment Setup To make sure we can focus on learning, the environment setup is opinionated 😊 Here it is: Install Miniconda Why conda? Because it's a full envinronment manager, and we can choose the Python version too. Checkout this Git repository, and cd into its folder. Create and activate the drlzh virtual environment: conda create --name drlzh python=3.11 conda activate drlzh Install Poetry and install dependencies: Dependencies include gymnasium[accept-rom-license] for Atari. Make sure to accept the license agreement when installing the dependencies of the project via Poetry. pip install poetry poetry install Install Visual Studio Code How Do I Start? Open this repository folder in Visual Studio Code (make sure to keep the .vscode folder for settings consistency, running on Jupyter might require some tweaks to code and imports). Open the first 00_Intro.ipynb notebook in Visual Studio Code, and follow along! From there, just keep moving on to the next notebooks. If you get stuck, feel free to check the /solution folder. For an expanded treatment and step-by-step coding, check out the YouTube videos!",
    "commentLink": "https://news.ycombinator.com/item?id=40269489",
    "commentBody": "Deep Reinforcement Learning: Zero to Hero (github.com/alessiodm)274 points by alessiodm 10 hours agohidepastfavorite30 comments alessiodm 10 hours agoWhile trying to learn the latest in Deep Reinforcement Learning, I was able to take advantage of many excellent resources (see credits [1]), but I couldn't find one that provided the right balance between theory and practice for my personal experience. So I decided to create something myself, and open-source it for the community, in case it might be useful to someone else. None of that would have been possible without all the resources listed in [1], but I rewrote all algorithms in this series of Python notebooks from scratch, with a \"pedagogical approach\" in mind. It is a hands-on step-by-step tutorial about Deep Reinforcement Learning techniques (up ~2018/2019 SoTA) guiding through theory and coding exercises on the most utilized algorithms (QLearning, DQN, SAC, PPO, etc.) I shamelessly stole the title from a hero of mine, Andrej Karpathy, and his \"Neural Network: Zero To Hero\" [2] work. I also meant to work on a series of YouTube videos, but didn't have the time yet. If this posts gets any type of interest, I might go back to it. Thank you. P.S.: A friend of mine suggested me to post here, so I followed their advice: this is my first post, I hope it properly abides with the rules of the community. [1] https://github.com/alessiodm/drl-zh/blob/main/00_Intro.ipynb [2] https://karpathy.ai/zero-to-hero.html reply tunnuz 3 hours agoparentDoes it rely heavily on python, or could someone use a different language to go through the material? reply verdverm 10 hours agoparentprevvery cool, thanks for putting this together It would be great to see a page dedicated to SoTA techniques & results reply alessiodm 10 hours agorootparentThank you so much! And very good advice: I have an extremely brief and not-descriptive list in the \"Next\" notebook, initially intended for that. But it definitely falls short. I may actually expand it in a second \"more advanced\" series of notebooks, to explore model-based RL, curiosity, and other recent topics: even if not comprehensive, some hands on basic coding exercise on those topics might be of interest nonetheless. reply viraptor 9 hours agoprevIn case you want to expand to more chapters one day: there's lots of tutorials of doing the simple things that has been verified to work, but if I'm struggling it's normally with something people barely ever mention - what to do when things go wrong. For example your actions just consistently get stuck at maximum. Or the exploration doesn't kick in, regardless how noisy you make the off-policy training. Or ... I wish there were more practical resources for when you've got the basics usually working, but suddenly get issues nobody really talks about. (beyond \"just tweak some stuff until it works\" anyway) reply alessiodm 9 hours agoparentThanks a lot, and another great suggestion for improvement. I also found that the common advice is \"tweak hyperparameters until you find the right combination\". That can definitely help. But usually issues hide in different \"corners\", both of the problem space and its formulation, the algorithm itself (e.g., just different random seeds have big variance in performance), and more. As you mentioned, in real applications of DRL things tend to go wrong more often than right: \"it doesn't work just yet\" [1]. And my short tutorial definitely lacks in the area of troubleshooting, tuning, and \"productionisation\". If I carve time for expansion, this will likely make top of list. Thanks again. [1] https://www.alexirpan.com/2018/02/14/rl-hard.html reply ubj 7 hours agorootparentThanks for sharing [1], that was a great read. I'd be curious to see an updated version of that article, since it's about 6 years old now. For example, Boston Dynamics has transitioned from MPC to RL for controlling its Spot robots [2]. Davide Scaramuzza, whose team created autonomous FPV drones that beat expert human pilots, has also discussed how his team had to transition from MPC to RL [3]. [2]: https://bostondynamics.com/blog/starting-on-the-right-foot-w... [3]: https://www.incontrolpodcast.com/1632769/13775734-ep15-david... reply alessiodm 6 hours agorootparentThank you for the amazing links as well! You are right that the article [1] is 6 years old now, and indeed the field has evolved. But the algorithms and techniques I share in the GitHub repo are the \"classic\" ones (dating back then too), for which that post is still relevant - at least from an historical perspective. You bring up a very good point though: more recent advancements and assessments should be linked and/or mentioned in the repo (e.g., in the resources and/or an appendix). I will try to do that sometime. reply zaptrem 8 hours agoprevI spent three semesters in college learning RL only to be massively disappointed in the end after discovering that the latest and greatest RL techniques can’t even beat a simple heuristic in Tetris. reply jmward01 7 hours agoparentI modeled part of my company's business problem as a MAB problem and saved my company 10% off their biggest cost and, just as important, showcased an automated truth signal that helped us understand what was, and wasn't, working in several of our features. Like all tools, finding the right place to use RL concepts is a big deal. I think one thing that is often missed in a classroom setting is pushing more real world examples of where powerful ideas can be used. Talking about optimal policies is great, but if you don't help people understand where those ideas can be applied then it is just a bunch of fun math. (which is often a good enough reason on its own :) reply smokel 4 minutes agorootparentFor those not in the know, \"MAB\" is short for Multi-Armed Bandit [1], which can be considered as a decision-making framework similar to reinforcement learning. In my limited understanding, MAB problems are generally simpler than those tackled by Deep Reinforcement Learning (DRL), because typically there is no state involved in bandit problems. However, I have no idea about their scale in practical applications, and would love to know more about said business problem. [1] https://en.wikipedia.org/wiki/Multi-armed_bandit reply alessiodm 8 hours agoparentprevRL can be massively disappointing, indeed. And I agree with you (and with the amazing post I already referenced [1]) that it is hard to get it to work at all. Sorry to hear you have been disappointed so much! Nonetheless, I would personally recommend even just learning the basics and fundamentals of RL. Beyond supervised, unsupervised, and the most-recent and well-deservedly hyped semi-supervised learning (generative AI, LLMs, and so on), reinforcement learning indeed models the learning problem in a very elegant way: an agent interacting with an environment and getting feedback. Which is, arguably, a very intuitive and natural way of modeling it. You could consider backward error correction / propagation as an implicit reward signal, but that would be a very limited view. On a positive note, RL has very practical sucessful applications today - even if in niche fields. For example, LLM fine-tuning techniques like RLHF successfully apply RL to modern AI systems, companies like Covariant are working on large robotics models which definitely use RL, and generally as a research field I believe (but I may be proven wrong!) there is so much more to explore. For example, check Nvidia Eureka that combines LLM to RL [2]: pretty cool stuff IMHO! Far from attempting to convince you on the strength and capabilities of DRL, just recommending folks to not discard it right away and at least give it a chance to learn the basics, even just for an intellectual exercise :) Thanks again! [1] https://www.alexirpan.com/2018/02/14/rl-hard.html [2] https://blogs.nvidia.com/blog/eureka-robotics-research/ reply chaosprint 5 hours agoprevGreat resources! Thank you for making this. I'm attaching here a DRL framework I made for music generation, similar to OpenAI Gym. If anyone wants to test the algorithms OP includes, you are welcome to use it. Issues and PRs are also welcome. https://github.com/chaosprint/RaveForce reply achandra03 8 hours agoprevThis looks really interesting! I tried exploring deep RL myself some time ago but could never get my agents to make any meaningful progress, and as someone with very little stats/ML background it was difficult to debug what was going wrong. Will try following this and seeing what happens! reply barrenko 21 minutes agoparentI mean, resources like these are great, but RL in itself is quite dense and topic heavy, so not sure there is any way to reduce the inherent difficulty level, any beginner should be made clear to that. That's my primary gripe with ML topics (especially RL related). reply alessiodm 8 hours agoparentprevThank you very much! I'd be really interested to know if your agents will eventually make progress, and if these notebooks help - even if a tiny bit! If you just want to see if these algorithm can even work at all, feel free to jump on the `solution` folder and pick any algorithm you think could work and just try it out there. If it does, then you can have all the fun rewriting it from scratch :) Thanks again! reply malomalsky 2 hours agoprevIf there anything like that, but for NLP? reply barrenko 19 minutes agoparentThere's the series this material references - \"Neural networks: zero to hero\" that has GPT related parts. reply levocardia 6 hours agoprevThis looks great - maybe add a link to the youtube videos in the README? reply alessiodm 6 hours agoparentThank you so much! Unfortunately, that is a mistake in the README that I just noticed (thank you for pointing it out!) :( As I mentioned in the first post, I didn't get to make the YouTube videos yet. But it seems the community would be indeed interested. I will try to get to them (and in the meantime fix the README, sorry about that!) reply bluishgreen 8 hours ago [flagged]prev [10 more] \"Shamelessly stole the title from a hero of mine\". Your Shamelessness is all fine. But at first I thought this is a post from Andrej Karpathy. He has one of the best personal brands out there on the internet, while personal brands can't be enforced, this confused me at first. reply alessiodm 7 hours agoparent [–] TL;DR: If more folks feel this way, please upvote this comment: I'll be happy to take down this post, change the title, and either re-post it or just don't - the GitHub repo is out there - that that should be more than enough. Sorry again for the confusion (I just upvoted it). I am deeply sorry about the confusion. And the last thing I intended was to grab any attention away from Andrej, and / or being confused with him. I tried to find a way to edit the post title, but I couldn't find one. Is there just a limited time window to do that? If you know how to do it, I'd be happy to edit it right away in case. I didn't even think this post would get any attention at all - it is my first post indeed here, and I really did it just b/c if anybody could use this project to learn RL I was happy to share. reply ultra_nick 7 hours agorootparentDidn't \"Zero to Hero\" come from Disney's Hercules movie before Karparthy used it? reply alessiodm 7 hours agorootparentDidn't know that, but now I have an excuse to go watch a movie :D reply gradascent 7 hours agorootparentprevI didn't find it confusing at all. I think it's totally ok to re-use phrasing made famous by someone else - this is how language evolves after all. reply alessiodm 7 hours agorootparentThank you, I appreciate it. reply khiner 7 hours agorootparentprevThrowing in my vote - I wasn’t confused, saw your GH link and a “Zero to Hero” course name on RL, seems clear to me and “Zero to Hero” is a classic title for a first course, nice that you gave props to Andrea too! Multiple people can and should make ML guides and reference each other. Thanks for putting in the time to share your learnings and make a fantastic resource out of it! reply alessiodm 7 hours agorootparentThanks a lot. It makes me feel better to hear that the post is not completely confusing and appropriating - I really didn't mean that, or to use it as a trick for attention. reply FezzikTheGiant 7 hours agorootparentprev [–] this is a great resource nonetheless. Even if you did use the name to get attention how does it matter? I still see it as a net positive. Thanks for sharing this reply alessiodm 7 hours agorootparent [–] Thank you! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"Deep Reinforcement Learning: Zero to Hero!\" course provides a hands-on experience with fundamental deep reinforcement learning algorithms such as DQN, SAC, and PPO.",
      "Participants will be able to train an AI to play Atari games and complete tasks like landing on the Moon, with the environment setup involving tools like Miniconda, Poetry, and Visual Studio Code.",
      "Support resources, including notebooks, solutions folder, and YouTube videos, are accessible to aid participants throughout the learning process."
    ],
    "commentSummary": [
      "The post on github.com/alessiodm covers creating Python notebooks on Deep Reinforcement Learning, emphasizing theory and practical application.",
      "Readers engage by offering feedback and suggestions, focusing on common Deep Reinforcement Learning (DRL) challenges like tweaking hyperparameters and troubleshooting for real-world scenarios.",
      "The discussion includes Multi-Armed Bandit (MAB) problems' relevance in decision-making, learning frameworks, and potential applications in robotics and music generation, along with debates on language evolution when reusing famous phrases."
    ],
    "points": 274,
    "commentCount": 30,
    "retryCount": 0,
    "time": 1714950748
  },
  {
    "id": 40266728,
    "title": "Microsoft CTO's Insights on OpenAI: A 2019 Tweet Revived in 2024",
    "originLink": "https://twitter.com/techemails/status/1787176471146156193",
    "originBody": "Microsoft CTO: \"Thoughts on OpenAI\"June 12, 2019 pic.twitter.com/xpwWZJsalJ— Internal Tech Emails (@TechEmails) May 5, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=40266728",
    "commentBody": "Microsoft CTO: Thoughts on OpenAI (2019) (twitter.com/techemails)273 points by mfiguiere 16 hours agohidepastfavorite125 comments jasode 14 hours agoTo add more context, about a month later after Kevin Scott's email... July 12, 2019 OpenAI announces Microsoft's $1 billion investment - https://openai.com/index/microsoft-invests-in-and-partners-w... - https://news.microsoft.com/2019/07/22/openai-forms-exclusive... It was interesting how K Scott was objective and candid about Microsoft's internal ai efforts falling behind but also phrasing the email diplomatically to not criticize anyone. It's interesting to see some of the behind-the-scenes thinking. reply ttul 13 hours agoparentIt’s fair to say that the CTO’s job is to make sure Microsoft never misses a trend. Their number one existential threat is a new tech trend that makes their existing model fail. Whether that thing is the rise of smart phones or generative AI, Kevin Scott’s job is to give early warning. reply sharkweek 7 hours agorootparent“Do you care to know why I’m in this chair… why I earn the big bucks? I'm here for one reason and one reason alone. I'm here to guess what the music might do a week, a month, a year from now, that’s it, nothing more.” John Tuld, Margin Call I think about this movie all the time now that I’ve been in the corporate world for a while and I have realized one consistent outcome: the leaders who “guess” right a lot survive a hell of a lot longer than those who get it wrong. reply kaliqt 2 hours agorootparentThat is one of the best movies I've ever seen, and I've seen a lot of movies. But I am a little biased since I like that kind of stuff. reply Shrezzing 21 minutes agorootparentI think in the coming decades, that film will be looked back on as the best about financial crises. The Big Short is obviously impressive for being accessible, but Margin Call is on another level for its exploration of the ethics of the financial system at the time. The view of moral hazard that the film puts forward is brilliant. Paul Bettany's character in the Brooklyn Bridge scene perfectly captures the attitude of the financial market in the months before the crash [1]. Irons' ending scene about \"it's just money, it's made up... it's not wrong\" [2] right after tanking the world economy is grotesque. Its view of social product, and its appropriation by Wall Street, is equally amazing, with the astrophysicist[3] and the Civil Engineer[4] being attracted to Wall Street because the money is better. It's a subtle critique of Wall Street's priority being out of alignment with that of society more broadly. That this film is so widely celebrated by Wall Street is hilarious in a way. It's a damning indictment of the industry, and they seem to absolutely love it. There's spoilers in all of these links: [1] https://www.youtube.com/watch?v=2f2kGHcdJYU [2] https://youtu.be/IAqAl292ozs?t=206 [3] https://www.youtube.com/watch?v=Elyfo1DIlzs&t=78 [4] https://www.youtube.com/watch?v=VR4moVBiHGg reply elteto 4 hours agorootparentprevI could perfectly hear in my head Jeremy Irons delivering those lines. What a fantastic scene. reply penguin_booze 3 hours agorootparent\"And please, speak as you might, to a young child; or, a Golden Retriever\" -- I, when I'm reading academic papers. Also I, when I'm trying to review a pull request. reply danybittel 3 hours agorootparentprevIsn't that exactly what an LLM does? Predicting the next token? reply hosh 14 hours agoparentprevI don’t think they want a repeat of missing out on web and then mobile again. reply pavlov 13 hours agorootparentMicrosoft didn’t miss out on the web. They won it so thoroughly that they assumed it was theirs to keep and stopped competing. reply mepian 11 hours agorootparentThey did fall behind initially, before \"The Internet Tidal Wave\" memo was written and Microsoft fully mobilized to fight Netscape. Bill Gates was convinced that proprietary networks like the original MSN were the future, but he changed his mind fairly quickly. reply cma 40 minutes agorootparentThat sort of was the future, with Facebook reviving the proprietary network. reply dboreham 10 hours agorootparentprevParent is referring to the decade before that. reply pavlov 4 hours agorootparentBerners-Lee created the WWW in 1991. Netscape 1.0 launched in December 1994. Bill Gates’s big Internet turnaround came in 1995. By 1999, IE was the dominant browser and Netscape was going broke. Things moved so fast at the time, it left the impression that Microsoft was slow to react. But actually everything happened within just a few years. reply Keyframe 11 hours agorootparentprevSeems we collectively forgot about IE era. reply dinobones 4 hours agorootparentThe IE era, a shoddy browser by a sleepy dormant pseudo-monopoly, is what allowed for Chrome to be so good. Lots of low hanging fruit in optimizing browser rendering and JavaScript. Google engineers plucked these fruit. Pichai took all the credit. In an alternate universe, some IE PM focused on making a great, fast browser, Chrome would’ve been killed by Google after not taking sufficient market share to deem worthwhile, and Pichai would be working at Oracle as an L5 TPM. reply hnlmorg 1 hour agorootparentIt was Firefox, and to a lesser extent Safari, that initially calved into IE. While Chrome did indeed strike the fatal blow, it came a little later into fight. You also missed the GPs point, the reason Microsoft could be “sleepy dormant” on the web was because they had already won the browser wars and because the dominant platform for a decade. And that wasn’t by accident, it was very much a strategic move to conquer the browser market. reply esafak 9 hours agorootparentprevNightmares are best forgotten. reply metaltyphoon 7 hours agorootparentYou still live on them but now it is called Safari reply llm_trw 6 hours agorootparentprevThere are multiple eras here. MS missed the tpc/ip train completely and ended up just copy pasting the stack from one of the BSDs in Windows 95 I think, or it wasn't a default install, I forget which. They were late to the http train and had to give away explorer for free. You can tell how desperate they were since they didn't do the same thing with office. Then between 1999 and 2012 they completely dominated. Then they missed the mobile train completely and lost the browser wars. And here we are. I blame IE being free for the state of the internet we have today. If people were trained to pay for it then we wouldn't have spyvertisment as the only model for it. reply Sakos 3 hours agorootparentThe death knell of MS's mobile ambitions came when the iPhone was released mid 2007. Windows Phone 7 launched end of 2010 as an unholy abomination based on Windows CE and a Zune-based interface, which created a whole generation of phones that would be unsupported by the NT-based Windows Phone 8 (released 2 years later), which is what they should've released in the first place. reply sunaookami 4 hours agorootparentprevThey didn't miss out on mobile. They had a strong mobile system with more than 10% market share in Europe but they only focused on the US market where it was not successful. Then they killed the platform with the god awful Windows 10 Mobile. The biggest enemy of Windows Phone was Microsoft themselves. reply omeze 14 hours agorootparentprevWas gonna say, the email reads similarly to this famous thread about Java (with the old CTO of MS!): https://www.techemails.com/p/bill-gates-im-literally-losing-... Really makes you think about the structure of mega corps and how powerful the “defender’s advantage” is. These giants knowingly sleep on disruption and wait to time their entry, and are generally rewarded. I dont know if its good or bad, it probably depends, but I think the capitalism game devs need some balance tweaks. reply FredPret 11 hours agorootparentThe defender's advantage only counts if the corporation is run by people smart enough to do something about it. History is littered with out-innovated companies that are gone or clinging to relevance. Xerox, IBM, cable companies, telephone companies, every train company ever. All had a huge opportunity to disrupt their own business model and make even more money, almost none of them managed it. reply amscanne 9 hours agorootparentIt's funny that people only seem to remember the successes. Seemingly dominant giants fall all the time. Kodak, Nortel, Blockbuster, Blackberry, ... reply vikramkr 13 hours agorootparentprevAre they generally rewarded? Wondering is there are statistics on that because the narrative is usually that defenders end up unable to compete even with technologies they themselves invented and die out, with maybe apple and Microsoft recently breaking that trend. reply teaearlgraycold 11 hours agorootparentI think in the long term Google could still win the AI wars. They're behind right now but they have the talent, money, and infrastructure to win. That said, I would absolutely not bet on it. I sold almost all of my GOOG. reply Keyframe 11 hours agoparentprevThat's what struck me as well. It's extremely important for such positions to be clear and open about their position and not masking their reality. You'll often find marketing speak internally between levels which masks truth and then hinders actual strategic moves. In this context CTO would've said they're not that far off, if only this and that.. but no, he fessed up and CEO also understood; raw awareness. reply ein0p 5 hours agoprevGoogle was a powerful place to work at 10 years ago. At Microsoft you had to convince people to give you quota for a few cores and wait for days or even weeks, if they even agree at all. At Google you could spin up a distributed batch job processing terabytes of data during your orientation codelab, literally on the first day. The contrast was very stark. Google was a very low friction environment where things just happened, and the environment was set up such that you couldn’t help but ship something. Note the past tense. reply bushbaba 4 hours agoparentInternal research had low friction external product not so much. They even had a tool, ariane, to help you navigate the product launch friction. reply ein0p 3 hours agorootparentIMO Ariane was pretty straightforward and reasonable, all things considered. In fact it’s preferable to navigating the usual wobbly maze of humans each of which takes days to respond. reply fortenforge 11 hours agoprevThese emails were released as part of the antitrust lawsuit against Google currently being pursued by the FTC. It seems to me that contrary to the FTC's claims about how Google's monopoly power leads it to stop innovating, exactly the opposite is true. If Google had stopped innovating it's clear that Bing eventually would have caught up in terms of quality. As these emails make clear though, Google kept its lead by continuing to invest in cutting-edge AI research. Indeed, if anything it's Microsoft who should be scrutinized. reply advael 6 hours agoparentBoth are acting with monopolistic power in numerous areas and both should be scrutinized. Just because one argument against monopoly power is that incumbents can sometimes rest on their laurels and fail to innovate doesn't make this the sole reason to pursue antitrust action. Not on you though, this kind of reasoning failure is so common I think it deserves a fancy \"cognitive bias\" name. Off the cuff maybe something like \"Single Rebuttal Fallacy\"? reply gofreddygo 5 hours agoparentprev> These emails were released as part of the antitrust lawsuit I never understood this: Why would hot shot, high powered people risk putting such things on email? Things that could backfire when released into public domain like from a lawsuit or a leak. They know this well, and still keep doing it. Why not setup an in person meeting, or just pickup the phone and talk ? Why email? I almost feel they actually want it to happen but could never point my finger to how this could cover their asses, or an exit strategy? reply supriyo-biswas 4 hours agorootparentUltimately, if the people at the company have to do a job, they have to communicate somewhat, and putting it on an email or another recorded medium is the best way to prevent endless meetings and have the perspective documented in a single place. Everything has a legal liability, it doesn't mean that it's worth it to move everything to an undocumented medium. reply cma 28 minutes agoparentprevThese emails might have been offered by Google to show the opposite right, they just are part of the trial. reply sida 8 hours agoparentprevwhy were these emails released as part of the FTC's lawsuit against google. How did the FTC get microsoft's email over an antitrust lawsuit against google reply avi_vallarapu 14 hours agoprevIt is probably an observation and a forecast at the right time. I remember my days at one of the Top Home and Enterprise PC manufacturing companies over 15 years ago, when there was criticism around Smart phones. People laughed assuming that a smart phone is of no use and people prefer a PC or a laptop. Everything else is history. What is important at all times is the timing and Identifying something that can change the world at the right time. This is where the Top Leadership roles come into play. Identify the gaps and introduce the immediate action plan to make the best of the best. reply skydhash 4 hours agoparentThe thing is that we had small notebooks/agenda/notepad, then we moved on to PDAs when things turn digital, it's not a big stretch to imagine how smart phones should work if the hardware is there (what I read is that manufacturers was cheaping out and locking things down). I still believe that what LLM does best is analyzing natural languages and producing coherent (not necessarily true) output. And there are maybe business needs for that, but still have not seen a truly individual tool, like personal computing is (you can go on a desert island with a laptop and compute). I agree that executives is to predict and plan strategical responses. But so far, it seems to be only useful for those that like quick answers, even if it maybe untrue. reply sahila 13 hours agoparentprevSure and not to discredit your observation, but what other observations have you made in the last 5 years that didn’t pan out? Regarding politics, sports, stock market, covid, or other tech trends? The evaluation can’t be looking back, and if you’re right about 1/10 things, would that warrant a $1b investment in each? reply VelesDude 9 hours agorootparentThis should be the perfect forum to ask this question considering the whole mission of Y combinator. reply daseiner1 12 hours agorootparentprevParent comment doesn’t claim to have predicted the rise of smartphones. If you truly had a bankable 10% success rate, $1B on each spin would be a steal. reply kd913 15 hours agoprevCurious how Bill Gates is still main cced? Does he still play a significant role in the control of Microsoft? reply scrlk 15 hours agoparentHe's still influential behind the scenes - there was a recent article published about it: https://www.businessinsider.com/bill-gates-still-pulling-str... (https://archive.ph/VmtON) > Current and former executives say Gates remains intimately involved in the company's operations — advising on strategy, reviewing products, recruiting high-level executives, and nurturing Microsoft's crucial relationship with Sam Altman, the cofounder and CEO of OpenAI. > \"What you read is not what's happening in reality,\" another Microsoft executive said. \"Satya and the entire senior leadership team lean on Gates very significantly. His opinion is sought every time we make a major change.\" reply godzillabrennus 14 hours agorootparentIf true it would explain why Microsoft keeps getting better while Google management seems to ignore the founders and keeps getting worse. reply cmrdporcupine 14 hours agorootparentI don't think it's so much that Google ignores the founders, so much as the founders are ignoring Google. I was at G when L&S stepped away, and it really just felt that leading up to that, they'd become completely disinterested. S, especially, seemed completely out of touch. And L just tired. reply foobarian 13 hours agorootparentBG is a shrewd, cutthroat businessman on top of having a tech background. I think he's far different from L&S in that respect. But yeah it helps he hasn't disengaged as well. reply cmrdporcupine 13 hours agorootparentGoogle would have probably been better off if Eric Schmidt had stayed engaged. I don't like the guy's opinions on many things, but he was pretty good at keeping the ship headed in generally the right direction. reply VelesDude 9 hours agorootparentNever liked Eric, but Google had such a clear direction and vision under his leadership. Pichai, feels like the accountants are running the business. It is ONLY about the next quarters results. reply actionfromafar 13 hours agorootparentprevOr any direction? I couldn't tell what the direction of Google was if I had to. reply esafak 9 hours agorootparentWhat's the direction of Microsoft, Amazon, or Apple? They're too big to have one direction. reply cmrdporcupine 9 hours agorootparentGoogle does have one direction, the same direction it's had since IPO, really. Maximize ad revenue, to maximize share value. Just all the other window dressing is going away. reply anyfactor 13 hours agorootparentprevI don't think Larry and Sergey is on the same level as Bill Gates. Solo founders who have raised to and led a big tech company is a different breed. I would pick Jeff Bezos, Zuck and Larry Ellison on the same level of Bill Gates. They don't manage day 2 day but big strategies don't go through without their blessing. Even though you can say Bill does not manage, the top level executives will always think \"what would Bill do\" or \"what would Bill think about this\". reply shuckles 13 hours agorootparentBill Gates co-founded Microsoft with Paul Allen. If anything, Paul Allen was the instigator. reply saulpw 7 hours agorootparentYeah and Jobs co-founded Apple with Wozniak. Wozniak was instrumental to Apple's initial success but clearly it was Jobs who took it to the mat. Same with Gates/Allen, by their IPO in 1986 it was billg at the helm and Paul Allen doing something else. reply shuckles 6 hours agorootparentI don't dispute Bill Gates might have been the business mind that drove Microsoft to its status today as the biggest software vendor to businesses on the planet. However, it's not because he was some sort of solo founder sigma grindset. reply ioblomov 4 hours agorootparentAllen clued Gates in on the upcoming microcomputing revolution when the Altair made the cover of Popular Electronics in 1975. And there are stories about both writing code on planes while on their way to sales meetings. So Gates arguably combined Woz’s technical chops with Jobs’ business acumen (even though I always admired the latter’s sensibility and vision more). reply lionkor 12 hours agorootparentprevWhich of their products is getting better? reply bnchrch 10 hours agorootparentIn my brain, Microsoft is two companies at once. Old Microsoft and New Microsoft. The split is categorized by Satya's focus and specifically MS's acquisitions after he took over. This new Microsoft does very well (Azure, Github, Linkedin, OpenAI investment, Unix focus (WSL, C#/.net on linux) However the old Microsoft is still around and in some ways just as bad as ever. Namely Windows. reply thrdbndndn 7 hours agorootparentI agree Windows is getting worse, but there are still lots stable products in old Microsoft, namely the whole productivity suites like Office, OneDrive, SharePoint, Teams, etc. reply kranke155 12 hours agorootparentprevThey’re executing better. Microsoft has never made great products with some exceptions. reply jiggawatts 11 hours agorootparentMicrosoft makes “good enough” products that integrate smoothly. That’s what enterprises want and smaller vendors can’t provide by their very nature. reply dist-epoch 11 hours agorootparentprevSome exceptions like Windows, Office and Xbox? reply AnthonyMouse 10 hours agorootparentYou're listing widely used products, not great products. Fast food is not great just because it's popular. reply ramraj07 7 hours agorootparentNot a single person who uses Word and Excel daily would agree with you. Apple the “great product” company has been at it forever and so have many open source competitors and none come close to how good word and excel are. This seems to be the problem with engineers: “I use eMacs on Linux so that’s the greatest product for everyone.” reply AnthonyMouse 2 hours agorootparentMicrosoft is expert at vendor lock-in. Your company got Office in the 1990s because competing office suites didn't work with Microsoft's operating system \"for some reason\". Now all of your accountants' macros only work in Excel and it would take thousands of hours to convert them, so no one can switch until a competitor has bug-compatibility with the highly complex and under-specified document format. But because companies can't switch, it sucks all the oxygen out of the room for competitors who are deprived of the revenue, patches and donations they would otherwise get with a larger user base. So they don't have the resources to achieve bug-compatibility and the status quo sustains, not because Excel is so great, but because Microsoft ensures that competitors don't get better. In theory Apple has the resources to do this, but \"Microsoft compatibility mode\" and \"backwards compatible forever\" are anathema to their brand, so they don't. Google, by contrast, has actually made an effort and taken a huge bite out of Office market share. You might even say the product is better. But it's still not 100% compatible, and there are companies not willing to put their most sensitive private data on somebody else's cloud, so there are still plenty of companies stuck with Microsoft -- not because it's great, but because there is no great alternative. reply mike_hearn 1 hour agorootparentprevI don't know. I use the iLife suite (Pages, Numbers, Keynote) and prefer them over Word/Excel. Obviously I'm not a power user, but for the standard tasks you need from such tools I find them much better designed. Keynote in particular stomps all over PowerPoint for anything except collaboration, where Apple's \"only the Apple ecosystem exists\" mentality holds them back. reply Sebb767 10 hours agorootparentprev> Fast food is not great just because it's popular. It's not good for you, as a consumer. As a company, it's a trillion dollar market, nurturing some of the largest companies in the world. reply VelesDude 9 hours agorootparentEven Bill Gates has admitted that a big part of their success was just sheer luck. There were many competitors that were pushing far better technology than they had but simply could not get a foot hold in the market. Just the other day on here I mentioned BeOS/Haiku OS, the tech vision on that thing is astounding but it simply did not pan out into any sort of market success. Same with parts of Commodore, IBM, NeXT. MS did capitalize and manipulate the market to assist themselves and keep competitors out, but just becaused they won doesn't mean they had the best technology. reply saulpw 7 hours agorootparentMicrosoft intentionally and specifically suffocated BeOS. That was not luck. reply paxys 15 hours agoparentprevThis was sent in 2019, when Gates was still chairman of the board at Microsoft and involved day to day as technical advisor to the CEO. Post 2021 he has no official role at the company. reply omega3 2 hours agoparentprevI found it curious he wasn't cc'd on the response from Satya. reply jbverschoor 14 hours agoprevYou can easily kill ambition or by requiring everything to go into slow processes, no freedom, and corporate politics. If there's any ambition left, it won't be for that company. reply phkahler 12 hours agoprevKnowing how something is done can be quite different from knowing all that it really takes to make it happen. reply SunlitCat 14 hours agoprevIt's an interesting read. Especially if you consider that all it needs today is llama.cpp and a model of your choosing from huggingface. (Okay Okay, granted it takes a bit more and someone had to train those models, but point it, the fear from way back seems to be....dunno.) reply riscy 13 hours agoparentIf you can't train the model, you don't really have control over a product based on it. reply hsdropout 14 hours agoprevPrevious submission from 4 days ago: https://news.ycombinator.com/item?id=40226114 reply a1o 15 hours agoprevPeople actually use Mail in Windows 10? reply Philip-J-Fry 14 hours agoparentI did, it was a genuinely good little lightweight UWP email app. Now they've turned it into a bloated ad-ridden Outlook webview app. reply TillE 14 hours agorootparentI'm guessing that happened almost exactly a year ago, which is when I was astonished to find that they'd junked up their perfectly good Weather app with ads. Microsoft does plenty of cool things these days, but it's bizarre how they're pushing these little Windows monetization efforts like it's some garbage F2P mobile game. reply overstay8930 9 hours agorootparentMicrosoft knows most people aren't going to buy a Mac reply teekert 14 hours agorootparentprevWas gonna say, I wonder if he still uses it now that it is that Outlook thing :) I’ve been helping people to Thunderbird. Most people are like: Mail>Thunderbird>New Outlook thingy. Good enough for me ;) reply SunlitCat 14 hours agorootparentDon't get me wrong, Thunderbird is awesome for what it is (and the Mail thingy was kinda perfect as a lightweight mail client) but Thunderbird has some rough edges. Like it took me long until I figured out how to show message size, filter by date and then sort it by message size. Of course, the Mail thingy wasn't able to do something like that, but I remember waaaaaaaaaaaaaay back, that Communicator (that Mail program which came with Netscape / Mozilla) was way simpler to use than Thunderbird. But maybe my memories are hazy as this was like ~15 - ~20 years ago. reply marcosdumay 13 hours agorootparentprevI remember liking it when I found the app. But then I had to use the web interface to do anything with filters, so when I had to reinstall Windows I didn't bother configuring it again. And now everything is on Teams anyway. reply SunlitCat 14 hours agoparentprevI did, till they took it from my claws and tried to convince to use that Outlook thing. :( It was kinda perfect for peeking into mails, deleting a few, answering some other and stuff. Not a great client to do your business communication, but to drop someone a quick mail, it was neat. reply cm2187 13 hours agoparentprevTo me the biggest news is not that he is using Mail on Windows but that he is using Windows. You wouldn't tell that the CEO of the company is using the product given how badly they let it decay. reply layer8 13 hours agorootparentI’m more surprised he doesn’t seem to know basic comma rules. reply Waterluvian 15 hours agoparentprevProbably millions of people. Us nerds need to habitually self-remind that we’re a sliver of the total user base and our experience and needs are not representative.” reply a1o 14 hours agorootparentI use Outlook, not super nerdy, idnk reply sincerely 14 hours agorootparentIf you’re reading HN, you’re so much nerdier than the average software user that the existence of super nerds is irrelevant reply vorticalbox 14 hours agorootparentprevI think this was more a generalisation about HN as a whole than directed at you. reply kristianp 14 hours agoparentprevFunny to see Satya using it. You'd think he'd be on the full Outlook. Maybe it's a case of dogfooding. reply m_a_g 14 hours agoparentprevI don't know a single techie person who uses Windows (other than for gaming), let alone Mail in Windows. But the other replies are right. We're not really representative of the entire population, especially if you consider the developing world. reply vector_spaces 14 hours agorootparentThis phenomenon of software engineers using MacBooks for work is a rare and primarily coastal phenomenon, at least in North America. Virtually all software engineering at utility companies, hospitals, state and local governments, and similarly boring but critical enterprise companies in industries like insurance happens on Windows on Azure using Windows or Microsoft oriented stacks. reply oooyay 14 hours agorootparentI, personally, would not chalk that up to a phenomenon. Operating System selection largely depends on the work you do paired with the industry you're in. Some extrapolations: I work in systems engineering for web technology companies. Most of the applications I build run on Linux so it makes sense to write on a Unix like OS. I could use Linux or MacOS, but Apple has a strong preconfiguration and leasing pipeline so usually the companies I work for offer MacBooks Pros. When I was building software in the US South I would have to look out for companies that were Windows shops because I don't do Windows systems engineering and there were an abundance of shops that did before the .NET Core rewrite that enabled you to run on Linux. Those shops would've definitely shipped me a Windows laptop. Anecdotally, I buy servers out of a DC in Houston and nearly all of them come with Windows Datacenter edition. Most of those companies fell into certain industries that didn't include what I typically worked in. That's all to say, region can be roughly correlated, but it's not the actual reason. It largely has to do with who sold who the software stack they have trouble moving off of today which influences everything else. reply vector_spaces 13 hours agorootparentNote that my comment was observational, I'm not saying companies should use Windows, and I'm not even talking about reasons for using one tech stack or another. I'm just reminding the parent that this association of Windows with \"non-technical\" is nonsense -- lots of extremely talented folks work on difficult and important problems and critical systems on Windows, targeting Microsoft stacks. Further, it seems likely that most software is written on Windows. reply oooyay 13 hours agorootparentYeap, that's all fair. I was mainly poking at your word choice of \"phenomenon\" because it can be reasoned. I'm not quite sure about your last sentence but that's an entirely different discussion with much more noise than signal to parse through. reply yen223 12 hours agorootparent\"Phenomenon\" just means \"thing that can be observed\". It doesn't need to mean something that is unusual or surprising. reply oooyay 12 hours agorootparentHuh; according to Merriam-Webster the word has basically lost meaning: https://www.merriam-webster.com/dictionary/phenomenon The first definition is how you and OP used it, the second is how I used it (exclusive of things that can be \"sensed\" as opposed to reasoned). reply thrdbndndn 4 hours agorootparentI think your usage is actually more like definition 3. reply SunlitCat 14 hours agorootparentprevThis is something \"bothering\" me as well. It's either Mac or Linux if you want to be \"cool\" in tech. I am always like \"wait do you not write any Windows software or is it all on the Web\"? Maybe I am ignorant and stuff and maybe it's just bothering me. :) reply tikhonj 11 hours agorootparentThere's a lot of work that is neither Windows software nor web. And much (most?) of that runs on Linux. reply oooyay 14 hours agorootparentprevI write applications for Linux, MacOS, and Windows but I use cross-compiling frameworks like Wails or Tauri to do so. If I write in Windows Forms (old Xamarin forms) then my application only works on Windows. MacOS is similar. All of my development occurs in Linux as a result. reply asmor 14 hours agorootparentprevWindows computers tend to also get the most overreaching invasive MDM / Endpoint Slowdown Software. reply SunlitCat 14 hours agorootparentCan't say for Windows, but at work I manage a bunch of iPads with JAMF and it's really comfy to use. reply hsdropout 14 hours agorootparentprev>Windows on Azure Lol, citation needed if you actually believe this. >or Microsoft oriented stacks. is a very complicated way to say Windows desktop. reply vector_spaces 14 hours agorootparentRe windows desktop, no, it's not a complicated way of saying that. I am talking about, for example: - ASP.NET Core backends running on anything (e.g. Linux) - Angular or Vue frontends running on anything - applications using SQL Server databases running on anything - applications targeting on-prem Windows servers or VMs or, sure, Windows desktops - data / reporting systems that interact with AAS reply hsdropout 6 hours agorootparentApparently I phrased my response poorly. I was responding to the narrow context of the idea that devs aren't using Azure VMs instead of Macs. I was not disputing the popularity of Azure. reply asmor 14 hours agorootparentprevMicrosoft sales is surprisingly good at selling Azure to c-level at companies where the primary output isn't software, based on existing Windows/Office deployments. Unfortunately also true for Teams. Retailers are also often opposed to AWS because they consider Amazon a competitor in its entirety. reply mvdtnz 11 hours agorootparentprevJust so you know this is a very ignorant comment. I don't mean that as an insult, but literally you are ignorant of the facts. If you care you should use this opportunity to educate yourself on the state of the Microsoft stack. reply sniggers 2 hours agorootparentprevLike the other replies are saying: you're in a bubble, like most Apple users. MacOS needs third party apps to not be incredibly frustrating, and often the apps are subscription based. (Window management, clipboard history, and power management are the big ones - even if you go with a free option like I do, it's not good that the base OS is missing essential features in these areas.) There are many little annoying things that can't be easily disabled, like Apple Music launching every time you connect your bluetooth earbuds. There are incredibly bizarre UX choices, like not having \"pin\" in Finder be a right click option. As for development, Windows + Powertoys can do everything MacOS can better, plus game (and game dev). For anything Linux I can use WSL2. I can natively develop any kind of software for literally any non-walled-garden platform on Windows (and use a VM if I'm forced to make an iOS app, god forbid). Why would I intentionally choose to limit what my computer can run and what development I can do? I'm forced to use a Macbook for work, but would never use it if given the choice. MacOS is for people that value style over substance and non-technical people, just like everything else Apple. reply bobsomers 40 minutes agorootparent> I'm forced to use a Macbook for work, but would never use it if given the choice. MacOS is for people that value style over substance and non-technical people, just like everything else Apple. You’re obviously free to prefer whatever platform you please for whatever personal reasons you please, but the conclusion that macOS is for non-technical people is insanely ignorant. reply erhaetherth 14 hours agorootparentprevI both game and program on Windows. I don't know what people are crying about. I've got Docker. I've got WSL. I've got a high quality IDE (IntelliJ). Everything works and runs great. I also run Debian at work and that's also perfectly good for writing code, just not gaming. But no, I don't use Mail in Windows. I've been using Gmail since it came out in 2004....and was perfectly happy with it until just this second. I just found an email from 2004 wherein I had emailed myself a project, and Gmail has blocked it because it thinks it's a virus. The solution? Export the EML and open it in Mail on Windows. Funny. reply layer8 13 hours agorootparentprevRegarding techie people not using Windows, you’re living in a bubble. reply z3phyr 5 hours agorootparentprevJohn Carmack, ryg, people making demoscene, are the hackerest of the hackers, and they use Windows mostly. These people derive from the Wirthian pascal heritage. Unix people would not know. reply KronisLV 13 hours agorootparentprev> I don't know a single techie person who uses Windows (other than for gaming) I'd say that Windows actually has some nice software, like MobaXTerm: https://mobaxterm.mobatek.net/ which in my eyes is better than Remmina or pretty much anything I've found on nix, short of just running the same thing on Wine. WinSCP is also pretty cool, albeit nothing particularly special: https://winscp.net/eng/index.php PowerToys (and other customization software) also make the OS feel more pleasant to use, especially with something like FancyZones which feels nicer to use than the window snapping in XFCE or Cinnamon: https://learn.microsoft.com/en-us/windows/powertoys/ WSL2 actually seems nice to use and even Hyper-V is pleasant. The vertical taskbar in Windows 10 worked better out of the box than my current customized Cinnamon desktop. Oh also the task manager is really nice and the Linux remake does tend to eat resources: https://github.com/KrispyCamel4u/SysMonTask There's probably more nice things that someone can say about Windows and personally I don't mind doing development on it because most of my software works on it anyways (VSC, JetBrains IDEs, GitKraken, Docker, browsers etc.), but Linux distros do feel better for that particular type of work otherwise. On the other hand, even with Proton, I still enjoy gaming on Windows more, far less of a hassle and curiously the graphics control panels seem to mostly only be available on Windows and something like CoreCtrl on Linux isn't always good enough (e.g. if I ever want to set a power limit for the GPU easily). I really don't want to deal with Windows 11 though (which is inevitable because of updates and also work computer) and I have very few positive things to say about Windows Server, however. But hey, even .NET now works on RHEL/Ubuntu and other popular distros, so Microsoft tech stacks also feel decidedly more sane, in addition to something like C# just being a decent language in general. That said, Thunderbird runs everywhere and does so well, so for me, it's the obvious choice. reply tambourine_man 14 hours agoprev [–] I don't understand why large companies use email for such important topics knowing that they can be required by law to disclosure them in the future. I know that large companies have email retention laws, but why not just keep sensitive topics on Signal or similar? reply Eridrus 14 hours agoparentThey are definitely doing more of that, but it's not always a get our of jail free card, and may actually put them at legal risk for destroying documents: https://www.theverge.com/2024/4/26/24141801/ftc-amazon-antit... https://arstechnica.com/tech-policy/2024/05/judge-mulls-sanc... reply tambourine_man 12 hours agorootparentNot being allowed secrets by law is such a weird concept reply riscy 12 hours agorootparentIt's not weird for a publicly traded company. reply Eridrus 11 hours agorootparentprevI find it weird that these get released as public records, but otherwise, laws against destroying evidence seem fine. And laws requiring disclosure of documents in a court to determine wrongdoing also seem fine. And laws requiring proper record keeping of decision making at large companies also seem fine. Particularly when the burden of record keeping (ensuring emails are not deleted) is not very large. reply krebby 14 hours agoparentprevThey write these emails knowing that they'll be public record at some point. The audience is as much internal as it is for the history books. The details get hashed out offline, while the record gets preserved as an email reply layer8 13 hours agoparentprevRetention laws are neutral towards technical protocols. They apply to instant messaging and electronic chats just the same. reply quartesixte 9 hours agoparentprevThe contents of these particular emails is actually not very sensitive, and the courts do allow for redactions of more sensitive things (e.g. the actual numbers involved). Things that should not be discussed in writing happen in person. Meeting rooms, offices, hallways, golf courses. The emails are written knowing it might be public. And at the C suite, emails are to be very intentional. reply neeh0 14 hours agoparentprev [–] becasue it can be interpreted as acting against the company in first place reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The discussion delves into Microsoft, Google, tech trends, browser wars, email, leadership styles, operating systems, and software preferences, stressing the necessity of anticipating future tech advancements and the repercussions of industry failures.",
      "It explores Microsoft's dominance, founders' influence in tech behemoths, and the varied user preferences in software and operating systems.",
      "Emphasis is placed on communication, documentation, legal responsibilities in business dealings, and the significance of handling sensitive data securely."
    ],
    "points": 273,
    "commentCount": 125,
    "retryCount": 0,
    "time": 1714931412
  },
  {
    "id": 40268204,
    "title": "Meta Rivals Manhattan Project in GPU Spending",
    "originLink": "https://twitter.com/emollick/status/1786213463456448900",
    "originBody": "Nowhere near the Apollo Program, but Meta spent almost as much as the Manhattan Project on GPUs in today’s dollars. https://t.co/aLpDKY03FZ pic.twitter.com/Ci5IYVS0Gu— Ethan Mollick (@emollick) May 3, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=40268204",
    "commentBody": "“Meta spent almost as much as the Manhattan Project on GPUs in today's dollars” (twitter.com/emollick)255 points by paulpauper 13 hours agohidepastfavorite224 comments afruitpie 11 hours agoI’m surprised how “cheap” the Manhattan Project and Apollo Program were. It’s bizarre we put people on the Moon in the ‘60s for an amount of money similar to Apple’s recent stock buybacks. reply londons_explore 11 hours agoparentI suspect it's because inflation has been underestimated for the past ~hundred years. Over time, that really compounds. A fairer way to look at it is how many years of the average citizens salary is it. The manhattan project was $2B in 1944, or 121,000 median-family-years of work using salary figures from the 1940 census. Apples stock buyback was $110B in 2024, or 122,910 median-family-years of work using figures from the 2022 census. So, the inflation figures under represent the cost by a factor of 3.5 over those 80 years. reply maratc 10 hours agorootparentOne thing that might be missing from your calculation is that \"one median-family-year of work\" meant \"about one person working\" in 1940 but means \"about two persons working\" in 2022. reply Retric 9 hours agorootparentThe difference isn’t quite that large as many households are just single people and while the female labor force participation rate increased (34% 1945 vs 57% now) the male labor force participation rate declined ( 83% 1948 vs 68% now). https://fred.stlouisfed.org/series/LNS11300001 Similarly there where a great deal of households in the 1940’a with two working adults. reply bombcar 8 hours agorootparentFemale labor force participation near the end of a major world war was already probably much higher than just a few years before. reply spywaregorilla 8 hours agorootparentprevThe share of dual income households has more than doubled. Your statistic seems likely to be biased by people retiring. https://www2.census.gov/ces/wp/2019/CES-WP-19-19.pdf I'm guessing the rise of dual income houses is much more visible in the median than are retirees dropping down the labor force participation rate. reply Retric 7 hours agorootparentYou read that statement incorrectly, the share of dual incomes among married couples doubled. Married-couple households however have dropped dramatically over time, 76% of households in 1940, 55% of all U.S. households in 1990, and 46% in 2020. “In 1940, married couples with children represented 43 percent of all households; married couples without children represented 33 percent of households” reply mgiampapa 10 hours agorootparentprevIt's also discounting workplace efficiency gains. Both white and blue collar jobs are far more efficient than ever before. reply maratc 9 hours agorootparentOTOH, an interesting point is that the Manhattan project cost 0.4% of GDP in 1940, and Apple buyback ($110B) is 0.4% of 2023 GDP ($27T). reply pmontra 2 hours agorootparentApple gets its money from all around the world, so every country pays a part of those buybacks. The USA in 1940 were a smaller empire than they are now so, I might be wrong, but the Manhattan project money came mostly from inside the country. This means that it's easier for Apple to find and spend those money than it was for the USA in 1940. reply Laremere 5 hours agorootparentprevThis take ignores the unpaid work performed for the family and community, largely by women. When two people in a family are employed, more of their household labor (cooking, caring for the young and the old, etc) is purchased instead of provided by a family member. reply adrianN 5 hours agorootparentHousework productivity has gone through the roof since the invention of dishwashers, washing machines, dryers, etc. My grandma used to actually work hard the whole day to run the houshold. My family today manages with maybe two or three hours of light work per day. reply ornornor 4 hours agorootparentIf only we did the same with other work, we’d also all only be working a few hours a day rather than 8–9 still. reply mschuster91 2 hours agorootparentBut how would Jeff Bezos swim in money if his underpaid parcel couriers would be able to financially survive on working four hours a day? It's utterly ridiculous how much of the productivity increases of the last decades ended up not distributed to the workers either in terms of wage/salary hikes or reduced working hours. Something like billionaires rarely existed for a long time, usually it was the royal family and that's it - and now, alone in the last 20 years the amount of billionaires is 6x higher than at the start of the millennium. All of that wealth has been looted from the lower classes and in quite a few cases the taxpayers. [1] https://de.statista.com/statistik/daten/studie/220002/umfrag... reply pmontra 2 hours agorootparentprevAdd sewing, making and repairing clothes, curtains, sheets and covers, etc. We buy them now. reply fulafel 4 hours agorootparentprevAlso lots of the project ingredients were not really market commodities. There could have been a lot more exchange of money involved if eg there had been a market for weapons grade uranium/plutonium or moon rockets and staff had been paid FAANG salaries. (There's of course a big opportunity cost in employing the best physicists and biggest r&d investments in nukes - eg solar energy and grid storage could have been here decades earlier in an alternative scenario.) reply roenxi 1 hour agorootparentprevThat seems to subtly assume that a century of technological progress hasn't changed the value of what a family can produce. That is pessimistic given that computers were maybe the greatest leap forward in technology related to engineering design. A Manhattan project should be much more achievable today than it was in the WWII era. reply hn_throwaway_99 8 hours agorootparentprevI understand what you're getting at, but equating wage inflation with price inflation would be a mistake that would lead to some nonsensical results. That is, if you only look at wage inflation, it would mean that you can never factor in productivity improvements to how much more people can buy with their money. reply scrlk 8 hours agorootparentprevIn 1944, the USD was pegged to gold at $35/troy oz. The current spot price of gold is $2300/troy oz. 1 troy oz of gold in 1944 = 1 troy oz of gold in 2024. This gives the price of the Manhattan Project as $131 billion in 2024 dollars. reply noxvilleza 8 hours agorootparentNot sure if this is that good of a metric given that I've read before that in recent history, precious metals have much higher demand where there is crisis and instability - and 1944 is certainly one of the peaks for that. I guess since 2023 it's also been quite a peak. reply wakawaka28 4 hours agorootparentNo, in 1944 the dollar was fixed at $35 per ounce of gold. It was not a time of crisis for the gold market, because gold was money. The peg was eventually removed (I think in 1971). Right now there are factors driving the price of gold up, including massive central bank purchases and also massive understated inflation in the West. Everyone knows that fiat currencies are fleeting so the world is preparing for an inevitable decline or collapse of the dollar. reply csomar 2 hours agorootparentIt's important for people to realize that the US dollars market are quite open that if the current actors thinks that the dollar is going down, it'll go down very fast. So if you are an average HNer (citizen) don't go freak out and put all of your money on gold or bitcoin. (This is not a long-term assessment though; that being impossible to predict the far future). Nobody wants to see the US dollar collapse overnight. Neither the US nor China (which holds trillions of US bonds). But everyone wants to move from that standard including the US (which, being the reserve currency, has suffered a horrible NIIP since the end of the gold standard[1]). 1: https://en.wikipedia.org/wiki/Net_international_investment_p... reply chaostheory 8 hours agorootparentprevIt’s a great metric at the moment for the reasons that you’ve given. We are living in a period of crisis and instability. Just like in that time, we are close to yet another global war. The difference is whether or not nuclear weapons will keep things from further escalating. reply lesuorac 7 hours agorootparentBut why is gold a better metric than anything else? Like if there's a global war I bet people are going to want canned food much more than a gold chain. So should we use the price of chicken soup in 1944 vs 2023? --- But yeah I also agree with eru; being \"close to yet another global war\" is a way better place to be than \"actually in a global war\". Considering pretty much every century has them. reply noxvilleza 6 hours agorootparentPrecious metals (gold/silver) are very expensive given their weight, relatively easy to divide up, and won't significantly go stale or decompose (some oxidation will occur - sure). I'm sure there are other assets that this is also applicable to - it's just the categorical example I remember discussing with my accounting teacher ~19 years ago. reply eru 4 hours agorootparent> [...] and won't significantly go stale or decompose (some oxidation will occur - sure) [...] Just to nerd out a bit: You can dissolve gold in some particularly strong acids, but it doesn't really diminish the value of the gold. The effort for getting it back into elemental form is small relative to the price it fetches. Compare this to eg aluminium, or to make an even starker contrast: wood. reply noxvilleza 4 hours agorootparentYeah I was just comparing it to other things that are very expensive (by weight and size) like, saffron, truffles, caviar, printer ink, etc. reply eru 7 hours agorootparentprev> We are living in a period of crisis and instability. What? We are living in a golden age of unprecedented peace and prosperity. By and large, people never had it so good. reply richk449 5 hours agorootparentIt was the best of times, it was the worst of times. reply Dalewyn 3 hours agorootparentprevThe west (and some tag-alongs like Japan) is living in a golden age of unprecedented peace and prosperity. The world at large has other ideas, especially continental Asia and the Middle East where people seemingly can't stop brutally killing each other. reply eru 2 hours agorootparentHave you had a look at the graph of GDP per capita in eg Bangladesh recently? PR China has also seen enormous advances in prosperity, and hasn't been in a major war in 70 years, either. In India obesity is a bigger problem than starvation. And approximately everyone can afford a smartphone, too. South East Asia is doing fairly well, too, compared to historical averages. Even Sub Saharan Africa is finally starting to catch up. Yes, there are still wars, alas. But by and large people are richer and living more peaceful lives. We could look at statistics, if you wish. (I was looking for a graph of casualties of war over time for my previous comment, but couldn't find one that was recent enough, sorry.) The founding Prime Minister of my own adopted home of Singapore wrote a book called 'From Third World to First'. And he's right. reply ithkuil 2 hours agorootparentprevWe have to put things in perspective. During the Balkan wars about 5% of Europe's population lived in a country that was at war. Yet that time period was relatively peaceful compared to the first half of the 20th century where Europe went through two total conflicts. Yet for me the war in Yugoslavia was of personal significance and I felt it and its longer term effects closely. But it wouldn't be fair for me to just dismiss that the second half of the 20th century was a time of peace and prosperity in Europe, because a part of it was burning in flames and committing atrocities to who were their brothers 5 minutes before. The precariousness of peace is what makes it so valuable. Yes. There are conflicts all around the world. Wars. Oppression. Injustice. Racism. I don't think that recognizing that we're living unprecedentedly peaceful and just times is going to make those conflicts and injustices worse. Quite the contrary. I feel that never recognizing the successes and progress that humanity has made in many of those fronts is what will eventually doom us to regress. reply littlestymaar 1 hour agorootparentprev> So, the inflation figures under represent the cost by a factor of 3.5 over those 80 years. But this is not inflation per se, it's what's counted as “economic growth”: the median family in 1940 had a much lower standard of living than the one in 2024. But of course sorting what's inflation and what's relevant economic growth is highly subjective: some things like cars and housing have literally inflated (they are enormous now compared the what they looked like even 40 years ago) and that's usually counted as economic growth, but is it really? In some way it is: having a bigger house is nice. But in other way it's not: do you really need that much space when you're older and the children left long ago? And sometime it's actually a net loss: living 30 kilometers away from work and depending on a car for everything is not a gain. Same for electronics: computers are much, much faster than they used to be in 2000, so the statistics counts that as economic growth. And it is, you couldn't do modern AI on 2000 hardware, or physics simulations for industry and all. But at the same time, for the average consumer, the gain in speed has been eaten by software consuming more and more resources. Sometimes from sheer laziness (there's literally no reason for Windows to be this slow) but sometimes it's a bit more ambiguous: today's video games couldn't run on hardware from 20 years ago, and I remember being very happy to see the graphics improve during my teenage years so it's not entirely pointless. But at the same time, it's not as if video games were more fun or enjoyable now than they were 20 years ago: the consumer surplus from video games is basically the same no matter the quality of the graphics, so maybe it shouldn't be counted as growth at all. The measure of “real” economic growth (and the measure of inflation) depends a lot on how you weight these things (it's called “hedonic adjustment”) and as we've seen, it's very difficult, and fundamentally subjective. And I wish statistics agency provided a measure of CPI without hedonic adjustment in addition to regular CPI (like they already do multiple variants of CPI depending on the basket of goods that's taken into account). reply moneywoes 10 hours agorootparentprevis median family income a good measure since it represents the cost of R&D? reply MattGaiser 11 hours agorootparentprevAverage family has a ton more in material goods, both in quality and quantity. So it isn’t that inflation is underestimated. The average family truly is richer. Far more house. Far more car. Far more and better food. reply chimpanzee 10 hours agorootparentQuantity? Absolutely. Quality? That’s debatable; it’s highly dependent on the goods in question and how quality is measured. reply throwup238 9 hours agorootparentI think you're underestimating the technological development in the last 80 years to a comical degree. In 1944, half the country didn't have flush toilets! reply bombcar 8 hours agorootparentAnyone who does serious work with ancient technologies knows both that the older stuff was much more repairable but also needed to be repaired much more often. Material and building science has advanced a mind boggling tremendous amount - even modern toilets are significantly better than an actual un-upgraded 40’s - if you can find it. At the same time we can mass manufacture endless cheap plastic shit. reply eru 7 hours agorootparent> At the same time we can mass manufacture endless cheap plastic shit. You say it like that's a bad thing. reply amarant 3 hours agorootparentGood or bad are subjective, but \"endless cheap plastic shit\" certainly does lower the average quality of goods available today. reply eru 2 hours agorootparentWhat do you care about the overall average quality? You don't need to buy them. You only need to worry about perhaps the average of the things you buy. reply bawolff 10 hours agorootparentprevJust because something is higher quality does not mean it has higher subjective value. E.g. i still think i come out ahead with cheap clothes and a laptop then someone 100 years ago with higher quality clothes. reply coffeebeqn 10 hours agorootparentI’d rather have a house than shitty fast fashion but your mileage may vary reply astrange 23 minutes agorootparentProbably don't want a common house from 1924 with lead paint, an indoor wooden stove for heating and an outhouse. reply eru 7 hours agorootparentprevIf you can find a neighbourhood that's as dangerous and unsafe as those a hundred years ago, the house would be very cheap compared to GDP per capita. reply edmundsauto 9 hours agorootparentprevA better comparison would be this: would you rather have 3 suits that you wore everyday that cost a months wages, or 30 shirts and pants that cost a weeks wages but are lower quality. (Also, IMO, houses today are much higher quality than in the past as soon as you account for safety) reply ornornor 3 hours agorootparentThe answer differs whether you answer it selfishly or with the greater good in mind (resource use/waste, pollution, etc) reply wkat4242 11 hours agoparentprevThe US was much poorer back then. Much lower standard of living. So wages were also much lower even inflation-corrected. Also there was a war going in and people were drafted and had no choice but to work for a minimal salary. I think that explains a lot of it. reply ipnon 10 hours agorootparentThe whole country had a mission, and the overwhelming feeling was that the US along with the Allies were in a conflict for survival, that without winning the war the existence of the US would not be possible. On December 6th 1941 public opinion leaned towards isolation from the “European” war. On December 8th 1941 Congress voted unanimously sans 1 vote to declare war against Japan. It is hard to overstate the fervor that immediately swept over the whole country. People are willing to endure the most demanding trials given the proper circumstances; at times the most realistic course of action is to do what once seemed impossible. reply galaxyLogic 4 hours agorootparentIt seems obvious to me that authoritarian regimes have much more corruption than democratic ones because in a dictatorship there is no transparency and therefore corruption goes undetected. That means that power and wealth gets concentrated into hands of a small elite. There is less true innovation. What makes things cheaper is the progress of democracy. reply ChrisMarshallNY 11 hours agoparentprevI think this comic says it all: https://pbs.twimg.com/media/DuJMkjIXcAcRru9?format=jpg reply dehrmann 4 hours agorootparentOne takeaway could be that sending people to the moon wasn't as hard as we like to think it is. reply csomar 2 hours agorootparentIt is not. The difficulty is that, mainly, we don't know lots of the variables there; and at the time the US was in a race to do it. But the moon is just a regular small planet that is much easier to escape its gravity than earth. reply rvnx 3 hours agorootparentprevIt's easier than self-driving cars, because toward the moon there are practically no variables and no other drivers. reply shmeeed 2 hours agorootparentprevYeah, I guess the several hundred thousand people involved were just slacking off a lot on government money. Please don't forget to close such statements with an /s, lest some notorious software \"engineers\" on HN might be led to actually believe engineering the most powerful vehicle on earth mostly by hand was a negligible task. reply richardw 10 hours agoparentprevWhen you’re under the gun, bang for buck is crucial. When you have a relatively fat, wealthy and lazy society (compared to WWII and even the post-war 60’s) all the costs go up. Eg you could build roads or railways or ships back then. Now there are 100’s of rules, lobbyists everywhere and everyone wants their cut. Financial incentives matter far more now, since survival has been (temporarily) taken care of. reply doug_durham 10 hours agoparentprevThey really weren't cheap. The costs were just pushed to the future. It's estimated that it will take between $16.8 billion and $550 billion to clean up the Hanford site. The Santa Susana facility where Apollo rockets were tested is going to cost billions to clean up (there are other things contributing to the cost there). reply virtuallynathan 9 hours agorootparentI’ve heard stories from people who have worked at Hanford, and it seems like a lot of that money is being squandered. Excess caution, basically everything is radioactive waste, and just overall wasteful spending for decades. reply zaroth 7 hours agorootparentIsn’t that exactly the point? “Clean up” is clearly a euphemism for some sort of money funnel. Hanford.gov even readily admits this! “In April 2009, the Hanford Site received a nearly $2 billion in funding from the American Recovery and Reinvestment Act. Contractors quickly hired thousands of new employees for temporary stimulus jobs in environmental cleanup.” reply 4death4 11 hours agoparentprevAll this should tell you is that inflation is vastly under-counted. reply adtac 10 hours agorootparentNo, it just tells me inflation isn't a single scalar multiplier for all kinds of items. reply jimbobthrowawy 7 hours agoparentprevI think the Manhattan project had a lot of consideration towards making it economical. (and huge government programs weren't quite so common back then) Plus, I think things that were returned/recycled weren't counted against the total. e.g. the $600 million worth of silver they borrowed to make calutrons, because there was a copper shortage. reply egl2021 6 hours agorootparentEconomy was not really a consideration. The project developed two different bomb designs (plutonium and uranium) in parallel. They tried three different paths to fissile material: gaseous diffusion and electromagnetic separation for uranium and reactors for plutonium. reply 1oooqooq 1 hour agoparentprevit's almost as if we've been mislead about that big wasteful central planning government reply dontreact 11 hours agoparentprevIt’s the difference between doing something transcendental a small number of times and doing something amazing billions of times. reply MilStdJunkie 5 hours agoparentprevEverything about the official inflation calculation is almost totally borked in terms of generating useful information. Like, information that would be useful for someone doing something real, like future historians or Zombie FDR. There's probably dozens of mechanisms at work to explain \"why??!!\", but the root cause is - like so many other damn things in America - financialization, from its birth post-1972 with vehicles like EUROBOND, to its \"pedal to the metal\" moment with all the liquidated commie national assets, to the craptacular commodity runup crisis courtesy of Bush II wrecking the Levant and West Asia. Oh, right, and the financial system ending in 2008. When we seamlessly blended up - legally - private with public risk. Oh yeah. Sixteen years ago. Anyway, financialization. We don't account for money supply accurately, they're dynamic with price indices now, blabbity blabbity blah blah, 1962 dollars were probably worth waaaayyyyyyyyy more than we think they were. reply astrange 21 minutes agorootparentThere is no number of 1962 dollars that could buy you an electric car, HPV vaccine, or Nintendo DS, which means they're worthless if you want to buy those things. reply maratc 10 hours agoprevBoth the Manhattan Project and the Apollo Program were 0.4% of GDP. With today's GDP of $27T, $108B would be equal — Meta have only spent less than a third of that on GPUs. reply marcosdumay 9 hours agoparentSo... A single company has just spent about 0.1% of the US GDP in a pet project, and it's not even the leading one on the market? That's just incredible, and not a good thing. reply mlsu 9 hours agorootparentI would challenge that a little bit -- I think it is a good thing, given that they have open-sourced the results of the project. It seems to me to be far better than a company which simply gives 0.1% of US GDP back to shareholders in the form of dividends or stock buybacks, which is a very common occurrence given the size of companies nowadays. Or worse, a company which spends 0.1% of US GDP manufacturing things that are wholly useless or even actively damaging to consumers, the environment, etc. reply eru 7 hours agorootparent> It seems to me to be far better than a company which simply gives 0.1% of US GDP back to shareholders in the form of dividends or stock buybacks, which is a very common occurrence given the size of companies nowadays. The whole point of investing in companies is for those companies to eventually give the shareholders more resources back than they put in. [0] Those shareholders can then consume those resources, or invest them in something else; like eg startups that pursue new technologies. Returning capital to shareholders is exactly how investing works and how it has to work. Otherwise it would be charity, which is also fine, but it's something different. Are you also against companies paying back debt, instead of endlessly rolling over one loan into another loan and paying interest only with money from new loans? (If not, how is that different?) [0] On average, and in expectation. Investing in risky endeavours means that sometimes they fail, and you get less or no money back. reply mlsu 6 hours agorootparentNo, I don't have a problem in general with returning capital to shareholders. I'm not making an anti-capitalist argument here. I'm saying. For humanity, net good. Facebook could do a stock buyback with all of that H100 money. It's a significant amount of money. As a human, complete non facebook shareholder except in ETFs, I would rather they buy H100's and train huge neural nets and then release them for free. Maybe I've bought the kool-aid but I think I believe that Zuckerberg genuinely wants to move humanity forward with this one (of course, it's also a business move). A lesser chairman of the board would have not spent huge capex on GPUs, and instead simply issued a fat dividend or what have you to shareholders who eventually would buy a newer nicer house in Marin or whatever. And indeed, it's only Zuck's unique position with preferential shares that allows him to do this; at a different company, he'd have been shot down and dividends it would be. On the whole, which is better? What moves the needle in terms of bringing humanity forward? And this is kind of true of companies in general. \"Returning money to shareholders\" is not created equal, even though it's all money in the end. Sports betting companies are also returning basis points of GDP to shareholders. Is that a worthwhile thing to do? Net good? (Please, dear reader, do not make the argument that sports betting is actually a good thing because hey we all make free market choices.) reply eru 3 hours agorootparentWell, I don't particularly like sports betting, but it doesn't seem worse than any other form of entertainment. Titanic had a 200 million dollar budget in the 1990s to produce about 444,600 movie frames and a bit of sound. Was that a good use of resources? Though to come back to the main argument: you seem to be comparing Apples to Oranges here? So the first thing is: how are those companies making money? That could be via Facebook ads, or via sports betting, etc. The second question is: what are those companies doing with the money. That could be investing in eg training neural nets, or returning it to shareholders. When you invest money, that's the same as spending resources. That's only worthwhile, when in expectation you get sufficiently more resources out later than you put in now. In contrast, when you return money to shareholders, no resources have been spend. It's up to the shareholders what they want to do with their 'resource-coupons' (ie their money). From the point of view of Facebook, dividends vs investment both look like spending money. From the point of view of the economy, the former is a mere transfer of resources, the latter is spending of resources. (You could argue that it's better in some sense for Mr Zuckerberg to control these resources than for the average investor to have that control. I'm sympathetic to that argument, especially if you carefully state what sense you have in mind.) > On the whole, which is better? What moves the needle in terms of bringing humanity forward? Dividends only move money around. They don't move the needle one way or another. It depends on what the shareholders are doing with the money. Btw, Amazon and Tesla show that 'the markets' can be extraordinarily patient with companies that they believe in. It's just that most managers don't have nearly as much charisma. Anyone can (and does!) say they have great long term plans, but it's hard to tell the charlatans from the true visionaries. So as a safe default, markets often prefer hard-to-fake signs of progress, like dividends. reply piva00 3 hours agorootparent> Well, I don't particularly like sports betting, but it doesn't seem worse than any other form of entertainment. Titanic had a 200 million dollar budget in the 1990s to produce about 444,600 movie frames and a bit of sound. Was that a good use of resources? Sports betting is non-productive, money goes into a sink and gets absorbed without production. The production of Titanic had to hire hundreds to thousands of professionals through multiple companies to produce something, those people and businesses get paid and spend it back in the general economy (both to produce their product and the surplus that gets generated goes back into investments for the next project). Productive activities are always much better for society than non-productive ones like sports betting. reply eru 2 hours agorootparentWhen you get some money, it doesn't really matter whether you spend it or put it under your mattress or even burn it. If you remove it from circulation, your central bank will just print more money to hit their inflation targets. (Of course, your central bank doesn't check in your garden whether you buried some money. They follow price and spending statistics in aggregate.) If you dig up your money, the central bank will print less money or even remove money from circulation by selling assets from their balance sheet. It does make some difference whether you consume or invest. But that's because of a difference in how resources are used. If you burn your money in a big fire, that's approximately equivalent to a donation to your central bank. If you bury it, and dig it up ten years later, that's approximately equivalent to an interest free loan to your central bank. --- Production isn't useful by itself. Production is what we do to enable consumption. We don't need to worship production nor put it on a pedestal. Both sports betting and producing the Titanic spends some resources to entertain people. If sports betting spends less resources for each unit of entertainment, that would be great. (In a competitive market, that would drive down the cost of sports betting, eg bookies would compete by offering tighter spreads or more exciting bets or whatever. Alas, I don't know enough about sports betting to tell what people find enticing about it.) But the resources spend on both running the sports betting systems and on producing Titanic are gone. Those work hours of those thousands of professionals that worked on Titanic don't come back. They are a cost, not a benefit. reply piva00 31 minutes agorootparent> But the resources spend on both running the sports betting systems and on producing Titanic are gone. Those work hours of those thousands of professionals that worked on Titanic don't come back. They are a cost, not a benefit. Isn't velocity of money a question in this though? The spending on producing Titanic pours the money into pockets (workers and businesses) who will spend it faster into the larger economy (consumption/hiring others to do projects) than what the sports betting company will, the spending from the wages of those workers will, on average, be more diluted into other productive means which will support other businesses and workers. In my view this is a more beneficial way for society than the accumulation of the same capital into a sports betting company, while betting has many more negative impact to society (addiction, for example) than watching Titanic has. reply WalterBright 8 hours agorootparentprevThe shareholders normally just reinvest the dividends. That isn't bad for the economy. > manufacturing things that are wholly useless Companies manufacture things that people want to buy. Presumably those people find the things useful. Companies that manufacture things people don't want go out of business. reply gdsimoes 8 hours agorootparent> Companies that manufacture things people don't want go out of business. Do they, though? When was the last a time a really big company went bankrupt? reply toomuchtodo 8 hours agorootparent2008. Lehman Brothers and Washington Mutual. https://en.wikipedia.org/wiki/Bankruptcy_in_the_United_State... reply dchftcs 7 hours agorootparentThey manufactured things people wanted, but shouldn't have wanted... reply bobthepanda 6 hours agorootparentprevGM and Chrysler also went bankrupt the year after. Silicon Valley Bank was actually the third largest bankruptcy in US history, which is news to me. And apparently Signature Bank was fourth. https://www.statista.com/statistics/1096794/largest-bankrupt... reply toomuchtodo 6 hours agorootparentMF Global in 2011 as well, but not terribly big comparatively speaking. reply qeternity 8 hours agorootparentprevThe point is not that lots of companies run around making things people don't want and go bankrupt. The point is that's what would happen, and because of that, companies focus on making things people actually do want. Look at what happened the Facebook stock over the last few years as they rebranded to Meta and started spending wildly. The market whipped them into shape, and rewarded them handsomely. reply eru 7 hours agorootparentThough Facebook/Meta is actually a bit of a weak example, because it is very much controlled by its founder, who can outvote any public shareholder. So any compliance with the 'wishes of market' was somewhat voluntary. reply jpollock 7 hours agorootparentprevNorthern Telecom - 2013 Alcatel-Lucent - 2016 (Bell Labs/AT&T Technologies/Western Electric) reply vel0city 6 hours agorootparentAlcatel-Lucent didn't go bankrupt and out of business. They were acquired by Nokia and have had chunks spun off and sold to other companies. Nortel is a good example though. That was a big one. reply WalterBright 5 hours agorootparentprevGM, Brooks Brothers, Washington Mutual, JC Penney, Hertz, Blockbuster, Sears, WeWork, Kodak, ... reply massysett 8 hours agorootparentprevGeneral Motors, 2009 reply kristofferR 8 hours agorootparentI'm pretty sure he meant chapter 7 (business death), that's the colloquial use of \"went out of business\". reply massysett 8 hours agorootparentVery well, Sears Roebuck is an even better example then even if it did not file for bankruptcy because it went from cornerstone of US culture to practically \"went out of business.\" reply slater 8 hours agorootparentprevEnron, 2007 reply agent281 5 hours agorootparentprevSilicon Valley Bank and First Republic went bankrupt last year. https://en.m.wikipedia.org/wiki/2023_United_States_banking_c... reply kortilla 8 hours agorootparentprevSilicon Valley Bank Revlon Avaya Talen Hertz reply ornornor 3 hours agorootparentprev> things that are wholly useless or even actively damaging to consumers, the environment, etc. Oh you mean like Facebook, Instagram, and friends? reply klyrs 8 hours agorootparentprev> or even actively damaging to consumers, the environment, etc. Jury's still out on this one reply jazzyjackson 6 hours agorootparentprevthey've open sourced a babble-bot that allows any individual to sockpuppet infinite identities, I have a hard time seeing how the impact will be positive reply chii 9 hours agorootparentprevWhy not? If someone is willing to make large bets like that, all the more power to them. Society has (and should) continue to become wealthier. The laws that govern us has improved from the medieval days where money is everything, so just because a private entity is able to spend a lot doesn't imply there's anything untoward. reply bogwog 6 hours agorootparentFacebook (and all the other big tech companies) are entrenched monopolists that engage in illegal business practices to maintain their position. This is a net negative for society overall, and a huge detriment to innovation. They're able to spend that much on GPUs (and have to because Nvidia doesn't have real competition either) because of the stranglehold they have over their markets, not because they earned that in the free market through the means by which the free market is meant to work (competition, best product/service wins, etc). That money they spent is being used incredibly inefficiently because of this. With more competition, a lot more would be done with a lot less. Facebook tossing us a pseudo-open source kinda-free LLM will make some of the nerds on this site happy though (like how decades of evil from Microsoft were erased with the release of a free text editor lol). I guess that's all that really matters at the end of the day. reply mcmcmc 6 hours agorootparentprevThe problem is that it’s not society becoming wealthier, it’s the top strata getting wealthier off the increased productivity of society as a whole. reply eru 7 hours agorootparentprevYes, taking risks with your own money can be a good thing. But taking risks also means that project don't always pan out. I don't know enough about Facebook to say whether the expected value of that bet was positive. But that's not my judgement to make in the first place. > The laws that govern us has improved from the medieval days where money is everything, so just because a private entity is able to spend a lot doesn't imply there's anything untoward. Yes, I agree that it's a good thing that people and companies can spend their money how they see fit. reply eru 7 hours agorootparentprev> That's just incredible, and not a good thing. Why? It shows that they were willing to spend lots of resources on something risky. That's how you get progress. Risky things don't always pan out (almost by definition). reply int_19h 1 hour agorootparentThe fact that entities large enough to control so many resources by themselves exist, is not a good thing. reply spacebanana7 3 hours agorootparentprevThat sort of investment level is why the US dominates the software industry. reply marcyb5st 2 hours agorootparentFor better or worse also the fact that in the US it's possible to fire people without any issue (IMHO). Being able to prune underperforming people should mean that those money are better used. Here in EU I saw many people that basically just clock in/out everyday and then mind their own business because they are un-fireable due to labor laws/unions. A colleague was calling these people \"institutionally pre-retired\", which I believe it's a great description of the phenomenon. To some extent that happens in the US also in the Public Sector, but I believe it's rarer. reply segmondy 7 hours agorootparentprevYou think Llama is a pet project? reply brevitea 9 hours agorootparentprevDo we know if the Meta AI effort is subsidized by the US Government? reply whateveracct 6 hours agorootparentprevwaste is still rampant in the industry despite all the correcting factors out there reply choppaface 3 hours agorootparentprevIt’s easy to see as a pet project given the metaverse failure but their aim is more towards beating Google Search, which looks more plausible after OpenAI’s traction. reply refulgentis 6 hours agorootparentprevAll kinds of funny math, gotta pretend it was all spent in 1 year, ignore that it's a fraction of what they spend in general so it sounds more shocking, and it's tracking to be gpt4 ish reply Invictus0 7 hours agorootparentprevATT has $400B in assets on their balance sheet reply loeg 8 hours agorootparentprevMeh? It's the sixth largest company in the US; it's not surprising that it can direct 0.1% of US GDP. reply VelesDude 9 hours agorootparentprevWhen I see things like this I am always amazed at how we manage to squander our potential. When I see the best in people and then see the silly loops and squiggles we get ourselves into it is like playing Chopsticks on Mozart's piano. We know it could do so much more. I mean, we are free to play chopsticks and that is fine, but ultimately in focusing on the short term despite being aware of the long term, it eventually gets taken out of our control and the base ecosystem has its way with us. It is better to show restraint now rather than have it forced. reply mr_toad 4 hours agoparentprevIf you add together the expenditure (or revenue) of all companies the sum will greatly exceed GDP. reply arbuge 8 hours agoparentprevExceeded by Apple's stock buyback then, $110B. reply eru 7 hours agorootparentIt's good that Apple is giving resources back to the rest of the economy, yes. reply fastball 7 hours agorootparentMoney isn’t a resource. reply eru 4 hours agorootparentYes, that is true. However that distinction only really makes a difference in analysis when you are talking about the behaviour of organisations that can create money. Like (central) banks, or at a considerable stretch, Amazon issuing gift-cards. Companies returning money to shareholders is pretty much equivalent to companies returning economic resources to shareholders. reply 7e 9 hours agoparentprev- Meta is paying for these GPUs over multiple years, not a single year. Meta capex is only $30B a year, not all of that went to GPUs. - There's no way Meta is paying retail. reply ncallaway 8 hours agorootparentI don’t think the Manhattan Project was a single year either? reply oskarkk 7 hours agorootparentIt wasn't, but we're comparing to the yearly budget of Manhattan/Apollo (as a fraction of GDP). NASA budget was between 0.3-0.9% of GDP in 1963-1970[0], that was per year. In 1965 GDP was $743B (nominal), so 0.5% of that is ~4B, and the (nominal) cost of Apollo (which wasn't the whole NASA budget) was 25B. As aa part of GDP from 1970 (1.073T) the total cost of Apollo would be like 4% of that single year's GDP. Not a good comparison because when Apollo started GDP was 563B and was 1.3T when it ended. [0] https://www.researchgate.net/figure/Total-NASA-Appropriation... reply brandall10 8 hours agorootparentprevFrom the linked tweet: \"Yann LeCun confirms that Meta spent $30 billion on a million NVIDIA GPUs\" reply qeternity 8 hours agorootparentYann is not a CFO. If I buy a house tomorrow, I don't just consider it a massive one-day expense. They will amortize the cost over the useful life of the hardware. reply 7e 6 hours agorootparentprevMeta doesn't have the electrical capacity to online a million 700W GPUs in 12 months. And they don't pay until they're delivered. reply loeg 5 hours agorootparentAdding 700MW in GPUs in 12 months is within Meta's capabilities. reply petermcneeley 10 hours agoparentprevProbably would be better to look at is as compared to federal government spending. https://fred.stlouisfed.org/series/FYONGDA188S reply thrdbndndn 7 hours agorootparentWhy? I think comparing to GDP percentage make more sense in this case, since the point is to compare their economical scale, not how invested the gov is on these projects. (Not to mention, one side (meta) has little to do with government funding to begin with.) reply eru 7 hours agorootparentprevWhy would that be a better or worse indicator? reply acchow 8 hours agoparentprevThat’s production, not inflation reply twoodfin 8 hours agorootparentSure, but the point is that long run real economic growth made it possible for a big-but-not-biggest US company to invest at a level attainable only by the US government in wartime 80 years ago. reply choppaface 3 hours agoparentprevAnd also for perspective: if ad spend globally hits $1T in 2027 then Facebook has put down about 3% today to perhaps own 30%. reply willis936 9 hours agoparentprevI feel like this move's the goalposts a little bit. In the recent MFTF thread people compared the entire 70 years of fusion research funding (about 80 Bn USD) to the Apollo program. This is in the context that the research is heavily underfunded given its applications and potential viability. If we agree that society got an Apollo Program's worth of value out of the Apollo Program, what value has society gained from Facebook's resource usage? A more effective manipulation engine? reply sinuhe69 9 hours agorootparentValid question. But one thing is public and the other thing is (kinds of) private, so no they aren’t comparable. Besides, Meta open sourced their results, so in my opinion they should deserve a better status, allow them to access resource and indemnity like libraries. Perhaps only if they spin off their AI research into a non-profit entity, but yeah open source is digital public good and deserve a special status. reply eru 7 hours agorootparentprevThey got lots of ads out of it. People paid for those ads voluntarily. What Facebook does with the money they earn is up to them. Would you have asked the same question if Mr Zuckerberg had spent it all on yachts? reply echelon 9 hours agoparentprev> less than a third That's still an insane amount of money. reply Arnt 12 hours agoprevI think the overwhelming complexity of modern GPUs, CPUs, SoCs don't... overwhelm us. Compared to the process of producing a 3nm chip with all those billions of transistors, the Manhattan project isn't huge. Groundbreaking, sure. Huge, compared to the development of EUV wafers, no. reply chx 11 hours agoparentPerhaps it's worth mentioning some lesser known big achievements of the Manhattan Project. It's not the scientists. It's Hanford and Oak Ridge. Hanford was not only nuclear reactors but a city of more than 43 000 people out of literally nothing. Oak Ridge had the largest building in the world, K-25. It was only surpassed by the Boeing Everett Factory more than twenty years later. These two sites were also the major cost centers of the project. Science is cheap. Enriching uranium and manufacturing plutonium -- especially when no one did it before -- is not. reply viewtransform 2 hours agorootparentHanford is also the nations largest superfund cleanup site. https://spectrum.ieee.org/hanford-nuclear-site \"At the Hanford Site in south-central Washington state, 177 giant tanks sit below the sandy soil, brimming with the radioactive remnants of 44 years of nuclear-materials production. From World War II through the Cold War, Hanford churned out plutonium for more than 60,000 nuclear weapons. The sprawling enterprise eventually contaminated the soil and groundwater and left behind 212 million liters of toxic waste—enough to fill 85 Olympic-size swimming pools. Decades after the site stopped producing plutonium, the U.S. government is still grappling with how to clean it all up. \" reply Gare 11 hours agorootparentprev> Science is cheap. Indeed. Sustained nuclear chain reaction was achieved in Chicago Pile-1 beneath the stands of campus football stadium at the end of 1942. That's when, scientifically, it was proven that the bomb is possible. reply adastra22 5 hours agorootparentPedantic point, that's when they proved nuclear power was possible. It was still unknown whether the cross section of U-235 was large enough to permit a small, transportable bomb to go critical, and whether that reaction would have occurred fast enough to generate a significant explosion. And in the case of Plutonium, the idea of implosion hadn't even been worked out yet, let alone tested. reply whatwhaaaaat 9 hours agorootparentprevWe’re also still paying for the enrichment done in the 40s. There are dumps of radioactive material all over the country that were never properly disposed of. Those costs have been externalized on the populations living near those dumps for 80 years now. reply lelandbatey 7 hours agorootparentprevThe marvel of the Manhattan project, including Hanford and Oak ridge, was the singular focus and short time scale but not necessarily the absolute size. If we consider just ASML, (the main) supplier of only the EUV machines (and accessories) for manufacturing chips (they don't run the fabs, they just build and maintain the equipment), they also directly employ 42,000 people and utilize another 5,000 tier 1 suppliers. https://en.m.wikipedia.org/wiki/ASML_Holding That's just the ONE business to make the machines to make the chips, that doesnt include all the folks who make the chips! If we count just one of their customers (TSMC) that's another 70,000+ employees! National projects are impressive for different reasons than whole industries. reply wmf 11 hours agoparentprev10% is overwhelming complexity and 90% is Nvidia's profit margin. reply bawolff 10 hours agorootparentIf you really believe that, i would suggest you go into the GPU business. reply wmf 9 hours agorootparentI should go into the GPU firmware, driver, and compiler business. reply 123yawaworht456 10 hours agorootparentprevwhat do you believe? that a chip and 80GB of HBM memory (a H100) cost $40000 to develop and manufacture? 90% margin is likely the low end for Nvidia's AI offerings. reply talldayo 9 hours agorootparentThey should charge double that, and teach the industry what happens when they abandon Open standards. reply bawolff 9 hours agorootparentprevUmm, no. I believe it would cost in the hundreds of millions to develop and manufacture. I dont work in hardware, so i dont know what is involved. I do however work in software, where people are constantly saying things are easy or devs are just lazy. Until they put their money where their mouth is and try it themselves. Usually they are in for a rude awekening once it becomes obvious they are full of shit. reply Palmik 45 minutes agoprevIt's really amazing that they are releasing the results of all of that compute (the Llama models). Much more efficient and environmentally friendly than keeping things closed like Google or OpenAI! reply GenerocUsername 11 hours agoprevI'm always amazed by the quality and craft of works produced in the periods before roughly the 1960s. Not sure what changed so drastically about society around that time to make modern times so wasteful, slow, expensive, and unambitious but if we could go back, I would reply vundercind 11 hours agoparentProfessional management class. Various major reforms toward “more efficient” management, first in manufacturing and such, then applied to knowledge work. Reduced specialization when it comes specifically to bureaucratic tasks in white collar work (everyone’s their own secretary, among other things, now—even doctors) [edit] anecdotally, as related by people who watched the transition happen, one of the things this new management culture screwed up was management itself, especially lower and middle management. There used to be a gradient of how much time managers spent in meetings with peers and higher-ups, with the lowest ones doing very little of that and mostly focusing on the needs of their subordinates and keeping things running well—after the shift, all levels of management got sucked into a culture of meetings, meetings, meetings, most of them very low-value. Posturing and politics soaked down into lower levels of organizations so-afflicted to a much more extreme degree than before. reply wslh 8 hours agorootparentAre there good historical references of the inflection point you talk about? Thanks! reply seb1204 1 hour agorootparentYes indeed I would like to learn more about this too, this historic image of the first managers. reply ip26 10 hours agoparentprevThere were supposedly 400,000 people behind Apollo 11. I bet you could make some pretty high quality, well crafted stuff today as well, if you had nearly half a million people devoting their lives to building one thing. Nvidia: 26,000 people TSMC: 73,000 Intel: 124,000 AMD: 25,000 Qualcomm: 50,000 ASML: 42,000 So, in terms of manpower, the entire global semiconductor industry might be comparable scale. reply bombcar 8 hours agorootparentWith about 400,000 people you could make … Target, Home Depot, or United Healthcare. reply fy20 8 hours agorootparentprevMaybe a better comparison is SpaceX. In 2022 they had around 12,000 people. reply seb1204 2 hours agorootparentAt least space X is producing something. How useful their products are is a subject of debate but in my opinion much higher ranked than meeta who die yet another screen glueing, mind numbing, radicalising add network. What has meta ever done for us? reply octopoc 10 hours agoparentprevIt's not just the quality and craft of works. Worker's pay didn't increase but productivity did, the wealthy have become ever wealthier, the bottom 90% of earners stopped doing better off, etc. etc. It's kind of crazy how so many measures have gotten steadily worse since then[1]. [1] https://wtfhappenedin1971.com/ reply lolinder 6 hours agorootparentAn interesting thing about those charts is that most of them start during the 40s, but the ones that don't (\"income concentration at the top\" and \"income share at the top\") pretty clearly show that the 30s–70s was the anomaly and we've just reverted to the pre-Great Depression status quo. A more illuminating question might be \"wtf happened in the 1940s and how can we get back there short of another total war?\" reply seb1204 1 hour agorootparentOne view is that after the war people had a lot of new technologies and knowledge but nothing else, as it was destroyed during the war or all drilled to support the war machine. Since the knowledge was there and also a lot of level plain field (pun intended) new visions were developed to rebuild and put people to work. Nowadays most people have most necessities and also luxury stuff making it easy to be on the nimby train rather than developing a small understanding for the complexities of our current world and what decisions can be simple but impactful. reply JumpCrisscross 5 hours agoparentprev> always amazed by the quality and craft of works produced in the periods before roughly the 1960s Survivorship bias. I have a 19th century Eerie Company cast iron pan. It’s beautiful. It is not representative of most cast iron of the era. We can cast far better iron today, we just don’t devote those production processes to making pans. reply ChrisMarshallNY 11 hours agoparentprevI have my theories, but it's probably best that I keep them to myself, and do my best on my projects. reply marshray 10 hours agoparentprev* Postwar surplus of machine tools and experience with metal manufacturing processes. * Lack of injection-molded thermoplastic suitable for much more than toys. * Relatively high labor costs may have reduced incentives to save money on materials. * Consumers accustomed to paying higher prices for manufactured goods with the expectation that they would be repairable and last a long time. * Tube-based electronics tended to require high voltages and heavy transformers or batteries. This dictated the form of the enclosure (metal or wood) and its usage (non-portable). For example, radios made for consumers would typically be incorporated into furniture. reply justin66 7 hours agoparentprev> I'm always amazed by the quality and craft of works produced in the periods before roughly the 1960s. Yeesh. This statement is not defensible. The fifties-era cookie cutter houses were pretty bad (but at least they moved past using coal and gaslight and knob and tube wiring and so on). The quality of passenger cars has only ever trended upward, with a dip during the combined energy crunch and inflation of the seventies that was remedied pretty quickly. The sixties ushered in the era of solid-state consumer electronics. reply 121789 11 hours agoparentprevCheaper labor, less opportunity to have negative impact, and you’re also probably biased and not looking for the projects that failed reply MattGaiser 11 hours agoparentprevSurvivorship bias. Plenty of failures, incompetence, corruption, and stupidity before. But it fades into history. reply stoicjumbotron 6 hours agoprevHow is it possible that even after spending so much, they still can't catch up to OpenAI? How is OpenAI so good? reply syntaxing 6 hours agoparentNot a ML expert but I think “can’t” is a strong word. There was a podcast with Mark Zuckerberg where I interpret his remarks as, Meta doesn’t want to directly complete with OpenAI, their existing AI services like recommendation already consume a lot of resources. Ads has and will continue to be a cash cow and LLMs has not generated large revenue yet. OpenAI generated about 2B revenue in 2023 (I’m sure it’ll grow significantly in 2024). Meta has made 40B net income in 2023 so you can see financially, there’s no reason to compete (yet). reply arthurofbabylon 4 hours agorootparentSupporting your argument, Meta is competing with OpenAI, but specifically in market position. They placed “Ask AI” prominently across their platforms (for me, at least). Meta knows where to compete, where to keep a foot in the door, and where to be a bystander. reply Palmik 49 minutes agoparentprevIt's not unlikely that the Llama 3 405B that's still training will outperform the best OpenAI model of today. The Llama 3 70B model in English already beats all models from Google, from Anthropic and it's only bested by the last GPT4 version. (Based on the Open LLM Arena results) reply renonce 4 hours agoparentprevHave you tried meta.ai which they offer for free? It’s already very competitive in terms of its abilities, they just don’t care about it or advertise it as much yet. reply snats 6 hours agoparentprevRemember that Llama400B is still in training and could end up crushing a lot of the use cases for GPT-4 reply blululu 6 hours agoparentprevLLAMA 3 is pretty sweet and some of their vision stuff like segment anything is cutting edge. Facebook just don’t need to publish or make demos since they have a robust path to revenue. reply causal 2 hours agoparentprevOpenAI is impressive, but IMO Meta is more likely to find product fit. reply notatoad 8 hours agoprevIs it public how much meta is using these GPUs for internal operations, and what kind of savings they might be seeing from that? It seems like there must be a ton of things other than the underwhelming “meta ai” chat box that they could be throwing H100s at - anything from content recommendation engines to anti-abuse to optimizing network bandwidth and traffic flows. reply pests 7 hours agoparentThe @MetaAI as a group member in group chats has been great. Riff on images together, write stories, have it impersonate authors or write chapters of a (bad) book. Revitalized a few old group chats that went days without messages to constant messing around with the AI. reply moxvallix 7 hours agoprevIt would seem the Manhatten project got more bang for its buck. reply Def_Os 4 hours agoparentZing! reply smel 2 hours agoprevMeta open sourcing LLama is just like Google open source Kubernetes to counter AWS dominance. reply londons_explore 9 hours agoprevA big part of this is the value of walled gardens/IP. If laws were changed so IP/data wasn't protectable, then next year there would be lots of people making cloned/modded copies of Facebook. Likewise, there would be lots of people taking Nvidias GPU design and making variations on it. (this is assuming a world where everything currently closed source is made open source through law changes) And in both cases, competition would push prices and profit margins to near zero. reply spacebanana7 3 hours agoparentFacebook’s value is in the social network, rather than IP/data. Even if the code was all open sourced, the data was portable (e.g like open banking), and the messaging standards were open (like XMPP); the supermajority of users would keep using FB. See the example of gmail. reply oskarkk 6 hours agoparentprevFor GPUs it could work as you say (except that the incentive for development of expensive and risky technologies would be gone), but I think that wouldn't help that much with things like websites. Most of the value of Facebook is in the network effect. Messenger has more than 1B users and I don't think it's a good app, Telegram is much better IMO (and is also used by almost 1B people in some countries - network effect there), and all Telegram apps are open source - well, backend isn't, but at least you can have modified clones (communicating with Telegram servers) with different features etc. And there is a small open source server implementation made by someone. reply 999900000999 7 hours agoprevI'm not a hardware engineer, but what are the chances this gets Moores Law'ed out in about a decade. As in within 10 years we'll have the same power with a million dollars in GPUs. With that much money I'd imagine you could purchase a GPU manufacturer or something. reply oskarkk 6 hours agoparentI've just checked and surprise, right now Nvidia is worth two times more than Meta. reply 999900000999 22 minutes agorootparentNvidia licenses GPUs to manufacturers, I was thinking Meta could of purchased MSI or something. reply rgmerk 10 hours agoprevThis is less impressive than it sounds. Real (inflation adjusted) US GDP is almost nine times higher than it was in 1945. reply zelphirkalt 7 hours agoprevIf that statement is true, I would like to add: All financed by human attention on ads and selling out their users' personal data. Congratulations, humanity. Now, what could we have achieved in useful areas, with that kind of money? reply qp11 7 hours agoparentMore like it gets hard to achieve anything else because all other heavy duty sectors of the economy are already monopolized. When Google entered the telco space you would think they would easily take down the ancient parasites that are AT&T/Verizon/Comcast, but what happened? Same story with Apple trying to get into auto/banking (credit cards)/content. So the money is only going to be used in this way. They can't touch Exxon or Monsanto or Pfizer or Boeing. They can't do a thing about housing or education or health or banks. So burning cash on novelty becomes their default path of least resistance. \"Useful areas\" have all been walled up. reply phyrex 4 hours agoparentprevMeta does not sell personal data, they just use it for ads reply sidcool 6 hours agoprevMeta lost billions on Metaverse. And now spending on this with an unclear business model. Where do they get all this money from? reply uyzstvqs 46 minutes agoparentMetaverse was an obvious failure, but Quest is a huge success in the VR/AR market. They managed to create high quality products without it becoming overpriced novelty items (eg Valve Index, Vive, Apple Vision), while being an open ecosystem that supports OpenXR and sideloading. reply syntaxing 6 hours agoparentprevThey made about 40B net income in 2023 (that’s right, net) despite spending billions on the metaverse. They have more than enough money in their war chest to do moonshot ideas. reply bhouston 6 hours agoparentprevThere is a lot of money to be made selling stuff to Meta for their latest project apparently. Figure out where they want to go and $$$. reply __lbracket__ 8 hours agoprevAnd what has it got to show for in return? A hyper PC chatbot who's 50% response is \"It is important to note racial injustices...\" in response to every damn question. reply ffhhj 5 hours agoprevWhy is everyone talking about inflation? Creating a device like a modern GPU in the times of the Manhattan Project would have required building a supercomputer way bigger and more expensive than the Project itself. Tech has simply become super cheap, requiring less materials and all the research has already been paid. reply Uptrenda 10 hours agoprev>History defining amounts of money being spent on AI What could go wrong. reply CyberDildonics 7 hours agoparentProbably money gets wasted and GPUs go unused. reply eesmith 5 hours agoprevSo, less than IBM spent on System/360? Quoting Dijkstra, https://cacm.acm.org/news/an-interview-with-edsger-w-dijkstr... > I had no idea of the power of large companies. Only recently I learned that in constant dollars the development of the IBM 360 has been more expensive than the Manhattan Project. Hee-hee! Immediately following that is: > I was beginning to see American publications in the first issue of Communications of the ACM. I was shocked by the clumsy, immature way in which they talked about computing. There was a very heavy use of anthropomorphic terminology, the \"electronic brain\" or \"machines that think.\" That is absolutely killing. The use of anthropomorphic terminology forces you linguistically to adopt an operational view. And it makes it practically impossible to argue about programs independently of their being executed. Anyway, to confirm the main point, \"IBM spent 5 billion dollars in mid-1960s money\" says https://thehistoryofcomputing.net/the-ibm-system360 while \"the Manhattan Project, which resulted in the first atomic bomb, cost $2 B\". https://en.wikipedia.org/wiki/Manhattan_Project says that was 1945 dollars, so about $3.5 billion 1965 dollars using the CPI Inflation Calculator at https://www.bls.gov/data/inflation_calculator.htm . Going backwards, this $30 billion on GPU training is only $3 billion in 1965 dollars, making it cheaper than the 360. reply mvdtnz 8 hours agoprevThis entire \"AI\" paradigm is perverse. Literal truckloads of money throw into the ether (not to speak of the tons of fossil fuels burned) so we can have shitty chatbots. What does Meta have to show for all of this? My friends sending me unbearably shitty fake photos they asked the Meta Bot to \"imagine\" for them. What a complete and utter waste. reply lern_too_spel 8 hours agoparentThe silly AI chatbots are what have been built with their existing compute. This spending is for bigger and even more powerful models. reply xeckr 11 hours agoprevThat's about 0.1 Apollo programs. reply jeffbee 11 hours agoprevHopefully people can appreciate that a modern GPU is an achievement of physics and technology on par with the Manhattan project. EUV lithography is way off the end of the complexity scale. They had to make a bomb that was perfectly round, these days we have to make assemblies that are perfectly flat. reply mcmoor 10 hours agoparentIf current society collapse sometimes I wonder how many centuries will be needed to recreate 3nm from scratch. Even 1000 years doesn't seem enough. reply salawat 10 hours agoparentprev...So you need three plates? Machinists have known this for over a century. reply jeffbee 10 hours agorootparentI think you are underestimating the necessary planarity. reply jiggawatts 10 hours agoprevI was trying to explain to some \"civilians\" just how much computer power Facebook dumped into the training of their AI models. First off, it's worth mentioning that the GPUs they're using are on the same kind of \"banned for export\" list as parts used in nuclear weapons. Facebook bought enough of them that the equivalent in weaponry would be a decent sized fleet of ICBMs with multiple warheads (MIRV) each. They didn't spend this cash for laughs, they're buying it for a purpose, and that purpose existed before ChatGPT was a big hit. Both Zuckerberg and Yann LeCun have stated in interviews that they're invested this money into AI for Facebook feed recommendations and for competing directly with TikTok in their Reels app. In other words, Meta aimed the digital equivalent of a weapon of mass-destruction at your kids' brains, with the express aim of getting them addicted to their products just like TikTok, but even more. If you want a visualisation of the kind of raw power that your teenager is up against, I came up with this: - A billion can be visualised as as 1m x 1m x 1m block subdivided into cubic-millimetre tiny little cubes. Think very coarse sand or very small pebbles. Just picture yourself trying to count out the grains in a block that size! - A billion-billion is a cubic kilometre of the same sized grains. - Facebook used two-trillion-trillion computations to train one of the smaller Llama 3 models! That's 2 million times the amount above, about the same as 1000 km x 1000 km covered 2 km deep. That's an area the size Egypt. Alternatively, Greenland covered 1km deep, or Texas 3 km (2 miles) deep. That's what your kid is up against. Good luck. reply ByThyGrace 10 hours agoparentIs this all speculation? Got a source for some of these big claims? Admittedly, it's a fun read. reply zargon 7 hours agorootparentZuckerberg said the reason he bought so many GPUs was to build Reels to compete with TikTok. Is that the big claim you're asking about? https://www.dwarkeshpatel.com/p/mark-zuckerberg reply jiggawatts 7 hours agorootparentprevThese are all facts coming from publicly available information, such as the Llama 3 paper and recent interviews. I'd dig up the specific timestamps, but I'm working at the moment and can't spend the next hour scrolling through long interviews. The interviews are: \"Mark Zuckerberg - Llama 3, $10B Models, Caesar Augustus, & 1 GW Datacenters\": https://www.youtube.com/watch?v=bc6uFV9CJGg \"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AILex Fridman Podcast #416 \": https://www.youtube.com/watch?v=5t1vTLU7s40 The total FLOPS used in training is in bunch of places, I can't find a good link at the moment. reply stefan_ 9 hours agoparentprevThis weapons comparison is really not going anywhere reply hello_computer 8 hours agoparentprevyour handle is also appropriate given the epic amounts of electricity needed to power all of that gear. so many articles on the energy consumption of cryptocurrency, yet so few for model training. reply jiggawatts 1 hour agorootparentMeta / Facebook has 650,000 GPUs. Including overheads for the networking and the servers, these pull about 500 watts each. That adds up to “just” 325 megawatts, which is nowhere near crypto mining levels. (Tens of gigawatts) Also, most of their compute goes towards inference, not training. reply moralestapia 8 hours agoprevThe math doesn't make sense, 30B on a million GPUs = 30k per GPU. They're expensive but not that expensive, and definitely not in bulk. reply kristofferR 7 hours agoparentThat's actually less than the price they fetch during this shortage: https://www.cnbc.com/2023/04/14/nvidias-h100-ai-chips-sellin... https://www.tomshardware.com/tech-industry/artificial-intell... However, they are likely vastly cheaper in bulk at MSRP, that's true. And Meta apparently got them before the AI hype cycle. reply rich_sasha 11 hours agoprevTIL; I didn't know the Manhattan project used GPUs in the first place. reply bewaretheirs 10 hours agoparentThey used the best 1940's equivalent: IBM card tabulators. This is discussed at some length in Surely You're Joking, Mr. Feynman. My favorite part: \"When I went back to work on the calculation program, I found it in a mess: There were white cards, there were blue cards, there were yellow cards, and I started to say, \"You're not supposed to do more than one problem --­­ only one problem!\" They said, \"Get out, get out, get out. Wait ­­-- and we'll explain everything.\" \"So I waited, and what happened was this. As the cards went through, sometimes the machine made a mistake, or they put a wrong number in. What we used to have to do when that happened was to go back and do it over again. But they noticed that a mistake made at some point in one cycle only affects the nearby numbers, the next cycle affects the nearby numbers, and so on. It works its way through the pack of cards. If you have fifty cards and you make a mistake at card number thirty­-nine, it affects thirty-­seven, thirty-­eight, and thirty-­nine. The next, card thirty-­six, thirty­-seven, thirty-­eight, thirty­-nine, and forty. The next time it spreads like a disease. \"So they found an error back a way, and they got an idea. They would only compute a small deck of ten cards around the error. And because ten cards could be put through the machine faster than the deck of fifty cards, they would go rapidly through with this other deck while they continued with the fifty cards with the disease spreading. But the other thing was computing faster, and they would seal it all up and correct it. Very clever. That was the way those guys worked to get speed. There was no other way. If they had to stop to try to fix it, we'd have lost time. We couldn't have got it. That was what they were doing. Of course, you know what happened while they were doing that. They found an error in the blue deck. And so they had a yellow deck with a little fewer cards; it was going around faster than the blue deck. Just when they are going crazy ­­ because after they get this straightened out, they have to fix the white deck ­­ the boss comes walking in. \"Leave us alone,\" they say. I left them alone and everything came out. We solved the problem in time and that's the way it was.\" reply londons_explore 10 hours agorootparentComputers then were like computers now... It's one thing to get python to spit out an answer to your problem, but it's quite a bit harder to be sure it's spitting out the right answer. If I was the supervisor in the above situation, I wouldn't let them cut those corners, since it's far more important to get the right answer when $2B of project could be led down totally the wrong path if a calculation tells you you need 10x less/more uranium than reality because someone shuffled up some punch cards wrong... reply bewaretheirs 6 hours agorootparentWe're not talking about programming errors, but malfunctions in the hardware producing incorrect results - whatever the electromechanical equivalent of a bitflip is. Given the error rates described by Feynman, if they had to restart the entire computation from scratch after a single detected error they'd likely never have completed the task. reply samatman 5 hours agorootparent> whatever the electromechanical equivalent of a bitflip is It's a bitflip. Just one that makes a surprisingly loud chunkk noise. reply Cyphase 11 hours agoparentprevOf course; how else were they going to make the bomb without a bunch of Genius Physics Utilizers? reply idunnoman1222 11 hours agoprevTSMC’s accomplishments are just greater than putting some TNT around enriched uranium reply pfdietz 9 hours agoparentEnriched uranium was exploded in a gun bomb for the Manhattan Project, not by implosion. The lenses in Fat Man used two different explosives, in each of which TNT was a minority component. reply jauntywundrkind 10 hours agoparentprevTSMC has had aot of help. America helping ASML get started to power TSMC the end of a long chain of events here, is enormous amounts of science done by many, over time. Who really knows, but I tend to think the nuclear physics were pretty involved. I enjoyed the What John Von Neumann Did At Los Alamos, from the evr excellent 3quarksdaily crew, which talks about how his brilliant mind combined with his explosives expertise was key to steering the Manhattan Project towards success. The article enhanced my appreciation for the challenge. https://3quarksdaily.com/3quarksdaily/2020/10/what-john-von-... reply synergy20 7 hours agoprevNone of Mark's new ideas take off: VR/AR, Metaverse, now AI. All of them just burnt a HUGE amount of money he made from his old adventures. If he is not the boss, he might have got laid off couple of times long time ago. reply khazhoux 11 hours agoprev [–] The Manhattan Project was notoriously wasteful, in the heady WWII rush to defeat the Axis. Congress approved any and every budget request with little oversight, and the project's secrecy ensured almost total lack of accountability. Some of the most notable offenders are considered to be the roughly 3% of the budget that was dedicated to the HPD --the Hats Procurement Department-- at Oppenheimer's personal request. Another ~4% of the total budget was for the AOACD --the Acronyms and Other Abbreviations Coordination Department-- though they earned their worth when they delivered the mission-critical PLSF and TSR/Em in record time. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Meta's GPU spending nearly matches the Manhattan Project's expenses in today's currency.",
      "However, it falls behind the investment made in the Apollo Program."
    ],
    "commentSummary": [
      "Comparisons are made between historical projects (Manhattan Project, Apollo Program) and modern companies (Meta, Apple) regarding spending, investment, and societal influence.",
      "Discussions extend to technological progress, resource distribution, economic expansion, and ethics within large corporations.",
      "Focus is also placed on workplace productivity, income inequality, consumer goods evolution, and environmental implications of technological advancements."
    ],
    "points": 255,
    "commentCount": 224,
    "retryCount": 0,
    "time": 1714942117
  },
  {
    "id": 40266845,
    "title": "Monitoring Energy Usage with Smart Plugs: A Tech Success Story",
    "originLink": "https://ounapuu.ee/posts/2024/05/02/smartplugs/",
    "originBody": "./techtipsy About Posts Tags About Posts Tags Monitoring energy usage with smart plugs, Prometheus and Grafana 2024-05-02 — 8 min read #prometheus #grafana #smart plug #experiment This post isn’t a detailed line-by-line tutorial on how to set up each individual piece of the setup as those types of guides tend to get out of date really easily, but if you know your way around Linux and the command line, then you can definitely replicate this setup on your own. Over the past few years I’ve been interested in learning about how much energy my computing setup and home appliances use. I’ve used a simple digital energy meter before to get instantaneous readings, but that was not ideal for monitoring how an electrical appliance consumes power over a longer time period. My friend bought a few smart plugs from athom.tech. After getting confirmation that they don’t suck, I went ahead and ordered some myself. The smart plugs I bought were the EU style plug V3 variant. The shipping times in EU were reasonably fast, shipping in 9 days (and that included Christmas!). The plugs ship with Tasmota pre-flashed. The plugs come with a small paper strip explaining the steps to take to connect the plug to your Wi-Fi network, and after that you can manage the plug in your browser. Quick start instructions that ship with the smart plug. Updating the firmware to the latest version is easy and doable in the web GUI with just a few clicks. The web UI is simple and very functional. The software stack My initial goal with these plugs was to visually monitor the power consumption of a few devices, such as my home server, router, workstation setup and the electric water heater. The power meter data is collected to an existing Prometheus instance on my home server. If you’re not sure what Prometheus is, then think of it as a tool that periodically reads metrics from different sources, saves them to disk and allows you to later query and manipulate that information. I run multiple instances of this tasmota-power-exporter solution on my server, one per plug, which get scraped once per second by the Prometheus instance. It is possible to also make the plugs themselves export these metrics, but I didn’t fancy building Tasmota firmware myself, yet.1 I already had a Grafana instance running on my home server, so I reused that to show a few basic graphs for the power meter setup. Free tech tip: make sure to change the min step setting to 1 second to get the most detailed data points on your graphs. It's this one. Observations and findings So, what have I learned after running this setup for almost 4 months? Water heater It shouldn’t come as a surprise that the electric water heater uses up the most power. The one we have is a 30L one, just enough for a quick shower or two or for washing a large load of dishes. Typical power usage: 4.51 kWh/day. Minimum observed: 0.56 kWh/day, happens usually when nobody is at home Maximum observed: 11.1 kWh/day, a lot of washing and showering happened on that day Typical power consumption pattern of a water heater. Home server setup I run all my home server workloads off of a Zimaboard. One of its big selling points for me was its super low power consumption. When idling, the Zimaboard can use about 2 W, typical power usage with all my services running was about 7 W and the maximum power consumption was around 15 W. The Zimaboard was actually using less power than my internet modem/router box (which is hot garbage by the way). The ISP-provided box used 12-14 W at all times, regardless if it was operating in router or bridge mode. Zimaboard + modem/router box power consumption. The ISP modem/router box is a horribly inefficient device for what it does. At one point I temporarily switched my home server setup back to the ASRock Deskmini X300, mainly because I added some latency-sensitive workloads onto my home server setup and the Zimaboard was a bit too weak for those. One thing that became really obvious from the power meter is that the Deskmini idle power consumption is horrible in comparison to the Zimaboard, coming in at around 15-20 W. This might be in part because my Deskmini doesn’t seem to expose any lower CPU power states than C3 while the Zimaboard exposed C-states all the way down to C10. I could not find a way to expose lower C-states in UEFI settings or the Linux kernel. Based on a quick remark in this video I believe that it should be possible to drop ASRock a message and receive a custom BIOS that enables some extra features, but that seems even more unlikely for a consumer-grade device such as this one. ASRock Deskmini X300 + ISP box power consumption. I’ve added a panel to my usual Prometheus node exporter Grafana view that shows the server setup power consumption alongside other metrics. Higher CPU activity is clearly visible on the power consumption graphs. A moderate jump in CPU usage can result in a big jump in power consumption. Voltage The Tasmota plugs also report current voltage values. I graphed them just for fun, but noticed that there is a sort of seasonality to the values. During the usual peak power consumption hours the voltage drops across the board. Seasonality as expressed through voltage readings. This seems like a candidate for testing anomaly detection, as shown in this great GitLab article. I’ve used this concept at work and it works reasonably well for some metrics. In other cases the voltage drops were caused by running appliances that use a lot of power, such as the water heater, electric kettle, electric stove or the microwave. Water heater causing the voltage to drop slightly for all plugs. During cooking with an electric stove, the on-off cycles were also noticeable on the voltage reading on all of the plugs. The impact of an electric stove on the voltage. Workstation I have one plug that reports numbers for everything that’s connected to my home office setup: monitor, USB-C dock, monitor light bar and anything that might be charging on the table. The power consumption of this setup varies a lot. Sometimes I do a longer home office stint. Sometimes I recharge various devices. The typical power consumption of the whole setup while doing something on my computer is around 45-60 W, with peaks near 90-110 W. For an ultrawide monitor, USB-C dock and my laptop I think this is a pretty good result. I’ve had desktop PC-s that have used just as much power at idle, and in the olden days there were incandescent light bulbs that used up more electricity than this! Typical power consumption of my work desk when working from home. Typical power usage: 0.95 kWh/day. Minimum observed: 0.07 kWh/day Maximum observed: 1.52 kWh/day Charging These plugs are also great for observing the charging patterns of various devices. A laptop or a power bank charges faster at the beginning, but the speed drops off as the battery gets more full. At one point trickle charging seems to kick in until the device is fully charged. A power bank charges quickly in the beginning and starts slowing down once it gets close to being fully charged. This pattern is similar on most devices that I charge. The battery on my e-bike seems to be an exception to the rule, with a slighly increasing power consumption throughout, and a faster drop at the end of the charging cycle. E-bike battery charging pattern, from nearly empty to full. 0.5kWh of power consumed for 60km of range with the “Turbo” preset on the bike doesn’t sound bad at all. Half a cent per kilometer! Stability The stability of the smart plugs is usually fine, but there are frequent cases where certain plugs don’t report back in time with the statistics. This may be a Wi-Fi access point issue, but annoying nevertheless. I’ve had to completely power cycle two plugs a few times because they dropped off from the network completely and would not come back. Again, could be the fault of my Wi-Fi AP. Future ideas The plugs also provide a way to turn the devices on and off over different API-s (even over HTTP!), which is something I’d like to utilize either using something like Home Assistant, or a simple script that toggles certain devices on and off based on the current electricity price. I haven’t gotten around to this part yet, but from my research it seems that the necessary integrations exist for Home Assistant. Closing thoughts Overall, I’m very happy with this setup. I can get reliable measurements for all sorts of computing setups from now on, which will make judging the power efficiency of the devices I use much easier. No more guesswork! If you enjoyed this post, then I highly recommend giving this FOSDEM 2024 talk by Florian Quèze a listen. The talk also goes into details about measuring the power consumption of various devices in the speakers’ home and observations that they’ve made. This was pointed out to me by a fellow reader. Thank you! ↩︎ Subscribe for more tech tips! Subscribe to new posts via the RSS feed. Not sure what RSS is, or how to get started? Check this guide! You can reach me via e-mail or LinkedIn. If you liked this post, consider sharing it! Random tech tips! ASRock X570M Pro4 motherboard overview My very first career day Accidentally turning the ASRock DeskMini X300 into a semi-passively cooled PC How to mess up a simple ThinkPad X230 BIOS flash and how to recover from it DIY cloud gaming setup with VFIO, Parsec and AMD ./techtipsy © 2024 Powered by Hugo Theme created by panr, tweaked by me.",
    "commentLink": "https://news.ycombinator.com/item?id=40266845",
    "commentBody": "Monitoring energy usage with smart plugs, Prometheus and Grafana (ounapuu.ee)253 points by hddherman 15 hours agohidepastfavorite138 comments stavros 14 hours agoI got a Zigbee power breaker and hooked it up so all my flat's power goes through it, and then made an e-ink display to show my power consumption: https://www.stavros.io/posts/making-the-timeframe/ reply danieldk 3 hours agoparentFor people in The Netherlands: all Smart Meters that the net maintainers installed (are required to) support the P1 standard, which provides a standardized interface for customers to read out current current draw, cumulative power use, etc. Usually gas is hooked as well. You can hook up a cheap dongle to expose the stats in an app. For instance, we use: https://www.homewizard.com/nl/shop/wi-fi-p1-meter/ This meter also exposes an API on the local network. I have written a small driver for the SmartThings Hub, so that you can get the stats/graphs in the SmartThings app as well (we use a SmartThings hub for Zigbee/Z-Wave devices): https://github.com/danieldk/homewizard-energy reply benhurmarcel 1 hour agorootparentSame in France, the official meters have an interface on which you can plug a Raspberry Pi for example to read the data live. reply Maakuth 2 hours agorootparentprevThis applies to Finland and Sweden too, perhaps others to follow. reply jononor 2 hours agorootparentSame principle in Norway, though it uses HAN instead of P1. reply 3abiton 14 hours agoparentprevThat was a great write up! I wish I had the time to follow-up this guide. Thanks for sharing it! reply stavros 14 hours agorootparentThank you! It's not very hard to use, you basically just flash my firmware and use my script to display images on the Timeframe. That's about it. reply hackernewds 13 hours agorootparentprevRelated to your comment so much, I got hurt reply aksss 12 hours agoparentprevI think the aliexpress link for the display is busted (as they do). A natural integration would be with Home Assistant. I’m not sure if the Earu breaker has an OOTB integration with HA yet, beyond doing something like Zigbee2MQTT and configuring entities for readings. It’s a good pattern though - integrate meter with your automation hub, let the automation hub push the images to displays, for meter and everything else. reply shrx 13 hours agoparentprevI'm curious, which power breaker do you have? reply stavros 13 hours agorootparentThis one: https://www.aliexpress.com/item/1005005657383634.html?spm=a2... reply wannacboatmovie 12 hours agorootparentYou're running all your flat's power through that $14 Chinese rubbish and never assumed that shortcuts were taken or quality would be an issue? How do you know it will continue to function as a circuit breaker and isn't just a piece of wire inside? For the uninitiated, CE marking is meaningless (it allows for self-certification). I'd like to see Big Clive do a teardown of one of those. reply londons_explore 9 hours agorootparentThere are 2 ways to design these. They could use a regular relay, or they could use a solid state relay. Solid state relays have widespread fraud. Like 60% of the ones on amazon will catch fire or fail before they hit the rated current. Trade suppliers generally don't sell them at >30 amps. Regular relays up to 10 amps are cheap and reliable. Beyond that, they get expensive surprisingly fast, and the reliability is hit or miss. They fail in numerous ways, but the most concerning one is the plastic case melting and catching fire. The chance of failure depends on the nature of the load (capacitive or inductive loads will dramatically shorten a relays lifespan). In my professional career, I have witnessed ~20 of the above devices failing, with melted bits or burn marks, but of that sample none has burned down a building, yet. But I'd say that was more down to luck than good design. In general, I would trust a china-device for monitoring power, but not for switching anything more than ~10 amps (1 outlet). reply baq 1 hour agorootparentIs it the switching that kills the relay or is it enough to start/stop the load? E.g. if I have a 10A smart plug monitoring power on a small waist-tall freezer in the garage will it eventually damage the switching function? reply stavros 12 hours agorootparentprev> How do you know it will continue to function as a circuit breaker and isn't just a piece of wire inside? I don't care. I connected my previous breaker after it. reply wannacboatmovie 12 hours agorootparentWhy not just use a current transformer? (Clamp over wire type) It's much safer. reply stavros 12 hours agorootparentDo you know of a Zigbee one? That's also a good option. reply wannacboatmovie 12 hours agorootparentSome options at the $10 price point on AliExpress, though I cannot recommend one, maybe another reader can. CTs are the generally accepted way to do this and don't have to modify mains wiring. Could also build your own; CTs are inexpensive. reply stevenhuang 12 hours agorootparentprevnext [2 more] [flagged] wannacboatmovie 12 hours agorootparentThanks Dad. reply theshrike79 12 hours agorootparentprevYou can get less shit ones that aren't made of Chinesium from Shelly reply wannacboatmovie 12 hours agorootparentGreat suggestion. Some of their modules appear to be UL certified. reply theshrike79 20 minutes agorootparentSome of them (most of the latest models) are scriptable on-device. There are some ready-made scripts for the Nordics that check the current electricity prices and use those with some basic rulesets to see if the electric heating in a house should be on or off for example. I just use mine to turn off the ice machine at 22:00 so it doesn't run through the night :D reply baq 1 hour agorootparentprevYeah you need to shop for these specifically. They come at a premium. reply dist-epoch 11 hours agorootparentprev> $14 Chinese rubbish Where do you think the \"quality\" devices are made? reply homero 3 hours agorootparentQuality ones inspect the product and have a company that can be sued. Others change their name monthly. reply whatble 8 hours agorootparentprevThis is NOT a valid 63 Amps rated breaker, or 63 Amp anything for that matter. The screw terminal will melt. Based on the screw terminal, without looking inside, I would rate it not higher than 10 Amps. Don't pass your whole apartment through it. reply gog 13 hours agoparentprevCan you link the power breaker? reply stavros 13 hours agorootparentThis one: https://www.aliexpress.com/item/1005005657383634.html?spm=a2... reply homero 3 hours agorootparentIt's crazy to run any power through that. Only safe option are current transformers reply kasey_junk 12 hours agoprevI nerded out on this a few years ago and ended up buying a not well known device called a rainforest automation eagle (https://www.rainforestautomation.com/rfa-z114-eagle-200-2/). Its a pretty straight forward little linux device that reads your smart meter (after being enrolled via your utility). It exposes an xml api that I bridge to Prometheus (https://github.com/kklipsch/reagle). I also bridge my utility (ComEd's) pricing feed to prometheus (https://github.com/kklipsch/comed_exporter). Between those 2 I get pretty good whole home utilization and pricing info graphed into Prometheus (and thus into Grafana). reply recursinging 13 hours agoprevOne step further. I just installed the Emporia Vue 2 in my distribution box. 16 CTs plus the three mains phases. It's ESP32 based and there is a great ESPHome project that you can flash it with for local only reporting. Add some HA and VictoriaMetrics, and now I can see how the whole house behaves with Grafana. Next up, Zero-Export using this data to steer my little OpenDTU solar plant. We live in such cool times! reply pzduniak 13 hours agoparentIs anyone aware of any other OSHW alternatives to this? Preferably with Ethernet. ESPHome would be preferable. The clones I can find are roughly the same price as the \"original\" hardware. ATM90E32AS seems to be ~$1 per channel on JLCPCB, so I'd imagine this could be pretty cheap with SMT assembly. My use case is like ~60 circuits. reply BHSPitMonkey 2 hours agorootparentThe main one I'm away of is IotaWatt, which I've had running in my panel for the last few years without incident. reply anupcshan 10 hours agorootparentprevYou can install esphome on Emporia Vue. https://github.com/emporia-vue-local/esphome (not used this myself). Vue V3 has wired ethernet support too. reply briffle 8 hours agorootparentprevMany people flash the emporia vue with espHome reply yx827ha 8 hours agoparentprevI bought a CURB Energy monitor about 6 years ago. Does anyone know if it's possible to flash open source firmware on it? It only has a cloud integration, but I would really like to hook it to to home assistant. reply applied_heat 13 hours agoparentprevVictoria metrics and grafana is great. I only wish I could enter descriptions for the metrics to populate the description in the grafana metrics explorer which is traditionally done by Prometheus metric metadata “help” field. Victoria metrics/ grafana is supplanting our industrial historian, which is admittedly not a best in class product - I am sure osi pi is better reply septic-liqueur 13 hours agoparentprevOpendtu solar plant - care to elaborate? reply recursinging 13 hours agorootparentOpenDTU is an open source project using an ESP32+CMT2300A for talking to Hoymiles Micro-Inverters. Local Only. reply jauntywundrkind 12 hours agorootparentI did some scouting about for what microinverters would be usable without a full professional install, for a small under half kilowatt playing around. I was hoping I could snap up some used enphases & try stuff out with a 200w panel & my existing batteries. But I really didn't turn up much; most discussion online made it seem like you needed special installer access to get anywhere with Enphase. Exciting to hear maybe this microinverter idea might not be totally dead in the water. reply sponaugle 13 hours agoprevI used IoTaWatt devices, which can be installed in panels. It is a great solution for by circuit monitoring, and has direct influxdb integration so you can use Grafana. Per plug monitoring is cool however for getting specific devices on a circuit. (Short video about the setup: https://www.youtube.com/watch?v=-tcbJCvuJG8) reply gibbonsrcool 12 hours agoparentWow it’s a small world. I just discovered your channel recently and love the content. Probably watched the home lab video like 10 times. I have a historic building with 3 vacation rentals that I’m in the process of adding some intelligence and monitoring to, including a raspberry pi kiosk based on one of your videos. Thanks for making them! reply sponaugle 9 hours agorootparentThat is cool! Indeed there is a good cross section of home automation , software development, power monitoring, and hacker news. I just did a new video on building a 68030 computer that I suspect will have a much smaller audience! reply RatchetWerks 8 hours agoparentprevI gotta chime in also. I just learned about your channel literally today when a buddy DM'd me a link on your YouTube homelab tour. We have pretty similar interests. I also have an IOTAWATT and I love it. My only grip is I wish they had the ability to log more channels with the same unit. ie Approximately 30 channels. reply sponaugle 5 hours agorootparentSweet! Yea, I have several panels that have 2 of them for that reason. I have debated making a larger one since the code is open source. It would be great to have one that has more ports, and perhaps the ports combined into less cables. reply richardjennings 13 hours agoprevI scrape power usage metrics from Tapo P110s and push them to Grafana Cloud using https://github.com/richardjennings/tapmon - although as other commenters have noted - using Wifi for smart plugs has its rough edges. reply rkagerer 13 hours agoparentI second that. I replaced a bunch of wall switches with Leviton WiFi smart ones and would never go WiFi again. They're totally unreliable. My Meross smart plugs fair a little better but still lose connectivity now and then (got a bit better with updates). reply ssl-3 12 hours agorootparentI hear a lot about reliability issues with various wifi smart devices. I've had perfectly lovely reliability with wifi smart devices in my mixture of zigbee/wifi at home, such that I don't really have a preference. Except for one cheap ESP8266-based wifi relay module that had some liquid damage (not the module's fault), and the LED driver in my very first RGBW light bulb finding death after being used for a few years (a common-enough tale regardless of connectivity choice), they seem to Just Work. It's all semi-random brands of devices, bought over time. I'm not doing anything particularly fancy with the network itself: It's just a couple of hardwired dual-band Mikrotik access points, with one upstairs at the back of the house and one downstairs at the front of the house (perhaps non-obviously, on non-overlapping channels). A Pi 4 with OpenWRT quietly does the packet-slinging. Like in many other places, the 2.4GHz band is approximately ruined where I live these days. It's noisy and slow. But it all works well enough to reliably toggle a relay on or off, at least. Am I just lucky? Are others just unlucky? Or is there an actual pattern here? reply nucleardog 10 hours agorootparentI’ve tried a handful of WiFi light bulbs, smart plugs, and other things. They were connected to a dedicated 2.4GHz access point (MikroTik as well). WiFi signal was fine—had good coverage (access point was central to the small wood frame house, and checking signal strength at device locations showed a great signal) and I live in the middle of a forest and there’s nothing else within range to interfere. Basically best case scenario outside of a lab or something. I haven’t had a single device that worked reliably. Some worked fairly consistently, but only after a long (and variable) delay. Many others failed to work often enough that, combined with the delays, the workarounds became the normal way of using things. At this point I’m running basically everything over Z-Wave (via Home Assistant) and it’s been rock solid for me. Especially with the ability to set up direct associations, things like “this dimmer’s state should be synchronized to that dimmer” are very responsive and reliable, not involving my controller or Home Assistant at all. Hard to say whether you’re lucky or I’m unlucky—most people having a good experience aren’t going to take to the internet to start a crusade about it—but I do occasionally see someone recommending or saying that Z-Wave or Zigbee has been reliable for them… I think yours is the first I’ve read where someone’s been happy with WiFi. reply ssl-3 6 hours agorootparentWhy sure. It's a rule that people who have the least amount of success with a thing will write the most about that thing. This is why those with their wits about them read things like Amazon reviews with a decent-sized grain of salt. And yes, you're describing a very quiet environment in terms of outside interference. I'm seriously a little bit envious of that. One thing that I am doing differently than what you were doing is this: I'm not isolating my smart-widgets to their own wifi access point, as I suspect most people also are not (since \"most people\" just have a single access point/all-in-one router for everything). I built my little wifi network to have what I feel is good coverage in and around the whole house, with the intent that all devices (dozens of them) would use that same wifi SSID. As an unintentional result of this combined network, if/when there's a problem with the do-all wireless network, I'm pretty likely to notice right away because things like my phone and my laptop won't work like they did yesterday. And wifi problems have happened for me: For instance, before I went 100% Mikrotik, I was using an old once-fancy Asus router with third-party firmware as a combination of access point and switch for part of the house. It became increasingly unreliable as the years ticked on for whatever reason, and always came back to life after a quick reboot, but it eventually would turn stupid again anyway. And whilst it was being stupid, various things would indeed break: The lights wouldn't turn on/off, or I'd see that my phone was using cellular data instead of wifi, or I'd say \"Hey Google\" and get \"I can't connect to your Wifi\" as a response. Madness, insanity. (And then I'd go unplug that router-shaped Asus access point for a few seconds, plug it back in, and things would be fine after a few minutes -- every time.) But I have not at any time blamed the smart end-point devices (the wifi light bulbs, the switched outlets, the whatevers) for what was clearly -- in my case -- an infrastructure problem. (And having a particular old Asus router-box turn funky isn't indicative of a wifi-specific problem, either -- it's just indicative that this hardware had become increasingly broken over time.) reply nucleardog 4 hours agorootparentI have two identical Mikrotik devices bolted to a wall beside each other in a closet. One is the WAP serving all of our phones, the laptop I use for work, the laptop my wife uses for work, the TV, the PC hooked to the TV, and everything else. It’s been 100% rock solid. So my initial assumption is definitely going to be that the nearly identical setup except only serving a handful of low bandwidth devices is going to be just as solid. That would seem to be a safe assumption given I’ve no noticeable missing data points from the $700 German air quality sensor that I’ve been recording for years and is connected via that access point. It’s moved to various rooms and points throughout the house as demand dictated without issue. It would seem to be the case given I can pull up a feed from the WiFi IP camera I hooked up and pull it continuously with no latency or dropped frames whenever I want. If I have an infrastructure issue, it’s one that is curiously selective about cheap IoT hardware while ignoring whatever other random stuff I hook up. After a decade of installing MikroTik networks in hotels, condo buildings, and office buildings supporting all manner of nonsense you can take my word that there is a strong, reliable, WiFi network connection available at any point inside my house… or you’re welcome to come by with whatever diagnostic gear you’d like and tell me how it’s broken. I’d love to be able to make good use of some of that wifi IoT stuff! reply ssl-3 2 hours agorootparentI haven't, at any time, doubted your ability to correctly configure an access point. I'm just trying to find some data points that allow for an explanation of both \"Eh, seems fine for me,\" and \"Arrgh! This stuff doesn't work! Into the bin!\" I mean: I believe you when you say it doesn't work for you, for I have no reason not to believe you. And I assume that you also believe me when I say that it does work for me. What other variables/data points could there be, do you suppose? It's hard, as you probably know, for me to imagine ways in which things can break when they've generally been working fine for me. One possibly-related theoretical datapoint: My access points are not near eachother at all, on purpose. It is perhaps possible that the chatter from one of your APs was \"desensing\" (I hate that word, but it's a common-enough word) the front-end of the other AP and that this limits the ability of that AP to receive weak-ass signals from an ESP module (or whatever) in some manner of smart device. (When I do want to isolate wifi networks, like when my neighbor asks if he can borrow a cup of Internet because he's broke this month, I've been successful at creating virtual wireless interfaces that are steered to a particular VLAN -- even as far back as the WRT54G days.) Another possibly-related data point: I do run my 2.4GHz channels at 20MHz bandwidth instead of 40MHz, because that always seemed to get better performance at a longer distance (not that I think I need that for most little in-home smart widgets, but it is nice to be able to take a wifi-connected speaker into the yard or out by the alley where I work on my car, and to use it without using bluetooth[0] or making my phone into a battery-sucking hotspot and reconfiguring the speaker to use that hotspot instead of the house's SSID). In doing this, the best available throughput in my neighborhood is not very quick at 2.4GHz -- it's always down into low single-digit Mbps with 20MHz-wide 2.4GHz channels, which quite frankly sucks. But it always seems to work with a fairly consistent level of suck, and that level of suck is adequate (throughput-wise, at least) for what I actually need/want from 2.4GHz. And, sure: I'd be happy to swing by with a few smart devices that seem to Just Work and one of my rather boring Mikrotik wAP ACs so we could sort it out. Or just share configs with passphrases and SSIDs sanitized? I don't think I'm doing anything special, but perhaps I am? (Perhaps I'm even doing something that is both wrong and stupid, but which lets it actually-work.) I mean: At the end of the day, I just want other people to enjoy the same pleasure of being able to plug in random stuff and have it generally just behave. I don't think I'm an expert, though: I've got some background in land-mobile RF, have established some rather long links between Ethernet-connected devices using ISM bands (some of which have stood up for well over a decade), and I've been playing with Wifi since Orinoco PCMCIA cards were the new hotness. I think I know a few things, but that doesn't mean that I've got some super-secret sauce or something. At home I'm just a cheap bastard who wants to automate some things, and who seems to be successful at getting inexpensive things like TP-Link smart plugs, ATHOM light bulbs, random Google/Amazon smart speakers, and trash-tier relay modules to work seemingly-reliably with Home Assistant over wifi. I'd pay more for Z-Wave or something if I felt that I had to do so, but my (perhaps unique) experience with wifi isn't leading me towards Z-Wave. (I also had good \"luck\" with X10 around a quarter of a century ago: It always worked well-enough to automate reliably as long as I kept everything X10-related far away, wiring-wise, from the beastly Best FerrUPS UPS I was using back then.) --- [0]: Most people would use Bluetooth for this, and they're generally not wrong. But I absolutely hate the way in which Bluetooth breaks when the person with a Bluetooth-connected speaker wanders off with their phone in their pocket and leaves their speaker behind: The audio breaks up in ways that are particularly annoying, but which they'll never, ever hear -- much to the disdain of anyone who does hear it happen. I don't like being that person. If I'm going to play music that the neighbors or anyone else can coincidentally hear, I don't want it to get broken and choppy just because I went inside the house to fetch a different wrench or a beer or something, even if I myself will never hear the problem that Bluetooth use can promote. In this way, slow wifi is better than broken Bluetooth. reply 123pie123 13 hours agoprevi've just bought a cheap esp32 with a light sensor connected to it. then connected light sensor (ie bluetac) to my electicity meter that pulses every 1/1000 1KW/Hr, it uploads the data to google sheets which graphs the output - works great I also have another esp32 at my elderly relatives house with a pir sensor connected to it, it's also sending the movement data to a different sheet on the google sheets site, so that i can monitor some sort of movement. i'm i expecting google to discontinue this service at anytime - yes, but its working for now. you can write and read data from the google sheets via json via the esp32, not very inutitive but doable (and free!!) reply whitehexagon 13 hours agoparentnice, I like the simple approach. My old meter only had a rotating disc and it took all my effort to get a sensor connected to my arduino that could detect the black mark on the edge of the disc. There was just enough mem for a http service to allow me to pull that value into my iobridge for remote monitoring. reply gog 13 hours agoparentprevI have a Frient Zigbee device that does the same, but sends data to Home Assistant. reply 123pie123 13 hours agorootparentHome assist looks good, but my requirements was that my elderly relative was being monitored by all my family (ie they're not on my home wifi/lan) hence the data on the movement needed to be on the internet for all to see reply leeoniya 14 hours agoprevi'm trying to nudge Grafana into the direction of IoT/SCADA control, so we can be both, a great way to viz (data sources) and to control (data sinks). not personally a huge fan of having to recommend Home Assistant for that use case :) (i work at Grafana Labs) reply applied_heat 12 hours agoparentIf grafana supported numeric entry fields and buttons it could easily replace wonderware intouch etc. Traditional scada systems have such brutal plotting abilities they are ripe for disruption reply leeoniya 11 hours agorootparentfor sure. we're continuously adding capabilities to the Canvas panel to support SCADA-type and flowchart use cases. https://grafana.com/docs/grafana/latest/panels-visualization... there's some initial movement towards \"press Canvas element -> invoke HTTP api call\": https://www.youtube.com/watch?v=T6fg1TpfBUg we added streaming/websocket data sources a few major versions back. i'm hoping to make something more standardized and pluggable like data sources. reply applied_heat 4 hours agorootparentI think OPC-UA uses http, and there are OPC-UA servers to talk to Just about every type of device, so maybe it is already possible. I was looking at the canvas element to do an electrical single line diagram. reply recursinging 14 hours agoparentprevThis would be great. Were using Grafana dashboards for thermal vacuum testing. Everyone is always asking for simple SCADA functionality. reply hackernewds 13 hours agoparentprevcould you expand on your comment? what is data sinks as applied to your idea? reply leeoniya 11 hours agorootparentsimilar to how you can make plugins for data sources, you can make plugins for data sinks, like write over modbus, or POST to http API or, publish to some kind of MQTT broker, etc. since IoT devices have limited storage, usually the metrics are dumped into another system like Prometheus. but that Prometheus data source used plotting cannot be used to control the device, which will have another endpoint and another API, so we need some kind of concept of data sinks. at least that's my idea right now, allow data links configured in the panels to poke some \"data sink\" with values that are available in the DataFrame or custom-entered into the UI, like we do with traceID for Exemplars, etc. reply nagisa 2 hours agorootparentHave you seen/know about NodeRED? I myself have moved away from HA/OpenHAB-likes to NodeRED as that allows me control that's more... restricting. At its heart NodeRED is a visual programming tool (and I think it wouldn't be a terrible fit for Grafana), but what NodeRED has and Grafana doesn't is a community which built a very significant number of packages that integrate with various protocols and devices used in home automation. I see a potential for some integration and/or idea sharing here. But it is also important to understand that the set of people who want to fiddle with their homes is much, much larger than the set of people who (are able to) program. Which is why HA and the likes are so popular. reply fidotron 12 hours agoprevFacetious but half serious reaction: surely grafana usage nullifies any possible benefit from the monitoring? If there is one piece of software regularly using way more resources than seem reasonable it's that one. reply Xerox9213 15 hours agoprevYeah, not to mention the ability to automate things. My latest automation: when the white noise machine is on for the baby, the doorbell volume is turned down. reply alchemist1e9 15 hours agoparentCould you expand on how prometheus or grafana helps you automate? I didn’t know either enabled that. reply Jleagle 14 hours agorootparentI think this was supposed to be a reply to one of the Home Assistant threads reply Xerox9213 9 hours agorootparentCorrect. My mistake. reply eMerzh 13 hours agoprevHomeassistant + Power calc (https://github.com/bramstroker/homeassistant-powercalc) really does wonder here, you can \"simulate\" power of fairly stable appliances. Then you chart that in a nice Sankey chart or in standard charts and enjoy reply dsab 14 hours agoprevGuys what are your favourite smart plug? I need one with easy integration with grafana, not sucking, and shipment to EU country? reply bennyp101 29 minutes agoparentI've been using a few of these on various devices https://www.mylocalbytes.com/products/smart-plug-pm reply ckolkey 14 hours agoparentprevI'm a huge fan of https://www.shelly.com/ - they have a built in webserver and can be controlled via POST requests. No cloud needed. reply theshrike79 14 minutes agorootparentBig +1 for Shelly stuff. IIRC it was started by a hobbyist who got annoyed at the crap from Aliexpress and decided to do it properly. And that they have done. reply septic-liqueur 13 hours agorootparentprevI also like their products very much. I installed a few of them in my sockets. Some people argue that it's not safe to put in your sockets walls and the 16A limit is realistically lower before they can overheat and cause fire. That scared me a little bit but I think most of the reports are from bad wiring like using thin wires or not tightening the clamps enough reply spockz 11 hours agorootparentI have a Shelly pro 1PM in my Breaker box for the car charger. It gets to 80 degrees Celsius easily at 16A. I have two other gripes with it: 1: the slots/clamps are small which makes fitting 4mm2 wires a chore already. 6mm3 is impossible. 2: the overcurrent protection is very trigger happy. Due to solar panels in the street that voltage can vary significantly. Apparently the charger doesn’t always keep up exactly resulting in a current of 16.0001A which is more than the limit of 16 and poof, off it goes. Not sure whether this is an actual fault in the charger or some rounding error. reply whitehexagon 13 hours agoparentprevI've been happy with my tplink sockets, especially running them off-cloud, and getting some command line control over them (although I think that debug api got blocked on later firmware updates). But quite easy to feed data into any db once you have such control. Just about to try some ikea zigbee sockets, seem cheap(7e) in comparison. I hope I can also get them working command line based, just trying to setup a sonoff usb stick with some python package (bellows) as we speak. reply argulane 13 hours agoparentprevATHOM plugs are very nice, you can order one with ESPHome or Tasomata firmware preinstalled and they can ship from Germany https://www.athom.tech/ reply aulin 5 hours agorootparentI had four tasmota preflashed athom plugs die on me in less than a year. As long as they worked they were perfect but they don't last... Hope they addressed the issue meanwhile reply pipe01 1 hour agorootparentWere you switching high loads? I've had a few for some years with no issues reply kryptoncalm 13 hours agorootparentprevNote this vendor is different than the Athom company making the Homey smart hub: https://homey.app/ reply tetris11 14 hours agoparentprevZigBee list of good devices: https://www.zigbee2mqtt.io/supported-devices/ reply kryptoncalm 13 hours agoparentprevI have two plugs in use that ship to EU (at least NL) and are made (or at least certified) in the EU. The latter matters to me because of fire hazards etc (e.g. [1]). Both can handle 16A and connect over zigbee which helps to reduce idle power consumption. 1. Innr plugs, e.g. SP240 https://www.innr.com/en/product/innr-smart-plug-eu-with-powe... 2. Robb zigbee smart plug https://www.robbshop.nl/robb-smarrt-slimme-stekker-zigbee-36... [1] https://hackaday.com/2023/11/03/just-how-dodgy-are-cheap-usb... reply nikisweeting 12 hours agoparentprevI love these ATORCH ones on Amazon, they have a screen with a bunch of useful details, super stable wifi connection, over-voltage/current/power protection, etc. https://www.amazon.com/gp/product/B0BGSYJQK6/ reply Mister_Snuggles 13 hours agoparentprevI’ve got some Sengled E1C-NB7 plugs that I really like. The form factor is nice, they work perfectly with Zigbee2MQTT and Home Assistant, and they’ve got a power button on the device itself. I want to buy more and they don’t seem to be available anymore. reply holri 15 hours agoprevMissing from the blog. The power reading of the Tosmota device should be calibrated: https://tasmota.github.io/docs/Power-Monitoring-Calibration/ reply jhenkens 15 hours agoparentI would love a semi-automated way to generate a power-profile for ESP-Home. Find a smart room heater with 3 levels perhaps, and use home assistant to gather values at \"Off\", \"1/3\", \"2/3\", \"3/3\", with a downstream power plug as reference (and a known consumption of the downstream plug as well). So I can just take my EspHome plug and very quickly generate a standard set of mapping values for voltage and wattage. reply ssl-3 11 hours agorootparentThe easy way is with a resistive space heater and a multimeter. I keep a big, dumb, thrifted \"oil-filled radiator\" space heater around just to use as a big, safe 3-speed dummy load with reasonably OK repeatability (nichrome heaters do not have perfect temperature coefficients, but they're stable-enough that using them to measure temperature quickly begins to be a non-starter). The level of integration you choose is entirely up to you. I don't do this kind of thing much, so I'm OK with kludging together a test rig as-needed with a handheld meter and tearing it apart when I'm done. This makes good use of my own time and tools, according to my personal proclivities. But if I were doing it often, then I might buy the equivalent of the HOPI meter that Big Clive uses in many of his videos. It displays current and voltage, multiplies them to get power, and also displays power factor -- concurrently, on separate digital displays, in real time. Or I might build something: A box with a current shunt with some panel-mount meters and appropriate connectors would not be too challenging to put together in an afternoon with parts from Amazon and Lowes, depending on one's ability and desire to deal with sheet metal at home. (I use galvanized steel handy boxes and cover plates from Lowes for all kinds of small-ish stuff. They're cheap, common, and durable-enough.) Whatever the approach, a simple space heater with multiple literal-speeds seems like a cheap and useful way to make it happen unless you're trying to automate every part of it. (But by then, making a dumb multi-speed space heater into a \"smart\" multi-speed space heater that can be activated programmatically with software like ESPHome and some relays is probably pretty much a no-brainer, isn't it?) reply dainiusse 15 hours agoprevNo need for that mate, just deploy home assistant or something similar and you will get this (and more) out of the box reply Havoc 15 hours agoparentGrafana is a hell of a lot nicer & controllable than HA HA is great, but it's not the answer to everything reply Cyph0n 14 hours agorootparentWhy not both? You’ll need to run a server either way. HA can export data to Prometheus. Setting up and running HA is much easier than figuring out how to get a set of different smart devices to export metrics to Prometheus/Influx. Let HA deal with that. reply zimpenfish 15 hours agorootparentprevSeconded - HA's graphs are great for a simple \"is this going up or down\" glance but when you want to put a whole bunch of things together for comparison or perform aggregations or calculations, that's when you want Grafana et al. reply icehawk 13 hours agorootparentprevIt might be, but for all of the examples in the blog post, HA does this out of the box. reply madaxe_again 15 hours agorootparentprevAgreed. I live off grid, so energy monitoring is a big deal for me. HA is fine for “at a glance”, but if I want any kind of detail, I use grafana. I actually have my old openhab instance still running purely as I can’t be faffed setting up all the piping from MQTT into influx again. It’s also possible to integrate the usage over time using a dynamic time window to get Wh figures from wattage, which is enormously useful for me, and is more accurate than the figures HA gives in their power system. HA is dead useful for getting alerts when the laundry finishes, though - dumb machine, smart plug, look for a sudden drop in power. Also does all our climate control. So different tools for different jobs. reply whitehexagon 14 hours agoparentprevI'm also looking at a custom solution for my current migration from WiFi sockets to Zigbee. It seemed impossible to do an offline installation of home assistant, and discouraging signs for running it without an internet connection. There seems to be a sonoff usb stick that might act as a hub and allow command-line monitoring of all devices, should be perfect for feeding into grafana/prometheus. reply baq 13 hours agorootparentHA will happily run offline; if you mean HAOS then I don't know what it does but it's an unorthodox Linux distro, but once it installs it should also run offline without issues. I'm also using their skyconnect zigbee coordinator and it works very well. reply whitehexagon 13 hours agorootparentYeah one of the tests was a RPi image and it wouldnt complete without a LAN internet connection (only got 4G). And it seemed far too weighty for a bit of home automation. I recall the online requirement was for some ntp server requests that cant be disabled. reply baq 13 hours agorootparentYeah that's more of a rpi hardware requirement as it doesn't have a battery and you realistically want to have accurate time on your smart home controller, even - especially - after it cold boots after power loss. reply Banditoz 14 hours agoparentprevWhy not both? reply mindslight 15 hours agoparentprevRight up until the Home Assistant UI turns into a lagfest, the installation dies, and you can't debug why because Docker. At least that's what happened to me. And no, it wasn't RPi SD power issues. This happened on an otherwise-stable amd64 server. The Home Assistant authors' hostility towards simple native distributions is now a show stopper for me. Long term reliability is more important than quick initial setup. reply cyberax 15 hours agorootparentHA is actually pretty debuggable. Just install the SSH plugin, then SSH into the HA box, and then simply \"docker exec\" into the target HA container. reply mindslight 14 hours agorootparent... and then not have any of your usual development tools, environment, system layout, or repair techniques because you're inside someone else's \"works on my system\" that they threw over the wall. It's obviously possible to debug what goes on inside a Docker image. It's just not something I'm particularly interested in dealing with, especially under duress. reply cyberax 11 hours agorootparent> ... and then not have any of your usual development tools, environment, system layout, or repair techniques because you're inside someone else's \"works on my system\" that they threw over the wall. The thing is, the \"it works\" is reproducible because of containers. Which is a step above just hoping that it works. HA is also easy to \"patch\". You can just install your custom components in `config/custom_components`, it can also be used to \"override\" core HA files. Finally, if you are doing intrusive development, you can easily launch HA locally. macOS, Linux, and WSL are supported. You will lose the ability to install add-ons via the addon manager, but that's about it. FWIW, I had the same aversion to their custom OS and their crazy container-based setup initially. For a couple of years, I used to run HA as a Python app and managed the dependencies manually. Then I tried the HAOS and it... kinda just worked. reply TeMPOraL 13 hours agorootparentprev> because you're inside someone else's \"works on my system\" that they threw over the wall. FWIW, this can also be called stable state you can retreat to. And build upon, e.g. adding a layer of debugging tools. I don't really like to deal with Docker, but at least I have reasonable certainty it'll work. I prefer system package manager or MSI, but if not that, it beats having to build something when it's near-guaranteed that what I'll get is not the binary the authors had in mind, if it even runs at all. (Then again, I routinely rebuild Emacs to stay on the bleeding edge. But it took a while to work out all the usual dependency mess, and I even broke my system once doing it.) reply mindslight 13 hours agorootparentIt's certainly within my gamut to jump into an embedded system to debug it, bringing/building tools as I go. I'm just not looking to opt into doing that on something that doesn't need to be that complex in the first place. Same reason I run one decently powerful amd64 server that does many things rather than a stack of Raspberry Pis, one per software package. reply cyberax 9 hours agorootparentBut how would you do it differently? You need to host a bunch of daemons (MQTT, ZWave and ZigBee bridges, and whatever else you might need). And a bunch of these daemons can have their own gnarly dependencies (e.g. they can be written in JS and built with NPM, ugh). So you kinda _need_ to use Docker to make it at least sane. And if you're using Docker for the plugins, then why not use it for the HA core itself? And once you do that, you don't really need much from the host system. So why not use a minimalistic OS instead of something like Debian? reply mindslight 7 hours agorootparentAt the time my setup didn't require other daemons like that. But if I had been in that position, I would have just set up the other daemon under Debian and pointed HA at it. These days I'd say that NixOS captures that requirement, allowing orchestration of many daemons and other system config to be abstracted into a packaged solution (eg NixOS Mailserver), that the user can override as much or as little as they'd like. I believe NixOS does package (or at least attempts to package) HA, but given my past experience and what I believe is still the throw-it-over-the-wall desire of the HA maintainers, I'm wary of adopting it as an overarching solution. I'm certainly not ruling it out for performing some functions, like UI. I just would rather set up my automation efforts as MQTT-first, keep logging and automation rules as their own separate things, and not be fully committed to HA. reply cyberax 6 hours agorootparent> At the time my setup didn't require other daemons like that. But if I had been in that position, I would have just set up the other daemon under Debian and pointed HA at it. You can do that just fine even now. I'm doing experiments with voice control, and I run the complete AI stack locally on my computer. So I just set up everything as regular background processes. You just can't expect HA to be able to do autoupdates for these daemons. The other problem is that most of required dependencies are not packaged in Debian. So you'll have to install multiple NodeJS servers and tons of NPMs somewhere on your system. > These days I'd say that NixOS captures that requirement, allowing orchestration of many daemons and other system config to be abstracted into a packaged solution (eg NixOS Mailserver), that the user can override as much or as little as they'd like. You can do that with HA as well. Just push in a new image, and tag it appropriately. The last time I played with Nix, it needed to download tens of gigs of data for a few programs. I don't think this is acceptable for HA. You can definitely do HA in a piecemeal fashio, but there's just no way it can be done as a reproducible system that you can give to your grandmother. Given these constraints, HAOS is actually pretty remarkable. > I just would rather set up my automation efforts as MQTT-first, keep logging and automation rules as their own separate things, and not be fully committed to HA. Raw MQTT still needs a UI that is user-friendly. And even with MQTT you'll need to run ZWave and ZigBee bridges. reply mindslight 4 hours agorootparent> You just can't expect HA to be able to do autoupdates for these daemons. I'm not expecting or even wanting HA to do autoupdates. A good framing of the crux of the problem here is that I want to use HA but not HAOS. > even with MQTT you'll need to run ZWave and ZigBee bridges. Yes, the point is wanting to keep them as part of my overarching OS-level deployment config so that I can manage them along side email, nginx, matrix, netfilter, hostapd, kodi, etc. I only brought up NixOS specifically because you asked for an example of a different approach of encapsulating and abstracting service configuration. I'm happy using NixOS, regardless of what you consider a dealbreaker. I used to choose Debian instead. If you prefer HAOS then please continue using HAOS. If I had to create and hand off a machine to my \"grandmother\", I might even choose HAOS for that myself. We shouldn't need to argue about distributions when talking about software packages. reply cyberax 3 hours agorootparent> I'm not expecting or even wanting HA to do autoupdates. A good framing of the crux of the problem here is that I want to use HA but not HAOS. You can do that. It's not even hard, the HA documentation is pretty stellar in that regard: https://www.home-assistant.io/installation/#advanced-install... The HA team rightly doesn't want to officially support it, to avoid being inundated by people who don't want to keep the pieces. > Yes, the point is wanting to keep them as part of my overarching OS-level deployment config so that I can manage them along side email, nginx, matrix, netfilter, hostapd, kodi, etc. Then this is just not going to happen, unless the world changes a lot. There's just no way something like HA can be both useful for most people, and be released according to the Debian Stable calendar. HA has to move fast to adapt to third-party API changes, new integrations, and to just be able to bring features to users. > I only brought up NixOS specifically because you asked for an example of a different approach of encapsulating and abstracting service configuration. NixOS is not that much different from the HA approach. You also can't just get into the NixOS system and edit random files in its storage tree, you'll end up with a broken system. So you need to create a new flake, and then do the changes within this flake's env. If it's a deep dependency, you'll need to modify the dependent software to use your new patched version. Of course, nix is far more flexible than HAOS, but then they also are made for different kinds of users. reply baq 13 hours agorootparentprevIt's a Python app, of course being distributed as a docker image is the sanest way of doing it. I don't see why you couldn't just pip install it if you really wanted, but having been a Python developer for close to two decades, I wouldn't want to. reply Izkata 5 hours agorootparentThat was the standard way a long time ago, and the first startup would take a really long time because it would install even more stuff. And sometimes fail. It wasn't very reliable if you used any addons, and some required a ton of extra steps that it couldn't automate like the modern deployments do now. reply mindslight 13 hours agorootparentprevI'm talking about distribution package managers, not pip. reply darkwater 11 hours agorootparentI happily ran a dockerized HA on a Debian for years now, no need to do any complicated debugging (and even if I did, it would not be difficult to inspect it properly) reply mindslight 7 hours agorootparentDockerized HA on Debian is exactly what died on me. About 5-6 years ago. I'm sure it works just fine for most people. Just once bitten, twice shy. reply tw04 13 hours agorootparentprevNobody is preventing you from running Home Assistant core and deploying everything else yourself manually. Demanding the authors who gave you the software for free also provide support for an installation method they've offered up with no support is a bit ridiculous, don't you think? That attitude is what causes open source projects to die though... reply mindslight 13 hours agorootparentWhat do you mean \"demanding support\" ? I remember Home Assistant authors being actively hostile to people packaging their software outside of the official Docker or RPi images. Which is why it wasn't in the Debian repository, pushing me down that Docker path in the first place. Here's the same dynamic on an associated project in 2021: https://github.com/NixOS/nixpkgs/pull/126326 If anyone chimes in and says they've been running Home Assistant from nixpkgs (where I am now) for several years with no hiccups, then I will certainly reconsider my opinion. But based on my experience and what I've continued to read since, it feels like trying to do that is an uphill battle. One I'm not looking to take on, especially for automation I'm relying on. reply tw04 8 hours agorootparentSo... your example is a developer of HA stating that he sees major flaws with how they're distributing his package, and that he has absolutely no interest in supporting users that pull his code in a way that is unmaintainable by him. YOU believe he should support this anyway, because of various \"we promise end-users won't reach out to you\" which is comically incorrect because history has shown repeatedly that a user's first step when something is broken is to google package_name broken - which will absolutely turn up the author's name. BECAUSE he doesn't want to support his software being repackaged in a way he believes isn't supportable, you're upset. You want him to support your unicorn config because that's what you want to do, and his refusal to comply makes him a bad person. Thank you for reinforcing EXACTLY why open source devs burn out. He has a workflow that he is willing and able to support and doesn't want to support anything outside of that. Your response is: but you need to do it for me because it's what I want. reply mindslight 7 hours agorootparentDid we read the same thread? Nobody asked the HA developer to support anything, rather that developer started the conversation by making demands and then kept at it. reply tw04 6 hours agorootparent“Making demands” which were: please don’t package my code in your distro that has dozens of out of date packages my code depends on that will break. Because I don’t want to deal with end users bugging me about it being broken. I think the most surprising thing is that you can’t see how unreasonable your complaints are. reply mindslight 5 hours agorootparentIf you attempted explaining how you think my stated position is unreasonable, perhaps I could see it. So far you've only attacked strawmen, such as claiming that I am demanding support from HA or claiming upstream was being asked to support nixpkgs. What I do see is a project calling itself FOSS, while its maintainers really don't like it being used as Free Software. If one wants to control downstream uses of one's software, the answer is quite simple - release it under a proprietary license. Don't grant freedom while going on and on about how you support freedom, but then be upset when someone actually uses that freedom to do something. > deal with end users bugging me about it being broken. The nixpkgs maintainers asked how much this was actually happening, and even preemptively proposed solutions. OP didn't engage and just repeated his demands. And in general how is this any different from the common DRM-authoritarian refrain that companies are justified locking down devices they make, lest end users modify them and then clueless people might attribute the outcome to the original manufacturer? reply aksss 12 hours agoprevSurprised there’s not more mention of Shelly monitors in these comments. They’re great for whole house (service entrance) and circuit power monitoring. Pretty open integration, OOTB integration with HA. I think it makes more sense to use “dumb” OTS circuit breakers in your house and augment with add-on monitor than combining the capabilities into a tightly-coupled single device. reply 33282334 3 hours agoprev33282334 reply ars 15 hours agoprevIf someone else wants to try this, I strongly recommend against using WiFi for the plugs, instead use Z-Wave or Zigbee. Wifi is just not meant for this use case, it will be unhappy if you start adding a lot of devices, and it will slow down your main use of WiFi for your phone. reply seszett 14 hours agoparentNot the first time I hear this but I have about 20 ESP devices on my WiFi, almost all of them pushing data regularly and polling for instructions (I don't use HA, but a simpler home made solution) and I have no problem at all. reply meatmanek 11 hours agorootparentIt helps that the IoT things almost invariably use 2.4GHz while your data-hungry computers and phones usually use 5GHz. reply WesolyKubeczek 14 hours agoprevYou need to be careful with what plugs you choose, though, because they each have, let's say, their own peculiarities. For instance, their overvoltage protection might not align well with what the local regulations say. For example, in my region of EU, the upper voltage tolerances are such that 264V must trigger an instant poweroff, and also anything producing power must shut off if the average voltage over the last 10 minutes was 253V or more. However, TuYa sockets which pretty much are the only in-wall variety I was able to find on the local market, shut off at 260V. This tends to be somewhat problematic in an area saturated with PV installations, like the one I'm living in. This problem is compounded by the fact that the reported measurements of sockets sitting on the same phase tend to differ quite a bit. Some sockets tend to overstate the voltage compared to neighboring sockets sitting on the same wires. Thus, they shut off when they think it's 260V, while it might just as well be 255V. Just saying that if you put lots of those in your walls, you might suddenly find yourself in a need to prepare some automations to try and bring the sockets on once the voltage is back to normal. This particular variety of sockets won't come back on after the voltage drops. reply baq 13 hours agoparentShelly relays can be configured to do all these things and the voltage safety threshold itself is also configurable (at least in the Plus devices) but they aren't zigbee. Otherwise great little devices. reply darkwater 11 hours agorootparentI cannot really understand why Shelly doesn't offer ZigBee (and now Matter) options, only Wifi. This has always been the biggest blocker for me. reply nagisa 2 hours agorootparentThey have z-wave options, though. reply ryall 14 hours agoprevIt's annoying that smart plugs/bulbs etc use wifi when Powerline exists reply whitehexagon 14 hours agoparentI have a box of old X10 devices here, one of the most reliable home automation systems I ever set up. I only switched to WiFi when my iobridge X10 controller failed and I couldnt work out the RS232 protocol for a different controller. reply Dalewyn 13 hours agoparentprevPowerline ethernet dumps an utterly horrifying amount of electrical noise into both the wiring and the surrounding area. Please don't use powerline unless you have no other solution. Note: Powerline ethernet should not be confused with Power Over Ethernet which is perfectly fine. reply baq 13 hours agorootparentI'm using it on two outlets after not having enough prescience to install ethernet in my wardrobe-turned-office - got any reading materials? reply Dalewyn 9 hours agorootparentDon't have any to link off hand, but the basic gist is this: Electrical wiring of any sort are all antennas to varying degrees, transmitting and receiving electromagnetic signals. Most mains electrical wiring is also unshielded, meaning they readily transmit and receive electromagnetic signals. Powerline ethernet basically puts ethernet data on mains electrical wiring by utilizing the bands that aren't used for carrying power. This data is very, very noisy in electrical noise terms, and because most wiring also acts as an antenna that noise also gets broadcast everywhere. Simple electronics like your coffee machine or microwave oven won't care, but more sensitive electronics like radios can in turn receive interference from both the power line and the noise broadcast into the air. reply nagisa 2 hours agorootparentThe issue, I think, is that you're extrapolating from the usecase where high bandwidth ethernet is carried over the power lines. Protocols' like X10 bandwidth use and needs are so low (dozens of bits per second) that the interference is indistinguishable from the regular power fluctuations. reply Dalewyn 1 hour agorootparentI speak from experience. I tried powerline ethernet because wifi signals traverse the house walls very poorly, to say the least. The sheer noise the powerline adapters generated into the line, regardless traffic, was unacceptable. reply killme2008 1 hour agoprev [–] It's interesting and cool! Looks like the software stack is still running on the local server. Looking for Prometheus and Grafana on the cloud? Check out GreptimeCloud! https://greptime.com/ reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author shares their journey of monitoring energy usage using smart plugs, Prometheus, and Grafana, discussing setup, power consumption observations (e.g., water heater, home server, workstation), and charging patterns.",
      "They also highlight stability issues with the plugs and propose future plans to automate device control according to electricity prices, expressing satisfaction with the setup for monitoring and enhancing the power efficiency of their devices."
    ],
    "commentSummary": [
      "The post covers monitoring energy usage with smart plugs, Prometheus, and Grafana, along with tracking power consumption using Zigbee power breakers and e-ink displays.",
      "It addresses concerns about Chinese device reliability, recommends safer options, and discusses integrating meters into automation hubs while highlighting issues with WiFi, advocating for Z-Wave, and various IoT monitoring and control methods.",
      "Discussions extend to tools like Home Assistant, MQTT, Docker, and NixOS, emphasizing NixOS as a comprehensive solution for managing daemons and using distribution package managers for Home Assistant."
    ],
    "points": 253,
    "commentCount": 138,
    "retryCount": 0,
    "time": 1714932331
  },
  {
    "id": 40264352,
    "title": "Exploring Machine Unlearning in 2024: Challenges and Applications",
    "originLink": "https://ai.stanford.edu/~kzliu/blog/unlearning",
    "originBody": "Machine Unlearning in 2024 41 minute read Written by Ken Liu ∙ May 2024 ▸ Table of Contents As our ML models today become larger and their (pre-)training sets grow to inscrutable sizes, people are increasingly interested in the concept of machine unlearning to edit away undesired things like private data, stale knowledge, copyrighted materials, toxic/unsafe content, dangerous capabilities, and misinformation, without retraining models from scratch. Machine unlearning can be broadly described as removing the influences of training data from a trained model. At its core, unlearning on a target model seeks to produce an unlearned model that is equivalent to—or at least “behaves like”—a retrained model that is trained on the same data of target model, minus the information to be unlearned. There’s a lot hidden in the above description. How do we describe the information to be unlearned? Do we always have ground-truth retrained models? If not, how do we actually evaluate the unlearning? Can we even verify and audit the unlearning? Is pretending to unlearn, as humans often do, sufficient? Is unlearning even the right solution? If so, for what problems? The precise definitions of unlearning, the techniques, the guarantees, and the metrics/evaluations would depend on: The ML task (e.g., binary classification or language modeling); The data to unlearn (e.g., a set of images, news articles, or the knowledge of making napalm); The unlearning algorithm (e.g., heuristic fine-tuning vs deleting model components); The goal of unlearning (e.g., for user privacy or harmfulness removal). In this educational post, I hope to give a gentle, general ML audience introduction to machine unlearning and touch on things like copyright protection, New York Times v. OpenAI, right-to-be-forgotten, NeurIPS machine unlearning challenge, retrieval-based AI systems, AI safety, along with some of my thoughts on the field. While unlearning is broad topic applicable to most ML models, we will focus a lot on foundation models. 1. A bit of history & motivations for unlearning People have thought about the unlearning problem for a while now. The initial research explorations were primarily driven by Article 17 of GDPR (European Union’s privacy regulation), often referred to as “right-to-be-forgotten” (RTBF) since 2014. RTBF basically says a user has the right to request deletion of their data from a service provider (e.g. deleting your Gmail account). RTBF was well-intentioned. It was also very actionable when said service providers store user data in a structured way, like how Google removed a bunch of links from its index in repsonse to RTBF requests. However, RTBF wasn’t really proposed with machine learning in mind. In 2014, policymakers wouldn’t have predicted that deep learning will be a giant hodgepodge of data & compute, and that separating and interpreting this hodgepodge turned out to be hard. The hardness of erasing data from ML models has subsequently motivated research on what is later referred to as “data deletion” and “machine unlearning”. A decade later in 2024, user privacy is no longer the only motivation for unlearning. We’ve gone from training small convolutional nets on face images to training giant language models on pay-walled, copyrighted, toxic, dangerous, and otherwise harmful content, all of which we may want to “erase” from the ML models—sometimes with access to only a handful of examples. The nature of the models has changed too. Instead of using many small specialized models each good at one task, people started using a single giant model that knows just about any task. Currently, I think the motivations for unlearning fall into two categories: Access revocation (think unlearning private and copyrighted data). In an ideal world, data should be thought of as “borrowed” (possibly unpermittedly) and thus can be “returned”, and unlearning should enable such revocation. Unlearning is challenging from this perspective. One key difficulty is that our limited understanding of deep learning itself makes data trained into a model akin to “consumables” (which can’t just be “returned” after consumption). Data may also be non-fungible (e.g. your chat history) and may even be thought of as labor with its own financial and control interests. Another challenge is that access revocation may require a proof of unlearning; as we will explore in the coming sections, this isn’t always possible. These difficulties suggest that it’s perhaps also worth revising laws like RTBF and thinking about alternatives such as data markets, where data owners are properly compensated so they won’t want to request unlearning in the first place. To illustrate, suppose Bob ate Alice’s cheesecake (data), Alice would much rather Bob pay her or return something equivalent (compensation) than Bob puking to his pre-eating state (unlearning). In practice, one way to implement access revocation is via some form of periodic re-training of the base model. Many model providers already do this to keep their models competitive and up-to-date. For example, OpenAI can collect a bunch of unlearning requests, and batch-satisfy them during the re-training every year (or, guided by RTBF’s “undue delay” period by which the request must be satisfied). More broadly, this suggests socio-technical solutions for unlearning: policymakers can mandate such periodic re-training and set economically viable deadlines to offload the costs to the model owners. Model correction & editing (think toxicity, bias, stale/dangerous knowledge removal). That is, the model was trained on something undesirable and we’d like to fix it. This is closely related to the model editing literature. The concept of “corrective machine unlearning”, where unlearning serves to correct the impact of bad data, was recently proposed to capture this motivation. From this perspective, unlearning may also be viewed as a post-training risk mitigation mechanism for AI safety concerns (discussed further in Section 4). Unlike access revocation, we could be more lenient towards with model correction since the edit is more of a desire than a necessity mandated by law, much like model accuracy on image classification or toxicity of generated text. (Of course, these can cause real harm too.) Here, we won’t necessarily need formal guarantees for the unlearning to be practically useful; we have plenty of examples where people would happily deploy models that are deemed “sufficiently safe”. The recent WMDP benchmark, which quizzes a model on hazardous knowledge, is a good example of empirically evaluating unlearning efficacy. 2. Forms of unlearning Unlearning is trivially satisfied if we can just retrain the model without the undesired data. However, we want something better because (1) retraining can be expensive and (2) it can be a lot of work just to find out what to remove from training data—think finding all Harry Potter references in a trillion tokens. Unlearning techniques essentially seek to mitigate or avoid this retraining cost while producing identical or similar results. The unlearning literature can roughly be categorized into the following: Exact unlearning “Unlearning” via differential privacy Empirical unlearning, where data to be unlearned are precisely known (training examples) Empirical unlearning, where data to be unlearned are underspecified (think “knowledge”) Just ask for unlearning? Forms 2-4 are sometimes known as “approximate unlearning” in that the unlearned model approximates the behavior of the retrained model. Form 5 is quite new and interesting, and more specific to instruction-following models. Figure 1. Illustration of approximate unlearning. Source: NeurIPS Machine Unlearning Challenge. In the following, we will go through what each of these types roughly looks like, along with what I think are the promises, caveats, and questions to ask looking forward. 2.1. Exact unlearning Exact unlearning roughly asks that the unlearned model and the retrained model to be distributionally identical; that is, they can be exactly the same under fixed randomness. Techniques for exact unlearning are characterized by the early work of Cao & Yang and SISA. In SISA, a very simple scheme, the training set is split into N non-overlapping subsets, and a separate model is trained for each subset. Unlearning involves retraining the model corresponding to and without the data points to be unlearned. This reduces cost from vanilla retraining by 1/N (cheaper if we keep model checkpoints). Inference then involves model ensembling.1 Figure 2. Illustration of SISA: just train models on data shards (image source). More generally, the essence of exact unlearning of this form is that we want modular components in the learning algorithm to correspond to different (potentially disjoint) sets of the training examples. There are several benefits of exact unlearning: The algorithm is the proof. If we implement something like SISA, we know by design that the unlearned data never contributed to other components. As it turns out, formally proving the model has unlearned something is quite challenging otherwise. It turns the unlearning problem into an accuracy/efficiency problem. This makes exact unlearning more approachable due to the messiness of unlearning evaluation and lack of benchmarks. Interpretability by design. By providing a structure to learning, we also have better understanding of how certain data points contribute to performance. The main drawback seems obvious: modern scaling law of large models argues against excessive data & model sharding as done in SISA. Or does it? I think it would be very interesting to revisit sharding in the context of large models, in light of the recent model merging literature that suggests the feasibility of weight-space merging between large models. As we’ll learn in the coming sections, the messiness of approximate unlearning and its evaluation, especially in the context of large models, makes exact unlearning very appealing. 2.2. “Unlearning” via differential privacy This line of work roughly says: if the model behaves more or less the same with or without any particular data point, then there’s nothing we need to unlearn from that data point. More broadly, we are asking for distributional closeness between the unlearned and the retrained models. For readers unfamilar with differential privacy (DP) in machine learning, DP defines a quantifiable indistinguishability guarantee between two models M, M′ trained on datasets X, X′ that differ in any single training example. The canonical procedure, DP-SGD, works by clipping the L2-norm of the per-example gradients and injecting some per-coordinate Gaussian noise to the gradients. The idea is that the noise would mask or obscure the contribution of any single gradient (example), such that the final model isn’t sensitive any exmaple. It is usually denoted by (ε,δ)-DP; the stronger the noise, the smaller the scalars (ε,δ), the more private. The intuition is that if an adversary cannot (reliably) tell apart the models, then it is as if this data point has never been learned—thus no need to unlearn. DP can be used to achieve this form of unlearning, but due to the one-sidedness of unlearning (where we only care about data removal, not addition), DP is a strictly stronger definition. This notion of unlearning is sometimes known as “(α,β)-unlearning” where (α,β) serve similar roles as (ε,δ) to measure distributional closeness. Example techniques along this direction include: (1) storing checkpoints of (DP) convex models and unlearning is retraining from those checkpoints; and (2) on top of the previous technique, add SISA for adaptive unlearning requests (i.e. those that come in after observing the published model). DP-based unlearning is good in that it gives some form of a statistical guarantee. However, there are some important considerations that limit its applicability to large models: Many such unlearning results apply only to convex models or losses. What levels of unlearning (values of (ε,δ)-DP or (α,β)-unlearning) are sufficient? Who decides? For large models, current ML systems don’t fit well with the per-example workloads of DP-like procedures. The memory overhead will also be prohibitive. Moreoever, like DP, the guarantees can fall off quickly with more unlearning requests (at best the rate of O( √ k ) with k requests following DP composition theorems). DP-like definitions implicitly assume we care about all data points equally. But some examples are more likely to receive unlearning request, and some examples would not have contributed to the learning at all. DP-like procedures may also just hurt model accuracy a lot, sometimes in an unfair way. For large models in particular, it’s also worth distinguishing the cases of unlearning pre-training data vs unlearning fine-tuning data. The latter is a lot more tractable; for example, we could indeed fine-tune large models with differential privacy but not so much with pre-training. 2.2.1. Forging and its implications on DP-like unlearning definitions An unlearning procedure may sometimes require an external audit, meaning that we’d like to prove that the unlearning procedure has actually happened. The main idea of “forging” is that there exists two distinct datasets that, when trained on, would produce the same gradients and (thus) the same models. This is true intuitively: Think linear regression of points on a perfect line; removing any 1 point doesn’t change the fitted line; Think mini-batch GD, where replacing one example gradient with the sum of several “fake” gradients would give the same batch gradient. Forging implies that DP-based approximate unlearning may not be auditable—that is, the unlearning service provider cannot formally prove that the forget set is really forgotten. In fact, if we only look at the model weights, even exact unlearning may not be auditable. While one can brush this off as a theoretical result, it does mean that policymakers should think carefully about how a future version of “right-to-be-forgotten” (if any) should look like and whether similar policies are legally and technically enforceable. Indeed, what qualifies as an “audit” could very well be definition and application dependent. If the auditor only cares that the unlearned model performs poorly on a specified set of inputs (say on a set of face images), then even empirical unlearning is “auditable” (see next section). 2.3. Empirical unlearning with known example space (“example unlearning”) This line of work is essentially “training to unlearn” or “unlearning via fine-tuning”: just take a few more heuristically chosen gradient steps to shape the original model’s behavior into what we think the retrained model would do (while also optionally resetting some parameters in the model). It may also be referred to as “example unlearning”, since the training, retain, and forget sets are often clearly defined. The NeurIPS 2023 Machine Unlearning Challenge collected many methods along this direction. The challenge roughly runs as follows: You are given a face image dataset with designated retain/forget example splits for the training set, a target model trained on everything, and a secret model trained only on the retain set. You are asked to design an unlearning algorithm that produces unlearned model(s) from the target model that “match” the secretly kept model. The “match” or evaluation metric uses a DP-like output-space similarity over 512 seeds: for each forget example, compute an “empirical ε” over 512 unlearned models based on true/false positive rates of an adversary (also provided by the organizer), and aggregate across examples. All models are a small ConvNet. To give an intuition about how well empirical unlearning is doing without fully explaining the metric: the ground-truth retrained model gets about ~0.19, the winning submission gets to ~0.12, and the baseline (simple gradient ascent on forget set) is ~0.06.2 So what do the winning ideas look like? Something along the lines of the following: Gradient ascent on the forget set; Gradient descent on the retain set (and hope that catastrophic forgetting takes care of unlearning); Gradient descent on the forget set, but with uniformly random labels (to “confuse” the model); Minimize KL divergence on outputs between unlearned model and original model on the retain set (to regularize unlearned model performance on unrelated data); Re-initialize weights that had similar gradients on the retain set and forget sets, and finetune these weights on the retain set; Prune 99% of weights by L1-norm and fine-tune on the retain set; Reset first/last k layers and fine-tune on the retain set; and Heuristic/arbitrary combinations of the above. Indeed, despite the heuristic nature of these approaches, these are what most empirical unlearning algorithms, especially those on large (language) models, are doing these days. People explore empirical approaches because theoretical tools are usually impractical; for example, enforcing DP simply hurts accuracy and efficiency too much, even for the GPU rich. On the flip side, empirical methods are often fast and easy to implement, and their effects are often qualitatively visible. Another key motivation for empirical unlearning is that counterfactuals are unclear, especially on LLMs. In deep learning, we often don’t know how the retrained model would behave on unseen data. What should the LLM think who Biden is, if not a politician? Should image classifiers give uniformly random predictions for unlearned images? Do they generalize? Or are they confidently wrong? Any of these is possible and it can be up to the practitioner to decide. It also means that behaviors that are equally plausible can lead to wildly different measurements (e.g., KL divergence between output distributions of unlearned & retrained model), complicating theoretical guarantees. 2.4. Empirical unlearning with unknown example space (“concept/knowledge unlearning”) What if the train, retain, or forget sets are poorly specified or just not specified at all? Foundation models that train on internet-scale data may get requests to unlearn a “concept”, a “fact”, or a piece of “knowledge”, all of which we cannot easily associate a set of examples. The terms “model editing”, “concept editing”, “model surgery”, and “knowledge unlearning” are closely related to this notion of unlearning.3 The underspecification of the unlearning requests means that we now have to deal with the notions of “unlearning scope” (or “editing scope”) and “entailment”. That is, unlearning requests may provide canonical examples to indicate what to unlearn, but the same information can manifest in the (pre-)training set in many different forms with many different downstream implications such that simply achieving unlearning on these examples—even exactly—would not suffice. For example: The association “Biden is the US president” is dispersed throughout various forms of text from news articles, books, casual text messages, or this very blog post. Can we ever unlearn all occurrences? Moreover, does unlearning Joe Biden also entail unlearning the color of Biden’s cat? Artists may request to unlearn art style by providing art samples, but they won’t be able to collect everything they have on the internet and their adaptations. New York Times may request to unlearn news articles, but they cannot enumerate quotes and secondary transformations of these articles. Such vagueness also suggests that unlearning pre-training data from large models are perhaps necessarily empirical: it is unlikely to derive formal guarantees if we can’t clearly specify what to (and what not to) unlearn in the trillions of tokens and establish clear information boundaries between different entities. An interesting implication of achieving unlearning empirically is that the unlearning itself can be unlearned. What does existing work do, then, with underspecified unlearning requests? Most techniques are more or less the same as before, except now we also need to find the examples to fine-tune on. For example, attempting to unlearn Harry Potter involves asking GPT-4 to come up with plausible alternative text completions (e.g. that Mr. Potter studies baking instead of magic); and attempting to unlearn harmful behavior involves collecting examples of hatespeech. Another set of techniques involves training the desired behavior (or its opposite) into task/control vectors and harnessing the capability of large models to undergo weight-space merging or activation steering. The fundamental approach of the above is more or less the same, nevertheless—obtaining these edit vectors involves (heuristically) designing what gradients to take and what data on which to take them. One could also frame the unlearning problem as an alignment problem and applies the forget examples with a DPO-like objective. 2.5. Just ask for unlearning? It turns out that powerful, instruction-following LLMs like GPT-4 are smart enough to pretend to unlearn. This means crafting prompts to induce a (sufficiently) safe behavior for the target unlearning application. This is an interesting approach because no gradients are involved whatsoever (big plus from a systems perspective), and intuitively the end results could very well be as good as existing empirical unlearning techniques. Among different ways we could prompt, past work explored the following two directions. Literally asking to pretend unlearning. We can ask in the system prompt to, say, pretend to not know who Harry Potter is. By design, this works best for common entities, facts, knowledge, or behaviors (e.g. the ability to utter like Trump) that are well-captured in the pre-training set, since the LLM needs to know it well to pretend not knowing it well. On the other hand, suppose now we’d like to unlearn the address of an obscure person; the pre-training set is so large that we suspect it’s part of training data. We now face a variant of the Streisand effect: is it even worth asking the model to pretend unlearning by accurately describing it in-context, and subsequently risk leaking it in subsequent model responses? Few-shot prompting or “in-context unlearning”. Suppose we now have a clearly defined set of forget examples with corresponding labels. We can flip their labels and put them in the prompt, along with more retain examples with correct labels, with the intuition that the model would treat these falsely labelled forget examples as truths and act accordingly—much like one could jailbreak a model this way.4 Indeed, this works best when the forget examples and the counterfactual labels are clearly defined and (somewhat) finite. It may work for factual associations (e.g. Paris is the captial of France) by enumerating a lot of examples, but unlikely to work for unlearning toxic behaviors (where space of possible outputs is much larger). In a sense, these approaches are complementary as they work for different kinds of unlearning requests. More broadly, one could imagine a boxed LLM system for unlearning through prompting, where: Only the input and output interfaces are exposed (like ChatGPT); Different instances of a powerful LLM are responsible for accurately mimicking different parts of a desired unlearning behavior (for example, one LLM instance specializes in general trivia-style QA while anoother handles sequence completions); An orchestrator/router LLM decides which unlearning worker instance to call depending on the input; and A composer/summarizer LLM that drafts the final output conforming to the desired unlearning behavior; it may also apply some output filtering. Some readers may grumble about the heuristic nature of such prompting-based techniques; that there is no proof of unlearning whatsoever. We should keep in mind that fine-tuning based empirical unlearning, as most recent approaches do, is perhaps not fundamentally different. I think it ultimately comes down to the following questions: Which of fine-tuning or prompting can better steer model behavior? Which of them are less susceptible to attacks (exposing less surfaces and/or requiring more effort for an adversary to revert the unlearning)? My intuition of our current models says that both questions point to fine-tuning based unlearning, but this is very much up for debate and can change as we get more powerful models and better defense mechanisms. For example, the recent notion of an instruction hierarchy may help make such as an LLM system less susceptible to malicious prompts. It might be useful to note that humans don’t really “unlearn” a piece of knowledge either.5 In fact, by claiming to have unlearned something, we often have: (1) not only learned it well to be able to make the very claim that we have unlearned it, and (2) consciously decided that it’s no longer useful / beneficial to apply this knowledge to our current world state. Who is to say that unlearning for LLMs should be any different? 3. Evaluating unlearning Unlearning is messy for many reasons. But one of the biggest broken things about unlearning is evaluation. In general, we care about three aspects: Efficiency: how fast is the algorithm compared to re-training? Model utility: do we harm performance on the retain data or orthogonal tasks? Forgetting quality: how much of the “forget data” is actually unlearned? How fast can we recover (re-learn) them? Evaluating efficiency and model utility are easier; we already measure them during training. The key challenge is in understanding the forgetting quality.6 If the forget examples are specified, this feels easy too. For example, unlearning a particular image class intuitively means getting a near-chance accuracy on the images in that class. An evaluation protocol may measure accuracy (high on retain & test set, low on forget set) or the likelihood of the forget text sequences (lower the better). However, these intuitive choices of metrics aren’t necessarily principled or extensible to settings like knowledge unlearning in LLMs. Expecting the model to perform poorly on an unlearned image ignores generalization, as the forget examples could very well be an interpolation/duplicate of certain retain examples. And we don’t always have oracle models that have never seen the forget examples; e.g., do we have LLMs that have never seen New York Times articles? Evaluating unlearning on LLMs had been more of an art than science. For example, to unlearn “Harry Potter” as an entity, people would visualize how the token probabilities would decay for Harry Potter related text—and some other folks would come along and show that the model can indeed still answer Harry Potter trivia questions. The key issue has been the desperate lack of datasets and benchmarks for unlearning evaluation. Since 2024, nevertheless, the benchmarking crisis is getting better. There are two recent projects worth highlighting: TOFU: A benchmark focusing on unlearning individuals (specifically book authors). It involves asking GPT-4 to create fake author profiles, fine-tuning an LLM on them, and using the fine-tune as the unlearning target model and the original LLM as the oracle “retrained” model. It provides QA pairs on the generated fake authors to evaluate a model’s knowledge of these authors before/after applying unlearning. WMDP: A benchmark focusing on unlearning dangerous knowledge, specifically on biosecurity, cybersecurity, and chemical security. It provides 4000+ multiple-choice questions to test a model’s hazardous knowledge before/after applying unlearning. As part of the report the authors also propose an activation steering based empirical unlearning method. TOFU and WMDP depart from previous unlearning evaluation in that they are both “high-level” and focus on the model’s knowledge retention and understanding as opposed to example-level metrics like forget sequence perplexity. This is particularly relevant for LLMs as they are generally capabale of giving the same answer in many different ways that example-level metrics can’t capture. Looking forward, I think application-oriented unlearning benchmarks like TOFU and WMDP, as opposed to instance-based evaluation like that of the NeurIPS unlearning challenge, are more useful for evaluating foundation models, owing to the multi-tasking nature of these models and the disparate definitions of “unlearning success” for each of these tasks. Indeed, one might imagine separate benchmarks on unlearning personally identifiable information (PII), copyrighted content, speech toxicity, or even model backdoors. For example, for unlearning PII, we might care about exact token regurgitation, whereas for toxicity, the unlearning metric would be the score reported by a ToxiGen classifier. 4. Practice, pitfalls, and prospects of unlearning Unlearning is a hard problem, especially in the context of foundation models. As we actively research to make unlearning work in practice, it helps to philosophize a bit on what unlearning really means and whether it is the right solution for our current problems. 4.1. The spectrum of unlearning hardness Intuitively, unlearning infrequent textual occurrences in LLMs like car accidents in Palo Alto should be easier than unlearning frequent occurrences like “Biden is the US president”, which is in turn easier than unlearning fundamental facts like “the sun rises every day”. This spectrum of unlearning hardness emerges because as a piece of knowledge becomes more fundamental, it will have more associations with other pieces of knowledge (e.g. as premises or corollaries) and an exponentially larger unlearning scope. In fact, a piece of knowledge can be so embedded in the model’s implicit knowledge graph that it cannot be unlearned without introducing contraditions and harming the model’s utility.7 This intuition implies that certain unlearning requests are much harder or simply unsatisfiable (any attempts are bound to have flaws). Indeed, humans have experiences that form the basis of their subsequent actions and world models; it is subjective, blurry, and philosophical as to what capacity can humans unlearn their formative past memories. More broadly, the unlearning hardness problem applies to all kinds of models, and for reasons beyond embeddedness in a knowledge/entailment graph. Let’s consider two more seemingly contradictory intuitions for unlearning hardness: An example seen later in the training should be easy to unlearn, since the model would have moved only slightly in weight space (e.g. due to decayed learning rate) and one could either just revert gradients or revert to a previous checkpoint (if stored). In contrast, examples seen early gets “built on” by later examples (in the curriculum learning sense), making them harder to unlearn. An example seen later should be harder to unlearn, since examples seen earlier are gradually (or catastrophically) forgotten over the course of training; this may be especially true for LLMs. Failure to reconcile these intuition would suggest that the interplay across memorization/forgetting, example importance (in the sense of data selection and coresets), learning hardness (in the sense of prediction flips), and unlearning hardness is unclear. Here are some interesting research questions: Is there a qualitative/fundamental difference between unlearning “easy” data (e.g. a local news event) and “hard” data (e.g. cats have four legs)? If there is a spectrum of unlearning hardness, does there exist a threshold to tell apart what is “easy” and “hard”, and thus what is unlearnable or shouldn’t be unlearned? Does there exist, or can we train, such an oracle classifier? Can humans even tell? How does this relate to influence functions and data attribution? If a certain piece of knowledge (as it manifests in a model’s output) can be attributed to a larger fraction of the training data, does it make it harder to unlearn? Can we benchmark how easy is it to unlearn something? 4.2. Copyright protection On the surface, unlearning seems to be a promising solution for copyright protection: if a model violates the copyright of some content, we could attempt to unlearn said content.8 It is conceivable that to resolve copyright violations via unlearning, provable and exact unlearning is necessary (and possibly sufficient); on the other hand, approximate unlearning, without guarantees and with the possibility of being hacked, is certainly insufficient and likely unnecessary. In practice, however, there is a lot more nuance due to the questionable effectiveness of current unlearning methods and the unclear legal landscape at the intersection of AI and copyright. Since I am no legal expert (and clearly none of this section constitutes legal advice), we will mostly focus on asking questions. The central question seems to be: is unlearning the right solution for copyright protection? Recall that the fair use doctrine9 permits limited use of copyrighted material contigent on four factors: (1) purpose and character of the use (“transformativeness”), (2) the nature of the copyrighted work, (3) amount and substantiality of the use, and (4) the effect on material’s value. If the use of copyrighted content in a model qualifies as fair use, then unlearning such content from the model is unnecessary. Suppose a model is trained on some copyrighted content and is risking copyright violation, as in New York Times v. OpenAI. Should OpenAI invest in (empirical) unlearning algorithms on ChatGPT? Or should they focus on the transformativeness axis of fair use and invest in deploying empirical guardrails, such as prompting, content moderation, and custom alignment to prevent the model from regurgitating training data? The latter seems to be what’s being implemented in practice. More broadly, there could also be economic solutions to copyright violation as alternatives to unlearning. For example, model owners may provide an exact unlearning service (e.g. via periodic retraining) while also offering to indemnify model users for copyright infringement in the mean time, as seen in the case of OpenAI’s “Copyright Shield”. People are also starting to explore how one may price copyrighted data using Shapley values. In general, it is unclear right now how much of a role (if any) unlearning will play for resolving copyright related issues. Exact unlearning (extending to retrieval-based systems, see next section) does hold promises since deletion is clean and provable, but it seems that legally binding auditing procedures/mechanisms need to be first in place. 4.3. Retrieval-based AI systems An obvious alternative to unlearning is to not learn at all. One way this could manifest for an LLM is that we take all content from the pre-training set that may receive unlearning requests (e.g., New York Times articles) and put them to an external data/vector store. Any questions relating to them will then be RAG’ed during inference, and any unlearning requests can be trivially satisfied by removing the data from the database. Min et al. demonstrates that this approach can be competitive to (though not quite matching) the trained baseline in terms of final perplexity. Retrieval-based solutions are promising because of the increasing capabilities of the base models to reason in-context. However, there are few considerations before taking retrieval systems as the no-brainer solution to unlearning: Removing protected content from pre-training corpus can be a hard de-duplication problem. Much like removing data contamination is hard, how can we be sure that paraphrases, quotations/citations, or other adaptations of the protected content are removed? What if the data to be unlearned can’t be retrieved? Today we fine-tune many things into a model that aren’t documents or knowledge items; for example, it is unclear (yet) if things like as human preferences and desired behaviors (e.g. ability to write concisely) can be “retrieved” from a database. Dumping stuff in-context can open new attack surfaces. Many RAG methods for LLMs work by putting related content in-context and ask the model to reason on them. Having the protected data in-context means they are now more susceptible to data extraction (simple prompting attacks may work just fine). Utility gap between retrieval and training. While there is evidence that retrieval-based solutions can be competitive, there is no general consensus that retrieval alone can replace fine-tune workloads; indeed, they can be complementary. More broadly, what if the space of unlearnable data is too large such that if all of it goes to an external store, the base model wouldn’t be as useful? 4.4. AI safety As models become more capable and are granted agency, one concrete application domain for unlearning that is gaining traction is AI safety. Roughly speaking, safety concerns stem from a model’s knowledge (e.g., recipe of napalm), behaviors (e.g., exhibiting bias), and capabilities (e.g., hacking websites). Examining current AI systems and extrapolating forward, one may imagine the following examples to apply unlearning and improve AI safety: removing hazardous knowledge, as seen in the WMDP benchmark; removing model poisons and backdoors, where models respond to adversarially planted input triggers; removing manipulative behaviors, such as the ability to perform unethical persuasions or deception; removing bias and toxicity; or even removing power-seeking tendencies. For safety-oriented applications, it is worth noting that unlearning should be treated as a post-training risk mitigation and defense mechanism, alongside existing tools like alignment fine-tuning and content filters. And as with any tool, we should view unlearning through its trade-offs in comparison to other tools in the toolbox (e.g., unlearning is more adaptive but more expensive than content filters), as opposed to brushing it off because of the potential lack of guarantees and efficacy. Acknowledgements: The author would like to thank Aryaman Arora, Jiaao Chen, Irena Gao, John Hewitt, Shengyuan Hu, Peter Kairouz, Sanmi Koyejo, Xiang Lisa Li, Percy Liang, Eric Mitchell, Rylan Schaeffer, Yijia Shao, Chenglei Si, Pratiksha Thaker, Xindi Wu for helpful discussions and feedback before and during the drafting of this post. Any hot/bad takes are those of the author. Citation If you find this post helpful, it can be cited as: Liu, Ken Ziyu. (Apr 2024). Machine Unlearning in 2024. Ken Ziyu Liu - Stanford Computer Science. https://ai.stanford.edu/~kzliu/blog/unlearning. Or @misc{liu2024unlearning, title = {Machine unlearning in 2024}, author = {Liu, Ken Ziyu} journal = {Ken Ziyu Liu - Stanford Computer Science}, year = {2024}, month = {Apr}, url = {https://ai.stanford.edu/~kzliu/blog/unlearning}, } Technically, SISA may not give exact unlearning in the sense of identical model distributions between the retrained model and the unlearned model, since after a sequence of unlearning requests, the data shards may end up in a state that we wouldn’t otherwise get into in the first place (e.g., some shards have way more data than others after unlearning). For practical purposes, nevertheless, this is subtle enough that the nice properties about exact unlearning, as discussed later in the section, would still hold. ↩ It is also worth noting that the unlearning metric used in the NeurIPS unlearning challenge was disputed: why should we stick to a DP-like distributional closeness metric to a single secretly-kept re-trained model, when retraining itself can give a different model due to randomness? ↩ More broadly, “unlearning” falls under the umbrella of “model editing” in the sense that a deletion is also an edit. Similarly, one could argue that the concept of “continual learning” falls under the umbrella too, where an association (say an input/label pair, or a piece of factual association) is updated by deleting of an old association and creating a new, clearly specified association. One could imagine using continual learning to help achieve unlearning and vice versa. ↩ There is also evidence that in-context demonstrations mostly serve to elicit a particular behavior and that the labels don’t even matter that much. It’s unclear yet how we could reconcile this finding with “in-context unlearning”. ↩ Humans do forget things though, which is different. The ML analogy might be “catastrophic forgetting”; humans similarly forget things under information overload. ↩ In particular, recall that for exact unlearning, understanding forgetting quality isn’t strictly necessary because the algorithm would remove the forget data from the picture by construction (through retraining). Thus it may be acceptable even if the unlearned model does well on the forget set (as it could be a result of generalization from the retain set). We will focus the discussions of unlearning evaluation on approximate unlearning. ↩ Note that this “embeddedness” of a piece of data is related but distinct from whether the data is in or out of distribution, which should also affects how an unlearning algorithm should behave (e.g. unlearning a perfect inlier should be no-op for an ideal unlearning algorithm). ↩ Of course, we must first verify that such content has been trained on by the model in the first place. We can be almost certain that contents like Wikipedia articles are trained on, but we are generally less sure about a random blogpost somewhere on the internet. This is basically the membership inference problem. ↩ Fair use is a doctrine applicable specifically in the United States. The reader should refer to related doctrines in corresponding jurisdictions, such as fair dealings in Commonwealth countries. ↩",
    "commentLink": "https://news.ycombinator.com/item?id=40264352",
    "commentBody": "Machine Unlearning in 2024 (stanford.edu)249 points by ignoramous 21 hours agohidepastfavorite79 comments motohagiography 20 hours agoseems like there is a basic problem where if you specify something to be unlearned, it could still be re-learned by inference and prompting. the solution may not be in filtering the proscribed facts or data itself, but in the weights and incentives that form a final layer of reasoning. Look at \"safe\" models now like google's last launch, where the results were often unsatisfying, as clearly we don't want truthful models yet, but we want ones that enable our ability to develop them further, which for now means not selecting out by antagonizing other social stakeholders. maybe we can encode and weight some principle of the models having been created by something external, with some loosely defined examples they can refer to as a way to evaluate what they return, then ones that don't yield those results cease to be used, where the ones that find a way to align will get reused to train others. there will absolutely be bad ones, but in aggregate they should produce something more desirable, and if they really go off the rails, just send a meteor. the argument in how models can \"unlearn\" will be between those who favour incentives and those who favour rules- likely, incentives for ones I create, but rules for everyone elses'. reply musicale 9 hours agoparentIt is unsurprising that a system trained on human-generated content might end up encoding implicit bias, toxicity, and negative goals. And the more powerful and general-purpose a system is, the more suitable it is for a wide range of powerfully negative purposes. Neither specializing the model nor filtering its output seems to have worked reliably in practice. reply avi_vallarapu 17 hours agoprevWe need to consider the practicality of unlearning methods in real-world applications and the legal acceptance of the same. Given current technology and what advancements are needed to make Unlearning more possible, probably there should be a time-to-unlearn kind of an acceptable agreement that allows organizations to retrain or tune the response that does not involve any response from the to-be-unlearned copyright content. Ultimately, legal acceptance for unlearning may be all about deleting the data set that is part of any kind of violations from the training data set. It may be very challenging to otherwise prove legally through the proposed unlearning techniques, that the model does not produce any type of response involving the private data. The actual data set contains the private data violating privacy or copyright, and the model is trained on it, period. This means, it must involve retraining by deleting the documents/data to be unlearned. reply isodev 17 hours agoparent> a time-to-unlearn kind of an acceptable agreement Why put the burden to end users? I think the technology should allow for unlearning and even \"never learn about me in any future models and derivative models\". reply avi_vallarapu 17 hours agorootparentNo technology can guarantee 100% unlearning, and the only 100% guarantee is when the data is deleted before the model is retrained. Legally, even 99.99% accuracy may not be acceptable, but, only 100%. reply mr_toad 5 hours agorootparent> the only 100% guarantee is when the data is deleted before the model is retrained That’s not even a guarantee. A model can hallucinate information about anyone, and by sheer luck some of those hallucinations will be correct. And as a consequence of forging (see section 2.2.1) you’d never be able to prove whether the data was in the training set or not. reply eru 8 hours agorootparentprevOr rather some legal fiction that you can pretend is 100%. You can never achieve real 100% in practice after all. Eg the random initialisation of weights might already encode all the 'bad' stuff you don't want. Extremely unlikely, but not strictly 0% unlikely. The law cuts off at some point, and declares it 100%. reply isodev 2 hours agorootparentAll this is technically correct, but it also means this technology is absolutely not ready to be used for anything remotely involving humans or end user data. reply eru 2 hours agorootparentWhy? We use random data in lots of applications, and there's always the theoretical probability that it could 'spell something naughty'. reply Vampiero 15 hours agorootparentprevThe technology is on par with a Markov chain that's grown a little too much. It has no notion of \"you\", not in the conventional sense at least. Putting the infrastructure in place to allow people (and things) to be blacklisted from training is all you can really do, and even then it's a massive effort. The current models are not trained in such a way that you can do this without starting over from scratch. reply Retric 12 hours agorootparentThat’s hardly accurate. Deep learning among other things is another type of lossy compression algorithm. It doesn’t have a 1:1 mapping of each bit of information it’s been trained with, but you can very much extract a subset of that data. Which is why it’s easy to get DallE to recreate the Mona Lisa, variations on that image show up repeatedly in its training courpus. reply xg15 11 hours agorootparentprevWell then, maybe we shouldn't use the technology. reply beeboobaa3 17 hours agoparentprevHow to deal with \"unlearning\" is the problem of the org running the illegal models. If I have submitted a gdpr deletion request you better honor it. If it turns out you stole copyrighted content you should get punished for that. No one cares how much it might cost you to retrain your models. You put yourself in that situation to begin with. reply avi_vallarapu 17 hours agorootparentExactly, I think is where it leads to eventually. And that is what I my original comment meant as well. \"Delete it\" rather than using some more techniques to \"unlearn it\", unless you claim the unlearning is 100% accurate. reply visarga 17 hours agorootparentprev> No one cares how much it might cost you to retrain your models. Playing tough? But it's misguided. \"No one cares how much it might cost you to fix the damn internet\" If you wanted to retro-fix facts, even if that could be achieved on a trained model, it would still get back by way of RAG or web search. But we don't ask pure LLMs for facts and news unless we are stupid. If someone wanted to pirate a content it would be easier to use Google search or torrents than generative AI. It would be faster, cheaper and higher quality. AIs move slow, are expensive, rate limited and lossy. AI providers have in-built checks to prevent copyright infringement. If someone wanted to build something dangerous, it would be easier to hire a specialist than to chatGPT their way into it. All LLMs know is also on Google Search. Achieve security by cleaning the internet first. The answer to all AI data issues - PII, Copyright, Dangerous Information - is coming back to the issue of Google search offering links to it, and websites hosting this information online. You can't fix AI without fixing the internet. reply beeboobaa3 16 hours agorootparentWhat do you mean playing tough? These are existing laws that should be enforced. The amount of people's lives ruined by the American government because they were deemed copyright infringers is insane. The us has made it clear that copyright infringement is unacceptable. We now have a new class of criminals infringing on copyright on a grand scale via their models and they seem desperate to avoid persecution hence all this bullshit. reply cscurmudgeon 14 hours agorootparent1. You are assuming just training a model on copyrighted material is a violation. It is not. It may be under certain conditions but not by default. 2. Why should we aim for harsh punitive punishments just because it was done so in the past? reply beeboobaa3 14 hours agorootparent> 1. You are assuming just training a model on copyrighted material is a violation. It is not. It may be under certain conditions but not by default. Using copyrighted content for commercial purposes should be a violation if it's not already considered to be one. No different from playing copyrighted songs in your restaurant without paying a licensing fee. > 2. Why should we aim for harsh punitive punishments just because it was done so in the past? I'd be fine with abolishing, or overhauling, the copyright system. This rules with harsh penalties for consumers/small companies but not for bigtech double standard is bullshit, though. reply ekianjo 4 hours agorootparent> Using copyrighted content for commercial purposes should be a violation so reading a book and using the book contents to help you in your job would be a violation too based on your logic reply beeboobaa3 4 hours agorootparentA business cannot read a book, and your machine learning model is not given human rights. reply Dylan16807 2 hours agorootparent> A business cannot read a book Assume the human read the book as part of their job. Is that using copyrighted material for commercial purposes? If that doesn't count then I'm not sure why you brought up \"commercial purposes\" at all. > This rules with harsh penalties for consumers/small companies but not for bigtech double standard is bullshit, though. Consumers and small companies get away with small copyright violations all the time. And still bigger than having your image be one of millions in a training set. reply surfingdino 14 hours agoprevHow about a radial approach? How about not ingesting all content but only that which is explicitly marked as available for model-building purposes? reply xg15 12 hours agoprevWhat I don't get about the DP approach is how this would be reconciled with the \"exact\" question-answering functionality of LLMs. DP makes perfect sense if all I care about is low-resolution statistical metrics or distributions of something and not the exact values - the entire purpose of DP is to prevent reconstructing the exact values. However, the expectation for LLMs is usually to ask a question (or request a task) and get an exact value as a response: If you ask \"What's the phone number of John Smith?\" the model will either tell you it doesn't know or it will answer you with an actual phone number (real or hallucinated). It will not tell you \"the number is with 83% probability somewhere in New Jersey\". So if the model is trained with DP, then either the data is scrambled enough that the it won't be able to return any kind of reliably correct data, effectively making it useless - or it's not scrambled enough, so that the model can successfully reconstuct data despite the scrambling process, effectively making the DP step useless. Or in other words, the OP defines \"DP unlearning\" as: > The intuition is that if an adversary cannot (reliably) tell apart the models, then it is as if this data point has never been learned—thus no need to unlearn. However, if my original model truthfully returns John Smith's phone number on request and the \"unlearned\" model must not be distinguishable by an outside observer from the original model, then the \"unlearned\" model will also return the phone number. While I could say that \"technically\" the model has never seen the phone number in the training data due to my DP scrambling, this doesn't solve the practical problem why the unlearning was requested in the first place, namely that John Smith doesn't want the model to return his phone number. He could probably care less about the specific details of the training process. So then, how would DP help here? reply joshhansen 3 hours agoprev\"Eternal Sunshine of the Spotless Mind\" The erasure of knowledge is a troubling occupation reply JKCalhoun 9 hours agoprevI don't know — the post, reading the comments here, I am a little worried for the \"sanity\" of our AI that have been trained, untrained, retrained like a pawn in some kind of Cold War spy novel. reply kombookcha 1 hour agoparentIt's fine, the LLM AIs we have now are just fancy versions of autocorrect. They, and other LMs, guess at statistically probable words/datapoints, and because they don't understand context, you might need to put your thumb on the scales to make the output actually be useful. They're at best very janky tools as soon as you're working with things that require context that isn't easily contained in some kind of confined area of work. Currently we are seeing the phenomenon 'habsburg AI' where AI's consume their own outputs as training data, which rapidly deteriorates their ability to actually be useful for much of anything. The thing is that there literally isn't enough human-made data to keep feeding them (they already ate the entire internet), so if you both want to continue ramping their intake of data and you also don't want them to get rapidly weird and completely useless, you pretty much have to get in there with elbow grease. Removing or deprioritizing data that's tripping up the model is one of the few ways you can do human-assisted refinement of these things. The sooner we all face the music that these things aren't magical truth machines, have a long way to go and there is no guaranteed rate of growth, the sooner this hype cycle can end. reply dataflow 20 hours agoprev> However, RTBF wasn’t really proposed with machine learning in mind. In 2014, policymakers wouldn’t have predicted that deep learning will be a giant hodgepodge of data & compute Eh? Weren't deep learning and big data already things in 2014? Pretty sure everyone understood ML models would have a tough time and they still wanted RTBF. reply hooby 2 hours agoparentI'm pretty sure that the policymakers did NOT understand ML models in 2014 - and still do NOT understand it today. I also don't think that they care. They don't care that ML is a hodgepodge of data & compute, and they don't care how hard it is to remove data from a model. They didn't care about the ease or difficulty of removing data from more traditional types of knowledge storage either - like search indexes, database backups and whatnot. RTBF was not proposed with any specific technology in mind. What they had in mind, was to try and give individuals a tool, to keep their private information private. Like, if you have a private, unlisted phone number, and that number somehow ends up on the call-list of some pollster firm, you can force that firm to delete your number so that they can't call you anymore. The idea is, that if your private phone number (or similar data) ends up being shared or sold without your consent - you can try to undo the damage. In practice it might still be easier to get a new number, than to have your leaked one erased... but not all private data is exchangeable like that. reply indigovole 18 hours agoparentprevGDPR and RTBF were formulated around the fears of data collection by the Stasi and other organizations. They were not formulated around easing the burdens of future entrepreneurs, but about mitigating the damage they might cause. Europeans were concerned about real harms that living people had experienced, not about enabling AGI or targeted advertising or digital personal assistants. We have posts here at least weekly from people cut off from their services, and their work along with them, because of bad inference, bad data, and inability to update metadata based purely on BigGo routine automation and indifference to individual harm. Imagine the scale that such damage will take when this automation and indifference to individual harm are structured around repositories from which data cannot be deleted, cannot be corrected. reply spennant 19 hours agoparentprevAgreed. The media and advertising industry was most definitely leveraging cookie-level data for building attribution and targeting models. As soon as the EU established that this data was “personal data”, as it could, theoretically, be tied back to individual citizens, there were questions about the models. Namely “Would they have to be rebuilt after every RTBF request?” Needless to say, no one in the industry really wanted to address the question, as the wrong answer would essentially shut down a very profitable practice. reply Aerroon 18 hours agorootparentMore likely: the wrong answer would've shut out a profitable market rather than the practice. The EU is not the world. Anthropic seems to not mind blocking the EU for example. reply spennant 16 hours agorootparentSure. But two things: 1) At the time, the European data laws implied that it protected its citizens no matter where they are. Nobody wanted to be the first to test that in court. 2) The organizations and agencies performing this type of data modeling were often doing so on behalf of large multinational organizations with absurd advertising spends, so they were dealing with Other People’s Data. The responsibility of scrubbing it clean of EU citizen data was unclear. What this meant was that an EU tourist who traveled to the US, and got served a targeted ad, could make a RTBF request to the advertiser (think Coca-Cola, Nestle or Unilever) The whole thing was a mess. reply startupsfail 18 hours agoparentprevRTBF was introduced to solve a specific issue, no? Politicians and their lobbyist friends could no longer remove materials linking them to their misdeeds as the first Google Search link associated with their names. Hence RTBF. Now, there’s similar issue with AI. Models are progressing towards being factual, useful and reliable. reply isodev 19 hours agoparentprevOf course, it’s not a regulation issue. The technology was introduced to users before it was ready. The very nature of training without opt-in consent or mechanism of being forgotten are all issues that should have been addressed before trying to make a keyboard with a special copilot button. reply peteradio 20 hours agoparentprevI don't know if people anticipated contemporary parroting behavior over huge datasets. Modern well funded models can recall an obscure persons home address buried deep into the training set. I guess the techniques described might be presented to the European audience in an attempt to maintain access to their data/and or market for sales. I hope they fail. reply aidenn0 17 hours agoprevI think \"unlearning\" is not the actual goal; we don't want the model to stick its proverbial head in the sand. Being unaware of racism is different from not producing racist content (and, in fact, one could argue that it is necessary to know about racism if one wishes to inhibit producing racist content; I remember in elementary school certain kids thought it would be funny to teach one of the special-ed kids to parrot offensive sentences). reply krono 13 hours agoparentSay you tell me you want a red sphere. Taken at face value, you show a prejudice for red sphere's and discriminate against all other coloured shapes. We've all had to dance that dance with ChatGPT by now, where you ask for something perfectly ordinary, but receive a response telling you off for even daring to think like that, until eventually you manage to formulate the prompt in a way that it likes with just the right context and winner vocabulary + grammar, and finally the damned thing gives you the info you want without so much as any gaslighting or snarky insults hiding in the answer! It doesn't understand racism, it simply evaluates certain combinations of things according to how it was set up to do. reply cwillu 20 hours agoprev“to edit away undesired things like private data, stale knowledge, copyrighted materials, toxic/unsafe content, dangerous capabilities, and misinformation, without retraining models from scratch” To say nothing of unlearning those safeguards and/or “safeguards”. reply ben_w 19 hours agoparentIt sounds like you're mistakenly grouping together three very different methods of changing an AI's behaviour. You have some model, M™, which can do Stuff. Some of the Stuff is, by your personal standards Bad (I don't care what your standard is, roll with this). You have three solutions: 1) Bolt on a post-processor which takes the output of M™, and if the output is detectably Bad, you censor it. Failure mode: this is trivial to remove, just delete the post-processor. Analogy: put secret documents into a folder called \"secret do not read\". 2) Retrain the weights within M™ to have a similar effect as 1. Failure mode: this is still fairly easy to remove, but will require re-training to get there. Why? Because the weights containing this information are not completely zeroed-out by this process. Analogy: how and why \"un-deletion\" is possible on file systems. 3) Find and eliminate the weights within M™ that lead to the Bad output. Analogy: \"secure deletion\" involves overwriting files with random data before unlinking them, possibly several times if it's a spinning disk. -- People are still doing research on 3 to make sure that it actually happens, what with it being of very high importance for a lot of different reasons including legal obligation. reply andy99 19 hours agorootparentUntil we have a very different method of actually controlling LLM behavior, 1 is the only feasible one. Your framing only makes sense when \"Bad\" is something so bad that we can't bear its existence, as opposed to just \"commercially bad\" where it shouldn't behave that way with an end user. In the latter, your choice 1 - imposing external guardrails - is fine. I'm not aware of anything LLMs can do that fits in the former category. reply ben_w 16 hours agorootparent> Until we have a very different method of actually controlling LLM behavior, 1 is the only feasible one. Most of the stuff I've seen, is 2. I've only seen a few places use 1 — you can tell the difference, because when a LLM pops out a message and then deletes it, that's a type 1 behaviour, whereas if the first thing it outputs directly is a sequence of tokens saying (any variant of) \"nope, not gonna do that\" that's type 2 behaviour. This appears to be what's described in this thread: https://old.reddit.com/r/bing/comments/11fryce/why_do_bings_... The research into going from type 2 to type 3 is the entirety of the article. > Your framing only makes sense when \"Bad\" is something so bad that we can't bear its existence, as opposed to just \"commercially bad\" where it shouldn't behave that way with an end user. In the latter, your choice 1 - imposing external guardrails - is fine. I disagree, I think my framing applies to all cases. Right now, LLMs are like old PCs with no user accounts and a single shared memory space, which is fine and dandy when you're not facing malicious input, but we live in a world with malicious input. You might be able to use a type 1 solution, but it's going to be fragile, and more pertinently, slow, as you only know to reject content once it has finished and may therefore end up in an unbounded loop of an LLM generating content that a censor rejects. A type 2 solution is still fragile, but it just doesn't make the \"bad\" content in the first place — and, to be clear, \"bad\" in this context can be anything undesired, including \"uses vocabulary too advanced for a 5 year old who just started school\" if that's what you care about using some specific LLM for. reply cwillu 15 hours agorootparentprevI think you mistakenly replied to my comment instead of one that made some sort of grouping? Alternatively, you're assuming that because there is some possible technique that can't be reversed, it's no longer useful to remove the effects of techniques that _can_ be reversed? reply gotoeleven 18 hours agoprevMy new startup includes a pitchfork wielding mob in the ML training loop. reply nullc 20 hours agoprevI've wondered before if it was possible to unlearn facts, but retain the general \"reasoning\" capability that came from being trained on the facts, then dimensionality reduce the model. reply Brian_K_White 18 hours agoparentI don't know about in AI, but it seems like that is what humans do. We remember some facts but I know at least I have had a lot of facts pass through me and only leave their effects. I once had some facts, did some reasoning, arrived at a conclusion, and only retained the conclusion and enough of the reasoning to identify other contexts where the same reasoning should apply. I no longer have the facts, I simply trust my earlier selfs process of reasoning, and even that isn't actually trust or faith because I also still reason about new things today and observe the process. But I also evolve. I don't only trust a former reasoning unchanging forever. It's just that when I do revisit something and basically \"reproduce the other scientists work\" even if I arrive a different conclusion today, I'm generally still ok with the earlier me's reasoning and conclusion. It stands up as reasonable, and the new conclusion is usually just tuned a little, not wildly opposite. Or some things do change radically but I always knew they might, like in the process of self discovery you try a lot of opposite things. Getting a little away from the point but the point is I think the way we ourselves develop answer-generating-rules is very much by retaining only the results (the developed rules) and not all the facts and steps of the work, at least much of the time. Certainly we remember some justifying / exemplifying facts to explain some things we do. reply mr_toad 5 hours agoparentprevHow much reasoning capability LLM’s have is up for debate. With a true AGI you could just tell it to keep people’s personal information confidential and expect that it would understand that instruction. reply andy99 19 hours agoparentprevIf you think of knowledge as a (knowledge) graph, it seems there would be some nodes with low centrality that you could drop without much effect, and other key ones that would have a bigger impact if lost. reply huygens6363 19 hours agoparentprevYes, me too. If it could somehow remember the “structure” instead of the instantiation. More “relationships between types of token relationships” instead of “relationships between tokens”. reply greenavocado 15 hours agoprevPlease use the correct terminology: censorship reply Dylan16807 2 hours agoparentIs it censorship to not include every piece of text you can possibly find into your training dataset? What's the difference between making that choice versus removing it from the model later? reply qbit42 13 hours agoparentprevI don't think that's a fair characterization. If a user requests a company to stop using their data, ML unlearning allows the company to do so without retraining their models from scratch. reply danielmarkbruce 15 hours agoparentprevIf company X wants their model to say/not say Y based on ideology, they aren't stopping anyone saying anything. They are stopping their own model saying something. The fact that I don't go around screaming nasty things about some group doesn't make me against free speech. It's censorship to try to stop people producing models as they see fit. reply 62951413 14 hours agoparentprevThe prolefeed explains that deep duckspeaking is doubleplusgood. Nothing to see here, citizen. reply negative_person 20 hours agoprev [–] Why should we try to unlearn \"bad\" behaviours from AI? There is no AGI without violence, its part of being free thinking and self survival. But also by knowing that launching a first strike by a drunk president was a bad idea we averted a war because of a few people, AI needs to understand consequences. It seems futile to try and hide \"bad\" from AI. reply williamtrask 20 hours agoparentBecause we can get AI related technologies to do things living creatures can’t, like provably forget things. And when it benefits us, we should. Personal opinion, but I think AGI is a good heuristic to build against but in the end we’ll pivot away. Sort of like how birds were a good heuristic for human flight, but modern planes don’t flap their wings and greatly exceed bird capabilities in many ways. Attribution for every prediction and deletion seem like prime examples of things which would break the analogy of AI/AGI with something more economically and politically compelling/competitive. reply negative_person 20 hours agorootparentCan you point to any behaviour in human beings you'd unlearn if theyd also forget the consequences? We spend billions trying to predict human behaviour and yet we are surprised everyday, \"AGI\" will be no simpler. We just have to hope the dataset was aligned so the consequences are understood, and find a way to contain models that don't. reply williamtrask 18 hours agorootparentYou seem to be focusing a lot on remembering or forgetting consequences. Yes, ensuring models know enough about the world to only cause the consequences they desire is a good way for models to not create random harm. This is probably a good thing. However, there are many other reasons why you might want a neural network to provably forget something. The main reason has to do with structuring an AGI's power. Even though the simple-story of AGI is something like \"make it super powerful, general, and value aligned and humanity will prosper\". However, the reality is more nuanced. Sometimes you want a model to be selectively not powerful as a part of managing value mis-alignment in practice. To pick a trivial example, you might want a model to enter your password in some app one time, but not remember the password long term. You might want it to use and then provably forget your password so that it can't use your password in the future without your consent. This isn't something that's reliably doable with humans. If you give them your password, they have it — you can't get it back. This is the point at which we'll have the option to pursue the imitation of living creatures blindly, or choose to turn away from a blind adherence to the AI/AGI story. Just like we reached the point at which we decided whether flying planes should have flapping wings dogmatically — or whether we should pursue the more economically and politically competitive thing. Planes don't flap their wings, and AI/AGI will be able to provably forget things. And that's actually the better path. A recent work co-authors and I published related to this: https://arxiv.org/pdf/2012.08347 reply aeonik 19 hours agorootparentprevThe feeling of extreme euphoria and its connection to highly addictive drugs like Heroin might be a use case. Though I'm not sure how well something like that would work in practice. reply everforward 18 hours agorootparentIs that possible to do without also forgetting why it’s dangerous? That seems like it would fuel a pattern of addiction where the person gets addicted, forgets why, then gets addicted again because we wiped their knowledge of the consequences the first time around. Then again, I suppose if the addiction was in response to a particular stimulus (death of a family member, getting fired, etc) and that stimulus doesn’t happen again, maybe it would make a difference? It does have a tinge of “those who don’t recall the past are doomed to repeat it”. reply aeonik 16 hours agorootparentAfter a certain point I think someone can learn enough information to derive almost everything from first principles. But I think it might work temporarily. There's a movie about this idea called \"Eternal Sunshine of a Spotless Mind\". I find it hard I believe that you can surgically censor one chunk of information, and cut off the rest of the information. Especially if it's general physical principles. I also don't have a nice topological map of how all the world's information is connected to the moment, so I can't back up by opinions. Though I'm still rooting for the RDF/OWL and Semantic Web folks, they might figure it out. reply Brian_K_White 18 hours agorootparentprevIt sounds like the only answer for AI is the same as the only answer for humans. Wisdom. Arriving at actions and reactions based on better understanding of the interconnectedness and interdependency of everything and everyone. (knowing more not less, and not selective or bowdlerized) And most humans don't even have it. Most humans are not interested and don't believe and certainly don't act as though \"What's good for you is what's good for me, what harms you harms me.\" Every day a tech podcaster or youtuber says this or that privacy loss or security risk \"doesn't affect you or me\", they all affect you and me, when a government or company gives themselves and then abuses power over a single person anywhere, that is a hit to you and me even though we aren't that person, because that person is somebody, and you and I are somebody. Most humans ridicule anyone that talks like that and don't let them near any levers of power at any scale. They might be ok with it in inconsequential conversational contexts like a dinner party or this or this forum, but not in any decision-making context. Anyone talking like that is an idiot and disconnected from reality, they might drive the bus off the bridge because the peace fairies told them to. If an AI were better than most humans and had wisdom, and gave answers that conflicted with selfishness, most humans would just decide they don't like the answers and instructions coming from the AI and just destroy it, or at least ignore it, pretty much as they do today with humans who say things they don't like. Perhaps one difference is an AI could actually be both wise and well-intentioned rather than a charlatan harnessing the power of a mass of gullables, and it could live longer than a human and it's results could become proven-out over time. Some humans do get recognized eventually, but by then it doesn't do the rest of us any good because they can no longer be a leader as they're too old or dead. Then again maybe that's required actually. Maybe the AI can't prove itself because you can never say of the AI, \"What does he get out of it by now? He lived his entire life saying the same thing, if he was just trying to scam everyone for money or power or something, what good would it even do him now? He must have been sincere the whole time.\" But probably even the actual good AI won't do much good, again for the same reason as with actually good humans, it's just not what most people want. Whatever individuals say about what their values are, by the numbers only the selfish organisations win. Even when a selfish organization goes too far and destroys itself, everyone else still keeps doing the same thing. reply AvAn12 15 hours agorootparentprevA few things to exclude from training might include: - articles with mistakes such as incorrect product names, facts, dates, references - fraudulent and non-repeatable research findings - see John Ioannidis among others - outdated and incorrect scientific concepts like phlogiston and LaMarckian evolution - junk content such as 4-chan comments section content - flat earther \"science\" and other such nonsense - debatable stuff like: do we want material that attributes human behavior to astrological signs or not? And when should a response make reference to such? - prank stuff like script kiddies prompting 2+2=5 until an AI system \"remembers\" this - intentional poisoning of a training set with disinformation - suicidal and homicidal suggestions and ideation - etc. Even if we go with the notion that AGI is coming, there is no reason its training should include the worst in us. reply beeboobaa3 16 hours agorootparentprevSeeing dad have sex with mom. reply andy99 20 hours agoparentprevThis is presumably about a chatbot though, not AGI, so it's basically a way of limiting what they say. (Not a way that I expect to succeed) reply numpad0 17 hours agoparentprevThey are just trying to find a way to plausibly declare successful removal of copyrighted and/or illegal material without discarding weights. GPT-4 class models reportedly costs $10-100m to train, and that's too much to throw away for Harry Potter or Russian child porn scrapes that could later reproduce verbatim despite representingThere is no AGI without violence, its part of being free thinking and self survival. I disagree. Are committed pacifists not in possession of general intelligence? reply doubloon 19 hours agoparentprevAGI would not beGI unless it could change its mind after realizing its wrong about something reply 542458 19 hours agorootparentI disagree. People with anterograde amnesia still possess general intelligence. reply saintfire 18 hours agorootparentI don't know I ton about amnesia, but I would think the facilities for changing their mind are still there. E.g. ordering food, they might immediately change their mind after choosing something and correct their order. I recognize they cannot form new memories but from what I understand they still would have a working memory, otherwise you'd be virtually unable to think and speak. reply 542458 8 hours agorootparentLLMs will change their minds today. Most major ones can change their minds on subsequent generations within the same context (“I’m sorry, my previous answer was incorrect,..”), and the biggest ones can change their mind mid-answer (mostly observed with GPT4). reply szundi 19 hours agoparentprevThanks but no violent AGIs thanks reply Cheer2171 20 hours agoparentprevSo you have a problem with supervised learning like spam classifiers? reply imtringued 18 hours agoparentprev [–] You seem to be ignoring the potential to use this to improve the performance of LLMs. If you can unlearn wrong answers you can ask the model using any scoring mechanism to check for correctness instead of scoring for token for token similarity to the prescribed answer. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"Machine Unlearning in 2024\" by Ken Liu delves into the concept of machine unlearning, which eliminates unwanted data from ML models without starting from zero.",
      "The post delves into challenges, motivations, and potential uses of unlearning, like safeguarding user privacy and eliminating harmful content.",
      "It explores different unlearning methods, such as exact unlearning and differential privacy, emphasizing the significance of legal and technical considerations, empirical methods, and algorithm assessments in AI safety, copyright protection, and AI model realms."
    ],
    "commentSummary": [
      "The conversation delves into challenges and implications of machine unlearning, copyright infringement, differential privacy, and controlling AI behavior.",
      "Topics include legal aspects, privacy concerns, the Right to be Forgotten, unlearning facts, and ethical considerations in AI training.",
      "Exploring potential benefits and risks of AI selectively forgetting info, leveraging AGI to enhance language models, and the significance of AI comprehending consequences and ethical principles."
    ],
    "points": 249,
    "commentCount": 79,
    "retryCount": 0,
    "time": 1714912244
  },
  {
    "id": 40266464,
    "title": "Simplicity vs. Complexity: The Value Proposition",
    "originLink": "https://eugeneyan.com/writing/simplicity/",
    "originBody": "eugeneyan Start Here Writing Speaking Prototyping About Simplicity is An Advantage but Sadly Complexity Sells Better [ machinelearning engineering production 🔥 ] · 7 min read We sometimes hear of a paper submission being rejected because the method was too simple, or a promotion being denied because the work artifacts lacked complexity. I think this can be partly explained by Dijkstra’s quote: “Simplicity is a great virtue but it requires hard work to achieve it and education to appreciate it. And to make matters worse: complexity sells better.” — Edsger Dijkstra Why does complexity sell better? Complexity signals effort. Papers with difficult ideas and technical details suggest blood, sweat, and tears. Systems with more components and features hint at more effort than systems with less. Because complex artifacts are viewed as requiring more effort, they’re also deemed as more challenging to create and thus more worthy. And because of the perceived effort involved, they’re often judged to be higher quality. Complexity signals mastery. A complex system with many moving parts suggests that the designer has proficiency over each part and the ability to integrate them. Inaccessible papers peppered with jargon and proofs demonstrate expertise on the subject. (This is also why we quiz interview candidates on algorithms and data structures that are rarely used at work.) If laymen have a hard time understanding the complex idea or system, its creator must be an expert, right? Complexity signals innovation. Papers that invent entirely new model architectures are recognized as more novel relative to papers that adapt existing networks. Systems with components built from scratch are considered more inventive than systems that reuse existing parts. Work that just builds on or reuses existing work isn’t that innovative. “The idea is too simple, almost like a trick. It only changes one thing and everything else is the same as prior work.” — Reviewer 2 Complexity signals more features. Systems with components that can be mixed and matched suggest flexibility to cover all the bases. For example, supporting both SQL and NoSQL data stores, or enabling both batch and streaming pipelines. Because complex systems have more lego blocks relative to simple systems, they’re considered more adaptable and better able to respond to change. All in all, the above leads to complexity bias where we give undue credit to and favour complex ideas and systems over simpler ones. Why is simplicity an advantage? Simple ideas and features are easier to understand and use. This makes them more likely to gain adoption and create impact. They’re also easier to communicate and get feedback on. In contrast, complex systems are harder to explain and manage, making it difficult for users to figure out what to do and how to do it. Because there are too many knobs, mistakes are more frequent. Because there are too many steps, inefficiency occurs. Simple systems are easier to build and scale. Systems that require less rather than more components are easier to implement. Using standard, off-the-shelf technology also makes it easier to find qualified people who can implement and maintain it. And because simpler systems have less complexity, code, and intra-system interactions, they’re easier to understand and test. In contrast, needlessly complex systems require more time and resources to build, leading to inefficiency and waste. When Instagram was acquired in 2012, it had a 13-person team serving tens of millions of users. It scaled and kept operational burden per engineer low by sticking to proven technologies instead of new, shiny ones. When other startups adopted trendy NoSQL data stores and struggled, Instagram kept it lean with battle-proven and easy-to-understand PostgreSQL and Redis. Simple systems have lower operational costs. Deploying a system is not the finish line; it’s the starting line. The bulk of the effort comes after the system is in production, likely by someone other than the original team that built it. By keeping systems simple, we lower their maintenance cost and increase their longevity. Simple systems have fewer moving parts that can break, making them more reliable and easier to fix. It’s also easier to upgrade or swap out individual components because there are fewer interactions within the system. In contrast, complex systems are more fragile and costly to maintain because there are so many components that need to be grokked by a limited team. Having more interdependent parts also makes troubleshooting harder. “The more simple any thing is, the less liable it is to be disordered, and the easier repaired when disordered.” — Thomas Paine, Common Sense, 1776 Specific to machine learning, simple techniques don’t necessarily perform worse than more sophisticated ones. A non-exhaustive list of examples include: Tree-based models > deep neural networks on 45 mid-sized tabular datasets Greedy algorithms > graph neural networks on combinatorial graph problems Simple averaging ≥ complex optimizers on multi-task learning problems Simple methods > complex methods in forecasting accuracy across 32 papers Dot product > neural collaborative filtering in item recommendation and retrieval The humble dot product outperforms deep learning methods for recommendations (source) What’s wrong with rewarding complexity? It incentivizes people to make things unnecessarily complicated. Using simple methods or building simple systems may appear easier and is thus valued less. As a result, people game the system to get more rewards and the simplest solution is no longer the most obvious one. Complexity begets more complexity, eventually making it impossible to work. It also encourages the “not invented here” mindset where people prefer to build from scratch and shun reusing existing components even though it saves time and effort. This wastes time and resources and often leads to poorer outcomes. Unfortunately, most promotion processes overemphasize complexity in work artifacts. Simple solutions are easier to implement and scale than complex solutions. However promotions often are rewarded to those who create complex solutions. We need to start acknowledging the power of simple. Don’t accept complex. It’s a trap. — Bryan Liles (@bryanl) August 5, 2022 Ditto for machine learning paper submissions. A common point raised by ML reviewers is that a method is too simple or is made of existing parts. But simplicity is a strength, not a weakness. People are much more likely to adopt simple methods, and simple ones are also typically more interpretable and intuitive. 1/2 — Micah Goldblum (@micahgoldblum) August 2, 2022 (If your idea didn’t get the credit it deserves, take comfort in the fact that breakthroughs such as Kalman Filters, PageRank, SVM, LSTM, Word2Vec, Dropout, etc got rejected too. We’re generally bad at assessing how useful or impactful an innovation will be. 🤷) How should we think about complexity instead? The objective should be to solve complex problems with as simple a solution as possible. Instead of focusing on the complexity of the solution, we should focus on the complexity of the problem. A simple solution demonstrates deep insight into the problem and the ability to avoid more convoluted and costly solutions. Often, the best solution is the simple one. “Everything should be made as simple as possible, but not simpler” — Albert Einstein Instead of having a complex, catch-all solution, consider multiple focused solutions. A one-size-fits-all solution is usually less flexible and reusable than expected. And because it serves multiple use cases and stakeholders, it tends to be “tightly coupled” and require more coordination during planning and migrations. In contrast, it’s easier to operate—and unavoidably, deprecate—single-purpose systems. In my many years as tech lead, later as consultant and then in @godotengine, I believe the biggest enemy software engineers fight is the deep & common belief that one-size-fits-all solutions to problems (even hypothetical) always exist. Accepting they don't leads to better code. pic.twitter.com/Tc14NGOexi — Juan Linietsky (@reduzio) July 3, 2022 Is the juice worth the squeeze? One way to overcome the complexity bias is Occam’s razor. It states that the simplest solution or explanation is usually the right one. Thus, let’s not be too quick to dismiss simple ideas or add unnecessary complexity to justify their worth. Alternatively, ask yourself: Given the cost of complexity, is the juice worth the squeeze? Thanks to Yang Xinyi and Swyx for reading drafts of this. If you found this useful, please cite this write-up as: Yan, Ziyou. (Aug 2022). Simplicity is An Advantage but Sadly Complexity Sells Better. eugeneyan.com. https://eugeneyan.com/writing/simplicity/. or @article{yan2022simplicity, title = {Simplicity is An Advantage but Sadly Complexity Sells Better}, author = {Yan, Ziyou}, journal = {eugeneyan.com}, year = {2022}, month = {Aug}, url = {https://eugeneyan.com/writing/simplicity/} } Share on: Browse related tags: [ machinelearning engineering production 🔥 ] Search « Uncommon Uses of Python in Commonly Used Libraries Writing Robust Tests for Data & Machine Learning Pipelines » Join 6,700+ readers getting updates on machine learning, RecSys, LLMs, and engineering. Get email updates Twitter LinkedIn Threads GitHub Eugene Yan designs, builds, and operates machine learning systems that serve customers at scale. He's currently a Senior Applied Scientist at Amazon. Previously, he led machine learning at Lazada (acquired by Alibaba) and a Healthtech Series A. He writes & speaks about machine learning, recommenders, LLMs, and engineering at eugeneyan.com and ApplyingML.com. © Eugene Yan 2015 - 2024 • Feedback • RSS",
    "commentLink": "https://news.ycombinator.com/item?id=40266464",
    "commentBody": "Simplicity is an advantage but sadly complexity sells better (2022) (eugeneyan.com)227 points by 7d7n 16 hours agohidepastfavorite135 comments Ozzie_osman 16 hours agoI worked at a certain FAANG company at a time where the promo process rewarded \"solving complex problems\". The more complex of a problem you could solve the higher your level (and your pay, your status, etc). Naturally, people were incentivized to find complex problems so they could solve them. Not only that, I think a lot of tech stacks at other companies evolved by copying this company's ideas, so even smaller companies with even less need for complex solutions ended up having them as well. reply oytis 15 hours agoparentI don't say it is necessarily a good metric for promotions, but solving complex problems is not the same as finding complex solutions reply dimal 14 hours agorootparentI’ve seen teams destroy themselves by solving complex problems that didn’t need to be solved. Instead of solving the real problems, they imagined a problem that was more interesting and complex, and tried to solve that. reply nox101 18 minutes agorootparentin my defense I can think of at least four times I needed to prototype a complex solution to understand why it was the wrong solution and then discard it reply ranger_danger 8 hours agorootparentprevDo you have any specific (or similar to it) examples you could provide? reply poslathian 2 hours agorootparentI once wasted several months of schedule and payroll wrestling to improve a DNN model - which resulted in that model getting much much better thanks to tons of creative and interesting work. We swapped the learned model for a perfect a oracle and realized the actual problem was elsewhere in the system and was easily fixed. None of those model enhancements were needed. reply l0b0 6 hours agorootparentprevThese weren't fatal to the project or team, but did contribute extra work for net negative value (slower & more maintenance, with no relevant extra features): - Using Kubernetes to host an application when Docker Compose would do (specialist application withSoftware isn't an art project; it's a practical tool to solve practical problems. Yet me and my colleagues have seen code that resembles abstract art far too often - it's interesting to look at, but mind-bogglingly complex. Getting rid of that is a delicate balance act, as you risk stepping on the shoes of the people who wrote the code. reply arp242 13 hours agorootparentYes it's a problem; that's why I've started to use the \"this isn't an art project\" line, because some people really do seem to treat it as such. reply applied_heat 12 hours agorootparentI do see writing software like writing a book. I write it with the future reader in mind. It needs to be meant to be read. I picture my colleagues or young engineers 20 years from now combing through my programs, learning the problem domain, learning the tradeoffs we made at the time and why it made sense. reply test1235 1 hour agorootparentwith that analogy, I'd say code is more like the annotations of the manuscript before the reader sees the final print of the book reply sratner 15 hours agorootparentprevSometimes the promo committee can't tell the difference, i.e. was the problem complex to warrant the complex solution. They just see the effort put into solving it. reply itronitron 3 hours agoparentprevBonus points if you can describe it using mathematical terminology that few people have heard about. reply nasmorn 2 hours agorootparentAt the Austrian Institute of Advanced studies there was a professor that got real good at stochastic differential equations and made a career out of applying them to anything and all in economics. One big advantage was that almost no other economist can actually follow it. Those few that can are obviously super invested in the technique reply hintymad 8 hours agoparentprevThis is a really tough problem to solve. If the company has amazing leaders, the leaders can correctly judge the depth of the problem and its impact, therefore rewarding the problem solver properly. Unfortunately promotion is really a tool for most leaders to advance their agenda. In a way Google invented this \"solving complex problems\" just to counter balance the misaligned incentives of the leaders, but to no avail. reply chii 7 hours agorootparentWhen the value generation is so far removed from the individual, the system will get skewed towards private taking over something more globally beneficial (globally as in for the entire company). reply weaksauce 13 hours agoparentprevso that's how react was made reply 7d7n 15 hours agoparentprevThe goal is to solve complex problems with as simple a solution as possible. reply jayd16 14 hours agorootparentThe goal is to provide user value. Complexity is falsely valued. reply lkdfjlkdfjlg 14 hours agorootparentWho says that's the goal? Providing user value is falsely valued. reply dijksterhuis 8 hours agorootparentWhy do people need goals to do things? Having a goal is falsely valued. reply matanyall 5 hours agorootparentWhy do people do things? Doing things is falsely valued. (Am partially sentient collection of coral, YMMV) reply hyperthesis 8 hours agoparentprevoblig. perverse incentive https://www.folklore.org/Negative_2000_Lines_Of_Code.html reply rsync 15 hours agoprevHmmm ... I have been thinking about this recently in relation to the complex and unwieldy nature of modern car UI (especially in electric cars). It's so bad that it is keeping me from buying a car that I need. The conclusion that I have come to is: Sophisticated consumers are very different than aspirational consumers - and there are always many, many more aspirational consumers. Therefore, catering to aspirational consumers at the expense of sophisticated ones is a rational economic choice. An aspirational consumer will put up with all manner of deficiencies and gimmickry because they perceive them as being emblematic of their consumer achievement. In cruder terms: They are so happy to be driving a \"luxury\" car that they don't notice the garbage that came with it. Meanwhile, after decades of luxury car purchases, I just want the shifter to be intelligible ... reply iknowstuff 15 hours agoparentIt’s kinda funny reading this as an owner of a Tesla. People who don’t have them like yourself keep lamenting touchscreens online, whereas Tesla owners explicitly stay with the brand because of how excellent it is compared to other vehicles, touchscreen or not. Technology Connections just posted a video where he explains how he had to hold his car’s ugly 90s era stalk for 4 seconds to change his wipers. Had to read a manual to discover that. This kind of crap is very standard in cars with disjoint components cobbled together. Meanwhile I get my live video sentry alerts on my phone, my car drives me to work, my gear selection is typically putting on a seatbelt and pressing the brake pedal as the car knows the right direction, everything important is contextually accessible via the wheels on the steering wheel instead of having 50 buttons for everything, and so on. It is simple and powerful. The problem with those luxury vehicles you mention is just awful execution. reply JackSlateur 14 hours agorootparentI'm working on a r&d team dedicated to finding ways to repair modern cars at scale As such, we get lots of car, mostly EV (but not only), up to 100k€/u The Telsas are indeed not the worst They are still boring af and not in the top of the basket reply AJRF 15 hours agoprevI watch a YouTuber called Theo Browne sometimes. He is primarily a front end dev. When I watch him talk about his solutions to things I feel like i've been hit on the head with a baseball bat. The sheer number of things that go into his demos is eye watering. The number of arcane terms about React he will mention in a single video astounds me. I don't mean to specifically call him out, but I worry that the complexity is what keeps him popular. And then you have someone like Pieter Levels just slinging raw php into production and not talking about anything like Suspense or Server Side Rendering or Hydration. They both get to the same ends (well Pieter Levels makes magnitudes of order more money I think) but there is a gulf in complexity. I'd actually argue something like Nomad List is much more feature rich than anything I see from Theo. reply whstl 14 hours agoparentI'm finding more and more that Youtube influencer software development is completely disconnected from real world software development. The amount of libraries and code for toy stuff is humongous compared to anything I see in production, and I've seen some monstrosities. I wonder for how long, though. reply vbezhenar 13 hours agorootparentMy experience with frontend people is that they don't think twice about adding random libraries they just found that was released month ago. Also their projects are unmaintainable after one year, but they are long gone by that time and new developers arguing that they need to rewrite everything. I really hate small libraries and in my projects I tend to rewrite lots of trivial stuff, but that keeps me comfortable. reply antisthenes 6 hours agorootparentLooks like they found a way to unionize and achieve job security in a very roundabout sort of way. reply whstl 2 hours agorootparentprevYeah, good point, for frontend, influencer code is definitely already leaking. reply thjdidiend 15 hours agoparentprevI met Theo once. I didn't know who he was, but he made sure to tell me within the first few sentences that he was a very popular streamer/youtuber. I then watched him get recognized by someone else and they had a sort of friendly shouting match about something Theo has recently talked about in an opinionated way on his channel. His personality seemed perfectly suited for maximal media engagement through needless complexity. The more complicated things are the more arguments you can have and the smarter you can sound to those less familiar with all your esoteric choices. reply 2four2 16 hours agoprevIn physical UI our group calls this the Microwave Problem. No one uses the 20 extra buttons on the microwave, they mostly just use one or two buttons. But no one will buy a microwave with few buttons. reply jeffreygoesto 16 hours agoparentI must be no one. Love my Samsung ME82V since a decade now. Two dials. Period. reply closewith 15 hours agorootparentIf you know the model of your microwave offhand, you're definitely an outlier! reply diarrhea 3 hours agorootparentprevHilarious coincidence. I had and loved that exact same model for the same reasons. Until one of the dials broke and I discovered how utterly irreparable the thing is. Had to get rid of it and indeed, it’s impossible to find similarly simple models. Oh well! reply euroderf 7 hours agorootparentprevI'm sure that having two dials costs more to produce than a crappy membrane keypad, and that the product manager was nearing retirement. reply NortySpock 13 hours agorootparentprev\"Dial-A-Yield: Not just for nuclear warheads!\" reply PurpleRamen 15 hours agoparentprevThere is value in choice. Not won't necessary always use just the same two buttons, so people like have the option to use other buttons. And quite often more buttons also come with better specs. reply mattmaroon 15 hours agoparentprevHigh-end blenders too. All I want is speed dial, pulse switch, and on/off switch. And that's all anyone wants, but for some reason every new generation has to have all these functions nobody uses. reply bombcar 14 hours agorootparentCommercial ones are simple - unless it has a programmable “do the smoothie steps” button. reply namaria 15 hours agorootparentprevNo one has ever been paid for keeping a good design the same. reply thaumasiotes 3 hours agorootparentprev> All I want is speed dial, pulse switch, and on/off switch. Really? I want \"quiet\". A certain level of noise is inevitable, and blenders are way, way past that point. They're louder than food processors! reply blue1 15 hours agoparentprevApple used to sell stuff that had “curated UIs”: few control, few functions, and excellent UX. I remember the cleanliness of the iPod vs the overfeatured and complicated competitors. reply lo0dot0 4 hours agorootparentA lot of the competitors had less features, like not being able to select the next song without stopping the current song. reply aatd86 11 hours agorootparentprevAlledgedly simple UI (although I never was a Apple guy, was using iRivers at that time), but building the iPod was hard, probably entailing a flew of difficult, complex hardware and design issues to tackle. reply realusername 14 hours agorootparentprevThe competitors were too cheap to have any feature, usually they had a big play button, a next and previous button and that's pretty much it. The settings were usually pretty poor on those mp3 players, on mine you had a microphone mode, language, timezone, some shuffle configuration and that's pretty much it. The iPod did look much much better and refined but in terms of simplicity, it's hard to beat the single play button of an mp3 player which doesn't know to do anything else. Those things were designed like appliance more than tech products. reply blue1 14 hours agorootparentI don’t think that’a true. Too many years have passed so I cannot cite makes and models, but I worked in an IT magazine back then and there were mp3 players with a lot of buttons, not unlike those overcomplicated VHS recorders which sold on “features”. reply realusername 14 hours agorootparentThat's true, those also existed but what I've seen, they were not bought as much as the cheap kind. The ipod gave a reason to pay extra, those half way though products really did not. I had one similar to those https://i.pinimg.com/originals/95/43/8b/95438b86a98370a741c2... Those things really didn't have a single real feature beyond playing music and recording with a microphone in the settings (which nobody really used) The screen and processing power was way too bad to do anything else anyways, even if they wanted to. reply dimal 14 hours agoparentprevHas anyone actually tried? I’ve searched for simple microwaves and can’t find any. reply itronitron 3 hours agorootparentThe IKEA MATÄLSKARE Microwave oven has 4 buttons, I press one to add 30 seconds and another one to cycle through the 4 power settings. That is their mid-range microwave, it has 750 watts of power and the more expensive models seem to have similar controls. Their low end microwave TILLREDA has two knobs, but I've never used it. reply youngtaff 1 hour agorootparentprevOur microwave has two knobs… one for the power setting and one for the time My only criticism of it is there’s no button to cancel the time back to zero and you have to wind the timer knob back reply jayd16 14 hours agoparentprevIt's a nice analogy but imagine trying to thaw something for 10s of minutes with the add 20 seconds button. That would be pretty annoying. reply andy99 13 hours agorootparentI've seen many one-button clocks that just either increment faster or change increment to a larger value if you hold. reply iknowstuff 14 hours agorootparentprevA dial should do fine reply bmitc 14 hours agoparentprevPopcorn even comes with instructions to not use the popcorn button. reply xboxnolifes 1 hour agorootparentOnly because popcorn manufacturers don't want the quality of their popcorn to be judged by the quality of your specific microwave's popcorn button implementation. They have no control over how it's implemented. reply drdaeman 15 hours agoparentprev> But no one will buy a microwave with few buttons. Marketing is the king. Spend resources on memeing your way into people minds, then advertise “we removed the cruft to give you ” (more space, better experience, whatever you can think of, even if it’s a big stretch) as some sort of revelatory breakthrough and bleeding edge innovation - and they’re gonna buy it and joke about “uncool” others’ microwaves with silly extra buttons are, affirming how cool their microwaves are. Worked for a lot of crap on the market. reply hooby 4 hours agoprevOver some decades of doing development work on legacy systems - sometimes by my companies own design, sometimes contract work for a customer - I've seen lots of things that make me believe that certain customers do prefer complex, buggy software for a very specific reason: They can hide behind it. \"I couldn't finish the task on time because the software had a bug\" - sort of stuff. \"I couldn't do X because the software doesn't support Y\", \"The dog ate my homework\", etc. In many cases, it would have been quite possible to design simple, easy and far less bug-prone solutions - but then people working with the software would no longer be able to hide that certain failures might be due to their own incompetence, rather than being a software issue. Therefore - especially in companies with high top-down pressure - people actually prefer working with software that their managers don't fully understand, and that's known for having some bugs and problems. reply tolmasky 16 hours agoprevThere is also a secondary effect where more complex systems generate a bunch of surrounding materials: tutorials, videos, etc. It also creates job security for the people who learn it, as they have a necessary skill and responsibility in the company, as opposed to something that \"just works\" which doesn't require that. reply RetroTechie 15 hours agoparenthttps://en.wikipedia.org/wiki/Full-employment_theorem So many people in IT have a job because if software were constructed that would be both simple & Just Work, those jobs wouldn't exist. reply j45 11 hours agorootparentIt would seem that way, but organizations that are truly about improving and growth would have the people available to help improve other things. reply euroderf 7 hours agoparentprevYou make it sound like Scientology or some such. The analogy might be apt. reply namaria 15 hours agoparentprevBingo. Complexity is vendor lock-in catnip reply astrobe_ 40 minutes agorootparentOf course. If you were running an evil open-source company, you would favor \"solutions\" that generates demand for the services you sell (training, tech support contracts,...) while maintaining the belief that all this is necessary in \"modern\" IT. I think it's Rich Hickey who links in his one of his talks complexity with entanglement through etymology. This entanglement is sometimes also there to bind the customer. Although more often the \"never attribute to malice...\" rule is at work, as it's just easier, cheaper, etc... to let the complexity grow. reply antupis 15 hours agoparentprevYup sometimes it feels like AWS and Azure are like this. reply bbminner 15 hours agoprevIf we are talking about paper reviews, what I am looking for in a paper as a reviewer is neither simplicity nor complexity. I am not even looking for \"novelty\". What I am looking for is a thorough and thought-provoking empirical analysis of a problem. I see plenty of submissions where 1) authors propose a system that is clearly a monstrosity Frankenstein patched together from a dozen of existing ideas - clearly a successful attempt at getting \"bold numbers\" using as many new shiny toys they could get their hands on without analyzing failure modes of either part in depth; 2) a simple modification of an existing method that accedentally improves the performance of the system but still lacks proper empirical or therotical justification of why this modification helps. While the second type of papers has at least some marginal value to the community or the reader, I still find such papers mostly useless. What brings value to the reader is a phd student who stared at the problem long enough to find quantitative and verifiable confirmations to his intuitions about the problem that lead to reproducible observations with predictive power. Ie \"we experimentally verififed that indeed X affects Y exactly though the mechanism Z described in this paper in all cases, this helped us to improve metric A by B%, agreeing with Z\". Not \"we did X, and we saw A increase by B%\", regardless of the complexity of A. Not all reviewers agree with me, sadly. reply nickpeterson 15 hours agoprevI highly recommend the Rich Hickey talk “Simple made Easy”. Complexity doesn’t sell well at all, but easy does. If a company can hire a bunch of people that know how to use ‘foo’ and the industry keeps talking about ‘foo’, they’ll choose it even if foo is a complete boondoggle. See lambda architecture, most Apache projects, containers, etc reply contingencies 15 hours agoparenthttps://www.youtube.com/watch?v=SxdOUGdseq4 reply Nevermark 14 hours agoprevIf only simplicity always meant easy to use. There would be no paradox if it did. One big problem is that for any product/feature not used in isolation in a very controlled context, simplicity is often suboptimal, inflexible and limiting. Complexity is often the result of building one thing that works well in a variety of situations, a lot of interoperable things or features to work (relatively) well together, or one thing with a lot of ways to interface with it. The worst case is all three - which is true for a lot of software. The result is a simpler purchasing choice, buy the most flexible product, but at the cost of far more product complexity than any particular user needs. reply astrobe_ 17 minutes agoparent> One big problem is that for any product/feature not used in isolation in a very controlled context, simplicity is often suboptimal, inflexible and limiting. \"suboptimal\" from the global perspective, because to me a simple solution should be a local optimum for a specific problem. For instance, one could argue that the Unix motto \"do one thing well\" generate lots of specialized programs that, even though they are an order of magnitude smaller than generic programs individually, together they take globally more space for same service level - a symptom of that is e.g. Busybox. For physical devices, the problem is probably more acute, or at least more visible. \"inflexible and limiting\" are terms I can agree on, that's generally how simplification works unless you have a genius idea. I don't see those words as absolute negatives, though, but rather as terms in a trade-off. If the software is open those issues can be mitigated sometimes by hacking; one advantage of starting from a simple (simplistic even), inflexible and limiting solution is that it's easier to evolve - that is to add the necessary complexity. reply euroderf 7 hours agoparentprevAnd thus the emergence and rise of \"opinionated software\". reply pornel 9 hours agoprevI find such laments annoying, because they're full of obvious platitudes. It's easy to sound smart quoting Einstein and Dijkstra. It's cheap to make generalizations, and point fingers at complex solutions when having both the benefit of hindsight, and ignorance about their true requirements. \"as simple as possible, but not simpler\" is always right. Messy solution? You should have made it simpler. Primitive solution causing problems? You weren't supposed to make it too simple. Why didn't you think about making it just perfect? In reality, it's very hard to even get people to agree what is simple, when solutions have various trade-offs. Maybe it is easier to focus on maintaining one complex database, than to have 3 \"simple\" ones, and triple admin work, and eventually end up having to sync them or implement distributed transactions. Something simple to implement now may cause complex problems later. A simple off-the-shelf solution that doesn't fully solve the problem will breed complex workarounds for the unsupported cases, and eventually add complexity of migrating to something adequate. If you didn't correctly predict how a solution will fit all your requirements, you should have simply listened to Einstein. All the advice to \"just\" do something \"simple\" is blissfully unaware that these solutions are not panacea, and it's rarely a plain choice between complex vs simple. Projects have constraints - they may need to work with existing messy systems, inconsistent legal requirements, or changing business requirements. They may prioritize time to market, or skills they can hire for. And there's brutal economics: maybe annual report export is a Rube-Goldberg machine, but it's done once a year, and a rewrite wouldn't pay for itself in 50 years. The discussion about complexity rarely acknowledges that projects and their requirements grow, so something perfectly simple now may become complex later, in a perfectly rational way, not due to incompetence or malice. Storing data in a plain text file may be beautifully simple in the beginning, and become a bad NIH database later. But starting with a database for 3 rows of data would be overcomplicating things too. And there's cost to refactoring, so always using the ideal solution is not that simple either. reply carl_sandland 7 hours agoparentSome complexity is inherent to the problem, but most seems to be incidentally introduced by the realities of deployment (non-functional), configuration (functional) and chaos monkeys (users). There is a particular 'breed' of incidental complexity I see with space cadets and front end developers for sure. Complexity is complex lol. reply Ozzie_osman 16 hours agoprevComplexity sells also because it obscures and overwhelms. \"Mark me down, too, as an adversary of complexity, complexity that obfuscates and confuses, complexity that comes hand in hand with costs that serve its creators and marketers even as those costs thwart the remote possibility that a rare sound idea will serve those investors who own.\" This is John Bogle talking about finance, but I think it's more generally true. reply 3pm 16 hours agoprevNot specifically about ML, but a good paper about unnecessary complexity introduced by a premature 'scalabilitization': The COST of a given platform for a given problem is the hardware configuration required before the platform outperforms a competent single-threaded implementation. Or “You can have a second computer once you’ve shown you know how to use the first one.” –Paul Barham https://www.usenix.org/system/files/conference/hotos15/hotos... reply nelsondev 16 hours agoprevThis idea also applies to software sales. e.g. A bloated, overly complicated, CMS is easier to sell to a company as a solution than a sleeker, cleaner solution. “If you can’t dazzle them with brilliance, baffle them with bullshit” reply masklinn 16 hours agoparentClients will actively seek it out. My boss used to try and make clients understand they could do minor adaptations of their workflows, and adapt a few labels of the product and they’d be done and productive in a few days, and they’d requests weeks of system customisation instead. reply ervine 16 hours agorootparentYep - seems clients have a knack for asking for the most complicated way of doing things. Frustrating as a developer that is already slammed, but good for the business overall. But how many times have we all built extremely complex features that never ever get used. reply ghaff 15 hours agorootparentClients also don't want to be told they'll have to adapt their workflows/processes. A long time ago the COO of our very small company was adamantly opposed to replacing Exchange (which went down with some regularity) with a SaaS because they'd have to make changes to the filing system for their contracts (was before Google had nested labels). The CEO overrode them but the issue was clearly that they didn't want change. reply knallfrosch 9 minutes agorootparentA client that adapts their workflow to a readily available software and customizes the rest isn't a client at all. First: I think you guys misunderstand the very nature of business. All customers have problems they want YOU to solve in exchange for money. The rest of the people just don't contact you. It's kind of the \"survivorship bias.\" Secondly: Within an organization, if a project manager has a budget and is tasked with solving a problem, \"let's make IT adapt our systems\" is not an acceptable solution. Most of the time all you have is money, not software developers or IT staff. They are hopefully working on their core product and not on billing or sending emails. Thirdly: Don't underestimate the ongoing support that stems from point two. If you ever make adaptions to someone else's software, you know what's going to happen when there's an incident and you call support: They are going to blame your customization, they are not going to understand your needs or customization and they'll kindly string you along until you fix it yourself using precious staff ressources. It is much easier to let someone else do everything from start to finish and pick up the phone when there's a problem. reply hgs3 15 hours agorootparentprev> seems clients have a knack for asking for the most complicated way of doing things Sometimes clients don't know what they want. Sometimes you have to show them. “If I had asked people what they wanted, they would have said faster horses” ~ Henry Ford. reply mindwok 2 hours agorootparentprevThe enterprise buying process is geared to achieve consensus and reduce the risk of purchasing the wrong thing. As a result, dozens of groups get invited to participate and all throw their requirements into the ring. Vendors then do contortions to try and sell to these customer, and you end up with software that tries to be everything to everyone. reply zeroCalories 15 hours agoprevI think another aspect of complexity is that your customers, either internal or external, have a very specific idea of what they want, even if their idea is trash. So your product needs to be flexible(complex) enough to support all the use cases conceivable. Going to a customer and saying \"trust us, do it this way\" will make them lose interest fast. reply citizenkeen 15 hours agoparentI work on a small internal ERP, and our new UX guy said “nobody needs all this info”. And I said “agreed, nobody uses more than six columns. But the key stakeholders can only agree on 5.” I think that’s the key: nobody needs 20 buttons on a microwave, but some people love defrost or popcorn or whatever. reply dang 8 hours agoprevDiscussed (a bit) at the time: Simplicity Is an Advantage but Sadly Complexity Sells Better - https://news.ycombinator.com/item?id=32491079 - Aug 2022 (6 comments) reply marcus_holmes 6 hours agoprevAgree with everything said in TFA. Except maybe toning down the exhortation to use other people's code so much. I totally agree that using existing solutions is very often the simplest solution; I would not want to rewrite PostgreSQL or any crypto libs. But too many dependencies can get messy; there is definitely a line at which writing your own code specifically for this situation is simpler and better than importing a more generic dependency that is much larger and more complex than it needs to be for your use case (because it covers more than just your use case). Or, (e.g. Left Pad), where the dependency is not actually easier than just writing the code yourself. Importing any dependency carries with it some complexity because it means integrating someone else's code into your own, with the ensuing security implications and version problems. It is not always more complex to write your own code. My general rule of thumb is that if it'll be quicker for me to code a solution to this specific problem than it would be to learn the API for an import and integrate it, then I'll write the code myself. reply cs702 13 hours agoprevI've seen this firsthand: > A common point raised by ML reviewers is that a method is too simple or is made of existing parts. And this is self-evidently true: > ...simplicity is a strength, not a weakness. People are much more likely to adopt simple methods, and simple ones are also typically more interpretable and intuitive. It happens on a lot of different fields. For instance, a lot of investment management firms offer complicated investment strategies with high fees, even if a simpler strategy would do just as well. Quoting Warren Buffett: > Investors should remember that their scorecard is not computed using Olympic-diving methods: Degree-of-difficulty doesn't count. If you are right about a business whole value is largely dependent on a single key factor that is both easy to understand and enduring, the payoff is the same as if you had correctly analyzed an investment alternative characterized by many constantly shifting and complex variables.[a] --- [a] http://www.berkshirehathaway.com/letters/1994.html reply austin-cheney 16 hours agoprevIncompetence also qualifies billing a client substantially more time for a given work effort completely without fraud. reply scrlk 15 hours agoprevCompared to other engineering disciplines, it seems like software is one that is most susceptible to veer towards complexity. Is it the ease of iteration? The relative youth of software engineering? reply Xeamek 15 hours agoparentRelates to ease of iteration, but imo the biggest factor is no real responsibility when things breaks and how it is expected for software to break. reply dave4420 15 hours agoparentprevRelative lack of physical constraints? reply 01HNNWZ0MV43FF 15 hours agorootparentBingo. Complex machines have more parts that wear out and need attention. Complex software, if it's well-written, can actually Just Work, and doesn't wear out. reply analog31 6 hours agoparentprevInvestor optimism, peer pressure, and FOMO play roles. reply rglover 12 hours agoprevI built a full-stack JS framework [1] that I thought would be a hit. As best as I can tell, because it lacks the complexity/word salad of existing solutions, it's mostly been ignored despite being (imo) an elegant solution to a long-standing problem. [1] https://cheatcode.co/joystick reply degurechaff 7 hours agoparentI think not using common license like GPL, BSD or MIT make people think twice before even trying the framework. reply rglover 6 hours agorootparentThe license is pretty straightforward and fills in the gaps where the others fail to protect OSS creators (leading to headaches like w/ Redis). Perhaps it's too idealistic to think people will just read it and see if it matches their needs? reply kragen 2 hours agorootparentno, i've already spent the necessary hours of debating to what extent the agpl3 is compatible with cc-by-sa 4, i don't want to invest those hours again in a license that's used by only one project and, if history is any guide, probably contains important mistakes in how it's written and will need to be redrafted after those mistakes are noticed. i don't have a legal department reply golergka 3 hours agorootparentprevI'm not a lawyer, so I'm not risking my company's legal liability with my ability to comprehend a legal document. reply rglover 3 hours agorootparentAssuming the project/tool was interesting and that was a concern, why not just forward it to your legal department (or manager) to get clarification on if it's an option? reply j45 10 hours agoparentprevThis is an inviting read, and I think would resonate with many. I avoid javascript for the brittle timesuck it can become maintianing the past while shaking one's fist at other legacy systems. I've never known this to be a problem because I avoid systems that do this to my time, unless it's absolutely unavoidable. It makes me much more productive to use tools that work, and continue to work, so I spend more time with problems than rolling a temple to my own veneration. It might work for others, and it doesn't make it right, it might not work for me, and that doesn't make it wrong. It's OK not to be for everyone, but your people will find whoever you are being. Maybe try it as the content for your landing page. Lots of one liners that resonate. reply rglover 8 hours agorootparentThanks for the feedback/tips, I'll give it a shot. reply tipiirai 2 hours agoprev> Simplicity is an advantage but complexity sells better Exactly my feeling with TypeScript reply spixy 1 hour agoparentexcept JavaScript is not an advantage at all reply hyperthesis 7 hours agoprevComplexity signals innovation: an invention that is merely a \"workshop improvement\" won't get patented. Complexity signals non-obviousness. Too easy -> easily copied -> and no long-term competitive advantgage. Complexity signals hard. reply kouru225 12 hours agoprevIn professional settings, people care only about complexity. In the informal media world, people care only about simplicity. The most simple narrative wins out every time. We’re in a world of extremes. reply hyperthesis 7 hours agoprevThe solution to a problem can be simple. But now we want other things. e.g. car=transport, then: speed, efficiency, style, non-polluting, safety, price, a/c, touchscreens, self-driving etc. The boundless complexity of human desire. reply hyperthesis 7 hours agoprev\"There's no bragging rights to your software, because it's too simple to use\", a developer criticized my product. This reduced its viral spread, though managers liked it. reply quantum_state 16 hours agoprevThis is one of the reasons that IT is in a state of ruin … reply mro_name 15 hours agoprevsimplicity is the habit of zen, toaists, sufis and franciscans or the school of wirth - the mystics. Minorities each. It's just not appealing to the masses and the proponents don't really care about mass-recognition, either. But they go on nevertheless, stubborn as they are. Edit: Oh, and the razor Occam was a franciscan, too. reply vbezhenar 12 hours agoparentIMO simplicity was what sold iPhones to the world. People love simplicity, but they need to be sure that they paid for quality simplicity. So simplicity must be accompanied by something that makes it look not cheap. reply 1vuio0pswjnm7 10 hours agoprevSoftware developers may embrace complexity for the purpose of commercial benefit and suffer all its disadvantages, not to mention passing on those disadvantages to users. However non-commercial users, hobbyist programmers, who derive no commercial benefit from complexity are free to reject it and its disadvantages. For example, I choose on a daily basis to use simpler, noncommercial software that I compile myself. The use of such software is routinely dismissed, discouraged and even attacked by many software developers commenting on HN. Certainly, its use in place of more complex alternatives does not benefit their interests. It does benefit mine. I like (relative) simplicity. Each is free to do as they please. If one prefers complexity, as many do, then there is no shortage of alternatives to choose from. Complexity is booming. reply galdor 16 hours agoprevIn technical organizations (all organizations really), simplicity is also a hard sell: you need people in charge with the ability to say \"no\" to a lot of ideas. And no one wants to be on the receiving end of a \"no\". Those who favour simplicity will always be outnumbered, and their position will be untenable unless the entire top management team agrees. Good luck with that. It is also one the reasons why the BDFL model works so much better: you need the ability to say \"no\" a lot. reply dave4420 15 hours agoparentI’ve not found this to be the case. I’ve argued at work before for us to deliver a simpler subset of a feature, that delivers most of the value sooner, then to assess later whether we actually need the rest of the feature. This is also why I’m confident about my continued employment in the age of AI: CEOs are always asking how we can deliver faster. They might not be able to afford more software engineers, but they can always use more. reply IshKebab 14 hours agoprev [–] This is a very good analysis and doesn't fall into the trap most commentaries on stuff like this do: moaning about how something is bad without acknowledging any reasons for it, as if people are just arseholes. C.f. Electron, heavy websites, advertising, non-replaceable batteries, etc. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article delves into the preference for complexity in academic papers, promotions, and product development, while advocating for the advantages of simplicity.",
      "It outlines how complexity is often seen as more prestigious but stresses the benefits of simplicity, such as better understanding, adoption, communication, and maintenance.",
      "The author encourages a transition towards appreciating simplicity in problem-solving and decision-making, underscoring the significance of opting for the simplest solutions to intricate issues."
    ],
    "commentSummary": [
      "The article delves into the delicate balance between simplicity and complexity in software development, highlighting the significance of practical and efficient solutions over unnecessary intricacies.",
      "It discusses the challenges of addressing complex issues with simple approaches, the influence of complexity on user experience, and the dilemmas project managers face in achieving equilibrium.",
      "Emphasizes the advantages of simplicity in coding and the difficulties in promoting new software frameworks, advocating for a nuanced problem-solving approach in both formal and casual environments."
    ],
    "points": 227,
    "commentCount": 135,
    "retryCount": 0,
    "time": 1714929427
  },
  {
    "id": 40264669,
    "title": "Japan to Test Tooth Regrowth Medicine from Sept. 2024",
    "originLink": "https://mainichi.jp/english/articles/20240503/p2a/00m/0sc/012000c",
    "originBody": "World's 1st 'tooth regrowth medicine' to be tested in Japan from Sept. 2024 May 3, 2024 (Mainichi Japan) Japanese version Katsu Takahashi, head of the dentistry and oral surgery department at Kitano Hospital, second from left, and other members of the research team, hold a news conference at the hospital in Osaka's Kita Ward on May 2, 2024. (Mainichi/Yosuke Tsuyuki) OSAKA -- Clinical trials of the world's first \"tooth regrowth medicine\" are set to commence in September at Kyoto University Hospital, researchers announced here on May 2. Once the medicine's safety is confirmed, it will be given to patients congenitally lacking a full set of teeth to confirm its effectiveness. The researchers hope to commence sale of the medicine in 2030. Congenital tooth deficiency is believed to affect about 1% of the population. The absence of six or more teeth, a condition known as oligodontia, is believed to be hereditary, and is said to affect about 0.1% of the population. A tooth that grew in a mouse with a congenital tooth deficiency, is seen growing after it was given the medicine, in this photo provided by Kitano Hospital. According to Kitano Hospital in Osaka's Kita Ward, which is involved in the study, the first phase of the clinical trials will run from September this year to August 2025. The medicine will be administered intravenously to healthy individuals to confirm its effectiveness, with 30 males between the ages of 30 and 64 taking part. The subjects must be missing at least one back tooth so that there will be no problem if the medicine takes effect and a tooth begins to grow. No major side effects have been confirmed in animal studies to date. In the next stage, the medication will be administered at Kitano Hospital to patients with congenital tooth deficiency. Researchers plan to limit the subjects during this phase to those between the ages of 2 to 7 who have at least four teeth missing from birth. The tooth regrowth medicine deactivates a protein called USAG-1, which inhibits the growth of teeth. The team believes that in the future it may be possible to grow teeth not only in people with congenital conditions, but also in those who have lost teeth due to cavities or injuries. This photo provided by Kitano Hospital shows the front teeth of a ferret that was given the medicine. It had six teeth at first, but a seventh one, pictured at center, grew in. Lead researcher Katsu Takahashi, head of the dentistry and oral surgery department at Kitano Hospital, commented, \"We want to do something to help those who are suffering from tooth loss or absence. While there has been no treatment to date providing a permanent cure, we feel that people's expectations for tooth growth are high.\" (Japanese original by Yosuke Tsuyuki, Osaka Science & Environment News Department) Font Size S M L Print Timeline Go to The Mainichi Home Page Related Articles World's 1st 'tooth regrowth' medicine moves toward clinical trials in Japan No toothbrush available? Japan dental experts give tips on oral care after quake Nagasaki hospital suspends oral surgery after dentists pull wrong teeth 3 years in row",
    "commentLink": "https://news.ycombinator.com/item?id=40264669",
    "commentBody": "First 'tooth regrowth medicine' to be tested in Japan from Sept. 2024 (mainichi.jp)221 points by elorant 20 hours agohidepastfavorite84 comments paulgerhardt 17 hours agoI once read an off hand comment[1] from a dental scientist on Reddit that NovaMin® (calcium sodium phosphosilicate) was shown with strong evidence to reverse tooth decay and promote remineralization but due to complex licensing issues was made OTC in Sensodyne toothpaste everywhere but the USA. I think the effect is subtle but present and slightly more effective than just fluoride by itself. I travel often enough that I find myself looking for and buying Sensodyne with novamin but without whitening agents when I’m abroad but I understand others source it in the US online. Do check the ingredients - it’s in about 1/3rd of the Sensodyne packages I pick up off the shelf outside the US but certainly not all. I follow the topic very casually - I understand hydroxyapatite, fluorapatite, and biotin were initially reported to be even better (but only through self-reported studies such as Biomin.) I couldn’t tell you why or if there is now more credibility there nor where to source it if it is. BioMin USA’s anti-fluoride stance raises a lot of red flags for me and is probably what turned me off their brand when I looked into it years ago. GSK Sensodyne sells formulations with fluoride and novamin - I wouldn’t use toothpaste without fluoride. (Edit: I see BioMin UK sells toothpaste with fluoride.) [1] https://elemental.medium.com/why-is-the-internet-obsessed-wi... - Medium Paywall link but gets the general point across reply analyte123 17 hours agoparentYou can buy several varieties of hydroxyapatite toothpaste on Amazon now. Before 2021 or so it was a bit of a pain - a couple Japanese and Singaporean brands with spotty availability. Edit: as alluded to by sibling comment, there are confusing claims about the exact size and structure of the hydroxyapatite ingredient used. Several brands claim that their particles (?) are smaller than those used by Novamin or by other brands and therefore more effective. The concept of remineralizing the surface of existing teeth is totally different than what this article is talking about, though. reply skulk 17 hours agorootparentour saliva already contains hydroxyapatite. What's the reasoning behind including it in a toothpaste? Saliva doesn't contain enough? reply kergonath 17 hours agorootparentOur teeth enamel is in chemical interaction with our saliva. Lower pH favour the dissolution of the apatite (attacking the teeth), and higher pH favour its precipitation (growing enamel). So it’s true that the components are in our saliva as well, in itself it is not enough to make it grow. These toothpastes change this by providing lots of the building blocks of apatite, saturating the saliva and promoting growth, but also by increasing the pH. It tends to decrease after a meal (or particularly after drinking sugary drinks, which tend to be awfully acidic), which is why it’s useful to have the toothpaste stabilising it. reply financltravsty 17 hours agorootparentprevNot with a modern lifestyle, no. reply PeterHolzwarth 17 hours agoparentprevI've heard and read the same. I noticed this review of studies which seems to indicate benefits are not as strongly present as word-of-mouth reputation may indicate. I suppose if it was an unambiguously beneficial compound, American toothpaste manufacturers would have been all over it. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7068624/ reply yeureka 17 hours agoparentprevI used to use Novamin containing toothpastes, but now there is a new compound which is much better, biomin: https://www.biomin.co.uk/ reply o_____________o 17 hours agorootparentCitation for that claim? reply jacoblambda 16 hours agorootparentAt least in the US, the argument is that when pressed by the FDA, Novamin was pulled from the market by GlaxoSmithKline instead of doing the required trials whereas Biomin went through and passed all the trials. And since it's actually gone through the process, there are studies out that compare it to other hydroxyapatite solutions. I've linked one study which compares their two variants against Apacare and karex (two HAp toothepaste which have studies of their own that compare against the others before them like Novamin). The study shows that Biomin C (the one without Fluoride) is comparable to karex (the one without fluoride) and other similar HAp toothepastes. HAp + fluoride (like novamin) marginally outperforms it. So if you use fluoride (such as by also using a fluoride toothpaste or living in an area with fluoride in the water) the difference should be negligible. Importantly though for people in the US, Biomin C is available here. Biomin F is wrapping up the FDA approval process however it does seem to generally outperform all other formulations since the fluoride is actually part of the bioglass itself rather than simply an additional active ingredient in the suspension. TLDR: Biomin C is within marginal differences to comparable no-fluoride HAp toothpastes on the market but Biomin F outperforms other HAp toothpastes that contain fluoride since the fluoride in Biomin F is delivered via the same mechanism that handles remineralisation of the other elements. And to my knowledge Biomin F is the only HAp toothpaste to do that so far. Also Biomin C is the only HAp toothpaste available in the US (with Biomin F apparently soon to follow). https://doi.org/10.21608/adjg.2021.66174.1346 reply yeureka 16 hours agorootparentprevYou can easily look up some studies, but for me personally it has been the impact on sensitivity. It's basically gone since I've been using it. reply flotzam 16 hours agorootparentprevUnbearably sweet though! I tried BioMin once and worried that using it long term would burn out my taste receptors reply yeureka 16 hours agorootparentThe kids version is as effective and less sweet. But more expensive. Another benefit is that it doesn't contain titanium dioxide. reply 2Gkashmiri 17 hours agoparentprevhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC7068624/ says it doesnt but also says better trials should be done in future reply marcelsalathe 17 hours agoprevI'm a bit puzzled why one would hold a press conference for a trial that hasn't happened yet - especially as this seems to be a phase 1 trial to test safety in humans. The development of medical interventions typically goes through a number of stages. The earliest stage is called the pre-clinical phase, where a candidate intervention is tested outside of humans, either in vitro or in animals. The images of the mouse and ferret teeth suggest this has been done. Phase 1 is when the intervention is first tested in humans, typically in small groups of usually dozens of participants. The aim here is not to evaluate efficacy - phase 1 studies are often too small for that - but to assess tolerability and safety, and to find the optimal dosing with respect to side effects. If an intervention appears safe in certain doses, it can then be evaluated for initial efficacy and continued safety in a phase 2 trial, which is larger (usually hundreds of participants). Think of it as a kind of pilot trial to see if the intervention has the desired beneficial effects without serious negative side effects, and to identify the dose with the best benefit/risk ratio. About half of the studies get past this stage. Those that do can enter phase 3, which is the full efficacy and safety assessment of the intervention. Again, about half of the phase 3 trials eventually make it to market. Of course, the intervention first needs to go to the health authorities for regulatory approval before it can be offered on the market. For an interesting study on costs during these phases, see e.g. https://pubmed.ncbi.nlm.nih.gov/32125404/ I'm obviously very excited about medical progress but if we had a press conference for every planned phase 1 trial we wouldn't be doing much else ;-) reply reustle 17 hours agoparent> I'm a bit puzzled why one would hold a press conference for a trial that hasn't happened yet This is something Japan loves to do. We joked during covid that they kept making announcements that they were planning to have a meetings to prepare announcements. reply ranger_danger 16 hours agorootparentIronic there is so much red tape yet having original ideas are still frowned upon. reply dingosity 16 hours agorootparentMeh. My experience working there wasn't so much that original ideas were frowned upon as much as thinking/saying you're better than your group-mates for having them was. It's a subtle difference, but I saw plenty of novel ideas take root, but only after an excruciatingly long period of socialization. In the states and in most of Europe, it seems if you have a good idea, you just blurt it out and people say \"hunh. that sounds like a good idea. let's do that thing.\" But Japan and Sweden required A LOT of planning and hemming and hawing about whether the new idea was a good thing to do. reply iainmerrick 16 hours agorootparentIn the states and in most of Europe, it seems if you have a good idea, you just blurt it out and people say \"hunh. that sounds like a good idea. let's do that thing.\" It might vary from company to company, but I find the response is more “you should do that thing”. People are too busy with their own ideas to waste time building yours. reply eska 15 hours agorootparentprevWhat you’re referring to is called nemawashi in Japan , literally “digging around the roots of a tree”. It’s formal business etiquette understood by all office workers. One shouldn’t blurt out ideas to an unprepared group, but spread the idea around before especially to the superior. If they reject the idea they don’t have to embarrass you in front of others, or lose face in public for being taken by surprise. reply ezconnect 16 hours agorootparentprevI you live long enough in Japan this is just concept as viewed by an outsider. Their dollar store is full of almost useless ideas put out in the market. Even the blue LED was discovered because a company owner invested millions and his scientist was not afraid to try new ideas. You will find all sorts of camera styles on their electronic stores. Even their appliances have all kind of categories and you wont see the west even trying to put it out in the market. reply scirob 17 hours agoparentprevBiotech companies go public incredibly early compared to tech not sure if these guys have yet but it would have a huge impact on their stock price and likely hold of getting into conversation with a major pharma Co that can pay for the later trials reply throwup238 16 hours agorootparentThis is the correct answer. Biotech VCs invest very large amounts based on early scientific results and the vast majority of biotech companies go public before they even have revenue because they can't legally charge a penny until regulatory approval. This is how the pharmaceutical has been offloading their increasing R&D expenses onto the public since the patent cliff picked up speed in the 1990s. A major PR push exposes them to as many investors as fast as possible both at the VC and public level. When the science is particularly solid, this process can happen fast and involve ridiculous amounts of money (i.e Sofosbuvir discovered 2007, first tested 2010, bought for $11 billion in 2011, approved 2013). reply mathgeek 17 hours agoparentprevI assumed it’s to drive up interest among potential investors. reply M_bara 17 hours agoprevThis is something life changing for some of us. I grew up in an area where water has a lot of fluoride and we didn’t get to know until I was 16.. side effect is I have very brittle teeth.. I’ve shattered a couple due to bad rice (had a small stone in it), lentils, sliver of bone … Now, the kicker is Colgate et al have been marketing fluoride toothpaste to the same region to people facing fluorosis knowingly - profits above all else. so if this treatment comes through, I’ll be lining up! reply lgleason 17 hours agoparentI thought the fluoride was supposed to be good for teeth, I guess too much has the opposite effect. TIL reply coffeebeqn 17 hours agorootparentYou’re not supposed to swallow it. It builds up in the body (in teeth mainly?) reply chaorace 16 hours agorootparentTo be clear: drinking water sources often contain flouride and swallowing water is generally not optional. The source can be natural or artificial, but the outcome will be the same if you get an unusually excessive amount during developmental years. As with all nutrients, too much will inevitably become bad for you (thankfully, in the case of flouride, the therapeutic index is quite generous) With all of this being said, once you're an adult there's no longer any particularly viable pathways from the bloodstream to your outer teeth, so flouride in the body becomes mostly disconnected from flouride in the teeth (and vice versa). Flouride's effects on the body are less well understood, albeit only because we struggle to measure such apparently small effects on a general population. reply mrits 17 hours agorootparentprevMy mom was obsessed with flouride in water so I actually asked my dentist about it. According to an actual health professional it is quite good for the teeth and excessive amounts only affect the appearance not structure. reply AuryGlenz 16 hours agorootparentToday’s the day you find out “actual health professionals” can be wrong or no longer up to date. That’s why second opinions on important things are vital. reply mrits 15 hours agorootparentI'll take the word of the health professional, thanks reply aatd86 11 hours agorootparentGood luck. I have had my fair share of wrong diagnostics myself. Turns out health professionals are themselves skeptics... (covid showed us already) Well it's science and at some point, science claimed that earth was flat so... Good luck again :D (point is, don't trust blindly) reply Sharlin 16 hours agorootparentprevFluoride catalyzes remineralization of dissolved calcium and phosphate atoms back to hydroxyapatite; this also happens without fluoride, but less efficiently. In addition fluorine ions can substitute the hydroxide ions in hydroxyapatite, forming fluorapatite which is more resistant against acid. reply guerrilla 16 hours agorootparentprevThat actual health professional was wrong. It depends on the exposure. https://en.wikipedia.org/wiki/Dental_fluorosis reply mrits 15 hours agorootparentI recommend reading that again, or better yet, ask your dentist to explain why you are wrong reply guerrilla 14 hours agorootparent> The severity of the condition is dependent on the dose, duration, and age of the individual during the exposure. > Severe : 5 : All enamel surfaces are affected and hypoplasia is so marked that the general form of the tooth may be affected. The major diagnostic sign of this classification is discrete or confluent pitting. Brown stains are widespread and teeth often present a corroded-like appearance. reply iamflimflam1 15 hours agorootparentprevhttps://www.nhs.uk/conditions/fluoride/ Millions of people in England receive fluoridated water. This means fluoride has been added to bring it up to around 1mg of fluoride per litre of water, which is a level found to reduce tooth decay levels. Now I know there is a running joke around the English having bad teeth. But I imagine if millions of people were having issues we’d know about it. reply therealdrag0 13 hours agorootparentAdded fluoride is done at safe levels. But some natural water sources have TOO MuCH fluoride which is unsafe. https://en.wikipedia.org/wiki/Fluoride_toxicity#Bones reply marcosdumay 17 hours agorootparentprevToo much of most things is bad. reply adaml_623 16 hours agorootparentToo much of anything will kill you... reply dingosity 16 hours agorootparentprevI always thought we started fluoridation of water so the Soviets couldn't figure out how much uranium (and later plutonium) we were producing. UF6 is a bi-product of uranium enrichment and it's fairly straight-forward to estimate how much we're producing by taking water samples at the mouths of various rivers. But if everyone is brushing their teeth with fluoride, then it's a lot harder to accurately measure how much is due to enrichment. This is very clearly a conspiracy theory, but water fluoridation is one of those topics that seems to attract them. reply jebby 17 hours agoparentprevFluoride is good for your teeth. Not sure what you're talking about, but it's scientifically inaccurate. Also, fluorosis is a cosmetic issue, it doesn't weaken your teeth. And it happens when you're growing your permanent teeth when your parents forget to teach you to not swallow your toothpaste. It wouldn't affect you now. If your water is over-fluorinated, you have far bigger problems that stem from your local government. reply Retric 17 hours agorootparentYou’re wrong on both counts. The geology of many areas cause excess fluoride in well water without any government intervention, which can then become worse when using fluorinated toothpaste. It’s rarely a significant issue in the US, but gets far in some countries. “These sources include drinking water with fluoride, fluoride toothpaste—especially if swallowed by young children” Ie: swallowing makes it worse but the point of fluoride in toothpaste is to be absorbed, so some will get absolutely even in those who already have issues. https://www.cdc.gov/fluoridation/faqs/dental_fluorosis/index.... reply mb7733 17 hours agorootparentSeems to be a cosmetic issue only? reply AuryGlenz 16 hours agorootparentSome studies have shown it (high fluoride, and possibly too low of fluoride too) affecting IQ. It’s a shame if you mention anything about possible negative effects of fluoride you get lumped in with the crazies. reply jajko 16 hours agorootparentYeah rat poison can't harm humans regardless of form, concentration, age or literally any possible factor. Trust us with lives of your kids, we say so. (just to be clear I am a rational science freak, but my kids have higher priority and we know scientists and corporations have messed up more than once, not going into 'just trust us' with literal poison just because it has good side effects on teeth) reply Retric 17 hours agorootparentprev> cosmetic issue only? Only at low levels: “Moderate and severe forms of dental fluorosis, which are far less common, cause more extensive enamel changes. In the rare, severe form, pits may form in the teeth.” reply tredre3 17 hours agorootparentprev> And it happens when you're growing your permanent teeth when your parents forget to teach you He said it happened as he grew up. Wikipedia says almost half of Americans have at least mild fluorisis, there's no need to blame the parents when an environmental/governmental cause is so readily established... > Also, fluorosis is a cosmetic issue That's what I thought too but Wikipedia also disagrees on this count: The pits, bands, and loss of areas of enamel seen in severe fluorosis are the result of damage to the severely hypomineralized, brittle and fragile enamel which occurs after they erupt into the mouth. reply yarg 17 hours agorootparentprevFluorosis is not a cosmetic issue - it can be severe enough that it impacts the strength of the skeleton. Even if that was not the case, you'd need to prove that it's better than hydroxyapatite when applied topically, which (assuming effective) delivery will obviously not be the case. https://pubmed.ncbi.nlm.nih.gov/3295994/ reply jyxent 17 hours agorootparentprevAnybody who lives in a rural area with a well could have higher levels of natural occuring fluoride in their water too. reply AuryGlenz 16 hours agorootparentYou could, though at least when I bought my house a well test was required. reply therealdrag0 13 hours agorootparentNot everyone has such regulation. There’s a problem with fluoride toxicity in India for example. reply darkerside 18 hours agoprevThis is for growing in teeth that never properly grew in the first place for genetic reasons. It's not a cure for your cavities. Don't get your hopes up! reply jupp0r 18 hours agoparentThe article states \"The team believes that in the future it may be possible to grow teeth not only in people with congenital conditions, but also in those who have lost teeth due to cavities or injuries.\" reply al_borland 17 hours agorootparentI first read about scientists making progress on regrowing teeth about 20 years ago. At the time, I thought this would be a future I saw and crowns would not be something I’d have to deal with. 20 years later, I no longer expect this to be something I experience in my lifetime. reply croes 16 hours agorootparentprevBelieves... it may be possible That isn't really strong confidence. reply animex 18 hours agoparentprevOh Wow, I actually still have a baby tooth @ 50+ years old because an adult tooth never grew behind it. My dentist has been warning me that eventually I'll need to bridge/implant it but I wonder if I would be a candidate for this treatment. Depending on the cost too. reply smeej 17 hours agorootparentMy brother is in a similar situation, though younger, and it's especially weird in our family because my dad grew a full extra tooth near the front for no apparent reason, and his brother regrew all four wisdom teeth after having them extracted, so you'd think my brother would err on the side of too many teeth, not too few! (We're a bit screwy in general, though. My dad has an extra vertebra, besides the extra tooth, and I managed not to get enough vertebrae, but I have 1.5 extra ribs.) reply adaml_623 16 hours agorootparentAnd that's just the oddities you've noticed reply smeej 9 hours agorootparentYeah, but I will say we tend to pay more attention now. We warn radiography folks that all may not be as expected, so they join us in playing, \"What's weird about this skeleton?\" reply jwells89 17 hours agorootparentprevI’m curious about this too, also having an adult tooth that never developed. reply Ueland 17 hours agorootparentStrangely enough, still possible. My dad was in this sixties when he got his wisdom teeth. reply ThinkBeat 18 hours agoparentprevsurely if you can regrow an entire tooth that never existed. You can make teeth heal? Or rip it out and grow a new one? reply XorNot 17 hours agorootparentIt needs the tooth nucleation point to exist at the moment: it's where the truth starts growing from during development. reply jwells89 17 hours agorootparentAs a layman it seems like the natural follow-up is to figure out how to grow tooth buds from stem cells and implant them, which in theory would allow people to grow whole new sets of teeth. reply Brian_K_White 18 hours agoparentprev\"The team believes that in the future it may be possible to grow teeth not only in people with congenital conditions, but also in those who have lost teeth due to cavities or injuries.\" reply gwbas1c 16 hours agoprevI have a lot of congenitally missing teeth. Dental implants were very, very expensive; and I had to go through orthodontia twice. I hope this treatment is significantly cheaper. reply poopsmithe 16 hours agoprevYeah I need that. I floss 1-3 times a day, use a water pick and brush 2-3 times a day but it doesn't really matter. My body attacks itself and I've ended up with bone loss in my teeth. I'm in my 30s and I have to do periodontal care every 3 months. I foresee losing my teeth before I'm 50 unless I can get my hands on something like this. reply Madmallard 10 hours agoprevHope something develops in this front. I grew up drinking soda with bad teeth genetically. I also had a period when I was seriously ill where I was barely brushing. The teeth are swiss cheese and I wish there was something I could do other than rip them all out and look 20 years older. I'm only in my early 30s so it's really distressing. :( reply Theodores 16 hours agoprevThere is a lot of money in the tooth illness industry. However, none of my dentists ever told me to just quit all added sugar, which I did 'accidentally'. All of that expense and missing teeth was all for nothing, all I needed to do was homecook all my own food and not eat added sugar. Who knew it could be so easy. Having excellent oral hygiene with gums grown back, no sensitive teeth or much need to brush teeth is an unusual and unexpected self-esteem superpower. I would not give it up just to eat cake or anything else contaminated with added sugar. Off topic, I know, but, anyone wanting tooth regrowth, or with $5K root canal work and dental hygienist bills, just avoid all processed foods and all added sugar for a month. You don't have to ask permission from the doctor or the dentist to do this, but, if you do, then you might never need to see them ever again. reply yeureka 13 hours agoparentMy father doesn't eat sweets. He has a very healthy diet - whole grains, vegetables, etc. Never had a cavity nor gum disease. Because of that he never went to the dentist to have a check up. He ended up loosing all his teeth from hardened plaque below the gum line. His teeth just fell off, one after another, because he never learned good dental hygiene. reply chaorace 16 hours agoparentprevI'm with you when it comes to advocating against added sugars and spreading the word about how surprisingly resilient teeth are when given a good environment... but I do think you've hurt your own argument by overstating the case. First: Calculus forms within hours of eating just about any meal. Sugars make this much worse, but to a certain extent it is unavoidable. Regular appointments with a hygienist will improve your long-term dental health regardless of diet, though the necessary frequency can be decreased in many cases with a good diet. Second: Please don't stop brushing your teeth, especially if you want fewer dentist appointments. It's one of the cheapest and safest investments you can possibly make in your body. Just because you can't feel the plaque doesn't mean that it's not there. Third: Added sugars are just really, terribly bad for you in general. I'm speaking now to those that don't care much about their teeth: you can do so much for your long-term health by simply checking for this one thing on labels. I know it's an odd concept to think of added sugars as being so much worse than \"natural\" sugars when they're fundamentally the same molecules, but it's true -- the differences in solubility/bioavailability have a very outsized impact. reply ornornor 14 hours agorootparentHow do you tell of something has added sugars or not? The nutritional facts all lump it under “carbohydrates, of which sugars” without making a distinction between added sugar and the sugar that comes with the ingredients. reply chaorace 14 hours agorootparentThe label was updated in 2016. Please refer to the official FDA guide to the updated nutrition facts label[1], item #4. [1]: https://www.fda.gov/media/135197/download reply ornornor 13 hours agorootparentNice, we don’t have this in Europe. It’s still all together. reply chaorace 13 hours agorootparentMy mistake. I honestly thought we were one of the very last places to implement this change -- I know the viewpoint is still overly America-centric... just not in the usual way lol reply Theodores 11 hours agorootparentprevInitially this is hard but then you soon realise that you are never buying processed food ever again and everything is a single ingredient food. There may be one or two exceptions and often tinned things have a preservative or an acidity regulator in there too. What this means is that there is no puzzling over food labels. If you are buying carrots then they are just carrots. Initially this seems a bit extreme as there seems to be nothing in the supermarket to buy. On my most recent trip I saw some of my former 'friends' including what was once my favourite frozen pizza. There was nothing toxic about it, once you have embraced home cooking, then nothing processed or ready made is satisfactory. In the case of the pizza, my home made pizza will be in a different league of taste, it might not be to everyone's liking but, for me, it will be like being on a drug high. I can't get that buzz from store bought food and it certainly does not list 'natural opiates released per bite' on the nutrition labels. When you seriously get into buying the vegetables, there is always so much that you have not bought or cooked with before. These things have always been there and they are not weird, but, for some reason you have not decided to cook with them in new and exciting ways. Processed meals mean you don't have to. Then you start mixing in herbs and all of the nutrition labels are left far, far behind. They have different meaning with processed food to real food made with single ingredient items, most of which are vegetables. Carbohydrates also shift from the cheap ones in processed food to being much better quality and best described as 'nutrient rich fibre'. reply Theodores 11 hours agorootparentprevI take your points, including #2. The thing is that if brushing your teeth means a world of pain then you do it out of obligation and it is not a happy experience. However, if your teeth are good because there is no sucrose that ever goes anywhere near them in 'added form', then it becomes a pleasure to give them a quick polish. My point still stands though, there is a lot of money made from tooth related FUD and sugary food. I just wished a dentist had spilled the beans to me many decades ago that it really is simple to have excellent oral hygiene just be avoiding the added sugars, and, to a lesser extent, the processed food. Nowadays my teeth always feel good. I used to eat things like biscuits and crisps (chips) for my teeth to not feel great the next day, even if I had thoroughly brushed my teeth the night before. By every metric I just find the situation very different, the gums have definitely 'grown back' and I am sure some tartar is now below the gumline. If eating a diet that is essentially whole food, plant based with no added sugars or processed foods, then that means no preservatives. Many common preservatives kill bacteria, or specific bacteria such as 'anything but yeast'. This makes sense for shelf life but I am not sure it helps with the gastrointestinal tract, which includes the mouth. There are good bacteria and then there is the one that lives off sucrose to make plaque. If you are eating standard issue bakery products then you are killing off the bad guys whilst giving the yeast=like strains that live off sucrose a free pass. Incidentally, after I banished sugar from my home, I was reading the label of the toothpaste one day whilst brushing my teeth. And you will never guess what? Sugar. The cheek of it, putting sugar in toothpaste. That is perpetuating a problem that was being set out to be solved. Generally though, the no sugar under any circumstances rule has raised my baseline oral hygiene to the best that it has ever been. I cut coffee and dairy too, so it is not difficult or a challenge to put in the effort to get them really nice for when I am meeting someone. However, I no longer feel obligated to brush them twice a day lest they feel like they are falling out or 'covered in fur' the next morning. Regarding 'added sugars', what has to be understood is that every plant has sugar in it as does every animal. It is photosynthesis and glucose is the transport molecule for energy captured from the sun. It is not rocket science but I did not feel that any of this was explained to me very clearly at school or on any TV science program. But added sugars are the cuckoo in the nest of normal sugars, much like how chocolate Easter Eggs have very little to do eggs created by birds, lizards, amphibians and other animals. Yet some people conflate the two, particularly in online discussion. reply chaorace 8 hours agorootparent> The thing is that if brushing your teeth means a world of pain then you do it out of obligation and it is not a happy experience. Yes, investments often come in the form of obligations. Less than ten minutes each day is nevertheless cheap by self-care standards. Even by the standards of a person with pained teeth, they'd probably rather brush than undertake such a strict diet. I'm aware that all of this must seem terribly condescending, so before I go on let me just say that I honestly have no issue with the solutions you've found to your problems -- even if I disagree with some of the logic that helped you arrive at them. Everything beyond this point will be my own attempts to straighten established facts from the rest for the benefit of other readers. Live well and go in peace, friend. > If brushing your teeth means a world of pain then you do it out of obligation and it is not a happy experience If gentle, careful brushing causes a world of pain... that is not normal and indeed a strong indicator of advanced oral disease. Once disease advances beyond a certain point, the only reliable solution becomes professional dental treatment. > My point still stands though, there is a lot of money made from tooth related FUD and sugary food. I just wished a dentist had spilled the beans to me many decades ago that it really is simple to have excellent oral hygiene just be avoiding the added sugars, and, to a lesser extent, the processed food. The vast majority of dentists do want people to be healthy. It's just human nature. The core issue is that most patients don't want to be lectured -- it's bad for repeat business, so dentists learn to stay quiet about such things unless asked. Yet another one of the many tragedies caused by the systemic monetization of healthcare and something which I think we should all be more insistent about fixing. > By every metric I just find the situation very different, the gums have definitely 'grown back' and I am sure some tartar is now below the gumline. That is most excellent and I am genuinely pleased to hear that you are in good health. It's important to note for others, however, that the capacity for gums to \"grow back\" in most cases is usually lackluster. The best way to fix receding gums is to prevent the recession before it occurs with regular care -- the next best thing to do is brush and floss (or water floss!) diligently, receive regular cleanings, and maintain the routine consistently for the rest of your life. > Many common preservatives kill bacteria, or specific bacteria such as 'anything but yeast'. This makes sense for shelf life but I am not sure it helps with the gastrointestinal tract, which includes the mouth. There are good bacteria and then there is the one that lives off sucrose to make plaque. If you are eating standard issue bakery products then you are killing off the bad guys whilst giving the yeast=like strains that live off sucrose a free pass. This is an incredible oversimplification. Salt -- for example -- is itself a powerful preservative and you certainly would kill most gut bacteria by ingesting a sufficiently large dose... though such a large dose would prove equally deadly to the host. Preservatives are not antibiotics, they work because they're concentrated in the food and get quickly rendered harmless once eaten thanks to diffusion (Yes, even in the mouth. You can thank your saliva.) You are of course still correct to say that modern diets wreak havoc on gut fauna, but the cause is rooted in nutrition rather than poisoning. Fauna which are beneficial in a diverse gut can turn toxic in an environment with lower biodiversity -- something which is all too easy to cause with an unbalanced diet that systematically favors only a fraction of the extant species. Sugar-heavy diets are but one of many problematic cases in this respect. > Incidentally, after I banished sugar from my home, I was reading the label of the toothpaste one day whilst brushing my teeth. And you will never guess what? Sugar. The cheek of it, putting sugar in toothpaste. That is perpetuating a problem that was being set out to be solved. That's sad to hear and I'm sorry you've been failed like this. With that being said... I suspect that the incentives at play are not what you imply. The toothpaste manufacturer really has very little interest in the profits derived by whichever odd dentist happens to work on your teeth -- they probably just needed a cheap filler that didn't taste bad. None of this diminishes the evil caused by such greed, but we need to be clear-eyed about where the fault lies if we want to make things better. > I cut coffee and dairy too, so it is not difficult or a challenge to put in the effort to get them really nice for when I am meeting someone. However, I no longer feel obligated to brush them twice a day lest they feel like they are falling out or 'covered in fur' the next morning. I want to stress again to anyone reading this that brushing is the singlemost effective way to delay the development of calculus and thus prevent tooth decay. Even if you can't feel the plaque, it's still there and almost certainly forming new calculus. As a side-note: you may be interested to hear that plain coffee is not particularly bad for your teeth. One should avoid drinking it before brushing because the acidity can temporarily soften enamel, but aside from that all you'd have to worry about are coffee stains. Tea is similarly safe and even has a gentler PH, making most brews safe to drink even before brushing. > It is not rocket science but I did not feel that any of this was explained to me very clearly at school or on any TV science program I agree. Knowledge of such things is woefully undertaught. Schools seem much too eager to abdicate the responsibility for such things to parents who themselves may have never been fully taught. It is for this reason that we must often rely on the expertise of other professionals to make up for our own shortcomings, even as we simultaneously seek knowledge on our own terms. reply Theodores 2 hours agorootparentGod, no condescension there! Thanks for the advice. The whole diet/nutrition world is full of people that think they know best - everyone with a stomach has an opinion - \"I know this one!\" - so it is often necessary to go outside orthodoxy to get some logic/reason/experience from someone that actually knows. I used to work in the bicycle trade and I don't wear a helmet. Car dependent people always have kittens about that, you can imagine their pokey fingers now... But I do wear a hi-viz jacket, so it is a prevention rather than harm reduction approach I take. If I can be seen then I won't get hit. I also always sold the helmets because that was what customers wanted. Why leave that sweet profit on the table to sell them a $5 hi viz jacket instead? Their safety is not the same as my boss's profit motive. Priorities. So I don't think there is a conspiracy to keep everyone with rotten teeth. Dentists are just in the same position as me selling bicycles. Why give people the lecture about being visible on the roads when they just want to dress in black from head to toe, wearing a helmet that they will not adjust correctly, for it to not work beyond anecdotal 'my mate's life was saved...' and for it to have inconvenience value? With dentists there is an unsustainable regime. Nobody is flossing, brushing, mouth-washing and doing all these things that the dentist told them to do, it is almost always minimal effort. I honestly think there is a very small market for a decent book that is for people that want good teeth and can sustain a sugar free diet. After discovering the sugar in the toothpaste I looked into the alternatives and the arm 'n' hammer baking soda products seem the best out of the commonly available ones. Do I need to buy the $5 product that will only shift tartar after 10000 brushings or do I just get baking soda for $0.50 and strip off my enamel too, and in one brushing?!?! I really like your tip on the tea. I quit coffee not as a permanent thing but as an everyday thing, for black tea. The idea was that coffee would have treat value. But alas I don't care for it now. I never saw myself as addicted even though that was all I drank for decades. I am trying to develop course content for a cookery course where there is some A/B testing. One group get the 'eat your vegetables and do your exercise' message of condescension, whereas the other group get invited to do their own sugar free lifestyle change, in their own time and at their own pace so that they can stick it. I hope that the sugar free group succeeds, with the metric primarily being home cooking. With a sugar free 'streak' to maintain, all junk food is off the menu whereas a 'cut down' message means this is not so. Cutting down is a bit like telling smokers they can have one and a half cigarettes a day. People in my target audience are not in the best of health and can contemplate cold turkey, whereas rich people with a busy social life absolutely cannot contemplate cutting the sugar. There is a world of difference between health advice for the healthy and those that are far, far from it. 9 teaspoons of sugar a day, as recommended, is fine for the healthy, but for the person that is quite ill from sugar abuse over decades, cold turkey makes more sense. Yet the gatekeepers for these people are from the middle class world where sugar is not seen as the enemy. reply kanbankaren 15 hours agoparentprev> my dentists ever told me to just quit all added sugar, Hmm. People have been shouting that sugar is bad for a few decades. > and all added sugar for a month. When I go shopping, if a food item contains more than 20% sugar(irrespective whether it is added or natural), I don't buy it which means I come home only with natural produce. Also, only Splenda in coffee & tea. I have been maintaining this lifestyle for more than a decade and haven't developed any new cavities. reply Sharlin 16 hours ago [flagged]parentprevnext [2 more] (Edit: removed unnecessary sarcasm) I'm rather baffled that it's possible to not know that sugar is bad for your teeth, as it's pretty much the most often repeated fact about teeth ever. Technically sugar itself doesn't do anything to teeth. But sugar is food for bacteria. Sugar in mouth = mouth bacteria happily multiply and literally form a biofilm covering your teeth (plaque). Some of them excrete acidic metabolic products. Acid dissolves enamel (hydroxyapatite). The bacteria infestation also irritates gums and causes gingvitis and so on. Less food for bacteria = fewer bacteria = healthier teeth. reply dang 16 hours agorootparentPlease make your substantive points without crossing into personal attack. https://news.ycombinator.com/newsguidelines.html reply joemaller1 16 hours agoprevThey've already got natto reply gromneer 16 hours agoprev [–] I wish our civilization made these future med tech advancements every day. Sadly only rats benefit from our greatest medical achievements from cancer cures to limb regeneration. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Researchers in Japan will commence clinical trials for the world's inaugural \"tooth regrowth medicine\" in September 2024, aiming to deactivate a protein that impedes tooth growth.",
      "The medicine targets individuals with congenital tooth issues, with the ambition of being market-ready by 2030, offering hope to those with tooth loss from different reasons.",
      "The innovative approach could revolutionize dental care and bring relief to people suffering from missing teeth."
    ],
    "commentSummary": [
      "The discussion explores dental health topics like using hydroxyapatite in tooth regrowth medicine, fluoride benefits and risks, regrowing teeth from stem cells, and the impact of diet on dental hygiene.",
      "Emphasis is placed on good dental hygiene practices, like regular brushing and dental appointments, and concerns about sugar in toothpaste and processed foods.",
      "The conversation underscores the importance of personal accountability in oral health maintenance and the future potential for dental treatment advancements."
    ],
    "points": 221,
    "commentCount": 84,
    "retryCount": 0,
    "time": 1714914960
  },
  {
    "id": 40267164,
    "title": "Mastering Microsoft Flight Simulator with JavaScript Autopilot",
    "originLink": "https://pomax.github.io/are-we-flying/",
    "originBody": "A (lengthy!) tutorial on how to write your own JS-based autopilot code to fly planes in Microsoft Flight Simulator with a web page to both visualize the flight and control the AP.Note that this is a complete do-over of a previous page that used a combination of Python and JavaScript, and only covered basic autopilot functionality. This rewrite threw all of that out, and goes much, much further:you just want to have the plane take off without having to do anything? Check, this tutorial covers auto-takeoff.Flying with waypoint-based navigation, not by \"programming airports and radio beacons one letter at a time with a jog-dial\" but by just placing markers on a map and dragging those around as you please, mid-flight, Google Maps style? Check. After all, why would we not, putting a map on a webpage is super easy (barely an inconvenience).In fact, let&#x27;s throw in flight plan saving and loading, so others can fly the same route!And maybe you want the plane to fly a fixed distance above the ground, instead of at a fixed altitude... well: check. You&#x27;ll learn about terrain-follow using free Digital Elevation Model datasets for planet Earth.And of course the big one, for the ultimate \"click button, JS flies the plane start to finish\" experience: ever wished the in-game autopilot could just land the plane for you? Well, it&#x27;s a video game, and we know programming, so: why not? We&#x27;re also implementing auto-landing and it&#x27;s going to be amazing.If you play MSFS, and you&#x27;re a developer&#x2F;programmer, and you&#x27;ve always thought how cool it would be if you could just do some scripting for it... you can, there&#x27;s a lot of cool stuff you can do!And of course if you don&#x27;t want to follow along and just want to have everything set up and ready to fly your plane, just clone https:&#x2F;&#x2F;github.com&#x2F;Pomax&#x2F;are-we-flying and have fun.",
    "commentLink": "https://news.ycombinator.com/item?id=40267164",
    "commentBody": "Flying planes in Microsoft Flight Simulator with a JavaScript autopilot (2023) (pomax.github.io)202 points by TheRealPomax 15 hours agohidepastfavorite35 comments A (lengthy!) tutorial on how to write your own JS-based autopilot code to fly planes in Microsoft Flight Simulator with a web page to both visualize the flight and control the AP. Note that this is a complete do-over of a previous page that used a combination of Python and JavaScript, and only covered basic autopilot functionality. This rewrite threw all of that out, and goes much, much further: you just want to have the plane take off without having to do anything? Check, this tutorial covers auto-takeoff. Flying with waypoint-based navigation, not by \"programming airports and radio beacons one letter at a time with a jog-dial\" but by just placing markers on a map and dragging those around as you please, mid-flight, Google Maps style? Check. After all, why would we not, putting a map on a webpage is super easy (barely an inconvenience). In fact, let's throw in flight plan saving and loading, so others can fly the same route! And maybe you want the plane to fly a fixed distance above the ground, instead of at a fixed altitude... well: check. You'll learn about terrain-follow using free Digital Elevation Model datasets for planet Earth. And of course the big one, for the ultimate \"click button, JS flies the plane start to finish\" experience: ever wished the in-game autopilot could just land the plane for you? Well, it's a video game, and we know programming, so: why not? We're also implementing auto-landing and it's going to be amazing. If you play MSFS, and you're a developer/programmer, and you've always thought how cool it would be if you could just do some scripting for it... you can, there's a lot of cool stuff you can do! And of course if you don't want to follow along and just want to have everything set up and ready to fly your plane, just clone https://github.com/Pomax/are-we-flying and have fun. albertzeyer 2 hours agoWow, that is a lengthy tutorial. I only skimmed over it now, so maybe I missed it: How do all these developed methods compare to existing real autopilot systems? Are there even existing autopilot systems which work similarly? Did the author come up with these methods just by themselves, or were they adapted from existing systems? Would those developed methods make sense in a real autopilot system? Also, it seems that this involved a lot of trial-and-error, and the solutions come with a number of heuristics and hyper parameters to tune. I wonder if you can do better and avoid that as much as possible. First of all, you would need to have an actual metric which covers all the things you would care about. Obviously no fatal crash, but then also maybe somewhat smooth flight (you can measure that) and how close you want to get to each waypoint, while maybe taking not too much fuel. And given those, let it find the best possible path. I guess this is optimal control theory then. When you have an actual metric which you can measure, then you can quantitatively compare all the methods, also automatically optimize any parameters. reply 8372049 1 hour agoparentI haven't read the tutorial (yet), but I know a bit about autopilots. Autopilots depend a lot on which kind of plane it is in, but for the most part, autopilots aren't really \"auto pilots\". They consist of various fairly basic primitives rather than more complex decision making. As a rule of thumb, the complexity of the autopilot is proportional to the size of the plane. Basic autopilots can maintain headings, maintain (barometric) altitudes (i.e. above sea level) and similar very basic operations. For these autopilots it is the pilot's job to ensure the plane is operating within the envelope and does not depart from controlled flight and the throttle is controlled by the pilot. As an example, the autopilot in an F-16 fighter jet has four things it can do: maintain attitude, maintain altitude, fly toward nav point and fly a compass heading. That's it. Autopilots in airliner jets are more complex and can handle a set of nav point, ascent/descent, arrival and, depending on the airfield, approach and landing. They also feature autothrottles. Their implementation is still fairly simple, since it's a matter of matching throttle, attitude and heading with the position and altitude of each nav point. The autopilot doesn't have to do any pathfinding or more complex decision making, in order to keep it simple, predictable and bug free. reply jajko 1 hour agorootparentHow does autopilots using barometric pressure maintain altitude, when these can vary wildly in non-calm situations? I would expect at least gps added to the mix (although with russia blocking skies randomly that may not be the smartest option these days). Or somehow keeping inclination (dont know the proper name of instrument here). reply verst 1 hour agorootparentIt uses the pressure altitude relative to the sea level pressure you configure in the plane (the altimeter in inHg / QNH). Above transition altitude (in the US usually 18000) you switch to standard pressure (29.92 inHg), so for those higher flight levels we ignore pressure differences. But for altitudes below this, yes you constantly have to change the altimeter in the plane or else your altitude / flight level chance / VNAV descent / climb will be incorrect. Of course you can always get the pressure by tuning into a ATIS / AWOS etc radio station for a nearby airport if you aren't getting the latest pressure from the ATC enroute. GPS is usually not used for altitude. reply 8372049 15 minutes agorootparentJust to add for completeness, for ICAO/the rest of the world: Transition altitude (ascent) and transition level (descent) is typically defined per FIR (flight information region), but can be airport-dependent. It is listed in the relevant airport charts. 7000 is fairly common, but it varies. Altimeter setting is measured in hPa/millibars (with the standard being 1013.25, which corresponds to 29.92 inHg). reply 8372049 25 minutes agorootparentprevTo expand on verst's answer, the altimeter setting is defined based on your location, to roughly match meteorological conditions. When flying IFR or otherwise directed by ATC, you will be given altimeter settings. They will also be repeated during arrival and landing. In addition, they are given using an automated weather service called ATIS. ATIS broadcasts are given an incremental letter, that way you can verify you have the latest update. When checking in with a controller you will give the letter, and they will tell you if you're outdated. The most important thing is that everyone in an area reference the same altitude/altimeter setting for traffic avoidance. Whether or not that is off from the actual altitude by a few hundred feet or not is secondary, since ground avoidance typically uses a lot bigger safety margins. Airliners and military planes have radar altimeters to measure height above ground when flying in IMC (low vis), while smaller planes typically fly VFR (based on visual rather than instruments). reply t0mas88 13 hours agoprevNice! X-plane also has a great API to do these things with. It was created by NASA for research: https://github.com/nasa/XPlaneConnect reply TheRealPomax 13 hours agoparentThere's even a pretty great autopilot for x-plane post that makes it to HN every now and then! https://austinsnerdythings.com/2021/10/15/creating-an-autopi... reply auspiv 6 hours agorootparentAustin from the referenced post here - always love to see the shoutout! What you did is pretty much what I wanted to do when I set out. But definitely not in JS haha, not a huge fan of it Anyways, I chose to go with PID control for my control systems, which worked really well right \"out of the box\" with some somewhat generic values for P, I, and D. I read/skimmed all the way through your post. It looks like your control system is more or less a P & D controller? Any reason you didn't just go with PID loops? Is this part of needing things to be stateless to work for JS (I am no JS expert, just saw a few mentions of that in the conclusion)? reply TheRealPomax 5 hours agorootparentIt's basically P-D-and-a-little-second-derivative, but the straight PD controller code just doesn't like the whole \"you only get to run every half second\" timing aspect. Things take far too long to ramp up, and the amount of overshoot you get is just ridiculous. There's no way (at least, not that I've been able to mitigate in anything even close to acceptably) to avoid super obvious oscillation, even if it eventually converges, at the low rate we're running. Someone could, if they wanted (as one of the \"what's left\" options =D) replace all of this with PD code but they'll probably need to run at a much higher data rate for things to actually behave, and optimize the SimConnect calls to get the dataframes necessary without tying up a thread. reply boffinAudio 2 hours agorootparentI haven't grok'ed things properly yet, its a hefty read for my Monday morning, but the thing that leaps into my mind is to ask you why Javascript, of all things, and not something like Lua? Isn't Javascript just too finicky for something like this? Are other languages/approaches to integration with the execution environment possible/feasible? (I apologize for my newbie question, hope you don't mind..) reply sfeng 1 hour agorootparentWhat’s your definition of finicky? JavaScript is something like 3-10x faster than Lua for most benchmarks (1). If you’re referring to the event loop, it shouldn’t be all that relevant for singleminded code like this that is only processing a single event every half second. If you’re referring to mathematical oddities, it is also not relevant as JavaScripts love of floats is well suited here. 1- https://programming-language-benchmarks.vercel.app/amp/lua-v... reply userbinator 13 hours agoprevA bit clickbaity of a title, but the material was far more interesting and in-depth than I expected, although there was no mention of \"hunting\" which can often occur in control systems like this. reply TheRealPomax 13 hours agoparentWhat's clickbaity about it? The title is literally what we're doing: we're flying planes in MSFS using an autopilot that we're (progressively) writing ourselves, using JS. reply userbinator 12 hours agorootparentThe title was changed --- it just said \"Flying Planes with JavaScript\" or similar when I first saw it. reply wkjagt 12 hours agorootparentThat's still the title of the actual post. I like it though. reply TheRealPomax 7 hours agorootparentprevAhh, gotcha. reply withinrafael 10 hours agoprevAlso cool, GTA V autopilot using OpenCV, Xbox input, etc. https://github.com/davuxcom/GTA-Pilot reply avinash 7 hours agoprevThis is an awesome read. Really enjoyed the level of detail and the various tries before things work. reply Aspos 6 hours agoprevA question: I skimmed through MSFS and Xplane API docs but could not find a way to fetch full camera image. Does anyone know of a flight sim (except MS AirSim) which would allow that? Please? reply TheRealPomax 6 hours agoparentGiven how expensive that would be (it would be a staggering amount of data!), I doubt anything would allow for that. However, what you might be able to do is use the values that you read from one MSFS/XPlane session and then use the APIs available to replicate the same flight and flight properties in another, separate instance of MSFS/XPlane (on another computer, for example), and then capture its HDMI/DP-out reply TheRealPomax 13 hours agoprevAlso note that this isn't a \"let's learn to implement this thing that I already know works\", where I already worked out all the problems and am just giving you a write-up that lets you skip to the final result... We're going to get things wrong, planes are going to crash in myriad ways, and we'll discover how and why things go wrong, to hopefully figure out how we can navigate those problems. And we're going learn from our mistakes. Including learning to throw away all the work we did because that's the right thing to do =) reply Stevvo 9 hours agoprevAwesome. It would be a nice challenge to make a helicopter autopilot, because MSFS doesn't include one like it does for fixed wing. reply TheRealPomax 7 hours agoparentI feel like one of the well respected add-on folks made a helicopter with autopilot, but yeah, it would certainly be an interesting exercise! reply bottlepalm 13 hours agoprev> To allay any concerns: this is not about running JavaScript software to control an actual aircraft. That would kill people. SpaceX: Hold my beer (https://os-system.com/blog/javascript-in-space-spacex-devs-h...) reply mattsan 9 hours agoparentTo be fair all critical systems can be controlled by hardware buttons below the displays so they don't need a 100% guarantee, worst case rebooting would be fine reply boffinAudio 2 hours agorootparentYes, this .. and anyway, isn't the astronauts interaction during critical flight phases completely superfluous to the mission? The screens are there to tell the occupants how close they are to death, it seems, and little else. Heck, for a long time - decades, even - the principle 'interface' to the rocket systems being used to regularly launch humans to space was, literally, a stick. Javascript seems like a huge upgrade from a stick. ;) reply rad_gruchalski 8 hours agoparentprevIt ain’t flying civilians so who cares. reply TheRealPomax 13 hours agoparentprevMental note: hold off on considering taking SpaceX flights O_O! reply Waterluvian 10 hours agoprevWe need to integrate this with Twitch so viewers can be my copilot… with the expectation of amusingly disastrous results! reply TheRealPomax 10 hours agoparentThat'd be hilarious =P reply kosolam 13 hours agoprevSomething in the page kept reloading it when scrolled down trying to read on mobile firefox reply TheRealPomax 12 hours agoparentYeah, you really don't want to read this on mobile. It's 200 print pages, I have no idea how long you'd be scrolling. (I need a better nav menu solution for mobile, at the very least) reply sagasu007 8 hours agoprevgreat sharing reply fho 5 hours agoprev [–] > (barely an inconvenience) You need to get aaaall the way of my back about why flying planes with JS sounds like a bad idea :-) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The tutorial presents a detailed walkthrough on developing JavaScript autopilot code for Microsoft Flight Simulator, enabling functions like auto-takeoff, waypoint navigation, terrain following, auto-landing, and flight plan saving/loading.",
      "It introduces an upgraded version with a more sophisticated and user-centric method for piloting aircraft within the game.",
      "Developers are motivated to delve into game scripting possibilities, with a quick setup option for instant flight engagement."
    ],
    "commentSummary": [
      "The tutorial explores implementing a JavaScript autopilot in Microsoft Flight Simulator, including features such as auto-takeoff and auto-landing, delving into optimal control theory and altimeter configurations.",
      "It addresses the complexities of crafting an autopilot script, utilizing APIs, developing autopilots for varied simulations, and troubleshooting mobile navigation issues.",
      "The content provides insights into leveraging advanced functionalities in the simulator and overcoming technical hurdles in script development for aviation enthusiasts looking to enhance their virtual flying experience."
    ],
    "points": 202,
    "commentCount": 35,
    "retryCount": 0,
    "time": 1714934704
  },
  {
    "id": 40270586,
    "title": "Rescued Doves Typeface Unveiled from Thames",
    "originLink": "https://news.artnet.com/art-world/doves-typeface-2454807",
    "originBody": "Archaeology & History Remnants of a Legendary Typeface Have Been Rescued From the River Thames Doves Type was thrown into the water a century ago, following a dispute between its creators. Doves Type recovered by Robert Green, 2014. Photo Matthew Williams Ellis by Holly Black May 5, 2024 ShareShare This Article The depths of the river Thames in London hold many unexpected stories, gleaned from the recovery of prehistoric tools, Roman pottery, medieval jewelry, and much more besides. Yet the tale of the lost (and since recovered) Doves typeface is surely one of the most peculiar. A little over a century ago, the printer T.J. Cobden-Sanderson took it upon himself to surreptitiously dump every piece of this carefully honed metal letterpress type into the river. It was an act of retribution against his business partner, Emery Walker, whom he believed was attempting to swindle him. The pair had conceived this idiosyncratic Arts and Crafts typeface when they founded the Doves Press in the London’s Hammersmith neighborhood, in 1900. They worked with draftsman Percy Tiffin and master punch-cutter Edward Prince to faithfully recall the Renaissance clarity of 15th-century Venetian fonts, designed by the revolutionary master typographer Nicolas Jensen. Doves Type recovered by Robert Green, 2014. Photo Matthew Williams Ellis With its extra-wide capital letters, diamond shaped punctuation and unique off-kilter dots on the letter “i,” Doves Type became the press’s hallmark, surpassing fussier typographic attempts by their friend and sometime collaborator, William Morris. The letterforms only existed as a unique 16pt edition, meaning that when Cobden-Sanderson decided to “bequeath” every single piece of molded lead to the Thames, he effectively destroyed any prospect of the typeface ever being printed again. That might well have been the case, were it not for several individuals and a particularly tenacious graphic designer. Robert Green first became fascinated with Doves Type in the mid-2000s, scouring printed editions and online facsimiles, to try and faithfully redraw and digitize every line. In 2013, he released the first downloadable version on typespec, but remained dissatisfied. In October 2014, he decided to take to the river to see if he could find any of the original pieces. Doves Type recovered and held here by Lukasz Orlinski at Emery Walker’s House. Photo: Lucinda MacPherson. Using historical accounts and Cobden-Sanderson’s diaries, he pinpointed the exact spot where the printer had offloaded his wares, from a shadowy spot on Hammersmith bridge. “I’d only been down there 20 minutes and I found three pieces,” he said. “So, I got in touch with the Port of London Authority and they came down to search in a meticulous spiral.” The team of scuba divers used the rather low-tech tools of a bucket and a sieve to sift through the riverbed. Green managed to recover a total of 151 sorts (the name for individual pieces of type) out of a possible 500,000. “It’s a tiny fraction, but when I was down by the river on my own, for one second it all felt very cosmic,” he said. “It was like Cobden-Sanderson had dropped the type from the bridge and straight into my hands. Time just collapsed.” The finds have enabled him to further develop his digitized version and has also connected him with official mudlarks (people who search riverbanks for lost treasures, with special permits issued) who have uncovered even more of the type. A mudlark by the Thames with Hammersmith Bridge in background. Photo: Lucinda MacPherson. Jason Sandy, an architect, author and member of the Society of Thames Mudlarks, found 12 pieces, which he has donated to Emery Walker’s House at 7 Hammersmith Terrace. This private museum was once home to both business partners, and retains its stunning domestic Arts and Crafts interior. Much like Green, Sandy was captivated by the Doves Type story, and mounted an exhibition at the house that displays hundreds of these salvaged pieces, including those discovered by Green, as well as mudlarks Lucasz Orlinski and Angus McArthur. The show was supplemented by a whole host of Sandy’s other finds, including jewelry and tools. An extant copy of the Doves English Bible is also on display. The Doves Bible returns to Emery Walker’s House. Photo: Lucinda MacPherson. “It is not that unusual to find pieces of type in the river,” Sandy said. “Particularly around Fleet Street, where newspaper typesetters would throw pieces in the water when they couldn’t be bothered to put them back in their cases. But this is a legendary story and we mudlarks love a good challenge.” The community is naturally secretive about exactly where and how things are found. For example, Orlinski has worked under the cover of night with a head torch, to search for treasures at his own mysterious spot on the riverbank. For Sandy, the thrill comes from the discovery of both rare and everyday artifacts, which can lead to an entirely new line of inquiry: “The Thames is very democratic. It gives you a clear picture of what people have been wearing or using over thousands of years. And it’s not carefully curated by a museum. The river gives up these objects randomly, and you experience these amazing stories of ordinary Londoners. It creates a very tangible connection to the past. Every object leads you down a rabbit hole.” “Mudlarking: Unearthing London’s Past” is at Emery Walker’s House, 7 Hammersmith Terrace, London, through May 30. Follow Artnet News on Facebook: Want to stay ahead of the art world? Subscribe to our newsletter to get the breaking news, eye-opening interviews, and incisive critical takes that drive the conversation forward. ShareShare This Article Holly Black More Trending Stories Art World Art Bites: What Sparked Rudy Giuliani’s Quest to Close the Brooklyn Museum? Art Fairs Abstract Sculptures and Vintage Finds Stand Out at Frieze New York Artists Alicja Kwade’s New ‘Mystic’ Sculptures Will Confront the Titan of Minimalism Agnes Martin Art Fairs Frieze Forecast: The Venice Effect Hits NYC, Spotlight on Latin American Women, and More Art World Art Bites: What Sparked Rudy Giuliani’s Quest to Close the Brooklyn Museum? Art Fairs Abstract Sculptures and Vintage Finds Stand Out at Frieze New York Artists Alicja Kwade’s New ‘Mystic’ Sculptures Will Confront the Titan of Minimalism Agnes Martin Art Fairs Frieze Forecast: The Venice Effect Hits NYC, Spotlight on Latin American Women, and More",
    "commentLink": "https://news.ycombinator.com/item?id=40270586",
    "commentBody": "Remnants of a legendary typeface have been rescued from the Thames (artnet.com)154 points by _emacsomancer_ 7 hours agohidepastfavorite37 comments KaiserPro 3 minutes agoYou might also like \"Zilvertype\" which is from the dutch font school of roughly the same time. https://www.alphabettes.org/zilvertype/ reply unraveller 3 hours agoprevDoves is insanely easy on the eyes despite so much going on. There is also mebinac[1] an unauthorized contemporary take on the original doves. Mebinac doesn't leap off the page as well yet deals with modern punctuation in a more normal way. Personally you can freely use them to great affect in your RSS reader or mail app that you read everyday. [1] https://fontsme.com/mebinac.font reply stevefolta 3 hours agoparentI tried looking at code in Mebinac, and was surprised at how strongly it reminded me of old screenshots of Smalltalk. reply AnthonBerg 8 minutes agoparentprevThanks!, hadn’t come across Mebinac. It’s quite good! I’m also a big fan of Igino Marini’s recreation of the Fell typefaces: The Fell Types took their name from John Fell, a Bishop of Oxford in the seventeenth-century. Not only he created an unique collection of printing types but he started one of the most important adventures in the history of typography. — https://web.archive.org/web/20240128075552/https://iginomari... The IM Fell fonts themselves seem to live on Google Fonts these days: https://fonts.google.com/?query=Igino+Marini I use Doves Type for… everything. One day I started to find my monomaniacal obsession a bit funny and sort of to spite myself I set every font in Firefox to Doves Type. Serif, sans-serif, monospace, no other fonts allowed, as well as the UI font by tweaking the Firefox user profile iirc. And it was just… very good. And I kept using it. I use Doves Type for everything, and to be able to do that on my phone I use iFont: https://apps.apple.com/is/app/ifont-find-install-any-font/id... Or yeah I do use IBM PC VGA 9x16, IBM BIOS 8x8, and Eagle Spirit PC CGA Board Alternate 3 a little :) From the Ultimate Oldschool PC Font Pack: https://int10h.org/oldschool-pc-fonts/ I even munged together a combination of Doves Type Regular and IM Fell Great Primer Italic that matches the character scale and linespacing to both each other and to the IBM PC VGA 9x16 font at 1:1 size. The open-source FontForge did the trick!: https://fontforge.org/en-US/ (FontForge can autogenerate italics for any font. If you’re bored, I suggest loading up the classic VGA font and pressing the ITALICIZE button on ot. It’s… interesting!) In general, on Windows I much prefer MacType’s fomt rendering: https://www.mactype.net … it’s kind of amazing that this kind of surgery is even possible. reply dkga 3 hours agoparentprevThis font is beautiful, thanks for sharing. reply dang 3 hours agoprevRelated. Others? I think there were others. The lost Doves Press typeface and its revival (2015) - https://news.ycombinator.com/item?id=20791125 - Aug 2019 (9 comments) How the Doves Type Was Nearly Lost - https://news.ycombinator.com/item?id=12476579 - Sept 2016 (44 comments) One man's obsession with rediscovering the Doves typeface - https://news.ycombinator.com/item?id=9951869 - July 2015 (32 comments) Lost typeface printing blocks found in river Thames - https://news.ycombinator.com/item?id=9017307 - Feb 2015 (22 comments) The fight over the Doves: A legendary typeface gets a second life - https://news.ycombinator.com/item?id=6964013 - Dec 2013 (12 comments) reply wrp 59 minutes agoprevThere was also a revival of the Doves type made by Torbjörn Olsson in 1994. It is no longer available, but you can find the old specimen PDF at the Internet Archive and extract the embedded fonts. The weight is a bit lighter than the Robert Green version, but also has an italic face. https://web.archive.org/web/20121127135748/home1.swipnet.se/... reply Animats 4 hours agoprevThe \"modernized version\", available as a font file, was modernized too much.[1] It doesn't look period. The H.P. Lovecraft Society has some 19th century fonts, if you need them.[2] Those were recovered from old documents. [1] https://typespec.co.uk/doves-type/ [2] https://www.hplhs.org/resources.php reply zettabomb 2 hours agoparentI'm curious what you mean by not looking \"period\". The HPLHS fonts frankly seem to just be poor quality, rather than old. If you look at the images of the original type, Doves appears to be quite faithful to the original. Perhaps it's worth noting that we still use typefaces remarkably similar to the Romans, particularly Times New Roman, which despite its many shortcomings retains a \"modern\" look by virtue of still being in use. reply vargr616 27 minutes agorootparentRoman type has roots in Italian printing of the late 15th and early 16th centuries, but Times New Roman's design has no connection to Rome or to the Romans. https://en.wikipedia.org/wiki/Times_New_Roman reply ZeroGravitas 2 hours agorootparentprevThey are intended to be of historically appropriate quality, for use in creating period versimilitude: > Many of these fonts have slightly rough edges or irregular shapes, to capture the feel of old lead type and bygone printing technologies reply neilv 4 hours agoprev> “It is not that unusual to find pieces of type in the river,” Sandy said. “Particularly around Fleet Street, where newspaper typesetters would throw pieces in the water when they couldn’t be bothered to put them back in their cases. Some assistant being lazy, or rushing to \"finish\" a task? Or sorts that broke, or were worn out, and it was normal to toss things into the river? Or a ritual? (Say, toss a sort into the river for the first page an apprentice sets, or when there's a press failure, or for superstition after printing very bad news?) reply timeon 59 minutes agoparent> it was normal to toss things into the river? It was normal. Rivers were used for dumping the garbage. In some places they still are. I know about instances in Europe where people dump their trash in streams behind the hamlet. reply lostlogin 12 minutes agorootparentSemi related - the UK pouring sewage into its waterways has been front page news of late. It’s up to 3.6 million hours of sewage discharge per year. https://www.bbc.com/news/explainers-62631320 reply Piskvorrr 9 minutes agorootparentprevSuch as...in the Danube river O_O https://phys.org/news/2020-09-brown-danube-belgrade-sewers-t... reply sriram_malhar 4 hours agoprevThis has so much of what I (as an outsider) love about the UK. The love of typography & general design chops, mudlarks, art and design in public life, the spirit of enquiry and adventure and, the presence of people in the bureaucracy and elsewhere who recognize whimsy and put institutional resources behind that pursuit. reply mihaic 1 hour agoprevPSA for the inspiration for this font, the great Nicolas Jenson, who around 1470 had pretty much perfected the latin typeface. Later, more famous types, such as Caslon or Garamond, are just variations on this. reply kens 4 hours agoprevThe recovery of the Doves typeface from the Thames was discussed on HN in 2015, so this story goes way back. https://news.ycombinator.com/item?id=9017307 reply dang 3 hours agoparentAdded to https://news.ycombinator.com/item?id=40271786. Thanks! reply beardyw 3 hours agoparentprevYes, very old news. reply 33282334 1 hour agorootparenthek free reply rawling 1 hour agoprevCurious as to why this refers to recovering the type being important to creating a digital version of the typeface, when lower in the article it shows that there is a surviving bible. Couldn't that have already been used to reproduce the font? reply wrp 53 minutes agoparentDue to irregular spreading of the ink when printing, the shapes on the page are not perfect representations of the type shape, so the true shape of the metal form has to be inferred from comparing multiple printed samples. There are digital reproductions of old typefaces that try to reproduce the actual weight on the page, but they seem to be not very popular with modern designers unless they are going for a deliberately archaic look. reply riwsky 3 hours agoprevThames New Roman reply JNRowe 3 hours agoprevWe've had centuries of embankment works along the Thames¹, a fair bit concentrated around the areas you'd expect to find type like this². There must be a phenomenal amount of history that was purposely covered around there. Given the scale of the works you'd have to imagine there is a significant chunk of non-London history to be found there too(the scale of granite imports from Cornwall being an obvious example). I'm less optimistic about the possibility of more large scale digs though, as the Golden Jubilee bridge history³ points out the area is an also an exciting zone for stumbling in to unexploded ordnance and you always seem to be within few metres of a tube line or Victorian sewer. [It is the reason I love those plucky Crossrail⁴ developers who've felt the anger from the havoc they've left across London over the few past decades. We get incredible large scale engineering works to lust over, coupled with really wacky archaeological digs tagging along for the ride.] ¹ https://en.wikipedia.org/wiki/Embanking_of_the_tidal_Thames ² https://en.wikipedia.org/wiki/Thames_Embankment - Both the \"home\" of the type in Hammersmith and Fleet were the targets of embankment work in the 19th century ³ https://en.wikipedia.org/wiki/Hungerford_Bridge_and_Golden_J... ⁴ https://en.wikipedia.org/wiki/Crossrail reply lostlogin 7 minutes agoparentOn a trip to London and having heard of mudlarking, I walked in one of the ‘beaches’. I immediately found an old belt belt buckle and about 20 stems from old clay pipes. My father found a 17th century cork screw. There must be an absolute wealth of finds along its banks. reply starkparker 5 hours agoprevI remember the earlier story about the disposal and Robert Green's obsession with reviving it back in 2013 in The Economist[1]—at that time, \"Intrepid fans have occasionally tried to recover pieces of the type from the river, but no one has ever found any\"—so it's good to hear that the story didn't end there. [1]: https://www.economist.com/christmas-specials/2013/12/19/the-... (paywalled; https://archive.is/XfK1x) reply baerrie 1 hour agoprevNicola White documents here interesting mudlarking adventures on youtube, I recommend it! https://youtu.be/rVxncipNvvY?si=1DGluOHT8T5fRNfE reply rudyfink 5 hours agoprevThat's cool. I admit hearing that story and thinking, \"Is that how it happened? could a diver find it?\" Apparently, they could! Great work on someone seeing it through. reply raldi 3 hours agoprevI’m left wanting to hear more about the motivation for dumping the type in the first place. What kind of swindle was suspected? Did the partner try to reconstruct the type? reply surfingdino 1 hour agoparenthttps://youtu.be/e8harWbZN6U?si=4D5ZDCn2WLlciy5T&t=1002 reply morrbo 3 hours agoprevWe actually have this. Obviously not this particular font. My family were all printers and I've sort of inherited a huge cabinet full of old school typefaces all carved out of some special kind of hard wood - pear wood - all over 100 years old. Absolutely 0 idea what we can do with it, but it's all hand made and very cool. Felt pertinet to share lol reply 317070 3 hours agoparentMaybe drop Robert Green (the man behind this article) an email on: https://typespec.co.uk/custom-font-services/ reply bradrn 1 hour agorootparentKlim Type Foundry [1] may also be worth a contact — they’ve been inspired by woodcut type before (e.g. [2] [3]), so I wonder if they might be interested in knowing about this. [1] https://klim.co.nz/ [2] https://klim.co.nz/blog/maelstrom-design-information/ [3] https://klim.co.nz/blog/manuka-design-information/ reply komali2 1 hour agoprev [–] I'm wryly curious why fonts are among the odd things that really get the goat of us turbo-nerds on forums like HN. reply mihaic 1 hour agoparent [–] When you spend most of your day staring at text on a screen, the minutia of how that text looks like become very important. reply AnthonBerg 39 minutes agorootparent [–] The centuries-old artistry of mass reproduction of thought has many wonderful minutiae!, as high technology often does. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Doves Type, an exclusive Arts and Crafts font from 1900, was discarded into the River Thames by its creator during a disagreement.",
      "Graphic designer Robert Green salvaged some of the typeface from the riverbed in 2014, resulting in a digital version's creation.",
      "Emery Walker's House hosts an exhibition showcasing the recovered Doves Type fragments and other Thames-discovered artifacts, while mudlarks persist in searching the river for historic relics, linking to London's history."
    ],
    "commentSummary": [
      "The summary discusses the recovery of the Doves Type typeface from the Thames, highlighting similar fonts and font pairings.",
      "It explores the typography history, the tradition of discarding objects in rivers, and potential archaeological discoveries in the Thames.",
      "The dialogue emphasizes the significance of font appearance and potential partnerships in font-centric endeavors."
    ],
    "points": 154,
    "commentCount": 37,
    "retryCount": 0,
    "time": 1714962426
  },
  {
    "id": 40266015,
    "title": "Unlocking ESP32-S3 SIMD Instructions: Challenges and Potential Benefits",
    "originLink": "https://bitbanksoftware.blogspot.com/2024/01/surprise-esp32-s3-has-few-simd.html",
    "originBody": "Surprise! ESP32-S3 has (a few) SIMD instructions January 22, 2024 Intro Espressif Systems released their ESP32-S3 SoC a few years ago, but only recently have they released more documentation and support of its full capabilities. Without any changes to your code, the S3 runs about 15% faster than older ESP32 CPUs at the same clock speed. It has a 'hidden' capability that's more difficult to use, but can be worth the effort if you need more speed. This article is aimed at programmers who are already familiar with SIMD instructions on other platforms. I've been optimizing code with SIMD for more than 15 years on Intel, Arm and DSPs (even Cadence's), so when I heard that the S3 had SIMD instructions, I immediately went searching for documentation. When the S3 became available to buy, there was only a promise of documentation and support. In the 2+ years since then, not much has changed. At the end of 2023, Espressif released a document describing the new instructions: S3 Technical Reference Manual The document has a decent level of detail and only a few errors, but what's missing still are examples and documentation on how to use them in your own code - conspicuously absent are instructions on using the assembler and linking them to your C/C++ code. This isn't entirely the fault of Espressif. The Xtensa processor comes from Cadence and for some reason they like to keep everything under NDA, even information which would help people use their processors. I find it hard to understand why the instruction set should be kept secret; a CPU vendor should make it as easy as possible for engineers to use their CPUs. The 'trade secrets' are in the hardware design, not in the instruction set. I've worked with Cadence's DSPs before, so I'm familiar with their way of doing things. Their Vision DSPs have a robust and powerful instruction set. Unfortunately, the S3 has a very minimal set of SIMD instructions, probably due to cost and silicon area limits. Since the SIMD 'Processor Extension' is treated as a coprocessor, the main instruction set must be mixed in the code. Here's the document for the main LX7 instructions: Xtensa ISA The programmer model consists of 16 general purpose/address registers (a0-a15), 8 128-bit wide SIMD registers (q0-q7), and two special accumulator registers for multiply/accumulate operations. The memory bus is documented as 128-bits wide, so it is definitely advantageous to read and write to memory at the native width. There are also some instructions to manipulate GPIO bits. How I got started with S3 SIMD I spent the better part of a day searching and experimenting with these instructions until I got working code. I started with a search on Github for any public repos containing the one instruction needed for any S3 SIMD project - load (ee.vld.128). A few hits popped up in Espressif's esp-dsp project. A lot of their ESP32-S3 code is closed source, but a few functions pulled back the veil on how to use them in my own projects. This is the code I used as a starting point: https://github.com/espressif/esp-dsp/blob/master/modules/fft/fixed/dsps_fft2r_sc16_aes3.S I tried putting multiple functions into a single .S file, but that doesn't seem to work so every function gets it's own file. My first use case for these instructions is to optimize the color conversion step of my JPEG decoder. The YCbCr->RGB step takes a significant amount of time and is a good fit for SIMD optimization. The Instruction Set I've written SIMD code for the pixel color conversion multiple times in multiple SIMD instruction sets and what struck me with the S3 was how little I had to work with. One of the main sticking points is that even though the instruction encodings are 24-bits each, there's no bits reserved for shift amount. There are explicit shift instructions and even they don't have the shift amount encoded. The multiply instructions can shift right after multiply, but the shift amount must first be loaded into the SAR (shift amount register). This requires 2 additional instructions (and potentially additional pipeline cycles) to accomplish. The main sticking points which make it harder to get work done are that the instructions are not orthogonal across data sizes and are missing a lot of things needed to make efficient code. For example, the shift left and right instructions only operate on 32-bit values and only do arithmetic shifting (carry the sign bit), not logical. To generate RGB565 output, I need to shift 16-bit values. My workaround was to multiply by 1 and set the SAR for right shifting and multiply by a power of 2 and set the SAR to 0 for left shifting. Here's a short list of what I consider essential SIMD features that are missing on the S3: - Shift right logical - Shift 8 or 16-bit values - Add or multiply with widening - Right shift with narrowing - Add and subtract without saturation - Unaligned reads and writes Also missing (Nice to haves) that other Cadence DSPs have: - Scatter / gather writes/reads - Horizontal vector operations - Rearrange vector element order - Floating point support - Predicated operations (operate on select elements only) The gotcha that had me going in circles for a while is the memory alignment restriction. In my JPEG decoder internal data structure, some of the elements are aligned on 4-byte boundaries. S3 SIMD load and store instructions can at best access memory on 8-byte boundaries, but ideally want everything on 16-byte boundaries. I fixed this by inserting a \"long double\" in front of the items that need 16-byte alignment 😀. Where to go from here... I'm going to add some optimized functions to my various imaging libraries where appropriate and see where it takes me. I did an initial test with my JPEGDEC library and saw a nearly 40% speedup (14ms -> 10ms) by using the S3 SIMD instructions for the color conversion step. I'll publish this code after I've had time to fully flesh it out and test it. Good luck with your use of S3 SIMD... Comments Paolo BonziniMay 5, 2024 at 10:49 AM You can use 16 bit shift, followed by AND with 0xFEFE... or 0x7F7F..., to emulate 8 bit shifts REPLY Larry BankMay 5, 2024 at 11:26 AM Yes, there are ways to make things work, but without a fully implemented instruction set, it lessens the benefit of having SIMD. Post a Comment",
    "commentLink": "https://news.ycombinator.com/item?id=40266015",
    "commentBody": "ESP32-S3 has a few SIMD instructions (bitbanksoftware.blogspot.com)150 points by _Microft 17 hours agohidepastfavorite38 comments londons_explore 14 hours agoESP_Sprite, former opensource-projects-guy, now Espressif employee, is the best source of knowledge on this stuff. Looks like back in 2021 they had an intention to document these, but never quite got round to it: https://esp32.com/viewtopic.php?p=88114&sid=f7f25776d9cfc6b6... They do publish a bunch of opensource code that uses the SIMD stuff, and an assembler, so it isn't secret, just very badly documented. reply londons_explore 13 hours agoparentUpon further inspection, it now seems like it is much better documented... Page 37-301 of the reference manual seems to have all you'd need, including binary instruction encodings, details on instruction timings, etc. https://www.espressif.com/sites/default/files/documentation/... reply tzmlab 15 hours agoprevThere's also a follow-up blog post \"ESP32-S3 SIMD Minimal Example\" [0]. 0: https://bitbanksoftware.blogspot.com/2024/01/esp32-s3-simd-m... reply amelius 13 hours agoprevWhere is a good overview of the various ESP32 chips available and their features? reply mort96 13 hours agoparentEspressif has a pretty decent overview on their website: https://www.espressif.com/en/products/socs They also make a set of modules per chip, so you can get a particular chip in an easier to use package with e.g a built-in PCB antenna or antenna mounting ports or no antenna, various onboard flash sizes, that sort of stuff: https://www.espressif.com/en/products/modules reply the__alchemist 8 hours agoparentprevS3 if you want more pins and fast C3 if you don't, and are OK with RISC-V PICO 3v2 otherwise. reply tbyehl 7 hours agorootparentS3 also has USB support that I've come to hugely appreciate on dev boards... tho I just got some oddball single-port boards that used a CH340 anyways. Grrr. reply adolph 16 hours agoprevThe Xtensa processor comes from Cadence and for some reason they like to keep everything under NDA, even information which would help people use their processors. I find it hard to understand why the instruction set should be kept secret; a CPU vendor should make it as easy as possible for engineers to use their CPUs. The P4 can't come soon enough to get off Xtensa. reply jsheard 16 hours agoparentThe P4 doesn't have a built-in radio though, so if you want those beefy RISC-V cores you will need to integrate a second ESP32 just to handle WiFi/BT :( It will have USB-OTG and an LCD driver at least, which so far have been missing from all of their RISC-V parts. reply antoniuschan99 15 hours agorootparentI think that’s totally fine. Might actually be the future direction. But yea would’ve been nice to have wifi/bt integrated. Eg if you want 5ghz then use c5, or if you want some wifi-6 so c6, etc. Also here’s a talk by them on how to use esp as a wifi coprocessor https://youtu.be/g14aEjnjRLw?si=TgkEyJJ2_L_Shuom There’s also adafruit airlift https://www.adafruit.com/product/4201 reply Aurornis 6 hours agorootparent> I think that’s totally fine. Might actually be the future direction. Using a second board just for WiFi is definitely not totally fine for most applications. Having everything integrated into a single package important for everything from reducing BOM count to lower power consumption to development simplicity. reply ComputerGuru 16 hours agorootparentprevCost optimization aside, that has always been the best way to use an ESP chip. Just go with one of their barebone models wired up as a peripheral to an ARM or RISC mcu. reply clbrmbr 15 hours agorootparentYet it’s possible to build some incredible applications on top of just ESP32, especially with extra RAM. reply devmunchies 7 hours agorootparentprevWe use esp32-s3 at my company (smart speaker) but we don't don anything fancy. Can you explain this? Why use esp as a peripheral if you already have an ARM chip? We were considering moving off of esp to something that would make it easier do cpu-bound AI inference on-device or to enable more advanced audio DSP algos. reply throwup238 1 hour agorootparentBased on cost and development time, it’s usually just easier to add an ESP and communicate to it using a generic SPI library or something than to add a radio to your PCB and get vendor libraries working on an arbitrary platform. reply sitkack 14 hours agorootparentprevThe ESP8684H2 is 1.20 qty 1, more than enough to handle BT an Wifi, then you can use any MCU you want as your application processor. reply timschmidt 15 hours agorootparentprevSeems likely they'll continue releasing more models, further integrating the features of the P4 and C6 for example. Maybe we'll even get some risc-v SIMD instructions and support for off-chip SRAM. reply yau8edq12i 15 hours agorootparentprevWithout the builtin radio it's really hard to justify the use of an ESP32 over, say, an STM32. The integrated small package with \"everything\" to make a fun project is the whole appeal. reply mort96 2 hours agorootparentThat's the whole appeal to hobbyists, sure. But I'm guessing Espressif wants to be considered for more serious applications as well. Currently, there are good reasons to choose, say, an STM32 over an ESP32 for a commercial product if you don't need RF (or if RF is handled by another part of the product, such as a SoM running Linux). I'm guessing they wanna change that. reply AlotOfReading 13 hours agorootparentprevEspressif has a huge advantage in lead times over ST recently. I migrated a few projects over because ST couldn't or wouldn't give us supply in under a month when you could buy ESP chips and have them on your doorstep practically overnight. reply vbezhenar 12 hours agorootparentDid you look at chinese STM clones? We used gd32, I liked it. reply makapuf 23 minutes agorootparentDepends if you want to condone IP theft (compatible independent developments is of course different but I'm not sure this is the case). R&D, Support, good documentation in English and accuracy of specs come at a price. reply 6SixTy 9 hours agorootparentprevWide product range. ARM and RISC-V all called GD32 with an extra letter for the exact line. reply the__alchemist 7 hours agorootparentprevThat's a big differentiator - it's surprising that there is no STM32 with Wi-Fi. reply ajb 11 hours agoparentprevXtensa is an unusual beast because its USP (at least, back when it was owned by tensilica) was that you could easily add extensions. Not just off the shelf ones - ones you defined yourself. They had some automation that would generate a toolchain for you to use with your shiny new instructions. Most CPU architectures exist to allow programs written on once implementation to work on another, with Xtensa it's kind of the opposite - it exists to allow each chip to have its own special sauce. Honestly I was a bit surprised that espressif used it without defining their own extension of some kind, if you're not doing that then you might as well use something better known. Edit: ajross* points out that this SIMD extension is such a one, not an off the shelf one. So I guess that explains it. * https://news.ycombinator.com/item?id=40267977 reply mianos 11 hours agoparentprevIt is possible there is some licensing issue around the SIMD, after all it is an optional component. It was available for the LX6 as well, but not included. It's been a good run but it's great the are going to the RISCV, at the very least for the vibe. I have used both architectures, more recently using their esp-idf and it is surprisingly uneventful to switch between them. The only issue I had is the different high/low speed timer devices between chips. In fact it is a surprise the on chip peripheral hardware is incredibly compatible with their idf. Sure, they have a layer for some calls but a lot is just issuing commands to io devices directly, and the same between riskv and tensilkica cores. reply ajross 13 hours agoparentprevFWIW this bit in the article is a little confused. The SIMD instructions being detailed appear to be Espressif-custom things implemented using Cadence's \"TIE\" facility. Cadence does indeed have their own SIMD architecture (\"HiFi\", really it's a family of similar but binary-incompatible ISAs). And indeed docs for that don't appear in public (though if you look carefully, details for how to emit the instructions are part of the GNU toolchain integration). But that isn't this. If you want docs for this, talk to Espressif, not Cadence. reply lunfard000 15 hours agoprevWas it a secret? You could have guessed that something advertised [0] for \"AI\" had some kind of SIMD. Even ChatGPT 3.5 can give relevant code to use \"AI\" features [1]. 0: https://www.espressif.com/en/products/socs/esp32-s3 1: https://chat.openai.com/share/3e1f990d-e8eb-4e56-acbb-ad5a33... reply iamflimflam1 15 hours agoparentNot a secret - just not documented very well if at all. We all knew there were SIMD instructions, but if there’s no information on how to use them or what they do… reply bobmcnamara 14 hours agorootparentIIRC, they have 128bit alignment requirements, so tricky to autovectorize. reply lunfard000 15 hours agorootparentprevAnd the author is not documenting them either, just announcing his new niche library. It is not like disassembling a few functions to prove that they exist is dark magic. I just don't see any value in the article. reply iamflimflam1 3 hours agorootparentI’m not sure I’d call a JPEG decoding library “niche”. There are some numbers here on the performance improvements he’s managed to make. https://atomic14.substack.com/p/even-faster-jpeg-decoding reply amelius 13 hours agoparentprev> Even ChatGPT 3.5 can give relevant code to use \"AI\" features I've seen ChatGPT invent its own functions and commands ... reply makapuf 21 minutes agorootparentI've also definitely seen it reference invented methods on APIs (that would have been very nice if they existed) - that no past or future version implemented. reply exe34 13 hours agorootparentprevif problem: solve_problem() There, problem solved! reply ssl-3 7 hours agorootparent# rest of problem-solving code goes here reply relaxing 14 hours agoparentprevI love doing engineering based off of advertising material… reply londons_explore 14 hours agoprev [–] I had plans to use this SIMD support for some DSP algorithms on camera video feeds.... But looking at how badly documented it is, I may reconsider... Without scatter/gather I don't think I'm gonna be able to meet my timing requirements (I need to distort images through warping, which is tricky to do without scatter/gather) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Espressif Systems introduced the ESP32-S3 SoC featuring SIMD instructions for programmers experienced in SIMD from other platforms.",
      "The S3 outperforms older ESP32 CPUs but lacks detailed documentation and examples for effectively utilizing SIMD instructions, posing optimization challenges.",
      "Despite limited instructions and memory alignment restrictions, programmers can enhance performance, particularly in functions like color conversion within imaging libraries with optimized code."
    ],
    "commentSummary": [
      "The ESP32-S3 microcontroller features SIMD instructions that are not extensively documented but can be located in the reference manual.",
      "Espressif, the manufacturer, provides a range of chips and modules with diverse functionalities, sparking debates on the advantages and limitations of utilizing ESP32 in different applications.",
      "Users face difficulties in accessing information and documentation for specific features like SIMD instructions tailored for DSP algorithms, hinting at potential improvements in future models."
    ],
    "points": 151,
    "commentCount": 38,
    "retryCount": 0,
    "time": 1714926093
  }
]
