[
  {
    "id": 40531100,
    "title": "Designing Apps for Extreme Connectivity Challenges: Lessons from Antarctica",
    "originLink": "https://brr.fyi/posts/engineering-for-slow-internet",
    "originBody": "Hello everyone! I got partway through writing this post while I was still in Antarctica, but I departed before finishing it. I’m going through my old draft posts, and I found that this one was nearly complete. It’s a bit of a departure from the normal content you’d find on brr.fyi, but it reflects my software / IT engineering background. I hope folks find this to be an interesting glimpse into the on-the-ground reality of using the Internet in bandwidth-constrained environments. Please keep in mind that I wrote the majority of this post ~7 months ago, so it’s likely that the IT landscape has shifted since then. Welcome back for a ~~bonus post~~ about Engineering for Slow Internet! For a 14-month period, while working in Antarctica, I had access to the Internet only through an extremely limited series of satellite links provided by the United States Antarctic Program. Before I go further, this post requires a special caveat, above and beyond my standard disclaimer: Even though I was an IT worker within the United States Antarctic Program, everything I am going to discuss in this post is based on either publicly-available information, or based on my own observations as a regular participant living on ice. I have not used any internal access or non-public information in writing this post. As a condition of my employment, I agreed to a set of restrictions regarding public disclosure of non-public Information Technology material. I fully intend to honor these restrictions. These restrictions are ordinary and typical of US government contract work. It is unlikely that I will be able to answer additional questions about matters I discuss in this post. I’ve taken great care to write as much as I am able to, without disclosing non-public information regarding government IT systems. Good? Ok, here we go. … actually wait, sorry, one more disclaimer. This information reflects my own personal experience in Antarctica, from August 2022 through December 2022 at McMurdo, and then from December 2022 through November 2023 at the South Pole. Technology moves quickly, and I make no claims that the circumstances of my own specific experience will hold up over time. In future years, once I’ve long-since forgotten about this post, please do not get mad at me when the on-the-ground IT experience in Antarctica evolves away from the snapshot presented here. Ok, phew. Here we go for real. It’s a non-trivial feat of engineering to get any Internet at the South Pole! If you’re bored, check out the South Pole Satellite Communications page on the public USAP.gov website, for an overview of the limited selection of satellites available for Polar use. South Pole's radomes, out in the RF sector. These radomes contain the equipment necessary to communicate with the outside world using our primary satellites. If you’re interested, perhaps also look into the 2021 Antarctic Subsea Cable Workshop for an overview of some hurdles associated with running traditional fiber to the continent. I am absolutely not in a position of authority to speculate on the future of Antarctic connectivity! Seriously. I was a low-level, seasonal IT worker in a large, complex organization. Do not email me your ideas for improving Internet access in Antarctica – I am not in a position to do anything with them. I do agree with the widespread consensus on the matter: There is tremendous interest in improving connectivity to US research stations in Antarctica. I would timidly conjecture that, at some point, there will be engineering solutions to these problems. Improved connectivity will eventually arrive in Antarctica, either through enhanced satellite technologies or through the arrival of fiber to the continent. But – that world will only exist at some point in the future. Currently, Antarctic connectivity is extremely limited. What do I mean by that? Until very recently, at McMurdo, nearly a thousand people, plus numerous scientific projects and operational workloads, all relied on a series of links that provided max, aggregate speeds of a few dozen megabits per second to the entire station. For comparison, that’s less bandwidth shared by everyone combined than what everyone individually can get on a typical 4g cellular network in an American suburb. Things are looking up! The NSF recently announced some important developments regarding Starlink at McMurdo and Palmer. I’m aware that the on-the-ground experience in McMurdo and Palmer is better now than it was even just a year ago. But – as of October 2023, the situation was still pretty dire at the South Pole. As far as I’m aware, similar developments regarding Starlink have not yet been announced for South Pole Station. As of October 2023, South Pole had the limitations described above, plus there was only connectivity for a few hours a day, when the satellites rose above the horizon and the station was authorized to use them. The satellite schedule generally shifts forward (earlier) by about 4 minutes per day, due to the difference between Sidereal time and Solar (Civil) time. The current satellite schedule can be found online, on the South Pole Satellite Communications page of the public USAP.gov website. Here’s an example of the schedule from October 2023: South Pole satellite schedule, for two weeks in October 2023. These small intermittent links to the outside world are shared by everyone at Pole, for operational, science, and community / morale usage. Complicating matters further is the unavoidable physics of this connectivity. These satellites are in a high orbit, thousands of miles up. This means high latency. If you’ve used a consumer satellite product such as HughesNet or ViaSat, you’ll understand. From my berthing room at the South Pole, it was about 750 milliseconds, round trip, for a packet to get to and from a terrestrial US destination. This is about ten times the latency of a round trip between the US East and West coasts (up to 75 ms). And it’s about thirty times the expected latency of a healthy connection from your home, on a terrestrial cable or fiber connection, to most major content delivery networks (up to 25 ms). Seriously, I can’t emphasize how jarring this is. At my apartment back home, on GPON fiber, it’s about 3 ms roundtrip to Fastly, Cloudflare, CloudFront, Akamai, and Google. At the South Pole, the latency was over two hundred and fifty times greater. I can’t go into more depth about how USAP does prioritization, shaping, etc, because I’m not authorized to share these details. Suffice to say, if you’re an enterprise network engineer used to working in a bandwidth-constrained environment, you’ll feel right at home with the equipment, tools, and techniques used to manage Antarctic connectivity. Any individual trying to use the Internet for community use at the South Pole, as of October 2023, likely faced: Round-trip latency averaging around 750 milliseconds, with jitter between packets sometimes exceeding several seconds. Available speeds, to the end-user device, that range from a couple kbps (yes, you read that right), up to 2 mbps on a really good day. Extreme congestion, queueing, and dropped packets, far in excess of even the worst oversaturated ISP links or bufferbloat-infested routers back home. Limited availability, frequent dropouts, and occasional service preemptions. These constraints drastically impact the modern web experience! Some of it is unavoidable. The link characteristics described above are truly bleak. But – a lot of the end-user impact is caused by web and app engineering which fails to take slow/intermittent links into consideration. If you’re an app developer reading this, can you tell me, off the top of your head, how your app behaves on a link with 40 kbps available bandwidth, 1,000 ms latency, occasional jitter of up to 2,000 ms, packet loss of 10%, and a complete 15-second connectivity dropout every few minutes? It’s probably not great! And yet – these are real-world performance parameters that I encountered, under certain conditions, at the South Pole. It’s normally better than this, but this does occur, and it occurs often enough that it’s worth taking seriously. This is what happens when you have a tiny pipe to share among high-priority operational needs, plus dozens of community users. Operational needs are aggressively prioritized, and the community soaks up whatever is left. I’m not expecting miracles here! Obviously no amount of client engineering can make, say, real-time video conferencing work under these conditions. But – getting a few bytes of text in and out should still be possible! I know it is possible, because some apps are still able to do it. Others are not. Detailed, Real-world Example One day at the South Pole, I was trying to load the website ofin my browser. It’s huge! It needed to load nearly 20 MB of Javascript, just to render the main screen! And of course, the app had been updated since last time I loaded it, so all of my browser’s cached assets were stale and had to be re-downloaded. Fine! It’s slow, but at least it will work… eventually, right? Browsers do a decent job of handling slow Internet. Under the hood, the underlying protocols do a decent job at congestion control. I should get a steady trickle of data. This will be subject to the negotiated send and receive windows between client and server, which are based on the current level of congestion on the link, and which are further influenced by any shaping done by middleware along the way. It’s a complex webapp, so the app developer would also need to implement some of their own retry logic. This allows for recovery in the event that individual assets fail, especially for those long, multi-second total connectivity dropouts. But eventually, given enough time, the transfers should complete. Unfortunately, this is where things broke down and got really annoying. The developers implemented a global failure trigger somewhere in the app. If the app didn’t fully load within the parameters specified by the developer (time? number of retries? I’m not sure.), then the app stopped, gave up, redirected you to an error page, dropped all the loading progress you’d made, and implemented aggressive cache-busting countermeasures for next time you retried. The app wasn't loading fast enough, and the developers decided that the app should give up instead of continuing to load slowly. I cannot tell you how frustrating this was! Connectivity at the South Pole was never going to meet the performance expectations set by engineers using a robust terrestrial Internet connection. It’s not a good idea to hardcode a single, static, global expectation for how long 20 MB of Javascript should take to download. Why not let me load it at my own pace? I’ll get there when I get there. As long as data is still moving, however slow, just let it run. But – the developers decided that if the app didn’t load within the parameters they set, I couldn’t use it at all. And to be clear – this was primarily a messaging app. The actual content payload here, when the app is running and I’m chatting with my friends, is measured in bytes. As it turns out, our Internet performance at the South Pole was right on the edge of what the app developers considered “acceptable”. So, if I kept reloading the page, and if I kept letting it re-download the same 20 MB of Javascript, and if I kept putting up with the developer’s cache-busting shenanigans, eventually it finished before the artificial failure criteria. What this means is that I wasted extra bandwidth doing all these useless reloads, and it took sometimes hours before I was able to use the app. All of this hassle, even though, if left alone, I could complete the necessary data transfer in 15 minutes. Several hours (and a shameful amount of retried Javascript) later, I was finally able to send a short, text-based message to my friends. A successful webapp load, after lots of retrying. 809 HTTP requests, 51.4 MB of data transfer, and 26.5 minutes of loading... ...all so that I could send a 1.8 KB HTTPS POST... ...containing a 6-byte message. Does this webapp really need to be 20 MB? What all is being loaded that could be deferred until it is needed, or included in an “optional” add-on bundle? Is there a possibility of a “lite” version, for bandwidth-constrained users? In my 14 months in Antarctica, I collected dozens of examples of apps like this, with artificial constraints built in that rendered them unusable or borderline-unusable. For the rest of this post, I’ll outline some of my major frustrations, and what I would have liked to see instead that would mitigate the issues. I understand that not every app is in a position to implement all of these! If you’re a tiny app, just getting off the ground, I don’t expect you to spend all of your development time optimizing for weirdos in Antarctica. Yes, Antarctica is an edge case! Yes, 750 ms / 10% packet loss / 40 kbps is rather extreme. But the South Pole was not uniquely bad. There are entire commercial marine vessels that rely on older Inmarsat solutions for a few hundred precious kbps of data while at sea. There’s someone at a remote research site deep in the mountains right now, trying to load your app on a Thales MissionLink using the Iridium Certus network at a few dozen kbps. There are folks behind misconfigured routers, folks with flaky wifi, folks stuck with fly-by-night WISPs delivering sub-par service. Folks who still use dial-up Internet connections over degraded copper phone lines. These folks are worthy of your consideration. At the very least, you should make an effort to avoid actively interfering with their ability to use your products. So, without further ado, here are some examples of development patterns that routinely caused me grief at the South Pole. Hardcoded Timeouts, Hardcoded Chunk Size As per the above example, do not hardcode your assumptions about how long a given payload will take to transfer, or how much you can transfer in a single request. If you have the ability to measure whether bytes are flowing, and they are, leave them alone, no matter how slow. Perhaps show some UI indicating what is happening. If you are doing an HTTPS call, fall back to a longer timeout if the call fails. Maybe it just needs more time under current network conditions. If you’re having trouble moving large amounts of data in a single HTTPS call, break it up. Divide the content into chunks, transfer small chunks at a time, and diligently keep track of the progress, to allow resuming and retrying small bits without losing all progress so far. Slow, steady, incremental progress is better than a one-shot attempt to transfer a huge amount of data. If you can’t get an HTTPS call done successfully, do some troubleshooting. Try DNS, ICMP, HTTP (without TLS), HTTPS to a known good status endpoint, etc. This information might be helpful for troubleshooting, and it’s better than blindly retrying the same end-to-end HTTPS call. This HTTPS call requires a bunch of under-the-hood stuff to be working properly. Clearly it’s not, so you should make an effort to figure out why and let your user know. Example 1 - In-App Metadata Download A popular desktop application tries to download some configuration information from the vendor’s website at startup. There is a hardcoded timeout for the HTTPS call. If it fails, the app will not load. It’ll just keep retrying the same call, with the same parameters, forever. It’ll sit on the loading page, without telling you what’s wrong. I’ve confirmed this is what’s happening by reading the logs. Excerpt from debug log for a commercial desktop application, showing a request timing out after 15 seconds. Luckily, if you kept trying, the call would eventually make it through under network conditions I experienced at the South Pole. It’s frustrating that just a single hardcoded timeout value, in an otherwise perfectly-functional and enterprise-grade application, can render it almost unusable. The developers could have: Fallen back to increasingly-long timeouts to try and get a successful result. Done some connection troubleshooting to infer more about the current network environment, and responded accordingly. Shown UX explaining what was going on. Used a cached or default-value configuration, if it couldn’t get the live one, instead of simply refusing to load. Provided a mechanism for the user to manually download and install the required data, bypassing the app’s built-in (and naive) download logic. Example 2 - Chat Apps A popular chat app (“app #1”) maintains a websocket for sending and receiving data. The initialization process for that websocket uses a hardcoded 10-second timeout. Upon cold boot, when network conditions are especially congested, that websocket setup can sometimes take more than 10 seconds! We have to do a full TCP handshake, then set up a TLS session, then set up the websocket, then do initial signaling over the websocket. Remember – under some conditions, each individual roundtrip at the South Pole took multiple seconds! If the 10-second timeout elapses, the app simply does not work. It enters a very long backoff state before retrying. The UX does not clearly show what is happening. Excerpt from debug log for chat app #1, showing the hardcoded 10-second timeout. We did indeed have Internet access at this time -- it was just too congested to complete an entire TCP handshake + TLS negotiation + websocket setup + request within this timeframe. With a few more seconds, it may have finished. On the other hand, a competitor’s chat app (“app #2”) does very well in extremely degraded network conditions! It has multiple strategies for sending network requests, for resilience against certain types of degradation. It aggressively re-uses open connections. It dynamically adjusts timeouts. In the event of a failure, it intelligently chooses a retry cadence. And, throughout all of this, it has clear UX explaining current network state. The end result is that I could often use app #2 in network conditions when I could not use app #1. Both of them were just transmitting plain text! Just a few actual bytes of content! And even when I could not use app #2, it was at least telling me what it was trying to do. App #1 is written naively, with baked-in assumptions about connectivity that simply did not hold true at the South Pole. App #2 is written well, and it responds gracefully to the conditions it encounters in the wild. Example 3 - Incremental Transfer A chance to talk about my own blog publishing toolchain! The site you’re reading right now is a static Jekyll blog. Assets are stored on S3 and served through CloudFront. I build the static files locally here on my laptop, and I upload them directly to S3. Nothing fancy. No servers, no QA environment, no build system, no automated hooks, nothing dynamic. Given the extreme connectivity constraints at the South Pole, I wrote a Python script for publishing to S3 that worked well in the challenging environment. It uses the S3 API to upload assets in small chunks. It detects and resumes failed uploads without losing progress. It waits until everything is safely uploaded before publishing the new version. If I can do it, unpaid, working alone, for my silly little hobby blog, in 200 lines of Python… surely your team of engineers can do so for your flagship webapp. It’s amazing the usability improvements that come along with some proactive engineering. I had friends at Pole with blogs on commercial platforms, and who routinely shared large files to social media sites. They had to carefully time their day to maximize the likelihood of a successful “one-shot” upload, during a satellite window, using their platform’s poorly-engineered publishing tools. Often it took several retries, and it’s not always clear what was happening at every step of the process (Is the content live? Did the upload finish? Is it safe / should I hit “Post” again?). Meanwhile, I was able to harvest whatever connectivity I could find. I got a few kilobytes uploaded here and there, whenever it was convenient. If a particular chunked POST failed, no worries! I could retry or resume, with minimal lost progress, at a later time. Once it was all done and staged, I could safely publish the new version. My custom publishing script for this blog, to handle intermittent and unreliable Internet. Bring-Your-Own-Download If you’re going to build in a downloader into your app, you have a high bar for quality that you have to meet. Otherwise, it’s going to fail in profoundly annoying or catastrophic ways. If I had to give one piece of advice: Let the user break out of your in-app downloader and use their own, if at all possible. Provide a manual download link, ideally one that leads to whatever differential patch file the app was going to download. Don’t punish the user by making them download the full installer, just because your in-app patch downloader doesn’t meet their needs. This has the following benefits: If your downloader fails, the user can still get the file manually, using a more robust downloader of their choice, such as a web browser. The user can download the file one time, and share it with multiple devices. The user can download the file on a different computer than the one running the application. The user has the flexibility to schedule or manage the download based on whatever constraints they face. Why is this all so important when considering users at the South Pole? Downloads are possible at the South Pole, but they are subject to unique constraints. The biggest constraint is the lack of 24x7 Internet. While I was there, I knew we would lose Internet access at a certain time! It’s a frustrating reality: with most apps that do their own downloads, we were powerless to do anything about this known break in connectivity. We just had to sit there and watch it fail, and often watch all our progress be lost. Let’s say I had a 4-hour window, every day, during which I could do (very slow!!) downloads. If the total amount of data I could download in those 4 hours was less than the total size of the payload I was downloading, then there is no way I could complete the download in one shot! I’d have to split it over multiple Internet windows. Often the app wouldn’t let me do so. And that’s not even considering the fact that access might be unreliable during that time! What if the underlying connection dropped and I had to resume the download? What if my plans changed and I needed to pause? I didn’t want to waste whatever precious little progress I’d made so far. A lot of modern apps include their own homegrown, artisanal, in-app downloaders for large payloads. By “in-app downloader”, I’m referring to the system that obtains the content for automatic updates, patches, content database updates, etc. The common theme here is that the app transparently downloads content for you, without you being exposed to the underlying details such as the URL or raw file. This includes UI patterns such as Check for updates, Click here to download new version, etc. An in-app download notification for a popular chat application. This app apparently wants to download 83 MB of data in the background! This is tough at the South Pole. Will the UI be accommodating of the unique constraints at Pole? Unfortunately, most of these in-app downloaders are woefully ill-equipped for the task! Many of them lack pause/resume functionality, state notifications, retry logic, and progress tracking. Many of them have frustrating restrictions, such as time limits for downloading the payload. While most of these issues are mere annoyances in the land of fast Internet, at the South Pole, they can make or break the app entirely. Unfortunately, that's a resounding \"no\". There's no speed indication, no ETA, no pause button, no cancel button, no URL indication (so we can download manually), and no way to get at the underlying file. It was always frustrating to face down one of these interfaces, because I knew how much time, and data transfer, was going to be wasted. Darn, it failed! This was expected -- an uninterrupted 83 MB download is tough. Unfortunately, all progress has been lost, and now it's not even offering a patch on the retry -- the download size has ballooned to 133 MB, the size of the full installer. Every app that includes an in-app downloader has to compete with an extraordinarily high bar for usability: web browsers. Think about it! Every modern web browser includes a download manager that contains Abort, Pause, and Resume functionality. It allows you to retry failed downloads (assuming the content URL doesn’t include an expiring token). It clearly shows you current status, download speed, and estimated time remaining. It allows you to choose where you save the underlying file, so you can copy it around if needed. And – it doesn’t include arbitrary performance cutoffs! If you really want to download a multi-gigabyte file at 60 kbps, go for it! A fully-featured download experience, from a major web browser. Downloading an app installer from the vendor's website. Note the status, speed, estimated time remaining, full URL, and pause / cancel buttons. A partially-downloaded file, for the above-mentioned download. Here are a few more examples of where in-app downloaders caused us grief. Example 1 - macOS Updates It’s no secret that macOS updates are huge. This is sometimes even annoying back home, and it was much worse at the South Pole. The patch size for minor OS updates is usually between 0.5 and 1.5 gigabytes. Major OS upgrade patches are sometimes 6+ gigabytes. Additional tools, such as Xcode, are often multiple gigabytes. Sigh, yet another 1 GB patch for my personal macOS device at the South Pole. If every single macOS device at the South Pole downloaded these updates, directly from Apple, we would have wasted a tremendous amount of bandwidth. And the built-in macOS downloader certainly wanted us to do this! Look at this interface – few controls, no way to break out and easily get the underlying patch files. If I canceled the download, or if it failed for some reason, it didn’t always intelligently resume. Sometimes, I lost all my progress. The macOS updater. No pause button! Was I expected to leave my laptop on, connected to the Internet, and untouched, for 15 days?? Now – Apple does have a caching server feature built into macOS. In theory, this should alleviate some of the burden! We should be able to leverage this feature to ensure each patch is only downloaded to the South Pole one time, and then client Macs will hit the cache. I experimented with this feature in my spare time, with a handful of my own Apple devices. In practice, this feature still required each client Macbook to make a successful HTTPS call directly to Apple, to negotiate cache parameters. If this call failed, which it often did (because of hardcoded short timeouts!!!), then the client Mac just fetched the patch from public Apple servers. No retry, no notification. The client Mac just made a unilateral decision to bypass the cache, without any recourse or even a notification for the user. In practice, this initial cache negotiation call failed often enough at the South Pole that the caching feature wasn’t useful. What we could do was to fetch the full installer (12 gigabytes!) from Apple. Links to the full installer packages are conveniently aggregated on the Mr. Macintosh blog. We could pull the full installer down to the South Pole slowly and conscientiously: throttled, at low, background priority, using robust, interrupt-tolerant tooling, with support for caching and resumption of paused or failed transfers. Once we had the file, we could distribute it on station. This process could take several days, but it was reliable. The macOS full installer, painstakingly and conscientiously downloaded to the South Pole. But even this didn’t solve the problem! If the client Mac is Apple Silicon, it still insisted on downloading additional content directly from Apple, even if you ran the update using the full, 12 GB installer. There is no way to bypass or cache this. If the OS update required certain types of firmware updates or Rosetta updates, then every Apple Silicon client Mac would still download 1-2 GB of data directly from Apple during the install process. Even worse, the download process was sometimes farmed out to a separate component in macOS, which didn’t even report progress to the installer! Installing a macOS update at the South Pole meant staring at a window that said “installing, 32 minutes remaining”, for several hours, while a subcomponent of macOS downloaded a gigabyte of un-cacheable data in the background. Apple naively assumed that the 1 GB download would be so fast that they didn’t bother incorporating download speed feedback into the updater’s time estimate. They did not anticipate people installing macOS updates from a location where a gigabyte of downloads can take several hours, if not days. You can’t cache it, and you can’t download it directly using a web browser (or other mechanism). You have to let Apple’s downloader do it directly. And, of course, there’s no pause button. It is a major inconvenience to users, and a major waste of bandwidth, for each individual client Mac to download 1-2 GB of data in a single, uninterrupted shot. Ways that Apple could make this significantly better for users with slow or otherwise-weird Internet: Compute the required patch, and then give us a download link, so we can download it outside of Apple’s downloader. Improve the built-in update download tool with pause/resume functionality and intelligent state management, to ensure progress isn’t lost. Fix the full installer, so it includes everything, including all the currently-excluded items such as firmware and Rosetta updates for Apple Silicon Macs. It would be much more useful if it included everything. I could download it once, and then distribute it, without worrying about each Mac still needing to fetch additional data from Apple. Improve the Apple Caching Server feature, so it’s more reliable in situations where direct Internet access is unreliable. Give us more controls so that we can force a Mac to use it, and so that we can force the caching server to proactively download an item that we know will be needed in the future. As it stands, it was a huge hassle for me to help people with macOS updates at the South Pole. Example 2 - Samsung Android Phone OS Updates My Samsung Android phone receives periodic OS updates. These updates include relevant Android patches, as well as updates to the Samsung UI and other OS components. The updater is a particularly bad example of an app that fails to consider slow / intermittent Internet use cases. Downloading an OS update for my Samsung Android phone at the South Pole. First, the basics. There is no speed indicator, no numeric progress indicator (good luck counting pixels on the moving bar), no pause button, no cancel button, no indicator of the file size, and no way to get at the underlying file to download separately. Second – if the download fails, it cannot be resumed. It will restart from the beginning. In practice, at the South Pole, the phone could not download an entire OS update on a single satellite pass. So – it inevitably failed as soon as connectivity dropped, and I had to restart it from the beginning. The only way I was able to get this done was by turning off the phone entirely, right before Internet access dropped, and then turning it back on when Internet access resumed at the next satellite pass. This tricked the phone into not giving up on the download, because it was totally off during the period without Internet. It never had a chance to fail. By doing this, I was able to spread out the download across multiple satellite passes, and I could complete the download. This is an absurd workaround! I should not have had to do this. My US carrier (Verizon) does offer a downloadable application for macOS and Windows which should, in theory, allow me to flash the OS updates from my computer, instead of relying on the phone to download the patches. In practice, the Verizon app is even worse. Buggy, unreliable, and also insists on using its own in-app downloader to fetch the update files (sigh…). I’m sure there’s a way I could have gotten the images and flashed them manually. This is not an invitation for a bunch of Android enthusiasts to email me and explain bootloaders and APKs and ROMs and sideloading and whatever else is involved here. That’s not the point. The point is – the mainstream tools that vendors ship are hopelessly deficient for users on slow Internet, and that’s a bummer. Example 3 - Small App Auto-Updater A small desktop app has an in-app downloader for updates. Can you spot the issues? Downloading an in-app update. Let’s count them: No pause button. No cancel button. No progress indicator of any kind. No speed / time remaining indicator. No way to get at the underlying URL, so I can use my own downloader. No progress tracking and no graceful resumption of an interrupted download. This is actually one of my favorite desktop apps! It’s a shame to call them out like this. A quick, easy way to make this MUCH better for users at the South Pole would be to provide a manual download link. Then, the developers wouldn’t need to reimplement all the nice download features that my browser provides. I could just use my browser. Example 4 - Yet Another App Auto-Updater Here’s another one! Downloading another in-app update. Let’s count the issues: No pause button. No numeric progress / speed indicator. No way to get at the underlying URL, so I can use my own downloader. No progress tracking and no graceful resumption of an interrupted download. It does have a few things going for it: Cancel button. Visual progress indicator. But – overall, still a frustrating user experience for users with slow or intermittent Internet access. Example 5 - Microsoft Office for Mac Credit where credit is due – Microsoft has a GREAT auto-updater built into Office for Mac! Check it out: Downloading Office for macOS updates at the South Pole. Look at all these nice features! Pause button! Cancel buttons! Progress indicator! Speed and time remaining indicators! Graceful resumption of interrupted downloads! The only thing that could have made this better is a link to get at the underlying URL, so I could use my own downloader. But, given how good this interface is, I didn’t mind using it, even at the South Pole. Conclusion I hope the examples I’ve shown in this post have been a helpful illustration of how minor oversights or under-developed features back home can become major issues in a place with slow Internet. Again, I’m not asking that every app developer spend a huge amount of time optimizing for edge cases like the South Pole. And I’m also definitely not asking for people to work miracles. Internet access at the South Pole, as of October 2023, was slow. I don’t expect immersive interactive streaming media to work under the conditions I described here, but it would be nice if apps were resilient enough to get a few bytes of text up or down. Unfortunately, what often ended up happening is that apps got stuck in a loop because of an ill-advised hardcoded timeout. I hope everyone found this helpful, or at least interesting. And thank you again to everyone who followed along with me on my Antarctic journey! I’ve been off-ice for about six months now, and going through my old posts here have brought back fond memories. I hope the current winter-over crew is doing well, and that everyone is enjoying the Polar Night. If the egg supply and consumption rate is the same as it was during Winter 2023, they should soon be finishing up The Last Egg. I won’t promise any more content, but I do have a handful of other half-finished posts sitting in my drafts. We’ll see!",
    "commentLink": "https://news.ycombinator.com/item?id=40531100",
    "commentBody": "Engineering for Slow Internet (brr.fyi)701 points by jader201 15 hours agohidepastfavorite276 comments padolsey 12 hours agoA lot of this resonates. I'm not in Antartica, I'm in Beijing, but still struggle with the internet. Being behind the great firewall means using creative approaches. VPNs only sometimes work, and each leaves a signature that the firewall's hueristics and ML can eventually catch onto. Even state-mandated ones are 'gently' limited at times of political sensitivity. It all ends up meaning that, even if I get a connection, it's not stable, and it's so painful to sink precious packets into pointless web-app-react-crap roundtrips. I feel like some devs need to time-travel back to 2005 or something and develop for that era in order to learn how to build things nimbly. In deficit of time travel, if people could just learn to open web tools and use its throttling tool: turn it to 3g, and see if their webapp is resilient. Please! reply lukan 8 hours agoparent\"I feel like some devs need to time-travel back to 2005 or something and develop for that era in order to learn how to build things nimbly.\" No need to invent time travel, just let them have a working retreat somewhere with only bad mobile connection for a few days. reply qingcharles 1 hour agorootparentAmen to this. And give them a mobile cell plan with 1GB of data per month. I've seen some web sites with 250MB payloads on the home page due to ads and pre-loading videos. I work with parolees who get free government cell phones and then burn through the 3GB/mo of data within three days. Then they can't apply for jobs, get bus times, rent a bike, top up their subway card, get directions. reply jmbwell 1 hour agorootparent\"But all the cheap front-end talent is in thick client frameworks, telemetry indicates most revenue conversions are from users on 5G, our MVP works for 80% of our target user base, and all we need to do is make back our VC's investment plus enough to cash out on our IPO exit strategy, plus other reasons not to care\" — self-identified serial entrepreneur, probably reply lukan 48 minutes agorootparentprevHaving an adblocker (firefox mobile works with uBlock origin) and completely deactivate loading of images and videos can get you quite far with limited connection. reply mkroman 4 hours agorootparentprevJust put them on a train during work hours! We have really good coverage here but there's congestion and frequent random dropouts, and a lot of apps just don't plan for that at all. reply rpastuszak 6 hours agoparentprevI lived in Shoreditch for 7 years and most of my flats had almost 3G internet speeds. The last one had windows that incidentally acted like a faraday cage. I always test my projects with throttled bandwidth, largely because (just like with a11y) following good practices results in better UX for all users, not just those with poor connectivity. Edit: Another often missed opportunity is building SPAs as offline-first. reply zerkten 5 hours agorootparent>> Another often missed opportunity is building SPAs as offline-first. You are going to get so many blank stares at many shops building web apps when suggesting things like this. This kind of consideration doesn't even enter into the minds of many developers in 2024. Few of the available resources in 2024 address it that well for developers coming up in the industry. Back in the early-2000s, I recall these kinds of things being an active discussion point even with work placement students. Now that focus seems to have shifted to developer experience with less consideration on the user. Should developer experience ever weigh higher than user experience? reply salawat 4 hours agorootparent>Should developer experience ever weigh higher than user experience? Developer experience is user experience. However, in a normative sense, I operate such that Developer suffering is preferable to user suffering to get any arbitrary task done. reply analyte123 2 hours agorootparentprevSPAs and \"engineering for slow internet\" usually don't belong together. The giant bundles usually guarantee slow first paint, and the incremental rendering/loading usually guarantees a lot of network chatter that randomly breaks the page when one of the requests times out. Most web applications are fundamentally online. For these, consider what inspires more confidence when you're in a train on a hotspot: an old school HTML forms page (like HN), or a page with a lot of React grey placeholders and loading spinners scattered throughout? I guess my point is that while you can take a lot of careful time and work to make an SPA work offline-first, as a pattern it tends to encourage the bloat and flakiness that makes things bad on slow internet. reply CM30 6 hours agorootparentprevOh, London is notorious for having... questionable internet speeds in certain areas. It's good if you live in a new build flat/work in a recently constructed office building or you own your own home in a place OpenReach have gotten to yet, but if you live in an apartment building/work in an office building more than 5 or so years old? Yeah, there's a decent chance you'll be stuck with crappy internet as a result. I still remember quite a few of my employers getting frustrated that fibre internet wasn't available for the building they were renting office space in, despite them running a tech company that really needed a good internet connection. reply joseda-hg 5 hours agoparentprevI live in a well connected city, but my work only pays for other continent based Virtual Machines so most of my projects end up \"fast\" but latency bound, it's been an interesting exercise of minimizing pointless roundtrips in a technology that expects you to use them for everyting reply iforgotpassword 10 hours agoparentprevTried multiple VPNs in China and finally rolled my own obfuscation layer for Wireshark. A quick search revealed there are multiple similar projects on GitHub, but I guess the problem is once they get some visibility, they don't work that well anymore. I'm still getting between 1 and 10mbit/s (mostly depending on time of day) and pretty much no connectivity issues. reply LorenzoGood 7 hours agorootparentWireguard? reply iforgotpassword 6 hours agorootparentHaha yes, thanks. I used Wireshark extensively the past days to debug a weird http/2 issue so I guess that messed me up a bit ;) reply LorenzoGood 5 hours agorootparentI do that too looking stuff up. reply devjab 11 hours agoparentprevWe design for slow internet, react is one of the better options for it with ssr, code splitting and http2 push, mixed in with more off-line friendly clients like Tauri. You can also deploy very near people if you work “on the edge”. I’m not necessarily disagreeing with your overall point, but modern JS is actually rather good at dealing with slow internet for server-client “applications”. It’s not necessarily easy to do, and there is almost no online resources that you can base your projects on if you’re a Google/GPT programmer. Part of this is because of the ocean of terrible JS resources online, but a big part of it is also that the organisations which work like this aren’t sharing. We have 0 public resources for the way we work as an example, because why would we hand that info to our competition? reply matthews2 8 hours agorootparentHTTP/2 push is super dead: https://evertpot.com/http-2-push-is-dead/ reply jiggawatts 10 hours agorootparentprevBy far the lightest weight JS framework isn't React, it's no javascript at all. I regularly talk to developers who aren't even aware that this is an option. reply mike_hearn 10 hours agorootparentIf you're behind an overloaded geosynchronous satellite then no JS at all just moves the pain around. At least once it's loaded a JS-heavy app will respond to most mouse clicks and scrolls quickly. If there's no JS then every single click will go back to the server and reload the entire page, even if all that's needed is to open a small popup or reload a single word of text. reply rcxdude 8 hours agorootparentThis makes perfect sense in theory and yet it's the opposite of my experience in practice. I don't know how, but SPA websites are pretty much always much more laggy than just plain HTML, even if there are a lot of page loads. reply nicoburns 5 hours agorootparentIt often is that way, but it's not for technical reasons. They're just poorly written. A lot of apps are written by inexperienced teams under time pressure and that's what you're seeing. Such teams are unlikely to choose plain server-side rendering because it's not the trendy thing to do. But SPAs absolutely can be done well. For simple apps (HN is a good example) you won't get too much benefit, but for more highly interactive apps it's a much better experience than going via the server every time (setting filters on a shopping website would be a good example). reply chefandy 4 hours agorootparentYep. In SPAs with good architecture, you only need to load the page once, which is obviously weighed down by the libraries, but largely is as heavy or light as you make it. Everything else should be super minimal API calls. It's especially useful in data-focused apps that require a lot of small interactions. Imagine implementing something like spreadsheet functionality using forms and requests and no JavaScript, as others are suggesting all sites should be: productivity would be terrible not only because you'd need to reload the page for trivial actions that should trade a but of json back and forth, but also because users would throw their devices out the window before they got any work done. You can also queue and batch changes in a situation like that so the requests are not only comparatively tiny, you can use fewer requests. That said, most sites definitely should not be SPAs. Use the right tool for the job reply nicoburns 3 hours agorootparent> which is obviously weighed down by the libraries, but largely is as heavy or light as you make it One thing which surprised me at a recent job was that even what I consider to be a large bundle size (2MB) didn't have much of an effect on page load time. I was going to look into bundle splitting (because that included things like a charting library that was only used in a small subsection of the app). But in the end I didn't bother because I got page loads fast (~600ms) without it. What did make a huge different was cutting down the number of HTTP requests that the app made on load (and making sure that they weren't serialised). Our app was originally going auth by communicating with Firebase Auth directly from the client, and that was terrible for performance because that request was quite slow (most of second!) and blocked everything else. I created an all-in-one auth endpoint that would check the user's auth and send back initial user and app configuration data in one ~50ms request and suddenly the app was fast. reply chefandy 1 hour agorootparentIn many cases, like satellite Internet access or spotty mobile service, for sure. But if you have low bandwidth but fast response times, that 2mb is murder and the big pile o requests is NBD.If you have slow response times but good throughput, the 2MB is NBD but the requests are murder. An extreme and outdated example, but back when cable modems first became available, online FPS players were astonished to see how much better the ping times were for many dial up players. If you were downloading a floppy disk of information, the cable modem user would obviously blow them away, but their round trip time sucked! Like if you're on a totally reliable but low throughput LTE connection, the requests are NBD but the download is terrible. If you're on spotty 5g service, it's probably the opposite. If you're on, like, a heavily deprioritized MVNO with a slower device, they both super suck. It's not like optimization is free though, which is why it's important to have a solid UX research phase to get data on who is going to use it, and what their use case is. reply bobbob1921 3 hours agorootparentprevMy experience agrees with this comment – I’m not sure why web browsers seem to frequently get hung up on only some Http requests at times, unrelated to the actual network conditions. Ie: in the browser the HTTP request is timing out or in a blocked state and hasn’t even reached the network layer when this occurs. (Not sure if I should be pointing the finger here at the browser or the underlying OS). However, when testing slow / stalled loading issues, this (the browser itself) is frequently one of the culprits- however, this issue I am referring to even further reinforces the article/sentiments on this HN thread (cut down on the number of requests / bloat, and this issue too can be avoided.) reply chefandy 1 hour agorootparentIf if the request itself hasn't reached the network layer but is having a networky feeling hang, I'd look into DNS. It's network dependent but handled by the system so it wouldn't show up in your web app requests. I'm sure there's a way to profile this directly but unless I had to do it all the time I'd probably just fire up wireshark. reply NohatCoder 7 hours agorootparentprevHaving written a fair amount of SPA and similar I can confirm that it is actually possible to just write some JavaScript that does fairly complicated jobs without the whole thing ballooning into the MB space. I should say that I could write a fairly feature-rich chat-app in say 500 kB of JS, then minified and compressed it would be more like 50 kB on the wire. How my \"colleagues\" manage to get to 20 MB is a bit of mystery. reply RunSet 7 hours agorootparent> How my \"colleagues\" manage to get to 20 MB is a bit of mystery. More often than not (and wittingly or not) it is effectively by using javascript to build a browser-inside-the-browser, Russian doll style, for the purposes of tracking users' behavior and undermining privacy. Modern \"javascript frameworks\" do this all by default with just a few clicks. reply bayindirh 10 hours agorootparentprevHowever, getting 6.4KB of data (just tested on my blog) or 60KB of data (a git.sr.ht repository with a README.md and a PNG) is way better than getting 20MB of frameworks in the first place. reply mike_hearn 7 hours agorootparentYeah, but your blog is not a full featured chat system with integrated audio and video calling, strapped on top of a document format. There are a few architectural/policy problems in web browsers that cause this kind of expansion: 1. Browsers can update large binaries asynchronously (=instant from the user's perspective) but this feature is only very recently available to web apps via obscure caching headers and most people don't know it exists yet/frameworks don't use it. 2. Large download sizes tend to come from frameworks that are featureful and thus widely used. Browsers could allow them to be cached but don't because they're over-aggressive at shutting down theoretical privacy problems, i.e. the browser is afraid that if one site learns you used another site that uses React, that's a privacy leak. A reasonable solution would be to let HTTP responses opt in to being put in the global cache rather than a partitioned cache, that way sites could share frameworks and they'd stay hot in the cache and not have to be downloaded. But browsers compete to satisfy a very noisy minority of people obsessed with \"privacy\" in the abstract, and don't want to do anything that could kick up a fuss. So every site gets a partitioned cache and things are slow. 3. Browsers often ignore trends in web development. React style vdom diffing could be offered by browsers themselves, where it'd be faster and shipped with browser updates, but it isn't so lots of websites ship it themselves over and over. I think the SCIter embedded browser actually does do this. CSS is a very inefficient way to represent styling logic which is why web devs write dialects like sass that are more compact, but browsers don't adopt it. I think at some pretty foundational level the way this stuff works architecturally is wrong. The web needs a much more modular approach and most JS libraries should be handled more like libraries are in desktop apps. The browser is basically an OS already anyway. reply Sesse__ 3 hours agorootparent> CSS is a very inefficient way to represent styling logic which is why web devs write dialects like sass that are more compact, but browsers don't adopt it. I don't know exactly which features you are referring to, but you may have noticed that CSS has adopted native nesting, very similarly to Sass, but few sites actually use it. Functions and mixins are similar compactness/convenience topics being worked on by the CSSWG. (Disclosure: I work on style in a browser team) reply mike_hearn 2 hours agorootparentI hadn't noticed and I guess this is part of the problem. Sorry this post turned into a bit of a rant but I wrote it now. When it was decided that HTML shouldn't be versioned anymore it became impossible for anyone who isn't a full time and very conscientious web dev to keep up. Versions are a signal, they say \"pay attention please, here is a nice blog post telling you the most important things you need to know\". If once a year there was a new version of HTML I could take the time to spend thirty minutes reading what's new and feel like I'm at least aware of what I should learn next. But I'm not a full time web dev, the web platform changes constantly, sometimes changes appear and then get rolled back, and everyone has long since plastered over the core with transpilers and other layers anyway. Additionally there doesn't seem to be any concept of deprecating stuff, so it all just piles up like a mound of high school homework that never shrinks. It's one of the reasons I've come to really dislike CSS and HTML in general (no offense to your work, it's not the browser implementations that are painful). Every time I try to work out how to get a particular effect it turns out that there's now five different alternatives, and because HTML isn't versioned and web pages / search results aren't strongly dated, it can be tough to even figure out what the modern way to do it is at all. Dev tools just make you even more confused because you start typing what you think you remember and now discover there are a dozen properties with very similar names, none of which seem to have any effect. Mistakes don't yield errors, it just silently does either nothing or the wrong thing. Everything turns into trial-and-error, plus fixing mobile always seems to break desktop or vice-versa for reasons that are hard to understand. Oh and then there's magic like Tailwind. Gah. I've been writing HTML since before CSS existed, but feel like CSS has become basically non-discoverable by this point. It's understandable why neither Jetpack Compose nor SwiftUI decided to adopt it, even whilst being heavily inspired by React. The CSS dialect in JavaFX I find much easier to understand than web CSS, partly because it's smaller and partly because it doesn't try to handle layout. The way it interacts with components is also more logical. reply Sesse__ 1 hour agorootparentYou may be interested in the Baseline initiative, then. (https://web.dev/baseline/2024) reply mike_hearn 46 minutes agorootparentThat does look useful, thanks! reply andrepd 9 hours agorootparentprevYes. It's inexcusable that text and images and video pulls in megabytes of dependencies from dozens of domains. It's wasteful on every front: network, battery, and it's also SLOW. reply LtWorf 4 hours agorootparentThe crap is that even themes for static site generators like mkdocs link resources from cloudflare rather than including them in the theme. For typedload I've had to use wget+sed to get rid of that crap after recompiling the website. https://codeberg.org/ltworf/typedload/src/branch/master/Make... reply RGamma 8 hours agorootparentprevAlso wonder how many savings are still possible with a more efficient HTML/CSS/JS binary representation. Text is low tech and all but it still hurts to waste so many octets for such a relatively low amount of possible symbols. Applies to all formal languages actually. 2^(8x20x10^6) ~= 2x10^48164799 is such a ridiculously large space... reply chris_pie 8 hours agorootparentCheck this proposal out: https://github.com/tc39/proposal-binary-ast reply pgraf 8 hours agorootparentprevShouldn’t HTTP compression reap most of the benefits of this approach for bigger pages? reply jiggawatts 7 hours agorootparentprevThe generalisation of this concept is what I like call the \"kilobyte\" rule. A typical web page of text on a screen is about a kilobyte. Sure, you can pack more in with fine print, and obviously additional data is required to represent the styling, but the actual text is about 1 kb. If you've sent 20 MB, then that is 20,000x more data than what was displayed on the screen. Worse still, an uncompressed 4K still image is only 23.7 megabytes. At some point you might be better off doing \"server side rendering\" with a GPU instead of sending more JavaScript! reply Lex-2008 5 hours agorootparent> \"server side rendering\" with a GPU instead of sending more JavaScript Some 7~10 years ago I remember I saw somewhere (maybe here on HN) a website which did exactly this: you gave it an URL - it downloaded a webpage with all its resources, rendered and screenshot'ed it (probably in headless Chrome or something), and compared size of png screenshot versus size of webpage with all its resources. For many popular websites, png screenshot of a page indeed was several times less than webpage itself! reply skydhash 5 hours agorootparentprevI read epubs, and they’re mostly html and css files zipped. The whole book usually comes under a MB if there’s not a lot of big pictures. Then you come across a website and for just an article you have to download tens of MBs. Disable JavaScript and the website is broken. reply jcgrillo 5 hours agorootparentprevIf your server renders the image as text we'll be right back down towards a kilobyte again. See https://www.brow.sh/ reply LtWorf 4 hours agorootparentprevSurely you're aware of gzip encoding on the wire for http right? reply RGamma 3 hours agorootparentSure, would be interesting to know how it would fare against purpose-made compression under real world conditions still... reply mwcampbell 9 hours agorootparentprevFalse dichotomy, with what is likely extreme hyperbole on the JS side. Are there actual sites that ship 20 MB, or even 5 MB or more, of frameworks? One can fit a lot of useful functionality in 100 KB or less of JSA lot of this resonates. I'm not in Antartica, I'm in Beijing, but still struggle with the internet. Not even that, with outer space travel, we all need to build for very slow internet and long latency. Devs do need to time-travel back to 2005. reply andrepd 9 hours agorootparentI'm sure this is not what you meant but made me lol anyways: sv techbros would sooner plan for \"outer space internet\" than give a shit about the billions of people with bad internet and/or a phone older than 5 years. reply geek_at 11 hours agoparentprevSounds like what would benefit you is a HTMX approach to the web. reply tleb_ 11 hours agorootparentWhat about plain HTML & CSS for all the websites where this approach is sufficient? Then apply HTMX or any other approach for the few websites that are and need to be dynamic. reply sethammons 11 hours agorootparentThat is exactly what htmx is and does. Everything is rendered server side and sections of the page that you need to be dynamic and respond to clicks to fetch more data have some added attributes reply tleb_ 11 hours agorootparentI see two differences: (1) the software stack on the server side and (2) I guess there is JS to be sent to the client side for HTMX support(?). Both those things make a difference. reply victorbjorklund 9 hours agorootparentThe size of HTMX compressed is 10kb and very rarely changes which means it can stay in your cache for a very long time. reply galangalalgol 5 hours agorootparentI'm embedded so I don't much about web stuff but sometimes I create dashboards to monitor services just for our team, tganks for introducing me to htmx. I do think html+css should be used for anything that is a document or static for longer than a typical view lasts. Arxiv is leaning towards HTML+css vs latex in acknowledgement that paper is no longer how \"papers\" are read. And on the other end, eBay works really well with no js right up until you get to an item's page, where it breaks. If ebay can work without js, almost anything that isn't monitoring and visualizing constant data (last few minutes of a bid, or telemetry from an embedded sensor) can work without js. I don't understand how amazon.com has gotten so slow and clunky for instance. I have been using wasm and webgpu for visualization, partly to offload any burden from the embedded device to be monitored, but that could always be a third machine. Htmx says it supports websockets, is there a good way to have it eat a stream and plot data as telemetry, or is that time for a new tool? reply mohn 11 hours agorootparentprevIt sounds like GP would benefit from satellite internet bypassing the firewall, but I don't know how hard the Chinese government works to crack down on that loophole. reply kylehotchkiss 12 hours agoparentprevI hear you on frontend-only react. But hopefully the newer React Server Components are helping? They just send HTML over the wire (right?) reply crote 10 hours agorootparentThe problem isn't in what is being sent over the wire - it's in the request lifecycle. When it comes to static HTML, the browser will just slowly grind along, showing the user what it is doing. It'll incrementally render the response as it comes in. Can't download CSS or images? No big deal, you can still read text. Timeouts? Not a thing. Even if your Javascript framework is rendering HTML chunks on the server, it's still essentially hijacking the entire request. You'll have some button in your app, which fires off a request when clicked. But it's now up to the individual developer to properly implement things like progress bars/spinners, timouts, retries, and all the rest the browser normally handles for you. They never get this right. Often you're stuck with an app which will give absolutely zero feedback on user action, only updating the UI when the response has been received. Request failed? Sorry, gotta F5 that app because you're now stuck in an invalid state! reply MatthiasPortzel 5 hours agorootparentYep. I’m a JS dev who gets offended when people complain about JS-sites being slower because there’s zero technical reason why interactions should be slower. I honestly suspect a large part of it is that people don’t expect clicking a button to take 300ms and so they feel like the website must be poorly programmed. Whereas if they click a link and it takes 300ms to load a new version of the page they have no ill-will towards the developer because they’re used to 300ms page loads. Both interactions take 300ms but one uses the browser’s native loading UI and the other uses the webpage’s custom loading UI, making the webpage feel slow. This isn’t to exonerate SPAs, but I don’t think it helps to talk about it as a “JavaScript” problem because it’s really a user experience problem. reply padolsey 11 hours agorootparentprevYes, server-rendering definitely helps, though I have suspicions about its compiled outputs still being very heavy. There's also a lot of CSS frameworks that have an inline-first paradigm meaning there's no saving for the browser in downloading a single stylesheet. But I'm not sure about that. reply chrisldgk 10 hours agorootparentprevYes, though server side rendering is everything but a new thing in the react world. NextJS, Remix, Astro and many other frameworks and approaches exist (and have done so for at least five years) to make sure pages are small and efficient to load. reply p3rls 3 hours agoparentprevEh, I'm a few miles from NYC and have the misfortune of being a comcast/xfinity customer and my packetloss to my webserver is sometimes so bad it takes a full minute to load pages. I take that time to clean a little, make a coffee, you know sometimes you gotta take a break and breathe. Life has gotten too fast and too busy and we all need a few reminders to slow down and enjoy the view. Thanks xfinity! reply jrochkind1 5 hours agoparentprevChrome dev tools offer a \"slow 3G\" and a \"fast 3G\". Slow 3G? With fresh cache on \"slow 3G\", my site _works_, but has 5-8 second page loads. Would you have consider that usable/sufficient, or pretty awful? reply auggierose 12 hours agoparentprevOr maybe, just get rid of the firewall. I am all for nimble tech, but enabling the Chinese government is not very high on my to-do list. reply oldherl 11 hours agorootparentPlease understand that Chinese government wants to block \"outside\" web services to Chinese residents, and Chinese residents want to access those services. So if the service itself decides to deny access from China, it's actually helping the Chinese government. reply padolsey 12 hours agorootparentprevWhether you like it or not, over 15% of the world's population lives in China. reply Arthur_ODC 4 hours agorootparentAre you a citizen of China, or move there for work/education/research? Anyway, this is very unrelated, but I'm in the USA and have been trying to sign up for the official learning center for CAXA 3D Solid Modeling (I believe it's the same program as IronCAD, but CAXA 3D in China seems to have 1000x more educational videos and Training on the software) and I can't for the life of me figure out how to get the WeChat/SMS login system they use to work to be able to access the training videos. Is it just impossible for a USA phone number to receive direct SMS website messages from a mainland China website to establish accounts? Seems like every website uses SMS message verification instead of letting me sign up with an email. reply auggierose 12 hours agorootparentprevnext [9 more] [flagged] padolsey 12 hours agorootparentWhat an incredibly naive and dismissive thing to say. reply jiggawatts 10 hours agorootparentIt isn't naive, and isn't dismissive. The problem is the CCP. The only fix is for the people to rise up against them. This doesn't even have to be violent. Most of the former Soviet Block governments fell without any bloodshed. What's the alternative? Wait for Xi to \"make his mark on history\" in the same way that Putin is doing in Ukraine because it's \"naive and dismissive\" to even talk about unseating him? reply wolvesechoes 7 hours agorootparentIt is always so funny to read Americans or Western Europeans saying \"just overthrow your dictator bro\". Usually told by people who never faced any political violence, or any violence for that matter. I was born and live in the ex-Soviet country, and stating that Soviet governments fell without any bloodshed is a proof of ignorance. reply Tade0 8 hours agorootparentprevBy 2017 Xi Jinping already had six failed assassination attempts against him, which prompted him to perform a large-scale purge within the ranks of the CCP. If it was all that easy, it would have been done a long time ago. reply sitkack 9 hours agorootparentprev> Most of the former Soviet Block governments fell without any bloodshed. That was Gorbachev. Most leaders of any country would roll tanks. reply jhonkola 6 hours agorootparentGorbachev sent the tanks rolling in Lithuania (https://en.wikipedia.org/wiki/January_Events). reply Thorrez 11 hours agorootparentprevAnd uncensored websites that function through the great firewall would help organize that government fixing. reply mavamaarten 12 hours agorootparentprevI mean, you're not wrong. But if you happen to not be in a position to overthrow the government, maybe the next best thing can be a more realistic approach. reply danpalmer 14 hours agoprevHaving a lot of experience commuting on underground public transport (intermittent, congested), and living/working in Australia (remote), I can safely say that most services are terrible for people without \"ideal\" network conditions. On the London Underground it's particularly noticeable that most apps are terrible at handling network that comes and goes every ~2 minutes (between stops), and which takes ~15s to connect to each AP as a train with 500 people on it all try to connect at the same time. In Australia you're just 200ms from everything most of the time. That might not seem like much, but it really highlights which apps trip up on the N+1 request problem. The only app that I am always impressed with is WhatsApp. It's always the first app to start working after a reconnect, the last to get any traffic through before a disconnect, and even with the latency, calls feel pretty fast. reply chipdart 13 hours agoparent> In Australia you're just 200ms from everything most of the time. (...) > The only app that I am always impressed with is WhatsApp. It's always the first app to start working after a reconnect, the last to get any traffic through before a disconnect, and even with the latency, calls feel pretty fast. The 200ms is telling. I bet that WhatsApp is one of the rare services you use which actually deployed servers to Australia. To me, 200ms is a telltale sign of intercontinental traffic. Most global companies deploy only to at most three regions: * the US (us-east, us-central, us-east+us-east) * Europe (west-europe), * and somewhat rarely far-east (meither us-west or Japan) This means that places such as south Africa, south America, and of course Australia typically have to pull data from one of these regions, which means latencies of at least 200ms due to physics. Australia is particularly hit because, even with dedicated deployments in their theoretical catchment area, often these servers are actualy located in an entirely separate continent (west-us or Japan) and thus users do experience the performance impact of having packets cross half a globe. reply toast0 12 hours agorootparent> I bet that WhatsApp is one of the rare services you use which actually deployed servers to Australia. To me, 200ms is a telltale sign of intercontinental traffic. So, I used to work at WhatsApp. And we got this kind of praise when we only had servers in Reston, Virginia (not at aws us-east1, but in the same neighborhood). Nowadays, Facebook is most likely terminating connections in Australia, but messaging most likely goes through another continent. Calling within Australia should stay local though (either p2p or through a nearby relay). There's lots of things WhatsApp does to improve experience on low quality networks that other services don't (even when we worked in the same buildings and told them they should consider things!) In no particular order: 0) offline first, phone is the source of truth, although there's multi-device now. You don't need to be online to read messages you have, or to write messages to be sent whenever you're online. Email used to work like this for everyone; and it was no big deal to grab mail once in a while, read it and reply, and then send in a batch. Online messaging is great, if you can, but for things like being on a commuter train where connectivity ebbs and flows, it's nice to pick up messages when you can. a) hardcode fallback ips for when DNS doesn't work (not if) b) setup \"0rtt\" fast resume, so you can start getting messages on the second round trip. This is part of noise pipes or whatever they're called, and tls 1.3 c) do reasonable-ish things to work with MTU. In the old days, FreeBSD reflected the client MSS back to it, which helps when there's a tunnel like PPPoE and it only modifies outgoing syns and not incoming syn+ack. Linux never did that, and afaik, FreeBSD took it out. Behind Facebook infrastructure, they just hardcode the mss for i think 1480 MTU (you can/should check with tcpdump). I did some limited testing, and really the best results come from monitoring for /24's with bad behavior (it's pretty easy, if you look for it --- never got any large packets and packet gaps are a multiple of MSS - space for tcp timestamps) and then sending back client - 20 to those; you could also just always send back client - 20. I think Android finally started doing pMTUD blackhole detection stuff a couple years back, Apple has been doing it really well for longer. Path MTU Discovery is still an issue, and anything you can do to make it happier is good. d) connect in the background to exchange messages when possible. Don't post notifications unless the message content is on the device. Don't be one of those apps that can only load messsages from the network when the app is in the foreground, because the user might not have connectivity then e) prioritize messages over telemetry. Don't measure everything, only measure things when you know what you'll do with the numbers. Everybody hates telemetry, but it can be super useful as a developer. But if you've got giant telemetry packs to upload, that's bad by itself, and if you do them before you get messages in and out, you're failing the user. f) pay attention to how big things are on the wire. Not everything needs to get shrunk as much as possible, but login needs to be very tight, and message sending should be too. IMHO, http and json and xml are too bulky for those, but are ok for multimedia because the payload is big so framing doesn't matter as much, and they're ok for low volume services because they're low volume. reply erinaceousjones 11 hours agorootparentWhatsApp is (or was) using XMPP for the chat part too, right? When I was IT person on a research ship, WhatsApp was a nice easy one to get working with our \"50+ people sharing two 256kbps uplinks\" internet. Big part of that was being able to QoS prioritise the XMPP traffic which WhatsApp was a big part of. Not having to come up with filters for HTTPS for IP ranges belonging to general-use CDNs that managed to hit the right blocks used by that app, was a definite boon. That, and the fact XMPP was nice and lightweight. As far as I know google cloud messaging (GCN? GCM? firebase? Play notifications? Notifications by Google? Google Play Android Notifications Service?) also did/does use XMPP, so we often had the bizarre and infuriating very fast notifications _where sometimes the content was in the notification_ but when you clicked on it, other apps would fail to load it due to the congestion and latency and hardcoded timeouts TFA mentions.. argh. But WhatsApp pretty much always worked, as long as the ship had an active WAN connection.... And that kept us all happy, because we could reach our families. reply toast0 4 hours agorootparent> WhatsApp is (or was) using XMPP for the chat part too, right? It's not exactly XMPP, it started with XMPP, but XML is big, so it's tokenized (some details are published in the European Market Access documentation), and there's no need for interop with standard XMPP clients, so login sequence is I think way different. But it runs on port 5222? by default (with fallbacks to port 443 and 80). I think GCM or whatever it's called today is plain XMPP (including, optionally, on the server to server side), and runs on ports 5228-5230. Not sure what protocol apple push is, but they use port 5223 which is affiliated with xmpp over tls. So I think using a non 443 port was helpful for your QoS? But being avaialable on port 443 is helpful for getting through blanket firewall rules. AOL used to run AIM on all the ports, which is even better at getting through firewalls. reply cheema33 11 hours agorootparentprev> I used to work at WhatsApp.. Do you know why there is a 4 device limit? I run into this limit quite a bit, because I have a lot more devices. And... Why is there is WhatsApp for most commonly used devices, but iPads? reply toast0 3 hours agorootparentI've been gone too long for accurate answers, but I can guess. For iPad, I think it's like the sibling notes; expected use is very low, so it didm't justify the engineering cost while I was there. But I see some signs it might happen eventually [1]; WhatsApp for Android Tablets wasn't a thing when I was there either, but it is now. For the four device limit, there's a few things going on IMHO. Synchronization is hard and the more devices are playing, the harder it is. Independent devices makes it easier in some ways because the user devices don't have to be online together to communicate (like when whatsapp web was essentially a remote control for your phone), but it does mean that all of your communications partner's devices have to work harder and the servers have to work harder, too. Four deviced covers your phone, a desktop at home and work, and a laptop; but really most of the users only have a phone. Allowing more devices makes it more likely that you'll lose track of one or not use it for long enough that it's lost sync, etc. WhatsApp has usually focused on product features that benefit the most users, and more than 4 devices isn't going to benefit many people, and 4 is plenty for internal use (phone, prod build, dev build, home computer). I'm sure they've got metrics of how many devices are used, and if there's a lot of 4 device users and enough requests, it's a #define somewhere. [1] https://www.macworld.com/article/668638/how-to-get-whatsapp-... reply swiftcoder 10 hours agorootparentprev> Why is there is WhatsApp for most commonly used devices, but iPads? I was frustrated by this a while back, so I asked the PMs. Basically when investing engineering effort WhatsApp prioritises the overall number of users connected, and supporting iPads doesn't really move that metric, because (a) the vast majority of iPad owners also own a smartphone, and (b) iPads are pretty rare outside of wealthy western cities. reply izacus 9 hours agorootparentprevYeah, it's very very noticeable that WhatsApp is architected in a way that makes experience great for all kind of poor connectivity scenarios that most other software just... isn't. reply kylehotchkiss 12 hours agoparentprevWhatsApp has a massive audience in developing countries where it's normal for people to have slower internet and much slower devices. That perspective being so embedded in their development goals certainly has given WhatsApp good reason to be the leading messaging platform in many countries around the world reply Scoundreller 5 hours agorootparentEven wealthy countries will have dead zones (Toronto subway until recently, and like 90% of the landmass), and at least in Canada, “running out of data” and just having none left (or it being extremely expensive) was relatively common until about the last year or two when things got competitive (finally!). Still have an entire territory where everything is satellite fed (Nunavut), including its capital. reply nicbou 10 hours agorootparentprevIt works remarkably well when your phone runs out of data and you get capped at 8 kbps. Even voice calls work smoothly. reply qingcharles 1 hour agorootparentLOL 8kbps. Damn. That takes me back. I built the first version of one of the world's largest music streaming sites on a 9.6kbps connection. I was working from home (we had no offices yet) and my cable Internet got cut off. My only back up was a serial cable to a 2G Nokia 9000i. I had to re-encode a chunk of the music catalog at 8kbps so I could test it from home before I pushed the code to production. Psychoacoustic compression is a miracle. reply camel-cdr 12 hours agoparentprevIt's not only the services them self. I have a very slow mobile connection, and one thing that bothered me immensly is downloading images in the browser: How is it, than when I go to a .jpg url to view an image in the browser it takes way longer and sometimes times out, than hopping over to termux and running wget. I had this problem with both firefox and chrome based browsers. Note that even the wget download usually takes 10-30 seconds on my mobile connection. reply armchair_expert 49 minutes agorootparentYou can try going into proxy settings and setting to \"none\" instead of autodetect. Also, the dns server used by the browser could be different (and slower). reply bombcar 4 hours agorootparentprevBrowsers usually try to multiplex things, sometimes even the same image if the server supports \"get specific byte range\" or whatever. There may be a setting to turn a browser back into a dumb wget visual displayer. reply chris_pie 6 hours agorootparentprevI have the same issue with nearly every static asset. reply throwaway211 5 hours agoparentprevThe London Underground not having any connectivity for decades after other metro systems showed only that high connectivity during a commute isn't necessary. reply HPsquared 4 hours agorootparentLondon fails to provide a lot of essentials. reply pimeys 13 hours agoparentprevI guess in London you get wifi only on stops, it's the same in Berlin. In Helsinki the wifi connection is available inside the trains, and in the stations. So you never get a connection loss when moving. I never understood the decision in Berlin to do this, why not just provide internet inside the train... And yeah, most of the internet works very badly when you drop the network all the time... reply bombcar 4 hours agorootparentWiFi at a stop is as easy as putting up a few wireless routers, it's a bit more complex than at home but the same general idea. Wifi inside the trains involves much more work, and to get them to ALSO be seamless across the entire setup - even harder. Easily 10x or 100x the cost. It's sad, because the Internet shouldn't be that bad when the network drops all the time; it should just be slower as it waits to send good data. reply Anotheroneagain 3 hours agorootparentI think they just put a wire in the tunnel. reply Liskni_si 4 hours agorootparentprevYes, right now it's mostly just wifi at stations only. However, they're deploying 4G/5G coverage in the tunnels and expect 80% coverage by the end of 2024 [1]. So… you can expect apps developed by engineers in London to get much worse on slow internet in 2025. :-) [1]: https://tfl.gov.uk/campaign/station-wifi reply pintxo 13 hours agorootparentprevBerlin did not have mobile connections inside the tunnels until very recently (this year, I believe). This included the trains not being connected to any outside network. Thus wifi on the subway was useless to implement. reply amaccuish 11 hours agorootparentThey did if you were on o2, that's why I'm still with Aldi Talk (they use the o2 network); they've had LTE through the entire network for a while now. The new thing is 5G for everyone. reply rkachowski 11 hours agorootparentprevDespite Berlin's general lack of parity with modern technology, I've never actually had a problem with internet access across the ubahn network in the past decade. I noticed that certain carriers used to have very different availability when travelling and so switched to a better one, but I was always surprised at being able to handle mobile data whilst underground. reply nicbou 10 hours agorootparentReally? I don't even get consistent internet on the Ringbahn. There are lots of holes in the coverage in Berlin. Which provider are you with? Vodafone is still dead in large parts of the U-Bahn, but I know that one of them works much better. reply brabel 8 hours agorootparentWow! I was in Berlin last week and kept losing connection... like all the time. I use 3 with a Swedish plan. In Sweden, it literally never drops, not on trains, not on metro, not on faraway mountains... it works everywhere. reply avh02 10 hours agorootparentprevused to have spotty coverage underground with vodafone, when i switched to telekom, internet suddenly magically worked underground on the routes i used. I believe someone published a map of the data coverage of different providers on the berlin ubahn, but probably outdated now reply rkachowski 8 hours agorootparentprevYeah, admittedly this year I've also started experiencing holes on the ringbahn (strangely and consistently around frankfurter allee), but the ubahn has been fine. I'm with sim.de which I believe is essentially an O2 reseller (apn references o2) reply saagarjha 12 hours agorootparentprevI was in Berlin earlier this month and the cellular connections underground were quite good now. So maybe this is less of a problem? reply nicbou 10 hours agorootparentIt's provider-specific reply alanpearce 4 hours agorootparentNot any more? https://unternehmen.bvg.de/pressemitteilung/grossprojekt-erf... Summary: since 2024-05-06, users of all networks also get LTE in the U-Bahn thanks to a project between BVG and Teléfonica (not surprising that Teléfonica deployed the infra because they had the best U-Bahn LTE coverage beforehand) reply infinet 3 hours agoprevI had similar problem on a ship with many users share a 2M VSAT Internet. Few tricks made Internet less painful: - block windows update by returning DNS query for microsoft update endpoints as NXDOMAIN. - use a captive portal to limit user session duration, so that unattended devices won't consume bandwidth. - with freebsd dummynet, pfSense can share bandwidth equally among users. It can also share bandwidth by weight among groups. It helps. - inside Arctic circle, the geosynchronous satellites are very low on the horizon and were blocked frequently when ship turns. I was able to read the ship's gyro and available satellites from VSAT controller and generate a plot to show the satellite blockage. It was so popular that everyone is using it to forecast next satellite online. reply nicbou 10 hours agoprevI travel a lot. Slow internet is pretty common. Also, right now my mobile data ran out and I'm capped at 8 kbps. Websites that are Just Text On A Page should load fast, but many don't. Hacker News is blazing fast, but Google's API docs never load. The worst problem is that most UIs fail to account for slow requests. Buttons feel broken. Things that really shouldn't need megabytes of data to load still take minutes to load or just fail. Google Maps' entire UI is broken. I wish that developers spent more time designing and testing for slow internet. Instead we get data hungry websites that only work great on fast company laptops with fast internet. --- On a related note, I run a website for a living, and moving to a static site generators was one of the best productivity moves I've made. Instead of the latency of a CMS permeating everything I do, I edit text files at blazing speed, even when fully offline. I just push changes once I'm back online. It's a game changer. reply kjkjadksj 3 hours agoparentGoogle used to be good about slow apps. Using gmail on the school computers in the day, the site would load so slowly it would detect that and instead load a basic html version. Now a days I download a 500mb google map cache on my phone and its like there is no point. Everything still has to fetch and pop in. reply Tade0 8 hours agoparentprevOne additional benefit of static sites, which I learned the hard way, is that you're mostly immune to attacks. I have a domain that's currently marked as \"dangerous\" because I didn't use the latest version of Wordpress. reply qingcharles 1 hour agorootparentI had a client that I set up with a static site generator. Sadly the client changed their FTP password to something insecure and someone FTP'd in and added a tiny piece of code to every HTML file! reply daveoc64 8 hours agoparentprev>Websites that are Just Text On A Page should load fast, but many don't. Hacker News is blazing fast, but Google's API docs never load. Things aren't always that simple. I'm in the UK, and my ping time to news.ycombinator.com is 147ms - presumably because it's not using a CDN and is hosted in the USA. cloud.google.com on the other hand has an 8ms ping time. So yes, Hacker News is a simple, low-JS page - but there can be other factors that make it feel slow for users in some places. This is despite me being in a privileged situation, having an XGS-PON fibre connection providing symmetric 8Gbps speeds. reply Sesse__ 6 hours agorootparentHN loads quickly for me _despite_ the 147 ms. I guess partially because it doesn't need 20 roundtrips to sent useful content to me. At some point, I wrote a webapp (with one specific, limited function, of course) and optimized it to the point where loading it required one 27 kB request. And then turned up the cwnd somewhat, so that it could load in a single RTT :-) Doesn't really matter if you're in Australia then, really. reply EdwardDiego 12 hours agoprevI gave a fellow who'd just come off the ice a ride while he was hitchhiking, he was saying that the blog author was somewhat resented by others because his blog posts, as amazing as they are, tended to hog what limited bandwidth they already had while the images uploaded, but he was given priority because the administration realised the PR value of it. Which I thought ties into the discussion about slow internet nicely. reply Aachen 47 minutes agoparentI was wondering about the practicalities indeed. Not everyone knows when their OS or applications decided it is now a great time to update. You'll have a phone in your pocket that is unnecessarily using all the bandwidth it can get its hands on, or maybe you're using the phone but just don't realise that watching a 720p video, while barely functional, also means the person trying to load it after you cannot watch even 480p anymore (you might not notice because you've got buffer and they'll give up before their buffer is filled enough to start playing). It seems as though there should be accounting so you at least know what % of traffic went to you in the last hour (and a reference value of bandwidth_available divided by connected_users so you know what % was your share if everyone had equal need of it), if not a system that deprioritises everyone unless you punched the button that says \"yes I'm aware what bandwidth I'm using in the next [X≤24] hour(s) and actually need it, thank you\" which'll set the QoS priority for your MAC/IP address to normal reply teleforce 12 hours agoprevThe kind of scenario screams a local-first applications and solutions, and it's the reason why the Internet was created in the first place [1][2]. People have been duped by the misleading no software of Salesforce's advert slogan that goes against the very foundation and the spirit of the Internet. For most of its life and duration starting back in 1969, the Mbps is the anomaly not the norm and the its first killer application of email messaging (arguably the still best Internet application) is a local-first [3]. Ironically the culprit application that the author was lamenting in the article is a messaging app. [1] Local-first software: You own your data, in spite of the cloud: https://www.inkandswitch.com/local-first/ [2] Local-first Software: https://localfirstweb.dev/ [3] Leonard Kleinrock: Mr. Internet: https://www.latimes.com/opinion/la-oe-morrison-use24-2009oct... reply luuurker 5 hours agoprevThose of us working on apps, websites, etc, need to remember that there are lots of people out there that are not connected to the fast Wi-Fi or fibre connections we have. Here in the UK, some networks started shutting down 3G. Some have 2G as a low energy fall back, but we're supposed to use 4G/5G now. The problem is that 4G is not available everywhere yet, some areas until recently only had good 3G signal. So I've been dropping to 2G/EDGE more often than I'd like and a lot of stuff just stops working. A lot of apps are just not tested on slow, high latency, high package loss scenarios. reply jimmaswell 4 hours agoparentShutting down 3G was a mistake. Besides turning so many devices into e-waste, it was a good backup when 4g was congested. reply luuurker 3 hours agorootparent3G devices should still work over 2G. It's much slower, but it works and should do so until well into 2030 in the UK. The problem with 3G as I understand it is that it uses more power and is less efficient than 4G/5G. They're starting to re-deploy the 3G bands as 4G/5G, so the other Gs will eventually benefit from this shutdown. reply HPsquared 4 hours agorootparentprevThe lower-bandwidth connections get completely saturated by modern phones with modern data allowances. Back in the day I had 500MB a month on 3G, for instance. I can use that in a few minutes these days. reply kjkjadksj 3 hours agorootparentThats been true since the iphone 3g with unlimited data plans though reply HPsquared 3 hours agorootparentModern phones and apps use a lot more though. YouTube 1080p60 or even 4K, for example. reply qingcharles 1 hour agoparentprevHere in the USA a great number of networks will drop back to 2G when their data plan runs out. And most poor people are on really low data limits, so they spend most of the month on 2G. Try using Google Maps to get around on 2G :( reply Karrot_Kream 9 hours agoprevSo I've hacked a lot on networking things over the years and have spent time getting my own \"slow internet\" cases working. Nothing as interesting as McMurdo by far but I've chatted and watched YouTube videos on international flights, trains through the middle of nowhere, crappy rural hotels, and through tunnels. If you have access/the power (since these tend to be power hungry) to a general-purpose computing device and are willing to roll your own my suggestion is to use NNCP [1]. NNCP can can take data, chunk it, then send it. It also comes with a sync protocol that uses noise (though I can't remember if this enables 0RTT) over TCP (no TLS needed so only 1.5 RTT time spent establishing connection) and sends chunks, retrying failed chunks along the way. NNCP supports feeding data as stdin to a remote program. I wrote a YouTube downloader, a Slack bot, a Telegram bot, and a Discord bot that reads incoming data and interacts with the appropriate services. On the local machine I have a local Matrix (Dendrite) server and bot running which sends data to the appropriate remote service via NNCP. You'll still want to hope (or experiment such) that MTU/MSS along your path is as low as possible to support frequent TCP level retries, but this setup has never really failed me wherever I go and let's me consume media and chat. The most annoying thing on an international flight is that the NNCP endpoint isn't geographically distributed and depending on the route your packets end up taking to the endpoint, this could add a lot of latency and jitter. I try to locate my NNCP endpoint near my destination but based on the flight's WiFi the actual path may be terrible. NNCP now has Yggdrasil support which may ameliorate this (and help control MTU issues) but I've never tried Ygg under these conditions. [1]: http://www.nncpgo.org/ reply alanpearce 3 hours agoparentThis sounds fascinating. Do you have some articles describing your setup? reply Karrot_Kream 39 minutes agorootparentHah no, but maybe I should. The reason I haven't is that most of my work is just glue code. I use yt-dlp to do Youtube downloads, make use of the Discord, Slack and Telegram APIs to access those services. I run NNCP and the bots in systemd units, though at this point I should probably bake all of these into a VM and just bring it up on whichever cloud instance I want to act as ingress. Cloud IPs stay static as long as the box itself stays up so you don't need to deal with DNS either. John Goerzen has a bunch of articles about using NNCP [1] that I do recommend interested folks look into but given the popularity of my post maybe I should write an article on my setup. FWIW I think it's fine that major services do not work under these conditions, though I wish messaging apps did. Both WhatsApp and Telegram IME are well tuned for poor network conditions and do take a lot of these issues into account (a former WA engineer comments in this thread and you can see their attention to detail.) Complaining about these things a lot is sort of like eating out at restaurants and complaining at how much sodium and fat goes into the dishes: restaurants have to turn a profit and catering to niche dietary needs just isn't enough for them to survive. You can always cook at home and get the macros you want. But for you to \"cook\" your own software you need access to APIs and I'm glad Telegram, Slack, and Discord make this fairly easy. Youtube yt-dlp does the heavy lifting but I wish it were easier, at least for Premium subscribers, to access Youtube via API. I find Slack to be the absolute worst offender networking-wise. I have no idea how, now that Slack is owned by Salesforce, the app experience can continue to be so crappy on network usage. It's obvious that management there does not prioritize the experience under non-ideal conditions in any way possible. Their app's usage of networks is almost shameful in how bad it is. [1]: https://www.complete.org/nncp/ reply gmuslera 4 hours agoprevIt is not just bandwidth or latency, and is not just for Antarctica. Not in all places of the world you have the best connectivity. Even with not so bad connectivity, you may have environmental interference, shared use or be just far from the wifi router. You may have a browser running in a not so powerful CPU, doing more things chewing processor, or the available memory, so heavy JS sites may suffer or not work at all there. You don't know what is in the other side, putting high requirements there may turn your solution unfit for a lot of situations. Things should be improving (sometimes fast, sometimes slowly) in that direction, but still is not something guaranteed everywhere, or at least in every place that your application is intended or needed to run. And there may be even setbacks in that road. reply kjkjadksj 3 hours agoparentI run into this daily on my phone. Where I live, its hilly, network is usually saturated, my speeds are crap usually and some sites more complicated than hn cannot even load at all without timing out sometimes. reply beeandapenguin 2 hours agoprevThis is why we need more incremental rendering[1] (or \"streaming\"). This pattern become somewhat of a lost art in the era of SPAs — it's been possible since HTTP/1.1 via chunked transfer encoding, allowing servers to start sending a response without knowing the total length. With this technique, the server can break down a page load into smaller chunks of UI, and progressively stream smaller parts of the UI to the client as they become available. No more waiting for the entire page to load in, especially in poor network conditions as the author experienced from Anartica. [1]: https://www.patterns.dev/react/streaming-ssr reply jasoncartwright 2 hours agoparentI remember doing this with ASPv3 pages back in the day on a content site. It made it easy to dump what HTML has already been completed out before continuing to generate the heavier, but much less important, comments section below. reply Tepix 13 hours agoprevI had a similar experience as the author on a boat in the south pacific. Starlink was available but often wasn't used because of its high power usage (60+ watts). So we got local SIM cards instead which provided 4G internet in some locations and EDGE (2G) in others. EDGE by itself isn't too bad on paper - you get a couple dozen kilobits per second. In reality, it was much worse. I ran into apps with short timeouts that would have worked just fine, if the authors had taken into account that loading can take minutes instead of milliseconds. Low bandwith, high latency connections need to be part of the regular testing of software. For Linux, there's netem (https://wiki.linuxfoundation.org/networking/netem) that will let you do this. An issue that the anonymous blog author didn't have was metered connections. Doing OS or even app upgrades was pretty much out of the question for cost reasons. Luckily, every few weeks or so, we got to a location with an unmetered connection to perform such things. But we got very familiar with the various operating systems' ways to mark connections as metered/unmetered disable all automatic updates and save precious bandwidth. reply throwaway2037 12 hours agoparentThe South Pacific should be very sunny. I guess that you didn't have enough solar panels to provide 60+ watts. I am genuinely surprised. And \"local SIM cards\" implies that you set foot on (is)lands to buy said SIM cards. Where did you only get 2G in the 2020s? I cannot believe any of this is still left in the South Pacific. reply RetroTechie 6 hours agorootparent> Where did you only get 2G in the 2020s? My previous smartphone supported 4G/3G/Edge, but for some reason the 4G didn't work. At all, ever, anywhere (not a provider/subscription or OS settings issue, and WiFi was fine). In my country 3G was turned off a while ago to free up spectrum. So it fell back to Edge all the time. That phone died recently. I'm temporarily using an older phone which also supports 4G/3G/Edge, and where the 4G bit works. Except... in many places where I hang out (rural / countryside) 4G coverage is spotty or non-existant. So it also falls back to Edge most of the time. Just the other day (while on WiFi) I installed Dolphin as a lightweight browser alternative. Out in the countryside, it doesn't work (\"no connection\"), even though Firefox works fine there. Apps won't download unless on WiFi. Not even if you're patient: downloads break somewhere, don't resume properly, or what's downloaded doesn't install because the download was corrupted. None of these issues over WiFi. Same with some websites: roundtrips take too long, server drops the connection, images don't load, etc etc. Bottom line: app developers or online services don't (seem to) care about slow connections. But here's the thing: for the average person in this world, fast mobile connections are still the exception, not the norm. Big city / developed country / 4G or 5G base stations 'everywhere' doesn't apply to a large % of the world's population (who do own smartphones these days, even if low-spec ones). Not that some low-tier mobile plans also cap connection speeds. Read: slow connection even if there's 4G/5G coverage. There's a reason internet cafe's are still a thing around the world. reply kjkjadksj 3 hours agorootparentI live in a developed country with 4g/5g everywhere and its still no better than the 3g era I remember. Modern apps and sites have gobbled up the spare bandwith so the general ux feels the same to the user in terms of latency. On top of that there are frequent connection dropouts even with the device claiming a decent connection to the tower. Using mobile internet seems like 4g often can’t bring the speed to load a modern junked up news or recipe site in sometimes any amount of time. reply chipdart 13 hours agoparentprev> Low bandwith, high latency connections need to be part of the regular testing of software. One size does not fit all. It would be a waste of time and effort to architect (or redesign) an app just because a residual subset of potential users might find themselves on a boat in the middle of the Pacific. Let's keep things in perspective: some projects even skip testing WebApps on more than one browser because they deem that wasteful and an unjustified expense, even though it's trivial to include them on a test matrix, and this is a UI-only. reply Nevolihs 6 hours agorootparentWebsites regularly break because I don't have perfect network coverage on my phone every single day. In a lot of places, I don't even have decent reception. This in Germany in and around a major city. Why do you think this only applies to people on a boat? reply chipdart 1 hour agorootparent> Websites regularly break because I don't have perfect network coverage on my phone every single day. Indeed, that's true. However, the number of users that go through similar experiences are quite low and even those who do are always a F5 away from circumventing that issue. I repeat: even supporting a browser other than the latest N releases of Chrome is a hard sell to some companies. Typically the test matrix is limited to N versions of Chrome and the latest release of Safari when Apple products are supported. If budgets don't stretch even to cover the basics, of course that even rarer edge cases such as a user accessing a service through a crappy network will be far from the list of concerns. reply fragmede 10 hours agorootparentprevit's not a total redesign, it's just raising a timeout from 30 to 3000 reply 0xWTF 14 hours agoprevI still think engineering for slow internet is really important, and massively under appreciated by most software developers, but ... LEO systems (like Starlink, especially StarLink) essentially solve the core problems now. I did an Arctic transit (Alaska to Norway) in September and October of 2023, and we could make FaceTime video calls from the ship, way above the Arctic Circle, despite cloud cover, being quite far from land, and ice. This was at the same time OP was in Antartica. Whatever that constraint was, it's just contracting for the service and getting terminals to the sites. The polar coverage is relatively sparse, but still plenty, due to the extraordinarily low population. https://satellitemap.space/ reply chipdart 13 hours agoparent> I still think engineering for slow internet is really important, and massively under appreciated by most software developers, but ... LEO systems (like Starlink, especially StarLink) essentially solve the core problems now. I don't think that this is a valid assessment of the underlying problem. Slow internet means many things, and one of them is connection problems. In connection-oriented protocols like TCP this means slowness induced by drop of packets, and in fire-and-forget protocols like UDP this means your messages don't get through. This means that slowness might take multiple forms, such as low data rates or moments of high throughput followed by momentary connection drops. One solid approach to deal with slow networks is supporting offline mode, where all data pushes and pulls are designed as transactions that take place asynchronously, and data pushes are cached locally to be retried whenever possible. This brings additional requirements such as systems having to support versioning and conflict resolution. Naturally, these requirements permeate onto additional UI requirements, such as support for manually synching/refreshing, displaying network status, toggling actions that are meaningless when the network is down, rely on eager loading to remain usable while offline, etc. reply robjan 13 hours agorootparentI'd say these days it's more common to deploy in ap-southeast-1 (Singapore) rather than Japan to cover most of APAC. reply kbr2000 12 hours agorootparentprevDelay/disruption tolerant networking (DTN) seeks to address these kind of problems, using alternative techniques and protocols: store-and-forward, Bundle protocols and Licklider Transmission Protocol. Interesting stuff, enjoy! reply drozycki 14 hours agoparentprevThere’s a diner in SF I frequent. I usually sit 15 feet from the door, on a busy retail corridor, with Verizon premium network access. My iPhone XS reports two bars of LTE but there’s never enough throughout for DNS to resolve. Same at my dentist’s office. I hope to live in a post slow internet world one day, but that is still many years away. (The XS does have an Intel modem, known to be inferior to the Qualcomm flagship of the era) reply radicaldreamer 13 hours agorootparentI think this is a tough because a lot of bands have been repurposed for 5G and an Xs doesn’t support any of those. reply drozycki 13 hours agorootparentI get 400 Mbps down standing at the door of that same diner. My understanding is that 4G bands are repurposed for 5G in rough proportion to the usage of 4G vs 5G devices at that tower, plus there’s some way to use a band for both. In any case I was having these indoor performance issues back in 2019. I’m pretty sure it’s an Intel issue, and any Qualcomm modem would be fine. reply themoonisachees 11 hours agorootparentI see this in my french city, there's a particular spot on my commute where my phone (mediatek) will report 2 bars of 5G but speeds will actually be around 3G. I've also noticed other people on the tram having their videos buffer at that spot, so it's not just me. The carriers do not care, of course. I think there's just some of these areas where operational conditions make the towers break in some specific way. reply rsynnott 6 hours agorootparentI mean, they're not breaking, they're just overloaded. Solution is generally to add more towers, but that's expensive. reply kjkjadksj 3 hours agorootparentWhat do we pay them for if not to build out our telecom towers? reply g15jv2dp 12 hours agoparentprevHow do LEO satellite help me when a commuter train full of people connecting to the same AP enters the station I'm in? I live in one of the most densely populated places on Earth, chock-full of 5G antennas and wifi stations. Yet I still feel it when poorly engineered websites trip up on slow/intermittent connections. reply kylehotchkiss 12 hours agoparentprevIdealistic! I think a lot of countries are going to block starlink in the future by interfering with the signals, much like the success some countries are having interfering so heavily with GPS. Their governments won't want uncensored web, or an American company being the gateway to the internet. They'll maintain whatever territorial networks they have now and the speed question is still relevant. Also the number of people worldwide whose only access to the internet is a $100 android phone with older software and limited CPU should be considered reply JeremyNT 5 hours agorootparentEven if people want to / are allowed to, I'm trying to imagine how well starlink could plausibly function if 2 billion people switched from their sketchy terrestrial service to starlink. As a luxury product used by a few people, maybe it \"solves\" the problem, but I don't think this is a very scalable solution. reply astro-throw 13 hours agoparentprevPole doesn't have Starlink. McMurdo does. There are reasons. Polar coverage from GEO satellites is limited because how close to the horizon GEO satellites are from Pole. Pole uses old GEO satellites which are low on fuel and have relatively large inclinations... then you can talk to them for ~ 6 hours per 24. Schedule: https://www.usap.gov/technology/1935/ reply sambazi 11 hours agoparentprevdoesn't take much stress to make starlink exacerbate packet loss levels way above docsis. it's ok for off-grid but not for the majority. reply polairscience 14 hours agoparentprevWhat ship were you on and was it the northwest passage? We haven't had good luck north of 80 degrees with starlink. reply Thlom 10 hours agorootparentFYI, Space Norway will launch two satellites this summer on a Falcon 9 that will be going in a HEO orbit, among the payloads on the satellites is a Viasat/Inmarsat Ka-band payload which will provide coverage north of 80 degrees. Latency will probably be GEO+, but coverage is coverage I guess. :-) reply giantrobot 14 hours agoparentprevSlow Internet isn't just remote places, it also crops up in heavily populated urban areas. It's sad that you had better connectivity above the Arctic circle than the typical connectivity with hotel WiFi. Bad connectivity also happens with cellular connections all over the place. reply el_benhameen 14 hours agoparentprevNot really the point of your post, but that sounds like a really cool trip. What were you doing up there? reply walterbell 12 hours agoprevIETF draft proposal to extend HTTP for efficient state synchronization, which could improve UX on slow networks, https://news.ycombinator.com/item?id=40480016 The Braid Protocol allows multiple synchronization algorithms to interoperate over a common network protocol, which any synchronizer's network messages can be translated into.. The current Braid specification extends HTTP with two dimensions of synchronization: Level 0: Today's HTTP Level 1: Subscriptions with Push Updates Level 2: P2P Consistency (Patches, Versions, Merges) Even though today's synchronizers use different protocols, their network messages convey the same types of information: versions in time, locations in space, and patches to regions of space across spans of time. The composition of any set of patches forms a mathematical structure called a braid—the forks, mergers, and re-orderings of space over time. Hope springs eternal! reply meindnoch 6 hours agoparentYes, please! Even more layers of needlessly complex crap will definitely improve things! reply walterbell 2 hours agorootparent> needlessly complex The optional Braid extension can _reduce_ complexity for offline-first apps, e.g. relative to WebDAV, https://news.ycombinator.com/item?id=40482610 You might be surprised at just how elegantly HTTP extends into a full-featured synchronization protocol. A key to this elegance is the Merge-Type: this is the abstraction that allows a single synchronization algorithm to merge across multiple data types. As an application programmer, you will specify both the data types of your variables (e.g. int, string, bool) and also the merge-types (e.g. \"this merges as a bank account balance, or a LWW unique ID, or a collaborative text field\"). This is all the application programmer needs to specify. The rest of the synchronization algorithm gets automated by middleware libraries that the programmer can just use and rely upon, like his compiler, and web browser. I'd encourage you to check out the Braid spec, and notice how much we can do with how little. This is because HTTP already has almost everything we need. Compare this with the WebDAV spec, for instance, which tries to define versioning on top of HTTP, and you'll see how monstrous the result becomes. Example here: https://news.ycombinator.com/item?id=40481003 reply klabb3 6 hours agoparentprevGrump take: More complex technology will not fix a business-social problem. In fact, you have to go out of your way to make things this shitty. It’s not hard to build things with few round trips and less bloat, it’s much easier. The bloat is there for completely different reasons. Sometimes the bloat is unnoticeable on juicy machines and fast internet close to the DC. You can simulate that easily, but it requires the company to care. Generally, ad-tech and friends cares very little about small cohorts of users. In fact, the only reason they care about end users at all is because they generate revenue for their actual customers, ie the advertisers. reply pnt12 5 hours agorootparent> Generally, ad-tech and friends cares very little about small cohorts of users. Sure, and it will keep being that way. But if this gets improved at the transport layer, seems like a win. As an analogy, if buses are late because roads are bumpy and drivers are lousy, fixing the bumpy road may help, even if drivers don't change their behavior. reply klabb3 2 hours agorootparent> But if this gets improved at the transport layer, seems like a win. What do you mean? TCP and HTTP is already designed for slow links with packet loss, it’s old reliable tech from before modern connectivity. You just have to not pull in thousands of modules in the npm dep tree and add 50 microservice bloatware, ads and client side “telemetry”. You set your cache-control headers and etags, and for large downloads you’ll want range requests. Perhaps some lightweight client side retry logic in case of PWAs. In extreme cases like Antarctica maybe you’d tune some tcp kernel params on the client to reduce RTTs under packet loss. There is nothing major missing from the standard decades old toolbox. Of course it’s not optimal, the web isn’t perfect for offline hybrid apps. But for standard things like reading the news, sending email, chatting, you’ll be fine. reply marcosdumay 4 hours agorootparentprev> fixing the bumpy road may help It really wouldn't. Lousy drivers are a way thinner bottleneck than the roads. But it will improve the services where the drivers are good. If the protocol is actually any good (its goals by themselves already make me suspicious it won't be), the well-designed web-apps out there can become even better designed. But it absolutely won't improve the situation people are complaining about. reply austin-cheney 5 hours agoprev70%+ of the web is putting text on screen and responding to user interactions, 25%+ is spyware and advertising, and the last 5% are cool applications. How complicated should that really be? This is a good example of why I gave up a career as a JavaScript developer after 15 years. I got tired of fighting stupid, but even stupid woefully unqualified people need to make 6 figures spinning their wheels to justify their existence. reply AdamH12113 1 hour agoprevIt's interesting that these are exactly the sort of conditions that the internet protocols were designed for. A typical suite of internet software from the 90s would handle easily. One key difference is that client software was installed locally, which (in modern terms) decouples UI from content. As the article points out, the actual data you're dealing with is often measured in bytes. An email reader or AOL Instant Messenger would only have to deal with that data (plus headers and basic login) instead of having to download an entire web app. And since the protocols didn't change often, there was no need to update software every few weeks (or months, or even years). Another key difference, which is less relevant today, is that more data came from servers on the local network. Email and Usenet were both designed to do bulk data transfer between servers and then let users download their individual data off of their local server. As I recall, email servers can spend several days making delivery attempts before giving up. reply jrhey 3 hours agoprevEngineering for slow CPUs next. No matter how fast our machines get these days, it's just never enough for the memory/CPU/battery hungry essential apps and operating systems we use nowadays. reply oefrha 14 hours agoprevThis is why I find it dreadful that evangelists here are heavily promoting live$whatever technology where every local state change requires at least one server roundtrip, or “browsers support esm now, bundling is a thing of the past!” etc. You don’t need to be at Antarctica to feel the latencies caused by the waterfall of roundtrips, or roundtrip on every click, as long as you’re a mere 200ms from the server, or in a heavily congested place. reply throw46365 13 hours agoprevAs a web developer I actually resisted much faster internet for ages. Until 2022 I had a rock-solid, never-failed 7 megabit/s-ish down, 640k up connection and I found it very easy to build sites that others describe as blazing fast. This was slow really by the standards of much of the UK population even by 2015. So all I had to do was make it fast for me. A change of provider for practical reasons gave me an ADSL2+ connection that is ten times faster; still arguably slower than a lot of residential broadband in the UK but not so helpfully. So now I test speed on mobile; even in the south east of England it is not that difficult to find poor mobile broadband. And when it’s poor, it’s poor in arguably more varied ways. reply oefrha 13 hours agoparentAs a web developer you can just throttle your connection in developer tools though, no self-limiting required. But nobody does that in big corporations building most of the sites needed by people with slow connections. reply throw46365 13 hours agorootparentYeah, though it doesn’t quite capture all of the experience of working with slower broadband. For example if you have a website that is meant to be used alongside a video call or while watching video, it’s difficult to really simulate all of that “feel”. Using a link that is slow in practice is an invaluable experience. reply xandrius 11 hours agorootparentYou can install programs at software level to emulate it as a whole. I remember using one for OSX and it working pretty well. reply jeroenhd 12 hours agorootparentprevIn my experience, browsers limit speeds in a way that's kind of nice and stable. You tell them to stick to 100kbps and they'll have 100kbps. Packet loss, jitter, it's all a single number, and rather stable. It's like a 250kbps fiber optic connection that just happens to be very long. In my experience, real life slow internet isn't like that. Packet loss numbers jump around, jitter switches second by second, speeds vary wildly and packets arrive out of order more than in order. Plus, with sattelites, the local router sends fake TCP acknowledgements to hide the slow data transfer, so the browser thinks it's connected while the traffic is still half a second away. There are software tools to limit connectivity in a more realistic way, often using VMs, but they're not used as often as the nice browser speed limiter. reply oefrha 11 hours agorootparentGood points, but it would still be a major step forward if websites start handling browser-simulated 3G well. Right now the typical webshit used by regular people more often than not ranges from barely usable to completely unusable on browser-simulated 3G, let alone browser-simulated 2G or real world bad connections. As a first step, make your site work well on, say, 200ms and 1Mbps. reply the__alchemist 4 hours agoprevThank you! I am in the East coast US, and consistently find web sites and internet-connected applications are too slow. If I am at home, they are probably fine, but on mobile internet? Coffee shops etc? Traveling? No! No excuses! It is easier than ever to build fast, interactive websites, now that modern, native Javascript includes so many niceties. Using JS dependencies is a minefield. At work, where I give less of a fuck, a dev recently brought in Material UI and Plotly. My god. reply lawgimenez 10 hours agoprevFrom where I'm from (Southeast Asia), slow internet is common in provincial and remote areas. It's like the OP's experience in South Pole but slower. That's why I always cringe at these fancy-looking UI cross-platform apps since I know they will never work in a remote environment. Also, that is why offline support is very important. I only use Apple Notes and Things 3, both work tremendously in such remote settings. Imagine your notes or to-do list (ehem Basecamp) not loading since it needs internet connection reply bombcar 4 hours agoparentWhat's sad is that the app-style setup on phones SHOULD be perfect for this - you download the app when you DO have a good connection, and then when you're out on the slow/intermittent connection the ONLY thing the app is sending is the new data needed. Instead almost all apps are just a bad web browser that goes to one webpage. reply palata 1 hour agoprevOne problem is that developers have the best hardware and Internet because \"it's their job\", so they are completely biased. A bit like rich people tend to not understand what it means to be poor. The other problem is that nobody in the software industry gives a damn. Everyone wants to make shiny apps with the last shiny tech. Try to mention optimizing for slow hardware/Internet and look at the face of your colleagues, behind their brand new M3. I worked in a company with some remote colleagues in Africa. There were projects that they could literally not build, because it would require downloading tens of GB of docker crap multiple times a week for no apparent reason. The solution was to not have those colleagues work on those projects. Nobody even considered that maybe there was something to fix somewhere. reply nirav72 2 hours agoprev> From my berthing room at the South Pole, it was about 750 milliseconds I’m currently on a moving cruise ship in the Mediterranean with a starlink connection. A latency of 300-500 ms seems to be normal. Although bandwidth is tolerable at 2-4 mbps during the day with hundreds of passengers using it. At night it gets better. But latency can still be frustrating. reply password4321 5 hours agoprevIt doesn't take much to slow down RDP over TCP (especially when port forwarding through SSH). I did find mention of increasing the cache¹ and lowering the refresh rate to 4 fps² (avoiding unnecessary animations), but I still feel the need for a server-side QUIC proxy that is less pushy based on network conditions. There is a red team project that has the protocol parsed out in Python³ instead of all the ActiveX control clients. ¹ https://superuser.com/questions/13487/how-to-increase-perfor... ² https://learn.microsoft.com/en-us/troubleshoot/windows-serve... ³ https://github.com/GoSecure/pyrdp reply rlv-dan 14 hours agoprev> Please keep in mind that I wrote the majority of this post ~7 months ago, so it’s likely that the IT landscape has shifted since then. Not sure if this is serious or intended as a joke. It made me giggle nonetheless. Which is kind of sad. reply kortilla 14 hours agoparentIt’s serious presumably because Starlink coverage includes the poles now. 7 months ago was around the time they did a demo with the McMurdo base IIRC. reply bizzyb 3 hours agorootparentMcMurdo has starlink. South Pole doesn't, but not due to technical reasons from starlink's side. From what I understand when they tested at Pole they noticed interference with some of the science experiments, its possible they will engineer around that at some point but for now starlink is a low priority compared to ensuring the science goes on. I forget the exact distance, but its something like 5 miles from pole that they ask groups traversing to turn off their starlink. reply modeless 14 hours agoparentprevThey're being obtuse. What \"it's likely the IT landscape has shifted\" actually means is \"they got Starlink and their connection is fast now, and I know this for certain but I want to downplay it as much as possible because I'm trying to make a point\". reply sham1 13 hours agorootparentOr they could be making a joke about how quickly trends shift in IT. It's like how people joke (or at least used to joke) that you'd get a dozen new JavaScript frameworks daily. Exaggeration for comedic effect, in other words. reply phaedrus 3 hours agoprevOne of my first professional software projects, as an intern, was I wrote a tool for simulating this type of latency. I modeled it as a set of pipe objects that you could chain together with command line arguments. There was one that would do a fixed delay, another that would introduce random dropped packets, a tee component in case you wanted to send traffic to another port as well, etc. reply RedShift1 12 hours agoprevI would highly recommend not only testing on slow network connections, but also on slow computers, tablets and smartphones. At least in my case there was some low hanging fruit that immediately improved the experience on these slow devices which I would have never noticed had I not tested on slower machines. reply xenodium 11 hours agoprevNot yet officially launched, but I’m working on a no-bloat, no-tracking, no-JS… blogging platform, powered by a drag/drop markdown file: https://lmno.lol Blogs can be read from just about any device (or your favourite terminal). My blog, as an example: https://lmno.lol/alvaro Shared more details at https://indieweb.social/@xenodium/112265481282475542 ps. If keen to join as an early adopter, email help at lmno.lol reply pech0rin 12 hours agoprevGreat post. One thing though. Maybe the engineers were misguided but its possible they were trying to mitigate slow loris attacks. Which are annoying to deal with and hard to separate from users who are just sending data at a really slow pace. Having had to mitigate these attacks before, we usually do a global timeout on the backend. Maybe different but definitely a possibility. reply mayormcmatt 2 hours agoprevThis topic resonates with me, because I'm currently building a horrible marketing static page with images and videos that top 150MB, prior to optimization. It causes me psychic pain to think about pushing that over the wire to people that might have data caps. Not my call, though... reply AllegedAlec 9 hours agoprevI currently develop applications which are used on machines with very spotty connections and speeds which are still calculated in Baud. We have to hand-write compression protocols to optimize for our use. Any updates/installs over network are out of the question. It's a great lesson in the importance of edge computing, but it also provides some harsh trusth about the current way we produce software. We cannot afford to deliver a spotty product. To get new updates out to all parties takes a prohibitively long time. This is hard for new people or outsiders giving courses to grok, and makes most modern devops practices useless to us. reply grishka 14 hours agoprevIMO author shouldn't have censored the app names. The world deserves to know. reply future10se 13 hours agoparentThose I recognize (from interface, branding, strings, etc.): * Slack -- https://brr.fyi/media/engineering-for-slow-internet/load-err... * Signal (main screen) -- https://brr.fyi/media/engineering-for-slow-internet/in-app-d... * 1Password (config/about page) -- https://brr.fyi/media/engineering-for-slow-internet/in-app-d... * Zoom (updater screen on Mac) -- https://brr.fyi/media/engineering-for-slow-internet/in-app-d... * Parallels Desktop (\"prl_client_app\" is the binary name) -- https://brr.fyi/media/engineering-for-slow-internet/hardcode... reply djtango 12 hours agorootparentYes - name and shame. Slack is INFURIATING on intermittent connectivity. That is simply not good enough for a product who's primary value is communication. Anyone who has tried to use Slack: - in the countryside with patchy connection - abroad - in China - on the London Underground Can attest to how poor and buggy Slack is on bad internet. These aren't weird edgecases - London is a major tech hub. Remote workers and open source communities rely on Slack around the world. China is the second largest economy in the world with a population of 1.7B (incidentally it's blocked at least it was when I was last there but even on VPN it was weird and buggy). How aren't these kinds of metrics tracked by their product teams. How isn't WhatsApp the gold standard now for message delivery, replicated everywhere. Neither email nor WhatsApp have the weird consistency issues Slack has with simply sending a message with dodgy internet. Not to mention the unreliable and sometimes user-hostile client state management when Slack can't phone home which can sometimes lead to lost work or inability to see old messages you literally were able to see until you tried to interact with stuff. reply don-code 5 hours agorootparentSlack additionally decides to hard-reload itself, seemingly without reason. I work on the road (from a train / parking lot / etc) for five or six hours per week. My T-Mobile plan is grandfathered in, so I can't \"upgrade\" to a plan that allows full-speed tethering without considerably impacting my monthly bill. Realistically, I hit around 1.5Mbps down. When Slack reloads itself, I have to stop _everything else_ that I'm doing, immediately, and give Slack full usage of my available bandwidth. Often times, it means taking my phone out of my pocket, and holding it up near the ceiling of the train, which (I've confirmed in Wireshark) reduces my packet loss. Even then, it takes two or three tries just to get Slack to load. reply djtango 2 hours agorootparentI feel your pain - one minute you're reading some messages or a note and the next you're locked out of slack with faded screens and infinite spinnies. Apparently we must be very niche amongst their user base because these kinds of fixes hasn't made it onto their roadmap in years reply tecleandor 5 hours agorootparentprevSlack and the Jira suite are terrible. Slack web downloads 40MB of Javascript. The macOS Slack client, that I guess should have all that stuff already, downloads 10MB of stuff just by starting it (and going directly to a private text only chat). reply lawgimenez 10 hours agorootparentprevTelegram also works well in remote places. reply djtango 7 hours agorootparentI doubt I'll ever work with a place that uses Telegram but yes its clear that resilient message delivery is a solved problem nowadays but Slack is still hopeless at the one most important key feature of its product. Now that WhatsApp also has groups there's even less of an excuse for Slack to perform so badly reply don-code 5 hours agorootparentprevI've also found that the AWS and Azure consoles behave this way. While not listed in the blog post, they load JavaScript bundles in the tens of megabytes, and must have a hard-coded timeout that fails the entire load if that JavaScript hasn't been downloaded inside of a few minutes. To Amazon's credit, my ability to load the AWS console has improved considerably in recent months, but I can't say the same for Azure. reply astro-throw 12 hours agorootparentprevMy experience is that Slack worked great last winter, when the broadband satellite was up. When it's down, folks use an IRC-style client to cope with the very limited & expensive bandwidth from Iridium. reply voyagerfan5761 13 hours agorootparentprevYou got all the same answers I did, which helps me determine how good my sleuthing skills are. I used exclusively strings, either API routes, error codes, or version/build numbers. reply koito17 14 hours agoparentprevQuerying an exact match of a few strings on Google shows me that Slack is the very first example given in the blog post. For additional confirmation, the \"6-byte message\" screenshot lists an xoxc token and rich_text object, both of which you will frequently encounter in the Slack API. To be honest, I was expecting it to be Jira at first since I was unaware of Slack's size. Searching for an exact match of \"PRL_ERR_WEB_PORTAL_UNEXPECTED\" gives away Parallels as the first example of a hard-coded HTTPS timeout. So on, so forth. reply Tepix 14 hours agoparentprevI agree! They call out Apple, Samsung and a few others, but not app vendors. reply grishka 14 hours agorootparentThe messaging app with an updater is clearly Signal. The checkmark icons gave it away for me. reply gbalduzzi 3 hours agoprevI agree with the overall take by OP, but I find this point quite problematic: > If you have the ability to measure whether bytes are flowing, and they are, leave them alone, no matter how slow. Perhaps show some UI indicating what is happening. Allowing this means easy DDOS attack. An attacker can simply keep thousand of connections open reply klabb3 2 hours agoparentClose after 10-60s of complete inactivity, don’t use JS bloatware and allow for range/etag requests should go a long way though. The issue is people setting fixed timeouts per request which isn’t meant for large transfers. reply parentheses 13 hours agoprevI recall using a device called the \"Mini Maxwell\" by this company: https://www.iwl.com/ It enabled you to simulate network slow-downs, packet-loss, packet corruption, packet reordering and more. It was so critical in testing our highly network sensitive software. reply RedShift1 12 hours agoparentFor browser apps, a network connection simulator is included in the Chrome developer tools' Network tab. reply christophilus 8 hours agorootparentIt doesn’t do a great job, though. A good simulator will have random drops, jitter, bursts, etc. reply y-c-o-m-b 1 hour agoprevSome of these web apps are from very profitable or big companies and that drives me insane because they have more than enough funding to do things right. Take Home Depot for example. Loading their website in a mobile browser is soooooooooo slow. The rendering is atrocious, with elements jumping all over the place. You click on one thing and it ends up activating a completely different element, then you have to wait for whatever you just clicked to load and jump all over the pl",
    "originSummary": [
      "An IT worker with the United States Antarctic Program shares experiences of using the Internet in bandwidth-constrained environments like McMurdo and the South Pole from August 2022 to November 2023.",
      "The post highlights severe connectivity challenges, including high latency, slow speeds, and frequent dropouts, impacting the usability of modern web applications.",
      "The author criticizes app developers for not accommodating slow and intermittent links and suggests improvements like flexible timeouts, incremental uploads, and robust download managers to enhance usability in remote areas."
    ],
    "commentSummary": [
      "The discussion highlights the challenges of internet use in areas with poor connectivity, urging developers to create more efficient and accessible web applications.",
      "Users criticize modern web apps for being bloated and inefficient, emphasizing the need for performant applications for all users, not just those with high-speed internet.",
      "The discussion explores solutions like custom VPN obfuscation, modern JavaScript frameworks, server-side rendering, and optimizing static site generators to improve web performance under variable and unreliable internet conditions."
    ],
    "points": 701,
    "commentCount": 276,
    "retryCount": 0,
    "time": 1717124765
  },
  {
    "id": 40530670,
    "title": "Japan Allocates ¥10 Billion to Make All Publicly Funded Research Open Access by 2025",
    "originLink": "https://www.nature.com/articles/d41586-024-01493-8",
    "originBody": "NEWS 30 May 2024 Japan’s push to make all research open access is taking shape Japan will start allocating the ¥10 billion it promised to spend on institutional repositories to make the nation’s science free to read. By Dalmeet Singh Chawla Twitter Facebook Email Japan plans to make all publicly funded research available to read in institutional repositories.Credit: Toru Yamanaka/AFP via Getty The Japanese government is pushing ahead with a plan to make Japan’s publicly funded research output free to read. In June, the science ministry will assign funding to universities to build the infrastructure needed to make research papers free to read on a national scale. The move follows the ministry’s announcement in February that researchers who receive government funding will be required to make their papers freely available to read on the institutional repositories from January 2025. The Japanese plan “is expected to enhance the long-term traceability of research information, facilitate secondary research and promote collaboration”, says Kazuki Ide, a health-sciences and public-policy scholar at Osaka University in Suita, Japan, who has written about open access in Japan. The nation is one of the first Asian countries to make notable advances towards making more research open access (OA) and among the first countries in the world to forge a nationwide plan for OA. The plan follows in the footsteps of the influential Plan S, introduced six years ago by a group of research funders in the United States and Europe known as cOAlition S, to accelerate the move to OA publishing. The United States also implemented an OA mandate in 2022 that requires all research funded by US taxpayers to be freely available from 2026. Institutional repositories When the Ministry of Education, Culture, Sports, Science and Technology (MEXT) announced Japan’s pivot to OA in February, it also said that it would invest ¥10 billion (around US$63 million) to standardize institutional repositories — websites dedicated to hosting scientific papers, their underlying data and other materials — ensuring that there will be a mechanism for making research in Japan open. Among the roughly 800 universities in Japan, more than 750 already have an institutional repository, says Shimasaki Seiichi, director of the Office for Nuclear Fuel Cycles and Decommissioning at MEXT in Tokyo, who was involved with drawing up the plan. Each university will host the research produced by its academics, but the underlying software will be the same. In 2022, Japan also launched its own national preprint server, Jxiv, but its use remains limited, with only a few hundred preprint articles posted on the platform so far. Ide says that publishing as preprints is not yet habitual for many researchers in Japan, noting that only around one in five respondents to his 2023 survey1 on Jxiv were even aware that it existed. Green OA Japan’s move to greater access to its research is focusing on ‘green OA’ — in which authors make the author-accepted, but unfinalized, versions of papers available in the digital repositories, says Seiichi. Seiichi says that gold OA — in which the final copyedited and polished version of a paper is made freely available on the journal site — is not feasible on a wide scale. That’s because the cost to make every paper free to read would be too high for universities. Publishers levy an article-processing charge (APC) if the paper is made free to read, rather than being paywalled, a fee that covers a publisher’s costs. APCs are increasing at an average rate of 4.3% per year, notes Johan Rooryck, a scholar of French linguistics at Leiden University in the Netherlands, and executive director of cOAlition S. Rooryck says that Japan’s green OA strategy should be applauded. “It’s definitely something that one should do,” he says. “Especially for all the content that is still behind the paywall.” Kathleen Shearer, executive director of the Confederation of Open Access Repositories in Montreal, Canada, says that the Japanese plan is “equitable”. “It doesn’t matter where you publish, whether you have APCs or not, you are still able to comply with an open-access policy,” she says. She adds that the policy will mean that Japan has a unified record of all research produced by its academics, because all institutional repositories are hosted on the same national server. “Japan is way ahead of the rest of us,” Shearer says. “More countries are moving in this direction but Japan really was one of the first.” Focusing on institutional repositories will have another benefit: it will not discriminate against research published in Japanese, Shearer says. “A big part of their scholarly ecosystem is represented in Japanese.” Japanese research is no longer world class — here’s why The plan to move to OA and support Japanese universities’ repositories comes as Japan grapples with its declining standing in international research. In a report released last October, MEXT found that Japan’s world-class research status is declining. For instance, Japan’s share in the top 10% of most-cited papers has dipped from 6% to 2%, placing it 13th on the list of nations, despite Japan having the 5th-highest research output. In March, Japan vowed to triple its number of doctorate holders by 2040, after another report found that the country’s number of PhD graduates is also declining, making it an outlier among the major economies. doi: https://doi.org/10.1038/d41586-024-01493-8 References Ide, K. & Nakayama, J.-I. Genes Cells 28, 333–337 (2023). Article PubMed Google Scholar Download references Reprints and permissions Latest on: Scientific community Research data Policy How I run a virtual lab group that’s collaborative, inclusive and productive CAREER COLUMN 31 MAY 24 Biomedical paper retractions have quadrupled in 20 years — why? NEWS 31 MAY 24 What is science? Tech heavyweights brawl over definition NEWS 31 MAY 24 Jobs Chief Editor Job Title: Chief Editor Organisation: Nature Ecology & Evolution Location: New York, Berlin or Heidelberg - Hybrid working Closing date: June 23rd... New York City, New York (US) Springer Nature Ltd Global Talent Recruitment (Scientist Positions) Global Talent Gathering for Innovation, Changping Laboratory Recruiting Overseas High-Level Talents. Beijing, China Changping Laboratory Postdoctoral Associate - Amyloid Strain Differences in Alzheimer's Disease Houston, Texas (US) Baylor College of Medicine (BCM) Postdoctoral Associate- Bioinformatics of Alzheimer's disease Houston, Texas (US) Baylor College of Medicine (BCM) Postdoctoral Associate- Alzheimer's Gene Therapy Houston, Texas (US) Baylor College of Medicine (BCM)",
    "commentLink": "https://news.ycombinator.com/item?id=40530670",
    "commentBody": "Japan's push to make all research open access (nature.com)599 points by sohkamyung 17 hours agohidepastfavorite91 comments low_tech_love 12 hours agoThe term “open access” is misleading and carefully engineered to generate good will, when in fact it should be termed “pay to publish” (as argued quite nicely by Brian McGill [1]). As it stands today, OA is mostly a public money sink, a big scheme to drain public money from European countries. Not only are we paying a ridiculous, ungodly amount of money for people to host PDFs on a website, but the entire idea of publishers competing for the quality of their research output (in order to get submissions) has also basically been eradicated and turned meaningless. Reviewers are pushed to accept papers instead of rejecting them, because a rejected paper makes no money, and now we are left with a deluge of noise that passes for scientific literature. I sincerely pity the PhD student who needs to run a serious, systematic literature review in 2024. Hell sounds more attractive. If you want open access, ditch the publishers and fund volunteer expert communities to edit and publish their own papers. You can’t have the cake and eat it too. [1] https://dynamicecology.wordpress.com/2024/04/29/the-state-of... reply impendia 6 hours agoparentI am a mathematics researcher. I agree with most of what you say, but > Reviewers are pushed to accept papers instead of rejecting them, because a rejected paper makes no money The only pressure I ever get is from editors (i.e., other mathematicians, who make all the final decisions), if I'm taking too long and they want me to hurry up. If a representative of the publisher attempted to pressure me for any reason, then my response would be less than polite. Occasionally I have gotten review requests from \"pay to publish\" journals which will publish pretty much anything, and which don't have any credibility within the math community. These, I simply delete as spam. reply abdullahkhalids 38 minutes agorootparentIt's possible for pressure to exist at a higher level. The editor simply stops asking reviews from people who reject papers above some threshold rate. Or at even a higher level, the journal replaces the editor who maintains a too low an acceptance rate. These pressures are harder to notice, without population level data. reply rscho 5 hours agorootparentprevJust occasionally? I get at least 3-4 of those daily! They've become seriously annoying, and confused senior professors around me often fall into the trap. reply low_tech_love 5 hours agorootparentprevI reject the invitations too, but the fact is that they exist, there are (many) people who do them, the papers are published, they come up in searches, and the whole thing becomes a muddy mess. Most people say “we’ll just don’t read/cite them” but the fact is that there is no clear red line; it’s all foggy. In order to look at a paper and say “I will not read you because you are crap” you need to spend some time with it, and if you have 2000 of them, that translates to a lot of wasted time. The reason why journals are supposed to do serious peer review was exactly so that we don’t have to do it ourselves. reply tmalsburg2 12 hours agoparentprevJapan's initiative is focused on \"green open access\" which is different from pay-to-publish. I recommend the section titled \"Green OA\" in the submitted article. Relevant quote: \"Japan’s move to greater access to its research is focusing on 'green OA' — in which authors make the author-accepted, but unfinalized, versions of papers available in the digital repositories, says Seiichi. Seiichi says that gold OA — in which the final copyedited and polished version of a paper is made freely available on the journal site — is not feasible on a wide scale. That’s because the cost to make every paper free to read would be too high for universities.\" reply cherryteastain 9 hours agorootparent>in which authors make the author-accepted, but unfinalized, versions of papers available in the digital repositories This is already the case, almost wveryone puts the preprints on arxiv/SSRN reply mcbuilder 7 hours agorootparentThat's true for most recent AI/ML work, but hardly true for all of research. reply cherryteastain 7 hours agorootparentI think it applies to most STEM fields. I'm a reviewer for several journals in a STEM field (not AI/ML specifically, but some manuscripts do try to apply AI/ML to this field) and the vast majority of authors seem to upload their preprints to arxiv etc. Social sciences may be behind though as you say, I do not know as I'm not in that field. reply ants_everywhere 6 hours agorootparentAs someone with graduate degrees in both STEM (math) and social science (psychology) fields, it's true that social science is way behind STEM in terms of preprints to digital archives. It's possible there's momentum here in the last 5 or so years that I'm unaware of though. That matters for a few reasons: (1) the average person more frequently encounters psychological, social and medical issues more than they do math problems. And since the research in those fields tends to be pay walled, people are at the mercy of things like SEO spam medical and health sites. (2) wrong ideas in medicine or psychology can (and have in the past) damaged entire generations of people. So in that sense their blast radius can be very large. This means that peer review is especially important and that there's a potential negative externality to posting preprints and drafts before they're finalized. I suspect we'd have to solve the peer review and quality problem before STEM-style preprint archives become the norms in all fields. reply whimsicalism 2 hours agorootparentprevNot true at all for bio and medical science in particular. (yes biorxiv exists but it is not most papers) reply jfkfif 6 hours agorootparentprevEcon has a big working paper culture reply alephnerd 5 hours agorootparentprev> I think it applies to most STEM fields Much of the medical and life sciences space does not publish on Arxiv or OA platforms. It's slowly changing with Green OA initiatives being pushed by government donors, but not there yet. > Social sciences may be behind though as you say Econ, a lot of PoliSci, Finance/Business, and Computational Linguistics was very early on the OA/Working Paper movement. reply cherryteastain 5 hours agorootparentIsn't biorxiv quite popular? reply insane_dreamer 5 hours agorootparentprevIt’s also true for biological research (bioxriv) reply consp 8 hours agorootparentprev> That’s because the cost to make every paper free to read would be too high for universities. That's just bs. They can make a law fixing that for free but won't. It's not can, but will which is the problem. reply low_tech_love 4 hours agorootparentThe truth is that OA is a childish illusion that got “absorbed” by the adults in the room who tapped the kids on their back and said “no worries, we’ll take it from here”. Then they turned traditional publishing, which was already an elaborate expensive ruse, into OA which is an even more expensive (but less elaborate) ruse. Now everyone is happy, except someone trying to do actual research and having to read 1000 meaningless papers a day. reply low_tech_love 11 hours agorootparentprevGreen OA is just as meaningless as—-or worse than—-Gold OA. You pay the publisher a large sum of money for the “right” to self-publish the preprint, and they still paywall it. The vast majority of people will find the paywalled version before yours, and anyway there is no guarantee that your preprint is accurate with the final, published version, so most people will still trust the paywalled version more than your PDF. Especially when performing systematic literature reviews where you need to document the sources of your references. The current implementation of OA (any of them) is basically a self-fulfilling prophecy: we convinced ourselves that “publishers are evil” and impossible to get rid of, and now we are paying them so that they don’t have to do their job. We basically retired publishers early with an extra pension, all because everyone “wants to believe” in open access. But guess what? This is not disruptive. OA is just as “capitalist evil” as the usual publishing, or even more so. Do you want to be disruptive? The disrupt. Get rid of the publishers. Or at least constrain funding only to not-for-profits for example. reply shellac 10 hours agorootparent> You pay the publisher a large sum of money for the “right” to self-publish the preprint, and they still paywall it There's no APC with Green OA, so what money are you talking about? Green OA is regular publishing, but with self archiving. There will be a version freely available, and the publishers aren't paid for that privilege. If you want a route to the death of publishers, green OA is a promising one. (I think the headline ought to emphasise that this is pushing green OA, which is the interesting bit) reply low_tech_love 5 hours agorootparentI’ve had to pay for the option to publish my preprints in a couple of CS journals. I’ll look into that. reply shellac 4 hours agorootparentAh, I somewhat see the confusion. Green OA doesn't require the publisher to publish other versions for free, it just means you are allowed to publish them. Typically you'd publish them via an institutional repository, preprint server (often discipline specific), or in one of a number of free online services. reply radford-neal 5 hours agorootparentprev\"there is no guarantee that your preprint is accurate with the final, published version, so most people will still trust the paywalled version more than your PDF\" I think this is backwards. The definitive version that should be cited is the freely-available one, since that is the version that everyone can read. No one should cite the paywalled version. reply kome 9 hours agorootparentprev\"Green OA is just as meaningless as—-or worse than—-Gold OA. You pay the publisher a large sum of money for the “right” to self-publish the preprint\" that's not how it works. you don't pay anything. reply low_tech_love 5 hours agorootparentThat’s not what I experienced in the past, at least not with IEEE (I stopped caring after a while). reply barfbagginus 1 hour agoparentprevGold OA is trash, don't confuse it with true OA modes: 1. Green OA: peer reviewed and free for reader and author; author waives copyright 2. Diamond OA: peer reviewed and free for reader and author; author KEEPS copyright 3. Black OA: free for readers, pirated by hackers; DISREGARDS copyright See my other note where I argue that BOA is the most important one, since it does not depend on state funding or publisher largesse. It simply seizes the knowledge that must be seized. reply norswap 5 hours agoparentprevIn CS, it's widely accepted that you can publish your \"drafts\" publicly. Basically just the real deal without the notices of publication (I guess maybe legally you have to leave some differences in? I never bothered, and I doubt the publishers who everyone kinda hates would dare creating this kind of controversy). As others have pointed out, some countries mandate national or institutional repositories. A good grassroots way out of this is educating researchers to always take one of these options. Let publishers overplay their hand and get crushed. Of course, it would be nicer if regulators realized how ridiculous this all is and crushed them without the need for public outcry, but one may dream. reply contrarian1234 4 hours agorootparentWhen I published with an Elsevier journal they explained the \"draft\" thing pretty clearly. They said you retain copyright on whatever you write yourself. But once you have reviewer feedback and incorporate feedback from the journal editor(s) then it's no longer entirely your work and you couldn't distribute it at your own will. You could pay the journal a huge amount of money (in effect for the work done by the editors), and then the paper would be open access. The fees might not be reasonable (it's kind of hard to judge) but the overall logic made sense to me So the first draft before review - the one you wrote all on your own - is what you can put up online. I'm not in CS but I assume the logic is similar in other fields. They also provided a separate link of the final published PDF that you could use to distribute the paper to colleagues and interested parties. This link worked for a sufficiently long period of X amount of days/months. After that it was paywalled and in their garden reply tokai 10 hours agoparentprevMcGill and you got it backwards. Open Access has a quite clear definition.[0] Publishers have co-opeded the term and dopne everything to confuse the issue. All the so-called open access colours are mainly publisher made, to water down the true OA definition. [0] https://www.budapestopenaccessinitiative.org/read/ reply low_tech_love 5 hours agorootparentSure, but none of that matters if the big publishers are the ones gobbling up the money anyway. For some reason it’s better to pay them a ridiculous amount of money to host PDFs than it is to pay for subscriptions? What’s the difference? Most university libraries have always had access to all the important journals and anyone can go there and do their research. The whole idea that subscription is evil and OA is good is ludicrous. We’re paying the same amount of money (or more) only to get a much, much shittier service, and a torrent of absolute meaningless “research” that only serves to drown out the good ones. PS: And by the way, if you say that “not every country has access to these libraries” then think about whether they have money to pay for APCs. reply davidparks21 2 hours agoparentprevI struggle with this question. The editor plays a valuable role and it's not hard to swallow the argument that editors make science better, they're the front line filter. Employing them full time is a core value that the current system provides. I'm wary (but not close minded) to the idea that volunteer orgs could scale to the top echelons of scientific publication, which require a lot of filtering. Perhaps if we look to the conference paper model where funding primarily comes from the conference? The conference brings benefits beyond publishing. In that model publishing equates to financial discount or free entrance to the conference, so it flips the equation. reply eslaught 1 hour agorootparentI work in CS and we do (mostly) conference publishing. I could be wrong, but I don't think PC chairs are paid, by anybody. Maybe they get free housing at the conference, but this is more of a consolation prize given the amount of work involved. Certainly the rank and file PC members don't get anything. I was on one PC mailing list where one of the organizers accidentally let slip that (some) organizers get free housing, and there was a big uproar in the PC. None of us get anything like that. So this is literally a system where the expert reviewers get nothing, and even the chairs in charge do it nearly for free. What part of this needs to cost money? The peer review comes from the community. The exclusivity and filter comes from the community. Even the funding comes from the community, because community members pay to go to the conference. What the publisher does is, as GP noted, mainly to host PDFs on a website and make sure they stay up. That costs something, but nothing like what the licensing fees for these services are (or the so-called \"open access\" fees that we now pay). reply mfld 7 hours agoparentprevI don't get it. Ditching publishers to fund expert communities? Shouldn't there be some form of organisation that connects those experts, selects some as journal editors, provides infrastructure for submission/review/paper access, etc? If yes, wouldn't \"publisher\" be suitable term for such an organisation? There exist publishers that, instead of being part of big media outlets, a run by research organizations themselves (e.g. EMBO press). I agree that it would be nice if more would be published there. reply low_tech_love 5 hours agorootparentI mean in the same way communities organize conferences regularly without becoming publishers. Anyway if you want to call them publishers fine, as long as they are not giant behemoths who feed on large amounts of public money to do nothing. At least when publishing was an actual market (instead of being just a money sink) the publishers had to actually do their job to survive. reply sharpshadow 9 hours agoparentprevIt's a shame that the public has to pay twice. Tax money pays it in the first place and then we have to pay for it again.. reply barfbagginus 1 hour agoparentprevDon't forget the most important OA mode Black OA means hacking journals and making their papers free for all. That's the only OA we need to invest in. Make the BOA ratio into a perfect 1.0, and all other problems with OA vanish. reply DiogenesKynikos 8 hours agoparentprev> Reviewers are pushed to accept papers instead of rejecting them, because a rejected paper makes no money Prestigious journals have very low (single-digit percentage) acceptance rates. One or two rungs down, the standard domain-specific journals are still rigorous, and you need to do substantial work to get published in them. It's a hurdle that any decent graduate student with a good advisor can pass, but it's not as if you can just write anything and get published. There are junk journals that will publish anything, but I don't think scientists generally pay much attention to those journals. > I sincerely pity the PhD student who needs to run a serious, systematic literature review in 2024. Hell sounds more attractive. Doing a literature review is not that difficult. You begin with a few highly cited papers. Then you look at what papers they reference in the intro. You also look at the list of papers that cite them and sort by the number of citations. You can work in both directions and pretty quickly establish what the important papers in the field are. Notice that the important thing here is to look at citations. Getting a paper into a journal is only the first, most basic step. Peer review is only an initial quality check. Over time, other scientists decide what papers are important and cite them. If a paper racks up 100 citations, that means a lot more than just passing peer review. reply low_tech_love 5 hours agorootparent> There are junk journals that will publish anything, but I don't think scientists generally pay much attention to those journals. Yes, but the point is not that the good journals are not there, it’s that the good ones are drowned out by the absolute humongous amount of garbage that comes out every day. If you do research you know that it’s not so simple like “oh I only read papers from journals X and Y”. Good research comes out everywhere, and if you don’t cite it people (I mean reviewers, if you can call them that) will complain, and anyway you’ll be putting yourself in a bubble which is bad practice. It used to be that journals would do the work for you to separate good from bad research, but nowadays it’s on the researcher to dig through 1000 papers to find a single good one. It’s ridiculous and meaningless. If we’re paying them and making them rich, at least I’d expect them to do their job. reply rscho 5 hours agorootparentprevYou are not describing a systematic literature review. Systematic reviews are hell indeed, at least in non-STEM fields. reply 2Gkashmiri 11 hours agoparentprevhttps://shodhganga.inflibnet.ac.in/ india's research repository. all phd thesis and everything is here. reply tugberkk 11 hours agorootparentsame here in Turkey. https://dergipark.org.tr/en has a lot of journals free of access. One can also find the phd and msc thesis too in https://tez.yok.gov.tr/UlusalTezMerkezi/giris.jsp reply xvilka 8 hours agorootparentprevIt would be nice to make a simple website that will list those repositories in easily discoverable way per country. Something akin to \"Awesome Research\" list. reply 9dev 11 hours agorootparentprevYou got to love India’s ability to pick the most indiscoverable domain names possible for their projects... reply datpiff 9 hours agorootparentIt's almost as if they're speaking a different language reply carlob 8 hours agorootparentor 22 reply pessimizer 2 hours agoparentprev> OA is mostly a public money sink, a big scheme to drain public money from European countries. Not only are we paying a ridiculous, ungodly amount of money for people to host PDFs on a website This is a choice that European countries are making that has nothing to do with open access. You can always choose to overpay well-connected insider contractors to accomplish any of the functions of government. reply WarOnPrivacy 12 hours agoparentprevHeadline: Japan's push to make all research open access > The term \"open access\" is misleading and carefully engineered to generate good will, when in fact it should be termed \"pay to publish\". [1] [1] Seems to be about how OA is done in countries that aren't Japan. reply gbnvc 3 hours agoparentprevNo, you can invent this idea that open access is a boogeyman, but it’s flat out corporate lies. This is free speech. I’m allowed to say I think a statement is untrustworthy. reply a_bonobo 13 hours agoprevAustralia already has a similar system: if the research is funded by a national funder, then the pre-publisher version has to be deposited at the institute's research repository (basically the Word/PDF file you sent to the publisher, before they add their copyrighted formatting). I'd judge the success so/so: it's barely enforced, librarians will remind you if you didn't upload one paper, but it's not like there's any consequences. It's also about the text of the paper, not about the associated data. And these repositories are generally built by for-profit providers and aren't exactly open: I know for a fact that my Google Scholar profile does not link to the institute's repository, papers in there are barely findable. it also doesn't change how researchers are evaluated, so the eternal rat-race to get into Nature/Science etc. continues, with all the associated problems. We've had this system for years now and it feels like it barely made a blip. >Seiichi says that gold OA — in which the final copyedited and polished version of a paper is made freely available on the journal site — is not feasible on a wide scale. That’s because the cost to make every paper free to read would be too high for universities. One should note that this isn't too expensive because of associated costs: it's that the for-profit publishers keep ratcheting up the cost. Nature-group journals now take more than ten grand in OA fees per paper, which is not justified by any associated cost. They're typesetting a PDF and hosting it for ten grand a PDF. reply dguest 11 hours agoparentThis means that journals can't prevent you from uploading a preprint though. A few publishers do this [1]. Even if the upload isn't enforced it means the restriction can't be either. But it's odd that the paper must be in the institute's repository and not one of a larger list of preprint servers. I would assume the authors would much rather put a paper on arXiv than on their shoddy university repository. [1]: https://en.wikipedia.org/wiki/List_of_academic_publishers_by... reply a_bonobo 7 hours agorootparentHaving submitted to biorxiv before many times: oh yeah!!! the ease of submission there is second to none. For-profit publishers' submission process is an overcomplicated pile in comparison. reply kergonath 13 hours agoparentprev> Australia already has a similar system: if the research is funded by a national funder, then the pre-publisher version has to be deposited at the institute's research repository (basically the Word/PDF file you sent to the publisher, before they add their copyrighted formatting). It’s more or less the same in the UK and France as well. > I'd judge the success so/so: it's barely enforced, librarians will remind you if you didn't upload one paper, but it's not like there's any consequences. IIRC in the UK it does not count in the national performance assessment it if is not in an open repository. > It's also about the text of the paper, not about the associated data. Yeah. Data is a big problem. There are many different types that cannot be handled just like a pdf (tons of files in the same dataset, huge archives and everything in between). It’s a huge lot of storage space, the vast majority of which will never be even accessed, never mind used. I’ve been part of several initiatives and it is not easy to set up. > And these repositories are generally built by for-profit providers and aren't exactly open: I know for a fact that my Google Scholar profile does not link to the institute's repository, papers in there are barely findable. That’s a damn shame. The British government is doing a lot of things right with their open access policy, but a thing that works quite well in France is that there is a single national repository ( https://hal.science : it voild be better indexed but it does show up on Google Scholar) and everyone uses it. This is the sort of things that makes sense at the national level. A myriad of small repositories on obscure university websites is not very helpful. That way it also does not depend on the (possible lack of) willingness of individual institutions to fund it and do it properly. > it also doesn't change how researchers are evaluated, so the eternal rat-race to get into Nature/Science etc. continues, with all the associated problems. If we’re really serious about open access, only articles in a public repository should count. All the legal issues there used to be were cleared up, the infrastructure is there (things like arxiv can be used as a last resort for governments or institutions that won’t get their ducks in a row). The issue is really lack of good will. reply 9dev 11 hours agorootparentTheoretically, something like a national OPDS feed would be perfect for this: OPDS is a publication distribution feed protocol that allows to include other sub-feeds, making for a distributed tree structure. A country could publish a collection of individual university feeds, which in turn linked to the papers of their researchers. reply karaterobot 15 hours agoprev> it also said that it would invest 10 billion yen (around US$63 million) to standardize institutional repositories — websites dedicated to hosting scientific papers, their underlying data and other materials — ensuring that there will be a mechanism for making research in Japan open. The important part isn't necessarily the papers, it's getting access to the data. I'll note that the hurdle for open access is not a lack of support from funders or from the government, it's from researchers. Researchers want everybody's else's data, but they don't want to give their data to anybody else, because they want to be the ones to publish on it. It's IP. It can take years to get access to a dataset, because the owners throw every obstacle in your way, and drag their feet as much as possible—it makes cancelling a gym membership look easy. And I am speaking about projects where open access is written into the funding as a requirement. My point is that I'd be interested to know what data governance processes they're putting in place if all the data is being consolidated into a single system, as the article says it is. reply Staple_Diet 13 hours agoparentI recall a paper that sought to explore how willing authors were to release data when they had published open access and stated 'data available on request'. Iirc it was below 50% that responded with data. See related here; https://osf.io/preprints/psyarxiv/jbu9r/ That being said, there are also issues with opening the gates to anyone to get your data and use it to publish; https://www.thelancet.com/journals/lancet/article/PIIS0140-6... My data is OA, but I'd appreciate any one wanting to use to first reach out and discuss any nuances that may influence their analysis. reply FredPret 12 hours agoparentprevThere should be some academic reward for just gathering quality data reply tkgally 15 hours agoprevLast year, my employer, the University of Tokyo, became the first Japanese university to sign the Declaration on Research Assessment (DORA) [1]. Like open access, that initiative is a positive move toward a better research environment, and it would be great if more institutions committed to it. [1] https://sfdora.org/ reply staunton 12 hours agoparentDid this change anything about how research is conducted? reply tkgally 11 hours agorootparentGood question. It's probably too early to tell. By signing DORA, the university leadership has stated support for a principle about how hiring and promotion decisions should be made. But it may take a few years before that principle is fully absorbed by the faculty committees who actually evaluate candidates. reply EchoReflection 3 hours agoprev\"Japan plans to make all publicly funded research available to read in institutional repositories. \" Oh, are you not a student or teacher or employee here at the University of XYZ? I'm sorry, this research is not 'freely' available to you. reply gigatexal 13 hours agoprevI support this 100%. All publicly funded research even in the least bit should be public. Shwartz would be proud. reply surfingdino 11 hours agoparentPlaying the Devil's advocate for a sec, I'd ask the public if it is prepared to fund hosting on those publications in perpetuity? And to answer my own question, I think we should demand that scientific papers are made available for free by publicly-funded scientific libraries. reply whimsicalism 2 hours agorootparent> I'd ask the public if it is prepared to fund hosting on those publications in perpetuity are we pretending pdf hosting is actually costly or what? the devil should hire better advocates reply gigatexal 9 hours agorootparentprevThe library of Congress could host it. Hell I’d ask for my taxes to be raised if needed to host these publications for everyone. reply jfkfif 6 hours agorootparentprevosti.gov reply amadeuspagel 7 hours agoprevWhy can't academics just publish their stuff on a blog like everyone else? reply ufo 5 hours agoparentPapers should be immutable, blogs bit rot, and anonymous peer review is important. The million dollar question is why don't researchers publish on journals that don't charge outrageous open-access fees. Here the main problemis that carreer advancement is predicated on publishing in prestigious venues. Such venues take advantage of that by jacking their price. Fixing this will require cooperation from governments, which have leverage on both sides of the equation: they can help fund alternative paper repositories, and they are ultimately responsible for the rating criteria of what venues are prestigious and how research grants get awarded. reply rscho 5 hours agoparentprevBecause: poor graduate students would contemplate suicide for having to gather hundreds of papers from just as many obscure blogs. reply insane_dreamer 5 hours agorootparentif authors all just uploaded a copy of their final PDF to sci-hub this would solve the problem reply tmalsburg2 12 hours agoprevSmart, because why would you use billions in tax money to fund research then let publisher parasites take the results hostage. reply rondrabkin 10 hours agoprevSo many problems with the existing process but questions could include -is anyone going to take down bad quality or fake research -is there a way to update published papers (like link rot in references ) -will the repositories be SEO optimized so people will find them reply jimbobthrowawy 5 hours agoprevNice change! I expect people to still use sci-hub, or something like it, to access these for UX reasons. reply lieblingautor 14 hours agoprevSounds great, hope it will inspire its neighboring countries to take a similar initiative. reply koolala 16 hours agoprevInternet Noodles of Babel! Let's have every corporation do this too! We can solve every problem that matters by working together like this, the rest is just corpo-gaming. reply TaylorAlexander 15 hours agoparentAbsolutely! I think the concept of proprietary knowledge for profit was actually a huge mistake, and that humanity can advance much faster and more effectively through open collaboration. This applies to patents too (and arguably copyright). reply jrhey 13 hours agorootparentThere is no advancement without profit. Advances are a product of market competition. When no company can profit from investing in R&D, there will be no R&D. reply immibis 7 hours agorootparentThe government invented the internet. reply waihtis 5 hours agorootparentwith money that came from ......? reply immibis 2 hours agorootparentNonsensical question. You invent things with brainpower, not money. reply waihtis 10 minutes agorootparentTry going without food for a month and see how many novel inventions you can make. See, even thoughts require participation in the capital markets. reply __loam 13 hours agorootparentprevWhat a sad view of things. reply koolala 13 hours agorootparentHi, I feel more advanced getting to talk with you so I am grateful for that, didn't cost us anything :) reply anigbrowl 12 hours agorootparentprevPfffffft reply gumbojuice 13 hours agoprevI could imagine a problem here in India, where there is no funding for publishing papers, especially for graduate students. reply thisisjaymehta 12 hours agoparentExactly, but government is doing its best to boost R&D, what matters more is private funding for R&D. Big companies like Reliance, TATA and Adani should setup research labs in universities and colleges. reply 2Gkashmiri 11 hours agoparentprevwhat do you mean? reply worthless-trash 16 hours agoprevThis could be the start of something beautiful. reply swader999 13 hours agoprevImagine if this was so advantageous that a large portion of the rest of the world got pulled into it. Interesting times. reply klysm 15 hours agoprev [–] This would be amazing to see in the US. Unfortunately I feel like lobbying is too powerful with the death grip publishers have. If you were to do a popular vote, it would be nearly unanimous I expect. reply pkaye 14 hours agoparentIf you read the article it mentions that US implemented an OA mandate in 2022. > The plan follows in the footsteps of the influential Plan S, introduced six years ago by a group of research funders in the United States and Europe known as cOAlition S, to accelerate the move to OA publishing. The United States also implemented an OA mandate in 2022 that requires all research funded by US taxpayers to be freely available from 2026. reply karaterobot 15 hours agoparentprev> The United States also implemented an OA mandate in 2022 that requires all research funded by US taxpayers to be freely available from 2026. reply jfkfif 6 hours agoparentprev [–] DOE already mandates this reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Japan is allocating ¥10 billion to universities to create institutional repositories, making all publicly funded research open access by January 2025.",
      "Researchers with government funding must make their papers freely available, focusing on 'green OA' (author-accepted versions) due to the high cost of 'gold OA' (publisher's version).",
      "This initiative aims to enhance research traceability, facilitate secondary research, promote collaboration, and address Japan's declining international research standing, aligning with global open access trends."
    ],
    "commentSummary": [
      "Japan's push for open access (OA) research has sparked debate over the financial and quality implications of OA publishing, often criticized as \"pay to publish.\"",
      "The discussion contrasts \"green OA\" (self-archiving) with \"gold OA\" (paying publishers), highlighting the high costs of the latter and suggesting alternatives like national repositories and volunteer-run peer review processes.",
      "The debate underscores the need for systemic changes to democratize research access, with concerns about the sustainability and quality of OA models and calls for better data governance and academic incentives for data sharing."
    ],
    "points": 599,
    "commentCount": 91,
    "retryCount": 0,
    "time": 1717119960
  },
  {
    "id": 40531984,
    "title": "Homeowner Hides Boat with Realistic Mural on Fence, Goes Viral",
    "originLink": "https://news.artnet.com/art-world/fence-boat-painting-artist-hanif-panni-2487875",
    "originBody": "Art World A Man Was Ordered to Build a Fence to Hide His Boat. He Asked an Artist to Paint the Boat on the Fence Hilarity—and virality—ensued. Etienne Constable's boat alongside the painted fence by Hanif Panni. Courtesy Hanif Panni, @hanifwondir. by Brian Boucher May 15, 2024 ShareShare This Article There’s a new high water mark for passive aggressiveness in the visual arts—and the pun is intended. When the town of Seaside, California ordered homeowner Etienne Constable to build a fence to conceal the boat in his driveway, he erected the fence all right. And then he hired his neighbor, artist Hanif Panni, to paint a mural on the fence—realistically depicting the boat itself. Call it passive-aggressive, call it malicious compliance, but in any event, call it hilarious. “I’m not a rule-breaker but I like to make a political statement as necessary as well as a humorous statement and a creative statement,” Constable told NBC affiliate KSBW of Monterey Bay. The city had threatened a $100 fine if Constable didn’t comply, but, ironically, he ended up paying for more than that for the artist’s fee alone. “It was his idea,” Hanif said in a phone interview. “There were Home Improvement-kind of moments, where we were chatting and joking about it, a ‘Wouldn’t that be hilarious?’ kind of thing. I think the more he got it ingrained in his head and brought it up a lot, the more I thought, ‘Yeah, that sounds great.’” Detail of the painted fence by Hanif Panni. Courtesy Hanif Panni, @hanifwondir. The story has gone wildly viral, reported in news outlets and catching fire on social media. “To be the number one Google result for ‘boat fence painting,’ that is a trophy that I never knew that I needed,” Hanif said. “He’s the quintessentially good neighbor,” he said about Constable. “We’re pals, we talk, we share a backyard fence, so we’ll be working in our respective gardens and chatting. His boat has always been a friend of mine. I’ve never been on it but he invites me. He brings vacuum-sealed fresh salmon for me and the family.” “Everybody loves it,” said Constable in a phone interview. “It’s amazing to not have the controversy that can come up from random things on the Internet. I love the fact that people appreciate my zany idea.” Why does Constable think it has caught fire the way it has? “I think the reason is because it’s absolutely in your face and it’s absolutely not offensive,” he said. “It was done very elegantly, it’s got a statement, it’s artwork so it’s open to interpretation, as is the political statement, and it all comes together in a perfect picture.” The mural is painstakingly done; the result looks exactly like Constable’s 19-foot Arima Sea Ranger. Not only that: the shrubs on one side of the driveway as well as Constable’s house on the other side are also faithfully depicted in Hanif’s fence painting. “This struck me as the right way to go, and something that makes my house unique,” the homeowner told USA Today. At least one local official is on Team Constable: “I applaud the constituent’s creative approach, which not only addresses compliance but also showcases a community-driven solution,” city council member Alexis García-Arrazola told the news outlet. Hanif studied art and art history at the University of Oregon and is a working multidisciplinary artist in a wide range of formats. He has worked with clients as varied as Lexus and the Pharcyde, as well as for Google (for whom he has done DJ work) and the Wu-Tang Clan (the Staten Island rap titans have displayed his banners onstage at their performances). The painting in progress. Courtesy Hanif Panni, @hanifwondir. The artist is being invited to paint other boat murals for other homeowners, he said. While he’s grateful for the offers of work, he’s a bit hesitant to be typecast. “I do some wild stuff and I’ve been at it for a long time,” he said. When asked what kind of wild, he responded: “Demigod head explosion introspective yet cosmic nature mixed with mechanical moving parts, you know? And a lot of multimedia. I pride myself on learning new technologies and approaching them with an analog hand. And yet it’s the boat, the realistic boat on my neighbor’s fence, that’s the one…” The completed fence, painted by Hanif Panni. Courtesy Hanif Panni, @hanifwondir. As makes sense for someone with an art background, Hanif’s work fits nicely into art-historical traditions. One, it is an example of trompe-l’oeil painting (French for “to fool the eye”), a tradition that stretches as far back as classical Greece. The tale goes that one artist, Zeuxis, painted a still life so convincing that birds came to peck at the grapes; his rival, Parrhasius, then invited Zeuxis to his studio to look at a painting hanging behind a curtain, but when Zeuxis went to pull it back, it was revealed to be a trick, and Parrhasius won the contest. Hanif’s work also echoes the great tradition of nautical painting, which also stretches back to ancient times and includes works by artists like Winslow Homer, Fitz Hugh Lane, and J.M.W. Turner. On top of it all, there’s the fact that Etienne Constable shares a surname with English painter John Constable. If the latter had painted nautical scenes or trompe-l’oeil works, well, that would have been a hat trick for art history nerds. “There are a lot of Easter eggs,” Hanif acknowledged. “I appreciate the conversations this thing is spawning,” he said. “It’s the reason I do public art—to get people to have these conversations and have it branch into something that it wasn’t before.” Follow Artnet News on Facebook: Want to stay ahead of the art world? Subscribe to our newsletter to get the breaking news, eye-opening interviews, and incisive critical takes that drive the conversation forward. Share This ArticleShare This Article Article topics Art News Brian Boucher The best of Artnet News in your inbox. Sign up to our daily newsletter. Please enter a valid email address Signup failed. Please try again later. Thank you! You have successfully subscribed to Artnet News. More Trending Stories Art Fairs Here Are the Dealers and Artists Making Big Debuts Amid Frieze New York Art World Collector Alex Abedine Balances His Life in Law With the Joyful Chaos of Art Art Fairs Frieze Forecast: The Venice Effect Hits NYC, Spotlight on Latin American Women, and More Art Fairs What Should You Buy at Frieze Los Angeles? Here Are 6 Picks From Top Advisors Art Fairs Here Are the Dealers and Artists Making Big Debuts Amid Frieze New York Art World Collector Alex Abedine Balances His Life in Law With the Joyful Chaos of Art Art Fairs Frieze Forecast: The Venice Effect Hits NYC, Spotlight on Latin American Women, and More Art Fairs What Should You Buy at Frieze Los Angeles? Here Are 6 Picks From Top Advisors",
    "commentLink": "https://news.ycombinator.com/item?id=40531984",
    "commentBody": "A man ordered to hide his boat painted the boat on his fence (artnet.com)422 points by geox 12 hours agohidepastfavorite350 comments vasco 11 hours agoAmerican land of the free is being able to bring a gun to school but getting a fine because your grass is too tall. These HOA type stories are always so funny. It blows my mind people buy in places with rules like these. reply db48x 10 hours agoparentSomeone always scoffs, but I think it is usually a sign that they don’t really understand freedom. In particular, the freedom of association. Many countries recognize it, so I don't need to get in to the exact legal justifications for it. Put plainly, this is the freedom to form groups that govern the behavior of their members. A group could be a dinner party, a study group, a book club, a circle of friends. It can also be much more serious. It could be a corporation, a union, a church, a political party, or a religion. Or it could be an HOA, or a town, or a state. If you like one or more of those types of groups then it is important to protect the freedom of association, even if you don’t like any of the others. Every group has rules, even the most informal. When you invite your friends out to dinner you probably don’t sit down and vote on the exact rules your dinner party will follow, but your party has rules regardless. And every group punishes violations of the rules somehow. If you get drunk and dance naked on top of a table, then you will suffer some form of sanction by the other members of the dinner party. At the very least you probably won’t be invited back. More formal groups like HOAs, corporations, and towns tend to have written rules and proscribed punishments for violations. They also have built–in ways of changing those rules. Ultimately some HOAs are useful while others are petty and run by vindictive idiots. An HOA that was useful for 50 years and never created any drama could turn bad next year when they elect some new board members. An HOA that is bad today _could_ elect new board members and then become drama–free and stay that way for 50 years, even if it seems unlikely. People who end up in a petty HOA usually regret it, and come to see joining any HOA as a big mistake. People in a sane HOA often wonder what the fuss is about. But here’s the truth: true freedom includes the freedom to make mistakes, even big mistakes. The freedom of association allows us to join any group we want, but sometimes joining will be a mistake. You can’t prevent people from making those mistakes without limiting their freedom. You can’t even reliably predict which mistakes they’re going to make, so you would end up preventing them from doing things that wouldn’t be mistakes too. reply robryk 8 hours agorootparentHOAs require more than (or a very specific extension of) freedom of association to exist: they require an ability to bind any future owners of a piece of land to join the HOA (regardless of how they acquired that land: via sale, inheritance, bankruptcy, ...). For property that's not real estate that's usually not possible (see e.g. inability of companies to provide binding promises on how the data they have are used after they go bankrupt). Given that this ability applies very inconsistently across types of property, it doesn't seem like an essential part of freedom of association. reply throw0101c 7 hours agorootparent> HOAs require more than […] freedom of association to exist: they require an ability to bind any future owners of a piece of land to join the HOA […]. You are often not buying a piece of land when you move to a HOA-linked residence, rather you can be buying a particular unit, but there are also 'common areas' that are the property of the legal entity of the HOA. If you do not want to be part of an HOA then you have to purchase what is called (at least in Canada) freehold land. > While most people hear the word “condo” and think of an apartment style unit, that’s not always the case. Some townhouses (and even certain detached homes) are considered condos too. * https://teamkate.ca/difference-freehold-condo-home * https://www.squareyards.ca/blog/freehold-vs-leasehold-regart... * https://wowa.ca/freehold-meaning * https://en.wikipedia.org/wiki/Freehold_(law) If you do not want to be part of an HOA tell your real estate broker that this is one of the criteria for where you want to live. reply javagram 4 hours agorootparentThe local government in many areas is often requiring an HOA to be formed for new developments to manage stormwater and road maintenance that they don’t want to be responsible for. In those situations the HOA could be seen as government imposed rather than purely “freedom of association”. I own a 100 year old home so there is no HOA but it’s difficult for most people to afford such homes since there’s a limited supply and they’re often either in highly desirable or blighted city centers. reply anymouse123456 6 hours agorootparentprevI have no idea about Canada, but in the US, there are lots of single-family home neighborhoods that have, \"Home Owner's Associations\". reply skeeter2020 5 hours agorootparentthe GP wasn't talking about the type of structure but the legal organization of the property ownership. Canadian provinces have a Condominium Property Act that defines the legislative framework. The implementation can vary by types of units (ranging from apartment-style to single family homes) and the ownership - typically \"you own the inside; communal outside\" or \"you own the inside & dirt; communal outside and shared public spaces\". You don't hear talk of \"The HOA\" but rather \"The Condo Board\" reply NoMoreNicksLeft 4 hours agorootparentprev> If you do not want to be part of an HOA tell your real estate broker that this is one of the criteria for where you want to live. Yes, tell your realtor \"I don't want to look at any of the properties you have for sale, or any properties within the greater [insert city here] metro area, or for that matter any geographical location within 100 miles of here\". He'd look at you less strangely, and might even chuckle instead of telling you to go fuck yourself and to not waste his time. And the best part of all is that these two requests are essentially identical. reply marssaxman 48 minutes agorootparentWhere do you live? That sounds terrible. I would never consider any house encumbered with an HOA, and I would fire any realtor who ignored my wishes by showing one - but it's never been an issue, here in Seattle. reply nilamo 1 hour agorootparentprevPeople talk to real estate agents? I thought things like Zillow completely eliminated that step... reply insane_dreamer 1 hour agorootparentAll Zillow does is put you in touch with a real estate agent. reply BobaFloutist 2 hours agorootparentprevIn all fairness, if the majority of the members of the HOA don't like them, surely they could vote to disband it? reply soco 10 hours agorootparentprevThere are many countries in the world which don't measure their freedom in eagles per square burger yet are totally free to associate, and they do. I cannot do much in my neighborhood either (Switzerland here) but there is a respected process for raising cases and handling exceptions and all that. We call this the rule of law, by no means perfect either. This doesn't seem to be the case with the HOA where the system guarantees you living under the will of the leadership - be they idiots or geniuses. So if liberty is defined by randomness, yes, HOA defines the land of the free. Otherwise please allows us to scoff. reply mattmaroon 6 hours agorootparentThat is not how HOAs work. They have a very defined set of rules, and a very defined process for changing and enforcing those rules. You are not at the mercy of anyone. Local laws supersede HOA rules where they conflict and often cause “grandfathering” when HOA rules change. For instance, my condo board tried to get me to remove my grill. I pointed out that nowhere in the condo bylaws was a grill mentioned. They said something about city fire codes, so I got a note from the fire department. They were unable to force me to do anything. HOAs in my city have passed rules saying owners may not use their condos for short term rental. But my city law says that changes do not apply to people who owned their rentals before the rules changed. So new owners may not but old ones still can. Rule of law still applies, and everything is agreed to when you buy the property. You could simply buy a home outside of an HOA, so this is, squarely, freedom of association. Nobody was implying that other countries do not have this freedom, in fact, he said the fact that they do is why he did not need to justify it. reply mechanicalpulse 4 hours agorootparent> That is not how HOAs work. They have a very defined set of rules, and a very defined process for changing and enforcing those rules. You are not at the mercy of anyone. This. I live in a drama-free HOA and appreciate it for what it is and does. I had to review and agree to the covenants of my HOA as a condition for closing. The rule most likely to be in violation here is something along the lines of “only operational automobiles in good condition are permitted to be parked in the driveway; any boats, recreational vehicles, broken down or unsightly vehicles must be behind a fence”. The whole point is to maintain a clean, safe, attractive, and ultimately desirable neighborhood so as to protect property values on behalf of the association’s members. Although this homeowner’s actions are characteristic of malicious compliance, I believe the net effect is precisely in line with the stated purpose of the HOA: in compliance with the rules, an elegant mural depicting a pristine boat now adorns a new fence that restricts sight of and access to a boat of unknown condition. Anyone driving by looking to buy might come away with a chuckle and the sense that the neighborhood is committed to maintaining a safe and attractive environment for homeowners. reply NoMoreNicksLeft 4 hours agorootparentprev> You are not at the mercy of anyone. In many jurisdictions within the United States, an HOA can assess fines for exceedingly trivial violations, then after the fines remain unpaid for a time period as short as 6 weeks, file to sell your home at auction to pay the fees. They can take your home from you, evict you from it, and this is deemed perfectly legal. Do they have to provide proof, for instance, that your grass was actually one half inch too tall? No, there is no adversarial process where they would have to prove that were the case. > I pointed out that nowhere in the condo bylaws was a grill mentioned. They said something about city fire codes, so I got a note from the fire department. They were unable to force me to do anything. It must be very thrilling to be able to be an amateur lawyer and win a case. Have fun, because those people tend to hold grudges. My experience is that you'll never have a week of peace for as long as you live there again. reply mattmaroon 2 hours agorootparentThis all happened 3 years ago. I’ve never heard a word from my HOA since. I think the HOA board actually believed it was against fire code, but it turns out that’s only true if you’re not on the ground floor. (My community is 4 stories so 3/4 of people cannot have grills). Other grills have appeared since. They can only assess you fines for violations of rules you agreed to. And fines are usually small. For instance, in my last HOA, they fined me $15 for having A flag holder. I ignored the warning because it was there when I bought the place and I didn’t really want it anyway, and then when the fine came, I simply removed the flag holder and paid $15. Most of the cases were the end up confiscating somebody’s property is because they are simply egregiously not doing something they had already signed a contract saying they would do. I am sure there are cases, they always are, but it isn’t a legitimate problem. reply kennethwolters 9 hours agorootparentprevSwitzerland is an \"ultra-localist\" country according to Nassim Nicholas Taleb and I'd agree. The Kantons have more power than the federal government. So in essence, Switzerland and the USA are much alike. It's just that the USA has a flavor of this localism so extreme and chaotic that the resident in a way is supposed to relocate to a better place if they don't like it (see residents fleeing California for better places). Switzerland seems to have a more orderly form of localism where instead of the resident, the representative moves. reply Lendal 6 hours agorootparentprevMost people in the US don't live in an HOA. You may join one and enjoy its benefits, or not. Nobody is forced to. Many people want to join an HOA to enjoy its benefits but not obey its rules. These are the type of people who believe they are the center of the universe yet are in such a unique position that the laws of the rest of the universe don't apply to them. reply _heimdall 6 hours agorootparentIts worth noting that its common, at least in my experience, for HOA membership to be part of a real estate purchase contract. Meaning, if you want to buy a house and it happens to be in a neighborhood with an HOA, you don't have the option to buy that house and not join the HOA. I've never understood why we allow that to be legal, just like neighborhoods or complexes that are age restricted, but we allow it none the less. reply mattmaroon 6 hours agorootparentWe allow it to be legal because: 1. Most homes aren’t in an HOA, so you have plenty of options if you don’t like it. 2. It prevents all sorts of tragedies of the commons. 3. HOAs would have no power at all if you could opt out. It has to be part of the contract. They can be dissolved entirely, but that’s the only mechanism for getting out. 4. We believe in individual freedom, which includes the ability to chose to trade freedom for certain benefits. I trade my freedom to leave a rusty old boat in my driveway for the benefit of not having my neighbors do that. Most of us don’t have or want the sorts of things that bring down property values and don’t want to suddenly find ourselves living next to people who do. This is perhaps a symptom of a larger problem: homes are very expensive. If homes were cheap and moving were easy, perhaps HOAs would never have been invented. But for most Americans who own one a home is a substantial portion of their net worth. We want to protect it. People who complain about HOAs existing are the same as the people who complain about EVs. If you don’t want one, don’t buy it. reply supplied_demand 5 hours agorootparent==If homes were cheap and moving were easy, perhaps HOAs would never have been invented. But for most Americans who own one a home is a substantial portion of their net worth. We want to protect it.== It may not have been the reason for their invention (I can't find information on that), but they certainly exploded in popularity in an effort to restrict people from selling their homes to non-white people (see: Racially Restrictive Covenants). It's kind of ironic to see people laud them as a form of \"freedom\" while their history as a tool of blatant racial discrimination is available for all to read. ==People who complain about HOAs existing are the same as the people who complain about EVs. If you don’t want one, don’t buy it.== Doesn't this act as a type of restriction on housing supply thus increasing the upward pressure on housing costs by limiting options? Not to mention the actual costs of funding the HOA, which directly increases housing prices. reply _heimdall 5 hours agorootparentprev> 1. Most homes aren’t in an HOA, so you have plenty of options if you don’t like it. That'd be an interesting stat to see, I assume it varies greatly by location. > 2. It prevents all sorts of tragedies of the commons. How so? Collective power can make tragedy of the commons problems worse. While a single family likely wouldn't buy up an adjacent piece of woods to clear for a neighborhood park, an HOA may be able to do that before someone else buys and develops the woods. > 3. HOAs would have no power at all if you could opt out. It has to be part of the contract. They can be dissolved entirely, but that’s the only mechanism for getting out. HOAs have whatever power the community gives them, there's nothing saying an HOA couldn't be optional. I actually lived in a neighborhood outside of Seattle with an optional HOA, it worked just fine. > 4. We believe in individual freedom, which includes the ability to chose to trade freedom for certain benefits. I trade my freedom to leave a rusty old boat in my driveway for the benefit of not having my neighbors do that. Most of us don’t have or want the sorts of things that bring down property values and don’t want to suddenly find ourselves living next to people who do. And that's totally reasonable. My point wasn't that there aren't incentives for homeowners to make that choice for themselves, only that it has always felt like an infringement of rights to legally bind a property to HOA governance. > People who complain about HOAs existing are the same as the people who complain about EVs. If you don’t want one, don’t buy it. I'm not sure where you're getting this anecdote, but here I am complaining about HOAs and I own an EV. This analogy really doesn't fit, one person's decision to buy an EV has no impact on other's right to choose what to buy where as an HOA does. reply mattmaroon 4 hours agorootparentI suppose the commons issue depends on how you define the commons. HOAs often have communal areas like a pool. A communal pool with many owners no HOA would quickly become a disaster and probably form something like an HOA really fast. An optional HOA would just be opted out of by the worst offenders. If the whole goal is to keep your neighbors from ruining your property value, the ones who do would just opt out. The guy with the car on blocks can just sell himself his own house to get out of it. How is anything you get into willingly an infringement on your rights? Nobody ever had to buy a house that had to remain inside an HOA. They chose to and can choose not to. That’s what I meant by the EV thing. You’re complaining about a choice other people made willingly. Why not just not make the choice and let others do it? reply _heimdall 49 minutes agorootparentThat's interesting, I'm not actually sure how a community pool would really exist without an entity that's responsible for it (either an HOA, the city/county, or a private club). > How is anything you get into willingly an infringement on your rights? Sure, once you sign the contract you've agreed to it. My point is just that it seems unreasonable for an HOA to effectively claim authority or ownership over a piece of property. I once lived in an HOA neighborhood that was in legal proceedings with a neighbor who refused to play along, the neighborhood was pretty damn close to foreclosing on the property before the owner finally sold and left. The EV example is still very different though. Buyers can choose to but any vehicle they want, there is no higher authority attempting to claim authority over some vehicles and requiring you submit to their rules and pay a membership fee. Buy an EV or don't, its totally up to the buyer. Additionally, when an EV owner wants to sell they aren't limited to the pool of buyers willing to join a private club, you just sell the car. reply NoMoreNicksLeft 4 hours agorootparentprev> Most homes aren’t in an HOA, so you have plenty of options if you don’t like it. If we are talking about suburban/urban homes, some large percentage are in an HOA. Most people who own instead of renting do have to contend with HOAs. There are not plenty of options. In many cases, there are no other options at all. > It prevents all sorts of tragedies of the commons. What sort of \"tragedy of the commons\" does it prevent, exactly? That because I was laid off and had to take a lower-paying, higher-houred job, my house goes unpainted a little too long, and now the paint's peeling and chipping? That because of weather, I had to wait 3 extra weekends to mow the lawn (can't do it when I go home after work, not allowed to do it after 6pm or whatever) and now it's too tall? These aren't tragedies. > HOAs would have no power at all if you could opt out. Exactly. But you never bothered to ask why they should ever have power at all. The people who have these powers are people who should never have power under any conceivable circumstances. If there were a way to somehow discover people who craved to be on HOA boards, I would support a constitutional amendment to rescind their voting rights. > We believe in individual freedom, which includes the ability to chose Spoken by the sort of person who cheers on as the non-HOA choices dwindle to nothing. I have a compromise that just occurred to me. If, for instance, only 1.2% of homes within a given region (perhaps legislative districts) could be included within an HOA, and if the HOAs had to bid on an HOA license, such that they're competing with each other to be included in the HOA... then the true cost of HOA apologism could be factored into the market. If you want to live in one, and if that HOA has to pay for a $12 million annual license (they could easily be bid up this high, and if you're honest with yourself you know I'm right), then who am I to tell you HOAs are bad? You're paying a premium for it and it's restricted to a tiny fraction of all available homes. > If homes were cheap and moving were easy, perhaps HOAs would never have been invented. Not even close to how this works. The people who favor HOAs are emotionally invested in how others manage their own households. They're not the kind of people who are easily chased away. Quite the contrary, they want to chase others away. You know, the wrong kind of people. > People who complain about HOAs existing are the same as the people who complain about EVs. If you don’t want one, don’t buy it. Funny that example, you belong to a political constituency that is doing whatever they can to make it illegal to sell any non-EV. reply freeopinion 5 hours agorootparentprevLet's say I sell you a basketball with some restrictions. For instance, you are never allowed to kick the ball. If you ever kick the ball, some pre-defined penalty is imposed. You might say that I didn't sell the ball. I sold a bundle of rights regarding the ball, but I retained some other bundle of rights. Your gym membership accords you some rights, but not all rights to the gym equipment. Do you think that should be illegal? As long as a contract clearly delineates which rights are included and excluded I don't understand why you would want this to be illegal. You might question why somebody would buy a car that contains a radio but the purchase of the car doesn't include the radio and the new owner is not allowed to manipulate the radio. It may not make sense to you. But that doesn't make the arrangement illegal. reply SpaceNoodled 3 hours agorootparentIllegality and immorality are too often at odds. reply _heimdall 48 minutes agorootparentLaws that the majority view as immoral should just be abolished. We don't do that nearly enough, but that'd be a part of any functional legal system that's actually designed by and for the people. reply hylaride 4 hours agorootparentprevI don't understand it myself, but some people like the forced conformity and the legal requirement to fund and maintain certain amenities, including streets, etc. Maybe it's an (over)reaction to \"to much freedom\" in some sense of the phrase. It's probably also a way to bypass greater government and keep infrastructure funds more hyper-local, which can have social consequences. That being said I live in a condominium, but it's an apartment-style one. It's the only way I can afford to live in the central city. In that case there needs to be some setup to maintain the elevators, overall structure, and other common elements. But if I owned a house with actual land, the only restrictions I'd want are on actual nuisances (mostly continuous noise - a party once in awhile is fine) or safety. reply maicro 5 hours agorootparentprevAs an American with a strong general preference for the metric system, \"eagles per square burger\" is my new favorite sarcastic unit. reply AmericanChopper 9 hours agorootparentprev> So if liberty is defined by randomness, yes, HOA defines the land of the free It’s the freedom to choose an HOA, or alternatively, anything else you like. Also, while HOAs might make decisions stupidly, I don’t think they do it randomly. reply em-bee 8 hours agorootparentthey are doing it to exclude those that don't fit into their style of life. it's discrimination, plain and simple. to compare with europe: as far as i know germany or austria don't have the equivalent of a HOA, but what is allowed and what isn't is defined by local law. the closest example i could find are rules a landlord is allowed to impose on their tenants. for example, if i rent a house with garden, then the landlord can demand that i take care of the garden, but they can not specify how often i am supposed to mow the lawn or how high i let it grow. if not even a landlord can make such rules for their own property, then a HOA would have even less chance to make such rules for homeowners. that said, local laws can be quite far reaching, especially when it comes to aspects that affect the character of a neighborhood which includes rules on the outside design of the house and the front yard. not something i am happy about myself. beyond that these rules are mostly about things directly affect and cause problems for the neighbors. otherwise a homeowner has the right to do whatever they want on their property. as for the freedom to choose an HOA. no such freedom exists. because my choice of where i live is not defined by the HOA but by other considerations like location and cost and most importantly, availability. the argument that i could have moved somewhere else if i didn't like this HOA is not really reasonable given the housing shortage in most places. the right to live somewhere trumps the freedom of association reply crmd 6 hours agorootparentIn the United States it would be discrimination if, for example, the rules were selectively enforced against people of a certain protected class such as race or religion. HOA control is simply a covenant written into the deed of certain properties by the original developer that is then inherited in perpetuity by future deed holders. The deeds of all properties on certain blocks in my neighborhood have a similar covenant saying that your building must be set back 14 feet from the curb in order to create a luxury boulevard-width sidewalk. A homeowner can’t sell me the right to build up to the curb or the right to ignore the HOA because they can’t sell a right they never owned in the first place. Freedom of association is not relevant here. reply em-bee 5 hours agorootparentit is also discrimination if a rule applies to everyone, but only a certain group is actually affected by the rule. for example forbidding to build a ramp would be discriminating against wheelchair users, whereas others don't even need a ramp. reply germanier 7 hours agorootparentprevIn Germany there is the in some - but not all - aspects similar concept of WEG (Wohneigentumsgemeinschaft). reply em-bee 6 hours agorootparentah, right, but those are only in apartment buildings. it's a good place to research though what responsibilities and powers those have. reply AmericanChopper 7 hours agorootparentprev> they are doing it to exclude those that don't fit into their style of life. it's discrimination, plain and simple. Freedom of association is the freedom to associate with whoever you choose and not to associate with anybody you choose not to. You can’t have feeedom of association without the freedom to discriminate against people you don’t want to associate with. > the argument that i could have moved somewhere else if i didn't like this HOA is not really reasonable given the housing shortage in most places. Less than a third of US households are a part of HOAs, with most of them being apartments (where that type of regulation is typical almost everywhere in the world). Your argument that HOAs are monopolising housing supply doesn’t stand up to the facts. reply AlexandrB 6 hours agorootparent> You can’t have feeedom of association without the freedom to discriminate against people you don’t want to associate with. So I should be free to leave an HOA? No wait, I can't without selling my house as well. Restricting the right to free association as a condition for purchasing housing seems onerous. reply AmericanChopper 6 hours agorootparentHow could you end up in an HOA without choosing the be in one? You couldn’t… Freedom of association isn’t a magic ability that means you can never regret entering a contract. reply em-bee 6 hours agorootparenthow? by not being able to find a house that is not governed by an HOA reply nradov 5 hours agorootparentDon't be ridiculous. Every metropolitan area has some homes for sale which aren't part of an HOA. If you don't like the price or location or school district or something then that's your problem, not an issue with the concept of HOAs. reply lupusreal 5 hours agorootparentprevBut you can, so what are you even complaining about? reply em-bee 6 hours agorootparentprev80% of homes on sale today come with an HOA (according to john oliver) reply beeboobaa3 6 hours agorootparentprevnext [5 more] [flagged] mlrtime 6 hours agorootparentAm I the only one that sees the quantity of these low value comments entering in HN? To the parent and top thread op, Please stay on reddit and off HN. reply beeboobaa3 4 hours agorootparentCalling out racists? It's not worth the energy to engage with them beyond that. > You can’t have feeedom of association without the freedom to discriminate against people you don’t want to associate with. Is code for \"no blacks or jews\", especially in the context of HOAs. reply AmericanChopper 3 hours agorootparentIt absolutely is not. Freedom of association means precisely what I have described it to mean. There are legal constraints on how that freedom can be used, as well as ethical considerations in how it should be used (which is the same with every conceivable freedom). If freedom of association didn’t grant you rights to discriminate against people you don’t want to associate with, then you wouldn’t have the liberty of discriminating against the racists you don’t want to associate with (though in this case they exist exclusively in your imagination). reply AmericanChopper 6 hours agorootparentprevI really hope you’re joking. Freedom of association is fundamentally the freedom to discriminate. Either you don’t understand that, or you just don’t like the concept so you’re trying to imply that I use it to discriminate against racial groups. reply gpvos 8 hours agorootparentprevFreedom? You usually have to live somewhere near your work or where other family lives, and most HOAs have very similar cookie-cutter rules with the differences determined by something akin to mob rule. reply db48x 7 hours agorootparentMost housing in the US is not part of an HOA. In fact, around half is apartments, and apartment dwellers are subject to rules made by their landlords. In an HOA (or condo association), at least you can vote on the rules or join the board. reply arghwhat 8 hours agorootparentprevYou do not have the freedom to choose an HOA, only the freedom to move somewhere else. Unless you define \"freedom\" as \"not held captive\", I don't think that really counts. A fun John Oliver video on how corrupt HOA's are: https://www.youtube.com/watch?v=qrizmAo17Os reply AmericanChopper 7 hours agorootparentI think HOAs are dumb, but who had ever been forced to buy a home that’s part of an HOA? reply lacksconfidence 5 hours agorootparentIt's not a force, it's a choice. When we bought our home every home built in the area in the last 40 years had an HOA. The only non-HOA choices are basically taking a giant gamble on how terribly the previous owners updated it to modern standards, instead of buying something built with modern plumbing and electrics. So sure, the HOA was a choice. But it was forced. reply em-bee 5 hours agorootparentprevaccording to the video, that would be 80% of those buying a home today. reply primitivesuave 9 hours agorootparentprevI would add that this statement might equally apply to government - that is, if you disagree with the policies of a local government (property tax too high, school district underperforming, etc), move somewhere else. If you disagree with your state, move to another one. And if you disagree with national policies, immigrate to a country whose policies you agree with. But therein lies the pitfall of freedom of association - if you disagree with your HOA, or your local/state/federal government, you may not have the financial or legal means to change your situation. I see a dichotomy of \"associations with oversight\" and the converse (associations with no oversight). If your HOA forces you to build a fence, there is no appeal process similar to if a government entity charges you with a crime (e.g. appeal to a state/federal circuit court and ultimately to a supreme court). Within each is an enforcement mechanism that may or may not work. In your dinner party example, which is an association with no oversight, a naked table dancer may simply crash the subsequent gatherings. In all levels of government, there are those who continue to hold their position regardless of the misdeeds and accusations levied against them. Perhaps that is the essential trade-off of freedom, to be free to associate with any group of people despite the possibility of being victimized by them (now or in the future). reply lores 8 hours agorootparentNot a dig on you, but I find it telling that in discussions about the freedom of upping up and leaving, money is always mentioned as a limiting factor, but never community. For many, moving means losing the social ties they have, and that can be devastating. I wonder if it's a HN or American oversight. reply alistairSH 7 hours agorootparentDoes HN have a higher than average # of immigrants or first-generation citizens? That could explain it. reply SpaceNoodled 2 hours agorootparentNo, introverts. reply nradov 5 hours agorootparentprevMy grandparents immigrated to the USA and lost all their social ties. They were better off for it. reply randomdata 6 hours agorootparentprevWhat do you mean? Per the discussion, escaping social ties is the whole point of wanting to leave. If you fit into the community, what reason would there be to leave? reply lores 3 hours agorootparentYou can have a community in the neighbourhood who aren't ruling the HOA. reply randomdata 3 hours agorootparentBut then said community will overthrow the HOA, so, again, no reason to leave. The earlier suggestion of leaving was based on a situation where you don't find the community you need. Escaping the social ties was the sole reason for leaving. Why would escaping a community that you don't fit into be the same limiting factor that keeps you there? That doesn't make any sense. reply lores 3 hours agorootparentWhy would you think that is the case? reply randomdata 3 hours agorootparentBecause I do not believe that HOAs are magic. What makes you think that they are? reply CogitoCogito 8 hours agorootparentprevYeah I'm having trouble understanding how /u/db48x's logic doesn't apply equally well to all political systems. Maybe the next time someone complains to me about Biden's policies, I should just respond that they don't understand freedom. reply db48x 7 hours agorootparentThe USA believes so hard in the freedom of association that we are willing to let people leave whenever they want. You don’t need permission, just find somewhere better and go there. Compare that with the Soviet Union, or East Germany, or China, or North Korea. In those countries leaving may technically be allowed, but may require a lifetime of asking for permission. The Soviet Union didn’t even provide its own people with maps lest they find out where the borders were. reply ohmyiv 5 hours agorootparent> Compare that with the Soviet Union, or East Germany, or China, or North Korea Would it be better to compare current, non-totalitarian governments, such as EU, Australia, or the like? It kind of sounds disingenuous to use extreme examples. (No dig to you or your opinion, just don't see how using those examples helps.) reply mlrtime 6 hours agorootparentprevAs an American \"leaving\" you may need to pay your taxes on the way out (giving up citizenship). reply jl6 8 hours agorootparentprevIt's an interesting tradeoff of freedoms when you consider how free a person is to leave a HOA. Purely legally speaking, they are free to leave at any time if the HOA starts to enforce unjust restrictions - but practically speaking, moving house is onerous and many are not in a position to do it. This is essentially a contract where you enter freely, but the other party has the ability to vary the terms unilaterally, and your only recourse is to exit. A tech analogy comes to mind. We freely choose to host our workloads in a Server Owner's Association (sorry, cloud service provider), and we take on the risk that the provider may change the offering and impose restrictions that we don't like. They will tell us that we are free to leave at any time, but switching clouds can be non-trivial to say the least, leaving us feeling trapped and decidedly unfree. The freedom to enter into private contracts is the freedom to bind oneself. reply Lendal 6 hours agorootparentThis is a great analogy. I can think of one area in which it might be different though. In smaller HOAs such as mine, the HOA almost never takes any action of its own volition. It's almost always the result of some neighbor making a complaint forcing the HOA's hand, then the neighbor hides behind whatever action they compel the HOA to make. The HOA becomes the boogeyman and goes on the news, while the actual complainant gets to remain safely anonymous. I've seen corporate firms that run HOAs professionally where they run a much more proactive/activist HOA, but in the case of my neighborhood we didn't like the results so we fired them. It was too much hawkishness about rules and the fees were heading through the roof. The more leisurely neighbor-complaint model seems to work better unfortunately. It's more efficient, even though it's quite a cowardly system if you ask me. It's better than the alternatives such as having anarchy. reply db48x 7 hours agorootparentprevI would say that your first recourse is the ability to read all the financial documents. As a property owner in an HOA you have every right to read the contracts that your HOA has signed, to audit the books, and to otherwise verify that the HOA board is doing their job correctly. Second recourse is to vote in the elections. If you elect petty tyrants, you get petty tyranny. Third is to run for the board yourself to make sure things are being done correctly. And if all that is too tiresome then yea, sell your property and buy a house that isn’t in an HOA. Most houses aren’t. reply pavel_lishin 6 hours agorootparentIsn't this just the \"if you don't like it, leave\" argument? Selling your house and buying a different one that isn't in an HOA isn't as trivial as you make it sound. And in some parts of the country, it's rather difficult to find a house that isn't part of an HOA - in Texas, many neighborhoods get built with an HOA built-in. reply FeloniousHam 6 hours agorootparentThere's a disconnect in these comments that suggest a homeowner is forced into an HOA; for the vast (vast!) majority of cases, the HOA (and it's publicly available rules) are pre-existing to the home purchase. The rules can be annoying, but the primary purpose is the preserve the sale value of the properties. I'm ambivalent about an HOA, but my father's neighbors probably wished they had one when they were selling their house (he had two broken cars and one broke down lawn mower in the driveway for years). reply pavel_lishin 5 hours agorootparentWell, that's true - nobody is forced to buy a house. (Though, sometimes, they inherit one that's hard to sell due to an HOA.) But on the other hand, housing supply is limited. Sometimes your choices are \"a decent house close to a decent school with an HOA\" or \"I guess we'll move seven hours away from everyone we've ever known\" or \"The neighbor's meth lab probably won't detonate this year, right?\" reply alistairSH 7 hours agorootparentprevMost houses arent I’m sure this is true nationally, but does it hold locally for areas with good employment prospects? And once you remove apartments (which don’t have HOAs but are subject to the tyranny of the landlord instead)? Genuinely asking because I can’t think of too many areas in NoVA that don’t have HOAs. Correction… There are areas of older housing without, but any subdivisions built since the 80s seems to have one. The areas without tend to be older “downtown” areas (Arlington - very expensive) or on the outskirts (Fairfax Station - very expensive). reply gizmo686 5 hours agorootparentprevHOAs are not a story of free association. They are a story of privatization. We used to have local government deal with local governance. But local governments decided they didn't want to do that anymore, so they started encouraging the creation of HOAs to deal with all of those pesky local governance issues. If all thar mattered was free association, the fix would be very simple. Enshrine into the the right to leave an HOA. But states won't do that, because once people leave, they will except their local governments to start providing the same services the HOAs do, and all of those local governments are going to complain to the state government. reply corysama 4 hours agorootparentFelt this when I bought my first house and a week after the sale finalized, my situation changed dramatically and unexpectedly. I needed to move across the country, but the HOA had a strict No Renters clause. So, I had to choose between 1) Spend 30 years paying the mortgage for a house no one can use. 2) Turn around and sell the house I just bought. reply em-bee 3 hours agorootparentthat's why i don't like to buy property. presumably you didn't want to just sell again because of the effort involved. if buying and selling was easier and guaranteed to never sell at a loss, then this would not be an issue. how did you resolve the problem? reply corysama 1 hour agorootparentI ate it. Turned around and sold at a small loss on price, bigger loss on all the costs associated with the process. reply em-bee 3 hours agorootparentprevthis is really the crux of the issue. HOAs should really be treated and organized like neighborhood level government councils. reply tuyiown 5 hours agorootparentprev> People who end up in a petty HOA usually regret it, and come to see joining any HOA as a big mistake. I'm pretty sure that freedom of association is strongly tied to being able to quit the association freely. I don't think that the condition of having to change home to quit an association is a reasonable term. reply insane_dreamer 1 hour agorootparentprevThe problem with HOA vs other types of voluntary associations is that it’s very difficult to get out of an HOA — you have literally one recourse: sell and move. That’s a huge barrier because even if you wanted to you may not be able to afford to sell and move. And HOA rules change so the terms you now object to were perhaps not even in place when you agreed to join the HOA. It becomes coercive. When my wife and I last moved to another city and were looking to buy, our first requirement was that our house not be encumbered with an HOA. reply kennethwolters 9 hours agorootparentprevThe more a person is dependent on \"associated or local governments\" [1] relative to \"national or supernational governments\", the more freedom they enjoy. This is somewhat counter-intuitive because one might think that if e.g. every resident of a nation had to obey a singular and simple set of national rules, that would promote freedom more than having governments and rules differ between regions. I wont go deeply into the underlying mechanisms of why the more chaotic choice promotes freedom while the more orderly choice reduces freedom but it has something to do with organisations competing for members and members being willing/free to move between those organisations. [1] HOAs, school districts, unions, chapters, guilds, bars, insurances reply em-bee 6 hours agorootparentbut there is no competition among HOAs. there isn't enough freedom to move around. there isn't even a chaotic choice because most HOAs are the same. and when 80% of homes sold today are governed by an HOA there is also no choice not to join one. and i am not buying the whole argument. if we accept a hierarchy of governing structures, from the HOA at the bottom, and let's go all the way, the united nations at the top, each level has a a certain set of responsibilities and powers. the difference in the levels is determined by how centralized or distributed those are. but there is no escaping the levels. the problem is that it is unclear who is responsible to deal with overreach. in the US HOAs are largely unregulated, and so there is no protection against overreach. and in particular, they are not government but private organizations. i am of the opinion that mandatory membership in an organization makes that organization an extension of government, and thus it should be bound by the rules local governments must follow. reply kennethwolters 4 hours agorootparentI anticipated this type of reply that states that for HOAs there is not enough freedom of movement and diversity of \"HOA flavours\" to reap the benefits of this localist/federalist system. And you are right. The situation seems dire and the question is how do we solve this problem? Do we do so by smashing HOAs (and giving the responsibilities to someone else)? I believe this is the wrong approach because it will destroy any potential for this localist (and thus inherently good, imo) system to heal. You're basically passing the baton onto another entity that might do a better job off the start but will inevitably fail because it is even less capable of evolving. I'd rather try to discourage rigidity/complacency and incentivise innovation in the HOA-space which is much more complex to implement but better than smashing HOAs. Regarding your point about \"not buying the whole argument\": I think it would be foolish to organise a military or pandemic-prevention-force on a municipal level. The levels need to exist and some higher levels have responsibilities that the lower levels don't have. I absolutely believe that this is good. In conclusion, somehow communities of a certain region need to be organised if they want to be organised, especially when the region is densely populated. I'd rather organise these communities through self-emergent associations than top-down mandated government. And yes, the self-emergent association-type HOA is broken. I'd rather fix it than replace it with a inherently worse alternative. reply em-bee 4 hours agorootparentsorry for not expressing myself clearly. by not buying the argument i was referring to the idea that we can be more or less dependent on levels of government. i believe we are always 100% dependent on all levels. but in some ways we are always less dependent on local levels of government because we can move. but i have the impression we are applying different definitions to terms here, especially to what it means to be dependent. for the most part i actually agree with you. i too favor a localist/federalist approach. and where i said you can't escape the levels, i might have added: nor should you want to. because as you say, the levels need to exist. one of the problems i see in the US is that while the system is doing relatively well at the federalist approach, not enough is done to protect the weaker individuals from those with power over them. especially at the local level. so while i agree that experimentation is good, certain basic rights need to be safeguarded. another thing is fixing vs replacing HOAs. again that comes down to what fixing or replacing mean. it is important to me that organizations that govern my life in a community are actually considered government. an HOA currently only involves homeowners, but not those who rent there. that's the first thing i would fix. so HOAs become neighborhood councils. and beyond that it is just a matter of levels. so if you mean that replacing local HOAs with a community council covering a larger area is a bad idea, then i agree with you. i mean to replace it with a different democratic structure at the same level, with largely the same responsibilities but with more clear restrictions to protect individual rights and freedoms. reply eru 8 hours agorootparentprevI'm OK with national governments, if the nation in question is a city state. All hail Singapore! reply anhner 7 hours agorootparentprevThere is no freedom of association if there is no freedom FROM association. That is, if your only option is to buy a house within a HOA or not but that house, you're not really free... reply mihaic 5 hours agorootparentprevYour position seems a bit ideological. Ultimate freedom doesn't exist, since pretty much all societies agree that they want to stop people from murdering one another for instance. Any legal system always has to balance individual liberties and the tyranny of the majority (or a law-giving minority). > An HOA that was useful for 50 years and never created any drama could turn bad next year when they elect some new board members. That's why countries for instance have constitutions, so the system is hard to change even when someone new takes power. > true freedom includes the freedom to make mistakes, even big mistakes. That's pretty much the reason why we don't have \"true freedom\" and we want to stop people from making really terrible and easily avoidable mistakes, like eating food that kills them. Limiting HOA against petty bullshit requirements (with more freedom for an initial set of bullshit requirements) at least is one of these limitations. reply jameshart 5 hours agorootparentprevThis is a good perspective, and a great case study in understanding how freedoms are always in tension. The usual remedy to protect people from freely making the mistake of entering into associations that harm them is regulatory, of course. Employment regulations place certain rules on what you can expect when you choose to throw yourself into association with a company as an employee, to prevent you accidentally finding yourself at the whim of an abusive employer. Regulating what kinds of things HOAs can demand is a perfectly reasonable thing for governments to do to make it safer for people to purchase houses with confidence they won’t be subjected to egregious restrictions. So we do end up limiting freedom of association. You can’t join up into an HOA with your neighbors that bans any of you from selling your house to a black family. reply UniverseHacker 4 hours agorootparentprevUnfortunately, in practice most people don’t choose HOAs, they simply can’t afford to live anywhere without one. reply _the_inflator 9 hours agorootparentprevInteresting details and case analysis from your side here. HOAs are subject to debate frequently. Philosophically, freedom has, among other dimensions, the dimension of negative and positive freedom: being free from something and free to do something. Freedom usually refers to individual rights against the state. Freedom to have private contracts is part of what distinguishes socialism from capitalism. What you describe regarding group associations is correct. Others are correct, too, when they appeal to freedom as the right to be spared from something. Freedom to form groups does not mean being forced to join them. That's the point here. HOAs are partly like cartels, in my view. reply Calavar 10 hours agorootparentprevFreedom of association is missing the point when it comes to HOAs. There's some very significant differences between freedom to associate in a book club and freedom to associate in an HOA. If you don't like a book club, there is no scenario in which you are contractually obligated to join it and pay membership fees indefinitely. And the bookclub doesn't have broad legal authority, backed by the courts, to dictate how you maintain, use, and sell your personal property or even aspects of your lifestyle. reply dullcrisp 9 hours agorootparentBook clubs aside, the ability to join a HOA is a freedom. Contrarily, if the Supreme Court forces New York City to allow guns in its Walmarts, that would be a restriction of freedom. Freedom isn’t the same as lawlessness. reply hiatus 8 hours agorootparentBusinesses can't discriminate against different races either, you give up some freedoms when you hold yourself open to the public. reply dullcrisp 8 hours agorootparentYou’re right, disallowing race-based discrimination is also a restriction of freedom. I’m not arguing that all freedoms are good. reply 15155 9 hours agorootparentprevNYC banning guns in Walmart is a restriction of freedom, per Bruen. reply dullcrisp 9 hours agorootparentThe SCOTUS doesn’t define the concept of freedom. reply gpvos 8 hours agorootparentprevIt would be a freedom if you could own a house in that area and not join the HOA. reply madeofpalk 7 hours agorootparentprevIs there a freedom to not join a HOA? Freedom to not associate? reply db48x 7 hours agorootparentOf course there is; just don't buy a house that is part of an HOA. Most houses in the US are not part of any HOA. Apartments are also not part of HOAs (but of course anyone living there has to follow the rules put down by the owners, and it’s not like the residents get to vote on those). Condos are always part of a condo association, but those are a fraction of the housing in the US. Also, if your neighbor asks you to vote to create an HOA, vote against it. reply beaeglebeachedd 7 hours agorootparentCovenants are separate from the deed and title searches fail to uncover them all the time. That's part of the problem, you buy a deed and some boomer crawls out from 6 feet under to unilaterally impose his will. It's infuriating as this crept up constantly when I was looking for land to build a tiny house and many sellers didn't even know some dead boomer made it illegal to not build a mcmansion in some ancient document the title clerk digs up. That some dead previous owner can unilaterally say 'none of that' and it be binding on subsequent transfers and heirs is opposite the freedom of association. reply maxerickson 7 hours agorootparentprevHOAs are often tied to a piece of land, with it being mandatory to join if you buy the land. You are free to not buy the land of course. reply throw0101c 7 hours agorootparent> HOAs are often tied to a piece of land, with it being mandatory to join if you buy the land. Legally speaking one is often not buying the land but rather buying the unit. The land on which the unit sits is owned by a corporation (HOA), and as a condition of the lease you must abide by the corporation's rules. * https://en.wikipedia.org/wiki/Leasehold_estate If you don't want to live by an HOA's rules then don't live on the HOA's land, but rather your own land: * https://en.wikipedia.org/wiki/Freehold_(law) reply db48x 10 hours agorootparentprevI never said that all groups were equivalent, only that they are similar. Book clubs have rules, and if you violate them they will punish you. Very likely the punishment will be limited to not inviting you back, but that is because book clubs are very informal. HOAs are formal, and are formed for the specific purpose of regulating the use of property, particularly common property. Thus it should be no surprise that they do so. reply kashunstva 7 hours agorootparent> regulating the use of property, particularly common property I don’t think anyone would argue with their regulation of common property; it’s their petty interference in private property that’s the rub. reply db48x 7 hours agorootparentRemember that everyone disagrees about what is petty and what is not. Take the current article: some would argue that requiring a vehicle (such as the boat) to be parked in a place where it cannot be seen from the street is petty, and that the HOA is petty for having such a rule. Yet many many cities and towns across America have similar parking rules. They were put in place by a reasonably democratic process, so it must be that there is a fair amount of disagreement over whether they are petty or not. You have the same recourse in both cases: vote, join the local government, or move somewhere less petty. reply shadowgovt 6 hours agorootparentprevBut when those groups are tied to real estate ownership they aren't free associations, they are serfdoms. reply ein0p 4 hours agorootparentprevYeah, mortgage debt slaves are forced to associate among themselves and pay money to the most insufferable local busybodies available. That’s the “freedom” we’re talking about here. I’m generally against big govt, but in this case I would gladly pay the same amount to the municipal government instead so that they’d maintain the road and public greenery, and otherwise get out of my hair. reply 2OEH8eoCRo0 6 hours agorootparentprevI agree. You can be free without having total anarchy. Some (but not too many) laws/rules/restrictions ironically make us more free. reply xdennis 4 hours agorootparentprev> Someone always scoffs, but I think it is usually a sign that they don’t really understand freedom. Others understand freedom from the perspective of the individual. Americans understand freedom from the perspective of the state. In the first case, you're free if you can practically do something. In the second case you're free if the government doesn't ban it, regardless of whether you can actually do it. Take for example the issue of ads on Smart TVs. There's no law that says you have to have ads on them, but in practice it's impossible to buy a TV which doesn't flood you with ads. People in the first case consider that if a law is passed to ban embedding ads on TVs, then that increases the freedom in the world. Americans consider that banning such ads is a horrible government overreach which hurts the feelings of poor billionaires, and that if people want, they can always buy TVs with no ads, even though no such TVs exist. They prefer an imaginary freedom to a practical one. reply okasaki 9 hours agorootparentprevThis is some extreme mental gymnastics where restrictions are actually freedoms. By the same logic, how about a freedom for the government to limit your speech? Now censorship is freedom and free speech is a restriction. reply eru 8 hours agorootparentThe ability to do censorship would be a freedom for the government, yes. It's also a restriction on you. I don't like HOAs, but the ability to form or join a HOA is definitely a part of individual freedom. Just like the ability to eg enter into a non-disclosure agreement or any other kind of contract. reply mensetmanusman 7 hours agorootparentprevTraffic signs are limits on freedom, but they increase the net freedom to travel. Some things have a higher second derivative of freedom. reply unsupp0rted 10 hours agoparentprevHOAs are a questionable entity in high-trust societies. In low-trust societies you wish for an HOA and would gladly pay 10x your property tax to be in one that has rules and enforcement teeth. Even something as simple as forcing the guy who keeps smoking and talking on speakerphone next to your window from 4pm to 1am to stop doing that. reply ikekkdcjkfke 9 hours agorootparentBut leaf blowers 7 am are fine xD reply db48x 7 hours agorootparent7am is well after sunrise in most places, most of the year. Your personal tolerance for sounds at that hour should not be regarded as universal. reply prmoustache 5 hours agorootparentLeaf blowers are noise pollution regardless of the time of the day, which has short and long term effects on health. reply mindslight 5 hours agorootparentprevAnd electric lighting exists. If you're going to define when noise is acceptable based on when any significant contingent of people are up and active, then noise at 1AM (3AM on weekends) also falls under that definition. Yet the early riser coalition tends to complain about that... And never mind that the main value add of leaf blowers (over the traditional rake) is to coat your neighbors with debris and dust, thus necessitating even more cleaning (inside too, if they dared to have their windows open). Also I'd say it's your characterization of freedom that is a narrow misunderstanding. I'd define freedom as the ability for individuals to make their own choices in the presence of society. Being the only person for miles and thus being \"free\" to do whatever you want isn't some sensible ideal of freedom, rather it's a pathological case that has made freedom irrelevant. In other words, freedom itself directly depends on tolerance. And by the time a city is forcing someone to build yet another ticky-tack fence to hide the mere sight of a perhaps-ill-advised boat it's clear that there is very little tolerance aka respect for freedom. reply yourapostasy 6 hours agorootparentprevA lot of my friends who in their younger years held this position rapidly backpedaled after they added babies to their homes. There are very effective sound mitigation solutions available though, if you're willing and able to afford to remodel the room(s) you want to defend against such external sounds. I interpret such actions as what to normally expect in a lower trust society and culture. We pay for such settings in many different ways. reply forgetfreeman 10 hours agorootparentprevNope. You'd be hard pressed to construct a narrative short of failed-state-need-24-hour-armed-security-to-survive where I'd entertain the notion of allowing 3rd parties any kind of say over what I can and can't do on my property. reply db48x 10 hours agorootparentYou say that, but I bet you already live in a village, town, or city that has quite a lot of rules about what you can and cannot do. Noise ordinances, building codes, safety rules, fire codes, historical preservation, it goes on and on and on. reply beaeglebeachedd 7 hours agorootparentNo I don't, I purposely bought a property with no code inspections and no covenants and there's been no code or fire check or noise ordinance for 20+ years. None of the scare stories you hear on here came true, but what did come true is you can actual afford a home because you're not the subject of constant harassment and corruption of code enforcers when building. reply db48x 7 hours agorootparentYou are in a minority. The huge salient fact about the last 200 years is the continual movement of the population from rural areas into urban areas. reply forgetfreeman 10 hours agorootparentprevThere's quite a bit of difference between building codes and empowering my neighbors to bitch about the color of my mailbox... reply andybak 10 hours agorootparentYes but I bet everyone will disagree about exactly where to draw the line between the two types of rule. It's the middle where it gets fractious. Question - Have you ever lived in a high-density environment? apartment block, terrace row etc.? Someone who has will have a different stance on what is acceptable regulation to someone that's only lived in the density of typical US suburbia. reply jeltz 9 hours agorootparentNot exactly comparable but as someone who has lived all my life in apartments in Sweden have to say that people who grew up Swedish suburbia and moved into the city seems to be the most into deciding what others are allowed to do compared to people who have lived a long time in dense environments. Could just be that are used to different rules but I would not say we have more rules in the cities. reply forgetfreeman 6 hours agorootparentprevI've lived in trailer parks, downtown apartments, mid-town housing projects, suburban neighborhoods, hyper rural farming communities, and my car on a few occasions. I don't fuck with my neighbors and won't tolerate being fucked with, regardless of setting. reply andybak 4 hours agorootparent> won't tolerate being fucked with, OK. So you have your own rules, which you enforce on others? That's swell but I can imagine it could lead to problems. Maybe some form of committee where people can settle disputes and agree on standards? reply forgetfreeman 43 minutes agorootparentIt's like the concept of common decency is mysterious to you people. Rules? Sure, don't be a complete gaping asshole. Short of that I can generally figure out a way to get along with folks. Do you need a committee to instruct you on how to act? reply andybak 6 minutes agorootparentNot sure who \"you people\" is referring to but if you're speaking to me then my response would be something along the lines of \"I admire your optimistic outlook but even a cursory study of society would show that social consensus is incredibly fragile and to regard it as anything else smacks of naivety\". So - as a society we've tried multiple different strategies to make it easy for people to live in close proximity and they all have different success rates and different trade-offs. Pick your poison. beaeglebeachedd 4 hours agorootparentprevThe peoples' glorious committee have never been a good way to protect property rights. Which is part of the reason why many rights are set up to not be subject to democratic opinion. reply meetingthrower 10 hours agorootparentprevDo you live in the country in a town with no zoning? You might be surprised... reply edgyquant 5 hours agorootparentI lived in a pretty small town growing up and we were forced to sell the car I was going to get at 16 because I was 15 and it had been sitting for months. The city gave us a warning and was going to start fining us everyday until we got rid of it reply meowster 4 hours agorootparentWhat was the warning for? Not being registered, not moving, etc? Those are things that your parents can fix without you being 16. reply rafaelmn 10 hours agorootparentprevUmm sorry but that's nonsense - externalities of your property impacts your neighborhood and having rules upfront about what's acceptable is a good way to deal with conflicts and setting expectations. Laws have to be defined for the entire country. If I buy a house in a nice neighborhood and I pay extra because it's a nice neighborhood - I want to know that everyone has some standard of maintenance. reply forgetfreeman 10 hours agorootparentTo put it as bluntly as possible I don't give a flying fuck at a rolling donut what you or others likeminded think is nonsense here. I live on 20 wooded acres surrounded by undeveloped forest lots ranging in size from 40 to 200 acres, the closest neighbor is half a mile away, and I couldn't pick anyone that lives there out of a police lineup if my life depended on it. I chose my location precidely to avoid this creeping horseshit notion that you've got rights over the things I own. reply fastball 9 hours agorootparentSo you wouldn't mind if someone bought the property closest to wherever your house is on your 20 acres, cut down the undeveloped forest, and built an abattoir right on the property line? reply ekidd 6 hours agorootparentHOAs are not the only thing that controls where people can build abattoirs. In much of the US, this is controlled by zoning. And zoning is decided on via local politics, not via a HOA. I'll take a small town politics over an HOA any day, because the few local HOAs are far more likely to make weird and obnoxious requests. I live on a dirt road in the woods. I don't care whether my next-door neighbor cuts his lawn or what color he paints his house, or whether he decides to start keeping goats. The landowner across the street is running a tiny part-time farm, and I'm a fan. I know most of my 20 closest neighbors. I don't mind the occasional HOA for people who want that sort of heavy community control. For me, it was an automatic \"no\" when buying a house. reply fastball 5 hours agorootparentHOAs are local politics. You're making a distinction without a difference. Some local politics sucks, some does its job with minimal fuss. The difference is not made by the label you put on the political entity. reply ekidd 3 hours agorootparentSmall town politics is based on voting and laws. I can participate in Town Meeting, or call the town manager, or volunteer for a role in town government. And unless I make a spectacular nuisance of myself (or ruin the groundwater or dirt roads), the town doesn't much care what I do with my land.[1] HOAs are basically private pseudo-governments, and most of the ones around here seem to impose far more stringent rules about how people use their land than the towns do. [1] If I were operating on a scale beyond an individual homeowner or a small farm, the town would pay more attention. Want to keep a dozen goats or turn 5 acres into a truck farm? Go for it. Want to raise 1,500 pigs with a manure lagoon? You'll need to go through a permitting process. reply yourapostasy 6 hours agorootparentprevThe general rule of thumb I've personally observed (YMMV) with those who hold such absolutist externality related positions is they hold it right up to the minute that someone else much more powerful and deep-pocketed than they start unilaterally imposing externalities upon them. Your abbatoir example is a good one, and the other one I've seen in real life is when a Swift, Cargill or other similar mega-agribusiness setting up a CAFO feedlot next door. The odor externality from the manure piles/lakes usually riles such characters right up, but that is the least of it. Even when the manure management challenges are properly mitigated (and there are no such CAFO feedlots that can mitigate to only 2-3 incidents per year, it is more like 20-30 per year for the average one), neighboring properties like your parent commentator's are adversely impacted with sharply increased pathogen and contaminants control in their groundwater. As rural properties like your parent poster's usually rely upon well water, this becomes a pretty acute issue for neighbors. Fracking operations also are frequently very sloppy with their adverse impact upon groundwater, even though they are very diligent upon staying on their property. Redress that makes whole including judicial system interaction expenses is difficult, expensive even for single-digit liquid assets millionaires, and rare. At the end of the day, this has many characteristics of a classic engineering modeling problem, subject to unperceived historical biases. People who hold absolutist property rights interpretations like your parent poster are generally historically biased to a time period when such interpretations were couched in an energy-poor and population-sparse time of our civilization. Back then, as long as one was within approximately human- and animal-power scales of energy over the generational time span, and rural population densities, externalities from large scaling scenarios were rare for most people, and mostly theoretical. With our current access to energy and ability to rapidly densify neighboring properties, the available externalities to present materially change, and the interpretations (which are actually the mental models we work with) break down in the face of a engineering-like scaling step change that requires categorically different solution spaces. I'm personally very sympathetic to your parent poster's position, but realistically, my personal experience has informed me of the dynamic I've shared, and I've since substantially modified my position. reply forgetfreeman 5 hours agorootparentprevThey couldn't if they tried, zoning laws, local labor costs, and the hyper-consolidation of the meat packing industry make that a complete non-starter. You're trying to goad me into admitting obnoxious neighbors are obnoxious which is self-evident. Generally there are ways to deal with obnoxious neighbors that don't require entitling the rest of the neighborhood to make decisions about one's property. Ironically I've taken the starch our of more than one asshat neighbor by threatening to build a hog pen next to their house, so there is that I suppose. reply fastball 5 hours agorootparentWhat exactly do you think zoning laws are if not the wider community making decisions about one's property? reply forgetfreeman 32 minutes agorootparentZoning laws are a combination of unelected civil servants, elected legislators, and the legal system establishing a minimum set of guardrails to prevent shit like building strip clubs and toxic waste incinerators in inappropriate places. They're far from foolproof, are frequently gimmicked to the benefit of wealthy political donors, and they're not something the public gets any kind of real say in. There's no relationship between zoning laws and HOA covenants around what color your mailbox can be or how many cars you can have in your driveway (for example). Some kind of minimal social contract is provably necessary to prevent society from devolving into some kind of libertarian Mad Max you-keep-what-you-kill hellscape, but that hardly requires granting the people around me authority to make arbitrary decisions around trivial aspects of how I choose to maintain my property. reply em-bee 4 hours agorootparentprevseems someone actually did that: https://news.ycombinator.com/item?id=40255509 reply forgetfreeman 27 minutes agorootparentOh sure yeah. That's actually a very old strategy I learned from my (now 94) year old grandfather. My impression is that was one of the intermediate steps on the continuum between trying to sit down with a neighbor and work things out and exchanging sporadic small arms fire. Country life was wild back in the day. reply rboyd 10 hours agorootparentprevYou described nearly exactly my little place here in the countryside too. I noticed property taxes were up this year, and the electric co-op cut down two of my favorite trees while I was on vacation because they said they were too close to the lines. There’s always something. sigh reply forgetfreeman 6 hours agorootparentThe power co-op cleared three problem trees after I called them out which opened things up enough that I've been able to drop a bunch more myself with no issues. Got an Alaska mill on the way to process the trunks into lumber, the greenhouse is going in this fall. My only real complaint is the bizarre amount of mice we have on the property, but that's a solvable problem. reply demondemidi 8 hours agorootparentprevFunny I lived on 11 acres carved from a 180 acre plot. The 90 year old owner of the surrounding plot died and his children had it logged. For three years loggers were running chainsaws near daily and 18 wheelers were tearing up my small road as they built new ones to feed it. The road was shared from my house to his through my property. Then his teenaged grandkids started running motorcycles through the clear cut and shooting guns. Sitting on my porch used to be pleasant but now it was constant noise. Since the land was unmaintained and unincorporated I had no recourse. I moved before they finished. I check google maps every few years and they are STILL logging. I forgot to mention that the lack of forest now means I can see floodlights from the local high school sports fields 20 miles away and can hear the freeway traffic that passes through the nearby town, also 20+ miles away. It’s not deafening just annoying and persistent instead of wind blowing through thick forest. reply nemo44x 7 hours agorootparentprevI think you did well to isolate yourself. But many people are willing to come to agreement on what type of community they want to live in and explicitly regulate antisocial behavior. It’s great this country is large enough to accommodate all types. reply 15155 9 hours agorootparentprevYou'll be paying taxes on that land, do you actually own it? reply beaeglebeachedd 7 hours agorootparentNot necessarily. Much of Alaska has no property tax, plus you get a paycheck on oil dividends. reply jasondigitized 7 hours agorootparentprevIt’s because you don’t give a flying fuck why HOAs exist. It separates those who are of different likeminds. There is a neighborhood in Austin called Apache Shores for people like you but who do give a flying fuck to know their neighbors. It’s best described as a series of junkyards next to zero scapes next to 70s A-frames next to medieval manor homes. It’s awesome for people who don’t care if their neighbors house looks like a donut shop. I don’t give a flying fuck that you don’t give a flying fuck about your neighbhors. I want neighbors that care about me and ask if they can have a stick of butter or borrow my table saw or carpool my kids to school. My neighborhood is a true community. We help each other. We organize with each other. We raise kids with each other. All from our huge McMansions with perfect lawns adorned with labradoodles who don’t bark at strangers. It makes my life better and I have never once gotten worked up about some bullshit notion that the HOA is treading on me. It’s possible to like conformity while eating suburban Chipotle while reading Kant. You can be a free thinker while also agreeing to not park your car on your lawn. If you want a pink mailbox and want to use your chainsaw in your birthday suit at 6am go right ahead, in any thousands of neighborhoods or hermitages that allow it in the land where the ratio of eagles to hamburgers is nearly equal. I’ll be in my garage with my neighbor smoking weed with our shirts off, making sawdust, listening to Steely Dan. Oh the tyranny reply mlrtime 6 hours agorootparentNot sarcasm but this sounds amazing, show me where to sign on the HOA docs. reply beaeglebeachedd 7 hours agorootparentprevHoa are the mushroom head of covenants, which I see as one of the greatest threats to liberty. Boomers unilaterally reaching out the grave to encumber property on all transfers and heirs. The contract of course is unsigned by the counterparty yet somehow runs with the land. Due to this nonsense it was almost impossible finding land allowing me to build a tiny house for my family despite no zoning or codes against it. reply phil21 6 hours agorootparentYep, this is my problem with HoAs. Everyone supporting them loves to bleat about how \"voluntary\" they are - but any new construction since perhaps the late 80's in my area have been nearly 100%, if not 100%, HoA. This is not voluntary - it's pay to play, and you are practically forced into these living arrangements if you are within a certain socioeconomic class and want to own a home. That I won the lottery and can \"opt out\" of the situation personally is immaterial. I find the situation at best unethical. These don't even exist for the standard reasoning any longer. It's turned into a financial grift for developers to avoid paying for improvements, and municipalities to expand the tax base without having to pay for infrastructure. Plenty of \"communities\" out there currently having their cake and eating it too - at least for the first one or two generations of owners. It's about as sustainable as you'd expect such arrangements to be the more you look into them. If you want suburban new home construction in my area you have had to buy within a HoA for the past 30 years. It's getting to the point where only the older \"city core\" and inner ring suburbs come devoid of a HoA. So it's either exceedingly expensive dense urban living or a 2 hour drive from a rural property. Such \"choice\" indeed. HoAs kill freedom more than they offer it from my experience. They also slowly suffocate any future development forcing everyone into exceedingly hard to break covenants of the past. The ossification and hollowing out of middle class America continues. reply rpdillon 5 hours agoparentprevYou mentioned HOA-type stories, and seem to have spawned an enormous discussion about HOAs. Just want to point out that this has nothing to do with an HOA. Second paragraph: > When the town of Seaside, California ordered homeowner Etienne Constable to build a fence to conceal the boat in his driveway, he erected the fence all right. And then he hired his neighbor, artist Hanif Panni, to paint a mural on the fence—realistically depicting the boat itself. So all the discussion about contracts entered to into freely with respect to HOAs simply don't apply here: this is the government imposing the requirement to build a fence. reply thbb123 10 hours agoparentprevNot American here, so would welcome some explanation. If I own a piece of land on which my house sits. Even if it's within a district that has a strong HOA, what prevents me from leaving the association and set my own rules over my patch of land? Surely in the individualistic land of the free, it should be a fundamental right? reply abestic9 9 hours agorootparentIn short: land covenants. In HOAs you will see titles containing terms about the property being subject to restrictions voted for/applied by the association. If you continually decline to adhere to the restrictions, you can be legally compelled to sell and move on. Note: I'm using phrasing local to me, I expect there may be minor differences but the same gist. reply 19870213 9 hours agorootparentprevBut to buy that home/land from the previous owner you have to sign a contract stating you can't leave the HOA and that when you sell you only sell to those who will join the HOA and contract. This goes back to the construction company/realtor investing to build the entire suburb and putting down the roads and such. reply bdowling 8 hours agorootparent> you have to sign a contract You have the right idea, but there isn’t a contract that a buyer has to sign. A restrictive covenant is attached to a property when a developer (e.g., of a condo complex or subdivision) deeds a subject property to an initial buyer. The restrictions “run with the land” and are enforceable against any subsequent buyer. (The restrictions are a public record kept with the county recorder, so any buyer is on “record notice” whether they actually knew about them or not.) Also, the way most HOAs work, there is no joining just as there is no opting out. You are a member if you own a subject property. That’s it. I mainly know about California, but it should be similar in other states. reply raverbashing 9 hours agorootparentprevI wonder how many times this way of doing contracts was tested in the Supreme Court reply dilyevsky 9 hours agorootparentprevSame thing that prevents you from declaring sovereign state on “your” land - the laws =) reply edgyquant 5 hours agoparentprevYou are not allowed to bring a gun to school. What a ridiculous statement reply gklitz 1 hour agoparentprevPeople buy in places like this because “the neighborhood looks nice” and then immediately complain about not being able to break all the rules that led to them having that opinion in the first place. reply Simon_ORourke 11 hours agoparentprevWhat would be the repercussions of not paying any fines and bringing a case against the city for over-reach on private property? reply sethammons 11 hours agorootparentIn NC, the HOA can force foreclosure on unpaid fees, even if the home is paid off. And 80% of available houses are in an HOA. They have us. Anti-HOA legislation is needed, but the legislature is greased by their lobbying. reply wdh505 34 minutes agorootparentI understood that cities just like free property tax (no cost of maintaining infrastructure) by making hoa zones reply db48x 10 hours agorootparentprevI could be wrong, but I am pretty sure that statistics like that only apply to newly–built housing. Older houses are far less likely to be in an HOA, because HOAs were simply not popular when they were built. reply newswasboring 11 hours agorootparentprevI may be ignorant on this, but how can HOAs have lobbyists? They are made up of local people right? Or am I behind on this and there is already a HOA industry? reply michaelt 10 hours agorootparent> how can HOAs have lobbyists? They don't. For all the power an HOA might have in their street or apartment block, outside of that very small boundary they're nothing. We're not talking about Boeing or Morgan Stanley or PG&E here. HOAs are actually popular by choice - developers wouldn't set them up for \"80% of available houses\" if the houses were more valuable without them. 2% of people have horror story experiences with corrupt HOAs pocketing their cash. 23% of people just get poor value for money where some asshole changes a lightbulb and charges 100 residents $1 each. 75% of people have a tolerable experience, and quite like the fact their neighbour can't park an RV on their driveway and rent it out on airbnb, and if that means limiting what people are allowed to do on their own driveway, so be it. reply hahajk 8 hours agorootparentCities also like them (alot) since they assume costs that cities usually pay for. Neighborhood parks are usually maintained by the HOAs. Less commonly sidewalks, roads, and water lines are maintained by the HOA. reply forgetfreeman 10 hours agorootparentprevOh they absolutely do. https://www.caionline.org/pages/default.aspx reply shiroiushi 11 hours agorootparentprevI'm not sure about this (someone please feel free to chime in), but HOAs are only \"local people\" after the developers have sold all the houses and moved on, at which point the HOA becomes controlled by the homeowners in the association with the most Nazi-like mentalities. Before this, the HOA is set up by the developer that bought all the land and developed the subdivision. So, presumably, the \"HOA lobbyists\" are actually the lobbyists for housing developers, which I think are definitely a real thing. reply themoonisachees 11 hours agorootparentprevThe HOA is generally allowed to put a lien on your house, through the contract you were forced to sign with them. reply LeafItAlone 11 hours agorootparent> through the contract you were forced to sign with them. Except for Texas, I have not heard of being forced into an HOA. You choose to join one when buying a property in a neighborhood that has one. reply mint2 4 hours agorootparentIt’s not a choice when the overwhelming majority of available homes to buy have HOAs. reply jolmg 11 hours agorootparentprev> Except for Texas, I have not heard of being forced into an HOA. What's the story with Texas? reply ikekkdcjkfke 9 hours agorootparentprevForced to sign a contract? reply afpx 8 hours agorootparentprevI tried to have the HOA enforce rules against cutting down of trees. I even took it to a lawyer. I found out that many HOA covenants are written so poorly that they're not enforceable. reply ornornor 11 hours agorootparentprevAren’t HOAs private entity? It was an HOA demand in this case, not a municipal one I think. reply graemep 7 hours agoparentprevAre Americans allowed to take guns to school? I would have thought most schools would ban guns? On the other hand we did have guns in my school in the UK (only for use on the rifle range). That said, I think it is very weird that Americans accept this level of regulation on what they do on their own property. reply Ancapistani 2 hours agorootparentThe only cases where students may possess firearms on school property of which I'm aware are when they are necessary for a school activity. For example, my oldest daughter is involved on the trap team (a shotgun sport). Her firearm is cased and unloaded except on the shooting range itself, and practice is outside of normal school hours. In practice, that means she only has it in my vehicle when I'm taking her to practice or picking her up from practice. It's a non-issue. As of about five years ago, when I went through instructor training for shooting sports, there had never been in an injury caused by youth shooting sports in our state in the ~100 year history of the program. Further, the group I was working with, 4-H, had a perfect safety record nation-wide. To muy knowledge that is still the case. reply disposition2 7 hours agorootparentprevBy and large, no. I’m not sure of any states / places that allow a student to bring a gun to school. A lot of schools have a police officer present, and I imagine they would typically be able to bring their gun. And some states are debating (or have authorized) allowing faculty to bring a gun to school; the caveat being (in most instances I have seen) said faculty would have to pass enhanced gun safety training. reply sokoloff 7 hours agorootparentprevHOAs serve a pretty useful purpose: to coarsely/imperfectly segregate people who want to live in an HOA away from people who would never want to live in an HOA and vice-versa. Those two groups are somewhat prone to having conflicts with each other and HOAs let both be happier than if they were to be annoyed by living next to each other. reply HEmanZ 7 hours agorootparentprevI’d be fascinated to see the geographic and demographic distribution of HOAs. In American Suburbia basically every neighborhood I’ve seen has an HOA. But in some cities I’ve lived they seem completely absent. I’m guessing there was a particular (white flighty) period of time and kind of people who really wanted these things. Neighborhoods built before this time don’t have them, and newer buildings (at least where I live) don’t seem to have them either. So these may be mostly phenomena of a specific period. reply trashface 4 hours agorootparentNowadays you see them everywhere, but originally they were made popular with retirement communities (age restricted 55+) because the HOA is what is required to establish that restriction. Governments can't discriminate on age legally but an HOA gets an exception. In practice they often soft-discriminate in a plausibly deniable way on many other characteristics including SES, race, etc. My parents are elderly and live in one. They have there share of stories, some of which are pretty bad. People have moved out solely due to conflicts. They kind of hate it actually. But I asked my dad if he agrees with all the enforcement and he said basically, well you don't want someone to paint their house purple. Which I think illustrates the preferences of people who seek these communities. reply mlrtime 6 hours agorootparentprevEvery single condo or apartment also has a \"HOA\". They are just called something different. When you buy a condo anywhere in the US, you also get a (sometimes huge) document of all the rules and regulations. You're also required to pay fees every month. reply alistairSH 5 hours agorootparentprevStudents - generally, not allowed to bring guns to school (too young to legally own/carry). Teachers/staff - depends on state/local laws. I'm assuming the OP meant the lack of overall regulation that leads to be people illegally bringing guns to school (and using them). reply nemo44x 7 hours agorootparentprevThere’s plenty of places share there’s no restrictions like this. But people that live in communities have the right to regulate how community members use their property. It’s for the greater good. reply alamortsubite 7 hours agorootparentprevA lot of them, yes. Half of U.S. states allow their schools or school districts to make their own rules regarding guns in schools. About a dozen states explicitly allow concealed firearms in schools. Edit to help those confused about U.S. and states' laws: The 1990 Gun-free School Act doesn't apply to concealed firearms. 32 states allow teachers and school employees to bring guns to school (with some restrictions). States that explicitly allow concealed firearms in schools in some form or another are Alabama, Alaska, Idaho, Illinois, Michigan, Missouri, New Hampshire, Oregon, Rhode Island, South Dakota, Utah, and Wyoming. reply Ancapistani 3 hours agorootparent... and none of them allow peopleAmerican land of the free is being able to bring a gun to school but getting a fine because your grass is too tall. What's wrong with bringing your gun to school? It was common (up until the 1970s, I think) to have shooting sports at school. That's how kids learned gun safety. reply Hamuko 11 hours agoparentprevYou have the freedom to sign up to all kinds of ridiculous but enforceable contracts. reply matsemann 11 hours agoparentprevIs it some kind of bias in that one only hears about the insane cases, and there are thousands of well-run HOAs for every crazy one? Or is it really that bad? We don't have that concept in my country. Or, we do have associations that make it so that houses in an area share the upkeep of communal stuff like a park, playground, organize events etc. All things I would think a \"normal HOA\" would be, but without the ability to impose fines or stuff like that. So I would feel a house with that kind of neighborhood here would be a net positive, as in costs can be shared among more. reply freetime2 10 hours agorootparentI lived in a community with an HOA for 5 years and never had any issue. Like you say, it existed primarily to ensure the upkeep of common facilities (pool, private road, gym, and community building). The only real rule I remember them enforcing (and fairly loosely, at that) was that residents were not allowed to park long term in visitor parking spots. Which to me seemed like a perfectly sensible rule. reply ghaff 6 hours agorootparentWhich is often the case. You tend to hear about the outliers. If a bunch of houses share a common road for example, the maintenance/plowing/etc. of that road has to be divvied up in some way--and it's a reasonable bet that, especially if it's on the longer side, there will be people who object to whatever formula is used. People will also routinely disagree about the degree of maintenance and upkeep that's necessary for any shared facility. reply mint2 4 hours agorootparentprevA lot of HOAs have reasonable boards but contract out a lot of management to companies (I wouldn’t be be surprised if they were owned by private equity firms). These companies are incentivized to find infraction to fine, and drive around regularly to make sure every rule is strictly adhered to. Trash bins out too early or late? Fine! Fence painted the wrong shade of soulless beige? Fine! reply ars 11 hours agorootparentprevIt's not that bad, the vast majority of people in an HOA like being in one. Always remember: If it's in the news it's not the norm. Whatever you hear about a country via the news is the opposite of what that country typically is. reply JumpCrisscross 10 hours agorootparentI’m in an HOA and they’re pretty fine. Mostly focus on snow removal and keeping people from parking their nine hundred snow mobiles in the guest spots. We meet once a year to vote on crap. Key is to not have a group of people with nothing better to do. reply gibolt 10 hours agorootparentThe problem is that you have little control into when one of those people infiltrate the board. Most HOAs start with no issue. It is only after someone new joins that the rule enforcement becomes insane or the big pot of money vanishes. reply bradfa 9 hours agorootparentThis can be solved by having all members vote and not just the board. reply JumpCrisscross 4 hours agorootparent> solved by having all members vote and not just the board Yup. I can’t imagine what an HOA would need to be doing to meet monthly, let alone delegate decision making to a board. reply kasey_junk 3 hours agorootparentThe problem is people talk about HOA like they are some monolithic thing. There are HOA that are only responsible for maintenance of the subdivision sign all the way up to ones responsible for multiple skyrise condo buildings. reply JumpCrisscross 1 hour agorootparent> HOA that are only responsible for maintenance of the subdivision sign all the way up to ones responsible for multiple skyrise condo buildings The latter is simply incorrect. Condo associations and co-op boards are separate creatures, in part due to there being an existential necessity to their existing. (You can't have people dismantling structural members their unit abuts.) HOAs are far more voluntary in nature. That said, I agree that we need a better framework for distinguishing those functioning as direct democracies from those where an elected board gets to appoint itself powers. reply willcipriano 11 hours agorootparentprevIt's this bad. The problem is Boomers. Boomers mostly worked one job between two people, and had the silent generation to take care of their kids so they had a lot of time for masterbatory lawn care/putting trashcans away the second the trash truck pulls away/nonsense. Now they are retired and have even more time, but also authority over people far busier than they ever have been. reply UweSchmidt 6 hours agorootparentEither address the fundamental economic issues and how to solve them in a pragmatic fashion ... or pour oil in the fire of the great culture war, here in the secondary frontline of young vs. old, and accomplish nothing. reply Lio 10 hours agorootparentprevPlease stop with the cheap ageism. Whatever generation[1] you're in will be doing something out of your control that the next generation will look down on too. 1. ...and no, I'm not a Baby Boomer either. reply piyushpr134 8 hours agorootparentprevyou would be surprised to note as to how universally true this comment of yours is. Reporting from India. Here, these folks fight these elections as if there is a billion dollars at the end and then do moral policing ALL day long. Level of turf fights, and self importance behaviour on display at these RWAs (we call them resident welfare association and believe me there is anything there apart from welfare!) is just mind boggling. It is always boomers who win these elections and are president, secretary and treasurer of these RWAs reply jack_riminton 10 hours agorootparentprevThis is one of my favourite HN comments, it’s so true. Here in the UK the boomers were able to work one job, get a house on an easy mortgage and now they’re old and have nothing to do they campaign against any development that might slightly inconvenience them or reduce their house price (spoiler, most developments wouldn’t) reply timthorn 10 hours agorootparentThey'll have had mortgages at 10% or higher, and tied to endowment policies that were often missold. Mortgages weren't easy for boomers, even if house prices were lower. reply jack_riminton 9 hours agorootparentThat is laughably incorrect. Mortgages were far easier to afford for boomers (typically 4 times income, whereas now they're more likely to be 9 or even 10 times income) https://www.financialreporter.co.uk/income-to-house-price-ra... reply gnfargbl 9 hours agorootparentOK, but you've missed the point about relative interest rates on those mortgages. You can't reasonably comment on affordability without taking that into account. As a child in the 1980s with two working \"boomer\" parents and a modest house in a low COL area, we had no money at all due mostly to mortgage interest rates. The very real problems with affordability of housing in the UK have a number of causes but the key issue is supply. Successive governments have simply failed to build sufficient housing to meet demand. NIMBYism alone can't explain this lack of investment. reply Jochim 7 hours agorootparentInterest rates get brought up every time this topic is discussed and the math remains the same. Even accounting for the difference in interest rates, people are much worse off today than they were in the 80s. reply gnfargbl 5 hour",
    "originSummary": [
      "Homeowner Etienne Constable in Seaside, California, responded to a town order to hide his boat by commissioning a mural of the boat on the fence, painted by artist Hanif Panni.",
      "The mural, praised for its trompe-l’oeil style and artistic quality, went viral, attracting significant media and social media attention.",
      "The creative solution has sparked discussions on public art and community creativity, with Panni receiving more mural requests due to the positive reception."
    ],
    "commentSummary": [
      "A man painted his boat on his fence to comply with an HOA rule, igniting a debate on the role and impact of Homeowners' Associations (HOAs).",
      "Opinions are divided: some argue HOAs are essential for maintaining property values and community standards, while others criticize them for excessive control and discriminatory practices.",
      "The discussion underscores the complexities of balancing individual freedoms with collective benefits, the challenges of avoiding HOAs in new developments, and the broader themes of freedom, property rights, and local governance."
    ],
    "points": 422,
    "commentCount": 350,
    "retryCount": 0,
    "time": 1717135706
  },
  {
    "id": 40531699,
    "title": "Global Sun Simulation Tool Visualizes Shadows in Real-Time",
    "originLink": "https://shademap.app",
    "originBody": "MapLibre Open Menu PRO Global sun simulation Contact Help Build Follow Trail shade GPX replay 5 PM 6 PM 7 PM 8 PM 9 PM Drag or click the time at the bottom of the page to see shadows change throughout the day OK Don't show again May 31 07:01 PM 64.9° ∡ 223.3° SW",
    "commentLink": "https://news.ycombinator.com/item?id=40531699",
    "commentBody": "[dupe] Every mountain, building and tree shadow in the world simulated for any time (shademap.app)376 points by wfme 13 hours agohidepastfavorite1 comment dang 57 minutes ago [–] Comments moved to https://news.ycombinator.com/item?id=40528045. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "MapLibre Open Menu PRO introduces a global sun simulation feature, enabling users to visualize shadow changes throughout the day by interacting with a timeline.",
      "The feature provides detailed sun position data, such as altitude and azimuth, enhancing the accuracy of shadow simulations.",
      "This tool is particularly useful for applications requiring precise sunlight and shadow analysis, such as urban planning and architecture."
    ],
    "commentSummary": [
      "A new tool called shademap.app has been released, which simulates shadows from mountains, buildings, and trees globally for any given time.",
      "This tool is gaining attention for its ability to provide detailed shadow simulations, which can be useful for various applications such as urban planning and outdoor activities.",
      "The original discussion about this tool on Hacker News has been relocated to a different thread, indicating significant interest and engagement from the tech community."
    ],
    "points": 376,
    "commentCount": 1,
    "retryCount": 0,
    "time": 1717132164
  },
  {
    "id": 40531301,
    "title": "Understanding AWS S3 Encryption: More Access Control Than True Encryption",
    "originLink": "https://blog.plerion.com/things-you-wish-you-didnt-need-to-know-about-s3/",
    "originBody": "AWS S3 Bucket Encryption Doesn’t Work The Way You Think It Works Let’s try all the different S3 encryption options, see why it’s more like access control than encryption, and why that matters.",
    "commentLink": "https://news.ycombinator.com/item?id=40531301",
    "commentBody": "Things you wish you didn't need to know about S3 (plerion.com)286 points by miles 15 hours agohidepastfavorite171 comments coolgoose 14 hours agoA lot of them are interesting points, but I am not sure I agree with the complaint the file system is case sensitive. That's how it should be and I am annoyed at macos for not having it. reply smt88 13 hours agoparent> That's how it should be Why? Windows is also not case-sensitive, so it's not like there's a near-universal convention that S3 is ignoring. Case sensitivity in file names is surprising even to non-technical people. If someone says they sent you \"Book Draft 1.docx\" and you check your email to find \"Book draft 1.docx,\" you don't say, \"Hey! I think you sent me the wrong file!\" Casing is usually not meaningful even in written language. \"Hi, how are you?\" means the same thing as \"hi, how are you?\" Uppercase changes meaning only when distinguishing between proper and common nouns, which is rarely a concern we have with file names anyway. reply josephcsible 13 hours agorootparent> If someone says they sent you \"Book Draft 1.docx\" and you check your email to find \"Book draft 1.docx,\" you don't say, \"Hey! I think you sent me the wrong file!\" But you also wouldn't say that if they sent \"Book - Draft 1.docx\", \"Book Draft I.docx\", \"BookDraft1.docx\", \"Book_Draft_1.docx\", or \"Book Draft 1.doc\", and surely you wouldn't want a filesystem to treat all of them as the same. reply quickslowdown 13 hours agorootparentThis is a personal reason, but the reason I prefer case sensitive directory names is I can make \"logical groupings\" for things. So, my python git directory might have \"Projects/\" and \"Packages/,\" and the capitalization not only makes them stand out as a sort of \"root path\" for whatever's underneath, but the capitalization makes me conscious of the commands I'm typing with that path. I can't just autopilot a path name, I have to consciously hit shift when tab completion stops working. That might sound like a dumb reason, but it's kept me from moving things into the wrong directory, or accidentally removing a directory multiple times in the past. I also use Windows regularly and it really isn't a hindrance, so maybe I wouldn't actually be bothered if everything was case sensitive. reply pwagland 6 hours agorootparentTBF, you don't need case sensitive FS for that, just case retaining is enough. And then have the option on how to sort it. reply josephcsible 2 hours agorootparentDon't you need case sensitivity for this part? > I can't just autopilot a path name, I have to consciously hit shift when tab completion stops working. On a system that's case retaining but not case sensitive, wouldn't \"pr\" autocomplete to \"Projects\"? reply notjoemama 3 hours agorootparentprevI like it! That's a great idea. To me, this sounds like a great practice for terminal environments but may be less intuitive when using file system apps. I could easily overlook a single letter capitalization in a GUI view of many directories. Maybe it's because at a terminal the \"view\" into the file system is narrow? Now I'm wondering how I can use this in my docker images. I mean that might irritate devops. Well, maybe they'll like it too. Man, thanks for posting this. reply ooterness 3 hours agorootparentprevYou have to draw the line somewhere, but I do appreciate when the UI sorts \"Book draft 2\" before \"Book draft 11\". That requires nontrivial tokenization logic and inference, but simple heuristics can be right often enough to be useful. On that note, ASCIIbetical sort is never the right answer. There is a special place in hell for any human-facing UI that sorts \"Zook draft 1\" between \"Book draft 1\" and \"book draft 1\". reply saghm 2 hours agorootparentI think there's a pretty big difference between how the UI orders things and how the filesystem treats things as equivalent. A filesystem treating names case sensitively doesn't prevent the UI from tokenizing the names in any other arbitrary way reply stronglikedan 4 hours agorootparentprevCapitalization isn't part of grammar. Those examples are different strings of characters altogether. reply selenography 1 hour agorootparentThe classic, if crude, counterexample: \"I helped my uncle Jack off a horse.\" (The uncapitalized version doesn't just have different semantics; it has a completely different parse-tree!) reply otteromkram 3 hours agorootparentprevI'll augment your statement by noting that punctuation is also not part of grammar. reply selenography 1 hour agorootparentAnother classic counterexample: \"This book is dedicated to my parents, Ayn Rand, and God.\" \"This book is dedicated to my parents, Ayn Rand and God.\" reply cmcconomy 13 hours agorootparentprevyou called it - those are different situations all right reply Lutger 10 hours agorootparentprevThere are just not the same characters. A filesystem should not have an opinion on what strings of characters _mean_ the same. It is the wrong level of abstraction. filenames might even not be words at all, and surely not limited to English. We shouldn't implement rules and conventions from spoken English at a filesystem level, certainly not S3. MacOS and Windows are just wrong about this. reply jodrellblank 8 hours agorootparentWindows doesn’t have it at the file system layer, NTFS is case sensitive. Windows has it at the Win32 subsystem layer, see replies and comments here: https://superuser.com/questions/364057/ reply marcosdumay 4 hours agorootparentThat's way worse than just putting it on the file system. Now you have hidden information, that you can't ever change, and may or may not impact whatever you are doing. reply lelanthran 7 hours agorootparentprev> Windows doesn’t have it at the file system layer, NTFS is case sensitive. I think the common phrasing is \"case-aware, not case-sensitive\". reply frizlab 8 hours agorootparentprevAnd so should we be able to have “é.txt” and “é.txt” in the same directory (with a different UTF-8 normalization?) What encoding should we use BTW? I’m not advocating for case-insensitive fs (literally the first thing I do when I get a Mac is reformat it to be on a case-sensitive fs), but things are not that simple either. reply marcosdumay 4 hours agorootparent> And so should we be able to have “é.txt” and “é.txt” in the same directory That's what Linux does. It does create some problems that seem to never happen on practice, while it avoids some problems that seem to happen once in a while. So yeah, I'd say it's a good idea. reply ozim 8 hours agorootparentprevYou look from technical perspective. From average person perspective, even files are too much technicality to deal with. As a user I want my work to be preserved, I want to view my photos and I want system to know where is my funny foto of my dog I did last Christmas. As a developer I need an identifier for a resource and I am not going to let user decide on the Id of the resource, I put files in system as GUID and keep whatever user feels as metadata. Exposing average people to the filesystem is wrong level of abstraction. That is why iOS and Android apps are going that way - but as I myself am used to dealing with files it annoys me that I cannot have that level of control, but I accept that I am quite technical. reply graemep 7 hours agorootparentDealing with files used to be something everyone interacting with computers had to do. It is something average people can do. I think too much abstraction is a mistake and adds a lot of unneeded complexity. People should learn something about technology they use. If you want to drive, you need understand how steering wheels work, if you want to drive a manual car (usual where I live and have lived) then you need to know how to work a gear stick and the effect of changing gear. reply mgkimsal 3 hours agorootparent> used to be something everyone interacting with computers had to do There were far fewer people 'interacting with computers' at that level years ago. reply koolba 8 hours agorootparentprev> Casing is usually not meaningful even in written language. \"Hi, how are you?\" How about: “pay bill” vs “pay Bill”? “Usually” in the context of automated systems design is a recipe for disaster. Computers store bytes, not characters that may just happen to mean similar things. Shall we merge ümlauts? How to handle ß? reply lathiat 6 hours agorootparentCase Preserving and Case Sensitive are subtly two different things. Most case insensitive file systems are case preserving and whatever the UTF8 equivalent is I forget the name. reply nostrebored 3 hours agorootparentBut the gps point is that assuming you know the semantic meaning of the case and if retention is enough is silly. Assuming case insensitivity is bizarre. reply ooterness 3 hours agorootparentprevPerfect is the enemy of good. It is quite acceptable to streamline the easy cases now and the hard cases later or never. reply lultimouomo 13 hours agorootparentprevCase insensitive matching is a surprisingly complicated, locale-dependent affair. Should I.txt and i.txt match? (Note that the first file is not named I.txt). Case insensitive filesystems make about as much sense as ASCII-only filenames. reply segfaltnh 3 hours agorootparentComplicated for who? I've little pity for developers and kernels ease of life as a user. reply HeatrayEnjoyer 11 hours agorootparentprevHow would locale matter? reply ckolkey 11 hours agorootparentOff the top of my head, in turkish, `i` doesn't become `I`, it becomes `İ`. And `ı` is the lower case version of `I` reply GolDDranks 11 hours agorootparentprevFor example, it depends on the locale if the capitalized form of ß is ß or SS. reply jodrellblank 7 hours agorootparentprevAnd yet case insensitive file name matching / string matching is one of my favourite windows features. It’s enormously convenient. An order of magnitude more convenient than the edge cases it causes me. People aren’t ASCII or UTF-8 machines; “e” and “E” are the same character, that they are different ASCII codes is a behind the scenes implementation detail. (That said, S3 isn’t a filesystem, it’s more like a web hashtable key-to-blob storage) reply lultimouomo 6 hours agorootparent> People aren’t ASCII or UTF-8 machines; “e” and “E” are the same character They are the same character to you, a native speaker of a Western language written in a latin script. They are the same to you because you are, in fact, an ASCII machine. Many many people in the world are not. reply kaashif 5 hours agorootparentprev> “e” and “E” are the same character They don't look like the same character to me. A character is a written symbol. These are different symbols. What definition of \"character\" are you using where they're the same character? I haven't ruled out that I am wrong, this is a naive comment. reply tacostakohashi 5 hours agorootparentYou are confusing characters with glyphs. A glyph is a written symbol. reply taeric 4 hours agorootparentAnd you seem to be conflating characters and letters. There are fewer letters in the standard alphabet than we have characters for the same, largely because we do distinguish between some letter forms. I suppose you could imagine a world where we don't, in fact, do this with just the character code. Seems fairly different from where we are, though? reply kaashif 5 hours agorootparentprevI thought that if they're different glyphs they're different characters. Surely the fact that they're represented differently in ASCII means ASCII regards them as different characters? Whether they're different glyphs or not depends on the font. reply Zambyte 5 hours agorootparentprev> It’s enormously convenient. An order of magnitude more convenient than the edge cases it causes me. Can you elaborate on this? reply raincole 11 hours agorootparentprev> Casing is usually not meaningful even in written language. \"Hi, how are you?\" means the same thing as \"hi, how are you?\" Uppercase changes meaning only when distinguishing between proper and common nouns, which is rarely a concern we have with file names anyway. The number of spaces is usually not meaningful in written language. \"Hi, how are you?\" means the same thing as \"Hi, how are you ?\". I don't think it's a good reason to make file system ignore space characters. reply sergeykish 9 hours agorootparentprevIf someone says they sent you \"Book Draft 1.docx\" and you check your email to find \"Ⓑⓞⓞⓚ Ⓓⓡⓐⓕⓣ ①.ⓓⓞⓒⓧ\", \"฿ØØ₭ ĐⱤ₳₣₮ 1.ĐØ₵Ӿ\" - these are different files. reply throwaway211 5 hours agorootparentI have a feeling you enjoyed that character set lookup. I know I did seeing it. reply Izkata 3 hours agorootparentAges ago on Flowdock at work (a chat webapp kind of like Slack that no longer exists), I used the circle ones for a short time as my nickname, and no one could @ me. reply zarzavat 9 hours agorootparentprevFile systems are not user interfaces. They are interfaces between programs and storage. Case insensitive is much better for programs. The user shell can choose however it wants to handle file names, a case sensitive file system does not prevent the shell from handling file names case insensitively. reply zarzavat 6 hours agorootparent> case insensitive is much better for programs Can’t edit my comment. I mean case sensitive is better for programs, of course. reply blahgeek 13 hours agorootparentprevNo offense, but I think that's a very western-centric view. Your example only make sense when the user is familiar to English (or other western languages, I guess). To me personally, I find it strange that \"D.txt\" and \"d.txt\" means the same file, since they are two very different characters. Likewise, I think you would also go crazy if I tell you \"ア.txt\" and \"あ.txt\" means the same file (which is hiragana and katakana for A respectively, which in a sense is equivalent to uppercase and lowercase in Japanese), or \"一.txt\" and \"壹.txt\" means the same file (which both means number 1 in Chinese, we call the latter one literally \"uppercase number\") reply taeric 4 hours agorootparentAgreed, and you could even take this into \"1.txt\" being the same as \"One.txt\". Which, I mean, fair that I would expect a speech system to find either if I speak \"One dot t x t\". But, it would also find \"Won.txt\" and trying to bridge the phonetic to the symbolic is going to be obviously fraught with trouble. reply JohnFen 2 hours agorootparentprev> To me personally, I find it strange that \"D.txt\" and \"d.txt\" means the same file, since they are two very different characters. As a native English speaker, I agree with this. reply ClumsyPilot 10 hours agorootparentprevThose are all the same, I don’t see an issue reply n_plus_1_acc 7 hours agorootparentWhat if Unicode updates some capitalization rules in the next version, and after an OS updates some filenames now collide and one of the is inaccessible? reply JohnFen 3 hours agorootparentprev> Why? Windows is also not case-sensitive, so it's not like there's a near-universal convention that S3 is ignoring. Not sure why what Windows does is relevant to this, honestly. Personally, I strongly prefer case sensitivity with filenames, but the lack of it isn't a dealbreaker or anything. reply CamperBob2 2 hours agorootparentWhat are some of the advantages of case sensitivity? Are you saying you actually want to save \"Book draft 1.docx\" and \"Book Draft 1.docx\" as two separate files? That just sounds like asking for trouble. reply JohnFen 2 hours agorootparentThe advantages that I value are that case sensitivity means I can use shorter filenames, it makes it easier to generate programmatic filenames, and I can use case to help in organizing my files. > Are you saying you actually want to save \"Book draft 1.docx\" and \"Book Draft 1.docx\" as two separate files? That's a situation where sensitivity can cause difficulty, yes, but for me personally, that's a minor confusion that is easy to avoid or correct. Everything is a tradeoff, and for me, putting up with that annoyance is well worth the benefits of case sensitivity. I do totally understand that others will have different tradeoffs that fit them better. I'm not taking away from that at all. But saying \"case sensitivity is undesirable\" in a broad sense is no more accurate than saying \"case sensitivity is desirable\" in a broad sense. Personally, I think the ideal tradeoff is for the filesystem to be case sensitive, but have the user interfaces to that file system be able to make everything behave as case-insensitive if that's what the user prefers. reply rzwitserloot 5 hours agorootparentprevAlso note that 'are these 2 words case insensitively equal' is impossible without knowing what locale rules to apply. And given that people's personal names tend to have the property that any locale rules that must be applied are _the locale that their name originates from_, and that no repository of names I am aware of stores locale along with the name, that means what you want, is impossible. In line with case insensitivity, do you think `müller` and `muller` should boil down to for example the same username for login purposes? That's... tricky. In german, the standard way to transliterate names to strict ASCII would be to turn `müller` into `mueller`. In swiss german that is in fact mandatory. Nobody in switserland is named `müller` but you'll find loads of `mueller`s. Except.. there _are_ `müller` in switzerland - probably german citizens living ther. So, just normalize `ü` to `ue`, easy, right? Except that one doesn't reverse all that well, but that's probably allright. But - no. In other locales, the asciification of `ü` is not `ue`. For example, `Sjögren` is swedish and that transliterates to `sjogren`, not `sjoegren`. Bringing it back to casing: Given the string `IJSSELMEER`, if I want to title case that, the correct output is presumably `IJsselmeer`. Yes, that's an intentional capital I capital J. Because it's a dutch word and that's how it goes. In an optimal world, there is a separate unicode glyph for the dutch IJ as a single letter so we can stick with the simple rule of 'to title case a string, upper case the first glyph and lowercase all others, until you see a space glyph, in which case, uppercase the next'. But the dutch were using computers fairly early on and went with using the I and the J (plain ascii) for this stuff. And then we get into well trodden ground: In turkish, there is both a dotted and a dotless i. For... reasons they use plain jane ascii `i` for lowercase dotted i and plain jane ascii `I` for uppercase dotless I. But they have fancy non-ascii unicode glyphs for 'dotted capital I' and 'dotless lowercase i'. So, __in turkish__, `IZMIR` is not case-insensitive equal to `izmir`. Instead, `İZMIR` and `izmir` are equal. I don't know how to solve this without either bringing in hard AI (as in, a system that recognizes 'müller' as a common german surname and treats it as equal to 'mueller', but it would not treat `xyzmü` equal to `xyzmue` - and treats IZMIR as not equal to izmir, because it recognizes it as the name of a major turkish city and thus applies turkish locale rules), or decreeing to the internet: \"get lost with your fancypants non-US/UKian weird word stuff. Fix your language or something\" - which, well, most cultures aren't going to like. 'files are case insensitive' sidesteps alllllll of this. reply up2isomorphism 12 hours agorootparentprevThen why don’t you just always write in lower case? reply paulddraper 5 hours agorootparentprev> Why? Because it introduces extra complexity. Now, \"Cache\" and \"cache\" are the same, but also...different because you'd care if Cache suddenly became cache. reply re-thc 13 hours agorootparentprev> you don't say, \"Hey! I think you sent me the wrong file!\" You do! Why not? It's a big trap. A lot of counterfeit, spam, phishing etc go by this method. You end up buying a fake brand or getting tricked. reply pkulak 13 hours agorootparentprevYeah, but that little bit of user friendliness ruins the file system for file system things. Now you need “registries” and other, secondary file systems to do file system things because you can’t even use base64 in file names. Make your file browsing app case insensitive, if that’s what you want. Don’t build inferiority down to the core. reply fortran77 13 hours agorootparentprev> Why? Windows is also not case-sensitive, so it's not like there's a near-universal convention that S3 is ignoring. You can enable case sensitivity for directories or disks, but this is usually done for special cases, like git repos reply imadj 2 hours agoparentprevI agree 100%. From a technical implementation pov 'A' & 'a' are well established as different characters (ascii, unicode, etc). Regardless of personal preference, I don't understand how can a developer/Sys admin be surprised and even frustrated that a file system is case sensitive. The developer is still free to abstract this away for the end user when it makes sense such as search results reply dagrz 11 hours agoparentprevAuthor here. There's no complaint. It's an observation rather than an absolute good or bad. It's something you have the consider in designing your application. reply ivanhoe 7 hours agoparentprev> That's how it should be Why exactly? I'm not aware of any benefits of filenames being case-sensitive, it just opens a room for tons of very common mistakes that literally can't happen otherwise. It's not like in coding where it helps enforce the code style and thus aids readability - and even in programming it was a source of PITA to solve bugs before IDEs became smart enough to catch typos in var names. One thing I loved in Pascal the most is that it didn't care about the case, unlike the C. reply capitol_ 6 hours agorootparentThe case-sensitivity algorithm needs a locale as input in order to correctly calculate the case conversion rules. The most common example is probably that i (U+0069 LATIN SMALL LETTER I) and I (U+0049 LATIN CAPITAL LETTER I) transform into each other in most locales, but not all. In locales az and tr (the Turkic languages), i uppercases to İ (U+0130 LATIN CAPITAL LETTER I WITH DOT ABOVE), and I lowercases to ı (U+0131 LATIN SMALL LETTER DOTLESS I). case-insensitive is all fine if you only handle text that consist of A-Za-z, but as soon as you want to write software that works for all languages it becomes a mess. reply jerf 5 hours agorootparentThis is the main point, and almost all the other chatter is not particularly relevant. A dumb computer and a human can agree with \"files are case sensitive and sometimes that's a bit weird but computers are weird sometimes\". If there was indeed exactly one universal way to have case insensitivity it would be OK. Case insensitive file systems date from when there was. Everything was English and case folding in English is easy. Problem solved. But that doesn't work today. And having multiple case folding rules is essentially as unsolvable a problem as the problems that arise from case sensitivity, except they're harder for humans to understand, including programmers. Simple and wrong is better than complicated and wrong and also the wrong is shoved under the carpet until it isn't. Though you still ought to declare a Unicode normalization on the file system. Which would be perfectly fine if it weren't for backwards compatibility. reply codeflo 7 hours agorootparentprevExcept at the UI layer (where you can easily offer suggestions and do fuzzy search), the opposite is true. There are so many different ways to do case-insensitive string comparisons, and it's so easy to forget to do that in one place, that case-insensitivity just leads to ton of bugs (some of which will be security critical). For example, did you know that Microsoft SQL Server treats the columns IS_ADMIN and is_admin as either the same or two different columns depending on the database locale (because e.g. Turkish distinguishes between i and I)? That's at least a potential security bug right there. reply sholladay 13 hours agoparentprevmacOS is case preserving, though. To me, it’s the best of both worlds. You can stylize your file names and they will be respected, but you don’t have to remember how they are stylized when you are searching for or processing them because search is case insensitive. reply alt227 6 hours agorootparentIMO this is the worst possible solution, as what you are seeing is not what you are getting. You do not actually know what is being stored on the file system, and your searches are fuzzy rather than precise. reply filleduchaos 2 hours agorootparent> You do not actually know what is being stored on the file system This makes no sense to me. Did the user's file explorer (whether GUI or via commands like `ls`) suddenly disappear? reply smt88 13 hours agorootparentprevWindows is also case-insensitive but case-preserving reply self_awareness 13 hours agorootparentprevMaybe macOS is case-preserving, but it's not encoding-preserving. If you create a file using a composed UTF-8 \"A\", the filesystem layer will decompose the string to another form, and create the filename using that decomposed form \"B\". Of course, \"A\" and \"B\" when compared can be completely different (even when compared using case insensitivity enabled), yet will point to the same file. More info here: https://eclecticlight.co/2021/05/08/explainer-unicode-normal... reply halostatue 2 hours agorootparentmacOS (Darwin) has always written filenames as NFD via the macOS APIs. The underlying POSIX-ish APIs may not do NFD, but Finder and every native macOS GUI program gets files in NFD format. reply josephcsible 13 hours agoparentprevmacOS has case sensitivity. It's just off by default and is a major pain to turn on. You have to either reinstall from scratch onto a case-sensitive partition, or change the \"com.apple.backupd.VolumeIsCaseSensitive\" xattr from 0 to 1 in a Time Machine backup of your whole system and then restore everything from it. reply JonathonW 13 hours agorootparentYou shouldn't do this if you value things working, though-- this is a pretty rare configuration (you have to go way out of your way to get it), so many developers won't test with it and it's not unheard of for applications to break on case-sensitive filesystems. If you absolutely need case-sensitivity for a specific application or a specific project, it's worth seeing if you can do what you need to do within a case-sensitive disk image. It may not work for every use-case where you might need a case-sensitive FS, but if it does work for you, it avoids the need to reinstall to make the switch to a case-sensitive FS, and should keep most applications from misbehaving because the root FS is case-sensitive. reply josephcsible 13 hours agorootparentMost things work fine, but it will break (or at least did break at one point) Steam, Unreal Engine, Microsoft OneDrive, and Adobe Creative Cloud. I'm rather surprised about the first two, since they both support Linux with case-sensitive filesystems. I took the opposite approach as you, though: making my root filesystem case-sensitive and creating a case-insensitive disk image if I ever needed those broken programs. reply danielheath 12 hours agorootparentprevI keep a case sensitive volume around to checkout code repositories into. For everything else I prefer it insensitive, but my code is being deployed to a case sensitive fs. reply jimvdv 7 hours agorootparentprevI just mount a case sensitive Apple File System disk image at ~/code, works well reply pjmlp 12 hours agoparentprevUNIX is one of the few OSes that went down that path. Others do offer the option if one is so inclined, and also prepared to deal with legacy software that expects otherwise. Which is also the case with macOS, because although it is a UNIX, OS X had to catter to the Mac OS developer community used to HFS and HFS+. reply codeflo 7 hours agorootparentCuriously, iOS and iPadOS file systems are case-sensitive. There's less legacy there, so they opted to do the correct thing. reply larkost 1 hour agorootparentPlease don't call this \"the correct thing\". Please recognize that there are multiple, valid, points of view. What you meant is \"the thing I like\". reply andruby 4 hours agoparentprevYou can format disks in MacOS to be case sensitive. reply segfaltnh 3 hours agoparentprevCase sensitive filesystems are a mistake. reply pavlov 13 hours agoparentprevCase insensitive is how humans think about names. “John” and “New York” are the same identifiers as “john” and “new york”. It would be pretty weird if someone insisted that their passport is invalid because the name is printed in all caps and that’s not their preferred spelling. IMO the best thing would be to call Unix-style case-sensitive file names something else. But it’s obviously too late for that. reply lalaithion 12 hours agorootparentThe word “Turkey” is not the same as “turkey”, “August” is not the same as “august”, and “Muse” is not the same as “muse”. https://en.m.wikipedia.org/wiki/Capitonym reply justincormack 10 hours agorootparentAnd “polish” and “Polish” are not even pronounced the same. reply scosman 7 hours agorootparentprevThey might be at the beginning of a sentence (depends on the reason for capitalization). It’s more like identifier reuse, on a case insensitive “system”. “John” isn’t the same as “John” if I’m talking about two separate Johns. reply pavlov 11 hours agorootparentprevYet \"TURKEY\" is not a separate word from \"Turkey\" and \"turkey\". Ultimately context disambiguates these words, not capitalization. reply Hamuko 12 hours agorootparentprevHumans will also treat \"Jyväskylä\", \"Jyvaskyla\" and \"Jyvaeskylae\" as the same identifiers but I don't think that's a good basis for file storage to have those be the same filenames. reply pavlov 11 hours agorootparentIn the era of Unicode, this battle is pretty much lost. Several different code point sequences can produce the glyph 'ä', and user input can contain any of these. You need to normalize anyway. reply glandium 8 hours agorootparentAnd macOS does that normalization at the filesystem level. reply justincormack 9 hours agorootparentprevPassport offices care and may object. reply bigstrat2003 12 hours agorootparentprevAgreed. I think case sensitivity in Unix filesystems is actually a pretty poor design decision. It prioritizes what is convenient for the computer (easy to compare file paths) over what makes sense for the user (treating file paths the same way human intuition does). reply 7bit 2 hours agorootparentIn Germany there is a lowercase letter ß. It actually is a ligature of the letters s and z. It does not have an uppercase variant, because there is no word that begins with it. One word would be Straße. If you write that all in uppercase, it technically becomes STRASZE, although you almost always see STRASSE. But if you write that all in lowercase without substituting SS with ß, you are making a mistake. And although Switzerland is a german-speaking country, they have different spelling and rarely use ß -- if not ever. This is just one of many cases, where case-insensitiy would give more trouble than it's worth. And others pointed out similar cases with the Turkish language in this post. reply sham1 4 hours agorootparentprevBut the thing is that the file system doesn't need to be case-insensitive for your system to support human intuition! As others have said, people don't look at and use filesystems, they use programs that interface with the filesystem. You can absolutely have a case-sensitive system that nonetheless lets you search files in a case-insensitive manner, for example. After all, to make searches efficient, you might want to index your file structure, and while doing that, you might as well also have a normalised file name within the index you search against. Now, as you said, UNIX did the choice that's easier for computers. And for computers, case-insensitive filesystems would be worse. There are things that are definitely strange about UNIX filesystems (who doesn't love linefeeds in file names!?), but case-sensitivity is not one of them. reply howerj 6 hours agorootparentprevExcept that is not true, it is sometimes convenient, and sometimes very inconvenient and not wanted. My reasoning for file systems that are case sensitive is the following: 1. Some people want file systems to case sensitive. 2. Case sensitive is easier to implement. This is very much not a trivial thing. Case insensitivity only really makes sense for ASCII. In the camp of wanting case insensitivity: 1. Some people want file systems to be case insensitive. There is more in favor of case sensitivity. reply kaashif 5 hours agorootparentprevI don't know if that's right. The most obvious way two characters can be the same is if they actually look exactly the same i.e. are homoglyphs https://en.wikipedia.org/wiki/Homoglyph But no filesystem I am aware of is actually homoglyph insensitive. Case insensitive filesystems picked one arbitrary form of intuition (and not even the .oat obvious one) in one language (English) and baked that into the OS at a somewhat deep level. You say \"human intuition\" - are those using different writing systems nonhuman then? reply JackSlateur 7 hours agorootparentprevBut end users do not speak to filesystems. Programs speak to filesystems. reply scosman 7 hours agoprevHere is a good one: deleting billions of objects can be expensive if you call delete APIs. However you can set a wildcard or bucket wide object expiry of time=now for free. You’ll immediately stop being charged for storage, and AWS will manage making sure everything is deleted. reply ericpauley 6 hours agoparentNit: the delete call is free, it’s the list call to get the objects that costs money. In theory if you know what objects you have from another source it’s free. reply bushbaba 2 hours agoparentprevBecause AWS gets to choose when the actual deletes happen. The metadata is marked as object deleted but AWS can process the delete off peak. It also avoids the s3 api server being hammered with qps reply dale_glass 12 hours agoprevThe case sensitivity one is easy, here's a thing that's more likely to be entirely unintuitive: S3 paths are fake. Yes, it accepts uploads to \"/builds/1/installer.exe\", and yes, you can list what's in /builds, but all of that is a simulation. What you actually did was to upload a file literally named '/builds/1/installer.exe' with the '/' as part of the name. So, \"/builds/1//installer.exe\" and \"/builds//1/installer.exe\" are also possible to upload and entirely different files. Because it's the name of a key, there's no actual directories. reply reddalo 12 hours agoparentYou're right, unless you use the new S3 \"Directory buckets\" [1], which make the entire thing even more confusing! [1] https://docs.aws.amazon.com/AmazonS3/latest/userguide/direct... reply temac 1 hour agorootparentand which also according to the command docs i read recently add random limitations to maybe half of the S3 API I'm not sure reusing (parts of) the protocol was a good idea given unrelated half of it was already legacy and discouraged (e.g. the insane permission model with policies and acls) or even when not, let's say... weird and messy. reply mdaniel 3 hours agoparentprevalso, don't overlook that \"/\" is only the default path delimiting character; one is free to use your favorite other character if you need a filename with a \"/\" in it: https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObje... reply cnity 10 hours agoparentprevAside from resolution of paths to some canonical version (e.g. collapsing redundant /'s in your example), what actually is an \"actual directory\" other than a prefix? reply ncruces 10 hours agorootparentAn actual directory knows its direct children. An actual directory A with 2 child directories A/B and A/C, each of which with 1 million files, doesn't need to go through millions of entries in an index to figure out the names of B and C. It's also not possible for A and A/ to exist independently, and for A to be a PDF, A/ an MP4 and A/B a JPG under the A/ ”directory\". All of which are possible, simultaneously, with S3. reply marcosdumay 4 hours agorootparentJust to add, \"directory\" is something you use to look-up information. The thing is called \"directory\" exactly because you go there to look what its children are. reply mock-possum 8 minutes agorootparentprevAn actual directory is a node in a tree. It may be the child of a parent and it may have siblings - its children may in turn be parents with children of their own. reply hnlmorg 12 hours agoparentprevYeah, the prefix thing is a source of so many bugs. I get why AWS took that approach and it’s actually a really smart approach but it still catches so many developers out. Just this year our production system was hit by a weird bug that took 5 people to find. Turned out the issue was an object literally just named “/“ and the software was trying to treat it as a path rather than a file. reply BirAdam 6 hours agorootparentHappened at my job yesterday. reply reddalo 13 hours agoprevI can't trust myself using S3 (or any other AWS service). Nothing is straightforward, there are too many things going on, too much documentation that I should read, and even then (as OP shows) I may be accidentally and unknowingly expose everything to the world. I think I'll stick to actually simple services, such as Hetzner Storage Boxes or DigitalOcean Spaces. reply chamomeal 11 hours agoparentI like digital ocean spaces, but it has its own annoying quirks. Like I recently found out yif you pipe a video file larger than a few MB, it’ll drop the https:// from the returned Location. So on every file upload I have to check if the location starts with https, and add it on if it’s not there. Of course the S3 node client GitHub issue says “sounds like a digital ocean bug”, and the digital ocean forums say “sounds like an S3 node client bug” lol reply slig 6 hours agoparentprevThe way that DO handles secrets should scare anyone. Did you know that if you use their Container Registry and set it up so that your K8S has automatically access to it, their service will create a secret that has full access to your Spaces? reply marcosdumay 4 hours agorootparentHum... Kubernetes is not on the GP's list... reply maximinus_thrax 2 hours agoparentprev> Nothing is straightforward, there are too many things going on, too much documentation that I should read, and even then (as OP shows) I may be accidentally and unknowingly expose everything to the world. I took a break from cloud development for a couple of years (working mostly on client stuff) and just recently got back. I am shocked at the amount of complexity built over the years along with the cognitive load required for someone to build an ironclad solution in the public cloud. So many features and quirks which were originally designed to help some fringe scenario are now part of the regular protocol, so that the business makes sure nobody is turned away. reply isoprophlex 14 hours agoprevThat bit on failed multipart uploads invisibly sticking around (and incurring storage cost, unless you explicitly specify some lifecycle magic)... just ugh. And I thought one of the S-es was for 'simple'. reply e____g 13 hours agoparentYes, that sucks. Blame ahenry@, then GM for S3. My proposal was that parts of incomplete uploads would stick around for only 24 hours after the most recent activity on the upload, and you wouldn't be charged for storage during that time. ahenry@ vetoed that. reply vbezhenar 10 hours agorootparentWhy would you propose something that makes company earn less money? I'm sure that at Amazon scale, this misfeature earned millions of dollars. reply spacebanana7 9 hours agorootparentCustomer relationships. I recall a Bezos quote along the lines of \"It's better to lose a refund than to lose a customer\". reply notatoad 3 hours agoparentprevthis one has cost us many thousands of dollars. we had cron script on a very old server running for almost a decade, starting a multipart upload every night, pushing what was supposed to be backups to a bucket that also stored user-uploaded content so it was normal that the bucket grows in size by a bit every day. the script was 'not working' so we never relied on the backup data it was supposed to be pushing, never saw the files in s3, the bucket grew at a steady and not unreasonable pace. and then this spring i discovered that we were storing almost 3TB of incomplete multipart uploads. and yes, i know that anecdote is just chock full of bad practices. reply scosman 6 hours agoparentprev“Simple” was coined when the alternative was managing a fleet of servers with disks . Time changes everything. reply teitoklien 13 hours agoparentprevThat S for simple stands for simple ways to skyrocket your expenses reply Hamuko 12 hours agoparentprevYeah, I've definitely treaded on the storage cost landmine. Thankfully it was just some cents in my case, but it's really infuriating how badly the console exposes the information. reply julik 6 hours agoprevA few more: * Multipart uploads cannot be performed from multiple machines having instance credentials (as the principal will be different and they don't have access to each other's multipart uploads). You need an actual IAM user if you want to assemble a multipart upload from multiple machines. * LIST requests are not only slow, but also very expensive if done in large numbers. There are workarounds (\"bucket inventory\") but they are neither convenient nor cheap * Bucket creation is not read-after-write consistent, because it uses DNS under the hood. So it is possible that you can't access a bucket right after creating it, or that you can't delete a bucket you just created until you waited enough for the changes to propagate. See https://github.com/julik/talks/blob/master/euruko-2019-no-su... * You can create an object called \"foo\" and an object called \"foo/bar\". This will make the data in your bucket unportable into a filesystem structure (it will be a file clobbering a directory) * S3 is case-sensitive, meaning that you can create objects which will unportable into a filesystem structure (Rails file storage assumed a case-sensitive storage system, which made it break badly on macOS - this was fixed by always using lowercase identifiers) * Most S3 configurations will allow GETs, but will not allow HEADs. Apparently this is their way to prevent probing for object existence, I am not sure. Either way - cache-honoring flows involving, say, a HEAD request to determine how large an object is will not work (with presigned URLs for sure!). You have to work around this doing a GET with a Range: of \"very small\" (say, the first byte only) * If you do a lot of operations using pre-signed URLs, it is likely you can speed up the generation of these URLs by a factor of 10x-40x (see https://github.com/WeTransfer/wt_s3_signer) * You still pay for storage of unfinished multipart uploads. If you are not careful and, say, these uploads can be initiated by users, you will be paying for storing them - there is a setting for deleting unfinished MP uploads automatically after some time. Do enable it if you don't want to have a bad time. These just off the top of my head :-) Paradoxically, S3 used to be revolutionaly and still is, onl multiple levels, a great products. But: plenty features, plenty caveats. reply dugmartin 4 hours agoparentThe one that caught me a couple of weeks ago is multipart uploads have a minimum initial chunk size of 5 MiBs (https://docs.aws.amazon.com/AmazonS3/latest/userguide/qfacts...). I built a streaming CSV post-processing pipeline in Elixir that uses Stream.transform (https://hexdocs.pm/elixir/Stream.html#transform/3) to modify and inject columns. The Elixir AWS and CSV modules handle streaming data in but the AWS module throws an error (from S3) if you stream \"out\" that totals less than 5 MiBs as is uses multi-part uploads which made me sad. reply pachico 12 hours agoprevI have the feeling that the entire case (in)-sensitive discussions are usually too much English-centric. Allow me to iterate: I have the feeling way too many language discussions, especially in IT, are too much English-centric. reply reddalo 12 hours agoparentSpeaking of non-English cultures, do Japanese case insensitive systems differentiate between hiragana and katakana? Because, in some ways, the two syllabaries remind me of uppercase and lowercase alphabets. reply presentation 4 hours agorootparentA lot of the time forms will specifically ask for hiragana or katakana, or specify full width or half width characters. But basically it’s a mess there too reply thaumasiotes 10 hours agorootparentprevThey're more like distinguishing between \"o\" and “ℴ”. Which is where the European idea of \"capital letters\" originates, but not how we think about them today. reply teitoklien 11 hours agoparentprev> too much English-centric. Pretty glad about it considering how much more simpler ASCII was to work with compared to Unicode. I say it as a non native english speaker, programming has so many concepts and stuff already, its best not to make it more complex by adding a 101 different languages to account for. Unicode and Timezone, the two things that try to bring more languages and cultures to be accounted for while programming and look what happens, it creates the most amount of pain for everyone including non native english programmers. I dont want to write computer programs in my non-english native tongue, if that means i’ll have to start accounting for every major language while im programming. Its fine that IT discussions are so English-centric. Diversity is more complexity, and no one owns the english language, its just a tool used by people to communicate, thanks to that universal language, I can express my thoughts to most people in India, China, Japan, South America, etc due to having 1 common language. They all own the english language too, the moment they decided to speak in it. No need to bring diversity politics in IT. Best to keep it technical. reply regentbowerbird 9 hours agorootparent> No need to bring diversity politics in IT. Politics is just \"how people think things should be\". Therefore politics are everywhere not because people _bring_ them everywhere but because they arise from everything. Your comment is in fact full of politics, down to your opinion that politics shouldn't be included in this discussion. ** > thanks to that universal language, I can express my thoughts to most people in India, China, Japan, South America, etc due to having 1 common language Personally my impression is that native speakers just run circles around everyone else during meetings and such. Being _truly_ comfortable in the language, mastering social cues, being able to confidently and fluently express complex ideas, mean that they effectively take over the room. In turn that means they will hold more power in the company, rise in rank more quickly, get paid more, etc.. There's an actual, significant consequence here. Plus, anglos usually can't really speak another language, so since they don't realize how hard it is they tend to think their coworkers are idiots and will stick to do things with other anglos rather than include everyone. > Diversity is more complexity In a vacuum I agree, but within the context of your comment this is kinda saying \"your existence makes my life too complex, please stop being different and join the fold\"; and I can't agree with that sentiment. reply BirAdam 7 hours agorootparentYou raise an interesting point about the nature of politics. I’ve been thinking about this a bit, but it seems to me that radical/revolutionary politics are talking about how people want things to be while quotidian political ideas are more about how people ought to do a few things. The distinction here being people’s timelines and depth of thought. If a policy has some seriously bad consequences, people may not notice because they weren’t really thinking of things should be, just the narrower thought of how a thing out to be done (think minimum wage driving automation rather than getting people a better standard of living, or immigration control driving police militarization). Of course, for most politicians, I am not sure either of these are correct. I think for politicians, politics is just the study of their own path to power; they likely don’t care much about whether it’s how things are done or how things ought to be so long as they are the ones with the power. I don’t know that this comment really ads anything to the conversation, but I do find it all interesting. Edit: also, on topic, languages are fun. The world is boring when everything is in one language. Languages also hold information in how they structure things, how speakers of that language view the world, and so on, and in those ways they are important contributors to diversity of thought. reply orphea 10 hours agorootparentprev> considering how much more simpler ASCII was to work with compared to Unicode. And elemental algebra is more simple than differential calculus. ASCII being simpler just means it is not adequate to represent innate complexity that human languages have. Unicode is not complex because of \"diversity politics\", whatever that means. It is because languages are complex. The same story with time zones: they are as complex as time is. reply mirchibajji 6 hours agorootparentprevI'm not sure why you characterise this as political. I wish the \"case\" was a modifier like Italic, bold. It would have been easier to _not_ have separate ASCII codes for upper and lower-case letters in the first place. What are your thoughts on MS Word using different characters for opening and closing quotes? reply squaresmile 8 hours agorootparentprev> thanks to that universal language, I can express my thoughts to most people in India, China, Japan, South America, etc due to having 1 common language. My lazy ass wishes that English is enough to access those communities too. There are many cool and interesting developers, projects and communities only or mostly working in their native languages. One of the major motivation for me to learn Chinese now is to access those communities. reply phrotoma 8 hours agoprevHere's another fun one that took a colleague and I several days of analysis to diagnose. S3 will silently drop all requests once a single TCP connection has sent 100 HTTP requests. https://github.com/aws/aws-sdk-go/issues/2825 reply seabrookmx 3 hours agoparentIt doesn't silently drop, it sends a header indicating it's closed the TCP connection. This is a pretty common pattern whereby you want keep-alive.for performance reasons, but you don't want clients running _too long_ creating hot spots on your load balancers. reply phrotoma 1 hour agorootparentWhat header? Our client was envoy and it's pretty standards compliant, and it just kept trying to use the connection. Edit: I see that it is `connection: close` I wonder if that is new behaviour or if envoy did not honour it at the time we encountered the issue. Thanks for the info! reply temac 1 hour agorootparentprevInstead of closing the connection it sends a message stating that it is closed? Wow. reply jimbobthrowawy 5 hours agoprevThose uploader decides rules are wild. Does that mean someone with a website poorly configured enough can have user content uploaded to, and later served from amazon glacier? (assuming a sufficiently motivated user) reply wmfiv 3 hours agoparentIt does. But if you're concerend about this (and many of the other items mentioned), you can control access to those features using IAM. https://docs.aws.amazon.com/service-authorization/latest/ref... The condition keys specifically are here and you can see keys to control access to storage class, tagging, etc. https://docs.aws.amazon.com/service-authorization/latest/ref... reply swiftcoder 10 hours agoprev> S3 isn’t the only service that works this way, Hosted Cognito UI endpoints do something similar (https://[your-user-pool-domain]/login). Basically every new service that expects to deal with a lot of traffic should work this way (we did this also for AWS IoT). It's a hell of a lot easier to deal with load balancing and request routing if you can segment resources starting at the DNS level... reply nh2 13 hours agoprevHow about the fact that S3 is not suitable for web serving due to high latencies (in standard storage classes)? Many people think you can just host the resources for your websites, such as images or fonts, straight on S3. But that can make for a shitty experience: > applications can achieve consistent small object latencies (and first-byte-out latencies for larger objects) of roughly 100–200 milliseconds. From: https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimi... reply teitoklien 13 hours agoparentMost folks use S3 as a source for AWS cloudfront for serving content You can even use cloudfront signed cookies to give specific users cdn access to only specific content owned by them on S3. How cool is that. reply WatchDog 13 hours agoparentprevTypically you would use cloudfront with S3 if you want to use it for serving web assets. It will cache frequently accessed assets, and in addition to reducing latency may reduce cost quite a bit. reply lulznews 13 hours agoparentprevPretty much everyone knows this … reply weird-eye-issue 5 hours agoparentprevThis has been well known for over a decade reply mcqueenjordan 10 hours agoparentprevs3 is not optimized to directly serve websites, but to durably store and retrieve ~unlimited data. reply metadat 13 hours agoparentprevPut a memcached instance in front and you're good. reply perpil 5 hours agoprevA workaround for some of the limits of presigned urls like not being able to specify a max file size is to front your uploads using CloudFront OAC and CloudFront Functions. It costs more (.02/GB) but you can run a little JavaScript code to validate/augment headers between your user and S3 and you don't need to expose your bucket name. https://speedrun.nobackspacecrew.com/blog/2024/05/22/using-c... reply croes 7 hours agoprev>Things you wish you didn't need to know about S3 >A time travel paradox in the title is a good place to start a blog post, don’t you think? Where is the paradox? reply aao 7 hours agoprevRelated: https://medium.com/@maciej.pocwierz/how-an-empty-s3-bucket-c... reply lotsofspots 7 hours agoparentAbout a week after that went viral - https://aws.amazon.com/about-aws/whats-new/2024/05/amazon-s3... reply anonymouse008 1 hour agoprevI’ve become old. If you look at these things in disgust (ACL vs Policies, Delete bucket with s3:*, etc), you’re missing the point of (deterministic ?) software. It does what it is written to do, faithfully, and error out when not. When it doesn’t do as written or as documented, then yes… go full bore. reply temac 1 hour agoparentThe doc is huge, and the principle of least astonishment is often not respected. Also third parties providers support a random subset of it given the protocol has nothing simple anymore (or maybe never had) reply CaliforniaKarl 12 hours agoprev> S3 buckets are the S3 API > … a relatively small part of the API requires HTTP requests to be sent to generic S3 endpoints (such as s3.us-east-2.amazonaws.com), while the vast majority of requests must be sent to the URL of a target bucket. I believe this is talking about virtual-hosted style and path-style methods for accessing the S3 API. From what I can see [0], at least for the REST API, the entire API works either with virtual-hosted style (where the bucket name is in the host part of the URL) and path-style (where the bucket name is in the path part of the URL). Amazon has been wanting folks to move over to the virtual-hosted style for a long time, but (as of 3+ years ago) the deprecation of path-style has been delayed[1]. This deprecation of path-style requests has been extremely important for products implementing the S3 API. For example… * MinIO uses path-style requests by default, requiring you set a configuration variable[2] (and set up DNS appropriately) to handle the virtual-hosted style. * Wasabi supports both path-style and virtual-hosted style, but \"Wasabi recommends using path-style requests as shown in all examples in this guide (for example, http://s3.wasabisys.com/my-bucket/my-object) because the path-style offers the greatest flexibility in bucket names, avoiding domain name issues.\"[3]. Now here's the really annoying part: The REST API examples show virtual-hosting style, but path style works too! For example, take the GetBucketTagging example. Let's say you have bucket \"karl123456\" in region US-West-2. The example would have you do this: GET /?tagging HTTP/1.1 Host: karl123456.s3.amazonaws.com But instead, you can do this: GET /karl123456?tagging HTTP/1.1 Host: s3.us-west-2.amazonaws.com !! How do I know this? I tried it! I constructed a `curl` command do to the path-style request, and it worked! (I didn't use \"karl123456\", though.) So hopefully that helps resolve at least one of your S3 annoyances :-) [0]: https://docs.aws.amazon.com/AmazonS3/latest/userguide/RESTAP... [1]: https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-... [2]: https://min.io/docs/minio/linux/reference/minio-server/setti... [3]: https://docs.wasabi.com/docs/rest-api-introduction reply xarope 11 hours agoprev> There are only losers in this game, but at least we’ve all got a participation ribbon to comfort us in moments of angst. I sense the frustration... reply prpl 12 hours agoprevNot one thing about different API limits for operations according to key prefixes, and how you need to contact support if you need partitioning and provide them the prefixes, huh? reply willsoon 14 hours agoprevMy wife - she's a future developer, I'm a lawyer - has had enough of me explaining some of these details in the form of concerns. I am somewhat satisfied to know that my perplexities are not imaginary. reply trallnag 7 hours agoparentWhat's a future developer? reply cynicalsecurity 6 hours agorootparentA developer that is learning programming or a specific technology? Isn't it self-explanatory or what problem did you have understanding it? reply krisoft 4 hours agorootparent> Isn't it self-explanatory No. > what problem did you have understanding it? It is simply not a phrase I commonly read. When I google it I find a visa consultancy under the name, and not much else. Curiously your comment is also among the results. The problem is that the phrase \" developer\" is used to describe what the person develops. A \"real-estate developer\" invests in real estate as a business. A \"web developer\" develops web applications, a \"game developer\" develops games, and so on and so on. So reading the word I immediately thought they mean someone who is developing the future? Like idk Douglas Engelbart or Tim Berners-Lee or someone like that. If you want to write that someone is learning to become a developer I would recommend the much less confusing \"developer in training\" phrase, or even better if you just write \"they are learning to become a developer\". reply willsoon 2 hours agorootparentBad English case. It just is. I pay a lot of attention when I write English, but you can always tell when someone isn't a native English speaker in about two words. Je suis désolé. reply krisoft 2 hours agorootparentNo worries! Glad that trallnag asked so we could clear it up. Wishing your wife the best of luck with her career! (and to you too!) reply willsoon 2 hours agorootparentprevWants to be. He reads a lot of code and tries to understand it and write his own. She will probably never be one of you, like and expert. But she's interested, she works laterally in the field - she's done a lot of hours on the intranet at her job. So I think she can be a developer in the future, a good developer. Now she has a degree in the field and... Wait. But then... she is already a developer. I don't know, man. It's a philosophical question. reply tedunangst 13 hours agoprevnext [2 more] [flagged] kthielen 6 hours agoparentA career change, or a job change? reply buggythebug 7 hours agoprevAudi S3? reply gymbeaux 4 hours agoprev [–] No mention of how AWS/S3 approximates the size of a file to save CPU cycles. It used to drive me up a wall seeing S3 show a file size as slightly different than what it was. If I recall correctly, S3 uses 1000 rather than 1024 to convert bytes to KB and MB and GB. This saves CPU cycles but results in “rounding errors” with the file’s reported size. It’s discussed here, although they’re talking about it being “CLI vs console” which may be true? https://stackoverflow.com/questions/57201659/s3-bucket-size-... reply abadpoli 4 hours agoparentThis almost certainly has nothing to do with “saving CPU cycles” and is most likely just that whoever created the Cloudwatch console used the same rounding that is used for all other metrics in CW, rather than the proper calculation for disk size, and it was a small enough issue that it was never caught until it was too late to change it because changing it would disrupt the customers that have gotten used to it. reply presentation 4 hours agorootparentIf anything a bit shift could do the 1024 division faster reply DougBTX 4 hours agoparentprevS3 doesn't format the size for human display, the object's Size is returned in bytes: https://docs.aws.amazon.com/AmazonS3/latest/API/API_Object.h... reply Self-Perfection 3 hours agoparentprev [–] But KB is indeed 1000 bytes, and MB is indeed 1000 KB. In case of 2^10 units the correct names are kibibyte (KiB) and mebibyte (MiB). Check https://en.wikipedia.org/wiki/Mebibyte#Multiple-byte_units Yeah we have long standing confusion that for historical reasons KB and MB often means 2^10 bytes units so now when you see KB you really don't know what it means. Therefore I am a staunch supporter of unambiguous KiB and MiB. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article examines the different encryption methods for AWS S3 buckets, highlighting that S3 encryption acts more like access control than conventional encryption.",
      "Understanding this distinction is crucial for grasping the security implications and correctly utilizing S3 encryption features."
    ],
    "commentSummary": [
      "The debate on Amazon S3's case-sensitive file system reveals differing opinions on its utility and complexity, with some finding it useful for organization and others finding it confusing due to non-case-sensitive systems like Windows.",
      "The discussion highlights the challenges of managing case-sensitive file names across different environments, noting benefits for terminals but less intuitiveness for GUIs, and critiques the handling of case sensitivity in Windows and MacOS.",
      "The conversation also covers practical aspects of AWS S3, such as managing large-scale deletions, costs of incomplete multipart uploads, and best practices for optimizing AWS services like CloudFront for caching and latency reduction."
    ],
    "points": 286,
    "commentCount": 171,
    "retryCount": 0,
    "time": 1717127491
  },
  {
    "id": 40534868,
    "title": "Snowflake Data Breach Exposes 400 Companies, Hacker Demands $20 Million Ransom",
    "originLink": "https://www.hudsonrock.com/blog/snowflake-massive-breach-access-through-infostealer-infection",
    "originBody": "This website uses cookies. Learn More Products CavalierBayonetUse Cases APIFree ToolsContact About Us Partner with UsCompanyBlogBlog Customer Login Schedule a Demo Customer Login Schedule a Demo May 31, 2024 Snowflake, Cloud Storage Giant, Suffers Massive Breach: Hacker Confirms to Hudson Rock Access Through Infostealer Infection Background In this research, we aim to shed light on one of the largest data breaches to date. By directly communicating with the threat actor behind the massive data breach of cloud storage giant, Snowflake, we gained unprecedented insight into the devastating impact of Infostealer infections. The story begins on May 26th, in a Telegram conversation with a threat actor claiming to have hacked two major companies, Ticketmaster and Santander Bank. The data from these companies was put up for sale on the Russian-speaking cybercrime forum, exploit[.]in. Database samples provided by the threat actor led Hudson Rock researchers to believe that the data is genuine. Santander bank data offered for sale on exploit.in In the conversation with Hudson Rock, the threat actor reveals that there is much more to the story than these two breaches, and that additional major companies suffered a similar fate, allegedly including many of Snowflake’s customers which can be found on their website https://www.snowflake.com/en/customers/all-customers/ Further explaining the source of the hack, the threat actor adds that all of these breaches stem from the hack of a single vendor — Snowflake. To understand how the hack was carried out, the threat actor explains that they were able to sign into a Snowflake employee’s ServiceNow account using stolen credentials, thus bypassing OKTA which is located on lift.snowflake.com. Following the infiltration, the threat actor claims that they were able to generate session tokens, which enabled them to exfiltrate massive amounts of data from the company. Method used to hack Snowflake as shared by the threat actor To put it bluntly, a single credential resulted in the exfiltration of potentially hundreds of companies that stored their data using Snowflake, with the threat actor himself suggesting 400 companies are impacted. The goal of the threat actor, as in most cases, was to blackmail Snowflake into buying their own data back for $20,000,000. However it seems the company was not responsive. Further evidence of the hack includes a CSV file that the threat actor shared with Hudson Rock’s researchers, which shows the depth of their access to Snowflake servers. This file documents over 2,000 customer instances relating to Snowflake’s Europe servers. Screenshot of “snowflake_eu-orgadmin.csv shared with Hudson Rock researchers One credential to rule them all Going over the data found in the CSV file, Hudson Rock researchers identified a Snowflake employee who was infected by a Lumma-type Infostealer on October 5th, 2023. Along with other sensitive credentials to Snowflake’s infrastructure, this employee’s login details (adelou) to a specific server (https://sfseeurope-demo_adelou.snowflakecomputing.com) were also compromised. When asked about the specific credentials used to carry out the hack, the threat actor confirmed to Hudson Rock researchers that indeed these are the same credentials they used, and shared a mutual sentiment with us around the absolute ease in which this gigantic hack could have been prevented. It is still undetermined what other companies were impacted by the hack. We expect that this information will be revealed slowly and over time as negotiations with the impacted companies are still ongoing. On may 31st, Snowflake released a statement in which they claim that they are investigating an industry-wide identity-based attacks that have impacted “some” of their customers. Hudson Rock will follow up with updates relating to this hack. Info-stealer infections as a cybercrime trend surged by an incredible 6000% since 2018, positioning them as the primary initial attack vector used by threat actors to infiltrate organizations and execute cyberattacks, including ransomware, data breaches, account overtakes, and corporate espionage. To learn more about how Hudson Rock protects companies from imminent intrusions caused by info-stealer infections of employees, partners, and users, as well as how we enrich existing cybersecurity solutions with our cybercrime intelligence API, please schedule a call with us, here: https://www.hudsonrock.com/schedule-demo We also provide access to various free cybercrime intelligence tools that you can find here: www.hudsonrock.com/free-tools Thanks for reading, Rock Hudson Rock! Follow us on LinkedIn: https://www.linkedin.com/company/hudson-rock Follow us on Twitter: https://www.twitter.com/RockHudsonRock Schedule a DemoAre you Compromised? September 12, 2023 An Avoidable Breach — FBI Hacker Leaks Sensitive Airbus Data A relatively unknown threat actor who goes by the alias “USDoD” posted a thread in which they offered the database of the FBI’s sharing system... August 14, 2023 100,000 Hackers Exposed from Top Cybercrime Forums Hudson Rock' researchers found that a staggering 120,000 infected computers, many of which belong to hackers, had credentials associated with cybercrime forums. July 16, 2023 Prominent Threat Actor Accidentally Infects Own Computer with Info-Stealer Threat actor “La_Citrix” is known for hacking companies — he accidentally infected his own computer and likely ended up selling it without noticing. January 1, 2023 Part 3 of a 3 Part Series in Collaboration with Cyrus: Botnets & Info-Stealers This is the third and final part of a blog post series presented in collaboration with Cyrus in which we dive into botnets & info-stealers. July 14, 2022 Part 2 of a 3 Part Series in Collaboration with Cyrus: Botnets & Info-Stealers In this second of a three part series, presented in collaboration with Cyrus, we dive into botnets & info-stealers. June 7, 2022 Part 1 of a 3 Part Series in Collaboration with Cyrus: Info-Stealers In this first of a three part series is presented in collaboration with Cyrus, we explain \"info-stealers\". June 1, 2022 Hudson Rock Featured on Cybernews! Hudson Rock Selected as One of Cybernews' Best Threat Intelligence Solutions! More posts: September 12, 2023 An Avoidable Breach — FBI Hacker Leaks Sensitive Airbus Data A relatively unknown threat actor who goes by the alias “USDoD” posted a thread in which they offered the database of the FBI’s sharing system... August 14, 2023 100,000 Hackers Exposed from Top Cybercrime Forums Hudson Rock' researchers found that a staggering 120,000 infected computers, many of which belong to hackers, had credentials associated with cybercrime forums. July 16, 2023 Prominent Threat Actor Accidentally Infects Own Computer with Info-Stealer Threat actor “La_Citrix” is known for hacking companies — he accidentally infected his own computer and likely ended up selling it without noticing. January 1, 2023 Part 3 of a 3 Part Series in Collaboration with Cyrus: Botnets & Info-Stealers This is the third and final part of a blog post series presented in collaboration with Cyrus in which we dive into botnets & info-stealers. July 14, 2022 Part 2 of a 3 Part Series in Collaboration with Cyrus: Botnets & Info-Stealers In this second of a three part series, presented in collaboration with Cyrus, we dive into botnets & info-stealers. June 7, 2022 Part 1 of a 3 Part Series in Collaboration with Cyrus: Info-Stealers In this first of a three part series is presented in collaboration with Cyrus, we explain \"info-stealers\". June 1, 2022 Hudson Rock Featured on Cybernews! Hudson Rock Selected as One of Cybernews' Best Threat Intelligence Solutions! In the Press: Users of cybercrime forums often fall victim to info-stealers, researchers find \"After analyzing millions of computers infected with info-stealing malware, researchers at cybersecurity firm Hudson Rock said they identified 120,000 that contained credentials used for logging into cybercrime forums.\" August 16, 2023 Cybersecurity firm links Piers Morgan Twitter hack to leak of 400m records \"Israeli cyber-intelligence firm Hudson Rock appeared to be the first to notice the posting offering the data of 400 million Twitter users, tweeting about the “credible threat” three days ago.\" January 1, 2023 533 million Facebook users' phone numbers and personal data have been leaked online \"A database of that size containing the private information such as phone numbers of a lot of Facebook's users would certainly lead to bad actors taking advantage of the data to perform social-engineering attacks [or] hacking attempts,\" Gal told Insider. April 1, 2021 T-Mobile hack is a return to the roots of cybercrime “In the case of the T-Mobile attack, the alleged hacker said they had different motivations. In a screenshot of a text conversation posted online by Alon Gal, co-founder of cybercrime firm Hudson Rock, the hacker appears to tell Gal that the attack was done “to harm US infrastructure.” March 1, 2021 Hudson Rock ™Terms of ServicePrivacy Statement",
    "commentLink": "https://news.ycombinator.com/item?id=40534868",
    "commentBody": "Snowflake breach: Hacker confirms access through infostealer infection (hudsonrock.com)276 points by zbangrec 5 hours agohidepastfavorite87 comments hangtime79 38 minutes agoI don't work for Snowflake but I spend a lot of time working with them and their SE organisation. When working and building demos with clients, SEs create demonstration environments on the same $400 Snowflake demo accounts anyone can. To build demos the client would grant access to that SE. The SE would take some of the data to the demo environment and then work on it. This is further confirmed by the name of the environment Hudson Rock just published. As far as I can tell, this is a process issue of clients not expiring an ID of someone who they were sharing data with and a threat actor swiping credentials. There is nothing novel about this as there is no exploit. Also congrats Hudson Rock you just outed a person who was taken due to having malware on their computer. This is no different then if you gave a contractor credentials and they had those swiped. Dicks. reply jupp0r 35 minutes agoparentAgreed, mentioning the login name of the compromised account seems really unprofessional and unnecessary. reply BlackjackCF 24 minutes agorootparentThis was a really effective anti-ad for Hudson Rock. reply waihtis 23 minutes agoparentprevThe whole blog post reeks of extreme self-congratulation and youre right, a total scum move to expose the victim. Altogether very weak performance from Hudson Rock. reply aurum 1 hour agoprevThe article doesn't seem very consistent with the headline of \"hundreds of breached customers\" 1. The password for lift/okta is only allowing access to a servicenow portal and not customer accounts, so the refresh token issue seems restricted to the servicenow portal and unrelated to any actual customer data being exposed from customer Snowflake accounts 2. The screenshot with 10 corporate accounts compromised shows 4 different Snowflake account credentials (one of which appears to be a personal demo account) so that might explain up to 3 customers being compromised but there's no details showing other customers being compromised. Assuming all of the SE's credentials were compromised for all of the customers they were working with, we can probably say the total customers compromised would be in the low double digits (each customer account would have had to provision access to the SE individually) Big leap to say that literally the entirety of Snowflake's customer base is compromised from a \"refresh token issue\" (in the internal Okta portal) that isn't even linked to any customer Snowflake account reply shrubble 38 minutes agoparentWithout knowing exactly how the compromised account is set up, and what access is granted, it may be difficult to say. At \"security focused large telecom\" I am aware of, you would be surprised what level of tech has access to what (though of course all access is logged). reply matsur 38 minutes agoparentprevVery possible there were creds etc accessible in Servicenow that could have been used to move laterally from there. Conjecture, obviously. reply skilled 2 hours agoprevAt least based on the wording of the perpetrator, Snowflake really did have the system designed in a way where a single administrator account gives you carte blanche to everything. > On may 31st, Snowflake released a statement in which they claim that they are investigating an industry-wide identity-based attacks that have impacted “some” of their customers. https://community.snowflake.com/s/question/0D5VI00000Emyl00A... This gives credibility to both Ticketmaster and Santander stories. Wow! Could be one of the biggest dumps of all time if the threat actors did everything correctly? reply gregates 2 hours agoparentThat's what the article implies, but I think it's overblown. They provide enough information (unfortunately) to identify the employee whose credentials were stolen, and she's a Sales Engineer. The data seems to have come from her own Snowflake account, which was used to build demos for customers or prospective customers. It's quite possible that those customers granted her access to some of their actual data, which was used in those demos, but it's a far cry from unfettered access to the customer's Snowflake database itself. It's also quite possible that the hacker exfiltrated fake-but-realistic data used for demo purposes and doesn't know the difference. reply hn_throwaway_99 1 hour agorootparent> They provide enough information (unfortunately) to identify the employee whose credentials were stolen, and she's a Sales Engineer. I'm not previously familiar with Hudson Rock, nor how \"standard\" disclosures around this work, but identifying the breached employee felt like an extremely shitty move to me. If a single infected laptop of a sales engineer (i.e. not even an admin with extensive access rights) resulted in a breach this large, the root cause problem is not the sales engineer - and I'd note that Hudson Rock says as much in their article. reply lesuorac 45 minutes agorootparentBut don't you see, we fired the problem so no more worries! Oh, what's that. How did we change our hiring process to avoid hiring a problem again? Sorry, my phones buzzing and I need to go. -- Although obviously yes the problem isn't with hiring, it's with the system where a what should be fairly untrusted device shouldn't be able to exfiltrate a ton of data without setting a flag off somewhere. reply NotEvil 39 minutes agorootparentThe problem isn't the employee or the hiring process. It's the security infrastructure! One compromised account, supposedly from sales, shouldn't bring down the whole company. reply foobiekr 46 minutes agorootparentprevExactly, how is an SE privileged enough to cause a problem? Or for the activities to go unnoticed? Like I would be very humiliated to have a system under my care that had this problem. reply hangtime79 20 minutes agoparentprevThis is the problem of making conjecture or trying to be authoritative when you don't know the process. One SE is working on many accounts. They don't build within the client environment. They set up a demo account like you or I do with the $400 in credits. SEs are constantly starting these. Why? They expire after the fact. The SE builds in the created demo account and shows the client. After 30 days Snowflake locks the account (no credit card) and subsequently drops the demo instance and data. For the to do the work the customer can do one or more of the following: The customer's SF instance shares data to that demo account created by the SE OR the customer has given access to that Snowflake SE through SSO. Either way, this is more of not orgs not being restrictive in their security posture. There is nothing novel about this exploit other than they found an SE who was working very and clients who had not properly scoped the security permissions of an employee/contractoe/guest. reply nattaylor 2 hours agoparentprevIn my experience with Snowflake support about a year ago, an administrator of the customer's account had to explicitly grant access to Snowflake in order for the Snowflake team to see or do anything -- and if I recall correctly the access had an expiry. reply skywhopper 1 hour agorootparentThat’s if they were following procedure. But on these internal systems, there are often hacks around the procedure that folks with the right mindset can easily find. reply tomashertus 2 hours agoparentprevIf the threat actor has played it right, there is a high possibility that this will be the largest data breach in history. reply c0njecture 49 minutes agorootparentThere's no evidence of that at all. The screenshot shows a few Snowflake professional services demo accounts only. These are accounts used by the sales engineer to demo features to customers. It's possible the attacker was able to deduce some information about certain customers, but they would not then be able to connect to those accounts to extract data as those accounts should not be accessible from the public Internet at all, and should require corporate authentication. reply p0seidon 2 hours agorootparentprevSo the account was without 2FA protection? reply holmesworcester 2 hours agoparentprev...All with just one successful malware-as-a-service attack against the right employee. (Lumma was the malware-as-a-service used.) Interestingly, there's another adjacent story on the frontpage about Pegasus being used against NGOs in Eastern Europe. The principle of least authority is important, but also device security is important! https://news.ycombinator.com/item?id=40535912 reply bbayles 2 hours agoprevThe screenshots of the chat logs are really something. This firm claims to be in communication with the actual criminal, and the actual criminal says that using their firm would have helped prevent the breach. I have updated my sense of the firm's trustworthiness accordingly. reply gregates 1 hour agoparentThis is just pure speculation, but it kind of looks like the hacker was being ignored by Snowflake, so they somehow got in touch with Hudson Rock and offered them this promotional opportunity (to break the news, more than the throwaway line in the article) with the goal of retaliating against Snowflake for failing to pay the ransom. And Hudson Rock agreed to play along and hype up the story, presenting it as a bigger breach than it really was. One wonders whether Hudson Rock was the first they went to, or just the first to take them up on the offer. reply bbayles 32 minutes agorootparentIt's also possible that the firm is being trolled by the \"threat actor.\" reply deinonychus 1 hour agoparentprevThat particular exchange is bizarre and cartoonish. I don’t know what to make of it. “should have bought protection from Hudson Rock could have saved them this one” “yes i agree it wouldve helped for sure” reply JohnMakin 1 hour agorootparentseems like a shameless marketing plug to me reply NeptuneSolo 30 minutes agorootparentIt did seem very cringe worthy reply chairmanwow1 1 hour agoparentprevin that you trust them less? reply itscrush 1 hour agorootparentAbsolutely the case for me. I don't give Snowflake much here, but Hudson Rock sells this exact type of \"protection\" and so far including BBC, no other independent verification? This from the GP's link does it: “should have bought protection from Hudson Rock could have saved them this one” reply skilled 1 hour agorootparentWe should thread carefully on this one. It might be that they genuinely geeked out. Hudson reputation would forever be scarred (badly) if they tried to manipulate the narrative. Going down this hole also means we discredit the perpetrator, even if he did specifically reach out to Hudson. Just wanted to say this so we don’t immediately jump to conclusions. reply yodon 2 hours agoprevFrom the official Snowflake response[0]: >We believe this is the result of ongoing industry-wide, identity-based attacks with the intent to obtain customer data. Research indicates that these types of attacks are performed with our customers’ user credentials that were exposed through unrelated cyber threat activity. To date, we do not believe this activity is caused by any vulnerability, misconfiguration, or malicious activity within the Snowflake product. [0]https://community.snowflake.com/s/question/0D5VI00000Emyl00A... reply rdtsc 1 hour agoprevSnowflake says it's not their fault, it's customer's fault apparently: https://community.snowflake.com/s/question/0D5VI00000Emyl00A... If https://www.hudsonrock.com/blog/snowflake-massive-breach-acc... has not completely fabricated the story, Snowflake are tiny bit less than truthful. > Research indicates that these types of attacks are performed with our customers’ user credentials that were exposed through unrelated cyber threat activity. To date, we do not believe this activity is caused by any vulnerability, misconfiguration, or malicious activity within the Snowflake product. Throughout the course of our ongoing investigation, we have promptly informed the limited number of customers who we believe may have been impacted. They are way too wishy-washy: * \"Research indicates\": Their own research or just general security research out there. * \"...do not believe this activity is caused by any vulnerability, misconfiguration, or malicious activity within the Snowflake product\": I think they know exactly how it happened. They enumerate all possible scenarios and methods it didn't happen: misconfiguration, vulnerability, malicious activity within the product. But skillfully skip the explanation for how it actually happened. They were contacted by the bad actor if hudsonrock is to be believed, so they probably had a good idea how it happened. reply mahogany 51 minutes agoparentDon’t they hint at how it happened in what you quoted? > performed with our customers’ user credentials that were exposed through unrelated cyber threat activity Unless they are just making this up, they seem to think that credentials were obtained elsewhere. They also say at the end that they told customers to review their account settings. So they are directing the blame away from themselves. You’re right that this seems to conflict with Hudson Rock. Unfortunately our sources are the company that allegedly was hacked and a company that shamelessly doxed an employee in the course of using this event to promote their product. I think we’ll need to wait for more details. reply rdtsc 43 minutes agorootparent> You’re right that this seems to conflict with Hudson Rock. Unfortunately our sources are the company that allegedly was hacked and a company that shamelessly doxed an employee in the course of using this event to promote their product. I think we’ll need to wait for more details. Fair point. Doxing the employee is shitty, no doubt. Makes Hudson Rock look bad as well. But at the same time, I was giving them some credibility as it would seems if they completely fabricated the screenshots, they might as well file for bankruptcy, given the market they seem to operate in. reply ActionHank 1 hour agoparentprevOr they had no idea and assumed that customers had a breach, so blamed them without doing a proper in-depth analysis. The hacker alleges that they weren't even expiring refresh tokens, that is pretty huge if true, it's just a massive, glaring issue. reply rdtsc 50 minutes agorootparentI think it's unlikely: * If Hudson Rock is to be believed, with the chat screenshot, Snowflake was notified and asked for a ransom. It would seem odd that all their 400 customers were all hacked randomly at the same time, by only one hacker group just based on those customers' own bad credentials * They enumerate all the possible ways they were not hacked, and seems to skips the exact one way they were hacked. It's like saying: \"It wasn't A, C, D, E, or F\" as the causes. Hmm, they skipped B it seems, I wonder why... reply NameError 4 hours agoprevThis article is claiming that the Ticketmaster breach from a few days ago was actually a much broader hack affecting 400+ companies, all through a Snowflake employee's stolen credentials. This seems like a pretty big story that's only being reported on hudsonrock.com now. I haven't heard of Hudson Rock before, does anyone know if they are a reputable source? reply proactivesvcs 3 hours agoparentBBC News report of a substantial hack of Santander bank; linked to Snowflake. https://www.bbc.co.uk/news/articles/c6ppv06e3n8o reply dralley 2 hours agorootparentBBC just linked back to Hudson Rock's allegation FWIW, they don't have any independent confirmation reply WhackyIdeas 1 hour agorootparentprevGreat, so these companies do not give a flying fuck about their customer data in making sure the data stored at cloud storage companies are end to end encrypted. To think these random cloud storage companies can access your bank information is utterly shocking. reply squigz 1 hour agorootparent> To think these random cloud storage companies can access your bank information is utterly shocking. Honestly this sort of thing shouldn't be shocking at all. reply oasisbob 57 minutes agorootparentNot surprised at all. Doesn't even depend on cloud vendors - I'm thinking back to the 2023 MOVEit vulnerability which resulted in the release of a ton of customer info from banks' own internal infrastructure. reply halfcat 56 minutes agorootparentprevCan a tool like Snowflake work if it doesn’t have access to the unencrypted data? reply orthecreedence 34 minutes agorootparentNo. E2E encryption doesn't really apply here. reply richbell 2 hours agoparentprev> I haven't heard of Hudson Rock before, does anyone know if they are a reputable source? I first learned of Hudson Rock after their \"CEO\" started spamming every security-related subreddit with low-effort blogspam over a period of months alleging numerous breaches. They've had several accounts banned, both by Reddit moderators and administrators. Personally, I would no consider them a reputable or reliable source. reply ransom1538 2 hours agoparentprevSnowflake employees need time to sell off all their shares. This news will hurt the SNOW stock price big. reply c0njecture 53 minutes agoprevSnowflake internal staff do not have access to read customer data, unless a customer grants it. Customers can use their own KMS to generate table keys. Snowflake has a lot of security features. But still, customers may well misconfigure their own Snowflake accounts and therefore be vulnerable. A well configured Snowflake account: - does not allow any access from the public Internet. Network policies set by the customer should restrict access to corporate networks only. - does not allow authentication unless with MFA or via corporate IDP / SAML - has dynamic masking / tokenisation Snowflake seems to have most of the Fortune 500 as customers. If Snowflake itself was somehow penetrated and all controls circumvented, it would certainly be huge and you'd be reading about a lot more than Santander and Ticketmaster. At this point it seems more like the \"AWS Hack\" that affected CapOne back in the day (that was CapOne's fault, not AWS!). reply foobiekr 45 minutes agoparent\"Snowflake internal staff do not have access to read customer data\" Do engineers in Snowflake have access to production systems? reply abadpoli 2 hours agoprevWhy in the world would obtaining a Snowflake employee’s credentials allow you to then obtain Snowflake’s customers’ data? Doesn’t this imply that people working at Snowflake can see all of the data that I put in it? Admittedly I don’t have much experience with Snowflake, but as a baseline I expect better from a “cloud storage giant”. reply i_k_k 2 hours agoparentThere’s something missing here. From what the Hudson Rock article shows, they were able to use an SE’s creds to access their demo account. This is not a customer account and shouldn’t (but of course could) contain sensitive info. It’s not clear to me how this snowballed into a larger breach. Perhaps customers had granted this SE access to their accounts and the data within. Or perhaps there’s a deeper hack. But this isn’t clear to me from what I’ve read. reply gregates 2 hours agorootparentI was just going to post the same thing. The files that they show in the screenshots are things like PROGRESSIVE_BID_CHANGE_202405271129.csv. Looks suspiciously like the Snowflake Sales Engineer's data for their job role closing a deal with Progressive, not Progressive's own data. And there's no reason to think that a SE would have broad access to customer data. There may be some overlap, but I doubt it contains sensitive customer data owned by Progressive. reply coredog64 1 hour agorootparentprevSomething similar happened at a previous employer. Contractor was hired to do a big data PoC, and they managed to cajole access to a prod data dump for a more impactful demo. They then managed to load all this PII data into an ElasticSearch instance that was open to the internet and was discovered by threat actors. I wouldn’t be surprised to find that something similar happened here, where an unscrubbed prod dataset was shared for a better demo. reply p0seidon 2 hours agorootparentprevThere may have been administrative access that was not properly secured. reply FromOmelas 2 hours agoparentprevNo, that sounds about right. This is a new, agile, cloud-first company that grew very quickly and has faced significant turnover. You don't get such growth by doing everything right. Looking at linked-in, the unlucky employee could be someone in a sales role, with only 7 months of tenure. Every company has a few sysadmins with a scary amount of reach, but that's not what happened here. Edit: A ServiceNow access request flow with poor internal controls would explain it. reply DowagerDave 2 hours agorootparent>> This is a new, agile, cloud-first company that grew very quickly and has faced significant turnover. This is not really true of Snowflake, which is not some 2-person YOLO startup, and it's also pretty irrelevant as the weakest link is often a single employee regardless of the size or industry of the company. In my experience the support and security is way better than average - example: as a client of both Snowflake and Sisense, Snowflake reached out to me about the Sisense breach before Sisense did. reply FromOmelas 1 hour agorootparentIts support and security posture could very well be better than average. Looking a other breaches (Qlik Attunity, Microsoft AAD, ...) indicates that being better then average is not enough if you're a sufficiently attractive target. reply c0njecture 42 minutes agorootparentprevIt's not a new tiny company. It's about 12 years old with 7000 employees. They know they are dead if they are not hot on security, so at the moment I would take this story with a big pinch of salt. Quite possible certain customer configurations have been attacked, but that is a different thing. reply Keyframe 2 hours agorootparentprev(new) sales person with an uber account that has access to carte blanche customer data. This is not only a disaster, if true, but also violates probably every certification under the sun, if they had any at all. Reminder Snowflake is a couple of sales persons from Oracle and a techie. reply FromOmelas 1 hour agorootparentI'm not sure it does, perhaps it violates the spirit but not the letter. You need a way to give your employees access to customer data; for support cases. So you build a \"request access\" form in your ITSM. Now you can tick off every box related to certification: There is a process. Only authorized persons have access. Every aspect of it can be audited. Later, perhaps sales people (the 1000's of new joiners) start using it as well for lead generation. It's a lot easier to sell if you know how your product is used by other companies in the same industry. Much later, someone's account is compromised, makes the same requests and it gets waved through. Why wouldn't it ? It is a valid request made by a current employee of the company. What other criteria would apply ? This is not a bank. reply bpicolo 1 hour agorootparent> What other criteria would apply? Many companies have processes that require 2 or more humans in the loop for sensitive prod data. reply BlackjackCF 2 hours agoprevWas their intent to dox the employee while discussing this beach? They show the employee’s username, which is easily Googleable. reply boringg 1 hour agoparentYeah that seems super sus to me as well. Super unprofessional. reply owlninja 1 hour agorootparentAs is the plug of 'should have bought protection from hudson rock' reply briffle 1 hour agoparentprevBut hey, they blot out the name of the alleged attacker in chat, because of privacy... reply WalterSobchak 2 hours agoprevIOCs: https://community.snowflake.com/s/article/Communication-ID-0... reply redwood 1 hour agoprevIt's unclear if it's customer metadata or real customer data data? reply siliconc0w 2 hours agoprevProtecting customer data from compromised insiders can be pretty hard. They often need the access to do their jobs. Still, in this case it's was far too easy - just one session cookie and a password shouldn't itself by sufficient to compromise all your customers. reply BillFranklin 2 hours agoprevIt sounds like they found a way to bypass MFA on snowflake (because snowflake didn’t expire session cookies), and stole an employees credential, obtained via a “Lumma-type Infostealer” which I guess is just a key logger in browser extensions and fake versions of software… reply DowagerDave 2 hours agoparentsomething doesn't add up, because I don't see how this extrapolates from stealing privileged Snowflake employee credentials. How does that become a keylogger on a client's computer? reply BillFranklin 1 hour agorootparentYeah it is a bit muddled honestly. I had to read it a couple times and I still don’t completely get what happened: 1. Employee installs a key logger 2. Snowflake does not expire session cookies 3. Malware steals their session cookie and password, so can bypass employee MFA/okta 4. ??? 5. Somehow this one employee has admin access to 4000 snowflake instances reply beretguy 5 hours agoprev> The data from these companies was put up for sale on the Russian-speaking cybercrime forum Just russia being russia, as usual. reply cabirum 1 hour agoparentThe screenshot shows the post in English. The website domain is Indian. The seller contact xmpp.cn in China. But no, we'll keep blaming Russia for everything. reply WhackyIdeas 1 hour agorootparentI agree. Every single security thing that happens is put on Russia. Typical propaganda. If we were pre-Ukraine, it’d be China getting all the blame (like it used to be). I’m not pro-Russia, I’m just anti-bullshit. reply dralley 1 hour agorootparent>I agree. Every single security thing that happens is put on Russia. This is transparently false. Both Russia and China have copped blame for various recent attacks. And the fact of the matter is that Russian hackers are extremely active at the moment, and were even before 2022. See for example: https://www.wired.com/story/notpetya-cyberattack-ukraine-rus... >If we were pre-Ukraine, it’d be China getting all the blame (like it used to be). Russia was behind the Solarwinds hack, which is the most widely covered one in the past decade. Ironically the Chinese were also independently exploiting Solarwinds to break into government agencies, but that didn't get much attention. reply WhackyIdeas 1 hour agorootparentWhen they don’t know who did something, they say Russia. I suppose it makes sense to say that I live in the UK. Depending on where you are in the world, you may be hearing blame on Russia passed a lot less. But in the UK, it is constant. It is tiresome. Regarding how active Russian hackers are, I bet they are not any more active than GCHQ, NSA etc. they are all at it. God, even Israel will sell their Pegasus to any dictator who will pay them the money. reply senderista 41 minutes agorootparentprevHave we already forgotten about all the cybercrime originating from Ukraine? reply Booleans 56 minutes agorootparentprevThere’s a reason a lot of malware won’t install on a user’s computer if it detects they’re using a Russian keyboard. reply mc32 2 hours agoparentprevPeople in ex-USSR also speak/write Russian, so while I have no doubt Russians frequent the forum, lots of others from their sphere of influence also visit that forum. Don't assume everyone on HN is a 5-eyes resident. reply KoftaBob 2 hours agoparentprevYour daily evidence that modern Russia is essentially just an organized crime ring with oil reserves and nukes. reply cabirum 1 hour agorootparentYes, any country that is not your ally or vassal is an \"organized crime ring\". It's safer to be with oil and nukes, than without them you know. reply shrubble 31 minutes agorootparentprevI don't think naming things that apply to USA also, is quite the win you think it is. reply pizzafeelsright 1 hour agorootparentprevThat's just \"government\" reply teej 1 hour agoprevThe query that Snowflake provides to identify privileged users is wrong. I wrote a better version here: https://gist.github.com/titan-teej/924dcb42604a98b90d6419262... TL;DR they don't check for users who can transitively gain admin access. reply RcouF1uZ4gsC 2 hours agoprevFrom their marketing page: > Snowflake’s single platform eliminates data silos I guess so, especially now. reply jupp0r 1 hour agoprevIt's surprising that SnowFlake didn't pay the $20M ransom. Seems like a no-brainer compared to the reputational damage this would cause. reply Bluestein 2 hours agoprev> To put it bluntly, a single credential resulted in the exfiltration of potentially hundreds of companies that stored their data using Snowflake, with the threat actor himself suggesting 400 companies are impacted You only have to \"fail\" once, as they say.- reply DebtDeflation 1 hour agoprevImagine having your enterprise data warehouse stolen. reply philipwhiuk 48 minutes agoprev [–] How did Snowflake not have 2FA? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "On May 31, 2024, Hudson Rock reported a significant data breach at Snowflake, a major cloud storage company, due to an Infostealer infection.",
      "The breach originated from stolen credentials of a Snowflake employee's ServiceNow account, leading to data exfiltration from potentially 400 companies and a $20 million blackmail attempt.",
      "Hudson Rock confirmed the breach's authenticity and emphasized that it could have been easily prevented, highlighting the increasing threat of Infostealer infections in cybercrime."
    ],
    "commentSummary": [
      "A hacker accessed Snowflake's demo environment using stolen credentials from a compromised Sales Engineer's computer, exploiting a process issue with non-expiring shared credentials.",
      "The breach did not involve actual customer accounts, countering claims of \"hundreds of breached customers,\" and highlights the importance of proper credential management and device security.",
      "The incident has sparked debates about Snowflake's security practices, the credibility of the reporting source Hudson Rock, and the broader implications of identity-based attacks in the tech industry."
    ],
    "points": 276,
    "commentCount": 88,
    "retryCount": 0,
    "time": 1717162088
  },
  {
    "id": 40529355,
    "title": "Energy-Efficient 1-Bit Language Models: Smaller, Faster, and Nearly as Accurate",
    "originLink": "https://spectrum.ieee.org/1-bit-llm",
    "originBody": "ARTIFICIAL INTELLIGENCE NEWS 1-bit LLMs Could Solve AI’s Energy Demands “Imprecise” language models are smaller, speedier—and nearly as accurate MATTHEW HUTSON30 MAY 20243 MIN READ GETTY IMAGES",
    "commentLink": "https://news.ycombinator.com/item?id=40529355",
    "commentBody": "“Imprecise” language models are smaller, speedier, and nearly as accurate (ieee.org)262 points by jnord 17 hours agohidepastfavorite138 comments mlsu 14 hours agoI don't believe that quantization comes for free. Someone made the observation that llama3 models quantize \"worse\" than llama2 models do -- they suffer in quality from quantization far more. My intuition is that a model which is undertrained suffers less from quantization, because the training process has not utilized each weight to its full potential. One of the key findings with llama, and why it punches above its weight for its size, is that they trained it for longer on a much larger dataset then was \"optimal\" according to the literature up to that point. Putting two and two together, it seems that: small model, lots of data, long training > large model + quantization That basically, quantization is a lossy shortcut to the tail of training long. Amount and quality of data is, as always, the most important part about all of this. reply kir-gadjello 13 hours agoparentWhile llama3-8b might be slightly more brittle under quantization, llama3-70b really surprised myself and others[1] in how well it performs even in the 2..3 bits per parameter regime. It requires one of the most advanced quantization methods (IQ2_XS specifically) to work, but the reward is a SoTA LLM that fits on one 4090 GPU with 8K context (KV-cache uncompressed btw) and allows for advanced usecases such as powering the agent engine I'm working on: https://github.com/kir-gadjello/picoagent-rnd For me it completely replaced strong models such as Mixtral-8x7B and DeepSeek-Coder-Instruct-33B. 1. https://www.reddit.com/r/LocalLLaMA/comments/1cst400/result_... reply d13 12 hours agorootparentHow does it compare against unequalised Llama 3 8B at 16fp? I’ve been using that locally and it’s almost replaced GPT4 for me. Runs in about 14GB of VRAM. reply iwontberude 4 hours agorootparentllama3 is nowhere near gpt4, though it is cool reply endofreach 2 hours agorootparentprev> surprised myself and others[1] in how well it performs even in the 2..3 bits per parameter regime I am too dumb for all of this ML stuff. Can you explain what exactly that means & why it's surprising? reply m1el 1 hour agorootparentArtificial neural networks work the following way: you have a bunch of “neurons” which have inputs and an output. Neuron’s inputs have weights associated with them, the larger the weight, the more influence the input has on the neuron. These weights need to be represented in our computers somehow, usually people use IEEE754 floating point numbers. But these numbers take a lot of space (32 or 16 bits). So one approach people have invented is to use more compact representation of these weights (10, 8, down to 2 bits). This process is called quantisation. Having a smaller representation makes running the model faster because models are currently limited by memory bandwidth (how long it takes to read weights from memory), going from 32 bits to 2 bits potentially leads to 16x speed up. The surprising part is that the models still produce decent results, even when a lot of information from the weights was “thrown away”. reply renewiltord 3 hours agorootparentprevHoly wow. Thank you for this. Very cool. I’ve been using 8b for things it might be worth using 70b for. reply acchow 9 hours agoparentprevQuantization isn’t a shortcut. It’s a test. More training leads to better compression. Trimming precision off the integers reveals that there was more compression that could be had. reply Xcelerate 6 hours agoparentprevHas anyone attempted to compress network size further by extracting the symmetry invariants of the network—e.g., those that correspond to the permutation invariance of node shufflings that leave the DAG unchanged? I did a rough calculation, and as the precision of the scalar weights decreases, the information content of the specific network permutation becomes a much higher percentage of its overall size. Depending on the particular neural network architecture, there may be other symmetries beside the symmetric group that also represent compressible redundancies. reply renonce 4 hours agorootparentFor a 4096x4096 matrix its symmetry group has size 4096!. The Stirling’s approximation gives ln(4096!)=4096ln(4096)-4096 which is about 10~11 bits per 4096 numbers. This is less than 0.003 bit per parameter saved. reply segmondy 6 hours agoparentprevdepends on how far. Q8's are pretty much on par with f16/f32 weights. Q5/Q6's if they drop off barely drop below 2%. folks are running Q2, Q3, etc due to being GPU poor, of course it's going to be terrible. When you want to run a 70B model that's originally 140gb and all you have is 16gb of vram. reply raverbashing 9 hours agoparentprevQuantization might need a 'relaxing' or 'normalizing' step before to improve results, it would be an interesting research area Quantize each block or vector as a unit, bound to some metric instead of simply quantizing the whole thing all at once. It's an interesting topic https://old.reddit.com/r/LocalLLaMA/comments/1ba55rj/overvie... reply 13alvone 12 hours agoprevI've been thinking about how far we've come with large language models (LLMs) and the challenge of making them almost perfect. It feels a lot like trying to get a spaceship to travel at the speed of light. We’ve made impressive progress, getting these models to be quite accurate. But pushing from 90% to 99.9999999% accuracy? That takes an insane amount of data and computing power. It's like needing exponentially more energy as you get closer to light speed. And just like we can’t actually reach the speed of light, there might be a practical limit to how accurate LLMs can get. Language is incredibly complex and full of ambiguities. The closer we aim for perfection, the harder it becomes. Each tiny improvement requires significantly more resources, and the gains become marginal. To get LLMs to near-perfect accuracy, we'd need an infinite amount of data and computing power, which isn't feasible. So while LLMs are amazing and have come a long way, getting them to be nearly perfect is probably impossible—like reaching the speed of light. Regardless I hope to appreciate the progress we've made but also be realistic about the challenges ahead. What do you think? Is this a fair analogy? reply kreyenborgi 10 hours agoparent\"Data\" isn't an inexhaustible resource, and also isn't fungible in the way energy is. Of the thousands of languages in the world, a fair chunk don't even have writing systems, and some have very few speakers left. Many are lost forever. Now ask the best llm trained on \"all the data\" to translate some fragment of some isolate language not in its training set and not very related to existing languages. You can't improve on that task by adding more sentences in English or by combining with learning on other modalities. reply OmegaPoint 8 hours agorootparent> \"Data\" isn't an inexhaustible resource Synthetic data are the answers. For example see Tiny Stories dataset (https://arxiv.org/abs/2305.07759). > Now ask the best LLM trained on \"all the data\" to translate some fragment of some isolate language not in its training set and not very related to existing languages. If you give them the dictionary and grammar book as in-context instructions, it can do pretty well. “Gemini v1.5 learns to translate from English to Kalamang purely in context, following a full linguistic manual at inference time. Kalamang is a language spoken by fewer than 200 speakers in western New Guinea. Gemini has never seen this language during training and is only provided with 500 pages of linguistic documentation, a dictionary, and ~400 parallel sentences in context. It basically acquires a sophisticated new skill in the neural activations, instead of gradient finetuning.” reply CaptainOfCoit 8 hours agorootparent> Synthetic data are the answers. For example see Tiny Stories dataset (https://arxiv.org/abs/2305.07759). Synthetic data might be the answer if you're fine with any data, but I haven't came across many synthetic datasets that are of high quality, and if you want high quality output from a LLM, I'm not sure Tiny Stories et al can provide that. Here is just one example from Tiny Stories (https://huggingface.co/datasets/roneneldan/TinyStories/viewe...): > Once, there was a girl who wanted to write a story. She thought and thought about what she could write about. She felt it was too boring to just write about trees and flowers. Suddenly, an idea came to her. She decided to write about her waist. She started to write about how her waist was round, and how it jiggled when she danced. Her story was so fun and exciting! She wrote about how she liked to put a belt around her waist and how it made her feel smarter. She even wrote a rhyme about her waist: \"My waist is round and jiggly, And when I dance, it's so wiggly.\" The girl was so proud of the story she wrote. She was no longer bored - writing about her waist was much more fun! Hardly high quality \"story\", and an LLM training on data like that won't have high quality output no matter how much you train it. Edit: Another example from Tiny Stories, just because how fun they end up being: > One day, a little boy named Jack was playing in his room. He decided to go and sit on his favourite chest. When he sat down, he noticed something unusual. The chest smelled smelly! Jack had never noticed a smelly smell before and he couldn't work out what it was. Jack's Mum heard him say 'That chest smells smelly', so she came into his room to see what was happening. When she saw the chest, she knew what was wrong. Jack's little puppy had been using the chest as a bed! His Mum scooped the naughty puppy up in her arms and took him outside. When the puppy was outside, the smelly smell went away. Jack was so relieved! He sat back down on the chest, and said 'Ahhh, much better!' Do people really expect to be able to train on this and get high quality output? \"Garbage in, garbage out\", or however that goes... reply HeatrayEnjoyer 5 hours agorootparentA smelly smell that smells... smelly. reply emporas 4 hours agorootparentIt's grammatically correct. Correct grammar despite of it being semantically nonsense, still not defined how small it can get. GPT-2's grammar was atrocious. reply torginus 11 hours agoparentprevBut it still might be worth it. A 90% accurate model will only successfully complete a task consisting of 10 subtasks 0.9^10 = 35%of the time, while a 99% will do so 90% of the time making the former useless, but the latter quite useful. reply CuriouslyC 7 hours agorootparentYes, but a 90% accurate model that's 10x faster than a 99% can be run 3x to achieve higher accuracy while still outperforming the 99% model, for most things. In order for the math to be in the big model's favor there would need to be problems that it could solve >90% of the time where the smaller model wasI'm surprised the comments here are so negative / cynical. And by \"the comments\" I'm assuming you mean the top comment [0] (and maybe its replies)? The rest don't really come off as negative at all, and you quoted directly from that top comment. FWIW, I don't find that comment either negative or cynical. It starts out with that sentence you quoted, but it goes on to make a very interesting point about quantization most likely working best for models which are undertrained—models which store less information than their architecture size would suggest. That's a very valid point that I found insightful and interesting, not cynical. [0] https://news.ycombinator.com/item?id=40531638 reply az09mugen 8 hours agoparentprevI totally agree with your statement. I consider LLMs imprecise in any case, it's not a perfect/exact science, just statistics. I only use LLMs for tasks where an error margin can be allowed. In that perspective I'm totally fine with using a 5 MB LLM like this one : https://neuml.hashnode.dev/train-a-language-model-from-scrat... reply jononor 3 hours agoparentprevSeveral years of ML research for CNNs indicate at least that one can do very well with 8 bit integers. Such quantization is basically standard now for any deployment outside of GPU (where 8bit isn't any faster anyway due to the hardware). reply k__ 7 hours agoparentprevlol, just like with humans. reply JoeAltmaier 2 hours agoprevLots of folks have the intuition that, 'slightly worse' LLM models means unacceptable rates of nonsense answers. What's an acceptable rate? Is it 99%? 99.9%? The closer it gets to 99.999% good answers, the more damaging the wrong ones become. Because people have been trained, too. Trained to trust the answers, which makes them lazy and vulnerable to lies. reply Decabytes 7 hours agoprevI feel like we are getting closer and closer to a finding the Goldilocks Llm, that with some smarter training and the right set of parameters, will get us close to got 3.5 turbo performance but at a size, cost, and time effort that is significantly lower and that is runnable locally. Combine that with what seems like every chip adding a neural engine and it feels like we are in the early days of high performance graphics again. Right now we are in the unreal engine voodoo era, where graphics cards/neural engines are expensive/rare. But give it a few generations and soon we can assume that even standard computers will have pretty decent NPUs and developers will be able to rely on that for models reply djeastm 6 hours agoparent>3.5 turbo performance Is this the level of performance people are relying upon? While I've always been impressed with the technology itself, it's only starting with GPT 4 that I think it approaches adequate performance. reply Decabytes 3 hours agorootparentFor the work that I do (which is mostly rag with a little bit of content generation) GPT 3.5-turbo-0125 with a 16k context window is the sweet spot for me. I started using the api when it was only a 4k context window, so the extra breathing room provided by the 16k context window feels cavernous. Plus the fact that it's $0.50 per 1Million tokens means that I can augment my software with LLM capabilities at a cost that is attractive to me as a small time developer. The way I rationalize it is that using 3.5-turbo is like programming on an 8-bit computer with Kilobytes of Ram, and gpt-4o is like programming on a 64bit computer with a 4080 ti and 32gb of ram. If I can make things work on the 8-bit system, they will work nicely on the more powerful system. reply moffkalast 7 hours agoparentprev3.5-turbo performance wasn't very good though, and according to API statistic analysis it's a Nx7B model so it's already rather small. Ultimately Llama-3-8B is already better in all measurable metrics except multilingual translation, but that's not saying much. reply segmondy 6 hours agoparentprevyeah, it's called phi-3-medium-4k-instruct https://huggingface.co/microsoft/Phi-3-medium-4k-instruct reply moffkalast 6 hours agorootparentIt's not called anything until the lmsys leaderboard ranks it. Microsoft's blatant benchmark overfitting on Phi-2 makes for very little trust in what they say about performance. As a man once said, fool me once, shame on you, fool me twice-can't get fooled again. reply gslin 32 minutes agoprevThis makes me watch https://en.wikipedia.org/wiki/11001001 again. reply bunderbunder 4 hours agoprevThey keep calling them 1-bit LLMs, but they're really 1-trit LLMs. If you can have 3 states, it's not a BInary uniT, it's a TRInary uniT. I don't think that this is just a nit, it implies a real mismatch between how these models work, and how modern computing hardware works. People have built ternary computers in the past, but to the best of my knowledge nobody's made a serious attempt at it in half a century. You can always use two bits to store a ternary value, of course. But then it wouldn't be a 1-bit LLM, now, would it? And that doubling in compute resources required would make a tangible difference in how one has to think about potential efficiency improvements. Also, ternary tritwise logic implemented on binary hardware is unlikely to be anywhere near as efficient as binary bitwise logic on hardware that was specifically engineered for the purpose. This leaves me thinking that the research teams involved continuing to refer to these as 1-bit LLMs must be interpreted as knowingly engaging in dishonest marketing tactics. reply orlp 3 hours agoparentPerhaps in the future we will see MsoTriState neural networks: https://learn.microsoft.com/en-us/dotnet/api/microsoft.offic... For those unaware, the MsoTriState is fairly self-explanatory: it is a tri-state boolean type with five possible values. reply corysama 2 hours agoparentprevNot long ago I managed to get someone working on sub-1-bit models to come out of the woodwork. https://news.ycombinator.com/item?id=39865855 reply IanCal 3 hours agoparentprevI don't think it's quite so clear cut, and the papers and names are often a bit more precise anyway. BitNet is (I think) actually one bit per parameter. BitNet 1.58b isn't, but then to be fair isn't describing itself as a 1 bit llm. I'm less sure but it seems OneBit is 1 bit. One of the processes mentioned in the article is a mix of one and two bits for the weights. reply freeone3000 3 hours agoparentprevIt’s stored in and calculated using a floating-point value, and uses log(2) bits. So it’s ~1.2 bits with float packing over 32m parameters. reply neolefty 2 hours agoparentprevThe article describes both. The 1,0,-1 networks are \"1.58 bits\" — that is, log2(3). But yeah it mostly focuses on 1-bit networks. reply jandrese 16 hours agoprevI'm starting to feel like the Thinking Machines CM-1 was 40 years ahead of its time. reply chuckadams 16 hours agoparentIt had the most wonderful blinkenlights too! :) reply deepfriedchokes 15 hours agorootparentOne of my favorites, CM-1 t-shirt: https://www.tamikothiel.com/cm/cm-tshirt.html reply celltalk 3 hours agoprevYou all run on 2-bit (DNA) and you all seem pretty stable to me. You run your brain with couple of watts per day and add big numbers without much hassle. reply neolefty 2 hours agoparentDNA could also be considered 6-bit, since codons are 3 base pairs: https://www.genome.gov/genetics-glossary/Codon reply aussieguy1234 16 hours agoprevWhen it needs to do more precise numerical calculations, perhaps, like a human, it could just use a calculator? reply kromem 16 hours agoparentThat's not really a concern. If you have a trillion parameter 8-bit fp network or a trillion parameter 1.5-bit ternary network, based on the scaling in Microsoft's paper the latter will actually perform better. A lot of the current thinking is that the nodes themselves act as superpositions for a virtualized network in a multidimensional vector space, so precision is fairly arbitrary for the base nodes and it may be that constraining the individual node values actually allows for a less fuzzy virtualized network by the end of the training. You could still have a very precise 'calculator' feature in the virtual space no matter the underlying parameter precision, and because each parameter is being informed by overlapping virtual features, may even have less unexpected errors and issues with lower precision nodes. reply xwolfi 14 hours agorootparentYup, your response makes me think they should just use a calculator, like everyone. reply imtringued 12 hours agorootparentI don't know what you mean. They already use the GPU as a calculator. reply ben_w 11 hours agorootparentI believe you four are talking about different things; the models are executed on very good \"calculators\" (if you want to call the GPUs that), but themselves are not very good at being used as calculators. LLMs are sufficiently good hammers that people see everything as a nail, then talk about how bad they are at driving screws. reply CuriouslyC 7 hours agoparentprevPeople are trying to make these monolithic god models right now because everyone's chasing OpenAI's example with ChatGPT, but that approach is going to run out of steam for a lot of reasons. We're eventually going to end up with an agent model that does just what you say, recognizes specialized problem types such as math and calls the appropriate tool, such as a calculator or symbolic computation engine. reply meroes 3 hours agorootparentHmm. And agent model for prompts we will ask K-12 math questions to? What are ultimately math questions often take lots of context to trudge through. It takes a solid core of natural language as well. I think there is always the need for a god model, because we want to speak in natural language as that’s easiest for us. I’m not a theoretician nor scientist, but train models on math and logical reasoning. Some very simple word problems contains tons of context, like family trees, implicit information, etc. and the process of step by step reasoning, not just the final answer, requires even more natural language processing. reply CuriouslyC 2 hours agorootparentYou don't need a god model to do that. You need a model that understands natural language really well and is good at identifying/extracting subtasks within a prompt, and then it needs to be able to dispatch those subproblems to the right place. reply zozbot234 6 hours agorootparentprevMany models support function calling/RAG already, these are very similar features from a structural POV. But of course it's harder to train a model for such tasks, compared to just fitting some existing training set. reply CuriouslyC 5 hours agorootparentI'm not sure having the LLM as the top level piece is the right approach though. Async is the direction we want to go, and LLMS are inherently synchronous. Additionally, LLMs are cumbersome to train and tune, having an agent that calls smaller models would unlock the power of the community to build customized models for specific needs. reply szundi 5 hours agorootparentWe would like to have something like us. My daughter is just 3.5yrs old, she is inherently hard to train too, decades left. However, we find ourselves quite good. reply CuriouslyC 5 hours agorootparentYou're assuming you're a monolith, but in fact you have many subnetworks in your brain. The neocortex acts to process information from other parts of the brain, and it itself comprised of a network of interacting modules. reply szundi 5 hours agorootparentprevChatGPT routinely does this already reply CuriouslyC 5 hours agorootparentYes, in a slow, synchronous, limited way. reply darkerside 5 hours agoparentprevIt should just be able to program. A programming language is the ultimate verbal-to-math translation layer. Tangent but... programming languages aren't designed for computers. Computers are perfectly happy with assembly or even binary. Programming languages are designed for humans, not just so we can see what others have done, but so that we can understand what we ourselves have done. We give the variable a name because we can't remember 0x0010101; but the computer remembers them both just fine. reply brcmthrowaway 15 hours agoprevAre there any 1-bit LLMs available on GitHub? reply lhl 14 hours agoparentBitNet Implementations: * https://huggingface.co/1bitLLM/bitnet_b1_58-large * https://github.com/Oxen-AI/BitNet-1.58-Instruct * https://github.com/nkotak/1.58BitNet See some followups that has some training advice: https://github.com/microsoft/unilm/tree/master/bitnet reply hughesjj 15 hours agoparentprevHeck they have some zero bit llm's https://github.com/kelseyhightower/nocode reply usgroup 9 hours agoprevIs there some work on how small a model with some specific epsilon perplexity could theoretically be? Given a fixed architecture and a fixed dataset, I presume there is a minimal number of parameters required for optimal representation. reply renonce 8 hours agoparentIf you are referring to what is theoretically possible with arbitrary computation in the model, it's called Kolmogorov complexity and it's not computable. reply usgroup 7 hours agorootparentWith a fixed architecture, and a fixed dataset, as mentioned. So, a specific kind of neural network, and a fixed dataset. reply jononor 3 hours agorootparentYou can estimate it empirically. However large changes in model parameters/capacity tends to interact with hyperparameters, so would want to do runs with multiple values of hyperparameters. And training processes give noisy results, so might want to do multiple repetitions. And each run may take several GPU days. So even a small experiment of 10 repetitions X 10 hyperparameters X 10 model sizes takes several thousand GPU days. But there are many papers from the large labs that do such. And the whole result is also conditional on the optimization/training process used. Which is an area where we have no reason to think that we are optimal... So we can do studies with practical results (given sufficient money), but we are far from being able to identify the actual maximums available. reply renonce 2 hours agorootparentprevThe closest research would be the Chinchilla scaling laws, which estimates the final loss as a function of number of parameters and tokens. Set the number of tokens to infinity would give a good estimate of minimum achievable loss. reply WiSaGaN 14 hours agoprevThis seems saying that the 1bit model is a bit better than GPTQ Q2. However, I find there are few situations where you would want to use GPTQ Q2 in the first places. You would want to run the F16 version if you want quality, and if you want to have a sweet spot, you usually find something like Q5_K_M of the biggest model you can run. reply qeternity 13 hours agoparentNobody is running llama.cpp in production… reply luke-stanley 12 hours agorootparentWhat do you mean? What makes you think that? reply qeternity 1 hour agorootparentBecause for anything other than CPU inference it is inferior to TensorRT and vLLM. reply szundi 5 hours agorootparentprevProbably more often than not reply qeternity 1 hour agorootparentHighly doubtful. If anyone is running it here in a production setting, please post and prove me wrong. reply renonce 8 hours agoprevI study LLM quantization and I have surveyed GPTQ and QuIP# and lots of quantization algorithms (specifically PTQ, post-training quantization) to develop my own, and my experience has led me to become extremely skeptical of many of the papers. I've seen lots of headlines like \"1-bit quantization\" (including this one and https://arxiv.org/abs/2310.16795). What I've found in this space is that the headlines can often be intentionally misleading about what is actually achieved. If you read closer the abstract of this paper, it claims 8.41 perplexity on LLaMA2-70B at 1 bit, which is a HUGE decrease from 3.120 perplexity in FP16 and they will never mention that in the headline. Even LLaMA2-7B at INT8 achieves 5.677 perplexity with half of storage place (better with LESS space and LESS training). Some claim 1.58-bit quantization (each weight is either -1, 0 or 1) but in practice require very small group sizes, which means one or two extra FP16 numbers for every 64 weights and that means another 0.5 bit, so it's actually 2-bit quantization. And every quantization algorithm can claim they make language models smaller, speedier, and use less energy, so there's nothing special about these. Here are the key metrics that I suggest checking when comparing quantization schemes: * Perplexity. Note that it also depends on the dataset (either WikiText2 or C4, WikiText2 numbers are usually lower than C4) and context size (1024, 2048 or 4096, higher context sizes usually means less perplexity). Dataset and context size must match to make a meaningful comparison. * Quantization bits. Many algorithms claiming 2-bit or 1-bit quantization has lots of extra parameters elsewhere, such as grouping. Download the quantized version of the model, check its file size, multiply by 8 and divide by the number of parameters. That gets you the ACTUAL quantization bits. * Performance. Weights may need to be dequantized during inference which could introduce overhead. Some libraries have custom matmul kernel for dequantization that achieves performance close to FP16, others can be slower. Check its generation speed and inference speed. Newer architectures such as Ampere contains INT8 cores which may make quantized version even faster than FP16, I haven't tried out yet. There is also a lot of misleading comparisons in this space. Some methods like GPTQ only provide vector-matrix multiplication kernels, which means only a single token can be generated and batched inference (which is needed for generating initial KV cache, or for serving multiple users) can be much slower. If an algorithm claims a 3x speedup for something, check if they refer to single stream latency or multiple stream throughput. Some of that speedup comes from running a model on 2 cards instead of 5 cards without specifying if the cards have NVLink configured (you shouldn't run inference on multiple cards without NVLink, or you should expecet huge slowdown simply because of using 5 cards). * Base model. Pick a STRONG base model like Llama-2-7b or Llama-3-8b etc. Not an undertrained model like SwitchTransformer etc which may have lots of redundant parameters in itself. My personal favourite remains QuIP# (https://github.com/Cornell-RelaxML/quip-sharp). It lacks in the \"performance\" part as its matrix multiplication performance isn't on par yet but there is room for improvement, and it wins every other metric. And sad news: it's very likely we won't have practical 1-bit LLMs, never ever. We are reaching the end game between 2.5~4 bits. By \"practical\" I mean it should beat 3-bit LLMs with 3x less parameters or 2-bit LLMs with half as many parameters. There is a Shannon limit to quantization whatever methods you use. reply regularfry 4 hours agoparentJust on your very last point, I think you've nailed why a 1-bit quant of a bigger LM can't beat 3-bit quants of an LM a third the size, if what you mean is that more extreme compression of a LM is more likely to introduce harmful artefacts, so you need a better quantisation method to produce 1-bit than you do at 3-bit to end up with a model with the same information content retained. What I don't think that tells us anything about is directly trained 1-bit LMs versus 3-bit LMs, because in that case there's no compression step to introduce quantisation artefacts. There might be an analogous training data size argument but it's not clear to me that there needs to be: a 3X parameter 1-bit LLM and a 1X parameter 3-bit LLM ought to be equivalent in terms of their information capacity. reply kromem 7 hours agoparentprevCompletely agree on PTQ, but curious on your thoughts for QAT, specifically BitNet 1.58 - in that paper it looks like parameter to parameter the constrained precision weights had improved perplexity vs floating point weights, particularly as the model size increased. While I'd love to see it scaled up to at least ~50B models, it looks like limited weight precision might actually offer improved network optimization over unconstrained weights for pretraining. Do you think that work is misrepresenting the gains, or that QAT is a different beast where quantization isn't as much a tradeoff as a potential net gain across the board? reply renonce 6 hours agorootparentCan't speak for QAT as I haven't yet dived into that area. I've quickly skimmed the BitNet and BitNet 1.58 paper. I think achieving comparable performance with a Llama model with the same number of parameters is impressive but unfortunately it seems they didn't release the training code so I can only tell from their paper. Fortunately they did talk about training details in the BitNet paper (not in BitNet 1.58 so I assume they remain the same): > Mixed precision training. While the weights and the activations are quantized to low precision, the gradients and the optimizer states are stored in high precision to ensure training stability and accuracy. Following the previous work [LSL+21], we maintain a latent weight in a high-precision format for the learnable parameters to accumulate the parameter updates. The latent weights are binarized on the fly during the forward pass and never used for the inference process. In this case there are two areas to optimize for: training efficiency and inference efficiency. If I understand correctly, it stores the weights, gradients and second-moment estimates in FP32 like every other mixed-precision training (the Gopher paper has details on why storing them in FP32 is important), and quantized weights are used in forward pass. What I'm not sure is whether latent weights are used in backward pass, and my instinction is that the \"Straight-through estimator\" requires high-precision latent weights so they may still be needed. Training FLOPS can be roughly estimated as 6 FLOP per parameter per token, where 2 is forward pass, 2 is gradient computation and 2 is gradient accumulation (see https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-la...). If only forward pass is quantized, this means only 1/3 of all FLOPS are optimized (and even then it has to be accumulated in FP32). So I'm skeptical of the gains in training efficiency here, and I can't find the numbers (how much energy or how much time is used for training, compared to regular FP16 mixed precision training? The papers boast inference energy savings which makes me even more skeptical of training energy savings) For quantization efficiency, while QAT can certainly avoid the quantization step, PTQ methods are very cheap (usuallyAsk questions where you can verify whether the answer is correct. If you’re going to verify (and you should), might as well skip the asking step. reply umanwizard 3 hours agorootparentThere are lots of things that are easier to verify than to figure out how to do from scratch. reply hn_throwaway_99 3 hours agorootparentprevExcept that's a million times slower. I'll often ask it to generate some complicated SQL queries for me (usually when I need to get info from system tables, less so my own data), and it's pretty trivially easy to verify the output. It would take me much, much longer if I had to write these queries from scratch. reply rockskon 10 hours agorootparentprevOnly if the trial-and-error process has no meaningful consequences for failure. reply umanwizard 3 hours agorootparentYeah. Which describes tons of stuff I encounter every day. reply umanwizard 11 hours agorootparentprevI’m blown away when people say stuff like this. GPT-4 makes me substantially more productive almost every day. It’s like we live in a different reality. reply devbent 13 hours agorootparentprev> Now it needs actual good products. I asked chatgpt for recommended food pairings for a soup I made today. It did a great job. Chatgpt also helped me debug a home automation issue I'd been having for over a year with some smart lights. I find uses for chatgpt every day. reply throw46365 13 hours agorootparentprevI had that feeling the other day, reading quotes from that OpenAI executive: https://archive.is/LpDuJ She said the executives told the board they \"didn't think he was the right person to lead the company to AGI…” The way I read it, it sort of sounded like you could substitute “AGI” with “Shangri-La”. It’s always going to be just down the road, but they are sort of emotionally convinced that is where they are headed. reply lucubratory 7 hours agorootparentprev>Honestly it feels like the bulk of the industry is acting out one big LARP where they're always right around the corner from developing AGI and doing something big and amazing and....it just never materializes. It's been less than two years since ChatGPT released. reply segmondy 6 hours agoparentprevThis is false. I have tested Q8s over f16 and the raw weights and pretty much seen no difference. I strictly run Q8s, I only keep the raw weights if I plan to do some fine tunes. reply staticman2 44 minutes agorootparentThis matches the general consensus at the LocalLLama subreddit that Q8 is basically indistinguishable from the full model. reply light_hue_1 14 hours agoprevThe performance loss from BiLLM is disastrous. It's basically useless. No one would ever want to use the resulting models. They hide their main results in the appendix: Table 8. page 15. https://arxiv.org/pdf/2402.04291 I won't go over the entire table in detail, but PIQA, BoolQ, HellaSwag, and WinoGrande should in the mid-to-high 70s for LLaMa2-7B. They drop that to 58, 62, 32, and 51. There are 700M parameter models that perform much better. What they should have reported is effective number of parameters. Does LLaMa2-7B with their quantization method outperform a model that has amount of computation but uses that compute with say.. 16-bit quantization? If the answer is no, and it seems like it very clearly is, then the method is wholly worthless. Just use a smaller model to begin with. The BitNet paper is better. But for some reason they only consider very small models. It's the obvious question and in their FAQ they don't provide a solid answer to it. Despite having all of the compute resources of MS. They could have easily run this experiment in the past year; I'm suspicious. reply imtringued 12 hours agoparentBiLLM is about post training quantization and BitNet trains models from scratch. You do realize that one of those is going to give significantly worse results and the other is going to be significantly more expensive, well into the millions of dollars? reply light_hue_1 9 hours agorootparentThere's no mathematical reason why doing quantization after would be worse than training from scratch. That's nonsense. There's also no practical reason. Training quantized networks is often harder! This is why people quantize after the fact or do distillation. Nor is there any reason why we won't find some projection of weights onto the BitNet manifold. If it was published by academics I'd believe the cost argument. This was published by MS. They can run this experiment trivially. I have friends at MS with access to enough compute to do it in days. Either the authors ran it and saw it doesn't work or they're playing with us. Not a good look. The reviewers shouldn't have accepted the paper in this state without an explanation of why the authors can't do this. This is the question that determines if this work matters or is useless. Publishing before knowing that isn't responsible on anyone's part. reply anon373839 7 hours agorootparentAfter Llama 3, does this paper’s result seem so far-fetched? That 8B parameter model showed that most of what the frontier models “know” can be represented much more compactly. So why couldn’t it be represented at low precision? reply roschdal 13 hours agoprev\"Nearly accurate\" as in \"incorrect\" reply CGamesPlay 13 hours agoparentDon't worry, the LLMs were only \"fairly accurate\" (as in \"incorrect\") to begin with. reply akira2501 9 hours agorootparentThe lesson being... We spent billions for incremental gains masquerading as tectonic shifts. Whoops. reply hiddencost 16 hours agoprevNope nope nope. Any time we find more efficiency, we can trade it for more quality by doing more compute. We'll always use as much compute as we can afford, until we stop getting quality gains that are worth the added cost. reply kazinator 15 hours agoparentCost is basically time here. \"How much compute we can afford\" is really \"how long we are willing to wait for the result\". reply trashtester 5 hours agorootparentThen again, time * size_of_cluster = cost. If you train on a cluster that costs >$1M/day to operate, the wait time is likely to be a smaller concern than the financial cost, unless you're REALLY in a hurry to beat some competitor. reply shikon7 15 hours agoparentprevYes, it seems the more efficient use we have for compute, the more valuable compute is to us, and the more compute we will be able to afford. reply XorNot 16 hours agoparentprevIsn't this article just about an optimization though, sans the title? I don't much care for all the \"oh but the energy usage\" claims in most tech things: it's all electricity, and it's all fungible. It usually seems to roll out as a proxy for \"I don't like this thing\". Like even with cryptocurrency, there were a lot of people mistaking the issue of scalability - namely that \"as a store of value\" crypto would consume incredible amounts of other resources (and a lot of people got stuck trying to figure out how somehow \"a hash\" could be reclaimed for useful resources) to do less then alternatives, with \"the energy usage itself is the problem\". Finding optimizations for LLMs is good because it means we can build cheaper LLMs, which means we can build larger LLMs then we otherwise could for some given constraint, which means we can miniaturize (or in this case specialize) more capable hardware. The thing which really matter is, can the energy usage be meaningfully limited to a sensible scaling factor given the capability that makes them useful? Because environmentally, I can install solar panels to do zero-carbon training (and if LLMs are as valuable as they're currently being priced, this is a no-brainer - if people aren't lying about solar being \"cheaper then fossil fuels\"). reply eru 15 hours agorootparent> Like even with cryptocurrency, there were a lot of people mistaking the issue of scalability - namely that \"as a store of value\" crypto would consume incredible amounts of other resources (and a lot of people got stuck trying to figure out how somehow \"a hash\" could be reclaimed for useful resources) to do less then alternatives, with \"the energy usage itself is the problem\". To be fair, technology-wise they mostly solved this problem via proof-of-stake. From an individual point of view you still expense enormous resources as a miner / validator in a proof-of-stake system. It's just that now the resources come in the form of lost opportunity costs for your staked tokens (eg staked Ethereum). But from aggregated perspective of society, staked Ethereum is essentially free. That has some parallels to how acquiring regular money, like USD, is something individuals spend a lot of effort on. But for the whole of society, printing USD is essentially free. > Because environmentally, I can install solar panels to do zero-carbon training (and if LLMs are as valuable as they're currently being priced, this is a no-brainer - if people aren't lying about solar being \"cheaper then fossil fuels\"). There's still opportunity costs for that energy. Unless you have truly stranded electricity that couldn't be used for anything else. > Finding optimizations for LLMs is good because it means we can build cheaper LLMs, which means we can build larger LLMs then we otherwise could for some given constraint, which means we can miniaturize (or in this case specialize) more capable hardware. The thing which really matter is, can the energy usage be meaningfully limited to a sensible scaling factor given the capability that makes them useful? I agree with that paragraph. It's all about trade-offs. If we can shift the efficiency frontier, that's good. Then people can decide whether they want cheaper models at the same performance, or pay the same energy-price for better models, or a combination thereof. Or pay more energy for even better model reply dartos 16 hours agorootparentprevIt’s good that we can build cheaper LLMs, but the problem companies guzzling energy for LLM training won’t use less energy, they’ll just have better models reply kaba0 14 hours agorootparentThat energy is still on the order of a household’s yearly electricity, not that of Argentina as cryptos were, and that’s just for training. Inferring is much cheaper and arguably provides quite a lot of value (even though I also think it is overhyped), for very little energy consumption, probably more is lost due to inefficiency for any physical product. reply dartos 6 hours agorootparentI got news for you. The 1M GPUs that meta purchased running at full (or less probably) load 24/7 is more than the energy of a single household. The energy cost is in training, not inference. reply kaba0 3 hours agorootparentWhich is the same order of magnitude? Also, how often do they train from scratch? reply dartos 14 minutes agorootparentI don’t understand your first question, sorry. But for the second: Llama 1, 2, and 3 all have different architectures and needed to be trained from scratch. Llama 1 was released February 2023. Same training story for openAI’s Sora, dalle, and 4o. All of mistral’s models Mamba, Kan, and Each version of rwkv (they’re on 6 now) Not that this list is a result of survivor bias. It’s only looking at their published models too. Not the probably 1000s of individual training experiments that go into producing each model. reply chx 9 hours agoprev [–] Nearly as accurate? I guess zero is close to zero. https://hachyderm.io/@inthehands/112006855076082650 > You might be surprised to learn that I actually think LLMs have the potential to be not only fun but genuinely useful. “Show me some bullshit that would be typical in this context” can be a genuinely helpful question to have answered, in code and in natural language — for brainstorming, for seeing common conventions in an unfamiliar context, for having something crappy to react to. > Alas, that does not remotely resemble how people are pitching this technology. reply Last5Digits 9 hours agoparent [–] Are you going to spam this same link in every single thread about LLMs on HN? People have provided good arguments refuting whatever you're trying to say here, but you just keep posting the same thing while not engaging with anyone. reply chx 3 hours agorootparent [–] As long as there's hype for LLMs, yes. Basically, every LLM article is alternate facts. It's not true. So I refute. For example > AI systems that power chatbots like ChatGPT, are getting better and better These master manufacturing plausible answers which make people believe they are correct. And perhaps they get even more plausible with each iteration -- but that does not mean they get genuinely better because they can not, correctness / factualness are not a property of LLMs. Kahneman's Thinking Fast And Slow tells you in great detail how our thinking is wired to mix up the two. > For LLMs that are ... environmentally friendly, No such thing can exist. reply Last5Digits 1 hour agorootparent [–] No, the answers aren't just \"plausible\", they are correct the vast majority of the time. You can try this for yourself or look at any benchmark, leaderboard or even just listen to the millions of people using them every day. I fact check constantly when I use any LLM, and I can attest to you that I don't just believe that the answers I'm getting are correct, but that they actually are just that. But they apparently actually don't get better even though every metric tells us they do, because they can't? How about making an actual argument? Why is correctness \"not a property of LLMs\"? Do you have a point here that I'm missing? Whether or not Kahneman thinks that there are two different systems of thinking in the human mind has absolutely no relevance here. Factualness isn't some magical circuit in the brain. > No such thing can exist. In the same way there can exist no piece of clothing, piece of tech, piece of furniture, book, toothpick or paperclip that is environmentally friendly; yes. In any common usage, \"environmentally friendly\" simply means reduced impact, which is absolutely possible with LLMs, as is demonstrated by bigger models being distilled into smaller more efficient ones. Discussing the environmental impact of LLMs has always been silly, given that we regularly blow more CO2 into the atmosphere to produce and render the newest Avengers movie or to spend one week in some marginally more comfortable climate. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "1-bit language models (LLMs) present a promising solution to the high energy demands of AI by being more efficient.",
      "These models are smaller and faster compared to traditional models, making them more practical for various applications.",
      "Despite their reduced size, 1-bit LLMs maintain nearly the same level of accuracy as their larger counterparts."
    ],
    "commentSummary": [
      "The discussion focuses on the benefits and challenges of using quantized language models (LLMs) to enhance efficiency and reduce resource requirements while maintaining accuracy.",
      "Advanced methods like IQ2_XS are highlighted for mitigating the quality loss in highly trained models like Llama3 due to quantization.",
      "The conversation also explores the future of LLMs, suggesting a shift towards modular systems and emphasizing the need to balance computational cost, quality, and environmental considerations."
    ],
    "points": 262,
    "commentCount": 138,
    "retryCount": 0,
    "time": 1717107482
  },
  {
    "id": 40535895,
    "title": "Standard Ebooks Celebrates 1,000th Release with James Joyce's \"Ulysses\"",
    "originLink": "https://standardebooks.org/ebooks/james-joyce/ulysses",
    "originBody": "Ulysses James Joyce 268,481 words (16 hours 17 minutes) with a reading ease of 74.9 (fairly easy) № 1 in the Modern Library’s 100 Best Novels set. № 46 in the Guardian’s Best 100 Novels in English (2015) set. Fiction Description Close this box Help us meet our stretch goal of 45 new patrons by June 3 0 28/45 25 45 When we started this drive, we set a goal of 25 Patrons Circle members by June 3. Thanks to the incredible generosity of literature lovers from all walks of life, we hit that goal! Since there are still a few days left in our drive, we thought we’d challenge our readers to help us reach our stretch goal of 45 patrons, so that we can continue on a rock-solid financial footing. Will you help us with a donation, and support free and unrestricted digital literature? Join the patrons circle James Joyce’s most celebrated novel, and one of the most highly-regarded novels in the English language, records the events of one day—Thursday the 16th of June, 1904—in the city of Dublin. The reader is first reintroduced to Stephen Dedalus, the protagonist of Joyce’s previous novel A Portrait of the Artist as a Young Man. Stephen is now living in a rented Martello tower and working at a school, having completed his B.A. and a period of attempted further study in Paris. The focus then shifts to the book’s protagonist, Leopold Bloom, an advertising canvasser and social outsider. It is a work day, so both Bloom and Stephen depart their homes for their respective journeys around Dublin. While containing a richly detailed story and still being generally described as a novel, Ulysses breaks many of the bounds otherwise associated with the form. It consists of eighteen chapters, or “episodes,” each somehow echoing a scene in Homer’s Odyssey. Each episode takes place in a different setting, and each is written in a different, and often unusual, style. The book’s chief innovation is commonly cited to be its expansion of the “free indirect discourse” or “interior monologue” technique that Joyce used in his previous two books. Ulysses is known not only for its formal novelty and linguistic inventiveness, but for its storied publication history. The first fourteen episodes of the book were serialized between 1918 and 1920 in The Little Review, while several episodes were published in 1919 in The Egoist. In 1921, the New York Society for the Suppression of Vice won a trial regarding obscenity in the thirteenth episode, “Nausicaa.” The Little Review’s editors were enjoined against publishing any further installments; Ulysses would not appear again in America until 1934. The outcome of the 1921 trial worsened Joyce’s already-considerable difficulties in finding a publisher in England. After lamenting to Sylvia Beach, owner of the Parisian bookshop Shakespeare and Company, that it might never be published at all, Beach offered to publish it in Paris, and Ulysses first appeared in its entirety in February 1922. The first printing of the first edition was filled with printing errors. A corrected second edition was published in 1924. Stuart Gilbert’s 1932 edition benefited from correspondence with Joyce, and claimed in its front matter to be “the definitive standard edition,” but was later found to have introduced errors of its own. The novel’s initial reception was mixed. W. B. Yeats called it “mad,” but would later agree with the positive assessments of T. S. Eliot and Ezra Pound, stating that it was “indubitably a work of genius.” Joyce’s second biographer Richard Ellmann reports that one doctor claimed to have seen writing of equal merit by his insane patients, and Virginia Woolf derided it as “underbred.” Joyce’s aunt, Josephine Murray, rejected it as “unfit to read” on account of its purported obscenity, to which Joyce famously retorted that if that were so, then life was not fit to live. The sheer density of references in the text make Ulysses a book that virtually demands of the reader access to critical interpretation; but it also makes it a book that is easily obscured by the industry of scholarship it has generated over the last century. The dismissal of a serious interpretation is tempting, but would trivialize Joyce’s enormous project as an extended joke or an elaborate exercise in ego. Likewise dismissing it as uninterpretable would ignore the profusion of earnest critical analyses. Today Ulysses is considered by many to be the zenith of 20th century literature: both one of the richest, and also the most difficult, books to ever be written. To appreciate it is not to think it unintelligible; rather, perhaps the best description of it is the one used of Ulysses himself in a 21st century translation of Homer’s epic—“complicated.” This Standard Ebooks edition is based on a transcription of the 1922 Shakespeare and Company first edition, with emendations from pre-1929 errata lists and the second edition in its 1927 ninth printing by Shakespeare and Company. It does not track any one particular edition, but rather is a blend of pre-1929 editions that aims to contain what scholars might consider to be the most accurate version of what was printed before 1929. Therefore, various probable misprints have been retained that may have been corrected in post-1929 editions. Consultation of various editions of the book and the historical collation list appended to Hans Walter Gabler’s Critical and Synoptic Edition is advised before contacting Standard Ebooks about potential mistakes. Read free This ebook is thought to be free of copyright restrictions in the United States. It may still be under copyright in other countries. If you’re not located in the United States, you must check your local laws to verify that this ebook is free of copyright restrictions in the country you’re located in before accessing, downloading, or using it. Download for ereaders Compatible epub — All devices and apps except Kindles and Kobos. azw3 — Kindle devices and apps. Also download the Kindle cover thumbnail to see the cover in your Kindle’s library. Despite what you’ve been told, Kindle does not natively support epub. You may also be interested in our Kindle FAQ. kepub — Kobo devices and apps. You may also be interested in our Kobo FAQ. Advanced epub — An advanced format that uses the latest technology not yet fully supported by most ereaders. Read about which file to download and how to transfer them to your ereader. Read online Start from the table of contents Read on one page A brief history of this ebook May 31, 2024 Fix broken long description May 31, 2024 Update link to The Odyssey May 31, 2024 Add more author links to long description May 31, 2024 Update content.opf May 31, 2024 Fix a typo in long description Read the full change history. More details This ebook’s source code at GitHub This book at Wikipedia Sources Transcriptions Transcription at Wikisource Page scans Page scans at the Internet Archive Page scans at the Internet Archive Improve this ebook Anyone can contribute to make a Standard Ebook better for everyone! To report typos, typography errors, or other corrections, see how to report errors. If you’re comfortable with technology and want to contribute directly, check out this ebook’s GitHub repository and our contributors section. You can also donate to Standard Ebooks to help fund continuing improvement of this and other ebooks. More free fiction ebooks",
    "commentLink": "https://news.ycombinator.com/item?id=40535895",
    "commentBody": "Standard Ebooks' 1,000th title: Ulysses (standardebooks.org)227 points by robin_reala 4 hours agohidepastfavorite85 comments AlbertCory 3 hours ago> a reading ease of 74.9 (fairly easy) Well, Joyce did say that it was just a lot of jokes. ========================= Episode 14 - Oxen Of The Sun DESHIL HOLLES EAMUS. DESHIL HOLLES EAMUS. DESHIL HOLES Eamus. Send us, bright one, light one, Horhorn, quickening and wombfruit. Send us, bright one, light one, Horhorn, quickening and wombfruit. Send us bright one, light one, Horhorn, quickening and wombfruit. Hoopsa, boyaboy, hoopsa! Hoopsa, hoyaboy, hoopsa! Hoopsa, boyaboy, hoopsa. Universally that person's acumen is esteemed very little perceptive concerning whatsoever matters are being held as most profitable by mortals with sapience endowed to be studied who is ignorant of that which the most in doctrine erudite and certainly by reason of that in them high mind's ornament deserving of veneration constantly maintain when by general consent they affirm that other circumstances being equal by no exterior splendour is the ..... reply acabal 2 hours agoparentYes, it's a bit silly! The reason the score is so off is because we use the Flesch Reading Ease algorithm[1] to calculate it, which was designed for the US Navy to be able to score technical manuals. It works very well for most prose too... except highly modernist prose! https://en.wikipedia.org/wiki/Flesch-Kincaid_Reading_Ease reply AlbertCory 2 hours agorootparentThanks. Maybe a simple fix is: don't use it for fiction. Since that's not its intent. reply acabal 1 hour agorootparentIt works just fine for fiction. Ulysses is a very special edge case in the pantheon of all literature, so it's no surprise it doesn't work well for this one case. reply hedora 2 hours agorootparentprevAs an end-user of Standard Ebooks, I've found it works pretty well on average. reply readthenotes1 11 minutes agoparentprevReading ease: each word makes sense. -25.1 points for no 4 words in a row making sense. reply perihelions 2 hours agoparentprev- \"a reading ease of 74.9 (fairly easy)\" Yeah, that's a very unnecessary misuse of AI. Is there an open-source human rating site for serious books, in how difficult they are to read—how tedious, how erudite, how much pain you have to go through to get whatever reward you think you get at the end? With Ulysses near the edge of one axis, Moby Dick demarcating another... Surely this is all common knowledge to bookish people, but, where do they write it down? reply robin_reala 2 hours agorootparentHardly AI, just a simple Python function that implements the Flesch reading ease algorithm: https://github.com/standardebooks/tools/blob/effcf0f6db05729... reply erikpukinskis 1 hour agorootparentAI stands for “artificial intelligence” and I think an algorithm which decides how easy a book is to read qualifies as some sort of intelligence. reply picture 1 hour agorootparentThe input to the algorithm is literally three numbers: total words, total sentences, total syllables. If this counts as AI, then your thermostat or film camera feels pretty AI too. https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readabi... reply wan23 13 minutes agorootparentPerhaps we could use AI to give us a score for how AI a given AI is reply xandrius 2 hours agorootparentprevEverything is AI to an untrained person. reply hedora 2 hours agorootparentAlso, these days, most AI is just a simple python program. reply tiagod 46 minutes agorootparentWell, sure, because all the complexity emerges in the weights reply squigz 1 hour agorootparentprevWho even mentioned AI here? reply joaorico 2 hours agoprevFor anyone diving into Ulysses, I highly recommend checking out The Joyce Project [1]. It's filled with interactive notes that are very useful for understanding the linguistic and cultural references. Here's my reading method that I found effective: 1. Read a section on paper. 2. Go through the same section on the site. 3. (Re-)read on paper. I toggled between 1-2-3, 1-2, or 2-3 depending on my mood, and it worked really well. [1] https://www.joyceproject.com/ reply WilTimSon 14 minutes agoparentI'll say this, with the caveat that I never did finish the book due to unrelated reasons, this is an excellent method. One may think \"It's fine, I'll simply read the text and then, if I have questions, absorb some scholarly articles on it.\" Trust me, you will enjoy it so much more when you understand Joyce's intent and clever writing as it happens. You simply can't take it all in post-factum, too much would be missed. reply patrick-fitz 1 hour agoparentprevThanks for the link, this does make it more approachable! reply poulpy123 2 hours agoprevFYI > Standard Ebooks is a volunteer-driven effort to produce a collection of high quality, carefully formatted, accessible, open source, and free public domain ebooks that meet or exceed the quality of commercially produced ebooks. I'm looking at doing that for calculus made easy by sylvanus Thompson but I need to overcome my lazyness first reply gjm11 2 hours agoparentSomeone else has done something similar for that work: https://www.sunclipse.org/?p=3194 -- you might find either that what he's done is good enough to satisfy you, or that you can build on what he did. reply jarvist 2 hours agorootparentThat's great! Their motivation & updates seem very sensible. Calculus Made Easy is a very nice compact book for university students who have started to forgot their high school training, and I will certainly point people in the direction of this project. reply hedora 1 hour agorootparentIf you want the opposite of that: University students that come into college with strong calculus fundamentals, then I suggest \"Calculus\" by Michael Spivak. I haven't used calculus in a decade, but I use things I learned from working through that book pretty much every day I write code. reply squigz 1 hour agoparentprevStandard Ebooks is a great resource. Consider supporting them https://standardebooks.org/donate reply robin_reala 2 hours agoparentprevYou can use Standard Ebooks tooling to create a white-label ebook skeleton if that would help you get started. reply NoMoreNicksLeft 1 hour agoparentprevOuch. Can't think of a more challenging one to work on. I can do most things epub now, but mathml is very much in the \"no thanks now go away\" category. reply resource_waste 0 minutes agoprevHappy to find this website, nice to have soo much philosophy in 1 place. https://standardebooks.org/ebooks?page=1&tags%5B0%5D=philoso... I just hate trying to google search to get an epub. reply ramijames 3 hours agoprevI'm so grateful that this is being done. Many free ebooks are high-quality content in a low-quality format. reply freedomben 3 hours agoparentIndeed, this is such an incredible and important project. Anyone who has tried to get free and/or public domain works in ebook format knows how bad the situation is. Some versions are good, but most are poor inputs processed through outdated and error-prone OCR. I'm extremely grateful to have even that, but the need for this project is very real. I'll be supporting financially when I can, but until then if anyone involved is reading, please accept my heartfelt thanks and appreciation for your noble efforts. reply tantivy 2 hours agoparentprevMany paid ebooks are in a low-quality format too. I've bought Kindle books from real, well-known publishing houses that had obviously not had a single proofreader go through after their OCR. A motivated volunteer would've done so much better. reply sherr 1 hour agorootparentI agree. I stopped reading on the kindle years ago because I got so fed up with the terrible mistakes in the text. Not just spelling but also formatting. I love paper books. Having said that, Standard EBooks are very good. reply alejohausner 1 hour agoprevDon’t read the book. Listen to it. I got a lot out of listening to a performance of the book, by Irish actors on RTE. It’s a lot more fun than reading the book. There’s a podcast for it: https://www.rte.ie/culture/2022/0610/1146705-listen-ulysses-... reply TechDebtDevin 59 minutes agoparentThat's honestly sacrilege unless doing so for accessibility reasons. reply samatman 32 minutes agorootparentOn the contrary, Bloomsday recitals are orthodoxy. reply Avid8329 5 minutes agorootparentExactly. Please can we stop gatekeeping somebody enjoying a book that's known to be highly inaccessible. However one chooses to engage with Ulysses is perfectly fine. reply daveoc64 3 hours agoprevI love the project, but it really does need a way to find the most popular books: https://github.com/standardebooks/web/issues/298 reply donatj 1 hour agoparentTheir tech stack is essentially a really weird really opinionated custom static site generator that makes improvements difficult. The build process is slow and troublesome. I talked with them about adding an index of authors, and were open to the idea. I put probably 5 hours into getting it running correctly and figuring out how to get it to work. They then were at the time unwilling to restructure their data into a way that made it feasible due to how they have books are structured for multiple authors. I've worked on hundreds of PHP apps over the last 20 years, and it's frankly one of the weirdest. It could benefit from even just a little SQLite db or something. reply bnycum 3 hours agoparentprevI was gonna comment on the sorting being less than ideal too. Only sort by release date (not publication date), author, ease, and length. Can't even sort by title. reply chasil 3 hours agoparentprevGutenberg.org is reporting the number of downloads; can't this be used? Are these two organizations otherwise not on friendly terms, and able to exchange ebooks? reply robin_reala 2 hours agorootparentWe talk a fair bit! Gutenberg is the source for 95%+ of our productions, and personally I upstream any transcription issues I find. What differs though is the mission. Gutenberg generally wants accurate digital translations of the original source texts, whereas Standard Ebooks tries to balance that out with better readability. Both are valid approaches, and there’s space for both of us. reply chasil 2 hours agorootparentJust curious... do you ever reach a consensus on a text that you decide to share between you? Can you actually share the full .EPUB file, or do they object to the expanded size with bundled fonts and formatting, resulting in identical HTML but otherwise different content in the ZIP? reply robin_reala 2 hours agorootparentIt hasn’t happened before but never say never. What we do do though sometimes is contribute our own transcriptions to Gutenberg first, then use that as a base for the Standard Ebooks version. reply raybb 2 hours agoparentprevYou can use the openlibrary.org API to get the popularity (want to reads, rating counts, reviews) to get you started. reply kstrauser 2 hours agoprevI have never loathed a book so much. I read every page out of a stubborn refusal to let it beat me, then put it high up on a back shelf so I'd never have to see it again. It's not that there was nothing of value in it. Sure, there is! It's more that the return on investment just wasn't worth it. My experience was like walking through a desert to find a candy bar. Even if it's a good candy bar, that's way too much of a hassle to go through to get it. I do hugely appreciate Standard Ebooks, though. Such a wonderful project! reply newhaus1994 1 hour agoparent\"Oxen of the Sun\" is, in a sense, the apex of English literature to me. The modernists had a knack for essentially believing that if they just tried hard enough and wrote with enough complexity, they could capture essential truths about existence, and this chapter is the height of Joyce's effort in that regard. It is a chapter both about and structured like the birth and evolution of language, incorporating a density of reference and linguistic type that is somewhat absurd in its breadth. This is all to say that Ulysses is a book written for people who like the idea of a Rube Goldberg machine of literature. I happen to be one of those people, but there are brilliant literary scholars who hate the book. reply klik99 2 hours agoparentprevEveryone I know who loves it also has a scholarly level of knowledge about fiction so I suspect it has a lot of requisites to fully enjoy, like having to know a lot about american media to enjoy arrested development. I've been wanting to give it another go since I didn't enjoy or finish it last time, and my favorite book of all time (Wolfes Book of the New Sun) took three false starts to really get into it. reply kstrauser 1 hour agorootparentI've harbored a wholly unsupported, yet persistent, notion that no one really likes Ulysses, but that it's become an \"emperor has no clothes\" situation, where those who decide which books are \"great\" picked it on a lark, and dared others to say they didn't like it, knowing that none would lest they appear as the rube who couldn't appreciate great literature, resulting in a lineage of thinkers who encountered the beast, read it in horror, then told their colleagues about how much they loved it because obviously it's the brilliantest work of English, all while secretly hoping they weren't grilled too closely about it. reply tantivy 48 minutes agorootparentI love Ulysses because of what it says about growing into middle age, facing irresolvable insecurities about yourself, and the solace against these that you can find in friendship—the way you can feel your soul sing when you find someone who wants to understand you. It's a very beautiful and humanistic book. I'm sorry that you weren't able to connect with it. reply block_dagger 2 hours agoparentprevI had the exact same experience. Struggled through it, searched hard for the genius that was supposedly exuding from the pages. I didn't see any of it and I don't remember a lick of the story. I was simply bored. Perhaps I'm not intellectual enough to \"get it.\" reply kstrauser 1 hour agorootparentSame, same. I'm an avid reader. I don't shy away from complex literature. I also don't take great pleasure from overcomplication. Computing analogy: It's like seeing that someone wrote \"fizzbuzz\" using a microservice architecture, quantum computing, multiple LLMs, and some custom hardware. Bravo! Cool that you pulled it off and made this monstrosity! I don't want to use it though, nor do I care to see the details of every bizarre choice, and I'm not impressed at the number of references to COBOL that you shoved into the diode layouts of the CPU you invented for it. I can see why other people would find it fascinating. I am not one of them. reply jderick 2 hours agoparentprevFor me, Joyce is the pinnacle of the English language. I can't say I understand too much of what is happening but noone writes more beautifully. I just love the sound of his words and the images he conjures. reply wozniacki 1 hour agorootparent> no one writes more beautifully Whats this obsession with beauty in language among some English-first speakers? Aren't meaning, insight and import of more consequence than beauty? Every single time I hear someone wax poetic about beauty and elegance in things it immediately sets off my bs meter. If you haven't got much of anything substantive to say, you use flowery artifices to mask it. Also non-English-first speakers, do you see this to be a very English-thing or is this sort of fixation if not fetish with beauty in language and other things present in your current day language too? reply CydeWeys 1 hour agorootparentThis is a bizarrely anti-English take. There's appreciation for the beauty of the language for literature/poetry in every language, as far as im aware. Look at Japanese poetry for one obvious example that takes appreciation of beauty in language to its absolute extremes. reply kstrauser 1 hour agorootparentprevThere's an entire genre -- poetry -- dedicated to expressing ideas in the most beautiful and elegant way possible. I consider flowery artifices to be the opposite. reply criddell 1 hour agorootparentprev> Aren't meaning, insight and import of more consequence than beauty? You are close to setting up a false dichotomy here. It isn’t those qualities or beauty, it’s those qualities and beauty. I first experienced it when I read Michael Ondaatje’s The English Patient. I was able to enjoy the book on the usual axes of plot, character, etc… but there was another level that had me rereading pages because every chapter, paragraph, and sentence felt perfectly constructed. Some of it I read out loud to myself because the rhythm of the words and sounds were musical. It is a great story, beautifully told. That said, I’m not a fan of Ulysses and I’m sure a lot of people here would call me an uncultured rube for enjoying Odaatje’s writing like I did. reply staunton 1 hour agorootparentprevSame thing in all languages I know well enough to read books in. I can't at all explain how or why it works, but certain kinds of writing style have an almost magical effect on me. This feeling of well-rounded beauty, even when the content is barely relevant, is just amazing. One could maybe describe it as a kind of brain hacking, which is also what drugs do. reply lxgr 1 hour agorootparentprevMaybe poetry is just not for you? That’s completely fine, but hopefully you can take people’s word on that it can be very beautiful to them. It feels a bit like saying “if you don’t have meaningful lyrics, why even sing a song”: Different people can appreciate different layers of literature differently. reply jderick 1 hour agorootparentprevFor me, reading fiction is all about beauty. I liken it to listening to good music. It isn't really to learn anything \"substantive.\" It is to experience a feeling or be transported to another place. In fact, I like to listen to (typically instrumental) music while I read as a sort of \"soundtrack.\" I would liken reading a good book to watching an epic movie. I guess you might occasionally gain some insight into the human condition, but it isn't primarily an informational medium. reply dvaun 1 hour agorootparentprevTo put it bluntly: > Different strokes for different folks Don’t you think you’re exaggerating? I would imagine there are fans of written language in every language. English isn’t special in this context. reply wozniacki 26 minutes agorootparentNot in the least bit. Far too many of these supposed greats works of literature get an easy pass from uncritical also-rans of the world, who just want to move on in the name of different-strokes-for-different-folks without ever calling out the bs for what it really is. I'm not saying there aren't valid detractors of these works - there are - but far too often they're drowned out. Far too many of these works hide behind the crutch of 'fiction' to spew utter hogwash without making an ounce of sense to the regular, impartial and non-dyed-in reader. Far too many of these books - when coupled with a lackadaisical populace in general who are more concerned about seeming non-fussy - get that stellar mythical hallowed status and lore. I'm not saying that there are not enough people who genuinely get entranced with these works (although if you run that through a fine comb your results may vary) - its that the gatekeepers of education seem to be entirely made up of these uncritical clowns who will nod away in affirmation, decade after decade in cementing the undeserved status of these works. reply dvaun 4 minutes agorootparentI agree with your take on the literature. My point wasn’t in response to Ulysses or criticism of it, but of your statement regarding over-obsession with the English language. dbspin 1 hour agorootparentprevAs a writer, I think I can speak to this. I can certainly understand a non-native speakers frustrations with the complexity of english grammar, the enormous number of synonyms and colloquialisms, the variety of 'codes' and kinds of 'jargon' that must make learning and reading English profoundly difficult. Especially where the non-native speaker or reader's mother tongue isn't a romance language. I get that it must make certain forms of English - from the dense AAVE of the wire, to Elizabethan sonnets all but impenetrable. However, I see this 'pragmatism in all things, including art' perspective quite a bit on hackernews, and rarely enough anywhere else. Most often concerning fiction, but also contemporary and modern art. I'm not sure if it's a neurodiverse perspective, or a philistine one, but I can confirm that it's missing the aesthetic function of art. i.e.: the pleasure many people obtain from creating and experiencing it. There seems to be a frustration that some people who don't or can't engage in producing or enjoying certain kinds of art have - that becomes a denial of the value of the work altogether. 'I don't get it, so there's nothing there to get'. Specific to Joyce and the modernists is an absolute mastery of the complexity and nuance of a wide breath of kinds of English (and in Joyce's case numerous other European languages). To a native speaker with a strong grasp of language, and a love of words, reading Joyce or TS Elliot, or Yeats etc, is like listening to a complex piece of classical music. The use of reference, of meter, of onomatopoeia, the play with homonyms and antonyms, and at a higher level with the structure of stories and narrative traditions etc - all give the reader pleasure. In the hands of a truly great writer, like those above, they also serve to create layers of meaning in the way a koan or painting can contain complex fractal patterns of meaning. Reading a great writer, working with the nuances of language and narrative can literally lift the reader into a state of heightened consciousness. A place where new realisations about society, the self, the emotional depths and nuances of other people are elucidated in a way that's genuinely mind expanding. It's absolutely fine if you don't find this in literature - whether in a second language or your own. It's naive to assume that it doesn't exist because you personally can't perceive it. Aptly enough - that's a contradiction many writers have explored. Our tendency to diminish the inner lives of others, or the worth of things we cannot appreciate. One piece that springs to mind is David Foster Wallace's essay 'This is Water' - https://fs.blog/david-foster-wallace-this-is-water/ reply j7ake 2 hours agoparentprevIt’s a good one for ebook. Read it once and never touch it again. reply AlbertCory 2 hours agoparentprevSame. I actually took a course on Ulysses, taught by a guy who's been teaching it for 50 years. So that's about as ideal a reading experience as you can have. Verdict: meh. Revolutionary for its time, maybe (100 years ago). reply Jun8 3 hours agoprev“… with a reading ease of 74.9 (fairly easy)“ Fairly easy my foot! This rating aside, thanks for a great project. reply robin_reala 3 hours agoparentYeah, our reading score isn’t always perfect. I had to fix it for the Mina Loy collection[1] I did as because it’s free-form poetry without many full stops it thought the sentences were so long that it awarded it a negative score. I’ll look to see if there’s any obvious reason why this is being marked as fairly easy, but it’s probably an issue with the standard algorithm we use more than the code. [1] https://standardebooks.org/ebooks/mina-loy/poetry reply complaintdept 3 hours agoparentprev00.0 must be the output of /dev/random reply mikub 3 hours agoprevYou boys and girls are doing an fantastic job. I read alot of the old classics thanks to you. reply technothrasher 3 hours agoparentMe too! I just finished Ivanhoe and was just heading over to Standard Ebooks to find another one. reply kabdib 2 hours agoprevI listened to an audiobook lecture about Ulysses, and that helped me understand the book better (at least through one professor's own opinions). It took me six attempts to get through Gravity's Rainbow (and it was worth it). My record is still just about halfway through Ulysses before I put it down for a year or two. Still worthwhile. reply apetresc 55 minutes agoprevI'm a huge fan of Standard Ebooks, but it's pretty funny that twice in both the subject line and in the body of the e-mail they sent out announcing this, they typo-ed \"1,000th\" as \"1,00th\", given their whole raison d'etre. reply SamBam 1 hour agoprevLooks very good, although I'm surprised there isn't a page in the book detailing the date of first publication, the name and artist of the cover image, maybe the date the copyright expired, and all that kind of information. (I was particularly surprised by the cover image attribution omission.) Is this not standard? reply acabal 55 minutes agoparentSome of this information is included in each ebook's colophon, at the end of the book. reply SamBam 31 minutes agorootparentSo it is! I was unfamiliar with the word \"colophon.\" reply pentagrama 1 hour agoprev> Standard Ebooks' 1,000th title: Ulysses On that page I'm not finding that is the 1,000th title, and browsing around the site I can't find a counter either. I trust the OP who choose that title is right, but there is any title counter on the Standard Ebooks website? reply acabal 1 hour agoparentThere is no counter online, but you can trust me (the S.E. Editor-in-Chief) when I say I used Bash to count them :) reply ckmate-king-2 2 hours agoprev1. I think \"fairly easy\" badly mischaracterizes its difficulty. 2. I found Harry Blackmire, The New Bloomsday Book: A Guide Through Ulysses, very helpful. reply LVB 3 hours agoprevThis site works surprisingly poorly in Firefox (Mac). When I click a link, I usually get a blank page along with some CSP errors and have to refresh. No problems in Safari or Chrome. reply robin_reala 2 hours agoparentI use Firefox on Mac and work on the site, and I have to say that I’ve never seen that. Something to do with an extension maybe? reply LVB 2 hours agorootparentGood call. It turned out to be React Developer Tools causing the issues. reply rafram 2 hours agoparentprevRestart your browser? I use this site frequently in Firefox on macOS and have no issues. reply madcaptenor 3 hours agoprevI didn't know about this project but I'm really glad you're doing it. How do you choose which books to do? reply acabal 2 hours agoparentOur volunteers pick what books they personally want to work on. Those books have to meet our collections policy criteria: https://standardebooks.org/contribute/collections-policy reply mordechai9000 3 hours agoprev\"reading ease of 74.9 (fairly easy)\" \"O, rocks! she said. Tell us in plain words.\" reply JusticeJuice 2 hours agoprevCongratulations! reply g9yuayon 2 hours agoprev [–] > 268,481 words (16 hours 17 minutes) with a reading ease of 74.9 (fairly easy) It's interesting when China introduced Ulysses years ago, publishers and reviewers often mentioned that how challenging the novel was. Often cited reasons included extensive use of stream of consciousness, long and complicated inner monologue, multiple languages and idioms used, and large vocabulary. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "\"Ulysses\" by James Joyce is a seminal novel in English literature, depicting a single day in Dublin on June 16, 1904, through the lives of Stephen Dedalus and Leopold Bloom.",
      "The novel is renowned for its innovative narrative techniques and stylistic diversity, drawing parallels to Homer's \"Odyssey.\" It faced initial legal challenges for obscenity, delaying its full publication until 1922.",
      "This Standard Ebooks edition is based on the 1922 first edition with corrections from pre-1929 errata and is free of copyright in the U.S., though it may still be restricted in other regions."
    ],
    "commentSummary": [
      "Standard Ebooks has released its 1,000th title, \"Ulysses\" by James Joyce, which has sparked a discussion on Hacker News about its Flesch Reading Ease score of 74.9, despite its complex prose.",
      "Users debate the appropriateness of readability algorithms for modernist literature and suggest alternative methods for rating book difficulty, while also discussing reading strategies for \"Ulysses\" and the quality of Standard Ebooks' offerings.",
      "The conversation includes issues with the site's tech stack, comparisons to Gutenberg.org, and the subjective nature of literary appreciation, especially for non-native English speakers."
    ],
    "points": 227,
    "commentCount": 85,
    "retryCount": 0,
    "time": 1717166755
  },
  {
    "id": 40527730,
    "title": "Mozilla Enhances Firefox with Tab Grouping, Vertical Tabs, and On-Device AI Features",
    "originLink": "https://blog.mozilla.org/en/mozilla/heres-what-were-working-on-in-firefox/",
    "originBody": "Mozilla Here’s what we’re working on in Firefox May 30, 2024 Mozilla Last week we shared a number of updates with our community of users, and now we want to share them here: At Mozilla, we work hard to make Firefox the best browser for you. That’s why we’re always focused on building a browser that empowers you to choose your own path, that gives you the freedom to explore without worry or compromises. We’re excited to share more about the updates and improvements we have in store for you over the next year. Bringing you the features you’ve been asking for We’ve been listening to your feedback, and we’re prioritizing the features you want most. Productivity boosters like Tab Grouping, Vertical Tabs, and our handy Sidebar will help you stay organized no matter how many tabs you have open — whether it’s 7 or 7,500. Plus, our new Profile Management system will help keep your school, work, and personal browsing separate but easily accessible. Customizable new tab wallpapers that will let you choose from a diverse range of photography, colors, and abstract images that suits you most. Intuitive privacy settings that deliver all the power of our world-class anti-tracking technologies in a simplified, easy-to-understand way. More streamlined menus that reduce visual clutter and prioritize top user actions so you can get to the important things quicker. Continuous work on speed, performance and compatibility Speed is everything when you’re online, so we’re continuing to work hard to make Firefox as fast and efficient as possible. You can expect even faster, smoother browsing on Firefox, thanks to quicker page loads and startup times – all while saving more of your phone’s battery life. We’ve already improved responsiveness by 20 percent as measured by Speedometer 3, a collaboration we’ve spearheaded with other leading tech companies. And in that collaborative spirit, we’re also working with the Interop project to make it easy for people to build sites that work great across all browsers. We value your support in our efforts to improve cross-browser compatibility which is why we’ve added new features to easily report when websites aren’t working quite right; this feedback is critical as we look to address even small functionality issues that affect your day-to-day online experience. Making the most of your time online — without sacrifice Ensuring your privacy is core to everything we do at Firefox. Unlike other companies, who ask you to exchange your data in order to do even basic, everyday things online — you don’t have to give up your personal information to get a faster, more efficient browser experience with Firefox. Reading a news story in a different language or signing a form for school or work shouldn’t require you to give up your privacy. So, we’ve worked hard to make things like translation and PDF editing in Firefox happen locally on your device, so you don’t have to ship off your personal data to a server farm for a company to use it how they see fit — to keep tabs on you, sell your information to the highest bidder, or train their AI. With Firefox, you have a lot of choice — but you don’t have to choose between utility and privacy. Your data is secure, and most importantly, just yours. We are approaching the use of AI in Firefox — which many, many of you have been asking about — in the same way. We’re focused on giving you AI features that solve tangible problems, respect your privacy, and give you real choice. We’re looking at how we can use local, on-device AI models — i.e., more private — to enhance your browsing experience further. One feature we’re starting with next quarter is AI-generated alt-text for images inserted into PDFs, which makes it more accessible to visually impaired users and people with learning disabilities. Join us on this journey Our progress is driven by a vibrant community of users and developers like you. We encourage you to contribute to our open-source projects and to engage with us on Mozilla Connect or Discourse, and don’t miss our upcoming AMA on Reddit, which we’ll announce soon. Your participation is crucial in shaping what Firefox becomes next. Get Firefox Get the browser that protects what’s important",
    "commentLink": "https://news.ycombinator.com/item?id=40527730",
    "commentBody": "What We're Working on in Firefox (blog.mozilla.org)227 points by HieronymusBosch 23 hours agohidepastfavorite237 comments blizdiddy 22 hours ago> More streamlined menus that reduce visual clutter and prioritize top user actions so you can get to the important things quicker Oh no, more of this. What about less used options? How much slower? How much less discoverable? Desktop applications are already for power users almost by definition, let’s not slow them down for the sake of reducing “clutter”. Absolutely annoying trend of the last 15 years or so. reply JohnFen 22 hours agoparentThe dumbing down of applications, including FF, is incredibly annoying and off-putting. I get it, though. FF probably wants to be friendly and approachable to the average non-techie person. I just wish that it (and other applications) would at least have an \"expert mode\" that isn't aimed just at the casual user. reply layer8 20 hours agorootparentI like the approach where it’s configurable which items are available in menus and toolbars. The application can come with a default configuration that is suitable for the average and novice user, and power users can change the menus and toolbars to their needs. When not all items are shown, the respective menu or toolbar can have a “More…” item that opens the configuration dialog. That way it’s discoverable. reply jimjimjim 21 hours agorootparentprevI would love for applications and even web sites to have an expert mode. Maybe when the UX teams aren't looking devs could add a mode that puts Everything back in the UI. reply Teever 22 hours agorootparentprevThe implication of what you're saying is that it somehow isn't friendly and approachable to your average non-techie person, which doesn't make sense because the UI paradigm of browsers hasn't really changed for 20+ years and Firefox has been just fine for technies to switch their parents/grandparents over to for years now. The problem with Firefox isn't the UI, the problem with Firefox is mismanagement by the board that has been captured by Google and the lack of proper antitrust action against Google by the US and EU. reply nindalf 22 hours agorootparentFirst time I’ve heard Google being blamed for Mozilla’s self inflicted problems. Google is highly incentivised to make sure Mozilla survives with some reasonable market share. Without Mozilla antitrust enforcement becomes much more likely. That’s why Google pays Mozilla for being the default search engine provider. That’s the main source of revenue for Mozilla. If Mozilla could maintain 10% market share in perpetuity and also keep signing search licensing agreements with Google that works just fine for Google. Sadly it has a lot less than that. reply palata 21 hours agorootparent> Google is highly incentivised to make sure Mozilla survives with some reasonable market share. Assuming that \"surviving\" is good and \"reasonable\" is... what... 3% and dropping? reply Teever 17 hours agorootparentprevFirst time huh? That's surprising. It's only mentioned every time Firefox and Mozilla come up. I'm sure Google would like a slightly healthier captured 'competitor' but that doesn't mean that they didn't put them in this place through subversive practices. reply schmidtleonard 21 hours agorootparentprev> the UI paradigm of browsers hasn't really changed for 20+ years (Checks calendar) ugh, it really has been 15 years since they ground up the url box + search box into the big mystery meat mash-up that mostly works but is so damn obnoxious when it doesn't. reply cassepipe 20 hours agorootparentI thought I would miss the search box when it disappeared but actually the \"awesome bar\" is pretty awesome : You can disable search suggestions and it will only search your keywords in history/bookmarks/opened tabs and then you just tab into the list, else if there's nothing of interest in the list you can always press enter and it will launch a search in your default search engine. All that power accessible with just Ctrl + L, Tab and Enter. You can also launch @search_engine $keyword directly in the bar, search bookmark/history/tabs directly with */^/% symbols. You can't disable search suggestions on Chrome (I wonder why...) This is the main reason I am still on Firefox despite the weird UI design choices... which can easily be fixed with that script : https://github.com/black7375/Firefox-UI-Fix reply JohnFen 20 hours agorootparentI dislike the \"awesome bar\" quite a lot. Fortunately, as you say, you can disable a lot of its \"awesomeness\" (enough that I can live with it), but not all of it. You do have to do a bit of research to find out the various magic \"about:config\" settings you need to change. reply cassepipe 3 hours agorootparentIt's easy enough to bring back the search bar without about:config though : right-clik on the chrome then drag and drop the search box item reply schmidtleonard 18 hours agorootparentprevI'm glad you like it. But yes, it's time to give Firefox another spin. Hey, looks like youtube is un-broken in base FF! Keeping FF \"base\" for debug purposes is far more important than fixing my petty grievances so I have to pass on the tips, but I appreciate the sentiment. Here's hoping its next stint in the \"daily driver\" seat is a long one. reply saurik 20 hours agorootparentprev> FF probably wants to be friendly and approachable to the average non-techie person. Yeah... the problem is that these people are frankly never going to use Firefox unless driven to by their techie friends, and Firefox seems to have given up on the power user market that they used to own. reply Barrin92 20 hours agorootparentprevI'm also for user choice but this consistent notion that uncluttered UIs are only for non-techie people is very annoying. I'm a software developer by trade but that doesn't mean I want every UI to look like the inside of an airplane cockpit or a 90s teletext. The trend in UIs to use space and reduce interface actions to the most important actions is both aesthetically more pleasing and more functional. I do not look back to the time where every interface cramped 20 buttons in a tiny space and five of the buttons did the same thing. (looking at you, KDE settings menus) reply JohnFen 4 hours agorootparent> The trend in UIs to use space and reduce interface actions to the most important actions is both aesthetically more pleasing and more functional. Aesthetics is a matter of taste, of course, so there's no right or wrong there. Only if you like it or not. But the trend towards minimalist UIs is something that actively gets in my way and makes the software more difficult for me to actually use. That's why I dislike it. reply rurp 22 hours agoparentprevThe amount of regressions Mozilla has inflicted on Firefox in recent years blows my mind. One of their few big advantages over Chrome is better user control, but they have been working continuously to destroy that. Getting rid of most extensions and removing about:config are two big wtf decisions. All that does is piss off power users, which are the main advocates for this browser. Plus so many other baffling changes to remove or worsen existing features, along with the pointless UI reshuffles. Edit: I should clarify that about:config has only been removed from mobile Firefox, so far at least. reply aquova 22 hours agorootparent> removing about:config about:config is right where it's always been, unless I've missed something? reply 0x_rs 21 hours agorootparentThey're talking about the mobile application, where only a small subset of extensions was available for a few years. Access to about:config was disabled in official builds (but forks and custom builds such as some on F-Droid still have it). https://github.com/mozilla-mobile/fenix/issues/21276 reply Intralexical 19 hours agorootparent> Access to about:config was disabled in official builds (but forks and custom builds such as some on F-Droid still have it). From your link, there are problems that are specific to `about:config` only on mobile, due to Android OS and GeckoView API specifics: - There are preferences that …is reset when the app restarts. - There are many preferences that …do nothing. - There are preferences …which breaks interacting with some websites. - There are platform preferences that …result in startup crashes …and reinstalling results in the deleting of data like bookmarks, passwords, history, etc. https://bugzilla.mozilla.org/show_bug.cgi?id=1813163 Look, it’s not like we’re insensitive to the desire for configuration; it’s that we know that on Android there are footguns that don’t exist on desktop! We want to figure out a way to do this in a way that makes it difficult to break GeckoView. I’m sorry that this isn’t good enough for many of you, but with all due respect, you’re not the ones on the receiving end when somebody breaks their browser because they didn’t know what they were doing! https://old.reddit.com/r/firefox/comments/i51k0q/mozilla_cou... Kinda reasonable, honestly? It sounds like more than just \"page may render funny\" types of footguns. Unknown quantities of unlabelled \"Lose All My Data\" buttons are probably what this site would call an antipattern. > They're talking about the mobile application, where only a small subset of extensions was available for a few years. On the above Reddit link, they also explain the extensions thing was because they were \"literally not done implementing the APIs yet\" and focused on the \"highest priority extensions\". So, the \"big wtf decision\" might be why they released it in that state, rather than \"getting rid of most extensions\". reply AnthonyMouse 19 hours agorootparentThese seem like bugs that could be fixed, but it also seems like there should be relatively simple ways to work around them without disabling about:config, e.g.: If some settings can cause a crash (and you don't have a complete list), create a separate launcher that only edits the settings in about:config without ever trying to run other Fenix code, so the user always has a way to revert settings if they're causing a crash on startup. If some settings can be applied but are reset the next time the app runs, save the setting value itself and then re-apply it on each startup. Other problems like settings doing nothing or breaking websites are just ordinary bugs. Until they're fixed, the user can just refrain from using those settings, or revert them if they cause problems. These sorts of issues are acceptable for users knowingly mucking around in about:config, it's fine, fix the bugs when you get time. It's no reason to disable access to the other settings that actually work in the meantime. reply Intralexical 13 hours agorootparentI don't think anybody's saying the issue is technically infeasible to fix or work around. They just haven't had anybody to spend engineering time doing so yet. Perhaps: https://codetribute.mozilla.org/projects/fenix https://firefox-source-docs.mozilla.org/setup/contributing_c... We may disagree with Mozilla's priorities, but they do have finite resources. E.G. Should the extensions API have been delayed even more than it already was, to first build this separate `about:config` launcher as a kludge? They're not being wilfully bullheaded in this case, is what I mean. The \"P5\"/\"Enhancement\" Bugzilla issue is open, but unassigned. reply AnthonyMouse 10 hours agorootparent> They just haven't had anybody to spend engineering time doing so yet. But this has been the other major criticism of Mozilla. They take the money they get from Google and spend it on not-Firefox when Firefox market share should be their top priority, because if that number goes to zero they're defunct. reply NegativeLatency 19 hours agorootparentprevHide those options then? I can imagine many ways to \"fix\" the issue of having all those bugs without ham-fistedly removing the entire feature reply dralley 21 hours agorootparentprevYou've missed nothing, OP is full of it. reply mrandish 22 hours agorootparentprevYes! FF PMs and designers seem to HATE user customization, add-ons and power user capabilities yet those are the main reasons I came to FF and why I still use it. While FF is still the best browser for me (barely), it's only achieved this by all the alternatives enshittifying and dumb-ifying their browsers even faster than FF. As it is I have to spend significant effort under the hood customizing UserChrome scripts just to maintain minimal usability. (...and thank the heavens for Lepton (https://github.com/black7375/Firefox-UI-Fix), a regularly maintained, well-curated, all-in-one collection of essential Firefox fixes (which can each be turned off individually via flags.) reply shrimp_emoji 20 hours agorootparentprevSounds like they hired GNOME devs. reply lxgr 19 hours agorootparentprev> Getting rid of most extensions What extensions did they get rid of? reply AnthonyMouse 19 hours agorootparentThey changed the extension model which broke the extensions using the old model. Many of the developers didn't have the resources to rewrite the extension from scratch, or the new model had restrictions that made what the extension was doing difficult to impossible, so they disappeared. reply lxgr 18 hours agorootparentThat was the switch from their own API and unsandboxed model to the standards-based one, right? I think all in all getting sandboxing as well as making it easier to provide cross-browser extensions was worth the breaking change. reply AnthonyMouse 10 hours agorootparentMandatory sandboxing increasingly seems like a misfeature. Now you can't install an extension that does a lot of things that are useful, ostensibly to protect you from malware. Optional sandboxing can be good because users can have more confidence that something can't hurt them. But it can also be bad because users will have more confidence that something can't hurt them, and then it does anyway. Meanwhile the ability to exercise arbitrary permissions given the user's authorization is something we've lost. And what have we gained, when the user just runs how_to_actually_uninstall_mcafee.avi.exe and gets the malware regardless? reply lxgr 5 hours agorootparentNo, you can still grant permissions to web extensions, but the difference is that it's now explicit, and it is at least possible to design extensions with minimally-required privileges. This vastly reduces the blast radius of an extension publisher takeover or an insider attack, for example. > what have we gained, when the user just runs how_to_actually_uninstall_mcafee.avi.exe and gets the malware regardless? Nothing would happen if I double-click that file, since it doesn't run on my OS. On the other hand, a compromised browser extension could deal an incredible amount of damage to me. Are you saying that because we can't protect all users from all harm, we should just give up? reply olejorgenb 22 hours agorootparentprev> and removing about:config What? reply josephcsible 18 hours agorootparentOn mobile. reply anal_reactor 20 hours agorootparentprev> Getting rid of most extensions I am still butthurt reply krapp 20 hours agorootparentWelp. Name checks out. reply shrimp_emoji 20 hours agorootparentSecond-order name checkout reply robertlagrant 20 hours agorootparentThird order if improperly prepared crustacean. reply shrimp_emoji 20 hours agorootparentStay away from Red Lobster. (Personal experience.) reply kreyenborgi 22 hours agoparentprevI hope they don't put more of the menu items into unremovable buttons in the address bar (which already contains four icons: tracker protection, certificates, containers, bookmark) or next to it (both an overflow menu for extensions and an overflow menu for ..other things? and a hamburger for ..other-other things? in addition to the buttons I actually do want there). On my laptop I can hardly read the domain name of the url unless I fullscreen the window. reply mrandish 22 hours agoparentprevMy cognitive style in using computer interfaces is to remember where things are, so features that move things around (or hide them) based on some algorithm are not only annoying but slow down my workflow and increase cognitive load. Hopefully, they'll offer some way to turn this off. reply iforgotpassword 22 hours agoparentprevIDK, they're just following what Chrome has been doing forever now, and it has worked there. I guess that really is what the vast majority of normal people want. And considering how even among my most tech-savy friends and colleagues Chrome appears to be the browser of choice, it seems even these folks don't really care... reply yjftsjthsd-h 22 hours agorootparentThat doesn't necessarily follow; a person could easily argue that Chrome won because Google abused its search monopoly to push it, then kept winning because of inertia. Unless your friends specifically prefer Chrome because of UX reasons, it could be unrelated. reply Kye 22 hours agorootparentFirefox certainly didn't help by becoming a bloated mess. Firefox was bad when Chrome came out. All Chrome did was the same thing Firefox did to IE in the same situation: take advantage of complacency. Google's heavy and sometimes ethically questionable promotion of Chrome was an amplifying force, but not sufficient. reply muxator 21 hours agorootparent> Firefox was bad when Chrome came out Funny how different people's experiences can be. When Chrome came out in 2008 I remember trying it just for fun (more than once), and it never clicked with me, so I stayed with Firefox. What was harder for me was keeping with all the unnecessary redesigns, empty eye candy, dumbing down, the Fennec debacle (exensions used to work, then they suddenly stopped for a bunch of years; plus, no keyword search for no reason). By that time staying away from Google had become a goal in itself, but I understand who migrated to chrome. Anyway, I am firmly convinced Chrome won because of Google's abuse of its web search monopoly. The innovations that its team introduced were groundbreaking from a technical point of view, but Firefox was a perfectly capable browser. reply palata 21 hours agorootparentprev> Google's heavy and sometimes ethically questionable promotion of Chrome was an amplifying force, but not sufficient. I strongly believe it was sufficient. Which does not mean that Chrome was not better when it came out. But most people really don't want to try new tech: because you were excited by Chrome back then does not mean, by far, that everyone on the planet was. reply yjftsjthsd-h 22 hours agorootparentprevYeah, that was always funny to me too... Google built a browser that was actually faster and more secure, and then decided to be anti-competitive instead of just letting it win. Of course, they didn't actually get destroyed in court for it, so I suppose being evil works in this case. reply palata 21 hours agorootparent> and then decided to be anti-competitive instead of just letting it win Or precisely because it was not enough to have the better product? Don't tell me you believe that the better product usually wins. Marketing wins. reply yjftsjthsd-h 21 hours agorootparentFirefox was doing fine beating IE on merit once MS was prevented from behaving anti-competitively. reply GuB-42 18 hours agorootparentThey did fine even before MS was prevented from behaving anti-competitively. When Microsoft had to put its browser choice banner in the EU, Firefox was already at its peak. Microsoft did it to themselves, IE4 was a great browser for its time. It was bundled in Windows, for free, it was anticompetitive, but its technical merits made a big part of its adoption. But as it became old, and Microsoft did nothing to improve the situation. When Firefox came out, it didn't need advertising from a tech giant. It was a godsend for developers (it followed standards!), who advertised for it on their webpages, user tried it and loved the cool new features (tabs!) and installed it on their friends computers, the whole tech world except for Microsoft rooted for Firefox because it was just so much better. When trade commissions arrived, the job was already done, Firefox had enough critical mass that it couldn't be ignored and IE-only sites were becoming a thing of the past (except for corporate intranets, these have always been the worst). If anything, they helped Chrome more than they helped Firefox as it was just launching during that time. Marketing alone doesn't win for browsers. It is really easy to switch browsers, all it takes is to have some semi-geek friend to come by and say \"hey, try this new browser, it is just like the old one, but better\", and if it really is better, that's it. That's how Firefox was winning in 2010. And look how hard Microsoft is trying with Edge, and fail, that's because Edge has nothing over Chrome, unfortunately, Firefox don't have much over Chrome either (yeah, privacy, maybe 5% care). reply dralley 21 hours agorootparentprevThe base of internet users was a lot smaller and more technically-inclined in 2004 than in 2014 reply Kye 20 hours agorootparentWas it though? This was well after AOL and the Endless September. There were rows of Complete Idiots Guides and For Dummies books on using the internet in major book stores. You've Got Mail made a quarter billion in the box office a half decade before. This was the time of normies. All the @aol addresses still in use were minted here. reply palata 20 hours agorootparent> There were rows of Complete Idiots Guides and For Dummies books Which says that people were trying to learn how to use their computer, right? Nowadays professors need to teach their university students what a file is... I can definitely accept that the users were more technically-inclined in 2004 than in 2014. reply strken 20 hours agorootparentprevThis is a great argument for why the average internet user in 2004 was less technical than in 1994, but it says absolutely nothing about 2014. reply Kye 20 hours agorootparentI thought that was just a given. How could the average be the same or more technical after going fully mainstream? reply palata 21 hours agorootparentprevWhich doesn't prove it would have been the case for Chrome. Especially if people had just changed to Firefox :-). reply iforgotpassword 11 hours agorootparentprev> That doesn't necessarily follow; a person could easily argue that Chrome won because Google abused its search monopoly to push it, then kept winning because of inertia. Sure, for the average Joe I might buy this, but also it's the same kind of people who can't tell the difference between the address bar and the search field on the Google homepage, so I'm still kinda skeptical they don't actually refer a UI with as few options and buttons as possible.... > Unless your friends specifically prefer Chrome because of UX reasons, it could be unrelated. ... But I absolutely can't follow you here. What other reasons should there be? That that one or two websites they infrequently visit don't work? If you can't be arsed to just use chrome for these specific occasions and stay with the customizable, powerful alternative for the rest of the time, I don't believe you're someone who'd like a UI with more than three buttons anyways. (And on that note I've yet to encounter a site that doesn't work in Firefox, but maybe that actually is a thing in places other than Europe...) reply palata 21 hours agorootparentprevIMO the vast majority of users cannot make the difference between Chrome and Firefox. They just use whatever they are used to and like it because they are used to it. Over the years, I have converted people to Firefox, Chrome, Brave, Safari. The only common denominator is that they all strongly prefer the one they use now. reply lainga 22 hours agorootparentprevQuickly thumbing through my product book for \"gaining market segment by making yourself indistinguishable from competition\" reply a0123 22 hours agorootparentYes, but the current design and direction are very clearly not working. You can hardly blame them for trying to emulate what's currently working for everyone else when everything you've tried so far has led to your downfall. reply rurp 20 hours agorootparentThey have been dumbing down Firefox and making it more Chrome-like for years now. I would argue that is the strategy which is clearly not working. reply pessimizer 21 hours agorootparentprevTheir downfall came when they changed everything that they were doing, and subsequently nuked every reason why someone would prefer firefox other than that they weren't google. They transformed firefox into wonky chrome, even copying superficial UI stuff. And then got very hostile about any criticism of this project, actively destroying all of the goodwill they built up. That alienation of their stubborn, fanatical base took years to accomplish. People would move back to firefox if it were still anything like firefox. reply palata 21 hours agorootparentNot that I disagree with the alienation of their fanbase. I personally use Firefox mostly because it is not Google, certainly not because it is Mozilla (and to be fair I like the Firefox Containers). But I am not convinced most people see it like this. Most of the people I know honestly don't make the difference between Firefox and Chrome: they just use what they know. I think Chrome won those people because Google managed to convince them that \"Internet == Chrome\", not because they consciously chose Chrome as a better alternative. reply bakugo 21 hours agorootparent> But I am not convinced most people see it like this. Most of the people I know honestly don't make the difference between Firefox and Chrome: they just use what they know. And those people are not Firefox's audience. They don't care about their browser as long as it loads facebook.com, so they're just going to use whatever browser is more popular or pushed harder onto them, which is Chrome. If Firefox positions itself as just a Chrome clone, there's no reason to use it over Chrome. If Firefox wants to survive, it needs to position itself as a niche alternative for people who actually have preferences for what their browser is like, and do not like Chrome. Of course, it won't ever be as popular as Chrome, but that battle was lost a long time ago. reply palata 21 hours agorootparent> If Firefox wants to survive, it needs to position itself as a niche alternative Or... I'm still hoping that the EU Digital Markets Act may help. reply dagurp 22 hours agorootparentprevOr is it Google deciding for you what you should want? reply squarefoot 21 hours agorootparentBefore someone flags the above comment as trolling, read on: \"One thing Mozilla does have going for it is a lot of money—more than $1 billion in cash reserves, according to its latest financial statement. The primary source of this capital is Google, which pays Mozilla to be the default search engine on the Firefox home page. Those payments, which started in 2005, have been increasing—up 50% over the past decade, to more than $450 million, even as the total number of Firefox users has plummeted. In 2021 these payments accounted for 83% of Mozilla’s revenue.\" source: https://www.bloomberg.com/news/newsletters/2023-05-05/why-go... I read it as: \"If Google says jump we jump\". reply lxgr 19 hours agoparentprev> What about less used options? Put them in sub-menus? I really don't see what's bad about doing user research and organizing feature discoverability by importance (rather than, say, profitability, which a for-profit browser might be tempted to do). reply userbinator 19 hours agoparentprevEvery time someone complains about \"clutter\" I suspect they're trying to turn users into lusers --- stunting their chance of growth by removing everything that could possibly catch their attention, so they can be lead blindly and exploited for whatever is most desirable. Meanwhile monitors and their resolutions have grown tremendously, so there's a lot more space for things. Yet they'd rather you give up control and argue that the tiny extra amount of space you get is a good idea. reply iza 20 hours agoparentprevI hope this means they're changing the main menu back to functioning like an actual menu instead of a weird panel thing, but somehow I doubt it. reply Zecc 8 hours agorootparentI don't know if you know this, but you can still get the traditional kind of menu if you tap the Alt key. It won't stay permanently visible if you change focus back to the content page though. reply mwcz 20 hours agoparentprevIf all actions and options were reachable from a fuzzy finder, then I'd be very happy with all UI components being hidden to reduce clutter. reply resource_waste 22 hours agoparentprevAnd all the old documentation/forum help to change settings is obsolete, but google still has them Indexed. The only company worse for this is Microsoft. Any google search for a setting has been outdated, the location has changed, the wording has changed. I'm completely convinced that there are FAAMG plants in FOSS company that have 1 job: Make pointless changes to waste engineering hours. Firefox and LibreOffice convinced me of this. reply yjftsjthsd-h 22 hours agoprev> We’ve been listening to your feedback, and we’re prioritizing the features you want most. > Tab Grouping, Vertical Tabs, and our handy Sidebar will help you stay organized no matter how many tabs you have open — whether it’s 7 or 7,500. > Plus, our new Profile Management system will help keep your school, work, and personal browsing separate but easily accessible. Wow, that ... actually is what users have been asking for, for ages. Nice. > More streamlined menus that reduce visual clutter and prioritize top user actions so you can get to the important things quicker. Oh, there's the other foot dropping. I wonder how SeaMonkey is doing... reply thwarted 21 hours agoparentI've been using multiple profiles in Chrome for years for keeping things separate… right up until my company started using Google's centralized browser/device management which infects the entire browser not just the single profile that is tied to my corporate Google account. Specifically, they forced a homepage setting browser-wide that can not be changed on a per-profile basis. So multiple profiles are seemingly difficult to get right/fully isolated. I hope Firefox doesn't have similar limitations. reply lxgr 19 hours agorootparentMy guess would be that your company probably wants the MDM profile to be globally enabled or is alternatively limited by their MDM's or Chrome's capabilities. > I hope Firefox doesn't have similar limitations. I hope Firefox doesn't implement MDM at all, or at least provides a non-negotiable way for the user to opt out of it on a per-profile basis. reply skydhash 18 hours agorootparentprev> my company started using Google's centralized browser/device management which infects the entire browser not just the single profile that is tied to my corporate Google account Personal profiles on your work computer? That's a no for me. The only account I brought to my work computer was my Github's one. And I'd rather use Incognito Mode or another browser if I want to do something I did not want associated with my google account. reply 1oooqooq 12 hours agorootparentlearn how to properly manage ssh keys and stop using your personal account. reply pleb_nz 21 hours agorootparentprevFirefox does have profiles and has had for as long as I’ve needed them. For some reason they’ve always been hidden away and needed different args on start to change the profile being started, or the use of an extension to allow profile switching. I don’t use them anymore but from what I remember they worked well and were very isolated. Hopefully they’re using the current implementation and just making it more user friendly? reply worble 21 hours agorootparentYou can use the url about:profiles to manage profiles too, which is still hidden away and not a great UI, but at least you don't need to use an extension or the command line. reply akdor1154 20 hours agorootparentprevAs of a recent update, the Profile Manager is available as a startup option if you right click the ff icon (on gnome at least, i assume this was implemented cross platform) reply yjftsjthsd-h 4 hours agorootparentIt's not just a GNOME thing, but it might not be that cross-platform. On Linux, there's a firefox.desktop file that describes the application to your DE/launcher, and it looks like this on my box: [Desktop Entry] Actions=new-private-window;new-window;profile-manager-window Categories=Network;WebBrowser Exec=firefox --name firefox %U ---snip--- [Desktop Action profile-manager-window] Exec=firefox --ProfileManager Name=Profile Manager So that'll work on anything that uses https://specifications.freedesktop.org/desktop-entry-spec/de... and there could be parallel features for Windows/Mac, but there also might not be or Firefox might not implement them. reply lxgr 19 hours agorootparentprevIt's definitely not an option on macOS, at least. Holding the \"option\" key is even somewhat of an UI convention on macOS to allow opening an alternative media library (e.g. for Photos, Music aka iTunes etc.), yet Firefox maps it to \"startup in safe mode\". reply jasonjayr 21 hours agorootparentprevCan you still launch chrome manually with the --user-data-dir=/custom/path option when MDM is enabled? reply tresclow 22 hours agoparentprevI remember around 2012 Firefox did use to have tab grouping, and they dropped the functionality after. I remember a friend of mine being annoyed for that, although I myself didn't appreciate much the functionality. reply yjftsjthsd-h 22 hours agorootparentOh, it's better than that: They had tab grouping as a native feature, and it was great, then they factored it out into an extension... and then they changed the way extensions worked, and didn't bother porting it. Kind of an insulting way to kill a feature IMO. reply Kye 22 hours agorootparentprevI could never figure out how it worked. reply jackson1442 21 hours agoparentprevI really hope the profile management system keeps tab containers working the same. It's incredibly useful to have different containers automatically generated for AWS console sessions. reply 1oooqooq 11 hours agorootparentcurrent implementation of container is terrible. basically something a google PM thought of so you can shop incognito and not be incognito. reply morsch 22 hours agoparentprevI'll take the \"streamlined\" menus if it gets me first party vertical tabs and auto segregation of profiles. The latter especially, I'm excited about whatever they come up with. The former, I think it'll take a long time to reach feature parity with existing add-ons, if they ever get there, seeing how they tend to keep novice users in mind. reply raffraffraff 22 hours agoparentprevhonestly, container tabs are way more useful to me than profiles every were. I can have 3 different Gmail accounts and a bunch of AWS accounts open in the same browser, with different colours for each so that I can figure out which is which. How about it they continued to iterate on container tabs and make it possible to pattern match urls to containers without a 3rd party addon. reply seba_dos1 21 hours agorootparentOh yes, native matching would be faster and remove plenty of glitching opportunities if done right - you can often see the tab being reloaded into a proper container and taking twice as long to load as it should, and sometimes the tab ends up being duplicated. You also lose the tab's history when it switches between containers at the moment. reply abdullahkhalids 19 hours agorootparentIf you have rule that gmail should open in the red container, then it will always create a new tab if you try to open gmail from a non-container. This the correct behavior. Otherwise, cookies/browser data would have to be invisibly switched as the user goes back and forth in history. Invisible switching is bad design. reply seba_dos1 19 hours agorootparentAs it is right now, if you go to a containerized URL using the adddress bar you may end up losing the tab's history completely when it gets switched between containers, without warning. You can't even get the tab back with Ctrl+Shift+T cause it instantly switches to the new container as well. That's much worse design. At least that's the behavior you get with Google Container and (Mozilla's own!) Facebook Container extensions. Twitter Container gets it right. Having it centralized could remove plenty of potential failure points. I just accidentally clicked at a YouTube link from a comment on this very page and couldn't get back! reply abdullahkhalids 17 hours agorootparentYou should get the generic Firefox Multi-Account Container extension [1] instead of individual ones. It has a setting to either replace or open a new tab, when clicking a link. [1] https://addons.mozilla.org/en-US/firefox/addon/multi-account... reply yownie 20 hours agoparentprevHeeyyyy are we actually getting Tab Grouping back? Finally? reply pessimizer 21 hours agoparentprevIf you had 7,500 tabs open, it would take firefox approximately three and a half weeks to start. reply yjftsjthsd-h 21 hours agorootparentHaving hit 4-5k tabs in a session, it's really not that bad of a perf hit. It's also an unreasonable thing to ask of the browser, but it holds up well enough. reply seba_dos1 21 hours agorootparentprevI regularly have 10 times as much open in Firefox and it works just fine. Chromium barely works at a fraction of that (or at least used to, haven't felt the need to recheck for a few years now). reply chgs 21 hours agorootparentYou have 75,000 tabs open? I rarely have more than about 300 to be honest, how do you keep track? reply seba_dos1 21 hours agorootparent> how do you keep track? I don't. I usually work with just the latest subset of a few dozen or so, the rest just sits there until I decide that it's time to close them all (like I did last week, so I'm at just 700 right now). I also use vertical tabs, see my other comment here for how it looks like. Keeping tabs open lets me quickly context-switch, which is super helpful when you get distracted and don't remember what you have worked on an hour ago. No need to clean them up, as they get eventually unloaded, but still remain quickly accessible if needed (which is much less cumbersome than the history view which I never really use unless trying to find something I was on months ago). reply chgs 5 hours agorootparent700 is fine, but that’s just 1% of 75k tabs reply seba_dos1 5 hours agorootparentDon't worry, it will get back to 75k again in just a few months. reply 1oooqooq 11 hours agorootparentprevanother case of someone refusing to use organized bookmarks. I've been there too reply seba_dos1 9 hours agorootparentBookmarks are completely irrelevant as they require intentional curation and are mostly useless to me. reply hammyhavoc 4 hours agorootparentprevPerformance of bookmarks also seems to take a nosedive in most browsers once you start getting into big numbers. reply darth_avocado 20 hours agoparentprevI just use Arc, it already has all of this reply iczero 19 hours agorootparent> I just use [yet another closed-source Chromium fork developed by some strange startup which forces you to sign in before you can do anything and will likely not even bother to continue supporting Manifest v2 APIs], it already has [such a long list of antifeatures that it could practically double as marketing material for the FSF] ok? reply torstenvl 22 hours agoprev> Intuitive privacy settings that deliver all the power of our world-class anti-tracking technologies in a simplified, easy-to-understand way. > More streamlined menus that reduce visual clutter and prioritize top user actions so you can get to the important things quicker. Stop it. Gnome-ification is incredibly user hostile. Almost every time folks \"simplify\" a UI, they actually make it far more complicated instead. If your users now have to use dconf or userChrome.css to do the things they are accustomed to doing, you have not simplified anything. reply illiac786 3 hours agoparentIt sounds like in your view simplifying a UI is not possible or has never been successfully implemented. I think it is very much possible, very much needed by lots of software out there, but indeed it’s hard. The hardest part is not alienating existing users when messing with muscle memory. reply gamepsys 22 hours agoprevAs a Firefox daily user, this list makes me better understand why Firefox has such a low market share. Reorganizing menus or adding alt text to PDFs won't make any impact on user adoption/churn because users don't care. The alt text specifically sounds like resume engineering for some developers that want \"on device generative ai\" as a bullet point on this LinkedIn. Bullet points like this wouldn't be a problem if Firefox had more resources, but the list of actually pretty small and doesn't have a lot of space for fluff. I think they should pick 2-5 things and focus on those, such as multi-device sync, privacy, speed, battery life performance, customization, etc. reply yjftsjthsd-h 22 hours agoparent> I think they should pick 2-5 things and focus on those, such as multi-device sync, privacy, speed, battery life performance, customization, etc. They did; they've been pushing on speed and privacy for a long time. It mostly turns out that the problem isn't usually technical superiority, unfortunately. (The problem is that Microsoft shoves Edge in users' faces every chance they get, and Google has been giving Chrome an unfair advantage for ages (notice that when people complain about Firefox being slow, it's usually on Google sites).) reply leoc 19 hours agorootparentDidn't they fire the Servo team? reply aquova 22 hours agoparentprevWithout even scrolling down, there's a header describing the work they're doing for \"Continuous work on speed, performance and compatibility\", exactly what you asked for. Anytime Firefox works on UI stuff, people seem to get really annoyed they aren't working on something else. You mention that \"Reorganizing menus or adding alt text to PDFs won't make any impact on user adoption\", I think it's the exact opposite. There is no greater thing the average everyday Joe cares more about than the UI, and if they just called it good 10 years ago, the project would be in an even worse state. reply kreyenborgi 22 hours agoparentprevAre you saying that they should avoid writing about features that aren't \"core\" (privacy, speed, battery life, sync) when telling you about the work they've done? I mean, they do say that > You can expect even faster, smoother browsing on Firefox, thanks to quicker page loads and startup times – all while saving more of your phone’s battery life. and regarding privacy > we’ve worked hard to make things like translation and PDF editing in Firefox happen locally on your device, so you don’t have to ship off your personal data to a server farm for a company to use it how they see fit – so they seem to have ticked your boxes, but they've also committed the sins of boasting about design and used the \"AI\" buzzword. Yes, \"most people\" don't care about \"adding alt text to PDFs\" but then most people don't even know what accessibility means. For the people who do need it, it's essential, and I think it would be awesome if it happens on-device instead of giving google/amazon/microsoft a description of every pdf image a blind person might want described. reply glenstein 21 hours agoparentprevI think this perpetuates the fiction of assuming browser share is directly tied to specifics of browser functionality instead of more important macro forces like differences in brand awareness and ability to leverage market sector domination. There's been a proliferation of conflicting just-so stories purporting to explain the history of Firefox's browser market share, and they seem to relate to idiosyncratic preferences for customization instead of actual market dynamics. reply em500 22 hours agoparentprevHow are they stretched for resources? Mozilla's revenue is around a half billion USD per year. reply luyu_wu 22 hours agorootparentIt's important to note here that Mozilla spends the majority of that money on non-Firefox projects (something that has pissed off people who have wanted to donate to Firefox specifically for ages). Firefox itself gets a sliver of that. reply fabrice_d 22 hours agorootparentLike many you are likely confused by the Foundation (MoFo) vs. Corp (MoCo) finances. You can only donate to MoFo, and indeed it doesn't spend much (if anything) paying Firefox staff. This is because Firefox staff is on the MoCo payroll, which gets its revenue mostly from their search engine deal with Google. MoCo actually pays MoFo to use the trademarks that are owned by MoFo. reply dralley 21 hours agorootparentprev>something that has pissed off people who have wanted to donate to Firefox specifically for ages Get pissed at tax law, not Mozilla. Mozilla Foundation cannot legally funnel tax-exempt donations into the development of products for a for-profit entity (Mozilla Corporation). That would be a crime. reply strken 20 hours agorootparentWow, if only there was a separate corporate entity which could accept donations but which wasn't the Foundation. I wonder what Mozilla would call such a Corporation. reply yjftsjthsd-h 14 hours agorootparentIANAL - I kinda thought a company can't take donations? So obviously the answer is a Firefox Pro that is 99% the same but lets us give them money. Maybe give it crazy features like a compact layout and the ability to run extensions locally. reply gravescale 20 hours agorootparentprevMoCo could start an OnlyFans for \"donations\", it's a pretty hot fox after all, what with being, y'know, on fire. reply strken 20 hours agoparentprevThe alt text thing sounds like someone who thinks disabled access to the internet is an Important Thing. I'm not sure it will end up being better than an extension or doing the image recognition in a screen reader, but it's not necessarily cynical resume padding. reply lern_too_spel 22 hours agoparentprev> As a Firefox daily user, this list makes me better understand why Firefox has such a low market share As another Firefox active user, I say this has nothing at all to do with it. I use Firefox because it lets me control what I need to control to be productive, but I recognize this isn't why most people used Firefox at its peak. 99% of the reason Firefox got so big was that IE was so awful. Edge is serviceable, so that takes away the vast majority of Firefox's former user base. Firefox isn't going to grow as big as it was before Edge came along unless one of the major consumer OS provides a broken default browser on its platform and allows Firefox to run on its platform (iOS fulfills only one of the requirements). It won't grow as big as it was before Chrome came along unless both of the earlier requirements are met and Firefox renders Google sites better than Chrome. reply jwells89 16 hours agorootparent> Firefox isn't going to grow as big as it was before Edge came along unless one of the major consumer OS provides a broken default browser on its platform and allows Firefox to run on its platform (iOS fulfills only one of the requirements). And even that “requirement” isn’t filled to anywhere the same degree — Safari has its issues but IE was on a different plane of existence in terms of brokenness, with some of the barest of basics being absent or needing hacks to make work (remember the DirectX filter hacks required just to have transparency support for PNGs?). reply gamepsys 21 hours agorootparentprevI know we aren't going back to peak popularity. The concern is that FF moves to so little marketshare that websites no longer build for compatibility. reply tapper 20 hours agoparentprevalt text is grate for blind people thanks. reply finack 19 hours agoparentprevFirefox has a low market share because Chrome is the default browser on so many systems and in IT departments everywhere. 99% of people don't think about which browser they want to use. reply Night_Thastus 23 hours agoprevI still remember the launch of Firefox 3 back in the day. I'm so glad it's still around, and they keep working on it. I don't think there's a single piece of software I've gotten more utility out of in all that time. It's definitely not perfect, but we'd be in a worse world without it. reply i80and 22 hours agoparentIt's funny to think about in retrospect how exciting Firefox 3 was. Planning began in 2006, and release was 2008. Acid2! Cairo! Better Linux theming! It was just such a different release cadence. reply OtomotO 22 hours agoparentprevI have fond memories of 3 and 4... Started using it somewhen around 2 :) reply sli 22 hours agorootparentI have fond memories of discovering Phoenix ~v0.1 during high school and mostly never looking back. Had a short stint with Chrome until it intermittently stopped even attempting to load pages. Switched back and couldn't imagine daily driving Chrome. reply dochtman 22 hours agorootparentprevI remember when it was still called Phoenix — the lean pure-browser reborn “from the ashes” of the Mozilla Suite (SeaMonkey). Amazing run… reply FounderBurr 22 hours agoprevI stopped donating when they declared ‘rss is too complex to be maintained and is also old and gross and stuff, so we removed it’. Oh by the way totally unrelated here is Pocket(tm)! reply yownie 20 hours agoparentYes I agree that was some straight up bullshit reply michael9423 21 hours agoprevThe way I see it, there was a time when FF had the chance to not accept the google contracts and go 100% independent with their own search engine or something similar. This would have resulted in FF actually siding with users. It would have resulted in FF radically rejecting ads, and this would have changed the entire web because people would have flocked to Firefox. But Mozilla leadership did not have an interest in this, in light of endless google money. Google essentially captured Mozilla back in the day. Now people have got used to ads because they only could chose between browsers that don't protect them. Brave is trying to correct the mistake of FF and is attracting a small userbase, but not large enough to make a big difference, because the times have changed a lot. Now with the new management at Mozilla and the dim outlook for the next 5 years, the realization is kicking in that listening to users is actually important. But the elephant in the room is the contracts with large ad-based search engines. reply palata 21 hours agoparent> This would have resulted in FF actually siding with users Sure, but would the users have sided with FF? I'm not convinced. The users use whatever BigTech tells them to use, that's how I see it. > Brave is trying to correct the mistake of FF Brave is Chromium. I don't see how this would even begin correcting the mistakes of FF. The one thing FF does right is that it is not Chromium. reply ajdude 8 hours agorootparent> End users do not care about the browser engine but they certainly care about the search engine. I remember several years ago when Firefox changed their default search from Google to Yahoo, so many clients that we supported complained that IT's response was just to switch everyone over to chrome. To be fair the biggest issue was that yahoo was worse with dangerous ads in their results then Google was the time. reply michael9423 17 hours agorootparentprevEnd users do not care about the browser engine. But as I said, Brave is trying. It won't be successful in the way FF could have been. reply palata 10 hours agorootparent> End users do not care about the browser engine. But that's the problem, isn't it? Google controls Chromium, everybody uses Chromium, so Google controls the web. reply michael9423 10 hours agorootparentNot really. If everyone would use a chromium version that radically blocks trackign and ads, google wouldn't control anything. reply palata 7 hours agorootparent> google wouldn't control anything. The web is more than tracking and ads. Chromium is not an ad engine. reply paulryanrogers 19 hours agoparentprev> ...this would have changed the entire web because people would have flocked to Firefox. One cannot compete with those who hold the choke points. Google owned the Web when they launched Chrome. Microsoft and Apple own desktop and mobile defaults. With three major competitors setting defaults, nagging users, and aggressively developing their browsers, Firefox was doomed. Only unskippable browser ballots, with random order, and outlawing promotion of owns own browser from a dominant business would help. reply michael9423 17 hours agorootparent15 years ago, lots of things were possible. Most people were using firefox, it had like 30 or 40% market share. reply paulryanrogers 5 hours agorootparentOnly because Microsoft was asleep at the switch for years beforehand, allowing Firefox to establish a beachhead. Not long after Chrome launched and mobile phones grew in importance (~2010), Firefox was in steady decline. My argument is that short of ballots and anti-trust enforcement, nothing could have stood against three chokepoint-holding and well resourced competitors. reply squidbeak 21 hours agoparentprevIs a successful search engine such an easy thing to produce? reply anthonyhn 19 hours agorootparent> search engine such an easy thing to produce? Yes, I run my own independent search engine[0]. > successful Now that's the challenging part, especially since Mozilla needs to fund browser development. The initial differentiation for a Mozilla search would have been difficult, but at their peak they had 30%+ market share, and if the default search on Firefox was Mozilla Search then they might have been able to make it all work financially. DuckDuckGo makes over $100 million in revenue per year[1], if a Mozilla Search made as much money annually, then even though it's below their current $500 million search contract with Google, with some fiscal responsibility they probably would have had enough to support a search engine and browser development concurrently. [0] https://ichi.do/ [1] https://techreport.com/statistics/software-web/duckduckgo-st... reply amanzi 21 hours agoprevMozilla asks people to contribute ideas on \"Mozilla Connect\", but then chooses to ignore most of the top-rated ideas. To be fair, better tab management is first and third of the list, so they are looking to improve that one. But PWAs is second on the list, and there's no mention of this in their announcement. Bringing back proper PWA support seems to align perfectly with Mozilla's mission, so it blows me away why they aren't actively working on it or talking about it. The other things mentioned are just features that I would expect to keep improving over time, but are nothing groundbreaking. reply altairprime 21 hours agoparentPWAs, as in \"File > Save Website As... > website.exe\" on a PC? reply cpeterso 20 hours agorootparentThe \"PWA support\" that people are clamoring for is the option to open a web page shortcut in a browser window without an address bar. reply tredre3 20 hours agorootparentThat's a big part of it, but PWA offers more than that! For example it has facilities for caching the web application so that it can work offline. They can also register URL handlers on the system, which is very useful. reply finack 19 hours agorootparentPrograms that don't require an entire web browser to run can already do all of those things. reply gsuuon 18 hours agorootparentThey can also do a lot of other things that a browser's sandbox would not allow reply finack 2 hours agorootparentUntil Google decides to give developers the ability to let the browser invade the rest of the system, which, in six months, will suddenly be necessary for everyone to do things like bank online and pay utility bills. reply Ciantic 21 hours agoprevI switched to Firefox about 3 weeks ago, what is killing me is dragging the tabs. It is so much worse with Firefox. Someone has made a video of this if one can't remember how they differ: https://www.youtube.com/watch?v=jRuBDH32hPI With Chrome I can drag a tab and it moves with me, with Firefox the tab being dragged blocks the view. Also since with Chrome, the tab moves with the mouse I can with one drag move a tab and use Windows' snap-to-edge feature. This is a two-step process with Firefox. Firefox is an underdog, it should focus on making the UI familiar and fast, but keep customizability. Another thing I noticed is that Firefox doesn't allow to keep unpacked extensions loaded as Chrome does, there is no good reason why not, luckily I fixed that with some userChrome scripts. reply seba_dos1 22 hours agoprevSomewhere in the last two months Firefox Nightly got a massive performance boost on GNU/Linux. Thanks! Nice that vertical tabs are officially coming back, although I've been using them in a DIY manner for years already; sidebar extension with a bit of userChrome.css can do wonders: https://dosowisko.net/firefox-tabcenter.gif reply privacyking 22 hours agoparentWhat do you mean coming back? I don't recall it ever having vertical tabs. Most people have just used treestyletabs for the past decade or two reply seba_dos1 22 hours agorootparentIt used to be a part of their Test Pilot program as \"Tab Center\", somewhere around 2016, but got discontinued. What's on the GIF above is a WebExtension-based reimplementation of that called \"Tab Center Reborn\" plus a bit of custom CSS that reimplements the original auto-hide behavior and makes it blend nicer with Plasma. reply tim333 3 hours agoprevAs a recent newbie attempter as switching to Firefox Vertical Tabs and Profile Management system - seem good. I was a bit shocked that to get the tree style tabs working properly I had to go typing code (to stop the old tabs being there too). The thing that got me back to Chrome this time was I started getting out of memory errors which I have not had in any other software in a couple of years of using the M1 macbook. Maybe they'll fix that under the \"speed, performance\" stuff? reply butz 4 hours agoprevHear me out, I know this might sound crazy, but, how about letting users to customize menus to their own liking and workflow. Link UI customization to profiles and you will have many happy users. In example, I would remove most development related options (hey there, \"Inspect Accessibility Properties\") from my \"leisure\" profile, but would move those to more accessible places in my \"work\" profile. reply up6w6 20 hours agoprevRelated news: Servo Web Engine Continues Advancing But Seeing Just $1.6k In Monthly Donations https://www.phoronix.com/news/Servo-Engine-May-2024 reply ghostpepper 22 hours agoprevI am happy it's nothing to do with Pocket, VPN or Colorways. Those are examples of visual clutter that I would love to see removed entirely. reply speckx 22 hours agoprevI would like Firefox to be able to set extensions to run only on click or allow me to provide a list of domains, like Chrome does, so every extension is not running on every page. See https://support.google.com/chrome?p=enable_extensions and https://connect.mozilla.org/t5/ideas/option-to-allow-extensi.... reply slobiwan 22 hours agoprevIs anybody currently using Sidebery who could comment on how the Firefox \"Tab grouping, Vertical tabs, and sidebar\" compare? reply cassianoleal 22 hours agoparentGiven that they seem to have only prioritised those features, I doubt anyone can since there's nothing material to compare to. reply babypuncher 22 hours agoparentprevThey haven't actually shown off their vertical tabs yet, so there isn't much to say. I am a Sideberry user though, and this excites me because a native implementation will likely look a lot cleaner, integrate better with your chosen theme, and actually allow the removal of the regular tab bar (you have to keep it with Sideberry because there is some functionality that cannot be replicated through an extension). Theming in particular might be a big one, I had to spend a couple hours tweaking the Sideberry CSS to make it look nice with the skin I use in Firefox. Not having to do that sure would be convenient, especially for the average user who doesn't know what a stylesheet is. reply 14113 9 hours agoprev> More streamlined menus For everyone here complaining about this, have you ever looked at how many ways there are to access your history on Firefox? At my last count, there were 4 different ways to do it, depending on which menu you picked first. Cutting down this kind of inconsistent, and repetitive flow is something that we should be applauding. reply no-dr-onboard 22 hours agoprevAfter using FF for the past 7 years, every day at work I switched to Arc. It's been __okay__. I'm not a fan of them trying to nix the bookmark system and much prefer FF's implementation of just good old fashioned, indexable bookmarks. If FF can manage a good implementation of vertical tabs, I'd switch in a heartbeat. reply neilv 22 hours agoprevPrioritization that I'd like to suggest for Firefox, driven by needs of society (from high to low): 1. Representing the public interest wrt Web -- especially fervent respect for privacy in the browser, almost as much emphasis on security, fighting commercial excesses around de jure and defacto standards, and social equity within the scope of what Mozilla does (e.g., software accessibility, encouraging all volunteers). 2. Competitive browser compliance and performance. To be viable as a daily driver, to have impact. 3. Forward-looking R&D on Internet public interest things not currently considered \"browser\", but potentially big in near term. 4. Fancy browser features that actually increase uptake and retention. (Much lower than the first 3.) 5. Designer's opinion/portfolio UX changes. (Could be improvements, and there's value in an appealing and moving-target modern appearance, but misalignment is a risk to be careful of.) 6. Necessary-evil money-making compromises. (Very, very carefully. Many past ones were negatives. And we still see arguable dark patterns around some of those, such as as Firefox updates sometimes reverting users' express Search-related preferences.) A marketing challenge of these priorities would be that even most techies can't assess how well you're doing #1. But that mix of things you do can include things understandable to your typical user, such as superior blocking of abusive ads. (But don't drop the ball on things that your typical user can't understand -- they're entrusting you as an expert to act in their interests.) Educational outreaches can also help. And if you get to the point that you're generating an order of magnitude fewer security vulnerabilities than your competitor, then marketing people can run with that. But be careful with positioning, so that communications about smart, hard activities don't get conflated in people's minds with confusing mission drift and fluffy PR. reply glenstein 21 hours agoparent>4. Fancy browser features that actually increase uptake and retention. Yeah, I would say I think these are actually a good thing as long as the cost-benefit ratio is acceptable and it doesn't compromise core efforts (people have asserted this kind of compromise has been happening, but rarely with any coherence or substantiation). I would consider the example of Opera in its heyday - whether it was Turbo, \"widgets\", extensions, or Unite, which in my opinion was a triumph of originality and innovation that went unsung. Those are good when done right. reply 1oooqooq 11 hours agoparentprevcurious about your motivations to typing all that here reply neilv 1 hour agorootparentIt should be said, I had a moment, and I type fast. reply drewg123 22 hours agoprevOne thing I wish they'd work on is making saved credit card information work correctly and reliably. I use FF on Macos laptop and FreeBSD desktop, with multiple profiles on each. I have different profiles where CC saving work fine, and others that don't across both OSes. I'm so annoyed at having to pull out my CC that I've gone back to chrome a lot of the time when I need to buy things... reply m463 19 hours agoprevI wish there was an open way to save all your settings across devices - you could host them yourself. maybe webdav? The current firefox sync where they are probably the only folks who can host it is not that good. (there's an old sync server but it requires a non-local account) Also, I wish firefox would stop uncontrollably phoning home. reply OtomotO 22 hours agoprevI am looking forward to synced tab groups. Currently using an addon, but that's not synced among PC and Laptop (or phones, but there I don't need it) reply gilgamesh3 22 hours agoprev>Tab Grouping, Vertical Tabs, and our handy Sidebar will help you stay organized no matter how many tabs you have open — whether it’s 7 or 7,500. Finally. Chrome has this feature for a while and it's really handy when working with lots of tabs, but I use Firefox on my main machine so I couldn't take advantage of this feature (and I don't like vertical tabs). reply difosfor 20 hours agoprevI wish they'd finish up their webcodecs implementation. Even Safari at least supports VideoDecoder. reply benatkin 21 hours agoprevOnly 81% of revenue from Google, who has a competing browser that they beg you to switch to... https://en.wikipedia.org/wiki/Mozilla_Corporation#Finances reply wegfawefgawefg 19 hours agoprevSome time in the last few years firefox changed the new boommark poput to be unresizable and very small. If you have more than twenty bookmark folders it is very difficult to use. Ive been forced to use sideberry, which I dont like. reply roesel 21 hours agoprevI still miss the icons for items in the hamburger menu, this was one of the worst changes ever for me personally. > More streamlined menus that reduce visual clutter If this means the same as for Thunderbird, I'm going to have to switch after 20 years... reply mrinterweb 21 hours agoprevI love that Firefox continues to be been focused on performance. There was a while where FF perf was looking kind of bleak compared to Chrome. These days FF and Chrome have similar performance. FF might have the edge now. reply domysee 11 hours agoprevI didn't think I'll ever get to see native vertical tabs in FF. reply behnamoh 22 hours agoprevLooking at the comments, I'm not surprised about FF's low market share. They chose to target geeks and nerds, and let's face it, we tend to be whiny about almost everything and complain about any new change. Meanwhile, Google Chrome doesn't even need to pretend like they care about privacy, so they automatically target the other \"ordinary\" people who are not so picky about every new release note. This kinda teaches you something about the importance of product positioning. reply yjftsjthsd-h 22 hours agoparentNo, it's worse than that - they chose to target geeks, then decided to throw out the geeks in an attempt to target a more mainstream audience[0] and then failed to actually get the mainstream userbase, leaving them with neither the nerds nor the normies. [0] Firefox has always been at its strongest as a power-user's browser, chock-full of extensions and tweaks. Mozilla met this advantage with... two extinction events for extensions, and removal of customization options. reply rurp 22 hours agorootparentYes, exactly. Google and Microsoft have huge anti-competitive advantages that they have been able to use to target casual users. As long as that continues Firefox will be facing an uphill battle to get casual downloads, but neither of those large companies care at all about power users. That gives Mozilla a big opening to get a niche of users that will heavily evangelize their browser and install it on friend and family devices. Unfortunately Mozilla's leadership does not see it that way and have steadily pissed off a huge number of former fans, without making inroads with any other groups. They are in a tough position regardless, but combining that with terrible product and organization management has gotten them into the state they're in now. reply 14113 9 hours agorootparentprevNo, Firefox never targeted geeks. It's just that, when it came out, the only people who used a browser other than Internet Explorer happened to be geeks. The audience came to them, rather than them going to the audience. reply lotsofpulp 22 hours agoparentprevIt also does not help that many of the big businesses break their website if you do not let them track you. I use Safari + Wipr Content blocker with iCloud Private Relay, and it is just so inconvenient to do anything on many major websites. At the least I have to reload the website without a content blocker, and at worst, I have to use Chrome, giving up my privacy. If a somewhat computer literate person like me who knows about content blockers/iCloud Private Relay/VPN/ublock Origin has to relent and use Chrome, then what chance does a layperson have? They will just use Chrome and go on about their business. They want to spend their life doing other things, not figuring out why Target is not letting them order this clothing item they spent 10 minutes looking for because they are getting flagged as a bot or fraudster or simply not letting themselves be tracked. I especially love when you block location access to a retailer's website, and then the retailer breaks their functionality for choosing the actual store you want to shop at. They have your billing and shipping address from previous orders, they have your 2FA login, and yet, every single time, they ask for access to your location, and when you decline, they set your store to some random store within a few hundred miles of you because (I assume) they are trying to use your IP address to show you the store. reply finack 19 hours agoparentprevI truly do not understand the complaints about Firefox, the browser (and not Mozilla's questionable stewardship thereof). Every time someone starts talking about their issue, it's either something super niche and inconsequential, or something that doesn't seem to have sufficient evidence to blame on the browser (turns out, crappy web apps will make any browser slow to a crawl). I used Chrome for a few years back in the mid-teens, then switched back to Firefox, and manage to use the Web just fine. It's a browser, it renders web pages. I really don't understand the fuss. Meanwhile, Google (the advertising company) would totally like you to use Google (the browser), for totally innocent reasons. reply qwertox 21 hours agoprevThat site needs a dark mode. I'm surprised it hasn't. Multiple rows of tabs, even more through mouse-wheel scrolling, and fixed width tabs for easy closing. Like Tab Mix Plus. reply gsuuon 22 hours agoprevLack of PWA / installable apps is why I switched back to chrome after trying to daily drive firefox for a while. Really wish they'd reconsider the decision to drop it. reply fabrice_d 21 hours agoparentUntested, but I think this Firefox fork added support back: https://floorp.app/en reply MrAlex94 20 hours agorootparentI believe a lot of the work comes from: https://github.com/filips123/PWAsForFirefox P.S. the project you linked is no longer FOSS. reply gsuuon 18 hours agorootparentprevInteresting, thanks! reply mateusz_ 19 hours agoprevPriority feature could be, well... Catching up with the standards? It hurts my meow meow when i have to run chrome to use bt or usb api reply finack 19 hours agoparentThose web APIs aren't actually standard (WebUSB, for example, is still in draft status), and they have some gnarly privacy implications. Google steamrolls over standards bodies, and then somehow people believe their browser is the standard. I'm not sure if this is a concerted marketing effort by Google, or simply intellectual laziness. Either way, though, Firefox is not at fault for not supporting non-standard Web APIs. reply sebazzz 6 hours agoprevSubtle reminder that ManifestV2 phase out now has a timeline at Chrome[0]. The best moment to switch to Firefox is now. [0]: https://blog.chromium.org/2024/05/manifest-v2-phase-out-begi... reply dbg31415 22 hours agoprevFor me, the big thing Firefox needs is better power management. Mac user here, and streaming video or doing a video call just kills my battery compared to Chrome or Safari. Would love to see more focus on energy consumption optimization. reply switch007 22 hours agoparentIs this a comment from 2018? I thought FF on MacOS was pretty good these days. And are you suggesting Chrome is similar to Safari in terms of power usage? That would surprise me Safari will of course always be better. It's no competition reply dsego 22 hours agorootparentMy experience as well, it used to be bad, it's okay now. reply cassianoleal 22 hours agoparentprevMac user here as well. Chrome kills my laptop (though I've uninstalled it about a year ago and never looked back). Firefox is pretty good. Safari is better, at least in terms of power management. Firefox is my main browser, and it's a beast. Very low footprint, snappy and very decent in terms of power consumption. reply e44858 22 hours agoparentprevIt's the opposite for me on Linux. Firefox can play 4k 60fps perfectly, while Chrome drops frames and gets hot. I guess hardware-accelerated video is hard to get right. reply yjftsjthsd-h 22 hours agoparentprev> Mac user here, and streaming video or doing a video call just kills my battery compared to Chrome or Safari. Is it not using hardware acceleration or something? reply finack 19 hours agoparentprevCould it actually be the hundreds of megabytes of JavaScript each website insists on executing? reply surfcao 20 hours agoprevAs long as I can hide the address bar to have more screen real estate and vim extensions work, I am happy.. reply throw7 20 hours agoprevAI generated text should always have a way that identifies/tells the user that it's fake. reply justinclift 20 hours agoprevThis is the crowd that \"listened to user feedback\" then decided to add a button (that can't be removed) to the taskbar whose only purpose in life is to open the Extensions page. An action that's used once when setting up a browser, and never again. And you're not allowed to remove it to put something else there instead that's actually important. Fucking Arseholes. :( :( :( reply joshka 18 hours agoprev> Customizable new tab wallpapers that will let you choose from a diverse range of photography, colors, and abstract images that suits you most. Can we just make this include some sort of dark mode. There is an addon that kinda works, but not all the time. That bright white flash is pretty annoying. reply amluto 22 hours agoprevI have a suggestion for Mozilla: find use cases that benefit your users and are not possible in other browsers. For example: Browsers are terrible at accessing devices on a local network. Make this work well in Firefox, extending protocols as needed. Browsers are terrible at configuring IoT devices. It device makers are terrible at making apps for this purpose that don’t suck. Make Firefox (on desktop and mobile!) be the premier way to usable and securely configure IoT devices. Get device vendors to sign on: someone could (and probably would) sell a gadget where the instructions suggest installing Firefox to set up the device. Leverage the same technology to go after enterprise users. Ever seen a piece of high-end, expensive networking gear with a web UI? Ever contemplated how pathetically insecure this is? Ever contemplated how unpleasant it to provision certificates to make it secure? Make this work, easily and securely, on Firefox, using new protocols as needed. Make everyone else play catch-up. Heck, start small. Allow setting domain constraints on imported root certificates. The moral: do something new and better. reply palata 21 hours agoparentPlease, no. reply finack 19 hours agoparentprevNone of these things are in scope for a program that is supposed to browse the web (by fetching web pages and rendering them). reply amluto 4 hours agorootparentHow is securely connecting to a device that isn’t, itself, connected to the cloud, fetching a webpage from it, and rendering it out of scope for a web browser? Certainly URLs like http://192.168.1.1 have been in scope for decades — why can’t a web browser figure out how to support that use case well? reply finack 2 hours agorootparentWeb browsers can already do that. I'm not sure what else is desired beyond what they already do. reply cooloo 14 hours agoprevWhen will Firefox will support webrtc encoding/ decoding offload on Linux? reply mvdtnz 19 hours agoprevWhy can I not auto fill credit cards in Firefox mobile even though I know this is a feature offered in other countries? reply causality0 20 hours agoprevHow about making it so all those stupid purple pop-up messages on mobile that tell me what I already damn know can be swiped away instead of sitting on my screen blocking the UI for an eternity every time I dismiss a tab or full-screen a video? reply xchip 21 hours agoprevNot very exciting, to be honest. I would like certain apps to open always in private browsing. reply meiraleal 22 hours agoprevWhat about catching up with PWA? reply some_furry 22 hours agoprevI just hope there's an option to totally disable the AI junk. I'd just like a simple, secure browser that gets the hell out of my way and let's me extend it however I want (i.e., with an adblocker). I don't need or want AI models involved in the process. reply eesmith 22 hours agoparentI like Firefox's built-in language translation feature. Does that count as \"AI\" these days? reply palata 21 hours agorootparentI don't think it counts as the modern \"AI\" buzz: it was there before and it is useful. The modern AI stuff is typically useless crap that was thrown there because... I don't know... FOMO? reply resource_waste 22 hours agoprevI wonder how much money they get from their ads and spammy home menu. (Oh you can turn it off! /apologists) I boot up a fresh Linux install and everything looks A+ nice, then you open up firefox and it feels like I'm on M$ Windows 11 + Edge. Chromium doesnt have ads! I'll be happy when something replaces Firefox, they are a legacy company that might only exist due to anti-trust purposes. Something that is merit based would be an improvement. reply kome 23 hours agoprevNice! Firefox is in a good place, because the browser, at this stage (version 126), is really good and fast. And this post seems to point that the development is going in a good direction. reply aldanor 20 hours agoprevIf Firefox natively integrated TreeStyle tab or Sidebery and made it lightning fast, searchable and syncable, I would probably return to FF on that day. Definitely not yet another \"reduction of visual clutter\". reply kwanbix 22 hours agoprevI really love Firefox. I even organized events for them on my home country, so don't get me wrong, wouldn't it be best for the web/devs that they switched to chrome's engine? They can still implement whatever changes to privacy and their changes just like MS does, and devs will only have to target one engine. Or am I missing something? Please discuss it if you agree or disagree, do not downvote. Thanks. reply JohnFen 22 hours agoparent> wouldn't it be best for the web/devs that they switched to chrome's engine? Why would that be best? I don't think the problem with Firefox is the engine (anymore), I think it's the design choices Mozilla continues to make. If Firefox switched to the chromium engine, then why would I bother to continue to use it? Surely it would be better to switch to one of the \"privacy-oriented\" Chromium-based browsers that have been around for a long time now. reply Kye 22 hours agoparentprevMonoculture in software rarely goes well. Right now Safari and Firefox exist to push back on Google completely controlling the fate of the web. You do not want Google to have the same unquestioned power Microsoft did in the IE6 era. reply kwanbix 6 hours agorootparentBut that wouldn't be the same. Everybody can work on the engine. Worst case, if google wants to take over, the others can push their own version of the engine, compatible whenever they feel like, and different otherwise. IE was different because was totally closed and only MS could add features. reply cultureulterior 18 hours agoprevBring back xul or keep your promise on supporting tab tree, please. reply worksonmine 21 hours agoprevGreat, now please give me back the option to set my new tab page without resorting to extensions. reply jimbobthrowawy 21 hours agoparentDoes setting it in about:config do it for you? I see what you mean though, since the option right above that in the menu lets you set custom url(s) for your start page. reply worksonmine 19 hours agorootparentThat would help a lot yes, what option am I looking for? A custom URL used to be a setting in all browsers back in the days. reply bdjsiqoocwk 22 hours agoprevI'm still upset that theyve removed the \"compact\" style. Granted, between ublock origin and tree style tabs Firefox is still my favorite browser, but why are they changing this stuff... Can someone please ELI5 - what does \"more streamlined menus\" mean? reply Am4TIfIsER0ppos 22 hours agoprev [–] Are you bringing back XUL? No? Then please cease operation. reply thisislife2 20 hours agoparent [–] PaleMoon - http://www.palemoon.org/ - is a hard fork of Firefox that still supports XUL. reply Am4TIfIsER0ppos 17 hours agorootparentWhich I already use. Thanks though. reply guilhas 17 hours agorootparentprev [–] Although PaleMoon is great, it creates a brain split Firefox had very advanced addons for the time, and very good developer community. XUL removal just gave everyone PTSD Don't know how good is the continuous fragmentation of the community reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Mozilla is upgrading Firefox with user-requested features such as Tab Grouping, Vertical Tabs, a Sidebar, and a Profile Management system to separate different browsing activities.",
      "New additions include customizable new tab wallpapers, simplified privacy settings, and performance improvements for faster page loads, better battery efficiency, and enhanced cross-browser compatibility.",
      "Privacy enhancements focus on local device processing for tasks like translation and PDF editing, and Mozilla is exploring on-device AI for features like AI-generated alt-text for PDFs."
    ],
    "commentSummary": [
      "Recent Firefox updates have simplified menus, leading to mixed reactions; some users appreciate the changes, while others feel it reduces discoverability and functionality for power users.",
      "Criticisms extend beyond UI design to Mozilla's management and external pressures, such as Google's influence, with debates on resource allocation and the balance between user customization and security.",
      "Despite a declining market share, Firefox retains a loyal user base due to features like the \"awesome bar\" and customization options, with suggestions for improvements including better tab management, extension control, and performance enhancements."
    ],
    "points": 227,
    "commentCount": 237,
    "retryCount": 0,
    "time": 1717097525
  },
  {
    "id": 40533761,
    "title": "Intel Motherboard CPU Upgrade Hack: Kapton Tape and BIOS Mods Enable Newer CPUs",
    "originLink": "https://hackaday.com/2024/05/31/intels-anti-upgrade-tricks-defeated-with-kapton-tape/",
    "originBody": "Intel’s Anti-Upgrade Tricks Defeated With Kapton Tape 39 Comments by: Arya Voronova May 31, 2024 Title: Copy Short Link: Copy If you own an Intel motherboard with a Z170 or Z270 chipset, you might believe that it only supports CPUs up to Intel’s 7th generation, known as Kaby Lake. Even the CPU socket’s pinout is different in the next generation — we are told, it will fit the same socket, but it won’t boot. So if you want a newer CPU, you’ll have to buy a new motherboard while you’re at it. Or do you? Turns out, the difference in the socket is just a few pins here and there, and you can make a 8th or 9th generation Coffee Lake CPU work on your Z170/270 board if you apply a few Kapton tape fixes and mod your BIOS, in a process you can find as “Coffee Mod”. You can even preserve compatibility with the 6th/7th generation CPUs after doing this mod, should you ever need to go back to an older chip. Contrasting this to AMD’s high degree of CPU support on even old Ryzen motherboards, it’s as if Intel introduced this incompatibility intentionally. There’s been a number of posts on various PC forums and YouTube videos, going through the process and showing off the tools used to modify the BIOS. Some mods are exceptionally easy to apply. For example, if you have the Asus Maximus VIII Ranger motherboard, a single jumper wire between two pads next to the EC will enable support without Kapton tape, a mod that likely could be figured out for other similar motherboards as well. There’s a few aspects to keep in mind, like making sure your board’s VRMs are good enough for the new chip, and a little more patching might be needed for hyper-threading, but nothing too involved. Between money-grab features like this that hamper even the simplest of upgrades and increase e-waste, fun vulnerabilities, and inability to sort out problems like stability power consumption issues, it’s reassuring to see users take back control over their platforms wherever possible, and brings us back to the days of modding Xeon CPUs to fit into 775 sockets. Don’t get too excited though, as projects like Intel BootGuard are bound to hamper mods like this on newer generations by introducing digital signing for BIOS images, flying under the banner of user security yet again. Alas, it appears way more likely that Intel’s financial security is the culprit. We thank [Lexi] for sharing this with us! Posted in computer hacks, how-toTagged coffee lake, intel, intel CPU, kaby lake, pinmod",
    "commentLink": "https://news.ycombinator.com/item?id=40533761",
    "commentBody": "Intel's anti-upgrade tricks defeated with Kapton tape (hackaday.com)218 points by sharpshadow 7 hours agohidepastfavorite96 comments donatj 4 hours agoWhat is the financial benefit to Intel in artificially limiting its CPU sockets like this? Logic and reason would have me believe they'd want to sell as many CPUs as possible, and keeping the socket compatible for as long as possible would seem logical. My thoughts on reasons they might have done this. I honestly have no idea, these are just uninformed guesses. - The Charitable Answer: There actually is some sort of minor incompatibility like a voltage difference to some pin where it can still boot but it's maybe not good for the CPU? - The Practical Answer: They make more off the associated sockets/compass direction bridges/etc than they would off increased numbers of CPU upgrades. - The Cynical Answer: Motherboard manufacturers paid them off - The Cynical Practical Answer: They have a schedule for socket changes as some sort of deal with motherboard manufacturers and some engineers decided to do so in the laziest way possible - The Silly Answer: They're an evil corporation and want you to suffer reply fizzynut 4 hours agoparentIntel basically made the same CPU for about 6 years straight because of 10nm process issues. They had to keep pretending the next gen \"Lake\" CPU was substantially different from the last, so they just took last gen product, made some minor tweaks to break compatibility and called it a new generation reply _the_inflator 3 hours agorootparentSame goes for most cars. No real revolution, tweaks or changes due to regulatory demands, but nothing groundbreaking. reply jeffhuys 3 hours agorootparentStill, when you’re due for a new car and look for the newest of the newest, would you go with manufacturer A, who released their latest car 8 years ago, or manufacturer B, who released it 1 year ago? Incremental upgrades get so much hate around the internet (mostly about phones) by people having the version before it. Saying things like “ah they changed almost nothing! Why would I upgrade?!” While for instance me, only on my 3rd smartphone EVER, would love all the incremental updates over the years when I finally decide I need a new one, because I always get the latest and greatest. If a company then doesn’t release anything for a few years, I’d go somewhere else. reply Dylan16807 7 minutes agorootparentNew models every year are fine if they're honestly labeled and have technologically reasonable compatibility. Cars and phones meet those criteria a lot better than Intel CPUs. The problem isn't releasing things, it's the way they pretend every release is a big advance and the way they make the motherboards almost never able to upgrade. reply traverseda 2 hours agorootparentprevThe one with a reliability data for the past 8 years. It's surprising to me that people would want to make a major financial decision like a car without knowing about its reliability history. reply jandrese 13 minutes agorootparentWhat if the reliability is like \"these bearings are known to fail every 10k miles or so, but we have no product refresh planned for at least 3 years so the problem will remain unresolved?\" This is what incremental improvements are supposed to be. Well that and discovering that the vehicle can last till the end of the warranty period with one less bolt in that spot, so you can eliminate it. reply Groxx 2 hours agorootparentprev8 years of the same parts, repair knowledge, and continued software support? Sign me up immediately. reply Zambyte 2 hours agorootparent> continued software support Unfortunately due to the extremely minimal software rights that exist (see: proprietary software) this is pretty much nonexistent in cars AFAIK. I would rather get a car that is old enough to not be limited by software constraints. Which is pretty disappointing, because I actually really like electric cars. I think they would work well for my needs. But they are all so intentionally kneecapped, I have no interest in any particular model that's available. reply graemep 3 hours agorootparentprevIn the case of cars and CPUs its not that people mind incremental upgrades, it is that they mind incremental upgrades sold as big upgrades. For phones the mindset of people who upgrade when they have to and/or buy cheaper phones is very different from those who regularly upgrade to the latest flagship phone. reply xattt 2 hours agorootparentprevCase in point: A new car released 8 years ago, but with incremental upgrades (i.e. Mitsubishi RVR in NA), still won’t have the same fundamental design considerations around safety or fuel efficiency as a more recent model. reply yellow_postit 1 hour agorootparentprevBuying first gen models is always a crapshoot. Often same for last gen if they try to squeeze new capabilities into a platform it wasn’t intended for. Tesla is particularly terrible but this has been true for every manufacturer. You want a couple years for them to work out the kinks. reply nfriedly 47 minutes agorootparentprevHonestly, from a reliability standpoint, the ideal new car is one that had a major refresh ~2 years ago. By then most of the kinks should be worked out. Or just pay attention to the warranty. If they guarantee it for 10 years, they probably expect it to run for 10+ years. reply boplicity 1 hour agorootparentprevCar buyers aren't always so dumb. When we bought our car, I was fully aware that major updates to models happen only so often. We bought used (of course), and the \"major update\" was our major criteria, more so than the specific year release date. (We bought a 2014 model in 2018; the year they released significant safety improvements compared to the 2013 model.) reply settsu 37 minutes agorootparentprevThis is arguably exactly what most people actually need in a vehicle that you are spending thousands of dollars on: accumulated refinements seamlessly incorporated over time. Year over year this typically results in good outcomes on a purely practical basis. However it just inherently makes for very boring publicity/promotional material. Edit to add: it can also admittedly result in older solutions getting baked in which prevent larger beneficial changes. (Toyota's North American 4Runner and Tacoma models might be good real world examples of this approach resulting in generally high reliability but also larger, \"riskier\" changes being seemingly eventually necessary.) reply lazide 3 hours agorootparentprevLuckily it’s not common to need to replace your garage every time you get a new car. reply someguydave 2 hours agorootparentIn this analogy, Intel sells the parts to make garages too reply BobaFloutist 1 hour agorootparentprevI mean I can't speak to ICE cars, but electric cars ranges seem to scale pretty dramatically with how new they are. reply pwg 2 hours agoparentprev> What is the financial benefit to Intel in artificially limiting its CPU sockets like this? They (Intel) also make the chipsets that go on the motherboards. So anyone who disposes of their old motherboard and buys a new motherboard because of this limitation results in: 1) new CPU sale to Intel 2) new chipset sale to Intel (indirect sale via the motherboard manufacturer) Given that a \"new CPU\" sale plus a \"new motherboard chipset\" sale is more revenue to Intel than just a \"new CPU\" sale alone the financial benefit becomes obvious. reply grandinj 4 hours agoparentprevIt costs money to validate a new processor on an old motherboard, and no corp wants to waste money on a product they have already sold. reply crote 3 hours agorootparentAMD ran into significant compatibility issues with AM4, with some motherboards not being able to supply the amount of power needed by the newer CPUs and PCI-E Gen 4 support being removed in the final BIOS release due to reliability issues. A lot of motherboards also didn't have enough flash space to contain the firmware needed for the whole CPU range, so the update to support the newer gen had to remove support for the oldest gen. Turns out it's really hard to guarantee compatibility with several years of third-party products which weren't designed with your new fancy features in mind. reply gregmac 3 hours agorootparentprevThis is my thought, too. I'd bet customer perception is also a factor; there's a risk the old boards (not even made by Intel) die and cause problems, and Intel wouldn't want to deal with press/comments like \"This CPU stopped working after 2 months\" or \"I installed this new CPU, and within 2 months it killed my motherboard that had been working fine for 7 years\". They've released CPU upgrades with the same socket before, I'm sure they have the sales data to know how that performs vs new socket. Laptops have outsold desktops for well over a decade, and their CPUs pretty much non-upgradable. I can't easily find a nice chart to reference, but intuition tells me the desktop industry is similarly trending towards complete \"system\" sales vs individual parts. In other words: Most people don't upgrade their CPU, they upgrade by replacing the entire system. If true, this also means the socket would be almost entirely irrelevant to sales performance. reply vel0city 3 hours agorootparentprevIt costs money to validate a new processor on an older motherboard design. How much does it cost to make a completely new motherboard design? reply wmf 1 hour agorootparentThey're going to make new motherboards every year regardless, so any support for old motherboards is in addition to that. reply lupire 4 hours agorootparentprevThe new processor isn't sold yet. reply thfuran 3 hours agorootparentSure, but why pay to validate on old boards when you can instead get many of their users to pay you for new ones? reply babypuncher 1 hour agorootparentprevMotherboard manufacturers don't seem to mind doing it for AMD chips reply Laforet 3 hours agoparentprevIntel actually intended for LGA1151 to remain unchanged for Coffee Lake but found out late in the testing process that many existing motherboards did not have enough power delivery capability to support the planned 6 and 8 core parts. Hence the decision to lock them out in software only. They are probably aware of the bad optics but decided that it’s better than trying to deal with the RMAs later. It’s very similar to what had happened in 2006 when the 65nm Core 2 series were released in the same LGA775 package used by 90nm Pentium 4s, however the former mandated a specific VRM standard that not all comtemporary motherboards supported. Later 45nm parts pretty much required a new motherboard despite having the same socket again due to power supply issues. AMD went the other route when they first introduced their 12 and 16 core parts to the AM4 socket. A lot of older motherboards were clearly struggling to cope with the power draw but AMD got to keep their implicit promise of all-round compatibility. Later on AMD tried to silently drop support for older motherboards when the Ryzen 5000 series were introduced but had to back down after some backlash. Unlike the blue brand they could not afford to offend the fanboys. P.S. Despite the usual complaints, most previous Intel socket changes actually had valid technical reasons for them: - LGA1155: Major change to integrated GPU, also fixed the weird pin assignment of LGA1156 which made board layout a major pain. - LGA1150: Introduction of on-die voltage regulation (FIVR) - LGA1151: Initial support for DDR4 and separate clock domains This leaves the LGA1200 as the only example where there really isn’t any justification for its existence. reply Rinzler89 3 hours agorootparentThank you for providing valuable insight. I wish these kinds of comments would end up at the top instead of the usual low quality \"hurr-$COMPANY evil, it's all because greedy obsolescence-durr\", from people who have no idea how CPUs and motherboards work together and the compatibility challenges that come when spining new CPUs designs with big difference that aren't visible to the layman who just counts the number of cores and thinks there can't possibly be more under the hood changes beyond their $DAYJOB comprehension. Here's a video from gamer's Nexus on AMD's HW testing lab, just to understand the depth and breadth of how much HW and compatibility testing goes into a new CPU, and that's only what they can talk about in public. https://www.youtube.com/watch?v=7H4eg2jOvVw reply Qem 3 hours agoparentprev> Logic and reason would have me believe they'd want to sell as many CPUs as possible, and keeping the socket compatible for as long as possible would seem logical. x86 market is a near monopolistic one, with two companies cornering most of the market. Monopolies can afford to sustain irrational/inefficient practices as long it helps to squeeze the consumer. I hope RISC-V succeeds in breaking this duopoly. Perhaps now with the latest round of sanctions against China, if their full industrial might is thrown behind open designs, we may have some hope to crash the duopoly. reply II2II 3 hours agorootparentIf you're going to bring in RISC-V, why not mention ARM? They're currently more of a threat to Intel than RISC-V and likely will be over the next decade. They have a near monopoly for anything that is not a computer that requires anything more than a low performance microcontroller, and are supported to varying degrees by the three major general purpose operating systems. RISC-V likely has a promising future, but the foundations are still being laid. reply hinkley 3 hours agorootparentprevIf you’re hoping RISC-V will get market share then it’s three companies, not two. Intel, AMD, and Apple. reply zackmorris 3 hours agoparentprevComing of age in the 90s and witnessing the sheer audacity of greed that followed, I can tell you that the cynical answer tends to be the right one. reply hinkley 3 hours agorootparentEspecially considering Intel came of age in the 90’s as well. reply fennecfoxy 2 hours agoparentprevTo be fair I don't upgrade PC all that often (my 1080ti still going strooong!) So when I do upgrade (every 6 years or so, it's currently been since 2018 and I still feel no need) then not only does the CPU technology need a bump, but the bridges on the mobo also need an update. Going from a 2018->2024 mobo is guaranteed to get you things like more/faster m.2 slots etc as well. I suppose they could make compatible with old and new boards but I imagine it's much easier for them to design 1:1 new cpu + chipset than to design and test 1:* new cpu, new chipset, old chipset 1, old chipset 2, etc. reply immibis 2 hours agorootparentMy i7-6700k was going strong until the motherboard died for the second time. I suspect a solder crack caused by issue vibration from a damaged fan (and my only evidence for that is the fact the fan is damaged and vibrates and that two different motherboards died). Might try reflowing it to revive it, eventually. After reviewing my options I would have either bought a third motherboard or upgraded to something like a Threadripper (I did the latter). Upgrading to a current desktop Intel system just didn't seem really worth it when I already had most of a working one and it was still fast enough. You actually have FEWER expansion slots on the newer desktop motherboards because they've pre-decided what you want: a desktop has a 16x GPU slot and a 4x NVMe slot, and that's it. Gone are the days of generic uncommitted expansion slots, mostly. reply utensil4778 3 hours agoparentprevIntel sells the chipset that goes along with the processor, as well as selling their own motherboards. I think the profit incentive here is obvious. Why sell just a CPU when you can sell a CPU and a chipset and a motherboard? reply ikekkdcjkfke 3 hours agoparentprevI am going to use this listing in my pre-prompt reply pessimizer 3 hours agoparentprevI would say that Intel is a company, not a person, and isn't motivated but directed. If the same people own both Intel and the mobo manufacturers, they win by forcing new purchases of new products primarily distinguished by a higher price. One computer owner in a thousand upgrades their processor alone, or would even know how to. reply Arelius 3 hours agorootparent> I would say that Intel is a company ... and isn't motivated but directed You know, it's a matter of perspective, but I'd disagree. I think we'd like to think companies are directed, but I think as they get larger and older, especially public companies, they operate less by difection, and the systemic forces take over, and they operate and function more by the aggregate sum of all the motivations of the actors involved. I think it's true that some companies do exist, perhaps by sheer force of personality of their leaders, that remain primarily \"directed\" but feel that's more the exception than the rule. reply accrual 4 hours agoprevA couple of historical CPU mods similar to this: - AMD K6-2+ can be converted to K6-3+ by moving a 0-ohm resistor under the IHS to unlock the full 256K of L2 cache only present on K6-3+ models. The CPUs were basically all the same and were binned into separate SKUs using the position of this resistor. - AMD K7 (Athlon XP) can be similarly unlocked by bridging conductive pads on the chip using something as common a graphite pencil. - Intel Pentium 3 Coppermine chips could be run on earlier Pentium 2 boards by using a slotket adapter with a modified socket. The socket could sometimes be further modified to support the signaling used by even later P3 Tualatin chips, allowing for Tualatin CPUs to run on 440BX chipsets which were never designed for them. Also needs a BIOS mod and sometimes a VRM replacement. reply litenboll 4 hours agoparentIf anybody else wonders why a 0-ohm resistor is a thing, apparently it is because it makes it possible to install a jumper on printed circuit boards with the same equipment that is used for normal resistors. https://en.m.wikipedia.org/wiki/Zero-ohm_link reply l33tman 3 hours agorootparentYou put 0-ohms where you think you might at some point want to have something else there like a > 0 ohm resistor or an inductor (for EMI blocking for example), it's much easier to just reprogram the SMD robot than to make a new PCB. reply nine_k 51 minutes agorootparentprevA \"0-ohm resistor\" could also be more clearly called a \"conductor\", but it's not, despite its electrical function. I suppose it's because the mechanical function is more salient: it is a standard SMD part like other resistors, capacitors, LEDs, etc, not a conductor etched on the PCB. reply 4gotunameagain 1 hour agorootparentprevIt can serve multiple purposes. You can have 0 ohm resistors to serve as a bridge to run a trace below if you're running out of space. You can have multiple resistor footprints and depending on where the 0 ohm is placed, different configurations are enabled. You can use it to be able to isolate parts of the circuit after the fact, although solder jumpers are more common for that purpose reply lazide 3 hours agorootparentprevResistor shaped wires are also a lot less dangerous than fuse shaped ones. In my experience. reply mrandish 3 hours agoparentprevNot a CPU mod because it was enabled by BIOS and motherboard but still perhaps the most legendary CPU ever for overclocking: Celeron 300A (1998). >50% was typical just by changing one setting. reply crote 3 hours agoparentprevI remember upgrading my ATI Radeon HD 6950 to a 6970 - it was just a simple firmware flash! Worked beautifully. The card did eventually die so it might not have been the best idea, but in the meantime it did mine enough Bitcoins to pay for itself. reply stordoff 32 minutes agorootparentI flashed my Radeon 9550 into a Radeon 9600 Pro. Bumping the core/memory clocks from 250/200 to 400/300 was a pretty decent upgrade, and there was still some overhead for overclocking (IIRC, I got it up to about 450MHz on the core). reply candiddevmike 3 hours agoparentprevI remember the AMD unlocks using just an advanced setting in the BIOs. My cheap ass got a quad core processor for the price of a dual core! reply scrlk 7 hours agoprevReminds me of the \"pencil trick\" to overclock CPUs in the late 90s/early 00s. You'd rub a pencil over contacts on the CPU to bridge them, which would unlock the clock multiplier. reply jsheard 5 hours agoparentYou rarely see those kinds of unlocks anymore unfortunately, since they started using eFuses to disable parts of the chip permanently. It still happens occasionally though, like the time when AMD decided to add a 4GB SKU alongside the originally planned 8GB SKU of one of their graphics cards so late in development that they didn't have time to actually change the hardware, so all of the initial batches had 8GB installed with half of it disabled in the VBIOS where applicable, which was easy to reverse by flashing the VBIOS from the 8GB version. The one time in history when \"download more RAM\" wasn't just a meme. reply terlisimo 5 hours agorootparentHonorable mention of ATI Radeon 9600 that was soft-upgradeable (via hacked drivers) to 9800 PRO for a 200% perf boost. Good times. reply ielillo 3 hours agorootparentActually it was the Ati Radeon 9500 non pro that could be modded into a Radeon 9700. Regarding the ATI Radeon 9600, there was a variant called Radeon 9550 that could be overclocked from 250 mhz to 400 mhz. reply micv 1 hour agorootparent> Actually it was the Ati Radeon 9500 non pro that could be modded into a Radeon 9700. Sometimes. At least some of those 9500s were binned parts that showed their broken bits when you modded them. I had one. The screen turned into a kaleidoscope when I tried to play a game. Was definitely worth a try if you had one though! reply Moto7451 3 hours agorootparentprevAnd my personal favorite, the Radeon 9100 which was actually a Radeon 8500 in PCI format instead of AGP. With a slightly tweaked 8500 Mac Edition bios you would have a very fast GPU for first gen PCI Macs through the Yikes G4. I believe some faster NVidia PCI cards ended up appearing and being made to run on Macs but I had moved from my XPostfacto PCI Macs to an Intel mini by then. reply iwontberude 4 hours agorootparentprevAnd NVidia GeForce 6600GS to 6800GT with pencil mod and flash. reply Astronaut3315 4 hours agorootparentprevWow, that’s even better than my old PNY GTX 465. It was really a 470 that was cut down by the VBIOS. I was able to download 256MB of VRAM, a wider memory bus and some GPU cores on that one. reply lithos 3 hours agorootparentprevIBM servers this was the default as well. Where the difference between ram amounts on AS400 systems was a phone call to enable more. reply jsheard 3 hours agorootparentHaving dormant hardware which is intended to be unlockable later is a separate thing, which is rarely seen on consumer hardware. Intel tried it a while ago but the backlash was so severe that they gave up and reverted to permanently fusing off the silicon. https://en.wikipedia.org/wiki/Intel_Upgrade_Service I believe they still have something similar which allows Xeon processors to be upgraded in the field though. Car manufacturers have been testing the waters too, e.g. installing heated seats in every model and making them a paid software unlock for lower end models that didn't have that option enabled from the factory. reply hinkley 3 hours agorootparentprevIBM’s problem was going to be inventory, labor, scheduling, and transportation costs. Your mainframe is slow and IBM is going to charge you 6 hours of labor to come out for 90 minutes next Friday, or we can deal with it over the phone. I can’t recall but did IBM use that extra hardware for physical redundancy in case of hardware failures? I know they researched letting equipment die and coming out and replacing it after multiple failures instead of single ones, but I don’t know if they applied that to shipping mainframes, to regular rackmount hardware, or just in Research. reply wrigby 6 hours agoparentprevI remember this being the way to unlock Athlon XP’s (though if you bought the mobile version, which used the same socket and ran just fine in a desktop motherboard, they came unlocked from the factory). reply laweijfmvo 2 hours agoparentprevand the initial \"fix\" from AMD to stop it was to laser burn a trough between the contacts, so that you couldn't draw a line between them! reply whalesalad 5 hours agoparentprevTaking me back to the Athlon XP Barton days reply stronglikedan 4 hours agoparentprevThey do just that at the end of the embedded video. reply benreesman 5 hours agoparentprevLikewise the felt marker on DVDs during the DRM Wars. reply daneel_w 5 hours agoprevI was myself once, before switching to AMD, a user of the related trick where one could run a used $35 quad-core Xeon on e.g. a Core 2 mobo by just switching two pins on the CPU's pin grid with a little sticker. Miles better experience than the $400 Core 2 Quad. reply aranchelk 4 hours agoprevIt’s called “Coffee Mod”? I would have gone with “Kapton Lake”. reply dghughes 5 hours agoprevThis reminds me of the olden days (1990s) when people would fill in laser cuts on hobbled CPUs with solder to boost...something. I'm old so l forget. reply xxs 5 hours agoprevIt's an old topic[0], the site does reference it. Back then it was widely discussed in the overclock community. Effectively Intel's [6-10] series are all skylake. Shorting the cpu pins (well LGA) was possible even with a pencil's graphite. [0]: https://community.hwbot.org/topic/175489-asrock-z170-mocf-li... reply sokoloff 7 hours agoprevThis is dated 2024, but is talking about Intel 8th gen chips released in 2017. reply jimbobthrowawy 5 hours agoparentThe hackaday post is from this year, and talks about forum threads/guides from 2019. I would guess the author only learned about it or decided to write about it now. Even if it's not that useful to people nowadays, it's interesting to learn such a thing is possible. reply xxs 5 hours agorootparent>threads/guides from 2019. 2018, April reply causality0 6 hours agoparentprevAnd? Submissions talking about the architecture of the Game Boy don't need to be tagged 1989. reply jsheard 5 hours agorootparentMentioning the Game Boy would date it implicitly, they don't make Game Boys anymore. Intel is still around. reply a1o 3 hours agorootparent\"they\" Nintendo doesn't. But \"they\" random people from the internet make FPGA motherboard of Gameboy that is compatible with factory buttons, case and accessories. reply qwerty456127 3 hours agoprevAnti-upgrade is disgusting. Upgradeability is a major reason to buy an tower PC. I would love to pay any remotely-reasonable amount of money for a motherboard which would let me just swap CPUs and cards for 15 years. This is what good motherboards were in the good old pre-PCIe days. It was so lovely to buy the best MB + cheapest everything else, then upgrade whatever as you need and can afford it. I've read Intel even used to make \"overdrive\" CPUs to fit really completely new generations into old sockets. reply immibis 2 hours agoparentOne reason I chose to buy the lowest-end Threadripper instead of the highest end Ryzen. In the future, even if there's never any compatibility with socket sTR5, the parts in the same generation provide a part-by-part upgrade path up to 96 cores, 1TB of RAM (or was it 2TB? I forget), and 128 PCIe lanes. reply userbinator 5 hours agoprevIt's interesting how the pins differ between motherboard manufacturers, which suggests they didn't copy Intel's reference schematics completely or used different revisions of them. They used to publish the reference schematics on their site up until the P4 era, but I guess it made things like this too easy. reply josephcsible 3 hours agoprev> Don’t get too excited though, as projects like Intel BootGuard are bound to hamper mods like this on newer generations by introducing digital signing for BIOS images, flying under the banner of user security yet again. Alas, it appears way more likely that Intel’s financial security is the culprit. Indeed. The rule of thumb is that if you don't have the ability to turn off some security feature in something you own, then it's really there to make the thing secure against you. reply irisgrunn 5 hours agoprevSimilar to how you can use a socket 771 Xeon in a socket 775 mainboard with changing a few pins and mod the bios. reply helf 5 hours agoparentI still have a system running a modded xeon in a 775 board lol reply DarkmSparks 5 hours agoprevnot saying this contributed significantly to Intels recent $7 billion loss in chip making, just that they probably shouldnt be pushing buyers away quite so hard given their current situation. reply xxs 4 hours agoparentcoffee lake was the 1st time Intel had to wake up and put more than 4 cores in a (consumer) CPU; so it was a big thing. reply graphe 6 hours agoprevhttps://www.overclock.net/threads/mod-lga775-support-for-lga... I did this mod back in the day, can't believe it's been 11 years. reply nottorp 3 hours agoprev> Don’t get too excited though, as projects like Intel BootGuard are bound to hamper mods like this on newer generations by introducing digital signing for BIOS images, flying under the banner of user security yet again. Alas, it appears way more likely that Intel’s financial security is the culprit. It's okay, soon we'll lock down all software too in the name of security. reply bell-cot 7 hours agoprevSounds like good* strategy by Intel. Cheap for them & the MB manufacturers to make a few little changes, all the potential issues with Old MB/New CPU systems are now \"sorry, not supported\", and the Rebel 0.01% can imagine that they've cleverly won a real victory - which \"victory\" will barely be a rounding error on any of the Big Players' financials. *\"Good\" by Capitalist Overlord standards reply yourusername 6 hours agoparentBut if i can buy a new CPU for my motherboard i would be almost guaranteed to stay with Intel. Now ever upgrade is a chance to jump ship to the competition. There are probably people on their 3rd AM4 CPU with the same motherboard. reply jimbobthrowawy 5 hours agorootparentIntel only doing one upgrade per compatible mobo is kind of annoying. I had to buy and return a 6000 series CPU just to update my board's firmware so I could use the 7700k I intended to. How's the compatibility on the AMD side? Will a board work enough to upgrade itself with a newer CPU than it launched with? reply InvaderFizz 5 hours agorootparentIf the board supports headless USB bios updates, yes. Those can update the bios without a cpu present. reply daneel_w 5 hours agorootparentprevI had no problems upgrading from a quad-core Zen 2 to a hexa-core Zen 3 on my particular motherboard (Gigabyte A520M H). reply mrguyorama 5 hours agorootparentprevWhen the Ryzen 3xxx series CPUs came out, some motherboards had the same situation. Some computer stores were anecdotally lending out 2xxx series Ryzen CPUs so you could update your BIOS reply xxs 4 hours agoparentprev>Rebel 0.01% Lower than that, much lower - as it required custom built/assembled BIOS, effectively. The hardware mod was the very easy part. reply amarcheschi 4 hours agorootparentI followed a guide to mod my bios (amd tho) to unlock features and with a tutorial is kinda easy... like, just follow the tutorial, open this, change this to that... i don't know however whether it would require a handmade mod for each bios or if a generalist tutorial would be okay for different oems mobos reply MrBuddyCasino 5 hours agoparentprevIt is a way to guarantee MB vendors a steady recurring revenue, thus making it a good business to build mobos for you. AMD supports upgrades for longer, which is nice but presumably doesn't do them any favours in the OEM relationship game. Luckily, as a customer you have a choice. reply wannacboatmovie 5 hours agoprev [–] If your system isn't unstable enough... I'm sure you could drop a Ford V8 into a Hyundai with a few simple mods; it doesn't make it an intelligent idea either. But boy would it generate clicks. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Intel motherboards with Z170 or Z270 chipsets officially support up to 7th generation Kaby Lake CPUs, but users can enable 8th and 9th generation Coffee Lake CPUs through a \"Coffee Mod\" involving Kapton tape fixes and BIOS modifications.",
      "The \"Coffee Mod\" allows continued compatibility with older CPUs, showcasing community efforts to bypass Intel's restrictive practices.",
      "Future Intel security measures, such as BootGuard, may impede these modifications, contrasting with AMD's broader CPU support."
    ],
    "commentSummary": [
      "Intel is criticized for limiting CPU socket compatibility, seen as a strategy to boost profits or due to deals with motherboard manufacturers.",
      "Despite minimal changes in CPUs over six years, Intel marketed each as a new generation by tweaking compatibility, complicating upgrades and frustrating users.",
      "Comparisons are made to more honest incremental updates in cars and phones, with discussions on technical justifications, market dynamics, and potential competition from RISC-V and ARM."
    ],
    "points": 218,
    "commentCount": 96,
    "retryCount": 0,
    "time": 1717155449
  },
  {
    "id": 40533484,
    "title": "Malenfant.net: A Decentralized Social Network with Privacy and Real-Time Features",
    "originLink": "https://malenfant.net/@didier/112528487189999791",
    "originBody": "Create accountLogin Recent searches No recent searches Search options Not available on malenfant.net. malenfant.net is part of the decentralized social network powered by Mastodon. Administered by: Server stats: Learn more malenfant.net: About · Privacy policy Mastodon: About · Get the app · Keyboard shortcuts · View source code · v4.2.9 SearchLive feeds Login to follow profiles or hashtags, favorite, share and reply to posts. You can also interact from your account on a different server. Create accountLogin About",
    "commentLink": "https://news.ycombinator.com/item?id=40533484",
    "commentBody": "Stop Using Discord (malenfant.net)216 points by ColinWright 8 hours agohidepastfavorite216 comments Springtime 6 hours agoUltimately someone has to pay for things. Many communities have apparently decided they don't want to use even modern forum software and become a webmaster, along with avoiding the costs and time associated, or use someone else's existing forum (which may or may not suit their needs). The quite liberal free file attachment limits (25MB each), ease of communication and ironically the fact Discord is closed off in nature means it's quite attractive for people to more easily share things that would require a separate file host normally (it's an almost universal experience that forums usually have meager attachment caps). I'd be quite interested in an article that understands Discord's benefits and use cases and then approaches a critique and alternative solutions to address it in a more open web friendly way. reply iforgotpassword 6 hours agoparentI tried discord only once, but to me it was a disorganized chaotic experience. People seem to really think that well organized faq sections on traditional websites and properly searchable and indexable forum threads should be replaced by \"just search through two years of chat backlog on discord\". Or something like that. reply lamontcg 14 minutes agorootparentI'm actually a moderator on a discord. We have a lot of FAQ pages (generally wiki pages on various GitHub repos) that a discord bot will send you to in response to various different commands. Occasionally I use search to find a chat that I personally had with someone X months ago, and it usually works if I can find the right search terms. I don't think it is any worse than IRC with chatlogs (arguably might be better depending on how well indexed the chatlogs are). And the fundamental problem is that social chats are noisy, and you should never be using them for knowledge retention. And web forums aren't a replacement for social chat. And 99% of the questions and answers in there are from newbies and are covered in FAQs and are just noise anyway. You can't index your way out of a horrible noise problem. They also do serve a purpose because as one of the subject-matter experts (who is a single-point-of-failure/final-escalation level), I don't have to get involved in most of the discussion. There's usually dozens of people on the server who can answer the easy questions, and I can monitor for people hitting real bugs that actually need fixing. reply theshrike79 5 hours agorootparentprevI have the same feeling about Matrix - and I've used IRC since the 90s so I'm used to jank. Discord has a clear segmentation where a \"server\" (community) has multiple \"channels\" (and \"forums\"). I can easily context-switch between servers. With Matrix there is no clear way (at least to me) to get all channels from a specific context - that is provided by someone else. I can of course manually organise channels somehow, but that's not what I want to do. Example: When I join the Retro Handhelds community[0] on Discord I get access to a huge amount of channels, created by the server admins. I then have a fancy UI (Channels & Roles) where I can pick which specific retro handhelds I am interested in (RG35XX and Anbernic) and it hides the other channels from my view. There are no other tools that can do this currently. Matrix has the technical capability, but their insistence on making the client NOT-Discord is getting a bit tedious. I WANT to like Matrix, but they're making it so damn hard. [0] https://discord.gg/retrohandhelds reply mxuribe 4 hours agorootparentI'm an admitted matrix fanboy, but i hear ya about organizing in the matrix world! There certainly are opportunities for improvement. That being said, i feel like the challenges are 80% ~ 90% UI/UX challenges that need to be evolved, and maybe 10% ~ 20% protocol/architectural challenges. I have a feeling that there will be improvements from the client/app side that will create a positive jump in experience in the matrix world...of course i am biased. :-) reply soerxpso 1 hour agorootparentprev> People seem to really think that well organized faq sections on traditional websites and properly searchable and indexable forum threads should be replaced by \"just search through two years of chat backlog on discord\" Nobody thinks that. Creating and maintaining a traditional website with a well-organized FAQ section requires non-zero effort. The discord backlog is already there on accident. This thread comes up every other month and is always full of people who seem to imagine a project lead sitting down to weigh pros and cons of possible options for their FAQ, and settling on, \"I think we'll use Discord!\" Documentation lives on Discord for the same reason that so many companies have critical infrastructure running in Excel. Nobody sits down, evaluates tech stacks, and decides, \"I think this part should run on Excel.\" It runs on Excel because the guy who initially wrote it was already interacting with a CSV file in Excel and didn't know how to do much else, or didn't want to spend the effort for something he thought might be a one-off job. reply slimsag 6 hours agorootparentprevI run a large discord. I am in several large discords. I do not know anyone proposing replacing a traditional FAQ with discord. If anything, I would argue discord tends to improve website-based documentation because people ask the same question over and over again, leading to a need for a quick way to link the answer. Who do you think is replacing FAQ's with Discord? reply iforgotpassword 5 hours agorootparentI don't want to throw dirt at the project in question, but I signed up specifically to ask a question regarding a configuration problem, and the reply was \"search forin the channel.\" like they apparently already had enough people ask the question that they immediately knew how to find the relevant discussion containing a solution. Versus in the old days, asking on irc and getting a link to a forum thread, or even better, finding the thread on Google. I don't know how much of an outlier this is, as said, I didn't ever use it again and that was at least two years ago. But at least here on hn I've read similar stories. reply aaravchen 4 hours agorootparentThat is the exact reason Discord isn't sufficient. I've had to support enough projects as a maintainer to know that you will always get hammered with the stupidest questions and the same questions all the time. The less you have to repeat yourself the better, and Discord, or any chat application, ensures you have to repeat yourself as much as possible. There's virtually no way to retroactively follow the stream-of-consciousness that is a chat-style discussion with no single topic instead of the post-style single-topic threaded discussion. I have seen exactly 1 project ever have a solution to this, and ironically it was actually on Discord. Of all the many many chat applications out there, Discord seems to have the most flexible admin control of retroactive chat organisation, and 1 huge development community I'm in has actually proactively created threads when a reoccurring topic comes up again and then moved them to a common FAQ channel. reply politelemon 5 hours agorootparentprevI've seen it on numerous game, book, and software related discords. Your experience is alien to mine. reply aaravchen 4 hours agorootparentprevI'm guessing you don't interact with the development community very much then? Almost every open source project I encounter at least has a Discord server, it's the new easier and more accessible version of \"find me on IRC\", but most that are less than 10 years old that I've encountered also don't have a forum and direct you to the Discord for even the most basic info. In fact I've had multiple popular GitHub repos that directed me in an Issue I was offering to file a PR for to go check Discord for an explanation of how the project was even used. reply AlienRobot 5 hours agorootparentprevThe forum threads ARE the FAQs. When enough people make the same thread to ask the same question, that is a frequently-asked question. reply skydhash 5 hours agoparentprevIt’s free and easy to setup. That was the same as Facebook, Twitter,… and that’s because corporations have money to throw at the problem. The issue is that their intention is not the same as the users and after a while, the illusionary cooperation stops. And then, the communities collapse and a whole part of the internet goes dark. The other part is that Discord feels much more like a club than a library. It may be nicer, but no one is interested in popularizing information. Instead you have to hope to be heard by the experts. And that’s after the requirement of being a member. A forum is open to anyone on the internet, who can consult the interactions anytime they want. And information categorization is already done. The real danger of Discord is that people want to use it for everything and forget to offer other means of communication like a Wiki or a bug tracker. Imagine if Arch Linux was only present on Discord. The alternative are still the same as it was ages ago. Host a forum software yourself or pay someone to do it. And build a Wiki to answer the most requested information. Relying on free products from companies is not the correct decision if you care about permanence. reply joenot443 6 hours agoparentprevMy sad realization is that in today’s nerd world, there just aren’t that many teenage webmasters. Unix and DNS and Apache just aren’t very sexy or fun to learn when the other cool kids are playing with LLMs and Flutter and WebGL. Obviously _we’d_ prefer if they learned to setup vBulletin and do it our way, but when Discord is free and all their friends are on it, it’s a tough sell to the other nerds. It’s not a “kids these days” grumble, just an honest observation that some computing skills are slowly being lost generationally, and it’s hard to say that anyone’s really to blame. reply hammyhavoc 3 hours agorootparentI don't know if skills are actually being lost. Skilled kids were always an anomaly when it came to tech on a meaningful level. You might even be able to argue there's greater skills than ever before as compute is so cheap and FOSS is so readily available. I was 1 of maybe 3 nerds in a school of 2500 people, 20 years ago. Being smart wasn't deemed to be cool, computers weren't cool—doing well in class made you a \"swot\". The people who do productive things with tech for their own enjoyment and curiosity has always been small relative to people who just \"consume\" as an end user. What is being lost is electronics design skills, despite KiCAD et al being available. Also not much interest in FPGAs and DSP from younger demographics, but at least MiSTer got some younger folks into FPGAs, but unfortunately, prices went through the roof to even get a foot on the ladder. reply opensource4ever 6 hours agoparentprevmatrix.org allows 100 mb attachment. It is federated, which means you can move your data to your server if you like later. There are many clients and server implementations now and it is e2ee. reply mtndew4brkfst 5 hours agorootparentI dip my toes in regularly for the last two years. Every time I do, Matrix has been an extremely janky experience with negligible command of good UX principles. It's awful, and I'm only trying to use two devices. If folks want to convince less ideologically motivated people to switch and support The Cause it still needs so much TLC. It's barely competing on table stakes features for a chat platform. reply theshrike79 5 hours agorootparentThis is my exact experience. The tech underlying Matrix is very interesting, but they really need to hire a proper UX person. reply thatloststudent 6 hours agorootparentprevMore importantly, it is indexable by search engines (if you use view.matrix.org). But it's not as simple to use as discord quite yet. I say this as a self-hosted matrix user, but it's not quite as simple to call someone else (until element call becomes a thing) or use custom emojis, to show a few problems. reply roenxi 5 hours agoparentprev> the fact Discord is closed off in nature means it's quite attractive This phrase was in context of sharing moderate amounts of data, but I think the un-indexable nature of Discord is a feature in more ways than that. The public internet has become decidedly less tolerant over the past decade and I can see why people would want to retreat from it and form more isolated communities. I thought Parler back in 2021 was an interesting case study. From a simple game theory perspective, any group with minority opinions should probably not be organising in public. Majorities are pretty brutal to people who do not conform. We seem to be entering a tense era, so something like the Holocaust or the Chinese Land Reform Movement is not out of the question in the next 50 years. Pseudonymity is a good start, but even better is not having publicly available records of what was said so the bureaucrats don't know what to deanonymise. [0] https://en.wikipedia.org/wiki/List_of_most-visited_websites reply Phiwise_ 6 hours agoprevSide note: I really dislike how shortsighted many orgs are with the extreme level of \"go to your customer\" that led to using the persistent IRC client I used to banter during game nights with my friends a decade ago as an all-encompassing support and outreach forum. Forget forum admins. Forget the future. Forget the \"open web\". It's dysfunctional right now for me, the current target market of whatever product you're selling or program you're running. I'm alienated from whatever your thing or community is by how badly Discord works for over twenty people, so I don't engage with you, and I must imagine many others feel the same way. Surely someone's aware of this in your org? Was the single-digit man-hours saved by not spinning up a FOSS forum or starting a subreddit really worth this? Who made that calculation? /rant reply anymouse123456 6 hours agoprevThis. \"Join my Discord\" is a 100% hard pass for me. Every time I've made an exception, the entire experience is a net-negative. Half the time, I can't even get authenticated and then once I'm in, everything is awful. The notifications, the constant chatter, the dozens of topics, the useless search, it shocks me that anyone over the age of 14 uses it at all. Add to all the broken basics, the fact that your data is now locked away in a vault that will absolutely disappear at some point in the not-too-distant future, and it just makes no sense at all. reply robertlagrant 6 hours agoparentI do find it very strange. It assumes I've been \"invited\" when I just clicked on a link to join from a website. I can't tell why I am or aren't logged in when I join. It's full of animations. Maybe it's the Gen Z take on IRC, but it seems a bit much. The client is very smooth in performance, though, which is nice. reply viraptor 6 hours agoparentprevI don't get this: > The notifications, You can turn them off > the constant chatter, the dozens of topics Topics are there to limit the messages to just what you're interested in, cutting down the chatter. If you expect it to be anything different than discord, you'll be disappointed, but there are communities where discord/slack/mattermost model works fine. reply theshrike79 5 hours agoparentprev> it shocks me that anyone over the age of 14 uses it at all Our group of 40+ old farts uses Discord exclusively, because we can do it all under the same roof. Want to chat about stuff, we've got a few channels just to keep them contained (one for each RPG campaign we're playing and separate ones for the few PC games we have time for). Need to chat during an online game? Discord has video and voice chat as well as screen sharing. Want to keep a log of a RPG campaign? You can do a \"forum\" post about it after every session, it's all there in a nice format under the same platform. When we play online games, like Helldivers 2, everyone can use Discord for voice even when I play with PS5 and the others are PC players. Discord works cross-platform. reply scruple 28 minutes agorootparentExactly the same experience here in the same demographic. In this friend group, we all have kids and wives and pets and jobs and everything else, but we want to still try to do stuff together. Video games, DnD (though we play PF2E and OSR stuff), we do some book club stuff, we talk about sports and our hobbies, we talk about our kids, our personal and professional lives... We can do text, voice, and video. It's been a great experience for us. We do meet in-person every couple of months for a DnD session but that's typically as often as we can manage with 5 busy schedules to juggle. reply ziddoap 6 hours agoparentprev>it shocks me that anyone over the age of 14 uses it at all. Really? I'm in my mid 30s and am in a couple servers with a bunch of 30-50 year olds. I don't know of any better platform to organize and play DnD with voice chat and screen sharing. It's also great for when I play larger online games with a group (e.g. World of Warcraft, Final Fantasy) as we have a guild server to organize absences, socialize while not raiding, etc. If you have a suggestion for a better way to voice chat, screen share, drop in and out of conversations, etc. I'm all ears. For what its worth, all of your complaints (except for search, which really does suck) are easily addressed. reply iteygib 5 hours agoparentprev\"Add to all the broken basics, the fact that your data is now locked away in a vault that will absolutely disappear at some point in the not-too-distant future, and it just makes no sense at all.\" Who cares? Why do you want to record every single thing forever? Do you write down every single thing when you converse with someone in real life? And you really can't figure out how to use Discord, when a 14-year old apparently can? Maybe it's time to retire. reply pwillia7 5 hours agorootparentyeah any complaints about UX should be seen as boomerism and fought against with your full might with no reflection on the critique. When the water wars come and you have to learn to use a slide rule -- remember this day. reply beretguy 7 hours agoprevThe rest of the quote: > Stop using Discord for anything else than just chatting. Stop dumping knowledge into a black hole. Use forums. Use the open web. Future nerds will thank you. reply CM30 6 hours agoparentThis is the key part here. There's nothing wrong with using Discord to chat about random stuff. If you're talking to your friends about your latest night out or playing online games with a community, then Discord is fine. The issue is how some communities use it as a wiki, documentation and forum replacement for information that should be available on the public internet. This is a huge problem for speedrunning and modding communities, which seem to love using Discord and Google Sheets and other awkward 'must know the link to access' tools for information that should probably be in a wiki somewhere. reply sneak 6 hours agorootparentThis is untrue. Talking to friends in DM on non-e2ee platforms normalizes ubiquitous surveillance of friendship interactions and makes all of society worse. Don’t be reachable on non-e2ee DM systems. Delete your account. reply CM30 6 hours agorootparentFair point. I guess a better explanation would be \"don't use messaging apps for publicly accessible content, regardless of if the system is open source, federated or e2ee\". The same general issue would apply if these communities used Matrix or Signal or Telegram or WhatsApp or whatever else. reply beretguy 5 hours agorootparentprevI don’t disagree, but challenge is getting other people getting onboard of another system. reply koolala 6 hours agoparentprevSome people like \"privacy\" or rather - natural emphemeral communication. Discord is instead... a living shared community memory? reply Rinzler89 6 hours agoparentprevYeah I also hate technical discussions on (legal) hacking and tinkering happening in some obscure Telegram or Discord groups that you only find out if you know a guy who knows a guy who found the Discord link on some Reddit post. WTF?! I get the need for non-open communication channels if you want to stay away from Nintendo's lawyers or something but otherwise come on, just have the discussions in a more open place FFS. I also hate Reddit but at least it can be reached via search engines. reply cl3misch 6 hours agoprevI'm not the first to say this, but Discord is essentially IRC. It's great for ephemeral chatting and arguably the best platform for that currently. The content on it is not indexed and not discoverable with web search engines. I have been wondering: is it even possible for a platform to fulfill both needs? For searchable, \"forum style\" discussions, we already have a large accessible platform: Reddit! I genuinely ask myself: why is \"Join our Discord\" much more popular than \"Join our Reddit\"? I personally prefer \"forum style\" discussion over \"IRC style\" discussions, but the polularity of Discord even in technical circles proves to me that a lot of people like the IRC style of communication. reply michaelt 4 hours agoparent> I genuinely ask myself: why is \"Join our Discord\" much more popular than \"Join our Reddit\"? Let's say the maintainers of Foo and the Debian Foo package disagree on which plugins should be in the default install. If they have that discussion on a public forum, maybe unhappy people get directed to the conversation, or someone decides to parade a comment around twitter and link it from HN so everyone can see what an asshole you are. Quite possibly with your real name attached. But if you have the same discussion on Discord? Much harder for people to link to, once it's scrolled off the screen it's practically forgotten, and almost nobody uses their real name. And you don't see people screenshotting Discord posts anywhere near as often as they screenshot Twitter posts (although it is possible, of course). I can see how an open source maintainer using their free time to provide unpaid support might find the latter more pleasant. reply aaravchen 4 hours agoparentprevReddit has become almost defunct in my experience, onky useful as a historical reference since virtually no new content is getting added to it for anything I follow, use, or have interest in. The alternatives mostly aren't, they either segment across servers (federating identity but not topics), or are a little-promoted-and-used forum with the standard 90s interface that scares away newcommers. I'm _still_ looking for a Reddit replacement that has sufficient community to make it at all useful, since Reddit died. reply ThrowawayTestr 6 hours agoparentprevYou can have both. The forum I use also have a irc-like chat section. reply cl3misch 6 hours agorootparentDo you care to share what forum that is? And is its irc-like chat section searchable? Otherwise it is just forum and chat available under the same domain. But information in the IRC section is still ephemeral. reply ThrowawayTestr 2 hours agorootparentrdrama .net. We have a cute orange cat as our mascot. reply poulpy123 6 hours agoprevI totally agree, and also stop using FB groups it's even worse. But the issue is no replacement is proposed by the author. Discord has the advantage of: - being free - being maintained by someone else - being used by a lot of people, so no need to create a new account for many users - being very useful as a chat and growing organically as support from there I don't see many competitors available. For example discourse is paid 50€ per month for 100 users maximum, which small project can pay that ? Edit: I rarely see FB mentioned on these debates but it's probably by far the biggest place used for forum. I'm a member of several 10k+ groups. Even discords don't come close of it. The official bambulab group is 60k people, the hiking group of my area (1 million inhabitants max) is 30k people. The numbers are insane, as is the information invisible and lost reply consp 6 hours agoparent> FB groups it's even worse You can't even view them partially anymore since you illegally have to give up your rights to do so. reply wavyknife 6 hours agoparentprevHello! I work for Discourse, it's open source so you can self-host most sites on a cloud provider like Digital Ocean for ~$20/mo reply poulpy123 3 hours agorootparentI actually enjoy Discourse, it's the best software for forum that I encounter as a user, but someone who wants to create a forum using it must either pay at least 1200€/year or know how to manage a server, including securing it. I'm not saying that the price is not justified (it is) and it's great that it is free software for anyone to install it, but for most non pro usage it cannot compete with discord. reply hprotagonist 6 hours agoparentprevlibera.chat is free. the network doesn’t compel registration, even! (though some channels do) reply poulpy123 6 hours agorootparentIt looks like a small IRC-like chat, not a replacement for a forum and not even for discord-as-a-forum reply hprotagonist 5 hours agorootparentit’s the new name for freenode — so it is IRC and large (by IRC standards…) reply hu3 6 hours agorootparentprevunfortunately, irc is not a replacement for discord. reply hprotagonist 5 hours agorootparentdiscord is just IRC in a trenchcoat. what’s to replace? reply theshrike79 5 hours agorootparentVoice chat, Video chat, emojis, embedded gifs, smart embeds of links. reply hu3 5 hours agorootparentalso: - edit/delete messages - code highlight - notifications - fast search that works for everyone: https://discord.com/blog/how-discord-stores-billions-of-mess... reply hprotagonist 4 hours agorootparentmost of these things are client-specific but available with IRC. (edits aren’t) reply stnmtn 2 hours agorootparentHow long would it take to explain to someone who just wants to voice/video chat with their friends how they can set this functionality up with the same ease as discord? And how much will it cost for them to do that? reply outime 6 hours agoprevIt will be as useful as other HN campaigns: stop using WhatsApp and use Signal, stop using Chrome and use Firefox, stop using Twitter and use this OSS alternative with 0.1% of the users, stop using Reddit, stop using Kubernetes, etc. reply skeeter2020 6 hours agoparentWhat's your alternative action? I tend to agree with you, but personally I've tried to stop pointing out problems without providing solutions; failing at something is better than preemptive surrender. reply adamtaylor_13 6 hours agorootparentSurrender implies there’s something inherently wrong with these alternatives. Something a lot of people just don’t agree with. I agree with you that it’s helpful to be proactive, but the incumbents are there for a reason: at one point a large consensus agreed they were the best for doing X, whatever X might’ve been in that situation. Maybe that’s changed. But a lot of folks don’t think so. Twitter is useful precisely due to its network effect. That’s why I’ll never move to mastodon. Doesn’t make sense for the reason that I use the tool. reply outime 4 hours agorootparentprevI'm not suggesting surrender, but I want to point out that a significant portion of the HN crowd seems to have a very specific perspective that doesn't align with reality. The average Discord user (hence, the largest percentage of the userbase) prioritize convenience, whether they're regular users or server creators. They don't care about being indexed by Google - they just want to connect quickly, chat, play games together, and so on. This convenience-driven mindset extends to technical communities on Discord. Creating a server there is as simple as clicking a couple of buttons, whereas setting up a new forum requires significant technical effort and ongoing maintenance, besides $$$. These practical aspects are often overlooked in discussions, hence the failure of all these campaigns lead by a bit too technical people i.e. those who are willing to spend time researching alternatives and enduring their particular pains, and don't care about networking effects. To challenge Discord effectively, create something equally convenient yet superior (like being indexable by Google). Wait for Discord's deep enshittification and seize the opportunity then. If it's not as convenient, don't expect users to switch. The average user won't invest time in learning new platforms when they can get everything faster and for free elsewhere. reply NeutralForest 6 hours agoprevI feel like we have this discussion every month, one way or another. People use Discord because it's useful and they like the experience. Discourse is nice for purely written communication but if you want to setup a call, direct message and, more importantly, write bots (presentation, topic organization, news, modding, now even AI Q&A); people will fall back to Discord. Pointing at Discourse when it doesn't have the same value proposition and blaming users is just not useful at all. reply kwanbix 6 hours agoparentI have been using computers for 40 years and I cannot see a single feature of discord that I cannot have on a forum or even reddit. In fact, the UX of Discord is abismal. reply lairv 5 hours agorootparentWhat I like about discord is that, in my experience, it lowers the barrier of entry to get a reply on an issue. On forums or GitHub issues, you usually need to be more formal and thoughtful in your posts, you don't want people on stack overflow to tell you that this is a dumb question, and even then you might wait days or weeks for a response. The instant chat format encourages people to simply reply even in informal ways Of course, this isn’t always the case, your messages can still go unanswered for days. But in my experience, when I see that a project has a Discord, I now think \"Great I might get an answer\". And this doesn't dismiss all the cons that others have already mention reply skydhash 5 hours agorootparentThat’s because you were expected to read the manual, and then afterward ask your question. That reduced the number of questions and it’s more likely to have an answer. The forum was an extension to to the manuals (for isolated cases) and a window to how the software/project is being used in the real world. Look at the VIM help as an example of great manual. Or the GNU software documentation. Now, you see “Join our Discord” instead. reply least 5 hours agorootparentprevForums sometimes do have (crappy) live chat experiences. Their main selling point is that they have easily searchable, indexable, and navigable threads that are usually pretty well organized into distinct topics. The UX on Discord is excellent. That's why so many people, including nerds that used to happily use IRC and Mumble, use it. It's easy to create your own server, administer it with granular permissions, and to invite your friends to it. You can instantly start voice and video calls with screen sharing, for free. It's also a live chat with excellent formatting capabilities and has multiline support, unlike IRC (unless this has changed recently?), which makes it much better suited for technical questions involving code. Oh, it also has actual code formatting built in so it's actually legible (though this remains a feature underutilized). The problem with discord is it's just not visible outside of discord, it can't be indexed, archived and searched through other means, and is vulnerable to the whims of the server owner. It's also not very good at being a forum replacement because its threading capabilities and UI wasn't designed for that purpose, so it feels bolted on (because it is). reply NeutralForest 6 hours agorootparentprevThen you're not the public they're targeting but if we want things to change, we need to be cognizant of why people like Discord. reply Klonoar 5 hours agorootparentprevOne of the most wildly successful chatting platforms of the modern era is not objectively “abysmal” just because it does things differently than your 30 or so years of understanding. People on this site really confuse this issue: your dislike of Discord is an ethical one, because it goes against what you believe a chat should be (free/open/whatever). Most people unfortunately just don’t give a shit. reply nehal3m 5 hours agorootparentprevI don't see Discord as a replacement for forums, but as a replacement for TeamSpeak/Ventrilo. The chat is an added bonus. As far as I know there is no other voice communication program that handles voice channels TeamSpeak style with a chat box next to it. If there is one I'd jump ship. reply badsectoracula 5 hours agorootparentprevForums and reddit do not have chat[0] and video calls/streaming. The real alternative to Discord is IRC, Matrix, etc, not Discourse and phpBB or whatever. [0] hacks aside reply theshrike79 5 hours agorootparentprevVoice and video chat on a forum? How would you do that sensibly? Embed Jitsi? reply hyperfuturism 6 hours agorootparentprevYou clearly don't have the live chatting feature in a forum or on reddit. I'm assuming it's a tongue in cheek comment, and not to be taken literal, but there are many features that Discord have that other products do not. reply chrisjj 6 hours agoparentprev> People use Discord because it's useful and they like the experience. Let's fix that. Discord-using people use Discord because it's useful to them and they like the experience. reply NeutralForest 6 hours agorootparentOk? That's just being pedantic. The point I'm trying to make, is that we need to find why people use Discord and offer something better for our (devs+technical users) use case; that'll also need to go the existing network effect. reply chrisjj 5 hours agorootparentThe point I'd make is you'd more need to find out why people who don't use Discourse (instead of fora) don't. The reason surely has little to do with the non-forum-like functions that appeal to those who do like it. Having said which, finding out is obstructed by the fact that very few organisers offer users both. reply ramon156 6 hours agorootparentprevIf I'm honest, those say the exact same thing to me, and doesn't de-power the argument reply badsectoracula 5 hours agoparentprevIndeed, Discord is chat, Discourse is forum. That said, most people use Discord because other people use it and there is much less friction to add a new \"server\" (i hate how they use that term instead of something like \"community\" or whatever) alongside those you are already part of than making a new account to yet another site. I know i'm using Discord mainly because the people actually go there. I used IRC since the 90s but for more than a decade it has been channels with 15-20 people all of whom are AFK and idling via some proxy bot or whatever with the occasional chat now and then by a couple of people (Discord also has a lot of idlers but it has 10x more people too so that couple of people becomes 20 people). IRC not having server-side storage of discussions does not help because i can't login to a channel, ask/answer a question and leave later. I know some people like that lack of permanence but personally i find Discord much easier on actually making or even starting discussions (e.g. i might respond to a message someone left 8 hours ago which is seen by someone else right after i post it and starts a discussion - this only happens because i was able to see that 8h old message). I know IRC supposedly added optional support for this recently but i don't know anything that uses it (and even then it wont solve the \"people go to Discord\" problem since other services like Matrix already supported that for years and people do not go to Matrix). reply Arch-TK 6 hours agoprevYes, please stop using discord. But also maybe stop using discourse. Nothing is more off-putting about an open source project than seeing that discord is the main support forum. I am in the fortunate position of being able to just solve my own problems by myself most of the time, but sometimes it genuinely is helpful to talk to someone about it. If you're an open source project, please use IRC, or matrix if you absolutely must. If you need a forum, well, discourse is not the worst option, but honestly no idea why forums need a tonne of javascript, a re-implementation of basic browser functionality like scrolling, and to override your keybindings. A mailing list is the ideal option. If you really must have something web based, I don't have any recommendations other than \"please don't use discord, at least not unless you feel actually satisfied with its search functionality\". You can self host a forum on a cheap VM and require some kind of user signup to try to mitigate the inevitable spam. reply nicoburns 6 hours agoparentZulip is a good middle ground here. Open source, browsable without logging in (if configured to be), and much better UX than IRC (or god forbid Matrix which has to be about the worst chat software I've used). reply kunley 6 hours agoparentprevValid points. By the way, maybe this is naive, but what's wrong with Github Discussions? reply Svip 6 hours agoprevWhere is the Wordpress.com (i.e. commercial hosting) for forums? I maintain a SMF forum that's lasted since 2000, but I get why most people aren't jumping into creating a new forum, because it's rather tedious compared to Discord. Hell, better ask instead; where is the Discord but for forums (real ones, not Discord)? reply lobsterthief 6 hours agoparentIt’s called Discourse and it’s really great reply bogwog 6 hours agorootparentI don't know what it is about Discourse, but I can never get into it. Maybe it's the UI? It feels like a disorganized mess to me. reply CharlieDigital 6 hours agoparentprevIsn't that just Reddit? reply beeboobaa3 6 hours agorootparentNo, on reddit you are subjected to the reddit administrators their policies. You have no real control. The user interface is also awful if you're interested in content older than a day. reply CharlieDigital 4 hours agorootparentIt's not that different from Discord; unless you run the platform, there's always the risk of deplatforming. reply boxed 6 hours agorootparentprevAnd the voting system builds toxicity for smaller user numbers than should otherwise be needed. reply Svip 6 hours agorootparentprevMostly, yes. Particularly in the terms of \"Discord-difying forums\". But you cannot have multiple subforums for your community on Reddit (unless you do something weird with subsubreddits, but those won't be clearly visible). Discord allows you to have several channels. Additionally, I'd like an alternative that isn't threaded like Reddit and HN. Threaded communities have their place, but classic chronological topics also do. reply rglullis 6 hours agoparentprevhttps://flarum.org/hosting reply hprotagonist 6 hours agoparentprevdiscourse has a few commercial hosting platforms. communiteq is one i’ve used. reply sneak 6 hours agorootparentI use communiteq for my website’s hosted forum and am happy with them. reply asplake 6 hours agoparentprevCircle? reply animaomnium 6 hours agoparentprevIf you're looking for the same sort of walled-garden corporate-degen vibe, try reddit? They even have nested conversations, like on Hacker News! /s But seriously, Discourse is good forum software. You can self-host it or pay for a hosted instance. An example instance is IRLO, https://internals.rust-lang.org reply bogwog 6 hours agoprevI've been on Discord channels where the mods are constantly getting pissed off because people keep asking the same questions, so much so that they ban people when it happens. I'm starting to think that this is part of the appeal. Having an emotional outburst and power tripping is fun for some people, and Discord provides ample opportunity to do it (+ in real-time) However, I will admit that I much prefer seeing that a community has a Discord channel over being exclusively located on Github/Gitlab issues. reply cjbgkagh 5 hours agoparentIt’s a change in culture, people are asking questions a lot more instead of doing their own searches for information. Then they get mad at me when I point out that google provides great results for that exact question and they should have started there. It seems to be an attempt to outsource their thinking to other people. reply consp 6 hours agoprevDiscord has succesfully claimed the word server out of it's meaning and giving people the idea it is \"their server\". It's not your server, it's not even a server at all. reply doommius 6 hours agoparenttbh on the backend they're called guilds: https://discord.com/developers/docs/resources/guild reply lulzury 6 hours agoprevI don't like Discord as much as the next person, but people use Discord because it's convenient. Why do people prefer Discord to IRC when the latter is free, open source, and indexable? reply benoliver999 6 hours agoparentIf you are offline on IRC you don't get messages. I use a bouncer etc but that's more than the average person should have to worry about. There is https://www.irccloud.com/ but I fear it's too little too late Having to create an account on every IRC server is also not great. I use and love IRC, but I get why centralised services take off. It's just a shame it's a sucky one like discord where everything just goes into a black hole. reply miah_ 6 hours agorootparentNot getting messages when I'm offline is a feature. reply gravescale 6 hours agorootparentYou also don't get messages from when you were offline which means if your group spans multiple timezones and you can only be online for a short time, you can completely miss someone. Yes, sure, bouncers, etc, but they were always a gigantic fiddly hassle compared to \"log in, here's what you missed\". reply benoliver999 6 hours agorootparentprevNah it's annoying when someone joins and people have to re-explain what we were just talking about reply skydhash 5 hours agorootparentLike you do for physical gatherings? reply TeMPOraL 2 hours agorootparentYes, if it's relevant, e.g. when running an actual community. But physical gatherings have the advantage of not going on continuously 24/7. My experience with Discord, aa well as communities run by IRC earlier, is that if you log out before work and log in after, you'll spend an hour or more just going through the backlog. Chats are for kids, students and unemployed. reply poulpy123 6 hours agorootparentprevyou're mistaking messages and notifications. reply michaelt 5 hours agoparentprev> Why do people prefer Discord to IRC when the latter is free, open source, and indexable? I am an IRC lover, it's a classic platform. But my classic IRC platform needs someone to host the server. And it needs NickServ to add on the missing persistent usernames. And it needs ChanServ to ensure opers don't lose control of the channel. And it needs a bot so you can leave messages for offline users. And it needs some sort of third-party image hosting and file hosting. And you need something like irssi+screen if you want to see messages that arrived while you were away. And you need to rename yourself to michaelt_afk when you're away. And you need some way to block spammers and other unwanted bots. And if your channel wants a browser-based option, someone's going to have to figure that out. And you've got to take care not to reveal your IP address for DDOS by griefers. And of course the normal ports might be blocked at work/school/college, maybe you'll need a workaround for that. IRC has solutions for all these problems, but not particularly easy solutions. Discord/Slack/Microsoft Teams have succeeded by fixing all the weaknesses of IRC that we IRC lovers have been working around for decades. reply morjom 6 hours agoparentprevYou already answered yourself. Convenience and because your friends are already there or because that one thing hosts its forum there. reply Suzuran 6 hours agoparentprevCheap and easy cross-platform session persistence without the hassle of maintaining a bouncer. reply vlan0 6 hours agoparentprev> but people use Discord because it's convenient. This is it. The brain is wired to expend the least amount of energy possible. Online communities of the past were held up by the few. Those willing to do the work. reply dotnet00 6 hours agoparentprevFlashy UI, channel grouping, custom emoji, server side persistent logs and now the network effect of most likely not having to ask someone to install and learn the ins and outs of another program. Matrix comes the closest to fitting all the same requirements, but I've only ever managed to get one friend to regularly use it, and even then we only use it between ourselves and not to participate in federated matrix servers. reply miah_ 6 hours agoparentprevI use irc primarily, but use Discord for D&D. Voice chat, plus screen sharing. Our DM shares their screen with the map, we use voice to play, and text chat for sharing details about initiative etc. We could probably use mumble and sometime else. The screen sharing part could probably be jitsi. It's just been easier to get everybody playing our game on discord as they already have it. reply jlpom 6 hours agoparentprevBecause of real-timeness (you don't need to refresh a webpage, you can see if someone is typing). I am not aware of a forum software that have this feature (I don't know them well, so I can be mistaken). But most of the time you don't need it. reply CivBase 6 hours agoparentprevFeatures. Is there any other tool that offers persistant text, voice, and video chats plus screen sharing? I've gone looking and there is simply no alternative with core feature parity. reply Ekaros 5 hours agorootparentThere is, but who wants to use Teams or others for hobbies... In the end there is nothing special about Discord, but it does most things people want well enough. reply CivBase 2 hours agorootparentI use Teams for work and to my knowledge it does not have persistant voice/video chats. You have to deliberately start a call and invite people. That's not the same and doesn't lend itself near as well to facilitating online hangouts. It makes sense for a business setting, but not so much in a more personal setting. reply azangru 6 hours agoprevHas anyone remarked on the irony that this appeal to stop using discord, because it isn't searchable from the outside, is published on mastodon, which is also not searchable from the outside? I tried to google for things I had earlier seen on Mastodon, but was never able to find the relevant post again. Even modern-day Twitter, which is impossible to just browse these days, is more googlable. reply eviks 1 hour agoparentThis post is searchable, so no irony reply viraptor 5 hours agoparentprev> on mastodon, which is also not searchable from the outside? By default. You can turn on indexing for your account if you want to. reply Springtime 3 hours agorootparentMy understanding when I looked into this a couple years ago is the optional wider scope search is only possible to enable for one's own posts/account and while logged in[1], but still limited. At a server level the only search possible (using the regular Mastodon) is for user-added tags on posts rather than full text and this was an intentional decision[2]. I'm not familiar with indexability outside Mastodon instances, though can't really recall coming across them naturally in search. [1] https://docs.joinmastodon.org/admin/elasticsearch/ [2] https://www.reddit.com/r/Mastodon/comments/yv1cl4/global_sea... Edit: updated with links from my prior research. reply azangru 5 hours agorootparentprevDoes this mean indexing within the database that drives this Mastodon instance, or indexing by web crawlers such as googlebot? reply TheRoque 5 hours agoprev178 points, 145 comments in 2 hours, how did this disappear from the front page ? Anyways, my biggest concern for discord is their unclear business model. They have a ton of free features (unlimited video and voice streaming, unlimited messages, accessible forever, including uploaded files). But they are free. How do they intend to pay for all this ? AFAIK, their paid stuff (Nitro, goodies, and other features I couldn't care less about) don't give them much. So my concern is that someday, as we'll see an update to the EULA stating that all the data stored forever is now sold to OpenAI or something like this. reply yodon 5 hours agoparentIt disappeared because people disagree on whether to upvote or downvote comments. When the HN algorithm sees signals like comments receiving large numbers of upvotes and downvotes simultaneously, it interprets that signal as \"this post is controversial.\" HN explicitly and intentionally downranks algorithmically identified controversial topics in order to reduce the odds commenters behave badly (which is disproportionately more likely in controversial topics). The HN algorithm is intentional about a primary goal of driving well behaved discussions, even knowing that means important controversial topics getting downranked. reply TheRoque 4 hours agorootparentInteresting, thanks for the insights reply swayvil 5 hours agoparentprev>178 points, 145 comments in 2 hours, how did this disappear from the front page ? Seriously. What's the probability that invisible masters gave the censor order? reply ziml77 5 hours agoprevI love Discord... for chat. I refuse to join servers for projects that are using it in place of a forum. I want to be able to browse the existing information without needing to announce that I'm looking (people joining a server may be announced in a specific channel and you are added to the user list) and without implicitly signing up for notifications (you get the chat notifications unless you make sure to mute the server and suppress @mentions). I want to be able to search for information without trudging through loads of BS chatter and interleaved conversations. Traditional forum software is great for this. Please use it! reply crtasm 1 hour agoprevDoes anyone here run a Discord to IRC bridge? I'd be interested to hear what you're using and your experiences. reply morjom 6 hours agoprevUse a matrix bridge to discord instead. Jokes aside the discord ux is good enough for the average user, it already has chokehold on its users and there is no equivalent. I would also prefer things to be easily searchable, archivable etc but consumers consume. reply lou1306 6 hours agoparentPoint is, if you are using it as a tool to reach out to your community it's mostly okay, but if you are doing support or documentation up there you should really really think twice. In this instance: Amiga 600 Discord where we post pics of our cool Amigas good, Amiga maintenance and schematics hidden in God knows which channel on God knows which Amiga Discord server bad. Best of both worlds: if someone asks question on a Discord and you know the answer, write a blog post (or whatever, as long as it is on the open Web) and link it back! You are guaranteed to have at least 1 readerr which is wildly above the average blog post lol reply michaelt 6 hours agoparentprev> it already has chokehold on its users and there is no equivalent. There are plenty of equivalents - Slack, Microsoft Teams etc offer the same IRC + persistent messages + searchable + persistent user accounts & admin + multi-device + image sharing + file sharing + spam protection product. They're just even worse, by most of the metrics that make people hate discord. reply ourmandave 5 hours agorootparentI'll never use MS Teams for personal stuff because it's way to work adjacent. The fear of a mis-click or wrong login is to stressful to even start. reply morjom 5 hours agorootparentprevIf they're worse are they equivalent? reply michaelt 4 hours agorootparentThey are equivalent in terms of the features I listed - the core product. The big difference is in vibes. Slack is unashamedly corporate and would love to sell you premium SSO integration. They could cut out (or offend) the free community tier, no sweat. On the other hand Discord is more gaming aligned, and would love to sell you some premium custom emojis. Offending the free tier users or getting rid of the free tier is a lot less compatible with their business model. reply giancarlostoro 6 hours agoparentprev> there is no equivalent. Guilded has entered the chat reply anonzzzies 6 hours agoprevLet’s please get back to forums for knowledge. I am/was in discords where very deep tech talks are shared and then it’s closed suddenly; all is gone. It’s depressing. It’s very terrible. People said here before ‘but you can index it like irc’; lovely, but we don’t now, do we? At least not in any structured way. Chat fine, do deep discussions that could benefit people months, years and decades from now somewhere else. reply cl3misch 6 hours agoprev> Stop dumping knowledge into a black hole. Use forums. Use the open web. I feel like there are two discussions to be had: (1) Discord as a commercial platform vs. open web; (2) Discord as an IRC-style platform vs. forums. For me, only (2) is about the point of OP of \"dumping knowledge into a black hole\". It's a black hole because it's IRC, not because it's commercial. reply jimbobthrowawy 6 hours agoprevStill moderately surprised there isn't an option built into discord to make certain channels public for reading/indexing. I have seen one site that seems to enable it by adding their bot to your \"server\", but it's only on a handful of programming-focused ones. reply dimitrisnl 7 hours agoprevValid. I migrated a Rails codebase from Webpacker to Vite. All the help I needed was hidden in an obscure Discord server. Nothing on GH. reply newsclues 6 hours agoprevDiscord and Reddit are terrible dark holes of knowledge reply sampullman 6 hours agoparentAt least reddit is indexed by search engines, right? I find that for a significant portion of my searches, appending \"reddit\" cuts through SEO/blogspam results and I get an answer without having to wade through too much nonsense. reply fireflash38 6 hours agorootparentFor now. How long until they put it behind a sign in barrier ala Instagram, Facebook, Pinterest, etc? reply tokai 6 hours agorootparentThey do that now. reply gl-prod 6 hours agoparentprevYou can find info in Reddit, Discord is a black hole reply bmacho 5 hours agoparentprevReddit you can save with archive.org if you want. reply lacoolj 3 hours agoprevDiscord has features similar to these forums with their threads, etc. Might be worthwhile to pose a feature request that allows these threads to be public and searchable (or at least crawl-able). The chat logs, etc clearly would be a problem but these isolated areas could be prime candidates for a bridge of this gap. reply jhanschoo 4 hours agoprevI don't understand the need to move away from discord. It seems to me that you can have your cake and eat it too if you 1. Have your community accept a TOS that allows you to reproduce and distribute content that they contribute to the server, and 2. have a bridge or logging bot to mirror all conversation on the server. reply bachmeier 6 hours agoprevThe D language forums[1] are a good example of an alternative approach. I don't know what their server/storage costs are, but all that information from the last couple decades is easy to find with a search. The forum is fast, it doesn't require registration, and it's open source.[2] [1] https://forum.dlang.org/ [2] https://github.com/CyberShadow/DFeed reply tmaly 3 hours agoprevIt use to be join this Facebook group, now everyone is pushing private discord servers. I think the old phpBB was way better. But if there was some type of way to output the static html, it would benefit everyone in the long term. reply Ekaros 5 hours agoprevWhy isn't lot of information just inside the project repositories for open source projects? It is text thus space it takes is trivial. At least if images are not needed. It would be versioned so stay in relatively sync with versions of code at time. FAQ and basic configuration things for commonly asked questions could be easily be stored in documents. Why is this not being done? Wouldn't it be trivial amount of work? reply alimoeeny 5 hours agoprevI recently learned about https://www.linen.dev/ which is claiming to fix this issue by \"Make Slack and Discord communities Google-searchable\" An example use by the Haxe community: https://www.linen.dev/s/haxe/c/haxe reply dools 6 hours agoprevI remember a similar sentiment being expressed in IRC chats back in the day, ie. that it was way better to use mailing lists because they publicly recorded all the problems and solutions. reply Kiro 6 hours agoprevStop Using Mastodon. reply azangru 6 hours agoparentWhat would you rather see people use instead? reply thefz 6 hours agoparentprevRight. Page took 10 seconds to load. reply koolala 6 hours agoprevThey forgot the Discord original amazing much better RFC pattern that lets anyone make channels about anything and connect any server, computer, or person together: Internet Relay Communication reply alimoeeny 6 hours agoprevI think this is a big problem we are going to face soon. And I wish I had a solution for it. For example, recently I've been using Haxe + Heaps.io for a game I am working on. And unlike their docs on the web, which is out of date, their discord community is vibrant and VERY VERY helpful and friendly. Which is both heartwarming and sad. Sad because all of this knowledge and advice is hidden from the public web. reply barkerja 6 hours agoprevI use Discord quite a bit in the Elixir community and find it quite useful. Messages are persisted. There's a wonderful forum feature now built into the platform; and it's all searchable with a pretty robust filtering mechanism. And with an active server, there's typically always someone around to help you chat through a problem almost immediately. I understand the sentiment, but it does seem misinformed. Slack, on the other hand. Absolutely! Stay away for anything related to knowledge-base. reply moooo99 6 hours agoparentI don’t think the sentiment is misinformed. I understand your position and I’ve had pleasant experiences on discord in the past with people being very willing to help. However, the help I received for my problem is basically impossible to discovered for anybody else (in fact, I have a hard time finding this thread myself). Discord is really great for realtime interactions, wether that be chatting or talking. But it absolutely sucks at retrieving information generated in the past. It’s not search engine indexable and the discord search itself usually isn’t of any help either. On conventional forums on the other hand, I have learned so much from reading threads from the past. Recently I’ve been diving a bit deeper into construction and I’ve found a forum in my native language. There are so many treasures there: experts from the field sharing experiences and advising on issues, people linking relevant threads, people asking questions and whole threads evolving from that. Sure, there is quite a bit of „toxic“ forum interaction as well, but there are so many gems that would absolutely be undiscoverable if the same interactions would happen on discord reply barkerja 6 hours agorootparent> On conventional forums on the other hand, I have learned so much from reading threads from the past. I agree, this is a great value. I don't argue against forums, they need to exist. I was just pointing out that Discord, in my experience, has not been a place of disdain like the original post claims. Discord has done a lot of work to help foster a good community for all types, especially for the software development communities. If you haven't yet experienced Discord's forum feature, it's worth at least exploring. Here's an example from the Elixir community: https://share.barkerja.dev/4o8Ozk.png If Discord ever did decide to make this content SEO, what then becomes the major point _against_ something like Discord and your traditional forum? reply codingcodingboy 6 hours agorootparentprevI am already distracted enough and now you are getting me interested into construction! reply thefz 6 hours agoparentprev> I use Discord quite a bit in the Elixir community and find it quite useful. Messages are persisted. There's a wonderful forum feature now built into the platform; and it's all searchable with a pretty robust filtering mechanism. Still have to register though and the content is not indexed by a search engine. reply Nevolihs 6 hours agoparentprevA single example of it being done well doesn't have any bearing on all the issues stemming from the use of Discord as knowledge bases, wikis and documentation. Out of curiosity, is any of that useful information on the Elixir Discord available to be searched for with Google or any other search engine? reply barkerja 6 hours agorootparent> Out of curiosity, is any of that useful information on the Elixir Discord available to be searched for with Google or any other search engine? It is not, and that is the one major knock against Discord. But see my response above about that. reply Insalgo 7 hours agoprevAny recommendations for a free engineering community forum? reply mikl 6 hours agoparenthttps://www.discourse.org is great, best forum software I ever saw. You have probably used it already somewhere. They have a paid hosting offer, but you can also self-host it for free. And it’s open source. reply 542458 6 hours agorootparentI don’t get the love for discourse. I’m not one of those “every webpage should work without JS” purists, but Discourse seems intent to reinvent basic web features with JS and has some performance issues and as a consequence of both often (maybe when hitting something not in cache?) takes ages to load during which time you get nothing but a poorly-styled loading throbber, you can’t even read the content as it loads in. Even official instances are incredibly slow at times. One topic on https://meta.discourse.org/ just took over 30 seconds to load for me. Discourse just feels somewhat overengineered. Also anything that’s $50 a month doesn’t compete at the low end (which is where most OSS discussion hosting would exist) with Discord, which is free. reply 4ggr0 6 hours agorootparentI wonder if there's anything wrong on your end. Clicked on your link, searched for a topic with lots of replies and the content loaded instantly. Then I proceeded to spam-click on topics and still everything loaded instantly... MacBook M3 Pro with Firefox. reply 542458 6 hours agorootparentI think it’s a server side cache thing. I was loading a specific old topic on that instance and it took very long to load, but once loaded in the first time refreshes were decently fast (even in private windows). I’ve observed the same behaviour on the Figma forums, which are also discourse. reply eviks 6 hours agorootparentprevMeasure the initial load time, subsequent loads are indeed faster And to test many replies you actually have to see them since they're not loaded on page load, only when you get to the bottom of that page, so just opening the top doesn't test anything re. lots of replies reply wavyknife 5 hours agorootparentprev> which is where most OSS discussion hosting would exist You can host it yourself for much less than $50/mo, some people run small instances on a raspberry pi (I work for Discourse) reply bmacho 5 hours agorootparentprevDiscourse works without javascript. AFAIK it even has email support as well (I don't know how that works). reply 542458 5 hours agorootparentIt barely works without JS. Essentially everything other than basic text display is broken, including search, pagination, sorting, etc. that said, I am surprised it works at all given the aforementioned loading throbber before you can see a page. reply Vvector 6 hours agorootparentprev$50/month for the basic hosted option. Self-hosting requires skills and resources not available to everyone. It's very easy (and free) to start on Discord. Then if it takes off, there is too much friction to move to Discourse. reply chrisjj 6 hours agorootparentprevDiscourse seems great until it isn't. E.g. sticks on a unremovable banner obscuring your content. reply Groxx 6 hours agorootparentprev> Managed forum hosting from $50/month Yeowch. $50/mo is a huge cost for anything that's not making money. reply poulpy123 6 hours agorootparentprevDiscourse is nice but only orga making money can use it reply kaathewise 6 hours agoparentprevI haven't self-hosted any, but I enjoy using Flarum [0] forums. They load faster than Discourse ones and feel snappy. [0]: https://flarum.org/ reply jslakro 6 hours agoparentprevhttps://github.com/flarum/flarum (PHP) https://github.com/NodeBB/NodeBB (js) reply alanbernstein 6 hours agoparentprevphpBB reply eviks 6 hours agorootparentthat's the one without per-post reactions and a weird markup language builtin? reply alanbernstein 3 hours agorootparentI think of it as the one that powered 10+ years of accessible forum posts. reply eviks 1 hour agorootparentAnd unfortunately hadn't evolved in those many years to be a good alternative for this year reply yownv 6 hours agoparentprevSimpleMachineForum reply ChrisArchitect 4 hours agoprevRelated from a few weeks back: Stop Using Discord https://news.ycombinator.com/item?id=40371595 reply Iuz 5 hours agoprevThey won't stop using it. It's simple as that. Should'nt we be doing something to scrape discord servers, push information to a acessible place instead of this? Is that doable? reply segmondy 6 hours agoprevfolks have used such communication channels for a long time. there were folks using bbs chat instead of bbs forums, folks using talk instead of mail, there were folks using irc instead of web forums. there's always room for both. reply erremerre 5 hours agoprevEveryone saying use forums. Like yeah, forums are great if you want to search something and find. But... Have you actually make an account and ask about your problem? Because I have, in reddit, specialised forums, and other forums, make an account and try to post. You have stupid limitations with your new account, most of them, do not impede you asking your question, but it does at linking image, video, or link that would help explaining your problem. You have to create an account for that specific forum. If you are on Reddit you need to deal with automod, who deletes your post the second you press send. Often without a warning, a real reason, other because you asked for help, please check the wiki. (Yes, my problem is not there) Then there is the second part, the community. Most of the time, if you overcome that situations, your post get no answers. Sitting there until it reaches the end of page 1 on the forum, and then it is forgotten forever. Discord is easy, you go, connect with your account, have some rules displayed, accept it, send a chat message, you might get an answer or not, most of the times you do, although is not helpful. So yes, Discord is a sinkhole of information. But the experience is way better, even if it is a shitty mess. reply donkulous 4 hours agoprevPlease tell me where I can download the server binary to run my \"Discord Server.\" Oh wait, I can't? So it's not really \"my\" \"server\"? I would have a very different take on Discord if I could host a server myself and ensure that what I and my community have built won't be deleted at the whim of the corporation or threat from a lawyer. There's no offline backup where you can just spin everything back up on a different host. And to those that think an obscure \"invite\" link will protect you from Nintendo's lawyers, you're living in a different reality than I am. Discord, a registered company with a lot to lose, will throw you under the bus in a heartbeat to lawyers or law enforcement. The current generation is way too comfortable with technical stuff being abstracted away behind pretty cloud based interfaces and Electron apps. reply astromd 5 hours agoprevDiscord isn’t the right tool for a knowledge base, but it’s great for communities. I run several servers but I wouldn’t recommend it for developer projects. reply pavel_lishin 6 hours agoprevCan we change the title to \"Stop using Discord for anything else than just chatting\", which more accurately reflects the author's point? reply gbrindisi 6 hours agoprevCurrent gen LLMs are trained on reddit, stackoverflow, etc. Discord is sitting on little data gold mine ready to be sold to train next gen LLMs. reply j16sdiz 6 hours agoprevThe main complain here is lack of archive/history of what was on the internet. Discord is a big offender, but it is not alone. Consider: Google Plus, Usenet, Facebook. reply iteygib 6 hours agoprevI don't understand the issue here. The author of the post is mad because people have the choice to not publicly discuss things? Why do they owe the author their conversations? Is this AI/data scraping related and the author is mad that they can't get a portion of data out there? reply pavel_lishin 5 hours agoparentThe author is mad - rightfully, in my opinion - that many projects use Discord as a defacto knowledge base/documentation/support hub, instead of setting up something that's much better suited for it. Of course people can privately discuss things, but if you were asking for help with your car, would you prefer that someone tell you which manual and page to look at to solve your problem, or to invite you to their car club that meets on weekends because Timberly probably knows how to solve that problem, and will probably be there next weekend, unless he's not? reply deafpolygon 6 hours agoprevMeanwhile... the mastodon page takes over 3 minutes to load. I don't know of any pre-2010 forums that are still functional and searchable. reply BSDobelix 6 hours agoparent>I don't know of any pre-2010 forums https://www.vogons.org/ ->Since 2002 https://bbs.archlinux.org -> Since 2003 And so on, just to name two important ones. reply GrantMoyer 5 hours agoparentprevThe game specific DarkMatters forums has been around for at least 10 years. I personally used it to resolve an issue I was having, when I found a thread[1] by web search. Some nice people helped the OP troubleshoot over almost 2 years until with their help the OP eventually found the issue and explained the fix. Then the thread went on for a couple more years helping people with similar issues for whom the orginal fix didn't work, until eventually another fix was posted. That other fix was copied over from another forum. [1]: https://darkmatters.org/forums/index.php?/topic/23158-help-s... reply Digit-Al 6 hours agoparentprevThat's odd, it loads in a fraction of a second for me; and I'm on a mobile running android Firefox. Plus I'm on hotel WiFi at the moment. reply dotnet00 6 hours agoparentprevThe OSDev forum (forum.osdev.org) has been around for a while reply yvdriess 6 hours agoparentprevsomethingawful reply edwcross 5 hours agoprevI once wanted to join the NixOS Discord. But of course people just call it \"Nix\", so I googled \"Nix discord\" and followed the first link (\"NIX Official Discord\"). Some of the channels seemed a bit weird, especially one full of memes, somewhat 4chan-like. It took me a few minutes of reading (and a few gross right-wing memes) before I realized that the Discord I joined had absolutely nothing to do with NixOS. The complete opaqueness of the process makes it impossible to know anything at all about the community you are about to join. reply edwcross 5 hours agoparentBy the way, when I actually joined the \"real\" NixOS Discord, and asked a few questions but got no answers, I realized it was better to be on my own. I don't know if there was much useful information inside it, but I can see how it can help kill an open-source sofware community: you either arrived early and had the time to digest the information, or you end up having a very bad experience trying to search for the information, trying to understand how the community works, which channels are useful and which are not... so you choose between being the chatty noob who annoys everyone, or the silent lurker who spends hours looking for information or waiting for a reply. reply davikr 6 hours agoprevLet's go back to Usenet. reply sneak 6 hours agoprevStop using Discord for “just chatting”, too. It’s not end to end encrypted and Discord and whoever acquires them can read all of your chat logs, including DMs, for AI training or whatever else. Friends don’t talk to friends on surveillance platforms. reply stnmtn 5 hours agoparentAre forums not this way as well? reply sneak 2 hours agorootparentThey are, but most people don’t expect privacy in forum DMs, and forum site DMs aren’t usually used that much. Lots of people mistakenly assume their DMs on Discord (and their iMessages) will be private. reply gardenhedge 6 hours agoprevAgreed. I also find it to be a terrible ux so I am bias reply RyJones 6 hours agoparentWe (Hyperledger) got a ton of positive feedback from our blind users when we moved to Discord. Apparently the UX is extremely well laid out for screen readers. reply gostsamo 6 hours agorootparentI'm blind and I don't like Discord. They implement accessibility well, but they, Slack, Teams and the like have decided that I need to know the sender and time stamp of every message in the chat. Read to me for every message. Jumping between screen sections is annoying because lists and headings are too nineties and the hell with them. Captchas? Should I mention that for some reason they disliked one of my logins and put an inaccessible captcha? Maybe your unhappy blind users are still locked out and can't tell you for their issues. Good that I was registered with a gmail account and circumvented them by adding a dot in the email field. Those are personal experiences and I don't know your previous solution, but for me Discord manages to be annoying in a very specific and unnecessary way. reply RyJones 6 hours agorootparentI’m sorry to hear that. I know two of the blind users personally and they are active in discord. I agree on captchas. Rocket.Chat was the previous platform. I will file a bug against discord on the timestamp issue. When I worked for Washington State University, one of our tests pre website update, was the developers had to use a desktop with a screen reader and the display turned off. I think we made great websites for blind users. That was in 2003, so things may have changed. reply gostsamo 5 hours agorootparentMany products would benefit if there is such a friday game in the product team: Achieve story goals without the monitor in fewest possible steps. PS: Regarding the timestamp issue, maybe I've formulated it incorrectly. Just, when you arrow on a message, it reads sender and timestamp. Some people write bursts of messages so you have a name and timestamp for every few words of the conversation and for many stupid emois. reply rightbyte 4 hours agorootparentprevOh... ye that is a great idea. I'll need to try that some time if I ever do a user facing app. But I guess I'll just put a blanket over the monitor. New OSes and GUI frameworks behave strangely with no target resolution. reply ThrowawayTestr 6 hours agoprevWhen you do find a community that you enjoy, make sure your support it. I donate $10 a month to the small forum I frequent. Hosting ain't cheap. reply swayvil 6 hours agoprevI dislike it. Clumsy and noisy. Just bad. reply NemoNobody 6 hours agoprev [–] Ok... does Mastodon always take that long too load? That's not really viable considering the page I ended up on had essentially only textual content - like an old website from way back. Then I read a Geocities suggestion - something from my elementary days Discord is actually pretty cool - a walled garden but no different than the rest of the internet. Google won't send anyone to forums or blogs/personal sites anyways, so this isn't really advice at all reply Andrex 1 hour agoparentMastodon will continue to suck as long as their default client refuses to support noscript access. > Google won't send anyone to forums or blogs/personal sites anyways, so this isn't really advice at all They had that capability in the past, and may again in the future, to say nothing of possible competitors focusing on forum or blog search specifically. Discord is a black box and that data will be lost forever. That's different from publishing on the public web. reply r4indeer 6 hours agoparentprevMastodon is decentralized / federated, so the page load time depends on the instance you're visiting. reply chrisjj 6 hours agoparentprev> Google won't send anyone to forums or blogs/personal sites anyways Oh? It does here. reply jlpom 6 hours agoparentprev [–] No, I think it's the instance reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Malenfant.net is a decentralized social network powered by Mastodon, allowing users to create accounts, follow profiles or hashtags, and interact with posts.",
      "The platform offers features such as live feeds, keyboard shortcuts, and provides information on its privacy policy and server statistics.",
      "This site exemplifies the growing trend towards decentralized social media, emphasizing user control and privacy."
    ],
    "commentSummary": [
      "Discord is lauded for its user-friendly interface, real-time communication, and integrated voice and video chat, making it popular for gaming and social interactions.",
      "Criticisms of Discord include poor organization, lack of searchability, and data privacy concerns, making it less ideal for knowledge retention and technical documentation.",
      "Alternatives like forums, Discourse, and IRC offer better information preservation but struggle with user adoption due to complexity and lack of modern features, reflecting a generational shift towards more convenient platforms."
    ],
    "points": 216,
    "commentCount": 216,
    "retryCount": 0,
    "time": 1717153211
  },
  {
    "id": 40528192,
    "title": "Local AI Solutions for Efficient PDF Search on Mac",
    "originLink": "https://news.ycombinator.com/item?id=40528192",
    "originBody": "As the title says, I have many PDFs - mostly scans via Scansnap - but also non-scans. These are sensitive in nature, e.g. bills, documents, etc. I would like a local-first AI solution that allows me to say things like: \"show me all tax documents for August 2023\" or \"show my home title\". Ideally it is Mac software that can access iCloud too, since that where I store it all. I would prefer to not do any tagging. I would like to optimize on recall over precision, so False Positives in the search results are ok. What are modern approaches to do this, without hacking one up on my own?",
    "commentLink": "https://news.ycombinator.com/item?id=40528192",
    "commentBody": "I have many PDFs – what is the best local way to leverage AI for search?215 points by phodo 22 hours agohidepastfavorite76 comments As the title says, I have many PDFs - mostly scans via Scansnap - but also non-scans. These are sensitive in nature, e.g. bills, documents, etc. I would like a local-first AI solution that allows me to say things like: \"show me all tax documents for August 2023\" or \"show my home title\". Ideally it is Mac software that can access iCloud too, since that where I store it all. I would prefer to not do any tagging. I would like to optimize on recall over precision, so False Positives in the search results are ok. What are modern approaches to do this, without hacking one up on my own? Ey7NFZ3P0nzAe 1 hour agoI am a medical students with thousands and thousands of PDF and was unsatisfied with RAG tools so I made my own. It can consume basically any type of content (pdf, epub, youtube playlist, anki database, mp3, you name it) and does a multi step RAG by first using embedding then filtering using a smaller LLM then answering using by feeding each remaining document to the strong LLM then combine those answers. It supports virtually all LLMs and embeddings, including local LLMs and local embedding It scales surprisingly well and I have tons of improvements to come, when I have some free time or procrastinate. Don't hesitate to ask for features! Here's the link: https://github.com/thiswillbeyourgithub/DocToolsLLM/ reply pierre 20 hours agoprevRAG cli from llamaindex, allow you to do it 100% locally when used with ollama or llamacpp instead of OpenAI. https://docs.llamaindex.ai/en/stable/getting_started/starter... reply nl 18 hours agoparentDoes the llamaindex PDF indexer correctly deal with multi-column PDFs? Most I've seen don't, and you get very odd results because of this. reply rspoerri 13 hours agorootparenti've made quite good conversions from pdf to markdown with https://github.com/VikParuchuri/marker . it's slow but worth a shot. Markdown should be easily parseable by a rag. i'm trying to get a similar system setup on my computer. reply nl 11 hours agorootparentThis looks worth exploring, so thanks. The author has done a bunch of work beyond what PyMuPDF does on multicolumn layouts. reply pierre 15 hours agorootparentprevLocally you can choose pypdf or mupdf wich are good but not perfect. If you can send your data online llamaparse is quite good. reply j45 13 hours agorootparentPulling the text out of the PDFs correctly and independently is correct. reply homarp 20 hours agoparentprevand at some point (https://github.com/ggerganov/llama.cpp/issues/7444) you will be able to use Phi-3-vision https://huggingface.co/microsoft/Phi-3-vision-128k-instruct but for now you will have to use python. You can try it here https://ai.azure.com/explore/models/Phi-3-vision-128k-instru... to get an idea of its OCR + QA abilities reply tspann 5 hours agoparentprevhttps://milvus.io/docs/integrate_with_llamaindex.md Pretty easy to run local and lightweight with Milvus Lite with LlamaIndex reply jd3 19 hours agoparentprevbasically, still the same answer(s) from https://news.ycombinator.com/item?id=38759877 https://news.ycombinator.com/item?id=36832572 reply ekianjo 19 hours agoparentprevllamaindex has an horrible API, very poor docs and is constantly changing. I do not recommend it. reply papichulo2023 17 hours agorootparentAny alternative? reply vladsanchez 0 minutes agorootparentLOL `papichulo`? Que tigre!? hm-nah 4 hours agorootparentprevVanilla python reply bastien2 19 hours agoprevYou don't. You use a full-text indexer and normal search tools. A chatbot is only going to decrease the integrity of query results. reply andai 17 hours agoparentI found that grep actually outperformed vector search for many queries. The only thing I was missing was when I didn't know how exactly to phrase something (the exact keyword to use). Do keyword search systems have workarounds for this? My own idea was for each keyword to generate a list of neighbor keywords in semantic space. I figured with such a dataset, I'd get something approximating vector search for free. I made some attempts at that (found neighbors by their proximity in text), but I ended up with a lot of noise (words that often go together without having the same meaning). So I'd probably have to use actual embeddings instead. More generally, any suggestions for full-text indexing? Elasticsearch seems like overkill. I built my own keyword search in Python (simple tf-idf) which was surprisingly easy. (Long-term project is to have an offline copy of a useful/interesting subset of the internet. Acquiring the datasets is also an open question. Common Crawl is mostly random blogs and forum arguments...) reply skydhash 15 hours agorootparent> The only thing I was missing was when I didn't know how exactly to phrase something (the exact keyword to use). I think that's the only things GUI (or TUI) directories have over CLI. I remember having Wikipedia locally (english texts, back in 2010) and the portals were surprisingly useful. They act like the semantic space in case you can't find an article for your exact word. So Literature > Fiction > Fantasy > Epic Fantasy will probably land you somewhere close to \"The Lord of The Rings\". reply ravetcofx 13 hours agorootparentprevDo you know of any way to build a fast index you can run grep against? Would love to have something as instantaneous as \"Everything\" on windows for full text on Linux so I can just dump everything in a directory reply everforward 3 hours agorootparentAs others have said, ripgrep et al are faster than regular grep. You would probably also get much faster results with an alias that excludes directories you don't expect results in (I.e. I don't normally grep in /var at all). I have seen some recommendations for recoll, but I haven't used it so can't comment. Anecdotally, I normally just use ripgrep in my home directory (it's almost always in ~ if I don't remember where it is). It's fast enough as long as my homedir is local (I.e. not on NFS). reply semi-extrinsic 13 hours agorootparentprevHave you tried the more modern solutions like gripgrep, ack, etc.? Or for something more comprehensive (to also search PDF, docx, etc.) there is ripgrep-all: https://github.com/phiresky/ripgrep-all reply jononor 10 hours agorootparentprevTracker is an open source project for that. It has been around for some 10+ years now. https://tracker.gnome.org/overview/ reply haiku2077 5 hours agorootparentprevTry ripgrep. reply j0hnyl 15 hours agorootparentprevThe point of vector search is to support semantic search. It makes sense that grep will outperform if you're just looking for verbatim occurrences of a string. reply SkyPuncher 15 hours agorootparentprevMost developers are going to outperform vector search. We “get” how computers do lookups so we build our queries appropriately. Vector search is amazing for using layman concepts. reply yreg 16 hours agoparentprev> decrease the integrity of query results What does that even mean. When you know the exact keywords then you use full-text. When you don't know them then other tools can be helpful. reply eviks 13 hours agorootparentIt means you'd use the same tool since it's more convenient and get worse results in one tool vs. the other reply Capricorn2481 12 hours agorootparentBecause they're two different tools for two different tasks. If you expect to always know the exact phrase than, yes, grep will be better. But if you search a semantically similar phrase you will get nothing reply vikramkr 19 hours agoparentprevYou wouldn't use a chatbot for the same query you'd use normal search tools for (and on a side note your answer would be much more useful with an example of what those tools would be, it's not really actionable). A vague natural language question over data whose structure you haven't fully understood using terms that might be inexact is not as likely to provide good results with normal search tools as with an llm based tool. reply skydhash 18 hours agorootparent> your answer would be much more useful with an example of what those tools would be Paperless, DevonThink, even Calibre (the ebook manager) can do it. You only need a day or two to categorize the documents. No need for huge amounts of RAM, or privacy concerns, or hallucinated answers. reply dotancohen 18 hours agorootparent> You only need a day or two For some of us, for some types of data, huge amounts of RAM, or even privacy concerns, or even the occasional hallucinated answer, is an easier pill to swallow. A recent example, maybe not the best example but recent, was the query \"What do the three headed dog from the Harry Potter books and the cat from Alien have in common\" reply brudgers 18 hours agorootparentThey are fictional. reply xeromal 18 hours agorootparentprevI never want to categorize stuff. I want it done for me. reply ajsnigrutin 17 hours agorootparentprevAnother (ugly but works nice): https://www.recoll.org/pics/index.html opensource, local, yada yada, almost zero configuration (just add folders, run indexer, wait). reply rahimnathwani 13 hours agoparentprevPaperless-ngx set up using docker compose is good for this use case. reply m0shen 21 hours agoprevPaperless supports OCR + full text indexing: https://docs.paperless-ngx.com/ As far as AI goes, not sure. reply whynotmaybe 17 hours agoparentYou can use Gpt4all with localdocs to analyze the folder where you store the output of paperless-ngx reply hm-nah 4 hours agoprevYou have the find a good OCR tool that you can run locally on your hardware. RAG depends on your doc processing pipeline. It’s not local, but the Azure Document Intelligence OCR service has a number of prebuilt models. The “prebuilt-read” model is $1.50/1k pages. Once you OCR your docs, you’ll have a JSON of all the text AND you get breakdowns by page/word/paragraph/tables/figures/alllll with bouding-boxes. Forget the Lang/Llama/Chain-theory. You can do it all in vanilla Python. reply elrostelperien 21 hours agoprevFor macOS, there's this: https://pdfsearch.app/ Without AI, but searching the PDF content, I use Recoll (https://www.recoll.org/) or ripgrep-all (https://github.com/phiresky/ripgrep-all) reply constantinum 15 hours agoprevThe primary challenge is not just about harnessing AI for search; it's about preparing complex documents of various formats, structures, designs, scans, multi-layout tables, and even poorly captured images for LLM consumption. This is a crucial issue. There is a 20 min read on why parsing PDFs is hell: https://unstract.com/blog/pdf-hell-and-practical-rag-applica... To parse PDFs for RAG applications, you'll need tools like LLMwhisperer[1] or unstructured.io[2]. Now back to your problem: This solution might be an overkill for your requirement, but you can try the following: To set things up quickly, try Unstract[3], an open-source document processing tool. You can set this up and bring your own LLM models; it also supports local models. It has a GUI to write prompts to get insights from your documents.[4] [1] https://unstract.com/llmwhisperer/ [2] https://unstructured.io/ [3] https://github.com/Zipstack/unstract [4] https://github.com/Zipstack/unstract/blob/main/docs/assets/p... reply jszymborski 15 hours agoparentApache Tika could help extract the relevant bits of PDFs, couldnt it? https://tika.apache.org/ reply fooker 14 hours agoparentprevModern LLMs are good enough at treating pdfs as images and groking the context. Well, Claude and GPT-4 seem to be. reply theolivenbaum 7 hours agoprevIf you're looking for something local, we develop an app for macOS and Windows that let's you search and talk to local files and data from cloud apps: https://curiosity.ai For the AI features, you can use OpenAI or local models (the app uses llama.cpp in the background, it ships with llama3 and a few other models, and we're soon going to let you use any .gguf model) reply skapa_flow 10 hours agoprevGoogle Drive. It doesn't fullfill the \"local\" criteria, but it works for us (small engineering firm). We synchronize our local file server with GD nighly and use it only for searching. Google is just good when it comes to search. reply brailsafe 17 hours agoprevLike many others have suggested, local indexing is what I use for this, although some more natural interface may be better for structured search and querying. What I haven't seen suggested though, is the built-in spotlight. Press CMD+Space, type some unique words that might appear in the document, and spotlight will search it. This also works surprisingly well for non-OCRd images of text, anything inside a zip file, an email, etc.. reply jeffreyq 4 hours agoprevTangentially related, but you can try https://macro.com/ for reading your PDFs. reply pawelduda 7 hours agoprevTry https://github.com/phiresky/ripgrep-all before going down the rabbit hole of AI and advanced indexers. Quick to set up and undo if that's not what you want, but I'm pretty sure you'll be surprised how far can this get you reply epirogov 8 hours agoprevCheap but full featured solution for batch AI processing of PDF documents on your local is an Aspose.PDF ChatGPT plugin https://products.aspose.org/pdf/net/chat-gpt/ reply yousnail 20 hours agoprevPrivateGPT is a great starting point for using a local model and RAG. Text-generation-ui, oogabooga, using superbooga V2 is very nice and more customizable. I’ve used both for sensitive internal SOPs, and both work quite well. Private gpt excels at ingesting many separate documents, the other excels at customization. Both are totally offline, and can use mostly whatever models you want. reply pixelmonkey 16 hours agoprevrga, aka ripgrep-all, is my go-to for this. I suppose grep is a form of AI -- or, at least, an advanced intelligence that's wiser than it looks. ;) https://github.com/phiresky/ripgrep-all reply gyrovagueGeist 16 hours agoparent+1 for this. I use rga all the time. it's a \"simple\" solution but often enough for what I actually needed. reply SoftTalker 16 hours agoprevIf you haven’t given some serious thought to getting rid of most of the documents then consider it. There is very little need to keep most routine documents for more than a few years. If you think you need your electric bill for March 2006 at your fingertips, why? reply datpiff 9 hours agoparentI was hoping someone would make this point. A lot of digital archiving is just delaying tossing things - a hard drive is easier to deal with than boxes of paper. The contents can still be useless. When it comes to a search solution - what kind of searches have you done in the past? What kind of problems did you come across? If the answer to either is \"none\" you are planning on building a useless system. reply gibsonf1 21 hours agoprevhttps://graphmetrix.com/trinpod-server reply bendsawyer 20 hours agoprevI looked into this for sensitive material recently. In the end I got a purpose-built local system built and am having it remotely maintained. Cost: around 5k a year. I used http://www.skunkwerx.ai, who are US based. The result is a huge step up from 'full text search' solutions, for my use case. I can have conversations with decades of documents, and it's incredibly helpful. The support scheme keeps my original documents unconnected from the machine, which I own, while updates are done over a remote link. It's great, and I feel safe. Things change so fast in this space that there did not seem to be a cheap, stable, local alternative. I honestly doubt one is coming. This is not a on-size-fits-all problem. reply Kikawala 20 hours agoprevQuivr: https://github.com/QuivrHQ/quivr SecureAI-Tools: https://github.com/SecureAI-Tools/SecureAI-Tools reply westcort 17 hours agoprevUse Recoll on Linux or File Locator Lite on Windows to do RegEx searches. Design the RegEx searches with GPT or llama running locally (or write them yourself). reply treetalker 16 hours agoprevOn MacOS, use HoudahSpot. It’s awesome. Not AI, but as others have said, you likely want plain text search, not “AI” or a chatbot, for something like this. If you’re having trouble thinking of search terms to plug into HoudahSpot (or grep etc.) then I suppose you could ask a chatbot to assist your brainstorming, and then plug those terms into HoudahSpot/grep/etc. reply Tylast 9 hours agoprevYou can try https://gpt4all.io/index.html reply ilaksh 15 hours agoprevIf you want to run locally you can look into this https://github.com/PaddlePaddle/PaddleOCR https://andrejusb.blogspot.com/2024/03/optimizing-receipt-pr... But I suggest that you just skip that and use gpt-4o. They aren't actually going to steal your data. Sort through it to find anything with a credit card number or anything ahead time. Or you could look into InternVL.. Or a combination of PaddleOCR first and then use a strong LLM via API, like gpt-4o or llama3 70b via together.ai If you truly must do it locally, then if you have two 3090s or 4090s it might work out. Otherwise it the LLMs may not be smart enough to give good results. Leaving out the details of your hardware makes it impossible to give good advice about running locally. Other than, it's not really necessary. reply gnicholas 15 hours agoparent> But I suggest that you just skip that and use gpt-4o. They aren't actually going to steal your data. Why do you have this confidence? Is it based on reading their TOS, and assuming they'll follow it? reply dudus 21 hours agoprevI tried Google's NotebookLM for this use case and was very pleased with the experience. If you trust Google that is. reply hobo_mark 21 hours agoparentNotebookLM is currently US only, limited to 20 documents (sorry, 'sources') per notebook, and only works with Google Drive. reply bendsawyer 20 hours agoparentprevNot offline. I do not trust anyone with some data, because I have contractually promised not to do so. reply 1123581321 20 hours agoprevDevonthink would do this with a tiny model to translate your natural length search prompts into its syntax and your folder/tag tree. If you're okay with some false positives, Devonthink would work as is, actually. reply bendsawyer 20 hours agoparentI used to use this, but the LLM approach allows for much deeper interactions. Not \"find all times I've typed X\" but \"act as an expert in Y, looking across all times I've typed X, summarize my changing position over thee years, and suggest other terms that have a similar pattern of change, in a list.\" The kind of thing I used to give to an intern over a month, with results that are not far off what that intern produced... reply edgyquant 20 hours agoprevUsing python to dump the PDF to text then use llama3 (8B) to parse reply nl 18 hours agoparentThe \"Using python to dump the PDF to text\" dramatically underestimates how hard this is. Tables and especially multi-column PDFs often need one-off handling and - worse - you don't know when one is being misparsed until you start getting weird search results. At that point you need to debug your entire search pipeline, which isn't fun! reply sciencesama 19 hours agoprevYou can tabulate the info 90% of your info will be from single source. There are online tools that sort costco and walmart bills !! reply hulitu 13 hours agoprev> I have many PDFs – what is the best local way to leverage AI for search? Adobe Reader can search all PDFs in a directory. They hide this function though. reply gandalfthepink 18 hours agoprevI use Curiosity AI. Good interface. reply jesterson 11 hours agoprevThe best tool I found for myself for similar goal was Devonthink. Using it for many years since and quite happy with it. There is no AI or any other modern fad, but fulltext search (including OCR for image files inside PDFs) works great reply kkfx 11 hours agoprevHonestly? ocrmypdf + ripgrep-all, recoll (GUI+XLI xapian wrapper) if you prefer an indexed version, for mere full-text search, currently nothing gives better results. The semantic search it's still not there, Paperless-ngx, tagspaces and so on demand way too much time for adding just a single document to be useful at a certain scale. My own personal version is org-mode, I keep all my stuff org-attached, so instead of searching the pdfs I search my notes linking them, a kind of metadata-rich, taggable, quick, full-text search however even if org-ql is there I almost not use it, just org-roam-node-find and counsel-rg on notes. Once done this allow for quick enough manual and variously automated archiving, do it on a large home directory it's a very long and tedious manual work. For me it's worth done since I keep adding documents and using them, but it took more than an year to be \"almost done enough\" and it's still unfinished after 4 years. reply finack 21 hours agoprevOCR and pattern matching on text are computationally cheap and incredibly easy to do. For example, tax documents often bear the name of your government's tax authority, which presumably you are familiar with and can search for. They also tend to have years on them. reply hiq 10 hours agoparentThis. I wanted to convert some equations from some maths textbook back into latex, and I found that taking a screenshot and feeding the image into some LLM service supporting images was a good way to do that. reply adyashakti 22 hours agoprev [–] getcody.ai reply borg16 21 hours agoparent [–] the op wanted a local method, and this does not seem to be local reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The user is looking for a local-first AI solution to manage and search sensitive PDFs stored on iCloud, focusing on content-based queries without manual tagging.",
      "The solution should prioritize recall over precision and be compatible with Mac software, emphasizing modern, ready-made approaches.",
      "Example query: \"show me all tax documents for August 2023.\""
    ],
    "commentSummary": [
      "A user is looking for a local AI solution to search sensitive PDFs on iCloud, preferring Mac software that doesn't require tagging and prioritizes recall over precision.",
      "Suggestions include custom tools using local large language models (LLMs), RAG CLI from LlamaIndex, and converting PDFs to markdown, with tools like pypdf, mupdf, and Phi-3-vision mentioned for OCR and QA.",
      "The discussion highlights the importance of efficient document processing pipelines, the potential of AI models in enhancing search capabilities, and concerns about data privacy and the utility of extensive digital archives."
    ],
    "points": 215,
    "commentCount": 76,
    "retryCount": 0,
    "time": 1717100645
  },
  {
    "id": 40533622,
    "title": "Krita Celebrates 25 Years of Evolution and Innovation in Digital Art",
    "originLink": "https://krita.org/en/posts/2024/krita-25-years/",
    "originBody": "Skip to content Features Download Learn Get Involved Shop Donate Toggle theme Light Dark 25 Years of Krita! Previous PostFriday, 31 May 2024Reading time: 10 minutesHalla Rempt Twenty-five years. A quarter century. That's how long we've been working on Krita. Well, what would become Krita. It started out as KImageShop, but that name was nuked by a now long-dead German lawyer. Then it was renamed to Krayon, and that name was also nuked. Then it was renamed to Krita, and that name stuck. I only became part of Krita in 2003, when Krita was still part of KDE's suite of productivity applications, KOffice, later renamed to Calligra... And I became maintainer of Krita in 2004, when Patrick Julien handed over the baton. That means that I've been around Krita for about twenty of those twenty-five years, so I'll hope you, dear reader, will forgive me for making this a really personal post; a very large part of my life has been tied up with Krita, and it's going to show. But let’s first go back to before when I needed a digital painting application; the first seeds for Krita were laid in 1998, even earlier than the first bits of code. There was this excitement around Linux back then, and there were lots of projects that attempted to create great applications for Linux. One of those projects was GIMP, and another project was Qt. The first was a digital image manipulation application, the other was a toolkit to create user-friendly applications in C++. But GIMP didn't use Qt, it used its home-grown user interface toolkit (although it originally used Motif, which wasn't open source). A Qt fan, Matthias Ettrich, did an experimental port of GIMP to Qt, and gave a presentation about it at the 1998 Linux Kongress. That wasn't received well, and resulted in the kind of spat that is typical of the open source community. People were young and tempers were hot. Well, in cases like this, the only solution is to go at it yourself, and that's what happened. It took several false starts, but on the last day of May 1999, Matthias Elter and Michael Koch started KImageShop: read the mail, because it's quite funny how we did and didn't follow the original vision (KOM was a Corba-like thing, and if you have never heard of Corba, that's probably because Corba was a terrible idea.). Development started, and believe it or not, there's still some actual code dating back to then in Krita's codebase, though most of the remaining code is opening and closing brackets. And then development stopped, because, well, doing a proper image manipulation application isn't easy or quick work. And then it started again, and stopped again, and started again. There were several maintainers before I was looking for a nice, performant codebase for a painting application in 2003. I didn't know C++; but I had written the first book on using Python and Qt together. Krita had been rewritten to the point where it didn't even have a paint tool, so that was the first thing I wanted to have. That was not easy! But... Being open about it not being easy meant people got interested, and we started gaining contributors. And so, in 2004, we had a small team of enthusiastic people. A lot happened in that year; Camilla Boemann rewrote the core of Krita so we had autosizing layers, Adrian Page wrote an OpenGL based backend, Cyrille Berger added the first inklings of plugins and scripting. Our approach was still pretty technical, though, and we didn't manage to make a release. It was only in 2005 that we released Krita as part of KOffice 1.4. Still very immature, but everyone agreed that it was promising, and we got nice reviews in some Linux magazines -- that was still a thing in 2005. Then came 2006. And Krita 1.5 was released with support for color managed CMYK. Krita 1.5 also had the short-lived real color mixing watercolor layer feature, but that was too complex to maintain. And in the same year, we released Krita 1.6: Linux Journal called it State of the Art. We thought it was a pretty mature release, but artists who gave us feedback still found it lacking a lot. And then disaster struck. Qt3 reached end-of-life, and Qt4 was released. The porting effort was huge and took ages, also because we, foolishly, decided to rewrite a lot of the 1.x code to make it possible to share components between KOffice applications. The rewrite took all of 2007, 2008 and half of 2009. In the meantime, when we were desperately trying to fix all the bugs the porting and rewrite were introducing, we held our first fundraiser: that was to get Wacom tablets for testing Krita with, complete with art pens. I am still using the Wacom Intuos 3 we got way back then! In 2009, we then released Krita 2.0. It was not really usable, but it was important for us to have something out that we could get people to test. Krita 2.1 was also released in 2009. We also got our first sponsored developer, Lukáš Tvrdý, whose task specifically was to fix all bugs. Later on, he also improved the performance of Krita's brushes. As Krita gained recognition, we got more and more feedback, and in 2010, we decided to have a big sprint in Deventer where we were going to determine what we wanted Krita to be for our users. A Photoshop clone? A GIMP clone? A Corel Painter Clone? Or something that was itself. Who were we making Krita for? The answer is true to today: we are making Krita for digital artists who are making art, mostly from scratch. Painting with Krita should be fun for artists of all kinds, all over the world. But it would be some time before we'd reach that goal. 2010 saw Krita 2.2 and Krita 2.3: we thought that Krita 2.3 was ready for artists, but it was only with Krita 2.4 and 2.5 in 2012 that Krita really became pretty good! In fact, we had a laser-precise focus: for some years our rallying call was “Make Krita usable for David Revoy!” – partly silly, but also partly serious. We spent time during dev sprints observing artists and allowing them to live comment on what they liked and didn’t like, without the observing developers being allowed to open their mouths, wether in rebuttal or to help the artist out. In the meantime, I had created the Krita Foundation so we could do fund-raisers to sponsor full-time developers. The first developer we sponsored was Dmitry Kazakov, who is still the lead developer for Krita. Back then, Krita was still part of KDE's office suite, but it was called Calligra now, because of an interminable conflict with just one KOffice developer, the KWord maintainer. All that energy spent on that conflict could have gone into development, it was a huge waste. From the Calligra days onwards, development went much smoother. Nokia was now involved with Calligra's development, and the resulting improvement in the central libraries all applications used also helped improve Krita, though, conversely, the complexity needed to support a very diverse set of applications is still burdening us today. Years went by. 2013 was completely uneventful. We made our releases (2.6, 2.7), did our fund-raisers, added features (like animation support), created a version of Krita with a special user interface for touch/tablet users (sponsored by Intel: we still have a great relationship with Intel, our main development fund sponsor). It was great to see the art people were creating, great to get feedback from users and just plain fun to tackle development. In 2014 we ported Krita to Windows, also because of the touch/tablet version of Krita. And we released eleven versions of Krita 2.9, which was really a very fine release. Also in 2014, we had our first Kickstarter campaign. Kickstarter was new and fresh back then, and it was really exciting. We got nearly 700 people to sponsor Krita! And we ported Krita to MacOS. For some time we would do a Kickstarter campaign every year, and they were fun both for us and for our developers, we'd set stretch goals and let people vote on what they wanted us to work on. I still had a day job back then, so it was all work done in the evenings and weekends, and on the train during my commute. We also started porting Krita, again, this time to Qt5. That wasn't as hard as the port from Qt3 to Qt4, but we lost support for the tablet version of Krita because Qt5 made it impossible to properly integrate our OpenGL based canvas in the touch version of Qt5's libraries. We spent months and quite a bit of money on that, but it was no-go. Then I broke my shoulder and lost my day job, with Blue Systems, and suddenly the Krita Foundation needed to pay me, too. Fortunately, we found a sponsor for the port to Qt5, and that was my first sponsored project. In 2016, we released Krita 3.0 -- it wasn't as good as Krita 2.9, but thankfully we still remembered the pain we had when doing a rewrite combined with a port, so we simply did the port first, and didn't combine it with a huge rewrite. This had animation! We also released our first and last paper artbook. A huge amount of work for me, which already started in 2015 and in the end, a huge money sink, too. We worked on improved versions of Krita 3.0 all through 2016 and 2017. 2018 rolled by, and we released Krita 4.0, with the results of Kickstarter-sponsored work. Though not all of it, because in 2017, I was preoccupied with the Great Tax Disaster. The Dutch tax office wanted us to pay tens of thousands of euros in VAT for the work Dmitry had done; that's when we hired a proper accountant instead of a small business administration office in a local town. When we went public with the problems, donations streamed in and PIA made a huge donation: they basically covered the bill. To avoid having this happen again, I brought all commercial activities into a separate one-person company. That became even more important, because in 2017, we put Krita in the Windows Store. That was the second store, after we put Krita on the Steam Store in 2014. Since then, we have released Krita on Epic Store, the Google Play Store and now even on the Apple MacOS Store. Time went on, and in 2018, we released Krita 4.1, in 2019 4.2, in 2020 Krita 4.3 and 4.4. Reasonably quiet years of active development, growing user base and popularity. More and more sponsored developers joined in, and Krita made a lot of progress. Although the Krita YouTube channel already existed, in 2019, we asked Ramon Miranda to work on regular videos for our channel: By now we've built up quite a list of impressive tutorials of all kinds, teaching everything from digital painting itself to creating brush presets! And then development slowed down. In 2020, the effects of Covid19 became more and more clear. We couldn't have sprints anymore, so no hyper-productive in-person development sessions anymore. Team members got sick, for some, really sick. Long Covid has crashed my own productivity: there are many days when I can do nothing but lie down in a darkened room. By 2021, even though we hadn't had to port Krita to a new version of Krita, we still decided to change vector layers from ODG to SVG, which made Krita files incompatible between versions 4 and 5. A major change in file format, in other words. We're still working on new versions of Krita 5: 5.1 in 2022, 5.2 in 2023. The future promises a very nice Krita 5.3! And also, groan, a Krita 6.0 because we have started porting Krita to Qt6. And that's no fun, because Qt6 is again a huge change in what Qt offers and allows. And that was 25 years of working on something I started dabbling in because I wanted to draw a map for a fantasy novel on my laptop! Join the Development Fund with a monthly donation. Or make a one-time donation here. Software Report a bug Roadmap Release History Source Code Sitemap Education Documentation FAQ Privacy Statement Tutorials Foundation About Donations Get Involved What is KDE Website license Contact",
    "commentLink": "https://news.ycombinator.com/item?id=40533622",
    "commentBody": "25 Years of Krita (krita.org)202 points by TangerineDream 7 hours agohidepastfavorite84 comments shortformblog 6 hours agoI use Krita for Photoshop-type stuff, and it works great. It is the only good solution I've found in FOSS-land for animated GIFs, and finding it made it easier to wean off Photoshop. Not even commercial alternatives to Photoshop offer frame-based GIF editing. But Krita, fortunately, does. I know they have a different focus these days, but it'd be nice if they promoted this. reply anonymousab 4 hours agoparentPhotoshop can do GIF editing per frame. At least, my old copy of CS2 could. It just represents each frame as a single layer in the stack, and then combines them in order on export to GIF. Nowhere near as nice as using an animation tool-style horizontal timeline, but it does work in a pinch. reply shortformblog 4 hours agorootparentMore recent versions of Photoshop have a frame-based editing setup that can be modified by hand, allowing use of filters and similar tools. This feature has essentially been ignored by every commercial alternative to Photoshop (Affinty, Pixelmator, etc.), which is why I don’t use them. I edit a lot of GIFs. Krita allows you to import videos or even other GIFs as frame-based animations, which you can then modify to your desire, even adding things like filters or layering images. reply BeFlatXIII 4 hours agoparentprevI bet the UI is nicer for Krita gif editing than the GIMP animation plugin. Time to spend time learning it for a few GIFs I’d like to make. reply shortformblog 3 hours agorootparentMy recommended strategy is to create a document to your preferred size and then go to File -> Import Animation Frames. You can import video or existing GIFs from there. The animation workspace is good for this, of course. reply sebstefan 7 hours agoprevWow. I could've sworn Krita was only around 6 years old. I've only very recently started hearing about it. reply Avshalom 5 hours agoparentWhile Krita has been pretty good for most of its existence it really wasn't until about ten years ago that it started having a life of it's own outside KOffice, then add some time to pick up more features and gain a reputation and you're right at 6 years or-so. reply resource_waste 7 hours agoparentprevThere seems to be a phenomena I observe in the FOSS community where we reflexively respond to suggestions despite them being far beneath their FOSS alternatives. Only after digging or running into issues, do you find the good stuff. Brand name -> Low quality, highly mentioned -> High quality, rarely mentioned Photoshop-> GIMP -> Krita Windows -> Debian-family/Ubuntu/Mint -> Fedora I almost feel like a religious Zealot when I teach people that Krita>>>GIMP, Fedora>>>>>>>>>>>>>>>>>>>>>Debian-family. Its unpaid work, its moral work. People shouldn't have to suffer with low quality products because something was popular a few years ago. reply red_trumpet 6 hours agorootparentDavid Revoy doesn't recommend Fedora 40 for digital artists any more. The main reasons are regressions in tablet software with Plasma 6, and Krita not being a Wayland app. [1] https://www.davidrevoy.com/article1030/debian-12-kde-plasma-... reply rcxdude 6 hours agorootparentYeah, sadly tablet support is one of the areas where wayland has been particularly slow in terms of getting its act together (it's explicitly disabled in krita because of this, which I think is a combo of standards not existing, compositor support, and Qt support). It's definitely the main reason I'm still on Xorg. reply wing-_-nuts 4 hours agorootparentWhich is hilarious when you realize that wayland started as an automotive infotainment project. reply red_trumpet 2 hours agorootparent> automotive infotainment project I haven't heard that story before. Got any references? reply pjerem 6 hours agorootparentprevYou are right, your theory works but it looks like you never tried OpenSUSE. It’s basically Fedora but with great administration tools, a better package manager (but it also uses rpm packages) and with optional AUR-like community packages. OpenSUSE >>> Fedora >>>> Debian Family reply BeFlatXIII 4 hours agorootparentI wonder if there OpenSUSE distros for the RPi or if I’m best sticking to Raspbian due to the hardware quirks. reply grumpyprole 6 hours agorootparentprevFedora and Debian (stable) are not really directly comparable, they have different goals. Fedora is essentially a beta channel for RedHat, equivalent to Debian Unstable/testing. Fedora will ship a more up-to-date desktop, but Debian will be better for servers and/or users who don't need the latest Gnome. reply resource_waste 5 hours agorootparent> Fedora will ship a more up-to-date desktop, but Debian will be better for servers This is the problem, people are recommending Debian-family for desktop. I don't think people are really talking about server linux, that is almost a solved problem. reply leni536 5 hours agorootparentLast time I wanted to try Fedora, it didn't ship some packages I wanted to use that Debian sid shipped, and Debian sid is similarly up-to-date in many stuff (they don't ship plasma 6 yet though, even in sid). I think Debian sid is underappreciated as a decent rolling release distro. I get that Debian doesn't advertise it that way, but it virtually never breaks for me. reply dluga93 6 hours agorootparentprevI used fedora 24 for a few years in the mid-late 2010s. Now I've been using Ubuntu for the past 4 years and I don't see too much difference in my daily use. I mostly use it for development - backends and data/spark mostly, nothing lower level. What does fedora do better? Is it something that was introduced in later releases? reply HeckFeck 6 hours agorootparentprevI wish I'd knew of Krita sooner. Genuinely one of the main drivers of my switch to Mac OS was Affinity Photo (which is excellent btw), because I was driven half insane by GIMP's frustrating keyboard controls. Need it be that difficult to unselect something? reply mkl 6 hours agorootparent> Need it be that difficult to unselect something? What's difficult about Ctrl-Shift-A? If it's a problem for you, it's easy to change the keyboard shortcut. You can also just click away. GIMP certainly makes some things unexpectedly challenging, but that seems like a strange example to pick. reply rcxdude 6 hours agorootparentprevFedora >> Debian is an interesting take. I've never particularly got along with rpm-based distros, and they always seemed much the same quality, just different. What makes the difference in your opinion? reply resource_waste 5 hours agorootparentCopypasted: \"Faster, feature rich, less bugs, things just work. Debian-family will have you running into hardware issues, peripheral issues, software conflicts, and using the terminal to update/install things. I used to think Linux was cheap/knockoff Windows.... No, that was just Debian-family. Fedora is better than Windows.\" reply acomjean 6 hours agorootparentprevThere is a lot of overlap, but I’d hardly call GIMP low quality. I use Gimp for photos (layers and such) . It wasn’t intuitive but I know how to use it now: I have Krita on an android tablet with a stylus for painting (it’s brushes are amazing) and frankly it’s pretty fun to use. Using open source can be “free” but if you use something a lot they alway need donations. reply rcxdude 6 hours agorootparentCompared to Krita's interface, abstractions, and capability, GIMP feels pretty archaic. It's a vaguely capable photo editor but it's awful for digital painting. reply sham1 5 hours agorootparentWell that might be because, well, GIMP is made for photo editing, not for digital painting. Of course, GIMP does have some drawing tools in it, but that's clearly not the task it was designed for. reply mr_mitm 6 hours agorootparentprevWait, can you explain Fedora>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Debian? Should I be looking at switching? reply shawnz 6 hours agorootparentprevI lost confidence in Fedora when they made the incredibly weird decision to disable swap by default based on the flawed argument that memory compression could replace it reply kirubakaran 6 hours agorootparentprevHow is Fedora better than Debian? (genuinely curious) reply imp0cat 6 hours agorootparentIt's not, it's just different. You mostly get newer stuff and more breakage. reply mr_mitm 6 hours agorootparentSo it's like Debian Unstable? reply resource_waste 5 hours agorootparentprev> more breakage. NO! STOP! Seriously, don't spread rumors. Debian \"Stable\" has bugs and issues that up-to-date distros have solved. The word \"Stable\" has 0 correlation with bug-free. It is entirely meaning \"outdated/old\" and used for bug consistency, not OS stability. reply resource_waste 5 hours agorootparentprevFaster, feature rich, less bugs, things just work. Debian-family will have you running into hardware issues, peripheral issues, software conflicts, and using the terminal to update/install things. I used to think Linux was cheap/knockoff Windows.... No, that was just Debian-family. Fedora is better than Windows. reply sgc 5 hours agorootparentAs somebody who has the family using debian for decades now, I am very averse to trying out a new daily driver. Of course if it really is better and not just a different way of going about things, it is worth a try. But this comment gives me no real information, just an opinionated take without pointing to anything specific other than I might need to use apt, which I like. The hardware, software, peripheral comments seem odd or at least about seven years out of date, since I rarely have trouble with any of that and many vendors focus on publishing Debian compatible drivers / software as their only Linux contrib, so they actually get things before other distros in many instances. reply resource_waste 3 hours agorootparentYou are probably the worst person to ask about this topic because you are used to a poorly performing distro. You already learned all the terminal commands to fix things. You already know all the stuff you need to upgrade. Most people havent been using linux for decades, and Fedora takes care of things you 'deal with'. > The hardware, software, peripheral comments seem odd or at least about seven years out of date, Sorry dude, this is just wrong. You literally could not run Nvidia GPUs on Debian last year due to Kernel 5. Your ignorance doesnt make this problem go away. Any new laptop + debian is a disaster. \"Don't use Nvidia, don't use laptops, learn the terminal, its not that hard to upgrade X, its not that hard to install Y, its not that hard to change Z setting\"- Debianer reply viraptor 6 hours agorootparentprevThe main one for me is not pushing snaps and other weird canonical ideas. reply Maskawanian 6 hours agorootparentCanonical is Ubuntu not Debian. reply vachina 7 hours agorootparentprevYou have converted me. reply AlienRobot 5 hours agorootparentprevKrita > GIMP, absolutely, but unfortunately both of them are centuries behind any proprietary software despite only being decades old. And I'm not even talking about PAID proprietary software. Free apps are also better. A frequent issue with FOSS is that they can handle a variety of edge cases because some technical person needed it once and went through the trouble of implementing it, but they offer a subpar experience at basic cases. They'll add a feature because it was in the bug tracker and then say \"it's done,\" and you need to hold a committee for 6 months in the forum to change anything in it. Meanwhile a proprietary app made by 1 person will gain 30 different niceties to the basic feature in 1 week. It's just very hard to imagine how an application that existed for 20 years can't select multiple layers at once or reorder its brushes. It almost feels like abandonware at this point, despite them getting new features from time to time. reply ykonstant 4 hours agorootparentWhich proprietary free app is better than Krita? reply AlienRobot 3 hours agorootparentDepending on your use case, fire alpaca, azpainter, graphics gale. The problem is that Krita/GIMP have many features that very few people use. Like you have digital artists who don't need CMYK, but the entire application needs to support it. Or you have people who want to write text on images. Or people that want to do photo manipulation. Or people who need to make assets for games. Or even doing something weird like saving a PNG without pre-multiplied alpha. So whenever you compare it with something else, you're able to point out to all these features they accumulated through the years. But if you started making an app today for a specific use case, in 1 month you would be able to add features that these applications have lacked for decades, and these will be just the first features you'll be able to think of. So it's always this strange feeling where an application clearly has had a lot of work put into it and yet seems to lack something extremely basic you would expect to find in an application that has had a lot of work put into it. reply Vt71fcAqt7 6 hours agorootparentprevWhy do you think Photoshop is low quality? In my mind Photoshop is very high quality. It just costs money and isn't open source. reply elaus 5 hours agorootparentIn their example Photoshop was \"brand name\", while Gimp was \"low quality\" and Krita was \"high quality\". reply Vt71fcAqt7 5 hours agorootparentI see that now. For some reason I read the first arrow as the arrow of implication. reply HelloNurse 4 hours agorootparentPhotoshop is also low quality, in its own special ways, but it's a different topic. reply onionisafruit 4 hours agorootparentprevThey were saying the Photoshop is the brand name and gimp is it’s low quality replacement. reply binkHN 6 hours agoprevI'm impressed by the quality of programs that are coming out of the KDE ecosystem. While KDE is known for its desktop environment, many of the applications that come out of this organization are simply awesome. reply haunter 55 minutes agoprevStarted using Krita heavily since the local generative AI plugin is available (3rd party not official). Awesome and works perfectly. reply juliangmp 6 hours agoprevI never knew that the project has been around for so long, let alone that its older than myself. But what I can clearly read from the blog post: Qt likes to break compatibility a lot between their major releases... reply influx 6 hours agoprevSounds like developing in QT is awful. They have had to do so much work porting it to new versions of QT. reply tux3 6 hours agoparentThat's just any big framework. Porting a large codebase between major releases of GTK (or Javascript frameworks) isn't much better. reply badsectoracula 6 hours agorootparentDepends on the framework. Qt5 was released in 2012. All of my LCL/Lazarus projects from that time open in modern straight-out-of-git Lazarus just fine. I'm pretty sure 99.9% of my LCL/Lazarus projects from 2005 (when Qt4 was released) also work out of the box, with the exception of those that tried to use strings as byte buffers (fixing this is a quick search and replace) since the datatype changed (this was a compiler/language change though, not a framework one). Retaining backwards compatibility is very important for LCL/Lazarus. That said Qt has been better than other frameworks when it comes to backwards compatibility, after all you can use the Qt5 docs for Qt6 code and vice versa and things work 95% of the time. Actually i was working on some Krita plugin recently and i only had the Qt6 docs around but i never encountered any issue with what i was using. AFAICT Krita's issue is mainly on the lower end of things and how they integrate with their custom OpenGL code. reply raverbashing 6 hours agorootparentprevHonestly the major open source native Widgets frameworks suck (for C/C++) MFC is slightly better, and MFC sucks (at least on MSVC 6.0 era) WxWidgets was one of the best/easiest OSS ones. The best library hands down was the one used by Borland C++ Builder (Windows only), VCL. QT felt like going around in circles and a bit like those \"useless superpower\" charts, GTK like nobody had invented C++ yet. reply AlienRobot 5 hours agorootparentI really think it's holding desktop Linux back. I was making windows apps when I was kid using Visual Studio. There is no way I'd be able to with any of Linux frameworks. The closest thing would probably be to use Java. reply asddubs 1 hour agorootparentqtcreator actually supports a workflow quite similar to Visual Basic 6 in terms of making guis/adding functionality to buttons, etc reply raverbashing 1 hour agorootparentTheoretically. In practice it was much more clunkier. reply RobotToaster 6 hours agoparentprevGUI development is awful in general tbh, outside of basic stuff anyway. reply hanikesn 6 hours agoparentprevImagine porting a web app to new frameworks over 25 years or keeping up with Macs ui Framework over that time frame ... reply Rinzler89 7 hours agoprevWow, is it just me or does that screenshot of 2005 Krita 1.4 UI[1] look absolutely amazing? All the icons are very distinctive and clear on what they do and the colors make them pop without stabbing your retinas. Perfect. Feels like the mid-2000's was the peak in software UI design and it slowly started going downhill afte that, with companies chasing the soulless corporate mono-chromatic flat design inspired from mobile devices where you have no idea which UI element is a button you can hit and making you always hunt down buttons instead of instinctively nailing them every time with your peripheral vision because now all buttons are monochromatic and just bend together (look at your current browser's toolbar for example). [1] https://upload.wikimedia.org/wikipedia/commons/6/69/Krita_1.... reply vvillena 7 hours agoparentThe different icon colors and shapes help a lot. It doesn't look uniform, and this is actually a big plus, as there's less thinking required to identify the target. The new interface looks nicer, but interfaces should focus on being usable, not just look good. Sometimes this means getting out of the way and being unobtrusive. Other times, it means standing out more. The modern Krita (which is not the final picture in the article) still does well. Most of the UI icons have clearly identifiable shapes, information density is even higher than in the old versions, and the style is still following the current fashion, with less color and a darker theme. reply scrlk 7 hours agoparentprevAgreed. I suspect the shift in design occurred when the base assumption moved from \"most users have never used a computer before\" to \"most users are familiar with computers\". IMO Windows Interface Guidelines from 1995 are still relevant today: https://ics.uci.edu/~kobsa/courses/ICS104/course-notes/Micro... reply Rinzler89 6 hours agorootparentAnd since the new generation of users is growing up with mobile touch devices, we're coming back full circle to \"most users never used a computer before\". reply HeckFeck 6 hours agoparentprevKudos for finding a screenshot on Wiki that hasn't been downsized to a postage stamp. For some reason they aggressively enforced a new policy a couple of years ago, and now many of the screenshots are tiny. Case in point: https://en.wikipedia.org/wiki/File:Vchat.PNG What a shame to lose useful illustrations of software history. reply Rinzler89 6 hours agorootparentIt says there the policy for shrinking down that screenshot is due to it being Microsoft copyrighted content. Probably for other copyrighted content as well. But Krita and other FOSS don't enforce this rule. reply HeckFeck 5 hours agorootparentSeems a strange rule though, there are countless other websites hosting screenshots of copyrighted software. Surely it is well covered under fair use, if there needs be any legal basis. reply Rinzler89 4 hours agorootparentIDK what Microsoft though, it could just be some trigger happy lawyers justifying their paychecks the same way Nintendo's do by aggressively defending any an all IP even if it's obsolete, since I doubt Microsoft and anyone there really cares that much about that V-Chat. reply badsectoracula 6 hours agoparentprevKrita at least can use any Qt theme installed on your system, so you can make it look like that (and actually i think even that specific theme is ported to Qt5). I use a theme called \"CDE-esque\" that adds little bevels where appropriate. It isn't really CDE/Motif, if anything it is closer to Plastique (which i think is the theme in your linked shot) but it isn't overly flat either. The icons are still monochrome but they look distinct enough for me to not bother me (and i tend to use the shortcut keys for most things anyway). reply pwitvoet 4 hours agoprevOne thing I particularly like about Krita is its wrap around mode. Very useful when making seamless textures! Another thing is that its file format contains a merged image, which can easily be extracted (.kra and .ora files are zip archives). I've used that to add native .kra/.ora support to a game texture conversion tool. .psd support took a lot more work in comparison. reply jmclnx 6 hours agoprevI am glad the KDE/QT applications are being developed. gimp is good, but my concern is the still rumored move/merge of the gtk into gnome lib. When I need a spreadsheet or WP application, I use calligra as opposed to libraoffice. It comes with my distro and I find these work just as well. I still use gimp only because I forget krita exists, will need to remember that and use it more often :) reply j_san 6 hours agoprevThe text in the \"Choose your membership\" column at https://fund.krita.org/ is broken for me, it's not visible. reply badsectoracula 5 hours agoprevI was (and sometimes still am) using Paint Shop Pro 7 even on Linux (via Wine) until recently, but at some point i wanted to do some texture painting and found the whole process of \"paint stuff, export to PNG, switch to Blender, reload texture, go back, etc\" to be a bit of a PITA. I looked into making a plugin for PSP7 to show the image on a 3D model in a window but PSP7 only supported Photoshop plugins and making Photoshop plugins (for the circa 2000 version of Photoshop) seemed very convoluted as it was a quickly hacked together port of the process used on pre-MacOSX Macintosh. So i thought i might try Krita instead. TBH i was never a fan of Krita's UI, it was too Photoshop-y for me (which i disliked, i found GIMP's UI better but PSP7's the best) but since it does have some digital painting focus and i prefer Qt apps to Gtk apps these days (GIMP is still on Gtk2 which i find perfectly fine, but very soon the Gtk3 version will replace it and i'm not a fan of Gtk3), i decided to check if i can make what i had in mind and if so stick with it. I managed to hack together a plugin[0][1][2] for Krita to display a 3D model with the current image as a texture, initially using pure Python but later changed to a combination of Python and C (to do some more complex processing, like a feature in the latest version to force a predefined 8bit palette[3] to a texture). I also have some WIP shader support[4], though it is currently very experimental (as they increase the complexity a ton, but i'd like to be able to do use Krita for making simple materials). I still think Krita is awkward in various places (i really dislike how paste works with automatic cropping, placing the new paste whenever it feels like and despite the 18913 paste commands there is no something like \"paste into selection\" to automatically scale the pasted image to fill the selection boundaries, which is extremely useful). These can be worked around of course (and the paste into selection thing can be done with a plugin), but i still find PSP7's UX much better overall. From a plugin perspective sadly the plugin API is very limited: there is no way to \"monitor\" an image for changes, for example, so in my plugin (and this is something i've seen many other plugins do) i have to use a timer to constantly grab the image and upload it to the GPU. This is fine for ~1024x1024 and ~2048x2048 textures (depending on yoru PC) but having some sort of \"the image changed at rect x1,y1,x2,y2\" event would really speed things up. Similarly for things like checking if a document is closed (so you can get rid of its references) you have to enumerate all open views and see if there are any remaining views with the document. I did consider building Krita myself and trying to contribute improvements but the build process is a bit shaky and even after i managed to build it, it crashed at startup. As Krita is something i'd probably rely on going forward for my own gamedev needs, i'll probably try again though. [0] http://runtimeterror.com/tools/kritaview3d/ [1] https://i.imgur.com/X4k0qeU.png [2] https://www.youtube.com/watch?v=ciAZmgKe3Ao [3] https://www.youtube.com/watch?v=ZAxiXQVLcRw [4] https://i.imgur.com/I5RXHqy.png reply cranberryturkey 7 hours agoprevKrita is a great replacement for photoshop. reply virtualritz 6 hours agoparent> Krita is a great replacement for photoshop. No it is not. Krita was meant to be an OSS alternative to Corel (ex Fractal Design, ex Meta Creations) Painter [1]. And that's what it became, eventually. And quite a bit more. I've been using Photoshop and Krita since the first version of both of these DCC apps came out (and Painter too, btw). Professionally. I.e. for most of my adult life. Krita is a great paint app. It's not a Photoshop alternative. Photoshop is a great image manipulation app. It's not a Krita alternative. Krita absolutely is a Painter alternative. Painter is no longer a Krita alternative, if you need certain features (that's a topic for an entire blog post). [1] https://en.m.wikipedia.org/wiki/Corel_Painter reply GeneralMaximus 6 hours agorootparentWould it be accurate to say that Krita is a Procreate alternative? reply random42_ 1 hour agorootparentConceptually yes (and Krita might be more feature rich than Procreate), but Procreate is an iPad only app and Krita is not available on that platform, unfortunately. reply virtualritz 4 hours agorootparentprevI don't know Procreate well enough to answer this conclusively. But one thing I can say is that using a tablet with a pressure/angle/etc.-sensitive pen and a screen in front of you, is very different from drawing on paper. Which is very similar, as far as hand/eye coordination and motor memory go, to using a pen with a tablet or a tablet with a built-in screen (e.g. a Wacom Cintiq). I.e. if you own such a tablet and use it with Krita you may get something very similar to Procreate and call it an alternative. But if you just own a normal tablet and a screen, learning the hand-eye coordination usually takes six months, from my experience. That's not really related to the app when you compare desktop painting/image editing apps. But it matters when it's one of the main features (similarity to pen & paper as far as learning goes). As is the case with Procreate. reply chongli 6 hours agoparentprevMaybe if you use Photoshop for painting or art. If you're a photographer who mainly uses Photoshop to do RAW conversion and global adjustments to exposure/colour/saturation/sharpness with a bit of local dodging/burning/healing then Krita doesn't seem to be set up for that sort of workflow. reply hennell 6 hours agoparentprevIf you're creating digital art / painting. It's a good program, but it doesn't do everything Photoshop can (nor does it try to). reply Gualdrapo 6 hours agoparentprevCall me paranoid, but I think it's harmful for Krita when someone frames it as a Photoshop alternative - because it really isn't. It is a program targeted specifically at digital painting. Yes, it can do some \"basic\" image manipulation like Photoshop does, and even you can do some animation stuff too - but it doesn't intend to target everything image manipulation wise as Photoshop does. I for one as a graphic designer use more tools alongside it, as GraphicsMagick or Digikam's Showfoto, because there's some stuff that it's easier to do with them, or they can do some stuff Krita can't. People hears that Krita is \"a photoshop alternative\" in countless comments, blog posts and etcetera and can get frustrated when they find something is lacking. And they can be one of those people that bitch about FOSS because they couldn't manage to do that specific task they can with propietary software. reply shortformblog 6 hours agorootparentI’ve used Photoshop for 25 years, a decade of that as a professional designer. I disagree. Krita is a worthy Photoshop alternative. It has things in areas software explicitly marketed as a Photoshop alternative does not have. Plus it feels intuitive, like Photoshop. I sort of feel like this shield is unnecessary and in some ways harms Krita, a great tool. reply RobotToaster 6 hours agorootparentprev> And they can be one of those people that bitch about FOSS because they couldn't manage to do that specific task they can with propietary software. I doubt they will bitch less if people recommend GIMP. reply jmix 6 hours agoprev [–] What a self-indulgent writeup. The article came nowhere near answering the central question: what are the devs doing other than constantly changing the name of the project and how is this thing better than gimp. reply asddubs 1 hour agoparentit's an anniversary celebration post, not a \"what is krita\" post. It's meant for people already following the project. Krita is an application primarily aimed at digital artwork rather than editing images. So a strong emphasis on drawing and related tasks. For this, it is much easier to use than gimp. I won't necessarily say it's better, but every time I tried using gimp I gave up and I actually stuck with krita. reply ranger207 4 hours agoparentprev [–] > constantly changing the name of the project Oh, I didn't know that Krita used to be called something else. I can't find any details of what its name was before it was Krita though; do you happen to know what it was? reply ttepasse 4 hours agorootparent [–] KImageShop → Krayon → Krita, according to this article from 2009: https://krita.org/en/posts/2009/the-history-of-kimageshop-kr... I googled that because the \"long dead German lawyer\" in the op's article made my spider-sense tingle. Of course it was Gravenreuth, the bête noire of the German internet in the 90s and 2000s. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Krita, a digital painting application, has evolved over 25 years from KImageShop to its current form, overcoming multiple restarts and technical challenges.",
      "Key milestones include transitioning from Qt3 to Qt4, expanding to Windows and MacOS, and overcoming financial hurdles through fundraisers and community support.",
      "The project is now preparing for Krita 6.0, involving a port to Qt6, with recent versions focusing on usability and new features, and ongoing development supported by donations and community involvement."
    ],
    "commentSummary": [
      "Krita, a free and open-source digital painting software, is praised for its animation tools and ease of use, particularly in frame-based GIF editing, which some commercial alternatives lack.",
      "Despite its 25-year history, Krita gained significant recognition only in the past decade, often being recommended over GIMP for digital painting tasks.",
      "Discussions highlight challenges with using Fedora 40 for digital artists due to tablet software issues and Krita not being a Wayland app, with some suggesting OpenSUSE as a better alternative."
    ],
    "points": 202,
    "commentCount": 84,
    "retryCount": 0,
    "time": 1717154331
  }
]
