[
  {
    "id": 37078719,
    "title": "Vim Boss",
    "originLink": "https://neovim.io/news/2023/08",
    "originBody": "About News Development Documentation Sponsors Search K Vim Boss August 2023 Bram is one of my heroes. That’s literal and recursive: when I say it, internally I check before making a frivolous claim, which is a feature of this particular role-model; “What would Bram do?” is a fixture in me which informs my choices. Those who studied vim_dev and the Vim source and docs, accumulated treasure from a stream of copious messages and spare impressions. But also from what he omitted: he never appealed to sensationalism or personal judgements. Even when treated rudely, Bram usually responded only to advance his understanding of a problem to solve. Bram was one of those humans quietly providing deep value to the universe, but there was no parade and little celebrity. Bram was anchored to reality, directly interested in results and adjusting what produced them. The “Problem/Solution” mantra in his commit messages is simple yet profoundly effective. He used that approach to help people in Uganda, managing resources directly instead of abstractly. Bram’s principles (as I observed them) extended beyond mere technical craftsmanship. The ability to adopt a position of modesty is a mind-trick that channels an endeavor through a “narrow waist”, a voluntary constraint. That lens can create a more composable and powerful result. Plugins like unimpaired riff on the theme. And this touches on a central point: the main utility—not ideology, but utility—of “lifestyle software” like Emacs and Vim, is that the ecosystem is alive, and has escape velocity, so its momentum is self-perpetuated. Neovim has always been intentionally positioned as a derivative of Vim, which means simultaneously it both continues and diverges from Vim. I’m convinced that forks create energy rather than destroy it. So although we can’t deliver Vim without Bram, we can continue some essential parts: Maintenance: Experimentation is good, and the world needs creative destruction and playful failures. But Neovim does not represent lust for the new (“neomania”). Documentation: the habits of Vim documentation are obvious, this is one of the biggest gains that Nvim acquired by building on vim. Extensibility: Bram’s own Agide project aspired to a similar sort of extensibility as Neovim: Agide is not a monolitic application. Separate tools can be plugged in. Thus you are not forced to use one editor. … Each tool implements part of the plugin interface. Embedding: Vim’s :help design-not for most of its life proclaimed this tenet of Neovim: Vim is not a shell or an Operating System. … This should work the other way around: Use Vim as a component from a shell or in an IDE. And another thing: Bram didn’t take himself too seriously. He had his own sense of humor. Neovim is a monument to Vim and Bram. We should be pragmatic, not dogmatic; we should remember what the goal is, and compare our actions to the results. — Justin M. Keyes P.S. Jan van den Berg wrote a nice post on Bram’s legacy. News Find more updates in the news archive. What is Neovim? Neovim is a Vim-based text editor engineered for extensibility and usability, to encourage new applications and contributions. Visit #neovim:matrix.org or #neovim on irc.libera.chat to chat with the team. Twitter RSS RSS clients can follow the RSS feed. About · News · Development · Documentation · Sponsors · Mastodon This site's source is hosted on GitHub.",
    "commentLink": "https://news.ycombinator.com/item?id=37078719",
    "commentBody": "Vim BossHacker NewspastloginVim Boss (neovim.io) 1084 points by bpierre 16 hours ago| hidepastfavorite91 comments ansible 15 hours agoThanks Bram.I&#x27;ve been using Vim (and sometimes more recently Neovim) for over 30 years (nearly every work day of my entire career), having used `vi` on various BSD systems in school. In all that time, I&#x27;ve never been on the mailing list for Vim, asked a question or submitted a bug report. I&#x27;ve never needed to do so. Because I&#x27;ve never run into a bug or had a question that couldn&#x27;t be answered by the built in documentation.A quality software project, lead by a quality man. You&#x27;ve been an inspiration. reply jayknight 6 hours agoparentIt was 25 years ago this fall that I got a job in the IT department of my university and my boss gave me the perl and vi O&#x27;Reilly books so I could start programming on the unix servers. Like you, I&#x27;ve used it (and vim) every day of my career since then. reply dkga 10 hours agoparentprevThis reply trymas 3 hours agoprevFrom a linked article [0] there&#x27;s a screenshot of Bram&#x27;s github activity and this text:> Sad> Perhaps all these things add up to why it hit me when I read the news that Bram had passed away. In the message, his family made it clear that Bram was ill (‘a medical condition that progressed quickly over the last few weeks‘). This was unknown to most people and made the news all the more surprising.> And it made me even sadder when someone pointed out Bram’s recent GitHub activity.https:&#x2F;&#x2F;github.com&#x2F;brammool[0] https:&#x2F;&#x2F;j11g.com&#x2F;2023&#x2F;08&#x2F;07&#x2F;the-legacy-of-bram-moolenaar&#x2F;EDIT: From BDFL list in Wikipedia Bram seems to be the first to be marked as †.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Benevolent_dictator_for_life reply emerongi 11 hours agoprevVim has soul. It is that chisel you inherited from your grandpa that you keep using. It fits well in your grip and is comfortable, even though it lacks the soft rubber that the new ones in the store have. It&#x27;s a tool with its own history.Reading the comments here, it seems that the hacker&#x27;s mentality still lives on. The new young billion-dollar company will be replaced in another 10-20 years. Vim lives on. I wish to build a Vim of my own one day. reply pavlov 2 hours agoparentA slight difference though. If a newbie picks up grandpa&#x27;s chisel, it doesn&#x27;t get glued onto their hand until they google the three magic words they must shout to make the chisel come off. reply gpmcadam 1 hour agorootparentI understand your metaphor but I think it&#x27;s probably a stretch.Maybe it&#x27;s better to say the tool has sharp edges that if you aren&#x27;t familiar with it, can bite you. But with a bit of practise and some knowledge you can master it. reply keepamovin 8 hours agoparentprevThat’s a great way to put it. Who are you? reply gitaarik 12 hours agoprevToday at a train station in the Netherlands (Utrecht) I noticed something special. I saw the letters hjkl on the sign in reverse order. These letters indentify the platform areas where you can enter the train. Depending on the length of the platform and the area where the train has entrances, it shows the letters.https:&#x2F;&#x2F;postimg.cc&#x2F;mcCWhMXwEither this was super coincidence, or someone at the NS (the Dutch train company) is a vim fan and payed tribute to Bram like this. reply gitaarik 12 hours agoparentAlthough, thinking about it now, the letters do come after each other in the alpbabeth except that the I is missing between the H and J, but maybe they did that intentionally because it can be confused with the lower case L. So maybe just coincidence after all, but still it was special for me. I never saw this at the trains and now suddenly the week after Bram passed away this shows up. reply mkl 9 hours agorootparentThe partially alphabetic middle row of QWERTY is a remnant of an alphabetic layout that preceded it. QWERTY reduced typewriter jamming by moving common consecutive letters away from each other. reply Affric 10 hours agorootparentprevRegardless it’s touching that it made you think of Bram reply emerongi 12 hours agoparentprevAlthough I use vim, I associate these letters more with the home row for touch typists. reply robbintt 8 hours agorootparentHome row is jkl; reply snvzz 3 hours agorootparentAs a reminder, that&#x27;s why most keyboards have physical indentations in f and j.They&#x27;re meant to be felt by index fingers. reply blahedo 1 hour agorootparentFun historical note: Mac keyboards until the late 90s or so had the bumps on the D and K for the middle finger. In a vacuum either&#x27;s a reasonable choice but going back and forth was always fun. reply zabzonk 12 hours agoparentprevnot a coincedence or tribute - same has been the same in the uk for many years. the letter \"i\" is typically not used for such purposes because it can so easily be confused with numeric \"1\" or alpha \"l\". reply tremon 1 hour agorootparentPretty sure it&#x27;s a tribute -- local trains in NL do not have named carriages FAFAIK, because there&#x27;s no reserved seating. Only international trains have those indicators, and they do not go to Den Helder. reply aprao 14 hours agoprevI reflected on my Vim journey after Bram passed away. It has been my only constant professional companion in the past 10 years - from university projects to FAANGs to everything-on-fire startups. All professional work I have done - the ones I am proud of, the ones that I am ashamed of, the ones that got me promotions and the ones that resulted in $M SEVs - was done on Vim. Oses&#x2F;DBs&#x2F;PLs&#x2F;companies&#x2F;co-workers come and go, but Vim has been forever.Thanks Bram. reply MSFT_Edging 15 hours agoprevI once interviewed an intern candidate who bragged about getting into an argument with Bram on the mailing lists over a possible vulnerability. Bram insisted it was not important, and this young gun insisted it was.We didn&#x27;t hire the guy. It&#x27;s interesting seeing these memorials for Bram, from what people say he was the polar opposite of this kid in a good way. reply anon7331 14 hours agoparentWas this kid arguing about the automatic encrypt&#x2F;decrypt file capability and Bram&#x27;s unwillingness to use a cryptographically secure algorithm? It was a long, long thread that got heated.I was with Bram though! It was never meant to be secure in a cryptographic sense... reply coldtea 13 hours agorootparent\"Never meant to be cryptographically secure\" should never go together with \"encrypt&#x2F;decrypt file capability\".This can even mean someone&#x27;s loss of life in the right (meaning wrong) regime, or property (e.g. storing bitcoin info there).It&#x27;s like advertising a \"self-driving technology\" when your car needs human supervision to not crash and kill you or someone you fall into. reply anon7331 13 hours agorootparentI am not sure it&#x27;s so black and white with encryption. It depends on your threat model. Keeping it secure from an angry ex-girlfriend is one thing, but keeping it secure from a three letter agency is another.The mistake you are referring to is someone that assumes \"encrypted\" means three letter agency safe, which is a pretty terrible way to leverage encryption. In that case, it&#x27;s exactly like hopping in a Tesla and assuming auto pilot will take you home without your supervision. reply coldtea 11 hours agorootparent>The mistake you are referring to is someone that assumes \"encrypted\" means three letter agency safe, which is a pretty terrible way to leverage encryption.That&#x27;s not a mistake, that&#x27;s table stakes. People reading that X offers \"encryption\", should assume its cryptographically safe to the standards of the day, and be given that.Not just some \"safe from your spouse, ...maybe...\" glorified rot13.Else, just don&#x27;t offer it. It&#x27;s not Vim&#x27;s place to offer \"file encryption\" anyway, especially if they can&#x27;t keep that promise. It&#x27;s fine not to offer it.And it doesn&#x27;t have to be a \"three letter agency\" that&#x27;s the threat. The \"angry ex-girlfriend\" could might as well be a programmer. Or have a script-kiddie nephew. Or know a person or two who can use off-the-shelf tools to decrypt it. And the file might have things like a person&#x27;s bank account passwords. reply H8crilA 13 hours agorootparentprevThis would make sense if only it wasn&#x27;t faster to run AES_GCM or some other AEAD, than whatever they did there. reply sitzkrieg 15 hours agoparentprevat least they cared enough to stick to their guns i guess reply dudus 15 hours agorootparentIndeed, not 100% clear but it felt like GP was criticizing the kid.The kid enters a security related conversation and Bram not only listens but engages in the conversation. Seems like you missed on a great hire. reply cweagans 14 hours agorootparentBram listened to and engaged in most conversations, I think. He was very approachable. reply twobitshifter 14 hours agorootparentprevThe article talks about modesty and meeting in the middle, but I get the impression that the younger dev didn’t want to budge or understand the other point being made by Bram. reply chaps 12 hours agoprevVim has had a legitimate impact on my life. Over the past 15 years I&#x27;ve been using it, \"set -o vi\" and similar in psql&#x2F;ipython get added into my rc files almost immediately on a new host. Many, many hours upon hours of living in a shell have been made so much more enjoyable because of his work.Thank you for posting this. Rest in peace, friend. reply imiric 9 hours agoparentSame here. Learning Vim commands has had one of the greatest ROI of anything I know. It&#x27;s been invaluable over the years, and has probably saved me hundreds of hours of interacting with a computer. It&#x27;s become muscle memory a long time ago, and editing text and moving a cursor on the screen takes no effort at all.It&#x27;s one of those things I always suggest to new programmers. Learning Vim may arguably have a steep learning curve, but it will open your eyes to a different and more efficient way of interacting with text, which is something you constantly do as a programmer.I&#x27;ve enabled Vi(m) mode everywhere it&#x27;s supported: from shells and everything that uses Readline, to making it the default in Emacs via evil-mode.The modal interface and mnemonic commands that can be combined in different ways is a far superior and more intuitive UI than the Emacs key chord UI. I love Vim. It&#x27;s what got me started on this journey 20+ years ago. I still use it daily for quick edits, but prefer Emacs as an IDE, as it&#x27;s a more extensible environment. With evil-mode, I get the best of both worlds. :)Bram has improved so many of our lives, and deserves to be recognized among the all time greats in our industry. RIP. reply RetroTechie 14 hours agoprevThought a linked article:https:&#x2F;&#x2F;j11g.com&#x2F;2023&#x2F;08&#x2F;07&#x2F;the-legacy-of-bram-moolenaar&#x2F;By Jan van den Berg, was a better read. One quote:\"Vim is a masterpiece, the gleaming precise machining tool from which so much of modernity was crafted\".And then this link:https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=eX9m3g5J-XA\"7 Habits for Effective Text Editing 2.0\"1h20m - If anyone has a transcript or summary: plz. But this was funny - first comment:\"I let auto play go on while I was sleeping for 7 hours and went from Billie Eilish to this\"Life is weird but I love it :-)Disclaimer: I&#x27;m not even user of either (Neo)vim or Emacs. As soon as I read \"programmable\", I&#x27;m off. I just prefer fixed-function editor that suits my taste , a few minutes of configuration & go. But that&#x27;s just me, what do I know?That said: good tools serve a purpose. Our world is a better place with good tools in it - and the ppl who made those tools. And sometimes their legacy, their philosophy, their way of doing things, lives on in the code (or the community!) they left behind. So kudoz to Bram! reply bla3 13 hours agoparentI don&#x27;t know if it&#x27;s necessary to be competitive about eulogies, but the one you posted was also touching. Thank you for posting it. The picture of Bram&#x27;s GitHub contribution graph is haunting. reply RGBCube 14 hours agoparentprev> Disclaimer: I&#x27;m not even user of either of (Neo)vim or Emacs. As soon as I read \"programmable\", I&#x27;m off. I just prefer fixed-function editor that suits my taste , a few minutes of configuration & go. But that&#x27;s just me, what do I know?Then Helix may be for you! It uses the Kakoune keybinds, which make SO MUCH SENSE compared to Vim or Emacs. And it&#x27;s already pre configured and includes a lot of useful features. I have been daily driving it and it&#x27;s pretty fast and good. Since it automatically uses LSPs if they are in the PATH, it requires very minimal configuration, my configuration only includes theme and some stylistic changes, you can take a look at it if you&#x27;d like: https:&#x2F;&#x2F;github.com&#x2F;RGBCube&#x2F;NixOSConfiguration&#x2F;blob&#x2F;master&#x2F;ma... reply __MatrixMan__ 10 hours agorootparentI&#x27;ve been a happy vimmer for 15 years. I&#x27;ve been dabbling in helix for a month and liking it a lot. The ast-aware selection capabilities--while not habit yet--feel like a fledgling superpower. reply nequo 9 hours agorootparentNeovim has a plugin for that too.[1] Is Helix different in how you can navigate the syntax tree?[1] https:&#x2F;&#x2F;github.com&#x2F;nvim-treesitter&#x2F;nvim-treesitter-textobjec... reply mlry 13 hours agoparentprev>\"7 Habits for Effective Text Editing 2.0\">1h20m - If anyone has a transcript or summary: plz. But this was funny - first comment:Here you go: https:&#x2F;&#x2F;moolenaar.net&#x2F;habits_2007.pdfCourtesy of tlamponi [1]:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37013315 reply petre 12 hours agoparentprev> As soon as I read \"programmable\", I&#x27;m off. I just prefer fixed-function editor that suits my tasteI never have to \"program\" Vim apart from set nocompatible, set ruler, backspace, indentation and maybe pick another colorscheme from the defaults (usually torte).On neovim I installed lightline and pathogen, monokai colorscheme and something else I forgot about, maybe syntax highlighting for some more exotic language.The point is Vim is a fixed function editor until you start loading it up like a xmas tree. reply gorjusborg 12 hours agorootparent> The point is Vim is a fixed function editor until you start loading it up like a xmas tree.Exactly, just because you can doesn&#x27;t mean you need to, or should. reply imron 10 hours agorootparentMy .exrc file ismap :perl use Text::FindIndent;VIM::DoCommand($_) for Text::FindIndent->to_vim_commands(join \"\\n\", $curbuf->Get(1..$curbuf->Count()));Text::FindIndent is the aforementioned heuristic as a Perl module. Sorry for the mangled quote, original at https:&#x2F;&#x2F;metacpan.org&#x2F;pod&#x2F;Text::FindIndentWhen I work on existing code, I hit F5 and work within the author&#x27;s preferences. reply snvzz 3 hours agorootparentprev>Vim is what I mainly use out of habbit. It only requires typing vim as opposed to nvim.I got used to typing vi instead. It&#x27;s shorter alias vi=vimbecomes alias vi=nvim reply petre 3 hours agorootparentIt also works on fresh linux installs without the \"damn I forgot to install vim\" moment. replyizolate 8 hours agoprevIn a world of fleeting software, Vim is a timeless masterpiece.I only wish I had the opportunity to thank Bram for the impact he&#x27;s had on my life and career, because I don&#x27;t think I would have come as far without Vim. reply bilekas 15 hours agoprevSo often we hear after someone dies how great they really where when infact they were like most people, a mixed bag. We all have ups and downs etc.There are some though that really do live up to the postmortem messages and for sure Bram is one of them, i was lucky to meet him and actually spend some time shooting shit with him. At the time i didn&#x27;t realize how big a deal he was, it was only later i would actually get to know and appreciate VIM, but from all of that it made me appreciate him a lot more. So when I read these kind of posts i can&#x27;t help feel sad of course but I do get shivers of how damn ossum he really was. The testimonials to him hit so hard knowing they&#x27;re 100% genuine. We need more like him, he will be really missed. reply topher200 13 hours agoparentI occasionally ponder on how frustrating it is that the deceased don&#x27;t get to hear their eulogies. It would have been amazing for Bram to have been able to experience all the outpouring of support and love from people he is interacted with during his life. Especially in a context where the speakers aren&#x27;t considering him as an audience -- they&#x27;re sharing their deep and true feelings.No one can really be sure how their acquaintances feel about them. Eulogies are the closest we get. Imagine if he were able to hear all these great things said about him... It would be such a joy. reply dmvdoug 11 hours agorootparentThis is why I always make it a point to tell people how much I appreciate them and why (when I do, I mean). It can be awkward at first, but I’ve developed a good self-deprecation that lets me excuse myself for being gushy (“I might start crying; I’m a crier!”), and that disarms people for the most part. I think it’s really important that we let people know how much we value them and why we honor&#x2F;respect them when we do. Because most of us do wander through life in a cloud of unknowing and uncertainty. reply abraae 12 hours agorootparentprevOff topic but your comment makes me think of Nick Drake. He died at 26, before even knowing he&#x27;d achieved anything, his music barely listened to, probably feeling a failure. Posthumously he&#x27;s one of the world&#x27;s most acclaimed recording artists. RIP Bram. reply block_dagger 12 hours agorootparentGood example, although Van Gogh is probably the more cited. reply halifaxbeard 11 hours agorootparent“The Doctor and Amy take Vincent Van Gogh - who struggled to sell a single painting in his own lifetime - to a Paris art Gallery in the year 2010”https:&#x2F;&#x2F;youtu.be&#x2F;ubTJI_UphPk reply jamiek88 9 hours agorootparentI remember the emotional impact of that scene hit me hard when I first saw it. It still hits 89% as hard now upon many repeat viewings. reply mongol 12 hours agorootparentprevAnother example is Stieg Larsson, author of the Girl with the dragon tattoo triology. He died before these books were published. reply flylikeabanana 5 hours agorootparentAlso John Kennedy Toole, his novel The Confederacy of Dunces was published by his mother after his suicide and it ended up winning the Pulitzer. reply slushh 4 hours agorootparentprevAt least in one of the Van Gogh movies, he says something like &#x27;one day, people will understand&#x27;. To me, that suggests that he knew the value of his work whereas OP suggests that Nick Drake didn&#x27;t know that he created something that people will value. reply bmiller2 12 hours agorootparentprevLarry David had the same thought, and it was the theme of the episode \"The Covid Hoarder\" in Curb Your Enthusiasm wherein Albert Brooks stages a funeral for his non-deceased self. reply dtgriscom 7 hours agorootparentprevCue \"Waking Ned Divine\" reference: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=UXe_kRQdHfU reply jfax 11 hours agorootparentprevThat&#x27;s what birthdays are for. I like to think, anyway. reply parentheses 13 hours agoprevAside from the POSIX toolchain&#x2F;kernel&#x2F;etc and browser (which I&#x27;d use if it neatly embedded into vim), vim and its siblings are together the most used program on my machine.It&#x27;s only today that I am forced to think about this: One person drove that program from an idea in their head to a high-quality, hackable program that I&#x27;ve fallen in love with over the years! A program that I have used for so many hours, but has never crashed once (neovim does crash, but vim has never done this for me.) This is an impressive feat given the sheer volume of vim plugins and configurations I&#x27;ve gone through!Thank you Bram! You built an amazing program and the community around it! reply 1-2-3-5-8 7 hours agoprevThank you Bram for everything. Your talk, 7 Habits For Effective Text Editing 2.0 inspired me and it started my Vim journey. Using Vim all these years has been a complete joy. You will not be forgotten.Link to the talk: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=p6K4iIMlouI reply beefsack 8 hours agoprevI thought this was quite well written. \"Be pragmatic, not dogmatic\" is a very sensible rule of thumb. reply 29athrowaway 12 hours agoprevBram is a guy who could have chosen to make millions but instead helped millions. reply zgluck 12 hours agoprevhttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Benevolent_dictator_for_life has a list of BDFLs. Bram is the first BDFL in that list who has passed away. We&#x27;re in for a generational shift the coming decade(s). It makes sense to prepare - that is how you preserve a legacy, and the community.I&#x27;ve been impressed with how Bram&#x27;s passing has been handled by his family and friends with respect to his legacy and the future of vim. reply michaelmrose 16 hours agoprevIt is nice to see tasteful and profitable disagreement in place of simple drama. It&#x27;s a credit to all parties. reply tequila_shot 16 hours agoparentsorry, what disagreement? I didn&#x27;t find any in the article. reply ampersandy 15 hours agorootparentThe implied disagreement for why neovim exists to provide functionality that wouldn’t have been merged into vim. reply BlackjackCF 15 hours agorootparentprevSome people take issue with NeoVim, because they believe it splits the Vim community and takes people power away from Vim development. reply SpaceManNabs 14 hours agorootparent> because they believe it splits the Vim community and takes people power away from Vim developmentI don&#x27;t understand this point, and I tried to parse it and still don&#x27;t. (I understand that you are just relaying it).If vim maintainers don&#x27;t want neovim to exist, they should have accepted the merges earlier. If they disagree with the merges (which I think they did), then that power doesn&#x27;t belong in Vim anyways.edit: this reminds me of this conversation from years agohttps:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=14245705Check DSMan195276&#x27;s comments.And finally before I derail, I want to bring stuff back to the focus: RIP Braam. reply coldtea 13 hours agorootparent>I don&#x27;t understand this point, and I tried to parse it and still don&#x27;t. (...) If vim maintainers don&#x27;t want neovim to exist, they should have accepted the merges earlier. If they disagree with the merges (which I think they did), then that power doesn&#x27;t belong in Vim anywaysIt&#x27;s a very simple point to understand: whether the merges are good or not, the presence of a fork still \"splits the Vim community and takes people power away from Vim development\".And if they&#x27;re bad (which is the way they see it), they do it for no good reason too.That&#x27;s regardless of people \"having the right to fork\". Yes they do. But also yes, if they exercize that right, they do split a community and divert interest from a project to 2 projects. reply LexiMax 11 hours agorootparent> That&#x27;s regardless of people \"having the right to fork\". Yes they do. But also yes, if they exercize that right, they do split a community and divert interest from a project to 2 projects.You&#x27;re not dividing the same-sized pie.The alternative to a fork is a single project with fewer contributors.The alternative to two communities is a single community that&#x27;s not as large as the two would&#x27;ve been, with a good chunk of those remaining having unfulfilled wishes or unheard complaints.The alternative to Vim as it is today is very likely Vim without a a few of its new features and improvements that came with Vim 8 and 9.Forks are good. reply coldtea 8 hours agorootparent>You&#x27;re not dividing the same-sized pie.We do. Neither vim-or-vim-fork editor users nor vim-or-vim-fork-potential-devs are going to see any significant jump in population from the presense of neovim. People who are new to vim-style-editors and go to neovim are mainly people who would have gone to vim if neovim didn&#x27;t exist.And from people contributing to vim via themes, plugins, etc., some have taken their talents to neovim, which would have stayed with vim if neovim didn&#x27;t exist.>Forks are goodWell, the prevalent wisdom of 30+ years of FOSS has been that they&#x27;re mostly bad. reply nequo 6 hours agorootparentYou are stating these things as though they were established facts. But they seem to be opinions. Or do you have data to back them up?> People who are new to vim-style-editors and go to neovim are mainly people who would have gone to vim if neovim didn&#x27;t exist.It seems reasonable to assume that a lot of new people would not pick up either Vim or Neovim without LSP integration and Tree-sitter.Vim has adopted a lot of the early features of Neovim and now Vim9 also has things like virtual text for rendering LSP diagnostics in the buffer[1] and there is a Vim9 LSP plugin too. But it does not at all seem likely that Vim would have these were it not for the push from Neovim.Besides, it looks like Vim still does not have mature support for Tree-sitter.[2]> Well, the prevalent wisdom of 30+ years of FOSS has been that they&#x27;re mostly bad.There are many famous forks from the past 30 years that hardly anybody calls bad. Some examples: Net&#x2F;Free&#x2F;OpenBSD, GNU&#x2F;XEmacs, Open&#x2F;LibreSSL. These projects allowed people with different goals or values to carry on in their own directions, while also motivating each other to pick up development pace. They have often also shared code with each other.[1] Which looks like this: https:&#x2F;&#x2F;sr.ht&#x2F;%7Ewhynothugo&#x2F;lsp_lines.nvim&#x2F;[2] One experimental plugin I came across: https:&#x2F;&#x2F;github.com&#x2F;mattn&#x2F;vim-treesitter reply pydave 6 hours agorootparentprevI&#x27;m not so sure.Neovim migrated to a more modern style of C and more tightly integrated Lua as its scripting language (instead of conscript). Those are two large potential stumbling blocks for contributors.From loosely following development over the years, I see names like chrisbra and justinmk who contribute to both projects, but there seem to be many who contribute to neovim but never contributed to vim.Neovim also seems to have influenced the development of vim: channels, jobs, terminal mode, and issues&#x2F;PRs on github (instead of mailinglists) felt like shifts in response to neovim.I think it&#x27;s also to Bram, justinmk, and other maintainers credit that the two projects contribute back and forth: many vim fixes are merged to neovim and I see big changes get brought back to vim too. reply shanusmagnus 12 hours agorootparentprevI guess you&#x27;re right, the logic you describe is simple, but I submit that there&#x27;s a more fundamental logic: If the folks who split away pull people out of the original community, it means there was demand for the things that prompted the split in the first place.If Neovim had nothing to offer vs vanilla Vim, nobody would have followed them. This seems like efficient exploration of an idea space, to me, and something to be celebrated. reply falcolas 13 hours agorootparentprevWhat splits it more, IMO, is that neovim went in a non-compatible direction with many of their design choices. They have good arguments for doing so, but it still means that writing community plugins that can work with either version of vim is inherently harder. reply coldtea 11 hours agorootparentI guess if they haven&#x27;t gotten in a non-compatible direction, then they wouldn&#x27;t have made enough change to guarantee a fork. reply anyonecancode 8 hours agorootparentprevThat feels a bit like criticizing all the c-style programming languages for splitting the C community... Which is an analogy I rather like, as it suggest Vim has become so foundational that its legacy goes beyond the direct users of it. I&#x27;ve certainly found that to be so, as in addition to using vim itself, I also use its key bindings wherever I can. reply prmoustache 14 hours agorootparentprevSome people can&#x27;t grasp how the world is working. reply michaelmrose 14 hours agorootparentprevDisagreement insofar as the direction of vim. Neovim choosing to go there on way on functionality they wanted to implement both ultimately enriched vim as some ideas found there way into vim proper and gave the community additional options.Such splits aren&#x27;t always handled well and value is lost when good ideas aren&#x27;t merged back because of personal reasons and when contributors stop contributing because they are turned off by the drama. See libav vs ffmpeg. reply dudus 15 hours agoparentprevIs there any disagreement on who is going to support Vim moving forward?I can see the fork maintainers going into a power struggle if the future is unclear. Maybe some will even write fluffy pieces on how much they loved Bram to gather support... reply lost_tourist 13 hours agorootparentNeovim and vim are very independent projects, so this doesn&#x27;t really affect neovim all that much. Is that what you&#x27;re talking about? reply jgb1984 11 hours agorootparentNot entirely correct since neovim never stopped merging upstream patches from vim. So what happens in vim does influence neovim. reply vjust 13 hours agoprevthanks Bram. reply djha-skin 15 hours agoprevI&#x27;m shocked. I&#x27;ve been a vim user my whole life. I use neovim lately, but I didn&#x27;t even know Bram was dead. I&#x27;ve never interacted with him personally, but I&#x27;ve interacted with the tool he wrote and the documentation he wrote almost on a daily basis. Vim is part of me.I know the project will likely continue, but I can&#x27;t help but thinking: what now?It brings up this issue of death in software for me. Software is getting old enough now that it is starting to outlive its authors. RIP Anthony Grimes and Ian Murdock. I have used both of these men&#x27;s software after their demise[1][2], and I am grateful for it.However, it does make me think. Grimes&#x27; software continues to have issues filed against it[3] by folks unaware that no one is getting notifications for them. On the other hand, Debian was popular enough to continue after Ian&#x27;s passing, and continues to gain momentum.I know it might be too soon for me to wonder about these questions to an audience. These were the giants on whose shoulders we continue to work today. I&#x27;m glad their code persists.1: https:&#x2F;&#x2F;github.com&#x2F;Raynes&#x2F;fs2: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ian_Murdock3: https:&#x2F;&#x2F;github.com&#x2F;Raynes&#x2F;fs&#x2F;pulls reply deltarholamda 15 hours agoparentThere is another thread on HN about vim development going forward, posted on the Google Group for vim development. Christian Brabandt mentions several issues that will need to be resolved, e.g. the hosting situation for the site.I feel that this would be an ideal thing for one of the various open source foundations to address. I.e., provide a single-source hosting environment for open source projects without needing to lean on for-profit corporations, which would include some form of hit-by-a-bus solution for those projects that lose a core (or sole) developer.Trying to piece this stuff together after the fact is always a chore. And no amount of \"well, they should have a plan in place\" will help. People don&#x27;t think they&#x27;re going to die tomorrow, so there&#x27;s always a reason to put it off for later, so they do.I don&#x27;t think anybody&#x27;s worried about Linux going poof if Linus unexpectantly returns to his home planet, but there are plenty of projects that might have some issues. Having all of the domains, DNS, hosting, etc. somewhere with a real employee that can be contacted to move control to new people if required is a good thing. reply nektro 11 hours agorootparenthttps:&#x2F;&#x2F;sourcehut.org&#x2F; fits this pretty well reply xvilka 5 hours agorootparentSourceHut is great but they still do not support[1] organization accounts. Team development, especially for relatively big projects like Vim is a pain in such environment. It&#x27;s also sad they stopped publishing regular \"What&#x27;s cooking on SourceHut\" blog posts.[1] https:&#x2F;&#x2F;todo.sr.ht&#x2F;~sircmpwn&#x2F;meta.sr.ht&#x2F;29 reply graphviz 11 hours agorootparentprevWhat? Project founders and software authors might not always be around? reply metadat 15 hours agoparentprevBram Moolenaar&#x27;s passing became public last Saturday:Bram Moolenaar has died - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37011324 (4272 points, 430 comments) reply dessimus 9 hours agoparentprev>On the other hand, Debian was popular enough to continue after Ian&#x27;s passing, and continues to gain momentum.I don&#x27;t believe these are parallel, as IIRC, Ian was not directly involved in Debian since the late 90&#x27;s, at least not in a day-to-day manner in the way Bram was, or say the way Linus is still managing Linux to this day. reply Dulat_Akan 13 hours agoprev [–] keep going bro good post replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Bram, the creator of Vim, is considered a role model in the tech community due to his principles of problem-solving and modesty.",
      "Neovim is a derivative of Vim that aims to experiment, improve documentation, enhance extensibility, and enable embedding capabilities.",
      "The article highlights the importance of pragmatism and keeping the goal in mind, emphasizing the need to compare actions with results."
    ],
    "commentSummary": [
      "Bram Moolenaar, the creator of Vim, has sadly passed away, leading to an outpouring of gratitude for his significant contributions to the programming community.",
      "Discussions are taking place regarding the legacy of Vim and alternative editors, as well as the split between Vim and NeoVim.",
      "Concerns have been raised about the future of these projects without their original creators, with discussions about the importance of having control over domains and hosting platforms like SourceHut."
    ],
    "points": 1086,
    "commentCount": 91,
    "retryCount": 0,
    "time": 1691686934
  },
  {
    "id": 37074452,
    "title": "The Future of the Vim Project",
    "originLink": "https://groups.google.com/g/vim_dev/c/dq9Wu5jqVTw",
    "originBody": "Groups Conversations All groups and messages Sign in vim_dev Conversations About Privacy • Terms     The future of the Vim project 32812 views Skip to first unread message  Christian Brabandt unread, Aug 9, 2023, 3:12:41 AM (yesterday)    to vim...@vim.org Hi, I started making a few changes to continue with Vim. This is the current status: - Access to the github organization is possible and Ken and me have been granted admin rights by Brams family, so we can continue with Github. (Thanks @Fokke!) - I invited a few more members to join the Vim organization: Yegappan Lakshmanan, Dominique Pellé, @mattn and @zeertzjq who have been contributed to Vim in the past. Congratulations and welcome guys! ;) - People from the github organization should also have access to huntr.dev (https://huntr.dev/repos/vim/vim/) where security problems are being reported. We'll need to take a look there. - I merged the first 2 commits. As mentioned elsewhere, for now I will try to merge only bug fixes, security related fixes, documentation updates and other clear improvements. For the main source of Vim I'll therefore like to have approval from the other project members before merging anything yet. Please expect some bumps here, we need a bit of time until we know how to properly handle all of this (and it may be subject to change, when we all agree of a better method). - After we have gone through the current backlog, I'd like to get a Vim 9.1 maintenance release ready, until then we should continue with incrementing the minor patch version. After the release, I am thinking about moving to a more modern approach, similar to how Neovim is doing it. But as discussed elsewhere, this may have some consequences for the various subprojects: vim-win32-installer, vim-appimage, macVim, so not sure what will be the best way. - I have access to the OSDN.net project page and am able to edit the vim.org homepage. However for various reasons, we may have to move the Vim homepage elsewhere. More on that further down. - Bram was owner of the all of the mailing lists. I don't know yet how he managed this and how to request access specifically for vim-announce and vim-mac (is this actually still used?) Does anybody have a contact to the googlegroups admins? - The mailing lists vim-dev and vim-use are currently managed by myself, Tony Mechelynk, John Beckett, Ben Schmidt and Ben Fritz (of whom I think the last two are no longer active at least for the Vim project, CC'ing them to see if they are still interested in managing this) - The Vim Domain is managed by @sec (CC'ed). Can you please confirm, you will be taking care of extending the domains (I think this is vim.org, vim8.org, vim9.org and possibly iccf-holland.org)? - I don't have access yet to the main Vim FTP Server. Currently checking with Brams family if they know the credentials. (CC'ed) - I am reaching out to all maintainers of the runtime files, to find out if they have sent anything directly to Bram, which may otherwise be lost. (to be done). Regarding the Vim Homepage, as you may all know, we have had problems with the stability of it for the past months, in particular the connection to the MySQL Server (I am also currently unable to access the vim project page directly, as osdn.net/projects/vim seems to be down for me, but I doubt that this page is actually being used by anybody). It is currently run by OSDN.net as offered by Shuji Sado (former CEO) since 2018. Unfortunately, OSDN.net is apparently now being owned by OSChina and we currently do not have any support by OSDN.net or OSChina teams. @Shuji, thank you for maintaining the Vim Website since 2018 and all the best for the future! I've reached out to OSChina regarding what their plans are, but haven't received any answers yet. Therefore I am also considering to move the Vim homepage to another provider. A good friend of mine, Marc Schöchlin offered to take care of the hosting. The Vim project is very thankful for the kind offer! Please note: This is just a consideration however, nothing has been decided yet. The hosting consists of the following: - PHP Files with access to a MySQL Database. I think it is currently using PHP 5, which seems to have reached end-of-life. - Vims Source in form of a Mercurial repository (which is mirrored from https://hg.256bit.org/vim). Is anybody actually using the mercurial repository (except for Tony ;)) ? - Sourceforge (where the vim homepage was hosted until 2018) recently announced to migrate to php7 in 2 weeks. But I don't know, if this is supported by the Vim homepage (at least https://vim.sourceforge.io/ does not seem to work with PHP 7 when testing using: https://sourceforge.net/p/forge/documentation/Project%20Web%20Services/#upgrading-from-php-5-net-subdomains) - expected traffic: 200-500k/moths page visits - Since I have access to the OSDN Shell Server, I can create a DB dump and move the content elsewhere, so that shouldn't be a problem. Once the hosting has been settled and we have proper contacts, I'll share the account credentials with the team. I can't do it right now, since this requires support from OSDN.net which we are currently missing. @Sec Please be aware that we need to move the Vim Domains if we decide to move the homepage. In the past, I have discussed with Bram to open-source the homepage, so that we will be able to take contributions and be able to keep it up-to-date and maybe have it more modern looking. However Bram did not want that, worrying to leak any sensitive information (or make it more easily discoverable any potential problems). That is certainly a valid point, so don't know how to handle it yet. That should be it for now, i hope I did not forget anything noteworthy. Let me know what you think. Best, Christian -- Was deprimierend ist: Du bist wie alle anderen. Was tröstlich ist: Alle anderen sind wie du. -- Johannes Groß Christ van Willegen unread, Aug 9, 2023, 3:30:38 AM (yesterday)    to vim...@googlegroups.com, vim...@vim.org Hi, On Wed, Aug 9, 2023 at 12:12 PM Christian Brabandtwrote: > - After we have gone through the current backlog, I'd like to get a Vim > 9.1 maintenance release ready, until then we should continue with > incrementing the minor patch version. After the release, I am thinking > about moving to a more modern approach, similar to how Neovim is doing > it. I think this is a sane decision. There are lots of PRs still open, some have been for a long time. I'm trying to lower your work load there by loking at what I think I can sanely decide is 'good', but that's mostly tests, code changes, text changes etc. I'm not well versed in syntax files... > - I am reaching out to all maintainers of the runtime files, to find out > if they have sent anything directly to Bram, which may otherwise be > lost. (to be done). In the future, doing this by PR seems a logical way forward. We _do_ need automatied test for syntax files, though, to prevent breakage and have a way to decide what syntax highlighting should 'look like' before it's included. I know Bram started this, but I'm not sure of the current status. I hope we will be able to sustain the community without Bram, but at them moment I think we can continue for quite some time 'in his spirit'. Christ van Willegen Yegappan Lakshmanan unread, Aug 9, 2023, 7:12:48 AM (yesterday)    to vim...@googlegroups.com Thanks Christian for putting this together. On Wed, Aug 9, 2023 at 3:12 AM Christian Brabandtwrote: > > Hi, > I started making a few changes to continue with Vim. This is the current > status: > > - Access to the github organization is possible and Ken and me have been > granted admin rights by Brams family, so we can continue with Github. > (Thanks @Fokke!) > > - I invited a few more members to join the Vim organization: Yegappan > Lakshmanan, Dominique Pellé, @mattn and @zeertzjq who have been > contributed to Vim in the past. Congratulations and welcome guys! ;) > Thanks. > > - People from the github organization should also have access to > huntr.dev (https://huntr.dev/repos/vim/vim/) where security problems > are being reported. We'll need to take a look there. > > - I merged the first 2 commits. As mentioned elsewhere, for now I will > try to merge only bug fixes, security related fixes, documentation > updates and other clear improvements. For the main source of Vim I'll > therefore like to have approval from the other project members before > merging anything yet. Please expect some bumps here, we need a bit of > time until we know how to properly handle all of this (and it may be > subject to change, when we all agree of a better method). > Should we look at the outstanding PRs and tag which ones are possible candidates for the 9.1 release? > > - After we have gone through the current backlog, I'd like to get a Vim > 9.1 maintenance release ready, until then we should continue with > incrementing the minor patch version. After the release, I am thinking > about moving to a more modern approach, similar to how Neovim is doing > it. But as discussed elsewhere, this may have some consequences for > Agreed. This is a more scalable approach. Regards, Yegappan  Christian Brabandt unread, Aug 9, 2023, 7:47:03 AM (yesterday)    to vim...@googlegroups.com On Mi, 09 Aug 2023, Yegappan Lakshmanan wrote: > Should we look at the outstanding PRs and tag which ones are possible > candidates for the 9.1 release? Good idea, yes please. Best, Christian -- Commitment, n.: [The difference between involvement and] Commitment can be illustrated by a breakfast of ham and eggs. The chicken was involved, the pig was committed. Yee Cheng Chin unread, Aug 9, 2023, 10:08:47 AM (yesterday)    to vim...@googlegroups.com Regarding the vim_mac mailing list, it's not very active, but I do see people posting there once in a while. It's usually a mix of issues / questions between MacVim and Vim (on macOS) as people usually conflate the two as the same thing. For MacVim, most discussions I do are usually on GitHub these days, but it seems that some people still prefer a mailing list format. If it's not too much trouble I think we could keep it.   -- -- You received this message from the \"vim_dev\" maillist. Do not top-post! Type your reply below the text you are replying to. For more information, visit http://www.vim.org/maillist.php --- You received this message because you are subscribed to the Google Groups \"vim_dev\" group. To unsubscribe from this group and stop receiving emails from it, send an email to vim_dev+u...@googlegroups.com. To view this discussion on the web visit https://groups.google.com/d/msgid/vim_dev/ZNOm4swFRyvdDINF%40256bit.org. John Beckett unread, Aug 9, 2023, 11:07:42 PM (yesterday)    to vim...@googlegroups.com Thanks for taking this on Christian. I can't help much but let me know if there is something I might be able to handle. I advise making things as simple as possible and dumping \"nice to have\" features. I think you've got the Mercurial repo running on automatic, but while I love Mercurial, it's just one more thing to think about and I would dump it. Similarly, sorry but FTP should go. Surely Sourceforge is no longer needed? Currently https://vim.sourceforge.io/ tells me \"The Vim website has moved, go to www.vim.org\" which is all that's needed. It seems to be trying to do a redirect which is not needed. The Vim Tips wiki is inactive and obsolete but I put the news about Bram on the main page. https://vim.fandom.com/wiki/Vim_Tips_Wiki Re github procedures, please be very conservative and skip good ideas and anything else that adds to complexity. Instead, focus on small and almost-guaranteed-not-to-break changes with at least two trusted people checking each before going live. I think you suggested that. Perhaps in a few months after things have settled down, more adventurous changes could be considered. John Beckett mattn unread, Aug 10, 2023, 5:14:55 AM (22 hours ago)    to vim_dev Christian, you should have the rights for vim project on OSDN. If you want to continue to use OSDN, and want to add members, please refer following URL. https://osdn.net/projects/vim/memberlist And click \"add member\". Thanks. 2023年8月10日木曜日 15:07:42 UTC+9 John Beckett:  ciu...@gmail.com unread, Aug 10, 2023, 8:28:44 AM (19 hours ago)    to vim_dev Hi Cristian -- I've helped with the Vim project in the past, let me know if I can help with hosting, release testing, Mac builds testing, Vim.org site (PHP, MySQL, etc. woes), or GitHub setup (e.g. consultation on permissions, project management, actions, etc.). Daily use of MacVim, and Vim native on macOS and Ubuntu Linux (various releases). My own personal and business websites run various things on top of PHP/MySQL -- upgraded all from PHP5 to PHP7.4 and now PHP8.1; MySQL also needs to be updated if you go from PHP5 to PHP8.1 - note that PHP7 is also deprecated. Let me know how I can help -- for starters, I can share the Dockerfiles that I use for hosting my stuff (e.g. Piwigo, Roundcube Mail, Bolt, etc.). I would suggest moving the hosting from metal to Docker for easier testing, troubleshooting, and management. Happy to oblige on anything the Vim crew needs. pr3d4t0r (aka Eugene Ciurana) Christian Brabandt unread, Aug 10, 2023, 8:33:12 AM (19 hours ago)    to vim...@googlegroups.com On Do, 10 Aug 2023, mattn wrote: > Christian, you should have the rights for vim project on OSDN. > If you want to continue to use OSDN, and want to add members, please refer > following URL. > > https://osdn.net/projects/vim/memberlist > > And click \"add member\". Yes, it is working now, previously, the webpage only returned HTTP 500. In the meantime OSChina replied and wanted to look at the connection problems. We'll see how it goes on then. Best, Christian -- Every time you manage to close the door on Reality, it comes in through the window. Christian Brabandt unread, Aug 10, 2023, 8:34:20 AM (18 hours ago)    to vim...@googlegroups.com  Thanks for the offer, I may come back to it. There is currently no pressure, and OSDN/OSChina hasn't announced when to migrate to PHP 7 (unlike Sourceforge, where we were previously hosted, but which is not used anymore). Best, Christian -- How many hardware guys does it take to change a light bulb? \"Well the diagnostics say it's fine buddy, so it's a software problem.\" mattn unread, Aug 10, 2023, 8:58:52 AM (18 hours ago)    to vim_dev I suggest moving to another hosting server, the OSDN console has lots of errors. Perhaps Christian should be able to export the DB. 2023年8月11日金曜日 0:34:20 UTC+9 Christian Brabandt:   Reply all  Reply to author  Forward",
    "commentLink": "https://news.ycombinator.com/item?id=37074452",
    "commentBody": "The Future of the Vim ProjectHacker NewspastloginThe Future of the Vim Project (groups.google.com) 653 points by mrzool 23 hours ago| hidepastfavorite362 comments OskarS 22 hours agoMy brother passed away very suddenly a few years ago, and I was put in charge of wrapping up and archiving his \"digital\" life. We were very lucky that we had access to a recovery email for his main gmail account (as well as a couple of passwords that his partner knew) and was able to access and archive virtually all data we could think of (services like Google Takeout were invaluable). I realized that if this had happened to me, it would have been virtually impossible to do, as all my passwords and credentials are in my password manager, and the password to that was only in my head.It&#x27;s a good thing to plan for this eventuality, to make it easy for your family and friends to wind up your \"digital life\" after you&#x27;ve passed. 1Password has a very good solution for this, with a \"recovery document\" you can print out and write down your password on, which contains instructions anyone else would need to access your 1Password account. I gave a copy of this document printed out to a small number of people I trust implicitly.You never know when something sudden can happen to you. For the sake of those you leave behind, it&#x27;s a nice gift to plan for this eventuality, even if it seems far off at the moment. reply DicIfTEx 20 hours agoparentUseful checklist: https:&#x2F;&#x2F;getyourshittogether.org&#x2F;Others in this thread have talked about safety deposit boxes and buried crates. I&#x27;d add that you can just give some trusted party a normal encrypted USB flash drive, and eliminate the risk of getting absolutely rinsed out in the event of a house burglary by splitting the password amongst an arbitrary number of your other contacts using the Shamir&#x27;s Secret Sharing algorithm. reply sshine 19 hours agorootparentI call this a Horcrux.Also, unless your arbitrary number of friends are cryptographers, it&#x27;s a sure way for them to collectively lose your shit. reply Someone 14 hours agorootparentCreate transparencies with the text If you put at least 3 of these on top of each other, my Google recovery code will appear here: 2 £ 3 > ]7 7#A E(each with different characters shown, of course. Ask a mathematician to make sure any 3 will show the full code, and any 2 won’t show enough to recover it)Put them in envelopes, write “open in case John Doe dies” on them, and distribute them among friends.If you distribute enough of them, I think there’s a reasonable chance they’ll recover your data.As an improvement, distribute them not to your friends, but to their kids (the probability is higher they’ll be sane of mind when you die), and tell your attorney who has one.I think that’s overkill, though. I’ve done it simpler: I wrote the full recovery codes down a few times and put them in a few places in my house.I think that’s fine if I assume burglars won’t take them or won’t know what to do with them and I won’t die in a disaster that also destroys my house. reply DicIfTEx 13 hours agorootparent> I think that’s fine if I assume … I won’t die in a disaster that also destroys my house.You might not die, but you might still end up in a pretty bad position: https:&#x2F;&#x2F;shkspr.mobi&#x2F;blog&#x2F;2022&#x2F;06&#x2F;ive-locked-myself-out-of-my...Incredibly unlikely, of course, but you&#x27;ll certainly feel like a dick if it does happen. reply Macha 10 hours agorootparentAs someone who _has_ had their house burn down and had to recover their digital life from backups (successfully), this is why I&#x27;ve not made the step to yubikeys. reply atoav 19 hours agorootparentprevI&#x27;d say a regular will, sealed at a notary is just okay.If you are really paranoid, why not write a service that works like a dead mans switch and when you don&#x27;t trigger it for n days it sends all the keys to the kingdom to those who should receive them. reply pr0zac 18 hours agorootparentGoogle actually provides this service: https:&#x2F;&#x2F;myaccount.google.com&#x2F;inactive?pli=1At this point I&#x27;ve moved almost everything off Google and basically now only use my Gmail account for logins on websites I don&#x27;t want to give my real email address to and to keep Inactive Account Manager setup to send the necessary info to get into my 1Password account to my brother if I die.I have a will setup with all my financial details and have set beneficiaries everywhere but this feels like a good backup in case I forget to update it with something or my family has trouble gaining access using other means. reply hobo_mark 15 hours agorootparent> The setting you are looking for is not available for your account.Google Apps strikes again... reply ed 17 hours agorootparentprevThat’s a helpful link! Do you know if google allows access to Gmail once inactivity is triggered? Ideally my contacts could use it to recover access to my password manager. reply pr0zac 15 hours agorootparentYou have the option to allow that yes. When you add people to be contacted you&#x27;re able to granularly decide what parts of your google account they get access to and can optionally add a personal message as well. reply jagged-chisel 17 hours agorootparentprevStep 1: Kidnap $targetStep 2: keep for N+1 daysStep 3: …*Step 4: profit!* where “…” just means “wait” reply civilitty 17 hours agorootparentKidnapping $target works for just about every contingency once you include $hammer. reply hospadar 17 hours agorootparentprevWhy bother waiting!?https:&#x2F;&#x2F;xkcd.com&#x2F;538&#x2F; reply __s 17 hours agorootparentprevOnly a couple need to be into cryptography. You can setup so than N-of-M keys work, so that you&#x27;re resilient to M-N losing their keyYou can have 2-of-20 which encrypts an encrypted blob whose key you share in your will reply flobosg 19 hours agorootparentprevAdditional links:* What to Do Before You Die: A Tech Checklist – https:&#x2F;&#x2F;archive.is&#x2F;6vjqQ* Cheat Sheet For If I&#x27;m Gone – https:&#x2F;&#x2F;archive.is&#x2F;lnWX6 –https:&#x2F;&#x2F;github.com&#x2F;christophercalm&#x2F;if-im-gone&#x2F;blob&#x2F;main&#x2F;exam... (HN discussion: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31748553) reply DicIfTEx 19 hours agorootparentAlso https:&#x2F;&#x2F;g3rv4.com&#x2F;2022&#x2F;04&#x2F;using-shamir-secret-sharing which includes a link to an easy-to-use SSS tool that can output shards as QR codes. reply xp84 8 hours agorootparentI am picturing a room full of non-programmers staring at all these documents, codes and Docker commands and saying “Well, Greg was obviously crazy. Instead of leaving any of his passwords, he just left pages of gibberish. I guess we’ll never be able to access anything.” reply stolsvik 1 hour agorootparentMe too, but then I realise that one should include at least a few tech savvy friends - or references to good coworkers or similar which these friends could contact. reply hardburn 19 hours agorootparentprevI wouldn&#x27;t trust USB flash drives with anything long term. Best archival method would be to print something out (perhaps an encrypted message in a QR code), have it put away somewhere secure, and use that for a key to unlocking everything else. reply augusto-moura 16 hours agorootparentYes, this, USB flash drives live at most 5 years or something. Things get even worse for SSD.If we are talking just about credentials, you can just print the password and access instructions to a password manager and give it to the people you trust. This is one place were having a cloud password manager might be helpful, otherwise you would need to also provide the access to a device containing the offline manager (or a updated copy of it)You probably don&#x27;t need to keep a whole flash drive for credentials. Unless you also want to keep some other files secure without being on other devices reply vimbtw 18 hours agorootparentprevBitcoiners have been thinking about this storage problem for a decade now. Secure electronic devices in faraday cages and tamper and water proof bags or engraved steel plates (possibly cut up and distributed) seem to be the way to go for storing small bits of extremely valuable information.Or of course you can use multiple key storage techniques and have a 2 out of 3 or more type setup. It all depends on how valuable the information is. reply semi-extrinsic 14 hours agorootparentEngraved steel plates cut up is actually extremely easy and virtually indestructible.You can buy a piece of 100x50x3 mm 316 stainless steel (thats 4\"x2\"x1&#x2F;8\" in freedom units) for around $5.Engraving it is a simple matter of using a $10 automatic center punch and write the password out in dot punched letters.If necessary, cut plate in N pieces with a hack saw, distribute among N people.I always mark bicycles this way, dot punch my last name underneath the bottom bracket shell. reply calfuris 15 hours agorootparentprevSecurity against unauthorized use and data lifespan are separate concerns. They&#x27;re not fully orthogonal—security tends to make things more brittle—but you can apply whatever form of security you like and then store the secured data in any way you like. Hardburn seems to have been talking purely about the useful life of the archived data. The charge in flash storage leaks, so the data is eventually lost if not refreshed. A flash drive is reliable for a year, but not a decade. If you want long term storage you&#x27;re going to want something else. Paper would be fine for most uses. An ordinary printout subjected to ordinary handling is good for a few decades with reasonable storage conditions. reply hoherd 22 hours agoparentprevI had a similar situation where I lost my brother very unexpectedly. I ended up having to run a password cracker on his windows account because nobody had a recovery email. Thankfully his windows credentials were not very strong and his gaming GPU was able to crack the password in a few days using a Linux livecd, and I was able to expand from there into his 1Password account. Like you, I realized that having a trusted second party with the keys to my digital kingdom would be a wise choice in case of a disaster. reply bombcar 22 hours agoparentprevThis is one of the reasons that I’ve begun to pare down the number of online financial accounts I have, even though churning for bonuses is fun.Every single one will have to be dealt with eventually by someone, so if I can reduce the number of banks I deal with it’s worth it, even at some small cost of not being “perfectly optimal”. reply dumpster_fire 22 hours agorootparentYeah I used to have a massive spreadsheet tracking the entirety of household finances. I was worried that no one would know where the money was if I suddenly died, so I started a monthly finance 1:1 with my spouse. Even wrote an \"upon death or incapacitation\" playbook for her.The second session was just her saying WTF I can&#x27;t keep track of the location, ownership and tax benefits of 40 accounts!I&#x27;ve since closed 3&#x2F;4 of all accounts because of that. reply bombcar 21 hours agorootparentThe killer for me was working out how much time I was spending playing Excel Warrior and how much it was “making” me and realizing that I was working in my free time for Pennie’s. reply em-bee 19 hours agorootparentthat is what keeps me from getting into all this detailed finance management or worse trading, likewise any other side business that could earn some money but just isn&#x27;t the kind of work i want to do.making a budget is useful, as is tracking your expenses, but that&#x27;s about it for me. reply darkwater 21 hours agorootparentprevGood and honest realization. It seems easy once you do it but it can be really hard to actually get to that point. reply OJFord 22 hours agorootparentprevBanks are the last thing I&#x27;d worry about (I wouldn&#x27;t) - they&#x27;re highly regulated, audited, and have been dealing with this forever, since before &#x27;passwords&#x27;. reply bombcar 22 hours agorootparentI have no worry about the funds being lost. It’s just imagining my heirs having to contact and close fifty+ accounts. reply kemotep 21 hours agorootparentAdditionally, if you don’t list a beneficiary on any of these accounts, they have to go through probate court to gain access. The fewer hurdles and headaches that your loved ones have to go through the better. Having dealt with this early this past year with a family member, having the list of “chores” to do upon passing helps and reduces the headaches. Especially if those funds are necessary to pay for the sudden bills that a funeral and so on can bring up. reply rokizero 20 hours agorootparentprevSerious question: How common is having dozens of accounts? I live in NL and have 3 at the same bank. Why would anyone have 50+ accounts? reply jonhohle 18 hours agorootparentMy wife and I have four kids and for each of them is a personal savings account, a savings account we keep for them to accumulate over time to avoid a taxable lump sump “gift”, and a tax advantaged educational savings account. For ourselves we have brokerage, savings, checking, and each have pretax and post-tax advantaged retirement accounts and health savings accounts. Many tech employees would also have 401k and possibly equity compensation accounts. And then any credit or debt (mortgage) accounts on top of that.Our family is big, and the number of accounts scales with the number of people, but that’s about 25 without getting into anything moderately interesting. reply detourdog 20 hours agorootparentprevThe US based financial industry is a make work project for bankers as far as I can tell. The US creates all sorts of classification of money causing the need for at least 3 retirement account types, a college savings account per Child per contributor.I&#x27;m only scratching the surface of the number and types of accounts an American can have. It is also useful to have different banks for different services. reply spiralpolitik 16 hours agorootparentprevPretty common in the US verses the EU&#x2F;UK. The basic middle income set would be:A checking account A savings account (might be at the same institution as the checking, might not)From your employment you may have.- A 401k Account - A Health Savings Account (HSA) - A Health Flexible spending accountThese will be at whatever institution your employer uses. As these change every time you change jobs you might have multiples of these in play unless you are diligent in rolling over and closing old accounts.You also might have:- Individual Retirement Account (possibly two, one Roth, non-Roth) - College Savings Account (if you have kids and want save for collage in a tax friendly way) - Money Market&#x2F;Broker account for stocks etc.If you live in a community property state then you probably have a second set of some these so you that you don&#x27;t mix individual assets with community assets.Market consolidation has made it easier to go with an single provider for a lot of the above, but it&#x27;s still busy work keeping on top of everything. reply bombcar 16 hours agorootparentprevYou sign up for them and never close them. I probably have nearly 50 credit card accounts alone and since they&#x27;re free of monthly&#x2F;yearly fees, I really don&#x27;t have much motivation to close them.On top of that I have two retirement accounts, four bank accounts (not counting various accounts AT those banks), and more. They collect if you don&#x27;t weed them out. reply saikia81 20 hours agorootparentprevThe only reasons I can imagine are fund transfer times, and some isolation security for big amounts. As a fellow dutchie, I just use one bank for personal, and one for my business. reply EtienneK 19 hours agorootparentprevNot so uncommon in NL. As a Dutchy you should have heard of Bunq which allows you to open unlimited accounts. The idea is to use each as savings pots for specific things such as a holiday, car, groceries, etc.Other banks allow the same thing via virtual accounts. reply throw0101b 20 hours agorootparentprevSomeone may have chequing+saving at one bank, a stock brokerage (or retirement savings) account at another institution, and a may have credit card(s) from completely different one(s).So that could potentially be (at least) three different financially-related accounts. reply Tyr42 19 hours agorootparentprevSign up bonuses. Some give an extra few percent on interest when you sign up, so you move all your money in, collect, then close the account out and go to another bank and repeat. reply macinjosh 20 hours agorootparentprevThat dude is probably quite wealthy. reply jonhohle 18 hours agorootparentIt’s not necessarily related to wealth (except that you need some savings), but when ING Direct was a thing my wife and I had an account for every savings goal and used it as our “envelope” system. There was no cost per account and you could open one in seconds, so we had a bunch of low value (dozens of dollars! Dozens!) accounts for saving for our next phone or vacation. That could have been done with a spreadsheet, but it was less work to just make separate accounts. reply xmdx 19 hours agorootparentprevIn the UK you can use something like this: https:&#x2F;&#x2F;www.deathnotificationservice.co.uk&#x2F;There are a few of these, one official one for notifying the government and a few private ones.I imagine this services exists in other countries, if not then might an opportunity to create it. reply chipsrafferty 19 hours agorootparentprevWhy would they have to? reply r3trohack3r 18 hours agoparentprevI’m sorry to hear about your brother.> all my passwords and credentials are in my password manager, and the password to that was only in my head.It’s not just death I worry about. Anything that causes me to lose my memory of the password, from disease to head injury, leaves those trying to help me locked out of everything.A password manager is an incredibly helpful tool to leave behind, it’s a compiled list of all vendors you have registered an online account with.But, IIUC, without legal authority to access those accounts on my behalf it might not be sufficient. I’m planning to talk to an attorney that specializes in taking care of the legal side too. IIUC there are accounts you need legal authority to access even if you have the password. For example, if I give my friend the password to my 401k with the purpose of managing my estate, them using that password can put them in a legally gray area.Also planning to work out a rough order of importance and context for a subset of accounts can help. Like writing down which financial vendor is managing the life insurance policy and whether that’s tied to my employer or not (if I lose my job leading up to my death, I.e. during a long battle with an injury, will I lose my life insurance before it pays out?)A “red binder” project is on my families short list - the “I’m dead or incapacitated, here’s what you do” playbook. The above is how I’m thinking about things. I would love to hear more thoughts&#x2F;perspectives reply WorldMaker 17 hours agorootparent> But, IIUC, without legal authority to access those accounts on my behalf it might not be sufficient.Relatedly, most digital accounts explicitly don&#x27;t survive the user in their Terms of Service agreements. I think there are a lot of legal battles to come over digital inheritance rights for accounts like Movies Anywhere and Steam and App Store purchases. reply petee 21 hours agoparentprevAnother consideration, when my dad passed we had his passwords but not his phone or tablet pins&#x2F;patterns.Both devices are encrypted, and the samsung I believe is set to wipe after a number of failed attempts. While there probably isn&#x27;t anything on them, it&#x27;s always been a pain to not know. reply ThrowAway1922A 21 hours agorootparent> While there probably isn&#x27;t anything on them, it&#x27;s always been a pain to not know.I know I wouldn&#x27;t care because I&#x27;d be dead, but I really do not want my family getting on to my personal devices after I&#x27;m dead. Those are things that I will never give them the passwords for, not everything is their business. reply petee 20 hours agorootparentI think that&#x27;s fine; albeit a potentially awkward conversation, I personally would rather have known \"hey, here&#x27;s what you can get into, here is what is private\" but we never talked about it at all.Especially important to communicate that in your case, on the off chance they want to hire a data recovery firm in some hope of saving wedding photos or something reply ThrowAway1922A 17 hours agorootparent> Especially important to communicate that in your case, on the off chance they want to hire a data recovery firm in some hope of saving wedding photos or somethingI share any photos with them they might want, but I hopeful that Apple&#x27;s security setup prevents any practical data recovery. I know my family too well, if I explicitly said \"this is private\" they&#x27;d be trying to get in the moment I was cold.There&#x27;s nothing bad on my devices, but there&#x27;s lots they don&#x27;t need to know about me. reply BeFlatXIII 14 hours agorootparentImagine once bronies age enough that they kick the bucket en masse and their families spend tens of thousands on data recovery only to discover \"damn, that&#x27;s a lot of horse porn.\" reply Hamuko 19 hours agorootparentprevSame. I&#x27;d also like them to throw all of my objects with locks on them into the nearest lake. reply cdelsolar 20 hours agorootparentprevyeah same here. Do I have to give out my passwords, or can I just make a doc with important things? reply petee 20 hours agorootparentDoesn&#x27;t matter as much what you do or how, but more importantly that you&#x27;ve communicated it to the people who will have to deal with your stuff if you were to kick the bucket tomorrow.My parents simply made a list of passwords on a piece of paper, buried with the other important papers. reply shever73 18 hours agorootparentprevI have a doc, which is password-protected and shared with my wife. It contains details of bank accounts, how to access shares, who we&#x27;re insured with and passwords &#x2F; passcodes for things she might need access to. reply em-bee 19 hours agorootparentprevit depends on where the important stuff is.i have all the family photos on encrypted devices, and the most efficient way to share them is to share the passwords for those devices. my phone they don&#x27;t need because the important stuff from the phone is backed up anyways, so they just need that backup.so i guess the easiest way is to keep separate backups of stuff you want to keep private and stuff you want to share. reply danielvaughn 20 hours agoparentprevMy brother-in-law&#x27;s mother passed away last year and he was in a similar situation. Even beyond the digital realm, there are so many details in a person&#x27;s life that have to be attended to. Insurance, mortgage, vehicle ownership, etc etc. It&#x27;s really an overwhelming process and it took a toll on him.I&#x27;ve been thinking about building a platform to help prepare for and guide families who are faced with this kind of situation. reply detourdog 20 hours agorootparentI had to go through this for my Mom an ex-pat in Israel. The account I was using was a joint-schwab account we had shared for 2 decades.I mentioned to Schwab that she had passed and they froze the account until I could prove the estate was less than $14 Million. Needless to say this was a disaster as I was in a foreign country writing checks left and right.The issue was that it was foreign addressed account of a US domiciled bank. The IRS places the liability of the taxes on the bank if the estate is over $12 Million. Schwab would recognise a letter from the IRS stating that or any US probate court. My Mom&#x27;s estate with no US assets has no US based probate court access. The IRS rule was enacted after our joint-account was opened and blew up our estate plan.Needless to say in 30 years I only had one problem with Schwab (which I had praised as the best bank ever until that moment). I have been unwinding all of my families Schwab accounts. reply bityard 20 hours agorootparentprevI think the hard part is more that it&#x27;s difficult to focus on the business decisions against the background of a loved one&#x27;s death, more than the process itself is difficult.You pretty much need an estate lawyer just to navigate all of the legal stuff associated with a person&#x27;s passing, and I went through this a few years ago with my own father.But the majority of attorneys who handle estates can also handle all of the financial and personal details as well. The only times this really NEEDS to get complicated is when the estate has a negative net worth (which means a potential lack of funds to close the estate), contains businesses that need to be sold or split up, or when survivors fight each other for their percentage of the inheritance. reply not_the_fda 22 hours agoparentprevI&#x27;ve recently been planning for my death, no urgent need, but you never know.I&#x27;ve put a backup of my keepass passwords on a USB as well as a printout of the passwords and the master password in a firebox. I also keep a list of assets and financial accounts in there along with birth certificates and passports. My spouse and I both have a key.I would have used a safe deposit box but those are disappearing. reply justanideaortwo 10 hours agorootparentPlease remember that USB storage needs to be recharged every so often, or the data will be lost. reply danielnoiz 20 hours agoparentprevI&#x27;ve had this[0] bookmarked for a while. A good list of things to keep note of somewhere safe. Probably the old \"3-2-1\" backup method at minimum.[0] https:&#x2F;&#x2F;www.bogleheads.org&#x2F;forum&#x2F;viewtopic.php?t=119346 reply pmckenna 22 hours agoparentprevWeirdly this is something I’ve been thinking about a lot recently. I had no idea 1Password provided a recovery kit until you mentioned it. Just had a look and looks good.I’ll definitely go through the steps, but I’m wondering what the best way to store it is. Feels weird keeping a document lying around giving access to all your passwords, bank cards, finances etc.I’ll have a think about storage. reply mathgorges 21 hours agorootparentBanks’ safe deposit boxes are a good choice if there are any near you, though they’re often expensive.If you have a lawyer&#x2F;attorney that you trust they’re often in the habit of securely storing physical documents, but again, expensive.Personally, I (a cheap-ass) keep a copy in a wooden box buried underground in a secure location along with a few other things.However, the security of physical things remains a difficult problem that is most easily solved with money! reply lm28469 22 hours agorootparentprev> I had no idea 1Password provided a recovery kit until you mentioned it.There are people out there with 1password and no recovery kit ? Some people like to live dangerously reply JohnMakin 20 hours agoparentprevI have a deadman switch where if I don’t log into a service for 90 days it sends all my passwords and information to my closest living relative. reply macinjosh 20 hours agorootparentIf you&#x27;d be willing I&#x27;d be interested to know more about how you did this. reply JohnMakin 19 hours agorootparentMy system is a little custom and complicated and tailored for me specifically, but basically my device periodically sends a ping to a service I built in cloud. I also have something like the google inactive account manager set up which is probably easiest for most people who use gmail:https:&#x2F;&#x2F;support.google.com&#x2F;accounts&#x2F;answer&#x2F;3036546?hl=en reply k1t 19 hours agorootparentprevEasy option: https:&#x2F;&#x2F;www.deadmansswitch.net&#x2F; reply wild_egg 18 hours agorootparentSeems there&#x27;s also a similar .com one reply akvadrako 13 hours agorootparentprevBitwarden has a service for this, fully E2EE. reply em-bee 19 hours agorootparentprevi am interested too. reply kqr 21 hours agoparentprevI&#x27;m lucky enough to never have been put in this situation, so please excuse my ignorance: why does someone need to be able to sign in to my accounts when I&#x27;m no longer? reply OskarS 20 hours agorootparentLots of other people have mentioned very good practical reasons (and you can read the linked post for others, like people being unsure of what the password to the Vim FTP is), but there are lots of good sentimental ones as well.When something like this happens, you lose your mind slightly, and you become obsessed with preserving whatever is possible to preserve of the person. I went so far as to record his voice-mail message, just because I felt i needed to.Of all the internet stuff, the thing that was most important to us was photos: he was a photographer, and he used a photo-uploading service (I think it was Google Photos, but I&#x27;m unsure, it&#x27;s been a couple of years) and I was able to get an archive with all his thousands of photographs.Eventually, I put everything I could possibly find (computers, internet services, whatever) into one big zip file, and put it on my local NAS (which is backed up to the cloud). I don&#x27;t think I&#x27;ll ever have the heart to go through it again, but my brother had young kids who never really got to know their dad. I figure one day they might want to look at it (even if it&#x27;s 30 years from now) so it makes me feel good to know that it&#x27;s been preserved. reply chasd00 21 hours agorootparentprevWhen my father in law passed away his wife asked me to get some photos on a thumb drive for her. I knew his password from watching him login, I also went ahead and deleted his browser history. reply ibejoeb 21 hours agorootparentI&#x27;d pay for this service reply sonofhans 18 hours agorootparentprevThe real hero is always in the comments. That was very humane of you. reply ncann 21 hours agorootparentprevIf you are perfectly prepared for your passing and made your will, arranged all the financial stuffs, told your friends and family everything needed etc. then you don&#x27;t need to give access to your accounts. But many people don&#x27;t have that luxury. When death is unexpected, things get messy, e.g you may want to continue paying the mortgage on time, or shutdown social media accounts, or make announcement of their passing using those accounts, or contact their lawyer, or cancel subscribed services, and so on. reply lowercased 21 hours agorootparentprevBeing able to unsubscribe to a service, as someone else mentioned.In social media world, posting some final \"xyz has passed on. this account will be closing\" or similar &#x27;wrap up&#x27; activity is often useful. reply Nzen 20 hours agorootparentprevWhile most people have focused on post-mortem account recovery, that&#x27;s not the only occasion. The small company I&#x27;ve worked for has had the last three administrators leave the company in arrears for a time with quick departures and little handoff of procedures. It is all the more frustrating because we overtly use a shared password managment tool for accessing client servers. reply bluGill 21 hours agorootparentprevWhen my dad died my mom needed access to all the accounts to do things like pay the electric bill. That is all done online so without him logging into the online bill pay she would have no way to know what was owed. I suppose after a few months of not paying they would send a paper bill, but then there are late fees and the like. reply maxfurman 21 hours agorootparentprevAnything with recurring billing needs to be cancelled, for a start. reply praseodym 21 hours agorootparentCall the deceased person&#x27;s bank to cancel all credit cards in their name and that&#x27;s taken care of. reply evanriley 21 hours agorootparentDoesn&#x27;t work for all services. Some services will, if unable to charge the card,send whatever amount to collections, and now they have to deal with that. reply bee_rider 21 hours agorootparentDebt collectors are persistent, but hey at least if they follow me to Hell maybe they can take a tour, check out apartments for their stay. reply devnullbrain 20 hours agorootparentOr pester your widow(er) reply humanistbot 19 hours agorootparentprevYour reply shows that you&#x27;ve never had to do this yourself. It&#x27;s a lot more complicated than that. reply zo1 20 hours agorootparentprevYou&#x27;d think it&#x27;s that easy. Some places literally are unable to do anything unless you&#x27;re some sort of \"signatory\" or the actual account holder. No amount of if, buts, maybes, and certified&#x2F;stamped copies of death certificates will convince them.I couldn&#x27;t even cancel the health insurance company&#x27;s recurring payments after a death. And they had the audacity to send a \"how was your hospital stay\" questionnaire to the account holder&#x27;s email after they were \"discharged\" by the hospital. reply yjftsjthsd-h 19 hours agorootparent> And they had the audacity to send a \"how was your hospital stay\" questionnaire to the account holder&#x27;s email after they were \"discharged\" by the hospital.That feels like an email that deserves an honest response:) replyRalfWausE 19 hours agoparentprevIts not just about passing away, even a sudden incapacitation from which you do recover may pose a bit of a challenge for the relatives:Some time ago my dad had some severe heart problems which lead to him being hospitalised for multiple months and a lengthy recovery where he was in full \"vegetable\" mode in the beginning. As he is somewhat of a \"patriarch\" personality the whole family finances, insurances etc. where all on his personal system.It really was \"fun\" to sort everything out for us and even more \"fun\" for himself making any sense of his whole accounting sheme after suffering some memory loss during the whole ordeal.So... having some \"letter of last resort\" deposed somewhere may even benefit yourself... reply em-bee 19 hours agorootparentthat&#x27;s what would scare me the most. to forget my passwords due to some accident. if i pass away it wouldn&#x27;t matter to me, but the thought of recovering from an illness but then not being able to access things that i had before is horrifying. reply swader999 21 hours agoparentprevThe finger doesn&#x27;t need to be alive to unlock the phone. But yeah, don&#x27;t put your loved ones through that. reply josephd79 21 hours agoparentprevNow that I think about it, I should probably do the same. I use Bitwarden and they allow you to add an &#x27;emergency contact&#x27; that is granted access after X amount of days. reply benreesman 15 hours agoparentprevMy condolences about the loss of your brother. My kid brother passed away a few years ago as well, and his digital footprint is one of the most vivid portraits of his last years, and to me a treasure beyond accounting.I strongly second the imperative to preserve as much as possible in the event that any of us suffer a mischief. reply phpisthebest 18 hours agoparentprevI keep a hard card in my safe next to my property titles, and other important paper work that has my bitwarden master password on it. From there who ever processes my estate should have no problems accessing everything reply xmdx 22 hours agoparentprevHad the same experience with my dad passing recently, we had access to everything because we know the passwords he uses and he was ok with sharing those with us.Will definitely look into the 1password emergency kit, thanks for mentioning it. 2fa is the other big challenge after that. reply patrick451 13 hours agoparentprevI&#x27;m genuinely curious why this needs to be done. Maybe I&#x27;m weird, but I don&#x27;t think I have anything valuable online that my family would want. Of course, there are all my financial accounts, but I would think that just a password wouldn&#x27;t do them much good with those, at least to legally drain them. I would think (and could be wrong) they would need to go through legal channels for that. What else is there? I can&#x27;t imagine they want download my email which is mostly just business transactions anyway. reply joe5150 13 hours agorootparentI can think of a handful of things that it might be nice or convenient for someone to be able to access (subscription services and whatnot), but I agree that it seems legally unwise to encourage anyone to try to log into my financial accounts and move money around after I die. Naming a beneficiary is a sounder strategy. reply hoseja 22 hours agoparentprevI&#x27;m with the other guy. Your unique pattern of electronic activity has ceased irrecoverably. No need for your patterns of electronic activity in computers to be any different. reply tomjen3 16 hours agoparentprevI am sorry about your brother.When we talk about these things it is always assumed that we want our family to have access to our digital lives after we die. I have lots of pictures from shared memories that I want my family to have - and they already do.Other things I want to die with me - things that were not shared with my family before I died shouldn&#x27;t be shared with them after I die.Why is it that we generally assume that we should get access to other peoples private stuff because they are dead?Again, not trying to make this an attack on you.And of course I am excepting getting access to bank accounts and insurance. reply itsthecourier 21 hours agoparentprevLastPass let you put two emails as your family members and you setup a 30 day limit to show you are alive. If your loved ones require access to your passwords and you are not there for 30 days, Lastpass release your passwords to them reply woleium 21 hours agorootparentBut we don&#x27;t use LastPass after they have demonstrated over and over that they are not to be trusted with our secrets, right? reply tome 20 hours agorootparentAnd it means they must be keeping the recovery passwords in plaintext, so it just gets worse... reply 0cf8612b2e1e 19 hours agorootparentThe email does not have to be the password. Something like, “password is taped to the back of my diploma” reply digging 19 hours agorootparentprevnot necessarily, it may send them an email inviting them to create an account with control of your account. reply ubermonkey 20 hours agoparentprevLet me add one other bit.I had a similar experience about 18 months ago when my friend A. suddenly died. He was fit and healthy, but for some reason had a seizure on a bike ride, crashed, and that was the end of his story.Unfortunately for his widow, she and he were not on a family account with Apple, and so it took a LOT of rigmarole to even access, say, the photos on his phone.Apple now has a \"Legacy Contact\" feature you can enable; this is a VERY VERY GOOD IDEA FOR MOST PEOPLE. I assume Google has something similar if you&#x27;re on Android.The tl;dr is really \"you just gotta have a plan.\" When you go, next week or decades from now, it&#x27;ll be hard for those you leave behind. Do whatever you can to make it easier for them. reply majewsky 20 hours agoprevCan someone copy-paste the text, please? I cannot read it because Google hates Firefox users apparently.EDIT: The Internet Archive has a functional copy. https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20230810094255&#x2F;groups.google.com... reply sigzero 20 hours agoparentNo issues with FF for me. Probably the HN effect maybe? reply nabakin 19 hours agorootparentInteresting that HN was able to DDOS a Google service reply cobbaut 18 hours agorootparentIt is called the Slashdot effect :) reply jefftk 19 hours agoparentprevNot a Firefox-specific issue; I don&#x27;t see it loading in Chrome either. reply thejammahimself 19 hours agoparentprevI&#x27;ve tried loading on Chromium but that doesn&#x27;t work either it seems. reply hoten 20 hours agoparentprevThe page isn&#x27;t loading on my mobile Chrome either. reply javier_e06 20 hours agoprevI&#x27;m saddened by the news.Required reading: https:&#x2F;&#x2F;evrone.com&#x2F;blog&#x2F;bram-moolenaar-interview\"Software development is much more of a craft. A craftsman uses whatever tools he thinks will get the best result, no matter if they are what everybody else is using or something different. And a good craftsman makes his own tools when needed\" -B. Moolenar.As someone who had made vim is part of the development dna: Thank you. reply jauntywundrkind 17 hours agoparentEmbodies the most amazing part about vim well. From text-objects to macros to registers, vim is a dynamic programming environment where we are capturing state and invoking little scripts.Vim is spellcasting in the fly. Not puzzling out complex rotes to perform dutifully, but building a potent ether around us & then applying a little twist just so to alter the universe around us. reply fl0ki 13 hours agorootparentThis is not a dig, just a concerned citizen: I would encourage you to check whether your microdose schedule has crept up from the standard 10-25µg into what looks like 50-75µg. It&#x27;s one of those things that other people suspect long before you do. In any case, have fun, practice safe sorcery. reply junon 22 hours agoprevBest of luck, this transition period is going to be chaotic for a while I&#x27;m sure. Glad to see the project in the hands of someone who seems to be on top of things. I also appreciate the carefulness of changes to the codebase after assuming leadership. reply fortunateregard 22 hours agoprevWhat is it that prevents the rest of the vim community from adopting neovim? From what I can observe, a great deal already have. But for the folks holding out, what is it that outweighs all that neovim has to offer? reply enriquto 22 hours agoparent> what is it that outweighs all that neovim has to offer?You should ask, and answer, the question in reverse. What does neovim offer me, as a regular vim user? I don&#x27;t see anything particularly interesting for my usage, so I don&#x27;t have any reason to change. Also, some features are missing (like gvim-gtk, that I enjoy using to edit LaTeX occasionally).Furthermore, as of today, plain vim has an aura of venerability due to Bram&#x27;s legacy that neovim cannot match. So I&#x27;ll stick with vim.It would make sense to port back some of the most popular neovim features into vim. It is a good thing to have innovative forks that experiment aggressively with new features. reply gorjusborg 21 hours agorootparent> You should ask, and answer, the question in reverse. What does neovim offer me, as a regular vim user?Well, one of the goals of neovim was to make it easier for new people to contribute.So, there&#x27;s an obvious feature you might be overlooking: the project surviving past its creators passing.Beyond that, async, lua, lsp and treesitter support are other things neovim brings to the table. I don&#x27;t use vim much anymore, but I know that neovim incorporated them first. If vim did follow on some of those innovations, consider how long it might take for new vim maintainers to get to a point where Bram needed to be to keep up, without his guidance.I wonder if there is a need for both vim and neovim in the future. Neovim was born out of wanting to do similar things to what the vim maintainers are considering. reply maleldil 20 hours agorootparentI know you listed Lua already, but it&#x27;s important to stress that. Bram _did not_ want Lua as the main scripting language, and created Vim9 Script as a \"better VimScript\" instead. That&#x27;s a stark difference in the direction for the project, and it signals to me that Vim is not meant to reintegrate with Neovim. It&#x27;s no longer a question of Vim catching up with Neovim.However, this was Bram&#x27;s vision. We don&#x27;t know how the new leadership will affect that. reply gorjusborg 18 hours agorootparent> Vim9 Script as a \"better VimScript\" instead. That&#x27;s a stark difference in the direction for the projectGreat point. I wasn&#x27;t following vim much, so didn&#x27;t know the context around the language changes.That does seem like a core philosophical difference. I&#x27;m interested to see what the future holds. reply ossusermivami 2 hours agorootparentprevit&#x27;s not quite just a \"better vimscript\" language it&#x27;s an incompatible language, if it was just improvement on top of the existing vimscript it would have been fine i think but what people are complaining about is if you are going to break compatibility why not choose an existing language like lua reply maleldil 1 hour agorootparentYes, you&#x27;re right. That&#x27;s what I meant, but I should&#x27;ve highlighted the incompatibility aspect. reply gkfasdfasdf 21 hours agorootparentprevNeovim&#x27;s language server integration is a killer feature that got me to switch. reply linsomniac 20 hours agorootparentAbsolutely! I started using LunarVim packages+configs on top of neovim), looks like a couple of years ago according to my work notes, and the LSP has been a huge improvement in my editing. reply bradly 12 hours agorootparentprevCan you explain the benefit? I&#x27;m not familiar with the feature. reply gkfasdfasdf 5 hours agorootparentBasically neovim can act as a client to a variety of different language servers (https:&#x2F;&#x2F;github.com&#x2F;neovim&#x2F;nvim-lspconfig&#x2F;blob&#x2F;master&#x2F;doc&#x2F;ser...) which give neovim IDE capabilities. This can be done in original Vim also but requires external plugins which can be a pain to compile and install. Neovim has it built in. reply BaseballPhysics 20 hours agorootparentprevOne of the big ones for me was Wayland support in neovim-gtk (https:&#x2F;&#x2F;github.com&#x2F;Lyude&#x2F;neovim-gtk), which therefore, unlike gvim, fully supports fractional scaling. reply infinitezest 22 hours agorootparentprevMaybe not important to some but the ability to use Lua over VimScript is a big win in my opinion. In fact, I use a mix of thr two where necessary. reply pmoriarty 21 hours agorootparentHow much of the rest of the vim ecosystem is Lua-based, though? As far as I know it&#x27;s still mostly vimscript.Vimscript&#x27;s dominance in vim is one of the things that got me to switch to emacs when I got interested in Lisp and Scheme more than a decade ago.Sure, even then I could write scripts for vim using Vim&#x27;s scheme compatibility mode, but I&#x27;d probably be one of the only ones doing so. Pretty much everyone else was using vimscript.In Emacs the whole ecosystem is in eLisp, so I&#x27;d feel right at home there, could naturally integrate other eLisp codebases&#x2F;projects, could easily get help on anything related to working with eLisp in emacs.If I&#x27;d written scheme scripts in vim my work would always be a second-class citizen in the vim ecosystem.How&#x27;s the vim ecosystem now? Is vimscript still dominant? reply ftaghn 20 hours agorootparent> How&#x27;s the vim ecosystem now? Is vimscript still dominant?You can have a full neovim experience with all sorts of modern extensions without using a single line of vimscript. Some people even replace their init (neovim&#x27;s vimrc) with lua, but I am of the opinion that it is a step too far, as lua isn&#x27;t particularly adapted to writing configuration files and the result is too verbose to my taste. reply berkes 17 hours agorootparentMy config is a horrible frankenstein of both.I use lua when examples are in lua or when I need some \"logic\" (such as assigning defaults to a var and then passing that around&#x2F;overriding). And that&#x27;s embedded in vimscript.I don&#x27;t really like either. VimScript has always been a horror to me, eventhough I&#x27;ve been using vim for some 20 years now, almost exclusively. Lua is \"that thing that I should really sit down and learn. But not now, I&#x27;ve got stuff to finish\".What does lua -for an end user- offer that something like yaml+python cannot offer? I really won&#x27;t mind setting all sorts of flags, defaults and vars from some init.yaml, and then have some init.py to handle the few places where I do need actual logic. Why was lua picked, why did vim build its own language and not move to an existing one for its config? Am I just weird for never sitting down and learning lua? Or vimscript? Or both? reply CorrectHorseBat 14 hours agorootparentLua is faster than Python and also easier to embed in vim [1] which are both big advantages for the end user. It means fast plugins and no hassle with having the correct python version.Why Bram build is own language and then doubled down on it with vimscript9 I don&#x27;t really understand.[1] https:&#x2F;&#x2F;neovim.discourse.group&#x2F;t&#x2F;why-was-lua-chosen-for-neov... reply enriquto 12 hours agorootparent> Why Bram built his own language [...] I don&#x27;t really understand.What language would have you chosen in 1991 instead of creating vimscript? The vimscript language is a very natural extension[0] of Bill Joy&#x27;s \"ex\" language, that was used in vi since 1976. The history of vimscript is not weird, it&#x27;s just a fairly natural continuation of existing practices. Vimscript9 is a least-friction update for modern times.[0] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Vim_(text_editor)#Vim_script reply CorrectHorseBat 3 hours agorootparentI should have written I don&#x27;t really know anything about.According to your link scripting was only available from &#x27;98, at which point Lua and Python were already around, but then I don&#x27;t know how viable that would have been. Funny you say that about ex, because I find running ex commands from vimscript the most clunky thing about it.In my opinion the least friction for the ecosystem would have been to follow neovim and adapt Lua. reply enriquto 16 hours agorootparentprevindependently of vim configuration, lua is a cute general-purpose language worth learning in itself. It is much simpler than python, and the documentation is outstanding. reply bongobingo1 18 hours agorootparentprevAh you see, you just wrap that lua in a lisp in some terse macros and there you go, instead of writing `:set formatoptions+=j` (yuck, vimscript!) or `vim.opt.formatoptions:append\"j\"` (vom, lua!) you can write the clearly superior `(opt formatoptions +j)` (heck yes, ivory tower). reply kzrdude 20 hours agorootparentprevI have mainly lua config but just put in some vim files where I put bits and functions that were already written in vimscript. reply apetresc 18 hours agorootparentprevNeovim&#x27;s adoption has been incredible. Virtually every major vimscript plugin has either been ported or overtaken by a Lua-native alternative in the Neovim ecosystem. reply kzrdude 20 hours agorootparentprevI think the ecosystem is more or less splitting. As a user, not dev, it looks like neovim \"distributions\" are mostly lua and would be feature complete (with some substituted plugins) even if 100% lua was enforced. reply __tmk__ 17 hours agorootparentprevI&#x27;ve also seen neovim plugins written in fennel [0], so if you want something lispy, that&#x27;s possible now.[0]: a Lisp that compiles to Lua, https:&#x2F;&#x2F;github.com&#x2F;bakpakin&#x2F;Fennel reply arp242 20 hours agorootparentprev> the ability to use Lua over VimScriptThe Neovim people went a bit further with the integration, but you&#x27;ve been able to use Lua with Vim for like 20 years or so, and you can use it to write perfectly functional plugins with it. Probably the main difference is that in Neovim Lua is always guaranteed to be available, which isn&#x27;t the case for Vim. reply oblio 22 hours agoparentprevInertia.Such base programs basically need to go through the Debian packaging gauntlet if they want to succeed.What I mean by that is that generally they need to persuade distros to be anointed the \"official\" tool, i.e. Debian or Fedora would have to select Neovim as the new default text instead of Vim.Then you need a few years for the changes to trickle down everywhere: Debian -> Ubuntu-> Mint -> ..., Fedora -> RHEL, ... After that distro releases need to be cut and people need to upgrade.I think a full cycle, where a tool becomes ubiquitous if it gets adopted in the base installs, is probably 10 years. See systemd. reply trealira 21 hours agorootparent> What I mean by that is that generally they need to persuade distros to be anointed the \"official\" tool, i.e. Debian or Fedora would have to select Neovim as the new default text instead of Vim.You&#x27;re probably right. However, I think it&#x27;s more likely that the Linux distros drop Vim entirely and make Nano the default editor than that they replace Vim with Neovim.The BSDs have nvi as the default editor, IIRC, so they won&#x27;t need to change anything. reply layer8 20 hours agorootparentNano already is the default editor (via &#x2F;etc&#x2F;alternatives&#x2F;editor). reply user3939382 21 hours agorootparentprev> Inertia.At some point I tried to switch and some setting broke in my config (mouse mode?) I forget what it was. It took me another 3-4 years before I tried again. reply planede 21 hours agorootparentprevIn what sense vim is default in Debian? reply layer8 20 hours agorootparentIt’s not, Nano is. reply massysett 19 hours agoparentprevMy Vim needs are very modest. I just don’t need anything Neovim has. It’s complication I don’t need.If Vim got no new features, I wouldn’t care. If Vim became unmaintained but still available from distributions, I’d still use it. If Vim became unavailable (e.g. due to lack of maintenance) I’d be more likely to switch to nvi than Neovim.I could probably switch to nvi now, but I have no reason to. reply throwaway290 18 hours agorootparentFWIW you can use Neovim like Vim with your existing config, without any of the other stuff. That made the switch easy for me. Of course this means you&#x27;d rely on Neovim maintainers honoring that compatibility in future... reply massysett 17 hours agorootparentOne of the things I really like about Vim is how it maintains compatibility with Vi. The manual obsessively points out features Not in vi. It retains some ill-advised things like :Print because Vi had them. Ancient platforms still work, supposedly.I care about this not because I used Vi (I never have) but because it’s less likely that some new Vim with a newer distribution release will do something I don’t expect. Vim does want I need it to do. I don’t want it to change.Neovim on the other hand exists entirely to change things. That’s all well and good. I just don’t want it. My editor is complete. reply throwaway290 15 hours agorootparentUhm if you read my comment it is specifically about how nvim doesn&#x27;t change the preexisting behavior of vim. And if I were to believe you then compatibility with vim means compatibility with vi reply massysett 12 hours agorootparentI did read your comment word for word: “Of course this means you&#x27;d rely on Neovim maintainers honoring that compatibility in future.”I knew that maintaining compatibility was extremely important to Bram Moolenaar. I also knew that Neovim made a big deal out of removing “cruft” and old stuff they decided no one needed. I preferred Bram’s values. reply throwaway290 10 hours agorootparentand I responded to your \"exists to change things\". Vim also exists to change things, otherwise it wouldn&#x27;t exist if Vi was enough. but it is compatible and from my experience nvim is compatible with vim.Having to rely on them for that is a downside, but I guess you are free to fork nvim if they abandon that promise... replyandrewla 16 hours agoparentprevFor me, it&#x27;s the use of Lua. I switched to neovim in no small part for the sane defaults, the XDG layout support, and the async&#x2F;terminal stuff (and I found the code simplification, addition of tests, and removal of ancient legacy stuff to be very appealing).The terminal and async stuff has been \"backported\" to vim, as it were. But the plugin ecosystem is diverging. The other items are still open, and maybe those will move forward.At this point Vim9 is the clearly superior language (don&#x27;t hate me) since it&#x27;s very domain-specific, while Lua has only very primitive hacky support. But the plugin ecosystems have diverged -- I don&#x27;t see NeoVim coming back home without native Lua support, and I definitely don&#x27;t want native Lua support in vim. reply jzb 20 hours agoparentprevLoyalty? Inertia? I never felt like there was a reason to switch. I admired Bram’s work on Vim, still do, and didn’t see any benefits to switching. Still don’t, as long as the rest of the Vim dev community are willing and able to carry it forward.For my purposes, Vim is complete. I don’t require new features, as long as it is maintained and runs on modern OSes. reply plugin-baby 21 hours agoparentprevI tried switching recently, mainly because I wanted treesitter. I couldn’t get it working after half a day messing about, a lot of documentation seemed out of date, and I don’t think my standard plugins + .vimrc worked out of the box. My main focus is getting things done, not fiddling with configs and versions, so I gave up. I figure I’ll try again in a couple of years if it’s still tempting. reply gorjusborg 21 hours agorootparentWeird. That hasn&#x27;t been my experience at all.I copied .vimrc to .config&#x2F;nvim&#x2F;init.vim, did a :PluginInstall and was up and running. reply colordrops 18 hours agorootparentprevI fiddled with it for half a day 5 years ago to get it working and have reaped the benefits for 5 years. reply OJFord 22 hours agoparentprevWell, why should I? I don&#x27;t need a reason not to do everything, by default I&#x27;m not doing things, it&#x27;s changing from status quo that requires a reason. reply rgoulter 20 hours agorootparentIt&#x27;s easier to maintain one codebase than to maintain two forks.If development of vim dropped and neovim was nominated as its successor, I&#x27;d think most vim users would be just fine. reply halostatue 20 hours agorootparentNot those of us who gvim&#x2F;MacVim.Give a real alternative to those instead of the many quarter-implemented GUI shells that are really no better than running Electron, and the switch might be able to happen. reply OJFord 20 hours agorootparentprevYes, if vim were unmaintained I would switch, that would be a reason to change. But it&#x27;s not, or at very least hasn&#x27;t been, so I haven&#x27;t, that hasn&#x27;t been a reason to. reply yjftsjthsd-h 19 hours agorootparentprev> It&#x27;s easier to maintain one codebase than to maintain two forks.Not necessarily; if the two codebases are maintained by groups of people with incompatible ideas about how the code should work and what it should do, then keeping things separate is much easier.> If development of vim dropped and neovim was nominated as its successor, I&#x27;d think most vim users would be just fine.Agreed; they don&#x27;t diverge that much from an end-user&#x27;s perspective. reply kps 20 hours agoparentprevEvery time I&#x27;ve tried it, something was missing or broken. Currently it&#x27;s at least :! which breaks apparently because it tries to do something ‘clever’ and complicated, whereas vi and vim just run the command.(I can live without cscope, since I no longer do significant C and mlcscope has been dead for aeons, though it&#x27;s shocking that LSPs are still less capable in some respects.) reply minusf 17 hours agorootparentnot a heavy :! user, what i used there worked, but afaik neovim recommends :te . that&#x27;s one of the bigger differences. neovim was very proud to have a fully integrated terminal reply layer8 19 hours agoparentprevBy now the two have diverged enough that switching configurations between both is an issue. Vim is still much more widely available (installed by default) than Neovim, which makes it the obvious choice to maintain your configurations for if you don’t specifically care about Neovim features. Furthermore, additions like Lua support, while pragmatic, make Neovim feel less organic to me. reply ansible 15 hours agorootparentIn that vein, you are probably fine with whatever version of Vim is installed by your distro. But if you want LSP support for more cutting-edge stuff (like Rust), you probably need to install the latest release of Neovim by hand, not using the OS packaging system. Because stuff is changing too fast. reply beej71 20 hours agoparentprevIf neovim had the same keystrokes to move out of a terminal window as it does to move out of editor windows, I&#x27;d switch. But as it is, it&#x27;s super clunky compared to vim.I know about rebinding to alt, but that doesn&#x27;t work in all my terminals.So for me, it&#x27;s that one lousy thing that keeps me from switching. And if someone knows the magic setting to make it mimic vim, please let me know. reply ansible 15 hours agorootparentI&#x27;ve never used those features, and instead rely on GNU screen (or `term`) to manage my terminal windows. Sometimes I also use horizontal &#x2F; vertical splits with GNU screen, but it is a bit clunky. reply QuadmasterXLII 8 hours agorootparentFor better or for worse ive ended up with a workflow heavily dependent on sending vim registers to the terminal reply kzrdude 17 hours agorootparentprevWhat keystrokes does vim use for this? I&#x27;ve never been a big user if terminals inside vim reply imbnwa 19 hours agorootparentprevI hear this and the workaround has been opening terminals in a floating window. The “toggleterm.nvim” plugin wraps a bunch of good behavior around this. reply h11h 21 hours agoparentprevMacVim and gVim. I&#x27;ve looked at neovim and there are many GUI options (paradox of choice), some of which I&#x27;ve tried, but at the end of the day, I&#x27;m more comfortable with what I&#x27;m already familiar with. reply rmwaite 20 hours agorootparentLikewise. I’m a heavy CLI user but gVim&#x2F;MacVim are so entrenched in my workflow that the lack of a stable GUI for Neovim made it pretty much a nonstarter for me. reply veilrap 13 hours agorootparentAs some one who only uses vim&#x2F;neovim in a terminal window - what’s the advantage of having GUI support in vim?Mouse support in my terminal seems fine, even over ssh. Being able to do things like run it inside of a tmux session has always made it seem like the GUI would be a step back? reply ciupicri 9 hours agorootparentYou can use gVim as a Notepad on steroids. You have the simplicity and familiarity of Notepad combined with the power vim, e.g. search & replace using regular expressions, syntax highlighting, the ability to delete 5 words by typing ESC d 5 w, plugins like fugitive and so on. I can&#x27;t select text in neovim by pressing Shift and arrow keys. I can&#x27;t copy text by pressing Ctrl+C. reply sigzero 20 hours agorootparentprevThis is my biggest hurdle as well. The UI not being part of the core is a non-starter for me and I have tried several of the neovim ones. reply w0m 16 hours agorootparentprevI did the Vim -> NeoVim switch a while back (pre-vim9script) and lack of standardized GUI is a non trivial issue. There are solution(s) - but allof them have had too much friction to fit in a workflow for me. Definitely a concern. reply ALoverOfLats 22 hours agoparentprevCan’t speak for everyone but in my day to day most systems I work with already come with vim preinstalled or more convenient to install than neovim. I’d rather just pull my vimrc (If I even have to) and get down to editing quickly. reply arp242 20 hours agoparentprev- It&#x27;s not backwards compatible.- Features I find useful have been removed.- I dislike Lua, and significantly prefer Vim9Script.- Gvim is useful at times. reply senknvd 14 hours agorootparentFrom what I&#x27;ve read[1], vim9script was pushed and developed almost exclusively by Bram. With him, a lot of knowledge about its internals and vision for its future dies.[1]: https:&#x2F;&#x2F;github.com&#x2F;vim&#x2F;vim&#x2F;discussions&#x2F;12736#discussioncomme... reply arp242 14 hours agorootparentYou&#x27;re correct it was very much a \"Bram project\", but that doesn&#x27;t mean the language needs to die with him: other people can work on it (and already have!) Vim9Script is also \"finished\", more or less, as \"finished\" as languages get anyway. The features Yegappan mentions are what we might call \"optional features\". reply nequo 18 hours agorootparentprevOut of curiosity to better understand the ways in which Vim can be used:> Features I find useful have been removed.Which ones specifically?> significantly other Vim9ScriptWhat do you like more in Vim9Script?> Gvim is useful at times.What are your use cases for the GUI? reply arp242 17 hours agorootparentI&#x27;m pretty sure I wrote a long comment about all of this at some point, but I can&#x27;t seem to find it right now; this is the closest: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=21954164If I look at some Lua plugins and compare that to some of my Vim9Script (or even \"legacy VimScript\") plugins then I think &#x2F;Vim9?Script&#x2F; \"wins\" hands-down; it&#x27;s just much more convenient for programming an editor. It also doesn&#x27;t help that IMHO Lua isn&#x27;t all that great of a language to start with – it&#x27;s not horrible either, just not great.On Windows gvim works loads better (even on Unix systems gvim is arguably better, because terminals kind of suck and you run in to loads of graphical and input limitations pretty quickly).But in general: neovim doesn&#x27;t offer me anything I want or need, I will have to spend time on migrating (e.g. my vimrc would error out, I need to deal with changed defaults, etc.), and for most things I prefer the \"highly compatible\" attitude from Vim&#x2F;Bram, which is a trade-off that&#x27;s not without its downsides, but I really like it (for most software). reply cossatot 17 hours agorootparentprevPersonally, I do heavy coding and writing in Neovim (with more extensive config, like LSP and Copilot), but I use macvim and gvim (depending on which OS I&#x27;m using) for quick edits or file viewing, because I often need to open files from Finder or Nautilus and I don&#x27;t have a terminal open in that directory. reply ciupicri 9 hours agorootparentprev> What are your use cases for the GUI?I don&#x27;t want to type like it&#x27;s 1976. I want something simple and easy to use similar Notepad &#x2F; gedit, but that still powerful when needed. reply nemetroid 18 hours agoparentprevnext [–]$ rpm -qi neovim package neovim is not installed $ dnf search neovim No matches.I don&#x27;t have anything against it, but prefer being able to use the same tool across environments. reply berkes 17 hours agorootparentstrange.> Neovim is available through EPEL (Extra Packages for Enterprise Linux)from https:&#x2F;&#x2F;github.com&#x2F;neovim&#x2F;neovim&#x2F;wiki&#x2F;Installing-Neovim#cent...> Neovim is in Fedora starting with Fedora 25 > sudo dnf install -y neovim python3-neovimfrom https:&#x2F;&#x2F;github.com&#x2F;neovim&#x2F;neovim&#x2F;wiki&#x2F;Installing-Neovim#fedo... reply nemetroid 16 hours agorootparentNot so strange if you’ve met the kind of sysadmins that don’t enable EPEL. reply mrzool 22 hours agoparentprevUbiquity. I have Vim on every machine I log into without installing anything extra. reply rgoulter 20 hours agorootparentWith caveats:- Different servers may have different versions of the program. Some distros are very \"stable\" and have very old versions of programs.- It&#x27;s common (especially for vim) for users to have significant configuration files to make use easier. reply infinitezest 22 hours agorootparentprevIn my experience, Nvim and Vim are similar enough that I don&#x27;t have any trouble SSHing into any server with Vim and using it after using Nvim all day for development. So far Nvim has been a purely opt-in experience for me. reply tambourine_man 22 hours agoparentprevFrom what I remember, NeoVim drops vi compatibility mode.vi is part of POSIX. That alone would be a reason to mantain Vim as a modern superset of vi. reply rgoulter 20 hours agorootparentIn what circumstances would you run `vi` and find yourself upset that you end up in `vim`? reply tambourine_man 18 hours agorootparentYou won’t, but vi is scriptable and part of POSIX.It’s a basic infrastructure that you don’t want to break. reply rgoulter 18 hours agorootparentApparently the original vi isn&#x27;t POSIX compliant; nor is vim. https:&#x2F;&#x2F;vimhelp.org&#x2F;vi_diff.txt.html#posixI think \"if the script worked before, I want it to work later\" has much more practical weight. Though, on the other side of practicality, I&#x27;ve heard sed, awk, and perl suggested for manipulating text in bash scripts far more than I&#x27;ve heard of vi recommended for the task. reply ansible 15 hours agorootparentYes. If you need to do some text manipulation in a shell script, `sed` or something like that is what I&#x27;d reach for first. (I haven&#x27;t used `awk` or Perl for quite a while.) I&#x27;d think even `ed` would be a more straightforward choice than `vi`. reply revscat 21 hours agorootparentprevNeovim is a superset of vim, so this argument applies to both. reply johncoltrane 21 hours agorootparentNo, Neovim is not a superset of Vim. Vim has things that Neovim doesn&#x27;t have and vice-versa. reply marcthe12 21 hours agorootparentprevNeovim remove set compatible (hardcodes set nocompatible). Vim support the POSIX vi via that option. reply tambourine_man 18 hours agorootparentNot quite. Vim is vi compatible by default. You have to explicitly opt in for the newer behavior. reply kolme 19 hours agoparentprevI would need to migrate my chunky config files. Some issues should be easy (moving files to ~&#x2F;.config), some other trickier (vim-specific functionality).But alas, the motivation is not big enough for me to invest the effort.Some features I&#x27;d like from Neovim are built-in LSP (but Vim has that thank to plugins), and tree-sitter based syntax highlighting.That&#x27;s not enough for me to move to Neovim. reply zelos 22 hours agoparentprevI&#x27;ve switched to Neovim, but there doesn&#x27;t seem to be a macOS UI nvim client that&#x27;s quite as polished as MacVim. reply anta40 21 hours agorootparentThat&#x27;s why I still keep vim updated on my mac system. For nvim, the closest one I found is VimR. reply zelos 21 hours agorootparentI&#x27;m using Neovide for now, which seems to be gathering momentum, but I still miss MacVim. I had a fair few crashes with VimR that ruled it out for me. reply BanazirGalbasi 18 hours agoparentprevI feel like I&#x27;ve said the same thing here 3-4 times now, but for some of us it&#x27;s about what&#x27;s the most minimal setup required to use the tool. As a sysadmin, I want to be used to the most common tools and configurations that will be on a server without having to take the time to install something new. I could include NeoVim in my Ansible configs for setting up new servers, but generally servers are kept lean so I would rather just use vi&#x2F;vim for basic edits anyway.I do use NeoVim with a lightly-customized LazyVim setup on my personal desktop, but I don&#x27;t use it much differently than I use Vim at the moment. I&#x27;m not a power-user, just someone who&#x27;s comfortable enough with the keybinds that I leave :w everywhere when using a non-vi editor. reply tomwheeler 16 hours agorootparent> for some of us it&#x27;s about what&#x27;s the most minimal setup required to use the tool.That&#x27;s was the case for me. When I moved from vi to vim 25 years ago, I devoted a lot of time to customizing it for maximum developer efficiency. Around that time, I got a job where I regularly used five different HP&#x2F;UX machines, a couple of Solaris boxes, and a few other random machines. At the next job, it was HP&#x2F;UX, AIX, and IRIX. Few of those machines had vim at all, let alone a version compatible with the setup I had on Linux.I eventually stopped doing the fancy things and settled into using plain vanilla vi, knowing that it would at least work consistently on every machine I used. reply guhidalg 18 hours agorootparentprevSerious question: how is neovim “heavier” than vim? reply thefifthsetpin 18 hours agorootparentvim often ships with the base image, so you&#x27;d be comparing installing neovim on top of that to installing nothing. reply svlasov 20 hours agoparentprevgvim and vimscript9 reply syndicatedjelly 17 hours agoparentprevI would like to use Neovim, but it&#x27;s not as fully featured as VSCode is, so when I need to boot up something more powerful than Vim, I go to VSCode instead. reply highpost 16 hours agorootparentThere is an excellent NeoVIM extension that integrates NeoVIM into VS Code. https:&#x2F;&#x2F;marketplace.visualstudio.com&#x2F;items?itemName=asvetlia...Best of both worlds. reply stcroixx 18 hours agoparentprevFor my use case of simple editing, vi was feature complete like 30 years ago. I don&#x27;t have any preference at all what open source project build implements those features. reply sdfghswe 22 hours agoparentprevFrom my admittedly somewhat limited experience, neovim felt a bit more sluggish to launch. Not a good sign. reply petepete 22 hours agorootparentnext [–]nvim --clean --startuptime &#x2F;tmp&#x2F;neovim 003.174 000.001: --- NVIM STARTED ---vs vim --clean --startuptime &#x2F;tmp&#x2F;vim 004.274 000.001: --- VIM STARTED ---Maybe your config had an impact on it, but config-free they&#x27;re incredibly close. reply sdfghswe 22 hours agorootparent3&#x2F;4 seconds??? Something&#x27;s wrong with your system, right? reply petepete 22 hours agorootparentThey&#x27;re milliseconds!I abbreviated the output, if you run the above command and check the file, at the top it says times in msec reply sdfghswe 21 hours agorootparentOh that&#x27;s awesome actually. replysuprjami 22 hours agoparentprevcscope support reply oblio 22 hours agoprevI hope both Vim and Neovim extend olive branches (and Git branches where it makes sense! :-D ).Even if the fork doesn&#x27;t heal, if they could align the code bases to bring them closer, both sides win. reply goku12 22 hours agoparentNeovim team has always been positive towards Vim. This post doesn&#x27;t paint the Vim team as being hostile either. However, I really wonder how practical a merge is, considering that neovim isn&#x27;t a fork or Vim, rather an implementation from scratch.Edit: Looks like I&#x27;m wrong about neovim not being a fork of vim. reply oblio 22 hours agorootparentIt&#x27;s not an implementation from scratch, do you know the history of the projects?Neovim started out as a large clean up and refactor of Vim code, plus the addition of async code.That&#x27;s a huge amount of work, partially re-implemented by Vim (Bram implemented his own version of async).Actually, after the Neovim launch a lot of the Vim features were just Bram chasing after Neovim features. Vim9script, :term, etc.I think there was bad blood there with Bram, I&#x27;m not sure how deep the emotional rift was between the 2 groups. From the outside a lot of it looked like stubbornness on the Vim side, at least 60% of the time.. reply kzrdude 22 hours agorootparentBram evidently had his own way, since he didn&#x27;t let anyone else make commits to the project. That&#x27;s fair, and in that situation forks are expected. reply rmwaite 20 hours agorootparentI’ve seen some reports that he did allow commits but he would “re”-commit them as part of his workflow. I’m not sure about the reasoning there. reply nzach 19 hours agorootparentprev>Neovim started out as a large clean up and refactor of Vim code, plus the addition of async code.I don&#x27;t think that is actually true.As far as I remember Justin and Thiago (I think) wanted to implement the &#x27;:term&#x27; into vim and Bram just shut them off because he didn&#x27;t wanted to lead Vim into the road that Emacs went.And this was what prompted the Neovim fork. And they took the oportunity to also make a big refactor in the codebase. reply fatbird 18 hours agorootparentIt was async that Justin and Thiago tried to add to vim, and spent several months wrestling with Bram to get their patch included. At the end, they concluded that they weren&#x27;t going to get it in and that a fork was necessary. Including the terminal followed quickly after neovim was set up and organized. reply fatbird 18 hours agorootparentprevI don&#x27;t think there was bad blood with Bram. Rather, vim was always Bram&#x27;s baby, even with contributions from the community, and he was very opinionated about how things should be implemented. IIRC, he found the new features of neovim interesting enough to want to implement them his own way, and a couple times commented that he thought that neovim&#x27;s approach to certain features was \"wrong,\" to his way of thinking.His choices were always a bit idiosyncratic, but the success of vim justified them, so it was never a problem... until he died, and now there are two projects, one of which is very modern, both in terms of the codebase and how the project is managed, and the other is likely to become something of a legacy project unless someone as dedicated as Bram is found to take over. reply biorach 22 hours agorootparentprevNo, Neovim is a fork> Neovim is a refactor, and sometimes redactor, in the tradition of Vim (which itself derives from Stevie). It is not a rewrite but a continuation and extension of Vim.https:&#x2F;&#x2F;neovim.io&#x2F;charter&#x2F; reply AlecSchueler 22 hours agorootparentprevAre you sure it&#x27;s not a fork? From their project wiki:> Neovim is a project that seeks to aggressively refactor Vim source code> It is important to emphasize that this is not a project to rewrite Vim from scratch reply bombcar 22 hours agorootparentprevThese types of mergers have occurred in the past and it usually is the dominant project adopting the use and feel of the features the other has different. Shells acting differently depending on how you call them, for example. reply 0cf8612b2e1e 21 hours agorootparentI am not sure how you reconcile some of the philosophical differences between the projects. One of the first objectives of NeoVIM was to dump code for obsolete computing platforms. A ton of legacy code was deliberately removed so as to streamline the project.If I recall correctly, one of the reasons the original async patch was ostensibly rejected was because it would rely on a C89(?) compiler. That was considered too disruptive a change that would make Vim accessible on fewer platforms. reply rolisz 20 hours agorootparentI think very, very, very few people still care about the C89 compiler.A bigger blocker to a merge&#x2F;collaboration would be the move to use Lua in neovim reply bombcar 21 hours agorootparentprevIt’s brutal to say it, but often you handle it by some of the people involved passing away.Even if neovim basically takes over the older vim code wouldn’t disappear, so it would continue to exist for older platforms. People still keep certain versions of GCC around for similar reasons.Or you abuse the preprocessor and automake. reply oblio 15 hours agorootparentScience advances... reply Aissen 21 hours agorootparentprevasync is water under the bridge since Vim 8, both Neovim and Vim have async apis. I foresee native LSP and everything-lua to be a bigger schism between Neovim and Vim. reply halostatue 20 hours agorootparentAlso the deliberate lack of GUI for neovim. The currently extant neovim GUI shells are all universally broken, and some of them crash more than they run. I have yet to find one that is remotely as good as MacVim on macOS. I suspect that most of them are merely tolerable on Linux and some might be barely acceptable on Windows. reply imbnwa 19 hours agorootparentWith their recent release a few days ago, Neovide has become my daily driver for Neovim on my personal cpu. It were definitely rough edges a few versions ago, but I’m rather pleased where they are now.I find a lot of Rust GUI projects are slow-going but this has had a good pace reply halostatue 17 hours agorootparentI’ll try it again in a few months, because it was completely unusable on macOS last time I tried it in March. At the moment, I have no time for something that may not work when MacVim works pretty much perfectly for me. reply sundarurfriend 14 hours agorootparentI&#x27;ve generally had more success with Goneovim than with Neovide, give that a try if you haven&#x27;t yet. reply halostatue 13 hours agorootparentI’ve tried so many different nvim GUIs, but when I last tried them (March or so), none of them behaved correctly on macOS, most of them had atrocious font rendering, didn&#x27;t like part of the configuration that I had set up (neovide in particular had a deep incompatibility with one of the alert replacement plugins), crashed regularly, or had things enabled by default which aren&#x27;t reachable with vim scripting.I’ve gone all in and have converted my vim config to Lua and all…and I have decided that don&#x27;t like Lua for configuration (there&#x27;s a massive impedance mismatch between neovim and Lua; you always feel like you&#x27;re working with a foreign interface).Goneovim was slightly more stable, but IIRC, the font rendering and macOS integration were awful enough that I uninstalled it shortly after launching it. Neovide lasted slightly longer (so that I could see that plugin incompatibility). VimR tried, but it isn&#x27;t MacVim.And that&#x27;s the problem: they aren&#x27;t MacVim, providing a native macOS experience on top of a native vim GUI, because the neovim leadership cabal, in their infinite wisdom, decided that a first-party GUI is \"useless\". reply pwpw 20 hours agoprevI never realized a tool I use every day was still being actively maintained. An endless amount of thanks to Bram Moolenaar, the many others that have contributed to it, and those that are now helping in the transition.—-As an aside, browsing this website on an iPad is terrible. It doesn’t respect my request to increase the font size, and when I zoom in, it starts moving and wrapping the text defeating the purpose of the zoom. That being said, the quote formatting and response is fantastic and how online communication should be. reply gpvos 21 hours agoprevI&#x27;m getting \"Something went wrong. Please try again later.\" (both latest FF and Chrome). I don&#x27;t think HN can slashdot Google yet, any ideas? reply ritzaco 21 hours agoparentI get the same on firefox with ublock origin, but it works if I turn off ublock or use chrome.on my android firefox, I can&#x27;t load any google properties - it just redirects to a help page about how to clear my cache (clearing the cache doesn&#x27;t help) reply bondant 19 hours agorootparentI had no problem browsing that page with firefox a few hours ago, but it&#x27;s not working anymore.So yes, maybe the servers handling google groups are a little too busy. reply gpvos 18 hours agorootparentIt&#x27;s showing for me now, so apparently there was some kind of temporary problem, maybe busy servers indeed. reply bjord 20 hours agoparentprevI had to join the group in order to see the conversation(s). reply 112 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The future of the Vim project is being discussed, including plans for a maintenance update and potentially adopting a more modern approach.",
      "The organization has granted access to GitHub and is inviting additional members to join.",
      "There are discussions about potentially moving the Vim homepage to a different provider, with support from other members and suggestions for future development."
    ],
    "commentSummary": [
      "Hacker News is a popular forum for discussing various topics, including managing digital assets after death and comparing text editors Vim and Neovim.",
      "Users on Hacker News share suggestions, personal experiences, and concerns about password management, secure storage of information, estate planning, and accessing financial accounts after death.",
      "The discussions also touch on the use of Lua in Neovim's configuration, the potential integration of Neovim features into Vim, and issues with Neovim GUIs on macOS."
    ],
    "points": 653,
    "commentCount": 362,
    "retryCount": 0,
    "time": 1691665050
  },
  {
    "id": 37081306,
    "title": "HashiCorp adopts Business Source License",
    "originLink": "https://www.hashicorp.com/blog/hashicorp-adopts-business-source-license",
    "originBody": "Contact sales Blog Home Products & Technology Company HashiCorp Voices All COMPANY Twitter share LinkedIn share Facebook share Copy URL HashiCorp adopts Business Source License HashiCorp adopts the Business Source License to ensure continued investment in its community and to continue providing open, freely available products. AUG 10 2023 ARMON DADGAR When Mitchell and I founded HashiCorp, we made the decision to make our products open source because of a few key beliefs: We believe strongly in freely available source code to make it easy for practitioners to freely download, inspect source code, and solve their own problems. We believe in building an ecosystem and community around our products to enable broad integrations. We believe in the importance of transparency for our users. For more than a decade, we’ve continued to build new products and features provided to the community under a free open source license. This has led to a large community of users, contributors, partners, and customers who participate in and benefit from the work on the HashiCorp products. Our open source model has been made possible by the thousands of commercial customers who partner with us on their mission-critical infrastructure. We invest tens of millions of dollars in research and development in our open source products annually, and our commercial efforts enable us to continue to support, and sponsor, our vibrant community of users. Our approach has enabled us to partner closely with the cloud providers to enable tight integration for our joint users and customers, as well as hundreds of other technology partners we work closely with. However, there are other vendors who take advantage of pure OSS models, and the community work on OSS projects, for their own commercial goals, without providing material contributions back. We don’t believe this is in the spirit of open source. As a result, we believe commercial open source models need to evolve for the ecosystem to continue providing open, freely available software. Open source has reduced the barrier to copying innovation and selling it through existing distribution channels. Many vendors have shifted increasingly to closed source for this reason, however we did not feel that would preserve our original goals in adopting open source. That is why today we are announcing that HashiCorp is changing its source code license from Mozilla Public License v2.0 (MPL 2.0) to the Business Source License (BSL, also known as BUSL) v1.1 on all future releases of HashiCorp products. HashiCorp APIs, SDKs, and almost all other libraries will remain MPL 2.0. BSL 1.1 is a source-available license that allows copying, modification, redistribution, non-commercial use, and commercial use under specific conditions. With this change we are following a path similar to other companies in recent years. These companies include Couchbase, Cockroach Labs, Sentry, and MariaDB, which developed this license in 2013. Companies including Confluent, MongoDB, Elastic, Redis Labs, and others have also adopted alternative licenses that include restrictions on commercial usage. In all these cases, the license enables the commercial sponsor to have more control around commercialization. Our implementation of BSL includes additional usage grants that allow for broadly permissive use of our source code. We believe this will offer a fair and sustainable way for HashiCorp to share its source code widely, for free use. We consulted with OSS licensing experts and other industry stakeholders when developing our license, so that our efforts would be in line with industry practices. Our first goal with this change is to minimize the impact to our community, partners, and customers. We will continue to publish source code and updates for HashiCorp products to our GitHub repository and distribution channels. End users can continue to copy, modify, and redistribute the code for all non-commercial and commercial use, except where providing a competitive offering to HashiCorp. Partners can continue to build integrations for our joint customers. We will continue to work closely with the cloud service providers to ensure deep support for our mutual technologies. Customers of enterprise and cloud-managed HashiCorp products will see no change as well. Vendors who provide competitive services built on our community products will no longer be able to incorporate future releases, bug fixes, or security patches contributed to our products. Our commitment to our community, partners, and customers has not changed. We understand the trust the community places in us and we’ve worked carefully to preserve our original goals in adopting an open approach. We look forward to continuing to invest in the community and our products. We know there will be additional questions, and we’ve assembled a set of frequently asked questions to help you better understand the changes. You can also watch the short video below for more information: HashiCorp Sign up for the latest HashiCorp news Send me news about HashiCorp products, releases and events. By submitting this form, you acknowledge and agree that HashiCorp will process your personal information in accordance with the Privacy Policy Sign Up More blog posts like this one AUGUST 03 2023COMPANY HashiCorp wins the Datadog Partner Network Integration Developer Partner of the Year Award HashiCorp wins the 2023 Datadog Partner Network (DPN) Integration Developer Partner of the Year Award. JULY 25 2023COMPANY HashiCorp State of Cloud Strategy Survey 2023: The tech sector perspective Technology firms are using multi-cloud to improve their business outcomes in many ways, from boosting reliability to dealing with skills shortages. JULY 13 2023COMPANY HashiCorp State of Cloud Strategy Survey 2023: The financial services perspective Financial services firms are using multi-cloud to improve their business outcomes in many ways, from boosting reliability to dealing with skills shortages. Sign up for the HashiCorp Newsletter Send me news about HashiCorp products, releases and events. By submitting this form, you acknowledge and agree that HashiCorp will process your personal information in accordance with the Privacy Policy Subscribe INFRASTRUCTURE Terraform Packer NETWORKING Consul SECURITY Vault Boundary APPLICATIONS Nomad Waypoint Vagrant INTERNATIONAL SITES German Spanish French Japanese Korean Portuguese RESOURCES Blog Tutorials Community Events Integrations Library Partners Podcast Support Training COMPANY About Us Careers WE'RE HIRING Press Center Investors Brand Contact Us System Status Cookie Manager Terms of Use Security Privacy Trademark Policy Trade Controls stdin: is not a tty We use cookies & other similar technology to collect data to improve your experience on our site, as described in our Privacy Policy and Cookie Policy. Manage PreferencesDISMISS",
    "commentLink": "https://news.ycombinator.com/item?id=37081306",
    "commentBody": "HashiCorp adopts Business Source LicenseHacker NewspastloginHashiCorp adopts Business Source License (hashicorp.com) 477 points by rpadovani 13 hours ago| hidepastfavorite541 comments jamestanderson 13 hours agoAll that I get from this is that HashiCorp is no longer an open source company.> However, there are other vendors who take advantage of pure OSS models, and the community work on OSS projects, for their own commercial goals, without providing material contributions back. We don’t believe this is in the spirit of open source.This is 100% in the spirit of open source. If this is a problem for them, why not adopt an open source license that compels developers to open source their code instead, like the AGPL?This is purely a way for HashiCorp to ensure they are the only ones who can commercialize these formerly open source projects. Which is fine. But just go closed source, then, and own that, instead of trying to have it both ways. reply joeduffy 12 hours agoparentPulumi Founder&#x2F;CEO here.The blog post is disingenuous. We tried many times to contribute upstream fixes to Terraform providers, but HashiCorp would never accept them. So we&#x27;ve had to maintain forks. They lost their OSS DNA a long time ago, and this move just puts the final nail in the coffin.Thankfully over time, they already pushed responsibility for most Terraform providers back onto their partners, so I&#x27;m hopeful the ecosystem of providers can still stay vibrant and open.We are deep believers in open source---heck my last project at Microsoft was to take .NET open source and cross-platform, our CTO helped found TypeScript, and Pulumi is an Apache open source project---it seems HashiCorp no longer is. reply redeux 8 hours agorootparent>We tried many times to contribute upstream fixes to Terraform providers, but HashiCorp would never accept them. So we&#x27;ve had to maintain forks. They lost their OSS DNA a long time ago, and this move just puts the final nail in the coffin.OSS doesn&#x27;t mean that you have to accept any PRs that showed up in your repo, nor does it mean that you have to let a competitor steer your project simply because you&#x27;re building in the open. Without further elaboration, what you&#x27;re calling \"upstream fixes\" may have been considered \"working as intended\" at HashiCorp. As I&#x27;m sure you&#x27;re well aware, every contribution has to be maintained and each increasing contribution comes with an additional burden. Responsible maintainers on large scale OSS projects must be selective about the code they let in. reply alexandre_m 7 hours agorootparentYou have to acknowledge that all these OSS projects officially backed by a corporation don&#x27;t want you to contribute certain features that are part of their enterprise offering. As soon as there&#x27;s an \"enterprise\" tier, contributions are not only based on their merit, but also evaluated as a threat to their business model.Sometimes it&#x27;s not even obvious for external contributors, but there may be some small overlap with other paid features that are part of their product roadmap.If a project on Github only has maintainers from the corporate side, you can be certain that they will ultimately drive the product for their own interest solely.We should always pay close attention to the governance model of projects we depend on or that we wish to contribute to. reply jupp0r 1 hour agorootparentOf course, but the major difference is that if I don&#x27;t like the maintainers, I can create a fork, build a community around it and happily run it in production. Imagine where nodejs would be right now if they had been BSL licensed in the iojs times. reply yjftsjthsd-h 5 hours agorootparentprevSure, OSS doesn&#x27;t mean you have to take all PRs, but if your claim is that others are just taking your code and not giving anything back, one of the alleged leeches showing up to talk about how they&#x27;ve tried to give back is very much pertinent. reply jsiepkes 3 hours agorootparentprevI&#x27;m not affiliated in any way with one of their competitors. Co-workers and I sent bug fix PR&#x27;s to for example Vault. The last couple of years almost none of them were merged. These were small bug fixes, not (large) feature additions. reply netheril96 1 hour agorootparentprevThen don&#x27;t complain about people not contributing to your projects. You reserve the right to reject my PR, and I reserve the right not to contribute any more. reply thayne 7 hours agorootparentprevI don&#x27;t know if it is the case for the fixes pulumi sent, but for PRs I&#x27;ve made to terraform providers it can take a very long time for them to be looked at, and even longer to get merged. And I think it is mostly from nor having enough resources to approve and merge PRs. Although that could possibly be fixed by inviting developers outside the company to help with approval and merging, especially for providers. reply jen20 7 hours agorootparentprevI’m sorry, but no. These are usually simple bugs like “forgot to a set a field during refresh”. They almost always correspond to one or more Terraform issues too, often ones that have been open for 4-5 years or have been “marked as stale” by some infuriating bot. reply fishpen0 11 hours agorootparentprevIf they think we&#x27;ll go crawling back to their 100x more expensive 6-7 figure Terraform Enterprise garbage just because we can&#x27;t use spacelift anymore, then I&#x27;ll show them the team of engineers we can hire for the same dollars to move the whole stack to pure pulumi or crossplane or the various CDKsThe bald faced disingenuous nature of this change here is wild. They can&#x27;t compete at their pricing because their pricing is absolutely insane over what the market can bear and they refuse to accept it.They are going out of their way to make it less expensive to stop using terraform altogether right as so many new options have entered the market reply fishnchips 10 hours agorootparentSpacelift co-founder here - please don’t panic. We will make sure you can continue to use Spacelift :) reply candiddevmike 10 hours agorootparentYou, humanitec, and env0 should start a community fork of pre-BSL Terraform and donate it to the CNCF. reply fishnchips 7 hours agorootparentI believe it&#x27;s a bit too early to make this call but based on the experience of interacting with Terraform the binary it would be absolutely amazing for the community if Terraform could be turned into a library that can become a building block for higher level services. reply Coryodaniel 7 hours agorootparentprevMassdriver would contribute here. reply gravlaks 1 hour agorootparentprevI don&#x27;t know the details of BSL, but can HashiCorp now require compensation&#x2F;$$$ from Spacelift, Scalr, Env0, etc? In that case, these products can be forced to offer similar pricing as Terraform Cloud. reply lifeisstillgood 11 hours agorootparentprevCan I ask where Pulumi gets revenue from? (Honest question first time I have heard of you, quick look seems to be a CentOS for hashicorp ?)I love the ethos of open source and have spoken at and helped run conferences, and had the pleasure of being paid to develop it - but the productivity I had when paid ten hours a day to work on OSS compared to whenever I get a chance between work family and everything else, well, it&#x27;s better for everyone to get paid and release code, than not get paid and not write the code.I see these semi-commercial licenses as the equivalent of a legal \"just don&#x27;t take the piss\".Would be interested in your side of the question. How do we keep on developing the code as well as keeping it open? reply paulgb 10 hours agorootparentI am a paying Pulumi user. Their tool integrates with a cloud platform and we pay per resource managed by Pulumi.Pulumi is one of several products where I like that it’s open source in case I need to move off their cloud, but hope that I don’t have to (Plausible is another). reply joeduffy 10 hours agorootparent[Joe, Pulumi Founder here.]Said well (and thank you for being a customer and valuable member of our community!)The analogy I draw sometimes is that our open source infrastructure as code SDK (\"Pulumi\") is like Git, and our commercial offering (\"Pulumi Cloud\") is like GitHub.Like GitHub, the Pulumi Cloud offers valuable features that go beyond the open source project for teams looking to manage lots of projects securely at scale, but we definitely love our open source community and want folks to have the choice to use Pulumi however makes the most sense for them.This approach also has the nice consequence that we can be fully transparent with our community at all times while also building a strong, long-term business. If a new feature is part of the infrastructure as code SDK, it&#x27;s open source and free; if it&#x27;s part of the Pulumi Cloud SaaS, it&#x27;s part of our commercial offering. This avoids needing to do things like artificially hold back features (like open core) or violating our commitment to the open source community (like Hashi&#x27;s new license). reply sanderjd 4 hours agorootparentSo here&#x27;s my perspective on these two competing models:1. I can read all of the code, modify it, and self-host it for my own purposes, but the license disallows me from re-selling it.2. I can read, modify, self-host, and commercialize a subset of the code, and the rest is an opaque SaaS.To me, as a customer with no interest in re-selling this code, I don&#x27;t see how #2 is better than #1 in any way. And I find it incredibly mystifying that I keep seeing companies ragged on here for doing #1 while the model in #2 is somehow held up as the paragon of ethical virtue or something.Can you help me understand? Why is it better for me to be able to read less of the code I&#x27;m running? reply hyperdrivetech 0 minutes agorootparentIt’s also fair to say this isnt black and white and open source is vast and there are certain software and companies where opting for #2 that definitely feel like a big rug pull, money grab, and smack in the face to the community that supported them. (Is red hat in my opinion)If you want to build something with a bunch of smart people for a long period of time the outcome is raising venture capital, paying people salaries that are competitive to share holders, and won’t implode. The bsl is a consequence of that but it is a rule to guard from the few bad apple in this case.What’s ethical or virtuous or perfect is very nuanced hyperdrivetech 10 minutes agorootparentprevConvenience and reliability from a business perspectiveFor #2 in good faith using the github model here,Sure there’s Git and Github. Also sourcehut, using google cloud source repository or any managed git service.Either 1) I need the software and I can have a team maintain it.Electing for the software-as-a-service vs self hosted model is in itself.1. I can compute, resources, maintenance and time The proprietary or a version of the product myself or a fork and get the feature functionality from other open source or provider.2. Pay for the GitHub licensing cost and using the service and ok with magic abstractions to operate the software. (Which admirably lately has been bad.Also frame it as from the beginning of the git project elected to also build all the same parity features, would it be the same tool, be the product that exists, or brain share it has today. Maybe not.I maybe misunderstanding you here but in these cases opaqueness is part your trade off to offload fairly complex for a marginal cost that’s s decision by you. reply pcthrowaway 7 hours agorootparentprevIs the Pulumi Individual Edition open for use by solo founders operating as a sole proprietership? I can&#x27;t find anything clarifying whether it&#x27;s individual (as in hobbyist, nonprofessional) or individual, as in, one person not collaborating with anyone else. reply ZiiS 3 hours agorootparentYes it is open to individual comercial use (companies much larger then sole proprieterships may only have one person doing inferstructure). They also have a version for nonprofits https:&#x2F;&#x2F;www.pulumi.com&#x2F;pricing&#x2F;open-source-free-tier&#x2F; reply pcthrowaway 2 hours agorootparentDo you have a source for that (or are you affiliated)? reply cnunciato 21 minutes agorootparentPulumi engineer here. There’s no restriction —- you’re free to use the Individual Edition commercially if you like. replytedivm 9 hours agorootparentprevPlausible is amazing, I love it. I moved off of another platform that started as open source and then went closed source, but Plausible ended up being a better platform over all anyways. reply vmatsiiako 9 hours agorootparentprevI&#x27;m a huge fan of Pulumi. After HCP&#x27;s license switch, I&#x27;m even more sure that Pulumi will be a clear winner over Terraform in the long term. reply Aeolun 9 hours agorootparentI really don’t think that was ever in doubt. You only need to use it for a very short time to find that the ergonomics are infinitely nicer than Terraform. reply hughesjj 9 hours agorootparentprev Ideology is great until people need to eat. That’s what revenue is forThat sounds like what the GP comment is saying. If someone said \"turns out open source doesn&#x27;t work for our business model\" it&#x27;s hard to argue with. If instead they talk about \"evolving open source models\" and whatnot, it feels like they want the best of both worlds. It&#x27;s been happening a lot recently that companies pretend they are \"open sourcing\" something for the PR but really use a much more restrictive license. reply toomuchtodo 12 hours agorootparentI argue the window is moving as to what “open source” means out of survival. Source available is the new open source, and what young technologists will grow up grinding on. You’ll have folks complain about it during the transition (as happens with any Overton window sort of event), but they’ll move on eventually and a new crop of tech industry will grow up with this as the new normal. Change is inevitable, broadly speaking. reply JoshTriplett 8 hours agorootparent> I argue the window is moving as to what “open source” meansOnly if we let it, and stop shouting about it and finding alternatives every time a company does this.This isn&#x27;t a new thing; companies have been trying to play \"almost open source\" games for decades, and they&#x27;ll continue playing those games as long as it either works or doesn&#x27;t have sufficiently large penalties for trying. (Much as companies will continue violating copyleft licenses as long as they either get away with it or the penalties for trying are simply an expected part of the risk.)The best possible response to a company doing this is that someone forks the code, starts or expands a competitor, and the original company&#x27;s revenue drops massively as a deterrent. reply toomuchtodo 8 hours agorootparent> The best possible response to a company doing this is that someone forks the code, starts or expands a competitor, and the original company&#x27;s revenue drops massively as a deterrent.Example of the last time this worked? reply riffraff 5 hours agorootparentJenkins&#x2F;Hudson?Oracle decided to make Hudson commercial, it was forked and Jenkins is still around but Hudson is dead. reply pjmlp 2 hours agorootparentMeanwhile Cloudbees has several product to sell you on.Turns out Jenkins development needs to be sponsored somehow. reply gkbrk 3 hours agorootparentprevElasticSearch? A lot of people moved to open source forks. reply pjmlp 2 hours agorootparentFrom where I am standing, no one cares they exist. reply BoorishBears 1 hour agorootparentprevI hate OpenSearch with a passion, an absolutely horrid lagging project that can&#x27;t get basic autocomplete working (https:&#x2F;&#x2F;github.com&#x2F;opensearch-project&#x2F;OpenSearch-Dashboards&#x2F;...)but still manages to suck the air out of the room when you want Elasticsearch because AWS already has the company&#x27;s billing details and no one wants to figure out paying another provider. reply yjftsjthsd-h 5 hours agorootparentprevI don&#x27;t know what the impact was on their revenue, but pretty much anything Oracle has ever touched. reply toomuchtodo 5 hours agorootparentBut still wildly profitable!https:&#x2F;&#x2F;investor.oracle.com&#x2F;investor-news&#x2F;news-details&#x2F;2023&#x2F;... reply pessimizer 12 hours agorootparentprev> I argue the window is moving as to what “open source” means out of survival.I don&#x27;t think this is happening at all. Open source means the same thing it&#x27;s always meant. Some people are just retreating from open source. Which is fine, they should be writing Free Software anyway if they want the world to have it, or use proprietary licenses if they don&#x27;t. Otherwise very wealthy people will live on your back. reply dragonwriter 2 hours agorootparentThe OSI Open Source Definition and the FSF Free Software Definition are for most practical purposes identical (and most licenses meet both or neither); historically, the Open Source and Free Software communities have somewhat different reasons for preferring the same thing, but the things are the same. reply hkt 1 hour agorootparentNot so: open source licenses tend not to have any clauses requiring reciprocation, free software licenses do. Think MIT or BSD vs GPL. reply account42 51 minutes agorootparentThat distinction is what makes copyleft licenses. Free software is just as overarching as open source, see e.g. the FSF&#x27;s list of free software licenses: https:&#x2F;&#x2F;www.gnu.org&#x2F;licenses&#x2F;license-list.html reply davidgerard 1 hour agorootparentprevI urge you to look up the open source definition at OSI. It doesn&#x27;t say that at all. reply JohnFen 11 hours agorootparentprevI agree. But there are an awful lot of younger devs who really do seem to confuse \"open source\" with \"source available\". It&#x27;s worth educating people about this. reply sanderjd 4 hours agorootparentSo, I don&#x27;t think this is a generational thing. I think most people of all ages and generations have mostly just not thought about this. But the reason more people are thinking about it now is that the distribution model has changed on a way that has highlighted an existential weakness with this model. reply sanderjd 4 hours agorootparentprevMy perspective is more like the parent&#x27;s. As someone who has grown up along with open source, I&#x27;ve found it surprising recently how up in arms people are about how critical the ability for anyone to commercialize a project is for the definition of open source. To me, I care a lot about whether I can see how software is implemented, and modify it for my own use, but it has never really occurred to me that I need to have the right to commercialize any arbitrary project.But :shrug: I guess different people care about different things, is what I&#x27;ve realized watching these discussions unfold.But I do think this purist perspective on open source is just going to result in more Snowflakes and fewer Hashicorps, because why bother with this fight? reply orra 2 hours agorootparent> But I do think this purist perspective on open source is just going to result in more Snowflakes and fewer Hashicorps, because why bother with this fight?Orgs like Hashicorp clearly think they benefit by pretending to be open source.They could simply stop being disingenuous about their source available proprietary software, and nobody would stop them. reply davidgerard 1 hour agorootparentprev> why bother with this fight?Because companies keep bringing this fight. reply umanwizard 6 hours agorootparentprev“Free software” and “open source” mean the same thing. reply riffraff 5 hours agorootparentprevThe window can&#x27;t move, as there is an official version of what \"open source\" means, the Open Source Definition, which does not restrict you from reselling stuff.We&#x27;ve had \"source available\" for a long time, which means something else.I don&#x27;t disagree that people may still use SA software more as time goes by, but I would argue that when possible people will prefer open source controlled by entities that keep it such. reply sanderjd 4 hours agorootparentThis is not how language works. The phrase \"open source\" will mean what people think it means. An organization with a lot of credibility and mindshare can affect that meaningfully by maintaining and explaining the official definition from their perspective, but they can never be guaranteed success in convincing people that their definition is what those words will mean forever. reply version_five 12 hours agorootparentprevI agree except I think it&#x27;s our of short term greed plus arrogance rather than survival. Maybe in some cases that&#x27;s not true, but when companies like Meta are championing pretend open source, it&#x27;s not existential for them, it&#x27;s trying to push for a world where they have more control. Like I said, I don&#x27;t have a problem with closed source business models, it&#x27;s the deliberate conflation that&#x27;s troubling, especially when it&#x27;s leveraged to get community contributions.On the other hand, if popular software becomes faux-pen source (I read that somewhere recently) and community members stop contributing, it&#x27;s a loss too because it means we all become takers on whatever company&#x27;s terms.Your almost certainly right about the window shifting, I&#x27;m going to keep complaining anyway. reply toomuchtodo 12 hours agorootparentI encourage you to continue to complain. Sometimes it’s the only way the rest of us have signal we might be wrong. reply davidgerard 1 hour agorootparentprev> I argue the window is moving as to what “open source” meansThis has been the case since the 2000s, as companies want the branding without the openness. This is extremely well worn by now.I argue that companies who want it both ways are continuing to throw up chaff. But we know this chaff extremely well.None of this discussion is new. \"open core\" has always been a euphemism for \"proprietary.\" reply seneca 8 hours agorootparentprevThis is what these companies want you to believe, that it&#x27;s a fait accompli and you just have to accept it. That&#x27;s not actually reality, and giving up words and communities to people who want to corrupt them is not the right reaction. reply 094459 12 hours agorootparentprevYup this is how I see things evolving too. It’s a long game though and I suspect there will be a few twists yet to come. reply cmiles74 12 hours agorootparentprevAs they mentioned, this is what the AGPL license is for. No one is suggesting that the people at HashiCorp should not be paid for their work.https:&#x2F;&#x2F;fossa.com&#x2F;blog&#x2F;open-source-software-licenses-101-agp... reply asymmetric 4 hours agorootparentIf they made their tools AGPL, they themselves couldn’t build a cloud offering with additional, closed-source features. reply MattJ100 3 hours agorootparentThat&#x27;s not true. As the copyright holder they are not bound by the licence that they release it to others under.The reason AGPL isn&#x27;t being adopted in these situations is that it doesn&#x27;t sufficiently protect against someone doing what e.g. AWS repeatedly does - turning open-source projects into services and then dominating the market while continuing to benefit from the upstream project. See the ElasticSearch licence change for a prominent example. reply ZiiS 2 hours agorootparentI am not sure being closed source is much comfort if AWS decide they want to crush you. reply mperham 3 hours agorootparentprevA license controls what OTHERS can do with your source. You, the copyright holder, can do anything you want. reply asymmetric 2 hours agorootparentThanks, I stand corrected. How does this play with third-party contributions? Others might be the copyright-holders of a sub-section of the code. reply duckmysick 1 hour agorootparentProjects can have a Contributor License Agreement (CLA). It gives the owner of the project a right to republish (or copyright) the contributions. You can&#x27;t contribute to the project without signing it. reply JimDabell 1 hour agorootparentprevYou ask contributors to sign a copyright assignment before accepting their changes. reply deno 3 hours agorootparentprevOnly if you accept contributions without a license grant CLA. reply _msw_ 2 hours agorootparentAnd this is why I think people who love Software Freedom should think twice about signing a CLA for their copyleft licensed contributions. [1]inbound=outbound license terms is a good norm for FOSS. Why should a software vendor play by different rules than everyone else when it comes to things like copyleft compliance?[1] https:&#x2F;&#x2F;meshedinsights.com&#x2F;2021&#x2F;06&#x2F;14&#x2F;legally-ignoring-the-l... reply bawolff 1 hour agorootparentFrom a pragmatic perspective, it is much easier to enforce an open source license with a lawsuit if you own 100% of it. replyMacha 11 hours agorootparentprevYou&#x27;re free to decide open source isn&#x27;t working for you. (Well, assuming you&#x27;re not using any open source software that has decided on viral licenses because that&#x27;s the payment _they_ expect)You&#x27;re not free to decide your source available model is open source and reap the marketing benefits of open source without the costs. reply pydry 10 hours agorootparentI think these projects should just dual license as AGPL and BPL&#x2F;EPL.That way all the \"it&#x27;s not really technically open source\" complainers couldn&#x27;t day that its not technically open source.It wouldnt substantively change anything of course, but that&#x27;s somewhat the point. BPL&#x2F;EPL&#x2F;SSPL was always fully within the spirit of open source, it just pissed off the same large corporations who also can&#x27;t stand the AGPL. reply Macha 10 hours agorootparentI&#x27;m way more fine with AGPL (without CLA). That&#x27;s perfectly within the spirit of open source, as it doesn&#x27;t privilege one group of users over another.BPL, EPL, SSPL are all \"not open source\", and AGPL+CLA is \"we&#x27;re setting up for a bait and switch with not open source versions\". reply bawolff 1 hour agorootparent> AGPL+CLA is \"we&#x27;re setting up for a bait and switch with not open source versions\".Which is fine imo as long as the moment they pull the bait&#x2F;switch they stop calling it open source (and others can fork at that point) reply Macha 12 minutes agorootparentI think I&#x27;d be fine _using_ an AGPL+CLA product, but not contributing. reply pydry 10 hours agorootparentprevI find it curious that Microsoft doesnt get more shit for demanding a CLA, especially given that embrace, extend, extinguish is in their DNA. reply JimDabell 1 hour agorootparentEven GNU projects ask people to sign a CLA. reply davidgerard 1 hour agorootparentprev> BPL&#x2F;EPL&#x2F;SSPL was always fully within the spirit of open sourceIt literally is not, and they only exist in order to not be. reply mfer 12 hours agorootparentprevLooking at the public data [1], Hashicorp looses money every quarter. At some point they need to stop burning cash because they have yet to figure out how to run a sustainable business.I don&#x27;t know enough about their operations to have good suggestions on how to become sustainable. But, I don&#x27;t like this move. There are many sustainable open source companies. Moving to source available from open source will likely never be a move I like.[1] https:&#x2F;&#x2F;www.google.com&#x2F;finance&#x2F;quote&#x2F;HCP:NASDAQ reply PoachedEggs 9 hours agorootparent> There are many sustainable open source companies.What are some examples? reply pcthrowaway 6 hours agorootparentSUSE, Canonical, Prisma (though their dashboard is closed source), Gitlab, pretty much every crypto company in the blockchain&#x2F;web3 space.And Hashicorp + Red Hat managed to make it work as open source companies for >10 years alsoThere are many more \"open core\" companies, like TimescaleDB, Docker, etc, who then sell proprietary services on top of the open source software reply sebastianz 3 hours agorootparentWhy do you think \"open core\", which most of those are, is somehow better than BSL?> pretty much every crypto company in the blockchain&#x2F;web3 spaceThe entire \"space\" has yet to prove to be sustainable. Most of these are hype-driven borderline scams, I would not list them in a _sustainable open source business_ context :) reply pcthrowaway 2 hours agorootparent> Why do you think \"open core\", which most of those are, is somehow better than BSL?Firstly, I don&#x27;t have a problem with BSL. I do think in general it&#x27;s a bit of a slap in the face to build a business on the backs of volunteer contributors, and then close-source all your codebases which are comprised of their work.Sure, when people contributed, they signed a CLA which gives Hashicorp the right to relicense the work (which has legitimate uses outside of killing open source, such as giving them the ability to make that software available under other terms in addition to their open source offering)But when as little as a year ago they promised \"We remain committed that the core of our technology will always remain open source.\" (https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20220703202305&#x2F;https:&#x2F;&#x2F;www.hashi...)it gives whiplash to the people who contributed based on that promise.Actually, I don&#x27;t even know if this is legal, but even if it is, it&#x27;s a huge violation of the trust of outside contributors to their software products.> The entire \"space\" has yet to prove to be sustainable.I agree that it&#x27;s unproven, but this downturn has made apparent that so are the majority of software companies which have not IPOed and shown a sustained profit for 5+ years.I&#x27;d give Uniswap pretty good odds of outliving the majority of YC startups. reply sebastianz 1 hour agorootparent> I&#x27;d give Uniswap pretty good odds of outliving the majority of YC startups.The majority of YC startups are definitely not sustainable.Probably even most of the successful ones are not sustainable, they make empty sustainability promises hoping to get bought byand end up on \"our amazing journey\" lists. reply sebastianz 1 hour agorootparentSorry for veering off-topic there. I agree that re-licensing and BSL&#x2F;AGPL are muddy-territory and I&#x27;m also not sure how to feel about them.But a surviving project moving to BSL is still clearly better then the company going under and the project ending in limbo.Even with BSL you can still access the code, audit it, learn from it, fork it to keep your own patches on top and keep up with upstream, etc. Huge value compared to closed source. replyZiiS 2 hours agorootparentprevWhat is so frustraiting is their model seems sound; they just had rediculusly high pricing. reply candiddevmike 10 hours agorootparentprevIf this doesn&#x27;t move the needle expect more increases to their licensing. Though I don&#x27;t know how it could become more expensive. reply jsight 10 hours agorootparentprev> Ideology is great until people need to eat. That’s what revenue is for.It isn&#x27;t just the need to eat. There&#x27;s also the issue of keeping investors happy and their continual drive to maintain growth or earnings at stratospheric levels.Strict IP laws are the only safe way to do that, and that is why so much software has leveraged them over the years. The internet era felt like an aberration for a while, but things seem to be shifting back to high double digit margins as the only desirable goal. reply davorak 11 hours agoparentprev> This is purely a way for HashiCorp to ensure they are the only ones who can commercialize these formerly open source projects. Which is fine. But just go closed source, then, and own that, instead of trying to have it both ways.Pragmatically I would rather bsl than closed source and I am more likely to use a product that is bsl, with reasonable transfer time and license, than a 100% closed source product. reply Rapzid 10 hours agorootparentI wish I could give you more upvotes.I&#x27;d also much rather have \"open source\" with commercialization restrictions than closed source.It&#x27;s still in most people&#x27;s best interest to contribute to these projects if they were before, or would have before. Many businesses(and this is where most of the contributions get funded from, let&#x27;s be real) rely on these projects and have no intention of selling them or competing with HashiCorp services. reply ramses0 7 hours agorootparent\"Source Available\" please. reply JoshTriplett 8 hours agorootparentprevI&#x27;d rather have proprietary than \"almost open source\". Both aren&#x27;t useful, but only one attempts to damage the common understanding of what \"Open Source\" means. reply LexiMax 7 hours agorootparentWhy? To me, the BSL comes off as a good faith attempt at a compromise between the letter of \"Open Source\" and the realities of not wanting to give free labor to your competition.The actual text of the BSL mandates - under threat of infringing on BSL&#x27;s trademark - that in at most four years the code will be available under a GPL 2.0 compatible license. In practice, the BSL license is usually a traditional open source one with caveats. The BSL FAQ also states and restates many, MANY times that it is not an open source license according to the OSI&#x27;s definition.I can&#x27;t help but feel like the outcry over this is just a tempest in a teapot. I have a hunch that \"Open Source\" will do just fine without us having to carry water for it. After all, the list of OSI&#x27;s corporate sponsors is quite illustrious: https:&#x2F;&#x2F;opensource.org&#x2F;sponsors&#x2F; reply sanderjd 3 hours agorootparentprevWhat? Why? I don&#x27;t get this at all. Like, the direct pragmatic benefits of being able to read and modify source code are just enormous. The benefits of maintaining this pure definition of open source are amorphous at best, in comparison. reply JoshTriplett 3 hours agorootparenthttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Focal_point_(game_theory)If we don&#x27;t have a common definition, everyone has their own, and there&#x27;s no common understanding of what rights you have for a piece of Open Source software. That commons has far far more value than any one or ten pieces of software. reply sanderjd 3 hours agorootparent> That commons has far far more value than any one or ten pieces of software.I don&#x27;t think so.I suppose I can understand why people who feel strongly that the OSI definition is perfect (or at least extremely good) are very intent on protecting it, whereas I see it as flawed and am thus less concerned about this fracturing. So I understand your perspective upon reflection, though I honestly have a lot of trouble imagining holding it myself.A lot of people in this thread have been saying stuff like \"just go full proprietary, that&#x27;s better\", and it sounds ridiculous to me every time I read a variant of it reply CrimsonRain 1 hour agorootparentprevYou do realize that it is people like you who are turning FOSS in to OSS, then claiming BSL (and non-OSI Stuff) are not OSS and blaming others for damaging understanding? reply davidgerard 1 hour agorootparentthey literally aren&#x27;t. These are jargon terms with precise meanings. reply davorak 8 hours agorootparentprevI do not call anything under BSL open source. I would prefer if companies when presenting the BSL talk about their schedule to open source, the schedule being when the Change License takes effect, at least if the Change License is an open source one. reply thayne 6 hours agorootparentprevI mostly agree. But I also think it is a jerk move to change the license like this after accepting many external contributions, even if it is legal due to CLAs.At least they admit it isn&#x27;t open source in the FAQ and are calling it the community version instead of the open source version. reply LambdaComplex 12 hours agoparentprevBased on multiple previous employers of mine, it seems like software companies start noticeably going downhill about 1.5 years after they go public. Let&#x27;s check Wikipedia and see how I did:> On 29 November 2021, HashiCorp set terms for its IPO...I&#x27;m starting to think I&#x27;m onto something. (I do welcome anecdata that either helps or hurts my hypothesis) reply FridgeSeal 12 hours agorootparent> it seems like software companies start noticeably going downhill about 1.5 years after they go public.I firmly believe this is a fact too.One place I recently worked, I joined as it was going public and it went downhill quickly. The friends I’d made there talked about the fun perks, holidays and benefits the company was known for. Over the space of less than a year most of the culture atrophied, people left in droves and there were exactly zero holidays or perks given out. reply klardotsh 11 hours agorootparentI hate to say it but this feels inevitable. We&#x27;ve accepted that the requirement (legally!) of a public company is to deliver the maximum possible returns to investors, and as such, employees become a cost center to optimize away, and just generally, anything that negatively impacts the quarterly report must be eliminated, even if that thing is the only thing that will keep the next quarterly report in the black.Quarterly scope for fiscal data is one of the most short-sighted decisions humans have ever done. Expecting quarterly up-and-to-the-right, where simple sustenance is not enough, but profit must grow quarterly, on a planet with finite resources in an economy with finite money, is a guaranteed, zero-exceptions, recipe for failure, by definition. reply scarby2 10 hours agorootparentFiduciary responsibility isn&#x27;t entirely about returns, it&#x27;s about the best interests of the investors. And if you&#x27;ve invested in a company with a certain mission it would be said to be in your interests for that mission to continue.If I were starting a business these days I&#x27;d be tempted to found it on the basis that 60% must be owned by employees of the company until the last living descendant of king Charles dies. reply dudus 9 hours agorootparentprev1.5 years?Their stock was down 60% only 3 short months after IPO.I think this is just a struggle to turn what was once technical excellence into something that gives money. I haven&#x27;t followed HashiCorp lately but was once a fan of some of their products. These days it seems things are slower over there. At least that&#x27;s what it feels at a distance. reply wmf 8 hours agorootparentThey IPOed at the peak of the ZIRP bubble. There&#x27;s nowhere to go but down. reply scarface_74 9 hours agorootparentprevHow many of those companies were profitable before going public? reply LambdaComplex 8 hours agorootparentAre you implying that it&#x27;s impossible for a company to be both profitable and have a good internal culture?That&#x27;s a scary thought. reply scarface_74 8 hours agorootparentNo I’m saying most tech companies went public without being profitable and that had more to do with the declining stock price. reply foobiekr 4 hours agorootparentIt&#x27;s not weird at all to go public before profitability, that was even the standard in tech for the longest time. The IPO was a _fundraising_ event, not a dump-the-company-on-the-public-and-move-on event. reply scarface_74 20 minutes agorootparentAll of the current BigTech companies except Amazon were profitable before going public - Apple, Microsoft, Google, Facebook.The vast majority of tech companies that IPOd since Facebook have been a disaster of an investment. replypessimizer 12 hours agoparentprev> Which is fine. But just go closed source, then, and own that, instead of trying to have it both ways.The opposite of open source isn&#x27;t closed source, the opposite of open source is restrictive. You&#x27;re not forced to refuse to let people see the source when you&#x27;re not open source. You&#x27;re not forced to eliminate everything that OSI-approved licenses must have if you&#x27;re not OSI-approved. There are no OSI cops that bust proprietary vendors for using a subset of their characteristics.Of course it would be better if it were Free software, but it would be better if all software were Free software. They&#x27;re doing them.edit: My objection comes when people pretend licenses are open source when they are not OSI-approved and couldn&#x27;t be. HashiCorp is not claiming to have remained open source: they&#x27;re now \"source-available.\" reply fasterik 12 hours agoparentprev>Which is fine. But just go closed source, then, and own that, instead of trying to have it both ways.This seems too black and white. Don&#x27;t their customers get value from having source code available, even if there are restrictions on how that source code can be used? reply dragonwriter 1 hour agorootparent> Don&#x27;t their customers get value from having source code available, even if there are restrictions on how that source code can be used?For the most part, no, the main direct customer benefit comes from the absence of lock-in with regard to maintenance and services thar results from the absence of usage restrictions.There&#x27;s some potential indirect ecosystem benefit for customers from the somewhat lower friction for partners in some source-available-but-use-restricted situations, but otherwise for most customers its the same as any other proprietary license. reply AYBABTME 3 hours agoparentprevHaving it both ways is what I wish for them, as a user. I want their source, I want to be able to use it, I want them to sell it, and I don&#x27;t want some copycat to undercut them.Open-source isn&#x27;t a gospel, it&#x27;s a religion to some but not the end of the story in terms of what humanity can come up with. God(s?) didn&#x27;t stop at \"closed or open source\". We can find alternatives while aiming for ideals. reply unknownian 8 hours agoparentprev>like the AGPL?As I explained in an earlier thread, MongoDB tried using AGPL. AGPL is not a barrier for Amazon, they still will resell your product without contributing. MongoDB ended up using a variant of AGPL that is even stricter (requiring the entire tech stack to be under the same license) but is no longer considered FOSS. Until the attitude changes around what FOSS is, this will keep happening. reply thayne 6 hours agorootparentUm. Mongodb changed its license before AWS offered a mongodb compatible service. And since I can&#x27;t get the source code for documentdb, either it isn&#x27;t actually using a fork of mongodb, or Amazon isn&#x27;t complying with the AGPL. I think the latter is pretty unlikely. reply _msw_ 4 hours agorootparentDisclosure: I work for Amazon.AWS never offered MongoDB as a managed service, or used any of their server software when it was licensed under APLv3, or SSPLv1.However, we have contributed patches to MongoDB even after their license change to improve its performance on Graviton processors. Because that&#x27;s what&#x27;s good for customers, and MongoDB is an important customer and partner.AGPLv3 gives all the permissions needed to offer software as a manged service, just like every other FOSS license does. Unfortunately, in my personal opinion, the license has been co-opted by companies that do not care about Software Freedom, and rather hope that companies fear the license so they choose an alternative commercial agreement [1]. I don&#x27;t think that&#x27;s good for the community.[1] https:&#x2F;&#x2F;sfconservancy.org&#x2F;blog&#x2F;2020&#x2F;jan&#x2F;06&#x2F;copyleft-equality... reply pabs3 3 hours agorootparentDoes Amazon have any policies against offering AGPv3 software as a service?Does Amazon contribute funding back to the software projects they offer as a service?Does Amazon contribute code changes back to the software projects they modified when offering them as a service? reply _msw_ 2 hours agorootparent> Does Amazon have any policies against offering AGPv3 software as a service?Each use of AGPLv3 licensed software has to be reviewed to ensure that the obligations of the license can be and will be met (and also screen for cases where it is known that the vendor of software does not prefer a company like Amazon import the software under a FOSS license). Today we use AGPLv3 licensed software internally, and include AGPLv3 licensed software in Amazon Linux (Ghostscript, in particular).> Does Amazon contribute funding back to the software projects they offer as a service?Yes, in varying ways. For example, it is not easy to provide \"funding\" for something like Apache Kafka. You need to have people working on the upstream.> Does Amazon contribute code changes back to the software projects they modified when offering them as a service?Yes, but not all changes are appropriate for upstream. reply pabs3 2 hours agorootparentThanks for the answers, very interesting. reply yjftsjthsd-h 5 hours agorootparentprevIt&#x27;s a little funny in this context, but allow me to pull this out from my quotes file:> Their proprietary license protecting their code set competitors and intentional clones back days, weeks or months ... years ago.- benologist, https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=17454032If AWS decides to copy your product, going closed-source or source-available just means they have to copy it from design docs or protocol specs. That&#x27;s more friction than being able to reuse code outright, but it&#x27;s not going to stop them. reply bostik 3 hours agorootparentprevMongo also offers a hosted, paid product (Atlas) directly on AWS. Which I think is pretty smart of them. reply wmf 8 hours agorootparentprevAGPL is not a barrier for Amazon, they still will resell your product without contributing.I don&#x27;t think this is true. reply orra 2 hours agorootparentprev> but is no longer considered FOSS.It&#x27;s no longer considered FOSS because it&#x27;s no longer FOSS.> Until the attitude changes around what FOSS is, this will keep happening.That&#x27;s a weird thing to say. You&#x27;re happy with it happening, and everybody else using bad definitions won&#x27;t change that. reply drdaeman 5 hours agorootparentprevI heard there was a lawsuit about this, but can&#x27;t find the outcome. Can someone please enlighten me how that story ended (if it had - but I think it should, it&#x27;s been quite a while ago)? reply rcme 12 hours agoparentprevSource-available is still hugely beneficial to users, even if it’s not open source. reply falcolas 12 hours agorootparentClosed source can (and does, see Windows) provide source to customers too. reply pessimizer 12 hours agorootparentNo, if words have meaning, \"closed source\" does not provide source. Closed source does not mean \"not open source.\" reply falcolas 11 hours agorootparentWell then, according to the meaning you are assigning these words, Windows must be open source.Which, of course, is obviously false. Which means the words have different meaning than what you ascribe to them.Source available means you can view the source code, by abiding to the terms the owner of the code sets.Closed source means you can’t change or redistribute that source, regardless of whether you are allowed to view it. reply ZiiS 2 hours agorootparentThe windows source is not avalible to _me_, even if they show it to some people. It is useful to have a term for source available to everyone. reply CrimsonRain 1 hour agorootparentprevSo where can I view windows 11 source code? Got a link? reply pseudalopex 10 hours agorootparentprevWindows would be not open source and not closed source according to them. I think most people would call it closed source however. reply rcme 9 hours agorootparentIt’s obviously a spectrum, but (some) people think of different things when they hear “closed source” vs. “source available”. Also, a company like Microsoft providing source to big customers doesn’t make their products source available, at least not in the commonly understood way. Source available usually means the source is freely available for reading and often compiling. That obviously does not apply to Windows. replydi4na 3 hours agoparentprevI will say.All that it achieve is making sure noone can build a better product. Aka \"yes our product is crap in a lot of places, but we don&#x27;t want to fix it. Better extract licensing fees by locking you all with us\".The behaviour of dying companies. Which make sense. Their business model never worked. reply da768 12 hours agoparentprev> without providing material contributions backThey need more open PRs? It&#x27;s not like these contributions would necessarily be welcome. reply bawolff 1 hour agoparentprev> This is 100% in the spirit of open sourceNot just in the spirit but a fundamental tenant. reply afiori 12 hours agoparentprev> This is 100% in the spirit of open sourceThat is the spirit of Free Software (ie restricting users as little as possible) Open Source is much smaller in scope. reply riffraff 4 hours agorootparentThe open source definition explicitly says that there must be no restrictions on reselling.So restricting competitors is as much against the OS spirit as it is against the FS spirit. reply afiori 2 hours agorootparenttouché reply andrewstuart2 11 hours agoprevThat&#x27;s pretty disappointing. I personally haven&#x27;t used much beyond vault (I&#x27;ve used but not enjoyed or built anything on terraform), but this is pretty diametrically opposed to what I appreciated most about hashicorp products. Heck, I&#x27;ve even contributed a chunk of the code I use the most from vault (Cert management) and now I&#x27;m going to have to reevaluate whether I can attempt to use that service for customers going forward, and whether I will contribute ever again.It definitely feels like the whole era of VC drying up is bringing out the worst possible future for some of these non GPL&#x2F;similar licenses. Which is unfortunate for any of us who have deliberately learned only OSS and operations around it, giving back the whole time, with dreams of building services that leverage that knowledge someday as a chance to be our own boss while also utilizing and giving back to the OSS that got us this far. reply JohnMakin 7 hours agoparentI read the rest of your comments on this topic and I’m sorry this happened to you.I have extensive experience with enterprise vault, implementing and managing it across a company infrastructure to manage application secrets, and during the few years we implemented vault and was in negotiations about our contract, I noticed the sales engineers would1) be dishonest or misleading about features “needed” for our user case or make long-term promises they couldn’t possibly keep about features. standard salesmanship stuff but was very aggressive.2) encouraged an integration style that would make migrating out of vault practically impossible, if not outrightly dangerous3) continually rug pulled features we thought would be free forever (okta&#x2F;mfa login being the biggest one I can think of). You can’t pass any serious compliance without that, and they realized anyone heavily relying on vault for secrets management would absolutely have to pay for this feature.Basically it just seemed like hard core vendor lock in and every year our bill would be a lot higher for essentially the same or fewer features. Not to mention nonsensical pricing that even their own engineers can’t explain and changes constantly and arbitrarily.So all this to say sorry this happened to you and for Vault specifically I am not surprised and would personally not rely on it for anything serious, even though I personally consider it fantastic software - I simply lost trust in hashicorp. reply chucky_z 7 hours agorootparent3 is a confusing one but understandable.You should be using the OIDC login method most of the time for MFA, and not their built-in MFA.I’m unsure if the equivalent software is worth the price when compared to Vault and not sure I can seriously suggest anything else even if I hate this new license. reply JohnMakin 7 hours agorootparentYou can use a mix of secrets manager and certificate manager products in AWS and accomplish essentially the same things Vault promises for much cheaper (and easier to manage).I’m underselling of course the vast capabilities of vault. but most companies don’t need those advanced features, and they don’t really sell them, they sell and lock you into features that once you implement are going to become an extraodinary hurdle to migrate out of.On the oidc - yes we were using okta as that. but at some point mid-contract the “okta” management features that connected to it became enterprise only, and we had reasoned that if we didnt need more advanced features (dr, replication) we could go back to OSS when we wanted. In fact that was even told to us, until that was no longer the case. reply Salgat 6 hours agorootparentAWS Secrets Manager was so easy to setup. With implicit auth using IAM roles on our EC2s and the aws sdk I was able to add secrets support in literally a day for all our services. reply JohnMakin 5 hours agorootparentand arguably more secure than self-hosted vault for this same purpose. reply andrewstuart2 7 hours agorootparentprevDefinitely appreciate your comment, so thank you. It means a lot.I think fortunately, I&#x27;m not doing all that much with vault aside from initializing it, configuring k8s auth, writing some small policies depending on some inputs for the specific user, and then setting cert-manager up to use it. It&#x27;s definitely one of the more simple projects to rip and replace.But it&#x27;s frustrating, for sure. I got some of my coolest code set up to unseal and initialize vault programmatically, and was hoping to eventually get it to a point where I&#x27;d be able to orchestrate that on a user&#x27;s behalf without having control myself, via e2e crypto. Maybe with another CA project I could achieve the same thing. But yeah, looking at the new license file in the vault project, I&#x27;m not sure how well any of this would work out if my code was orchestrating it. And certs are pretty fundamental to the project. reply randmeerkat 10 hours agoparentprev> That&#x27;s pretty disappointing.From the article: “End users can continue to copy, modify, and redistribute the code for all non-commercial and commercial use, except where providing a competitive offering to HashiCorp.”Literally nothing has changed, this isn’t disappointing, it’s smart, they’re protecting themselves against cloud providers that have repeatedly abused the goodwill of the open source community. reply andrewstuart2 10 hours agorootparentMaybe you missed my last sentence. I&#x27;ve been hacking on and off for a couple years on a side project I&#x27;d like to monetize, to capture some of my value add, while also giving back. (It&#x27;s sorta \"if you build it they will come\" at this point tbh so I don&#x27;t necessarily expect it to work). My project is sort of \"OSS platform as a service\" only I just deploy it for you and teach you to run it yourself, while jumping on a call occasionally if you need SRE for it, and continuing to iterate on tooling as well as make PRs to the tools as it makes sense. Vault and consul (as a vault backend only) are components I&#x27;ve used for that (via cert-manager so they&#x27;re replaceable tbh) and I&#x27;m no longer sure if that&#x27;s viable.And generally as a contributor to the vault codebase, however small, I&#x27;m not thrilled they want to capture more value from it themselves while not offering me a miniscule chunk of that.The whole cloud provider argument really feels a lot like Displaced Aggression. You&#x27;re probably punishing the people smaller than you a lot more than you are the billion dollar cloud providers who can afford both expensive lawyers and can very easily afford to fork your codebases as we see with OpenSearch vs ElasticSearch. reply vmatsiiako 10 hours agorootparentIf so, you can check out Infisical (https:&#x2F;&#x2F;github.com&#x2F;Infisical&#x2F;infisical) as an open source alternative to Vault. The absolute majority of our codebase is licensed under MIT and we have no intentions to change that.Disclaimer: I&#x27;m one of the founders. reply thayne 8 hours agorootparent> we have no intentions to change thatI suspect that Hashicorp would have said the same thing a couple years ago. reply andrewstuart2 9 hours agorootparentprevI&#x27;ll definitely check it out. That said, I&#x27;m starting to feel a lot more skeptical of the ability for even founders to manage stuff like this. I would say the same of my own OSS as a \"founder,\" but if my company controls it in some way then I&#x27;m not sure there&#x27;s a reasonable way for me to ensure that continues in perpetuity. At least not via a split model like a lot of these recent news stories have revolved around.From what I&#x27;ve seen of Mitchell as well, at least in the past, I kind of doubt this is something he would have gone through with on his own. reply civilitty 6 hours agorootparentI think the easiest way to manage it is essentially to do nothing. Accept open source contributions without a contributor license agreement and their copyright locks in future maintainers, yourself included. Extricating those contributions eventually becomes impossible without a cleanroom rewrite that is usually economically impractical and way too risky to a business with revenue. reply nyanpasu64 4 hours agorootparentThis requires a copyleft license, and can be bypassed if all contributors agree to sign away their code to a company trying to relicense and monetize the code (as the Audacity contributors did for some reason). reply Operyl 9 hours agorootparentprevI am absolutely a huge fan of your company providing 30 minute walk throughs of the codebase for new contributors, I have never seen that before! reply brunoqc 6 hours agorootparentprev> The absolute majority of our codebase is licensed under MITWhat is not MIT licensed?When you self-host, do you have access to every features for free? reply TheRealPomax 10 hours agorootparentprevBut your contributions stopped being \"your\" contributions the moment you signed off on them being merged into the vault codebase. Why would they owe you anything when you already indicated you were cool with the fact that you didn&#x27;t want anything in return by contributing?This change protects the project from getting outright shut down because huge companies use it to extract value without some of that value going into guaranteeing the project stays supported. If you contributed to it, the minuscule chunk you get is \"it keeps existing and you get to keep using it\" instead of \"this is not worth our time, we&#x27;re sunsetting this\". reply andrewstuart2 9 hours agorootparentI guess you can chalk that up to naivety on my part. I&#x27;ve always assumed there is a social contract on top of the CLA I probably signed, that the software I wrote would continue to be available and maintained via contributions from both the company and community. And since they very obviously benefit from a plethora of OSS themselves, including the language they&#x27;ve used to build their products and the platforms they undoubtedly run on.I guess I&#x27;m always free to fork the codebase I care about under its current license and try to build a community around that. But I think we all know that&#x27;s not as viable.Anyway, I&#x27;ll just be reconsidering my use of software open sourced by companies, I guess, regardless of how permissively it&#x27;s licensed. The free lunches I thought we collectively agreed were awesome and ought to keep helping each other provide, are apparently ending as money gets tight. reply tedivm 9 hours agorootparentJust because they have the legal right doesn&#x27;t make it ethical, and even if someone makes the argument that it is ethical that doesn&#x27;t change that it&#x27;s a bit of a slap in the face to the open source community. You&#x27;re allowed to be annoyed by this. reply hughesjj 9 hours agorootparentprevSo, I don&#x27;t understand -- had you known that this license change to prevent competitor use of their product, would you not have bothered to add the functionality you contributed? reply andrewstuart2 9 hours agorootparentNo idea. I contributed 8ish years ago when vault was a significantly smaller project (it was 9mo old at the time) and IIRC there was nothing like hosted hashicorp back then. I just wanted to put into vault a CA keypair I used easyrsa to generate and that didn&#x27;t work without a good deal more crypto plumbing, and I tried to make it a bit more futureproof while I was there. I had no real idea that 8 years down the road I would be tired of corpo life and tired of having to fight to contribute to OSS and might want to earn money in that sphere.Today, absolutely. I would simply choose another piece of software to build on and contribute to. Or build my own if I thought something open enough and good enough didn&#x27;t exist yet. reply bootsmann 1 hour agorootparentprevYou can ask them to license vault to you under different terms, they go quite in-depth about this in their FAQ.Don&#x27;t know how much will come of it, but it is worth a shot. reply 38 9 hours agorootparentprev> I&#x27;ve been hacking on and off for a couple years on a side project I&#x27;d like to monetizeOK so you want to use their software, make money off it, and give nothing back.if thats the case, you cant do that any more. you can either stick to personal use, or purchase a commercial license from them. reply andrewstuart2 8 hours agorootparentSeems like you missed the part where I literally typed \"giving back.\" Or that I literally contributed part of the hashicorp codebase, specifically vault. And it&#x27;s been hard continuing to do that at $DAYJOB consistently, so I&#x27;ve hacked on a side project in my spare time (also open sourcing plenty of useful tools during that hacking) as a means to the end of eventually finding ways to keep giving back directly and teaching others to do the same. reply 38 8 hours agorootparentwith all due respect, unless \"giving back\" means giving them money, its probably not worth what you think its worth. I maintain some small projects, and most of the people \"giving back\" contribute such a tiny amount of code that its almost not worth mentioning.that might not be your situation, but I know as a maintainer, in most cases I would much prefer a monetary contribution than a pull request. edit, 2015, ouch:https:&#x2F;&#x2F;github.com&#x2F;hashicorp&#x2F;vault&#x2F;commits?author=andrewstua... reply andrewstuart2 8 hours agorootparent> edit, 2015, ouch:> https:&#x2F;&#x2F;github.com&#x2F;hashicorp&#x2F;vault&#x2F;commits?author=andrewstua...Not sure what you&#x27;re getting at with your edit. I&#x27;ll try to assume positive intent.I maintain quite a few projects as well, also pretty small. Code to me means a great deal more than a small amount of money. The money is nothing compared to what I&#x27;ve made in my career thanks almost entirely to the code that exists publicly and my ability to run and modify it as needed to learn. I am glad to get code because it tells me something is useful enough for someone else to bother, which to me is what giving back is all about. reply 38 7 hours agorootparent> The money is nothing compared to what I&#x27;ve made in my careerright, but thats not the case for everyone. you have been fortunate, but for many they cant even pay their bills with the tiny donations that come in. hence why the need arises for a license like this. to force people to either go away, or pay up.as you&#x27;ve noticed, its not ideal. in a perfect world I would license my code without restriction, but I need to pay rent like everyone else. reply aatd86 7 hours agorootparentprevImagine that someone see your open-source code and creates a competitor product by assimilating it... Imagine that this entity is much bigger than you even...I don&#x27;t think you&#x27;d be happy, would you?I know I wouldn&#x27;t be... :-) reply aatd86 1 hour agorootparentPeople downvote but from a strategic point of view it makes no sense at all.It&#x27;s all good because usually open-source is an investment for bigger companies that they can recoup somewhere else.But for small-players, it makes absolutely no sense to create a business and also make it easy for competitors to compete or even for competitors to appear.But people probably realize that otherwise there wouldn&#x27;t be so many debates.Who funds open-source? (not talking about source-available but the most permissive Open-Source licenses) reply ms4720 7 hours agorootparentprevLike Free bad? reply ms4720 7 hours agorootparentShould be FreeBSD, damned Auto correct replykmeisthax 6 hours agorootparentprevAdding a non-compete clause to your license is not \"literally nothing\" - in fact, it might be extremely problematic for a large number of downstream users.As for \"abusing the goodwill of the open source community\", that&#x27;s kind of the point of FOSS. Free riding is not stealing. That&#x27;s proprietary world logic, and everyone saying we need to stop people from free riding FOSS is calling for the enclosure of the commons.Let me be perfectly clear: there is no license condition you can put on software that will let everyone use it as if it were in the commons but prevent Amazon Web Services from hosting it. reply piaste 4 hours agorootparent> Let me be perfectly clear: there is no license condition you can put on software that will let everyone use it as if it were in the commons but prevent Amazon Web Services from hosting it.\"The following licence is granted to everyone except the following entities: Amazon, Alphabet, Apple, Microsoft, Meta, Oracle\" reply Vespasian 1 hour agorootparent\"Today Amazon introduces the AWS Partnered Software Supplier Programm. Selected companies are invited to offer their products as deeply integrated AWS services.The first announced Parter are: Not-at-all-amazon-for-hashicorp inc, Not-at-all-amazon-for-mariadb inc.\" reply mindB 10 hours agorootparentprev>Literally nothing has changedThis is super disingenuous in a world where things like the GPL exist and any other license that prevents you from putting further restrictions on the combined product. reply jchw 9 hours agorootparentprev>Literally nothing has changed.Uhm, yeah, something DID change: the license and terms. I don&#x27;t understand what kind of argument this is. reply growse 4 hours agorootparentprevFor me, bait-and-switching their code contributors is almost the textbook definition of \"abusing the goodwill of the open source community\". reply thayne 7 hours agorootparentprev> they’re protecting themselves against cloud providers that have repeatedly abused the goodwill of the open source communityThis seems a lot more likely to be targeting other startups that build on terraform like spacelift, env0, maybe pulumi (although I think they interface with providers directly, so this might not affect them as much), etc. And maybe there are similar companies for their other offerings, although I&#x27;m less familiar with those. reply ugh123 10 hours agorootparentprev> they’re protecting themselves against cloud providers that have repeatedly abused the goodwill of the open source community.e.g. AWS -> Elasticsearch. reply adrr 7 hours agorootparentElasticsearch is based off lucene so they are making money off another open source project. reply paulddraper 5 hours agorootparentTechnically true, but practically different.Lucene is the core piece but %-wise the minority of the ES product.There&#x27;s no Lucene \"competitor\" to Elasticsearch. reply Xylakant 4 hours agorootparent> There&#x27;s no Lucene \"competitor\" to Elasticsearch.Apache Solr, which existed long before Elasticsearch was conceived. reply nostrebored 9 hours agorootparentprevAbsurd. Providing a managed instance of open source software that’s complicated to manage is fine. It’s not AWS’s fault that Elastic did a bad job of selling into AWS accounts. They should have worked on their value prop. reply ericb 9 hours agorootparent> They should have worked on their value prop.AWS customers like that there&#x27;s one bill and one account. Who wants to deal with multiple vendors if they don&#x27;t have to? It isn&#x27;t a level playing field when Amazon offers a service.Amazon could have chosen a cooperative, long-term, strategy and shared some revenue with the authors for some quid-pro-quo, and the world and Amazon, would be better for it. Instead, Amazon chose to cook the goose that lays the golden eggs, now they have to figure it out themselves, and whole ecosystems of services they could have hosted are running away from them. reply jen20 7 hours agorootparentThe reason historically it hasn’t been a level playing field is because Amazon have:- The ability to invent new infrastructure to suit a need. For example, multi legged ENIs that provide the EKS control plane are simply not available to others,- The ability to integrate with IAM natively,- The ability to build common network architectures without outrageous costs (traffic over peering links being a good example since that is what basically all vendors have to do). reply sanderjd 4 hours agorootparentThis is overthinking it. For nearly every given service, Amazon can just slap together a managed version and it will be the easiest one for people to discover and use if they are already using AWS. It doesn&#x27;t require any of that special sauce fanciness. Most of their managed offerings are mediocre also-ran versions of things, but they are just easier to use within the existing ecosystem, so they win. reply sanderjd 4 hours agorootparentprevWhat I find absurd is the perspective that it is 1. a giant no no breach of the spirit of open source software to provide software with open source code but a license that restricts commercial resale, and 2. it&#x27;s totally great for megacorporations to reap the vast majority of the rewards that accrue to popular mature open source software services.I honestly can&#x27;t fathom this worldview or how so many people here seem to be so sure it is the one that makes sense.If you win this battle for mindshare, people will just stop making open source services like these. Every service will just be fully closed source SaaS. Amazon can&#x27;t just re-sell that no matter what, and people just crap on you if you make it open source (edit: re-reading this, should have said \"source available\" here), so why would anyone bother?Increasingly people who make open source software services are damned if they do and damned if they don&#x27;t, and if that is how it&#x27;s gonna be, people are going to just stop bothering with it. reply j1elo 9 hours agoparentprevThe huge difference is where the copyright of the code lays. OSS projects that require contributors to assign their copyright away, should not be trusted, and should not receive goodwill contributions to begin with. Otherwise, what today is Apache 2.0, tomorrow can become Commercial, while asking nobody for permission, because the maintainers have ownership of 100% of the code.Not that OSS projects backed by commercial-driven entities usually receive any meaningful amount of contributions from external people... but still, an important detail to think about OSS. reply andrewstuart2 8 hours agorootparentIt&#x27;s super frustrating, because I want to assume the best, but I&#x27;m starting to agree more and more with this perspective as stuff like this makes me more cautious&#x2F;cynical. Unless my CLA assigns copyright to a foundation, in which case I am more likely to believe it will be kept in line with the foundation&#x27;s charter, e.g. reply sanderjd 4 hours agorootparentprevI would much rather contribute to a project with a CLA and the possibility to be commercially licensed by the entity driving the work on the project. I&#x27;m not that interested in working for Amazon for free... reply growse 4 hours agorootparentYou can&#x27;t have it both ways. You either believe in, and enthusiastically participate in the development of free software (the philosophy of which requires freedoms to be available on an equitable basis), or you don&#x27;t. reply sanderjd 3 hours agorootparentI am not an ideologue (and I frankly struggle to understand why people find it appealing to be ideologues).I just want there to be a lot of software made that is useful to me. Software is far more useful to me if I can read and modify its source code.So my metric is just whether I think something will incentivize people to make more or less of that kind of software. I worry that people will make less and less software that runs as a service in this fashion, as it n becomes more and more obvious that the benefits of that software flow to giant integrated cloud platform providers rather than to the people who make the software.Being a chump is not motivating, and I think that&#x27;s the current incentive structure. So I like seeing people try different structures that seem to me like they structure the incentives better.But you&#x27;re totally right, philosophy has nothing to do with it for me. reply growse 2 hours agorootparent> ... as it n becomes more and more obvious that the benefits of that software flow to giant integrated cloud platform providers rather than to the people who make the software.The whole point of open source is that everybody has the opportunity to contribute to the benefit of everybody. You don&#x27;t get to say \"oh, but nazis &#x2F; Amazon &#x2F; rapists don&#x27;t get to use this because I disagree with their morals\".The incentive to contribute comes from being part of a community that gets to experience better quality software. For many (me included), that&#x27;s plenty. I really don&#x27;t care what Amazon does - I&#x27;m neither a customer, nor an investor. reply hamandcheese 7 hours agorootparentprevDoes assigning copyright with a CLA mean that I would not be free to, say, submit the same PR to more permissive fork as well as Hashicorp&#x27;s vault? reply dragonwriter 1 hour agorootparent> Does assigning copyright with a CLA mean that I would not be free to, say, submit the same PR to more permissive fork as well as Hashicorp&#x27;s vault?A CLA, by definition, licenses rather than assigns copyright. A CAA assigns copyright. Typically, a CLA does not restrict the licensors right to license the same contribution elsewhere (if it is legally derivative of a project whose own license is restrictive, that may prevent it, however.) reply seabass-labrax 7 hours agorootparentprevIt depends on the CLA, and there is often very little similarity between one CLA and another. On a technical note, CLAs don&#x27;t usually assign copyright, they only grant a licence, but one which permits the recipient of the CLA to relicense the contribution whenever they choose to. reply ec109685 6 hours agorootparentprevYes, they would be allowed. reply lima 7 hours agorootparentprevThis is incorrect, this can happen with any permissive license (BSD, Apache 2...) regardless of copyright assignments. reply j1elo 1 hour agorootparentThe rule is simple: If you want to relicense a project, then you need approval from all the individual copyright holders. It does not matter if you are relicensing between permissive OSS licenses such as, for example, from Apache 2.0 to MIT. They are different licenses, so you need permission for the change.That&#x27;s why OSS is commonly called a \"Community\". The software&#x27;s copyright belongs to the hands of all the community of developers who have written and contributed code.What a CLA usually does is grant perpetual permission to do anything with the code, including to relicense without asking. Practically speaking, CLAs grant full control to a single hand. Thus, OSS projects with such CLAs are not part of any so-called \"Open Source Community\" in any meaningful way.EDIT: Changed \"CLAs grant full ownership\" -> \"CLAs grant full control\", to avoid misunderstandings. reply dragonwriter 1 hour agorootparent> What a CLA usually does is grant perpetual permission to do anything with the code, including to relicense without asking. Practically speaking, CLAs grant full ownership to a single hand.No, they don’t. CLAs don’t transfer ownership. reply j1elo 1 hour agorootparentWhat CLAs do, is explained in the first half of the sentence. What it means in practice (in the context of talking about relicensing) is hence what \"practically speaking\" means.Anyway I&#x27;ve edited it to avoid using the word \"ownership\", which might confuse the intended meaning. reply ec109685 6 hours agorootparentprevNo, it’s correct. BSD at least allows anyone to create a restrictive fork of the code.With a CLA, it grants Hashicorp rights to your contribution no other corporation has. reply metadat 9 hours agoprevFunny how @mitchellh has decided not to join the conversation. Pretty sure he had the ultimate input on this decision, and historically he&#x27;s engaged with HN directly. Hmm.Overall it seems like a loser move. Look what happened to Elasticsearch - to me and most others, ES no longer exists. I&#x27;ve happily moved on to OpenSearch and not looked back at poor kimchi. Due to their own actions, Elasticsearch is no longer relevant.Will Hashicorp&#x27;s move spur a similar effort to fork the last open-source license version of Terraform and other Hashicorp tools? What other choice is there when the creator gets petty and insecure, and goes hostile against the open source community that helped create it? Extremely disappointed with the Hashicorp leadership team. MitchellH and your little sidekick Armon Dadgar - you owe your community better than this.I interviewed with Hashicorp back in 2016 and ended up turning down the job. I used to have a small amount of regret about this decision, but now that true colors have been revealed, I know I made the right call.What&#x27;s that saying about trust?Trust takes years to build, seconds to break, and forever to repair.It&#x27;s surprising to learn that people I thought were so smart could turn out to be this dumb! reply milar 6 hours agoparentMitchell isn’t in leadership at hashicorp any more, for some time, and has said so many times. No need to insult a person who has done a ton. reply evanriley 7 hours agoparentprevHasn&#x27;t mitchellh stepped down from a leadership role? Why would he have \"the ultimate input on this decision\"? reply xyzzy_plugh 6 hours agorootparentDid he give up his seat on the board? His voting rights? I assumed he still has, or effectively has, full control. reply EspressoGPT 2 hours agorootparentHe also stepped down from the HashiCorp board[1].[1] https:&#x2F;&#x2F;www.hashicorp.com&#x2F;blog&#x2F;mitchell-s-new-role-at-hashic... reply koolba 9 hours agoparentprevDoes anybody even pay for terraform? Outside the “workspace” hosted product, it’s all free as in beer for all the providers. reply glenngillen 8 hours agorootparentYes, check some of the previous HashiConf keynotes to see the types of customers that are paying for it and which products they use. Also HashiCorp&#x27;s financials are public, although without a per-product breakout. You&#x27;ll have to connect the dots between some of these things to try and get into the rough ballpark. reply Coryodaniel 7 hours agorootparentI read their s1 and it looked like a lot of the revenue came from professional services.I’m curious how this affects consultants and PSOs that might be competing w Hashi’s services business and running terraform on jenkins or whatever. reply glenngillen 7 hours agorootparentprevious quarter: https:&#x2F;&#x2F;ir.hashicorp.com&#x2F;node&#x2F;8351&#x2F;pdfCloud: $16.5M (+88% YoY) Subscriptions: $133.6M (+35% YoY) ProServ: $4.4M (+75% YoY) reply myroon5 4 hours agorootparentCloud revenue is a subset of Subscription revenue, and Subscription revenue is mostly Support ($101.9M). In fact, 3&#x2F;4 of their total revenue is Support reply glenngillen 3 hours agorootparentI don’t think Support is what you think it is here. Ask any Vault or Terraform Enterprise customer how much they pay and what it’s for. reply softveda 3 hours agorootparentCorrect, HashiCorp support price is built into its SKU tiers like Bronze, Silver and Gold. They don&#x27;t charge for support tickets. replymetadat 8 hours agorootparentprevIrrelevant in the grand scheme of things, at this scale of developer community it&#x27;s all about mindshare. They could&#x27;ve created a compelling paid support or other product offerings, but haven&#x27;t, or maybe took too much funding and the VCs forced their hand. Regardless, it&#x27;s a 1-trick pony. Even though they have other cool shit, Terraform is the golden goose, and they just strangled it.Now someone will fork it to \"Terrafoam\" or whatever and that&#x27;ll be it. MitchellH&#x27;s vision and expertise is no longer critical to the project. reply yellowapple 13 hours agoprev> As a result, we believe commercial open source models need to evolve for the ecosystem to continue providing open, freely available software.To imply that a non-open-source license like the BUSL is part of such an evolution of \"open source\" models (commercial or otherwise) betrays either severe confusion or a deliberate attempt to mislead.Like, has anyone of any significance used a Hashicorp product to meaningfully compete with Hashicorp? reply jrsdav 13 hours agoparentI haven&#x27;t looked to see what licenses are involved, but Pulumi makes liberal use of Terraform providers[1]. And I would definitely consider them to be a Hashicorp competitor.[1]: https:&#x2F;&#x2F;www.pulumi.com&#x2F;docs&#x2F;concepts&#x2F;vs&#x2F;terraform&#x2F;#:~:text=U....> Pulumi is able to adapt any Terraform Provider for use with Pulumi, enabling management of any infrastructure supported by the Terraform Providers ecosystem using Pulumi programs. reply yellowapple 12 hours agorootparentTF providers are just wrappers around other APIs - and the vast majority ain&#x27;t even developed by Hashicorp in the first place.If Pulumi - another open-source infra-as-code tool (that ain&#x27;t even a fork of any Hashicorp product AFAICT) - is really the thing scaring Hashicorp away from open source, then that doesn&#x27;t really do Hashicorp any favors here. reply quacker 11 hours agorootparent> TF providers are just wrappers around other APIsIf it&#x27;s so simple, Pulumi could have done this themselves. Why didn&#x27;t they? Would Pulumi have been as successful without leveraging the vast ecosystem of existing Terraform providers? Now they are a growing Terraform competitor. reply yellowapple 11 hours agorootparent> If it&#x27;s so simple, Pulumi could have done this themselves. Why didn&#x27;t they?Because that would be a waste of time unless there&#x27;s some specific case where an existing provider is insufficient.> Would Pulumi have been as successful without leveraging the vast ecosystem of existing Terraform providers? Now they are a growing Terraform competitor.Pulumi itself is free software, just like Terraform was (and hopefully still is). There is literally nothing stopping Hashicorp from doing the exact same thing in the opposite direction. reply quacker 11 hours agorootparentAre you pretending they aren&#x27;t a competitor profiting off that work though?Pulumi is free, except you pay for the features that aren&#x27;t free: https:&#x2F;&#x2F;www.pulumi.com&#x2F;pricing&#x2F;. So, Pulumi has grown their directly competing product partially on top of the Terraform ecosystem - and I&#x27;d argue they&#x27;d be half as successful without reusing Terraform providers - and make money off of that product. It&#x27;s at least understandable to me that Hashicorp doesn&#x27;t like this. reply lijok 10 hours agorootparentMost TF providers are not maintained by Hashicorp, but by the company whose product the provider integrates into. Development of the provider is an investment the company makes to lower CAC. reply quacker 9 hours agorootparentSure, but the providers for some of the biggest platforms are maintained by HashiCorp[1] - like the AWS, Azure, GCP, and Kubernetes providers[2], and it appears the Pulumi AWS provider (for example) _does_ use the Terraform AWS provider, even to this day[3].1. https:&#x2F;&#x2F;developer.hashicorp.com&#x2F;terraform&#x2F;registry&#x2F;providers... - \"official\" providers are maintained by HashiCorp2. https:&#x2F;&#x2F;registry.terraform.io&#x2F;browse&#x2F;providers?tier=official - The filtered list of \"official\" providers maintained by HashiCorp3. https:&#x2F;&#x2F;github.com&#x2F;pulumi&#x2F;pulumi-aws&#x2F;tree&#x2F;008c4360bc9fc24303... - Just prove it to myself, I can see the `upstream` git submodule, which embeds pulumi&#x2F;terraform-provider-aws, which is a fork of hashicorp&#x2F;terraform-provider-aws, although the repo was not created as a fork in Github, so it is not marked as a \"fork\" and so I have to compare commit histories to tell that it is a fork. reply jkodroff 8 hours agorootparentThe Pulumi Kubernetes provider is a native provider. It does not take a TF provider as a dependency. Instead, it works directly based off the k8s API spec.The Google TF provider is actually maintained by Google via Magic Modules, a single source for both TF and Ansible . The generated TF provider does reside in HashiCorp’s GH org tho. reply hughesjj 9 hours agorootparentprevImo this is notable as historically hashicorp has beaten even AWS to (cloud formation, and, thus, cdk) support for their newly launched services. reply Aeolun 9 hours agorootparentprevIs there anything stopping Hashicorp from implementing their own pulumi backend and profiting off the pulumi client?I kind of feel that this is the point of open source. That the work one group of people does can be leveraged by all of humanity. reply hughesjj 9 hours agorootparentI&#x27;ve always seen open source as more of a spectrum. On one extreme, you have gpl3 stuff coming from the church of GNU itself. Moving up the spectrum you have stuff like the Linux kernel or certain CNCF projects, which can be used in proprietary distributions etc. Above that you&#x27;ll have company specific licenses like the Amazon software license or whatever hashicorp+elastic are doing these days. At some point you&#x27;re with RHEL where you need to sign a license to get their source, and above that you&#x27;re signing ndas, and above that its closed source, and above that they&#x27;re cryptographically obfuscating their source code with anti reverse engineering licensing.I appreciate open source in all it&#x27;s forms, as it&#x27;s so much easier to 1. Read the code in case of a bug or just to understand your dependencies better, 2. Many of the semi closed licenses still allow you to learn things like syntax for GitHub actions or do analytics or whatever, which is still more value than an entirely closed system, and 3. It allows me to reproduce binaries and independently audit security.That said, I&#x27;m worried about a tragedy of the commons situation. I&#x27;m not a fan of capitalism, but at the end of the day I can&#x27;t pay for housing or taxes in clout or goodwill, and the only way I see likely to both attract talent and survive is one which works within the system while sacrificing their principles as little as possible. These wonderful engineers at elastic and hashicorp and pulumi and every other company mentioned in this thread need to eat.I&#x27;m not saying I have an answer, but I am saying that I respect and understand why companies like elastic and hashicorp are resorting to drastic measures. If the choice is between either of those companies going under or having their source code come with a little bit more restrictions, I&#x27;d chose the latter. I&#x27;d much rather them have a noncompete clause in their license than have them just shut the window into their source code altogether. reply Coryodaniel 6 hours agorootparentprevI think that would be an acknowledgment too painful for the brand. reply ec109685 6 hours agorootparentprevAre you asserting that Terraform would be as widely adopted if the third party authored TF providers didn’t exist? reply thayne 6 hours agorootparentprevWould terraform have been successful without the ecosystem of providers provided by third parties, and many, many external contributions to providers they do manage? reply arianvanp 13 hours agorootparentprevBut they don&#x27;t out-compete them in any shape or form. I&#x27;d call it healthy competition.Pulumi made Hashicorp build Terraform CDK. Which is a great result.And the only reason Hashicorp was able to build CDK quickly is because they built it on top of Amazon&#x27;s open source Amazon CDK. Another competitor. reply manojlds 11 hours agorootparentCalling Amazon a competitor because they have CDK is ridiculous. reply deadbunny 9 hours agorootparentAmazon is a competitor due to CloudFormation, Secrets Manager, however many ways to run containers there are now... reply hughesjj 9 hours agorootparentAnd SAM, and controltower, and of course the aforementioned cdk reply arianvanp 4 hours agorootparentprevCould you be more constructive than just saying it&#x27;s ridiculous? If you&#x27;re an Amazon shop CDK&#x2F;CloudFormation definitely is a competitor.Also Terraform now directly integrates with CDK. So apparently Hashicorp felt the heat .Looks like competition to me. reply Aeolun 9 hours agorootparentprevNot really. The big question in infra land at our company these days is terraform vs cdk vs pulumi. reply reilly3000 12 hours agorootparentprevThey used to do that but now also have their own providers created by API catalog introspection. reply arianvanp 12 hours agorootparentWhich terraform then copied with the AWS Native provider... reply LapsangGuzzler 13 hours agoparentprev> Like, has anyone of any significance used a Hashicorp product to meaningfully compete with Hashicorp?Just because nobody has tried yet doesn’t mean that it won’t ever happen. Companies are doing this precisely because companies like Amazon abuse FOSS licenses to stand up their own hosted versions of open source projects. reply jamestanderson 13 hours agorootparent> companies like Amazon abuse FOSS licenses to stand up their own hosted versions of open source projectsThis is not an abuse of FOSS licenses. If developers have a problem with this, there are open source licenses that would make this use case less attractive for Amazon, like the AGPL. reply FridgeSeal 11 hours agorootparentThat licence tends to have the dual effect of dissuading otherwise valid users from using it, because a lot of devs and corps see “something something GPL” and just shut down. reply fmajid 10 hours agorootparentThat&#x27;s where dual-licensing comes in like Artifex&#x27; AGPL + commercial licensing. But yes, most large Tech companies have a blanket ban on AGPL. reply justinjlynn 6 hours agorootparent> most large Tech companies have a blanket ban on AGPLsounds like it&#x27;s doing its intended job well then? reply madeofpalk 11 hours agorootparentprevOr, like HashiCorp Adopts Business Source License reply tedivm 9 hours agorootparentprevScalr and Spacelift come to mind.Spacelift is a significantly better product than Terraform Cloud, and since they apparently can&#x27;t compete on quality they&#x27;re going with this instead. reply yellowapple 13 hours agorootparentprev> Just because nobody has tried yet doesn’t mean that it won’t ever happen.That nobody has tried suggests paranoia at best.> Companies are doing this precisely because companies like Amazon abuse FOSS licenses to stand up their own hosted versions of open source projects.The AGPL exists and already fully addresses that. reply gardenfelder 11 hours agorootparentI&#x27;m thinking that AGPL is, indeed, the Ebola of viral licenses, but it does not cover the case where a large cloud vendor simply takes an AGPL-licensed product and offers it as a cloud service. AGPL does not cover that case. Nothing the cloud vendor does challenges any of the AGPL&#x27;s terms.The cloud vendor issue is license agnostic. ElasticSearch comes to mind. For the AGPL side, MongoDB and Neo4J come to mind.Recall that AGPL came into play by way of a hole in the GPL terms, the one where you can modify a GPL codebase but you don&#x27;t have to say anything unless you publish it. GPL was weak in therms of the definition of \"publish\". AGPL closed that hole.But, that hole only becomes toxic the moment you modify the code or plug proprietary stuff into it. Cloud vendors don&#x27;t do that. reply wmf 10 hours agorootparentCloud providers build a proprietary control plane to deploy copies of the program and it could be argued that such code is a modification of the program itself and thus has to be released. That&#x27;s why they won&#x27;t touch AGPL code. reply not_a_bot2890 12 hours agorootparentprev> That nobody has tried suggests paranoia at best.That&#x27;s hardly paranoia. Why wait until you need to change it under pressure from one of the big CSPs (a la Elastic&#x2F;AWS)? It is proactive at best. reply yellowapple 12 hours agorootparentAWS in particular already has its own competing services that predate or otherwise do not derive from Hashicorp&#x27;s offerings (CloudFormation, Secrets Manager, etc.). Same with pretty much every other major cloud provider. The whole point of going with Vault or Consul or Terraform or what have you is to not tie oneself to a particular provider&#x27;s offerings.Put simply: Hashicorp&#x27;s target market for a given product is largely not going to be interested in a cloud provider&#x27;s locked-down equivalent, and a cloud provider is not going to be interested in deprecating its own tightly-integrated product in favor of customizing some third-party offering.And again: if this was a legitimate fear, then the AGPL already fully addresses it. reply toenail 13 hours agorootparentprevIf you call that abuse.. using \"open source\" as a marketing gimmick to attract developers and users is abuse as well. reply madeofpalk 12 hours agoparentprevctrl+f for \"open source\", and note where they stop using it. Note that they describe this change to maintain \"open, freely available products\", not open source.I think they&#x27;re being more honest than others in not saying that changing their license is not to remain \"open source\". It&#x27;s evident they know the pushback they would get from calling this \"open source\". reply jen20 13 hours agoparentprevPulumi is the product that comes to mind immediately. Then things like env0, SpaceLift etc at the service end. reply reilly3000 12 hours agoparentprevI think Upbound’s Crossplane is the best positioned here. It’s an easy migration path and far more sane way to manage state. reply swyx 11 hours agorootparentcould you say more about what hashicorp products Crossplane compares to&#x2F;or doesnt? reply reilly3000 10 hours agorootparentSure, Crossplane leverages the K8s control plane for managing desired state for systems outside of the kubernetes cluster. It functions in a very similar fashion to terraform but it’s storing its own state in etcd instead of another state store. That makes GitOps + Crossplane play really nicely together and avoid the extra complexity that happens with terraform apply operations. They have a terraform wrapper provider for importing existing config and state.It’s really nice to have infra and apps flow through the same pipes. reply ec109685 6 hours agoparentprevFly.io was built off of nomad for a while. That wouldn’t be allowed in this new world. reply LVB 5 hours agorootparentWhy not? What part of Fly.io is competing with any HashiCorp products? reply growse 3 hours agorootparentI mean, they allow you to create a manifest that then auto provisions some infrastructure, so... ShrugI&#x27;m being half-serious. Of course the point is that \"competing\" is deliberately very poorly defined, and it&#x27;s entirely on the whim of hashicorp to decide whether or not you&#x27;re competing and then impose a lot of legal costs on you.It&#x27;s a racket. reply27",
    "originSummary": [
      "HashiCorp, a technology company, will use the Business Source License (BSL) for its future product releases to support its community and provide open software.",
      "The BSL allows copying, modification, redistribution, non-commercial use, and commercial use under specific conditions.",
      "The change in license aims to address concerns about vendors benefiting from open-source projects without giving back and will still involve publishing source code and working closely with partners and customers."
    ],
    "commentSummary": [
      "HashiCorp's adoption of the Business Source License (BSL) is generating criticism and sparking a debate on the balance between open source principles and commercial viability.",
      "There is disagreement about the definition of \"open source\" and concerns that the promotion of \"source available\" software as \"open source\" may dilute the term.",
      "The conversation explores copyright assignments, license terms, and the distinction between open core and not open-source models, with varying opinions on the best approach for open-source projects and commercialization.",
      "The potential negative impact of going public on software companies and the differences between open source and source-available software licenses are also discussed.",
      "The BSL is questioned on whether it should be considered open source, and the implications of proprietary vendors using OSI-approved license characteristics are debated.",
      "The conversation touches on the implications of open-source software and licenses on competition and the importance of copyright ownership.",
      "The revenue sources of HashiCorp, including the pricing structure of their support services, and potential competitors in the market are discussed.",
      "Overall, the conversation highlights the challenges and considerations faced by HashiCorp and other companies in the open-source software industry."
    ],
    "points": 475,
    "commentCount": 540,
    "retryCount": 0,
    "time": 1691699197
  },
  {
    "id": 37082771,
    "title": "Source code for Quake 2 rerelease",
    "originLink": "https://github.com/id-Software/quake2-rerelease-dll",
    "originBody": "Skip to content Product Solutions Open Source Pricing Search or jump to... Sign in Sign up id-Software / quake2-rerelease-dll Public Notifications Fork 39 Star 699 Code Pull requests Actions Security Insights id-Software/quake2-rerelease-dll main 1 branch 0 tags Go to file Code Latest commit id-mikerubits Initial commit 60f2956 Git stats 1 commit Files Type Name Latest commit message Commit time fgd Initial commit original Initial commit rerelease Initial commit LICENSE.txt Initial commit README.md Initial commit README.md Quake II Rerelease Game Source This repository contains the game code for the 2023 rerelease of Quake II, for users who wish to mod the game, along with the original game code that was use for reference. Mods can be loaded into the rerelease the same way as the original game: launch the game with +set game mymod or type game mymod into the console while the game is running. We recommend installing mods into your %USERPROFILE%\\Saved Games\\Nightdive Studios\\Quake II directory to ensure the original game files do not get modified. id Software is unable to provide support for this release, however we urge you to take advantage of the depth of community-driven resources already available. The rerelease of Quake II uses a new version of the API to communicate between the server & the game module. It also introduces a very thin \"client game\" module, akin to Quake III Arena's cgame module, to allow for extended modding opportunities that change previously hardcoded client behavior. It also has a new network protocol, version 2023. This codebase is a combination of the separate game modules that were part of the original game: baseq2, ctf, rogue, and xatrix. It requires a C++17 compiler. In cases of conflicting spawnflags, maps were modified in order to resolve issues, so original expansion pack maps may not load correctly with this DLL. The combined FGD as used for development is also available for users wishing to make new maps. A modified TrenchBroom Quake II gameconfig.cfg is included as there are modified textureflags. Because the game export interface has changed, existing mods may be able to be moved over to support the API changes. However, in order to support all expansion packs under one codebase and new features in the rerelease, there have been some major changes to structure and layout, so old mods wishing to use the new codebase may need to be rewritten. The API changes discussed here are written from the perspective of the game DLL. Compiling The game DLL has only been tested with Clang, VS2019 and VS2022. The code can compile under both C++17 and C++20. Using C++20 allows you to skip fmtlib as a dependency. Required preprocessor definitions GAME_INCLUDE must be defined, tells game.h that this is the game DLL compiling it. KEX_Q2GAME_EXPORTS must be defined, tells game.h that we are exporting the GetGameAPI function. KEX_Q2GAME_DYNAMIC must be defined. The game DLL supports static linking on console platforms, but is always dynamic on PC. NO_FMT_SOURCE: this is only here because of a limitation in the internal build system. It must be defined. Optional preprocessor definitions KEX_Q2_GAME: must be defined if compiling for Kex. This changes the behavior of the say command to go through the lobby. KEX_Q2GAME_IMPORTS: only used by engine, tells game.h that we are importing GetGameAPI. USE_CPP20_FORMAT: if explicitly defined, C++20'slibrary will be used instead of fmtlib; otherwise fmtlib usage will be autodetected. Dependencies fmtlib: If USE_CPP20_FORMAT is not set, the library needs to be available in the fmt subdirectory. jsoncpp: Must be placed inside json subdirectory. Both of these can also be installed via vcpkg: vcpkg install jsoncpp:x64-windows fmt:x64-windows Windows (Visual Studio 2019 / 2022): We recommend placing the source in a subfolder within a mod directory. For example, alongside baseq2, make a folder called mymod, enter that folder, make a folder called src, and copying the contents of the rerelease directory into the newly-created src subfolder. Open game.sln Build solution Debugging the DLL is possible when attaching to the engine EXE. Note that if you are using VS2022 Hot Reload, due to an internal Hot Reload issue, current edits will be lost when disconnecting from the server, or changing levels using the map command. 40hz Tickrate Support As part of this release, all internal logic in the game DLL has been adjusted to run at 40hz compared to the original 10hz of the original engine. This provides a better gameplay experience, and allows maps and game logic to run at more precise steps than the original 100ms. 40hz was chosen as it is a multiple of the original 10hz, operates at an integral 25ms step, and was the best balance between bandwidth and CPU concerns around the original tech. Print Adjustments As part of the API cleanup, the game DLL no longer uses varargs in any of its functions. Varargs are compiler-specific and not very portable, so instead, the onus is on the caller to handle formatting. As a bonus, this allows the game DLL to more easily hook in modern formatting providers; the game DLL uses fmt almost exclusively. Several built-in types, like edict_t and vec3_t, can be formatted directly. Math Changes Since C++ is now used in the game DLL, math functions were made constexpr where appropriate, and operator overloads are used to make math easier to work with and closer to QuakeC. For instance, VectorMA(a, s, b, c) can now be written as c = a + (b * s), which expresses the operation better. Type Changes qboolean, which was aliased to int32_t, is now instead aliased to bool. This type should be equivalent to C's _Bool. Info Keys In the original Quake II, all infokey values are purely ASCII with upper bits stripped. Kex and the Quake II rerelease engine supports UTF-8 for things like player names, which necessated a change to the way info keys work. Instead of implementing a whole UTF-8 system in the game DLL, these functions are now imports, so the engine is in control of the parsing and string management. Userinfo variables are now suffixed with a number for split screen support. For instance, name and skin have become name_0 and skin_0 for the first player, name_1 and skin_1 for the second player, and so on. Extensions In an attempt to remain compatible with future community engines, all engine interfaces contain stubbed functions for GetExtension. This is currently unused and will only return nullptr, however other engines may wish to support returning named structs containing extra function pointers based on string parameters. This is similar to getextension that has become standard in many QuakeC environments. Conforming engines should return nullptr if an extension is not supported, otherwise they should return a non null pointer value that is relevant to the requested feature. Supporting engines should use namespaces for features to prevent name collisions around common and possibly incompatible implementations. Player Movement Player movement (\"pmove\") is now handled as an export in both game_export_t and cgame_export_t. This allows a game module to modify physics while still working with client prediction. Pmove also received several upgrades, such as more bits for flags and full float positioning. Because a lot of movement quirks in Quake II were indirectly caused by the compression, these behaviors were retained. Trick jumping is now an explicit movement type, to allow for things like the Mega Health box jumps to still work. Some fixes were made, like jumping higher below the 0 z point of the map. Frame visibility As part of network improvements, some changes were made to the \"entity is visible to client in frame\" methods: Split-screen support since all clients share a frame. Entities with shadows will be visible if their shadows may be visible to a player. Sound attenuation culling is now calculated formulaically to match the sound system, and takes loop_attenuation into account. Entity linkage To fix a legacy bug where lasers only relied on one position for culling, RF_BEAM entities now set their absmin & absmax properly. This can cause them to be a bit inflated if they are angled, but should not cause any issues otherwise. In a similar vein, gi.linkentity no longer automatically sets old_origin for beams. This is to make it a bit easier to work with beams, as otherwise you'd be forced to link twice to get it linked into the world correctly. This might break old mods that depends on this behavior. Audio positioning Entity spatialization underwent an overhaul (CL_GetEntitySoundOrigin mainly). Brush models will use the closest point on the bmodel's absmin/absmax to the listener's origin. This allows moving brush models with sounds to make consistent sounds, and be full volume if you are inside of them. Beams now support s.sound, and plays their sound on the nearest point between the two beam origins. As a secondary fix to the above, S_StartSound has slightly different logic now surrounding what origin to pick when playing a sound: if (entnum1 && (cl_entities[entnum].serverframe == cl.frame.serverframe || (cl_entities[entnum].current.solid == PACKED_SOLID_BSP || (cl_entities[entnum].current.renderfx & RF_BEAM)))){ ps->fixed_origin = false;}else{ VectorCopy (origin, ps->origin); ps->fixed_origin = true;} } elseps->fixed_origin = false; fixed_origin is set to flags & SND_EXPLICIT_POS for svc_sound packets, and is false otherwise. If the playsounds' fixed_origin field is set, then the ps->origin value will always be used over automatically trying to determine its position. Client entity adjustments Beam origin points are interpolated if they exist between frames. TE_ELECTRIC_SPARKS and TE_SCREEN_SPARKS/TE_SHIELD_SPARKS will only play sounds once on any given frame. Entities will never play the same footstep sound twice in a row. Beams now squash the ends of their beams so they don't intersect walls or end early. Alpha and transparency settings now get copied over to all sub-modelindices. Lightramps are now interpolated, which looks nicer and helps with epilepsy. delta_angles are interpolated now, although this is never used at all in the game. screen_blend and damage_blend are interpolated now, but only if the frame prior didn't have a clear color. Configstrings Configstrings have been overhauled. There is now a theoretical maximum of 32767 entries, although you are still bound by the game APIs value of MAX_CONFIGSTRINGS. The maximum length of a configstring has increased from 64 to 96. The API now canonizes that certain spans (CS_STATUSBAR and CS_GENERAL) can span multiple lines. A CS_SIZE function is provided to calculate the total size (in bytes) that can be written to for a given configstring ID. A convenience function, CS_REMAP, is provided to help remap old configstring IDs to new ones. This is used in our engine to provide old demo support. MAX_MODELS, MAX_SOUNDS and MAX_IMAGES have been increased from 256 to 8192, 2048, and 512, respectively. CS_STATUSBAR This entry now spans entries 5 to 58 instead of 5 to 28, giving you an effective size of 5184 bytes (minus one for the null terminator) for the statusbar strings, up from 1536 bytes. CS_SHADOWLIGHTS This new span each consists of a shadow light entry. Its format is semicolon-separated numerical values, in the following type & order: int entity_order int type (0 = point, 1 = cone) float radius int resolution float intensity float fade_start float fade_end int lightstyle float coneangle float direction_x float direction_y float direction_z CS_WHEEL_WEAPONS This new span consists of entries for the weapon wheel. It consists of pipe-separated integral values, in the following order: CS_ITEMS item index CS_IMAGES image index CS_WHEEL_AMMO ammo index (or -1 for no ammo) minimum ammo to use weapon whether the weapon is on the powerup wheel or the weapon wheel additional sort integer quantity to warn on low ammo on whether this weapon is droppable or not CS_WHEEL_AMMO This new span consists of entries for the weapon wheel ammo types. It consists of pipe-separated integral values, in the following order: CS_ITEMS item index CS_IMAGES image index CS_WHEEL_POWERUPS This new span consists of entries for the powerup wheel. It consists of pipe-separated integral values, in the following order: CS_ITEMS image index CS_IMAGES image index if 1, it is a togglable powerup instead of having a count additional sort integer whether we can drop this powerup or not CS_WHEEL_AMMO ammo index, if applicable (-1 for no ammo) CS_CD_LOOP_COUNT Integer which determines how many times to loop the music before switching to the ambient track. Leave blank to use the clients' preferred value, otherwise it is forced to this value (a value of zero means never switch to ambient track). CS_GAME_STYLE Inform the client about the type of game being played. Structures Quake II has two main shared structures: gclient_t and edict_t. These split off into various other shared structures that have to be the same between the game & server. Like the original release, the \"shared\" aspects of these structs must be identical, and are stored in game.h as edict_shared_t and gclient_shared_t respectively. The structure changes will be listed from the bottom-up. Full listings of the structures can be found in the source. cvar_flags_t CVAR_USER_PROFILE (bit 5) This is a new flag that is solely for the client; it indicates that a cvar is treated like userinfo for the purposes of storage, but is not sent to the server like userinfo usually is. For example, this flag is applied to cl_run_N, which controls the individual Always Run flags for each split screen player. contents_t CONTENTS_PROJECTILECLIP (bit 14) This new content flag will be collided against by CONTENTS_PROJECTILE entities. CONTENTS_PLAYER (bit 30) This special content type flag is set only on player entities, and allows tracing to exclude/include them. CONTENTS_PROJECTILE (bit 31) This special content type flag is set only on projectiles, and allows tracing to exclude/include them. surfflags_t SURF_ALPHATEST (bit 25) This bit is widely supported by other engines and is supported in the rerelease. SURF_N64_UV (bit 28) This flag is specific to N64, and halves texture sizes. SURF_N64_SCROLL_X (bit 29) This flag is specific to N64, and causes textures to scroll in the X axis. SURF_N64_SCROLL_Y (bit 30) This flag is specific to N64, and causes textures to scroll in the Y axis. SURF_N64_SCROLL_FLIP (bit 31) This flag is specific to N64, and flips the scroll axis. csurface_t This structure has undergone canonization of the 3.2x changes by Zoid. char[32] name Uses the proper name length now. uint32_t id This value must represent a unique ID that corresponds to its texinfo. The same ID must always reference the same texinfo, but they don't necessarily have to be sequential. Zero must always mean \"no texinfo\". This is used by the client for indexing footstep sounds. char[16] material The material ID for this texinfo, from the corresponding .mat file. trace_t csurface_t *surface The only change is a contractual one: this value must never be null. cplane_t plane2 / csurface_t *surface2 When a trace impacts multiple planes at the destination, the collision system will now report both of them. The \"second best\" plane and surface are stored here. surface2 must be null if a second surface was not hit. This is used to solve some epsilon issues with the player movement system. cvar_t int32_t modified_count The old qboolean modified; has been changed into an integral value. This value is increased when the cvar has been changed, but is never zero. The reason for this is so that \"is cvar modified\" checks always succeed on the first check, assuming you initialize the last modified value to 0. The function Cvar_WasModified is provided as a convenience function to perform this task for you. int32_t integer A common extension to Quake II, the integral value is stored at the end of the struct for you to use. player_state_t int32_t gunskin This is a new value which sets the skin number used on the weapon. int32_t gunrate This value sets the frame rate (in hz) of the players' weapon. For backwards compatibility, a value of 0 is equivalent to a value of 10. This ia mainly used for the Haste powerup, but in theory it could be used for future mods to have higher tickrate weapons in general. float[4] screen_blend / damage_blend The full-screen blend value was split into two values: screen_blend and damage_blend. screen_blend is the same as the original one, and is a full-screen color change. It is mainly used now for full-screen fades. To reduce the amount of screen flashing, the base game avoids flashing the screen whenever possible. damage_blend is a new type of blend that occurs around the edge of the screen; this is used to replace many events that previously would flash the full screen. refdef_flags_t rdflags The only adjustment to rdflags was the addition of a new flag: RDF_NO_WEAPON_LERP. This occupies bit 4, and can be used to temporarily disable interpolation on weapons. short[64] stats MAX_STATS was increased from 32 to 64. Note that because stats are now handled by the game & cgame modules, you are not limited to a short for the purposes of packing down data/ uint8_t team_id For teamplay-oriented games, the player's team is sent in player state. While the client could derive this from entity state in theory, in practice that's a bit ugly since the players' entity may not even be visible (for instance if you've been gibbed), so this was the cleaner approach. usercmd_t The fields upmove, impulse and lightlevel have been removed. button_t buttons BUTTON_HOLSTER (bit 2) This button corresponds to the new +holster command, which will keep the weapon holstered until depressed. It is used by the weapon wheel to allow the player to start switching weapons before the weapon wheel is dismissed. BUTTON_JUMP (bit 3) BUTTON_CROUCH (bit 4) These two new bits replace usercmd_t::upmove, and determine the players' jumping and crouch states. vec3_t angles These are now full float precision, allowing for players to aim more precisely. float forwardmove / sidemove These are now full float, to allow controller inputs to be more precise. uint32_t server_frame New entry sent along with every usercmd, which tells the server which server frame that the input was depressed on. This is used for integrity checks, as well as for anti-lag hitscan. pmove_state_t pmtype_t pm_type Two new pmove types have been added before PM_SPECTATOR, offsetting it and its subsequent entries by 2. PM_GRAPPLE (1) Used for the grappling hook; it informs client prediction that you should be pulled towards velocity and are not affected by gravity. PM_NOCLIP (2) This is what PM_SPECTATOR used to be, and prevents all clipping. PM_SPECTATOR (3) This value now represents spectator mode; you cannot enter walls, but can go through brush entities. vec3_t origin / vec3_t velocity / vec3_t delta_angles These fields now have full float precision versus the original release. See Pmove for more details. pmflags_t pm_flags This type has had its capacity increased to int16. The following flags are new or adjusted: PMF_NO_POSITIONAL_PREDICTION This flag was originally called PMF_NO_PREDICTION; it now only disables prediction on origin, allowing angles to be predicted. This is a backwards-incompatible change, but should have very minimal impact on running old mods. This improves the feeling of the grappling hook. PMF_ON_LADDER This bit is used to signal back to the game that we are currently attached to a ladder. PMF_NO_ANGULAR_PREDICTION The angular equivalent of PMF_NO_POSITIONAL_PREDICTION. PMF_IGNORE_PLAYER_COLLISION This flag is input only, and tells Pmove to ignore CONTENTS_PLAYER contents. PMF_TIME_TRICK If set, then pm_time is the time remaining to start a trick jump. uint16_t pm_time pm_time is now expressed in milliseconds instead of 8 * ms; since the code clamped subtractions on this to 1, it meant that high framerate players experienced slightly different physics, and in the case of trick jumps, had a smaller time gap to perform them. int8_t viewheight A new field describing the viewheight output; this is for crouch prediction. pmove_t The field viewheight has been removed, since it is now part of pmove_state_t. touch_list_t touch The list of touched entities has been replaced with a list of traces, allowing the game to react better to touches. cplane_t groundplane The plane that you're standing on is now returned by pmove. edict_t *player An opaque handle to the player object, passed back to trace. trace / clip trace is now sent the passent and contentmask, so it can perform more complex tracing routines. clip is also now available to pmove, should you need it. It is currently only used in spectator movement, to clip solely against the world and nothing else. vec3_t viewoffset The player's viewoffset is now passed in, to allow for accurate blending. Pmove is now semi-responsible for screen blends. vec3_t screen_blend An output variable containing full-screen blends to apply to the view. refdef_flags_t rdflags An output variable containing flags that should be merged with the server's representation. bool jump_sound An output variable to tell the game to play a jumping sound. float impact_delta When new ground is achieved, the impact is stored here for fall damage checks. edict_shared_t NOTE: the following members of the old edict_t struct have been removed, and were moved server-side: link_t area int num_clusters int clusternums[] int headnode sv_entity_t sv Most of the meat of the bot system is contained in the server code, and doesn't have direct access to the games' representation of the state of the game. Bots use this thin interpretation of the game state data about entities to understand how to use entities to its advantage - similar to how the client receives a thin portion of entities to understand how to render them. bool linked This boolean indicates whether the entity is currently linked into the world or not. It is the replacement of checking for area.prev being non-null. svflags_t svflags For new functionality, some new flags were added to svflags. This may cause backwards-incompatibility in older mods that have modified this enum! This enum is server-specific, so it is always incorrect for mods to modify this. SVF_PLAYER This flag causes the object to be treated as CONTENTS_PLAYER for collision. All players have this flag. SVF_BOT This flag marks the entity as a bot. SVF_NOBOTS This flag tells the bot subsystem to ignore this entity. SVF_RESPAWNING This flag is a hint to the bot subsystem to inform it about how items respawn. SVF_PROJECTILE This flag treats the entity as CONTENTS_PROJECTILE for collision. SVF_INSTANCED This flag marks the entity as being instanced - it may be invisible or hidden for certain players. SVF_DOOR This flag is for the bot subsystem, and informs it that this entity is a door. SVF_NOCULL This flag overrides the client frame building culling routines, causing an entity to always be sent regardless of where it is (ignoring PVS/PHS, essentially). Its only use in our code is to keep no-attenuation looping speakers in frame always. SVF_HULL This flag adjusts the servers' method of clipping movement to entities. Normally, only SOLID_BSP entities will use their proper clipping bounds for collision, but if this is set on a SOLID_TRIGGER brush entity, traces will have to collide with the actual BSP tree of the trigger instead of solely touching its bounding box. This is used in our game DLL to allow for certain triggers (like trigger_gravity or trigger_flashlight) to be activated when you are actually touching their brushes, allowing for angled triggers to finally exist. entity_state_t uint32_t number Number was changed to uint32_t (from int32_t) to better represent its use and to only have to catch out of bounds in one direction. int32_t skinnum Skinnum now packs a bit more data into it. // [Paril-KEX] player s.skinnum's encode additional data union player_skinnum_t { int32_t skinnum; struct { uint8_t client_num; // client index uint8_t vwep_index; // vwep index int8_t viewheight; // viewheight uint8_t team_index : 4; // team #; note that teams are 1-indexed here, with 0 meaning no team// (spectators in CTF would be 0, for instance) uint8_t poi_icon : 4; // poi icon; 0 default friendly, 1 dead, others unused }; }; effects_t effects The type effects_t was changed from uint32_t to uint64_t since we have way more effects to express. EF_BOB (bit 4) Bit was unused in Quake II. This was repurposed into a weapon bobbing effect, similar to Quake III. EF_POWERSCREEN (bit 9) This effect uses a different model that is scaled to the monster's size now. EF_DUALFIRE (bit 32) This bit is used for a special effect, similar to EF_QUAD, but for Dualfire Damage. EF_HOLOGRAM (bit 33) This bit is used for the N64 hologram effect; it adds a spinning ball of green particles around the object. EF_FLASHLIGHT (bit 34) This bit marks a player entity as having a flashlight enabled. The effect itself is rendered separately by the client. EF_BARREL_EXPLODING (bit 35) This effect is used before an explobox explodes; it emits steam particles from the barrel, as if it is experiencing a decompression event. EF_TELEPORTER2 (bit 36) This effect is used for the teleporter FX in the N64. EF_GRENADE_LIGHT (bit 37) This effect creates a small light on monster grenades, to make them slightly easier to track visually. EF_FIREBALL (EF_ROCKETEF_GIB) This mutually-exclusive bit combo did nothing in the original game, since these special trails could only render one or the other. In the rerelease, it will render a fireball trail that begins yellow and large, tapering off into an orange trail, to mimick the effect on N64. renderfx_t renderfx RF_NO_ORIGIN_LERP (bit 6) This effect had a confusing name originally. Its name now reflects what it does: it disables origin interpolation. RF_BEAM (bit 7) You can now create custom segmented beams by setting a non-one modelindex on beams. RF_CUSTOMSKIN (bit 8) This effect was unused originally. It is now implemented and works as intended: specifying a skinnum will change the skin on the model to the skin specified in CS_IMAGES + skinnum. For RF_FLARE, frame must be used instead however, as skinnum is used for color data. RF_NOSHADOW (bit 13) This effect was client-only originally. RF_CASTSHADOW (bit 14) This effect marks an entity that casts light in the world; it is only used by dynamic_light (or dynamic light entities), and should not be used otherwise. RF_SHELL_LITE_GREEN (bit 19) This is the equivalent shell color for EF_DUALFIRE. RF_CUSTOM_LIGHT (bit 20) This flag creates a custom dynamic light at the position of the object. Its used in the N64 campaign, as it has custom light entities (target_light). s.frame is the light's radius, and s.skinnum is the light's current color (packed RGB). RF_FLARE (bit 21) This flag marks an entity as being rendered with a flare instead of the usual entity rendering. Flares overload some fields: s.renderfx & RF_SHELL_RED causes the flare to have an outer red rim. s.renderfx & RF_SHELL_GREEN causes the flare to have an outer green rim. s.renderfx & RF_SHELL_BLUE causes the flare to have an outer blue rim. s.renderfx & RF_FLARE_LOCK_ANGLE causes the flare to not rotate towards the viewer. s.renderfx & RF_CUSTOMSKIN causes the flare to use the custom image index in s.frame. s.modelindex2 is the start distance of fading the flare out. s.modelindex3 is the end distance of fading the flare out. s.skinnum is the RGBA of the flare. RF_OLD_FRAME_LERP (bit 22) This flag signals that s.old_frame should be used for the next frame and respected by the client. This can be used for custom frame interpolation; its use in this engine is specific to fixing interpolation bugs on certain monster animations. RF_DOT_SHADOW (bit 23) Draw a blob shadow underneath the entity. RF_LOW_PRIORITY (bit 24) This flag marks an entity as low priority; if the renderer runs out of entity slots, these entities will be eligible for replacement. For instance, a monster is more important than a gib, so gibs are marked low priority so they can be replaced by a monster if the limit is reached. RF_NO_LOD (bit 25) The original MD2 models will be used for LOD. Setting this bit prevents this behavior. RF_NO_STEREO (bit 2) This is an overloaded flag that only applies to non-rendered entities that contain sounds. If set, stereo panning is disabled on this entity. RF_STAIR_STEP (bit 26) The tick rate increase caused a bit of a visual bug with monsters and players: they now stepped up steps within 0.025 seconds instead of 0.1, causing jarring hitching. To fix this, entities set this flag when they detect they have stepped up a stair, and the client will interpolate their height difference over 0.1 seconds. RF_BEAM_LIGHTNING (RF_BEAMRF_GLOW) This mutually-exclusive bit combo causes a laser to become a lightning bolt, for N64 support. uint32_t solid This was changed from int32_t to uint32_t, and now packs more data into it to better represent bounding boxes to clients. For backwards compatibility, 31 is still the magic value used for BSP entities. The actual packed data, however, is now as follows: union solid_packed_t { struct { uint8_t x; uint8_t y; uint8_t zd; // always negative uint8_t zu; // encoded as + 32 } p; uint32_t u; }; // packing: packed.p.x = ent->maxs[0]; packed.p.y = ent->maxs[1]; packed.p.zd = -ent->mins[2]; packed.p.zu = ent->maxs[2] + 32; // unpacking: packed.u = state->solid; ent->mins[0] = -packed.p.x; ent->maxs[0] = packed.p.x; ent->mins[1] = -packed.p.y; ent->maxs[1] = packed.p.y; ent->mins[2] = -packed.p.zd; ent->maxs[2] = packed.p.zu - 32; This is similar to Quake III Arena, and essentially allows any integral bbox to make it to the clients unchanged. entity_event_t event Two new events were added: EV_OTHER_FOOTSTEP (8) Allows non-players to send footsteps. They have idle attenuation, whereas regular footsteps have normal attenuation. EV_LADDER_STEP (9) Ladder climbing 'footstep' event. float alpha This value allows you to specify exact transparency values for entities. For backwards compatibility, setting it to zero should be equivalent to an unchanged value, but any non-zero value should be respected as changed. float scale This value allows you to scale an entity by the given amount. For backwards compatibility, setting it to zero should be equivalent to an unchanged value, but any non-zero value should be respected as changed. uint8_t instance_bits This value is not meant to be set directly by the game code, but will have non-zero bits set for split-screen players that cannot see this entity. float loop_volume / loop_attenuation Looping noises can now have volume and attenuation explicitly specified. For both, a value of zero indicates default/unchanged, for backwards compatibility. For loop_attenuation, a value of -1 indicates full level audio (like ATTN_NONE). int32_t owner An entity's owner is now networked, allowing for it to ignore collision properly. int32_t old_frame Only sent when renderfx & RF_OLD_FRAME_LERP - indicates that this frame is the frame to lerp from. Import/Exports Game Import (read-only) tick_rate / frame_time_s / frame_time_ms This holds the server's tick variables. They will be set at the start of the server, before PreInit. tick_rate stores the tick rate, in hz. frame_time_s is the time a game frame will take in seconds. frame_time_ms is the time a game frame will take in ms. These are provided pre-calculated for convenience. Broadcast_Print This function writes message with the print type of printlevel to all players. See Print Adjustments. This is kept for compatibility purposes, Loc_Print replaces it. Com_Print This function writes message to the server. See Print Adjustments. Client_Print This function writes out message with the print type of printlevel to the specified ent player. See Print Adjustments. This is kept for compatibility purposes, Loc_Print replaces it. Center_Print This function writes message to the specified ent player in the center of their screen. See Print Adjustments. This is kept for compatibility purposes, Loc_Print replaces it. sound / positioned_sound The channel enum has a single new flag: CHAN_FORCE_POS (bit 5) If set (and an origin is not supplied), the entity's origin will be forced to be used as the origin point of the sound even if there is a better position available. local_sound This function was introduced to deal with some split-screen issues that popped up. It's designed to mimick localsound of QuakeC; it will directly send a sound packet to the specified player, using a dupe_key if supplied (see unicast). See sound for info about the channels. get_configstring This function fetches a configstring from the servers' current configstring data. Com_Error See Print Adjustments. clip This is a new function designed to fit a specific purpose: it will test if the box specified by mins & maxs, moved from start to end, will clip against the specified entity with the given contentmask. As an example, you could use this to detect if an entity is actually intersecting a brush in a trigger instead of just being within its bounding box. inPVS / inPHS This function now accepts a boolean, portals, which changes whether or not it should ignore areaportals. BoxEdicts This function was modified with a simple filtering callback, greatly extending its purpose and removing some limitations that would occur with previous uses. The filter callback is called for every entity discovered, and you can choose to include or skip entities that it finds, or even completely abort the search. In addition, you can now call the function with a 0 maxcount, and the function will still continue to filter and find entities, reporting the final count. To match the old behavior, if a non-zero maxcount is supplied, the return count will cap out at maxcount. Note that it is disallowed to modify world links (linkentity/unlinkentity, etc) in a filter callback, it can only be used for filtering. unicast The dupe_key parameter is new, and is to solve a very peculiar issue with split screen players. When unicast is used to spawn effects or sounds, it may not be desirable to replay the same effect on multiple split screens, since split screen players are all the same client and share views. For example, if you do a unicast for a TE_BLASTER somewhere in the world for every player, for a split screen client with 4 players, that effect will play 4 times - even though all four players are viewing the same world. The game DLL also has no knowledge or understanding of split screen, so there's no way for the game to work around it. Instead of having the game need to know this kind of implementation detail and prevent double-sending, for effects that are going to potentially be sent to multiple players that may be on a split screen, you can specify a dupe key value. This value, when non-zero, will be marked as \"already sent\" for that client, and won't be sent again for the next packet if it was already tripped. The game DLL provides the GetUnicastKey global which will give you a rolling value to directly pass into unicast or local_sound. WriteFloat Implemented; this was stubbed out of the old code. WritePosition Now sends full float positions. WriteDir / WriteAngle Unchanged - WriteAngle is for compressed angles, when high precision is not necessary. WriteEntity New function to write an entity, to make it easier to write them without needing to WriteShort directly. GetExtension See Extensions. Bot_RegisterEdict Informs the bot subsystem that an entity needs to be registered. Bot_UnRegisterEdict Informs the bot subsystem that an entity needs to be unregistered. Bot_MoveToPoint Forces a bot to move to the specified point. Bot_FollowActor Forces a bot to follow the specified actor. GetPathToGoal The main pathfinding function; with the given pathfinding request, you'll be given info about the operation, the path, etc. Loc_Print The new primary entry point for printing. This function replaces all of the others (except Com_Print). For basic usage, it can be called on an entity (or nullptr for broadcasting) with the correct level, with the message to send in base, and nullptr args along with 0 num_args. For actual localized messages, however, you can send additional arguments via the args/num_args parameters which are sent to the client for further processing. In addition to localization, level now has new values and bit flags. PRINT_TYPEWRITER (4) Causes the message to be printed out one at a time, like a typewriter. Used for objectives, similar to the N64 version. PRINT_CENTER (5) An instant centerprint, like the legacy centerprints. PRINT_TTS (6) Identical to PRINT_HIGH in importance, but additionally causes text to speech narration to activate if enabled on the client. PRINT_BROADCAST (bit 3) Message will be sent to all players. PRINT_NO_NOTIFY (bit 4) Message will not be sent to the notify system. Draw_Line / Draw_Point / Draw_Circle / Draw_Bounds / Draw_Sphere / Draw_OrientedWorldText / Draw_StaticWorldText / Draw_Cylinder / Draw_Ray These functions are debugging aids that only render on the server. ReportMatchDetails_Multicast This function is solely for platforms that need match result data. ServerFrame Returns the server's frame number. SendToClipBoard Copy data to the server's clipboard, useful for debugging. Info_ValueForKey / Info_RemoveKey / Info_SetValueForKey See Info Keys. Save Games One of the major changes to this release of Quake II is the save system. Instead of storing pointer offsets and copies of memory, the level & game data is written to UTF-8 JSON. This makes save data much easier to navigate for a human & developer that wants to look into a bug, while also being quick and efficient for storage. The save system, as a result, no longer interfaces with the filesystem at all. Other mods are not required to use JSON, any text format will work as the server and client do not interact with the data. Game Export (read-only) apiversion The version # reported by the server. PreInit This function is called before InitGame, and should be where you initialize your mod's latched cvars. This can be used to fix any conflicting latched cvars, which will be \"locked in\" after this is called. SpawnEntities All three parameters are now properly marked const. WriteGameJson See Save Games. ReadGameJson See Save Games. WriteLevelJson This function is now informed whether the level write is from a level transition or a manual save. See Save Games. ReadLevelJson See Save Games. CanSave This new export now dictates whether the game is saveable or not. ClientChooseSlot ClientChooseSlot is intended to take in a bunch of information about the client that is connecting, and choose which edict_t entity this player should occupy. It is used in the rerelease to reorder players consistently throughout coop games, and ensure that everybody always gets the correct slot. Callers are given the player's userinfo and social_id (the social ID is a unique value per player on certain platforms), which you can use to find the correct slot from the current saved client data. You're also told whether the client isBot, which should always use non-saved available slots first. The ignore field will give you a list of slots up to num_ignore entities that are already occupied or were reported as such, so they can be safely skipped over. Finally, the cinematic parameters will tell you whether the loaded map is a video, which in most cases reordering will not be necessary. ClientConnect The function is now given the social_id and isBot state of the connecting client. RunFrame This function now receives a boolean to tell whether the call is from the main game loop, or from some other source (the game is settling, or running frames to advance level transitions). If the latter is occurring, you can use this boolean to speed up level transitions by skipping logic that is not necessary but is CPU-intensive, such as enemies searching for players to attack. PrepFrame This function used to be in the server, but is now controlled by the game DLL. It's ran after the game has execute a frame & has sent the packet data over to all players. Things like hit markers and one-shot events are cleared in here. edict_size / num_edicts / max_edicts These were changed to size_t and uint32_t/uint32_t respectively, to better represent their use. server_flags This is an integer shared between server and game, which stores bits for special states that the server cares about. Pmove See Pmove. GetExtension See Extensions. Bot_SetWeapon Called by the bot subsystem to switch weapons. Bot_TriggerEdict Called by the bot subsystem to trigger an entity. Bot_UseItem Called by the bot subsystem to use an item. Bot_GetItemID Fetch an item ID by a classname; for the bot subsystem. Entity_IsVisibleToPlayer This function is for item instancing; the rerelease of Quake II supports instanced items, which will display only for the players who haven't picked it up yet. For online players, this simply removes the item if you've gotten it, but for split screen players it will show a ghost where the entity was on players that have already picked it up. GetShadowLightData This function fetches data for the given shadow light for building client frames. Player State In the original client, player state was often accessed directly to perform various tasks or render things. Much of this has been moved into the cgame module to allow increased customization. Client Game Import (read-only) tick_rate (read-only) frame_time_s (read-only) frame_time_ms This holds the server's tick variables. They will be set at the start of the client, before Init. tick_rate stores the tick rate, in hz. frame_time_s is the time a game frame will take in seconds. frame_time_ms is the time a game frame will take in ms. These are provided pre-calculated for convenience. Com_Print Print a debug message to the client. get_configstring Fetch the given configstring data from the client. Com_Error Abort error for client. TagMalloc / TagFree / FreeTags Same as server. cvar / cvar_set / cvar_forceset Same as server. AddCommandString Push command(s) into the command buffer on the client side. GetExtension See Extensions. CL_FrameValid Returns true if the current frame being rendered is valid. CL_FrameTime Returns the current frame time delta. CL_ClientTime Returns the client's current time (server-bound). CL_ClientRealTime Returns the client's current real, unbound time. CL_ServerFrame Returns the client's server frame. CL_ServerProtocol Returns the client's connected server protocol. CL_GetClientName Returns a UTF-8 string containing the givern player's name. CL_GetClientPic Returns a string containing the given player's icon. CL_GetClientDogtag Returns a string containing the given player's dogtag. CL_GetKeyBinding Returns a key binding for the given key. Returns an empty string if the key is unbound. Draw_RegisterPic Precache the given image. Draw_GetPicSize Returns the size of the given image. SCR_DrawChar Draw the given conchars char at the specified position. A shadow parameter has been added to draw a drop shadow. SCR_DrawPic Draw the given pic at the specified position. SCR_DrawColorPic Draw the given pic at the specified location, with the specified color. SCR_SetAltTypeface Change whether the alternate (accessibility) typeface is in use or not. SCR_DrawFontString Draw a string to the screen, using the Kex KFONT which includes non-latin characters. SCR_MeasureFontString Measure the size of the string as it would be rendered. SCR_FontLineHeight Returns the line height of the font. CL_GetTextInput Returns a pointer to the current text input, and whether this input is for team say or not. CL_GetWarnAmmoCount For the given weapon ID, get the amount that is considered to be low ammo. Localize Localize the given string and arguments to an output buffer. SCR_DrawBind Draw a user bind to the screen, returns the Y offset from rendering. CL_InAutoDemoLoop Returns true if the engine is running the attract demo loop. Client Game Export (read-only) apiversion API version. Init / Shutdown Lifecycle functions for the client game. Note that the cgame does not control UI, so the cgame only exists when you are connected and in-game. DrawHUD This function is called by the client when their HUD needs to be rendered. isplit contains the split screen index of the player. data contains a pointer to some transient information from the server. This includes currently active layout, and the player's active inventory when the inventory is open. hud_vrect contains the unpadded rectangle of the HUD being rendered. hud_safe contains the size of the safe area. Only x and y are set, w and h are unused. scale is the integral scale of the HUD being rendered. playernum is the player's client index. ps is a pointer to the player's current player state. TouchPics Function called for precaching images used by the HUD. LayoutFlags For the given player state, return the layout_flags_t that would match it. GetActiveWeaponWheelWeapon / GetOwnedWeaponWheelWeapons / GetWeaponWheelAmmoCount / GetPowerupWheelCount The weapon wheel is in the client, but uses these callbacks to fetch data from player_state_t. GetHitMarkerDamage Returns how much damage was done for this player. Pmove See Pmove. ParseConfigString When a configstring is received, the cgame is also notified of changes. The cgame module can react to configstring updates here. ParseCenterPrint When a centerprint-like message is received by the client, it is sent to the cgame via this function. isplit is the split screen player it was sent to. instant is true if the message is a centerprint that is drawn without the typewriter effect. ClearNotify The client will call this when the notification area should be cleared. ClearCenterprint The client will call this when centerprints should be cleared. NotifyMessage When a notify message is received, the client will send it to this function. isplit is the split screen player it was sent to. is_chat is true if it was a chat-like message. GetMonsterFlashOffset To simplify the server to client muzzleflash communication, the cgame now exports muzzleflash origins via this function. GetExtension See Extensions. Quake II server protocol - version 2023 The Quake II rerelease features an updated server protocol. Most of the messages are backwards compatible, but some needed adjustments to work with new or changed features, or raised limits. This document will only outline the changes since the original release, rather than the whole protocol. (out of band, clientserver) challenges The out of band challenges have been removed. (out of band, client -> server) connect The connect message is similar to the original, but has redundant information removed. Port and challenge are handled at a lower level, so that information is not included. The connect message is in the following format: connect {protocol} {num split} {socials...} {userinfo...} protocol is 2023 num split is the number of split screen players socials is num split number of arguments containing each players' social identifiers userinfo is the clients' userinfo string, split up by groups of 510 characters each (since command arguments have a maximum length). This can often span 2 or more arguments, since each userinfo var has a value per player. See Info Keys. (out of band, server -> client) client_connect This message is sent when the server accepts the connection. It is in the following format: client_connect {protocol} protocol is 2023 The protocol version is sent mainly for backward compatibility with demos. (packet, server -> client) svc_muzzleflash (1) The following enum values are now accepted. MZ_BFG2 (19) Secondary muzzleflash for the BFG, sent when the BFG actually fires. MZ_PHALANX2 (20) Secondary muzzleflash for the Phalanx, sent for the second projectile. MZ_PROX (31) Sent when the Prox Launcher is fired. MZ_ETF_RIFLE_2 (32) Sent when the other barrel of the ETF Rifle is fired. (packet, server -> client) svc_muzzleflash2 (2) MZ2_BOSS2_MACHINEGUN_L2 / MZ2_BOSS2_MACHINEGUN_R2 (74 / 134) These two values were just copies of L1/R1, but were repurposed to make Hyperblaster-specific sounds for the new Hornet. The following enum values are now accepted. MZ2_SOLDIER_RIPPER_1 - MZ2_SOLDIER_HYPERGUN_8 (211 - 226) Muzzleflashes for the ripper & blue hyperblaster guards. MZ2_GUARDIAN_BLASTER - MZ2_ARACHNID_RAIL_UP2 (227 - 231) Muzzleflashes for the PSX monsters. MZ2_INFANTRY_MACHINEGUN_14 - MZ2_INFANTRY_MACHINEGUN_21 (232 - 239) Muzzleflashes for the Infantry's run-attack animation. MZ2_GUNCMDR_CHAINGUN_1 - MZ2_GUNCMDR_GRENADE_CROUCH_3 (240 - 250) Muzzleflashes for the Gunner Commander. MZ2_SOLDIER_BLASTER_9 - MZ2_SOLDIER_HYPERGUN_9 (251 - 255) Muzzleflashes for the guards' new prone-firing animation. (packet, server -> client) svc_muzzleflash3 (32) This packet was necessitated from running out of bits in svc_muzzleflash2. The only difference is the byte for id is a ushort. MZ2_GUNNER_GRENADE2_1 - MZ2_GUNNER_GRENADE2_4 (256 - 259) Alternate firing animation for the Gunner's grenade launcher. MZ2_INFANTRY_MACHINEGUN_22 (260) Alternate firing animation for the Infantry. MZ2_SUPERTANK_GRENADE_1 (261 - 262) Supertank's grenade launcher. MZ2_HOVER_BLASTER_2 / MZ2_DAEDALUS_BLASTER_2 (263 / 264) The Icarus and Daedalus' opposite side blaster. MZ2_MEDIC_HYPERBLASTER1_1 - MZ2_MEDIC_HYPERBLASTER1_12 / MZ2_MEDIC_HYPERBLASTER2_1 - MZ2_MEDIC_HYPERBLASTER2_12 (265 - 276 / 277 - 288) The Medic and Medic Commander's Hyperblaster firing animation sweep. (packet, server -> client) svc_temp_entity (3) As documented in WritePosition, WritePos now writes full float precision, so ReadPos has to read full float. TE_SPLASH (10) The \"color/splash\" enumeration accepts a new value: SPLASH_ELECTRIC (7) A spark used exclusively in N64, which spawns blue/white particles and makes sparking noises. The following new enum values are accepted: TE_RAILTRAIL2 (31) This effect was unused in Quake II, and was retooled to a lighter railgun effect used for Instagib mode. TE_BLUEHYPERBLASTER (56) \"Correct\" version of the old buggy TE_BLUEHYPERBLASTER, which is now TE_BLUEHYPERBLASTER_DUMMY. ReadPos ReadDir TE_BFG_ZAP (57) Laser when an entity has been zapped by a BFG explosion. ReadPos (start) ReadPos (end) TE_BERSERK_SLAM (58) Large blue flash & particles at impact point towards a direction. ReadPos ReadDir TE_GRAPPLE_CABLE_2 (59) The grappling hook in Quake II 3.20 used a larger message that didn't allow the cable to render like other player-derived beams. ReadEntity ReadPos (start) ReadPos (end) TE_POWER_SPLASH (60) Effect sent when a power shield evaporates. ReadEntity ReadByte (1 for screen, 0 for armor) TE_LIGHTNING_BEAM (61) A lightning bolt that originates from the player, like the heat beam. Unused. ReadEntity ReadPos (start) ReadPos (end) TE_EXPLOSION1_NL / TE_EXPLOSION2_NL (62 / 63) Variants of explosion that don't include any dynamic light. ReadPos (packet, server -> client) svc_sound (9) Since MAX_EDICTS is now 8192, this packet required changes to support higher entity numbers. MAX_SOUNDS being increased to 1024 also necessitated the sound index changing from byte to ushort. ReadByte (flags) ReadShort (soundindex) [if flags & SND_VOLUME] ReadByte (volume) [if flags & SND_ATTENUATION] ReadByte (attenuation) [if flags & SND_OFFSET] ReadByte (offset) [if flags & SND_ENT] [if flags & SND_LARGE_ENT] ReadLong (entchan) [if !(flags & SND_LARGE_ENT)] ReadShort (entchan) [if flags & SND_POS] ReadPos (origin) entchan is encoded as such: struct sndchan_t {uint8_tchannel : 3;uint32_t entity : 29; } flags contains the following bits: SND_VOLUME (bit 0) SND_ATTENUATION (bit 1) SND_POS (bit 2) SND_ENT (bit 3) SND_OFFSET (bit 4) SND_EXPLICIT_POS (bit 5) SND_LARGE_ENT (bit 6) Note that SND_POS is always set. This is to fix a legacy bug where sounds played on entities outside of your PVS will play at the origin instead of their real location. The client should pick the real position if the entity is in their frame, but otherwise fall back to the sound packets' position. (packet, server -> client) svc_print (10) This packet now supports PRINT_TYPEWRITER and PRINT_CENTER values. See Loc_Print. (packet, server -> client) svc_stufftext (11) For security reasons, this packet will only allow commands things to be executed. (packet, server -> client) svc_serverdata (12) ReadLong (protocol) ReadLong (spawncount) ReadByte (0 = game, 1 = demo, 2 = server demo) ReadByte (tickrate) ReadString (gamedir) ReadShort[N] (playernums; see below) ReadString (level name) To parse playernums, read the first short and check its value. If it is -2, then read an additional short, which is the number of split screen entities to follow. Read that number of shorts to get each entity number for each split screen player. Otherwise, the value returned by the initial ReadShort is the playernum of the client. The special value -1 will be used in cinematics, to indicate that the player has no entity. (packet, server -> client) svc_frame (20) ReadLong (serverframe) ReadLong (deltaframe) ReadByte (surpressCount) For each player in this client's numSplit the following data is parsed: ReadByte (areabits length) ReadData (using above byte) ReadByte (value will be svc_playerinfo) ParsePlayerState (see svc_playerinfo) Then, back to svc_frame data: client entity events should all be cleared back to EV_NONE ReadByte (value will be svc_packetentities) ParsePacketEntities (see svc_packetentities) svc_playerinfo (17) Bits #define PS_M_TYPE (1> 13 gunindex = gunindex_temp & ~0xE000 [if flags & PS_WEAPONFRAME] #define GUNBIT_OFFSET_X (1> 9 gunframe = (gunframe_temp & ~0xFE00) [if gun_bits & GUNBIT_OFFSET_X] gunoffset_x = ReadFloat() [if gun_bits & GUNBIT_OFFSET_Y] gunoffset_y = ReadFloat() [if gun_bits & GUNBIT_OFFSET_Z] gunoffset_z = ReadFloat() [if gun_bits & GUNBIT_ANGLES_X] gunangles_x = ReadFloat() [if gun_bits & GUNBIT_ANGLES_Y] gunangles_y = ReadFloat() [if gun_bits & GUNBIT_ANGLES_Z] gunangles_z = ReadFloat() [if gun_bits & GUNBIT_GUNRATE] gunrate = ReadByte() [if flags & PS_BLEND] screen_blend_r = ReadByte() / 255.f screen_blend_g = ReadByte() / 255.f screen_blend_b = ReadByte() / 255.f screen_blend_a = ReadByte() / 255.f [if flags & PS_FOV] ReadByte(fov) [if flags & PS_RDFLAGS] ReadByte(rdflags) ReadLong(statbits) for (i = 0; iclient) svc_splitclient (21) This packet indicates to the client which split screen player the next messages are directed towards, for unicast messages. ReadByte (isplit) Note that isplit will be offset by 1 (that is to say, a value of 1 indicates split screen client 0). (packet, server -> client) svc_configblast (22) Compressed configstring data. This is to make connection faster by sending fewer packets. ReadShort (compressed size) ReadShort (uncompressed size) ReadByte[compressed size] (buffer) The received buffer is directly passed through to zlib's uncompress. After decompression, until the buffer is exhausted, the following data repeats: ReadUShort (index) ReadString (str) (packet, server -> client) svc_spawnbaselineblast (23) Compressed baseline data. This is to make connection faster by sending fewer packets. ReadShort (compressed size) ReadShort (uncompressed size) ReadByte[compressed size] (buffer) The received buffer is directly passed through to zlib's uncompress. After decompression, until the buffer is exhausted, read in the data contained in a svc_spawnbaseline packet. (packet, server -> client) svc_level_restart (24) Sent when the server executes a restart_level command. The client should be prepared to do a \"soft wipe\" of their state, but might want to defer it until the full frame is read since effects might come in after this command is executed. This message's data contains configstrings that were changed by restarting the level. The following should be repeated until an exit condition is met: ReadShort (id) [if id is -1, exit] ReadString (str) (packet, server -> client) svc_damage (25) This message is sent after accumulating damage on a player. It gives the player a rough idea of the damage they're receiving and from where. ReadByte (count) For count number of loops, read the following: ReadByte (encoded) ReadDir encoded is in the following format: struct packed_damage_t {uint8_t damage : 5;uint8_t health : 1;uint8_t armor : 1;uint8_t shield : 1; } health provides a 1,0,0 addition to color. armor provides a 1,1,1 addition to color. shield provides a 0,1,0 addition to color. The damage value is also divided by 3, so multiplying it by 3 will get you an approximation of the real damage amount. The color is then normalized. (packet, server -> client) svc_locprint (26) This packet is the new entry point for prints. ReadByte (flags) ReadString (base) ReadByte (num args) ReadString[num args] (args) The base string is a fmtlib formatted string. The information in Print Adjustments and Loc_Print explains how formatting works. (packet, server -> client) svc_fog (27) enum bits_t : uint16_t {// global fogBIT_DENSITY = bit_v,BIT_R = bit_v,BIT_G = bit_v,BIT_B = bit_v,BIT_TIME = bit_v, // if set, the transition takes place over N milliseconds// height fogBIT_HEIGHTFOG_FALLOFF = bit_v,BIT_HEIGHTFOG_DENSITY = bit_v,BIT_MORE_BITS = bit_v, // read additional bitBIT_HEIGHTFOG_START_R = bit_v,BIT_HEIGHTFOG_START_G = bit_v,BIT_HEIGHTFOG_START_B = bit_v,BIT_HEIGHTFOG_START_DIST= bit_v,BIT_HEIGHTFOG_END_R = bit_v,BIT_HEIGHTFOG_END_G = bit_v,BIT_HEIGHTFOG_END_B = bit_v,BIT_HEIGHTFOG_END_DIST = bit_v }; ReadByte (bits) [if bits & BIT_MORE_BITS] ReadByte (morebits), bits |= (morebitsclient) svc_waitingforplayers (28) Sent when there are players waiting to join before the game can start (or zero if all players are in). ReadByte (count) (packet, server -> client) svc_bot_chat (29) Bots talking to players. ReadString (bot name) ReadShort (client index, or 256 if no particular player) ReadString (loc string) (packet, server -> client) svc_poi (30) Spawn a POI. enum svc_poi_flags { POI_FLAG_NONE = 0, POI_FLAG_HIDE_ON_AIM = 1, // hide the POI if we get close to it with our aim }; ReadUShort (key) ReadUShort (time) ReadPos (pos) ReadUShort (image index) ReadByte (palette index) ReadByte (flags) If a non-zero key is specified, only one of that POI key can exist at any given time. If time is 0xFFFF, the POI that matches the key will be removed. If time is zero, the POI will last forever, key should be set in order to allow the POI to be cleaned up later. (packet, server -> client) svc_help_path (31) Spawns the Compass help path effect at the given location. ReadByte (start) ReadPos (pos) ReadDir (dir) (packet, server -> client) svc_achievement (32) ReadString (id) (packet, client -> server) clc_stringcmd (4) ReadByte (isplit) ReadString (s) Note that isplit is offset by 1, so 1 is the first split screen client. About No description, website, or topics provided. Resources Readme License GPL-2.0 license Activity Stars 699 stars Watchers 10 watching Forks 39 forks Report repository Releases No releases published Packages No packages published Languages C 66.0% C++ 34.0% Footer © 2023 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact GitHub Pricing API Training Blog About",
    "commentLink": "https://news.ycombinator.com/item?id=37082771",
    "commentBody": "Source code for Quake 2 rereleaseHacker NewspastloginSource code for Quake 2 rerelease (github.com/id-software) 450 points by steveklabnik 11 hours ago| hidepastfavorite181 comments dietrichepp 8 hours agoQuake II was my first experience writing C.The code is clear, coherent, and straightforward. I’m not going to say it’s the best source code I’ve ever read, but it set a high bar. I’ve read source code to games since then, and I’ve seen all sorts of weird stuff… I’ve seen functions with nesting levels that go past the right side of the screen, I’ve seen functions a mile long that do a million things, you know. It was an early lesson that you could do cool things with simple code.We used Quake and Quake II to teach a VR class to kids in the late 1990s and early 2000s. You got a VR headset, you got a half-dozen PCs with Q.E.D. or Qoole, and you taught a room of 12-14 or 14-18 year-old students how to make their own virtual reality game.“Make your own virtual reality game” was, essentially, making your own Quake or Quake II level, without any monsters or guns in it. The story for the game could be told through on-screen messages triggered in the level. Students made all sorts of levels. One group made a level with slime you had to cross, and a button that turned on pumps to drain the slime (very much like what you would see in a standard Quake II level). Another group made a 3D maze out of water that you had to swim through, floating in space. I still have some of these student-made levels on my hard drive, after all these years, although I have trouble getting some of them to work.For the class, I made a mod that gave the players a flaregun instead of the blaster. Basically, it was still the blaster, but with a different sound effect and no weapon model. I modified the blaster bolt to be affected by gravity, bounce off of things, and go out after a certain amount of time.(If you were in that class—you may not have gotten the headset. We couldn’t always get it to work.) reply NovaDudely 3 hours agoparent\"I’ve read source code to games since then, and I’ve seen all sorts of weird stuff… I’ve seen functions with nesting levels that go past the right side of the screen, I’ve seen functions a mile long that do a million things, you know. It was an early lesson that you could do cool things with simple code.\"Guilty as charged!I used to chronically write macaroni code. So you would have like 25 lines of code that ended up being the back bone of like a dozen things that would have another dozen things stacked on top of each. Would execute super fast BUT would amount technical debt quickly and eventually said functions would become untouchable because you would risk breaking a lot of stuff build on top of stuff.Comments would try to clear it up, but communication skills where not the best. Explain function but not workings. Things like - \"Does SINE table\". \"Table Defrag\". \"Binary resolve\" etc.Get a lot of people going \"What the F*K is this shit?! But... it does the job damn fast\". If it was any field other that video games, I would not have lasted long and neither would the product... reply trilinearnz 6 hours agoparentprevCertainly id&#x27;s code is clean and well-structured, however I always had a tough time trying to grok it as a layperson. Comments (those within the functions) are relatively sparse, and the code often contains quite cryptic expressions that are somewhat bewildering to those without an high understanding of 3D graphics.This, of course, makes perfect sense as an output from an industry leader in the field (Carmack). However it definitely requires a lot of foreknowledge and, dare I say it, a mathematical bent to follow with confidence. reply pydave 7 hours agoparentprevAppreciation for that id code means you may appreciate \"John Carmack on Inlined Code (2014)\": https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=12120752 reply majestic5762 1 hour agoparentprevWhoa, I still remember watching a show about VR on the Discovery Channel around 1999 when I was a kid. They used Quake 2 as a demo, and it blew my mind. I was playing Quake 2 on my PC also, but the concept of VR was revolutionary to me at the time. It&#x27;s stuck in my head to this day. To now learn that using Quake 2 was a common practice for teaching VR. Maybe it was you in that documentary! reply metadat 7 hours agoparentprevCan you share any screenshots of the results of this course? It sounds amazing! reply pengaru 6 hours agoparentprevI never scrutinized the id software source, but man have I seen some horrifying game source. The bar is extremely low.What&#x27;s surprised me is that successful, fun to play, and stable enough games have been shipped with such absolute unmitigated disasters behind the curtain.It makes me question sometimes all the effort(and time) I put into preventing the chaos, when such carelessly bodged together garbage can be perfectly profitable. reply djmips 3 hours agorootparentIt&#x27;s nice when you can have both but I have yet to see a game review that mentions how clean the source code is! reply audunw 2 hours agorootparentprevThe id software source code gets increasingly cleaner for each game I think. I&#x27;ve read the Doom source code and Quake II&#x27;s. I remember Quake II being very clean and elegant. Doom is not quite there. It&#x27;s fairly well organized, the functions are generally short and concise, but it&#x27;s a mess of global variables. reply eddyfromtheblok 2 hours agorootparentprevThat garbage code brought about the speed run culture, highlighted by charity events like Awesome Games Done Quick. The speed runner and their crew on stage will explain the latest level skips, wall clipping to avoid tedious areas, enemy weaknesses and exploits, and frame perfect input strings needed to accomplish those. And some will perform speed runs blindfolded with only the audio cues to work with. reply flohofwoe 3 hours agorootparentprevThat&#x27;s usually because the high level gameplay code needs to be incrementally tweaked over months or years based on very subjective feedback like \"this doesn&#x27;t feel quite right, can you maybe try...\".You can start with a great plan, but no plan survives gameplay- and balance-tweaking for very long.Ideally the lower layers are much more structured though, usually those need to be maintained over a longer time and across games. reply augustk 1 hour agoparentprevTalking about clear and coherent, does anyone know if there is a deeper reason for using `break&#x27; and `continue&#x27; instead of pure structured programming? The snippet i = ent->client->chase_target - g_edicts; do { i++; if (i > maxclients->value) i = 1; e = g_edicts + i; if (!e->inuse) continue; if (!e->client->resp.spectator) break; } while (e != ent->client->chase_target);for instance, from the function ChaseNext in the file original&#x2F;rogue&#x2F;g_chase.c can be reduced to i = ent->client->chase_target - g_edicts; do { i = i % maxclients->value + 1; e = g_edicts + i; } while ((! e->inuse || e->client->resp.spectator) && (e != ent->client->chase_target));which in my opinion is clearer since the exit condition for the loop is in one place. reply _flux 47 minutes agorootparentUsing % vs comparison and reset was probably more efficient with the compilers of the time; might still be.Also I doubt everyone would agree that the one exit condition is more clear. For example, if I know that if e->inuse is 0 it will continue with the next round. Arguably it&#x27;s more difficult to understand that from the single combined expression—because it isn&#x27;t so: if !e->inuse but e == ent->client->chase_target, the original will loop but yours will exit.Though I&#x27;m guessing ent->client->chase_target->inuse is probably never 0. reply augustk 16 minutes agorootparent> if !e->inuse but e == ent->client->chase_target, the original will loop but yours will exit.No, the continue statement will jump to the test part also for `do&#x27; loops:\"The continue statement is related to break, but less often used; it causes the next iteration of the enclosing for, while, or do loop to begin. In the while and do, this means that the test part is executed immediately\"-- \"The C programming language (Second Edition)\" reply nsteel 38 minutes agorootparentprevPretty sure your line 3 and the original lines 3-5 are not equivalent. Check it with `i` entering the loop with value `maxclients->value`.The best reason not to re-write for style reasons is you break it. reply augustk 8 minutes agorootparentThanks, now corrected. reply andybak 44 minutes agorootparentprevI think it&#x27;s debateable which is clearer. Your while condition requires some mental parsing of the booleans whereas the original can be analysed one at a time.I&#x27;d say I prefer the original. reply mmh0000 10 hours agoprevOh man! Memories.Back in 1999, Quake II was the game I played (like, literally the only one). I sunk so much time in QII and I wrote my very first code building a Mod.I think it&#x27;s fair to say, Quake II turned me from a computer user into a computer programmer. And set the course for my entire adult life and career.From my original mod, I learned how to calculate vectors, learn an existing code base, integrate autobuilds for rapid testing.I also dabbeled in making maps, learning 3rd art, but, quickly realized I am no artist or designer.Sadly, my version control and backup policies at the time were non-existant and I&#x27;ve lost all my original code.The next game that really got my attention in the same way was Tribes 2, the scriptings and modding you could do there was truly amazing. reply mmh0000 10 hours agoparentIn my endless files, I just found the binary of my mod.https:&#x2F;&#x2F;xn0.co&#x2F;Xeno-Mod_1.9_Setup.exeBecause I&#x27;m sure everyone on HN just downloads execs from a random internet source and runs them!But if you have the OG QII, are curious, it is safe to run. reply koolba 8 hours agorootparentNo way I’m running that unless you publish it on NPM.Then you could even do: npx run quake-2-xeno-mod@1.9 reply epolanski 2 hours agorootparentSomeone making a Nix with quake 2 and that specific mod and making sure it runs fine on OpenBSD as we speak. reply mulmen 7 hours agorootparentprev> Because I&#x27;m sure everyone on HN just downloads execs from a random internet source and runs them!Not true, some of us have JavaScript disabled. reply giantrobot 10 hours agorootparentprev> Because I&#x27;m sure everyone on HN just downloads execs from a random internet source and runs them!...should I not? reply gonzo41 8 hours agorootparentThis is what work computers are for. reply rabbitofdeath 8 hours agorootparentThis. If you aren&#x27;t helping test your company&#x27;s security - who is?? reply klipklop 10 hours agorootparentprevWhat does your mod do? reply mmh0000 10 hours agorootparentThis is all from memory so, I might be missing some things:Adds 10 extra weaponsChanges the damage ratio of all weaponsImports the \"gravity mines\" from one of the Q2 expansion packsAdds togglable \"moon boots\" (low gravity, on a per-player basis)Adds a grappling hook, with swing physicsHas \"secret\" command to turn invisible. (I can&#x27;t remember what it is). Because back in 1999 I didn&#x27;t realize people could unpack binaries and read them. reply thadt 5 hours agorootparentMy cousin and I burned hours in Quake2 with a mod that did these. Jumping around in low G while grappling around all over the map while trying to hit each other was fantastic - thank you for some of the best early FPS gaming I can remember! reply mentos 10 hours agoparentprevTribes 2 was my first taste of modding, the game holds a super special place in my heart too. Then came Ultima Online emulators Sphere and RunUO that turned me from a gamer into a programmer too. reply n4te 8 hours agorootparentI had a lot of fun making the HaVoC Tribes 1 mod. It was really cool seeing there was 1000 people online playing my mod at once. reply harrid 3 hours agorootparent_the_ havoc? That&#x27;s awesome. I was toying around with tribes modding too but only trivial stuff. Havoc was always the ultimate thing you could do and an inspiration for me. Thanks! reply mmh0000 8 hours agorootparentprevYou made HaVoC?! I loved HaVoC! Thank you so much for making it and putting time into it.I assume this[1] is your website, please do not ever let the website go offline. It is such a perfect reminder of the time it is from.[1] http:&#x2F;&#x2F;havoctribes.com&#x2F; reply n4te 8 hours agorootparentHello! Yes! It&#x27;s cool you remember. I resurrected the site a while ago and gave it a domain. I also have a HaVoC server running on a Linode, so there will always be a server up. The master servers are a bit of a mess and it&#x27;s a little hard to setup a client, but it&#x27;s possible. I skipped T2 and haven&#x27;t played T1 in a long time. I played Tribes: Ascend for a while, it&#x27;s pretty good but now dying&#x2F;dead. I miss T1. PubG and others just don&#x27;t do it for me. Nowadays I play Rocket League occasionally and that&#x27;s it. reply ljm 9 hours agorootparentprevTribes 2 and Unreal Tourney were childhood faves.. when I played something other than CS 1.6 reply johngalt2600 8 hours agorootparentThis man computes reply mcronce 9 hours agorootparentprevRune was mine. A buddy and I got into level design, and I got into modding along with it reply anotherhue 10 hours agoparentprevDiscovering the drop down console was a revelation.An homage: http:&#x2F;&#x2F;guake-project.org&#x2F; reply bee_rider 9 hours agorootparentI wish I could find a use for that, it is a funny idea. Unfortunately in a world where all of you stuff is in terminals, having one special drop down one is less useful. reply bombcar 3 hours agorootparentNever use sudo again! make the drop down be always at your root shell. reply q87b 40 minutes agorootparentsv_cheats 1 reply Shared404 8 hours agorootparentprevI use Yakuake (KDE&#x27;s equivalent) for my password manager at work.I find most DE&#x27;s painful without that ability. The main exceptions are i3&#x2F;sway or cwm, where I just launvh a terminal and then kill it. reply cyberax 8 hours agorootparentprevI use iTerm2 that pops up F12, it&#x27;s surprisingly close to that drop console behavior. reply AdmiralAsshat 9 hours agorootparentprevThe Windows terminal now has a quake-style dropdown mode by default. reply hyperion2010 8 hours agoprevI&#x27;ll drop my Quake II anecdote here. Quake II was the first FPS I ever beat (at least I think I beat it). I did it over a week in the summer when visiting a friend out of state (never would have been allowed otherwise).When I started, I had only ever played the original DOOM, so I used those controls, and lo and behold they worked, and I thought nothing of it. At some point as I was playing along, my view suddenly became locked to the ceiling and I was thoroughly confused and stuck. Turns out I had bumped the mouse, and managed to figure out that mouse look was a thing after extended fiddling with the system to try to determine the cause and get back to killing badguys. Needless to say, the game got quite a bit easier after that.There wasn&#x27;t internet, neither I nor my friend knew what mouse-look was, we had never seen anyone play a true 3d fps. It is hard to imagine how obscure and alien the interfaces for games are because of how pervasive the knowledge has become.Glad to see the source code released! reply Zambyte 8 hours agoparent> Glad to see the source code released!In case you were unaware, this is actually the source code of the rerelease of Quake II. The source code for the original Quake II has been released for many years[0], along with many of the id Software classics[1].[0]: https:&#x2F;&#x2F;github.com&#x2F;id-Software&#x2F;Quake-2[1]: https:&#x2F;&#x2F;github.com&#x2F;id-Software reply ramranch 7 hours agorootparentIn case you were unaware, the source code of the original is included along with the re-release in this repository, as stated in the first sentence of the readme. reply stefncb 2 hours agorootparentI think Zambyte just wanted to let the GP know that the code for the original had already been released before this one. No need to be mean. reply epolanski 2 hours agoparentprevYou reminded me of the first time I saw a lan full of people playing an fps.I was at a ComicCon in Rome, Italy, circa 1999, anyway Half Life just hit gold production and released multiplayer demos there were at least 20 people all playing on lan multiplayer half life, with mouses too! Crazy.I went to many many lan parties since then up to 2005, then online match making killed quakenet and socializing. reply karim79 9 hours agoprevIncredible and just sent me way back to the nineties. To be fair, I&#x27;m talking about the first Quake game here. I had a PC rig on my bedroom floor which as I recall was a Pentium 166MMX, and my friend came over with a similar machine which was a bit faster (a 233) perhaps. No LAN cards insight.Using one of my dad&#x27;s ancient parallel cables and fooling Windows into thinking it was a LAN connection, which worked well enough, we played for days, until it got kind of tedious. So we then got into the whole modding scene - not creating anything, but downloading and testing the things which were out there. I will never, ever forget Girobot and KQP (Killer Quake Patch).The Girobot bots were a force to be reckoned with when they didn&#x27;t get stuck in the terrain. It was KQP which added all kinds of crazy mods&#x2F;weapons&#x2F;bot systems to the game that was the real gold. The Vampire Gun left us in stitches every time someone was hit by it, because it basically guaranteed a slow but not too slow death in which the hit player would crumble into little chunks of flesh. I miss the nineties very, very much. reply jasonb05 9 hours agoparent\"Girobot\", do you mean the \"Gyrobot\" by \"Gyro Gearloose\" [1]?Also, I believe KQP had ZeusBots included (for reference).[1] https:&#x2F;&#x2F;github.com&#x2F;Jason2Brownlee&#x2F;QuakeBotArchive reply samplatt 5 hours agorootparentZeusBots were where it was at, for me.First few computers were too slow to run Quake effectively, but then I had a Duron 900 (they ran slow AND hot!), bought a &#x27;PC Powerplay&#x27; magazine that came with a CD full of quake mods, and a map with 20-40 of ZeusBots worked just fine! reply jasonb05 3 hours agorootparentOh man, PCPowerPlay! I had my q3 mod written up in there in 2000 [1].[1] https:&#x2F;&#x2F;github.com&#x2F;Jason2Brownlee&#x2F;HumanDebrisArchive&#x2F;blob&#x2F;ma... reply karim79 8 hours agorootparentprevThank you for the correction, sorry, it has been forever. The link is fantastic, thank you for that as well. reply hypercube33 9 hours agoparentprevI had 8gb of downloaded-over-modem and sometimes T1 mods, maps, skins for quake 2 multiplayer. I miss the mods so much. reply autoexec 7 hours agorootparentThe endless amounts of user created mods, maps, and skins many of us enjoyed in early days were killed off so that game publishers could make a fortune charging us for a small selection of sanctioned DLC. In the process they killed off an entire ecosystem that grew up around user created content. Forums, review sites, file hosting, and generations of new talent who found their love of programing by creating mods all lost due to greed and a desire for control. reply dmclamb 8 hours agoparentprevmy dad&#x27;s ancient parallel cablesWith a null modem? I had same setup with my college roommate in our apartment. This was mid 90s, so it was Doom 2 and other FPS from that time.Great memories. reply karim79 8 hours agorootparentIndeed. My brother and I played starcraft using the same trick. When the fight started to escalate and there were hundreds of units fighting, the lag got really really bad, but it worked well enough that we kept coming back to it. That and Warcraft 2, Heretic, Hexen, all the rest. The best days in gaming IMO. reply ido 4 hours agorootparentit reminds me of the saying: the golden age of sci-fi is 15. reply jandrese 10 hours agoprevI saw grumbling elsewhere about how the re-release doesn&#x27;t include a Linux version, but here they are with the source and it claims to be tested with clang. Big props to iD for sticking to their principles. reply generichuman 10 hours agoparentI wonder why they released the source code for this one. Is it purely because of the principles, or did they have a legal requirement since the old one is also GPL licensed?I&#x27;m assuming they don&#x27;t have legal requirements since they owned the old code anyway. reply hadlock 10 hours agorootparentJohn Carmack has been fairly vocal about releasing the source code for his games, particularly the Quake&#x2F;Doom series for as long as I can remember, going back to at least Quakecon 2004 during one of his keynotes there. I think at the keynote when they announced Doom 3, he even talked at length about eventually releasing that engine as open source, I think in 7 years (which eventually did happen). He is not the flag carrier for open source games, but he may be one of the most consistent at following through on open sourcing their base engine over the last 20 years. Before leaving Meta as CTO he was speaking about making sure the GO and Quest 1 were usable in offline mode with sideload capability when they become EOL&#x27;d. In my view this release of code is not at all surprising in the least. reply PcChip 9 hours agorootparentI went to quakecon 2004, it was an amazing experience. I keep wanting to go back but never do, maybe next year I&#x27;ll go as a 20-year follow up. I have no excuse because I live very close to the hotel! reply pentagrama 8 hours agorootparentQuakecon 2023 is happening right now till Sunday, maybe you can get back some of those memories watching the Quake Pro League matches live stream https:&#x2F;&#x2F;youtube.com&#x2F;@Quake reply tekchip 10 hours agorootparentprevID has traditionally release code for their games when they reach...I forget, 10 years, or end of official support or something. This is just continuing the tradition of giving their games the longest of tale by opening them up. Check it out. https:&#x2F;&#x2F;github.com&#x2F;id-Software reply generichuman 10 hours agorootparentThey used to open source the code after 5 years, but I think they stopped open sourcing their games after Carmack left.It could just be that they started using other proprietary components in their code though.Would&#x27;ve loved to read the source code for Doom 2016! reply jsheard 9 hours agorootparentYeah they use a substantial amount of proprietary middleware now, so even if they wanted to open source the engine it would be difficult to pull off. As of Doom Eternal they&#x27;re using middleware for at least audio (Wwise), video (Bink), compression (Oodle), physics (Havok), culling (Umbra) and networking (Playfab), plus any references to console SDKs would have to be meticulously scrubbed from a public release, which was less of an issue back when they were a PC-first studio. reply badsectoracula 8 hours agorootparentRage should still be releaseable as Carmack wanted to release the source code at some point but he left id before that happened.I always found amusing that to replace him Zenimax built a whole new studio in Germany to hire the top programmers from Crytek and even then Doom 2016 used a litany of middleware :-P. reply jandrese 6 hours agorootparentShame they never finished Rage. It was a pretty good game up to the point where the boss came in and said they needed to have it out the door in a week. reply badsectoracula 6 hours agorootparentIt probably went through some heavy development hell, it was id&#x27;s longest game in development (7 years) and IMO it wasn&#x27;t really that great as a game - the shooting bits were great and felt very good (the best part of the game), the driving sections felt bad with the levels being too small to drive around and too large to walk and the quests felt tacked on. The various \"bonuses\" (like those things that hang in the air and you can pick up by making stunts) feel like excuses to pad out the game&#x27;s length.It was like the game didn&#x27;t know if it wanted to be an FPS or some seamless open world ARPG so it tried to do a bit of both and the only reason the FPS side didn&#x27;t felt bad was because id was masters at making FPSs by that time - but it was certainly dragged down.It would have been way better if it was a regular FPS with a linear progression of levels like id&#x27;s previous games. reply squeaky-clean 7 hours agorootparentprevDoom 3 even had that issue to a smaller degree. They had to rewrite some sort of lighting middleware before open sourcing. That&#x27;s just one piece of middleware though, and modern stuff is probably much more hairy and complex. reply jsheard 7 hours agorootparentThat one wasn&#x27;t middleware but rather a patent issue, the code in question was written by Carmack but he had unwittingly rediscovered a technique that had already been patented by Creative Labs. Id licensed it from Creative for Doom 3s commercial release but legal wouldn&#x27;t sign off on including it with the open source version so Carmack swapped it out for a different, slower algorithm.https:&#x2F;&#x2F;www.gamedeveloper.com&#x2F;design&#x2F;patent-issue-forces-new... reply chii 4 hours agorootparentThis is why software patents shouldn&#x27;t be a thing at all =( replyabrookewood 10 hours agorootparentprevIt&#x27;s such a good thing for the company to do - gives back to the community, generates a load of good will and ensures that we don&#x27;t lose the history of games. Props to ID. reply cma 9 hours agorootparentIt also helped them out for porting to phones and stuff for official releases if I remember right. reply fineIllregister 6 hours agorootparentprevI remember in one if Carmack&#x27;s last Quakecon keynotes, after the acquisition, he was talking about future source code releases. He didn&#x27;t say anything definitive, but he did tell a story where he spoke with a Zenimax lawyer, and they told him how important the GPL releases were. reply jsheard 9 hours agoparentprevThey shipped this version on all of the major platforms, and I&#x27;m reasonably sure that Nintendo and Sony both use Clang&#x2F;LLVM for their SDK toolchains nowadays. Maybe Linux was on their mind but getting it to build for modern consoles will have been the main motivation. reply wg0 8 hours agoprevThere are so many games that aren&#x27;t being sold anymore and aren&#x27;t playable on modern software&#x2F;hardware stack.I wish there were a law that if you don&#x27;t release a patch for your software for 20 years to make it workable for newer platforms, you&#x27;ve to release it as open source from the perspective of cultural significance. reply hoten 10 hours agoprevWhat are people&#x27;s thoughts on using \"_t\" as a suffix for types in C&#x2F;C++, as this source does? My understanding is that it is technically reserved for language defined types, but I haven&#x27;t come across a reasonable alternative (and having no suffix makes for odd code, for example a custom type for a \"viewport\" is better imo as \"viewport_t viewport;\" than \"viewport viewport;\") reply uxp100 10 hours agoparentI don’t believe that’s a c language thing, but a posix thing. anyway, in my own code for myself I do it, but it is frequently forbidden in coding standards I’ve worked under (even in a non posix environment) and it doesn’t rise to the level of pushing back on it usually, I’ll name types however the style guide wants, even if it’s worse. t prefix, worse. The whole word typedef, worse. reply bentcorner 9 hours agoparentprevIt&#x27;s useful when you have an IDE that doesn&#x27;t help you very much and you need to know what a thing is just from what code is in front of you. reply genocidicbunny 10 hours agoparentprevProbably not everyone&#x27;s cup of tea, but I tend to use CamelCase for types, and under_scores for variables. It does result in constructs like &#x27;Viewport viewport;&#x27; but at least it&#x27;s easy enough to tell that Viewport is the type. reply cpeterso 8 hours agorootparentMixing CamelCase type names and snake_case variable names feels chaotic to me, but it’s the standard coding style for Python, Ruby, Rust, and Google’s C++ code. I wonder who used it before Python. reply gdprrrr 54 minutes agorootparentIt&#x27;s been in Java since Java 1.0 reply flohofwoe 3 hours agoparentprevIt&#x27;s totally fine and legal. Only POSIX reserves the _t for types, but POSIX is not the C standard (but even when writing code for POSIX, it&#x27;s not like POSIX changes much these days, so unexpected type collisions because of POSIX updates will be very unlikely). reply OnACoffeeBreak 9 hours agoparentprevWhere I work the style guidelines for C specify to use \"_type\". Interestingly enough, the huge code base for embedded products with a custom OS does not rely ontypes for fixed-width integer types: \"uint32_t\", for example. Instead, a company-wide header defines \"uint32\" etc. without the \"_t\" suffix. I don&#x27;t know what guided that decision. reply flohofwoe 3 hours agorootparentFor a very long time the fixed-width types in stdint.h were not universally available, or in other headers, especially Visual Studio trailed behind. I guess this problem has stuck even after all C compilers caught up with C99. reply LexiMax 8 hours agoparentprevFrom what I understand, it&#x27;s POSIX that reserves the _t suffix, not C.That being said, I picked up the _s&#x2F;_t habit from working on idtech codebases, and decided that I&#x27;m fine with my own C codebases forever having a \"not POSIX compliant\" stamp on it, whatever that means in practice. For C++ I just use InitialCamelCase. reply badsectoracula 8 hours agoparentprevI&#x27;ve being using it for a long time and the only time i had a problem was when i ported an old 3d game engine of mine to Mac OS X and i had a type \"key_t\" which conflicted with some Mac header.I just used the preprocessor to #define key_t mykey_t (or something like that) after the Mac-specific headers in the couple of files where the conflict was and never thought about it again. reply djbusby 10 hours agoparentprevI&#x27;ve seen that suffix on loads of C stuff from ages ago. All stuff that circa 90s codes (some older) reply cxr 9 hours agoparentprevRecently I was musing to myself about adopting the convention `func_r`, to denote \"the thing that the function `func` returns\".Why spend the energy trying to think up two names (one for the function you&#x27;re writing and one for what you should call the type of its result)? It&#x27;s also amenable to preprocessor macros, if you&#x27;re so inclined. reply snvzz 7 hours agoparentprevI use and prefer capitalization for typedefs.Cube instead of cube_t. reply andrewmcwatters 9 hours agoparentprevYou&#x27;re not supposed to do it, but anyone who works with the Quake family of engines is used to it, so when in Rome... reply Cincinnati2 9 hours agoprevIf you were a Quake 2 fan and have access to an Oculus Quest 2, try out Quake2Quest. It was a real trip running around in maps that I spent so much time in back in the day. reply dopeboy 10 hours agoprevThis was the game for me growing up. It introduced me to clans, ladders, and really online gaming culture.My map was The Edge and my weapon was either the rail gun or rocket launcher. I was on dial up back then so finding a server was good latency was critical. Most annoying of course were the folks using that gun that caused lag for everyone.I know Q3 is the bigger game that perfected the genre but my favorite will always be Quake 2. Not only was there a solid single player + expansion packs, but the pace was just right for me. Everything got too fast when Q3 came around. reply emmelaich 4 hours agoparentYou might like the Quakelive servers that use VQL physics rather than PQL.VQL is much closer to Q2, whereas PQL is more CPMA or Quakeworld.Bonus, \"The Edge\" is also available in Quakelive. reply npace12 7 hours agoparentprevrail gun and you finish with the machine gun, gg reply xaduha 10 hours agoprevQuake II soundtrack is still in some sort of copyright limbo. It&#x27;s easy enough to find on Youtube, but it&#x27;s not on streaming services as far as I know.EDIT: maybe it&#x27;s getting sorted out, there&#x27;s even a fricking vinyl releasehttps:&#x2F;&#x2F;www.lacedrecords.com&#x2F;products&#x2F;quake-2-limited-editio... reply boredemployee 9 hours agoparentQuake always had amazing soundtracks. Nine Inch Nails \"nailed\" it on Quake 1. Quake 3 soundtrack is dope as well, but my all time favorite is the Quake 3 Rocket Arena vibes: https:&#x2F;&#x2F;youtu.be&#x2F;clkOT6vRKDg reply koala_man 9 hours agoparentprevPresumably this is a general comment, since this release does not (and wasn&#x27;t intended to) include any of the Quake II models, levels, graphical assets, sound effects, or music. It&#x27;s purely engine source code. reply nmfisher 9 hours agoparentprevI was a bit annoyed when I bought Quake II on Steam a few months back and found it didn&#x27;t come with the soundtrack. I just grabbed it from archive.org and dumped the files in the right folder, worked fine. reply bsimpson 10 hours agoparentprevIt was so good that I would sometimes put the game disc in a CD player just to vibe.Unfortunately that CD cracked, so I no longer have access to the music nor the game. reply ChrisMarshallNY 9 hours agorootparentI think Sonic Mayhem did QII. NIN did Quake I (or maybe just Trent). I remember the ammo boxes for the nailgun had the NIN logo. reply progmetaldev 9 hours agorootparentIf you use Spotify, you can actually find the Quake I soundtrack under Nine Inch Nails. A real blast from the past. Quake was my first FPS with multiplayer, and I hogged the only telephone line my family had, so I could play mods like capture the flag, including the grappling hook. This was also around the time I started exploring Linux with Slackware, and dialing into my local university professor&#x27;s Linux server to build web pages (before CSS was even available in browsers). Those are my favorite days of the internet, although it was much slower. reply johnwalkr 9 hours agorootparentYou can still buy Q1 soundtrack on vinyl. reply c-hendricks 9 hours agorootparentIt&#x27;s beautiful. It&#x27;s a double LP but only takes up 3 sides.The remaining side has audio code from the game etched into it. reply chungy 8 hours agorootparentprevIf you buy Quake 2 on GOG, the CD images are part of it. reply npunt 9 hours agoparentprevYeah Sascha (Sonic Mayhem) says digital may come: “I think digital outlets will follow. Not 100% tho.”https:&#x2F;&#x2F;twitter.com&#x2F;sonicmayhem&#x2F;status&#x2F;1689688600362037249 reply GreyMolecules 8 hours agoprevThe Quake series&#x27; source code is a must-read if you wish to learn how to create a fast-paced multiplayer game from scratch. Obviously, you can use Mirror or other 3rd-party middleware nowadays, but reading it gave me multiple layers of knowledge regarding the netcode aspect of a game. reply raffraffraff 3 hours agoprevI played the original Quake in college the year it came out. They gave away so much of the game on demo floppy disks that your could play deathmatch for free. We took over a whole computer room in the journalism school (trash 486s, but nobody really used that room so we were left alone).Quake II was at my first proper job, at DEC. I worked on the server&#x2F;storage support team. At 5pm when the phone lines closed we&#x27;d fire up Quake II on a server and at least 20 people would hang around for an hour or two after work to smoke cigarettes and play Q2. The mods are terrific. One person used a perfectly crafted Homer Simpson mod, complete with sounds effects sampled from the show. Also remember a \"Bananas in pyjamas\" skin and a small floaty robot (which was a bit of a cheat because it was harder to see than Homer). And the maps! Christ, so many great maps. Some of them are in Xonotic, and I still play DM in them occasionally. reply jfoster 5 hours agoprevThis is essentially Microsoft releasing this. After ZeniMax acquired id, they were acquired by Microsoft. reply r1ch 8 hours agoprevThe title is a bit misleading. In Quake-speak, the \"game\" is the what defines the game rules, hence what enables modding. This source release does not include the actual game engine, only the gameplay logic, which has traditionally always been released. reply schemescape 9 hours agoprevI didn’t realize Quake 2 had been re-released!For what it’s worth, the first expansion (The Reckoning) was probably my favorite single player Quake 2 experience. Favorite multiplayer was Devastation Quake, where one player is in a mech and everyone else has to team up to knock them out. reply UnlockedSecrets 9 hours agoparentQuake 2 remaster was surprise released and this was released at the same time. reply mjfl 9 hours agoprevWhy do people like Quake so much? I&#x27;m too young to understand. Does it do something unique compared to other shooter games? reply afavour 9 hours agoparentIt was absolutely revolutionary in its time, it’s difficult to think of any current day parallels because the industry has matured so much.For instance, Quake was the first FPS where the maps were truly 3D and could have one piece of floor on top of another. It would be considered insane for a game to not have that now. Quake(World) was the first truly massive online gaming FPS. Quake II was the first game I ever owned that used a 3D graphics card, I still remember the first time loading it up after I installed my card to set this wondrous coloured lighting that followed a rocket when I fired it.Anyway, point is you’re right, you are too young to understand. I don’t mean that in a patronising way, it’s just that it’s not impressive today in any way. The enthusiasm you see here is nostalgia. It’s a museum piece. reply cesarb 8 hours agorootparent> For instance, Quake was the first FPS where the maps were truly 3D and could have one piece of floor on top of another.And you could see both at the same time. The Build Engine used by Duke Nukem 3D could also have one piece of floor on top of another floor (or even going through another floor), but due to how it worked, if both were visible at the same time you would have a \"hall of mirrors\" effect instead of rendering properly. And also water: while the Build Engine faked it by teleporting the player to another room when entering or leaving a water area, Quake did not need that kind of trickery.Other than Duke Nukem 3D, other popular FPS games from that era also used \"2D map with varying height\" (like the Doom family), or even simpler things like rectangular 90-degrees-only single-height maps (for instance, Wolfenstein 3D). reply whoopdedo 8 hours agorootparentOr simulated floors with sprites as in System Shock. Also one of the few 2.5D engines to allow sloped floors.Also, \"first true 3D FPS\" depends on how you would classify Descent. reply badsectoracula 8 hours agorootparentprev> For instance, Quake was the first FPS where the maps were truly 3D and could have one piece of floor on top of anotherBethesda&#x27;s Terminator: Future Shock predated Quake by a year and also had truly 3D maps - which were often more expansive than Quake&#x27;s (also you could drive vehicles, fly a helicopter or something like that IIRC, enter various places to find loot - though the loot variety was limited to basically ammo, weapons and grenades). I think Quake got more attention because Doom made way more of a noise than Bethesda&#x27;s previous game (which AFAIK was a poor Wolf3D clone[0]) and unlike TFS Quake also had multiplayer support which was HUGE (TFS was criticized at the time for not having it).Amusingly enough TFS controls pretty much like a modern FPS with its mouse look and the ability to configure WASD whereas Quake was still a hybrid of mouse being used to move forward&#x2F;backward (unless you typed the \"+mlook\" command in the console) and again TFS was criticized for this.[0] ok, they also made Arena but still id&#x27;s previous game was way bigger and even at the time your last game was what people paid most attention to for the next one reply anthk 15 minutes agorootparent1: It was ID, not Betsheda.2: Doom&#x27;s impact was more huge than Quake. The MOD community still releases PWADs today. Doom basically boosted LAN multiplayer matches. reply bentcorner 9 hours agoparentprevIt was one of the very first FPS games where you had true mouselook. All the predecessors either didn&#x27;t (wolf3d&#x2F;doom&#x2F;rott), or faked it (ultima underworld). Notable exception is Descent (which is probably the actual first 3d FPS game), but IMO controlling a spaceship in 3d was a barrier to entry that Quake didn&#x27;t have. reply aeyes 8 hours agorootparentDescent had mouselook? Now I feel stupid. Or maybe I just didn’t have a mouse when I used to play it with friends, can’t remember. reply kanwisher 2 hours agorootparentDescent was closer to DOOM tech, not fully 3d but kind of a 2.5d . Quake was one of first true 3d games reply _flux 32 minutes agorootparentWhat was 2.5d about it? I mean the rooms needed to be convex IIRC and it had sort of a teleport thing going on, but it was basically all 3d, right? reply bentcorner 7 hours agorootparentprevTBH I could certainly be wrong about that. I remember playing with two hands on the keyboard and a gravis on the floor that I used my feet on to press buttons because movement was so complicated. reply InvaderFizz 7 hours agorootparentprevYes. Very difficult to play with a mouse, but the pros with a mouse were amazing. I had a few friends that were mouse players.The most popular choice of accessory was the Sidewinder 3D Pro. That was my choice of input device and I was easily a top 5% player in my teens.I never touched one, but many people swore by the SpaceOrb 360 as the pinnacle of controllers for Descent. reply h2odragon 9 hours agoparentprevIt did it all first.They did a lot to open the idea that game mods could be more than cheats and cracks, too.I still remember the grenade launchers and wild modded grenades fondly. the bouncing ones were so much fun in hallways. reply balls187 8 hours agorootparentOne of my favorite modes for Quake II: Weapons of Destruction. reply mewse-hn 8 hours agoparentprevQuake didn&#x27;t have the best gameplay but it had the best technology. Entering a true 3d world on your computer was possible for the first time.Duke 3D had gameplay that was more fun, but it was a 2.5D game with a Doom-style engine (it was much more sophisticated with scripting and slanted ceiling&#x2F;floors but still not true 3D like Quake).Post-release, Quake had patches called GLQuake which added 3d accelerator support, and Quakeworld which added internet movement prediction so playing online didn&#x27;t feel as jank. Quake 2 came out with these features integrated and more - colored lighting, transparent surfaces, etc. reply thefifthsetpin 9 hours agoparentprevWith DOOM (and a little bit with Wolfenstein), ID developed a game that encourages an almost reckless play style.You can get into a rocket fight with your opposition, sustain a fair number of hits, heal in the next room, and repeat. Their goal was to make you feel like an amazing demon-killing machine.Quake was one of the first PVP FPS games to bring that feel to the game. Death had minimal consequence, your goal was to find and kill the opposition, lethality was set low enough that you could shoot a rocket at your feet to \"jump\" higher.At least, that&#x27;s what made Quake (and ID games of that era) stand out to me. reply diegof79 7 hours agoparentprev> I&#x27;m too young to understandAllow me to share the perspective of someone around 18 years old at the time.First, Id Software created the FPS category with Wolfenstein. However, the company didn&#x27;t gain widespread recognition until they released Doom. Doom was exceptionally unique, to the extent that it made headlines in mainstream news.With the launch of Quake, although other FPS games existed, the expectations were high as everybody saw it as the successor of Doom. Also, Quake had a genuine 3D environment as opposed to Doom&#x27;s 2.5D. 3D graphics cards were not common, making Quake a marvel of PC gaming. Moreover, Quake featured an advanced mod system and a level editor, which were rare in games of that time.FPS games are now ubiquitous, and even budget smartphones have more 3D capabilities than high-end PCs of that era. So, the “wow” factor of Quake was extremely high. reply tekni5 9 hours agoparentprevPerfect blend of nostalgia, aesthetics, gameplay, simplicity not to mention that id software releasing everything open source at the end has continued development and created a massive amount of third party content, mods, etc. The same applies to Doom, Duke Nukem and to a lesser extent a few other older fps games. reply buerkle 8 hours agoparentprevCorrect me if I&#x27;m wrong, but wasn&#x27;t Quake the first shooter to have a separate server where players could join and quit at will? At least that&#x27;s what I remember at the time. In many games before Quake, if one player quit the whole game would quit. Rise of the Triad was great, but damn, it was painful if there was any sort of network hiccup. Quake was a revelation in that regard. reply proxyon 7 hours agoparentprevThe firsts are actually insane. Here are some of them:- First real 3D FPS game using OpenGL (GLQuake)- First real multiplayer 3D game with 16 players- First to introduce the idea of a \"clan\" or \"guild\" for a multiplayer game. It&#x27;s where the term \"clan\" came from- First esports game- First instance of Capture the Flag mod later used in other games- First instance of Team Fortress later used in other games (e.g. TF2)- First game to have \"demos\" (aka VODs) where you can rewatch &#x2F; replay a match- First game with a multiplayer competition match mode (Clanring or CRCTF or CRDM)- First web service allowing you to see what&#x27;s going in in games from the internet [qstat](https:&#x2F;&#x2F;github.com&#x2F;Unity-Technologies&#x2F;qstat)Aside from all of these firsts Quake 1 and Quake 2 were incredibly executed.- Soundtracks made by Nine Inch Nails. Who at the time ever heard of a tier 1 famous band making entire videogame soundtracks?- Creepy gothic lovecraftian demonic theme combined with space. The space parts were later ripped off by Half-Life. And you can see the influences even on modern cultural artifacts like the show Stranger Things (which itself seems to rip off Half Life&#x27;s storyline).Quake is basically the original online multiplayer game and had almost everything we have today but had it all the way back in 1996. The only things that game didn&#x27;t really have was deep API integrations to websites the way games have now or complex matchmaking with elo and ranks. reply mrbungie 6 hours agorootparentGreat summary. Just wanted to say that the Q2 OST (at least the base game) was made by Sonic Mayhem while the Q1 one was made by NIN.Both are great OSTs, Q1 is a \"ambient-horror-esque\" styled album [1] while Q2 OST [2] is practically industrial metal made using synths.[1] https:&#x2F;&#x2F;youtu.be&#x2F;5n8iU7abqkQ [2] https:&#x2F;&#x2F;youtu.be&#x2F;svRkAEoT5Hc reply xoac 8 hours agoparentprevI don&#x27;t think it&#x27;s that much any more based on what I&#x27;m seeing when I log into Quake Champions. For me Q3 Arena and variations was the pinnacle shooter duel experience. reply mensetmanusman 8 hours agoparentprevBunny hopping! reply Pxtl 7 hours agoparentprevTo be honest, a lot of it is nostalgia. The gameplay was pretty danged good, but feels pretty repetitive and meaningless to modern players - basically no plot or progression to speak of, an endless array of elaborately sculpted mazes with monsters in them for you to explore to find the way to get to the next level.However, Carmack was the most brilliant engine developer of his time and was heavily committed to open platforms, so that meant these games looked head and shoulders above their peers and played amazing online and had massive communities of modders and admins running the online games. Basically they created the modern FPS genre.Until Unreal Tournament and Counterstrike came along, the Quake series was the king of online FPS gaming. And even after they appeared, Quake 3 held its own. reply mrbungie 6 hours agorootparentNo plot or progression and just endless shooting? That&#x27;s almost some kind of meditation for me and other people.Both the resurgence of \"boomer shooters\" (at least in PC) and by extension the recent remastering of both Q1&#x2F;Q2 indicates an existing target group for these kind of games.Point taken that younger players may see no fun or meaning in games like this, but modern gameryoung gamer. reply badsectoracula 6 hours agorootparent> No plot or progression and just endless shooting? That&#x27;s almost some kind of meditation for me and other people.Indeed, some years ago i&#x27;d wake up, take a shower, make some coffee, do a quick play of the first episode of Quake (i stopped at first death but i pretty much knew the game so well by that point that i rarely died before the last couple of maps) to wake up and then go to work :-P. reply Pxtl 6 hours agorootparentprevHonestly if it were endless shooting I&#x27;d like it more -- for me that&#x27;s the Serious Sam series. Q1&#x2F;Q2 is a lot about navigating the map and rationing your ammo. Doom 1&#x2F;2 had more fun pitched battles but also had some very tedious map design -- games where I have to look for corpses to figure out where I&#x27;ve already been hold no nostalgia for me. reply ido 4 hours agorootparentMy main experience at the time was with Heretic (only slightly more sophisticated than Doom I&#x2F;II) - you&#x27;re right that it&#x27;s a bit tedious but at the time (when 3d was still a novelty) it somehow added to the feeling of really being there. Additionally I was 12 years old in 1995 which must have helped :) reply mrbungie 5 hours agorootparentprevYes, you&#x27;re right. 90s-early 2000s&#x2F;boomer shooters have different levels of \"map complexity\" ranging from open pseudo-flat landscapes or big rooms ala Serious Sam (afair, I haven&#x27;t played one in years) with tons of enemies, to more complex mapping and enemy positioning like Q1&#x2F;Q2&#x2F;D3D&#x2F;etc.One can drift naturally to one or another extreme, but the more the variety the better imho. I just enjoy simpler games both in graphics and gameplay. replysmellyshaft1 7 hours agoprevUpvote if the name \"Thresh\" rings a bell reply Solvency 6 hours agoparentUpvote if you remember watching the curbstomp that was Thresh vs Billox. reply CamperBob2 7 hours agoparentprevMakes me wonder whatever became of Carmack&#x27;s 328GTS. reply benreesman 4 hours agoprevI’m sure it’s widely known among readers of this thread, and I can’t vouch whatsoever for its veracity, but god damn is “Masters of Doom” a fun read. reply npace12 7 hours agoprevThis game and slackware3.4 are most of my memories of my early teen years. I played Q2 for hundred (if not thousands) of hours in multiplayer, more than any other FPS. And I only ever played q2dm1 (the edge) to the point to where when I tried other maps, it felt like I was playing a totally different game. The amount of subtle tricks like long jumps you could do due to the openGL \"bug\", or rocket launcher jumps were so awesome. I remember I played at fov 110 and somehow never missed with the railgun. reply tinix 3 hours agoprevhacking games based on the quake engine is how I learned to program initially (ld preload hooking and offsets and such). this brings back memories of hacker servers where we played against other hackers. it was a mix of programming skills and human agility. really fun times... the quake engine sent all world state to every player so you could build ESP functionality and other neat things. reply glonq 6 hours agoprevQuake 2 was surprisingly playable online even with late-1990s-era ping times.I played the hell out of the LMCTF mod back in the old days. Great times. reply goykasi 1 hour agoparentI played a ton of LMCTF too. The offhand grapple was such a game changer in terms of speeding across maps. The LMCTF community was also great: hours spent on IRC, competitive ladders, forums (nachos), etc. It was a great time to be into online games.Alliance for q3 was also great too, but it wasnt quite the same.There is a Facebook community alive for LMCTF. Last time I checked they have a few servers online and organize pick up games. reply jillesvangurp 2 hours agoparentprevWe had a thing in university where most of the research staff would do a few rounds of Quake 2 death match around the end of the afternoon. We were all on the same local network; so ping times were great. The same levels over and over again. Great fun. And very satisfying to hear the groans of agony after a rail gun blast. reply ljm 9 hours agoprevMy first thought is - what will the modders do.I watched some videos about MyHouse.wad (a Doom 2 level&#x2F;mod that became popular with the indie horror crowd), and it&#x27;s seriously impressive.I wonder what happens in the context of Quake, after seeing the community take Doom to places Carmack could never have imagined.I&#x27;m a lot more interested in how people mess with the rules of the game without necessarily breaking them, leading to some super subversive storytelling, rather than just porting the code onto a smart fridge or some shit. reply altano 9 hours agoparentIf anyone&#x27;s curious, start here: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=5wAo54DHDY0If you don&#x27;t care about Doom (or even video games, honestly): I haven&#x27;t even thought about Doom in 20 years. Just click the link and watch.One of my favorite YouTube rides of all time. reply dleslie 9 hours agoparentprevThe Quake source has been around for quite a while, this release is just for the rerelease.Quaddicted is a great site to find mods; QuakeSpasm is a fantastic binary for enjoying them; and nQuake is where the multiplayer is at. reply darepublic 7 hours agoprevI got in trouble for playing this in class from a teacher in her early thirties who I had a crush on. The red hot chili peppers had just released Californication. The matrix was all the rage and felt threatening to my young Christian sensibilities reply mvdtnz 7 hours agoprevWhat do the comments like `&#x2F;&#x2F; ROGUE` or `&#x2F;&#x2F; PMM` `&#x2F;&#x2F; STAND` mean? Also, can anyone explain the code in insane.cpp? Lots of magic numbers and hand-crafted arrays. reply trilinearnz 6 hours agoparentThis nomenclature was often used in licensed Quake codebases to indicate places where a vendor would put their own Mission Pack-specific alterations into the code. I guess they just &#x27;up-merged&#x27; the modified code to the main id source when they were done, or perhaps the changes were done by id on behalf of them?I believe there were similar bits in the Quake 1 source, e.g. for Scourge of Armagon (Mission Pack No. 1) you would see things like &#x27;&#x2F;&#x2F; HIPNOTIC&#x27;. reply mensetmanusman 8 hours agoprevAnyone have a history of being addicted to Action Quake 2? First time programming jumps! reply misterflibble 8 hours agoparentYes I really loved that mod reply bombcar 3 hours agorootparentAs someone pointed out to my similar comment, https:&#x2F;&#x2F;www.aq2world.com exists and is on steam for free. reply tekni5 9 hours agoprevQuake 2 Enhanced is very impressive, check out the subtle but clear differences they have made for each model: https:&#x2F;&#x2F;youtu.be&#x2F;UVKxhoA77Vo reply afavour 9 hours agoprevObviously it makes sense but it feels sacrilegious that there’s an official version of the Quake II codebase that relies on… JSON?! Everything should be either a tightly packed binary file or a .ini! (or something) reply Dulat_Akan 7 hours agoprevOh, this is the best cool game of my childhood, we play it on a sony playstation and the screen was divided into 4 reply Lorin 5 hours agoprevQball mod was likely the inspiration for Rocket League many years later. reply akamaka 9 hours agoprevIf I wanted to read the source code of just one version of Quake, to learn from it, which do you think would be the most interesting? reply badsectoracula 6 hours agoparentQuake 1 is most likely the simplest and most straightforward though it does have a bunch of alternative things for stuff like GLQuake vs software rendering.Quake 3 was designed to be used by 3rd parties and is more well put together, focuses on OpenGL and Carmack considers it his best C code.I&#x27;d say one of these two. reply bombcar 3 hours agoparentprevI would wait for Fabien to write a book on Quake I :)https:&#x2F;&#x2F;fabiensanglard.net&#x2F;three_books_update&#x2F;index.html reply ovao 9 hours agoparentprevThe question would be “by what dimension of interesting”?Quake 3’s source is pretty interesting from the perspective of id just transitioning from C to C++, and famously has an implementation of fast inverse square root. It’s also a pretty clean codebase overall with a reasonably modern engine architecture. reply bhelyer 9 hours agorootparentQuake 3 was still completely C, IIRC. You might be thinking of Doom 3? reply RockRobotRock 9 hours agoprevAre there any open source asset packs you can get? reply phkahler 8 hours agoprevCan they just offer the art assets too? Anyone interested is just going to hunt them down anyway. reply manual89 8 hours agoprevQ2 explosions were actual meshes which I thought was rather unique. reply bombcar 9 hours agoprevAction Quake II remains my perfect shooter. reply tekni5 9 hours agoparentStill has a tiny but active community on discord, also has been re-released as AQtion on Steam. reply mensetmanusman 8 hours agorootparentWhere on discord? reply tekni5 8 hours agorootparentdiscord.aq2world.com reply Pxtl 7 hours agoprevI hope we&#x27;ll see some old Quake 2 mods dusted off and reapplied to this new version. There were some greats back in the day - Action Quake 2, Transformers Quake 2, Superheroes Quake 2, etc. reply grendelt 5 hours agoparentDarn right. Lithium mod was my favorite. The grappling hook and runes made for some incredibly fun LAN parties! reply sbjs 5 hours agoprevCan anyone get this working? I followed all the instructions in the readme and can&#x27;t get it compiling. reply synergy20 6 hours agoprevrerelease in c++ that is,cool reply DrNosferatu 9 hours agoprev [–] ...WASM version anyone? replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The repository contains the code for the upcoming 2023 re-release of Quake II, offering new modding opportunities.",
      "The codebase requires a C++17 compiler and is compatible with multiple platforms.",
      "Changes have been made to enhance gameplay, optimize features like print formatting and sound attenuation, and fix bugs. The update also includes modifications to data values and structures, as well as new functions and variables.",
      "The save system has been improved to support importing/exporting JSON files.",
      "Updates to the server protocol ensure compatibility and introduce new features.",
      "Detailed information is provided on server-client communication, including commands and messages exchanged between the server and client."
    ],
    "commentSummary": [
      "The conversation explores various aspects of the Quake series, including the recent release of its source code and the nostalgia surrounding classic games.",
      "Participants delve into the impact of Quake on the gaming industry and its significance in terms of modding and technological advancements.",
      "The conversation also touches on coding conventions, different versions and platforms of the games, and the use of JSON and WASM in Quake."
    ],
    "points": 449,
    "commentCount": 181,
    "retryCount": 0,
    "time": 1691708230
  },
  {
    "id": 37076523,
    "title": "MS Teams channels cannot contain MS-DOS device names",
    "originLink": "https://learn.microsoft.com/en-us/microsoftteams/limits-specifications-teams",
    "originBody": "Skip to main content Learn Documentation Training Certifications Q&A Code Samples Assessments Shows Events Sign in Microsoft Teams Developers IT Pros and admins Learning Paths Troubleshooting & Support Developer resources Microsoft Teams admin center Download Teams Toolkit Search Welcome to Teams Teams Premium overview Get started Security, compliance, and privacy Manage and monitor Teams Teams, chats, and channels Meetings and audio conferencing Voice - Teams Phone and PSTN connectivity Devices and rooms management Manage apps in Teams Industries and government guidance Reference Teams settings and policies reference Limits and specifications for Teams Meeting templates reference Licensing Move from Kaizala Interoperability with Microsoft 365 and Office 365 Teams PowerShell Developer documentation for Teams Teams content updates Country and region availability for Audio Conferencing and Calling Plans Accessibility and conformance Shared channels errors Release notes for Teams admin features Download PDF Learn Microsoft Teams Add Limits and specifications for Microsoft Teams Article 08/01/2023 56 contributors Applies to: Microsoft Teams Feedback In this article Teams and channels Messaging Channel names Meetings and calls Show 8 more This article describes some of the limits, specifications, and other requirements that apply to Teams. Teams and channels Feature Maximum limit Number of teams a user can create Subject to a 250 object limit¹ Number of teams a user can be a member of 1,000² Number of members in a team 25,0006 Number of owners per team 100 Number of org-wide teams allowed in a tenant 5² Number of members in an org-wide team 10,000 Number of teams a global admin can create 500,000 Number of teams a Microsoft 365 or Office 365 organization can have 500,000³ Number of channels per team 200 (includes deleted channels)4 Number of Private channels per team 30 (includes deleted channels)4 Number of members in a Private channel 250 Maximum size of distribution list, security group or Microsoft 365 group that can be imported in to a team 3,500 Maximum number of members in a Microsoft 365 group that can be converted to a team 10,0006 Channel conversation post size Approximately 28 KB per post5 1 Any directory object in Azure Active Directory counts towards this limit. Global admins are exempt from this limit, as are apps calling Microsoft Graph using application permissions. 2 This limit includes archived teams. 3 To further increase the number of teams, you must contact Microsoft support and request further increase to the number of Azure Active Directory objects in your tenant. Increase is only made for real-life production scenarios. 4 Deleted channels can be restored within 30 days. During these 30 days, a deleted channel continues to be counted towards the 200 channel or 30 private channel per team limit. After 30 days, a deleted channel and its content are permanently deleted and the channel no longer counts towards the per team limit. 5 28 KB is an approximate limit because it includes the message itself (text, image links, etc.), @-mentions, number of connectors, and reactions. 6 Shared channels members from outside the team count toward this limit. Further note that teams/channel mentions are blocked in teams with over 10,000 members. Limits for shared channels The following table describes the maximum number of channels and members. Maximum... Value Notes Members in a team 25,000 Includes all users in the team and direct members in shared channels. Shared channels per team 200 Hosted and shared with the team. (Includes deleted channels during their 30-day recovery window.) Teams a channel can be shared with 50 Excluding parent team Members in a shared channel 5,000 direct members, including up to 50 teams. (Each team the channel is shared with counts as one member for purposes of this limit.) Real time updates are only available to 25,000 users at a time and only 25,000 users will appear in the channel list. The following limitations also apply: Only Azure AD work or school accounts are supported for external participants. Shared channels support tabs except for Stream, Planner, and Forms. Bots, connectors, and message extensions are not supported. Org-wide teams are not supported to be added as members of a shared channel. When you create a team from an existing team, any shared channels in the existing team won't be copied over. Notifications from shared channels are not included in missed activity emails. Shared channels are not supported in class teams. Messaging Chat Users who participate in conversations that are part of the chat list in Teams must have an Exchange Online (cloud-based) mailbox for an admin to search chat conversations. That's because conversations that are part of the chat list are stored in the cloud-based mailboxes of the chat participants. If a chat participant doesn't have an Exchange Online mailbox, the admin won't be able to search or place a hold on chat conversations. For example, in an Exchange hybrid deployment, users with on-premises mailboxes might be able to participate in conversations that are part of the chat list in Teams. However, in this case, content from these conversations isn't searchable and can't be placed on hold because the users don't have cloud-based mailboxes. (For more, see How Exchange and Microsoft Teams interact.) Feature Maximum limit Number of people in a private chat1 2502 Number of people in a video or audio call from chat 20 Number of file attachments3 10 Chat size Approximately 28 KB per post4 1 If you have more than 20 people in a chat, the following chat features are turned off: Outlook automatic replies and Teams status messages; typing indicator; video and audio calling; sharing; read receipts. The \"Set Delivery Options\" button (!) is also removed when private group chats contain more than 20 members. 2 Only 200 members at a time can be added to a group chat. See this article for more information. 3 If the number of attachments exceeds this limit, you'll see an error message. 4 28 KB is an approximate limit because it includes the message itself (text, image links, etc.), @-mentions, and reactions. Emailing a channel If users want to send an email to a channel in Teams, they use the channel email address. When an email is part of a channel, anyone can reply to it to start a conversation. Here are some of the applicable limits for sending email to a channel. Feature Maximum limit Message size1 24 KB Number of file attachments2 20 Size of each file attachment Less than 10 MB Number of inline images2 50 Note There is a throttling limit on how many emails you can send to a channel. The limit is six emails per ten seconds per channel per user and eight emails per ten seconds per tenant per user. 1 If the message exceeds this limit, a preview message is generated and the user is asked to download and view the original email from the link provided. 2 If the number of attachments or images exceeds this limit, you'll see an error message. For more information, see Exchange Online limits. Note Message size, file attachments, and inline images limits are the same across all Microsoft 365 and Office 365 licenses. Emailing a channel is not available in Teams for Office GCC/GCCH/DOD organizations. Channel names Channel names can't contain the following characters or words: Type Example Characters ~ # % & * { } + / \\ :?' \" , .. Characters in these ranges 0 to 1F 80 to 9F Words forms, CON, CONIN$, CONOUT$, PRN, AUX, NUL, COM1 to COM9, LPT1 to LPT9, desktop.ini, _vti_ Channel names also can't start with an underscore (_) or period (.), or end with a period (.). Meetings and calls Feature Maximum limit Number of people in a meeting (can chat and call in) With Microsoft 365 Business Basic, Microsoft 365 Business Standard, Microsoft 365 Business Premium, and Microsoft 365 A1 plans, you can host online meetings and video calls for up to 300 people using Microsoft Teams. With Microsoft 365 E3/E5, Microsoft 365 A3/A5, and Microsoft 365 Government G3/G5 plans, this limit increases up to 1,000 people. Number of people in a video or audio call from chat 20 Max PowerPoint File Size 2 GB Teams keeps meeting recordings that don't get uploaded to Microsoft Stream, available for local download 20 days Meeting recording maximum length 4 hours or 1.5 GB. When this limit is reached, the recording will end and automatically restart. For more information, see Meetings, webinars, and live events. Note Breakout rooms can only be created in meetings that have fewer than 300 attendees. In addition, creating breakout rooms in a meeting automatically limits the number of meeting attendees to 300. Advise your end-users to not initiate breakout rooms in meetings where they expect more than 300 participants. For more information on large Team meetings, share the guidance Best practices for a large Teams meeting with your end-users. Meeting expiration Note A meeting URL will never stop working. The expiry only relates to any PSTN dial-in numbers, CVI coordinates, and/or underlying meeting policies and settings. Meeting type Meeting expires after this much time Each time you start or update a meeting, expiration extends by this much time Meet now Start time + 8 hours Regular with no end time Start time + 60 days 60 days Regular with end time End time + 60 days 60 days Recurring with no end time Start time + 60 days 60 days Recurring with end time End time of last occurrence + 60 days 60 days Note Microsoft Teams meetings have a time limit of 30 hours. Live Events Live events are structured meetings that enable your organization to schedule and produce events that stream to large online audiences—up to 20,000 people. With live events, the audience interaction is a managed Q&A experience. Feature Maximum limit Audience size Up to 20,000 attendees 1 Duration of event 4 hours Concurrent Live Events running in a Microsoft 365 or Office 365 organization 2 15 1 The usual 10,000 is increased to 20,000 through December 31, 2023. You can schedule even greater numbers with live events in Viva Engage and/or Microsoft Stream. For more information, see Live events across Microsoft 365. Note that events over 20,000 attendees require the Live Events Assistance Program. 2 You can schedule as many Live Events as you want, but you can only run 15 at a time. As soon as the producer joins a live event, it's considered to be running. The producer who attempts to join the 16th live event gets an error. For more information about live events, go to Teams live events. See also Schedule a Teams live event. Important Microsoft 365 live event limit increases To continue supporting our customers' needs, we will extend temporary limit increases for live events through December 31, 2023, including: Event support for up to 20,000 attendees 50 events can be hosted simultaneously across a tenant Event duration of 16 hours per broadcast Additionally, Live Events with up to 100,000 attendees can be planned through the Microsoft 365 assistance program. The team will assess each request and work with you to determine options that may be available. Learn more. Presence in Outlook Teams presence in Outlook is supported on the Outlook 2013 desktop app and later. To learn more about presence in Teams, see User presence in Teams. Storage Each team in Microsoft Teams has a team site in SharePoint Online, and each channel in a team gets a folder within the default team site document library. Files shared within a conversation are automatically added to the document library, and permissions and file security options set in SharePoint are automatically reflected within Teams. Note Each private channel has its own SharePoint site (previously called \"site collection\"). If you don't have SharePoint Online enabled in your tenant, Microsoft Teams users cannot always share files in teams. Users in private chat also cannot share files because OneDrive for Business (which is tied to the SharePoint license) is required for that functionality. By storing the files in the SharePoint Online document library and OneDrive for Business, all compliance rules configured at the tenant level will be followed. (For more, see How SharePoint Online and OneDrive for Business interact with Microsoft Teams.) Because Teams runs on a SharePoint Online backend for file sharing, SharePoint limitations apply to the Files section within a Team. Here are the applicable storage limits for SharePoint Online. Feature Microsoft 365 Business Basic Microsoft 365 Business Standard Office 365 Enterprise E1 Office 365 Enterprise E3 Office 365 Enterprise E5 Office 365 Enterprise F1 Storage 1 TB per organization plus 10 GB per license purchased 1 TB per organization plus 10 GB per license purchased 1 TB per organization plus 10 GB per license purchased 1 TB per organization plus 10 GB per license purchased 1 TB per organization plus 10 GB per license purchased 1 TB per organization Storage for Teams Files Up to 25 TB per site or group Up to 25 TB per site or group Up to 25 TB per site or group Up to 25 TB per site or group Up to 25 TB per site or group Up to 25 TB per site or group File upload limit (per file) 250 GB 250 GB 250 GB 250 GB 250 GB 250 GB Channels are backed by folders within the SharePoint Online site (previously called \"site collection\") created for the team, so file tabs within Channels share the storage limits of the team they belong to. For more information, see SharePoint Online limits. Class teams Microsoft Teams for Education provides templates designed for unique education scenarios, such as classroom teaching. More information about team types, including class teams, is available in Choose a team type to collaborate in Microsoft Teams. A class team is a template type with additional apps included, and with limits separate to the number of team members. Note Using class teams requires an Office 365 Education license. Limits for class teams are listed in the following table: Feature Maximum limit Number of members in a team See the Teams and channels section of this article Number of members to use Assignments in a class team 300 Number of members to use a OneNote Class Notebook in a class team 300 A class team can support more than 300 members. However, if you plan to use either the Assignments app or Class Notebook app within your team, you will need to keep the number of members below the maximum limits above. Tags Feature Maximum limit Number of tags per team 200 Number of suggested default tags per team 25 Number of team members assigned to a tag 200 Number of tags assigned to a user per team 25 Contacts Teams uses these contacts: Contacts in your organization's Active Directory Contacts added to the user's Outlook default folder Teams users can communicate with anyone in your organization's Active Directory and can add anyone in your organization's Active Directory as a contact and to their contact lists by going to Chat > Contacts or Calls > Contacts. Teams users can also add a person who isn't in your organization's Active Directory as a contact by going to Calls > Contacts. Browsers Teams fully supports the following Internet browsers, with noted exceptions for calling and meetings. This table applies to operating systems running on desktop computers. Browser Calling - audio, video, and sharing Meetings - audio, video, and sharing1 2 Internet Explorer 11 Not supported Meetings are supported only if the meeting includes PSTN coordinates. To attend a meeting on IE11 without PSTN coordinates, users must download the Teams desktop client. Video: Not supported Sharing: Incoming sharing only (no outgoing) Microsoft 365 apps and services will not support Internet Explorer 11 starting August 17, 2021 (Microsoft Teams will not support Internet Explorer 11 earlier, starting November 30, 2020). Learn more. Please note that Internet Explorer 11 will remain a supported browser. Internet Explorer 11 is a component of the Windows operating system and follows the Lifecycle Policy for the product on which it is installed. Microsoft Edge, RS2 or later Fully supported, except no outgoing sharing3 Fully supported, except no outgoing sharing Microsoft Edge (Chromium-based), the latest version plus two previous versions Fully supported Fully supported Google Chrome, the latest version plus two previous versions Fully supported Fully supported Sharing is supported without any plug-ins or extensions on Chrome version 72 or later. Safari 15+ 1:1 calls fully supported.Safari 14+ 1:1 calls not supported. Group calls fully supported. Video: Fully supported Sharing: Fully supported Meetings: Fully supported Video: Fully supported Sharing: Fully supported Safari 13.1+ 1:1 calls not supported. Group calls supported with full audio support. Video: Incoming only Sharing: Fully supported Meetings are supported with full audio support. Video: Incoming only Sharing: Fully supported Firefox, the latest version plus two previous versions Not supported Meetings: Fully supported Video: Fully supported Sharing: Fully supported Note that users are required to have the OpenH264 plugin in Firefox for full support. Browsers without this plugin may see disruptions in the meeting, including in screen sharing activity. Learn more at Mozilla Firefox Support. Safari versions before 13 Not supported Meetings are supported only if the meeting includes PSTN coordinates. To attend a meeting on Safari without PSTN coordinates, users must download the Teams desktop client. Video: Not supported Sharing: Incoming sharing only (no outgoing) Safari is enabled on versions higher than 11.1 in preview. While in preview, there are known issues with Safari's Intelligent Tracking Prevention. 1 To give and take control of shared content during sharing, both parties must be using the Teams desktop client. Control isn't supported when either party is running Teams in a browser. This is due to a technical limitation that we're planning to fix. 2 Teams meetings on browsers are limited to a single stream; either incoming video feed of the current speaker or screen sharing. 3 Edge RS2 or later doesn't support sending real-time audio and video traffic through HTTP proxies. Note Running Teams in a browser is supported on PCs and Macs that meet the minimum Hardware requirements for Microsoft Teams. For example, running Firefox on the Linux operating system is an option for using Teams. On mobile devices we recommend that you use the Teams app. The Teams app is available from the Android and iOS stores. Operating systems For information about operating system requirements, see Get clients for Microsoft Teams. Feedback Submit and view feedback for This product This page View all page feedback English (United States) Theme Previous Versions Blog Contribute Privacy Terms of Use Trademarks © Microsoft 2023",
    "commentLink": "https://news.ycombinator.com/item?id=37076523",
    "commentBody": "MS Teams channels cannot contain MS-DOS device namesHacker NewspastloginMS Teams channels cannot contain MS-DOS device names (microsoft.com) 425 points by tapoxi 19 hours ago| hidepastfavorite314 comments kentonv 16 hours agoCirca 1998 I was a teenage Linux zealot who would attend LAN parties carrying a Linux box. It actually worked -- at the time, WINE practically existed to support Starcraft, Quake 2 could run natively, and that covered like 95% of what people were playing.One time I thought it would be funny to run a shell script that looped through every Windows share on the network and tried to open `CON&#x2F;CON` on it, resulting in a prompt Blue Screen of Death for each machine.For some reason my friends did not think it was funny. reply madrox 15 hours agoparentThis brings back memories. Every LAN group had that one friend like you. I&#x27;m sure you made up for it if, like my BSD friend, you were handy with network troubleshooting and always brought spare CAT-5. reply kentonv 14 hours agorootparentWell eventually I just... provided all the machines... and the house... https:&#x2F;&#x2F;kentonshouse.com reply deagle50 13 hours agorootparentYou&#x27;re a gentleman and a scholar. reply Mxrtxn 14 hours agorootparentprevThis is really cool! Did you build a similar house? reply kentonv 14 hours agorootparentNot similar, I built that house. (I&#x27;m Kenton.)Or do you mean the new one, alluded to on the site? It&#x27;s almost done... not ready to share yet. ;) reply rexreed 12 hours agorootparentGreat site - I am reading all your posts about the LAN party house now. Do you have the real estate listing of the house when it was up for sale? I&#x27;m just curious as to where it was located - you mentioned a sliver of land, and I saw the asking price ($2M in Palo Alto) so I&#x27;m curious to see what it looked like from the outside and its location, if you don&#x27;t mind sharing! reply en3r0 9 hours agorootparentprevNo way! The Internet is a small place. I remember seeing your original post and feeling very inspired to do something similar. One of these days I will find some time. reply cheerioty 14 hours agorootparentprevCAT-5, lucky you :) We still got zapped! reply Phrodo_00 12 hours agoparentprevWas that over IPX? I don&#x27;t think I ever configured IPX on Linux. By the time I was using enough Linux to run Starcraft on Wine, it already supported IP. reply AndrewKemendo 10 hours agoparentprevClassic! My favorite “anyone can do it hack” was ping flooding people during a lan-partyGood old memories reply code_runner 16 hours agoparentprevlove this story very much. I had no idea WINE had been around for that long and was able to play starcraft etc. I have really great memories of playing starcraft with friends back in the day.... that game had enough staying power that we could all get into and out of in middle school and then again in college! reply cogman10 15 hours agorootparentWine is nearly as old as linux. First released in 1993.I had some awkward conversations with my parents as a teenage nerd looking into \"winehq\". They didn&#x27;t believe me that it had nothing to do with alcohol. reply FirmwareBurner 14 hours agorootparentYour parent being concerned about you browsing something about wine?You&#x27;re definitely not be French&#x2F;European. Here our parents give us wine :) American? reply cogman10 14 hours agorootparentmormon family. reply FirmwareBurner 14 hours agorootparentThanks, Mormons are some of the most nicest and chill Americans I&#x27;ve met in Europe. Had no idea alcohol is taboo. reply astrange 12 hours agorootparentThey&#x27;re also good at getting security clearances since they never do anything, so those might&#x27;ve been CIA agents. reply FirmwareBurner 12 hours agorootparent>They&#x27;re also good at getting security clearances since they never do anythingI never understood why you should mass deny security clearances to people who like to \"party\". reply TazeTSchnitzel 12 hours agorootparentThey don&#x27;t want people who might have done something embarrassing in their past, because foreign adversaries might try to blackmail them with that. reply stouset 4 hours agorootparentOn the flip side, those are the exact people who often wouldn’t be embarrassed or ashamed of that kind of behavior. I’ve done a lot of things, and I’d openly admit just about all of it if anyone asked.On the other hand if you do find a Mormon who took some liberties in an earlier time in their life… reply boffinAudio 1 hour agorootparentprevThey&#x27;re good at being CIA agents because they are supple and compliant and the CIA has tools to make that compliance into nationalistic zealotry that rivals the capabilities of most religions, having refined it through operations such as paperclip, etc.. replysaghm 13 hours agorootparentprevEven today, Blizzard games tend to run pretty easily on Wine. I always attributed it to them using a fairly old tech stack, but maybe it&#x27;s the reverse and Wine development has just continued to go above and beyond for those games. reply neurocline 4 hours agorootparentIt was mostly on purpose, that Blizzard games worked well in Wine. We even fixed a few things in our code from time to time that accidentally made Wine emulation hard. reply kcb 12 hours agorootparentprevI know WoW is aware that&#x27;s it&#x27;s running in Wine. There are some setting greyed out in game with a message of not compatible when running in Wine. reply DoesntMatter22 15 hours agorootparentprevIt&#x27;s still being played heavily and professionally in Korea and there are many non Korean tournaments too.I watch pro matches everyday! reply RainaRelanah 14 hours agorootparent> heavilySadly not the case, as much as I love ASL the scene has zero new blood. SC2 really killed the Brood War scene, and League of Legends really killed SC2. The days of packing an aircraft hanger with 30,000+ fans for OSL finals are long gone.I was invited to the Stormgate alpha, and while I can&#x27;t say anything about it (NDA), I am hopeful it will bring new life into RTS esports. reply vitaflo 12 hours agorootparentIvOry just qualified for ASL. That’s definitely new blood. reply DoesntMatter22 13 hours agorootparentprevIt&#x27;s still heavily played. There are tons of show matches and ASL which is good. There are newer players like Scan, and we had a couple new guys last season.Is it in it&#x27;s heyday? No. Is it still fantastic? Yeah.KESPA killed Brood War primarily because Blizzard forced them to. Then KESPA itself died.Lots of great BW still being played no question about that reply qsdf38100 11 hours agorootparentprevAh, good times. Lim yo han!! (Aka slayers boxers ? Right ?). As a Terran player, I was in such awe. And then came Flash (lee young ho), the GOAT. I watched every one of his games. StarCraft 2 never came close to the accidental perfection that SC1 is. reply KETpXDDzR 7 hours agoparentprevOh good old Windows nuker. reply EspressoGPT 16 hours agoparentprev> Circa 1998 I was a teenage Linux zealot who would attend LAN parties carrying a Linux box.Arch, I suppose. reply saalweachter 15 hours agorootparentArch wasn&#x27;t around in 1998.I&#x27;m going to put my money on Slackware. reply kentonv 15 hours agorootparentI think it was RedHat.I had tried Debian first but switched to RedHat pretty quickly afterwards.(Back then, RedHat had nothing to do with \"enterprise\", it was just the most polished Linux distro around. Ubuntu didn&#x27;t exist yet.)(These days FWIW I use boring old Debian, and I&#x27;m not a zealot about it. It works for me. Use what works for you, I don&#x27;t care. I have a separate machine for gaming and it runs Windows. I know Steam works pretty well on Linux these days but Windows is still less hassle for games.) reply doubleg72 8 hours agorootparentI ran Mandrake back then but also toyed with RH and Slackware. Back when all the leets ran enlightenment or fluxbox! reply godzillabrennus 15 hours agorootparentprevBack when Redhat was the name of the enterprise releases and the open source releases. reply kentonv 15 hours agorootparentI think there was no separate enterprise release at the time. But could be misremembering. reply chungy 14 hours agorootparentRed Hat Linux 6.2E was the first \"enterprise\" release they made. Basically just standard RHL 6.2 but with long-term support. This release can be retroactively thought of as \"RHEL 1\"; The proper distribution that debuted as Red Hat Enterprise Linux 2 was based on Red Hat Linux 9, with the similar promise of long-term support. Future RHEL releases are carved out of Fedora releases, the spiritual successor to RHL. reply Agingcoder 14 hours agorootparentprevNo you’re right. If memory serves me well, I used to switch between Slackware 3, redhat 2 and Debian 1.2(?) at about that time. I never liked redhat, enjoyed Slackware but ended up not enjoying the lack of a package manager, so Debian did the trick a few years later. There was a single redhat release. These days, just like you, I use Debian. reply xtracto 14 hours agorootparentprevOoooh that brought me memories. At the time testing (around &#x27;98) all the nerdy rage was on testing the different distros such as Caldera, Debian, Corel, Mandrake. I remember at the same time compiling my first kernel in FreeBSD (ordered the CDs from WalnutCreek website! what a time). My parents where not amused that I broke the home PC several times installing this thing over Windows reply JoelMcCracken 15 hours agorootparentprevI used to love slackware, by far the best linux experience. It just worked well. reply HeyLaughingBoy 14 hours agorootparentProbably. I just remember installing from a dozen+ 3.5\" floppies onto my pizzabox &#x27;386 and breathing a sigh of relief when it booted. reply mkl 14 hours agorootparentI think I needed 17 floppies ... and I could only scrounge up about 8, so I assumed it wouldn&#x27;t need the earlier ones again and reused them for later parts. It worked! I didn&#x27;t use Slackware much though, as I had no way to connect that machine to the internet, so it was really hard to learn how to do stuff. reply HeyLaughingBoy 12 hours agorootparentI think I actually still have an \"Introduction to Slackware Linux\" book. reply saalweachter 14 hours agorootparentprevI started with Slackware and was happy with it, but must admit that is entirely because my Linux book from B&N came with Slackware on a CD. reply vmlinuz 12 hours agorootparentprevI still do. It still does... reply Q6T46nT668w6i3m 15 hours agorootparentprevI’d bet RedHat 5.2. reply timcambrant 13 hours agorootparentThat was my first. Then Slackware 3.6. Both as cover CD:s of computer magazines because that download was too much to handle on 56k. reply g105b 16 hours agorootparentprevif it were arch you&#x27;d know already reply gerdesj 14 hours agorootparentSorry, how remiss of me! On behalf of Archoles everywhere: \"I use Arch actually\" (and so does my wife). reply conscion 19 hours agoprevThis is most likely due to those names not being allowed for files or folders in the Windows file system. MS Teams channels create a matching folder in SharePoint where file attachments are stored. reply Zelphyr 15 hours agoparentMicrosoft, when confronted with a problem, think \"I know, I&#x27;ll build it on top of SharePoint.\" reply reilly3000 11 hours agorootparentEvery time, it seemed like it was going to be so easy and powerful. Each time, it turned out to be impossible to implement relatively vanilla functionality. reply mattgreenrocks 13 hours agorootparentprevIs Teams built atop SharePoint, or does it just integrate with it? reply twobitshifter 13 hours agorootparentCreating a new team makes a sharepoint group complete with calendars, a site, email addresses, and file folders. reply gadders 16 hours agoparentprevI worked at a bank that had a windows-based trading system and for some reason created folders to hold the details of each book. There were issues when a trader decided to call a book \"LPT1\". reply boppo1 14 hours agorootparentWhat is a &#x27;book&#x27; in the context of trading? reply jll29 13 hours agorootparentshort for \"orderbook\" - the set of pending trades pertaining to a client account reply grumpyprole 14 hours agorootparentprevJust an old name for a portfolio of transactions reply 0x264 14 hours agorootparentprevSet of related trades. reply yellow_lead 19 hours agoparentprevI bet there is another vulnerability here, although they also seem to blacklist % and .. reply Dwedit 17 hours agorootparent\\\\?\\GLOBALROOT\\ anyone? Now you&#x27;re opening devices from the NT object namespace rather than files.Bonus: You can create the file C:\\con\\con using NT Native API filesystem calls. Someone even made a video of installing Windows into C:\\con\\con, and the kernel-side stuff works beautifully, and the explorer shell side dies horribly. reply dustingetz 16 hours agorootparentAOL chat would bluescreen a Win95 box if someone sent a chat message like “}s\\con\\con” - }s was the directive to play a sound reply rejectfinite 16 hours agorootparentprev>the kernel-side stuff works beautifully, and the explorer shell side dies horribly.So, just like running Windows normally then? :D reply postmodest 14 hours agorootparentNT is Dave Cutler&#x27;s best ideas for OS design, atop which Very Bad People piled the Win95 API and UX reply naikrovek 18 hours agorootparentprevmost of those are the usual forbidden path and filename characters.% is used to surround environment variables for interpretation, for example. reply lostlogin 18 hours agorootparent> the usual forbidden path and filename characters.It’s pretty irritating when files and folders on my Mac can’t be uploaded to Teams due to this limitation. It’s only forbidden for Windows users. reply reaperducer 18 hours agorootparentIt’s only forbidden for Windows users.I think the only character that Macs don&#x27;t allow is the colon. AFAIK, everything else is fair game, even emojis.I just created a project with \"ê\" in the title. I wonder if I&#x27;ll be able to share that with my Windows coworkers on Teams. reply yjftsjthsd-h 17 hours agorootparent> AFAIK, everything else is fair game, even emojisFWIW, I generally expect emojis to be more compatible than other symbols, because they have no legacy meanings - ex. ™ has never been a path separator, or indeed anything else. reply extraduder_ire 17 hours agorootparentMain problem with \"emojis\" is that they live outside the basic multilingual plane, and so make bad utf-16 handling really obvious. This is a blessing in disguise, because it acts as a no-brown-m&ms thing, since it&#x27;s more unarguably broken than not being able to use cuneiform, or musical symbols. reply yjftsjthsd-h 17 hours agorootparentYeah, point; I probably should have said something more like \"emoji shouldn&#x27;t have alternative meanings, so they should work if your unicode support is functional (which is a big caveat).\" On the bright side, yeah, emoji have been great at pushing things to handle unicode nicely and act as a... I dunno, natural fuzzing case? reply wizofaus 14 hours agorootparentprevThe problems show up when that name gets stored in a VARCHAR column somewhere... reply veave 16 hours agorootparentprev™ is not an emoji. It&#x27;s part of the \"Letterlike Symbols\" block. reply yjftsjthsd-h 16 hours agorootparentOops, good point. I think I&#x27;m going to leave it as-is on account of not knowing of any actual emoji that HN allows, but yes that is technically incorrect on my part. reply veave 16 hours agorootparentI think the confusion comes from the fact that some platforms render it as a drawing as if it was an emoji. I never understood why... reply yjftsjthsd-h 15 hours agorootparentSpeaking only for myself, the confusion comes purely from the fact that I think of all text as \"ASCII\" or \"Unicode\" (...or \"something else that nobody should use in the modern era\"), and I don&#x27;t really distinguish within \"valid unicode that isn&#x27;t ascii\". reply veave 1 hour agorootparent™ is part of extended ASCII :) replyduskwuff 15 hours agorootparentprev> I think the only character that Macs don&#x27;t allow is the colon.Colon is allowed in the filesystem; it&#x27;s displayed as a forward slash in the UI.(There&#x27;s historical reasons they do this: Classic Mac OS used colon as a path separator.) reply recursive 18 hours agorootparentprevYes. NTFS has no problem with non-latin letters and emojis in file names. reply qingcharles 18 hours agorootparentI had to test it, but this is true. I just renamed my TODO.txt to [sobbing emoji].txt reply justsomehnguy 17 hours agorootparentprevNTFS itself has not that many forbidden characters (though &#x27;:&#x27; is one of them, it denotes alternate stream).What people think about those CON, PRN, AUX, NUL etc are not filesystem limitation.And while we are here - nor backslash, nor forward slash are used in NTFS. It can care less about what char do you use for a directory separator. Just be sure to update your APIs. reply nxobject 16 hours agorootparentNTFS on non-Windows systems aside, I wonder whether there are any \"pure NT\" environments you can access in Windows where you can create and use these folders. reply bombcar 15 hours agorootparentYou just need to come in below whatever layer notices them. I&#x27;m sure they&#x27;ve been used to smuggle viruses in, so the layer that blocks them may get lower and lower. reply tenebrisalietum 16 hours agorootparentprevYou can do some stuff under Cygwin that&#x27;s bothersome under explorer.exe, like delete files with really long pathnames. reply aidenn0 16 hours agorootparentprevLinux treats filenames as bytes and only disallows the ascii slash and null. All other bytes are fair game. reply cesarb 15 hours agorootparent> [...] and only disallows the ascii slash and null. All other bytes are fair game.There&#x27;s also the special treatment of \"\", \".\", and \"..\" (that is, a file or directory name consisting entirely of zero, one, or two dots), and the convention that a name starting with a dot is hidden. reply inkyoto 5 hours agorootparent> and the convention that a name starting with a dot is hidden.It is a UNIX shell implementation level convention. The file system and the kernel don&#x27;t care, and a shell is not obliged to honour the convention. reply slaymaker1907 16 hours agorootparentprevYes, and it was an absolutely horrible mistake in the other direction. If I give you a path “.&#x2F;something&#x2F;example.txt” it could be a file called example.txt in a folder called something or it could be a single file called “something&#x2F;example.txt”. reply markrages 9 hours agorootparent> disallows the ascii slash reply hn92726819 16 hours agorootparentprevIt looks like non-utf8 is not allowed: https:&#x2F;&#x2F;superuser.com&#x2F;questions&#x2F;204287&#x2F;what-characters-are-f...Maybe it goes without saying, but for completeness, &#x2F; is not allowed in filenames, and neither is null terminator (0x00 or \\0) on mac either.On Linux, only &#x2F; and null terminator are banned from filenames. reply slaymaker1907 16 hours agorootparentSadly, you are not correct about slashes. They are allowed in filenames for Macs https:&#x2F;&#x2F;alexwlchan.net&#x2F;2021&#x2F;slashes&#x2F; reply hn92726819 6 hours agorootparentThanks for the correction! It seems more complicated after looking more into it.According to https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Filename , reserved characters are:HFS:> :HFS+:> : on disk, in classic Mac OS, and at the Carbon layer in macOS; &#x2F; at the Unix layer in macOSAPFS:> In the Finder, filenames containing &#x2F; can be created, but &#x2F; is stored as a colon (:) in the filesystem, and is shown as such on the command line. Filenames containing : created from the command line are shown with &#x2F; instead of : in the Finder, so that it is impossible to create a file that the Finder shows as having a : in its filename.TIL about a &#x27;carbon layer&#x27; and &#x27;POSIX layer&#x27; reply vtee44 16 hours agorootparentprevIt doesn’t make sense tbh, it just causes confusion when someone is using terminal. Slashes in file names are forbidden everywhere except Mac, it needs to be changed in order to send it anywhere or use in some apps. But I think colon is used much more in names. I don’t get why did they do that. reply lostlogin 15 hours agorootparentBased on this logic, do we ban everything that is banned somewhere? Surely we can aim higher than the lowest common denominator. reply wizofaus 14 hours agorootparentGiven the preponderance of 3 major operating systems, I&#x27;d think it&#x27;s sensible for user-level applications to disallow creation of filenames that would cause problems on any of them. Except arguably that could even include using spaces or periods... obviously in an ideal world such restrictions wouldn&#x27;t exist, but I&#x27;m not sure how to realistically push for such a world. E.g. my suggestion would be to reserve non-printable characters (below Ascii 32) for use as separators&#x2F;delimiters in as many contexts where that&#x27;s workable. Obviously some sort of convention would then need to exist as to how they were displayed and typed in, and I very much doubt I&#x27;ll ever see it happen, but I&#x27;m sure it would solve a lot of mis-parsing bugs that show up with frustrating regularity. reply lostlogin 9 hours agorootparentI suppose I’m actually criticising Microsoft and the backwards compatibility that is now dictating practices due to long gone limitations.It is painful being forced up update things due to software changing underneath you, but there must be a middle road. reply vtee44 14 hours agorootparentprevSlashes are used in paths, so most programs that aren’t Mac-exclusive would use them to build a path to file. To make it work properly in cross-platform programs you’d need to write platform-specific code to handle that. It just adds complexity and possible errors. Even system terminal doesn’t display it as slash and it doesn’t work if you write slashes.For users of other platforms (at least 90% of desktop market) it would just display as slashes. Just not implementing this workaround would make it predictable when moving and using files. replyOnlyMortal 18 hours agorootparentprev“:”? Really? It’s not 2001 anymore. reply reaperducer 18 hours agorootparentI guess you&#x27;re criticizing Microsoft&#x27;s NTFS, since it cannot use colons, either.CON:? Really? It&#x27;s not 1974 anymore.People in glass houses shouldn&#x27;t throw stones. reply lostlogin 18 hours agorootparent? No it is not 1974, but not everything is better.Edit: argh, HN deleted the emoticon. Ironic. replymihaaly 11 hours agorootparentprevOnce I planned to share files between a Mac and a Win PC through a product synching folders but gave up very soon due to the constant errors and problems with file names worked in one but not in the other, dominantly filenames that I did not choose but received (e.g. link dragged from address bar to folder, but others too) but some I choose following some preexisting logic. reply andix 15 hours agoparentprevAnd matching Active Directory groups. It&#x27;s kind of cool to use them, because it&#x27;s a really easy way to let the users administrate access to resources without opening a support ticket. reply nullindividual 10 hours agoparentprevSharePoint stores files as binaries in a SQL database (or a reference to an Azure Blob Store). The filesystem doesn’t come into play. reply toyg 2 hours agorootparentBut it now has a feature to integrate seamlessly with OneDrive, linking or replicating a folder on the user machine. Hence, I expect it has the same limitations in terms of naming. reply whalesalad 16 hours agoparentprevsharepoint has its claws in msft and it will forever be their achilles heel. reply Brian_K_White 19 hours agoprevI was about to say \"Which is essentially leaking the existence of something terrible inside. They should be embarrassed to say something like this in public. Like saying you can&#x27;t have %s or $PS1. Why the hell not? What are you doing with this user-supplied input?\"But maybe it&#x27;s more about what everyone else might do with a channel name. Ie they might cut & paste it anywhere, and I guess windows users aren&#x27;t expected to escape their own strings when pasted into cmd or powershell or wsl.The teams code itself can probably handle it just fine, but maybe not all the unknown janky random things out there that might handle channel names.Other people have pointed out the SharePoint folders associated with the channels. Not sure I would excuse that myself since it&#x27;s easy enough to just escape or modify or encode to create a safe version for the directory, but maybe it&#x27;s important elsewhere for the channel name and the directory name to be identical. Within one app you could simply encode and decode both the channel name and directory name the same way and totally hide the encoding from the user, but if the directory is used outside of the app, then it would look bad with URL encoding or something that everything else will just display as it is, not decoded.So the directory has to be safe for everything else, and so the channel name has to be the same.Essentially choosing to have these limits rather than have directory names that look ugly sometimes. It&#x27;s ultimately not even a safety or breakage thing, just a cosmetic thing. All directories will always look natural and good, because they don&#x27;t allow anything that would have needed to be encoded. reply m3047 18 hours agoparent> leaking the existence of something terrible insideIf you should get the opportunity to look at a raw (DNS) NXDOMAIN passive DNS (PDNS) feed there&#x27;s a lot of plain brokenness, but the nuggets can be truly alarming. This is what happens when translating between naming services: naming services typically have application domains, and names in one context are interpreted differently in another. Bobby Tables is well known, but how about that special file \"-rf\"? Was a time when the happy path for Active Directory essentially trusted DNS domain names implicitly for things like file shares. Sounds ok until you realize executable files might be on those \"drives\".(Honestly I don&#x27;t find the string \"MS-DOS\" anywhere in that document.) (Edit: Did find the reference to e.g. CON, LPT1...) reply m3047 17 hours agorootparentLooking at the list of characters and character ranges the omission of 7F is curious. reply inkyoto 5 hours agorootparentIf my memory serves me well, 7F was historically used for the «Delete» key code, and outputting it on a real terminal (or in terminal emulator), would result in the in-place text deletion from the output.Not that it has been relevant in last 2-3 decades tho. reply thefz 19 hours agoparentprev> essentially leaking the existence of something terrible insideIt&#x27;s just a restriction imposed on SharePoint folder names bubbling up. Nothing fancy. reply BasedAnon 19 hours agorootparenti was going to write a bunch of suggestions for solving the problem but then i remembered i hate microsoft reply Brian_K_White 18 hours agorootparentI at least upvoted. This is the most level headed and sane reaction to this article. reply matheusmoreira 15 hours agorootparentprevName checks out. reply gadders 16 hours agorootparentprevIt goes lower than that. Try creating a directory called \"LPT1\" in DOS reply Brian_K_White 8 hours agorootparentomg reply eastbound 19 hours agorootparentprevThose should obviously be UUIDs. Labels and titles should be a simple changeable, internationalizable attribute.Like usernames. You don’t use usernames as primary keys for anything, do you? What happens when people marry? reply jdwithit 16 hours agorootparentExtremely relatable post. I worked someplace that had a policy of absolutely never changing your Active Directory username because it was the primary key in like a dozen internal systems. Someone finally made a massive stink about it to HR when their name had legally changed but IT was forcing them to use their old name (and they were absolutely right to complain. It was a ridiculous policy only in place due to terrible architecture). They still defaulted to telling people no but they did at least document all the highly tedious manual steps necessary to change the name in all affected systems if (when) another employee refused to take no for an answer.Was certainly a great lesson in schema design, among other things. reply SoftTalker 18 hours agorootparentprev> You don’t use usernames as primary keys for anything, do you?HAHAHAHHHAAHAHASeen this so many times I literally laughed out loud. reply yndoendo 7 hours agorootparentWindows actually uses SIDs to map internal users to windows settings such as profile locations and group policies. Open regedit and under HKEY_USERS there will be the SID for your user account which maps to HKEY_CURRENT_USER.Linux has UIDs that map to the username.Both methods allow for changing the user name without having to change permissions on files. Both need manual manipulation to transition the user&#x27;s folder name to the new username. Both also allow for creating a user folder that does not match the username. Example HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\ProfileList on Windows.Best practice and reality are two different things. Even Microsoft often will use the fully qualified user profile path in their coding instead of %USERPROFILE%, example would be OneDrive. Just look at the Environment key in regedit for the current user. reply eddythompson80 16 hours agorootparentprevI too love folders with 200 UUID subfolders in them as well as URLs with 14 different UUIDs. reply hardware2win 18 hours agorootparentprevAnd what happens when you sync sharepoint to your windows files? reply merb 14 hours agorootparentprev> Like usernames. You don’t use usernames as primary keys for anything, do you? What happens when people marry?well it&#x27;s extremly painful to rename the primary mail address or the UPN when it comes to microsoft 365 and active directory, especially in a hybrid environment. of course you can change upn&#x27;s but it&#x27;s definitly some kind of \"primary key\" for a user. in fact most systems at least use a UPN reply kayodelycaon 18 hours agorootparentprev> You don’t use usernames as primary keys for anything, do you?Sure you do. Saves doing joins all over the place just to get someone&#x27;s username. Makes hand writing SQL easier.Apply head to desk. reply mihaaly 11 hours agorootparentprevOr the terribleness falls one down very fancy? reply marcosdumay 18 hours agoparentprev> I guess windows users aren&#x27;t expected to escape their own stringsEh... When did the users of any kind of system start to fit that expectation? And what is that utopia system? reply Brian_K_White 10 hours agorootparentMaybe they do and maybe they don&#x27;t, but I am certainly expected to deal with whatever the results might be if I paste something into a terminal.I know there exist terminals and shells that do try to be \"helpful\" and do auto-escape pastes, but that just breaks things that must not be modified, when the only reason you&#x27;re pasting is because you need it to be an exact copy without any risk of human typos.Some are less invasive and just warn when there are included \\n before proceeding, but do not presume to munge the data. reply paulddraper 17 hours agoparentprevAWS has character restrictions on virtually everything.The message bodies of SQS messages has restrictions on which whitespace characters can be used. reply SamuelAdams 17 hours agorootparentI noticed this recently too. We set up alerts on cloud watch logs and I tried to put an emoji in the alert message, which delivers to an SNS topic, which goes to an email. And cloudformation complained loudly that there was an unsupported character in the alarm message. reply wizofaus 14 hours agoparentprevTry creating a value in a SharePoint choice column that has ;# in it, and you might not be so certain this isn&#x27;t being done because MS don&#x27;t trust their own code. reply nullindividual 10 hours agorootparent;# is the Choice column separator value. That restriction makes sense in that context. reply wizofaus 8 hours agorootparentBut it doesn&#x27;t restrict you! You can create a choice field with a value that contains ;# and select it, but then it can&#x27;t display correctly. Relatively minor bug as bugs go, but still... reply naikrovek 18 hours agoparentprevthey&#x27;re SharePoint limits, man. Teams is backed by SharePoint and it&#x27;s not a secret, nor is it embarrassing. reply recursive 18 hours agorootparent> nor is it embarrassingI worked on a sharepoint project once. Let&#x27;s just say I&#x27;m not putting it on my resume these days. reply syndicatedjelly 18 hours agorootparentI avoid putting stuff on my resume that I don&#x27;t want to work on again. I just pretend to re-learn Sharepoint for every new job I have reply TYPE_FASTER 14 hours agorootparentIt used to change so frequently that I didn&#x27;t have to pretend. reply lostlogin 18 hours agorootparentprev> nor is it embarrassingIsn’t it? The folders work fine on a Mac.Teams is not a things of beauty, and is rough as all hell for daily users. reply kbenson 17 hours agorootparentIt isn&#x27;t. Services have limitations, it&#x27;s nothing new. This isn&#x27;t about the portion of the app running on Mac, or Linux for that matter, this is because the app is backed by a service that has a name limitation and that bubbles up through teams names.In the end it&#x27;s no different than if some service doesn&#x27;t allow all numeric names or names to start with numerals or be too short, of which there are plenty of constraints on plenty of services. reply lostlogin 15 hours agorootparentI think I expect more from my tools than you. I have a fair idea why the behaviour is there, I just don’t accept that it’s ok.There are problems large and small throughout the app and it really feels like someone went ‘close enough’ and called it done. reply santoshalper 17 hours agorootparentprevAnd SharePoint itself is backed by the Windows Filesystem, which respects a lot of old DOS stuff for backwards compatibility. reply nullindividual 10 hours agorootparentThis is wrong when it comes to content stored in SharePoint, of which resides in content databases or a remote blob storage, but the file names are in guid format, not the source file name, which wouldn’t work for a file of any name and SharePoint due to shredding. reply Roark66 19 hours agoprevWhy is it that most \"chat&#x2F;conference\" apps become horrible sooner or later? I still remember when teams used to be an OK app. It even had a Linux desktop client. I remember when slack was actually fast, I remember Skype out being more reliable to make phone calls than my mobile&#x2F;cell service. Today slack is extremely slow if you add few organisations to it (but at least you can add more than one). Teams has deprecated their Linux desktop client and the only way to use it on Linux is via chrome, but wait, if you use it as part of office365&#x2F;sharepoint you need to use Firefox for \"some\" sharepoint links. So essentially you need 2 browsers at all times. Chrome for teams (screen sharing and video), Firefox for some sharepoint links. reply eddythompson80 15 hours agoparentBecause believe it or not, a chat&#x2F;conference is the most easy gateway to an \"everything app\" just like with WeChat. Afterall, a chat&#x2F;conference app is a microcosm of the \"internet\".Your chat app is great but imagine if we can send&#x2F;share audio clips too.Your chat app is great but imagine if we can send&#x2F;share video clips too.Your chat app is great but imagine if we can send&#x2F;share live video too.Your chat app is great but imagine if we can send&#x2F;share money too.Your chat app is great but imagine if we can send&#x2F;share conference meetings too.Your chat app is great but imagine if we can send&#x2F;share calendar invites too.Your chat app is great but imagine if we can send&#x2F;share food delivery requests too.Your chat app is great but imagine if we can send&#x2F;share gaming sessions too.Your chat app is great but imagine if we can send&#x2F;share X too.There is no limit on X. The internet is about sharing X. a chat app is about sharing X. There is no bound to how much it can grow really. reply h2odragon 15 hours agorootparent\"every program expands until it can read email... make that netnews...\" reply wolfgang000 5 hours agorootparentprevI completely agree, I did a very small chat app just for fun, I thought, this is only going to take me a couple of days and before I realized it, that couple of days became weeks and it never felt finished, there was always something to add. reply akira2501 15 hours agorootparentprevInstant Message Chat + Anything Else isn&#x27;t actually a model for anything useful. It&#x27;s a gross kitchen sink with chunks of forgotten meals clogging up the drain. The insult is these companies have nothing other than \"productivity tool\" as a label to slap on the side of this fetid and entirely unproductive mess. reply throwaway290 14 hours agorootparentprevThat&#x27;s why I&#x27;m not enthusiastic about Musk&#x27;s vision for X... reply prepend 17 hours agoparentprevThe key is to be terrible and simple from the beginning. IRC works just as poorly today as 30 years ago. reply mschuster91 17 hours agorootparentWhich is why its usage has declined ever more and more thanks to Discord, which is a pain in itself. Freenode&#x27;s collapse in particular pushed a lot of people to just say \"fuck it\" to IRC in general. reply mmis1000 16 hours agorootparentI think it is already killed by telegram even before discord emerges. The only thing you can&#x27;t do on other IM other than IRC is having a really big chatroom that contains thousands of people. But it is no longer the case after telegram.BTW, i think an irc bot is a good target if you are starting to learn writing a network program. The protocol is really simple(don&#x27;t need complex xml parser...etc) yet requires all technique you need to write a proper client. reply tenebrisalietum 16 hours agorootparentIRC is so trivial that hosting your own is easy. Telegram is awesome but it&#x27;s still centralized. reply mmis1000 5 hours agorootparentThe same argument applies to discord. Well discord is great but no self-host, and teamspeak can be self-hosted.The reality? Nobody cares. Even for most tech related communities. Most people just want somewhere they can share a join link to other to allow other to join without hassle. reply prepend 11 hours agorootparentprevCheck again in 30 more years. I’m guessing discord will have zero users and Teams will be on its 10 different product name.But I expect IRC will still have its users.That’s like comparing the US empire to the Roman empire. They haven’t been around long enough to talk about popularity. reply zelphirkalt 17 hours agorootparentprevAlthough, at least what works today works tomorrow on IRC. Same cannot be claimed from MS Teams or Discord ... reply Dalewyn 11 hours agorootparentprevFreenode collapsing had everything to do with humanity being terrible and nothing to do with IRC itself.Given similar people, similar collapses can happen to Discord or any other communication medium of your choice. reply nxobject 17 hours agoparentprevI think part of it is the constant drive to add features – sure, we can do chat and video! But, what if we put background blurring in there? Crap, Zoom has polls, we&#x27;ll have to add polls now... well, shoot, if we&#x27;re cranking out features like this and iterating quickly, we might as well use Electron. reply evouga 16 hours agoparentprevBecause after your lean, highly-productive startup team creates the app that everyone loves, you get a bunch of funding and hire thousands of extraneous software developers and then have to find something for them to do. reply db48x 15 hours agorootparentThousands of engineers and managers and product designers who can only get promoted if they look successful, and who can only look successful if they can add features. reply graphviz 19 hours agoparentprevThe market rewards features and integration, not performance, as long as an app is usable. Developers devel... I mean features features features. reply zelphirkalt 17 hours agoparentprevWait, the MS Teams desktop app is deprecated? Never got any message about that ... Guess I might soon be forced to use double the evil, in Chrome and Teams in Chrome? Wow, the world becomes more dystopian by the day. Perhaps I should quit my job when I am forced to use Chrome. Well gonna use the desktop app for as long as I can. They will probably never fix their broken shit app, so that one can use it from any browser. reply duxup 11 hours agoparentprevThe app that does everything always seems to stink.It&#x27;s no coincidence that my favorite note taking app is ... Apple&#x27;s Notes App. I&#x27;ve used other apps but I&#x27;ve found that all that heft from all the extra features makes it more of a hassle for me in the end.I get how it happens, even my current employer &#x2F; small team are looking into internal documentation routes and ... oh man the list of things people want just goes on and I fear leads to some beastly solution. reply cwkoss 16 hours agoparentprev> Why is it that most \"chat&#x2F;conference\" apps become horrible sooner or later?To &#x27;become&#x27; horrible there has to have been a period where it wasn&#x27;t. AFAIK that doesn&#x27;t apply to teams, lol reply HPsquared 19 hours agoparentprevIt&#x27;s easier to add things than fix annoyances. reply ikekkdcjkfke 16 hours agoparentprevEntreprise wishes i guess? Just look at all the group policies for windows update.. reply keepamovin 19 hours agoprevAhahaha, I love this! I love how this new fandango thing harkens back to the earliest days of MS-DOS when it ran on a 086 or 286, back in the early (very early) 90s.Have to respect MS&#x27; backwards compatibility fanaticism. Impossibly as if a native port of MS Teams would be created for MS-DOS 3.1 (hahahaha). When more plausibly the MS Teams servers run on an ancient crazy proprietary MS-DOS 3.1 mainframe (still implausible, but hey).I know that this device name restriction also applies to Windows file names, so it&#x27;s not that surprising (if you are inclined to be less fun than possible), but if you like fun, you can pretend the former.Relevant frag link: https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;microsoftteams&#x2F;limits-spec... reply mikestew 19 hours agoparentearliest days of MS-DOS when it ran on a 086 or 286, back in the early (very early) 90s.About a decade off: MS-DOS was running on 8086 in the early 80s. reply keepamovin 19 hours agorootparentOh my god. Well i didn’t receive my first one until the 90s. And i grew up thinking i was middle class, my oh my. reply dustymcp 18 hours agorootparentdepending on your country they might not have been availible before reply paulryanrogers 19 hours agoparentprevAgree BC is a laundable goal and appreciate their efforts. Though I do wish it didn&#x27;t include limiting passwords to obscenely short lengths or absurd subsets of characters. reply keepamovin 19 hours agorootparentYeah those 8+3 file names sure birthed some creative abbreviations. Necessity, mother, all that reply glonq 18 hours agorootparentWhenever I create a filename that is long or has spaces, I still pause and worry \"okay, what might break if I do this\", even though such concerns probably died a couple decades ago.Actually scripts breaking if files or paths have a space in them seems to be a thorn in the side of dev&#x2F;ops&#x2F;it folks that never goes away, does it? reply Anthony-G 17 hours agorootparentUnix shell scripts will break when filenames&#x2F;paths have spaces if variable expansions are not quoted, resulting in word splitting and filename expansion (globbing). This is something that is drilled into learners by all good teachers, e.g., https:&#x2F;&#x2F;mywiki.wooledge.org&#x2F;Quotes#When_Should_You_Quote.3F reply nomel 5 hours agorootparentWait until you hear about `make`, completely cementing the idea of &#x27;_&#x27; as space for decades to come [1]!Related, any time I join a new group, that uses bash in any capacity, I introduce them all to the `shellcheck` utility [2], and run a shell script of the person with the largest ego through, as a \"demo\".[1] https:&#x2F;&#x2F;stackoverflow.com&#x2F;a&#x2F;9838604&#x2F;1487072[2] Online version: https:&#x2F;&#x2F;www.shellcheck.net&#x2F; reply ChrisSD 18 hours agorootparentprevOn Unix perhaps. Windows forced IT folks to deal with the issue by having folders like \"Program Files\". Even the user&#x27;s folder used to be in \"Documents and Settings\" (but they backed down from that eventually and now it&#x27;s just \"Users\"). reply WirelessGigabit 17 hours agorootparentTo this date one needs to be careful with executing applications and spaces in the path.For example: D:\\test>dir ... 08&#x2F;10&#x2F;2023 09:29 AM. 08&#x2F;10&#x2F;2023 09:30 AMFoo Bar 08&#x2F;10&#x2F;2023 09:35 AM 79 Foo.bat ... D:\\test>dir \"Foo Bar\" ... 08&#x2F;10&#x2F;2023 09:30 AM. 08&#x2F;10&#x2F;2023 09:29 AM.. 08&#x2F;10&#x2F;2023 09:35 AM 79 Foo.bat ... D:\\test>type Foo.bat @echo off echo Executable: %0 echo Path to executable: %~dp0 echo Params: %* D:\\test>type \"Foo Bar\\Foo.bat\" @echo off echo Executable: %0 echo Path to executable: %~dp0 echo Params: %* D:\\test>Foo.bat hello Executable: Foo.bat Path to executable: D:\\test\\ Params: hello D:\\test>Foo Bar\\Foo.bat hello Executable: Foo Path to executable: D:\\test\\ Params: Bar\\Foo.bat hello D:\\test>\"Foo Bar\\Foo.bat\" hello Executable: \"Foo Bar\\Foo.bat\" Path to executable: D:\\test\\Foo Bar\\ Params: hello D:\\test>Mess up the quotes and you&#x27;re executing something one level up. reply xp84 15 hours agorootparentprevThis was actually genius, though it was&#x2F;is so annoying typing these verbose paths reply justsomehnguy 17 hours agorootparentprev> but they backed down from that eventuallyOnly because they were started to hit MAX_PATH there: C:\\Users\\Johnathan Aparecido da Silva\\AppData\\Local\\ASUS\\ASUS System Control Interface\\AsusSoftwareManager C:\\Documents and Settings\\Johnathan Aparecido da Silva\\AppData\\Local\\ASUS\\ASUS System Control Interface\\AsusSoftwareManager reply queuebert 19 hours agorootparentprevBrevity forces thought, which I prefer to \"Putting a Sentence in a File Name.docx\".FORTRAN originally had the same limitation for function names, and that lead to some classics such as GEMM and SAXPY. reply xp84 15 hours agorootparentWhen it comes to file names, I disagree that brevity serves a useful purpose. I name my files like I’m an Amazon reseller so that I’m sure to find it with Search later no matter which detail I remember. “Budget Spreadsheet (worked on with Jane) with projections with and without buying a new Tesla Model Y or Chevy Chevrolet Bolt - 2023 2024 2025.xlsx” that’s my idea of a file name. reply queuebert 9 hours agorootparentModern OSes can search within files for text, which mostly obviates using the filename for keywords. reply Dalewyn 11 hours agorootparentprevIf it floats your boat, you do you my dude.But that being said, you&#x27;re the reason why file systems are abstracted away more and more with all file system queries force fed through some kind of search engine. reply djbusby 18 hours agorootparentprevThose two are obviously GetExtendedMemoryMap and Saxophone. reply pluijzer 18 hours agorootparentprevVery loosely related, but I really dislike the excepted design pattern for that cases where functions names are something like AssertThatOnePlusOneIsReturnAlwaysTwoAndNeverFive. It looks ridiculous, smells like a hack, and I cannot think of any reason why giving a description of the test couldn&#x27;t be handled by the testing framework in a more graceful way. reply prepend 17 hours agorootparentMe too, I prefer function names like “assert_that_one_plus_one_is_return_always_two_and_never_five.”All kidding aside, but I gave up on this argument because I think people either get it or don’t and aside from trying a little conversation or something with guidance it’s a “can’t fix stupid” situation.People usually want to explain why it’s so important and that’s worse than suffering their long function name.I think it’s better to just accept ridiculous than to try to get consensus on what’s ridiculous and a spiral of wasted time.The upside is that it doesn’t matter any more since long function names are supported and work. And autocomplete means it’s just as easy to use as “ATOPOIRA” or whatever madness they would name it with some restrictions. replyred_hare 12 hours agoprevReminds me of the rumor that the reason it went \"Windows 7\", \"8\", \"10\" and skipped \"9\" is because the fear of a codebase that went: if(version.StartsWith(“Windows 9”)) { &#x2F;* 95 and 98 *&#x2F; ... } reply fantyoon 12 hours agoparentIs the Windows version ever exposed as a string in the Windows API? Seems strange in my mind, but I have no experience with Windows. On the one hand it sounds like something Microsoft would do for backwards compatibility, but on the other hand it seems like a weird API to provide.I found GetVersion[1] but that returns the version as two numbers.[1] https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;windows&#x2F;win32&#x2F;api&#x2F;sysinfoa... reply Phrodo_00 12 hours agorootparentI don&#x27;t know if the rumor is true, but even if Windows itself doesn&#x27;t provide an API that looks like that (I also looked around real quick and didn&#x27;t find anything), it&#x27;s not unreasonable that there could be libraries that provide the version that way. reply TheRealSteel 12 hours agorootparentThe rumour is not true, despite reddit&#x27;s insistence. Windows exposes it&#x27;s version is a series of numbers, and even if it didn&#x27;t, Windows 9 could have returned \"Windows Nine\", \"Window v9\" or loads of other things. reply IshKebab 11 hours agorootparentprevYou&#x27;re imagining that all developers do things properly. I&#x27;ve seen Bash code that used a regex to get the first two digits from `python --version`. Hopefully they&#x27;ll never release a Python 3.10! reply TacticalCoder 16 hours agoprevIt&#x27;s bad, ok... But honestly what&#x27;s the goal? To let people name anything, including the name of a channel, anything they like, like, say: \"rm -rf &#x2F;*\"Wait, I&#x27;ve got better: that rm -rf, but written \"fr- mr\" with RLO left&#x2F;right overrides.Surely that is something to aim for because nothing shall ever go wrong?Thankfully having a file named https:&#x2F;&#x2F;example.org is illegal in Linux (and Windows too right?).Seriously: is this a problem of you? And if it&#x27;s a problem, where do you draw the line?What about codepoint 0? What about Hangul fillers and RLO characters: do you think applications who refuse these do suck?There are, thankfully, limitation on what filenames can contain. And I think the restrictions aren&#x27;t anywhere near restrictive enough. Same for usernames, same for channels, same for oh-so-many things.Does anyone really find it problematic that, say, Twitter only allows visible alphanumeric characters and underscore? (and 15 chars max)This seems very smart to me. I take that any day over longing for people being able to use poop emojis in their usernames and channels names. reply aidenn0 16 hours agoparentWhile you can&#x27;t have a file named https:&#x2F;&#x2F;example.org you can absolutely have a path like that, as internal duplicate slashes are ignored and you can have a directory named \"https:\" with a file named \"example.org\" reply dylan604 16 hours agoparentprevI&#x27;ve found it better to use dd to wipe out data than an rf. reply owlninja 19 hours agoprevWords: forms, CON, CONIN$, CONOUT$, PRN, AUX, NUL, COM1 to COM9, LPT1 to LPT9, desktop.ini, _vti_ reply Brian_K_White 19 hours agoparentI was about to say \"Which is essentially leaking the existence of something terrible inside. They should be embarrassed to say something like this in public. Like saying you can&#x27;t have %s or $PS1. Why the hell not? What are you doing with this user-supplied input?\"But maybe it&#x27;s more about what everyone else might do with a channel name. Ie they might cut & paste it anywhere, and I guess windows users aren&#x27;t expected to escape their own strings when pasted into cmd or powershell or wsl. reply dharmab 19 hours agorootparentThe Something Terrible in this case is early DOS which did not have separate folders. So the device files were in the same namespace as the user&#x27;s files, so the user couldn&#x27;t name a file CON or LPT1. These are preserved for backwards compatibility. To this day you can write a program that writes to a CON file in any folder, and the program will print the write to console. reply xp84 15 hours agorootparentAnother interesting aspect of this, which I remember specifically from reading MS-DOS manuals in the 90s as a kid (I was an incredibly fun 12 year old) is that this choice was an intentional one done in the name of flexibility. These device names were correctly written with a colon, but you are allowed to omit the colons. For instance:copy con lpt1Andcopy con: lpt1:were interchangeable. If they’d chosen to not save users keystrokes and force colons when referencing devices, I’m not positive, but I think it would have eliminated the need for many of these reserved words in file names in general.Also, side note, the “copy con outputfilename.txt” idiom is one that I still can’t ever remember how to do the equivalent on UNIX&#x2F;Linux! reply gnosek 13 hours agorootparent> Also, side note, the “copy con outputfilename.txt” idiom is one that I still can’t ever remember how to do the equivalent on UNIX&#x2F;Linux!cat > outputfilename.txt reply Brian_K_White 10 hours agorootparentprevOne of my favorite bbs sig tags was \"REAL programmers use COPY CON FILE.EXE\" reply ChrisSD 18 hours agorootparentprevThis has actually changed a bit in the last few years. In Windows 11 writing to `.\\CON` will write to the file but writing to `CON` will still write to the console. reply vel0city 19 hours agorootparentprev> What are you doing with this user-supplied input?Creating Sharepoint shares which can be mounted as network shares in Windows where these filenames are not allowed? reply tapoxi 19 hours agorootparentprevI thought this as well but there&#x27;s plenty of dangerous commands that aren&#x27;t escaped here. I&#x27;m assuming it&#x27;s a limitation of it actually writing something to the filesystem, hopefully your own (with the desktop app) and not one in Azure. reply cueo 17 hours agorootparentprev> I was about to sayYet you said it twice[1].[1] https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37076913 reply transitorykris 18 hours agoparentprevIn the mid-90s there was a fun and short period of time where IRC clients like mIRC could be set to auto-receive files through DCC.. and would happily write to names like LPT1 (which of course would just write the data directly to the recipient’s printer) reply grishka 17 hours agorootparentAlso, send a COM1 file containing \"+++ATH\" to someone you dislike to see them disconnect. I just made this up but it should work, right? reply zten 12 hours agorootparentFor the absolute worst modems, I believe you could just send that in a PING on IRC and their client would write the string back and hang up reply Ekaros 18 hours agoparentprevNow I actually wonder were there ever any machines that had LPT9, COM9 I could barely see. reply tssva 14 hours agorootparentBack in the day many of my clients had Windows NT&#x2F;2000 machines running RAS or RRAS with multi-port serial cards connected to modems for employee remote access. Usually these would be 8 or 16 serial ports per card with multiple cards installed per server. reply fredoralive 17 hours agorootparentprevIf you do embedded stuff you can easily get above COM100 if you&#x27;re doing something like testing (if a USB to serial converter has a serial number, it gets a unique COM port). Although I don&#x27;t think these high numbers have any effect on the file system, just the legacy low numbered ones DOS could have. reply zaxomi 18 hours agorootparentprevMSDOS 3.3 had COM1, COM2, COM3, COM4, LPT1, LPT2, and LPT3. LPT4-9 and COM5-9 was not part of MSDOS. reply olyjohn 12 hours agorootparentIIRC those were mapped to specific I&#x2F;O and IRQ Ports back in the day. COM1: I&#x2F;O port 0x3F8, IRQ 4 COM2: I&#x2F;O port 0x2F8, IRQ 3 COM3: I&#x2F;O port 0x3E8, IRQ 4 COM4: I&#x2F;O port 0x2E8, IRQ 3I also remember, there was a while where there was no IRQ sharing. So some machines couldn&#x27;t use all the COM ports available. Or you&#x27;d add in like a modem, and would have to disable the onboard COM port if it was using the same IRQ. reply blueflow 18 hours agorootparentprevYou can reassign real device to any of these names reply Ekaros 18 hours agorootparentReassign and actually use all of them are different things. reply maerF0x0 16 hours agoprevGeneral tip for structuring user data. Try to treat it as obscure blobs, whenever possible.. Imagine it&#x27;s encrypted so it&#x27;s not even printable&#x2F;human readable.> forms, CON, CONIN$, CONOUT$, PRN, AUX, NUL, COM1 to COM9, LPT1 to LPT9, desktop.ini, _vti_If they have to restrict those because some user input is going straight into their FS, then they mucked up. Probably should have been given a safe ID (perhaps uuid4, perhaps something more like a digest of the channel name instead of using the user input directly.For me it&#x27;s a smell when someone says \"You cannot use these characters\". I automatically think \"Why not? You&#x27;re not using this unencoded or plaintext, right?\" eg passwords, or usernames, or content that will show up on a webpage like a comment or such.This all being said, perhaps it&#x27;s just an easter egg gone sideways... Perhaps they were just trying to have a bit of fun :) reply graypegg 19 hours agoprevIf this IS a case where the title of a channel is used as a SharePoint folder, I’m surprised there isn’t some standard way to escape specifically these strings? I know it would break compatibility for applications that rely on these magic device files, but SharePoint should NEVER actually want to speak to COM1. Weird to me it isn’t handled already in SharePoint! reply Kwpolska 17 hours agoparentSharePoint can sync with your filesystem on Windows, and Windows&#x2F;Win32 doesn&#x27;t support those names for backwards compatibility. reply lol768 19 hours agoprevDoes this imply it&#x27;s writing a file to the fs for its internal storage, with the name matching the channel&#x27;s name instead of a channel ID? reply icegreentea2 19 hours agoparentNot necessarily - it could mean that at some point they believe they&#x27;ll need to create folder or file with the same name as a team, and they don&#x27;t wanna have to deal with weird collisions.It&#x27;s been a while since I&#x27;ve used Teams+Sharepoint, but I vaguely remember being able to have a Sharepoint workspace per team channel, and then being able to mount those workspaces as a shared&#x2F;network drive. Stuff that might end up in the workspace is like... shared&#x2F;uploaded files or something? reply i_am_jl 19 hours agorootparentnext [–]It&#x27;s been a while since I&#x27;ve used Teams+Sharepoint, but I vaguely remember being able to have a Sharepoint workspace per team channel, and then being able to mount those workspaces as a shared&#x2F;network drive. Stuff that might end up in the workspace is like... shared&#x2F;uploaded files or something?This is correct. Every channel is backed by a Sharepoint folder (maybe this is configurable?) that contains stuff like uploaded images, videos, recordings and transcripts of meetings, etc. reply Roark66 18 hours agorootparentIt&#x27;s hilarious on Linux you need chrome to use teams for screen sharing &#x2F; video to work, and Firefox for sharepoint file sharing to properly work.If someone shares a file on a channel, you can open it in chrome. But you can&#x27;t share any files yourself, because either the upload button doesn&#x27;t work or when it does it looks like you shared the file in the chat for a split second then it dissapears. reply ant6n 19 hours agorootparentprevThis%20is%20what%20escape%20sequences%20are%20for. reply kevincox 17 hours agorootparentIf these folders are user-visible (for example browsable in SharePoint) then this leads to a bad user experience. The downside of restricting channel names needs to be weighed against the downside of needing to escape the SharePoint folder names. reply randomdata 17 hours agoparentprevCould be someone taking DRY a bit too seriously, using validation code that is shared with places where the names do matter. reply Brian_K_White 19 hours agoparentprevI think it only implies they worry about people cutting and pasting channel names into other things like random powershell scripts. reply markstos 19 hours agoparentprevYes. Or something equally head-scratching. reply prepend 19 hours agoprevTeams is so weird, “ Number of org-wide teams allowed in a tenant” is limited to 25.So my company can only have 25 “general” teams.I think it would be neat to learn the rationale behind some of these settings. reply terom 17 hours agoparentThe 5² limit seems small, but at least you can have 500,000³ teams in Microsoft 365 Office organization - 1.25E17 should be plenty, that&#x27;s around 15M teams per person on the planet;)Perhaps they should rethink the use of numbered superscripts for notes in that table.... reply rejectfinite 18 hours agoparentprevYou want the ENTIRE company to be auto added to more than 25 teams? reply prepend 11 hours agorootparentI don’t want the entire company to be added to any teams. Maybe one.But I think it’s funny that someone said “20 is not enough but 30 is too many.” And then implemented it.Setting such an arbitrary limit so low is what is interesting to me. Fight for an opinion of “none” or “one and only one.” But allowing the hellscape of 25 teams where people accidentally invite all employees to their meeting, but not allowing 100 teams is funny. reply tymscar 8 hours agorootparentAnd usually when this happens it’s for some weird technical reason, and it almost always is a power of two number. Here it’s just plain weird! reply ratg13 19 hours agoparentprevProbably to protect people from their own ignorance.If you are participating in 25 teams, it&#x27;s likely already too many, general or not.Having to navigate 25 teams that are full of stuff doesn&#x27;t concern you sounds like an absolute nightmare.You&#x27;d burn out everyone in the organization with a setup like this. reply Ekaros 18 hours agorootparentAnd this is about organization wide teams, meaning those teams that people are automatically joined in, with up to 10000 members... So probably something you want to steer people away from. reply glonq 18 hours agoprevI&#x27;m old[school], so if I&#x27;m at the windows command prompt and need to bust out a quick batch file or script I will do \"copy con foo.bat\" instead of using notepad or vscode. Old habits die hard! reply xp84 14 hours agoparentSame! I know there are one or more ways to do this in a Unix&#x2F;Linux shell, but despite being off Windows for almost 20 years I haven’t memorized it. I miss it though! reply Mister_Snuggles 18 hours agoprevI sort of get most of these - they&#x27;re internal to various bits of Microsoft technology. &#x27;CON&#x27; plus the ones ending in $ are internal device names dating back to MS-DOS, &#x27;desktop.ini&#x27; is a magic file that Explorer uses, &#x27;_vti_&#x27; is something I&#x27;ve seen but can&#x27;t remember what it was from.But &#x27;forms&#x27;? Why is &#x27;forms&#x27; a bad word? reply tbyehl 17 hours agoparent_vti is the mortal enemy of anyone involved in web hosting during the mid-90s to early 2000s. Those are the server-side scripts and configuration for FrontPage, originally developed by Vermeer Technologies Incorporated. reply Mister_Snuggles 17 hours agorootparentThank you! That was bugging me, but now that you mentioned FrontPage I clearly recall all of the _vti stuff. reply monitron 18 hours agoparentprevI did some quick looking and it seems like SharePoint creates a hidden folder called “forms” to store, unsurprisingly, forms associated with a resource. Aren’t polluted namespaces grand? reply Mister_Snuggles 17 hours agorootparentAh, that makes some amount of sense.But... Why not just call it \"_forms\" or \".forms\", which are already prohibited due to the first character? reply air7 19 hours agoprevThis backwards compatibility chain reminds me of the age old tale of how rockets are the width of two horses... (1)(1) http:&#x2F;&#x2F;astrodigital.org&#x2F;space&#x2F;stshorse.html reply failuser 18 hours agoparentThis is a classic joke, but settling on a common gauge was really difficult, it was not just copying some old Roman standard. There were even riots when different railways started to standardize because common rail gauge meant you can move through the town without engaging with the local economy, see Erie gauge war. Russia still has a wider gauge than Europe and Australia has 3 different ones. reply cf100clunk 18 hours agorootparentI&#x27;ve come across an apocryphal tale in the U.S. west describing why certain towns and cities have main streets that are the width required to fully turn a wagon pulled by x number of horses.Likewise, in the Canadian House Of Parliament, the width between the Prime Minister&#x27;s desk and that of the Leader Of The Opposition is said to be that of two swords raised, tip to tip, based on the British House Of Commons tradition that they must remain &#x27;&#x27;two swords and one inch apart.&#x27;&#x27; reply bombcar 15 hours agorootparentEven today size of streets in many cities is determined by the space needed to either turn a fire truck or have two fire trucks pass. reply jmount 17 hours agorootparentprevI rode a train in Spain where the train changed gauge at a station! reply elnezah 17 hours agorootparentSpain has a peculiar train gauge. When you cross the border with France, the train slows down and gauge change happens on the fly. reply jonny_eh 17 hours agorootparentThey also have to change sides: https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;MapPorn&#x2F;comments&#x2F;29i4hq&#x2F;side_where_... reply michaelmrose 17 hours agorootparentprevThanks for the anecdote that was interesting. I think the Erie gauge war demonstrates that people feel entitled to any advantage of circumstances that they have profited from in the past as if such were their property even when the actual matter are issue is in fact others property. reply extraduder_ire 17 hours agorootparentI&#x27;ve seen this happen with planned bypasses of small towns, either through individuals fighting CPOs for their land, or people in the town opposing planning permission notices. (there&#x27;s a few I can think of where the council&#x27;s current plan involves waiting for certain people to die, since that&#x27;s less effort overall)Usually such bypasses are great for these towns, as it removes most of the articulated road shipping that needs to pass through there, and generally makes it a nicer place to be. reply bombcar 15 hours agorootparentIt depends on the town, it can also kill or relocate it. The town in Pixar&#x27;s Cars is realistically depicted in that way. reply joncrane 17 hours agorootparentprevThis is precisely what large companies in the US do, they gain an advantage, then when the advantage is threatened, they use every tool at their disposal, including political lobbying, to keep their advantage.This is done under the guise of preserving shareholder value. reply Svip 18 hours agoparentprevThe Romans did not use war chariots, since they are basically useless in combat. They only used chariots for sports in the hippodrome. reply masklinn 17 hours agorootparentChariots are not “useless in combat”, but they are very expensive to field and only work in pretty narrow circumstances: wide, open, and relatively solid areas (aka not bogs).These circumstances were not a thing for Roman legions until they reached spain at least, possibly North Africa and the Middle East. As a result they were not part of Roman war doctrines.On the other hand they were very much part of bronze-age Egyptian and Hittite armies.An other factor to their lack of appeal tho may have been improvements in horse riding technique and gear, as well as training. Riders can also be heavily armed, but have less travel restriction, and you can field a warrior per horse rather than need two horses and a driver per. reply anthk 17 hours agorootparentSpain is mountainous as hell; a chariout outside the Castille plains and Madrid would be scrap.Get a height map of Spain and you can obviously see that a chariot trying to ride at Picos de Europa would be a nightmare. reply r0b1n 17 hours agorootparentprevThey used chariots not only in the hippodrome, also for processions and other \"shows\".But Roman warfare was a no-nonsense kind of affair, so no chariots. https:&#x2F;&#x2F;acoup.blog describes this really well. reply joncrane 16 hours agorootparentprevThe chariots in this example were more for transport and logistics, not battle. reply mordae 19 hours agoparentprevThis is pure gold. Thanks, you made my day! :-)Also, you might be interested in this: https:&#x2F;&#x2F;youtu.be&#x2F;1NqRbBvujHY?t=2236 reply Nzen 17 hours agorootparentWhile I enjoyed rewatching the latter half of episode 2 of season 1 of James Burke&#x27;s Connections tv series, I don&#x27;t see how the invention of the cloud chamber (which you start the video at) is relevant to standards for rail, horses, or rockets. What&#x27;s the connection ? reply msla 17 hours agorootparentprevIt&#x27;s a dumb joke with no basis in fact. reply naikrovek 18 hours agoparentprevI would love to know who fabricated this myth and decided to just tell the story as if it were fact. reply prpl 18 hours agoparentprevSimilar reason why many telescope mirrors are exactly 8m across. reply chx 18 hours agoparentprevtale it ishttps:&#x2F;&#x2F;www.snopes.com&#x2F;fact-check&#x2F;railroad-gauge-chariots&#x2F; reply gmiller123456 17 hours agorootparentSnopes is not a good reference to cite, they have changed many artlcles based on research they missed. They tend to just want to get a page up when a topic is popular, then research it later. reply minsc_and_boo 17 hours agorootparentI mean, correcting factual errors is pretty good trait for a fact-checking site, right? reply Wildgoose 2 hours agorootparentYes, but surely you would do that first? reply asynchronous 18 hours agorootparentprevKind of a garbage article because coincidence or not they are similar widths. reply dymk 18 hours agorootparentIt being a coincidence versus causal is entirely what makes it interesting or not reply chrisweekly 19 hours agoparentprevhahaha, that&#x27;s great. thanks for sharing reply msla 17 hours agoparentprevPeople actually still think this dumb joke is true? reply ilyt 17 hours agorootparentWell, there is no real way to disprove it and there is no solid proof where it actually came from. reply bmitc 19 hours agoparentprevThat was fun. reply 47 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The first article discusses the limits and specifications of Microsoft Teams, covering topics like team and member limits, messaging, meetings, and calls.",
      "The second document focuses on the limitations and features of Microsoft Teams, including meeting participants, recording limits, storage, and browser compatibility.",
      "The third document provides an overview of browser support for Microsoft Teams, recommending the use of certain browsers and mentioning any limitations and known issues."
    ],
    "commentSummary": [
      "The discussion explores the limitations and restrictions of Microsoft Teams and SharePoint.",
      "Participants discuss the challenges of file and function naming across different operating systems.",
      "Frustrations with bloated software applications are also addressed."
    ],
    "points": 425,
    "commentCount": 314,
    "retryCount": 0,
    "time": 1691677306
  },
  {
    "id": 37084677,
    "title": "Firefox desktop extensions coming soon for the upcoming Android release",
    "originLink": "https://blog.mozilla.org/addons/2023/08/10/prepare-your-firefox-desktop-extension-for-the-upcoming-android-release/",
    "originBody": "Skip to main content Skip to sidebar Mozilla Add-ons Community Blog Mozilla Prepare your Firefox desktop extension for the upcoming Android release Scott DeVaney No responses yet AUG 10 2023 In the coming months Mozilla will launch support for an open ecosystem of extensions on Firefox for Android on addons.mozilla.org (AMO). We’ll announce a definite launch date in early September, but it’s safe to expect a roll-out before the year’s end. Here’s everything developers need to know to get their Firefox desktop extensions ready for Android usage and discoverability on AMO… Firefox will become the only major Android browser to support an open extension ecosystem For the past few years Firefox for Android officially supported a small subset of extensions while we focused our efforts on strengthening core Firefox for Android functionality and understanding the unique needs of mobile browser users. Today, Mozilla has built the infrastructure necessary to support an open extension ecosystem on Firefox for Android. We anticipate considerable user demand for more extensions on Firefox for Android, so why not start optimizing your desktop extension for mobile-use right away? “There is so much creative potential to unlock within the mobile browser space. Mozilla wants to provide developers with the best support we can so they’re equipped and empowered to build modern mobile WebExtensions.” — Giorgio Natili, Firefox Director of Engineering To support our ecosystem of extension developers, we will create additional guides, resources and host community events to support your transition to a managed multi-process environment like Android. Transition background scripts to non-persistent event pages We recently introduced support for multi-process in Firefox for Android Nightly. This means extensions are no longer hosted in the main process as Firefox’s user interface. This is a key consideration since Android is prone to shutting down resource-intensive processes, such as extensions. To mitigate the risk of unexpected extension termination, we’ve introduced event page architecture to be non-persistent and more resilient to process termination. Thus we strongly encourage developers to transition from persistent backgrounds to non-persistent Event pages to improve their extension’s stability. In summary, this means: Update your manifest.json background key and add “persistent”: false. Ensure listeners are registered synchronously at the top-level. Record global state in the storage API, for example storage.session. Change timers to alarms. Switch from using extension.getBackgroundPage for calling a function from the background page, to extension messaging or runtime.getBackgroundPage. Once you’re ready to test the mobile version of your extension, create a collection on AMO and test it on Firefox for Android Nightly. If you’d prefer to polish your extension before publishing it on AMO, you can also debug and run the extension with web-ext. This is an exciting time for developers seeking to expand the reach of their desktop extensions into the mobile Android space. For community support and input, you’re welcome to join the conversation on Firefox Add-ons Discourse. CATEGORIES: developers, mobile No responses yet Post a comment Post Your Comment Name (required) E-mail (required, will not be published) Spam robots, please fill in this field. Humans should leave it blank. Your comment Submit Comment Scott DeVaney More from Scott Looking for Add-ons? Visit the gallery Categories builder compatibility contests contribute developers documentation end users events featured addons featured contributors general jetpack manifest v3 mobile policy releases restartless sdk themes webextensions Return to top Mozilla Except where otherwise noted, content on this site is licensed under the Creative Commons Attribution Share-Alike License v3.0 or any later version. Contact Us Privacy Policy Legal Notices Report Trademark Abuse Theme Code",
    "commentLink": "https://news.ycombinator.com/item?id=37084677",
    "commentBody": "Firefox desktop extensions coming soon for the upcoming Android releaseHacker NewspastloginFirefox desktop extensions coming soon for the upcoming Android release (blog.mozilla.org) 426 points by rc00 7 hours ago| hidepastfavorite163 comments godelski 3 hours agoThere&#x27;s a lot of hate any Firefox and the \"blessed apps.\" But come on, chrome and Safari don&#x27;t sorry add-ons at all!I know there are other apps that support add-ons but the fact is that Firefox is __the only__ \"mainstream\" browser that does this (quotes because it&#x27;s 0.25% in the US, as the 4th most popular, behind Samsung fucking Internet).It&#x27;s popular to hate on Firefox, but this is the nerdiest of nerd issues and if we&#x27;re being honest, all this has accomplished in the last decade is a decrease in market share and keeping Safari and (more so) Chrome dictate the Internet and user experience. Firefox being so low puts no pressure on Chrome to add add-ons. Firefox being so low just means Chrome can continue pushing the ad first Internet.Yeah, it&#x27;s fun to complain, but which do you hate more? That Firefox only provides 80-90% of what you want (when major competitors provide __0%__), or Google dictating the Internet (like AMP)? Complain, but maybe tone it down a bit. This is not the holy war you want. reply batrat 1 hour agoparent>behind Samsung fucking InternetSupports adblock (addons) and shows tabs on top just like desktop so it&#x27;s easy to switch, dark mode for all websites, etc. It&#x27;s relative fast and lots of options. I use FF with no addons for some websites. reply godelski 41 minutes agorootparent> shows tabs on top just like desktopHonestly, I hate this on the phone. But I get it. I had that initial cognitive disruption when FF did this. But once that was gone (idk, like 30 minutes?) I realized it was a lot nicer because I could actually reach the address bar and tabs without needing to stretch my hand. Or if I have a bigger phone, use a second hand. I mean there&#x27;s a reason navigation buttons on the phones (for the OS) were always on the bottom. It&#x27;s just easier. But yeah, you gotta get through the initial cognitive disruption. But you know what? I also got used to when Android switched the time from being at the top right to top left and I&#x27;m willing to bet most people did too and don&#x27;t really think about it anymore. As for the dark mode... well you got a point there. Though dark reader was one of the first add-ons on FF mobile. Though as much as I love dark mode, some websites are just unreadable with it and it drives me nuts. Specifically anything with a transparent background... reply drozycki 3 hours agoparentpreviOS Safari has supported declarative content blocking extensions since iOS 9 in 2015 and a full extension API since iOS 15 released about two years ago. reply the_gipsy 2 hours agorootparentDeclarative blocking is the weakest, it barely removes the simplest ads.uBlock doesn&#x27;t exist for iOS because it is made impossible by apple, their extension API is not a \"full\" API. reply drozycki 2 hours agorootparentI haven’t seen ads on mobile Safari for as long as I can remember. Whether you choose to believe that is up to you. Can you point to an example where an extension like AdGuard would let an ad slip through?Precompiled declarative blocking is more performant and power efficient than running a content script in a JS VM at page load, which matters on a CPU and battery constrained device.I was responding to a comment claiming that mobile Safari doesn’t support add-ons. Which is simply not true! reply eipi10_hn 1 hour agorootparentIt&#x27;s not only the ads slipping through. Safari is so lacking in terms of API that for many sites with anti-adblock, the only way to bypass those are allowing tracking &#x2F; ads connections.Check the `@@` filters for `ios` here: https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;AdguardTeam&#x2F;AdguardFilters...Adguard&#x27;s default settings already do not include many tracking connections blocking filters, due to the lack of Safari&#x27;s API for redirecting to neutered sources which could easily lead to breakages &#x2F; anti-adblock.In terms of privacy-protection, Safari is one of the worst. reply the_gipsy 59 minutes agorootparentprev> Can you point to an example where an extension like AdGuard would let an ad slip through?Yes, see Twitter.I have AdGuard installed on iOS and I do see ads. I do not see them when opening on Firefox desktop with uBlock Origin. If I disable uBlock then I do see them like on Safari.As I said, precompiled declarative blocking only works for the weakest of blatant ads. Of course it&#x27;s \"more performant and power efficient\" because it only runs on the bare HTTP requests. That&#x27;s not what I want.> I was responding to a comment claiming that mobile Safari doesn’t support add-ons. Which is simply not true!Fair enough. But personally I don&#x27;t count Safari&#x27;s extensions as real extension, just like I don&#x27;t count Firefox&#x27; approach of a handful of \"blessed\" extensions for Android as real extensions. reply eptcyka 17 minutes agorootparentprevI see ads on Safari on my iOS device all the time, whether you choose to believe that is up to you. reply _thisdot 2 hours agorootparentprevIf anything, my problem is sometimes it’s too strong. Searching for a product on Google and clicking the Amazon links don’t work reply veave 1 hour agorootparentprev>Declarative blocking is the weakest, it barely removes the simplest ads.That&#x27;s misinformation. It may not be as good as ublock origin (I don&#x27;t know) but in my experience it removes practically everything. reply jorvi 1 hour agorootparentprevuBlock doesn’t, but plenty of browsers have built-in adblocking, with one of the very browsers missing it being.. drumroll.. iOS Firefox!Mozilla refuses to address how having no adblock is a security risk, so I have moved all my friends and family over to iOS Brave.Firefox built its own demise by having a bizarre focus on woke stuff instead of features that users direly need. reply the_gipsy 52 minutes agorootparentiOS Firefox is just a re-skinned Safari, just like iOS Chrome because Apple has some rule against \"dynamic code execution\" in their AppStore.Which sure is nice to combat some malware but not really MY problem.It also coincidentally happens to also rule out ANY browser engines. Firefox and Chrome can only use the provided webkit \"WebView\" component. Apple has effectively crippled Browsers on iOS to Safari, or a worse re-skinned version of Safari. reply prmoustache 53 minutes agorootparentprevI think you reached the new modern Godwin point. reply michaelmrose 46 minutes agorootparentprevBrave&#x27;s adblocking seems decidedly worse than ublock origin. https:&#x2F;&#x2F;github.com&#x2F;brave&#x2F;brave-browser&#x2F;issues&#x2F;16935 except on iOS where apple&#x27;s restrictions are drastically limiting.You are so insistent that the feature built in primarily because the platform you&#x27;ve chosen has limited your ability to do what literally every other platform can do. I know there are some benefits privacy wise to the Apple ecosystem but it appears that Apple continues to \"refuse to address\" the need for actual browsers other than safari on iPhone rather than skins on top of Webkit.So neither party is doing what you please but you gave hundreds to thousands of dollars to one and nothing to the other so perhaps go ask the fellow who has all of your money if they can implement the feature.Alternatively wait for some combination of law and or technology to work around Apple.https:&#x2F;&#x2F;www.theregister.com&#x2F;2023&#x2F;02&#x2F;03&#x2F;googles_chromium_ios&#x2F;... reply cubefox 2 hours agorootparentprevDoesn&#x27;t count. reply drozycki 2 hours agorootparentDoes too. reply tbossanova 1 hour agorootparentOh I&#x27;m sorry, is this a five minute argument, or the full half hour? reply Dalewyn 2 hours agoparentprev>It&#x27;s popular to hate on Firefox,I don&#x27;t hate Firefox, I hate Mozilla. reply autoexec 2 hours agorootparentI&#x27;ll always love Mozilla, or at least some fictional idealized version of it, so I&#x27;m stuck being increasingly disappointed in Mozilla while holding out for the times they do something right. Giving add-on support back to firefox users is a good thing so here&#x27;s hoping it won&#x27;t be done in a way that disappoints me. I hope they bring back about:config next. reply godelski 57 minutes agorootparent> or at least some fictional idealized version of itThis might be the most real comment here. All our perceptions are some fictionalized version of . Doesn&#x27;t matter if it is Apple, Google, Mozilla, or whatever. Our judgements are based on narrow experiences and highly affected by the PR generated around them. reply asu_thomas 2 hours agorootparentprevWhy? reply DoingIsLearning 2 hours agorootparent- They lost focus with just about every other project other than Firefox & Thunderbird which were the actual products I donated money for- They are (recently) very top heavy for a non-profit foundation and are spending non-negligible amounts of money on C-level and execs- Ideologically I disagree with them taking money with strings attached from Alphabet- (Personally) their recent trends of UX with &#x27;dummyfication&#x27; and mimicking chrome is a terrible loss reply godelski 52 minutes agorootparentHave you considered that they are doing some of these things to survive? That Chrome nearly killed them? Literally the strings attached from Alphabet are that they give you Google as a default search engine. Default, as in... you can change it... trivially. This isn&#x27;t like they are installing a keylogger into your browser and using it to learn about your behaviors. It isn&#x27;t like they are making their browser the default on your phones and devices. It isn&#x27;t like they are focused on selling ads and generating long URLs that have tracking data attached to them. Sorry, I got a little side tracked, but I guess that makes two of us. reply maleldil 1 hour agorootparentprev> They lost focus with just about every other project other than Firefox & Thunderbird which were the actual products I donated money for> Ideologically I disagree with them taking money with strings attached from AlphabetHow would they get more sources of funding if not exploring other projects? Firefox would never pay for itself.I totally agree with the exec thing, however. reply input_sh 1 hour agorootparentYou see, Mozilla is stuck in this limbo where people criticise them for taking money from Google, but also criticise them when they explore alternate funding sources (Pocket, Mozilla VPN, Firefox Private Network, MDN Plus) because they&#x27;re not focusing on Firefox and Thunderbird. Sometimes literally in the same comment, as the one above.In other words, HN crowd will shit on them regardless of what they do. reply DoingIsLearning 41 minutes agorootparent> where people criticise them for taking money from Google, but also criticise them when they explore alternate funding sourcesWhat aggravates me is nobody is asking why do you need so much cash?- Why has it become so big that it needs to spend 81M on Management payroll per year? [0]- Why has it grown so big that it needs to spend 21M in external consultants per year? [0]- Why do you need 1000+ staff to roll out an open source browser engine &#x2F; browser and an email client?Mozilla had almost 8M in no strings attached donations in 2021. If I go to most open source projects and say you can burn through 8M a year, most of them would agree they could build amazing things. This is even without taking into account Investment returns.The cow grew too fat and too close to SF&#x27;s money vortex.[0] https:&#x2F;&#x2F;assets.mozilla.net&#x2F;annualreport&#x2F;2021&#x2F;mozilla-fdn-202... reply maleldil 26 minutes agorootparentI think this is valid criticism, but it&#x27;s more nuanced than the usual \"mozilla bad because google money\" and \"mozilla bad because not firefox\". Thank you for providing data.Is there a more sustainable path where they don&#x27;t rely on Google so much? Maybe? Developing a browser doesn&#x27;t seem to be cheap. Do we know how many resources are used in Firefox versus the rest? I&#x27;d like to see a strong argument that Mozilla could be fully&#x2F;more independent while still developing Firefox.The thing is, even if they&#x27;re badly managed today, they&#x27;re still the best agent fighting for the open web. Apple helps, but I don&#x27;t think they care much about it and would happily change if it were better for their bottom line. reply gmerc 48 minutes agorootparentprevSo basically you want them to fund the top end engineers working on it … how? reply blackoil 1 hour agorootparentprev> money with strings attached from AlphabetWhat are those strings, apart from being default search engine? reply asu_thomas 1 hour agorootparentprevThanks for explaining. I can&#x27;t help but ask why you believe these things have happened? What went wrong and what is the alternative to Firefox?I am approximately as cynical as they come, and I&#x27;m not sure how Firefox could even exist if Google weren&#x27;t keeping Mozilla alive (on life support), so I am intrigued by your criticism. I&#x27;d like to understand your viewpoint and what ideology is behind it, if any. In your view, what should be done? reply benterix 1 hour agorootparentYeah, I&#x27;m asking myself the same questions, too. It&#x27;s obvious they have been trying many options. Some of them worked, some didn&#x27;t. I criticized some of them in the past but objectively, if I was in their position, I might have made worse mistakes. reply godelski 49 minutes agorootparentThat&#x27;s actually pretty big to say. All I have to say is really just make sure you make criticisms rather than complaints. They need to be constructive (__actionable__) and less emotional. Otherwise the truth is that you may be just throwing fuel onto the fire rather than water. It&#x27;s easy to get these two confused. replyveave 1 hour agoparentprevMost of Firefox&#x27;s publicity is based on the fact that they are \"on your side\". That means that they are held to a much higher standard. Pretending to be on the side of the user and then screwing users up over and over seems like wanting their cake and eating it too?It&#x27;s funny because, you know, I expect chrome to screw me over while I expect firefox not to screw me over but chrome has screwed me over fewer times than firefox over the years. reply Gud 1 hour agorootparentHow exactly has Firefox “screwed you over”? I’ve used Firefox since it was called Phoenix… although I don’t like the governance of the project, I would hardly say I’ve been screwed over.Firefox has been an amazing product since day one.Meanwhile, Chrome has literally been developed to probe your ass for data they can sell to advertisers. reply veave 1 hour agorootparentLast time it was 2 months ago: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36077360In that thread there are a few more examples, most of them somewhat recent: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36077683 reply godelski 1 hour agorootparentI have never seen these myself, being on FF for most of it&#x27;s life, but I also fail to see how these \"screw you over.\" Oh my god an ad! I better instead use a browser that has a built in keylogger and created the ad based internet! That&#x27;ll show those assholes! reply gmerc 49 minutes agorootparentprevGoogle: Let’s put DRM in the browser, hijack the address bar, webmanifestOmg, firefox screwed me over by showing me an ad.I see. I think no further illustration needed. reply BiteCode_dev 1 hour agorootparentprevI&#x27;ve yet to notice any event where firefox \"screw users over\" that was not vastly overblown, especially compared to the competitors.People tend to just judge Mozilla way more harshly.It&#x27;s very unfair: they do good, and we ask even more of them. reply worble 36 minutes agorootparentI know that its against the rules to imply this, but seriously, there is always so much vitriol that I just don&#x27;t understand thrown towards Firefox every update that it really gets the old noggin get going.Can we seriously not even contemplate the idea that there might be a certain corporate entity out there with a vested interest in ensuring that Firefox usage is kept deliberately low while at the same ensuring they _just_ enough funding to keep going so they can point to it and say \"look, we&#x27;re not a monopoly!\" reply godelski 1 hour agorootparentprevThere&#x27;s no better way to get someone to stop listening to you than to turn everything they do into evil. Which is an implicit point I was getting at in my original comment. The more people make mountains out of molehills, the more they are actively training Mozilla to just not give a fuck and ignore them. You can&#x27;t have it both ways. You can&#x27;t have a company that listens and works with you while you also vehemently attack it at every opportunity. I mean look around at the comments. There are people just taking the opportunity to attack them unrelated to the topic and people that are happy are \"this is nice, but it could be better.\" It&#x27;s amazing that people are more passionate about hating Mozilla than they are about airlines or their ISP. Or... you know... the people who scrub up all their data and turn it against them to sell shit, ideas, and make them hateful balls of anxiety. reply shinryuu 1 hour agorootparentprevPerhaps that is because they screw you over in secret? Whenever Firefox makes a mistake it gets very publicised, but whenever chrome does it it&#x27;s just business as normal so no one really publicises it. reply Nextgrid 2 hours agoparentprevThe other browsers you mention though don’t pretend to be on your side and don’t use every possible opportunity to tell you about it.Firefox on the other hand, despite having a lot of user-hostile functionality and design decisions, will constantly nag you pretending to be on your side, misleading you into a false sense of security. reply cycomanic 2 hours agorootparentThose arguments always remind me of this comic: https:&#x2F;&#x2F;www.cjr.org&#x2F;wp-content&#x2F;themes&#x2F;cjr2017&#x2F;images-body&#x2F;NY... reply 1vuio0pswjnm7 5 hours agoprevOut of curiosity I recently did an experiment to investigate Add-Ons in Firefox for Android.Some facts:One does not need Firefox Nightly to run more extensions than the \"supported\" ones.Creating a \"custom\" collection of Add-Ons on addons.mozilla.org can be done using a disposable email address.There is no visible Settings option to add a custom collection in Fennec from F-Droid. To make it visible, one has to go the About page under Settings and tap the Firefox logo three or more times.One needs to enter an \"ID\" in Fennec to add a \"custom\" collection. The addons.mozilla.org site does not tell the user her ID, it&#x27;s in the URL that points to her collection.After a custom collection is added, one loses the default collection of Add-Ons \"supported\" by Mozilla. No way to easily switch between collections. Need to remove the current collection.After installing a custom collection, whenever the user access the Add-Ons menu in Fennec, Firefox tries to connect to addons.mozilla.org. Mozilla&#x27;s definition of user privacy excludes telemetry and data collection by Mozilla.As another commenter mentioned, many of the Add-Ons that are not \"supported\" by Mozilla work just fine. They work just as well as uBblock origin, NoScript and the other \"supporteed\" ones. reply yukIttEft 3 hours agoparent> Creating a \"custom\" collection of Add-Ons on addons.mozilla.org can be done using a disposable email address.I seriously don&#x27;t get it. Why do I have to create a \"custom collection\" with an email address to install an extension. Why can&#x27;t it work like it does on PC ? reply input_sh 2 hours agorootparentBecause extension support wasn&#x27;t considered ready for normal (non-developer) usage up until now, and this announcement you&#x27;re commenting on literally starts with saying it&#x27;s gonna be ready by the end of the year. reply Springtime 37 minutes agorootparentThe irony is addons were natively supported by the regular version of Firefox for Android up until they made the Quantum update, when they removed them and only later with Nightly allowing them to return.I used to have a number of addons installed at that time, including Stylish (prior to the hostile changes), uBlock Origin and UnMHT. reply Sabinus 2 hours agorootparentprevBecause this is just for devs to install and test their extension code before the feature is turned on officially. reply hexo 3 hours agorootparentprevBecause of stage 2.5 - i.e. User abuse by company. reply andyjohnson0 1 hour agorootparent> Because of stage 2.5 - i.e. User abuse by company.Not everything fits into the framework of enshitification. This is about the transition to ff&#x2F;android supporting the full range of extensions. Its a transitional measure. reply danShumway 5 hours agoprevThis is a big surprise and a welcome one. I honestly had given up hope of this happening. It&#x27;s disappointing that it took this long, but it is a big deal to have full extension support on Android again. I&#x27;m very happy to see this news. reply ncann 4 hours agoparentIn case you didn&#x27;t know, you can have full extension support on Android with Kiwi Browser, which is based on Chromium. reply specproc 3 hours agorootparentI did not like the look of Kiwi Browser one bit, lots of weird permissions and dodgy ad-tech smells. Plus it&#x27;s not Firefox, I&#x27;d rather use the same browser on both desktop and Android.This is great news. Can&#x27;t wait to have a no-fuss paywall bypass on mobile again. reply rvnx 1 hour agorootparentThe permissions are the Chromium ones.There is literally no ads nor external SDK or ad-tech or whatever you are referring to (before there was AdMob, but it didn&#x27;t make much money anyway).https:&#x2F;&#x2F;reports.exodus-privacy.eu.org&#x2F;en&#x2F;reports&#x2F;com.kiwibro...The way Kiwi makes money is by partnering with Bing &#x2F; Yahoo.You launch the browser, it starts with Bing or Yahoo (depending on your country), and... that&#x27;s it.Bing then pays back a revenue-share to the browser each time the user uses the search engine.The same way that DuckDuckGo gets a revenue-share from Yahoo and Bing too when you use DuckDuckGo.So, if you prefer Firefox and Google, or Kiwi and Bing, this is just a matter of choice. reply friend_and_foe 3 hours agorootparentprevIceraven has been allowing a ton more extensions than Firefox all this time, no need to go chromium. reply specproc 3 hours agorootparentYeah, I was doing Iceraven for a bit, but it was a pain to keep updated.Super stoked with this news this morning. reply friend_and_foe 3 hours agorootparentHow was it a pain? Ffupdater has had it available for a very long time, and there&#x27;s Unobtainium that lets you update from git repo releases among other things. replyfriend_and_foe 3 hours agoparentprevCurious, why didn&#x27;t you try Iceraven this whole time? reply orra 3 hours agorootparentHere&#x27;s a good reason not to install Iceraven:> No warranties or guarantees of security or updatesIn general it&#x27;s fine for users to fork software to scratch their own itches. But browsers are super security sensitive. reply friend_and_foe 1 hour agorootparentThat&#x27;s just their way of saying \"we don&#x27;t have to build this for you\". They merge upstream changes, as long as the project is maintained you&#x27;ll get those updates as they&#x27;re released. Last I checked you can&#x27;t sue Mozilla for a security vulnerability and the open source license has a clause about no warrant of merchantability or fitness for a particular purpose. They don&#x27;t guarantee security either. reply heap_perms 1 hour agoprevI want to load my own XPI files.Because apparently, I&#x27;m not allowed to embrace the full potential of my browser by installing any extensions that haven&#x27;t been hand-delivered and serenaded by the majestic Firefox Marketplace.Please share if you&#x27;re familiar with a method. I&#x27;m contemplating the creation of my own customized version of Android Firefox, incorporating code to enable installation from XPI files. reply prmoustache 50 minutes agoparenthave you tried fennec from the f-droid repo? AFAIK it allows you to do that, by taping on an xpi file in your phone storage it proposes to install it for you. reply heap_perms 21 minutes agorootparentThanks, I didn&#x27;t know that. I&#x27;ll check it out.Edit: Just tried it, not working: Tabbing on the xpi file -> Open with. Getting the toast \"None of your Apps can open this file\" even tough I just installed fennec. reply ptman 2 hours agoprevWe need to be able to directly fund Firefox instead of all the other mozilla stuff. https:&#x2F;&#x2F;connect.mozilla.org&#x2F;t5&#x2F;ideas&#x2F;ability-to-donate-money... reply kikokikokiko 2 hours agoparentWe need to donate to a new team&#x2F;foundation&#x2F;corporation, anyone at all, that would fork Firefox and continue it&#x27;s development. I would never, ever, give a single cent to the Mozilla Foundation. It&#x27;s current form is not focused on what the users of it&#x27;s products want or need. The ideology Mozilla loves to shove in it&#x27;s users faces is just the cherry on top. I&#x27;ve been using Firefox ever since the early 2000s, and even developed some addons for it on the good old days. Nowadays I just use it while clipping my nose, to avoid giving even more power to the Devil incarnate itself that is Google. reply bjord 2 hours agorootparentJust for record, I&#x27;m not with you as far as the anti-mozilla stance. That being said, what&#x27;s stopping you (or anyone) from funding a team that continues to work on firefox within the same fork, rather than potentially duplicating their work? Is your impression that mozilla&#x27;s ideology would prevent that other team from doing meaningful work or going in a direction that you believe they need to go? reply evilpie 36 minutes agorootparentIgalia is regularly contracted to work on new Firefox features, so this is already happening. reply CoBE10 4 hours agoprevGreat! Firefox on Android was perfect until it was redesigned, and only a few extensions were allowed. I switched to the Kiwi Browser just because of that. I hope that it will work as well as it did before the extensions were removed, so I can go back. reply orbital-decay 1 hour agoparentAll this time non-approved extensions were actually available for Firefox, in the form of \"collections\". (source: I use them on Mull) reply friend_and_foe 3 hours agoparentprevYou can also check out iceraven, if you want a gecko based browser that lets you do what you want with your property. reply shultays 2 hours agoprevnext [–]coming*coming backThey removed it. It was one good thing about Firefox mobile and they removed it.Now please bring back the old UI and fix other issues you broke with that update (so the window that shows tabs so it doesn&#x27;t forget where its position was, or make external video player work again) and I might switch back. reply lemper 4 hours agopreveven though i am an exclusively firefox user both on desktop and mobile, i don&#x27;t even know what kind of desktop version&#x27;s extension that i would use on mobile. currently, i only use adblock and so far my experience is quite stellar.though it should be noted that i use browser on phone just to consume text. no video, no audio, and even picture is rarely the focus.so, what do i miss? reply Sabinus 2 hours agoparenthttps:&#x2F;&#x2F;addons.mozilla.org&#x2F;en-US&#x2F;firefox&#x2F;addon&#x2F;nitter-redire...So I can read Twitter links without dealing with the login modals. reply alraj 3 hours agoparentprevFrom the add-ons I use in my desktop which I cannot install in android firefox from addons.mozilla.org are:1. OneTab 2. Unhook 3. Unclutter 4. ViolentmonkeyTampermonkey shows in Add-on manager list in android, but not in addons.mozilla.org! reply lemper 3 hours agorootparentthat OneTab looks nice. will definitely help with my current usage pattern. thanks. reply prmoustache 49 minutes agoparentprev> so, what do i miss?multi-containers.How do you want to separate cookies&#x2F;identities without that? reply mrd3v0 3 hours agoparentprevDark Reader, LocalCDN, Firefox Relay, Firefox Translations, LanguageTool among others I have installed on Iceraven, a Firefox for Android fork. reply maccard 1 hour agorootparentDark Reader and Relay are both on Android. I&#x27;m not sold on LocalCDN being necessary or even helpful these days (but there was a time when it was. A bit like antiviruses other than Windows defender these days, or those registry cleaner tools).The others are missing for sure, though. reply Macha 1 hour agorootparentprevconsent-o-matic, tampermonkey reply cubefox 2 hours agoparentprevAn extension to block video auto play perhaps. reply Sakos 1 hour agoparentprevSingleFile is the single most useful addon that I use (behind uBlock). It lets me download any website as HTML while keeping most of its layout&#x2F;elements intact.https:&#x2F;&#x2F;addons.mozilla.org&#x2F;en-US&#x2F;firefox&#x2F;addon&#x2F;single-file&#x2F; reply kikokikokiko 2 hours agoparentprevBehindtheoverlay is a little extension that should not be as usefull as it is. I ude it daily, both on mobile and desktop, and I do not use Firefox mobile for this exact same reason. reply quantumsequoia 3 hours agoparentprevI use the nightly firefox app so I can use the bypass paywalls extension. Looking forward to being able to use it on the stable app reply NovaDudely 3 hours agoprevAbout time!I mean I am happy enough with Firefox on android even with it quirks, and the main add-ons I use are already supported but this will fill in the last few gaps.Very welcome! reply qwerty456127 1 hour agoprevThat&#x27;s awesome. I really want the Copy All Tab URLs extension. I will probably switch from Brave (which doesn&#x27;t have it either, so I already have hundreds of tabs opened in it, waiting for a solution) to Firefox just for this. I don&#x27;t want to sign up for any acount and use any cloud for my browsing history, just copy everything and dump it to another app I use to move data between devices.That&#x27;s very sad and weird mobile browsers usually don&#x27;t have extensions. Firefox can probably repeat the history here - as I remember it was the first to introduce extensions (on desktop) and they are a cornerstone part of what have made it great in the first place. reply jokoon 32 minutes agoprevFunny, I realized 30min ago I could not install reddit enhancement suite on firefox android. reply speedgoose 4 hours agoprevI remember using extensions on Firefox android years ago. Is my memory failing in this or did they remove them before putting them back? reply friend_and_foe 3 hours agoparentNope, youre right. With the Firefox Quantum update they took away extension support with the exception of a handful of white listed extensions. reply wvenable 4 hours agoparentprevYes. Firefox on Android used to be basically identical to Firefox on the desktop and it was great. It supported all extensions that the desktop version did.But then they decided they needed a mobile-specific experience and created the current dumbed down version with limited add-on support. reply lopis 4 hours agorootparentThe old Firefox was a terrible App, performance wise. The new one tried to create a better experience so they restricted addons until they knew it wasn&#x27;t a problem... reply friend_and_foe 3 hours agorootparentWhy not just put up a warning for extensions that dont work well and let the user decide? I&#x27;ve used several unsupported extensions in iceraven for years and had no problems, which leaves me feeling suspicious about their stated motivation. reply orra 3 hours agorootparent> Why not just put up a warning for extensions that dont work well and let the user decide?Because the typical user will ignore the warning and install the extension. Then down the line Firefox will be slow and they forget why. Then they uninstall Firefox.Sure, you can avoid this trap personally. But the statistical behaviour is harmful to a healthy Firefox userbase. reply friend_and_foe 1 hour agorootparentBut the typical user doesn&#x27;t use Firefox. People who (claim to) want to empower themselves do. It doesn&#x27;t have a healthy userbase, they keep screaming from the rooftops every time someone mentions that they&#x27;ve switched for brave or something.If you want the typical user to use Firefox, setting yourself apart as the mobile browser that has extensions for example, or keeping features that people use and like as they used to do when they had dominant market share would be good ways to do it. They used to understand their selling point.And beyond that, as I have personally seen using iceraven and others have noted in here, tons of extensions that firefox blacklisted work fine and have for years. They still blacklisted them. Even if we agreed that disempowering the user so that they have a good experience made sense, which I don&#x27;t, the actual number they would&#x27;ve needed to blacklist would&#x27;ve been much lower than the ones they actually blacklisted. In at least one of those instances the stated reason was a blatant lie. reply iudqnolq 4 hours agorootparentprevI was quite skeptical up until now. But this article includes legitimate reasons Android has different API needs, like the absence of reliable long-lived processes. If they actually finish and release this then I&#x27;ll be convinced they had a good reason to need to iterate on the API design in a semi-closed beta. reply shmde 4 hours agoparentprevYour memory is failing you. Been using firefox android ( stable and beta) with the addons (ublock etc) since a few years now. reply speedgoose 2 hours agorootparentAnother comment mentions the quantum Firefox update from 2017. I think I was using Firefox on Android from around 2014. But I probably also used it after the update. I guess all my extensions were quickly whitelisted by Mozilla. reply unmole 4 hours agorootparentprevYou could install custom extensions on Firefox Android. Now you have to jump through hoops to install anything that isn&#x27;t blessed by Mozilla. reply heap_perms 1 hour agorootparentIndeed. They have too much authority with the marketplace. Does someone know if it is feasible to build Fenix from source, fork it and then add a startup function that sideloads any extension the user chooses, not just the blessed ones? I&#x27;m considerung doing this, but don&#x27;t want to spend a lot of time doing something impossible. reply unmole 1 hour agorootparentThis can be a starting point: https:&#x2F;&#x2F;github.com&#x2F;fork-maintainers&#x2F;iceraven-browser reply prmoustache 47 minutes agorootparentprevFennec allows you to install extensions you want afaik, even xpi files in your phone storage. replyvsskanth 5 hours agoprevFinally I can get off Firefox Nightly and move to the stable release reply friend_and_foe 5 hours agoparentOr you can get off Firefox and stop feeling like a kicked dog all the time. It doesn&#x27;t have to be a chromium fork either, there&#x27;s iceraven. reply c0nducktr 5 hours agorootparentWhat does iceraven get you compared to firefox? reply doctor_radium 4 hours agorootparentIceraven&#x27;s reason for existence has been about allowing (pretty much?) all Firefox addons since day one. And about:config.Wish you hadn&#x27;t downvoted him. reply c0nducktr 3 hours agorootparent> Wish you hadn&#x27;t downvoted him.If this directed at me, well, I didn&#x27;t downvote anybody. I saw the parent post being downvoted a bunch, and wanted to know what they were talking about.I don&#x27;t currently have an Android phone. Having a better browser is one of the few things which would want me to move from iOS.It was an honest question. reply c0nducktr 3 hours agorootparentEdit: btw, I had originally upvoted them, even though their original statement was a little rude, and now flagged off the page (I think). reply doctor_radium 3 hours agorootparentprevSorry then. Just working off context clues. reply c0nducktr 2 hours agorootparentNo worries.Are you able to set Firefox as the default browser for Android for the whole phone? For instance, if I clicked on a link in an app, would uBlock Origin filter everything loaded? i.e, the app loaded the page in the same app, can I have that rendering engine set to Firefox with Ublock?Edit: Sorry for anyone reading this, I know I&#x27;m using the wrong terms, hopefully you know what I mean. reply Jailbird 2 hours agorootparentChiming in: on my Android, I do have FF Nightly as my default browser for the whole phone. Recently some Google apps have begun to use Chrome, ignoring the system setting, but mostly, the setting is honored. There might be a few other exceptions but again... mostly honored. (Phone is a Samsung Galaxy S23) reply friend_and_foe 3 hours agorootparentprevYep, and I still have to deal with my settings being reset every time I update, their user hostile UX flows, strange highlights on web page elements, and all the other stuff Firefox says aren&#x27;t important to users. But I do it, why? So that all those scaremongers who screech \"but you have to use Firefox! Otherwise youre helping the evil only other game in town!\" like they always do don&#x27;t have a single argument. Mozilla shows utter disdain for their users with just about every announcement, update, new feature or new restriction. The irrationality with which people defend them is jarringly similar to a cult, it&#x27;s really unsettling. I just want a web browser that doesn&#x27;t feel like it fucking hates me. reply orra 2 hours agorootparent> Mozilla shows utter disdain for their users with just about every announcement, update, new feature or new restrictionThis is absolute hyperbole. Have you forgotten the context? You&#x27;re on a story about full extension support being implemented, which is exactly what you wanted. reply friend_and_foe 1 hour agorootparentTaking that away was just the last straw. Trust has been broken, and I don&#x27;t like being bullshitted and abused, and then brow beaten and guilt tripped for not liking it. I use tools that respect me and do what I need them to do, I refuse to be herded into a set of acceptable behaviors and thoughts like a cow, and by a corporation no less. The people making decisions at Mozilla should be ashamed of themselves. reply prmoustache 46 minutes agorootparent> and I don&#x27;t like being bullshitted and abused, and then brow beaten and guilt tripped for not liking it.So after your lil&#x27;karen rant you switched to proprietary browsers that do worst? replyfriend_and_foe 4 hours agorootparentprevI&#x27;ve had access to all these extensions with it for years. Which tells me this restriction Firefox has had this whole time was not technical and their explanations were a bunch of bullshit. reply itvision 3 hours agoprevWhat next?Allowing access to about:config?Nah, I&#x27;m dreaming. Will continue to use Nightly. reply Nextgrid 2 hours agoparentDoes Nightly have a different user-agent or browser fingerprint?If so, that’s very generous of Mozilla to force users to segment themselves into an (even smaller) bucket so they can be easily tracked by fingerprint alone. reply jerrygoyal 3 hours agoprevit&#x27;s already possible via this Firefox fork https:&#x2F;&#x2F;github.com&#x2F;fork-maintainers&#x2F;iceraven-browser&#x2F;release... reply gary_0 6 hours agoprevBetter late than never! reply Zuiii 5 hours agoparentWhile true, Mozilla unfortunately had already forfeited the Android browser market to other competitors by not enabling this day one (contrary to their claims, most extensions worked fine). I would be really interested in reading the rationale behind their decision. The real one. reply NoboruWataya 1 hour agorootparentDisabling extensions had nothing to do with their market share. We have extensions on desktop and they are still being squeezed out there. And judging by some other comments here there is apparently a fork of Firefox for Android that allows extensions - which even I had never heard of until now. reply friend_and_foe 5 hours agorootparentprevYou&#x27;ll never get the real one unfortunately, because corpos (profit and non profit alike) do everything in marketingpseak and think their customers and users are stupid children that have to be bullshitted into behaving. reply Zuiii 5 hours agorootparentTangentially related, but your comment reminded me of this classic: https:&#x2F;&#x2F;connect.mozilla.org&#x2F;t5&#x2F;discussions&#x2F;mozilla-now-only-... reply orra 3 hours agorootparentprev> Mozilla unfortunately had already forfeited the Android browser market to other competitors by not enabling this day oneEr, Mozilla marketshare on Android is low because of Google anti competitively bundling Chrome with the Play Store. reply Dalewyn 2 hours agorootparentFirefox defeated IE which was bundled with Windows at the time.Chrome continues to defeat everyone, including Edge which is bundled with Windows.Firefox&#x27;s reason for irrelevance today has nothing to do with whether its competitors are bundled with operating systems. reply orra 2 hours agorootparentChrome is, and majorly was, pushed by Google websites. Using one near monopoly to create another.Also, Firefox never defeated Internet Explorer. You are misremembering that. Firefox once made impressive market gains despite the anticompetitive competitor bundling. That doesn&#x27;t mean it&#x27;s feasible to keep doing it. reply blackoil 54 minutes agorootparentprevIE team was disbanded and had no release for 5 years. replysfmike 5 hours agoprevWould it be possible to inject chrome extensions somehow into firefox that is the final piece where some websites force to just use chrome and it&#x27;s really annoying and the singular time you have to switch for their chrome based widget&#x2F;extension to do that websites functionality? reply lolinder 5 hours agoparentI have never run into this. What websites are you referring to? reply maccard 1 hour agorootparentIn $LAST_JOB we used an ancient CI tool called Electric Commander. I don&#x27;t know if it was us or them, but it didn&#x27;t work in FF.We currently use playfab in work, and the admin panel is missing a few features on Firefox compared to chrome. Google Meets has been doing some nonsense over the last few weeks on FF causing it to break for our team (and pushed us back to slack huddles). My bank&#x27;s search functionality doesn&#x27;t work on FF (but the app works so I use that instead)Ironically, the biggest source of website breakages for me is extensions. UBlock breaks _way_ more, and I just disable it on a site by site basis. reply asaddhamani 4 hours agorootparentprevBtw there are Chromium based browsers on Android that have allowed extensions for a long time. I do not recall the names as I’ve been on iOS for years now but I know they exist.EDIT: It’s kiwi and yandex browser apparently. reply langsoul-com 4 hours agoprevFinally, now it&#x27;s possible to have actually good history with proper search on mobile! reply liminalsunset 5 hours agoprevThere used to be a way to sideload extensions onto Android FF but I found it too cumbersome so I switched to Kiwi, a build of Chromium with extension support. Will definitely give this a go when it&#x27;s ready, and I hope Temporary Containers works as I miss having that on mobile reply vsskanth 5 hours agoparentThe real annoying part about Kiwi is it doesn&#x27;t seem to play well with 1password, never understood why. reply xs83 5 hours agoprevIf Adblock works on this then I will switch to Firefox from Chrome - the internet is unusable without it reply mordnis 5 hours agoparentI&#x27;m using Firefox with Ublock Origin for years on Android. reply lolinder 5 hours agoparentprevYou can already use uBlock Origin from Firefox Android. reply kome 5 hours agoparentprevublock origin already works on firefox on mobile (android). it&#x27;s already supported, it&#x27;s already available. and it&#x27;s awesome.i don&#x27;t know why and how people still use chrome on mobile. it is an awful web experience. reply dekatron 4 hours agorootparentIt is an awful experience indeed. I switched to Kiwi browser when Chrome forced tab groups on everyone with no option to disable it. Kiwi has add-on support and removes many of Chrome&#x27;s annoying aspects. I could never go back to browsing on mobile without add-ons like uBlock Origin and Sticky Ducky. reply sexy_seedbox 4 hours agorootparentNever heard of Sticky Ducky, will add. For me, the other essential to install on Kiwi is Bypass Paywalls. reply cute_boi 6 hours agoprevNice!I wish Mozilla release altstore&#x2F;sideload version of IOS app with all the feature available in android. reply lazycouchpotato 6 hours agoparentIt would probably be a lot of work for not a lot of people.You can use Firefox extensions on iOS via Orion Browser.[1] https:&#x2F;&#x2F;apps.apple.com&#x2F;app&#x2F;orion-browser-by-kagi&#x2F;id148449820...[2] https:&#x2F;&#x2F;browser.kagi.com&#x2F;faq.html#iosext reply saradhi 4 hours agorootparentFor a moment, I got excited for this. It did not turn up good, as the chrome extension I wanted to try is not working. reply 38 6 hours agoprev> Once you’re ready to test the mobile version of your extension, create a collection on AMO and test it on Firefox for Android NightlyJust in case anyone missed this, it seems that Android Firefox STILL will not allow anonymous installation of extensions. Even if you wrote the extension yourself, and want to install on your own device.Only workaround is to only install approved extensions, or be forced into registering on AMO. if I am wrong I will be happy to hear it, but this upcoming release doesn&#x27;t seem to offer any meaningful change in regards to user privacy and control. reply danShumway 5 hours agoparent> Once you’re ready to test the mobile version of your extension, create a collection on AMO and test it on Firefox for Android Nightly. If you’d prefer to polish your extension before publishing it on AMO, you can also debug and run the extension with web-ext.I think this mirrors the desktop behavior (now correct me if I&#x27;m wrong, I very well might be ;)). Firefox Developer Edition on Desktop has options for sideloading, but otherwise to sideload it needs to be signed. You can however temporarily load an extension to test it if you&#x27;re a developer in I think any version of Firefox.Their link (https:&#x2F;&#x2F;extensionworkshop.com&#x2F;documentation&#x2F;develop&#x2F;developi...) suggests that Android will work (almost) the same -- you can temporarily load an extension to test it, but it needs to be signed unless you&#x27;re on nightly.I do think forcing every extension to be signed is a mistake and I wish they&#x27;d do away with that policy. But at least on first impression, it looks to me like mobile will be as open as desktop is? Which would be a huge improvement considering how locked down the current Firefox for Android is. reply lolinder 6 hours agoparentprevI think you&#x27;re reading too much into that one sentence—that seems more like a temporary measure to me—but it&#x27;s possible that you&#x27;re correct. However:> any meaningful change in regards to user privacy and control.There&#x27;s an enormous difference in user control between the 22 extensions that are currently allowed on Firefox Android and an open library of add-ons, even if Mozilla is retaining control of the add-on installation process for now. I don&#x27;t usually jump to Mozilla&#x27;s defense, but saying this is no \"meaningful change\" is rank hyperbole. reply plorg 5 hours agoparentprevAMO account isn&#x27;t actually required for the user, it&#x27;s required for some entity to create the \"collection\", but you don&#x27;t actually have to have an account to use whichever collection with FF on your phone. Certainly most people who are using this method now correspond 1:1 to AMO account holders, but that isn&#x27;t a necessity of the software. Any sufficiently motivated person could make a collection called \"Too Cool For Mozilla Extensions\" and tweet out the user name and collection name, and anyone who wanted could use that collection instead.You also can sideload apps in Nightly, although I understand that is not ideal. reply NoahKAndrews 6 hours agoparentprevI believe that&#x27;s just instructions for developers to test now, before the open ecosystem ships. reply mkl 6 hours agoparentprevI&#x27;m hoping I&#x27;ll be able to load my own extensions with something like ViolentMonkey, TamperMonkey, etc., as I do on desktop. reply selcuka 5 hours agorootparentTampermonkey already works with the current version of Firefox Android. reply mkl 5 hours agorootparentOh, great! I hadn&#x27;t checked for a while. reply hn92726819 5 hours agoparentprevThe article is about changing your add-on in preparation for their changes. Maybe he&#x27;s saying that right now, before the change, you have to register as a collection? reply tonyg 5 hours agoparentprevSo far there&#x27;s no indication you&#x27;re wrong : - ( reply friend_and_foe 5 hours agoprev> the only major Android browserSo Chrome and Firefox? I already have extensions. It&#x27;s too little too late Mozilla, you burned me by removing them years ago. reply bjord 2 hours agoparentkiwi =&#x2F;= chrome reply friend_and_foe 1 hour agorootparentThat&#x27;s why I find it funny firefox is saying \"only major browser\" as if it&#x27;s some club. Users who do not keep themselves in pens have had extensions in mobile browsers for years now. Firefox brings nothing distinguishing with this announcement, I don&#x27;t have to use a \"major browser\". reply HexDecOctBin 3 hours agoprev [–] Meanwhile, on the iOS version, they can&#x27;t even support the OS-provided content blocker API, meaning Safari has better support for privacy-related add-ons than Firefox.https:&#x2F;&#x2F;github.com&#x2F;mozilla-mobile&#x2F;firefox-ios&#x2F;issues&#x2F;7374 reply nabakin 3 hours agoparentYeah, well, if iOS didn&#x27;t force everyone to use the Safari engine, maybe the situation would be better reply Daedren 2 hours agoparentprevYou can’t use the App Store Safari extensions on webviews.They’d have to add their own content blocking list, much like they did with Firefox Focus. reply newscracker 3 hours agoparentprev [–] If you’re fine with a single tab browser, Firefox Focus [1] comes with a built-in content blocker. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Mozilla is introducing support for an open ecosystem of extensions on Firefox for Android.",
      "Developers can optimize their desktop extensions for mobile use on Firefox for Android.",
      "Firefox for Android will be the only major Android browser to support this extension ecosystem."
    ],
    "commentSummary": [
      "Mozilla plans to release desktop extensions for its upcoming Android release of Firefox to give users more customization options and compete with Chrome and Safari.",
      "There is criticism, ideological disagreements, and speculation about corporate influences regarding Mozilla, but the announcement of upcoming extension support for Android is generally seen as positive.",
      "Users have mixed feelings about Firefox's current extension support on Android, expressing both excitement and frustration, while Mozilla aims to enhance the user experience and attract more users by introducing extensions on Firefox for Android."
    ],
    "points": 419,
    "commentCount": 163,
    "retryCount": 0,
    "time": 1691723370
  },
  {
    "id": 37076210,
    "title": "Do Machine Learning Models Memorize or Generalize?",
    "originLink": "https://pair.withgoogle.com/explorables/grokking/",
    "originBody": "EXPLORABLES Do Machine Learning Models Memorize or Generalize? By Adam Pearce, Asma Ghandeharioun, Nada Hussein, Nithum Thain, Martin Wattenberg and Lucas Dixon August 2023 In 2021, researchers made a striking discovery while training a series of tiny models on toy tasks [1]. They found a set of models that suddenly flipped from memorizing their training data to correctly generalizing on unseen inputs after training for much longer. This phenomenon – where generalization seems to happen abruptly and long after fitting the training data – is called grokking and has sparked a flurry of interest [2, 3, 4, 5, 6]. 0% 20% 40% 60% 80% 100% Accuracy 0 5,000 10,000 15,000 20,000 25,000 30,000 35,000 40,000 45,000 50,000 Training Step → 47,500 An Example Of Grokking: Memorization Followed By Sudden Generalization The model quickly fits the training data with 100% accuracy... ...but doesn't do better than random guessing on the test data After more training, accuracy on the test data improves — the model generalizes! Mouse over to scrub through training The model quickly fits the training data with 100% accuracy... ...but doesn't do better than random guessing on the test data After more training, accuracy on the test data improves — the model generalizes! Mouse over to scrub through training Do more complex models also suddenly generalize after they’re trained longer? Large language models can certainly seem like they have a rich understanding of the world, but they might just be regurgitating memorized bits of the enormous amount of text they’ve been trained on [7, 8]. How can we tell if they’re generalizing or memorizing? In this article we’ll examine the training dynamics of a tiny model and reverse engineer the solution it finds – and in the process provide an illustration of the exciting emerging field of mechanistic interpretability [9, 10]. While it isn’t yet clear how to apply these techniques to today’s largest models, starting small makes it easier to develop intuitions as we progress towards answering these critical questions about large language models. Grokking Modular Addition Modular addition is essentially the fruit fly of grokking.1 The above line chart comes from a model trained to predict 𝑎 + 𝑏 mod 67 .2 We start by randomly dividing all the 𝑎 , 𝑏 pairs into test and training datasets. Over thousands of training steps, the training data is used to adjust the model into outputting correct answers, while the test data is only used to check if the model has learned a general solution. The model’s architecture is similarly simple: ReLU ( 𝑎 one-hot 𝑊 input + 𝑏 one-hot 𝑊 input ) 𝑊 output — a one-layer MLP with 24 neurons.3 All the weights of the model are shown in the heatmap below; you can see how they change during training by mousing over the line chart above. 0 10 20 30 40 50 60 Input Number 0 5 10 15 20 Neuron 0 W_input Training Step 47,500 0 10 20 30 40 50 60 Output Number 0 5 10 15 20 0 W_output Training Step 47,500 -4 -2 +0 +2 +4 The model makes a prediction by selecting the two columns of 𝑊 input corresponding to inputs 𝑎 and 𝑏 then adding them together to create a vector of 24 separate numbers. Next it sets all the negative numbers in the vector to 0 and finally outputs the column of 𝑊 output that’s closest to the updated vector. The weights of the model are initially quite noisy but start to exhibit periodic patterns as accuracy on the test data increases and the model switches ⏵ to generalizing. By the end of training, each neuron — each row of the heatmap — cycles through high and low values several times as the input number increases from 0 to 66. This is easier to see if we group the neurons by how often they cycle at the end of training and chart each of them as a separate line: W_input — Frequency 4 Training Step 47,500 0 5 10 15 20 25 30 35 40 45 50 55 60 65 Input Number -5 0 5 Value W_input — Frequency 5 Training Step 47,500 0 5 10 15 20 25 30 35 40 45 50 55 60 65 Input Number -5 0 5 Value W_input — Frequency 7 Training Step 47,500 0 5 10 15 20 25 30 35 40 45 50 55 60 65 Input Number -5 0 5 Value W_input — Frequency 26 Training Step 47,500 0 5 10 15 20 25 30 35 40 45 50 55 60 65 Input Number -5 0 5 Value W_output — Frequency 4 Training Step 47,500 0 5 10 15 20 25 30 35 40 45 50 55 60 65 Output Number -5 0 5 Each line shows a single neuron Neurons repeating 7 times at the end of training are in this row W_output — Frequency 5 Training Step 47,500 0 5 10 15 20 25 30 35 40 45 50 55 60 65 Output Number -5 0 5 W_output — Frequency 7 Training Step 47,500 0 5 10 15 20 25 30 35 40 45 50 55 60 65 Output Number -5 0 5 W_output — Frequency 26 Training Step 47,500 0 5 10 15 20 25 30 35 40 45 50 55 60 65 Output Number -5 0 5 The periodic patterns suggest the model is learning some sort of mathematical structure; the fact that it happens when the model starts to solve the test examples hints that it’s related to the model generalizing. But why does the model move away from the memorizing solution? And what is the generalizing solution? Generalizing With 1s and 0s Figuring out both of these questions simultaneously is hard. Let’s make an even simpler task, one where we know what the generalizing solution should look like and try to understand why the model eventually learns it. We’ll take random sequences of thirty 1s and 0s and train our model to predict if there is an odd number of 1s in the first three digits. e.g. 000110010110001010111001001011 is 0 while 010110010110001010111001001011 is 1 — basically a slightly trickier XOR with some distraction noise. A generalizing model should only use the first three digits of the sequence; if the model is memorizing the training data, it will also use the subsequent distracting digits [5, 11]. Our model is again a one-layer MLP, trained on a fixed batch of 1,200 sequences.4 At first only training accuracy increases — the model is memorizing the training data. As with modular arithmetic, test accuracy is essentially random and then sharply rises as the model learns a general solution. 50% 60% 70% 80% 90% 100% Accuracy 0 500 1,000 1,500 2,000 2,500 3,000 3,500 4,000 Training Step → 3,680 Accuracy Over Training Test Accuracy Train Accuracy While memorizing ⏵ , the model looks dense and noisy with lots of high magnitude weights (shown as dark red and blue squares) spread across the chart below – the model is using all the inputs to make a prediction. As the model generalizes ⏵ and gets perfect test accuracy, we see all the weights connected to the distracting digits gray out with very low values and the model focusing on the first three digits — mirroring the generalized structure we expected!5 0 5 10 15 20 25 Bias Digit Index 0 5 10 15 20 25 30 Neuron 0 W_input Step 3,680 Only the first three digits provide a generalizing signal The rest are distractions W_output Step 3,680 -4 -2 +0 +2 +4 With this simplified example it’s easier to see why this happens: we’re pushing our model to do two things during training — output a high probability for the correct label (called minimizing loss 6) and have weights with low magnitudes (known as weight decay 7). Train loss actually slightly increases before the model generalizes as it exchanges loss related to outputting the correct label for having lower weights. 1e+0 1e-2 1e-4 1e-6 Loss 0 500 1,000 1,500 2,000 2,500 3,000 3,500 4,000 Training Step → 3,680 Loss Over Training Test Loss Train Loss Train loss gets a bit worse... Train loss gets a bit worse... The sharp drop in test loss makes it appear like the model makes a sudden shift to generalization. But if we look at the weights of the model over training, most of them smoothly interpolate between the two solutions. The rapid generalization occurs when the last weights connected to the distracting digits are pruned by weight decay. 0 500 1,000 1,500 2,000 2,500 3,000 3,500 4,000 Training Steps → 3,680 -3 -2 -1 0 1 2 3 Weight Value W_input Over Training First Three Digits Distraction Digits Bias Term ...while almost all the weights shrink Distraction digit weights removed When Does Grokking Happen? It’s important to note that grokking is a contingent phenomenon — it goes away if model size, weight decay, data size and other hyper parameters aren’t just right. With too little weight decay, the model can’t escape overfitting the training data.8 Adding more weight decay pushes the model to generalize after memorizing. Increasing weight decay even more causes test and train loss to fall together; the model goes straight to generalizing. And with too much weight decay the model will fail to learn anything. Below, we’ve trained over a thousand models on the 1s and 0s task with different hyperparameters. Training is noisy so nine models have been trained for each set of hyperparameters. High Test LossGrokkingTrain/Test Drop TogetherHigh Train Loss 258 128 64 32 16 Num Neurons 750 1000 1250 1500 1750 Train Examples Weight Decay: 0.01 258 128 64 32 16 Num Neurons Weight Decay: 0.03 258 128 64 32 16 Num Neurons Weight Decay: 0.10 258 128 64 32 16 Num Neurons Weight Decay: 0.30 258 128 64 32 16 Num Neurons Weight Decay: 1.00 Min Loss 1.00e-5 Max Test/Train Loss Ratio 5.00e+4 Train Loss Test Loss 0k 5k 10k 15k 20k steps 1e+0 1e-1 1e-2 1e-3 1e-4 1e-5 1e-6 1e-7 1e-8 loss 0k 5k 10k 15k 20k steps 0k 5k 10k 15k 20k steps 0k 5k 10k 15k 20k steps 1e+0 1e-1 1e-2 1e-3 1e-4 1e-5 1e-6 1e-7 1e-8 loss 0k 5k 10k 15k 20k steps 0k 5k 10k 15k 20k steps 0k 5k 10k 15k 20k steps 1e+0 1e-1 1e-2 1e-3 1e-4 1e-5 1e-6 1e-7 1e-8 loss 0k 5k 10k 15k 20k steps 0k 5k 10k 15k 20k steps Num Neurons: 128Train Examples: 750Weight Decay: 0.01 Less constrained model generalize slowly Very constrained models aren't able to fit the train data We can induce memorization and generalization on this somewhat contrived 1s and 0s task — but why does it happen with modular addition? Let’s first understand a little more about how a one-layer MLP can solve modular addition by constructing a generalizing solution that’s interpretable. Modular Addition With Five Neurons Recall that our modular arithmetic problem 𝑎 + 𝑏 mod 67 is naturally periodic, with answers wrapping around if the sum ever passes 67. Mathematically, this can be mirrored by thinking of the sum as wrapping 𝑎 and 𝑏 around a circle. The weights of the generalizing model also had periodic patterns, indicating that the solution might use this property. We can train a simpler model with a head start on the problem, constructing an embedding matrix that places 𝑎 and 𝑏 on a circle using cos ⁡ and sin ⁡ .9 𝑊 embed = ( … … cos ⁡ ( 𝑖 2 𝜋 67 ) sin ⁡ ( 𝑖 2 𝜋 67 ) … … ) 0 10 20 30 40 50 60 Input Number Cos Sin W_embed -1 +0 +1 Then we train 𝑊 in-proj and 𝑊 out-proj in this one-layer MLP: activations = ReLU ( 𝑎 one-hot 𝑊 embed 𝑊 in-proj + 𝑏 one-hot 𝑊 embed 𝑊 in-proj ) logits = activations 𝑊 out-proj 𝑊 embed ⊤ With just five neurons the model finds a solution with perfect accuracy. 0% 20% 40% 60% 80% 100% Accuracy 0 2,000 4,000 6,000 8,000 10,000 12,000 14,000 Training Step → 15,000 Accuracy Over Training Train and test accuracy don't diverage; the model is too small to memorize Cos Sin Input 0 1 2 3 4 Neuron 0 W_in-proj Step 15,000 Cos Sin Output 0 1 2 3 4 0 W_out-proj Step 15,000 -150 -100 -50 +0 +50 +100 +150 Eyeballing the trained parameters, all the neurons converge ⏵ to roughly equal norms. If we directly plot their cos ⁡ and sin ⁡ components, they’re essentially evenly distributed around a circle: -100 0 Input Cos Component 100 -100 0 Input Sin Component 100 W_in-proj 0 1 2 3 4 -100 0 Output Cos Component 100 -100 0 Output Sin Component 100 W_out-proj 0 1 2 3 4 Connect the adjacent neurons on the 𝑊 in-proj circle and an intriguing pattern emerges: 𝑊 out-proj is rotating around the circle twice as fast as 𝑊 in-proj . -100 0 Input Cos Component 100 -100 0 Input Sin Component 100 W_in-proj 0 1 2 4 3 -100 0 Output Cos Component 100 -100 0 Output Sin Component 100 W_out-proj 0 1 2 4 3 The details of how this solution works aren’t essential — check out Appendix A to see how the doubled rotation allows the model to map inputs like 1 + 0 mod 67 and 2 + 66 mod 67 to the same place — but we have found a 20 parameter construction that solves modular addition. Can we find the same algorithm hidden in the 3,216 parameter model we started with? And why does the larger model switch to the generalizing solution after memorizing? It’s Full of Stars Here’s the 𝑎 + 𝑏 mod 67 model that we started with — it’s trained from scratch with no built-in periodicity. 0% 20% 40% 60% 80% 100% Accuracy 0 5,000 10,000 15,000 20,000 25,000 30,000 35,000 40,000 45,000 50,000 Training Step → 49,500 Accuracy Over Training Test Accuracy Train Accuracy 1e+0 1e-2 1e-4 1e-6 Loss 0 5,000 10,000 15,000 20,000 25,000 30,000 35,000 40,000 45,000 50,000 Training Step → 49,500 Loss Over Training Test Loss Train Loss W_input — Frequency 4 Training Step 49,500 0 5 10 15 20 25 30 35 40 45 50 55 60 65 Input Number -5 0 5 Value W_input — Frequency 5 Training Step 49,500 0 5 10 15 20 25 30 35 40 45 50 55 60 65 Input Number -5 0 5 Value W_input — Frequency 7 Training Step 49,500 0 5 10 15 20 25 30 35 40 45 50 55 60 65 Input Number -5 0 5 Value W_input — Frequency 26 Training Step 49,500 0 5 10 15 20 25 30 35 40 45 50 55 60 65 Input Number -5 0 5 Value W_output — Frequency 4 Training Step 49,500 0 5 10 15 20 25 30 35 40 45 50 55 60 65 Output Number -5 0 5 W_output — Frequency 5 Training Step 49,500 0 5 10 15 20 25 30 35 40 45 50 55 60 65 Output Number -5 0 5 W_output — Frequency 7 Training Step 49,500 0 5 10 15 20 25 30 35 40 45 50 55 60 65 Output Number -5 0 5 W_output — Frequency 26 Training Step 49,500 0 5 10 15 20 25 30 35 40 45 50 55 60 65 Output Number -5 0 5 Unlike the constructed solution, where 𝑊 embed rotates around the circle once, this model has many different frequencies. Below, we’ve isolated the frequencies using the discrete Fourier transform (DFT).10 This factors out the learned periodic patterns across inputs, leaving us with the equivalent of 𝑊 in-proj and 𝑊 out-proj from the constructed solution. For each neuron, this gives a cos ⁡ and sin ⁡ value for every possible periodic frequency from 1 to 33. The wave charts we show above use this to group neurons into frequencies by finding their largest cos ⁡ and sin ⁡ value across all frequencies.11 5 10 15 20 25 30 Frequency 0 5 10 15 20 Neuron 0 dft_W_input Training Step 49,500 5 10 15 20 25 30 Frequency 0 5 10 15 20 0 dft_W_output Training Step 49,500 Just like in the 1s and 0s task, weight decay encourages this representation to become much sparser as the model generalizes ⏵ . Grouping neurons by their final trained frequencies, and plotting the cos ⁡ and sin ⁡ components of the DFT for each neuron, we see the same star shapes from the constructed solution appear. -20 -10 0 Input Cos Component 10 20 -20 -10 0 Input Sin Component 10 20 dft_W_input — Frequency 4 2 1 4 3 0 -10 0 Input Cos Component 10 -10 0 10 dft_W_input — Frequency 5 5 11 7 10 8 6 9 -10 -5 0 Input Cos Component 5 10 -10 -5 0 5 10 dft_W_input — Frequency 7 18 15 13 12 16 14 17 -20 -10 0 Input Cos Component 10 20 -20 -10 0 10 20 dft_W_input — Frequency 26 19 20 22 21 23 -20 -10 0 Output Cos Component 10 20 -20 -10 0 Output Sin Component 10 20 dft_W_output — Frequency 4 2 1 4 3 0 -20 -10 0 Output Cos Component 10 20 -20 -10 0 10 20 dft_W_output — Frequency 5 5 11 7 10 8 6 9 -15 -10 -5 0 Output Cos Component 5 10 15 -15 -10 -5 0 5 10 15 dft_W_output — Frequency 7 18 15 13 12 16 14 17 -20 -10 0 Output Cos Component 10 20 -20 -10 0 10 20 dft_W_output — Frequency 26 19 20 22 21 23 This trained model is using the same algorithm as our constructed solution! Below, the contribution to the output generated by the neurons in each frequency are shown and we can see them calculating cos ⁡ 2 𝜋 ( 𝑎 + 𝑏 ) 𝑓 𝑟 𝑒 𝑞 67 .12 Notice what happens to the group of neurons with a frequency of 7 when test loss improves ⏵ after the short plateau at 45,000 steps — they start to snap into a star shape and their outputs more closely approximate a wave. a: 13 b: 19 0 10 20 30 40 50 60 32 Output Number -5 0 Logit 5 Frequency 4 Logits 0 10 20 30 40 50 60 32 Output Number -6 -4 -2 0 2 4 6 Frequency 5 Logits 0 10 20 30 40 50 60 32 Output Number -4 -2 0 2 4 Frequency 7 Logits 0 10 20 30 40 50 60 32 Output Number -5 0 5 Frequency 26 Logits 0 10 20 30 40 50 60 32 Output Number -20 -10 0 Logit 10 20 All Logits To lower loss without using higher weights (which would be punished by weight decay), the model uses several frequencies, taking advantage of constructive interference.[10] There’s nothing magical about the frequencies 4, 5, 7 and 26 — click through other training runs below to see variations of this algorithm get learned. Freqs: 1 15 17 23 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 5 6 9 0k 25k 50k Freqs: 8 9 12 0k 25k 50k Freqs: 7 12 18 22 0k 25k 50k Freqs: 6 8 11 24 0k 25k 50k Freqs: 1 13 22 0k 25k 50k Freqs: 4 7 33 0k 25k 50k Freqs: 15 20 23 25 0k 25k 50k Freqs: 19 20 25 0k 25k 50k Freqs: 17 22 24 32 0k 25k 50k Freqs: 2 6 33 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 5 6 14 0k 25k 50k Freqs: 20 24 29 0k 25k 50k Freqs: 8 13 14 0k 25k 50k Freqs: 2 20 24 28 0k 25k 50k Freqs: 2 11 22 0k 25k 50k Freqs: 2 4 24 31 0k 25k 50k Freqs: 3 5 27 0k 25k 50k Freqs: 1 10 19 26 0k 25k 50k Freqs: 3 12 13 29 0k 25k 50k Freqs: 8 11 12 17 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 1 6 13 28 0k 25k 50k Freqs: 15 30 32 33 0k 25k 50k Freqs: 3 8 15 27 0k 25k 50k Freqs: 2 7 26 0k 25k 50k Freqs: 8 28 29 33 0k 25k 50k Freqs: 5 26 29 0k 25k 50k Freqs: 10 12 29 0k 25k 50k Freqs: 13 15 16 21 0k 25k 50k Freqs: 24 28 30 31 0k 25k 50k Freqs: 12 14 21 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 1 14 16 0k 25k 50k Freqs: 4 6 17 32 0k 25k 50k Freqs: 6 14 20 23 0k 25k 50k Freqs: 9 14 29 0k 25k 50k Freqs: 7 13 31 0k 25k 50k Freqs: 4 7 17 33 0k 25k 50k Freqs: 2 6 13 18 0k 25k 50k Freqs: 1 20 31 0k 25k 50k Freqs: 6 14 30 0k 25k 50k Freqs: 10 13 31 32 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 2 5 8 26 0k 25k 50k Freqs: 1 6 27 29 0k 25k 50k Freqs: 5 11 23 0k 25k 50k Freqs: 1 7 23 33 0k 25k 50k Freqs: 4 7 23 0k 25k 50k Freqs: 6 8 12 32 0k 25k 50k Freqs: 3 19 20 0k 25k 50k Freqs: 11 14 27 33 0k 25k 50k Freqs: 14 15 25 0k 25k 50k Freqs: 24 25 28 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 4 12 24 30 0k 25k 50k Freqs: 10 28 33 0k 25k 50k Freqs: 18 23 32 0k 25k 50k Freqs: 11 13 21 0k 25k 50k Freqs: 1 6 10 20 0k 25k 50k Freqs: 12 14 29 0k 25k 50k Freqs: 18 20 26 0k 25k 50k Freqs: 4 10 11 22 0k 25k 50k Freqs: 2 18 28 31 0k 25k 50k Freqs: 2 3 10 29 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 7 28 30 31 0k 25k 50k Freqs: 7 9 10 32 0k 25k 50k Freqs: 1 5 18 28 0k 25k 50k Freqs: 21 24 31 0k 25k 50k Freqs: 1 20 31 0k 25k 50k Freqs: 3 22 27 0k 25k 50k Freqs: 3 4 14 22 0k 25k 50k Freqs: 2 7 26 0k 25k 50k Freqs: 24 27 31 0k 25k 50k Freqs: 12 16 18 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 16 30 31 0k 25k 50k Freqs: 2 10 27 0k 25k 50k Freqs: 2 17 20 23 0k 25k 50k Freqs: 23 25 26 0k 25k 50k Freqs: 5 8 22 0k 25k 50k Freqs: 3 9 17 19 0k 25k 50k Freqs: 6 8 9 0k 25k 50k Freqs: 3 25 27 0k 25k 50k Freqs: 1 7 24 0k 25k 50k Freqs: 20 24 30 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 9 20 22 0k 25k 50k Freqs: 7 16 18 30 0k 25k 50k Freqs: 21 26 29 0k 25k 50k Freqs: 10 11 17 22 0k 25k 50k Freqs: 1 2 14 20 0k 25k 50k Freqs: 5 7 14 22 0k 25k 50k Freqs: 3 4 23 0k 25k 50k Freqs: 8 25 28 29 0k 25k 50k Freqs: 7 12 21 0k 25k 50k Freqs: 17 19 24 25 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 3 4 6 19 27 0k 25k 50k Freqs: 8 26 30 0k 25k 50k Freqs: 3 4 27 32 0k 25k 50k Freqs: 5 16 22 0k 25k 50k Freqs: 9 14 26 0k 25k 50k Freqs: 8 9 13 26 0k 25k 50k Freqs: 4 24 30 32 0k 25k 50k Freqs: 2 10 24 25 0k 25k 50k Freqs: 23 26 28 0k 25k 50k Freqs: 16 24 26 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 9 19 22 24 0k 25k 50k Freqs: 15 21 30 32 0k 25k 50k Freqs: 5 10 26 27 0k 25k 50k Freqs: 3 6 18 20 0k 25k 50k Freqs: 3 4 11 24 0k 25k 50k Freqs: 4 7 27 0k 25k 50k Freqs: 3 5 17 0k 25k 50k Freqs: 5 26 33 0k 25k 50k Freqs: 1 7 17 0k 25k 50k Freqs: 7 19 25 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 4 8 33 0k 25k 50k Freqs: 11 14 24 0k 25k 50k Freqs: 19 22 31 0k 25k 50k Freqs: 1 8 14 33 0k 25k 50k Freqs: 12 14 23 28 0k 25k 50k Freqs: 7 15 26 0k 25k 50k Freqs: 7 19 25 27 31 0k 25k 50k Freqs: 4 12 24 33 0k 25k 50k Freqs: 7 10 22 0k 25k 50k Freqs: 8 11 17 32 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 20 21 28 0k 25k 50k Freqs: 1 8 11 0k 25k 50k Freqs: 8 12 28 30 0k 25k 50k Freqs: 22 28 30 0k 25k 50k Freqs: 2 6 15 0k 25k 50k Freqs: 19 27 30 0k 25k 50k Freqs: 24 27 33 0k 25k 50k Freqs: 6 10 27 0k 25k 50k Freqs: 7 18 23 31 0k 25k 50k Freqs: 3 14 18 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 1 21 28 0k 25k 50k Freqs: 6 8 15 18 0k 25k 50k Freqs: 5 9 15 23 26 0k 25k 50k Freqs: 13 20 23 32 0k 25k 50k Freqs: 4 13 19 0k 25k 50k Freqs: 5 9 25 0k 25k 50k Freqs: 6 8 17 20 0k 25k 50k Freqs: 9 15 22 0k 25k 50k Freqs: 11 21 27 0k 25k 50k Freqs: 5 15 24 30 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 1 7 32 0k 25k 50k Freqs: 3 7 8 23 0k 25k 50k Freqs: 21 22 31 0k 25k 50k Freqs: 5 15 17 33 0k 25k 50k Freqs: 6 20 22 0k 25k 50k Freqs: 12 18 27 30 0k 25k 50k Freqs: 8 17 30 0k 25k 50k Freqs: 9 19 29 31 0k 25k 50k Freqs: 3 9 13 0k 25k 50k Freqs: 9 19 25 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 8 11 17 32 0k 25k 50k Freqs: 9 13 27 33 0k 25k 50k Freqs: 3 10 20 31 0k 25k 50k Freqs: 8 22 28 0k 25k 50k Freqs: 5 13 19 0k 25k 50k Freqs: 2 9 30 0k 25k 50k Freqs: 1 3 13 29 0k 25k 50k Freqs: 13 20 22 30 0k 25k 50k Freqs: 1 17 23 33 0k 25k 50k Freqs: 16 19 23 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 2 4 14 22 0k 25k 50k Freqs: 23 26 28 0k 25k 50k Freqs: 8 9 27 0k 25k 50k Freqs: 9 22 23 0k 25k 50k Freqs: 17 22 32 0k 25k 50k Freqs: 7 10 29 0k 25k 50k Freqs: 4 8 19 0k 25k 50k Freqs: 13 22 24 27 0k 25k 50k Freqs: 8 17 19 0k 25k 50k Freqs: 6 14 26 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 11 23 28 32 0k 25k 50k Freqs: 23 25 29 0k 25k 50k Freqs: 12 15 17 0k 25k 50k Freqs: 4 16 21 0k 25k 50k Freqs: 11 13 31 0k 25k 50k Freqs: 10 26 28 30 0k 25k 50k Freqs: 1 24 27 32 0k 25k 50k Freqs: 14 25 31 33 0k 25k 50k Freqs: 6 10 32 0k 25k 50k Freqs: 8 23 27 33 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 9 16 32 0k 25k 50k Freqs: 22 25 33 0k 25k 50k Freqs: 4 17 24 0k 25k 50k Freqs: 12 14 27 30 0k 25k 50k Freqs: 1 7 18 0k 25k 50k Freqs: 3 6 7 18 0k 25k 50k Freqs: 11 27 33 0k 25k 50k Freqs: 2 7 12 22 0k 25k 50k Freqs: 7 10 23 0k 25k 50k Freqs: 9 20 21 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 9 11 15 0k 25k 50k Freqs: 15 31 32 0k 25k 50k Freqs: 7 30 33 0k 25k 50k Freqs: 2 6 21 28 0k 25k 50k Freqs: 5 6 21 0k 25k 50k Freqs: 10 14 21 0k 25k 50k Freqs: 6 24 26 27 0k 25k 50k Freqs: 4 8 30 0k 25k 50k Freqs: 3 10 19 24 0k 25k 50k Freqs: 1 5 23 26 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 4 10 13 15 0k 25k 50k Freqs: 12 21 31 0k 25k 50k Freqs: 2 13 18 0k 25k 50k Freqs: 4 15 21 0k 25k 50k Freqs: 16 21 29 0k 25k 50k Freqs: 9 14 28 0k 25k 50k Freqs: 12 15 25 0k 25k 50k Freqs: 9 20 25 0k 25k 50k Freqs: 9 13 29 33 0k 25k 50k Freqs: 22 26 29 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 8 15 21 0k 25k 50k Freqs: 4 7 10 22 0k 25k 50k Freqs: 1 6 9 29 0k 25k 50k Freqs: 1 29 31 0k 25k 50k Freqs: 4 10 33 0k 25k 50k Freqs: 3 12 33 0k 25k 50k Freqs: 17 30 33 0k 25k 50k Freqs: 1 8 23 32 0k 25k 50k Freqs: 6 11 27 30 0k 25k 50k Freqs: 1 7 31 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 5 6 20 0k 25k 50k Freqs: 4 16 23 0k 25k 50k Freqs: 2 7 11 17 0k 25k 50k Freqs: 6 24 33 0k 25k 50k Freqs: 5 7 10 0k 25k 50k Freqs: 4 7 30 0k 25k 50k Freqs: 4 16 26 0k 25k 50k Freqs: 7 9 14 29 0k 25k 50k Freqs: 12 16 25 0k 25k 50k Freqs: 12 23 28 29 31 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 6 20 29 0k 25k 50k Freqs: 11 16 17 20 0k 25k 50k Freqs: 5 6 14 33 0k 25k 50k Freqs: 3 7 11 0k 25k 50k Freqs: 7 9 19 0k 25k 50k Freqs: 5 8 12 0k 25k 50k Freqs: 2 21 28 0k 25k 50k Freqs: 1 17 23 26 0k 25k 50k Freqs: 12 15 23 0k 25k 50k Freqs: 4 7 20 33 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 2 11 19 0k 25k 50k Freqs: 18 22 25 0k 25k 50k Freqs: 16 23 31 0k 25k 50k Freqs: 9 18 23 0k 25k 50k Freqs: 4 6 24 0k 25k 50k Freqs: 2 12 22 27 0k 25k 50k Freqs: 4 9 13 18 28 0k 25k 50k Freqs: 11 14 15 0k 25k 50k Freqs: 4 14 16 29 0k 25k 50k Freqs: 4 9 15 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 8 12 26 0k 25k 50k Freqs: 1 6 19 0k 25k 50k Freqs: 2 20 28 0k 25k 50k Freqs: 2 6 16 27 0k 25k 50k Freqs: 15 29 31 33 0k 25k 50k Freqs: 8 9 13 26 0k 25k 50k Freqs: 1 23 29 0k 25k 50k Freqs: 19 21 22 0k 25k 50k Freqs: 2 14 32 0k 25k 50k Freqs: 12 15 25 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 12 17 24 0k 25k 50k Freqs: 1 2 20 0k 25k 50k Freqs: 9 24 30 32 0k 25k 50k Freqs: 3 4 23 0k 25k 50k Freqs: 12 14 17 33 0k 25k 50k Freqs: 3 13 29 33 0k 25k 50k Freqs: 8 29 33 0k 25k 50k Freqs: 13 22 33 0k 25k 50k Freqs: 4 16 26 0k 25k 50k Freqs: 10 26 28 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 13 19 30 0k 25k 50k Freqs: 3 5 33 0k 25k 50k Freqs: 3 17 21 22 0k 25k 50k Freqs: 3 7 8 25 0k 25k 50k Freqs: 18 20 24 0k 25k 50k Freqs: 2 11 14 0k 25k 50k Freqs: 5 9 16 20 33 0k 25k 50k Freqs: 9 12 17 27 0k 25k 50k Freqs: 9 12 28 0k 25k 50k Freqs: 5 22 23 25 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 10 26 33 0k 25k 50k Freqs: 4 7 21 0k 25k 50k Freqs: 3 4 32 0k 25k 50k Freqs: 12 14 30 0k 25k 50k Freqs: 9 28 33 0k 25k 50k Freqs: 4 7 31 0k 25k 50k Freqs: 2 9 31 0k 25k 50k Freqs: 11 25 32 0k 25k 50k Freqs: 2 7 11 0k 25k 50k Freqs: 1 13 21 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 23 24 28 0k 25k 50k Freqs: 16 24 25 0k 25k 50k Freqs: 9 16 19 0k 25k 50k Freqs: 10 16 33 0k 25k 50k Freqs: 13 15 21 0k 25k 50k Freqs: 14 29 31 32 0k 25k 50k Freqs: 14 24 27 28 0k 25k 50k Freqs: 21 27 30 0k 25k 50k Freqs: 7 14 26 28 0k 25k 50k Freqs: 2 19 20 33 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 4 5 30 0k 25k 50k Freqs: 1 2 17 27 32 0k 25k 50k Freqs: 2 3 15 26 0k 25k 50k Freqs: 17 20 24 0k 25k 50k Freqs: 2 14 28 31 0k 25k 50k Freqs: 13 23 24 0k 25k 50k Freqs: 4 6 19 22 0k 25k 50k Freqs: 8 25 30 0k 25k 50k Freqs: 16 17 18 27 0k 25k 50k Freqs: 1 13 32 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 14 24 31 0k 25k 50k Freqs: 3 6 7 20 0k 25k 50k Freqs: 1 5 23 0k 25k 50k Freqs: 2 10 33 0k 25k 50k Freqs: 2 7 18 19 0k 25k 50k Freqs: 20 25 29 0k 25k 50k Freqs: 22 26 28 0k 25k 50k Freqs: 2 3 20 31 0k 25k 50k Freqs: 14 19 32 0k 25k 50k Freqs: 1 4 27 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 8 18 33 0k 25k 50k Freqs: 12 22 30 0k 25k 50k Freqs: 12 22 25 0k 25k 50k Freqs: 8 10 13 14 0k 25k 50k Freqs: 12 15 21 32 0k 25k 50k Freqs: 12 22 30 31 0k 25k 50k Freqs: 3 7 12 13 0k 25k 50k Freqs: 18 22 33 0k 25k 50k Freqs: 4 14 27 0k 25k 50k Freqs: 1 13 20 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 8 11 15 28 0k 25k 50k Freqs: 3 10 16 0k 25k 50k Freqs: 4 17 31 0k 25k 50k Freqs: 6 11 20 0k 25k 50k Freqs: 15 16 25 0k 25k 50k Freqs: 2 22 31 0k 25k 50k Freqs: 10 11 15 29 0k 25k 50k Freqs: 11 17 18 0k 25k 50k Freqs: 1 6 22 26 0k 25k 50k Freqs: 3 5 21 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 10 23 28 0k 25k 50k Freqs: 10 27 29 0k 25k 50k Freqs: 16 18 28 0k 25k 50k Freqs: 1 21 32 0k 25k 50k Freqs: 2 24 29 0k 25k 50k Freqs: 4 20 21 0k 25k 50k Freqs: 2 14 33 0k 25k 50k Freqs: 2 12 28 31 0k 25k 50k Freqs: 4 10 15 28 0k 25k 50k Freqs: 18 27 30 32 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 7 17 18 0k 25k 50k Freqs: 2 14 31 0k 25k 50k Freqs: 20 22 24 31 0k 25k 50k Freqs: 7 17 18 0k 25k 50k Freqs: 6 25 29 0k 25k 50k Freqs: 3 24 29 0k 25k 50k Freqs: 7 13 25 0k 25k 50k Freqs: 6 18 27 28 31 0k 25k 50k Freqs: 14 16 17 28 0k 25k 50k Freqs: 15 17 23 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 8 12 23 25 0k 25k 50k Freqs: 5 9 28 0k 25k 50k Freqs: 3 17 23 0k 25k 50k Freqs: 20 23 30 32 0k 25k 50k Freqs: 22 24 27 0k 25k 50k Freqs: 6 13 30 33 0k 25k 50k Freqs: 1 4 7 30 0k 25k 50k Freqs: 3 14 19 0k 25k 50k Freqs: 2 14 15 0k 25k 50k Freqs: 7 23 25 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 15 19 21 22 0k 25k 50k Freqs: 12 20 30 0k 25k 50k Freqs: 4 14 23 0k 25k 50k Freqs: 15 26 32 0k 25k 50k Freqs: 11 27 28 0k 25k 50k Freqs: 3 28 29 30 0k 25k 50k Freqs: 4 9 14 16 32 0k 25k 50k Freqs: 12 21 23 27 0k 25k 50k Freqs: 15 21 23 0k 25k 50k Freqs: 4 9 12 19 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 11 13 16 17 0k 25k 50k Freqs: 11 14 18 22 0k 25k 50k Freqs: 1 13 18 0k 25k 50k Freqs: 6 16 27 0k 25k 50k Freqs: 4 11 24 0k 25k 50k Freqs: 4 28 29 0k 25k 50k Freqs: 7 15 17 0k 25k 50k Freqs: 12 16 27 0k 25k 50k Freqs: 11 18 22 24 30 0k 25k 50k Freqs: 7 8 13 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 1 13 15 22 0k 25k 50k Freqs: 17 19 20 21 0k 25k 50k Freqs: 7 9 11 21 0k 25k 50k Freqs: 6 10 13 21 0k 25k 50k Freqs: 19 26 27 0k 25k 50k Freqs: 5 22 26 0k 25k 50k Freqs: 2 20 23 31 0k 25k 50k Freqs: 1 21 30 0k 25k 50k Freqs: 9 10 25 0k 25k 50k Freqs: 1 6 26 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 8 11 23 0k 25k 50k Freqs: 23 25 28 33 0k 25k 50k Freqs: 18 21 31 0k 25k 50k Freqs: 3 11 26 0k 25k 50k Freqs: 2 10 13 0k 25k 50k Freqs: 7 10 18 31 0k 25k 50k Freqs: 2 22 31 0k 25k 50k Freqs: 4 23 25 32 0k 25k 50k Freqs: 3 8 20 32 0k 25k 50k Freqs: 6 12 28 32 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 3 7 15 32 0k 25k 50k Freqs: 9 13 14 0k 25k 50k Freqs: 29 33 0k 25k 50k Freqs: 4 6 15 0k 25k 50k Freqs: 22 26 29 0k 25k 50k Freqs: 6 11 20 32 0k 25k 50k Freqs: 15 20 33 0k 25k 50k Freqs: 6 9 25 0k 25k 50k Freqs: 1 6 26 0k 25k 50k Freqs: 9 21 27 29 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 1 8 17 30 0k 25k 50k Freqs: 5 21 27 0k 25k 50k Freqs: 3 8 12 0k 25k 50k Freqs: 7 23 25 31 0k 25k 50k Freqs: 1 13 20 0k 25k 50k Freqs: 23 26 28 0k 25k 50k Freqs: 6 18 26 0k 25k 50k Freqs: 15 19 25 29 0k 25k 50k Freqs: 4 26 28 0k 25k 50k Freqs: 9 22 28 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 23 27 31 0k 25k 50k Freqs: 8 14 27 0k 25k 50k Freqs: 12 14 19 25 0k 25k 50k Freqs: 5 13 19 0k 25k 50k Freqs: 9 26 27 0k 25k 50k Freqs: 3 14 23 0k 25k 50k Freqs: 9 19 25 0k 25k 50k Freqs: 7 9 13 0k 25k 50k Freqs: 19 26 27 0k 25k 50k Freqs: 5 20 23 0k 25k 50k 1e+0 1e-2 1e-4 1e-6 1e-8 loss Freqs: 17 22 23 32 0k 25k 50k Freqs: 21 27 29 30 0k 25k 50k Freqs: 11 16 22 0k 25k 50k Freqs: 6 20 24 0k 25k 50k Freqs: 11 26 29 0k 25k 50k Freqs: 6 22 24 0k 25k 50k Freqs: 9 14 18 24 0k 25k 50k Freqs: 16 19 20 30 0k 25k 50k Freqs: 8 18 21 25 0k 25k 50k Open Questions While we now have a solid understanding of the mechanisms a one-layer MLP uses to solve modular addition and why they emerge during training, there are still many interesting open questions about memorization and generalization. Which Model Constraints Work Best? Directly training the model visualized above — ReLU ( 𝑎 one-hot W input + 𝑏 one-hot W input ) W output — does not actually result in generalization on modular arithmetic, even with the addition of weight decay. At least one of the matrices has to be factored: W input = 𝑊 embed 𝑊 in-proj W output = W out-proj W embed ⊤ We observed that the generalizing solution is sparse after taking the discrete Fourier transformation, but the collapsed matrices have high norms. This suggests that direct weight decay on W output and W input doesn’t provide the right inductive bias for the task. Broadly speaking, weight decay does steer a wide variety of models away from memorizing their training data [12, 13]. Other techniques that help avoid overfitting include dropout, smaller models and even numerically unstable optimization algorithms [14]. These approaches interact in complex, non-linear ways, making it difficult to predict a priori which will ultimately induce generalization. Collapsing 𝑊 embed 𝑊 in-proj instead of W out-proj W embed ⊤ , for example, helps in some setups and hurts in others: High Test LossGrokkingTrain/Test Drop TogetherHigh Train Loss L2 Weight Decay Untied W_embed Untied W_proj-in T F collapsed_in T F collapsed_out Tied W_proj-in T F collapsed_in Tied W_embed Untied W_proj-in T F collapsed_in T F collapsed_out Tied W_proj-in T F collapsed_in L1 Weight Decay Untied W_embed Untied W_proj-in T F collapsed_in T F collapsed_out Tied W_proj-in T F collapsed_in Tied W_embed Untied W_proj-in T F collapsed_in T F collapsed_out Tied W_proj-in T F collapsed_in Min Loss 1.00e-5 Max Test/Train Loss Ratio 5.00e+4 ReLU ( 𝑎 one-hot 𝑊 in-embed 𝑊 in-proj-a + 𝑏 one-hot 𝑊 in-embed 𝑊 in-proj-b ) W out-proj W out-embed ⊤ 1 .3 .1 .03 .01 Weight Decay 1e-2 1e-3 1e-4 Learning Rate Neurons: 32 1 .3 .1 .03 .01 Weight Decay Neurons: 64 1 .3 .1 .03 .01 Weight Decay Neurons: 128 1 .3 .1 .03 .01 Weight Decay Neurons: 256 1 .3 .1 .03 .01 Weight Decay Neurons: 512 L2 Weight Decay: 0.3Learning Rate: 0.01Neurons: 128 0k 100k 200k steps 1e+4 1e+2 1e+0 1e-2 1e-4 1e-6 1e-8 loss 0k 100k 200k steps 0k 100k 200k steps 0k 100k 200k steps 1e+4 1e+2 1e+0 1e-2 1e-4 1e-6 1e-8 loss 0k 100k 200k steps 0k 100k 200k steps 0k 100k 200k steps 1e+4 1e+2 1e+0 1e-2 1e-4 1e-6 1e-8 loss 0k 100k 200k steps 0k 100k 200k steps Train Loss Test Loss Why Is Memorization Easier Than Generalization? One theory: there can be many more ways to memorize a training set than there are generalizing solutions. So statistically, memorization should be more likely to happen first, especially if we have no or little regularization. Regularization techniques, like weight decay, prioritize certain solutions over others, for example, preferring “sparse” solutions over “dense” ones. Recent work suggests that generalization is associated with well-structured representations [15]. However, it’s not a necessary condition; some MLP variations without symmetric inputs learn less “circular” representations when solving modular addition [4]. We also observed that well-structured representations are not a sufficient condition for generalization. This small model (trained with no weight decay) starts generalizing, then switches to memorizing with periodic embeddings. 1e-1 1e-2 1e-3 1e-4 1e-5 1e-6 1e-7 1e-8 Loss 0 2,000 4,000 6,000 8,000 10,000 12,000 14,000 16,000 18,000 20,000 Training Step → 800 Reverse Grokking With A Small Model Test Loss Train Loss A memorizing model can learn larger weights to reduce loss if there's no weight decay 0 10 20 30 40 50 60 Input Number 0 2 4 6 8 Neuron 0 W_input Training Step 0,800 0 10 20 30 40 50 60 Output Number 0 2 4 6 8 0 W_output Training Step 0,800 -3 -2 -1 +0 +1 +2 +3 It’s even possible to find hyperparameters where models start generalizing, then switch to memorizing, then switch back to generalizing! 13 1e-1 1e-2 1e-3 1e-4 1e-5 1e-6 1e-7 1e-8 Loss 0 10,000 20,000 30,000 40,000 50,000 60,000 70,000 80,000 Training Step → 3,200 Loss Over Training Test Loss Train Loss 0 10 20 30 40 50 60 Input Number 0 5 10 15 Neuron 0 W_input Training Step 3,200 0 10 20 30 40 50 60 Output Number 0 5 10 15 0 W_output Training Step 3,200 -20 +0 +20 What About Larger Models? Does grokking happen in larger models trained on real world tasks? Earlier observations reported the grokking phenomenon in algorithmic tasks in small transformers and MLPs [1, 10, 4]. Grokking has subsequently been found in more complex tasks involving images, text, and tabular data within certain ranges of hyperparameters [2, 16]. It’s also possible that the largest models, which are able to do many types of tasks, may be grokking many things at different speeds during training [17]. There have also been promising results in predicting grokking before it happens. Though some require knowledge of the generalizing solution [10] or the overall data domain [18], some rely solely on the analysis of the training loss [19] and might also apply to larger models — hopefully we’ll be able to build tools and techniques that can tell us when a model is parroting memorized information and when it’s using richer models. Understanding the solution to modular addition wasn’t trivial. Do we have any hope of understanding larger models? One route forward — like our digression into the 20 parameter model and the even simpler boolean parity problem — is to: 1) train simpler models with more inductive biases and fewer moving parts, 2) use them to explain inscrutable parts of how a larger model works, 3) repeat as needed. We believe this could be a fruitful approach to better understanding larger models, and complementary to efforts that aim to use larger models to explain smaller ones and other work to disentangle internal representations [20, 21, 22]. Moreover, this kind of mechanistic approach to interpretability, in time, may help identify patterns that themselves ease or automate the uncovering of algorithms learned by neural networks. Credits Thanks to Ardavan Saeedi, Crystal Qian, Emily Reif, Fernanda Viégas, Kathy Meier-Hellstern, Mahima Pushkarna, Minsuk Chang, Neel Nanda and Ryan Mullins for their help with this piece. Appendix A: How the Circular Construction Works We can almost calculate 𝑎 + 𝑏 mod 67 using two circular embeddings and a completely linear model. a = 2 b = 12 a + b mod 67 = 14 a - b mod 67 = 57 embed 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 a b First, project the two input points around a circle and average their positions. unembed 1 3 5 7 9 11 13 14 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 Then, double the angle with the unembedding. The answer is the point closest to the averaged position. It works! But we’re cheating a bit, do you see how unembed loops around the circle twice? We need to output a single prediction for “14“ — not separate predictions for “14“ and “81“. Directly adding the two predictions for a number together won’t work since they’re on opposite sides of the circles and will cancel each other out. Instead, let’s incorporate a ReLU ( 𝑥 ) to fix the repeated outputs. embed, in-proj and ReLU 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 First, project the averaged position along 5 evenly spaced directions and apply a ReLU to keep only the positive components. out-proj and unembed 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 Then, rotate the 5 directions around the circle twice as fast. The answer is the point on the circle closest to sum of the positive projections. We’ve essentially wrapped the circle around in on itself and the model outputs a single prediction for “14“. Formally, this is the constructed model: activations = ReLU ( 𝑎 one-hot 𝑊 embed 𝑊 in-proj + 𝑏 one-hot 𝑊 embed 𝑊 in-proj ) logits = activations 𝑊 out-proj 𝑊 embed ⊤ With modulus 𝑀 and 𝑁 evenly spaced neurons/directions: 𝑊 embed = ( … … cos ⁡ ( 𝑖 2 𝜋 𝑀 ) sin ⁡ ( 𝑖 2 𝜋 𝑀 ) … … ) , 𝑊 in-proj 𝑇 = ( … … cos ⁡ ( 𝑖 2 𝜋 𝑁 ) sin ⁡ ( 𝑖 2 𝜋 𝑁 ) … … ) , 𝑊 out-proj = ( … … cos ⁡ ( 2 𝑖 2 𝜋 𝑁 ) sin ⁡ ( 2 𝑖 2 𝜋 𝑁 ) … … ) . Interestingly this circle has a few wrinkles: this construction doesn’t give an exact answer! Every out-proj and unembed The error goes to 0 when the angle aligns with a direction; an odd number of neurons is more accurate. In our sweep of models trained from scratch, 5 is the most common number of neurons in a frequency. Frequencies with 6 neurons are typically closer lopsided pentagons than hexagons. Small Errors -30 -20 -10 0 10 20 30 Target -30 -20 -10 0 10 20 30 Output -30 -20 -10 0 10 20 30 Target -0.2 -0.1 0.0 0.1 0.2 Output - Target Neurons Modulus Using 𝑥 2 instead of ReLU ( 𝑥 ) as the activation function, as suggested by [6] gives a provably exact solution! For simplicity, let 𝜔 : = 2 𝜋 𝑀 (the angle between numbers in 𝑊 embed ) and 𝜃 : = 2 𝜋 𝑁 (the angle between neurons in 𝑊 in-proj 𝑇 ). Let’s rewrite logits 𝑎 , 𝑏 as an 𝑀 -dimensional vector ∥ 𝑙 ∥ 𝑀 where: 𝑙 𝑗 = ∑ 𝑖 = 0 𝑁 − 1 ( [ cos ⁡ ( 𝑎 𝜔 − 𝑖 𝜃 ) + cos ⁡ ( 𝑏 𝜔 − 𝑖 𝜃 ) ] 2 cos ⁡ ( 𝑗 𝜔 − 2 𝑖 𝜃 ) ) ) This follows from the logits 𝑎 , 𝑏 equation above by plugging in the definitions of W in-proj and W out-proj and applying the trigonometric identity that cos ⁡ ( 𝑥 ) cos ⁡ ( 𝑦 ) + sin ⁡ ( 𝑥 ) sin ⁡ ( 𝑦 ) = cos ⁡ ( 𝑥 − 𝑦 ) . We can then prove the following: arg max ⁡ 𝑐  ⁣ logits 𝑎 , 𝑏 = 𝑎 + 𝑏 mod 𝑀 Applying the two trigonometric identities of cos ⁡ ( 𝑥 ) + cos ⁡ ( 𝑦 ) = 2 cos ⁡ ( 𝑥 − 𝑦 2 ) cos ⁡ ( 𝑥 + 𝑦 2 ) and cos ⁡ 2 ( 𝑥 ) 𝑐 𝑜 𝑠 ( 𝑦 ) = 1 / 4 [ 2 cos ⁡ ( 𝑦 ) + cos ⁡ ( 2 𝑥 − 𝑦 ) + cos ⁡ ( 2 𝑥 + 𝑦 ) ] , we have: logits 𝑎 , 𝑏 = ∑ 𝑖 = 0 𝑁 − 1 ( [ cos ⁡ ( 𝑎 𝜔 − 𝑖 𝜃 ) + cos ⁡ ( 𝑏 𝜔 − 𝑖 𝜃 ) ] 2 cos ⁡ ( 𝑐 𝜔 − 2 𝑖 𝜃 ) ) = ∑ 𝑖 = 0 𝑁 − 1 ( 2 [ cos ⁡ ( 𝑎 − 𝑏 2 𝜔 ) cos ⁡ ( 𝑎 + 𝑏 2 𝜔 − 𝑖 𝜃 ) ] 2 cos ⁡ ( 𝑐 𝜔 − 2 𝑖 𝜃 ) ) = cos 2 ⁡ ( 𝑎 − 𝑏 2 𝜔 ) ∑ 𝑖 = 0 𝑁 − 1 ( 2 cos ⁡ ( 𝑐 𝜔 − 2 𝑖 𝜃 ) + cos ⁡ ( ( 𝑎 + 𝑏 − 𝑐 ) 𝜔 ) + cos ⁡ ( ( 𝑎 + 𝑏 + 𝑐 ) 𝜔 − 4 𝑖 𝜃 ) ) Note that ∑ cos ⁡ ( 𝛾 𝑖 ) = 0 where 𝛾 𝑖 is equally spread around the circle. The first and the third sum terms wrap around the circle with 2 𝜃 and 4 𝜃 increments respectively. The sum of the first terms equals zero for 𝑁 > 2 and the sum of the third terms equals zero for 𝑁 > 4 . Therefore, we have: logits 𝑎 , 𝑏 = cos ⁡ 2 ( 𝑎 − 𝑏 2 𝜔 ) cos ⁡ ( ( 𝑎 + 𝑏 − 𝑐 ) 𝜔 ) Since the first term is a positive constant w.r.t inputs, the equation is maximized when cos ⁡ ( ( 𝑎 + 𝑏 − 𝑐 ) 𝜔 ) is maximized, which is when 𝑐 = 𝑎 + 𝑏 mod 𝑀 . Essentially ReLU ( 𝑥 ) activations with weight decay (a very typical model setup) gives the model an inductive bias that’s close enough to the exact generalizing solution of 𝑥 2 activations with a sparse discrete Fourier transform to push in the direction of generalization but not so close that it won’t also learn to fit the training data with memorization. Footnotes 1. In modular addition, we have two input numbers, 𝑎 and 𝑏 , and a modulus 𝑚 . We want to find the remainder of 𝑎 + 𝑏 when divided by 𝑚 . This type of addition is often called clock-face addition, because when adding two times, we often report the result modulo 12 (i.e. 5 hours after 8 o’clock is 1 o’clock). Modular addition sounds simple and it is. We can easily train 1,000s of models and treat them like fruit flies in neuroscience: small enough such that it is feasible to extract their connectome synapse-by-synapse, yet providing new interesting insights about the system more broadly. We can get a good understanding of the small models we’ve trained by visualizing all their internals. 2. 67 isn’t a magic number – we could pick many numbers to illustrate grokking, but 67 is not so small that the task is trivial and also not so large that the visualizations are overwhelming. 3. The model is trained with cross-entropy loss, AdamW and full batches. The section on regularization and training colab have additional details. If you’re not familiar with MLPs, playground.tensorflow.org is a great place to start. A quick notation explanation: The columns of 𝑊 input and 𝑊 ouput represent the numbers from 0 to 66. 𝑎 one-hot and 𝑏 one-hot are how we encode the model’s inputs; each pick a single column from 𝑊 input . ReLU replaces negative numbers with 0s; it is a fancy) way of writing max ⁡ ( 𝑥 , 0 ) . 4. With a small twist — we’re only outputting 1 or 0, so 𝑊 output can be a single column. In the modular addition task we needed a column for every output number. The last column of 𝑊 input is also fixed to 1 to provide a bias term. 5. Appendix D of “A Tale of Two Circuits: Grokking as Competition of Sparse and Dense Subnetworks” has an explanation of the 4 neuron solution generalizing solution here 6. So far we’ve been charting accuracy, the percentage of sequences where the correct label is the most likely. Training typically instead optimizes a differentiable objective function. All the models in this post use cross entropy loss which heavily penalizes incorrect predictions with high probabilities. Note that while some formulations of loss include a weight decay or regularization term, the loss plots here depict the cross entropy component alone. 7. On the 1s and 0s task here, we use L1 weight decay 𝐿 1 ( 𝑤 ) = ∑ 𝑖 ∣ 𝑤 𝑖 ∣ . L2 weight decay 𝐿 2 ( 𝑤 ) = ∑ 𝑖 𝑤 𝑖 2 is a more typical choice. It pushes for lots of small weights leading to redundant neurons on this task: 8. A model overfits the training data when it performs well on the training data but poorly on the test data — this is what we see with our memorizing models. In general, simpler models are less prone to overfitting as, due to their simplicity, decision rules are coarser and are required to make more generalizations. Of course, if a model is too simple for a task, it may not be able to learn good decision rules that capture the nuances of the task. Researchers force models to be simpler through a variety of techniques, including having models with fewer parameters or encouraging the parameters that the model does have to be small in size with weight decay. 9. Here’s what W embed looks like on the unit circle: 10. The Discrete Fourier Transform helps analyze the periodic nature of a sequence of values (in this case the weights for a particular neuron) by breaking it down into sine and cosine functions. The more periodic a function is, the easier it is to represent with sine and cosines, and the sparser the output of the DFT. 11. We’ve reindexed the neurons by their final frequency and phase to make this grouping easier to see . 12. The model generates probabilities by taking the dot product of the neuron activations for a given input with W output and softmaxing. If we calculate the dot product using only the activations from neurons of a single frequency, we can see which outputs the frequency group is making more or less likely. Appendix A explains why these logits form a wave — each group of frequencies is essentially outputting how close the correct answer is to every number on a version of W embed with the group’s frequency. 13. Both of these models are quite small. The bottom model has tweaked hyperparameters to encourage eventual generalization: it’s slightly larger to allow it to exit local minimums, it has more training data (making low loss memorizing solutions harder to find) and it has weight decay. References 1. Grokking: Generalization Beyond Overfitting On Small Algorithmic Datasets Power, A., Burda, Y., Edwards, H., Babuschkin, I., & Misra, V. (2022). arXiv preprint arXiv:2201.02177. 2. Omnigrok: Grokking Beyond Algorithmic Data Liu, Z., Michaud, E. J., & Tegmark, M. (2022, September). In The Eleventh International Conference on Learning Representations. 3. A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations Chughtai, B., Chan, L., Nanda, N. (2023). International Conference on Machine Learning. 4. The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks Zhong, Z., Liu, Z., Tegmark, M., & Andreas, J. (2023). arXiv preprint arXiv:2306.17844. 5. Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit Boaz Barak, Benjamin L. Edelman, Surbhi Goel, Sham Kakade, Eran Malach, Cyril Zhang. (2022) Advances in Neural Information Processing Systems, 35, 21750-21764. 6. Grokking modular arithmetic Andrey Gromov (2023). arXiv preprint arXiv:2301.02679. 7. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?🦜 Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021, March). In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency (pp. 610-623). 8. Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task Li, K., Hopkins, A. K., Bau, D., Viégas, F., Pfister, H., & Wattenberg, M. (2022, September). In The Eleventh International Conference on Learning Representations. 9. Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases Olah, C., 2022. Transformer Circuits Thread. 10. Progress Measures for Grokking via Mechanistic Interpretability Nanda, N., Chan, L., Lieberum, T., Smith, J., & Steinhardt, J. (2022, September). In The Eleventh International Conference on Learning Representations. 11. A Tale of Two Circuits: Grokking as Competition of Sparse and Dense Subnetworks William Merrill, Nikolaos Tsilivis, Aman Shukla. (2023). arXiv preprint arXiv:2303.11873. 12. Unifying Grokking and Double Descent Davies, X., Langosco, L., & Krueger, D. (2022, November). In NeurIPS ML Safety Workshop. 13. Double Descent Demystified: Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle Rylan Schaeffer, R., Khona, M., Robertson, Z., Boopathy, A., Pistunova, K., Rocks, J., Rani Fiete, I., & Koyejo, O. (2023). arXiv preprint arXiv:2303.14151. 14. The Slingshot Mechanism: An Empirical Study of Adaptive Optimizers and the Grokking Phenomenon Thilak, V., Littwin, E., Zhai, S., Saremi, O., Paiss, R., & Susskind, J. (2022). arXiv preprint arXiv:2206.04817. 15. Towards Understanding Grokking: An Effective Theory of Representation Learning Liu, Z., Kitouni, O., Nolte, N. S., Michaud, E., Tegmark, M., & Williams, M. (2022). Advances in Neural Information Processing Systems, 35, 34651-34663. 16. The Goldilocks Zone: Towards Better Understanding of Neural Network Loss Landscapes Fort, S., & Scherlis, A. (2019, July). In Proceedings of the AAAI conference on artificial intelligence (Vol. 33, No. 01, pp. 3574-3581). 17. The Quantization Model of Neural Scaling Eric J. Michaud, Ziming Liu, Uzay Girit, Max Tegmark, O. (2023). arXiv preprint arXiv:2303.13506. 18. Grokking of Hierarchical Structure in Vanilla Transformers Murty, S., Sharma, P., Andreas, J., & Manning, C. D. (2023). arXiv preprint arXiv:2305.18741. 19. Predicting Grokking Long Before it Happens: A Look Into the Loss Landscape of Models Which Grok Notsawo Jr, P., Zhou, H., Pezeshki, M., Rish, I., & Dumas, G. (2023). arXiv preprint arXiv:2306.13253. 20. Language models can explain neurons in language models Bills, S., Cammarata, N., Mossing, D., Tillman, H., Gao, L., Goh, G., Sutskever, I., Leike, J., Wu, J., & Saunders, W. 2023. OpenAI Blog 21. Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla Tom Lieberum, Matthew Rahtz, János Kramár, Neel Nanda, Geoffrey Irving, Rohin Shah, Vladimir Mikulik (2023). arXiv preprint arXiv:2307.09458. 22. Toy Models of Superposition Elhage, N., Hume, T., Olsson, C., Schiefer, N., Henighan, T., Kravec, S., Hatfield-Dodds, Z., Lasenby, R., Drain, D., Chen, C., Grosse, R., McCandlish, S., Kaplan, J., Amodei, D., Wattenberg, M. and Olah, C., 2022. Transformer Circuits Thread. 23. The Connectome of an Insect Brain Winding, M., Pedigo, B. D., Barnes, C. L., Patsolic, H. G., Park, Y., Kazimiers, T., … & Zlatic, M. (2023). Science, 379(6636), eadd9330. 24. Multi-Scale Feature Learning Dynamics: Insights for Double Descent Pezeshki, M., Mitra, A., Bengio, Y., & Lajoie, G. (2022, June). In the International Conference on Machine Learning (pp. 17669-17690). PMLR. 25. Superposition, Memorization, and Double Descent Henighan, T., Carter, S., Hume, T., Elhage, N., Lasenby, R., Fort, S., Schiefer, N., and Olah, C., 2023. Transformer Circuits Thread. More Explorables Are Model Predictions Probabilities? Machine learning models express their uncertainty as model scores, but through calibration we can transform these scores into probabilities for more effective decision making. What Have Language Models Learned? By asking language models to fill in the blank, we can probe their understanding of the world. How Federated Learning Protects Privacy Most machine learning models are trained by collecting vast amounts of data on a central server. Federated learning makes it possible to train models without any user's raw data leaving their device. Searching for Unintended Biases With Saliency Machine learning models sometimes learn from spurious correlations in training data. Trying to understand how models make predictions gives us a shot at spotting flawed models. Can a Model Be Differentially Private and Fair? Training models with differential privacy stops models from inadvertently leaking sensitive data, but there's an unexpected side-effect: reduced accuracy on underrepresented subgroups. From Confidently Incorrect Models to Humble Ensembles ML models sometimes make confidently incorrect predictions when they encounter out of distribution data. Ensembles of models can make better predictions by averaging away mistakes. Datasets Have Worldviews Every dataset communicates a different perspective. When you shift your perspective, your conclusions can shift, too. Measuring Diversity Search results that reflect historic inequities can amplify stereotypes and perpetuate under-representation. Carefully measuring diversity in data sets can help. Measuring Fairness There are multiple ways to measure accuracy. No matter how we build our model, accuracy across these measures will vary when applied to different groups of people. Why Some Models Leak Data Machine learning models use large amounts of data, some of which can be sensitive. If they're not trained correctly, sometimes that data is inadvertently revealed. Hidden Bias Models trained on real-world data can encode real-world bias. Hiding information about protected classes doesn't always fix things — sometimes it can even hurt. Collecting Sensitive Information The availability of giant datasets and faster computers is making it harder to collect and study private information without inadvertently violating people's privacy.",
    "commentLink": "https://news.ycombinator.com/item?id=37076210",
    "commentBody": "Do Machine Learning Models Memorize or Generalize?Hacker NewspastloginDo Machine Learning Models Memorize or Generalize? (withgoogle.com) 410 points by 1wheel 20 hours ago| hidepastfavorite181 comments mostertoaster 17 hours agoSometimes I think the reason human memory in some sense is so amazing, is what we lack in storage capacity that machines have, we makeup for in our ability to create patterns that compress the amount of information stored dramatically, and then it is like we compress those patterns together with other patterns and are able to extract things from it. Like it is an incredibly lossy compression, but it gets the job done. reply ComputerGuru 17 hours agoparentThat’s not exactly true, there doesn’t seem to be an upper bound (that we can reach) on storage capacity in the brain [0]. Instead, the brain actually works to actively distill knowledge that doesn’t need to be memorized verbatim into its essential components in order to achieve exactly this “generalized intuition and understanding” to avoid overfitting.[0]: https:&#x2F;&#x2F;www.scientificamerican.com&#x2F;article&#x2F;new-estimate-boos... reply halflings 15 hours agorootparent> That’s not exactly true [...] Instead, the brain actually works to actively distill knowledge that doesn’t need to be memorized verbatim into its essential components...but that&#x27;s exactly what OP said, no?I remember attending an ML presentation where the speaker shared a quote I can&#x27;t find anymore (speaking of memory and generalization :)), which said something like: \"To learn is to forget\"If we memorized everything perfectly, we would not learn anything: instead of remembering the concept of a \"chair\", you would remember thousands of separate instances of things you&#x27;ve seen that have a certain combination of colors and shapes etcIt&#x27;s the fact that we forget certain details (small differences between all these chairs) that makes us learn what a \"chair\" is.Likewise, if you remembered every single word in a book, you would not understand its meaning; understanding its meaning = being able to \"summarize\" (compress) this long list of words into something more essential: storyline, characters, feelings, etc. reply cmpalmer52 11 hours agorootparentThere’s a story by Jorge Luis Borges called “Funes the Memorious” about a man who remembers everything, but can’t generalize. There’s a line about him not knowing if a dog on the square glimpsed at noon from the side is the same dog as the one seen from the back at 12:01 or something like that. Swirls of smoke from a cigarette are memorized forever. He mostly sits in a dark room. reply DevKoala 9 hours agorootparentThank you for reminding me of this story. He is my favorite author. reply JieJie 14 hours agorootparentprevMy mind is a blurry jpeg of my life.(https:&#x2F;&#x2F;www.newyorker.com&#x2F;tech&#x2F;annals-of-technology&#x2F;chatgpt-...) reply ComputerGuru 12 hours agorootparentprev> but that&#x27;s exactly what OP said, no?Not precisely. We don’t know if verbatim capacity is limited (and it doesn’t seem to be) but the brain operates in a space-efficient manner all the same. So there isn’t necessarily a causative relationship between “memory capacity” and “means of storage”.> Likewise, if you remembered every single word in a book, you would not understand its meaningI understand your meaning but I want to clarify for the sake of the discussion that unlike with ML, the human brain can both memorize verbatim and understand the meaning because there is no mechanism for memorizing something but not processing it (i.e. purely storage). The first pass(es) are stripped to their essentials but subsequent passes provide the ability to memorize the same input. reply SanderNL 11 hours agorootparentWe know for certain it is limited. Do brains not adhere to physics? reply whimsicalism 11 hours agorootparentprev> verbatim capacity is limitedI am but a simple physicist and I can already tell you it is. reply ComputerGuru 8 hours agorootparentI mean in terms of our ability to reach those limits, naturally. reply WanderPanda 13 hours agorootparentprevCompression = Intelligencehttp:&#x2F;&#x2F;prize.hutter1.net&#x2F; reply oneTbrain23 4 hours agorootparentprevYou obviously hand wave alzheimer and dementia. Human don&#x27;t know exactly how brains works. The computational storage is just an estimate of what we understand von Neuman computer storing data 1 and 0. In every psychological test conducted on human mind, they clearly have a limit. reply jjk166 16 hours agorootparentprevDistilling knowledge is data compression. reply w10-1 14 hours agorootparentYou&#x27;re conflating memorization with generalization, no? reply jjk166 12 hours agorootparentMemorization is storing data. Generalization is developing the heuristics by which you compress stored data. To distill knowledge is to apply heuristics to lossily-compress a large amount of data to a much smaller amount of data from which you nevertheless can recover enough information to be useful in the future. reply staunton 10 hours agorootparent> Generalization is developing the heuristics by which you compress stored datasuch that> you nevertheless can recover enough information to be useful in the future.I disagree (in case you meant to imply it) that compression implies generalization. reply gattilorenz 13 hours agorootparentprevIs there a “realistic upper bound” in things that should be memorized verbatim? Ancient greeks probably memorized the Iliad and other poems (rhyming and metre might work as a substitute for data compression, in this case), and many medieval preachers apparently memorized the whole Bible… reply firecall 9 hours agorootparentprevDoes the brain require more energy to store more information?Or is it always running at the same pace regardless of if it’s empty or not?I guess the Brian doesn’t really work like that…. But I’m curious :-) reply ReactiveJelly 8 hours agorootparentThe brain doesn&#x27;t seem to ever \"clock down\" when it&#x27;s idle, which is interesting to mehttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Human_brain#Metabolism> The energy consumption of the brain does not vary greatly over time reply gmadsen 7 hours agorootparentmaybe between sleep and normal waking idle because there is actually quite a bit going on during sleep. There has been quite a bit of research though regarding higher \"clock up\" states consuming far more energy, such as grandmasters playing a chess tournament reply TheRealSteel 12 hours agorootparentprevYou seem to have just re-stated what the other person said. reply whimsicalism 11 hours agorootparentThank you, thought I was losing it for a second reply downboots 16 hours agorootparentprevCan \"distill knowledge\" be made precise ? reply ComputerGuru 16 hours agorootparentAs best as I’ve been able to research, it’s still under active exploration and there are hypotheses but no real answers. I believe research has basically been circling around the recent understanding that in addition to being part of how the brain is wired, it is also an active, deliberate (if unconscious) mechanism that takes place in the background and is run “at a higher priority” during sleep (sort of like an indexing daemon running at low priority during waking hours then getting the bulk of system resources devoted to it during idle).There are also studies that show “data” in the brain isn’t stored read-only and the process of accessing that memory involves remapping the neurons (which is how fake memories are possible) - so my take is if you access a memory or datum sequentially start to finish each time the brain knows this is to be stored verbatim for as-is retrieval but if you access snapshots of it or actively seek to and replay a certain part while trying to relate that memory to a process or a new task, the brain rewires the neural pathways accusingly. Which implies that there us an unconscious part that takes place globally plus an active, modifying process where how we use a stored memory affects how it is stored and indexed (so data isn’t accessed by simple fields but rather by complex properties or getters, in programming parlance).I guess the key difference from how machine learning works (and I believe an integral part of AGI, if it is even possible) is that inference is constant, even when you’re only “looking up” data and you don’t know the right answer (i.e. not training stage). The brain recognizes how the new query differs from queries it has been trained on and can modify its own records to take into account the new data. For example, let’s say you’re trying to classify animals into groups and you’ve “been trained” on a dataset that doesn’t include monotremes or marsupials. The first time you come across a platypus in the wild (with its mammaries but no nipples, warm-blooded but lays eggs, and a single duct for waste and reproduction) you wouldn’t just mistakenly classify it as a bird or mammal - you would actively trigger a (delayed&#x2F;background) reclassification of all your existing inferences to account for this new phenomenon, even though you don’t know what the answer to the platypus classification question is. reply esafak 15 hours agorootparentprevhttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Rate%E2%80%93distortion_theory reply clord 16 hours agorootparentprevimo, it amounts to revisiting concepts once more general principles are found — and needed. For instance, you learn the alphabet, and it&#x27;s hard. the order is tricky. the sounds are tricky, etc. but eventually, it get distilled to a pattern. But you still have to start from A to remember what letter 6 is, until you encounter that problem many times, and then the brain creates a 6=F mapping. I think of it in economic terms: when the brain realizes it&#x27;s cheaper to create a generalization, it does so on the fly, and that generalization takes over the task.Somtimes it&#x27;s almost like creating a specialist shard to take over the task. Driving is hard at first, with very high task overload, lots to pay attention to. With practice, it becomes a little automated part of yourself takes care of those tasks while your main general intelligence can do whatever it likes, even as the \"driver\" deals with seriously difficult tasks. reply pyinstallwoes 11 hours agorootparentIt’s almost like a tuning fork. When the mapping becomes 6=F the two separate tuning forms are now sympathetic to a frequency. reply __loam 14 hours agorootparentprevUnless you know something the neuroscientists don&#x27;t, it cannot. reply nonameiguess 16 hours agorootparentprevI&#x27;ve thought about this a lot in the context of the desire people seem to have to try and achieve human immortality or at least indefinite lifespans. If SciAm is correct here and the upper bound is a quadrillion bytes, we may not be able to hit that given the bound on possible human experiences, but someone who lived long enough would eventually hit that. After a hundred million years or whatever the real number is of life, you&#x27;d either lose the ability to form new memories or you&#x27;d have to overwrite old ones to do so.Aside from having to eventually experience the death of all stars and light and the decay of most of the universe&#x27;s baryonic matter and then face an eternity of darkness with nothing to touch, it&#x27;s yet another reason I don&#x27;t think immortality (as opposed to just a very long lifespan) is actually desirable. reply mewpmewp2 13 hours agorootparentI imagine there would be perhaps tech or technique which you can choose to determine which memories to compress and countless of others techniques like extra storage that you can instantly access, so I don&#x27;t see all of these as being real arguments why not become immortal. If I have to choose to be dead and memoryless compared to losing some of my memories, but being still alive, why should I choose being dead and memoryless?And when losing memories you would first just discard some details, like you lose now anyway, but you would start compressing centuries into rough ideas of what happened, it&#x27;s just the details that would lack a bit.I don&#x27;t see it being a problem at all. And if really something happens with the Universe, sure I can die then, but why would I want to die before?I want to know what happens, what gets discovered, what happens with humanity, how far do we reach in terms of understanding of what is going on in this place. Why are we here. Imagine dying and not even knowing why you were here. reply p1necone 8 hours agorootparentprevMy naive assumption would be that it would be a fairly gradual process. You&#x27;d just always have a sliding window of the last N years of memories, with the older ones being progressively more fuzzy and unreliable. reply imtringued 14 hours agorootparentprevLongtermists argue that we will be harvesting hawking radiation from blackholes trillions of years after the heat death of the universe. reply __loam 14 hours agorootparentThe last civilizations will be built around black holes. reply bufferoverflow 15 hours agoparentprevThere are rare people who remember everythinghttps:&#x2F;&#x2F;youtu.be&#x2F;hpTCZ-hO6iI reply svachalek 14 hours agorootparentIt&#x27;s pretty fascinating to me how \"normal\" Marilu Henner seems to be. I&#x27;m getting older and my memory is not what it was, but when I was younger it was pretty extraordinary. I did really well in school and college but over time I&#x27;ve realized it was mostly due to being able to remember most things pretty effortlessly, over being truly \"smart\" in a classic sense.But having so much of the past being so accessible is tough. There are lots of memories I&#x27;d rather not have, that are vivid and easily called up. And still, I think it&#x27;s only a fraction of what her memory seems to be like. reply 93po 13 hours agorootparentAs someone on the other end of the spectrum, I have an awful memory, and don&#x27;t remember most of my life aside from really wide, sweeping generalizations and maybe a couple hundred very specific memories. My way of existence is also very sad, and it makes me feel like I&#x27;ve not really lived. reply obscurette 5 hours agorootparentIt&#x27;s likely that you actually have memories about details, but don&#x27;t have a way to recall these memories. I always wondered how the heck people write memories until I saw someone to do it. He used a lot of triggers – photos, newspapers, letters etc. Later I had a chance to visit museum where typical home environment of my childhood was exhibited (yes, I&#x27;m that old) and realized how many memories small things can trigger in my brain. reply TheRealSteel 12 hours agorootparentprev\" I did really well in school and college but over time I&#x27;ve realized it was mostly due to being able to remember most things pretty effortlessly\"Same! They thought I was a genius in primary school but I ended up a loser adult with a dead end job. Turns out I just liked technology and was good at remembering facts and names for things. reply hgsgm 13 hours agorootparentprevIs there scientific evidence of that or just claims? reply badumtsss 13 hours agorootparentsome people don&#x27;t want to be studied or tested. reply mr_toad 10 hours agoparentprevArtificial neural networks work a lot like compression algorithms in their ability to predict the future. The trained network is a compression algorithm - it does not store compressed data.We don’t know if the animal brain works the same way, but I suspect it is mostly compression algorithms designed to predict things, and doesn’t store much data at all. reply tbalsam 13 hours agoparentprevFor more information and the related math behind associative memories, please see Hopfield Neural Networks.While the upper bound is technically \"infinity\", there is a tradeoff between the amount of concepts stored and the fundamental amount of information storable per concept, similar to how other tradeoff principles like the uncertainty principle, etc work. reply scrps 11 hours agorootparentThank you reply bobboies 15 hours agoparentprevGood example in my math and physics classes I found it really helpful to understand the general concepts, then instead of memorizing formulas could actually derive them from other known (perhaps easier-to-remember) facts.Geometry is good for training in this way—and often very helpful for physics proofs too! reply lacrimacida 5 hours agorootparentToo bad this method is penalized most on tests (timed) where memorization is favored. But deriving results reinforce knowledge, understanding and patterns best in my opinion. reply pillefitz 17 hours agoparentprevThat is essentially what embeddings do reply nightski 16 hours agorootparentMaybe, except from my understanding an embedding vector tends to be much larger than the source token (due to the high dimensionality of the embedding space). So it&#x27;s almost like a reverse compression in a way. That said I know vector DBs have much more efficient ways of storing those vector embedding. reply jncfhnb 16 hours agorootparentTokens are not 1:1 with vectors. reply pyinstallwoes 11 hours agoparentprevMaxwell’s demon to entropy reply BSEdlMMldESB 14 hours agoparentprevyes, when we do this to history, it becomes filled with conspiracies. but is merely a process to &#x27;understand&#x27; history by projecting intentionalities.this &#x27;compression&#x27; is what &#x27;understanding&#x27; something really entails; at first... but then there&#x27;s more.when knowledge becomes understood it enables perception (e.g. we perceive meaning in words once we learn to read).when we get really good at this understanding-perception we may start to &#x27;manipulate&#x27; the abstractions we &#x27;perceive&#x27;. an example would be to &#x27;understand a cube&#x27; and then being able to rotate it around so to predict what would happen without really needing the cube. but this is an overly simplistic example reply NovaDudely 9 hours agorootparentThis was the thinking I was taking. It is a useful tool at first but taken too far can be a bad thing in some situations. reply greenflag 19 hours agoprevIt seems the take home is weight decay induces sparsity which helps learn the \"true\" representation rather than an overfit one. It&#x27;s interesting the human brain has a comparable mechanism prevalent in development [1]. I would love to know from someone in the field if this was the inspiration for weight decay (or presumably just the more equivalent nn pruning [2]).[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Synaptic_pruning [2] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Pruning_(artificial_neural_net... reply tbalsam 14 hours agoparentML researcher here wanting to offer a clarification.L1 induces sparsity. Weight decay explicitly _does not_, as it is L2. This is a common misconception.Something a lot of people don&#x27;t know is that weight decay works because when applied as regularization it causes the network to approach the MDL, which reduces regret during training.Pruning in the brain is somewhat related, but because the brain uses sparsity to (fundamentally, IIRC) induce representations instead of compression, it&#x27;s basically a different motif entirely.If you need a hint here on this one, think about the implicit biases of different representations and the downstream impacts that they can have on the learned (or learnable) representations of whatever system is in question.I hope this answers your question. reply naasking 5 hours agorootparent> because the brain uses sparsity to (fundamentally, IIRC) induce representations instead of compressionWhat&#x27;s the evidence for this? reply heyitsguay 28 minutes agorootparenthttps:&#x2F;&#x2F;bernstein-network.de&#x2F;wp-content&#x2F;uploads&#x2F;2021&#x2F;03&#x2F;Lect... this has an awesome overview of the current understanding of neural encoding mechanisms. reply mmmmpancakes 9 hours agorootparentprevcan you please spell out what MDL is an acronym for? reply sva_ 9 hours agorootparenthttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Minimum_description_length reply mmmmpancakes 9 hours agorootparentthanks reply joaogui1 13 hours agorootparentprevThat looks interesting, do you know what paper talks about the connection between MDL, regret, and weight decay? reply tbalsam 13 hours agorootparentI would start with Shannon&#x27;s information theory and the Wikipedia page on L2&#x2F;the MDL as a decent starting point.For the first, there are a few good papers that simplify the concepts even further. reply visarga 19 hours agoparentprevThe inspiration for weight decay was to reduce the capacity to memorize of the model until it perfectly fits the complexity of the task, not more not less. A model more complex than the task is over-fitting, the other one is under-fitting. Got to balance them out.But the best cure for over-fitting is to make the dataset larger and ensure data diversity. LLMs have datasets so large they usually train one epoch. reply nightski 19 hours agorootparentIt sounds nice in theory, but the data itself could be problematic. There is no temporal nature to it. You can have duplicate data points, many data points that are closely related but describe the same thing&#x2F;event&#x2F;etc.. So while only showing the model each data point once ensures you do not introduce any extra weight on a data point, if the dataset itself is skewed it doesn&#x27;t help you at all.Just by trying to make the dataset diverse you could skew things to not reflect reality. I just don&#x27;t think enough attention has been paid to the data, and too much the model. But I could be very wrong.There is a natural temporality to the data humans receive. You can&#x27;t relive the same moment twice. That said, human intelligence is on a scale too and may be affected in the same way. reply visarga 18 hours agorootparent> I just don&#x27;t think enough attention has been paid to the data, and too much the model.I wholly agree. Everyone is blinded by models - GPT4 this, LLaMA2 that - but the real source of the smarts is in the dataset. Why would any model, no matter how its architecture is tweaked, learn about the same ability from the same data? Why would humans be all able to learn the same skills when every brain is quite different. It was the data, not the modelAnd since we are exhausting all the available quality text online we need to start engineering new data with LLMs and validation systems. AIs need to introspect more into their training sets, not just train to reproduce them, but analyse, summarise and comment on them. We reflect on our information, AIs should do more reflection before learning.More fundamentally, how are AIs going to evolve past human level unless they make their own data or they collect data from external systems? reply ben_w 18 hours agorootparent> It was the data, not the modelIt&#x27;s both.It&#x27;s clearly impossible to learn how to translate Linear A into modern English using only content written in pure Japanese that never references either.Yet also, none of the algorithms before Transformers were able to first ingest the web, then answer a random natural language question in any domain — closest was Google etc. matching on indexed keywords.> how are AIs going to evolve past human level unless they make their own data?Who says they can&#x27;t make their own data?Both a priori (by development of \"new\" mathematical and logical tautological deductions), and a posteriori by devising, and observing the results of, various experiments.Same as us, really. reply riversflow 15 hours agorootparentI see this brought up consistently on the topic of AI take-off&#x2F;X-risk.How does an AI language model devise an experiment and observe the results? The language model is only trained on what’s already known, I’m extremely incredulous that this language model technique can actually reason a genuinely novel hypothesis.A LLM is a series of weights sitting in the ram of GPU cluster, it’s really just a fancy prediction function. It doesn’t have the sort of biological imperatives (a result of being complete independent beings) or entropy that drive living systems.Moreover, if we consider how it works for humans, people have to _think_ about problems. Do we even have a model or even an idea about what “thinking” is? Meanwhile science is a looping process that mostly requires a physical element(testing&#x2F;verification) to it. So unless we make some radical breakthroughs in general purpose robotics, as well as overcome the thinking problem I don’t see how AI can do some sort tech breakout&#x2F;runaway. reply kaba0 2 hours agorootparent> Do we even have a model or even an idea about what “thinking” isAt the least, it is a computable function (as we don’t have any physical system that would be more general than that, though some religions might disagree). Which already puts human brains ahead of LLM systems, as we are Turing-complete, while LLMs are not, at least in their naive application (their output can be feeded to subsequent invocations and that way it can be). reply ben_w 12 hours agorootparentprevStarting with the end so we&#x27;re on the same page about framing the situation:> I don’t see how AI can do some sort tech breakout&#x2F;runaway.I&#x27;m expecting (in the mode, but with a wide and shallow distribution) a roughly 10x increase in GDP growth, from increased automation etc., not a singularity&#x2F;foom.I think the main danger is bugs and misuse (both malicious and short-sighted).-> How does an AI language model devise an experiment and observe the results?Same way as Helen Keller.Same way scientists with normal senses do for data outside human sense organs, be that the LHC or nm&#x2F;s^2 acceleration of binary stars or gravity waves (or the confusingly similarly named but very different gravitational waves).> The language model is only trained on what’s already known, I’m extremely incredulous that this language model technique can actually reason a genuinely novel hypothesis.Were you, or any other human, trained on things unknown?If so, how?> A LLM is a series of weights sitting in the ram of GPU cluster, it’s really just a fancy prediction function. It doesn’t have the sort of biological imperatives (a result of being complete independent beings) or entropy that drive living systems.Why do you believe that biological imperatives are in any way important?I can&#x27;t see how any of a desire to eat, shag, fight, run away, or freeze up… help with either the scientific method nor pure maths.Even the \"special sauce\" that humans have over other animals didn&#x27;t lead to any us doing the scientific method until very recently, and most of us still don&#x27;t.> Do we even have a model or even an idea about what “thinking” is?AFAIK, only in terms of output, not qualia or anything like that.Does it matter if the thing a submarine does is swimming, if it gets to the destination? LLMs, for all their mistakes and their… utterly inhuman minds and transhuman training experience… can do many things which would&#x27;ve been considered \"implausible\" even in a sci-fi setting a decade ago.> So unless we make some radical breakthroughs in general purpose roboticsI don&#x27;t think it needs to be general, as labs are increasingly automated even without general robotics. reply imtringued 14 hours agorootparentprevIt&#x27;s not just a series of weights. It is an unchanging series of weights. This isn&#x27;t necessarily artificial intelligence. It is the intelligence of the dead. reply whimsicalism 11 hours agorootparentprev> Yet also, none of the algorithms before Transformers were able to first ingest the web, then answer a random natural language question in any domain — closest was Google etc. matching on indexed keywords.Wrong, recurrent models were able to do this, just not as well. reply Salgat 18 hours agorootparentprevThis is definitely current models&#x27; biggest issue. You&#x27;re training a model against millions of books worth of data (which would take a human tens of thousands of lifetimes) to achieve a superficial level of conversational ability to match a human, which can consume at most 3 novels a day without compromising comprehension. Current models are terribly inefficient when it comes to learning from data. reply og_kalu 17 hours agorootparentModern LLMs are nowhere near the scale of the human brain however you want to slice things so terribly inefficient is very arguable. also language skills seemingly take much less data and scale when you aren&#x27;t trying to have it learn the sum total of human knowledge. https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.07759 reply Salgat 13 hours agorootparentScale is a very subjective thing since one is analog (86B neurons) and one is digital (175B parameters). Additionally, consider how many compute hours GPT 3 took to train (10,000 V100s were set aside for exclusive training of GPT 3). I&#x27;d say that GPT 3 scale vastly dwarfs the human brain, which runs at a paltry 12 watts. reply kaba0 2 hours agorootparentNeumann’s Computer and The Brain book is way out of date in terms of today’s hardware, but funnily it is still relevant in this metric. Biological systems are more analogous to a distributed system of small, very slow CPUs. Even GPUs that somewhat close the gap in-between the few, crazy fast CPUs vs the aforementioned many, slow ones - are still much faster than any one neuron in calculations, but are still overly serial. It is not the number of CPUs, but the number of their connections that make biological systems so powerful. reply whimsicalism 11 hours agorootparentprevYou have to count the training process from the origin of the human brain imo, not from the birth of any individual human.Neural nets look much more competitive by that standard. reply Salgat 6 hours agorootparentYet humans designed the models, so the training process for chat gpt etc includes human evolution by your logic. reply whimsicalism 5 hours agorootparentThis is a good point and the level of so-called task specific \"inductive bias\" in models is an active point of discussion, but I don&#x27;t think it is fair to add all of our evolution to the model inductive bias because most of evolution was not towards giving better understanding of language to the model, it was towards better understanding of language in humans. reply imtringued 14 hours agorootparentprevThey are inefficient by design. Gradient descent and backpropagation scale poorly, but they work and GPUs are cheap, so here we are. reply crdrost 18 hours agorootparentprevAnd there have been a lot of approaches to do this, my favorite one being the idea that maybe if we just randomly zap out some of the neurons while we train the rest, that forcing it to acquire that redundancy might privilege structured representations over memorization. Just always seemed like some fraternity prank, “if you REALLY know the tenets of Delta Mu Beta you can recite them when drunk after we spin you around in a circle twelve times fast!” reply two_in_one 6 hours agorootparent> just randomly zap out some of the neurons while we train the restIt&#x27;s already done: https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;generated&#x2F;torch.nn.functiona... reply whimsicalism 11 hours agorootparentprevhttps:&#x2F;&#x2F;nitter.net&#x2F;Yampeleg&#x2F;status&#x2F;1688441683946377216 reply kaibee 16 hours agorootparentprev> But the best cure for over-fitting is to make the dataset larger and ensure data diversity.This is also good life advice. reply BaseballPhysics 19 hours agoparentprevThe human brain has synaptic pruning. The exact purpose of it is theorized but not actually understood, and it&#x27;s a gigantic leap to assume some sort of analogous mechanism between LLMs and the human brain. reply pcwelder 18 hours agoparentprevAfaik weight decay is inspired from L2 regularisation which goes back to linear regression where L2 regularisation is equivalent to having gaussian prior on the weights with zero mean.Note that L1 regularisation produces much more sparsity but it doesn&#x27;t perform as well. reply nonameiguess 15 hours agorootparentThis. Weight decay is just a method of dropping most weights to zero which is a standard technique used by statisticians for regularization purposes for decades. As far as I understand, it goes back at least to Tikhorov from 1970 and was mostly called ridge regression in the regression context. Normal ordinary least squares attempts to minimize the L2 norm of the squared residuals. When a system is overdetermined, adding a penalty term (usually just a scalar multiple of an identity matrix) and also minimizing the L2 norm of that biases the model to produce mostly near-zero weights. This helps with underdetermined systems and gives a better conditioned model matrix that is actually possible to solve numerically without underflow.It&#x27;s kind of amazing to watch this from the sidelines, a process of engineers getting ridiculously impressive results from some combo of sheer hackery and ingenuity, great data pipelining and engineering, extremely large datasets, extremely fast hardware, and computational methods that scale very well, but at the same time, gradually relearning lessons and re-inventing techniques that were perfected by statisticians over half a century ago. reply tbalsam 14 hours agorootparentL1 drops weights to zero, L2 biases towards Gaussianality.It&#x27;s not always relearning lessons or people entirely blindly trying things either, many researchers use the underlying math to inform decisions for network optimization. If you&#x27;re seeing that, then that&#x27;s probably a side of the field where people are newer to some of the math behind it, and that will change as things get more established.The underlying mathematics behind these kinds of systems are what has motivated a lot of the improvements in hlb-CIFAR10, for example. I don&#x27;t think I would have been able to get there without sitting down with the fundamentals, planning, thinking, and working a lot, and then executing. There is a good place for blind empirical research too, but it loses its utility past a certain point of overuse. reply whimsicalism 11 hours agorootparentprevthis comment is so off base, first off no l2 des not encourage near 0 weights, second off they are not relearning, everyone already knew what l1&#x2F;l2 penalties are reply gorjusborg 18 hours agoprevGrr, the AI folks are ruining the term &#x27;grok&#x27;.It means roughly &#x27;to understand completely, fully&#x27;.To use the same term to describe generalization... just shows you didn&#x27;t grok grokking. reply erwald 18 hours agoparent\"Grok\" in AI doesn&#x27;t quite describe generalization, it&#x27;s more specific that that. It&#x27;s more like \"delayed and fairly sudden generalization\" or something like that. There was some discussion of this in the comments of this post[1], which proposes calling the phenomenon \"eventual recovery from overfitting\" instead.[1] https:&#x2F;&#x2F;www.lesswrong.com&#x2F;posts&#x2F;GpSzShaaf8po4rcmA&#x2F;qapr-5-gro... reply tbalsam 14 hours agorootparentPart of the issue here is posting a LessWrong post. There is some good in there, but much of that site is like a Flat Earth conspiracy theory for neural networks.Neural network training [edit: on a fixed point task, as is often the case {such as image->label}] is always (always) biphasic necessarily, so there is no \"eventual recovery from overfitting\". In my experience, it is just people newer to the field or just noodling around fundamentally misunderstanding what is happening, as their network goes through a very delayed phase change. Unfortunately there is a significant amplification to these kinds of posts and such, as people like chasing the new shiny of some fad-or-another-that-does-not-actually-exist instead of the much more &#x27;boring&#x27; (which I find fascinating) math underneath it all.To me, as someone who specializes in optimizing network training speeds, it just indicates poor engineering to the problem on the part of the person running the experiments. It is not a new or strange phenomenon, it is a literal consequence of the information theory underlying neural network training. reply PoignardAzur 10 hours agorootparent> Part of the issue here is posting a LessWrong postI mean, this whole line of analysis comes from the LessWrong community. You may disagree with them on whether AI is an existential threat, but the fact that people take that threat seriously is what gave us this whole \"memorize-or-generalize\" analysis, and glitch tokens before that, and RLHF before that. reply tbalsam 9 hours agorootparentI think you may be missing the extensive lines of research covering those topics. Memorization vs Generalization has been a debate before LW even existed in the public eye, and inputs that networks have unusual sensitivity to have been well studied as well (re:chaotic vs linear regimes in neural networks). Especially the memorization vs generalization bit -- that has been around for...decades. It&#x27;s considered a fundamental part of the field, and has had a ton of research dedicated to it.I don&#x27;t know much either way about RLHF in terms of its direct lineage, but I highly doubt that is actually what happened, since DeepMind is actually responsible for the bulk of the historical research supporting those methods.It&#x27;s possible ala the broken clock hypothesis + LessWrong is obviously not the \"primate at a typewriter\" situation, so there&#x27;s a chance of some people scoring meaningful contributions, but the signal to noise ratio is awful. I want to get something out of some of the posts I&#x27;ve tried to read there, but there are so many bad takes written with more bombastic language that it&#x27;s really quite hard indeed.Right now, it&#x27;s an active detractor to the field because it pulls attention away from things that are much more deserving of energy and time. I honestly wish the vibe was back to people even just making variations of Char-RNN repos based on Karpathy&#x27;s blog posts. That was a much more innocent time. reply Noumenon72 5 hours agorootparentprevWhat are the two phases? What determines when you switch? reply tbalsam 5 hours agorootparentMemorization of individual examples -> generalization, I can&#x27;t speak about the determinant of switching as that is (partially, to some degree) work I&#x27;m working on, and I have a personal rule not to share work in progress until it&#x27;s completed (and then be very open and explicit about it). My apologies on that front.However, I can point you to one comment I made earlier in this particular comment section about the MDL and how that relates to the L2 norm. Obviously this is not the only thing that induces a phase change, but it is one of the more blatant ones that&#x27;s been covered little more publicly by different people. reply tbalsam 14 hours agorootparentprevTo further clarify things, the reason there is no mystical &#x27;eventual recovery from overfitting &#x27; is because overfitting is a stable bound that is approached. Adding this false denomination to this implies a non-biphasic nature to neural network training, and adds false information that wasn&#x27;t there before.Thankfully things are pretty stable in the over&#x2F;underfitting regime. I feel sad when I see ML misinformation propagated on a forum that requires little experience but has high leverage due to the rampant misuse of existing terms and complete invention of a in-group-language that has little touch with the mathematical foundations of what&#x27;s happening behind the scenes. I&#x27;ve done this for 7-8 years at this point at a pretty deep level and have a strong pocket of expertise, so I&#x27;m not swinging at this one blindly. reply ShamelessC 13 hours agorootparentprev> Part of the issue here is posting a LessWrong post. There is some good in there, but much of that site is like a Flat Earth conspiracy theory for neural networks.Indeed! It’s very frustrating that so many people here are such staunch defenders of LessWrong. Some&#x2F;much of the behavior there is honestly concerning. reply gorjusborg 17 hours agorootparentprevWhoever suggested &#x27;eventual recovery from overfitting&#x27; is a kindred spirit.Why throw away the context and nuance?That decision only further leans into the &#x27;AI is magic&#x27; attitude. reply jeremyjh 16 hours agorootparentNo, actually this is just how language evolves. I&#x27;m glad we have the word \"car\" instead of \"carriage powered by internal combustion engine\" even if it confused some people 100 years ago when the term became used exclusively to mean something a bit more specfic.Of course the jargon used in a specific sub-field evolves much more quickly than common usage because the intended audience of paper like this is expected to be well-read and current in the field already. reply smolder 16 hours agorootparentLanguage devolves just as it evolves. We (the grand we) regularly introduce ambiguity --words and meanings with no useful purpose, or that are worse than useless.I&#x27;m not really weighing in on the appropriateness of the use \"grok\" in this case. It&#x27;s just a pet peeve of mine that people bring out \"language evolves\" as an excuse for why any arbitrary change is natural and therefore acceptable and we should go with the flow. Some changes are strictly bad ones.A go-to example is when \"literally\" no longer means \"literally\", but its opposite, or nothing at all. We don&#x27;t have a replacement word, so now in some contexts people have to explain that they \"literally mean literally\". reply krapp 15 hours agorootparentLanguage only evolves, \"devolving\" isn&#x27;t a thing. All changes are arbitrary. Language is always messy, fluid and ambigious. You should go with the flow because being a prescriptivist about the way other people speak is obnoxious and pointless.And \"literally\" has been used to mean \"figuratively\" for as long as the word has existed[0].[0]https:&#x2F;&#x2F;blogs.illinois.edu&#x2F;view&#x2F;25&#x2F;96439 reply smolder 15 hours agorootparentI&#x27;m going to take a rosier view of prescriptivists and say they are a necessary part of the speaking&#x2F;writing public, doing the valuable work of fighting entropic forces to prevent making our language dumb. They don&#x27;t always need to win or be right.That&#x27;s the first time I&#x27;ve seen literally-as-figuratively defended from a historical perspective. I still think we&#x27;d all be better off if people didn&#x27;t mindlessly use it as a filler word or for emphasis, which is generally what people are doing these days that is the source of controversy, not reviving an archaic usage.Also, it&#x27;s kind of ironic you corrected my use of \"devolves\", where many would accept it. :) reply mdp2021 13 hours agorootparentprev> devolving isn&#x27;t a thingIncompetent use is devolution. reply gorjusborg 13 hours agorootparentAlso being overlooked is that the nuances in what we accept is in large part how we define group culture.If you want to use the word &#x27;irregardless&#x27; unironically there are people who will accept that. Then there are the rest of us. reply kaba0 2 hours agorootparentJust as an added data point, some languages (e.g. Hungarian) do use double negative “natively”, and I have definitely caught myself having to fight some native expression seeping into my English, including ‘irregardless’. For example a Hungarian would say “I have never done nothing bad” over “anything bad”, but it is used not in a logical sense, but more as an emphasis, perhaps?(!)Regardless, what I’m trying to say is that due to the unique position of English as the de facto world language, it has to “suffer” some non-idiomatic uses seeping in from non-natives. Actually, I would go even further and say that most smaller languages will slowly stop evolving and only English will have that property going forward (most new inventions no longer gets a native name in most languages, the English one is used). replygorjusborg 13 hours agorootparentprev> No, actually this is just how language evolvesStop making &#x27;fetch&#x27; happen, it&#x27;s not going to happen. reply mxwsn 17 hours agoparentprevThey&#x27;re just defining grokking in a different way. It&#x27;s reasonable to me though - grokking suggests elements of intuitive understanding, and a sudden, large increase in understanding. These mirror what happens to the loss. reply benreesman 16 hours agoparentprevSci-Fi Nerd Alert:“Grok” was Valentine Michael Smith’s rendering for human ears and vocal cords of a Martian word with a precise denotational semantic of “to drink”. The connotational semantics range from to literally or figuratively “drink deeply” all the way up through to consume the absented carcass of a cherished one.I highly recommend Stranger in A Strange Land (and make sure to get the unabridged re-issue, 1990 IIRC). reply whimsicalism 11 hours agoparentprevI literally do not see the difference between the two uses that you are trying to make reply dogcomplex 5 hours agoparentprevSame thing. To grok is to fully incorporate the new into your intuitive view of the world - changing your view of both in the process. An AI is training their model with the new data, incorporating it into their existing world view in such a way that may even subtly change every variable they know. A human is doing the same. We integrate it deeper the more we can connect it to existing metaphor and understanding - and it becomes one less thing we need to \"remember\" precisely because we can then recreate it from \"base principles\" because we fully understand it. We&#x27;ve grokked it. reply paulddraper 17 hours agoparentprevWhat the difference between understanding and generalizing?And what is the indicator for a machine understanding something? reply NikkiA 18 hours agoparentprevI&#x27;ve always taken &#x27;grok&#x27; to be in the same sense as &#x27;to be one with&#x27; reply gorjusborg 17 hours agorootparentYeah, there is definitely irony that I&#x27;m trying to push my own definition of an extra-terrestrial word, complaining that someone is ruining it.If anyone wants to come up with their own definition, read Robert Heinlein&#x27;s &#x27;Stranger in a Strange Land&#x27;. There is no definition in there, but you build an intuition of the meaning by its use.One of the issues I have w&#x2F; the use in AI is that using the word &#x27;grok&#x27; suggests that the machine understands (that&#x27;s a common interpretation of the word grok, that it is an understanding greater than normal understanding).By using an alien word, we are both suggesting something that probably isn&#x27;t technically true, while simultaneously giving ourselves a slimy out. If you are going to suggest that AI understands, just have the courage to say it with common english, and be ready for argument.Redefining a word that already exists to make the argument technical feels dishonest. reply snewman 16 hours agorootparentActually the definition of &#x27;grok&#x27; is discussed in the book; you can find some relevant snippets at https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Grok. My recollection is that the book says the original &#x2F; literal meaning is \"drink\", but this isn&#x27;t supported by the Wikipedia quotes and perhaps I am misremembering, it has been a long time. reply mr_toad 10 hours agoparentprevIn programming circles ‘grok’ has long been used to describe that moment when you finally understand the piece of code you’ve been staring at all day.So the AI folks are just borrowing something that had already been co-opted 30+ years ago. reply thuuuomas 14 hours agoparentprev“Grok” is more about in-group signaling like “LaTex credibility” or publishing blog posts on arxiv. reply 93po 13 hours agoparentprevI have heard grok used tremendously more frequently in the past year or two and I find it annoying because they&#x27;re using it as a replacement for the word \"understand\" for reasons I don&#x27;t \"grok\" reply jjk166 16 hours agoparentprevI&#x27;ve always considered the important part of grokking something to be the intuitiveness of the understanding, rather than the completeness. reply momirlan 17 hours agoparentprevgrok, implying a mystical union, is not applicable to AI reply Filligree 16 hours agorootparentWhy not? reply jimwhite42 13 hours agoprevI&#x27;m not sure if I&#x27;m remembering it right, but I think it was on a Raphaël Millière interview on Mindscape, where Raphaël said something along the lines of when there are many dimensions in a machine learning model, the distinction between interpolation and extrapolation is not clear like it is in our usual areas of reasoning. I can&#x27;t work out if this could be something similar to what the article is talking about. reply _ache_ 19 hours agoprevDoes anyone know how that charts are created ? I bet that it&#x27;s half generated by some sort of library and them manually improved but the generated animated SVG are beautiful. reply 1wheel 18 hours agoparentBasically just a bunch of d3 — could be cleaned up significantly, but that&#x27;s hard to do while iterating and polishing the charts.I also have a couple of little libraries for things like annotations, interleaving svg&#x2F;canvas and making d3 a bit less verbose.- https:&#x2F;&#x2F;github.com&#x2F;PAIR-code&#x2F;ai-explorables&#x2F;tree&#x2F;master&#x2F;sour...- https:&#x2F;&#x2F;1wheel.github.io&#x2F;swoopy-drag&#x2F;- https:&#x2F;&#x2F;github.com&#x2F;gka&#x2F;d3-jetpack- https:&#x2F;&#x2F;roadtolarissa.com&#x2F;hot-reload&#x2F; reply iaw 18 hours agorootparentI was going to ask the same question. Those are some great visualizations reply westurner 5 hours agoprevIf you omit the training data points where the baseball hits the ground, what will a machine learning model predict?You can train a classical ML model on the known orbits of the planets in the past, but it can presumably never predict orbits given unseen n-body gravity events like another dense mass moving through the solar system because of classical insufficiency to model quantum problems, for example.Church-Turing-Deutsch doesn&#x27;t say there could not exist a Classical &#x2F; Quantum correspondence; but a classical model on a classical computer cannot be sufficient for quantum-hard problems. (e.g. Quantum Discord says that there are entanglement and non-entanglement nonlocal relations in the data.)Regardless of whether they sufficiently generalize, [LLMs, ML Models, and AutoMLs] don&#x27;t yet Critically Think and it&#x27;s dangerous to take action without critical thought.Critical Thinking; Logic, Rationality: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Critical_thinking#Logic_and_ra... reply ComputerGuru 17 hours agoprevPSA: if you’re interested in the details of this topic, it’s probably best to view TFA on a computer as there is data in the visualizations that you can’t explore on mobile. reply flyer_go 15 hours agoprevI don&#x27;t think I have seen an answer here that actually challenges this question - from my experience, I have yet to see a neural network actually learn representations outside the range in which it was trained. Some papers have tried to use things like sinusoidal activation functions that can force a neural network to fit a repeating function, but on its own I would call it pure coincidence.On generalization - its still memorization. I think there has been some proof that chatgpt does &#x27;try&#x27; to perform some higher level thinking but still has problems due to the dictionary type lookup table it uses. The higher level thinking or agi that people are excited about is a form of generalization that is so impressive we don&#x27;t really think of it as memorization. But I actually question if our wantingness to generate original thought isn&#x27;t as actually separate from what we currently are seeing. reply smaddox 14 hours agoparent> I have yet to see a neural network actually learn representations outside the range in which it was trainedGeneralization doesn&#x27;t require learning representations outside of the training set. It requires learning reusable representations that compose in ways that enable solving unseen problems.> On generalization - its still memorizationNot sure what you mean by this. This statement sounds self contradictory to me. Generalization requires abstraction &#x2F; compression. Not sure if that&#x27;s what you mean by memorization.Overparameterized models are able to generalize (and tend to, when trained appropriately) because there are far more parameterizations that minimize loss by compressing knowledge than there are parameterizations that minimize loss without compression.This is fairly easy to see. Imagine a dataset and model such that the model has barely enough capacity to learn the dataset without compression. The only degrees of freedom would be through changes in basis. In contrast, if the model uses compression, that would increase the degrees of freedom. The more compression, the more degrees of freedom, and the more parameterizations that would minimize the loss.If stochastic gradient descent is sufficiently equally as likely to find any given compressed minimum as any given uncompressed minimum, then the fact that there are exponentially many more compressed minimums than uncompressed minimums means it will tend to find a compressed minimum.Of course this is only a probabilistic argument, and doesn&#x27;t guarantee compression &#x2F; generalization. And in fact we know that there are ways to train a model such that it will not generalize, such as training for many epochs on a small dataset without augmentation. reply jhaenchen 14 hours agoparentprevThe issue is that we are prone to inflate the complexity of our own processing logic. Ultimately we are pattern recognition machines in combination with abstract representation. This allows us to connect the dots between events in the world and apply principles in one domain to another.But, like all complexity, it is reduceable to component parts.(In fact, we know this because we evolved to have this ability. ) reply agalunar 14 hours agorootparentCalling us \"pattern recognition machines capable of abstract representation\" I think is correct, but is (rather) broad description of what we can do and not really a comment on how our minds work. Sure, from personal observation, it seems like we sometimes overcomplicate self-analysis (\"I&#x27;m feeling bad – why? oh, there are these other things that happened and related problems I have and maybe they&#x27;re all manifestations of one or two deeper problems, &c\" when in reality I&#x27;m just tired or hungry), but that seems like evidence we&#x27;re both simpler than we think and also more complex than you&#x27;d expect (so much mental machinery for such straightforward problems!).I read Language in Our Brain [1] recently and I was amazed by what we&#x27;ve learned about the neurologicial basis of language, but I was even more astounded at how profoundly little we know.> But, like all complexity, it is reduceable to component parts.This is just false, no? Sometimes horrendously complicated systems are made of simple parts that interact in ways that are intractable to predict or that defy reduction.[1] https:&#x2F;&#x2F;mitpress.mit.edu&#x2F;9780262036924&#x2F;language-in-our-brain reply superkuh 16 hours agoprevThere were no auto-discovery RSS&#x2F;Atom feeds in the HTML, no links to the RSS feed anywhere, but by guessing at possible feed names and locations I was able to find the \"Explorables\" RSS feed at: https:&#x2F;&#x2F;pair.withgoogle.com&#x2F;explorables&#x2F;rss.xml reply taeric 18 hours agoprevI&#x27;m curious how representative the target function is? I get that it is common for you to want a model to learn the important pieces of an input, but a string of bits, and only caring about the first three, feels particularly contrived. Literally a truth table on relevant parameters of size 8? And trained with 4.8 million samples? Or am I misunderstanding something there? (I fully expect I&#x27;m misunderstanding something.) reply jaggirs 18 hours agoparentI have observed this pattern before in computer vision tasks (train accuracy flatlining for a while before test acc starts to go up). The point of the simple tasks is to be able to interpret what could be going on behind the scenes when this happens. reply taeric 17 hours agorootparentNo doubt. But I have also seen what people thought were generalized models failing on outlier, but valid, data. Quite often.Put another way, it isn&#x27;t just how simple this task seems to be in the number of terms that are important, but isn&#x27;t it also a rather dense function?Probably better question to ask is how sensitive are models that are looking at less dense functions to this? (Or more dense.). I&#x27;m not trying to disavow the ideas. reply visarga 17 hours agorootparentMaybe humans are also failing a lot in out of distribution settings. It might be inherent. reply taeric 16 hours agorootparentWe have names for that. :D. Stereotypes being a large one. Racism being motivated interpretation on the same ideas. Right? reply lucubratory 11 hours agorootparentYes, although there are less political examples. PTSD, the difficulty of learning higher dimensional mathematics in a way you can genuinely understand, substance abuse, mass killings. replyesafak 17 hours agoprevI haven&#x27;t read the latest literature but my understanding is that \"grokking\" is the phase transition that occurs during the coalescing of islands of understanding (increasingly abstract features) that eventually form a pathway to generalization. And that this is something associated with over-parameterized models, which have the potential to learn multiple paths (explanations).https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Percolation_theoryA relevant, recent paper I found from a quick search: The semantic landscape paradigm for neural networks (https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.09550) reply mjburgess 19 hours agoprevStatistical learning can typically be phrased in terms of k nearest neighboursIn the case of NNs we have a \"modal knn\" (memorising) going to a \"mean knn\" (&#x27;generalising&#x27;) under the right sort of training.I&#x27;d call both of these memorising, but the latter is a kind of weighted recall.Generalisation as a property of statistical models (ie., models of conditional freqs) is not the same property as generalisation in the case of scientific models.In the latter a scientific model is general because it models causally necessary effects from causes -- so, necessarily if X then Y.Whereas generalisation in associative stats is just about whether you&#x27;re drawing data from the empirical freq. distribution or whether you&#x27;ve modelled first. In all automated stats the only diff between the \"model\" and \"the data\" is some sort of weighted averaging operation.So in automated stats (ie., ML,AI) it&#x27;s really just whether the model uses a mean. reply autokad 16 hours agoparentI disagree, it feels like you are just fusing over words and not what&#x27;s happening in the real world. If you were right, a human doesn&#x27;t learn anything either, they just memories.you can look at it by results: I give these models inputs its never seen before but it gives me outputs that are correct &#x2F; acceptable.you can look at it in terms of data: we took petabytes of data, and with an 8gb model (stable difusion) we can output an image of anything. That&#x27;s an unheard of compression, only possible if its generalizing - not memorizing. reply ActivePattern 16 hours agoparentprevI&#x27;d be curious how much of the link you read.What they demonstrate is a neural network learning an algorithm that approximates modular addition. The exact workings of this algorithm is explained in the footnotes. The learned algorithm is general -- it is just as valid on unseen inputs as seen inputs.There&#x27;s no memorization going on in this case. It&#x27;s actually approximating the process used to generate the data, which just isn&#x27;t possible using k nearest neighbors. reply visarga 17 hours agoparentprev> Statistical learning can typically be phrased in terms of k nearest neighboursWe have suspected that neural nets are a kind of kNN. Here&#x27;s a paper:Every Model Learned by Gradient Descent Is Approximately a Kernel Machinehttps:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2012.00152 reply bippihippi1 18 hours agoparentprevit&#x27;s been proven that all models learned by gradient descent are equivalent to kernel machines. interpolation isn&#x27;t generalization. if theres a new input sufficiently different from the training data the behaviour is unknown reply drdeca 7 hours agorootparentCan you say what that says about the behavior described with the modular arithmetic in the article?And, in particular, how to interpret the fact that different hyperparameters determined whether runs, obtaining equally high accuracy on the training data, got good or bad scores on the test data, in terms of the \"view it as a kernel machine&#x2F;interpolation\" lens?My understanding is that the behavior in at least one of those \"models learned by gradient descent are equivalent to [some other model]\" papers, works by constructing something which is based on the entire training history of the network. Is that the kernel machines one, or some other one? reply xapata 18 hours agorootparentprevOne weird trick ...There&#x27;s some fox and hedgehog analogy I&#x27;ve never understood. reply visarga 17 hours agorootparentprevbut when the model trains on 13T tokens it is hard to be OOD reply SimplyUnknown 16 hours agoprevFirst of all, great blog post with great examples. Reminds me of distill.pub used to be.Second, the article correctly states that typically L2 weight decay is used, leading to a lot of weights with small magnitudes. For models that generalize better, would it then be better to always use L1 weight decay to promote sparsity in combination with longer training?I wonder whether deep learning models that only use sparse fourier features rather than dense linear layers would work better... reply medium_spicy 16 hours agoparentShort answer: if the inputs can be represented well on the Fourier basis, yes. I have a patent in process on this, fingers crossed.Longer answer: deep learning models are usually trying to find the best nonlinear basis in which to represent inputs; if the inputs are well-represented (read that as: can be sparsely represented) in some basis known a-priori, it usually helps to just put them in that basis, e.g., by FFT’ing RF signals.The challenge is that the overall-optimal basis might not be the same as those of any local minima, so you’ve got to do some tricks to nudge the network closer. reply qumpis 16 hours agoparentprevSlightly related but sparsity-inducing activation function Relu is often used in neural networks reply lachlan_gray 16 hours agoprevIt looks like grid cells!https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Grid_cellIf you plot a head map of a neuron in the hidden layer on a 2D chart where one axis is $a$ and the other is $b$, I think you might get a triangular lattice. If it&#x27;s doing what I think it is, then looking at another hidden neuron would give a different lattice with another orientation + scale.Also you could make a base 67 adding machine by chaining these together.I also can&#x27;t help the gut feeling that the relationship between W_in-proj&#x27;s neurons compared to the relationship between W_out-proj&#x27;s neurons looks like the same mapping as the one between the semitone circle and the circle of fifthshttps:&#x2F;&#x2F;upload.wikimedia.org&#x2F;wikipedia&#x2F;commons&#x2F;thumb&#x2F;6&#x2F;6f&#x2F;Pi... reply huijzer 16 hours agoprevA bit of both, but it does certainly generalize. Just look into the sentiment neuron from OpenAI in 2017 or come up with an unique question to ChatGPT. reply lsh123 5 hours agoprevCurrent ML models neither memorize or generalize, but instead approximate. reply davidguetta 19 hours agoprevhierarchize would be a better term than generalize reply version_five 19 hours agoparentAnything would be better than \"grokking\".From what I gather they&#x27;re talking about double descent which afaik is the consequence of overparameterization leading to a smooth interpolation between the training data as opposed to what happens in traditional overfitting. Imagine a polynomial fit with the same degree as the number of data points (swinging up and down wildly away from the data) compared with a much higher degree fit that could smoothly interpolate between the points while still landing right on them.None of this is what I would call generalization, it&#x27;s good interpolation, which is what deep learning does in a very high dimensional space. It&#x27;s notoriously awful at extrapolating, ie generalizing to anything without support in the training data. reply Jack000 19 hours agorootparentdouble descent is a different phenomenon from grokking reply visarga 17 hours agorootparentprev> It&#x27;s notoriously awful at extrapolating, ie generalizing to anything without support in the training data.Scientists are also pretty lousy at making new discoveries without labs. They just need training data. reply 3cats-in-a-coat 19 hours agoparentprevGeneralize is seeing common principles, patterns, between disparate instances of a phenomena. It&#x27;s a proper word for this. reply Chabsff 18 hours agorootparentThat&#x27;s a common mechanism to achieve generalization, but the term is a little more general (heh) than that. It specifically refers to correctly handling data that lives outside the distribution presented by the training data.It&#x27;s a description of a behavior, not a mechanism. Which may or may not be appropriate depending on whether you are talking about *what* the model does or *how* it achieves it. reply 3cats-in-a-coat 16 hours agorootparentKinda fuzzy what&#x27;s \"in the distribution\", because it depends on how deeply the model interprets it. If it understands examples outside the distribution... that kinda puts them in the distribution.General understanding makes the information in the distribution very wide. Shallow understanding makes it very narrow. Like say recognizing only specific combinations of pixels verbatim. reply Chabsff 16 hours agorootparentI think you are misinterpreting. The distribution present in the training set in isolation (the one I&#x27;m referring to, and is not fuzzy in the slightest) is not the same thing as the distribution understood by the trained model (the one you are referring to, and is definitely more conceptual and hard to characterize in non-trivial cases).\"Generalization\" is simply the theoretical measure of how much the later extends beyond the former, regardless of how that&#x27;s achieved. reply davidguetta 16 hours agorootparentprevGeneralize has a tendency to imply you can extrapolate. And in most case it&#x27;s actually the opposite that happens: neural nets tend to COMPRESS the data. (which in turn is a good thing in many case because the data is noisy) reply 3cats-in-a-coat 16 hours agorootparentThe point of compression is to decompress after. That&#x27;s what happens during inference, and when the extrapolation occurs.Let&#x27;s say I tell GPT \"write 8 times foobar\". Will it? Well then it understands me and can extrapolate from the request to the proper response, without having specifically \"write 8 times foobar\" in its model.Most decompression algorithms focus on predicting the next token (byte, term, etc.), believe it or not. The more accurately they predict the next token, the less information you need to store to correct misprediction. reply ot 18 hours agoparentprev\"hierarchize\" only describes your own mental model of how knowledge organization and reasoning may work in the model, not the actual phenomenon being observed here.\"generalize\" means going from specific examples to general cases not seen before, which is a perfectly good description of the phenomenon. Why try to invent a new word? reply davidguetta 16 hours agorootparent> hierarchize\" only describes your own mental model of how knowledge organization and reasoning may work in the model, not the actual phenomenon being observed hereIt&#x27;s not true, if you look at deep CNN the lower layers show lines, the higher complex stuff like eyes or football players etc.. Herarchisation of information actually emerges naturally in NNs.Generalization often implies extrapolation on new data, which is just not the case most of the time with NNs and why i didn&#x27;t like the word reply djha-skin 15 hours agoprevHow is this even a shock.Anyone who so much as taken a class on this knows that even the simplest of perceptron networks, decision trees, or any form of machine learning model generalizes. That&#x27;s why we use them. If they don&#x27;t, it&#x27;s called overfit[1], where the model is so accurate on the training data that its inferential ability on new data suffers.I know that the article might be talking about a higher form of generalization with LLMs or whatever, but I don&#x27;t see why the same principle of \"don&#x27;t overfit the data\" wouldn&#x27;t apply to that situation.No, really: what part of their base argument is novel?1: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Overfitting reply halflings 15 hours agoparentThe interesting part is the sudden generalization.Simple models predicting simple things will generally slowly overfit, and regularization keeps that overfitting in check.This \"grokking\" phenomenon is when a model first starts by aggressively overfitting, then gradually prunes unnecessary weights until it suddenly converges on the one generalizable combination of weights (as it&#x27;s the only one that both solves the training data and minimizes weights).Why is this interesting? Because you could argue that this justifies using overparametrized models with high levels of regularization; e.g. models that will tend to aggressively overfit, but over time might converge to a better solution by gradual pruning of weights. The traditional approach is not to do this, but rather to use a simpler model (which would initially generalize better, but due to its simplicity might not be able to learn the underlying mechanism and reach higher accuracy). reply timy2shoes 12 hours agorootparentIt&#x27;s interesting that the researchers chose example problems where the minimum norm solution is the best at generalization. What if that&#x27;s not the case? reply mashygpig 10 hours agorootparentYea, this is what’s really going on here and feels like it’s been shrouded in language to make it seem more grandiose. That being said, I would believe generalization to occur from minimum norm solutions in some sense, but whether that corresponds to minimum norm weights or not is a different question, and one you probably won’t know a priori (not to mention even knowing which norm to choose). reply rosenjcb 11 hours agoparentprevThere&#x27;s so many idiots in the AI space that are completely ignorant of how Machine Learning works. The worst are the grifters that fearmonger about AI safety by regurgitating singularity memes. reply godelski 15 hours agoparentprevIt&#x27;s because you over generalized your simple understanding. There is a lot more nuance to that thing you are calling overfitting (and underfitting). We do not know why it happens or when it happens, in all cases. We do know cases where it does happen and why it happens, but that doesn&#x27;t me we don&#x27;t know others. There is still a lot of interpretation left that is needed. How much was overfit? How much underfit? Can these happen at the same time? (yes) What layers do this, what causes this, and how can we avoid it? Reading the article shows you that this is far from a trivial task. This is all before we even introduce the concept of sudden generalization. Once we do that then all these things start again but now under a completely different context that is even more surprising. We also need to talk about new aspects like the rate of generalization and rate of memorization what what affects these.tldr: don&#x27;t oversimplify things: you underfitP.S. please don&#x27;t fucking review. Your complaints aren&#x27;t critiques. reply MagicMoonlight 14 hours agoprevMemorise because there is no decision component. It attempts to just brute force a pattern rather than thinking through the information and making a conclusion. reply blueyes 17 hours agoprevIf your data set is too small, they memorize. If you train them well on a large dataset, they learn to generalize. reply visarga 17 hours agoparentthey only generalise with big datasets, that is the rule reply blueyes 11 hours agorootparentThat&#x27;s what I said. reply ajuc 16 hours agoprevI was trying to make an AI for my 2d sidescrolling game with asteroid-like steering learn from recorded player input + surroundings.It generalized splendidly - it&#x27;s conclusion was that you always need to press \"forward\" and do nothing else, no matter what happens :) reply wwarner 9 hours agoprevThis is such a good explainer reply aappleby 7 hours agoprevThey digest. reply agumonkey 11 hours agoprevThey ponderize. reply tipsytoad 14 hours agoprevSeriously, are they only talking about weight decay? Why so complicated? reply lewhoo 15 hours agoprevSo, the TLDR could be: they memorize at first and then generalize ? reply drdeca 6 hours agoparentdepends on the hyperparameters, and the architecture (and probably the task) reply tehjoker 16 hours agoprevWell they memorize points and lines (or tanh) between different parts of the space right? So it depends on whether a useful generalization can be extracted from the line estimation and how dense the points on the landscape are no? reply xaellison 15 hours agoprev [–] what&#x27;s the TLDR: memorize, or generalize? replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Researchers have uncovered a phenomenon in machine learning called \"grokking\" which describes the transition from memorizing training data to generalizing on unseen inputs.",
      "Through their study, they found that weight decay plays a crucial role in enabling models to generalize effectively.",
      "The occurrence of \"grokking\" depends on factors like model size, weight decay, and data size, highlighting the importance of these elements in machine learning.",
      "The text also explores different techniques to prevent overfitting in machine learning models.",
      "It discusses the relationship between angles in mathematical operations and neural network outputs.",
      "Additionally, the text references various papers and articles related to machine learning and artificial intelligence."
    ],
    "commentSummary": [
      "The debate revolves around whether machine learning models primarily memorize or generalize information, compared to how human memory works.",
      "Participants discuss data compression as a form of intelligence and explore the mechanisms of memory in the human brain.",
      "Other topics include limitations of AI language models, regularization techniques in linear regression, the use of the term \"grok\" in AI, and the generalization capabilities of neural networks."
    ],
    "points": 409,
    "commentCount": 181,
    "retryCount": 0,
    "time": 1691675784
  },
  {
    "id": 37081789,
    "title": "My Overkill Home Network",
    "originLink": "https://blog.networkprofile.org/my-home-network-complete-details-2023/",
    "originBody": "NetworkProfile.org Home Contact Me Discord My Overkill Home Network - Complete Details 2023 spookyghost Aug 10, 2023 • 22 min read In this post I will hopefully detail my entire home network. Some of this has been in separate posts explaining single items, but nowhere do I have all of the network in one post with all the changes since last year. Here is a full shot of the rack in my house. Its in a centrally located closet which happens to have a 2ft x 2ft chase into the attic, which is very handy for running network cables. The rack itself is the 25u adjustable StarTech Rack which I've been quite pleased with. Starting at the top we have all of the networking gear and patch panels. At the very top are 2 x Monoprice keystone patch panels. They are mostly filled with Cat6 keystone jacks, but I also have a few LC-LC Keystone jacks for long fiber runs. All of these connections end up somewhere else in the house, either to cameras, AP's or to network jacks etc. Keystone patch panels are clearly the best, a you can easily add and remove jacks without moving the entire patch panel and possibly breaking other connections. This is a generic 1u cable management device off of Amazon, which does a pretty good job along with these Monoprice SlimRun Cat6a cables I am using. Below the patch panels is a 1u Supermicro Server running PFSENSE It has an Intel Pentium Gold G5500, 8GB of ECC RAM, Quad onboard Intel NIC's and a Mellonox Connect-X3 10Gb NIC for LAN + VLANS's. More details in this post Updated Low Power PFSENSE Build for Gigabit Fiber Since moving into a house I now pay for power, so I wanted to upgrade my PFSENSE box while at the same time saving power I also got gigabit fiber, so it needed to be able to handle that. This is the updated 2020 build of my pfSense Firewall. I NetworkProfile.org spookyghost Around the back you can see the connections for my redundant internet. Ignore where it says T-Mobile, that should say Verizon! I am using a 10Gb connection for the LAN and VLAN's because I do inter-VLAN routing on the firewall, so having that extra bandwidth helps. Also because my Internet is 1Gb symmetrical, it was very easy to saturate the 1Gb Link back to the switch. Here is the Verizon Gateway sitting on top of the rack. I get pretty good signal here and get the rated speed, of around 300Mb/s down and 20Mb/s up. This costs $50/mo with no contract and no data cap, and serves as a backup to my main AT&T Fiber connection. Here is the AT&T Fiber ONT, on the other side of the room. This is 1Gb symmetrical with no cap. Both connections are in a Gateway Group in PFSENSE, so I always use the AT&T Fiber, until there is an outage The AT&T connection is in passthrough mode so I can do inbound connections, however the Verizon connection is not. I found that the IP changes very often, and PFSENSE can sometimes get hung up on that, and cause the connection to not work. So I left it in its default configuration which gives PFSENSE an internal IP. Inbound connections are not important on the Verizon connection, however I do still need inbound remote access VPN to work, as well as a connection for some security devices. For that to work, I have a site-to-site Wireguard VPN setup to a VPS in Linode. Because my local PFSENSE box is the client, and the VPS is the server, I essentially bypass any NAT. This is very useful if you are stuck with an internet connection with CGNAT. On the VPS, I just have forwarding enabled so I can access my local network, and then I have NGINX Proxy Manager forwarding specific ports all the way through to my home network. This means I use the same IP or DNS record to get into my network no matter what goes on at home, as the tunnel just uses whatever gateway PFSENSE tells it to. This is also how you are probably getting to this blog, which is hosted at home. More details here Moving my Blog off of Linode and back Home (Sort of) At the end of 2021, I moved my blog into Linode. The main goal for this was to keep it up in case of disaster or outage at home, and utilize the much better networking of Linode. Here is the post: Moving my Blog from Home to LinodeA while ago NetworkProfile.org spookyghost You can read more about the 5G setup here Redundant WAN - Ditching T-Mobile 5G for Verizon 5G If you’ve been reading recent articles, you will know I got T-Mobile 5G Home internet as a secondary WAN connection. It is fantastic knowing I should always have internet. My main connection is AT&T Fiber 1Gb/1Gb, but I like having a secondary connection. It started out GREAT. For NetworkProfile.org spookyghost Below the firewall is my main 1Gb Switch. This is a Dell X1052P which handles the majority of the 1Gb connections in my house, and also all of the PoE in the house. This switch Does PoE and PoE+. I am using that to power AP's, some small network devices and a lot of IP cams. All of the blue connections are the IP Cameras on the house, connected in their own VLAN (I have more connected to the garage, coming up later) This witch has a 20Gb LACP trunk that connects back to my main 10Gb Switch. I am using Cisco DAC's for this for no other reason than I ran out of spare transceivers when I setup the connection. Overall the Dell X1052P is a nice switch in that it has 4 x 10Gb connections, PoE+, sliding rails, good quality fans etc, however the software is TERRIBLE. There is no way to configure via CLI, and the Web interface is the worst I have ever used. Under that switch is a Cisco SG300-28 switch. This switch is here simply because I started running out of 1Gb connections on the Dell X1052P. I have a bunch of low bandwidth things connected here, such as IPMI for servers, Printers, etc. This is connected to the Dell switch with a single 1Gb connection. On this switch I also have the second port of the Verizon Gateway connected on a separate VLAN. This lets me connect a device DIRECTLY to the Verizon internet, bypassing PFSENSE. This is useful for a Ripe Atlas network probe that we will get to later, and makes troubleshooting very easy. Its also good for testing external connections, as it completely takes my internal DNS and firewall rules out of the question. Below the SG300 is my favorite switch, the Cisco SX350-24F. This is a 24 Port 10Gb SFP+ switch, with 4 of the ports being combo ports for copper. This is my \"Main\" switch. This switch connects everything that needs 10Gb. There is Dual 10GB connections to my NAS, one of my ESXi server, single 10Gb connections to mine and my wife's desktop PC, the Firewall uplink, the 10Gb connection back to my garage rack and the 20GB LACP trunk to the Dell switch. I am also using some WiiTek Tranceivers to get 2.5Gb connections to my HTPC. Even though the switch only support 1Gb and 10GB, the Transceiver figures out the rest to let you have Multigig support. You can see that connection here on the far right. Below the networking are 2 shelves, one with lower power devices, and below that my three VMware ESXi hosts. Starting on the top row we have the following Hubitat Hub Raspberry Pi 3B+ NTP server with GPS/PPS time source A new Ripe Atlas Probe connected directly to the Verizon Connection Another Raspberry Pi, this time a Pi W Zero with adapter board, also running NTP with GPS An older Ripe Atlas Probe connected to the AT&T Connection The Hubitat hub has now almost entirely been replaced by a HomeAssistant virtual machine. The only reason it is here is to connect the Z-Wave and Zigbee devices I have. However, I replace those devices with WiFi versions wherever possible. In general, I'm not a huge fan of the Hubitat Hub anymore, and I've had quite a few problems with it. Its configured to just pass the devices into HomeAssitant, so its just a radio at this point. The devices that I still need it for are important though, like Smoke Detectors and leak detectors. The Ripe Atlas Probes are very cool, and generally just help the internet in general. Here is their website with more information RIPE Atlas docsWhat is RIPE Atlas?Docs RIPE Atlas Docs Docs The NTP servers I have made an entire guide on, which you can read here. They give all the devices on my network super accurate time, even if there is no internet. I have one connected to the Dell Switch, and one to the Cisco switch. GPS Raspberry Pi NTP Server This post details how to create a stratum-1 NTP Server using a Raspberry Pi utilizing GPS and PPS, and get time within 100 nanoseconds of real time, directly from the atomic clocks located in the GPS satellites above your head. The best part about this guide is that this will NetworkProfile.org spookyghost And if you were wondering how I have a Pi Zero with ethernet, you can read about that also Turn a Raspberry Pi Zero into a full Raspberry Pi with Ethernet I needed another Raspberry Pi for a project, but with the limited availability I couldn’t find one. The project doesn’t need a very fast CPU, but it must have ethernet. I have an old Raspberry Pi Zero W, but being limited to wireless is a real problem. I also need NetworkProfile.org spookyghost I have however put them in new cases, the HighPi Pro. These have plenty of space to fit the GPS modules. Here you can see the antenna connection on the side I just drilled a hole the same size as the connector and connected it. I put some hot glue on the back to stop it rotating when tightening also For the Pi Zero and the other Pi's, I just used some generic standoffs to fit the GPS modules to a mounting hole where I could The Pi Zero build has the wires soldered directly to the module saving space But even the ones with the large connector cables fits well The GPS antennas I am using are magnetic, so they just stick on top of the rack. Almost everything on that top shelf including the Pi's are powered by this Anker USB power supply that I have had for years. https://www.amazon.com/gp/product/B00P936188/ There is one Pi on the shelf below, and that is an early TinyPilot device to give my NVR Machine remote access. Its a Raspberry Pi 4 with a custom USB interface and HDMI capture, so you can get BIOS level control of the machine. This is needed as my NVR Server (We'll get to that) does not have any remote management. Moving over is the start of the VMware ESXi hosts. All are running ESXi 7.0 U3, as I have not yet made the jump to 8.0. The Move from 6.7 to 7.0 was full of bugs, so I am going to stick on 7.0 U3 for a while. I will detail all of the VM's I am running in another post as its quite extensive, however here is a quick list of services they host Veeam Backup, Borg Backup, Arq Backup, HomeAssistant, HomeBridge, NGINX Proxy Manager, vCenter, Rekor Scout/OpenALPR, Grafana, InfluxDB, LibreNMS, General use VDI VM's, Test VM's, Windows Domain Lab, Portainer, Kiwix, NextCloud, Mealie, Navidrone, Solarr, PLEX, public SFTP Server, Netbox, qBitTorrent, Prowlarr, This blog, and a few other services. The first is an ASUS PN50 with 64GB of DDR4, 2TB NVMe storage and an AMD Ryzen 4800U which has 8 cores and 16 threads. This is really a beast of a CPU for such a small box, and it handles all of the compute heavy VM's on my network that don't need fast networking. The main issue with this host is that the onboard NIC is not supported in ESXi, so I have a 1Gb Intel M.2 NIC sticking out the back. I tried to get a 2.5Gb NIC working, however I never could. To the left are my 2 Lenovo ESXi hosts. The bottom is an old M73 Tiny with just 16GB of RAM (The max it will take) and an old i5-4750T and a 1TB SATA SSD. This host handles some light VM's, however it needs to be replaced, as the NIC will no longer be supported by VMware in 8.0 Above is the Lenovo M720q Tiny, which has an i7-8700T, 64GB RAM, 2TB NVMe Disk, and best of all, an Intel X520 Dual 10GB NIC in the PCI-E Slot. This host handles all of the network heavy VM's. This is what I should have got instead of the Asus PN50. Here is a shot of the NIC. Its a standard PCI-E NIC, with the IO cover removed. I put some Kapton tape on the NIC to make sure it doesn't touch anything it shouldn't. As you can expect a full size NIC in a system this small is quite a squeeze. Overall this small system has 21Gb/s of total bandwidth available to the network once you factor in the built in 1Gb NIC. Below this we have 3 x 2u Supermicro chassis. The top used to my ESXi server, however now is just a disk shelf. This disk shelf contains 12 x 8TB SAS disks that are my main media storage and connects back to my NAS below it. Nothing important is stored on here, and its in a big RAIDZ2 array. You can see here that the control lights are not lit, and the power button doesn't even do anything. The IO shield is just a blank And other than power cables, the only things connected are these Mini SAS SFF-8088 cables that run down to my TrueNAS box. To get the system to power on, I just jumpered the PSU connector with a paperclip and taped it in place. Power control is not important, as this will all stay on 24/7/365. There is a better way to do it, which is to get a Supermicro JBOD control board like the SuperMicro CSE-PTJBOD-CB2. However at the time of deploying this system, they were out of stock EVERYWHERE. Even now, they seem to only be available from China. So I just skipped it. Below the JBOD is an almost identical Supermicro Chassis, but this one is my NAS. It runs TrueNAS on a Xeon E3-1270 V5, 64GB ECC RAM, a dual port 25Gb NIC and a few HBA's, here are the full specs For storage, the LSI 9207-8e (e for external) is connected to the 12 x 8TB SAS disks. The LSI 9300 is connected to 6 x 4TB SAS disks in striped mirrors for my important data. There is also 2 x Intel S3700 DC 800GB Enterprise SSD's for Metadata storage (Also called a Fusion Pool) Since building it, its been completely stable, and I wouldn't change much about the build. Sadly I don't have many pictures of the inside, as its so important to my network I can't power it off to get some nice pictures! You can read more about my storage config in this post. Nothing much has changed other than the replication destination. It now replicates to another NAS in the garage. My Data Backup Plan - 2021 This post will detail how I am protecting my important data. The old post can be found here: My Backup Plan & Lessons learnedI’ve posted about my backup plan before, but a lot of things have changed, and Ihave learned a lot about what works, and what doesn’t work NetworkProfile.org spookyghost Below that server is my Blue Iris NVR running in an 8-bay Supermicro chassis. This has an i7 8700K, 16GB of DDR4, an NVIDIA Tesla P4 (For AI) and an LSI 9207-8i for the storage disks. I have around 48TB of storage for recording, and it does local AI to alert me to people walking on my property etc. Networking is handled by an Intel i350-T4 quad port gigabit NIC, with one port on the LAN, and one on the CCTV VLAN. This lets the server access the cameras directly, as the CCTV VLAN is completely firewalled off from everything and the internet. This is the server that I use the TinyPilot for, as its a consumer board, it has no remote management. Below that is my PDU's and my UPS. The PDU's are CyberPower PDU15M2F10R Metered PDU's, and I've been very happy with them. The front outlets are handy for quickly plugging something in that needs UPS power. The UPS is an APC SRT3000RMXLA double conversion/online UPS powered from a 30a 120v receptacle. Here you can see everything plugged in to the PDU's The large orange cords actually go go back into the wall, where they power other parts of my house, like my desk, my wifes desk and the 2 locations we have TV's. This lets me use a single UPS for all of the critical devices in my house, and not have to worry about changing batteries in a bunch of different UPS's I made a detailed post about that here Home Server Room Power Upgrade + Multi-room UPS This post shows how I upgraded the power delivery to my home server room, as well as installing the infrastructure to replace 5 UPS’s around my house with just one large UPS. Why Upgrade the power? The electrical in my home when I moved in was terrible, it was also NetworkProfile.org spookyghost UPS Upgrade - APC SRT3000RMXLA Double Conversion I have upgraded my lab power setup. The old setup can be found here My Lab Power SetupA lot of people wanted to know what UPS’s and PDU’s I am running, and where.Here are all the details I would highly suggest you read my post on setting NetworkProfile.org spookyghost For a while I was able to run everything in here with no cooling, however adding more devices and more disks made the room way too hot. So I added this AC Infinity Fan which sucks the air out into a spare closet behind this one. The room its attached to we just use as storage, so the extra heat doesn't matter too much Here is a closeup of the fan. I still need to neaten up the hole The temp probe the controller are mounted in front of the rack Along with that is my Vertiv/Giest Watchdog 15-P, which is a PoE Powered enviromental moniotor with the ability to send alerts if the room gets above a certain temp or humidity level. I have it to alert on high temp, and low humidity Here are some of the charts it can give you directly on the device. However, it also support SNMP so you can get the data into whatever monitoring solution you are using. Next we can move into the garage. In the patch panel you can see this run of SMF (Single Mode Fiber) This runs in conduit to the garage to provide networking there It runs though this breezeway, along with the AT&T Fiber to the garage on the right of the picture Here it exits the breezeway in the conduit. The AT&T Fiber goes left, and the SMF keeps on going to the rack The AT&T Fiber goes down into the wall here on the other side And into their box And onto the pole You can see here why I have the backup Verizon internet. As having that thin fiber run below 80ft oak trees, in a place that gets a lot of thunderstorms, isn't great. Knock on wood, I've never had a problem though (My neighbor did though!) The SMF runs along the back of the garage to the rack, along with other ethernet runs that terminate in this rack for things like AP's and IP Cameras. I located the rack in front of my truck, so use some of the dead space The rack is a small 12u rack I got for free, mounted to the studs. Sadly the bottom U isn't actually usable though for anything with depth, so its really 11u. In this rack I have another patch panel, a Cisco WS-C2960S-48LPD-L switch which has 48 x PoE+ 1Gb ports, and 2 x 10Gb SFP+ Ports. Below that is an APC Surge Protector PDU, and then my backup TrueNAS server built from scrap parts, and using 4 x 8TB SAS disk in RAIDZ2. The only thing this NAS does is receive replicated snapshots from my main NAS in the house. So if something were to happen to the NAS, I could instantly get the data from here instead. There is more details on that NAS here Deploying a TrueNAS Backup Server to my hot Texas Garage If you’ve read my older posts, you know I have a rack in my unconditioned detached garage with a Cisco switch, an APC UPS, a couple of Raspberry Pi’s and some other misc items. Everyone told me it would all fail because it will get too hot in summer, too NetworkProfile.org spookyghost Since the NAS is using an old consumer board, it has no remote management. So this new TinyPilot device comes in very handy here I have another Raspberry Pi with GPS running here too, and the antenna is on top of the rack. The UPS is an old SMT1000RM2U which has plenty of capacity for this small rack and devices. It sits at about 20% load at most. I am using the same tactic of giving UPS power to other devices through a power inlet here too. It powers my IoTaWatt Energy monitor, and a Raspberry Pi located inside my standby generator (Also where the run of MMF goes you can see in the above images) More details on those here Power Monitoring Setup (IoTaWatt + Grafana) After getting solar panels I got used to checking the Enphase app for power details, but it’s not very reliable as sometimes the data is delayed, it requires internet access and it can’t get me any information other than solar and grid consumption. So if I want to see how NetworkProfile.org spookyghost Generac Maintenance, Oil Report and more Genmon Changes I could roll this into more than one post, but I’m not sure at what point there is too many generator posts. This post will go over some changes to Genmon (The FINAL CHANGES!) and some notes on maintenance, and the Blackstone Labs Oil Report. First, Genmon. I think I’ve NetworkProfile.org spookyghost Up in the rafters there is also an antenna mounted for ADS-B. This connects back to a Pi with a SDR (Sofware Defined Radio) to feed ADS-B data from planes back to FlightAware, FlightRadar24, ADSB-X and a few others Here is some live data. It would be better if I mounted the antenna outside, which is a project I will be taking on soon. The location of these planes is being reported entirely over ADS-B from the antenna, which is think is pretty neat. There is also a WeatherFlow Tempest Receiver in the garage rack. This connects back to my Weather Station which is in the back yard on a pole up high Using MQTT I am able to get the data locally into HomeAssistant without using the internet, so I can have automations based on the sensors As for WiFi, I have access points spread around the house and garage. I am using 4 x Ruckus AP's. Here is an R510 on the ceiling in the house as an example. And here is an R320 mounted outside on the side of the garage (Note, this is not an outdoor AP, but its been there for over 3 years now working fine) The Ruckus AP's don't look nearly as nice as the Ubiquiti AP's, but I've found they work MUCH better. Since I've installed these over 3 years ago, I've not had to tough the config at all apart from new SSID's etc. They just work. I am using them in Unleashed mode, which lets an AP act as the controller for all of the AP's, and doesn't require a controlled of any kind. For Power, on the outside of the garage is the ATS for the generator which gives me backup power, and my main electrical panel(s) The generator is a 27kw Generator which powers everything in the house. This means the UPS's only need at most 10 seconds of runtime, as that's how long it takes for the generator to start and switch There are some future projects I am working on getting setup, like permanently installing one of these Meshtastic LoRa T-Beam's to get encrypted mesh communication even with no internet. With one at home and one on me, I was able to communicate around 2 miles just with the stock antennas, and no height advantage. If I get a better antenna and mount it up high, I will be able to communicate throughout the entire neighborhood with no internet. I currently have one setup as a repeater sitting on my desk. Configuration is much easier now they have made the WebUI 100x better. The device connects to my network over WiFi I think that's all! Melted UPS Batteries For over 3 months now one of my UPS's has been complaining about its batteries. Being lazy, I just ignored it. This is a short post about what happens when you do that. It started telling me the battery is not installed properly every day or so after a while Aug 6, 2023 2 min read Moving my Blog off of Linode and back Home (Sort of) At the end of 2021, I moved my blog into Linode. The main goal for this was to keep it up in case of disaster or outage at home, and utilize the much better networking of Linode. Here is the post: Moving my Blog from Home to LinodeA while ago Aug 1, 2023 8 min read TinyPilot Voyager 2a - A GIANT Leap Forward Around 3 years ago I made a post about TinyPilot, a Raspberry Pi based KVM over IP device which solved the problem of controlling computers/servers that don't have IPMI/BMC. TinyPilot lets you view and control the console and mount virtual media, etc with a very easy to use Jul 30, 2023 7 min read NetworkProfile.org © 2023",
    "commentLink": "https://news.ycombinator.com/item?id=37081789",
    "commentBody": "My Overkill Home NetworkHacker NewspastloginMy Overkill Home Network (networkprofile.org) 403 points by monstermunch 13 hours ago| hidepastfavorite317 comments 8fingerlouie 1 hour agoI used to have an overkill home network, complete with self hosting everything and 10G backbone.These days i&#x27;m more pragmatic. I have exactly 2 wired machines, and everything else runs Wifi. The network \"rack\" is simply a firewall and a 16 port POE switch, everything else network related has been retired to the closet.My NAS is gone. With electricity frequently going above €1&#x2F;kWh last winter, it seemed like a bad idea to have a NAS sitting there, consuming ~35 kWh&#x2F;month. Instead i spent about €20&#x2F;month on cloud storage, and simply upload everything to the cloud (encrypted with Cryptomator), and i have redundant cloud storage (different provider) included in the cost which is where my backup goes.All that&#x27;s left at home is a small ARM based machine that handles making daily backups of my cloud data locally, as well as run a small Plex library on a single USB3 external drive.Besides saving about half my previous electricity cost (as well as all the hardware cost!), i have gained SO MUCH spare time. My firewall is completely closed (save a wireguard VPN), and i no longer have to maintain anything except the normal patch routines. reply wasksrhh 1 hour agoparentI recommend you checkout Rclone (https:&#x2F;&#x2F;github.com&#x2F;rclone&#x2F;rclone) instead of Cryptomater, much more robust and powerful software. I used to use Cryptomator as well, but after my data exceeded certain size, it became slow and difficult to operate. Plus, once I went to pull my data out of it, I realised that it had reset file metadata on all my files (I didn&#x27;t know what picture was taken when anymore, for example) reply 8fingerlouie 21 minutes agorootparentI do use Rclone as well, but Cryptomator has one feature that rclone does not, it allows me to access my data \"on the go\" from my phone.Personally i have never experienced any of the issues you mention, but my Cryptomator vaults are also mostly below 1TB in size. reply NKosmatos 55 minutes agoprevCome on, let&#x27;s admit it fellow HNers... Most of us are jealous&#x2F;envious of this network setup and would love to have the money and time needed in order to own something similar. Extra points for powering all these from solar panels and also for all the \"little\" gizmos (ADB, NTP, LoRa, TinyPilot, MQTT and temp&#x2F;humidity&#x2F;power monitoring).@monstermunch You&#x27;re living the dream :-) reply wink 43 minutes agoparentI&#x27;d guess a good portion of us have been at that point in our lives but has moved on and downsized or shudders at the thought of keeping that monstrosity of a rack in service or updated :P reply kilroy123 8 minutes agoparentprevI&#x27;m definitely jealous. I think it&#x27;s ridiculous and awesome at the same time. I imagine it uses a lot of electricity though. reply throwawaylinux 28 minutes agoparentprevIt&#x27;s very cool, I just don&#x27;t know what I would ever do with it. And I&#x27;m not saying that means therefore I don&#x27;t see why anybody else would need or want it, I would never use it though. I was thinking about running an ethernet cable or two around the house to hook up a 1gb link but I couldn&#x27;t be assed in the end because wifi is good enough. It&#x27;s not like I can max out my 11mb&#x2F;1mb home internet. \\My new laptop&#x27;s wifi is a bit flaky so maybe I&#x27;ll do it this year if the drivers and firmware don&#x27;t come good.Cameras and weather station are nice though. reply ivolimmen 3 hours agoprevI find this really disturbing. On the one hand this looks really nerdy and cool but then I see the immense amount of hardware that also relies on power... so much power. This is completely unnecessary but then I also see the pictures of an F150.. so yeah it makes sense. I have to categorize this as a (un)typical American person that does not care about the environment.Do like the fact that he runs a RIPE probe though. reply cjdell 3 hours agoparentI also have mixed feelings. I stopped home labing because of the energy costs. I now have everything I need (HomeAssistant &#x2F; NAS &#x2F; Router) running on an old Dell Precision laptop. Laptops make great servers because they are optimised for low power consumption + you get a free console and UPS built-in!I&#x27;m tempted by the new Alder Lake N100 mini PCs that are available on Chinese websites. But I need to wait until the laptop dies before I can justify it. It&#x27;s not just about power consumption but the (hidden) cost of manufacturing. Make use of the stuff you already have before buying new stuff.At least he had a big solar array... reply abwizz 1 hour agorootparent> Laptops make great servers because they are optimised for low power consumption + you get a free console and UPS built-in> Make use of the stuff you already have before buying new stuff.this is the way! reply norman784 54 minutes agorootparentprevMaybe you could consider an old framework motherboards? I personally don&#x27;t own one, but is tempting if you own a framework laptop or are planning on buy one, it seems to be one of the most sustainable options out there. reply maxmalkav 46 minutes agorootparentprevI am a big fan of thin clients as servers. It is true you lose the \"built-in console and UPS\", but they usually come in very nice and compact form factor, many are fanless and consume 10~15W tops while being acceptably capable (in general much more than a RPi but with much better memory and storage specs) reply vladvasiliu 2 hours agorootparentprevMy main problem with labbing is the noise (I live in an apartment). His servers are mostly small PCs, so I think the most power draw is from the disk drives. If you need the storage, say for the cameras and whatnot, I&#x27;m not sure that running a laptop instead would save that much power. reply Borg3 1 hour agorootparentprevLaptops are great servers? Hmm I beg to differ. They are very compressed, so they heat up a lot.You need low power server? Go for Atom like CPU with good case. My home server running D2550 and 2x 1TB HDD is less than 40W. PSU is cold. reply cjdell 18 minutes agorootparentWell it depends what you&#x27;re doing with it. In my case my CPU is averaging about 1% with the odd burst here and there. As a precaution I propped it up to allow increased airflow underneath.You do have a point in that certain devices can suffer from overcharging and bulging batteries. This issue is largely addressed in newer devices. I can see that the BMS in my laptop server is holding at 90% charge. Definitely worth checking if you plan to use an old phone&#x2F;laptop&#x2F;tablet as a 24&#x2F;7 device. reply varjag 43 minutes agorootparentprevRun the same load you do on your Atom on a laptop and it&#x27;ll be cold as Thatcher gaze. reply ivolimmen 1 hour agorootparentprevI run a server at home. It&#x27;s a PINE64... uses 10w or something. My NAS uses more but I only turn that one when I need to backup stuff. reply abwizz 1 hour agorootparentprevtypical laptop goes below 10W while serving a gbps of samba reply Gareth321 28 minutes agoparentprevIn many U.S. states, their primary energy generation is nuclear. This is extremely environmentally friendly, meaning no issues with OP using a bunch of power.We really need to stop demonising the use of energy. Energy is amazing. It is the reason we have seen to much prosperity in so little time. It&#x27;s the backbone of our economies. Of medicine and the internet and EVs and well insulated homes. We should be doing everything we can to make energy cheaper, enabling more people to use more energy. Few things have been as damaging for climate activism than the war on energy. It&#x27;s unnecessary and misses the point of the entire movement: saving the environment. Focus on cleaner sources of energy rather than demonising people using energy. reply b3orn 5 minutes agorootparentI looked it up, 5 US states get the majority of their electricity from nuclear power, 4 from wind, 3 from hydro. The other states use coal, natural gas and petroleum.Source: https:&#x2F;&#x2F;www.nei.org&#x2F;resources&#x2F;statistics&#x2F;state-electricity-g... reply Karrot_Kream 12 minutes agorootparentprevIt&#x27;s not about the energy for me. A lot of these posts make networking and homelabs seem like a dark art that only folks with tons of space, capital, and time can get into. Networking is anything but, it&#x27;s about sending a packet of data over a wire! But when we glorify these elaborate, expensive setups, we not only lose focus on what the goal of a network should be (to satisfy needs for its users), we also discourage newcomers from tinkering with networks which to me is the bigger crime. reply Karrot_Kream 1 hour agoparentprevI do a lot of similar stuff in my home network with a lot less equipment. I have a trunk switch that sits behind my router, Ethernet running into each of my rooms (came with the house), and small point switches that sit in front of each patch. I have 3 APs in my house. I have a UPS which connects to our router, trunk switch, and a single AP that uses PoE which is good enough if we need Wifi when the power goes down. To much shock and horror, I just bought discount Cat6 cabling and didn&#x27;t bother buying a roll and crimping and making my cabling look nice (but I did test every point in my network exhaustively to make sure I don&#x27;t have high amounts of latency or jitter anywhere.)The rest of the services that OP runs I run on a combination of a 12 year old SBC and a Raspberry Pi. Modern computers (readI do a lot of similar stuff in my home network with a lot less equipment.This is also my case. For Internet access, I use a router which has 4 2.5 GbE ports for the internal LAN, which was made several years ago with an Intel NUC and 4 USB Ethernet dongles, but which could be done today with a small and cheap fanless router using an \"Alder Lake N\" Intel N100 or N200 CPU. There are also such small routers with more 2.5 GbE ports for the internal LAN, e.g. 6 or 8. That router implements a lot of network services, including DNS servers for my domain and my own e-mail server.I also have 5 servers that are interconnected by a 10 GbE subnetwork, but that is done without external switches. Each server has a dual-port 10 Gb&#x2F;s NIC and they are interconnected directly, in a ring. The servers are normally powered off. Whenever needed, I power on the first of them through Wake-on-Ethernet. Through it, I can power on, also using Wake-on-Ethernet, as many of the others as needed. reply Karrot_Kream 29 minutes agorootparentHuh the ring topology is a really cool idea. With the right routing rules it could open up some ports on my trunk switch, though I&#x27;d need to upgrade my NICs. Great idea for whenever I decide to retire the SBC though! reply adrian_b 13 minutes agorootparentWhen all the servers of the ring are powered on, the packets are distributed on both links, so the throughput of the network doubles.When a part of the servers are powered off, the topology of the subnetwork becomes linear and the throughput becomes limited to 10 Gb&#x2F;s, but with less active servers the traffic is also less.Greater performance can be obtained by using two Ethernet cards with dual 10 Gb&#x2F;s ports in each computer. With 4 ports per computer total interconnection can be achieved up to 5 nodes and above that each node can be connected to two intersecting rings, which is a topology very frequently used in HPC.An alternative is to use a single card with dual 25 Gb&#x2F;s ports, per computer. Such cards are not much more expensive than the 10 Gb&#x2F;s NICs, but the 25 Gb&#x2F;s switches are very expensive, so a ring topology without switches is even more attractive. reply kriops 3 hours agoparentprevI find your comment to be at the absolute peak of sanctimony. Who are you to decide if it is necessary or not? Furthermore, why are you justified in negatively stereotyping Americans – a diverse group of individuals? reply Toutouxc 1 hour agorootparentA diverse group of individuals who consume 12 MWh of power per capita, while it&#x27;s around 6 or 7 MWh for modern European countries. It&#x27;s not even a stereotype, just statistics. (got my data in two minutes from Wikipedia, so maybe not up-to-date) reply ryan-c 33 minutes agorootparentI wonder how much of the difference is air conditioning? reply Karrot_Kream 26 minutes agorootparentI don&#x27;t really think 2x the power usage comes from AC. The US has plenty of areas that only need a bit of AC for only part of the year. reply ryan-c 6 minutes agorootparentIt&#x27;s definitely not all of the difference, but it&#x27;s gotta be some. reply snerbles 3 hours agorootparentprevThe satisfaction from serving up self-righteous condemnation is one hell of a drug. reply Gibbon1 11 minutes agorootparentGiving attitude from 50 degrees north latitude. reply rb666 2 hours agorootparentprevHow is it an act? Caring about the climate is clearly morally superior than not doing so. reply drexlspivey 2 hours agorootparentThe \"using electricity is bad\" meme must be one of the stupidest ones in the last few years reply maleldil 1 hour agorootparentWhy? IMO we should all avoid waste, whether that&#x27;s plastic or energy. reply drexlspivey 1 hour agorootparentThen you must hate that almost all of sun&#x27;s energy hitting earth is wasted reply maleldil 1 hour agorootparentThis is such a bad faith comment. The sun isn&#x27;t causing climate change; we are.Do you disagree that humans need to minimise waste in order to fight climate change? reply drexlspivey 1 hour agorootparentUsing energy doesn&#x27;t cause climate change. Producing energy from fossil fuels causes climate change. Humans don&#x27;t need to minimise energy usage to fight climate change, instead they need to produce energy from non fossil sources. I wish we produced 10x energy that we do now but from renewable sources. reply globular-toast 43 minutes agorootparentYou&#x27;re right, in the most meaningless sense of the word. I too wish we had 10x the energy from renewable sources, but we don&#x27;t and won&#x27;t for the foreseeable future. What we actually can do is get maybe 1&#x2F;10 of our energy from renewable sources and then, to be sustainable, we need to use 1&#x2F;10 of the energy we currently use. replya-user-you-like 1 hour agorootparentprevHow do you know the man’s power is not cleanly generated? reply antoinec 1 hour agorootparentLooks like he is powering at least part of his installation through solar: https:&#x2F;&#x2F;blog.networkprofile.org&#x2F;17kw-enphase-solar-install&#x2F; reply uoaei 1 hour agorootparentWhat is the CO2e ROI on producing and installing solar, including all the transportation and installation equipment being used?When is it net-negative compared to pulling energy from the grid? reply antoinec 1 hour agorootparentGood question, not sure how I would know the answer though. You probably should ask the guy. Given how he seems to track everything, I wouldn&#x27;t be surprised if he came up with a way of measuring that as well. replyrimliu 2 hours agorootparentprevThe question was not addressed to me, but I have some tongue-in-cheek answers: > Who are you to decide if it is necessary or not? Inhabitant of the Earth. The one affected by climate change. > Why are you justified in negatively stereotyping Americans Tell me, what is the best-selling truck in US for 46 years in a row? reply checkyoursudo 1 hour agorootparentprevThat particular comment was \"the absolute peak of sanctimony\"? Like, the most sanctimonious comment you have ever read on the internet? reply kriops 12 minutes agorootparentYes. This is a prime example of what people refer to when they say that someone is being an \"asshole\". The reason being that the OP is being incredibly condescending while simultaneously lacking any trace of empathy and knowledge.I&#x27;ve seen stuff that or more vile etc. of course, but as far as sanctimoniousness goes, this is the worst I&#x27;ve seen in my several decades on the internet. reply Cthulhu_ 1 hour agoparentprevI dunno, a lot of people on here use more power without noticing it, because it&#x27;s on whatever cloud provider they use. Every time I push some commits, an array of servers spins up in an AWS datacenter somewhere to run all of my verifications and do a build.Second, this person seems to have a very large house anyway. My house manages with a modem, a wifi router and two IP cameras, but that can just scale up linearly.When I grow up I want a switch rack like this in a basement or closet, wired network everywhere and PoE security cameras. Bigger house also means I can get an array of solar panels and become net positive. reply Karrot_Kream 1 hour agorootparent> I dunno, a lot of people on here use more power without noticing it, because it&#x27;s on whatever cloud provider they use. Every time I push some commits, an array of servers spins up in an AWS datacenter somewhere to run all of my verifications and do a build.Most AWS (and generally cloud) instances are already virtualized from bigger machines. Power, heating, and cooling in these large racks is running much more efficiently than running these things in your home on dedicated compute that is only being used for your purposes. Even more efficient is using something like serverless compute or something like Cloudflare workers. While cloud compute probably is not running at low idle power, it&#x27;s probably maximizing it&#x27;s power utilization as much as possible.> Bigger house also means I can get an array of solar panels and become net positive.There&#x27;s a lot of hidden costs associated with becoming \"net positive\", everything from adding more peak load during inclement events to issues associated with grid fragility.Look I don&#x27;t think it&#x27;s good form to roast OP on what&#x27;s really standard behavior on the internet: showing off your cool digs. But I also think we should be intentional about what it is: showing off your cool digs. reply m463 3 hours agoparentprevHe said in another post he has 17kw of solar, so... reply irusensei 2 hours agorootparentIt&#x27;s never about how green is the source of energy. It&#x27;s about people doing things with their energy in which they don&#x27;t approve. I mean... just read their xenophobic remarks about a giant country of diverse people.Don&#x27;t bother appeasing climate hystericals. reply ryan-c 52 minutes agoparentprevI am not running serious network gear right now due to power, which is ruinously expensive in the UK to the extent that I find myself longing for PG&E&#x27;s rates in California.Currently having a house remodeled, which I&#x27;m putting as much solar (and battery) on as possible, and the gas is being removed entirely. Going to try not to use more than the system can generate, though I am not sure I&#x27;ll be successful. reply SanderNL 2 hours agoparentprevI find it very interesting how awareness of climate change (and let’s not forget: the price of energy) has had so much impact on my and others’ judgement in a relatively short time.5-10 years ago I would think this is perfectly fine. I believe I was not alone in this, but maybe I was. The energy would have cost pennies too and why whine about it?Now I think it’s exorbitant as well, for me. The F150 etc, it’s telling a different story now.I do think it’s kind of below the belt to snipe any single culture for this. It’s too easy to target Americans. I don’t know about this guy’s life. Maybe he has a ranch with 20 square miles of renewables and I don’t care actually. It’s just personal reflection. reply alex3305 1 hour agorootparent> 5-10 years ago I would think this is perfectly fine. I believe I was not alone in this, but maybe I was. The energy would have cost pennies too and why whine about it?When I moved into my own home (8 years ago) I brought my &#x27;homeserver&#x27; with me. Which was just a simple i5-2600 build with some shucked drives in it. I never thought about electricity prices when I lived with my parents. But that changed rather fast. With the server gobbling up a constant 90W, I quickly realized that, even back than, it would cost me 15 euro&#x27;s a month on electricity alone.I than proceeded to put a Pi next to it, that would listen to incoming Plex requests and would start up and shut down the server with WoL. That only reduced costs by about a third. The next couple of years I would move on to a NUC with a NAS that would only consume about 29W&#x2F;h on average. Which was decent, but not great considering the poor performance of both machines (J4105 and J1800).Last month I have gone back to the DIY route. Now with a i5-13500. I&#x27;m still in the process of optimizing it for power efficiency. Although much more stressful than the prebuilts, I love the hunt for the last watt.Anyway, what I wanted to say is that I notice that family and friends don&#x27;t really care about saving power in general. They mostly just pay for it and there&#x27;s that. While my house runs 100% on electricity and I&#x27;m really proud if I can get 9kWh&#x2F;day on average. Even when I see that (for example) the 8-bit guy uses 100kWh&#x2F;day on average [1]. Which seems out of this world for me.1. https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=bXd-aP06lug&t=45s reply djhn 53 minutes agorootparentHold on a second - how large of a household are we talking? House or apartment? What consumes all that power? Electric vehicle? Work from home?I&#x27;m environmentally conscious (0 cars, 0 pets) but I haven&#x27;t spent any time measuring and optimising electricity usage.I&#x27;ve had a look at my own electricity usage every once in a while, now averaging 48 kWh&#x2F;month (between 38 and 68) for a larger than average apartment for two people. reply Karrot_Kream 19 minutes agorootparent48 kWh &#x2F; month is amazing! That&#x27;s 4 kWh &#x2F; day. Just cooking on our induction burner and instant pot can use up 1 kWh on a day with lots of meal prep. reply pprotas 2 hours agoparentprevSnide categorizations aside, there is a point somewhere in your comment. It is a running joke in the &#x2F;r&#x2F;homelab community that their servers use an unreasonable amount of power that they have to hide from their wives.It really IS unnecessary, as you can tell by the author of the article referring to their home network as \"overkill\". reply miked85 3 hours agoparentprevFunny, just seems like a hobby to me. reply ur-whale 3 hours agoparentprev> I have to categorize this as a (un)typical American personHolier-than-thou much? reply maleldil 1 hour agorootparentAmericans tend to drive larger cars than other nationalities. These cars use more fuel. Most of these people do not need large cars and would be fine with smaller ones that use less fuel. Therefore, by choosing larger cars, they&#x27;re using more fuel than necessary. An explanation is that only care about their own comfort, not the environment. reply abwizz 1 hour agorootparenttbf a lot of murricans live in rural environments where a truck does indeed make sense because of the lots of unpaved roads.city dwellers not so much reply kristjank 3 hours agoparentprevI hope you&#x27;re trolling. Power can be generated without an environmental impact. There are companies (and F150s) dumping kgbjillions more CO2 than this person&#x27;s homelab is. Let people learn something and have some fun. reply maeln 2 hours agorootparentPower cannot be generated without an environmental impact. Even solar panel and wind turbine generate a non negligible amount of pollution when produced, and a non negligible amount of waste when they have to be decommissioned. Also, it is highly unlikely that he is using his own grid, he probably pulling most of his power from the U.S grid, which (like most grid to be fair) contains a fair amount of CO2 heavy sources. And that is not even talking about the amount of pollution and waste used to produced all that hardware he is running.I get it, it looks cool and damn fun. I dream of this kinds of setup. but brushing off the environmental impact is also wrong.Now admittedly, he is but a drop in vast ocean when it come to pollution and waste, and there is definitely far worse offender. But keeping our environmental impact in mind should be a necessity in this era. reply Cthulhu_ 1 hour agorootparentThere, it&#x27;s in mind; what can we do now to stop the big companies from making things worse?Individual accountability is important, but self-flagellation or (vicarious) shame because forces bigger than us are causing problems is a waste of energy (ha!). reply ptman 3 hours agorootparentprevIt cannot be generated without an environmental impact. Equipment has an impact and a limited life. reply komali2 54 minutes agorootparentprevThis is the argument that holds the most weight with me. It&#x27;s why, as much as I approve of Extinction Rebellion and their kin, I think it&#x27;s counterproductive to deflate SUV tires, when the environment harm caused by SUVs is a fraction of that caused by the companies profiting off oil and gas extraction and refining. If you&#x27;re gonna do a crime anyway...It&#x27;s similar to when there&#x27;s hypocrisy charges levied against \"champagne socialists.\" Nice virtue signalling for worker&#x27;s rights for someone with a 6 figure income... meanwhile that 6 figure salary accounts for 0.01% the net worth of a billionaire.Go for the high stakes first, then we can worry about homelabs pulling more power than they should. reply bmitc 11 hours agoprevIs there a post that covers the \"why?\" of all this? As someone researching maybe running some Ethernet for more stable WiFi and faster Internet access for general stuff, maybe some fiber for infotainment other purposes since it can lengthen various cables, and hooking up some security and nature cameras, I can&#x27;t fathom why all of this is needed outside of it just being a hobby or running some sort of business (trading?) at home. For example, isn&#x27;t $600 for redundant Internet for like the minutes the Internet is out every year worth it? Why is that level of connectivity needed? reply matthew-wegner 9 hours agoparentI built a pretty serious \"homelab\" over a few years, in the sense of building out 24&#x2F;7 services as robustly as possible. There is an element of practicality for very long-term maintenance if you run i.e. VMs off a different storage layer.But mostly? It felt exactly like exploring computers in my childhood for the first time (I&#x27;m 43 now). I was bumping into things I had never bumped into it before, and it was really satisfying to figure all that stuff out.I&#x27;m a game developer, too, and I realized awhile ago a big part of that is I like technical puzzles, and game development is very fertile soil. But there is a whole new world of enterprise networking&#x2F;storage&#x2F;virtualization&#x2F;etc things that you wouldn&#x27;t normally bump into in the course of software development.And as a bonus, I have set up a lot of build systems for friends; being infrastructure-savvy is like the digital equivalent of owning a truck and helping people move all the time. reply fzeindl 5 hours agorootparent> There is an element of practicality for very long-term maintenance if you run i.e. VMs off a different storage layer.That is very good advice. I know about a datacenter outage&#x2F;degradation in a very large company, which all of you know.A couple of network switches got overloaded. The switches connected the SAN to the VMs, so it caused all kinds of weird problems in different applications. reply nanidin 4 hours agorootparentAWS or cloudflare? reply hqsolomo 11 hours agoparentprevSpeaking personally, it&#x27;s just another form of tinkering. Nobody HAS to do it, much like nobody has to buy and maintain their own computer, car, home, etc... We do it because we can and it makes us happy.Extra note- I would NEVER recommend a business use their home resources for work unless you know what you&#x27;re doing (and why you&#x27;re doing it). Even my homelab has vulnerabilities and I&#x27;m a security professional- more moving parts = much larger attack surface. There&#x27;s a reason why corporations pay big buck$ for managed security services. Most homelabs I&#x27;ve seen are mostly for fun and personal comfort.\"What good is knowledge that is never applied\" is what drives me to stuff like this. I can&#x27;t speak for others but I&#x27;m sure I&#x27;m not the minority here. reply bmitc 10 hours agorootparentGotcha. Thanks for the reply! To be clear, it wasn&#x27;t a judgment but just a question to understand there weren&#x27;t some unapparent reasons why. I think it makes total sense as a hobby and learning, but perhaps still overkill as the post title mentions. Haha. reply hqsolomo 10 hours agorootparentI apologize if it seemed like I was irate- I&#x27;m not and your question is 100% the question we all ask ourselves before doing things like this (or for some of us, it&#x27;s the first question on the wife acceptance factor audit). If anyone is running a data center in their home for a serious reason they either have small loads to justify the power consumption (rpi k8s cluster says hi), stacks to blow, regulatory pressure, or isn&#x27;t factoring the costs in and is in for a rude awakening. I don&#x27;t think (but also don&#x27;t know) these labbers are the majority, and us homelabbers are already a rarity.If it makes you feel better, megacorps are getting out of the \"self-managed data center\" industry and embracing the cloud, to exemplify your very point. reply bmitc 9 hours agorootparentLol. I was under zero impression that you were irate or anywhere close. My question was a little judgemental perhaps but not necessarily meant in any way. I was also curious if there was some interesting need that had come up. My primary thing that I want is a bunch of PoE powered nature cameras, buy I&#x27;m still figuring that out. It will affect whatever comes up though. Oh, and stable WiFi coverage.I certainly have hobbies that go above any need or reasonable collection, namely synths and books. Haha. reply hqsolomo 9 hours agorootparentIt&#x27;s interesting that you mention PoE nature cams- I designed a PoE home surveillance system for a friend that involved setting up a solar panel on a 30&#x27; pole that fed a box with a shitty camera system in it at ground level. From there he set the cams around his property- particularly where the foxes and coyotes would travel to get to his hens. The whole project was apparently about $800 aside from the solar panel (I just gave him the idea- I didn&#x27;t help him build it).He eventually got rid of the cameras because, well... They were shitty and only told him the critters were near AFTER he popped &#x27;em. I think he&#x27;s got a for-purpose system (in his own words \"the new cameras didn&#x27;t fall off a truck\") now but it was a fun project! reply resonious 6 hours agorootparentprevI still think your question is reasonable. Even when someone is doing something for joy, there&#x27;s usually a spark - an essence to it. There&#x27;s usually an initial motivation that sent them down the rabbit hole, and it&#x27;s interesting to hear what it was. reply Brajeshwar 7 hours agoparentprevI have been working remotely since about 2005-2006. I have always love a good Internet connection with backups (wherever possible). I remember befriending the local cable guy so I can get 1Mbps in 2000.Now, I have three Internet connections bonded and balanced. It is not about the minutes of disconnection but more about the disconnection when I needed it most. I have ample non-internet time and personal&#x2F;family downtime. However, when the time comes, I’m happy that the Internet is never a bottleneck in my work and play. I have had this setup since around the early days of the Pandemic and our home, per se, “never had an Internet Outage” since.I&#x27;m not into servers, devops, networking or anything of that sorts, but I love tinkering with them. I would love to have an \"overkill home network\" one day.Internet is super cheap in India and I can afford all three for a really decent price. https:&#x2F;&#x2F;www.instagram.com&#x2F;p&#x2F;CUWeopdPVOp&#x2F; reply LeafItAlone 5 hours agorootparentSounds pretty neat!Just wondering: 1. How often do you actually need the third? As in, both primary and secondary are down. 2. Is it that important that you always have an internet connection? reply Brajeshwar 5 hours agorootparentAfter the 2nd, it was more of a fun and why-not! It is cheap, and comes built-in with a free Netflix subscription and quite a few others. I think it even has an unlimited voice call if I want to plug in a phone and use it. reply gumby 7 hours agoparentprevWhy not? I had a pretty elaborate setup in the house I sold during covid (fortunately the purchaser was enthusiastic about it) with a wiring closet in each wing and a fibre spine running between them. Not at all as elaborate an external connection as the author but I had a lot of machines and wanted to keep wireless bandwidth for things that moved around.One handy trick: I had at least one drop in each room, often one for each wall of the room. Behind the wallplate was a NEMA box with a conduit running straight down to the crawlspace (6&#x27; high in my house so hardly \"crawl\"). That made it easy to pull cable, not only initially but if I found I needed an extra run, which hardly ever happened. Instead of trying to run through a rat&#x27;s nest of conduit the conduit was a straight shot and then the cables could easily be managed. reply lostlogin 9 hours agoparentprevAnother personal anecdote.Wifi is rock solid, networking is fast, backups good and outages are almost nonexistent and are always due to me messing with things. I have Netflix, Apple TV+ (is that what it’s called?) and a few others. Particularly with Netflix, the quality is junk so I watch off Plex.Other benefits are pretty neat too. POE just works. You can power cycle things remotely if you want (have never needed to) and the abundant local storage makes everything easy.Enterprise stuff that is getting old is also dirt cheap. Converting sections of the network to 10gbe was very inexpensive.It’s all rather addictive… reply syntaxing 10 hours agoparentprevI mean, “need” is a pretty strong word. No one needs a MacBook Pro, you can do everything on a Raspberry Pi just slower (exaggerating but you get the point). Personally, having reliable internet has been a significant increase in quality of life but admittedly, mesh routers are getting closer and closer. The biggest draw are VLANs. When even the FBI recommends separate networks, it shows how prevalent these issues are. I know a bunch of people that got affected by cryptolocker. While having good practice is probably what helps me a lot, a hardened network helps tremendously. reply Emjayen 1 hour agoparentprevAlthough not as elaborate as this, I have what one would call a \"homelab\" and it&#x27;s for, well, testing and experimenting -- I write a lot of high-performance server&#x2F;network-centric software (e.g, saturating a 100G link with 64B frames is a common test) and virtual machines just aren&#x27;t suitable&#x2F;capable most of the time. reply csomar 6 hours agoparentprevThere is no \"why\". Some people like boats. Some people like servers. People spend money and time on things they like. reply heavyset_go 9 hours agoparentprevIt&#x27;s a hobby that can pay dividends in multiple ways, don&#x27;t really have to look deeper than that. reply m463 3 hours agoparentprevSetting up a home network, especially with cisco gear, can easily pay for itself 1000x in sysadmin or network engineer employment opportunities.This is exactly the kind of nuts and bolts guy that is indispensable when keeping a cloud running. reply wesapien 10 hours agoparentprevMost of the time it&#x27;s the IT Infrastructure, Operations, Security guys that love to do this for (WHY) learning, testing and as a hobby. Check out r&#x2F;homelab and r&#x2F;selfhosted for more on this. reply hqsolomo 9 hours agorootparentServethehome forums and smallnetbuilders are also good resources, if you&#x27;re not a fan of Reddit for some reason! reply comprev 11 hours agoparentprevFor the same reason some people cycle around the world instead of just a casual ride on Sunday morning like many cyclists.Some tech people enjoy \"homelabbing\" and happily throw money at their hobby. reply frankreyes 11 hours agorootparentI&#x27;d say similar like sport sailing, but yeah there&#x27;s some bikes that cost more than a sailboat. reply comprev 10 hours agorootparentBeing out on the water is such a peaceful experience. reply thombles 9 hours agorootparentTake too long to winch in the jib and it&#x27;s another matter though. :) reply cortesoft 4 hours agoparentprevThis is clearly the guys hobby. He does it for the act itself. reply theossuary 9 hours agoparentprevThere&#x27;s a reason other than \"because it&#x27;s fun to tinker.\" I do it because I love the capabilities it provides. I don&#x27;t love managing Ceph & Proxmox & whatnot, but I love being able to deploy whatever I want into a beefy cluster with 10gb without having to worry about cloudspend. If I wanted to replicate what I run in my home infra (I don&#x27;t consider it a lab), it&#x27;d cost 200-300 in compute and easily another 300 in storage a month. Instead I spent ~2k on hardware and ~35&#x2F;month in electricity.It&#x27;s the same thing with 3d printers, some like them because they want to install clipper and tune the best&#x2F;fastest benchy. I do it because they love being able to CAD something and have it in my hands as soon as possible.That said, nobody goes as far as this guy without really enjoying the tinkering reply hqsolomo 9 hours agorootparentVery well said- good catch! That&#x27;s a very compelling reason! reply bmitc 9 hours agorootparentprevWhat do you run on your cluster? CAD compute? reply theossuary 9 hours agorootparentNo, but that&#x27;d be neat! I don&#x27;t have GPUs in my cluster though.My big services are a Ceph cluster, a VPN, Borg (backups) and a K8s cluster (using kubespray).In K8s I have the main stuff: Plex, Gitlab, Gitlab runner (for CI), vaultwarden (passwords), miniflux (RSS, might be moving to freshrss though), rust desk (remote-desktop), home assistant (smarthome).My next project is to stream metrics into grafana. I have soil moisture sensors in my garden connected to stm32 boards, I just need to setup the receiving side and I can control my drip lines (they&#x27;re using opensprinkler) with soil moisture information along with weather info. reply j45 3 hours agoparentprevIt’s a good question, as long as the why is not a burden to be met.First it’s a useful skill set that helps troubleshoot problems in code and apps that run on networks by knowing how such things work.Second, try to think of it as a private cloud instead of a network. Because it uses a lot of the same types of software cloud provider do. Proxmox is one example that is a self hosted vps provider that is tremendous. So, it’s a private hybrid cloud that can push from your cloud to the other voids (or back). Build using a private hybrid cloud and you can push to,or between manu cloudsWifi is for convenience, wired is for reliability. When wifi gets jammed, spotty, interfered with which can happen more often than imagined, especially during break ins, wired wins. Transferring files? Wired wins. 4K streaming ther doesn’t cut out when multiple devices are doing it at once? Wired wins.Self-hosting is much easier than it was 5,10,15 years ago. Tools like proxmox loaded with yunohost running on a 1L usff&#x2F;mini&#x2F;tiny pc with 64 gb of ram and mirrored ssds ther maybe mirror to another i de oval box can sip power but power production grade apps for you personally. If you buy big beefy servers it will cost a bit more electricity.Still, hosting locally can quickly pile up the savings on saas not spent.Imagine being able to keep up all your test projects you might spin up a paid VPS for and run for way too long. They might not be production grade, but there is something valuable about having them around.Data backups - the cloud is just someone else’s computer sold as convenient but not secure. Having your own local copy when the internet goes down means not as much of your life or work goes down. Remember it’s not a backup if there is only one copy of it.Backing up your computers in the cloud are only so helpful when they are down, a lot of time to download them. Local backups win again.Multiple connections can matter for people who wfh and can’t be down, or have spotty internet that is up.If you like smart home tech, it’s a ticking clock until the cloud based supports for it that are usually free dissapear leaving perfectly good gear unusable. Instead you can run a local instance of home assistant, etc.I used to host morethan I wanted ina. Data centre much like this rack. I was hesitant to come back to self hosting or homelabs but j have realized a home server that runs like an appliance (in between that wiring) capturing the sum of all my data and worn as services come and was pretty much unavoidable. Luckily it’s getting easier and easier to do.Hope that helps. reply envyac 10 hours agoprevCurious why you didn&#x27;t stack them like...1. 24p Patch Panel (Odd ports of switch) 2. 48p Switch 3. 24p Patch Panel (Even ports of switch) 4. 1u Cable MgmtThen you could have bought 4-6\" Cat 5e&#x2F;6a pre-made patch cables to connect in the majority of your patch panel ports. The only cables that would traverse through the cable mgmt panel would be longer cables to the servers in your rack.I have a similar Cisco switch as your 24 port. I am not fan of how they arranged all the ports at the right side of the enclosure. Why not make the 24p one row along the top (or bottom), like the 48p with the 2nd row removed? It would be a lot easier for cable mgmt. reply monstermunch 7 hours agoparentThe reason its arranged like that is because it started as 1 switch and 1 patch panel, and slowly evolved, so its not ideal, but I am fairly happy with the outcomeIf I could re-rack it all, I might make some changes. But that means turning everything off, and I&#x27;m not sure when, if ever, I want to do that reply petesergeant 6 hours agorootparent> But that means turning everything off, and I&#x27;m not sure when, if ever, I want to do thatDecent robustness check reply greggyb 9 hours agoparentprevI noticed this as well. The patch panels seem only to bring some wire to the front of the rack, only to immediately send the patched wire toward the back of the rack again. reply imiric 9 hours agoprevThis is inspiring. Great work!My homelab is a mess of cables, and half-baked deployments running on old Supermicro and custom servers, that is way too noisy, and probably pulls a lot of power. I wouldn&#x27;t know, as I&#x27;ve yet to track power usage... (One of those projects I never get around to &#x27;;-D)Everything&#x27;s stacked on a Lack rack, even though I have a 24U rack in a closet doing nothing. This is partly because of space and cabling constraints, but _one day_ it will all be neat and tidy, clean and efficient. One of these days... reply monstermunch 7 hours agoparent>My homelab is a mess of cables, and half-baked deployments running on old Supermicro and custom servers, that is way too noisy, and probably pulls a lot of power.I dunno man, sounds identical to mine... reply trustingtrust 4 hours agoprevBiggest problems I have faced for home networks : ISPs are poorly configured and do very shady things sometimes like DNS hijacking. More often than not poor ONU firmware or hardware means you will see weird things like latency spikes or straight up packet drops. Sometimes IPv6 won&#x27;t work if the ONU is in bridge mode etc. etc.This post is overkill but what mostly works for home networks:SQM - like cake sqm in openwrt - will work wonders if don&#x27;t have a great ISP. I have seen as much as 5% packet loss on one ISP who said thats acceptable for a home network. Just reduce the load until a point where packet loss is really low. Then use a DNS resolver. DNS resolver will be much more reliable like unbound for your local network as packet loss has cause really problems with DNS for me. Cheap home network equipment dns is not reliable. You could really just buy a cheap Pi and run a local resolver. It would work a lot better than your home router. You may also want to consider replacing the ONU. This is rather easier than you think. The ISP ONU are really bad. A cheap NOKIA SFP GPON is like 20$ on eBay and if you can figure out the PLOAM password and Serial and Mac and VPN ID of your ISP, you will be able to simply swap it for a more stable link.All in all I would suggest for a simple home setup: get a cheap SFP GPON and replace your ONU. Get a cheap router (either router on stick with Rpi4 for >> Verizon Gateway sitting on top of the rack. I get pretty good signal here and get the rated speed, of around 300Mb&#x2F;s down and 20Mb&#x2F;s up. This costs $50&#x2F;mo.There are offers [0] in the EU of 10Gbps &#x2F; 25 Euros&#x2F;mo.[0] https:&#x2F;&#x2F;www.digimobil.es&#x2F;fibra-optica&#x2F; reply qwerty456127 1 hour agoprevI wish every apartment would have at least some units of at least half-depth of rack space. Fitting anything (even a basic setup of a PaspberryPi + the PON modem + a minuscule switch + a PoE inhector) into the tiny space behind hatch where the cables from all the wall sockets end in my condo apartment is extremely hard. And I am still looking for a 5-port switch which would be manageable, support vlans, handle gigabit ethernet and inject PoE if possible while being really small. reply tosie 1 hour agoparentWhen powered adequately, the Ubiquiti Flex (USW-Flex) [1] might fit your needs.[1] https:&#x2F;&#x2F;eu.store.ui.com&#x2F;eu&#x2F;en&#x2F;collections&#x2F;unifi-switching-ut... reply qwerty456127 37 minutes agorootparentLooks great but I woud prefer to avoid Ubiquiti because I don&#x27;t want to involve a 3-rd party cloud to manage it.But I really appreciate your suggestion, perhaps I am going stick to it as I&#x27;ve seen no alternative so far and my previous experience with Ubiquiti makes me confident it will do the actual job great. reply jamespo 13 minutes agorootparentHow about MikroTik hEX S 5 Port Router ? reply hqsolomo 12 hours agoprevNice and clean, great work!If you don&#x27;t mind me asking, does your energy bill take a huge blow because of this? I had a modest homelab set up and had to start shutting things off due to how much it costs to keep it runningI apologize if I missed this info in the blog! reply monstermunch 8 hours agoparentI have 17kw of solar, so usually very close to $0 or below! reply hqsolomo 8 hours agorootparentHoly smokes, living the dream! Kudos! reply phito 4 hours agoparentprevI had bought an used enterprise server (Dell something), but it just used too much power and made some much noise. It&#x27;s not as professional or reliable as op&#x27;s homelab, but I&#x27;m now hosting most of my stuff on an Intel NUC. It&#x27;s a much better fit for my use case and budget reply momirlan 10 hours agoparentprevenergy bill might go up, but the heating bill goes down ... reply monstermunch 8 hours agorootparentWe don&#x27;t do heating here in Houston. Maybe for the 2 days of Winter reply ihattendorf 6 hours agorootparent> Maybe for the 2 days of WinterThat’s what the generator is for. reply hqsolomo 10 hours agorootparentprev\"Chestnuts roasting on an open server rack\" reply RajT88 10 hours agorootparentprevGreat for the arctic circle. reply dheera 11 hours agoparentprevHaving a rack alone doesn&#x27;t consume anything, it&#x27;s what you put on the rack.I have a rack setup and most of the time it consumes around 200 watts during the daytime and 100 watts at night, but can spike upto 600 watts if I put a heavy CPU+GPU load on it.I also put my desktop into suspend at night, something which I think a lot more people with desktops could do. Don&#x27;t run 24&#x2F;7 services (e.g. Home Assistant) on your massive desktop with an i7&#x2F;i9 and a GPU. Run that stuff on a NUC or Pi4 or anything that has low power consumption. Then turn your desktop on only when you&#x27;re actually using it. reply hqsolomo 11 hours agorootparentI get that but the blog discussed a pretty beefy setup. My whole rack still used less power than my gaming PC at load but after doing the math I ultimately saved more money by going serverless for my apps and dumping (non-critical) data into a B2 bucket on paper. In reality I just started shutting things off and only turn them on when I need them.I&#x27;m curious as to what others are doing to save costs if anything. I love the hobby but we&#x27;re in a recession, lol! reply CTDOCodebases 5 hours agorootparentIf you don’t require ECC RAM a common setup these days is a Proxmox cluster set up on refurbished USFF PCs like the Lenovo m720. They sip power at idle.My personal approach is separate archived storage from working storage and keep archive storage offline until I need it. Keeping hard drives spinning is costly. reply getwiththeprog 10 hours agorootparentprevMiniITX boards are low idle power, 17 watts for idle with a Ryzen 5600 compared to a full size boards 50 watts. I would no longer buy anything larger, even for a gaming machine. reply hqsolomo 9 hours agorootparentAmen on that. The past few rigs I built for myself were all ITX machines. I went through and undervolted my CPU, RAM, and GPU. Once my current gaming rig dies I&#x27;m just going to stick with the consoles for gaming until they goes to shit as well.In an era of abundance we seem to be trying to use more power to support what we already do, rather than rethinking what we do to use less power. It&#x27;s a shame! reply dheera 6 hours agorootparentHonestly I could I could easily solar power all of my compute needs (and probably a lot more) from my apartment&#x27;s roof.But at last, I&#x27;m a renter and my property management isn&#x27;t going to allow that.We&#x27;re literally wasting clean energy because of a bunch of rich turds hogging real estate with shitty rules about what you can&#x27;t do. When my property changed management the new management even banned EV charging on the property and locked up all the outlets in the parking lot. These kind of management are not welcome in Silicon Valley but somehow they are here. reply hqsolomo 5 hours agorootparentIt jumbles my jimmies hearing about outright rejection of modern tech by industries that would best benefit from said tech.I&#x27;d almost be willing to bet money that your landlord would charge a \"renewable energy\" fee even if they&#x27;re just dumping power back to the grid for $$$ even if they had the panels reply dheera 6 hours agorootparentprevI really wish more Mini-ITX boards came out that allowed GPUs to be horizontal so it could fit in a 2U case. Unfortunately making it vertical makes it 4U, negating the size advantages of Mini-ITX. reply hqsolomo 5 hours agorootparentSo I actually \"repurposed\" a riser cable from an old ITX build to slap it into my primary proxmox server to play around with home-rolled VDI. Surprisingly, it worked. If you&#x27;re feeling bold you could try that route replypm2222 11 hours agoparentprevPerhaps there’s solar. reply hqsolomo 11 hours agorootparentThat would definitely be a big help, lol. I also imagine part of my problem is my house wiring- the previous owner had the place reno&#x27;d in &#x27;96 and clearly got the landlord special reply monstermunch 8 hours agorootparentprevCorrect, 17kw of solar on the roof reply riley_dog 7 hours agoprevThis person is pretty average over on &#x2F;r&#x2F;homelab. reply monstermunch 7 hours agoparentI&#x27;ve stopped posting there as much, when my colo post got removed for not being \"in my house\" i knew it was going downhill reply teddyh 4 hours agorootparentIf you think a colo is a “homelab”, then maybe you’ve gone downhill. reply jancsika 8 hours agoprevLooking at this setup makes me think it would be interesting to have a public-facing chat service on a 30-second clock where the user has to pad tweet-sized messages with a pre-determined number of zeros and complete sending them off before the deadline is up:1g lounge: must pad message with 1gig of zeros10g lounge: must pad with 10 gigs of zeros100g lounge: must pad with 100gigs of zeros1t lounge: must pad with 1 terabyte of zeros100t lounge: intelligence community meet-and-greetEdit: relevance :) reply harry8 6 hours agoprevI couldn&#x27;t find it, so many details. What&#x27;s the power draw on all that?A kilowatt? reply asynchronous 5 hours agoparentWay, way too much is the answer. That amount of storage alone has got to kill his electricity bill. reply edent 1 hour agoprevCan someone explain what the multiple GPS receivers are for?I get using one for NTP.I guess a second for redundancy even though it is picking up the same satellites as the first.And the rest? reply defrost 43 minutes agoparentThere are 2x Internet connections + racks - one in a house closet rack, the other in the garage.Garage rack has a NAS that mirrors storage of the closet rack.Garage rack has one Raspberry Pi GPS for time services.Closet rack has two (different) Raspberry Pi GPS for time services.Looking at it that way, if you accept the 2x Internet connections each being independant and acting as cross backups in case of ??? then it&#x27;s just the second GPS on the closet rack that&#x27;s \"extra\".That can likely be written up as a home experiment in messing about with a different Pi setup.It&#x27;s overkill .. but often these home setups are there to keep current on new technology and different ways of doing the same thing .. in that light it&#x27;s not uncommon to have multiple ways of achieving the same end just to gain knowledge on the proscons of different approaches. reply znpy 1 hour agoparentprevOverkill… as the author says in the title. reply thecosas 11 hours agoprev\"This is also how you are probably getting to this blog, which is hosted at home.\"Curious to know how hitting HN impacted your setup (if at all)! reply monstermunch 8 hours agoparentNot much, because a lot of it is cached by CloudFlareTotal bandwidth over the past 5 hours is 253.37 GB. But total cached and served by CloudFlare is 237.82 GB. Uncached is just 15.54 GBHere is CPU load on the VM: https:&#x2F;&#x2F;files.networkprofile.org&#x2F;apps&#x2F;files_sharing&#x2F;publicpr...Networking on the VM: https:&#x2F;&#x2F;files.networkprofile.org&#x2F;apps&#x2F;files_sharing&#x2F;publicpr... reply mhb 9 hours agoprevAre there any building code considerations (e.g., chimney effect for a fire) regarding a multi-floor wiring chase? Isn&#x27;t this why laundry chutes are no longer permitted in some places? reply monstermunch 8 hours agoparentIts a single story home, however it is from 1968, so its possible that in new code its outlawed or something reply gnopgnip 6 hours agoparentprevRiser or plenum rated cables are required by code if you go between floors, except in plenum rated conduit. Conduit is usually more expensive though.Non rated cable more easily catches fire and makes a lot of smoke. Plenum rated cable is about twice the price, but ethernet is pretty cheap for a single home. reply mindslight 9 hours agoparentprevI&#x27;ve also been curious about this in the context of making a dumbwaiter, but not enough to do the research. I would think having passive automatically closing fire rated doors would take care of it, but I don&#x27;t know. reply bombcar 4 hours agorootparentYou could look into it but I think the reason laundry chutes aren&#x27;t made anymore has more to do with kids getting stuck in them and less to do with fire code (after all, stairways are chutes connecting floors). reply TacticalCoder 11 hours agoprev> The generator is a 27kw Generator which powers everything in the house. This means the UPS&#x27;s only need at most 10 seconds of runtime, as that&#x27;s how long it takes for the generator to start and switchThat Generac generator though: it powers everything but only in case there&#x27;s a power outage right?P.S: there&#x27;s a slight typo in TFA: it&#x27;s Mellanox, not Mollonox. reply monstermunch 8 hours agoparentCorrect, only if there is an outageThanks, I&#x27;ll correct that! reply bombcar 4 hours agorootparentDo you regularly switch to the generator to test it? Is that automated? reply Thlom 2 hours agoprev>The Hubitat hub has now almost entirely been replaced by a HomeAssistant virtual machine. The only reason it is here is to connect the Z-Wave and Zigbee devices I have. However, I replace those devices with WiFi versions wherever possibleThere&#x27;s cheap and reliable ZigBee&#x2F;Z-Wave&#x2F;433MHz USB dongled antennas you can replace the Hubitat with. reply ThatPlayer 4 hours agoprevWhat are the plans for the Meshastic mesh? I think having one hooked up to a server to do responses could be interesting. Or have you actually convinced other people to carry one around for messages? reply jolux 12 hours agoprevI just bought a Ubiquiti Dream Machine SE along with a U6 Enterprise a few weeks ago and so far it’s my favorite tech purchase ever. The management interface is years if not decades ahead of everything else I’ve used before.My only complaint is that there’s no public API and thus no official Terraform providers. reply tw04 11 hours agoparentI would take Ruckus Unleashed over unifi all day long. Ubiquiti, unfortunately, feels like everything is constantly in beta, both hardware and software. Wait until they release a UDM SE v2 and abandon firmware on the UDM SE.The UDM in particular is a masterclass in how to upset all your customers. (coming from a previous all-in UBNT customer that had a first gen UDM Pro).Ebay Ruckus + OPNsense and my network has never been more stable and performant. reply lotsofpulp 9 hours agorootparentHow does Aruba InstantOn compare to Ruckus Unleashed? reply syntaxing 11 hours agoparentprevI actually moved away from ubiquiti stuff to OPNsense + TP link AP. The firewalls rules on OPNsense makes a ton more sense and the plugins are pretty awesome. reply LeoPanthera 9 hours agorootparentYou can have both, as I do. I use UniFi switches and APs controlled by a Cloud Key, but my router runs OPNsense because UniFi routers are pretty bad. reply petepete 4 hours agoparentprevI love my Dream Machine too. I bought it about six months ago and it&#x27;s been solid and was really easy for me, a novice with advanced networking, to get going with vlans and firewall rules.I don&#x27;t need this setup at all, like I don&#x27;t need cat6 ports in every room, there&#x27;s definitely an element of it being a hobby. Maybe I just like flashing LEDs.Sonos gear still using STP was an unwanted pain, all part of the fun I guess. reply whalesalad 12 hours agoparentprevThe Unifi software is pretty incredible. I am not using their router though (well... an ER-4 but it does not share the same management mechanism) so I am missing out on a lot of the goodies. reply donutshop 12 hours agorootparentERX here. The EdgeMax line is still rock solid and have incredible value for the price. reply whalesalad 11 hours agorootparentI see absolutely no reason to jump ship, it&#x27;s one of the most solid pieces of infrastructure in my network. reply whalesalad 12 hours agoprevI keep meaning to produce a post like this. My setup is not nearly as pretty though. Love to see the cannabis grow operation exhaust fan, haha. Very well done! reply f-securus 11 hours agoparentI didn’t see a filter on the exhaust fan. He is so thorough I’m sure he thought of it but I think he needs a filter to keep the fan running long term unless he is filtering the air into the room and that is good enough? reply monstermunch 8 hours agorootparentNope, its ALL completely full of cat hair 24&#x2F;7&#x2F;365 reply monstermunch 8 hours agoparentprevI hope you do, we need more like it! reply sillysaurusx 7 hours agorootparentThank you for encouraging others. You’ve earned a loyal fan.And thanks for sharing your knowledge. Infrastructure has been an off and on hobby, but it’s hard to know how to take it to the next level. Or absurd levels (in the best possible way), in your case. reply tdhz77 11 hours agoprevReading this post makes me happy we are in the world of cloud providers, but realize they don’t magically work. People build them. I’m glad I don’t. reply wredue 11 hours agoparentIt’s really not all that complicated. Although I still haven’t figured out if there’s some secret way to properly creating working Ethernet cables.I get blisters on blisters on my finger tips when making lots of cables. reply monstermunch 8 hours agorootparentYour time is too valuable to make ethernet cables. Either punch down to a keystone jack or patch panel, or buy patch cablesGave up on that years ago reply bmitc 11 hours agorootparentprevDo you use a crimper? reply WaitWaitWha 8 hours agoprevDid I miss the logical and physical diagrams?I appreciate your hobby of a well installed home network. Add the diagrams and you will increase your readers&#x27; comprehension several fold. reply monstermunch 8 hours agoparentGood idea, I&#x27;ll work on some reply ListenLinda 7 hours agoprevI knew this wasn&#x27;t an overkill network when I saw an SG300 switch in the first pic. reply monstermunch 6 hours agoparentCall be crazy, but I dig the SG switches reply stn8188 12 hours agoprevWow, and my wife says my network is complicated! :)In all seriousness, thanks for sharing, this is really incredible. I see a few similarities (fellow Harbor Freight shopper, ADS-B receiver)... but I took the mostly lazy way out and just use the TP Link Omada router, controller and access points. Works great for well over 50 wireless clients that we had at a recent BBQ. I particularly love your note about encrypted LoRa networks at the bottom there, I&#x27;ll be interested in a follow-up on that topic. Thanks again! reply monstermunch 8 hours agoparentThanks!I&#x27;ve been very interested in the new TP-Link stuff, they have really come a long way. I don&#x27;t know if I&#x27;ve really ever heard of much wrong with the TP-Link stuff reply applied_heat 5 hours agorootparentI have the tp-link Omada APs with controller running in docker on synology nas and am happy with it reply xpe 12 hours agoparentprevExactly. Those 50 wireless clients must be fed Internet while their 50 subjugated humans eat BBQ and prioritize device interactions over human ones. (I’m exaggerating of course! At least you are having gatherings — better than many of us!) reply meitham 4 hours agoprevThis is beautiful and inspirational! I am about to redo my messy homelab and have two questions: 1. I don’t see any labels attached to the cables, how do you know which-is-what? 2. I am on pfsense too but I was considering a move to OPNSense, would you explain why you choose pfsense over opnsense? Thanks reply rb666 2 hours agoparentStick to Opnsense, it is better software that is better managed with a clean historical sheet. reply deepsun 11 hours agoprevSpeedTest.net tests are useless, as ISPs give higher priority to traffic to known speed checkers.They can legally do that as we don&#x27;t have Net Neutrality anymore. reply pixl97 10 hours agoparentEh kinda... priority traffic shows the maximum potential speed of your link. The max speed of your link has nothing to do with the speed you&#x27;ll get on any particular node on the internet. For example if the ISP has too little peering bandwidth to some other speed test.net host you will still be capped by that link.These days I rarely need large bandwidth to any single host, but have a lot of bandwidth use between many hosts including streaming applications. Bufferbloat is far more apt to be problematic under this workload than ISP priority, unless there is a bandwidth shortage on the local links. reply NoPicklez 9 hours agoparentprevI believe that&#x27;s why Netflix introduced the fast.com speedtest, which runs through Netflix servers. Reducing the likelihood that the test gets shaped. reply judge2020 9 hours agorootparentThe link - https:&#x2F;&#x2F;fast.comCloudflare&#x27;s - https:&#x2F;&#x2F;speed.cloudflare.com reply SushiHippie 9 hours agorootparentfast.com is the only browser based speedtest that shows the accurate results for me. The results from there resembles the download speeds I get when downloading files from different sites or installing upgrades through my package manager.Cloudflare&#x27;s speedtest often performs worse for me.iperf3 - https:&#x2F;&#x2F;iperf.fr Is the most accurate CLI speed test I&#x27;ve found (depending on which server you choose). reply judge2020 7 hours agorootparentWhat&#x27;s \"accurate\" is going to be different for every website &#x2F; destination network anyways. Unless you suspect there&#x27;s some issue with the speedtest software running on the browser &#x2F; target, it&#x27;s showing you the accurate speed you get to Cloudflare or whatever server you&#x27;re trying to pull from. If you just want to see what your ISP is limiting your bandwidth to, sure, you need to find a server that can hit that.The best speed test is to download some large linux ISOs with 100+ peers, so you max out your connection to tons of networks. reply SushiHippie 6 hours agorootparent\"accurate\" is for me the speed I have on average throughout the day no matter what endpoint and not the max througput. If I&#x27;m doing a speedtest, most of the time I just want to know if I&#x27;m now downloading file $x from $y what download duration can I expect.Downloading packages with $PACKAGE_MANAGER, downloading files from $CLOUD_STORAGE, downloading videos from youtube, downloading linux ISOs, receiving files through $INSTANT_MESSENGER, ... they all are very close to the speed I get with fast.com. replynfriedly 10 hours agoparentprevThat&#x27;s the reason fast.com comes from the same servers as Netflix content - it&#x27;s difficult to prioritize one without prioritizing the other. reply bonestamp2 6 hours agoparentprevhttps:&#x2F;&#x2F;speed.cloudflare.com&#x2F;I like this one because your speed really depends on what you&#x27;re downloading and this one tests some of those scenarios. reply cardsofinhuman 11 hours agoparentprevIs this true? Do you have a link? reply RajT88 10 hours agorootparentI used to work for a small ISP. The boss specifically wanted speed tests to be exempted from traffic shaping. reply smegger001 10 hours agorootparentNot surprised. I have long noted that when my my browser is strugeling to load tabs all i have to do is try connecting to speedtest.net for all open tabs to suddenly load and i have decent speed for the next 15-30 minutes. reply hqsolomo 11 hours agorootparentprevNot coming in with proof but I have personally seen this- a buddy and I did some testing and the Ookla tests were the only ones we did that were much higher than the others.I&#x27;m sorry for not contributing with hard evidence but I&#x27;ve seen this behavior (though I never heard about it from anyone else before now) reply godman_8 11 hours agoprevNice setup! I have a very similar Homelab minus the Generac (I regret not getting one before inflation kicked in, especially since I already have LNG to the home.)My only recommendation would be switching your virtualization over to Proxmox (LXC &#x2F; KVM) and setting up an HA cluster with Ceph and MLAG. It&#x27;s relatively easy and free and will give you a lot more features than plain ESXi and even free vSphere&#x2F;vCenter. reply monstermunch 7 hours agoparentThanks!Yeah, the price on this genset I think has gone up around $4000 since I bought it, not including the installI&#x27;ve been meaning to try Proxmox, but my day job heavily relies on ESXi, so its nice having something to mess with at home. I am also running vSphere with an Enterprise licence, so I get all the fancy stuff reply bazmattaz 12 hours agoprevThis is incredible. I still can’t believe some people get 1gb symmetrical in their home. In the UK I’m stuck with 70mbs down and 6mbps up. Pitiful reply stwr 3 hours agoparentI’m in Spain and get 10Gbit symmetric which comes down to about 7000mbps up and down on speedtests to my airbnb (in a small town 30min away from a touristic hotspot) for 30 euro’s. My home in Barcelona gets 1gbit symmetric for 25 euro.Competition here has gotten pretty good with lots of virtual carrier being allowed by law to use telefonicas ftth which has driven prices down.I’m the opposite to you where I’m always surprised when I hear people in fairly big towns&#x2F;cities in US&#x2F;UK etc are still on less then 100mbit symmetric! reply tdfirth 11 hours agoparentprevI&#x27;m in the UK (Oxfordshire) and have a 1gb symmetrical connection at home. It&#x27;s provided by Gigaclear - there&#x27;s a handful of other similar operators that do fibre in more \"rural\" areas. It costs £79 a month, so it&#x27;s not cheap to be honest, but I love it. reply jonathantf2 11 hours agorootparentRing em up and tell them you’re leaving, they’ll drop you down to the new customer pricing. I’ve got the 1G up and down for something like £38 a month now? reply monstermunch 8 hours agoparentprevWhen I bought this house, a requirement was fiber internet. Not dealing with garbage cable! reply PaulKeeble 12 hours agoparentprevShould hopefully change in the next few years, about 54% of houses have fibre to the property and the plan is over the next five years to expand that to nearly 100%. I have had 100&#x2F;1000 for a while now but symmetric is still a rare product that only smaller competing fibre companies are rolling out. reply toast0 11 hours agoparentprevI had 1g symmetric in the bay area (thanks at&t) and it was nice, but am now around your speeds (85m&#x2F;13m) and it&#x27;s clearly worse, but not really terrible. Certainly not terrible enough to pay $50k+ install to get munifiber, even though I&#x27;d enjoy it a lot. Maybe if one of the ISPs on munifiber starts offering 10g to residences. Not that I need it, but it&#x27;d be fun. reply bashy 11 hours agoparentprevMaybe search around. I got YouFibre[0] few months ago and it’s 1000&#x2F;1000.[0] https:&#x2F;&#x2F;www.youfibre.com reply vel0city 8 hours agoparentprevA chunk of AT&T&#x27;s residential fiber actually supports 5Gbit symmetrical for ~$110&#x2F;mo in the US. reply throwaway1777 6 hours agorootparentI’m really hoping they upgrade my area some day but not counting on it. reply rjsw 12 hours agoparentprevI&#x27;m in the UK, getting 500MB symmetrical installed tomorrow, could have ordered 900MB for £2 per month more. reply frankreyes 10 hours agoprevI need a silent ECC server. Doesn&#x27;t need to be super powerful but I&#x27;d rather have at most one fan. Any advice? reply greggyb 10 hours agoparentI&#x27;d start with something like this[0]. The only search term I used was \"xeon-d\". You can easily find other form factors and combinations. This line of CPUs is lower power (the linked one is rated at 45W), so should be trivially cool-able with a very quiet system. They support ECC RAM. You&#x27;re likely to find mostly Mini-ITX and uATX boards, so they will fit in just about any case you want.[0] https:&#x2F;&#x2F;www.ebay.com&#x2F;itm&#x2F;166190039675?epid=17034031881&hash=... reply favorited 9 hours agorootparentI&#x27;ve done this a few times, and the killer has always been the RAM. By the time you&#x27;re buying decommissioned enterprise hardware, it&#x27;s hard (or just expensive) to source RAM from its QVL list - even harder so when you want ECC, which not all of the QVL&#x27;d SKUs will be... And, in my painful experience, if it&#x27;s not QVL RAM there&#x27;s a good chance it just won&#x27;t post. reply bombcar 3 hours agorootparentI&#x27;ve had no problem with getting the RAM (either new or just more eBay stuff) - and even got a replacement mainboard for relatively cheap when one burned up (it was easier to swap the mainboard than reconfigure everything for a new server).The killer is the power consumption. It&#x27;s better now compared to old enterprise gear of 20 years ago, but you can sometimes still pay for a brand new low-power system just in power savings alone.Of course, if you have solar onsite that doesn&#x27;t matter. reply greggyb 7 hours agorootparentprevThe top spec 32GB RAM on the qualified list for the specific board I linked earlier runs $40-$41 on ebay. The same speed and capacity for generic DDR4 runs in the mid $30s. The board has 4 RAM slots. So the premium for QVL RAM for that linked board is $20 over generic if you want to max out at 128GB.Supermicro did have RAM issues in the early teens.This sort of research is the non-dollar price of finding deals on server-class hardware.If you want to get something pre-validated, and usually with a warranty, it can be worth it to look for resellers for the major OEMs that also offer refurbished hardware. It&#x27;s more expensive than the typical options available on eBay, but still much cheaper than new hardware. reply willis936 7 hours agoparentprevI&#x27;ve been wanting to make an X570D4U-2L2T + CS381B build for a year, but haven&#x27;t been able to justify the price.The Latte Panda Sigma looks pretty amazing too if in-band ECC is sufficient. reply hqsolomo 10 hours agoparentprevSome Ryzen chips (I think- might be TR) often support ECC- see if you can find a compatible mobo that supports ECC modules and Ryzen sand! When I last checked there were a few products out there reply frankreyes 10 hours agorootparentI&#x27;ve just checked and there&#x27;s some mobos for $60 with ECC. Is that for real? Supermicro costs like $1000, half that for refurbished. reply hqsolomo 10 hours agorootparentWithout looking at the listing I can&#x27;t earnestly say what you&#x27;re seeing isn&#x27;t fake but I know back when I had the same question you&#x27;re asking here I saw similar offerings. Ultimately I bought a used ITX supermicro mobo+CPU combo unit from eBay for like $200 reply rjsw 7 hours agoparentprevI have a HP MicroServer with ECC RAM. reply monstermunch 8 hours agoparentprevDo you really need ECC? Most people don&#x27;t reply bombcar 3 hours agorootparentMany people got scared by the ZFS&#x2F;ECC howling years ago, and didn&#x27;t really think it through (because they&#x27;re running without unverified backups).https:&#x2F;&#x2F;www.klennet.com&#x2F;notes&#x2F;2022-11-25-zfs-and-ecc-rant.as...IF you can get ECC cheap, get it, otherwise, meh. reply physhster 11 hours agoprev...and I thought mine was overkill! Great job!You might however want to read about the grocer&#x27;s apostrophe: https:&#x2F;&#x2F;www.grammar-monster.com&#x2F;lessons&#x2F;apostrophe_error_wit... reply ojbyrne 10 hours agoparentThat page includes the sentence “A word that ends in a vowel is more likely to attracting this mistake.”I wouldn’t bring it up but… glass houses. reply monstermunch 8 hours agoparentprevI&#x27;ll have to go over the whole post, I&#x27;m terrible at writing! Quite a few errors have been pointed out. But I can just edit them and pretend they never happened reply physhster 6 hours agorootparentBasically, you don&#x27;t want to pluralize acronyms with &#x27;s. The more you know :) reply iAm25626 6 hours agoprevwhat no 100g or IB? Those price point have come to a point where for home lab is bearable. Well we are talking about overkill. If one want to play around with network do it virtually is a much cheaper way to go: https:&#x2F;&#x2F;www.gns3.com&#x2F; reply selectodude 5 hours agoparentI run GNS3 on a three node HA proxmox cluster :) reply hqsolomo 6 hours agoparentprevInfiniband? Don&#x27;t tease me with a good timeJK, wife would kill me reply 65 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author provides a detailed description of their home network setup, including the use of Wireguard VPN and Ripe Atlas Probe.",
      "They discuss their guide on using RIPE Atlas for accurate time synchronization and creating an NTP server using a Raspberry Pi and GPS.",
      "The author mentions various Raspberry Pi models and GPS modules used in their setup, along with ESXi hosts, storage servers, and NVRs. They also talk about implementing backup plans and power upgrades. Other components mentioned include fiber optic cables, a TrueNAS backup server, power monitoring, a TinyPilot device, an antenna for ADS-B data, a weather station, and a WiFi setup.",
      "The author plans for future projects and shares a cautionary story about ignoring battery issues in a UPS."
    ],
    "commentSummary": [
      "The conversation revolves around home networking and infrastructure, discussing power consumption, environmental impact, software options, and internet connectivity.",
      "Participants have different preferences, with some advocating for cloud storage and virtualized instances for efficiency, while others enjoy building and maintaining their own home networks.",
      "Topics covered include the benefits of hosting locally, concerns about the obsolescence of cloud-based technology, the use of low-power devices and solar power, bandwidth usage, and server hardware considerations. The overall emphasis is on the importance of researching and finding the right setup for individual needs."
    ],
    "points": 398,
    "commentCount": 309,
    "retryCount": 0,
    "time": 1691701520
  },
  {
    "id": 37079534,
    "title": "eSignature Beta for Google Docs and Google Drive",
    "originLink": "https://workspaceupdates.googleblog.com/2023/08/esignature-google-docs-google-drive.html",
    "originBody": "Updates This official feed from the Google Workspace team provides essential information about new features and improvements for Google Workspace customers. Introducing eSignature for Google Docs and Google Drive: launching to open beta for Workspace Individual subscribers, launching to beta for Google Workspace customers Wednesday, August 9, 2023 What’s changing In June 2022, we began alpha testing the ability to request and capture eSignatures in Google Docs. Based on the feedback we received, we’re ready to move this feature to the next level: eSignature is now available as an open beta for Google Workspace Individual subscribers — no additional sign-up is required to use the feature. eSignature will be available in beta for select Google Workspace customers — see the “Additional details” section below for more information. eSignature in Google Drive eSignature in Google Docs Who’s impacted Admins and end users Why you’d use it For solopreneurs and small businesses, keeping track of contracts, customer agreements, and other binding documents can be challenging. To help streamline this workflow, we’re natively integrating eSignature in Google Docs, allowing you to request and add Signatures to official contracts, directly in Google Docs. eSignature makes it easier to: Quickly request signatures, see the status of pending signatures, and find completed contracts. Sign an official contract right from Google Drive without having to switch apps or tabs. Create a new copy of the contract for each request so that you can use your document as a template and initiate multiple eSignatures requests. Additional details Later this year, we will introduce support for the following new eSignature capabilities: Audit trail: all completed contracts will automatically contain an audit trail report. Multi-signer: the ability to request a signature from more than one user. Non-Gmail users: the ability to request an eSignature from non-Gmail users Initiating eSignature on PDF: the ability to initiate an eSignature on PDF files stored in Drive Beta availability for Google Workspace customers Select Google Workspace editions (see the “Availability” section below) can apply to beta test eSignature using this form. This feature will be available as part of a larger beta, which includes access to new custom email layouts in Gmail. These new email layouts allow users to customize existing templates, reuse a custom layout in multiple email campaigns, or create a brand new layout from scratch. Once you sign up for the beta you will see the eSignature and new Gmail features in the coming weeks. Getting started Admins: Eligible Google Workspace admins can sign up for the beta using this form. Workspace Individual subscribers: Visit the Help Center to learn more about sending signature requests & sign documents with eSignature. Rollout pace eSignature for Workspace Individual users Gradual rollout (up to 15 days for feature visibility) starting on August 8, 2023 eSignature beta for Workspace customers: We will be accepting beta applications and allowlisting customers over the next several weeks. Availability Available to Google Workspace individual subscribers Eligible for beta: Google Workspace Business Standard, Business Plus, Enterprise Starter, Enterprise Standard, Enterprise Plus, Enterprise Essentials, Enterprise Essentials Plus, Education Plus, and Nonprofits customers Resources Beta Application Google Help: Send signature requests & sign documents with eSignature The Keyword: New Google Workspace features to help solo business owners Labels: Beta , Google Docs , Google Drive , Other    Filter by product   Filter by date  Subscribe by feed Subscribe by email Localized Google Workspace Updates Español Français 日本語 Português Useful Links Join the official community for Google Workspace administrators In the Google Cloud Community, connect with Googlers and other Google Workspace admins like yourself. Participate in product discussions, check out the Community Articles, and learn tips and tricks that will make your work and life easier. Be the first to know what's happening with Google Workspace. ______________ Learn about more Google Workspace launches On the “What’s new in Google Workspace?” Help Center page, learn about new products and features launching in Google Workspace, including smaller changes that haven’t been announced on the Google Workspace Updates blog. ______________ Google Privacy Terms",
    "commentLink": "https://news.ycombinator.com/item?id=37079534",
    "commentBody": "eSignature Beta for Google Docs and Google DriveHacker NewspastlogineSignature Beta for Google Docs and Google Drive (googleblog.com) 394 points by rc00 16 hours ago| hidepastfavorite158 comments danielrhodes 15 hours agoFirst this is a great idea. Finally there is some movement on making Google Docs better.I haven&#x27;t used the feature, but from reading the blog post it is sad to see how poorly Google executes on products, even at the beta stage.Some context:I&#x27;ve built such a product before and eSigs are more simple than they seem as long as you do a few things. 1) You need to verify somebody&#x27;s identity. Sending them an email with the request is a valid way of doing this. 2) You need to make the final signed document easily accessible (e.g. emailing all parties a copy of the signed document. 3) You need to not obscure what the person is signing (they need to be able to easily see the entire document if they wish) 4) You need to make it possible for all parties to verify the validity of a signature, which is done with an audit trail appended to the back of the document. 5) Some types of contracts are not valid to be signed with eSign. 6) The parties need to agree to do business electronically, but this is mostly up to them.You do not need to create fake wet signatures, cryptographically sign a document, or do encryption beyond what is necessary for normal compliance. Those are all UX or marketing features: they don&#x27;t hurt, just that according to experts I have spoken to, they aren&#x27;t a factor when going to court.And then we have what Google is offering:From the screenshots, I think it is only a fake wet signature. I&#x27;m not sure how valid that is.Apparently you cannot ask for an eSignature from non-Gmail users right now. But how are you supposed to know they are Gmail users? Can Google not send people a simple email with a link? This alone makes the feature almost worthless, and for something that seems so trivial.They also said they will only be adding an audit trail later this year. This is really sketchy because I believe it means you, nor anybody else can actually verify if it has been signed. Again, this is quite trivial: you store the audit trail in a database, and you append the log as pages onto the back of the PDF document. reply crazygringo 12 hours agoparent> Finally there is some movement on making Google Docs better.I just find these comments to be so strange. You can look at all of the articles in that blog to see all of the feature improvements that have launched in Docs over the past 2 years. It&#x27;s a lot of stuff.Maybe they&#x27;re not features that matter to you, some are available only to paid customers as opposed to free tier, or maybe you just haven&#x27;t bothered to even notice. But they&#x27;re there.It&#x27;s nothing about Google specifically -- I see people make these comments about so much software, where they assume a project is or has been dead, just because they can&#x27;t even be bothered to look at the changelog. It baffles me.It&#x27;s like, unless it&#x27;s a radical total UX overhaul, people don&#x27;t notice the work developers are putting in on actual features. And if it is a radical total UX overhaul, people complain about the change because they assume it&#x27;s superficial rather than actual features. reply puppymaster 5 hours agorootparentAs a daily Google Sheet users, I am thankful and amazed by the monthly, or even daily improvement of the product for the past 5 years. Folks will be surprised how many hedge funds and quants use them for quick idea sketch reply pie420 3 hours agorootparenthedge funds and quants also use pen and paper for quick idea sketch. reply candiddevmike 12 hours agorootparentprevYou&#x27;ve both summed up everything wrong with design churn and also justified it. Truly a cursed comment. reply rcme 8 hours agorootparentprevI don&#x27;t know. From what I&#x27;ve seen, a lot software gets built by a small group of people. It becomes wildly popular and the is maintained by a behemoth churning out minor features at a snails pace while everyone frets about the even the smallest product changes.It&#x27;s easy to experiment and build when your product is small and few people use it. When you&#x27;re bringing in real money and most of the Fortune 500 is using your software, it&#x27;s a lot harder to add anything meaningful for fear of rocking the boat. And note, this isn&#x27;t necessarily a bad thing. If the business is doing well, there might not really be a need for many new features. reply TeMPOraL 4 hours agorootparentAnd yet Microsoft manages to make regular, significant feature additions (and occasional, less nice&#x2F;useful large changes), despite being a critical software dependency for pretty much all of the Fortune 500. reply jppittma 8 hours agorootparentprevWell, yeah, because the risk&#x2F;reward of the decision making changes, purely in terms of dollars&#x2F;second for an outage, never-mind legal and reputation risk. reply josephg 5 hours agorootparentAlso, the value of features follows a power law. Some features are incredibly valuable to everyone. Some features only a few people care about. And then there’s a long tail.In a product as old as Google docs, they honestly have all the important features that most people actually care about already. You can save. You can track changes. You can set up custom styles. Or embed images. You can edit docs collaboratively. Or programmatically via their api. Documents look the same on basically everyone’s computer. And they can be in shared folders for teams.At some point every product runs out of high value, visible features to add that anyone cares about. This isn’t a failure mode. It’s the opposite. This is the final state that most successful software should aspire to reach.I have a lot more respect for teams that understand this, and let their products find their UX steady state. Fastmail. Git. Vim. WhatsApp. And yes, Google docs. reply remus 4 hours agorootparent> In a product as old as Google docs, they honestly have all the important features that most people actually care about already. You can save. You can track changes. You can set up custom styles. Or embed images. You can edit docs collaboratively. Or programmatically via their api. Documents look the same on basically everyone’s computer. And they can be in shared folders for teams.I think this is pretty interesting, because I don&#x27;t necessarily agree. If we were starting from a blank slate in a world where a spreadsheet hadn&#x27;t existed before I suspect the perfect spreadsheet software would look pretty different to Google sheets or excel, but because this software has been around for a long time they&#x27;re part of the standard interfaces for computers, like a mouse or keyboard. If you deviate too far from the current design people get confused and default back to what they know i.e. excel with all it&#x27;s quirks. reply jupp0r 8 hours agorootparentprevI looked through these posts for the last three years and there are less than five features that actually pertain Google Docs. reply realprimoh 7 hours agorootparentIf you use Google Docs daily and even bother to look at the menus, the amount of changes added just the past 2 years is pretty massive. The reason they&#x27;re in menus is because the dumbest thing Google can do is kill the native user experience.Just look at how slow Notion is and how snappy Docs is. Yet, a lot of Notion features are now in Google Docs. reply ulfw 7 hours agorootparentprevI&#x27;ve seen it regress a lot in the last two years.I have a Google Spreadsheet that I used to track investments I have in various markets. Three months ago Google stopped showing any stock quotes for ETFs in Europe. No explanation, just &#x27;f u, we showed them for years now all your spreadsheet breaks&#x27;. I had to help myself with getting some quotes from Yahoo News via script but it&#x27;s half-baked.[1]Google doesn&#x27;t care about it&#x27;s products and anyone who thinks different hasn&#x27;t opened their eyes.Btw their lack of quality and care is not new for Google Docs&#x2F;Spreadsheets [2] Issues keep popping up but are usually fixed after a while. This time they just stopped bothering.I wish Google had Product leaders who actually cared about the quality of their products rather than just launching new features that make a splash and then slowly wither on a vine. Fewer features I could actually rely on would go a long way.[1] https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;googlesheets&#x2F;comments&#x2F;13i2gf6&#x2F;googl...[2] https:&#x2F;&#x2F;issuetracker.google.com&#x2F;issues&#x2F;76403135?pli=1 reply flappyeagle 10 hours agorootparentprev> But they&#x27;re there.They&#x27;re useless. I&#x27;ve been paying for Google Docs for over a decade. Nothing of note has been added until they finally... FINALLY let me get rid of the concept of \"pages\" recently.If you have to look at a changelog to understand what changed, then the stuff that&#x27;s being shipped isn&#x27;t making a difference. reply noodlesUK 14 hours agoparentprevIn Europe, including the UK and EEA, there is the eIDAS regulation, which outlines the requirement for eSignatures (amongst other things).In order for a signature to be recognised, it has to meet one of three trust levels1. Electronic signature: this is basically something that puts a distinguishing mark on a file.2. Advanced electronic signature: these use cryptography according to the specifications set out in the regulation, but anyone can make them. There are some requirements about how a person’s identity is linked to their signature.3. Qualified electronic signature: these are advanced electronic signatures which have been produced with a recognised trust service provider. Each country has a list of trust service providers, and each other country mutually recognises their trust lists.It is a bit more detailed than that, but in general, simple transactions can pretty much use any electronic signature (but this varies from jurisdiction to jurisdiction. The uk generally doesn’t need anything fancier than docusign). In order for something to be completely watertight, it needs to be a qualifying electronic signature.In order for these features to be useful in Europe, google will need to meet the requirements of the regulations, and specify a trust level. reply londons_explore 14 hours agorootparentThis isn&#x27;t the case for most agreements. Most agreements have no &#x27;form&#x27; requirements - ie. you could write it on a napkin at a restaurant. As long as both parties believe they are making a legally binding agreement, then they have made a legally binding agreement.Use of technology doesn&#x27;t change that. The only time a court would throw out a signature made online is if the online platform was somehow deceiving the parties - for example by showing different text to each side when they click &#x27;i agree&#x27;. reply galvin 9 hours agorootparentThe issue isn&#x27;t necessarily the form of the agreement, it&#x27;s verifying the identity of the signatories.If you can&#x27;t see the other person signing the agreement, you need to verify that the right person is signing it. It&#x27;s much the same issues that exist with simple passwords versus 2FA; shared accounts, piracy, identity theft, etc.eIDAS is a set of standards for verifying identity that is backed by law. It allows businesses to follow protocols (some similar to 2FA) with the assurance that it will be held up in court. reply danielrhodes 14 hours agorootparentprevUltimately where this matters is if one party contests the validity of a contract. I&#x27;m not sure how it works in the EU, but in the US that would mean a court would ultimately make that decision. To that end, they could decide that two people agreeing in an email thread is enough. It&#x27;s all about your risk appetite.However, notice that Google has not said they are compliant with any regulation. reply pkaye 13 hours agorootparentApparently in Canada a thumbs up emoji is valid as a signature for a contract.https:&#x2F;&#x2F;www.theguardian.com&#x2F;world&#x2F;2023&#x2F;jul&#x2F;06&#x2F;canada-judge-t... reply bbarnett 13 hours agorootparentNot a signature, just that he accepted the deal. And note that he had accepted deals the same way before.Accepting the terms, just as a handshake, on a deal, can bind one. reply angry_octet 11 hours agorootparentIt’s more than a handshake deal — they are making their mark on a contract. This stems from the time before wide literacy and the ability to write, where people would make an X mark on the paper. Sometimes neither party could not read the actual contract, which was written by scribes. So the emoji is a mark.https:&#x2F;&#x2F;www.swanngalleries.com&#x2F;news&#x2F;autographs&#x2F;2023&#x2F;01&#x2F;the-s... reply bbarnett 8 hours agorootparentThat was not the court decision. The newspapers are giving snippets of the judge&#x27;s decision, out of context.https:&#x2F;&#x2F;nationalpost.com&#x2F;news&#x2F;canada&#x2F;farmer-ordered-to-pay-a...The nuance here us that:- the emoji was bring used to reply with an OK- this was used in the past on other contracts between the twoNote that judge looked into the meaning of the emoji, looked into how it was interpreted before, and said \"but nevertheless under these circumstances this was a valid way to convey the two purposes of a ‘signature,”‘ Keene wrote in his decision.\"Note purposes of a signature, not signature. FYI, making a mark is still legal, judges know it, yet that language was not used.And you don&#x27;t look into how a emoji was used by other people, if it is being used as a uniquoe \"mark\" aka \"signature\" aka \"trade mark\".Because a mark, a signature is unique. An emoji is as un-unique as a word such as YES.This is also why someone cannot trademark a colour, or a generic font. Trademarks aka marks aka signatures mist be unique.In this case, the decision was \"he accepted the deal\", just as a handshake or verbal OK. reply viscountchocula 13 hours agorootparentprevNot is: can be reply josefx 11 hours agorootparentprevAs far as I understand it is mostly relevant when the electronic signature is already explicitly required beforehand. For example some government projects might require a valid electronic signature on offers among a million other things. In this case the electronic signature isn&#x27;t important because it is a signature, but because it is a requirement and missing any requirement can get your offer and even already signed contracts thrown out if a competitor catches wind of it. reply mistrial9 13 hours agorootparentprevwhat is the legal name for \"monopoly company requires ID and location to view common document\" ? reply angry_octet 11 hours agorootparentCapitalism. reply jsnell 14 hours agoparentprevThe article links to the help page for the feature: https:&#x2F;&#x2F;support.google.com&#x2F;docs&#x2F;answer&#x2F;12315692. It seems likely that it&#x27;d be a lot better at answering your questions than looking at screenshots.> They also said they will only be adding an audit trail later this year. This is really sketchy because I believe it means you, nor anybody else can actually verify if it has been signed.From the help page it seems obvious that both the sender and signers are able to check whether the contract has been signed. \"1. Open the respective PDF file in Drive or through the link in the email notification. 2. Click View details in the upper right corner of the PDF to open the right side panel and view eSignature details.\"Are you saying that the only possible valid implementation of an audit log is one appended to the contract pdf?> Can Google not send people a simple email with a link?Obviously they could. But equally obviously from your description, this is not a feature where sending one email with one link is sufficient. Based on the help page the final signed contract is \"saved in your Drive\", an operation that&#x27;s not meaningful for a random email address that won&#x27;t have an associated Google Drive. It seems likely that this is a feature that would be blocked on e.g. embedding the audit log in the pdf as per the discussion above. reply hn_throwaway_99 12 hours agorootparent> > Can Google not send people a simple email with a link?> Obviously they could. But equally obviously from your description, this is not a feature where sending one email with one link is sufficient. Based on the help page the final signed contract is \"saved in your Drive\", an operation that&#x27;s not meaningful for a random email address that won&#x27;t have an associated Google Drive. It seems likely that this is a feature that would be blocked on e.g. embedding the audit log in the pdf as per the discussion above.None of this really matters. I totally agree with the parent comment: having an e-signature product that is only usable by other Gmail users (and of course their is really no way to know if any particular email address is a Gmail user) makes it useless. What, so if I need some docs signed I&#x27;ll get half of them signed with DocuSign and the other half with Docs? Of course not, I&#x27;ll just use the product that works with anyone.Google used to have this same \"can only share docs with other Drive users\" feature, though it&#x27;s improved somewhat. In general I think the enterprise doc sharing features are so bad in Drive that the only way I can wrap my head around it is to think that Google is scared of antitrust concerns if they too closely tried to emulate features from the likes of Dropbox, Box and others. reply nextos 14 hours agoparentprev> Finally there is some movement on making Google Docs better.Recently, they have also improved the UI on Google Docs and Gmail making it much less laggy.On my old NUC computer, it makes a big difference.It seems that middle-management is becoming a bit less sclerotic. reply ec109685 12 hours agorootparentChrome improvements might be helping there too. reply nextos 12 hours agorootparentMaybe, but this is something I experienced on Firefox ESR. reply ct520 14 hours agoparentprevAs someone that consults in this space I more or less agree with you. Requirements are derived from interpretation of UETA and ESIGN Act. Itext has a good write up and great library to leverage when taking on project like this. Docusign has a good writeup on esign&#x2F;UETA. ESRA is the goto body on the subject, and if you need legal opinion DLA Piper is the goto in the industry. This stuff is fairly simple once you know it. reply Duwensatzaj 6 hours agoparentprev> You do not need to create fake wet signaturesI’ve seen US Fed agencies reject signed documents when the signature is typed out, so the need exists. reply radium3d 6 hours agoparentprevAck, are you sure \"some movement on making Google Docs better\"... Google Docs is miles better than o365&#x27;s alternatives. Holy cow is Microsoft&#x27;s attempt laggy, bloated and a pain to have to use. reply Galanwe 10 hours agoparentprev> And then we have what Google is offering:> From the screenshots, I think it is only a fake wet signature. I&#x27;m not sure how valid that isWelcome to 2023! I&#x27;ve bought a house, incorporated a company, signed a work contract and contracted a lawyer just based on Adobe mobile wet signatures on PDFs. reply dudus 9 hours agorootparent2023? All my signatures at least in the last decade were fake wet signatures. All leasing docs, employment docs, and banks...I use the Preview App that comes standard on OSX. You scan a signature on a piece of paper with your webcam. Then you can just \"paste it\" anywhere, I even give it a dark blue shade.I had a single person give me shit and say they needed \"ink on paper\" insisting me to print&#x2F;sign&#x2F;scan. I just ran the PDF file through a website that makes it look like it was scanned with some imperfections and tilt. I wouldn&#x27;t be able to tell it myself. reply blitzar 3 hours agorootparentI do the same - if anyone asks, I tell them it is a scanned copy.I honestly did look at conformed signatures etc with a desire to do things \"properly\". However, all too often it was a problem - so fake signature it is.I do one extra step and \"print to pdf\" so that signature and annotations (if I have typed in content on a form) are flattened in the final pdf. reply kccqzy 9 hours agorootparentprev> I just ran the PDF file through a website that makes it look like it was scanned with some imperfections and tilt.I don&#x27;t trust any websites for that. Instead I display the document on the laptop monitor where the page has a white background but everything beyond the page is black. Then I use my phone&#x27;s builtin scanner to \"scan\" my laptop screen. reply haldujai 9 hours agorootparentprevSame here. I don&#x27;t think I&#x27;ve signed anything by hand other than a restaurant bill or the occasional time I&#x27;m handed a document in person within the last 8 years or so.Adding to your list, even government documents as well as those related to my medical practice and licensing&#x2F;registration stating \"electronic signatures are not accepted\" have taken the wet signature from Apple.Presumably this is all valid? reply nulbyte 10 hours agoparentprev> From the screenshots, I think it is only a fake wet signature. I&#x27;m not sure how valid that is.This seems perfectly acceptable in my jurisdiction. Then again, Kentucky is pretty laid back. By statute, an electronic signature is just a mark made electronically that is intended by the signer to be a signature. That&#x27;s about it. Same as if I were to sign by hand, but electronic.Call me naive, but these laws that make electronic signatures some complicated thing seem messy to me. It&#x27;s a signature. If you want to verify ID, then do that, but don&#x27;t conflate the two. reply andsoitis 8 hours agoparentprev> Finally there is some movement on making Google Docs better.Google Docs have been improving a LOT over the last year or so. reply whitej125 14 hours agoprevWe use DocuSign here and the amount of time I spend hopping back and forth (exporting and importing) between Google and DocuSign is annoyingly high. If Google were to enter this space... I would personally welcome it. reply princevegeta89 13 hours agoparentPersonally I found the entire design and the organization of files&#x2F;docs&#x2F;sheets between Google Drive and other apps in the Google ecosystem is confusing as hell. There was no way to follow a specific hierarchy and the commenting&#x2F;reviewing system felt clunky.Wish I never had to use this Google suite of products if it wasn&#x27;t for my employer. reply dudus 9 hours agorootparentMaybe you should give it another look. Hierarchy eas terrible because it was originally designed to have tags instead of folders and these were prone for cyclic references. When they implemented folders they implemented them on top of tags and things never worked out great.It got a lot simpler since they nuked tags in favor of folders. reply candiddevmike 12 hours agorootparentprevYou would hate Microsoft 365 then. I find Google Drive extremely clean and easy to use. reply baby 2 hours agoparentprevI think this is the death of docusign right here that we’re witnessing reply Varqu 2 hours agorootparentYeah, unless Google kills this product in 2 years, which has a quite high probability. reply tootie 9 hours agoparentprevI think if any enterprise reevaluated their e-signature needs they&#x27;d decide they don&#x27;t need it at all. There&#x27;s just better ways to track consents and contracts. reply calderwoodra 6 hours agorootparentCould you elaborate on this? I&#x27;d love to stop collecting signatures, but I can&#x27;t imagine an alternative. reply johnfonesca 44 minutes agoprevWe are developing a digital signatures solution, https:&#x2F;&#x2F;bulksign.com which also has a on-premise version (in addition to SAAS). In Europe, at least, we see an increase in interest for customers having the solution installed on their own servers for security reasons. So more people become aware that sending your confidential documents to a 3rd party is not that great security wise. reply gauravphoenix 15 hours agoprevThis isn&#x27;t a DocuSign killer yet and won&#x27;t be for a while until Google get its enterprise story right. DocuSign is deeply integrated with third party workflows in large organizations. That&#x27;s where the big bucks are and you can build a sticky business. reply danpalmer 13 hours agoparentOn the one hand this is true, however there&#x27;s another side to this that makes it quite compelling.IT sign-off and buying new IT services is hard in many large businesses. This means that it&#x27;s almost always worth using a built-in feature of software you already have, than going for external software. Add to this the fact that companies almost never have one company account, each team re-buys the same software because services are hard, and the fact that most companies already use DocuSign matters a bit less.Much of Slack&#x27;s growth has been built on this[^1]. You can sign up for a free account and start inviting your immediate team. Paying for a small team is well under the typical auto-approved expenses limit, and when you&#x27;ve sunk your cost, got a bunch of people onboard, and proved the value, only then do you go to IT to get it rolled out across the business.Individuals using this sort of thing, termed \"shadow IT\", is a good way to grow usage. This feature of docs seems well positioned to grow that way even if DocuSign is available.[^1]: I worked on a team that was required by the business to use MS Teams. While we waited for the Teams workspace to be provisioned and all the authentication crap to be sorted out we signed up for Slack, invited everyone, and got work done for the 6 months it took to get a working Teams workspace. reply hackernewds 7 hours agoparentprevI would counterargue that the people who do use it (such as myself) find it a pain to switch between interfaces and will campaign to get DocuSign replaced. If switching over is less painful than that of using DocuSign, we will switch over. Most will put their daily workload above what&#x27;s better for the company financially reply drewda 13 hours agoparentprevYeah, I&#x27;ve been surprised to learn how DocuSign is used by some large corporations and government agencies. It&#x27;s the system that manages all sorts of forms and workflows within the orgs, not just contracts going outside the org. reply pie420 3 hours agoparentprevGoogle won&#x27;t release an \"X-killer\" until Google is broken up by anti-monopoly regulators. Until then, google will let millions of dollars of easy money go by in order to avoid giving regulators examples of google using it&#x27;s control of the internet to replicate products and push the incumbent companies out of business. This is why all google features are half-baked and killed. Fear of regulators. reply tootie 12 hours agoparentprevDocuSign shmocusign. Digital signatures is the most bizarrely anachronistic technology on the Internet. Even credit card vendors gave up on signatures. reply krona 14 hours agoparentprevThird party workflows like... Google Docs? reply aodin 14 hours agorootparentLike Salesforce reply ptman 3 hours agoprevThere are alternatives: https:&#x2F;&#x2F;github.com&#x2F;docusealco&#x2F;docusealI would like esignatures to be backed by strong electronic authentication, like EIDAS reply endisneigh 15 hours agoprevMakes sense. For Google implementing this is probably regulatory mainly, not technical. Given how expensive alternatives are, this will be a boon for small contractual actions, though I doubt any huge enterprises will (immediately) switch.Major limitation for now though is:> Non-Gmail users: the ability to request an eSignature from non-Gmail usersHopefully it&#x27;s addressed sooner (September&#x2F;October) rather than later (November&#x2F;December). reply j_san 15 hours agoprevIs this targeted for the US market or also the EU? Does this qualify as a an advanced electronic signature (AdES) under the eIDAS regulation? reply hbaum 15 hours agoparentAuthor of open-pdf-sign here. AdES alone is difficult to do without proper signatures and user verification. Besides that, if Google would go for advanced electronic signatures, I&#x27;d expect it showing up in the EU Trusted List, which it isn&#x27;t. So unless Google is not utilizing their own Google Trust Services certificate authority, I&#x27;d say it&#x27;s unlikely that they will launch with AdES that are compatible with eIDAS. reply rvnx 15 hours agoparentprevNo it doesn&#x27;t, not considered as a trusted (certified) provider and doesn&#x27;t meet the level for secure user authentication.It&#x27;s like a gadget in Europe then.But still, it is useful.It can be used if you want to ask your daughter to promise to \"Get good grades at school\" in exchange for an extra Christmas gift, for example.And make it look like official.It&#x27;s like pretending to be signing. reply TeMPOraL 12 hours agorootparentIn other words: is strictly less useful than a \"fake analog signature\" script that uses imagemagick to paste a PNG&#x2F;SVG with a signature (or a random one from a directory of signatures) on the last page of the document, and then to apply some or all of random {sub-2deg rotation, tiny gaussian blur, tiny non-linear transform, color threshold, strong desaturation}, to make it seem like the document was printed out, signed, and scanned back. reply blitzar 2 hours agorootparentI skip the script and (if they bother asking) tell people I have a really good scanner, a really good pen and am incredibly consistent with my signature even when I sign totally different sizes. reply zx2c4 6 hours agorootparentprevSounds like you have a handy script to paste here... :) reply danpalmer 15 hours agoparentprevI don’t have any inside info here, but anecdotally, Google seems good at meeting regulatory requirements. I wouldn’t be surprised if it does meet this if it’s available in the EU. reply plumeria 15 hours agorootparentMany Latinamerican countries also use digital signatures (e.g PAdES). I wonder if they support this? LATAM always seems to be forgotten by big tech. reply bombolo 11 hours agorootparentprevBeing available doesn&#x27;t mean it&#x27;s considered legally valid. I can sign with the private key on my national id… reply notfried 16 hours agoprev14 months in testing, followed by a beta, is really moving too slow. reply danpalmer 15 hours agoparentThe alternative is launching too soon, getting it wrong, and being accused of killing everything. reply julianozen 7 hours agorootparentIs that the only alternative? reply crazygringo 12 hours agoparentprevNot when it&#x27;s a legal feature designed for enterprises.A partner company willing to test it is going to take 3 months to review and choose to adopt it, another 3 months trying to convert a few internal flows, and another 6 months to observe how it changes processes and whether it&#x27;s an improvement or not.The point here is to get it right, not to get it quick. Enterprise software is the literal opposite of \"move fast and break things\". reply rvnx 15 hours agoparentprev14 months in testing, 0 support of eIDAS, the main electronic signature regulation and platform in Europe. reply btown 13 hours agorootparentGiven that there are entire research papers titled things like \"Analysing the impact of the GDPR on eIDAS\" I&#x27;m not surprised that they launched without support... reply summerlight 15 hours agoparentprevThis is normal for Google, especially when some legal matters are involved. reply benhurmarcel 2 hours agoparentprevWhat&#x27;s the rush for this? reply pie420 3 hours agoparentprevThis is an intentionally crippled product. Google can&#x27;t afford to give regulators a reason to go after them for a couple billion dollars. Google or Microsoft could easily have created a DocuSign competitor and killed DocuSign, like they killed netscape, wordperfect, etc. Now that they respectively control the internet and the enterprise software suite, they have one primary goal: protect and grow the cash cow. Everything else is a marketing exercise or a concessionary project given to keep busy the tens of thousands of computer scientists that they are paying to keep out of competitors&#x2F;startups payrolls. reply crooked-v 15 hours agoprevThe important question is, when is Google going to kill this functionality? DocuSign only has to hold out until then. reply hedora 15 hours agoparentI wonder what happens to documents that were signed with this thing after they kill the product.Also, I wonder if the person signing the document has to agree to Google&#x27;s entire ToS in order to sign the document.Congress could fix both issues with some well-thought-out legislation:- Signatures from these things have to support external validation via standard tools (e.g., Google uses PGP or whatever to sign the signature + document + metadata).- If the act of accessing or signing a contract implicitly incorporates other contracts, then either (a) the signature is non-binding of (b) the incorporated contracts are rendered unenforceable, regardless of whether they were agreed to via other means. reply booleandilemma 12 hours agorootparentDocuSign should start working on their \"Import from Google Docs\" feature. reply HatchedLake721 10 hours agorootparentThey can even run a marketing&#x2F;TV campaign around this, lol.“Google Docs ships with esignatures now… but how long until Google kills another one of their products? Stay safe with DocuSign, import your Google Docs signatures and keep them forever, with or without Google.” reply jbverschoor 13 hours agorootparentprevIt&#x27;s funny, this is exactly what I was wondering. reply geodel 14 hours agoparentprevReminds me of Keyenes : Markets can remain irrational longer than you can remain solvent reply duringmath 15 hours agoparentprevYeah wouldn&#x27;t want it to go the way of Google Toolbar, that thing would definitely be relevant today. reply krasin 14 hours agorootparent> Yeah wouldn&#x27;t want it to go the way of Google Toolbar, that thing would&#x27;ve definitely been relevant today.Hm... I briefly was a part of the Google Toolbar team back in 2008. How would it be relevant today? All of the features that I remember, are now a part of the browser itself (whether it&#x27;s Firefox, Safari, Edge or Chrome&#x2F;Chromium).That said, the eSignature for Google Docs feature would definitely benefit from some strong (preferably, legal & irrevocable) commitment to keep it alive for 40+ years or more. Otherwise, I fail to see how it&#x27;s useful. reply duringmath 14 hours agorootparentExactly. I was pointing out that not every app&#x2F;service has to be preserved and supported forever. reply codetrotter 14 hours agorootparent> Anyway, just thought that whipping out \"Google kills products\" meme for a mere feature was a bit much.I know I’m beating a dead horse^W^W^W dead Google products but, they really do kill a lot of popular productshttps:&#x2F;&#x2F;killedbygoogle.com&#x2F;That being said, if they add eSig for Google Docs I agree; I don’t see why nor how they’d kill that individual feature unless they kill Docs all-together. Which, hopefully, they won’t do for many years yet. reply delecti 14 hours agorootparentA great many of their killed products ended up being merged&#x2F;evolved into other ones (Duo into Meet). A great many more weren&#x27;t even \"killed\"; over the past 20 years their in-gmail chat program has evolved from Talk to Hangouts, and then from Hangouts to Chat. It has kept the same chat history throughout the past 20 years, but \"killed by google\" lists Talk and Hangouts as two killed products. reply TeMPOraL 12 hours agorootparent> It has kept the same chat history throughout the past 20 yearsSo how come I can&#x27;t access chats from before mid-2013? Not even by Google Takeout?(It so happens that some of the most important conversations in my life happened in 2012, so I&#x27;m to date super annoyed that, for no good reason, 2013 is some kind of cut-off date.) reply delecti 12 hours agorootparentYou seem to be right. I&#x27;m pretty confident in my memory that the history persisted from Talk to Hangouts, but the Talk history may not have persisted again from Hangouts to Chat. That would line up with your data, as the Talk to Hangouts switch&#x2F;rename happened in 2013. reply dvngnt_ 12 hours agorootparentprevbut the other part is even when they replace services the new one lacks the same features or ux that made the original special.inbox vs gmail play music vs youtube music reply delecti 11 hours agorootparentThey certainly kill services, my point is the \"Google Graveyard\" is very overblown. reply duringmath 14 hours agorootparentprevSome popular products,sure.Much of the listed \"casualties\" were throwaway wrapper apps that have perfectly fine webapp replacements, apps for platforms that no longer exist, duplicates, and stand alone apps that are now features of bigger apps etc. replyrenewiltord 14 hours agoparentprevHaha, I asked ChatGPT to make up some HN comments and it&#x27;s classic.https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;9b69427a-b28c-4080-b097-6a0a78...The HN&#x2F;LLM concordance ratio is approaching 1. Eventually, I can just remove comments and then fill them in with ChatGPT instead. reply someonehere 10 hours agoprevLast time I had to implement Okta integration for DocuSign at my employer it was absurdly expensive. If Google does this right then I’d be ever so happy.DocuSign on the SSO Tax site: https:&#x2F;&#x2F;sso.tax&#x2F; reply therealmarv 15 hours agoprevEverything which takes away from DocuSign is a win! reply gumby 13 hours agoprevMy ID card has a chip in it and can be used to sign documents. Why not support that? reply amf12 11 hours agoparentBecause their biggest markets do not? And it&#x27;s a just announced, beta feature! reply gumby 8 hours agorootparentHmm, I just looked and it turns out not to be a requirement of all EU ID cards. reply bombolo 11 hours agoparentprevWhy make a proper real legal signature when you can just copy paste a scanned .jpg file at the end of a document (or do something equally useless)? reply pluto_modadic 5 hours agorootparentcopy-paste a png of you typing your name recording your IP......vs cryptographic proof someone had access to your ID card reply bombolo 4 hours agorootparent> ...vs cryptographic proof someone had access to your ID cardWhich isn&#x27;t what we are discussing here, this was the point… :) reply haliskerbas 15 hours agoprevAnother set of startups that is just a feature on one of the big platforms. reply candiddevmike 15 hours agoprevThis is enough for me to upgrade from Workspace Starter to Standard. DocuSign is extremely expensive for what you get. reply ew 14 hours agoprevWe attempted to create this last year with https:&#x2F;&#x2F;pleasesign.me. Definitely a case of waiting too long on a good idea. Ah well, it still has some features Google might not launch :) reply sethhochberg 7 hours agoparentMany, many, many years ago I operated pleaselocate.me when mapping SKDs and geoip were first becoming available to any kid with a website - it brought me some joy to see people are still using this style of domain reply jeremycarter 13 hours agoparentprevLooks great reply aFaid7see0ni 12 hours agoprevI was hoping it&#x27;s this https:&#x2F;&#x2F;ec.europa.eu&#x2F;digital-building-blocks&#x2F;wikis&#x2F;display&#x2F;D... but unfortunatey we are not there yet reply Waterluvian 14 hours agoprevAs long as this doesn’t pretend like a signature is a cryptographic tool, I’m generally a fan of doing away with nonsense like Docusign. reply 30minAdayHN 13 hours agoprevAt this point I&#x27;m worried if Google will sunset their new features. Does anyone else have similar worries? At a personal level, I got bitten couple of times, or may be even more... (one was for the Inbox - or zero email gmail concept and second time with Google Domains) reply wanderingmind 12 hours agoprevThe main thing about eSignature is the willingness and ability to go to court to defend your tool and process. For the org that can&#x27;t be be bothered to have a barebone customer service, I&#x27;m not sure how this will workout. reply tadfisher 12 hours agoparentIt&#x27;s not like DocuSign does any sort of identity verification. I signed up for a free account and they validated my email address, that&#x27;s it. That would be the only identifier associated with my signature, so Google actually has more information than them. reply wanderingmind 11 hours agorootparentDoesnt matter. Ultimately Docusign will show up in court to defend their product and the contract. Google can&#x27;t even bother to provide an automated response reply jsnell 9 hours agorootparentDo you have any particular cases in mind where Docusign mounted a spirited defense in court despite not being one of the parties? How did those cases turn out?At least their terms of service don&#x27;t suggest any kind of general legal indemnification of customers, only a limited one around loss of confidentiality caused by security breaches in their systems. reply quantumsequoia 9 hours agorootparentprevWhat makes you think Google won&#x27;t show up in court? They do kinda neglect their free end users, but go to great lengths to appease enterprise clients (see: Google Cloud Platform) reply amf12 11 hours agoparentprevThere is customer service available to paid workspace accounts. reply awinter-py 8 hours agoprev&#x27;in tandem with our shrinking ambitions the size of company we are considering cloning is also shrinking&#x27; reply mr_toad 11 hours agoprevFor a tiny moment I thought this might be about making cryptographic signatures available to the masses. A sign that there’s still some good left in Google.I should have known better. reply theogravity 13 hours agoprevDocusign stock doesn&#x27;t seem to have taken a hit (yet):https:&#x2F;&#x2F;finance.yahoo.com&#x2F;quote&#x2F;DOCU&#x2F; reply joezydeco 6 hours agoparentBut thankfully Jump had some insider knowledge.https:&#x2F;&#x2F;beststocks.com&#x2F;jump-financial-llcs-significant-reduc... reply dna_polymerase 28 minutes agorootparentJump are market makers, not a hedge fund, that position was $6.5M at peak. Probably part of an options trade. But at this size 100% not an actual investment.Those articles are auto-generated from regulatory filings. Down in that article they write about a 270 share investment, that&#x27;s neither size nor notable. reply shallichange 10 hours agoprevNo x509 certificates involved?In countries with legally binding digital signatures (I.e. Argentina) you need a cert issued following identity verification. reply iudqnolq 10 hours agoparentSo it covers basically all use cases in the USA and most private commercial use cases in the EU. Seems reasonably useful for many people.Most countries that accept eSignatures don&#x27;t require digital certificates. There aren&#x27;t many other countries as strict as Argentina. See for ex https:&#x2F;&#x2F;www.hellosign.com&#x2F;blog&#x2F;electronic-signature-laws-aro... reply hintymad 12 hours agoprevThe top items in my wishlist are spreadsheet-like formula anywhere in a gdoc, spreadsheet-like tables, and support of Latex or MathJax. That is, keep up with Quip. reply rodolphoarruda 15 hours agoprevThe only chance for it to fly among the SMB in Brazil where DocuSign and ClickSign are the major players is by adding WhatsApp to the signature workflow. Notifications via email alone won&#x27;t work. reply xnx 15 hours agoprevHappy to see more useful features added to tools I already use. Along with improvements to Google Forms, I expect a lot of light DocuSign and JotForm users will cancel their subscriptions. reply EGreg 11 hours agoprevWhat is the minimum required functionality to roll your own e-signature which would be LEGALLY BINDING under the e-sign act?Seems all we need is:Consent to do business electronically — All parties must agree to conduct transactions electronically, either explicitly or implied.Intent to sign — E-signatures are only valid if the signer intended to sign. Signature requests need to be declinable.Association of signature with the record — Signers must make a visible mark or statement on the e-document.Attribution — Whether a name or a unique mark, the signature must be attributable to the person signing and only linked to them.Record retention — Signed electronic documents must be saved, viewed, or printed by either party and stored for future reference. reply xmly 15 hours agoprevBad news for DocuSign reply homero 2 hours agoprevWhat&#x27;s an e-sign anyway? It&#x27;s basically an IP address that&#x27;s often a VPN now reply namanaggarwal 15 hours agoprevWill this be admissible in court of law ? I know docusign is.On what basis is it determined that particular signing tech is admissable reply 8organicbits 11 hours agoparentIt varies by jurisdiction, but examples like this show that very little can be needed.https:&#x2F;&#x2F;www.businessinsider.com&#x2F;judge-ruled-thumbs-up-emoji-... reply jeffbee 15 hours agoparentprevVaries by jurisdiction of course. Under American law and similar systems the admissibility of a type of evidence is established by it having been admitted before, and the initial admission is based on the judgement of some individual judge. reply JumpCrisscross 14 hours agorootparent> American law and similar systems the admissibility of a type of evidence is established by it having been admitted beforeIt’s also been statute since 2000 [1].[1] https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Electronic_Signatures_in_Glo... reply mbauman 15 hours agorootparentprevIt would also vary by use-case. Some regulated industries have specific standards required for an electronic signature — and hitting those standards is typically an add-on cost for systems like Docusign. reply DiskError_ 11 hours agoprevFinally, I was wondering when Google Docs was going to implement this. eSignature platforms had a solid boom in the past ten years, but they all seemed to gatekeep rudimentary features behind tiered paywalls. reply s-xyz 15 hours agoprevBye bye Docusign? reply fifteen1506 15 hours agoprevRIP Dropbox reply I_am_tiberius 15 hours agoprevNo pricing === you are the product reply dctoedt 15 hours agoparent> No pricing === you are the productTrue, but is this no pricing? At a glance it appears to be available only to Workspace customers, which I believe you have to pay for (at least I&#x27;ve been paying for it for years). reply sassifrass 15 hours agoparentprevGoogle Workspace is a paid-for product? reply dexterdog 14 hours agoparentprevWith google you are the product whether you pay or not. Some of their most valuable data comes from the companies who use google for all of their email, calendar and documents. reply rrdharan 13 hours agorootparenthttps:&#x2F;&#x2F;workspace.google.com&#x2F;learn-more&#x2F;security&#x2F;security-wh...I&#x27;d argue it&#x27;s the least valuable data. They can&#x27;t look at it, they have regulatory, legal and contractual commitments to protect it, they have paying customers that will be very angry if it&#x27;s lost or unavailable, and yet they can&#x27;t mine it or train models with it or monetize it. reply alberth 9 hours agoprev [–] Locked account.I can only imagine the pain this will cause people when they try to execute&#x2F;sign a contract, and Google randomly locks your account … preventing you from logging in. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google Workspace is launching an open beta for its eSignature feature in Google Docs and Google Drive.",
      "This feature will streamline the workflow for solopreneurs and small businesses, allowing them to request and add signatures to official contracts directly within Google Docs.",
      "Additional capabilities, such as audit trail, multi-signer support, and compatibility with non-Gmail users, will be introduced later this year."
    ],
    "commentSummary": [
      "Google has introduced a new eSignature feature for Google Docs and Google Drive, receiving both positive and negative feedback from users.",
      "Some users appreciate the improvement, while others criticize its limited functionality, particularly in terms of identity verification, accessibility of signed documents, and an audit trail.",
      "Alternative eSignature solutions like DocuSign are mentioned, raising discussions about the potential impact on competitors, the need for legislative measures, and the availability and pricing of the Google eSignature tool."
    ],
    "points": 393,
    "commentCount": 158,
    "retryCount": 0,
    "time": 1691690347
  },
  {
    "id": 37075730,
    "title": "Show HN: Applite – Clean Homebrew front end app for macOS built with SwiftUI",
    "originLink": "https://aerolite.dev/applite/index.html",
    "originBody": "Applite Home Key Features FAQ Screenshots Download Contact Troubleshooting Make managing third party applications a breeze with Applite Download, update and uninstall your Mac apps with a single click. Download Free and Open Source Key Features Download and manage with a single click Download, update and uninstall apps with a single click Clean and simple UI Designed for non-technical users Free and open-source No costs, no tracking, fully transparent Applite uses the Homebrew package manager under the hood. Homebrew is a free and open source project that makes it easy to install developer tools and desktop applications on macOS. What apps are available on Applite? Any application that can be found on the Homebrew Catalog is available on Applite. Is it secure? The macOS built-in protection (Gatekeeper and XProtect) will scan the application for potential malware the first time you open it and notify you if anything is suspicious. Also, most applications in the Homebrew Catalog are notarized, which means they come from a registered developer. The apps may not be sandboxed, which allows them to affect your system with elevated privileges. Nevertheless, be careful, as some applications may contain malware, especially those that have few downloads. Applite itself is also not sandboxed. What information does Applite track? None. Advanced FAQ Can I use Applite with my existing Homebrew installation? Yes, you can. The first time you open the application, you will be asked if you want to use your own brew or if you want to create a new installation just for Applite. If you choose to create a new installation it will be stored at: ~/Library/Application Support/Applite/homebrew. Can I manage already installed apps with Applite? If you chose to use your existing brew installation all casks installed with it will appear in Applite. Any other apps installed manually (e.g. from DMG or PKG files) will not show up as installed in Applite. You can add them by reinstalling them. See troubleshooting page for more information on adding already installed apps. GALLERY SCREENSHOTS DOWNLOAD Download DMG or brew install --cask applite Free and Open Source macOS 13+ CONTACT GitHub github.com/milanvarady Email milan@aerolite.dev Discord Server Join Official Server Twitter @MilanVarady DONATE If you like the project, please consider supporting it. © Copyright Applite. All Rights Reserved Template used: © Copyright Bootslander. All Rights Reserved Designed by BootstrapMade",
    "commentLink": "https://news.ycombinator.com/item?id=37075730",
    "commentBody": "Show HN: Applite – Clean Homebrew front end app for macOS built with SwiftUIHacker NewspastloginShow HN: Applite – Clean Homebrew front end app for macOS built with SwiftUI (aerolite.dev) 248 points by milanvarady 21 hours ago| hidepastfavorite77 comments Syzygies 16 hours agoI applaud this, for people who prefer a GUI interface.I started to write something to better organize synchronizing my Homebrew installation across multiple machines, then thought someone must have done this well already. Sure enough,https:&#x2F;&#x2F;pumpingco.de&#x2F;blog&#x2F;brewfile&#x2F;A fantastic, clean way to reinstall or synchronize Homebrew setups, using the command line. I&#x27;m surprised that Applite doesn&#x27;t accept a brewfile to ease migration.In my case I depend on Bash, Tailscale, and Ruby as installed by Homebrew, so a clean reinstallation is delicate. I have scripts to make sure I don&#x27;t screw this up:1 brew bundle dump.sh 2 uninstall homebrew.sh 3 install homebrew.sh 4 renew bash.sh 5 renew tailscale.sh 6 renew ruby.sh 7 renew brew bundle.sh choose-shell.sh reply dividedbyzero 12 hours agoparentOne more neat trick for Brewfiles: They&#x27;re evaluated as Ruby code, so you can use if-else etc. to conditionally install things on different machines with a single Brewfile. I use that together with yadm to manage both work and private laptops with minimum duplication. reply JamesSwift 13 hours agoparentprevI have an idempotent provision script I use to install&#x2F;update apps. The homebrew portion is basically brew upgrade HOMEBREW_NO_AUTO_UPDATE=1 brew bundle --file=Brewfile --cleanup --no-upgradeWorks really nicely to keep everything tidy, up-to-date, and transferable. reply jmspring 8 hours agoparentprevAgree, it&#x27;s good for those that prefer a GUI. I&#x27;m not sure the intended audience since if you are using Brew, you are generally doing a lot of commandline stuff.Your mention of the cross multiple machine synchronizing, however, is interesting! Good to find. reply eviks 7 hours agoparentprevhow to you update you config with new&#x2F;deleted apps if you&#x27;ve manually added categories and comments like the blog&#x27;s author shows on his site? reply macrael 16 hours agoprevMy first thought is that it&#x27;s a classic macOS dev strategy to take a CLI tool and build a UI for it. In retrospect brew is a perfect candidate reply donatj 10 hours agoparentIt&#x27;s been about a decade since I ran desktop Linux in any meaningful capacity but I feel like that was the way a significant portion of the gui apps worked. reply ChrisMarshallNY 9 hours agorootparentXcode is easily the largest (non-game) app on my computer, and it’s really just a glorified CLI client. reply ccorcos 7 hours agoprevOne feature I would love is an ability to export a list of downloaded apps so I can import and re-install from a new machine. Even better if you can sync with Mackup.I’ve been keeping a shell script of all the install commands I run so I can get up and running quickly reply yett 6 hours agoparentYou can already accomplish this with Homebrew. If you type &#x27;brew bundle dump&#x27; it creates a Brewfile (basically just bunch of install commands) containing all the apps you&#x27;ve installed via Homebrew. Simply copy this file over your new machine and type &#x27;brew bundle install&#x27;. reply TheCapeGreek 4 hours agoprevBit of a tangent: I noticed in the screenshots a Jetbrains-like logo for an IDE called AppCode. Been considering tinkering with Swift&#x2F;Obj-C development for a bit. I know XCode&#x27;s reputation as being awful though, so it sounded great that Jetbrains had a product for this. To my dismay they discontinued it[0] last December. There is at least a freemium&#x2F;community plugin for Swift support on other IDEs[1].[0] https:&#x2F;&#x2F;blog.jetbrains.com&#x2F;appcode&#x2F;2022&#x2F;12&#x2F;appcode-2022-3-re...[1] https:&#x2F;&#x2F;plugins.jetbrains.com&#x2F;plugin&#x2F;22150-swift-support reply sunnybeetroot 1 hour agoparentXcode is great. reply SmellTheGlove 16 hours agoprevI wonder if you’d have a monetization path here via enterprise support contracts and&#x2F;or additional device fleet management features. The tech companies I’ve worked at have all had some amount of “use brew to install …” in our dev spinup docs. And IT doesn’t love it because when they get asked to support it, there’s no quick way to see what’s installed or what version without logging into the specific machine. And for whatever reason, it’s also challenging to roll out dev laptops with specific packages already installed. I presume it’s because maintaining them is difficult. reply ljm 15 hours agoparentAssuming everyone&#x27;s on a Mac, I&#x27;m actually surprised there isn&#x27;t that much use of something like homebrew-bundle[1]. It&#x27;s definitely nicer to have your tooling run natively rather than, say, trying to wrap everything in Docker, or trying to get everybody on board with nix or guix.I think the only real issue here is that you can&#x27;t really pin to specific versions unless a formula exists, and there is no guarantee that a formula with a pinned version will stick around because homebrew likes to stay lean. So to that extent it can still be unclear what version of software someone is running on after enough time.[1]https:&#x2F;&#x2F;github.com&#x2F;Homebrew&#x2F;homebrew-bundle&#x2F; reply Ambroos 11 hours agoparentprevThere&#x27;s a decent open source option: https:&#x2F;&#x2F;github.com&#x2F;munki&#x2F;munkiI got to use it at work at Meta (as end user), and it seemed to work quite well. They delivered Android SDKs&#x2F;IDEs and a bunch of other things that I&#x27;d personally install through Brew with Munki. reply tmikaeld 15 hours agoparentprevThat&#x27;s my first thought as well, there&#x27;s a lot of IT companies supporting Macs that would LOVE a tool like this, especially if it&#x27;s commercially supported. reply _1 15 hours agorootparentMy company manages Macs with jamf. It comes with a \"self service\" tool that can install oss apps, apps we&#x27;ve licensed, change configurations, etc. reply baal80spam 12 hours agorootparentSame here. There are at least a few of such tools, and they are highly customizable. reply tough 15 hours agoparentprevThis sounds like a great idea and a good COSS lifestyle company to run reply avinassh 3 hours agoprevI am on macOS Monterey and seems this app requires Ventura or higher. Does it use any APIs from the newer OS version? If not, could you please consider relaxing this and allow older version installations too? reply milanvarady 1 hour agoparentI wanted to at least target Big Sur, but the problem is that Apple doesn&#x27;t understand the term \"backwards compatibility\" when it comes to SwiftUI. Every new feature they add to SwiftUI is only for the latest version of macOS. And since SwiftUI is a relatively new UI framework, they add things that are missing or greatly improved from the previous solution.In conclusion, if I wanted to add extended support, I would have to put a ton of work into workarounds for features that are Ventura only. reply runxel 2 hours agoparentprevYeah I was wondering the same. Why tf does this need to be on the latest MacOS? reply avinassh 2 hours agorootparentmost likely milanvarady used the defaults from xcode to build and distribute reply mikkelam 2 hours agoprevFeature suggestion: Add a &#x27;popular&#x27; section. Should be easy enought to use the data from https:&#x2F;&#x2F;formulae.brew.sh&#x2F;analytics&#x2F;.I also noticed some of the app icons are a bit grainy on the edges.Cool app! reply nathansherburn 2 hours agoprevAm I correct that this only shows GUI apps that can be installed via Homebrew? I&#x27;m sure I have some command line utilities installed that aren&#x27;t showing up in Applite. reply milanvarady 1 hour agoparentYes this is casks only reply zapw 13 hours agoprev> What information does Applite track?> None.Absolutely love this part of the page---we need more software with this philosophy. Kudos. reply fewald_net 16 hours agoprevSo many animation on this website. reply spiderfarmer 16 hours agoparentThis is the \"Username checks out\" comment of HN. reply dreadlordbone 15 hours agoparentprevThats AOS for ya [1][1]https:&#x2F;&#x2F;michalsnik.github.io&#x2F;aos&#x2F; reply paxys 7 hours agoparentprevSeriously. It&#x27;s like they found an animation library and couldn&#x27;t resist using it for every single element on the page. reply catskull 12 hours agoparentprevJust looking at your site now. I&#x27;m inspired by your projects page. reply campfireshuffle 11 hours agoprevIn a similar vein, I created a web tool that can take your cask selection, save it to a temporary link and let you install all apps without creating an account https:&#x2F;&#x2F;coldbrew.vercel.app reply prxtl 4 hours agoprevThis is quite neat! But is it just me or does scrolling in the app feel... unnatural? Specifically in comparison to the rest of macOS? reply tony-allan 6 hours agoprevI installed Applite but it didn&#x27;t automatically pick up already installed brew apps (as shown by \"brew list\").I don&#x27;t really want to install&#x2F;reinstall them manually. Has anyone who has used Applite got any suggestions? reply sharp11 15 hours agoprevI&#x27;m not the target audience (I prefer CLI), but as an iOS dev just wanted to say kudos. Great idea and looks to be very well executed. I&#x27;m curious to know how your experience with SwiftUI was on MacOS? reply milanvarady 14 hours agoparentVery mixed. I hadn&#x27;t done any app development before this project, only games with Gamemaker and the Godot engine. Coming from that background, SwiftUI was very easy to pick up. In general, if SwiftUI has the feature or UI element you want, it&#x27;s super nice and concise. If it doesn&#x27;t, you&#x27;re pretty much screwed. Also, Apple doesn&#x27;t really understand the term \"backwards compatibility\" when it comes to SwiftUI. Without doing a ton of work on workarounds, I was only able to target Ventura or newer. reply pheeney 13 hours agoprevInstalled via brew cask command and get the following error:App load error Couldn&#x27;t load app catalog. Check internet your connection, or try restarting the app.I am running LuLu which I allowed the process to go through, tried to Retry and Quit but doesn&#x27;t work for me.My 2c, I like the logo on the website better than the app icon. The heavy drop shadows make it a bit dated. The website logo is minimal and clean in comparison and would look nice in the dock. reply pheeney 12 hours agoparentI think it was a DNS issue. I changed my DNS servers and now it works. Great project! reply graypegg 11 hours agoprevNice job! Looks really slick. I totally would’ve made good use of this when setting up a new Mac a couple months ago! Sometimes having browsable categories is perfect when you’re trying to remember everything you normally have installed. reply dmix 13 hours agoprevI was browsering through the app list, which is like a store, that&#x27;s cool.Does anyone know an good open source apps for limiting the amount of time you spend on distracting sites (like HN)? I&#x27;ve found some firefox extensions but nothing very good. reply efitz 15 hours agoprevWhere do the app icons come from? reply milanvarady 14 hours agoparentFrom a similar project called App Fair (https:&#x2F;&#x2F;github.com&#x2F;App-Fair&#x2F;App). They have a separate repo containing supplemental data for casks. reply brundolf 6 hours agoprevDoes it delegate to the homebrew CLI or is it a full reimplementation? reply milanvarady 1 hour agoparentIt uses the CLI under the hood reply ChrisMarshallNY 9 hours agoprevThat looks great!I feel that SwiftUI apps seem to have a fairly “uniform” UI, but it’s actually perfect for this. reply adamredwoods 14 hours agoprevNeat project, can I swap out brew and use macports? reply milanvarady 14 hours agoparentNo. I&#x27;m not currently planning to add macports support. If a lot of people request it, I might consider it, but it would be a lot of work. reply e12e 12 hours agorootparentTbh, I&#x27;m not sure if mixing the two in one app would make sense. Maybe a second app at one point?I moved from macports to brew as for a long while there were no build servers for arm for macports - having to regularly build clang and gcc from source just to patch a few utilities is not much fun. But I believe that&#x27;s been fixed for a while.I honestly think brew&#x27;s policy of \"latest, nothing or @specialcase\" isn&#x27;t great for a stable work environment. But it has a lot of mind share.In my view NixOS is too much, GNU Guix for better or worse won&#x27;t support propitary toolchains like Mac, pkgsrc looks best on paper - but seems abandoned? Which leaves macports as perhaps the best option and homebrew a close second. reply ljm 11 hours agorootparentI find myself preferring macports for a lot of things, but homebrew has brought its own niceness to the table.It&#x27;s just a bit of a nightmare as a software author because so much rework is done to repackage your stuff in n formats: one for nix, one for guix, one for debian, one for redhat, one for homebrew, one for macports, snap, flatpak, AUR, cross-compilation for alpine, maybe add a PPA in there for the ubuntu fans...and then some of them try to wrap packages on npm, rubygems, pypi, etc.it&#x27;s all done by the community but at the same time, so many of the people you want to help with your software won&#x27;t touch it if their only option is to clone the repo and compile it (which in itself is simple, if done thoughtfully) reply e12e 8 hours agorootparentThis is one reason I like pkgsrc better on paper.> clone the repo and compile it (which in itself is simple, if done thoughtfully)I find rust to be fantastic in this regard. And golang is quite good as well. replygorbypark 14 hours agoprevDoes it only do Casks, or CLI apps as well? reply dmix 13 hours agoparentSeems to just like my cask apps in the UI. reply milanvarady 12 hours agoparentprevJust casks reply dimmke 16 hours agoprevThere’s another decent application for this called Cakebrew, but not sure if it uses native Apple UI APIs reply milanvarady 14 hours agoparentAs far as I know, Cakebrew is no longer maintained. There are quite a few alternatives though, you can see the full list on the GitHub page (https:&#x2F;&#x2F;github.com&#x2F;milanvarady&#x2F;Applite#alternatives) reply koinedad 5 hours agoprevLooks super clean! reply threesevenths 21 hours agoprevAny tracking or privacy concerns we should be aware of that a polite adds on top of home brew? For example, does a polite collect data on the type of device, installed casks, user data, anything? reply milanvarady 20 hours agoparentApplite developer here. No, Applite has zero tracking built in, it&#x27;s a 100% free and open-source. Only Homebrew tracks the number of downloads. reply vaxman 14 hours agorootparentIf you want to track, charge or close-source your code, it is okay, the Streisand-effect from all the freeloaders will save you. reply driggs 16 hours agoparentprevThis is in the FAQ, which you don&#x27;t even need to click to read from the front page:https:&#x2F;&#x2F;aerolite.dev&#x2F;applite&#x2F;index.html#details > Q: What information does Applite track? > A: None. reply kryptozinc 16 hours agoprevWhat is your monetization strategy with this app? reply milanvarady 14 hours agoparentNothing. It&#x27;s a passion project of mine. I accept donations to cover the Apple Developer Program fees. reply w10-1 14 hours agorootparentThere would be a lot of value in whitelist and audit features for IT to manage their risk when letting developers install applications themselves (if not also a binary repository).One model (and possible partner) here is Sonatype, with a growth path into binary repositories for development (Swift packages, cocoa pods et al).I realize you might not want the burden, but you might partner with someone or some company. (And I would encourage others to collaborate before forking.) reply merryje 16 hours agoparentprevIt has an MIT license on GitHubhttps:&#x2F;&#x2F;github.com&#x2F;milanvarady&#x2F;Applite reply EduardoBautista 16 hours agorootparentI am not a license expert, but doesn&#x27;t MIT mean we can build upon it with proprietary changes and then sell it as proprietary software? reply paxys 7 hours agorootparentYes. You can also sell the project as-is with zero changes if you want. The only thing you have to be wary of is trademarks, which normally aren&#x27;t covered under standard OSS licenses. reply xcdzvyn 15 hours agorootparentprevYes. reply notpushkin 16 hours agoparentprevDoes everything have to have a monetization strategy? reply rubicon33 16 hours agorootparentAs a user of an app, its always a fair question to ask.\"Free\" usually means you are the product. reply e12e 12 hours agorootparentNo, capital F Free (as in freedom ) usually does not mean that. Closed source and gratis often do.You&#x27;re not \"the product\" for the Apache web server or the Linux kernel... reply princevegeta89 15 hours agorootparentprev\"Open Source\" means you are not though. reply kryptozinc 13 hours agorootparentprevI don&#x27;t want to be the \"product\" of this free app. reply tchbnl 15 hours agoparentprevThis being HN, I can&#x27;t tell if this comment is a joke or not. reply nonameiguess 11 hours agoparentprevI love the guy&#x27;s actual answer. It&#x27;s always interesting to see this be a question. What is yum&#x27;s monetization strategy? What is apt&#x27;s? Does any package manager have a monetization strategy? If anything, they might be components to a larger distro that itself sells enterprise support packages, but the software itself is rarely if ever monetized. Homebrew is BSD-licensed and currently run by a nonprofit. The original developer never tried to make it into a product and \"monetized\" by parlaying the experience he gained doing it into highly paid engineering jobs in spite of having no formal training in computer science or software development, and eventually started his own company when he got tired of working for Apple. reply server_man3000 14 hours agoprevDude, neat! reply malablaster 7 hours agoprev [–] A GUI for homebrew is totally unnecessary. replyGuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Applite is a free and open-source application for macOS that simplifies the management of third-party applications.",
      "It uses the Homebrew package manager and provides a user-friendly interface for non-technical users.",
      "Applite is secure, does not track user information, and can be used alongside existing Homebrew installations."
    ],
    "commentSummary": [
      "Applite is a macOS app that simplifies managing and syncing Homebrew installations with a user-friendly interface.",
      "It supports GUI apps installed through Homebrew and has the potential for enterprise support.",
      "The app is free and open-source, allowing users to donate to cover fees. Discussions have arisen regarding adding support for other package managers and implementing IT management features."
    ],
    "points": 247,
    "commentCount": 77,
    "retryCount": 0,
    "time": 1691673237
  }
]
