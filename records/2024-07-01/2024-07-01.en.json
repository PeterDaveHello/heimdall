[
  {
    "id": 40843867,
    "title": "I created an After Effects alternative",
    "originLink": "https://pikimov.com/",
    "originBody": "Many years ago, I made VJ softwares (to mix live visuals in clubs) for unexpected platforms like the Game Boy Advance, the Playstation 2 and the Raspberry Pi. This year, I’m back with a new web-app: Pikimov.Inspired by Photopea (a free Photoshop clone), I created this web-based motion design & video editor as an alternative to After Effects, to fill empty void.It&#x27;s free, without signup, without cloud uploads (your files stay on your machine), and your projects are not used for AI models training.",
    "commentLink": "https://news.ycombinator.com/item?id=40843867",
    "commentBody": "I created an After Effects alternative (pikimov.com)596 points by clementpiki 10 hours agohidepastfavorite166 comments Many years ago, I made VJ softwares (to mix live visuals in clubs) for unexpected platforms like the Game Boy Advance, the Playstation 2 and the Raspberry Pi. This year, I’m back with a new web-app: Pikimov. Inspired by Photopea (a free Photoshop clone), I created this web-based motion design & video editor as an alternative to After Effects, to fill empty void. It's free, without signup, without cloud uploads (your files stay on your machine), and your projects are not used for AI models training. doctorpangloss 3 hours agoThis is from my point of view as an experienced developer and VFX artist, including in-depth knowledge of AE. A 30fps limit is surprising 23.976 as an fps does not seem to be dealt with gracefully Clicking report bug should take me to a bug report form with info Interacting with \"rectangle\" is surprising because the corners aren't draggable. Probably should default to full size I should be able to wind input fields by clicking and dragging on them, such as for rotation When I drag keys around in the dope sheet, and let go with momentum, they appear to keep moving and \"settle\" in a sort of random place. They should not do that I see that this momentum behavior is also in the play head in the timeline. It really should not do that. Putting in a value in the dope sheet should be enabled, and it should automatically set a key I liked the visualizations of the easing, but it should probably be communicated with the keyframe's icon shape in the dope sheet like it does in AE. It is difficult to move a key to the beginning of the timeline. Architecturally, the preview rendering probably has to work the way it does in After Effects. Guaranteeing a real time visualization is pretty important. I kind of stopped there because it's a bunch of goal-less fiddling. This is really great, there is a tremendous amount of product development. Hopefully you will think deeply about your audience and objectives, and the amount of product development that goes into After Effects. reply clementpiki 3 hours agoparentThank you for your feedback, the 30fps limit is temporary, until I have fully tested perfs with video files at a higher framerate can you tell me more about 'and let go with momentum', not sure what you mean? Is it about loss of precision, not being responsive, bad snapping to closest frame? reply mubu 2 hours agorootparentHave you considered opening a staging or experimental branch on a subdomain perhaps? I think people would definitely be interested in testing out new features before they're released without hosting it themself, and I believe you said hosting Pikimov is very cheap. reply doctorpangloss 1 hour agorootparentprevWhen I click and drag on the timeline play bar or a key frame widget in a layer; then, if I release the mouse button with velocity; the play head and key frame keep moving for a little bit, even though I am not dragging them anymore, as though they have momentum and are being dragged to a stop; the widgets should not behave this way. The big picture feedback is that often there is an alignment between “I have a lot of ideas for unique UX I want to implement” and “I want to wake up this morning and work on this all day.” IMO just copy AE’s UX affordances first if your goal is to make something you want other people to use. Blender spent a decade in the doldrums until it found an audience the moment they relented and copied Maya and Unity’s superior, conventional controls. reply jancsika 2 minutes agorootparent> IMO just copy AE’s UX affordances first if your goal is to make something you want other people to use. @clementpiki I second this advice and strongly suggest to document it as a guideline for your project as soon as possible. Surf on the shoulders of AE's UI giants! Otherwise a small but vocal number of devs users who implicitly love FOSS software will end up rationalizing all the little UI quirks as features. Worse, their lack of expertise in UI design will be proportional to their level of energy and post length on all your communication channels. It's the epitome of bike-shedding. But if you just standardize on \"whatever AE does\" nobody will spend a moment advocating for putting quirks into the UI. (Plus they'll be very forgiving about quirks since it's obvious that getting the UI to work exactly like a proprietary piece of software is a very difficult problem.) reply green-eclipse 48 minutes agoprevSide note: I just wanted to comment on the variance of HN submission popularity. OP submitted this same amazing tool as a \"Show HN\" 20 days ago, with very similar title and shorter description [0]. It got no votes. Today this submission is climbing rapidly. Was the tool very different 20 days ago? I honestly don't know! But you never know how submissions will perform here. It feels pretty random, which makes sense, and also is part of the fun of HN. [0] https://news.ycombinator.com/item?id=40645009 reply jrhizor 19 minutes agoparentIt went viral on Twitter this weekend, which is probably why more people are noticing it now. reply enobrev 4 hours agoprevThis is excellent! Several years ago I built a prototype of a video renderer on nodejs (v0.8-ish). It's a silly thing to do, rendering video in javascript (especially a decade ago), but it worked well enough to prove that it could be done and a startup pivot was born - one that was eventually acquired by Vimeo. While a colleague (and now friend) was working on porting my silly little renderer to C/C++ to try to get closer to real-time, I built out a UI to allow us to \"templatize\" dynamic video for our users. This made it possible for our designers to design the video experience for the videos that were dynamically generated for our users' content. That UI very much resembled Flash. Since then I've always wanted to do what you've done here. Before acquisition, I was asking our designers to walk me through how they use After Effects, in the hopes of building our tools in that direction, but then Vimeo showed up and... not too long after I left to start a travel startup. I haven't revisited the video space since. I love this. I _knew_ that it could be done - especially now that WASM is stable - and I'm excited to see someone has done it. And free, no less! Edit: Thought it was open source - thanks for the correction. Also I was way off on the node version. reply wavewash 3 hours agoparentIt is free to use but not open source according to the faq: https://pikimov.com/faq/ reply ramses0 3 hours agoparentprev2006: Flash-based, online/web NLVE, focused on clipping / remixing (eg: \"TikTok / Stitch Me\"). https://techcrunch.com/wp-content/uploads/2006/04/jumpcut.gi... ...sadly, acquired by Yahoo, and you know what happened next. I can't find a lot of screenshots / video from the era, but the one above should give you a sense of it. reply bluelightning2k 8 hours agoprevAs a developer it's rare to see something which leaves me feeling \"how would you even build something like that...\" but this is one of those. Huge cudos for even attempting and following through with something like this! reply mclightning 1 hour agoparentyou follow HN, and some guy posts a video editing js library, another guy builds an app taking that library further, you get inspired and build something even bigger. reply suyash 5 hours agoparentprevYou start by building your knowledge around computer graphics and modern web development, basically WebGL and modern web APIs have made doing sophisticated graphic applications on the browser. Only limitation right now I see is single threaded limitation on the browser. reply ptsd_dalmatian 4 hours agorootparentWhat about workers? Isn’t it basically the same as offloading computation heavy work to separate thread? Also, we have WebAssembly. Afaik, js threads can now share memory using SharedArrayBuffers. So I don’t think we have this single thread limitation anymore :) reply qingcharles 4 hours agoprevThis is amazing. I just installed AE yesterday after I decided that I needed to add \"motion graphics creator\" to my skillset in order to make decent Instagram stories. What's needed now is a page where people can share the templates they've created. reply mrandish 1 hour agoprevWow! This is wildly impressive and deeply inspiring - a truly incredible achievement. Even more unbelievable that it's by a solo dev. Please keep up the amazing work on this project. AE is powerful and feature rich but also bloated with decades of legacy code and niche professional use cases to support, so there's a huge need for what you've built. Not everyone is a high-end animator or special effects compositor. Many of us just want to make cool motion graphics for personal videos, social media and art projects. Based on the great response you've gotten so far, I'd suggest focusing on ways for the community to expand on what you've built with templates and plug-ins. reply alfl23 6 hours agoprevI really hope you charge money for it and make it awesome, it's about time Adobe gets disrupted and this is a wonderful idea! It will take significant resources, cash and teams to make this into a serious contender, and folks that have problems to solve will always be happy to pay decent dollars for great software. reply majani 1 hour agoparentHe's clearly trying to follow the photopea playbook and make money through ads reply mrbluecoat 6 hours agoprevVery cool! Although other more complex open source solutions exist, for me the sweet spot between capable and not overwhelming is perfect. I prefer Photopea over Photoshop for that same reason (..just don't go crazy on the anti-adblocker like Photopea did) reply tombert 4 hours agoparentWhich open source solutions exists as an alternative to After Effects? reply jdiff 4 hours agorootparentAFAIK, the only thing that can even be stretched in a motion direction is Blender, but an AE alternative it is not. reply emigrantdd 3 hours agorootparentprevalso interested reply Andrew_nenakhov 7 hours agoprevCan you please clarify, which exactly key features are missing from Firefox that make this app say 'use Chrome or Edge'? reply ayhanfuat 7 hours agoparentFrom the FAQ: > Why no Firefox support Firefox is my daily web browser. As a web developper, I always make sure my work is comptatible with all major browsers. But you can guess a web based video editor is a complex task to achieve, and Pikimov uses several key features that only exist in Chrome, Edge, and maybe Opera, and maybe, maybe, Brave. That's why Pikimov cannot currently work on Firefox (as of today: v127), there's nothing I can do to fix this, it is just not possible. For the curious ones, here are some of the web API Pikimov requires, but are missing from Firefox: - audio data - window showsavefilepicker - videoencoder Note: There is no Safari support due to similar obstacles. https://pikimov.com/faq/ reply rom1v 7 hours agorootparentFor AudioData and VideoEncoder, it should be possible to enable webcodecs by setting dom.media.webcodecs.enabled to true in about:config. - https://github.com/mozilla/gecko-dev/blob/af09c55c0e6253ba8f... - https://github.com/mozilla/gecko-dev/blob/af09c55c0e6253ba8f... reply jampekka 4 hours agorootparentFor VideoEncoder at least there's also a polyfill using WASM compiled libav (aka ffmpeg). https://github.com/ennuicastr/libavjs-webcodecs-polyfill reply padenot 3 hours agoparentprevI (Firefox developer working on anything media related) got in contact with the dev on Twitter, and he told me that Web Codecs was missing (and we're shipping this in a month or so, it's been in Nightly for some time), and something to save project file to disk (https://developer.mozilla.org/en-US/docs/Web/API/Window/show...). So I spoofed the user-agent in a nightly build here on my Linux desktop workstation, then had to alias one method that we should have implemented years ago but only have with a `moz` prefix (`HTMLMediaElement.mozCaptureStream`). This is on us to fix. Then it looks like a worker script is served with the `Content-Type` `text/html` instead of `application/javascript` or something like that. We also have a pref flip to bypass that check, so I did that, but this is on the dev to fix. When you do this it works, I've loaded project demos containing videos, audio, various things composited on top, scrubbed the timeline aggressively in a debug build, moved things around in various bits of the interface and also in the rendering frame, etc., things seem to work as they should, perf is as I'd expect it to be (and again, I'm running it in a debug build with optimizations disabled for anything media related, enabled for other parts of the browser). What's missing is `window.showSaveFilePicker` and file system related stuff. It's possible to use https://developer.mozilla.org/en-US/docs/Web/API/File_System... instead (that we ship, e.g. Photoshop on the Web uses it). We think that it's much less scary than giving access to the file system to a content process of a Web browser. Maybe because videos can sometimes be extremely big files, direct access to the FS could be of use there. Thankfully, we also ship extremely modern video encoders to make them tiny instead, but that's currently a limitation Firefox has, for better or worse. https://paul.cx/public/pikimov-firefox-nightly.webm reply clementpiki 2 hours agorootparentWow, so you do have a workaround for the missing window.showSaveFilePicker, that's promising! reply mrandish 1 hour agorootparentprevThanks for taking the time to investigate what's currently the gap with FF. As a long-time Firefox user, I'm hoping you can guide this dev regarding ways to get things working from his end while also using this app's needs to inform FF improvements from your end. reply uh_uh 6 hours agoparentprevThe fact that he's a solo dev giving his project away for free (even for a limited time) is a good enough reason. reply martin293 4 hours agorootparentWe are not doubting his reasons. We were interested in why. reply AshleysBrain 7 hours agoparentprevAs I mentioned in another comment, in short, WebCodecs and File System Access API I believe. Both pretty essential for an app designed for editing large video files from disk. reply jampekka 4 hours agoparentprevIME the features are often mostly there same but there are small implementation differences/bugs at least in the newer APIs. Firefox is no more buggy (often less), but it's easier to code for one set of bugs. Safari is by far the worst. I use Firefox for all my browsing, but do web app development with and for Chromium. I'd gladly do it for Firefox, but people, especially users, suck and sometimes one has to accept this. Glad to see no time is wasted for Safari/iOS support. It's a huge waste of time and people using Apple devices are to blame. reply Aldo_MX 3 hours agoparentprevWhy the entitled tone demanding \"to clarify which exactly key features\"? It is the right of developers to say \"I don't want to support your browser\" and you should respect that decision even if you disagree with it. As a reality check, see this ticket: https://bugzilla.mozilla.org/show_bug.cgi?id=390936 It took Firefox 17 years of back and forth with developers to add parity with an Internet Explorer feature that Chrome supported since version 1. This late in the game IE is already dead for good. Not everybody has infinite time or infinite money to support Firefox, as an aside, you knew what you signed up for when you made Firefox your main browser. So please, change the \"clarify why you don't support Firefox\" tone with \"I want to make the site work with Firefox, how can I help you?\". And good luck making the Firefox team change their mind when they decide not to support X feature, because it is also their right to do not implement the whole spectrum of features that Chrome supports. reply sadops 4 hours agoparentprevProbably the same features that computers have had since the 1960s, but nobody writes native applications anymore. Guess I'll have to pass on this one. I wish Chrome weren't the only operating system people chose to write software for. reply czhu12 2 hours agoprevHaving never used after effects, I would like to say that photopea has probably been my favorite piece of software in the last 10 years. I diligently disable Adblock on that website and have donated a few times in the past to support them. I hope to see many more alternatives on the market, and lend my support to anyone building! reply anovc 6 hours agoprevlooks impressive! What are your plans for further development? I guess for this complex project to evolve in order to meet the needs of the professional users it will require lots of work/team/resources/etc. Do you plan to monetize it somehow in the future? or how are you going to sustain it? Another free alternative to AE (targeted at more casual users) that comes to mind is CapCut... which is obviously a ByteDance product. And they already offer tons of features for free, so the competition could be tough... reply kypro 6 hours agoparentGiven it's free, it might be worth open source the project, or at least opening it up to community plugins so that community can build & fund additional functionality. reply SillyUsername 3 hours agoprevWhere did you get the time to do this as a hobby? I spend evenings and weekends working on a project for the last 6 months and it's only half as good. 10x developer maybe, small team, millionaire? xD reply clementpiki 3 hours agoparentI had previous experience doing editors, and I am not ashamed to admit ChatGPT gave me a hand on some features I was not comfortable with, like editing audio in javascritt for example. reply InsideOutSanta 4 hours agoprevDang, the level of feature depth you've achieved here is amazing, and the UX is great if you have some experience with other video editors. This is definitely workable, at least for smaller projects. reply deweywsu 1 hour agoprevWonderful. You just \"created\" this?! This seems like it took many years and lots of hours. Great work! reply alok-g 6 hours agoprevSide comment: Comparing a web-based software that runs on your own computer vs. installing a (say native) software and frequently updating, isn't it interesting that the former is faster to do? When using a web-based software to ru on your own machine, you are effectively, momentarily, installing it and are able to uninstall by clearing the cache. reply whartung 3 hours agoparentThis is just a testament to the maturity of the platform, and the work that has gone into the portable engine segment. Targeting Chrome targets all of the platforms, and the machines and platform is \"fast enough\" to do the job without having to dig deep into specific nature of the platforms. It also leverages, I'm assuming, the deep knowledge the developer has of doing other things for the browser platform. They probably could have targeted some other portable GUI toolkit, but this was more familiar. It may well be an even smoother experience than using other cross platform GUI toolkits, plus, of course, the platform is free. Finally, distribution is familiar and likely easier, it's truly cross platform (no need to build executable on the individual platforms, even if its all from the same source base), etc. No bundling, no signing, no app stores. Just a URL shared in a tweet and you're on your way. If it was OSS, it could be parked on a Github page for all eternity. Overall, it's a really attractive platform for developers, just not yet fully embraced I think, as client based applications I mean. reply crazygringo 3 hours agoparentprevAnd when you start a web app you might not even be \"installing\" 5% of the code, which is great for speed. You can load+interpret JavaScript files dynamically as the user accesses certain features. reply conception 6 hours agoparentprevWell, it’s rarely “running” on your machine but is just a client for the server it is running on. reply sbarre 5 hours agorootparentThe app is client-side Javascript, it's most definitely \"running\" on your machine. reply billconan 8 hours agoprevbut I think each browser tab has a memory limit of 4gb? this means a web based video editor can only work on short clips? reply crazygringo 3 hours agoparentVideo editing is generally not done in memory at all. The videos themselves sit on disk and are played back from disk, and the desired cuts/effects are composited on the fly. For speed, an editing app may also produce lower-res versions in memory for quick seeking and smooth playback, as a kind of quick preview. But that's easy to control how much memory you allocate for that, and even those previews can be stored or cached on disk. Video editing is not especially memory-bound on modern machines. It's much more CPU/GPU-bound when it comes to applying effects, and IO-bound (including decoding-bound) when it comes to larger videos like 4K and 8K. reply clementpiki 7 hours agoparentprevWhile doing my testings, it does not appear that a tab is limited to 4GB. A typical project in After Effects is not longer than a few minutes of duration. Let's say you work on a 2 hours movie, each AE project would be about a 1 minute shot where sfx need to be applied. Pikimov was created with the same behavior in mind, not made to edit a full movie in one go reply billconan 6 hours agorootparentsome say the default limit is now 12gb https://issues.chromium.org/issues/40691287 reply jsheard 7 hours agoparentprevWASM memories do have a hard 4GB limit, at least until the 64bit extension lands, but there's nothing technically stopping a tab from using more than that as a whole if the implementation chooses to allow it. I just tried creating a dozen TypedArrays of 1GB each and Chrome didn't panic, and the heap profiler shows ~12GB allocated as expected. Mobile browsers are much more strict about memory though so don't expect those to be so forgiving. reply ffsm8 8 hours agoparentprevLots of factors at play that makes your assumption not necessarily correct. I.e. 1. Newer API such as directory access, which would let the app utilize something like swap as necessarily. And only ever loading the data it can currently handle from the filesystem (https://developer.mozilla.org/en-US/docs/Web/API/File_System...) 2. Input doesn't necessarily have to be RAW/8k. You get several h of 1080p AV1 video within 4gb. I think it ultimately depends on how much effort the Devs wants to invest to support large video inputs. If none is invested, then your assumption would be true. I was unable to find the source code, so I wasn't able to check for myself. We'll have to wait for the author to chime in, if they're willing. reply rootlocus 7 hours agorootparent1. Swapping to disk would kill the performance to an unnacceptable level. 2. I'm assuming video editing software works on a raw format in memory with access to individual frames? Just like Photoshop would need access to individual pixels from an importend JPEG, the actual canvas uses a lot more memory than the compressed input format. reply clementpiki 6 hours agorootparentThat's correct, access to each pixel of each video frame is needed. Each video frame must be inflated from its codec compression. reply ohthatsnotright 6 hours agoparentprevI had a Macintosh Quadra 660AV that had 16MB of RAM and was able to edit multiple-gigabytes of video. Not everything needs to be in memory all of the time to effectively edit video in a non-linear fashion. reply atum47 8 hours agoparentprevBump for interest reply n3storm 9 hours agoprevSite and brand looks amazing and it's like something that really can compete with AE. Best of the lucks and I hope you will make it open source some day and make it work on Firefox :D reply Melatonic 6 hours agoparentNuke blows AE out of the water but is mainly only used by professionals reply jagged-chisel 6 hours agoprevCan I export projects to use in other places? Or import projects from other places? Blender comes to mind. But also, I would like to use motion graphics in an app where the software engineers don't have to re-implement each asset in code. reply aio2 1 hour agoprevYou're crazy. This is really cool. reply dorkwood 7 hours agoprevIs there an option for custom ease curves, like a graph editor? I love the idea of an After Effects alternative, but if it only has a few simple ease functions to choose from I can't see myself using it much, sadly. reply clementpiki 7 hours agoparentNot yet, but it has already been asked several times, so I'll keep that in mind. There's balance hard to find: should I try to make it as complete as AE, or make it a simpler so it's more accessible. reply alt227 6 hours agorootparentIf you are advertising it as an after effects alternative (which it looks like you are) then you should be trying to include all features that after effects has. reply dewey 5 hours agorootparentIf most people use 20% of the features of an app, and you create an app that covers these most popular feature really well I think it's fair to call it an alternative. It doesn't say \"full clone\" or \"feature parity\". reply clementpiki 2 hours agorootparentI could not seriously pretend of a 'feature parity', if I can match 10% of features, that should good enough for most users. I'm not expecting the next Marvel movie to get its SFXs done with PIkimov. reply 1668911361 3 hours agoprevFirst of all, this is really really cool! Great job! Would you consider building in support for color spaces into the software? It looks like the working color space is linear sRGB, but it'd be nice to at least know, and also support other color spaces as well. Frequently, when I'm rendering from blender, the raw renders will have out-of-gamut colors, which I'll then correct and bring back into sRGB when compositing. reply atum47 7 hours agoprevI was well on my way to create a software like this [1], it started out as a flash clone, but since I didn't define any scope it started to look like after effects, on the back end I mean, never actually wrote a single line of code for the UI. Are you planning on creating a company out of this? Are you going to monetize it? 1 - https://m.youtube.com/playlist?list=PL3pnEx5_eGm9BbCp2ZTj6LT... reply jampekka 4 hours agoprevVery impressive, and very needed. Current open source video editors uniformly suck. Glad to see browser technology being put to use. Browser is by far the best API for desktop applications too, despite the very common ignorant complaints on HN. Kudos! reply emigrantdd 3 hours agoprevOmg I've tried this it's really amazing. I see how many thing you did here, how many hours did u spend to build that? reply 999900000999 5 hours agoprevAny reason you couldn't bundle this into an Electron application? It's cool, and free. But what happens when you get bored and take the site down ? reply vunderba 27 minutes agoparentThis is exactly why I would hands down always prefer an electron wrapped application over a web app if given a choice between the two. This is even more important if the app uses some kind of proprietary project file format, since if the website goes down you won't even be able to recover your project (without heavy reverse engineering potentially). reply KronisLV 5 hours agoparentprev> Any reason you couldn't bundle this into an Electron application? I know people hate on Electron (sometimes rightfully so), but this does seem like a nice use case, meet your users wherever they are, even offline! As for when I need something native, Kdenlive is pretty nice: https://kdenlive.org/ (it's FOSS, they could probably also use a donation https://kdenlive.org/en/fund/) Oh and DaVinci Resolve, even though effects aren't the main focus (it's a fully featured editing suite): https://www.blackmagicdesign.com/products/davinciresolve reply fl0id 9 hours agoprevWow looks cool. Now if only it could be used in Firefox. And yeah I know it doesn't have some chrome-only things. But afaik that's mostly because google/chrome does what ever they want, and there is no such thing as standards anymore apparently. reply Rizu 5 hours agoprevcongrats op, would you mind sharing the techstack used in creating this reply JackYoustra 1 hour agoprevJust curious - why not open source? (really cool btw, super impressive) reply lancesells 4 hours agoprevCongrats OP! I don't have Chrome to test, but this is really nice from what I can see. reply vlugorilla 7 hours agoprevthis is awesome. I hope some video editor comes up that can compete with Premiere. Then with pikimov and photopea, I could totally ditch Adobe for one. Have you considered open sourcing the app to benefit from contributors and build a community? reply akanet 7 hours agoparentThere are many editors that compete with Premiere, which at this point is a janky unusable mess for me. Resolve has been a joy to work with. reply gatinsama 9 hours agoprevGreat idea and the product looks great. I looked everywhere for a suitable substitute for AE and there was none. reply rabf 7 hours agoparentAs well as the already Davinci Resolve there is Natron: https://natrongithub.github.io/ \"Open Source Compositing Software For VFX and Motion Graphics.\" reply skrebbel 7 hours agoparentprevDaVinci Resolve has a very generous free version. It's enormous but also powerful. reply briandear 8 hours agoparentprevDaVinci, Apple Motion reply mdrzn 8 hours agoprevLooks very good! Will give it a try whenever I need a quick video edit on the fly. reply whywhywhywhy 7 hours agoprevFew realize that for many AE is the real jewel in the Adobe crown, Photoshop and Illustrator and Premier all have viable alternatives. AE however stands alone as the only tool with it's unique feature set. Yeah other compositing tools exist but they lack the animation/mograph tools of AE, or animation tools exist but lack the scripting/filters/compositing. reply itslennysfault 4 hours agoprevWhat's that little blue bird icon in the top right corner? Seems to go to X, but I'm not sure why. ...jk! On a serious note, this is REALLY cool. Great job. Tiny feedback, your console is extremely chatty. I get this is a beta, but it still might be worth disabling that logging for production deploys. I hope to see this evolve into a commercial product I really think it fills a need and it seems to work incredibly well. reply cloogshicer 9 hours agoprevWow, this looks really amazing! Must've been a lot of work. reply wdb 6 hours agoprevDoesn‘t seem to work in Safari? reply david033 6 hours agoprevNice, but too many animations/chaos on the landing page for my taste. Keep it simple. reply blueboo 4 hours agoprevWhat would it take to support AE plugins? Nail that and you have something to make Adobe nervous. reply martin-adams 6 hours agoprevThis is a very nice project and any competition to After Effects is very welcome. One model I'd love to see is a web based front end, but all video processing happens in the backend. Then ship the app as a combined front/backend, or just the front end that connects to a remote backend. That backend could be a server in a studio with beefed up specs, or offloaded to the cloud for solo animators working on complex projects. Seeing a project like this give me hope that we could decouple what the app does, vs how to control the app. reply langitbiru 5 hours agoparentA good idea is the backend or the cloud could be a paid feature for this application. reply pier25 4 hours agoparentprevThis model would be great even for some desktop apps. Imagine you could offload rendering Blender projects to the cloud automatically. reply clementpiki 2 hours agorootparentDon't they call it 'render farms'? I will add cloud features, I don't want to deal with massive files hosting. Maybe login box to G. Drive or 1Drive could be a solution. reply beardyw 8 hours agoprevI feel like that web only is a positive way forward. If only it was possible to prove nothing goes back to the server I think it would gain a lot more trust. Though companies who want to see your data might not be so keen. On my phone, but will try it out when I get home. reply dartos 6 hours agoparentWhy do you think web only is positive? 15 years from now, will this site still be up? Will you be able to open your projects from today, then? I think web only is a really compelling way to get someone to try a product, but I’d much rather install a tool like this. Unless you could host the site yourself, of course. reply dylan604 5 hours agorootparentWeb only does bring with it the notion of web scale rendering. Cloud render farms are already a thing, so it would be a compelling feature. Lots of video acquisition is already cloud based, so the footage is already there. There are still plenty of times where the render stage takes enough time that rendering on my local single machine is not pleasant. reply beardyw 4 hours agorootparentprevI see what you mean, but I'm not using anything from 15 years ago today apart from Linux. reply cush 3 hours agorootparentYou're using a 15 year old version of Linux? reply thiht 5 hours agorootparentprevHonestly there’s 2 situations: - if the tool is updated continuously for 15 years it’ll still be up - if it’s not updated, it will be technically irrelevant anyway and you’ll have switched to another tool by then Future support is overrated for tools, just use one now and worry about tomorrow later. reply glenneroo 5 hours agorootparentWinamp would like to have a word with you :) Granted I'm using the latest release from 2018, but I still sometimes load up v2.x released in 1998 just to show people that 25+ year old software still works just fine... even the AVS visualizer and Shoutcast internet radio features work, which is to me just insane. I also use older software quite often that has long since been updated, such as older versions of Audacity, Ableton, Adobe Premiere, etc. for various reasons such as: not wanting to spend money, avoiding spyware, ads (see: Windows 11), and other bloat which often IMO negatively outweighs the positivity of new features. There are a lot of other small utilities that I still use that are 10+ years old because they still work fine and I know how to use them blind-folded. There are also tools that haven't received updates in many years but still work great, why would I bother to look for something new that potentially will spy on me and not offer the same functionality? reply thiht 1 hour agorootparentWe’re not talking about Winamp. A more accurate comparison is the Adobe CS suite: no one use CS5 anymore. reply jbstack 5 hours agorootparentprevStrong disagree on the second point. I don't want the choice of whether I switch tools to be based on an arbitrary factor such as when a website suddenly doesn't exist anymore. I might be very heavily invested into that tool in terms of project files, learning curves, workflow integration etc. I also might be in the middle of something very important with a deadline at the moment that I'm unable to access the site. Other points that weren't raised - I want to be free to work in situations where I have poor or no internet e.g. when traveling. Tying tools down to whether or not a website is available and you have reliable internet access is a huge step backwards in my opinion. reply Timon3 6 hours agoparentprevI've long wished for something like OpenBSDs pledge to be available in browsers, ideally both through meta tags and through JS APIs. Once a pledge is made, the resource will be unavailable to the page until it's closed, like: - I pledge to only make network connections to X, Y and Z - I pledge to only make GET requests to http://example.com/foo/* - I pledge not to use canvas, iFrames or storage APIs This info wouldn't be immediately useful to most users, but it could massively help experienced users with trusting local utilities. reply gwervc 6 hours agorootparent>- I pledge to only make GET requests to http://example.com/foo/* Doesn't solve any trust issue since data can be send as part of the URL, and the backend response can change at will. reply Timon3 5 hours agorootparentThat was just an example - it fully solves trust issues if the pledge is \"only make GET requests to exactly example.com/favicon.ico or example.com/style.css\". This way you can't send any data (as there's no body, and encoded data wouldn't match the URLs). reply Tajnymag 5 hours agorootparentprevWhat you are describing are essentially an extended version of various security http headers. * first requirement can already be done using Content-Security-Policy header * haven't found a suitable header for the second requirement * third requirement can be done with Permissions-Policy header reply Timon3 4 hours agorootparentThat's partially true, but it would be important for this to both work without a server, and at runtime. Not relying on a server makes this functionality available for downloaded sites. I'm a big fan of offering single file builds for web utilities, and the pledge should be part of that build instead of something the user supplies. Having this as a runtime API would enable easier integration - say I'm developing a video editor that needs some WASM blobs. It might be a lot easier to load the blobs and pledge no further network access than having the URLs known on the server-side. reply ilrwbwrkhv 5 hours agorootparentprevSerenity OS also makes use of pledge. The episode in which Andreas kicks it off was delightful to watch. reply robxorb 8 hours agoparentprevSome crypto wallets, facing similar concerns but with I suppose higher stakes, will provide the user download a local copy of the software, load offline a private tab, close it when done and only then, go back online again. A bit fiddly for sure - but seems comprehensive enough. reply beardyw 7 hours agorootparentNeeds browser support really. Probably harder than we imagine. reply lifty 6 hours agorootparentThere should be an electron local app for running offline web apps. Shouldn't be too hard to build. reply creshal 6 hours agorootparentDesktop browsers have surprisingly reasonable support for offline PWAs and integrate them as desktop shortcuts etc. Better than Android and iOS, in my experience, although neither is a hard bar to clear. reply PetitPrince 6 hours agoparentprevNaive answer: isn't the browser network tab enough? reply langcss 6 hours agorootparentIt only shows past behaviour so not completely a proof that nothing could be sent. reply Kye 5 hours agorootparentIn which browser? It's a live view in Chrome and Firefox. reply Zambyte 5 hours agorootparent\"Live view\" means a log of the past, not potential futures. reply Kye 5 hours agorootparentI don't know what you're on about, but it does show the past and any new network activity. reply aaarrm 5 hours agorootparentThey're saying you won't know until after a request is already sent, and seem to be implying that this somehow stops someone from learning if data is sent to the server or not. I think they've forgotten the original point of this thread because their replies are missing the point reply Kye 5 hours agorootparentI'm not sure. The impression I get is they're not aware that the tab isn't just a log of stuff before the page \"finishes\" loading, or not aware that the notion of a static page that can't make network requests at any time without a full reload went out with AJAX in the 2000s. reply Zambyte 5 hours agorootparentprevYes. Exactly. It omits future network calls (things that have not yet happened by the moment you look), which is what the person you were replying to was talking about. reply Kye 5 hours agorootparentIt does not omit future network calls. You can, in fact, use the network tab to monitor a page's ongoing network activity as originally suggested. reply recursive 1 hour agorootparentYou won't be able to see that activity until after it has happened. An empty network monitor list isn't a guarantee of future behavior. Or current behavior. reply Kye 48 minutes agorootparentOkay. Then solve p=np. Until then, we monitor and reverse engineer to verify as best we can. reply recursive 46 minutes agorootparentIt doesn't need to be that hard. A reasonable solution is to quarantine the tab/app. Proactively revoke its network access after its loaded. reply sadops 4 hours agoparentprevIn what way? Good luck using this thing if the network is down, or if the website is down, or if DNS is down, or if the domain expires, or if the author disappears. A program you download and run is yours forever, a website can disappear tomorrow, or get acquired and get enshittified. It happens every single time, and then there's a thousand-comment thread here, until the next web app that everyone loves, and the cycle repeats itself. Do we never learn? Am I taking crazy pills? Break the cycle. reply MitPitt 6 hours agoparentprevWhat about just turning off your network? reply sadops 4 hours agorootparentWhat about programs that run on your computer so you don't even need the Internet for them? reply latexr 6 hours agorootparentprevThat means you have to keep the network off for as long as you’re using the app, which is inconvenient. reply Mashimo 6 hours agorootparentIn chrome you can turn off the network per tab. reply 42lux 6 hours agorootparentIn Firefox I use this extension for the same purpose https://addons.mozilla.org/en-US/firefox/addon/work-offline-... reply sadops 4 hours agorootparentThis only works on Google's OS, Chrome. reply AshleysBrain 7 hours agoparentprev> If only it was possible to prove nothing goes back to the server That's an interesting question, but I think it's also equally difficult to prove for non-browser software. reply _flux 6 hours agorootparentWhy do you say it's equally difficult? By limiting network operations of a local application you can indeed prove this, as long as you trust the facilities provided by the operating system. With web applications doing the same is more difficult, because you need to pass some requests, and some requests need to pass while others could be smuggling data. reply RamblingCTO 7 hours agorootparentprevOn macs I feel like little snitch or LuLu are the norm. I wonder why, given that Windows and windows apps are historically more inclined to install stuff you don't want. Anyway, both are outgoing network monitors/firewalls and it's one of the first things I install on a new system. reply fl0id 7 hours agorootparentPretty sure they are very far from the norm, in % of Mac users reply bananamerica 7 hours agorootparentNo making any accusations but I used Little Snitch extensively at a shop that didn't pay licenses for either Final Cut or the Adobe Suite. reply ottorocket 9 hours agoprevFunny how \"No AI\" has become an feature. As someone how doesn't know anything about motion design, this looks great! reply smolder 8 hours agoparentThe other day I recommended someone try out pixlr as a free image editor, as I remember it being a nice tool, and they told me \"I don't want an AI tool. I hate AI.\" I was confused, went to the site, and saw it is plastered with adverts for some AI image generation features. The actual editor seems buried. People do NOT like having AI stuff shoved in their face. The investors out there pushing every company to develop an AI strategy or whatever are misguided. reply tigeroil 7 hours agorootparentYou say that but in my experience the same kinds of people who will say that are also the kinds of people who demonstrably don't actually even know what AI is. They're the same types who'll insist that DALL-E is just making collages of other artists' work, for example. reply spookie 6 hours agorootparentI think it's important to contextualize the situation. The Adobe TOS were updated, and people where understandably concerned over the rights Adobe now has over private customer data. The biggest issue was this part: \"Licenses to Your Content. Solely for the purposes of operating or improving the Services and Software, you grant us a non-exclusive, worldwide, royalty-free sublicensable, license, to use, reproduce, publicly display, distribute, modify, create derivative works based on, publicly perform, and translate the Content.\" People looked at this and immediately assumed this was added to allow Adobe to train models with people's private work. Adobe has now updated their TOS, but this was a breach of trust. Either way, in the end, this potential AI threat is just another reason to not store stuff in the \"cloud\". reply latexr 6 hours agorootparentprev> You say that but in my experience the same kinds of people who will say that are also the kinds of people who demonstrably don't actually even know what AI is. Funny. In my experience AFK it is the people who use AI that have zero idea what it is and think answers can be blindly trusted. The ones who don’t like it can enumerate the drawbacks clearly. reply Kye 5 hours agorootparentThe pro and anti sides of generative text and generative art seem to be completely separate, so I don't think they can be mixed up like this. On the generative art side, I find the users to be well-versed while the artists who worry about it are currently deleting years of posted work to reupload with questionable anti-AI tools like Glaze long after that art has already been scraped and trained on. FWIW I'm on the pro-artist side and anti-the current state of things, and wish we could start over with a collaboration between technologists and artists rather than each side having nothing but sneering contempt for the other. reply rootlocus 7 hours agorootparentprevIt's a black box that takes human produced artwork without consent and spits out superficial mediocre content a dime a dozen. It also takes away developer time and focus from other aspects of the software. I don't think you need to understand the algorithms underneath to have a problem with that. reply andybak 6 hours agorootparentIt's possible to be a reasonable, thoughtful person and disagree with aspects of what you just wrote. Personally I dislike being morally steamrollered on complex, nuanced topics. reply whywhywhywhy 7 hours agorootparentprev> People do NOT like having AI stuff shoved in their face People don't like having bad or unhelpful AI features crammed into products but seeing the growth of ChatGPT, Midjourney, Adobe Generative Fill, Udio and Luma people definitely do like AI that actually works. reply Kye 4 hours agorootparentThe ML-based similar sound search in Live 12 has been a huge help. It replaced searching for the right percussion sound with a button on the drum rack that moves through similar sounds at an instrument or rack level. I can also use it to search for similar Foley to add variety in the textures that tie the track together. reply rgbrgb 2 hours agoparentprevVery interesting marketing development but I do not at all understand why that would be a feature. Would love someone to explain. \"No crypto\" labels in the last cycle made sense to me. Similar to \"no ads\", it points to the business model incentives and how the product is intended to evolve over time. Conversely, \"no ai\" feels like a very fuzzy line around which editing features will be included (smart lasso tool? object tracking to frame shots? background / foreground selection?). reply kennydude 6 hours agoprevLooks fantastic, and the comment about Photopea. Photopea is such a gem, and can't wait for their Vectorpea to launch as it's got me out of trouble so many times when I don't need an Adobe license for opening a file once a month or so (I just wish Photopea was OpenSource) reply clementpiki 6 hours agoparentYou will be happy to learn vectorpea.com has been online for many monthes already reply kennydude 6 hours agorootparentah nice :) will have to give it a go someday when i need to deal with an illustrator file reply notachatbot1234 8 hours agoprevIt says \"Privacy respected\" but there are Google Ads and Analytics included. reply clementpiki 7 hours agoparentWithout Analytics, I'm blind: I can't tell which feature are popular and which aren't. I need those infos to undertand where I should focus, what are users the more interested in. By \"Privacy respected\", I meant that I am not asking you for your email, your name, and what you do on the editor stays on your machine: no cloud uploads of your files. reply creshal 6 hours agorootparent> Without Analytics, I'm blind: I can't tell which feature are popular and which aren't. There's plenty of alternatives to google analytics though. This sort of basic breakdown could be done with goaccess (or an awk one-liner); plausible, matomo or simple analytics would be decent options that cover most reasonable requirements. reply latexr 6 hours agorootparentprev> Without Analytics, I'm blind: I can't tell which feature are popular and which aren't. I need those infos to undertand where I should focus, what are users the more interested in. I have a suggestion which has always served me well: Ask. Or don’t even ask, users will tell you what they want anyway. Analytics will only give you skewed information, as you are unable to distinguish the popularity of a feature is due to its usefulness, its prominence, or a general lack of clarity. reply joseda-hg 4 hours agorootparentThat's a totally reasoble way to do it, but it leaves you with other blindspots People will tell you what they want anyway but also, sometimes people aren't aware of what they want or need Non visible parts of aproject tend to get neglected a lot more if you just ask your users What will get more people talking to you, a 10% speed up split among many small interactions, or a visual glitch that doesn't affect usability but it's front and center? reply notachatbot1234 5 hours agorootparentprevWhat exactly is tracked though? What information about my videos and my usage patterns are send to Google? It would be great if you could use a self-hosted analytics platform instead! :) And I would strongly advise not to develop based on anonymous analytics, users might tell you different desires if you ask them and use completely different workflows if added. Optimizing for web analytis metrics has ruined many projects. reply smolder 8 hours agoparentprevWith ads/analytics, Google and the site operator know you're hitting certain pages at certain times, but assuming what they wrote under \"privacy respected\" is true, none of your content is uploaded. That's an important distinction to make especially for a browser based app. It's also a very low bar IMO, but one that many other companies aren't clearing anymore, like MS, Adobe, and others. reply aloisdg 8 hours agorootparentit is so easy to avoid Google analytics as a product developer nowadays that this is really a misplay (e.g. goatcounter, etc.) reply liquidise 7 hours agorootparentIt is so easy to avoid GA as a web user that i’m surprised anyone concerned about this isn’t using an blocker that blocks GA scripts and requests themselves. reply baal80spam 8 hours agoprevnext [3 more] [flagged] fragmede 7 hours agoparentYou can install it as a chrome app if that's a problem for you. reply latexr 6 hours agorootparentChrome Apps are suffering a slow death by Google. https://www.theverge.com/2020/1/15/21067907/google-chrome-ap... https://en.wikipedia.org/wiki/Google_Chrome_App > Support for Chrome Apps in the Chrome Web Store was removed from Chrome in June 2022, except on ChromeOS where support has been extended until at least January 2025. reply pointlessone 9 hours agoprev [–] So… What are the features vital that only available in Chrome? reply AshleysBrain 7 hours agoparentWebCodecs and File System Access API I believe. Both pretty essential for an app designed for editing large video files from disk. reply clementpiki 7 hours agorootparentYes, File System Access API is the main issue with Firefox reply bnt 8 hours agoparentprev [–] Not OP but guessing: easier to build/test/debug in Chrome for starters, and if it gains traction try to fix cross-browser bugs. Especially if this is a 1 person show. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Pikimov is a newly launched web-based motion design and video editor, inspired by Photopea, and serves as a free alternative to After Effects.",
      "It requires no signup, keeps files on the user's machine, and ensures that projects are not used for AI training.",
      "The creator has a history of developing VJ software for platforms like the Game Boy Advance, Playstation 2, and Raspberry Pi."
    ],
    "commentSummary": [
      "Pikimov is a new, free, web-based motion design and video editor created as an alternative to Adobe After Effects, requiring no signup or cloud uploads.",
      "The tool has received positive feedback for its potential to challenge Adobe's dominance, with users suggesting improvements in frame rate limits, bug reporting, and keyframe handling.",
      "Currently, Pikimov supports only Chrome and Edge due to specific web APIs, with future plans to add community features and possibly monetize the app."
    ],
    "points": 597,
    "commentCount": 166,
    "retryCount": 0,
    "time": 1719824246
  },
  {
    "id": 40843778,
    "title": "RegreSSHion: RCE in OpenSSH's server, on glibc-based Linux systems",
    "originLink": "https://www.qualys.com/2024/07/01/cve-2024-6387/regresshion.txt",
    "originBody": "Qualys Security Advisory regreSSHion: RCE in OpenSSH's server, on glibc-based Linux systems (CVE-2024-6387) ======================================================================== Contents ======================================================================== Summary SSH-2.0-OpenSSH_3.4p1 Debian 1:3.4p1-1.woody.3 (Debian 3.0r6, from 2005) - Theory - Practice - Timing SSH-2.0-OpenSSH_4.2p1 Debian-7ubuntu3 (Ubuntu 6.06.1, from 2006) - Theory, take one - Theory, take two - Practice - Timing SSH-2.0-OpenSSH_9.2p1 Debian-2+deb12u2 (Debian 12.5.0, from 2024) - Theory - Practice - Timing Towards an amd64 exploit Patches and mitigation Acknowledgments Timeline ======================================================================== Summary ======================================================================== All it takes is a leap of faith -- The Interrupters, \"Leap of Faith\" Preliminary note: OpenSSH is one of the most secure software in the world; this vulnerability is one slip-up in an otherwise near-flawless implementation. Its defense-in-depth design and code are a model and an inspiration, and we thank OpenSSH's developers for their exemplary work. We discovered a vulnerability (a signal handler race condition) in OpenSSH's server (sshd): if a client does not authenticate within LoginGraceTime seconds (120 by default, 600 in old OpenSSH versions), then sshd's SIGALRM handler is called asynchronously, but this signal handler calls various functions that are not async-signal-safe (for example, syslog()). This race condition affects sshd in its default configuration. On investigation, we realized that this vulnerability is in fact a regression of CVE-2006-5051 (\"Signal handler race condition in OpenSSH before 4.4 allows remote attackers to cause a denial of service (crash), and possibly execute arbitrary code\"), which was reported in 2006 by Mark Dowd. This regression was introduced in October 2020 (OpenSSH 8.5p1) by commit 752250c (\"revised log infrastructure for OpenSSH\"), which accidentally removed an \"#ifdef DO_LOG_SAFE_IN_SIGHAND\" from sigdie(), a function that is directly called by sshd's SIGALRM handler. In other words: - OpenSSH buf, 0, buffer->alloc); 38 xfree(buffer->buf); 39 } ------------------------------------------------------------------------ 51 xfree(void *ptr) 52 { 53 if (ptr == NULL) 54 fatal(\"xfree: NULL pointer given as argument\"); 55 free(ptr); 56 } ------------------------------------------------------------------------ Consequently, we started to read the malloc code of this Debian's glibc (2.2.5), to see if a first call to free() can be interrupted by SIGALRM and exploited during a second call to free() inside the SIGALRM handler (at lines 341-344, above). Because this glibc's malloc is not hardened against the unlink() technique pioneered by Solar Designer in 2000, we quickly spotted an interesting code path in chunk_free() (which is called internally by free()): ------------------------------------------------------------------------ 1028 struct malloc_chunk 1029 { 1030 INTERNAL_SIZE_T prev_size; /* Size of previous chunk (if free). */ 1031 INTERNAL_SIZE_T size; /* Size in bytes, including overhead. */ 1032 struct malloc_chunk* fd; /* double links -- used only if free. */ 1033 struct malloc_chunk* bk; 1034 }; ------------------------------------------------------------------------ 2516 #define unlink(P, BK, FD)\\ 2517 { \\ 2518 BK = P->bk;\\ 2519 FD = P->fd;\\ 2520 FD->bk = BK;\\ 2521 BK->fd = FD;\\ 2522 } \\ ------------------------------------------------------------------------ 3160 chunk_free(arena *ar_ptr, mchunkptr p) .... 3164 { 3165 INTERNAL_SIZE_T hd = p->size; /* its head field */ .... 3177 sz = hd & ~PREV_INUSE; 3178 next = chunk_at_offset(p, sz); 3179 nextsz = chunksize(next); .... 3230 if (!(inuse_bit_at_offset(next, nextsz))) /* consolidate forward */ 3231 { .... 3241 unlink(next, bck, fwd); .... 3244 } 3245 else 3246 set_head(next, nextsz); /* clear inuse bit */ .... 3251 frontlink(ar_ptr, p, sz, idx, bck, fwd); ------------------------------------------------------------------------ To exploit this code path, we arrange for sshd's heap to have the following layout (chunk_X, chunk_Y, and chunk_Z are malloc()ated chunks of memory, and p, s, f, b are their prev_size, size, fd, and bk fields): -----|---+---------------|---+---------------|---+---------------|----- ... |p|s|f|b| chunk_X |p|s|f|b| chunk_Y |p|s|f|b| chunk_Z... -----|---+---------------|---+---------------|---+---------------|----- ||user data - First, if a call to free(chunk_Y) is interrupted by SIGALRM *after* line 3246 but *before* line 3251, then chunk_Y is already marked as free (because chunk_Z's PREV_INUSE bit is cleared at line 3246) but it is not yet linked into its doubly-linked list (at line 3251): in other words, chunk_Y's fd and bk pointers still contain user data (attacker- controlled data). - Second, if (inside the SIGALRM handler) packet_close() calls free(chunk_X), then the code block at lines 3230-3244 is entered (because chunk_Y is marked as free) and chunk_Y is unlink()ed (at line 3241): a so-called aa4bmo primitive (almost arbitrary 4 bytes mirrored overwrite), because chunk_Y's fd and bk pointers are still attacker- controlled. For more information on the unlink() technique and the aa4bmo primitive: https://www.openwall.com/articles/JPEG-COM-Marker-Vulnerability#exploit http://phrack.org/issues/61/6.html#article - Last, with this aa4bmo primitive we overwrite the glibc's __free_hook function pointer (this old Debian version does not have ASLR, nor NX) with the address of our shellcode in the heap, thus achieving remote code execution during the next call to free() in packet_close(). ------------------------------------------------------------------------ Practice ------------------------------------------------------------------------ Now they're taking over and they got complete control -- The Interrupters, \"Liberty\" To mount this attack against sshd, we interrupt a call to free() inside sshd's parsing code of a DSA public key (i.e., line 144 below is our free(chunk_Y)) and exploit it during one of the free() calls in packet_close() (i.e., one of the lines 341-344 above is our free(chunk_X)): ------------------------------------------------------------------------ 136 buffer_get_bignum2(Buffer *buffer, BIGNUM *value) 137 { 138 u_int len; 139 u_char *bin = buffer_get_string(buffer, &len); ... 143 BN_bin2bn(bin, len, value); 144 xfree(bin); 145 } ------------------------------------------------------------------------ Initially, however, we were never able to win this race condition (i.e., interrupt the free() call at line 144 at the right time). Eventually, we realized that we could greatly improve our chances of winning this race: the DSA public-key parsing code allows us to call free() four times (at lines 704-707 below), and furthermore sshd allows us to attempt six user authentications (AUTH_FAIL_MAX); if any one of these 24 free() calls is interrupted at the right time, then we later achieve remote code execution inside the SIGALRM handler. ------------------------------------------------------------------------ 678 key_from_blob(u_char *blob, int blen) 679 { ... 693 switch (type) { ... 702 case KEY_DSA: 703 key = key_new(type); 704 buffer_get_bignum2(&b, key->dsa->p); 705 buffer_get_bignum2(&b, key->dsa->q); 706 buffer_get_bignum2(&b, key->dsa->g); 707 buffer_get_bignum2(&b, key->dsa->pub_key); ------------------------------------------------------------------------ With this improvement, we finally won the race condition after ~1 month: we were happy (and did a root-shell dance), but we also felt that there was still room for improvement. ------------------------------------------------------------------------ Timing ------------------------------------------------------------------------ Don't worry, just wait and see -- The Interrupters, \"Haven't Seen the Last of Me\" We therefore implemented the following threefold timing strategy: - We do not wait until the last moment to send our (rather large) DSA public-key packet to sshd: instead, we send the entire packet minus one byte (the last byte) long before the LoginGraceTime, and send the very last byte at the very last moment, to minimize the effects of network delays. (And we disable the Nagle algorithm.) - We keep track of the median round-trip time (by regularly sending packets that produce a response from sshd), and keep track of the difference between the moment we are expecting our connection to be closed by sshd (essentially the moment we receive the first byte of sshd's banner, plus LoginGraceTime) and the moment our connection is really closed by sshd, and accordingly adjust our timing (i.e., the moment when we send the last byte of our DSA packet). These time differences allow us to track clock skews and network delays, which show predictable patterns over time: we experimented with linear and spline regressions, but in the end, nothing worked better than simply re-using the most recent measurement. Possibly, deep learning might yield even better results; this is left as an exercise for the interested reader. - More importantly, we further increase our chances of winning this race condition by slowly adjusting our timing through involuntary feedback from sshd: - if we receive a response (SSH2_MSG_USERAUTH_FAILURE) to our DSA public-key packet, then we sent it too early (sshd had the time to receive our packet in the unprivileged child, parse it, send it to the privileged child, parse it there, and send a response all the way back to us); - if we cannot even send the last byte of our DSA packet, then we waited too long (sshd already received the SIGALRM and closed our connection); - if we can send the last byte of our DSA packet, and receive no response before sshd closes our connection, then our timing was reasonably accurate. This feedback allows us to target what we call the \"large\" race window: hitting it does not guarantee that we win the race condition, but inside this large window are the 24 \"small\" race windows (inside the 24 free() calls) that, if hit, guarantee that we do win the race condition. With these improvements, it takes ~10,000 tries on average to win this race condition; i.e., with 10 connections (MaxStartups) accepted per 600 seconds (LoginGraceTime), it takes ~1 week on average to obtain a remote root shell. ======================================================================== SSH-2.0-OpenSSH_4.2p1 Debian-7ubuntu3 (Ubuntu 6.06.1, from 2006) ======================================================================== ------------------------------------------------------------------------ Theory, take one ------------------------------------------------------------------------ I sleep when the sun starts to rise -- The Interrupters, \"Alien\" The SIGALRM handler of this OpenSSH version does not call packet_close() anymore; moreover, this Ubuntu's glibc (2.3.6) always takes a mandatory lock when entering the functions of the malloc family (even if single- threaded like sshd), which prevents us from interrupting a call to one of the malloc functions and later exploiting it during another call to these functions (they would always deadlock). We must find another solution. CVE-2006-5051 mentions a double-free in GSSAPI, but GSSAPI (or Kerberos) is not enabled by default, so this does not sound very appealing. On the other hand, PAM is enabled by default, and pam_end() is called by sshd's SIGALRM handler (and is, of course, not async-signal-safe). We therefore searched for a PAM function that, if interrupted by SIGALRM at the right time, would leave PAM's internal structures in an inconsistent state, exploitable during pam_end() in the SIGALRM handler. We found pam_set_data(): ------------------------------------------------------------------------ 33 int pam_set_data( 34 pam_handle_t *pamh, .. 37 void (*cleanup)(pam_handle_t *pamh, void *data, int error_status)) 38 { 39 struct pam_data *data_entry; .. 57 } else if ((data_entry = malloc(sizeof(*data_entry)))) { .. 65 data_entry->next = pamh->data; 66 pamh->data = data_entry; .. 74 data_entry->cleanup = cleanup; ------------------------------------------------------------------------ If this function is interrupted by SIGALRM *after* line 66 but *before* line 74, then data_entry is already linked into PAM's structures (pamh), but its cleanup field (a function pointer) is not yet initialized (since the malloc() at line 57 does not initialize its memory). If we are able to control cleanup (through leftovers from previous heap allocations), then we can execute arbitrary code when pam_end() (inside the SIGALRM handler) calls _pam_free_data() (at line 118): ------------------------------------------------------------------------ 104 void _pam_free_data(pam_handle_t *pamh, int status) 105 { 106 struct pam_data *last; 107 struct pam_data *data; ... 112 data = pamh->data; 113 114 while (data) { 115 last = data; 116 data = data->next; 117 if (last->cleanup) { 118 last->cleanup(pamh, last->data, status); ------------------------------------------------------------------------ This would have been an extremely simple exploit; unfortunately, we completely overlooked that pam_set_data() can only be called from PAM modules: if we interrupt it with SIGALRM, then pamh->caller_is is still _PAM_CALLED_FROM_MODULE, in which case pam_end() returns immediately, without ever calling _pam_free_data(). Back to the drawing board. ------------------------------------------------------------------------ Theory, take two ------------------------------------------------------------------------ Not giving up, it's not what we do -- The Interrupters, \"Title Holder\" We noticed that, at line 601 below, sshd passes a pointer to its global sshpam_handle pointer directly to pam_start() (which is called once per connection): ------------------------------------------------------------------------ 202 static pam_handle_t *sshpam_handle = NULL; ------------------------------------------------------------------------ 584 sshpam_init(Authctxt *authctxt) 585 { ... 600 sshpam_err = 601 pam_start(SSHD_PAM_SERVICE, user, &store_conv, &sshpam_handle); ------------------------------------------------------------------------ We therefore decided to look into pam_start() itself: if interrupted by SIGALRM, it might leave the structure pointed to by sshpam_handle in an inconsistent state, which could then be exploited inside the SIGALRM handler, when \"pam_end(sshpam_handle, sshpam_err)\" is called. ------------------------------------------------------------------------ 18 int pam_start ( .. 22 pam_handle_t **pamh) 23 { .. 32 if ((*pamh = calloc(1, sizeof(**pamh))) == NULL) { ... 110 if ( _pam_init_handlers(*pamh) != PAM_SUCCESS ) { ------------------------------------------------------------------------ 319 int _pam_init_handlers(pam_handle_t *pamh) 320 { ... 398 retval = _pam_parse_conf_file(pamh, f, pamh->service_name, PAM_T_ANY ------------------------------------------------------------------------ 66 static int _pam_parse_conf_file(pam_handle_t *pamh, FILE *f .. 73 { ... 252 res = _pam_add_handler(pamh, must_fail, other ------------------------------------------------------------------------ 581 int _pam_add_handler(pam_handle_t *pamh ... 585 { ... 755 the_handlers = (other) ? &pamh->handlers.other : &pamh->handlers.conf; ... 767 handler_p = &the_handlers->authenticate; ... 874 if ((*handler_p = malloc(sizeof(struct handler))) == NULL) { ... 886 (*handler_p)->next = NULL; ------------------------------------------------------------------------ At line 32, pam_start() immediately sets sshd's sshpam_handle to a calloc()ated chunk of memory; this is safe, because calloc() initializes this memory to zero. On the other hand, if _pam_add_handler() (which is called multiple times by pam_start()) is interrupted by SIGALRM *after* line 874 but *before* line 886, then a malloc()ated structure is linked into pamh, but its next field is not yet initialized. If we are able to control next (through leftovers from previous heap allocations), then we can pass an arbitrary pointer to free() during the call to pam_end() (inside the SIGALRM handler), at line 1020 (and line 1017) below: ------------------------------------------------------------------------ 11 int pam_end(pam_handle_t *pamh, int pam_status) 12 { .. 31 if ((ret = _pam_free_handlers(pamh)) != PAM_SUCCESS) { ------------------------------------------------------------------------ 925 int _pam_free_handlers(pam_handle_t *pamh) 926 { ... 954 _pam_free_handlers_aux(&(pamh->handlers.conf.authenticate)); ------------------------------------------------------------------------ 1009 void _pam_free_handlers_aux(struct handler **hp) 1010 { 1011 struct handler *h = *hp; 1012 struct handler *last; .... 1015 while (h) { 1016 last = h; 1017 _pam_drop(h->argv); /* This is all alocated in a single chunk */ 1018 h = h->next; 1019 memset(last, 0, sizeof(*last)); 1020 free(last); 1021 } ------------------------------------------------------------------------ Because the malloc of this Ubuntu's glibc is already hardened against the old unlink() technique, we decided to transform our arbitrary free() into the Malloc Maleficarum's House of Mind (fastbin version): we free() our own NON_MAIN_ARENA chunk, point our fake arena to sshd's .got.plt (this Ubuntu's sshd has ASLR but not PIE), and overwrite _exit()'s entry with the address of our shellcode in the heap (this Ubuntu's heap is still executable by default). For more information on the Malloc Maleficarum: https://seclists.org/bugtraq/2005/Oct/118 ------------------------------------------------------------------------ Practice ------------------------------------------------------------------------ I learned everything the hard way -- The Interrupters, \"The Hard Way\" To mount this attack against sshd, we initially faced three problems: - The House of Mind requires us to store the pointer to our fake arena at address 0x08100000 in the heap; but are we able to store attacker- controlled data at such a high address? Because sshd calls pam_start() at the very beginning of the user authentication, we do not control anything except the user name itself; luckily, a user name of length ~128KB (shorter than DEFAULT_MMAP_THRESHOLD) allows us to store our own data at address 0x08100000. - The size field of our fake NON_MAIN_ARENA chunk must not be too large (to pass free()'s security checks); i.e., it must contain null bytes. But our long user name is a null-terminated string that cannot contain null bytes; luckily we remembered that _pam_free_handlers_aux() zeroes the structures that it free()s (line 1019 above): we therefore \"patch\" the size field of our fake chunk with such a memset(0), and only then free() it. - We must survive several calls to free() (at lines 1017 and 1020 above) before the free() of our fake NON_MAIN_ARENA chunk. We transform these free()s into no-ops by pointing them to fake IS_MMAPPED chunks: free() calls munmap_chunk(), which calls munmap(), which fails because these fake IS_MMAPPED chunks are misaligned; effectively a no-op, because assert()ion failures are not enforced in this Ubuntu's glibc. Finally, our long user name also allows us to control the potentially uninitialized next field of 20 different structures (through leftovers from temporary copies of our long user name), because pam_start() calls _pam_add_handler() multiple times; i.e., our large race window contains 20 small race windows. ------------------------------------------------------------------------ Timing ------------------------------------------------------------------------ Same tricks they used before -- The Interrupters, \"Divide Us\" For this attack against Ubuntu 6.06.1, we simply re-used the timing strategy that we used against Debian 3.0r6: it takes ~10,000 tries on average to win the race condition, and with 10 connections (MaxStartups) accepted per 120 seconds (LoginGraceTime), it takes ~1-2 days on average to obtain a remote root shell. Note: because this Ubuntu's glibc always takes a mandatory lock when entering the functions of the malloc family, an unlucky attacker might deadlock all 10 MaxStartups connections before obtaining a root shell; we have not tried to work around this problem because our ultimate goal was to exploit a modern OpenSSH version anyway. ======================================================================== SSH-2.0-OpenSSH_9.2p1 Debian-2+deb12u2 (Debian 12.5.0, from 2024) ======================================================================== ------------------------------------------------------------------------ Theory ------------------------------------------------------------------------ Now you're ready, take the demons head on -- The Interrupters, \"Be Gone\" The SIGALRM handler of this OpenSSH version does not call packet_close() nor pam_end(); in fact it calls only one interesting function, syslog(): ------------------------------------------------------------------------ 358 grace_alarm_handler(int sig) 359 { ... 370 sigdie(\"Timeout before authentication for %s port %d\", 371 ssh_remote_ipaddr(the_active_state), 372 ssh_remote_port(the_active_state)); ------------------------------------------------------------------------ 96 #define sigdie(...) sshsigdie(__FILE__, __func__, __LINE__, 0, SYSLOG_LEVEL_ERROR, NULL, __VA_ARGS__) ------------------------------------------------------------------------ 451 sshsigdie(const char *file, const char *func, int line, int showfunc, 452 LogLevel level, const char *suffix, const char *fmt, ...) 453 { ... 457 sshlogv(file, func, line, showfunc, SYSLOG_LEVEL_FATAL, 458 suffix, fmt, args); ------------------------------------------------------------------------ 464 sshlogv(const char *file, const char *func, int line, int showfunc, 465 LogLevel level, const char *suffix, const char *fmt, va_list args) 466 { ... 489 do_log(level, forced, suffix, fmt2, args); ------------------------------------------------------------------------ 337 do_log(LogLevel level, int force, const char *suffix, const char *fmt, 338 va_list args) 339 { ... 419 syslog(pri, \"%.500s\", fmtbuf); ------------------------------------------------------------------------ Our two key questions, then, are: Does the syslog() of this Debian's glibc (2.36) call async-signal-unsafe functions such as malloc() and free()? And if yes, does this glibc still take a mandatory lock when entering the functions of the malloc family? - Luckily for us attackers, the answer to our first question is yes; if, and only if, the syslog() inside the SIGALRM handler is the very first call to syslog(), then __localtime64_r() (which is called by syslog()) calls malloc(304) to allocate a FILE structure (at line 166) and calls malloc(4096) to allocate an internal read buffer (at line 186): ------------------------------------------------------------------------ 28 __localtime64_r (const __time64_t *t, struct tm *tp) 29 { 30 return __tz_convert (*t, 1, tp); ------------------------------------------------------------------------ 567 __tz_convert (__time64_t timer, int use_localtime, struct tm *tp) 568 { ... 577 tzset_internal (tp == &_tmbuf && use_localtime); ------------------------------------------------------------------------ 367 tzset_internal (int always) 368 { ... 405 __tzfile_read (tz, 0, NULL); ------------------------------------------------------------------------ 105 __tzfile_read (const char *file, size_t extra, char **extrap) 106 { ... 109 FILE *f; ... 166 f = fopen (file, \"rce\"); ... 186 if (__builtin_expect (__fread_unlocked ((void *) &tzhead, sizeof (tzhead), 1871, f) != 1, 0) ------------------------------------------------------------------------ Note: because we do not control anything about these malloc()ations (not their order, not their sizes, not their contents), we took the \"rce\" at line 166 as a much-needed good omen. - And luckily for us, the answer to our second question is no; since October 2017, the glibc's malloc functions do not take any lock anymore, when single-threaded (like sshd): https://sourceware.org/git?p=glibc.git;a=commit;h=a15d53e2de4c7d83bda251469d92a3c7b49a90db https://sourceware.org/git?p=glibc.git;a=commit;h=3f6bb8a32e5f5efd78ac08c41e623651cc242a89 https://sourceware.org/git?p=glibc.git;a=commit;h=905a7725e9157ea522d8ab97b4c8b96aeb23df54 Moreover, this Debian version suffers from the ASLR weakness described in the following great blog posts (by Justin Miller and Mathias Krause, respectively): https://zolutal.github.io/aslrnt/ https://grsecurity.net/toolchain_necromancy_past_mistakes_haunting_aslr Concretely, in the case of sshd on i386, every memory mapping is randomized normally (sshd's PIE, the heap, most libraries, the stack), but the glibc itself is always mapped either at address 0xb7200000 or at address 0xb7400000; in other words, we can correctly guess the glibc's address half of the time (a small price to pay for defeating ASLR). In our exploit we assume that the glibc is mapped at address 0xb7400000, because it is slightly more common than 0xb7200000. Our next question is: which code paths inside the glibc's malloc functions, if interrupted by SIGALRM at the right time, leave the heap in an inconsistent state, exploitable during one of the malloc() calls inside the SIGALRM handler? We found several interesting (and surprising!) code paths, but the one we chose involves only relative sizes, not absolute addresses (unlike various code paths inside unlink_chunk(), for example); this difference might prove crucial for a future amd64 exploit. This code path, inside malloc(), splits a large free chunk (victim) into two smaller chunks; the first chunk is returned to malloc()'s caller (at line 4345) and the second chunk (remainder) is linked into an unsorted list of free chunks (at lines 4324-4327): ------------------------------------------------------------------------ 1449 #define set_head(p, s) ((p)->mchunk_size = (s)) ------------------------------------------------------------------------ 3765 _int_malloc (mstate av, size_t bytes) 3766 { .... 3798 nb = checked_request2size (bytes); .... 4295 size = chunksize (victim); .... 4300 remainder_size = size - nb; .... 4316 remainder = chunk_at_offset (victim, nb); .... 4320 bck = unsorted_chunks (av); 4321 fwd = bck->fd; .... 4324 remainder->bk = bck; 4325 remainder->fd = fwd; 4326 bck->fd = remainder; 4327 fwd->bk = remainder; .... 4337 set_head (victim, nbPREV_INUSE4338 (av != &main_arena ? NON_MAIN_ARENA : 0)); 4339 set_head (remainder, remainder_sizePREV_INUSE); .... 4343 void *p = chunk2mem (victim); .... 4345 return p; ------------------------------------------------------------------------ - If this code path is interrupted by SIGALRM *after* line 4327 but *before* line 4339, then the remainder chunk of this split is already linked into the unsorted list of free chunks (lines 4324-4327), but its size field (mchunk_size) is not yet initialized (line 4339). - If we are able to control its size field (through leftovers from previous heap allocations), then we can make this remainder chunk larger and overlap with other heap chunks, and therefore corrupt heap memory when this enlarged, overlapping remainder chunk is eventually malloc()ated and written to (inside the SIGALRM handler). Our last question, then, is: given that we do not control anything about the malloc() calls inside the SIGALRM handler, what can we overwrite in the heap to achieve arbitrary code execution before sshd calls _exit() (in sshsigdie())? Because __tzfile_read() (inside the SIGALRM handler) malloc()ates a FILE structure in the heap (at line 166 above), and because FILE structures have a long history of abuse for arbitrary code execution, we decided to aim our heap corruption at this FILE structure. This is, however, easier said than done: our heap corruption is very limited, and FILE structures have been significantly hardened over the years (by IO_validate_vtable() and PTR_DEMANGLE(), for example). Eventually, we devised the following technique (which seems to be specific to the i386 glibc -- the amd64 glibc does not seem to use _vtable_offset at all): - with our limited heap corruption, we overwrite the _vtable_offset field (a single signed char) of __tzfile_read()'s FILE structure; - the glibc's libio functions will therefore look for this FILE structure's vtable pointer (a pointer to an array of function pointers) at a non-zero offset (our overwritten _vtable_offset), instead of the default zero offset; - we (attackers) can easily control this fake vtable pointer (through leftovers from previous heap allocations), because the FILE structure around this offset is not explicitly initialized by fopen(); - to pass the glibc's security checks, our fake vtable pointer must point somewhere into the __libc_IO_vtables section: we decided to point it to the vtable for wide-character streams, _IO_wfile_jumps (i.e., to 0xb761b740, since we assume that the glibc is mapped at address 0xb7400000); - as a result, __fread_unlocked() (at line 186 above) calls _IO_wfile_underflow() (instead of _IO_file_underflow()), which calls a function pointer (__fct) that basically comes from a structure whose pointer (_codecvt) is yet another field of the FILE structure; - we (attackers) can easily control this _codecvt pointer (through leftovers from previous heap allocations, because this field of the FILE structure is not explicitly initialized by fopen()), which also allows us to control the __fct function pointer. In summary, by overwriting a single byte (_vtable_offset) of the FILE structure malloc()ated by fopen(), we can call our own __fct function pointer and execute arbitrary code during __fread_unlocked(). ------------------------------------------------------------------------ Practice ------------------------------------------------------------------------ I wanted it perfect, no wrinkles in it -- The Interrupters, \"In the Mirror\" To mount this attack against sshd's privileged child, let us first imagine the following heap layout (the \"XXX\"s are \"barrier\" chunks that allow us to make holes in the heap; for example, small memory-leaked chunks): ---|----------------------------------------------|---|------------|--- XXX| large hole |XXX| small hole |XXX ---|----------------------------------------------|---|------------|---~8KB| 320B- shortly before sshd receives the SIGALRM, we malloc()ate a ~4KB chunk that splits the large ~8KB hole into two smaller chunks: ---|-----------------------|----------------------|---|------------|--- XXX| large allocated chunkfree remainder chunk |XXX| small hole |XXX ---|-----------------------|----------------------|---|------------|---~4KB~4KB| 320B- but if this malloc() is interrupted by SIGALRM *after* line 4327 but *before* line 4339, then the remainder chunk of this split is already linked into the unsorted list of free chunks, but its size field is under our control (through leftovers from previous heap allocations), and this artificially enlarged remainder chunk overlaps with the following small hole: ---|-----------------------|----------------------|---|------------|--- XXX| large allocated chunkreal remainder chunk |XXX| small hole |XXX ---|-----------------------|----------------------|---|------------|---~4KB || artificially enlarged remainder chunk - when the SIGALRM handler calls syslog() and hence __tzfile_read(), fopen() malloc()ates the small hole for its FILE structure, and __fread_unlocked() malloc()ates a 4KB read buffer, thereby splitting the enlarged remainder chunk in two (the 4KB read buffer and a small remainder chunk): ---|-----------------------|----------------------|---|------------|--- XXX| large allocated chunk|XXX| FILE |XXX ---|-----------------------|----------------------|---|--|---------|---~4KB |||4KB read buffer remainder - we therefore overwrite parts of the FILE structure with the internal header of this small remainder chunk: more precisely, we overwrite the FILE's _vtable_offset with the third byte of this header's bk field, which is a pointer to the unsorted list of free chunks, 0xb761d7f8 (i.e., we overwrite _vtable_offset with 0x61); - then, as explained in the \"Theory\" subsection, __fread_unlocked() calls _IO_wfile_underflow() (instead of _IO_file_underflow()), which calls our own __fct function pointer (through our own _codecvt pointer) and executes our arbitrary code. Note: we have not yet explained how to reliably go from a controlled _codecvt pointer to a controlled __fct function pointer; we will do so, but we must first solve a more pressing problem. Indeed, we learned from our work on older OpenSSH versions that we will never win this signal handler race condition if our large race window contains only one small race window. Consequently, we implemented the following strategy, based on the following heap layout: ---|------------|---|------------|---|------------|---|------------|--- XXX|large hole 1|XXX|small hole 1|XXX|large hole 2|XXX|small hole 2|... ---|------------|---|------------|---|------------|---|------------|---~8KB| 320B| ~8KB| 320BThe last packet that we send to sshd (shortly before the delivery of SIGALRM) forces sshd to perform the following sequence of malloc() calls: malloc(~4KB), malloc(304), malloc(~4KB), malloc(304), etc. 1/ Our first malloc(~4KB) splits the large hole 1 in two: - if this first split is interrupted by SIGALRM at the right time, then the fopen() inside the SIGALRM handler malloc()ates the small hole 1 for its FILE structure, and we achieve arbitrary code execution as explained above; - if not, then we malloc()ate the small hole 1 ourselves with our first malloc(304), and: 2/ Our second malloc(~4KB) splits the large hole 2 in two: - if this second split is interrupted by SIGALRM at the right time, then the fopen() inside the SIGALRM handler malloc()ates the small hole 2 for its FILE structure, and we achieve arbitrary code execution as explained above; - if not, then we malloc()ate the small hole 2 ourselves with our second malloc(304), etc. We were able to make 27 pairs of such large and small holes in sshd's heap (28 would exceed PACKET_MAX_SIZE, 256KB): our large race window now contains 27 small race windows! Achieving this complex heap layout was extremely painful and time-consuming, but the two highlights are: - We abuse sshd's public-key parsing code to perform arbitrary sequences of malloc() and free() calls (at lines 1805 and 573): ------------------------------------------------------------------------ 1754 cert_parse(struct sshbuf *b, struct sshkey *key, struct sshbuf *certbuf) 1755 { .... 1797 while (sshbuf_len(principals) > 0) { .... 1805 if ((ret = sshbuf_get_cstring(principals, &principal, .... 1820 key->cert->principals[key->cert->nprincipals++] = principal; 1821 } ------------------------------------------------------------------------ 562 cert_free(struct sshkey_cert *cert) 563 { ... 572 for (i = 0; i nprincipals; i++) 573 free(cert->principals[i]); ------------------------------------------------------------------------ - We were unable to find a memory leak for our small \"barrier\" chunks; instead, we use tcache chunks (which are never really freed, because their inuse bit is never cleared) as makeshift \"barrier\" chunks. To reliably achieve this heap layout, we send five different public-key packets to sshd (packets a/ to d/ can be sent long before SIGALRM; most of packet e/ can also be sent long before SIGALRM, but its very last byte must be sent at the very last moment): a/ We malloc()ate and free() a variety of tcache chunks, to ensure that the heap allocations that we do not control end up in these tcache chunks and do not interfere with our careful heap layout. b/ We malloc()ate and free() chunks of various sizes, to make our 27 pairs of large and small holes (and the corresponding \"barrier\" chunks). c/ We malloc()ate and free() ~4KB chunks and 320B chunks, to: - write the fake header (the large size field) of our potentially enlarged remainder chunk, into the middle of our large holes; - write the fake footer of our potentially enlarged remainder chunk, to the end of our small holes (to pass the glibc's security checks); - write our fake vtable and _codecvt pointers, into our small holes (which are potential FILE structures). d/ We malloc()ate and free() one very large string (nearly 256KB), to ensure that our large and small holes are removed from the unsorted list of free chunks and placed into their respective malloc bins. e/ We force sshd to perform our final sequence of malloc() calls (malloc(~4KB), malloc(304), malloc(~4KB), malloc(304), etc), to open our 27 small race windows. Attentive readers may have noticed that we have still not addressed (literally and figuratively) the problem of _codecvt. In fact, _codecvt is a pointer to a structure (_IO_codecvt) that contains a pointer to a structure (__gconv_step) that contains the __fct function pointer that allows us to execute arbitrary code. To reliably control __fct through _codecvt, we simply point _codecvt to one of the glibc's malloc bins, which conveniently contains a pointer to one of our free chunks in the heap, which contains our own __fct function pointer to arbitrary glibc code (all of these glibc addresses are known to us, because we assume that the glibc is mapped at address 0xb7400000). ------------------------------------------------------------------------ Timing ------------------------------------------------------------------------ We're running out of time -- The Interrupters, \"As We Live\" As we implemented this third exploit, it became clear that we could not simply re-use the timing strategy that we had used against the two older OpenSSH versions: we were never winning this new race condition. Eventually, we understood why: - It takes a long time (~10ms) for sshd to parse our fifth and last public key (packet e/ above); in other words, our large race window is too large (our 27 small race windows are like needles in a haystack). - The user_specific_delay() that was introduced recently (OpenSSH 7.8p1) delays sshd's response to our last public-key packet by up to ~9ms and therefore destroys our feedback-based timing strategy. As a result, we developed a completely different timing strategy: - from time to time, we send our last public-key packet with a little mistake that produces an error response (lines 138-142 below), right before the call to sshkey_from_blob() that parses our public key; - from time to time, we send our last public-key packet with another little mistake that produces an error response (lines 151-155 below), right after the call to sshkey_from_blob() that parses our public key; - the difference between these two response times is the time that it takes for sshd to parse our last public key, and this allows us to precisely time the transmission of our last packets (to ensure that sshd has the time to parse our public key in the unprivileged child, send it to the privileged child, and start to parse it there, before the delivery of SIGALRM). ------------------------------------------------------------------------ 88 userauth_pubkey(struct ssh *ssh, const char *method) 89 { ... 138 if (pktype == KEY_UNSPEC) { 139 /* this is perfectly legal */ 140 verbose_f(\"unsupported public key algorithm: %s\", pkalg); 141 goto done; 142 } 143 if ((r = sshkey_from_blob(pkblob, blen, &key)) != 0) { 144 error_fr(r, \"parse key\"); 145 goto done; 146 } ... 151 if (key->type != pktype) { 152 error_f(\"type mismatch for decoded key \" 153 \"(received %d, expected %d)\", key->type, pktype); 154 goto done; 155 } ------------------------------------------------------------------------ With this change in strategy, it takes ~10,000 tries on average to win the race condition; i.e., with 100 connections (MaxStartups) accepted per 120 seconds (LoginGraceTime), it takes ~3-4 hours on average to win the race condition, and ~6-8 hours to obtain a remote root shell (because of ASLR). ======================================================================== Towards an amd64 exploit ======================================================================== What's your plan for tomorrow? -- The Interrupters, \"Take Back the Power\" We decided to target Rocky Linux 9 (a Red Hat Enterprise Linux 9 derivative), from \"Rocky-9.4-x86_64-minimal.iso\", for two reasons: - its OpenSSH version (8.7p1) is vulnerable to this signal handler race condition and its glibc is always mapped at a multiple of 2MB (because of the ASLR weakness discussed in the previous \"Theory\" subsection), which makes partial pointer overwrites much more powerful; - the syslog() function (which is async-signal-unsafe but is called by sshd's SIGALRM handler) of this glibc version (2.34) internally calls __open_memstream(), which malloc()ates a FILE structure in the heap, and also calls calloc(), realloc(), and free() (which gives us some much-needed freedom). With a heap corruption as a primitive, two FILE structures malloc()ated in the heap, and 21 fixed bits in the glibc's addresses, we believe that this signal handler race condition is exploitable on amd64 (probably not in ~6-8 hours, but hopefully in less than a week). Only time will tell. Side note: we discovered that Ubuntu 24.04 does not re-randomize the ASLR of its sshd children (it is randomized only once, at boot time); we tracked this down to the patch below, which turns off sshd's rexec_flag. This is generally a bad idea, but in the particular case of this signal handler race condition, it prevents sshd from being exploitable: the syslog() inside the SIGALRM handler does not call any of the malloc functions, because it is never the very first call to syslog(). https://git.launchpad.net/ubuntu/+source/openssh/tree/debian/patches/systemd-socket-activation.patch ======================================================================== Patches and mitigation ======================================================================== The storm has come and gone -- The Interrupters, \"Good Things\" On June 6, 2024, this signal handler race condition was fixed by commit 81c1099 (\"Add a facility to sshd(8) to penalise particular problematic client behaviours\"), which moved the async-signal-unsafe code from sshd's SIGALRM handler to sshd's listener process, where it can be handled synchronously: https://github.com/openssh/openssh-portable/commit/81c1099d22b81ebfd20a334ce986c4f753b0db29 Because this fix is part of a large commit (81c1099), on top of an even larger defense-in-depth commit (03e3de4, \"Start the process of splitting sshd into separate binaries\"), it might prove difficult to backport. In that case, the signal handler race condition itself can be fixed by removing or commenting out the async-signal-unsafe code from the sshsigdie() function; for example: ------------------------------------------------------------------------ sshsigdie(const char *file, const char *func, int line, int showfunc, LogLevel level, const char *suffix, const char *fmt, ...) { #if 0 va_list args; va_start(args, fmt); sshlogv(file, func, line, showfunc, SYSLOG_LEVEL_FATAL, suffix, fmt, args); va_end(args); #endif _exit(1); } ------------------------------------------------------------------------ Finally, if sshd cannot be updated or recompiled, this signal handler race condition can be fixed by simply setting LoginGraceTime to 0 in the configuration file. This makes sshd vulnerable to a denial of service (the exhaustion of all MaxStartups connections), but it makes it safe from the remote code execution presented in this advisory. ======================================================================== Acknowledgments ======================================================================== We thank OpenSSH's developers for their outstanding work and close collaboration on this release. We also thank the distros@openwall. Finally, we dedicate this advisory to Sophia d'Antoine. ======================================================================== Timeline ======================================================================== 2024-05-19: We contacted OpenSSH's developers. Successive iterations of patches and patch reviews followed. 2024-06-20: We contacted the distros@openwall. 2024-07-01: Coordinated Release Date.",
    "commentLink": "https://news.ycombinator.com/item?id=40843778",
    "commentBody": "RegreSSHion: RCE in OpenSSH's server, on glibc-based Linux systems (qualys.com)542 points by robinhoodexe 10 hours agohidepastfavorite211 comments FiloSottile 9 hours agoInterestingly, the RCE fix was \"smuggled\" in public almost a month ago. When PerSourcePenalties are enabled, sshd(8) will monitor the exit status of its child pre-auth session processes. Through the exit status, it can observe situations where the session did not authenticate as expected. These conditions include when the client repeatedly attempted authentication unsucessfully (possibly indicating an attack against one or more accounts, e.g. password guessing), or when client behaviour caused sshd to crash (possibly indicating attempts to exploit sshd). When such a condition is observed, sshd will record a penalty of some duration (e.g. 30 seconds) against the client's address. https://github.com/openssh/openssh-portable/commit/81c1099d2... It's not really a reversable patch that gives anything away to attackers: it changes the binary architecture in a way that has the side-effect of removing the specific vulnerability and also mitigates the whole exploit class, if I understand it correctly. Very clever. reply fanf2 9 hours agoparentThat's not the RCE fix, this is the RCE fix https://news.ycombinator.com/item?id=40843865 That's a previously-announced feature for dealing with junk connections that also happens to mitigate this vulnerability because it makes it harder to win the race. Discussed previously https://news.ycombinator.com/item?id=40610621 reply FiloSottile 8 hours agorootparentThe ones you link are the \"minimal patches for those can't/don't want to upgrade\". The commit I am linking to is taken straight from the advisory. On June 6, 2024, this signal handler race condition was fixed by commit 81c1099 (\"Add a facility to sshd(8) to penalise particular problematic client behaviours\"), which moved the async-signal-unsafe code from sshd's SIGALRM handler to sshd's listener process, where it can be handled synchronously: https://github.com/openssh/openssh-portable/commit/81c1099d22b81ebfd20a334ce986c4f753b0db29 Because this fix is part of a large commit (81c1099), on top of an even larger defense-in-depth commit (03e3de4, \"Start the process of splitting sshd into separate binaries\"), it might prove difficult to backport. In that case, the signal handler race condition itself can be fixed by removing or commenting out the async-signal-unsafe code from the sshsigdie() function The cleverness here is that this commit is both \"a previously-announced feature for dealing with junk connections\", and a mitigation for the exploit class against similar but unknown vulnerabilities, and a patch for the specific vulnerability because it \"moved the async-signal-unsafe code from sshd's SIGALRM handler to sshd's listener process, where it can be handled synchronously\". The cleverness is that it fixes the vulnerability as part of doing something that makes sense on its own, so you wouldn't know it's the patch even looking at it. reply djmdjm 8 hours agorootparentprevNo, it's a fix. It completely removes the signal race as well as introducing a mitigation for similar future bugs reply loeg 47 minutes agoparentprevHas this fix been pushed to / pulled by distributions yet? reply jamilbk 15 minutes agoprevFrom the diff introducing the bug [1], the issue according to the analysis is that the function was refactored from this: void sigdie(const char *fmt,...) { #ifdef DO_LOG_SAFE_IN_SIGHAND va_list args; va_start(args, fmt); do_log(SYSLOG_LEVEL_FATAL, fmt, args); va_end(args); #endif _exit(1); } to this: void sshsigdie(const char *file, const char *func, int line, const char *fmt, ...) { va_list args; va_start(args, fmt); sshlogv(file, func, line, 0, SYSLOG_LEVEL_FATAL, fmt, args); va_end(args); _exit(1); } which lacks the #ifdef. What could have prevented this? More eyes on the pull request? It's wild that software nearly the entire world relies on for secure access is maintained by seemingly just two people [2]. [1] https://github.com/openssh/openssh-portable/commit/752250caa... [2] https://github.com/openssh/openssh-portable/graphs/contribut... reply NelsonMinar 4 hours agoprevOne interesting comment in the OpenSSH release notes > Successful exploitation has been demonstrated on 32-bit Linux/glibc systems with ASLR. Under lab conditions, the attack requires on average 6-8 hours of continuous connections up to the maximum the server will accept. Exploitation on 64-bit systems is believed to be possible but has not been demonstrated at this time. It's likely that these attacks will be improved upon. https://www.openssh.com/releasenotes.html reply fanf2 9 hours agoprevIt’s also worth reading the release notes https://www.openssh.com/releasenotes.html This is actually an interesting variant of a signal race bug. The vulnerability report says, “OpenBSD is notably not vulnerable, because its SIGALRM handler calls syslog_r(), an async-signal-safer version of syslog() that was invented by OpenBSD in 2001.” So a signal-safety mitigation encouraged OpenBSD developers to put non-trivial code inside signal handlers, which becomes unsafe when ported to other systems. They would have avoided this bug if they had done one of their refactoring sweeps to minimize the amount of code in signal handlers, according to the usual wisdom and common unix code guidelines. reply djmdjm 8 hours agoparentTheo de Raadt made an, I think, cogent observation about this bug and how to prevent similar ones: no signal handler should call any function that isn't a signal-safe syscall. The rationale is that, over time, it's too way easy for any transitive call (where it's not always clear that it can be reached in signal context) to pick up some call that isn't async signal safe. reply ralferoo 6 hours agorootparentI'm kind of surprised advocating calling any syscall other than signal to add the handler back again. It's been a long time since I looked at example code, but back in the mid 90s, everything I saw (and so informed my habits) just set a flag, listened to the signal again if it was something like SIGUSR1 and then you'd pick up the flag on the next iteration of your main loop. Maybe that's also because I think of a signal like an interrupt, and something you want to get done as soon as possible to not cause any stalls to the main program. I notice that nowadays signalfd() looks like a much better solution to the signal problem, but I've never tried using it. I think I'll give it a go in my next project. reply qhwudbebd 4 hours agorootparentIn practice when I tried it, I wasn't sold on signalfd's benefits over the 90s style self-pipe, which is reliably portable too. Either way, being able to handle signals in a poll loop is much nicer than trying to do any real work in an async context. reply formerly_proven 3 hours agorootparentprevThis isn't the case for OpenSSH but because a lot of environments (essentially all managed runtimes) actually do this transparently for you when you register a signal \"handler\" it might be that less people are aware that actual signal handlers require a ton of care. On the other hand \"you can't even call strcmp in a signal handler or you'll randomly corrupt program state\" used to be a favorite among practicing C lawyers. reply fanf2 8 hours agorootparentprevExactly, yes :-) Signal handlers have so many hazards it's vital to keep them as simple as possible. reply rwmj 5 hours agorootparentA rule I try to follow: either set a global variable or write to a self pipe (using the write syscall), and handle the signal in the main loop. reply cesarb 1 hour agorootparent> either set a global variable IIRC, the rule is also that said global variable must have the type \"volatile sig_atomic_t\". reply growse 7 hours agorootparentprevI'm not overly familiar with the language and tooling ecosystem, but how trivial is this to detect on a static analysis? reply INTPenis 8 hours agoparentprevSo it's very likely that some young sysadmin or intern that will have to patch for this vuln was not even born when OpenBSD implemented the solution. reply creshal 4 hours agorootparentn>=1, one of our juniors is indeed younger than the OpenBSD fix and dealing with this bug. reply qhwudbebd 7 hours agoprevOnce I'd finished upgrading my openssh instances (which are linked against musl not glibc) I thought it'd be interesting to have a poke at musl's syslog(3) and see if it allocates too and so is easily exploitable in the same way. But as far as I can see, it doesn't: https://github.com/bminor/musl/blob/master/src/misc/syslog.c Everything there is either on stack or in static variables protected from reentrancy by the lock. The {d,sn,vsn}printf() calls there don't allocate in musl, although they might in glibc. Have I missed anything here? reply qhwudbebd 1 hour agoparentConfirmation from Rich: https://fosstodon.org/@musl/112711796005712271 reply singron 5 hours agoparentprevIf you are right about the allocations, then I think the worst it can do is deadlock since the locks aren't recursive. Deadlock in sigalrm could still lead to a DOS since that might prevent it from cleaning up connections. reply mananaysiempre 5 hours agorootparentHeretical opinion: signal handler activations should count as separate threads for the purposes of recursive locking. reply bhawks 1 hour agorootparentHow would be done without introducing deadlock? reply mananaysiempre 1 hour agorootparentYou’d get a deadlock, absolutely. But I’m fine with that: if the thread wants to access some state protected by a mutex, then (effectively) spawns a signal handler activation and waits for it to complete, and the signal handler tries to accept some state protected by the same mutex, then the program has just deadlocked (mutex → signal handler → mutex) and deserves to hang (or die, as this is a very simple situation as far as deadlock detection goes). That’s in any case better than corrupted state. reply qhwudbebd 4 hours agorootparentprevYes, true: if the alarm signal arrives right in the middle of another syslog() call, this should deadlock. (Safer that it not deadlocking and accessing the static state in the signal handler of course!) reply cperciva 9 hours agoprevPatch out for FreeBSD. Not clear if affected (it has only known to be exploitable with glibc, which we don't use) but best to be safe. https://www.freebsd.org/security/advisories/FreeBSD-SA-24:04... reply rfmoz 9 hours agoprevFrom the report: > Finally, if sshd cannot be updated or recompiled, this signal handler race condition can be fixed by simply setting LoginGraceTime to 0 in the configuration file. This makes sshd vulnerable to a denial of service (the exhaustion of all MaxStartups connections), but it makes it safe from the remote code execution presented in this advisory. Setting 'LoginGraceTime 0' in sshd_config file seems to mitigate the issue. reply yjftsjthsd-h 6 hours agoparentHang on, https://www.man7.org/linux/man-pages/man5/sshd_config.5.html says > If the value is 0, there is no time limit. Isn't that worse? reply pm215 4 hours agorootparentThe bug is a race condition which is triggered by code which runs when the timeout expires and the SIGALRM handler is run. If there is no time limit, then the SIGALRM handler will never run, and the race doesn't happen. (As the advisory notes, you do then have to deal with the DoS which the timeout setting is intended to avoid, where N clients all connect and then never disconnect, and they aren't timed-out and forcibly disconnected on the server end any more.) reply yjftsjthsd-h 1 hour agorootparentThanks for the explanation; I'd skimmed a little too fast and assumed that this was the more traditional \"how many attempts can we squeeze in each connection\" rather than something at the end. I guess this makes the hardening advice about lowering that time limit kind of unfortunate. reply sodality2 6 hours agorootparentprevIt sounds like at the end of the default 600 seconds is when the race condition occurs. Set no limit, and there is no end > In our experiments, it takes ~10,000 tries on average to win this race condition; i.e., with 10 connections (MaxStartups) accepted per 600 seconds (LoginGraceTime), it takes ~1 week on average to obtain a remote root shell. reply toast0 3 hours agorootparentprevIf you can turn on TCP keepalive for the server connections, you would still have a timeout, even if it's typically 2 hours. Then if someone wants to keep connections open and run you out of sockets and processes, they've got to keep sockets open on their end (but they might run a less expensive userspace tcp) You can belt and suspenders with an external tool that watches for sshd in pre-auth for your real timeout and kills it or drops the tcp connection [1] (which will make the sshd exit in a more orderly fashion) [1] https://man.freebsd.org/cgi/man.cgi?query=tcpdrop reply 0x0 9 hours agoprevPatch out for Debian 12; Debian 11 not affected. https://security-tracker.debian.org/tracker/CVE-2024-6387 reply wiredfool 7 hours agoparentLooks like Focal (20.04) isn't on an affected version. Jammy (22.04) looks like it is. reply feurio 6 hours agorootparentMy procrastination pays off ... reply metadat 4 hours agorootparentprevWhat about, uh, 18.04? Edit: 18.04 Bionic is unaffected, the ssh version is 7.6 which is too old. reply creshal 3 hours agorootparentIf you have extended support: Just update (if it's not so old that it's not even affected in the first place) If you don't have extended support: You're vulnerable to worse, easier to exploit bugs :) reply theandrewbailey 7 hours agoparentprevJust ran an apt update and upgrade on my Debian 12 server. OpenSSH packages were the only ones upgraded. reply hgs3 4 hours agorootparentYes, the Debian 12 fix is out. You can verify you're patched by running 'ssh -V' and verifying you see 'deb12u3'. If you see 'deb12u2' then you're vulnerable [1]. [1] https://security-tracker.debian.org/tracker/CVE-2024-6387 reply nubinetwork 8 hours agoparentprevCan confirm, Pi OS bullseye also has the updated openssh. reply ementally 22 minutes agoprevhttps://dustri.org/b/notes-on-regresshion-on-musl.html reply CJefferson 9 hours agoprevThis is a really good find. One thing which (as an independant person, who isn't doing any of the work!) is it often feels like in order to 'win', people are expected to find a full chain which gives them remote access, rather than just finding one issue, and getting it fixed / getting paid for it. It feels to me like finding a single hole should be sufficient -- one memory corruption, one sandbox escape. Maybe at the moment there are just too many little issues, that you need a full end-to-end hack to really convince people to take you seriously, or pay out bounties? reply rlpb 8 hours agoparentThere are many wannabe security researchers who find issues that are definitely not exploitable, and then demand CVE numbers and other forms of recognition or even a bounty. For example, there might be an app that crashes when accepting malformed trusted input, but the nature of the app is that it's never intended to and realistically never will be exposed to an adversary. In most people's eyes, these are simply bugs, not security bugs, and while are nice to fix, aren't on the same level. It's not very difficult to find one of these! So there is a need to differentiate between \"real\" security bugs [like this one] and non-security-impacting bugs, and demonstrating how an issue is exploitable is therefore very important. I don't see the need to demonstrate this going away any time soon, because there will always be no end of non-security-impacting bugs. reply tetha 2 hours agorootparentSo many \"Security Researchers\" are just throwing ZAP at websites and dumping the result into the security@ mail, because there might be minor security improvements by setting yet another obscure browser security header for cases that might not even be applicable. Or there is no real consideration if that's actually an escalation of context. Like, \"Oh if I can change these postgres configuration parameters, I can cause a problem\", or \"Oh if I can change values in this file I can cause huge trouble\". Except, modifying that file or that config parameter requires root/supervisor access, so there is no escalation because you have full access already anyhow? I probably wouldn't have to look at documentation too much to get postgres to load arbitrary code from disk if I have supervisor access to the postgres already. Some COPY into some preload plugin, some COPY / ALTER SYSTEM, some query to crash the node, and off we probably go. But yeah, I'm frustrated that we were forced to route our security@ domain to support to filter out this nonsense. I wouldn't be surprised if we miss some actually important issue unless demonstrated like this, but it costs too much time otherwise. reply PhilipRoman 6 hours agorootparentprevAgreed, I've seen all kinds of insane stuff, like \"setting this public field of a java class to a garbage value will cause a null pointer exception\" reply nubinetwork 8 hours agorootparentprev> There are many wannabe security researchers who find issues that are definitely not exploitable, and then demand CVE numbers and other forms of recognition or even a bounty I believe this has happened to curl several times recently. reply umanwizard 6 hours agorootparentIt happens constantly to any startup with a security@ email address. reply michaelt 8 hours agoparentprev> Maybe at the moment there are just too many little issues, that you need a full end-to-end hack to really convince people to take you seriously, or pay out bounties? Let me give you a different perspective. Imagine I make a serialisation/deserialisation library which would be vulnerable if you fed it untrusted data. This is by design, users can serialise and deserialise anything, including lambda functions. My library is only intended for processing data from trusted sources. To my knowledge, nobody uses my library to process data from untrusted sources. One popular library does use mine to load configuration files, they consider those a trusted data source. And it's not my job to police other people's use of my library anyway. Is it correct to file a CVE of the highest priority against my project, saying my code has a Remote Code Execution vulnerability? reply mtrantalainen 4 hours agorootparentI think that if the documented interface of your library is \"trusted data only\", then one shouldn't even file a bug report against your library if somebody passes it untrusted data. However, if you (or anybody else) catch a program passing untrusted data to any library that says \"trusted data only\", that's definitely CVE worthy in my books even if you cannot demonstrate full attack chain. However, that CVE should be targeted at the program that passes untrusted data to trusted interface. That said, if you're looking for bounty instead of just some publicity in reward for publishing the vulnerability, you must fullfil the requirements of the bounty and those typically say that bounty will be paid for complete attack chain only. I guess that's because companies paying bounties are typically interested in real world attacks and are not willing to pay bounties for theoretical vulnerabilities. I think this is problematic because it causes bounty hunters to keep theoretical vulnerabilities secret and wait for possible future combination of new code that can be used to attack the currently-theoretical vulnerability. I would argue that it's much better to fix issues while they are still theoretical only. Maybe pay lesser bounty for theoretical vulnerabilities and pay reduced payment for the full attack chain if it's based on publicly known theoretical vulnerability. Just make sure that the combination pays at least equally good to publishing full attack chain for 0day vulnerability. That way there would be incentive to publish theoretical vulnerabilities immediately for maximum pay because otherwise somebody else might catch the theoretical part and publish faster than you can. reply leftcenterright 8 hours agoparentprevHaving been on the reporting side, \"an exploitable vulnerability\" and \"security weakness which could eventually result in an exploitable vulnerability\" are two very different things. Bounties always get paid for the first category. Reports falling in the second category might even cause reputation/signal damage for a lack of proof of concept/exploitability. There are almost always various weaknesses which do not become exploitable until and unless certain conditions are met. This also becomes evident in contests like Pwn2Own where multiple vulnerabilities are often chained to eventually take the device over and remain un-patched for years. Researchers often sit on such weaknesses for a long time to eventually maximize the impact. Sad but that is how it is. reply tptacek 1 hour agoparentprevBuyers pay for outcomes. Vendors do pay for individual links in the chain. reply lenlorijn 8 hours agoparentprevAs the security maxim goes: POC || GTFO reply bobmcnamara 7 hours agoparentprev> It feels to me like finding a single hole should be sufficient -- one memory corruption, one sandbox escape. It should be. > Maybe at the moment there are just too many little issues... There are so many. reply djmdjm 10 hours agoprevOpenSSH release notes: https://www.openssh.com/txt/release-9.8 Minimal patches for those can't/don't want to upgrade: https://marc.info/?l=oss-security&m=171982317624594&w=2 reply morsch 9 hours agoparent> Exploitation on 64-bit systems is believed to be possible but has not been demonstrated at this time. reply aaronmdjones 8 hours agorootparentExploits only ever get better. Today's possible is next month's done. reply djmdjm 9 hours agorootparentprevI'm confident that someone will make a workable exploit against 64-bit systems. reply runjake 2 hours agorootparentContext here: djmdjm is Daniel Miller, an OpenSSH/OpenBSD developer. reply yjftsjthsd-h 7 hours agoprev> Exploitation on non-glibc systems is conceivable but has not been examined. ( https://www.openssh.com/txt/release-9.8 ) Darn - here I was hoping Alpine was properly immune, but it sounds more like \"nobody's checked if it works on musl\" at this point. reply _ikke_ 3 hours agoparent> OpenSSH sshd on musl-based systems is not vulnerable to RCE via CVE-2024-6387 (regreSSHion). https://fosstodon.org/@musl/112711796005712271 reply jesprenj 9 hours agoprev> Finally, if sshd cannot be updated or recompiled, this signal handler race condition can be fixed by simply setting LoginGraceTime to 0 in the configuration file. This makes sshd vulnerable to a denial of service (the exhaustion of all MaxStartups connections), but it makes it safe from the remote code execution presented in this advisory. reply ttul 5 hours agoprevTLDR: this vulnerability does appear to allow an attacker to potentially gain remote root access on vulnerable Linux systems running OpenSSH, with some important caveats: 1. It affects OpenSSH versions 8.5p1 to 9.7p1 on glibc-based Linux systems. 2. The exploit is not 100% reliable - it requires winning a race condition. 3. On a modern system (Debian 12.5.0 from 2024), the researchers estimate it takes: - ~3-4 hours on average to win the race condition - ~6-8 hours on average to obtain a remote root shell (due to ASLR) 4. It requires certain conditions: - The system must be using glibc (not other libc implementations) - 100 simultaneous SSH connections must be allowed (MaxStartups setting) - LoginGraceTime must be set to a non-zero value (default is 120 seconds) 5. The researchers demonstrated working exploits on i386 systems. They believe it's likely exploitable on amd64 systems as well, but hadn't completed that work yet. 6. It's been patched in OpenSSH 9.8p1 released in June 2024. reply INTPenis 8 hours agoprevCorrect me if I'm wrong but it seems like sshd on RHEL-based systems is safe because they never call syslog. They run sshd with the -D option already, logging everything to stdout and stderr, as their systemd already catches this output and sends it to journal for logging. So I don't see anywhere they would be calling syslog, unless sshd does it on its own. At most maybe add OPTIONS=-e into /etc/sysconfig/sshd. reply betaby 1 hour agoprevIn some setups I decided to have jumphost via HAproxy ssl as described there https://www.haproxy.com/blog/route-ssh-connections-with-hapr... so no ssh directly exposed at all. reply poikroequ 5 hours agoprevAfter the xz backdoor a few months ago, I decided to turn off SSH everywhere I don't need it, either by disabling it or uninstalling it entirely. While SSH is quite secure, it's too lucrative a target, so it will always pose a risk. reply rwmj 5 hours agoparentSo .. how do you handle remote logins? reply daneel_w 3 hours agorootparent\"everywhere I don't need it\" likely implies computers he or she only accesses directly on the console. reply kraftverk_ 4 hours agoparentprevWhat do you use in place of it? reply password4321 1 hour agorootparenthttps://www.aurga.com/ ? Because an $80 black box wireless KVM from a foreign country is way more secure! (Just kidding, though it is not internet-accesible by default.) reply daneel_w 3 hours agorootparentprevA keyboard and a display, aka \"the console\". For example when using one's laptop or sitting at their stationary PC. reply lupusreal 5 hours agoparentprevI now only bind services to wireguard interfaces. The bet is that a compromise in both the service and wireguard at the same time is unlikely (and I have relatively high confidence in wireguard.) reply devsda 4 hours agorootparentI'm confident in making ssh changes while logged in via ssh. Compared to ssh, wireguard configs feel too easy to mess up and risk getting locked out if its the only way of accessing the device. reply someplaceguy 4 hours agorootparentprev> The bet is that a compromise in both the service and wireguard at the same time is unlikely An RCE in wireguard would be enough -- no need to compromise both. reply gavinhoward 4 hours agoprevAs someone who does unspeakable, but safe, things in signal handlers, I can confirm that it is easy to stray off the path of async-signal-safety. reply guerby 3 hours agoparentI agree and I'm surprised OpenSSH developpers did not remove the use of SIGALRM and replace it by select/poll timer and explicitly managed future event list. Likely more portable and safe by default from this class of bugs that has bitten ssh code more than one time now... Defensive programming tells us to minize code in signal handlers and the safest is to avoid using the signal at all when possible :). reply letters90 9 hours agoprev> In our experiments, it takes ~10,000 tries on average to win this race condition, so ~3-4 hours with 100 connections (MaxStartups) accepted per 120 seconds (LoginGraceTime). Ultimately, it takes ~6-8 hours on average to obtain a remote root shell, because we can only guess the glibc's address correctly half of the time (because of ASLR). Mitigate by using fail2ban? Nice to see that Ubuntu isn't affected at all reply mmsc 8 hours agoparent>Mitigate by using fail2ban? In theory, this could be used (much quicker than the mentioned days/weeks) to get local privilege escalation to root, if you already have some type of shell on the system already. I would assume that fail2ban doesn't block localhost. reply udev4096 7 hours agorootparentHow is local privilege escalation relevant here? Fail2ban should be able to block the RCE reply mmsc 7 hours agorootparentHow is it not? If fail2ban isn't going to blocklist localhost, then it isn't a mitigation for this vulnerability because RCE implies LPE. reply DEADMINCE 6 hours agorootparentPeople are generally not trying to get root via an SSH RCE over localhost. That's going to be a pretty small sample of people that applies to. But, sure, in that case fail2ban won't mitigate, but that's pretty damn obviously implied. For 99% of people and situations, it will. reply mmsc 5 hours agorootparent>People are generally not trying to get root via an SSH RCE over localhost. That's going to be a pretty small sample of people that applies to It's going to apply to the amount of servers that an attacker has low-privileged access (think: www-data) and an unpatched sshd. Attackers don't care if it's an RCE or not: if a public sshd exploit can be used on a system with a Linux version without a public Linux LPE, it will be used. Being local also greatly increases the exploitability. Then consider the networks where port 22 is blocked from the internet but sshd is running in some internal network (or just locally for some reason). reply DEADMINCE 5 hours agorootparent> It's going to apply to the amount of servers that an attacker has low-privileged access (think: www-data) and an unpatched sshd. Right, which is almost none. www-data should be set to noshell 99% of the time. > or just locally for some reason). This is all that would be relevant, and this is also very rare. reply infotogivenm 4 hours agorootparentThink “illegitimate” access to www-data. It’s very common on linux pentests to need to privesc from some lower-privileged foothold (like a command injection in an httpd cgi script). Most linux servers run openssh. So yes I would expect this turns out to be a useful privesc in practice. reply DEADMINCE 4 hours agorootparent> Think “illegitimate” access to www-data. I get the point. My point was the example being given is less than 1% of affected cases. > It’s very common on linux pentests to need to privesc from some lower-privileged foothold Sure. Been doing pentests for 20+ years :) > So yes I would expect this turns out to be a useful privesc in practice. Nah. reply infotogivenm 4 hours agorootparent> Nah I don’t get it then… Do you never end up having to privesc in your pentests on linux systems? No doubt it depends on customer profile but I would guess personally on at least 25% of engagements in Linux environments I have had to find a local path to root. reply DEADMINCE 3 hours agorootparent> Do you never end up having to privesc in your pentests on linux systems? Of course I do. I'm not saying privsec isn't useful, I'm saying the cases where you will ssh to localhost to get root are very rare. Maybe you test different environment or something, but on most corporate networks I test the linux machines are dev machines just used for compiling/testing and basically have shared passwords, or they're servers for webapps or something else where normal users most who have a windows machine won't have a shell account. If there's a server where I only have a local account and I'm trying to get root and it's running an ssh server vulnerable to this attack, of course I'd try it. I just don't expect to be in that situation any time soon, if ever. reply mmsc 2 hours agorootparent>I test the linux machines are dev machines just used for compiling/testing and basically have shared passwords, or they're servers for webapps or something else where normal users most who have a windows machine won't have a shell account. And you don't actually pentest the software which those users on the windows machine are using on the Linux systems? So you find a Jenkins server which can be used to execute Groovy scripts to execute arbitrary commands, the firewall doesn't allow connections through port 22, and it's just a \"well, I got access, nothing more to see!\"? reply DEADMINCE 2 hours agorootparent> And you don't actually pentest the software which those users on the windows machine are using on the Linux systems? You really love your assumptions, huh? > it's just a \"well, I got access, nothing more to see!\"? I said nothing like that, and besides that, if you were not just focused on arguing for the sake of it, you would see MY point was about the infrequency of the situation you were talking about (and even then your original point seemed to be contrarian in nature more than anything). reply mmsc 2 hours agorootparentprev>www-data should be set to noshell 99% of the time. Huh? execve(2), of course, lets to execute arbitrary files. No need to spawn a tty at all. https://swisskyrepo.github.io/InternalAllTheThings/cheatshee... >This is all that would be relevant, and this is also very rare. Huh? Exploiting an unpatched vulnerability on a server to get access to a user account is.. very rare? That's exactly what lateral movement is about. reply DEADMINCE 2 hours agorootparentInstead of taking the time to reply 'huh' multiple times, you should make sure you read what you're replying to. For example: > Huh? Exploiting an unpatched vulnerability on a server to get access to a user account is.. very rare? The 'this' I refer to is very clearly not what you've decided to map it to here. The 'this' I refer to, if you follow the comment chain, refers to a subset of something you said which was relevant to your point - the rest was not. reply sgt 5 hours agorootparentprevConfirmed - fail2ban doesn't block localhost. reply skeetmtp 5 hours agoparentprevUbuntu released patches though https://ubuntu.com/security/notices/USN-6859-1 reply ulrikrasmussen 9 hours agoparentprevWhere do you see that Ubuntu isn't affected? reply rs_rs_rs_rs_rs 6 hours agorootparent>Side note: we discovered that Ubuntu 24.04 does not re-randomize the ASLR of its sshd children (it is randomized only once, at boot time); we tracked this down to the patch below, which turns off sshd's rexec_flag. This is generally a bad idea, but in the particular case of this signal handler race condition, it prevents sshd from being exploitable: the syslog() inside the SIGALRM handler does not call any of the malloc functions, because it is never the very first call to syslog(). No mention on 22.04 yet. reply djmdjm 8 hours agoparentprevUbuntu isn't affected _by this exploit_ reply jgalt212 6 hours agorootparentas opposed to the other exploits not being discussed. reply nubinetwork 8 hours agoparentprevUbuntu has pushed an updated openssh. reply simonjgreen 9 hours agoparentprevFor servers you have control over, as an emergency bandaid, sure. Assumes you are not on an embedded system though like a router. reply letters90 9 hours agorootparentI didn't consider embedded, probably the biggest target for this. reply paulmd 6 hours agoparentprev> Ultimately, it takes ~6-8 hours on average to obtain a remote root shell, because we can only guess the glibc's address correctly half of the time (because of ASLR). AMD to the rescue - fortunately they decided to leave the take-a-way and prefetch-type-3 vulnerability unpatched, and continue to recommend that the KPTI mitigations be disabled by default due to performance costs. This breaks ASLR on all these systems, so these systems can be exploited in a much shorter time ;) AMD’s handling of these issues is WONTFIX, despite (contrary to their assertion) the latter even providing actual kernel data leakage at a higher rate than meltdown itself… (This one they’ve outright pulled down their security bulletin on) https://pcper.com/2020/03/amd-comments-on-take-a-way-vulnera... (This one remains unpatched in the third variant with prefetch+TLB) https://www.amd.com/en/resources/product-security/bulletin/a... edit: there is a third now building on the first one with an unpatched vulnerabilities in all zen1/zen2 as well… so this one is WONTFIX too it seems, like most of the defects TU Graz has turned up. https://www.tomshardware.com/news/amd-cachewarp-vulnerabilit... Seriously I don’t know why the community just tolerates these defenses being known-broken on the most popular brand of CPUs within the enthusiast market, while allowing them to knowingly disable the defense that’s already implemented that would prevent this leakage. Is defense-in-depth not a thing anymore? Nobody in the world would ever tell you to explicitly turn off ASLR on an intel system that is exposed to untrusted attackers… yet that’s exactly the spec AMD continues to recommend and everyone goes along without a peep. It’s literally a kernel option that is already running and tested and hardens you against ASLR leakage. The “it’s only metadata” is so tired. Metadata is more important than regular data, in many cases. We kill people, convict people, control all our security and access control via metadata. Like yeah it’s just your ASLR layouts leaking, what’s the worst that could happen? And I mean real data goes too in several of these exploits too, but that’s not a big deal either… not like those ssh keys are important, right? reply JackSlateur 4 hours agorootparentWhat are you talking about ? My early-2022 ryzen 5625U shows: Vulnerabilities: Gather data sampling: Not affected Itlb multihit: Not affected L1tf: Not affected Mds: Not affected Meltdown: Not affected Mmio stale data: Not affected Reg file data sampling: Not affected Retbleed: Not affected Spec rstack overflow: Vulnerable: Safe RET, no microcode Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization Spectre v2: Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected Srbds: Not affected Tsx async abort: Not affected Only regular stuff reply SubzeroCarnage 2 hours agorootparentKPTI won't be default enabled on Linux on AMD CPUs is the issue here. Yet it provides valuable separation between kernel and userspace address ranges. iirc the predecessor to KPTI was made before these hw flaws were announced as a general enhancement to ASLR. AMD aside, Spectre V2 isn't even default mitigated for userspace across the board, you must specify spectre_v2=on for userspace to be protected. https://www.kernel.org/doc/html/latest/admin-guide/kernel-pa... reply SubzeroCarnage 2 hours agorootparentprevAlso if you don't have a bios update available for that newer microcode, give my real-ucode package a try: https://github.com/divestedcg/real-ucode The linux-firmware repo does not provide AMD microcode updates to consumer platforms unlike Intel. reply maxmalkav 3 hours agoprevQuoting some ska tune in a SSH vulnerability report really caught me off ward, but I loved it. reply nfriedly 5 hours agoprevI stopped exposing SSH to the internet years ago. Now I connect over WireGuard, and then run SSH through that when I need to remotely admin something. reply acatton 5 hours agoprevYearly reminder to run your ssh server behind spiped.[1] [2] [3] [1] https://www.tarsnap.com/spiped.html [2] https://news.ycombinator.com/item?id=29483092 [3] https://news.ycombinator.com/item?id=28538750 reply gruez 4 hours agoparentWhat's the advantage of this relatively obscure tool compared to something standard like wireguard or stunnel? reply acatton 2 hours agorootparent* The tool is not obscure, it's packaged in most distributions.[1][2][3] It was written and maintained by Colin Percival, aka \"the tarnsnap guy\" or \"the guy who invented scrypt\". He is the security officer for FreeBSD. * spiped can be used transparently by just putting a \"ProxyCommand\" in your ssh_config. This means you can connect to a server just by using \"ssh\", normally. (as opposed to wireguard where you need to always be on your VPN, otherwise connnect to your VPN manually before running ssh) * As opposed to wireguard which runs in the kernel, spiped can easily be set-up to run as a user, and be fully hardened by using the correct systemd .service configuration [4] * The protocol is much more lightweight than TLS (used by stunnel), it's just AES, padded to 1024 bytes with a 32 bit checksum. [5] * The private key is much easier to set up than stunnel's TLS certificate, \"dd if=/dev/urandom count=4 bs=1k of=key\" and you're good to go. [1] https://packages.debian.org/bookworm/spiped [2] https://www.freshports.org/sysutils/spiped/ [3] https://archlinux.org/packages/extra/x86_64/spiped/ [4] https://ruderich.org/simon/notes/systemd-service-hardening [5] https://github.com/Tarsnap/spiped/blob/master/DESIGN.md reply sisk 15 minutes agorootparent> The private key is much easier to set up than stunnel's TLS certificate, \"dd if=/dev/urandom count=4 bs=1k of=key\" and you're good to go. The spiped documentation recommends a key size with a minimum of 256b of entropy. I'm curious why you've chosen such a large key size (4096b) here? Is there anything to suggest 256b is no longer sufficient for the general case? reply SparkyMcUnicorn 2 hours agorootparentprevWireguard can also run in userspace (e.g. boringtun[0], wireguard-go[1], Tailscale). [0] https://github.com/cloudflare/boringtun [1] https://git.zx2c4.com/wireguard-go/about/ reply betaby 1 hour agoparentprevI run sshd behind the HAProxy https://www.haproxy.com/blog/route-ssh-connections-with-hapr... reply djernie 7 hours agoprevRedHat put an 8.1 score on it: https://access.redhat.com/security/cve/cve-2024-6387 reply zshrc 7 hours agoparentDoesn’t affect RHEL7 or RHEL8. reply chasil 3 hours agorootparentOr RHEL9. $ rpm -q openssh openssh-8.7p1-38.0.1.el9.x86_64 reply indigodaddy 50 minutes agorootparentVersions from 4.4p1 up to, but not including, 8.5p1 are not vulnerable. The vulnerability resurfaces in versions from 8.5p1 up to, but not including, 9.8p1 https://blog.qualys.com/vulnerabilities-threat-research/2024... reply Ianvdl 3 hours agorootparentprev> Statement > The flaw affects RHEL9 as the regression was introduced after the OpenSSH version shipped with RHEL8 was published. reply chasil 2 hours agorootparentHowever, we see the -D option on the listening parent: $ ps axgrep sshdhead -1 1306 ? Ss 0:01 sshd: /usr/sbin/sshd -D [listener] 0 of 10-100 startups As mentioned elsewhere here, is -D sufficient to avoid exploitation, or is -e necessary as well? $ man sshdsed -n '/ -[De]/,/^$/p' -D When this option is specified, sshd will not detach and does not become a daemon. This allows easy monitoring of sshd. -e Write debug logs to standard error instead of the system log. RHEL9 is also 64-bit only, and we see from the notice: \"we have started to work on an amd64 exploit, which is much harder because of the stronger ASLR.\" On top of writing the exploit to target 32-bit environments, this also requires a DSA key that implements multiple calls to free(). There is a section on \"Rocky Linux 9\" near the end of the linked advisory where unsuccessful exploit attempts are discussed. reply Arnavion 5 minutes agorootparent>As mentioned elsewhere here, is -D sufficient to avoid exploitation, or is -e necessary as well? https://github.com/openssh/openssh-portable/blob/V_9_8_P1/ss... sshd.c handles no_daemon (-D) and log_stderr (-e) independently. log_stderr is what is given to log_init in log.c that gates the call to syslog functions. There is a special case to set log_stderr to true if debug_flag (-d) is set, but nothing for no_daemon. I can't test it right now though so I may be missing something. marcus0x62 7 hours agoprevPatch out for Arch Linux https://archlinux.org/packages/core/x86_64/openssh/ edit be sure to manually restart sshd after upgrading; my systems fail during key exchange after package upgrade until restarting the sshd service: % ssh -v 192.168.1.254 OpenSSH_9.8p1, OpenSSL 3.3.1 4 Jun 2024 ... output elided ... debug1: Local version string SSH-2.0-OpenSSH_9.8 kex_exchange_identification: read: Connection reset by peer Connection reset by 192.168.1.254 port 22 reply jiripospisil 5 hours agoparentSame here. It's caused by the sshd daemon being split into multiple binaries. In fact, the commit which introduced the change mentions this explicitly: > NB. if you're updating via source, please restart sshd after installing, otherwise you run the risk of locking yourself out. https://github.com/openssh/openssh-portable/commit/03e3de416... Edit: Already reported at https://gitlab.archlinux.org/archlinux/packaging/packages/op... reply matthewcroughan 6 hours agoprevFor my own setup, I'm looking into Path Aware Networking (PAN) architectures like SCION to avoid exposing paths to my sshd, without having to set up a VPN or port knocking. https://scion-architecture.net reply pgraf 5 hours agoparentGenuinely curious, how would you block an attacker from getting to your SSH port without knowing the path you will connect from (which is the case for remote access) at configuration time? I don‘t see how Path-Aware Networking would replace a VPN solution reply nubinetwork 9 hours agoprevI haven't seen an increase of ssh traffic yet, but the alert only went out a couple hours ago... hopefully distros will ship the patches quickly. reply cperciva 9 hours agoparentThis is the sort of bug which pre-announcement coordination is designed for. Anyone who doesn't have patches ready was either forgotten (I've seen a few instances of \"I thought you were going to tell them!\") or isn't on the ball. reply nubinetwork 9 hours agorootparentGentoo announced it at the same time as qualys, but they're currently trying to backport and bump users to a patched version. https://bugs.gentoo.org/935271 reply nubinetwork 8 hours agorootparentGentoo has pushed the patched version now. reply booi 9 hours agoparentprevi would assume all the distros have patches ready to go awaiting the embargo lift. reply megous 10 hours agoprevIn our experiments, it takes ~10,000 tries on average to win this race condition, so ~3-4 hours with 100 connections (MaxStartups) accepted per 120 seconds (LoginGraceTime). Ultimately, it takes ~6-8 hours on average to obtain a remote root shell, because we can only guess the glibc's address correctly half of the time (because of ASLR). MaxStartups default is 10 reply Haemm0r 9 hours agoparentQuestion regarding this from a non-guru: - Is it correct that this only works for user root if login with password/key for root is allowed? - Is it correct, that this only works if the attacker knows a login name valid for ssh? reply aflukasz 8 hours agorootparentI believe knowing existing user name or using host-depended value does not matter. The exploit tries to interrupt handlers that are being run due to login grace period timing out - so we are already at a point where authentication workflow has ended without passing all the credentials. Plus, in the \"Practice\" section, they discuss using user name value as a way to manipulate memory at a certain address, so they want/need to control this value. reply JackSlateur 4 hours agoparentprevDefault is 100: https://github.com/openssh/openssh-portable/blob/master/serv... reply JeremyNT 3 hours agorootparentThe config option called MaxStartups accepts a tuple to set 3 associated variables in the code. It wasn't clear to me which value people were referring to. reply vesinisa 9 hours agoparentprevEven if it means the attack takes 10x as long, it doesn't seem to be limited by bandwidth, only time. Might not take long before the bots appear that try to automatically exploit this on scale. reply jmclnx 1 hour agoparentprevThe default for MaxStartups is 10:30:100 10:30:60 is mentioned in the man for start:rate:full, so I set mine to that value. Thanks for the quote reply ale42 8 hours agoparentprevSuch an amount of connections should anyway trigger all possible logging & IDS systems, right? reply Piskvorrr 7 hours agorootparentIt should trigger fail2ban, that's for sure. Alerting is useless, with the volume of automated exploits attempted. reply TacticalCoder 7 hours agorootparent> It should trigger fail2ban, that's for sure. But people here are going to explain that fail2ban is security theater... reply BrandoElFollito 4 hours agorootparentI am one of the people who see fail2ban as a nuisance for the average administrator. Average means that they know things on average and sooner or later fail2ban will block unexpectedly. Usually when you are away canoeing in the wilderness. This is all a matter of threat and risk management. If you know what you are doing then fail2ban or portknocking is another layer on your security. Security theater in my opinion is something else: nonsense password policies, hiding your SSID, whitelisting MACs, ... reply DEADMINCE 6 hours agorootparentprevCan you link to any comment in this thread of someone actually claiming that? reply Piskvorrr 6 hours agorootparentprevIt's a doorstop, not a fix. Useful nonetheless. reply johnklos 2 hours agorootparentprevIf you have a public facing Internet server, you're probably already running something like blocklistd or fail2ban. They reduce abuse, but they don't do anything to avoid an issue like this except from naive attackers. More resourceful attackers could automate attempted exploit using a huge botnet, and it'd likely look similar to the background of ssh brute force bots that we already see 24/7/365. reply stefan_ 8 hours agorootparentprevIf you don’t value your time, sure? There’s thousands of systems trying to log into publicly accessible SSH servers all the time. reply XorNot 7 hours agorootparentYeah slow bruteforces are running all over the net all the time. This means there's no reason not to throw this attack into the mix. reply megous 8 hours agorootparentprevI doubt most servers use any such thing. reply lostmsu 5 hours agoprevOut of curiosity, does Windows have anything as disruptive as signals? I assume it is also not vulnerable, because SSH server there do not use glibc. reply MaximilianEmel 6 hours agoprevWas this abused in the wild? reply TacticalCoder 7 hours agoprevAnd who was notoriously not exploitable? The ones hiding sshd behind port knocks. And fail2ban: would work too. And a restrictive firewall: would help too. I don't use port-knocking but I really just don't get all those saying: \"It's security theater\". We had not one but two major OpenSSH \"near fiasco\" (this RCE and the xz lib thing) that were both rendered unusable for attackers by using port knocking. To me port-knocking is not \"security theater\": it adds one layer of defense. It's defense-in-depth. Not theater. And the port-knocking sequence doesn't have to be always the same: it can, say, change every 30 seconds, using TOTP style secret sequence generation. How many exploits rendered cold dead in their tracks by port-knocking shall we need before people stop saying port-knocking is security theater? Other measures do also help... Like restrictive firewalling rules, which many criticize as \"it only helps keep the logs smaller\": no, they don't just help keep the logs smaller. I'm whitelisting the three ISP's IP blocks anyone can reasonably be needing to SSH from: now the attacker needs not only the zero-day, but it also need to know he needs to be on one of those three ISPs' IPs. The argument that consists in saying: \"sshd is unexploitable, so nothing else must be done to protect the server\" is... Dead. reply DEADMINCE 6 hours agoparent> I don't use port-knocking but I really just don't get all those saying: \"It's security theater\". It's not security theater but it's kind of outdated. Single Packet Authentication[0] is a significant improvement. > How many exploits rendered cold dead in their tracks by port-knocking shall we need before people stop saying port-knocking is security theater? Port knocking is one layer, but it shouldn't be the only one, or even a heavily relied upon one. Plenty of people might be in a position to see the sequence of ports you knock, for example. Personally, I think if more people bothered to learn tools like SELinux instead of disabling it due to laziness or fear, that is what would stop most exploits dead. Containers are the middleground everyone attached to instead, though. [0] https://www.cipherdyne.org/fwknop/docs/SPA.html reply throw0101b 6 hours agoparentprev> […] rendered unusable for attackers by using port knocking. Port knocking renders SSH unusable: I'm not going to tell my users \"do this magic network incantation before running ssh\". They want to open a terminal and simply run ssh. See the A in the CIA triad, as well as U in the Parkerian hexad. reply JackSlateur 4 hours agoparentprevPort-knocking is a PITA in theory and even worse in real world : people do not have time nor the will to do wild invocations before getting job done. Unless you are talking about your own personal use-case, in which case, feel free to follow your deepest wishes Firewall is a joke, too. Who can manage hundreds and thousands of even-changing IP ? Nobody. Again: I'm not talking about your personal use-case (yet I enjoy connecting to my server through 4G, whereever I am) Fail2ban, on the other hand, is nice: every systems that relies on some known secret benefits from an anti-bruteforce mechanism. Also, and this is important: fail2ban is quick to deploy, and not a PITA for users. Good stuff. reply password4321 54 minutes agorootparentI am interested in any tools for managing IP allow lists on Azure and AWS. It seems like there should be something fairly polished, perhaps with an enrollment flow for self-management and a few reports/warnings... reply fullspectrumdev 3 hours agorootparentprevI’ve written a few toy port knocking implementations, and doing it right is hard. If your connections crap and there’s packet loss, some of the sequence may be lost. Avoiding replay attacks is another whole problem - you want the sequence to change based on a shared secret and time or something similar (eg: TOTP to agree the sequence). Then you have to consider things like NAT… reply growse 7 hours agoparentprevWhat benefits does port knocking give over and above a simple VPN? They're both additional layers of authentication, except a VPN seems much more rigorous and brings potentially other benefits. In a world where tailscale etc. have made quality VPNs trivial to implement, why would I both with port knocking? reply noname120 5 hours agorootparentPort-knocking is way simpler and relies on extremely basic network primitives. As such the attack surface is considerably smaller than OpenSSH or OpenVPN and their authentication mechanisms. reply johnklos 2 hours agorootparentprevAre you assuming that VPNs are more secure than ssh? reply growse 1 hour agorootparentNo? reply jgalt212 5 hours agorootparentprevVPNs drop your bandwidth speeds by 50% on average. And if tailscale has to use a relay server, instead of a direct connection, bandwidth will drop by 70-80%. reply gruez 4 hours agorootparent>VPNs drop your bandwidth speeds by 50% on average Source? Wireguard can do 1GB/s on decade old processors[1]. Even openvpn can do 258 Mb/s, which realistically can saturate the average home internet connection. Also, if we're talking about SSH connections, why does throughput matter? Why do you need 1 gigabit of bandwidth to transfer a few keystrokes a second? [1] https://www.wireguard.com/performance/ reply jgalt212 1 hour agorootparentWe ran iperf on our multi-cloud and on prem network. > Also, if we're talking about SSH connections, why does throughput matter? scp, among other things, runs over ssh. reply gruez 1 hour agorootparent>scp, among other things, runs over ssh. Ironically scp/sftp caused me more bandwidth headaches than wireguard/openvpn. I frequently experienced cases where scp/sftp would get 10% or even less of the transfer speed compared to a plain http(s) connection. Maybe it was due to packet loss, buffer size, or qos/throttling, but I wasn't able to figure out a definitive solution. reply philodeon 6 hours agoparentprevPort knocking is a ludicrous security measure compared to the combination of: * configuring sshd to only listen over a Wireguard tunnel under your control ( or letting something like Tailscale set up the tunnel for you) * switching to ssh certificate authn instead of passwords or keys reply kazinator 6 hours agorootparentDoes Wireguard work in such a way that there is no trace of its existence to an unauthorized contacting entity? I used port knocking for a while many years ago, but it was just too fiddly and flaky. I would run the port knocking program, and see the port not open or close. If I were to use a similar solution today (for whatever reason), I'd probably go for web knocking. In my case, I didn't see it as a security measure, but just as a way to cut the crap out of sshd logs. Log monitoring and banning does a reasonable job of reducing the crap. reply zekica 5 hours agorootparentThat's one of the original design requirements for wireguard. Unless a packet is signed with the correct key, it won't respond at all. reply _joel 7 hours agoparentprevThose not notoriously exploitable were those using gated ssh access only via known IPs or connecting via tailnets/vpn. reply abofh 6 hours agoparentprevPort knocking works if you have to ssh to your servers, there are many solutions that obviate even that, and leave you with no open ports, but a fully manageable server. I'm guilty of ssm in aws, but the principal applies - the cattle phone home, you only have to call pets. reply benterix 7 hours agoparentprevSure, if you can't afford a more robust access control to your SSH server and for some reason need to make it publicly available then port knocking etc. can be a deterring feature that reduces the attack rate. reply daneel_w 5 hours agoparentprevCamouflage is after all one of nature's most common defenses. Always be quick with patching, though. reply ugjka 6 hours agoparentprevI put all my jank behind wireguard reply cies 6 hours agoparentprevI use port knocking to keep my ssh logs clean. I dont think it adds security (I even brag about using it in public). It allows me to read ssh's logs without having to remove all the script kiddie login attempt spam. reply boxed 6 hours agorootparentSaying you use it publicly doesn't defeat the security it gives though. Unless you publicly say the port knocking sequence. Which would be crazy. reply cies 3 hours agorootparentI meant to say: I dont use it for security, I use it for convenience. reply nj5rq 7 hours agoprevOpenBSD is notably not vulnerable, because its SIGALRM handler calls syslog_r(), an async-signal-safer version of syslog() that was invented by OpenBSD in 2001. Saving the day once again. reply fmbb 6 hours agoparent“async-signal-safer” Just this morning was the first time I read the words MT-Safe, AS-Safe, AC-Safe. But I did not know there were “safer” functions as well. Is there also a “safest” syslog? reply OJFord 6 hours agorootparentFor a word like 'safe', or at least in CS, I would assume that the 'safe' one actually is 'safest'; that 'safer' is ehh it's not safe but it's an improvement on the unsafe one. It's safer. reply ralferoo 6 hours agorootparentSimilarly, safest is normal English means not completely safe, but more safe than the other options. So safe > safest > safer > safe-ish > unsafe. reply p51-remorse 6 hours agorootparentWait, that seems backwards to me as a native English speaker. The superlative version feels more safe. Safest > Safe > (…) reply ralferoo 5 hours agorootparentOne example to help think about this. Say you have 3 friends. Your friend Bob has a net worth of $1 - he is the least rich. Your friend Alex has a net worth $10 - he is richer. Another friend Ben has a net worth of $100 - he is the richest. Richest here is comparative against all 3 of them, but none of them are actually rich. Bill Gates is rich. Bezos is rich. Musk is rich. Someone with a net worth of $100 isn't. You can still have comparisons between the rich too, so Bezos is richer than Gates and he's also the richest if you're just considering the pair. But add Musk to the mix, and he's no longer the richest. I guess that last example looks like you have two attributes - rich as some objective \"has a lot of money\" and comparatively rich (richer, richest). For safe, it's kind of similar, except that as soon as you are saying one thing is safer than the other, then you are implicitly acknowledging that there are areas where the thing isn't safe, and if you're admitting that you can't also call it safe without contradicting yourself. reply ralferoo 5 hours agorootparentA better example is \"pure water\". By it's definition, that's just H2O molecules floating around with nothing else. If you add a single grain of salt to a glass of that water, it's no longer pure. Drinking it you probably wouldn't notice, and some people might colloquially call it \"pure\", but we know it isn't because we added some salt to it. If you add a teaspoon of salt to to a different glass of pure water, it's also no longer pure, and now most people would probably notice the salt and recognise it's not pure. If you add a tablespoon of salt to to a different glass of pure water, it's definitely not pure and you probably wouldn't want to drink it either. You could say the teaspoon of salt glass is purer than the tablespoon of salt glass, the grain of salt glass is purer than both of them and so the purest of the three. And yet, we know that it isn't pure water, because we added something else to it. So pure > purest > purer > less pure. Also note that I was required to use \"less pure\" for the last one, because all of them except pure are \"impure\" or \"not pure\", even though were what I originally thought of writing. reply DEADMINCE 6 hours agorootparentprevNah. If something is 'safe', it's safe, period. If something is safest is, it's only the best of the available options and not necessarily 'safe'. reply nj5rq 6 hours agorootparentprevI would assume he refers to \"safe\" being absolutely safe, while \"safest\" refers to the safest of the existing alternatives? reply fmbb 5 hours agorootparentprevI’m would assume the same. Hence my question. reply apache101 7 hours agoparentprevTheo and team way ahead of their time like always. reply pjmlp 6 hours agorootparentNot always, 36C3 - A systematic evaluation of OpenBSD's mitigations https://www.youtube.com/watch?v=3E9ga-CylWQ reply daneel_w 6 hours agorootparentWouldn't a good systematic evaluation need (or at least benefit from) a few actual working exploits/PoCs? I keep asking this as a long-time OpenBSD user who is genuinely interested in seeing it done, but so far everyone who has said \"it's flawed\" also reserved themselves the convenience of not having to prove their point in a practical sense. reply DEADMINCE 6 hours agorootparent> Wouldn't a good systematic evaluation need (or at least benefit from) a few actual working exploits/PoCs? Sure, see any of the previous exploits for sshd, or any other software shipped in the OpenBSD default install. > I keep asking this as a long-time OpenBSD user who is genuinely interested in seeing it done, but so far everyone who has said \"it's flawed\" also reserved themselves the convenience of not having to prove their point in a practical sense. The point is they have very little in the way of containing attackers and restricting what they can. Until pledge and unveil, almost all their focus in on eliminating bugs which hey, great, but let's have a little more in case you miss a bug and someone breaks in, eh? An insecure dockerized webserver protected with SELinux is safer than Apache on a default OpenBSD install. reply daneel_w 5 hours agorootparent> Sure, see any of the previous exploits for sshd, or any other software shipped in the OpenBSD default install. Would you like to point to one that successfully utilizes a weakness in OpenBSD itself, which is the topic and implied statement of the video, rather than a weakness in some application running under the superuser? Just to underline, I'm not interested in discussing the hows and whys of containing arbitrary applications where one or more portions are running under euid 0. I'm interested in seeing OpenBSD successfully attacked by an unprivileged process/user. reply yjftsjthsd-h 5 hours agorootparentNow to be fair, sshd on OpenBSD is part of OpenBSD rather than an add-on application and I think it would be fair to count exploits in it against the OS, if it had vulnerabilities there. reply DEADMINCE 5 hours agorootparentAny vulns in any package in OpenBSD's package repositories that they audited should count as a vuln against OpenBSD itself. If OpenBSD users installed it through OpenBSD repositories and are running it will they be affected? Yes? Then it counts against the system itself. reply yjftsjthsd-h 5 minutes agorootparentI'm not sure that's fair; was log4j a vulnerability in Ubuntu itself? How about libwebp ( https://news.ycombinator.com/item?id=37657746 )? DEADMINCE 5 hours agorootparentprev> Would you like to point to one that successfully utilizes a weakness in OpenBSD itself, which is the topic and implied statement of the video, rather than a weakness in some application running under the superuser? I'm sorry, what? What kind of nonsense distinction is this? Are you trying to very disingenuously try and claim only kernel exploits count as attacks against OpenBSD? Why the hell wouldn't a webserver zero-day count? If an OS that claims to be security focused can't constrain a misbehaving web server running as root then it's sure as hell not any type of secure OS. > I'm interested in seeing OpenBSD successfully attacked by an unprivileged process/user. You realize there is very little that OpenBSD does to protect against LPE if there is any LPE vuln on their system, right? Surely you're not just advocating for OpenBSD based on their own marketing? If you want to limit the goalposts to kernel vulns or LPE's that already require an account you're free to do so, but that's rather silly and not remotely indicative of real world security needs. If it's a security focused OS, it should provide ways to limit the damage an attacker can do. OpenBSD had very very little in that regard and still does, although things are slightly better now and they have a few toys. And hey, fun fact, if you apply the same OpenBSD methodology and config of having a barebones install, you'll suddenly find at least dozens of other operating systems with equivalent or better track records. Plan 9 has had less vulnerabilities than OpenBSD and has had more thought put into its security architecture[0], so by your metric it's the more secure OS, yeah? [0] http://9p.io/sys/doc/auth.html reply daneel_w 3 hours agorootparent> I'm sorry, what? What kind of nonsense distinction is this? > Are you trying to very disingenuously try and claim only kernel exploits count as attacks against OpenBSD? Not at all. I clearly underlined that I'm not looking for cases fitting that specific scenario. The only moving of goalposts is entirely on your behalf by very disingenously misrepresenting my question in a poor attempt to try make your answer or whatever point fit. And on top of that, the tasteless pretending to be baffled... reply DEADMINCE 3 hours agorootparent> Not at all. I clearly underlined that I'm not looking for cases fitting that specific scenario The thing is, we're trying to talk about the security of OpenBSD compared to its competition. But you're trying to avoid letting anyone do that by saying only an attack against something in the default install you can do with a user account counts, which is absolutely ridiculous. I'm not moving the goalposts nor am I pretending in any sense. Your approach just doesn't make sense, measure or indicate anything useful or relevant about the security of OpenBSD. I stated so and explained why. But hey, keep believing whatever you want buddy. reply zshrc 6 hours agorootparentprevNot always, but they make it their goal to be. Code standards are very strict in OpenBSD and security is always a primary thought... reply j16sdiz 7 hours agoprevNow, how many remote exploit do we have in openbsd? reply cyberpunk 7 hours agoparentNo more than before this; openbsd is not vulnerable to this exploit due to a different syslog() implementation. reply ZiiS 7 hours agorootparentWorth being explicit here. The OpenBSD syslog is not just 'different' enough that it was luckily uneffected. It was intentionally designed to avoid this situation more than 20 years ago. reply fulafel 7 hours agorootparentprevAlso there's no publicly known exploit for this one yet even for Linux. The advisory says Qualys put exploit development on hold to coordinate the fix. reply yjftsjthsd-h 7 hours agoparentprevTwo in living memory? If you know something with a better track record do speak up. reply DEADMINCE 6 hours agorootparentSEL4 and derivatives. For starters. And if you want to simply go by vulnerability counts, as though that meant something, let's throw in MenuetOS and TempleOS. reply yjftsjthsd-h 5 hours agorootparentOkay, let's say if you know something useful with a better record. TempleOS doesn't have network, so while it's genuinely cool it's not useful to most people. MenuetOS does have network but poor software compatibility. I would actually love to see a seL4 distro but AFAIK it's pretty much only ever used as a hypervisor with a \"real\" (normal) OS under it, often (usually?) Linux-based. We can certainly consider to what degree OpenBSD is useful with just the base system, but it does include everything out of the box to be a web server with zero extra software added, including sshd in its native environment. reply DEADMINCE 5 hours agorootparent> Okay, let's say if you know something useful with a better record. Oh, SEL4 is without any doubt useful, it wouldn't be as popular and coveted if it wasn't, but I think you are trying to say widespread. However, you seem to have taken my examples literally and missed my point, which is trying to judge the security of an OS by its vulnerabilities is a terrible, terrible approach. > but it does include everything out of the box to be a web server Sure, and so do plenty of minimal linux distros, and if you use the same metrics and config as OpenBSD then they'll have a similar security track record. And honestly, Linux with one of the RBAC solutions puts OpenBSD's security to shame. Do yourself a favor and watch the CCC talk someone else linked in the thread. reply yjftsjthsd-h 5 hours agorootparent> Oh, SEL4 is without any doubt useful, it wouldn't be as popular and coveted if it wasn't, but I think you are trying to say widespread. There is a laptop running OpenIndiana illumos on my desk. I mean useful, though through the lens of my usecases (read: if it can't run a web browser or server, I don't generally find it useful). I've only really heard of seL4 being popular in embedded contexts (mostly cars?), not general-purpose computers. > However, you seem to have taken my examples literally and missed my point, which is trying to judge the security of an OS by its vulnerabilities is a terrible, terrible approach. No, I think your examples were excellent for illustrating the differences in systems; you can get a more secure system by severely limiting how much it can do (seL4 is a good choice for embedded systems, but in itself currently useless as a server OS), or a more useful system that has more attack surface, but OpenBSD is a weirdly good ratio of high utility for low security exposure. And yes of course I judge security in terms of realized exploits; theory and design is fine, but at some point the rubber has to hit the road. > Sure, and so do plenty of minimal linux distros, and if you use the same metrics and config as OpenBSD then they'll have a similar security track record. Well no, that's the point - they'll be better than \"fat\" distros, but they absolutely will not match OpenBSD. See, for example, this specific sshd vuln, which will affect any GNU/Linux distro and not OpenBSD, because OpenBSD's libc goes out of its way to solve this problem and glibc didn't. > Do yourself a favor and watch the CCC talk someone else linked in the thread. I don't really do youtube - is it the one that handwaves at allegedly bad design without ever actually showing a single exploit? Because I've gotten really tired of people loudly proclaiming that this thing is so easy to exploit but they just don't have time to actually do it just now but trust them it's definitely easy and a real thing that they could do even though somehow it never seems to actually happen. reply cbrozefsky 4 hours agorootparentI’m an OpenBSD fanboi, and the review of mitigations, their origins, efficacy, and history is well worth the time to watch or just review slides. Its not about some claim of vulz. reply DEADMINCE 4 hours agorootparentprev> I mean useful, though through the lens of my usecases Better to stick to standard definitions in the future so you won't have to explain your personal definitions later on. > No, I think your examples were excellent for illustrating the differences in systems; you can get a more secure system by severely limiting how much it can do So you not only missed the point but decided to take away an entirely different message. Interesting. Yes, limiting attack surface is a basic security principle. The examples I gave were not to demonstrate this basic principle, but to show that trying to gauge security by amount of vulnerabilities is foolish. > seL4 is a good choice for embedded systems, but in itself currently useless as a server OS Plan 9 then. Or any of other numerous OS projects that have less vulns than OpenBSD and can meet your arbitrary definition of 'useful'. The point is that trying to measure security by vuln disclosures is a terrible, terrible method and only something someone with no clue about security would use. > but OpenBSD is a weirdly good ratio of high utility for low security exposure. OpenBSD is just niche, that's it. Creating OpenSSH brought a lot of good marketing, but if you really look at the OS from a security perspective and look at features, it's lacking. > Well no, that's the point - they'll be better than \"fat\" distros, but they absolutely will not match OpenBSD. They absolutely will be better than OpenBSD, because they have capabilities to limit what an attacker can do in the event they get access, as opposed to putting all the eggs in the 'find all the bugs before they get exploited' basket. OpenBSD isn't anything special when it comes to security. That, really, is the point. Anything otherwise is marketing or people who have fell for marketing IMO. > I don't really do youtube There's a lot of good content only on that platform. Surely you can use yt-dlp or freetube or something. > is it the one that handwaves at allegedly bad design without ever actually showing a single exploit? That summary isn't remotely accurate, so I'd have to say no. > Because I've gotten really tired of people loudly proclaiming that this thing is so easy to exploit but they just don't have time to actually do it just now but trust them it's definitely easy and a real thing that they could do even though somehow it never seems to actually happen. They have remote holes listed on their homepage. Both those cases led to remote root and this supposedly secure OS had nothing to offer, while most Linux distros did. Let's make this simple. Linux allows you to contain a remote root exploit with tools like RBAC and MAC extensions. OpenBSD offers nothing. In the event both systems have the same vulnerability (of which this titular instance is not an example of) allowing remote root, Linux will be the safer system if set up correctly. But honestly, I've gotten really tired of OpenBSD stans regurgitating that it's supposedly secure and thinking that being able to point to a lack of vulnerabilities in a barebones default install is some kind of proof of that. reply sneak 3 hours agoprev [–] Why are we all still running an ssh server written in an unsafe language in 2024? reply bauruine 3 hours agoparentBecause nobody has written an sshd in a memory safe language with the same track record of safety as OpenSSH. I personally wouldn't trust a new sshd for a few years at least. reply fullspectrumdev 3 hours agorootparentThere’s a Rust library that implements most of the protocol, but I’ve not found a “drop in replacement” using said library yet. Might actually make for a fun side project to build a SSH server using that library and see how well it performs. reply SoftTalker 3 hours agorootparentDoes Rust have some invulnerability to race conditions? reply px43 4 minutes agorootparentThis bug seems to be exploitable due to a memory corruption triggered by the race condition, it's the memory corruption that rust would protect from. reply pornel 24 minutes agorootparentprevIt does have invulnerability to data races. However, that guarantee applies only to data types and code in Rust. The dangerous interaction between signals and other functions is outside of what Rust can help with. reply hosteur 3 hours agoparentprev [–] What is the better working alternative? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A critical vulnerability (CVE-2024-6387) in OpenSSH's server on glibc-based Linux systems allows remote code execution (RCE) due to a signal handler race condition.",
      "The issue, a regression of CVE-2006-5051, affects OpenSSH versions 3.4p1, 4.2p1, and 9.2p1, and involves exploiting the SIGALRM handler to cause heap corruption and execute arbitrary code.",
      "Mitigation includes applying patches that move async-signal-unsafe code out of the SIGALRM handler or setting `LoginGraceTime` to 0, though the latter may cause denial of service."
    ],
    "commentSummary": [
      "A Remote Code Execution (RCE) vulnerability was discovered in OpenSSH's server on glibc-based Linux systems, potentially allowing attackers to gain remote root access.",
      "The fix for this vulnerability was implemented by moving unsafe code from the signal handler to the listener process, making it difficult to backport.",
      "The issue primarily affects 32-bit systems, with exploitation on 64-bit systems believed possible but not yet demonstrated; various distributions have already released patches."
    ],
    "points": 542,
    "commentCount": 211,
    "retryCount": 0,
    "time": 1719823254
  },
  {
    "id": 40841980,
    "title": "Pipes: A spiritual successor to Yahoo Pipes",
    "originLink": "https://www.pipes.digital/docs",
    "originBody": "Pipes Documentation What Pipes is How to use Pipes Sharing Pipes Pipe Output Support License and selfhosting Blocks Feed Filter Replace Combine Duplicate Merge Items Unique Truncate Shorten Sort Download Extract Images Insert Build Feed Pipes as Blocks Webhook Text Input ForEach Tweets (legacy) Integrations Documentation What Pipes is Pipes is a spiritual successor to Yahoo! Pipes, but if you did not know that site, you can think of Pipes as a visual programing editor specialized on feeds, or a visual shell, or simply as a glorified feed configurator. Pipes gives you blocks that can fetch and create feeds, and manipulate them in various ways. Think filtering, extracting, merging and sorting. All you need to do is to connect those blocks with each other. Data just flows through such a pipe, it flows from block to block. At the end Pipes gives you a new feed, which you can give to other programs that support open web standards - such a program could be your feed reader. As input formats Pipes supports RSS, Atom and JSON feeds, it can scrape HTML documents, and it can work with regular text files. How to use Pipes At the left side of the editor is your list of blocks. You can drag and drop those into the canvas to your right, and the regular block with its inputs and outputs will appear. That block can again be dragged around, but more importantly, if you have multiple blocks you can connect their inputs and outputs. Those are the circles at the side of a block - left the inputs, right the outputs. To connect them, click first on the output of one block, and then on the input of another block. Or drag the input onto the output. You should also fill the user inputs, the form elements in the center of some blocks that control their behaviour. Don't forget to connect your last block to the pipe output, the red circle at the far right. See the video above for a short demonstration of how simple it can be to filter an RSS feed. Sharing Pipes You can set pipes to be pulicly visible in the My Pipes menu, where you can also describe them and set tags to make the pipe searchable. Public pipes will be listed in the public pipes page, where they can be liked and forked by others. Note that pipe output feeds are publicly accessible by everyone who knows their url, which contains a ranzomized id and is thus not easy to guess. If you just want someone else to use the feed, you can just give him the pipe feed url. Pipe Output The default output format of a pipe is RSS. RSS is an open format which can be used in a lot of other programs, like feed readers. Each pipe has a pipe output url like https://pipes.digital/feed/pipeid which is shown in the editor of a saved pipe, or in the list of private pipes under My Pipes. If you want to get only the feed items' content you can open https://pipes.digital/feed/pipeid.txt, which will strip all xml feed elements. This is especially useful when your starting point was a text document that got transformed into a feed to work with its data in Pipes. Support Report bugs and feature requests via the Github issue tracker, or send an email to support@pipes.digital. License and selfhosting Pipes started as a non-free project. Now there is Pipes CE, a FOSS version of Pipes under AGPL, that you can run locally. Its home is the same Github Repository used as issue tracker for this site. You could think of this as an open core model. Blocks Feed Download a RSS, Atom or JSON feed. If not pointed directly to the url of such a feed, it will try to find aelement in the head of the linked page, and then download that feed. The feed block is usually the starting point of a pipe. Note that Pipes caches all downloads for a few minutes. It has no block input, but it takes as user input the url to download. In this example usage, the pipe consists of only one single feed block. That's not very useful yet. But note that the feed block input points to a normal HTML page, not an RSS feed. Because Hacker News specifies its RSS feed as a rel=\"alternate\", this pipe will output that feed instead. Filter Filter the input feed for a keyword, and either show only items that contain it or filter them all out. It searches for the word in the items title, description (if existing) and content. The search is case sensitive. If given a regular expression, it will interpret that regular expression instead, enabling more powerful filtering. Write that expression as /expression/, but regular keywords without the outer slashes. It takes the feed it searches through as block input, and the keyword to search for as user input. The second input, the checkbox, toggles whether found items are to be included or to be blocked. In this example usage, the filter block will filter the downloaded feed for all items that contain the keywords \"Bitcoin\" or \"bitcoin\". Filter Language This block checks all feed items for their language they are written in. The default logic is to keep only the items in the selected language. It checks multiple fields of an item for that, but you can limit the check to some specific fields, like only the title or only the description. And with the last checkbox you can turn the logic around and remove all items that are in the selected language. Use this block if you have a multi-language feed and you want to only keep items in your target language, or remove items in the one language that does not work for you. Be aware that the used n-gram detection will not be 100% accurate, and it even will be very inaccurate for very short texts (less than 10 words is bad, 5 should be the absolute minimum). Replace While the filter block is like the search tool grep, the replace block is like sed or the replace function of your texteditor. The first input is the search pattern, the second is its replacement. If given a regular expression, it will interpret that regular expression instead, enabling more powerful filtering. Write that expression as /[aA]bc/, as a regualr expression with surrounding slashes. The replacement pattern supports backreferences (\\1). It takes the feed it searches through as block input, and the regex-enabled keyword to search for as first user input. The second input is its replacement pattern. In this example usage, the filter block will filter the downloaded feed for all items that contain the pattern \"soccer\" or \"Soccer\" and replace them with \"football\". Combine Combine two or more input feeds into one single feed. Combining those feeds will merge their items lists, and will also append their feed title and descriptions. The sort order follows the input order: The output feed has first all items from the first input feed, then all items from the second input feed, and so on. It takes the feeds to merge as block inputs, and has no user input. In this example usage, the pipe downloads multiple feeds and then combines them together. Duplicate Duplicate the input feed into multiple output feeds. You can also think of this as a splitter. This block does not manipulate the input feed in any way. It takes one feed as input, and has no user input. This is the one block that has multiple outputs. In this example usage, the duplicate block duplicates a feed that was already filtered for \"Bitcoin\", to filter it as well for \"Linux\" and \"MacOS\". Those two feeds then are combined together. Merge Items Use this block to merge items together. It will take step by step one item from each input feed and merge them together into one single item. You can optionally specify a replacement format. If it contains \\1 and \\2 it will replace those with the content of the two items. If it is just a regular string, then that string will be between the two item contents. If it is empty they will just be concatenated. The block takes two feeds as input. The user input serves as format, as replacement pattern, but is optional. In this example usage the title and the link of the hacker news feed are placed after one another, a dash is between them and an end added. Unique Remove duplicate items from a feed. Duplicate items could be an accident in the original feed, but more likely is that they occur after merging filtered feeds with items that containing multiple search words. This block uses the items guid to determine that two items are equal. It takes one feed as input, and has no user input. In this example usage, the pipe downloads a feed, duplicates it, filters for \"Bitcoin\" and \"Linux\" and then merges the results. The unique block now makes sure that items that contain both \"Bitcoin\" and \"Linux\" occur only one time in the output feed. Truncate Truncate an input feed to a given length, with the first x items remaining. This is unlikely to be very helpful in a pipe, apart from when an input feed is really long and you want to improve performance. But imagine you have a website and want to show the last few items of a specific feed, then you could make that easier by producing a feed that has exactly as many entries you want to display. It takes one feed as input, and the target amount of items as user input. In this example usage, the feed of the New York Times homepage gets truncated to three items. Shorten Shorten the text inside the selected element. The number defines how many characters are the target text length. Sort Sort the items of an input feed. You can specify the sort order, and which item element to use for sorting. Default is to sort by the items date (its updated element). It takes one feed as input, and two user inputs to set the sort order and the sort item. In this example usage, the Hacker News feed is re-sorted by date, to have the newest items at the top of the list. Download Download a page. This is different from the feed block as it will not try to find a feed. This block is really meant for downloading some data, like a html page, that is then transformed by other blocks to create a regular feed. As such the this block then needs to be connected to the extract block or the builder block, to create a regular feed. It is an alternative starting point for a pipe. Note that Pipes caches all downloads for a few minutes. It has no block input, but it takes as user input the url to download. If on a paid plan, it also allows setting a checkbox to interpret javascript on the downloaded page. In this example usage, the Hacker News page is downloaded and set as feed output. That's not really useful yet. But note that it really is the Hacker News website that the pipe downloads, not its feed. Extract Extract content from a feed, downloaded HTML page or from downloaded JSON. You can use XPath for XML, css selectors for HTML and JSONPath for JSON to select items, and select whether to extract their content or an attribute (which has no effect with JSONPath). If you have a feed which contains data in its item.content - think JSON data in the RSS feed generated by the webhook block - then you can enable start at item.content to use i.e. a JSONPath to get your data out of the XML feed. All matched elements will be the content of the items of a newly created RSS feed, which becomes this blocks output. It takes one feed, or preferably a downloaded HTML page, as input. The css selector or XPath expression is the first user input, the second is to select an attribute instead of the default element content. The checkbox at the bottom control whether the search expression is used for the whole input data, or only for the data in the input feeds item.content. In this example usage, the link targets of the articles on the Hacker News homepage are extracted and set as feed output. Images Extract images from a feed or a downloaded HTML page. The block will select all images currently embeded into the page or feed items. It also gives access to a gallery that opens in an overlay and shows all the images selected. Just for displaying those images and not as block output, the gallery will try to create absolute urls for those images that on the source page were linked with relative urls. It takes one feed or downloaded HTML page as input. The output is a feed with each item containing one image. In this example usage, the images of the articles on the pipes blog are extracted and set as feed output. Tables Extract HTML tables from a feed or a downloaded HTML page. The block will select all HTML tables currently embeded into the page or feed items and convert them to JSON. It then outputs a feed with only the JSON tables as item content. It takes one feed or downloaded HTML page as input. The output is a feed with each item containing a JSON object. In this example usage, the tables of a crawled page are extracted and set as feed output. Insert Insert content taken from one feed and insert it at the specified position of a different feed. This is meant to be combined with the extract block. Take something from one feed with the extract block, connect it to this block, and then connect the feed where you want to add the extracted element. It takes two feeds as inputs. From the first it will take the first item, the second input is the feed that will be modified. The XPath expression specifying where to insert the element is the user input. The third input, the checkbox, toggles whether the new content should be appended to the already existing content at the target item or whether to replace it. In this example usage, the /rss/channel/image node of a soundcloud feed is taken and inserted into the Hacker News feed. Build Feed Build a new feed. The various input feeds are set as its items attributes. That way one can use extracted elements to create a new regular feed, whose items have a proper title, content, date and link. You can also connect a downloaded text file to this, that will then get transformed line by line into feed items. The order is important here: this block will go through all its input feeds simultaneously, and at each step get the content of the current item of the content feed, as well as the content of the current item of each other connected feed, to create a complete feed item. This block can be used in various ways, but its most obvious usage is to create a feed for a website that has no feed of its own. It takes four feeds as input, and the markings in the block show which attribute they will fill. The second input for example will become the new feeds content, and is the only required input. In this example usage, the pipe downloads the Hacker News homepage, duplicates it and extracts three elements: The title, the score and the link target. Those are then given to the feed builder to set the new items title, their content and their link target. Pipes as Blocks Use a pipe as input block. Since pipes output RSS, their feed can effortlessly be used in other Pipes. This allows the creation of more complicated pipes by combining multiple simpler ones, like submodules. It has one output, which is the result feed of the corresponding pipe. In this example usage, the pipe combines two filtered feeds that are created by other pipes. Webhook Use data sent by a webhook as pipe input. When you create this block, it will show a URL, which is your new webhook endpoint. You can instruct other sites - like Github - to POST data to that endpoint. The webhook block will create a feed out of the received data, which can then be manipulated as usual. Note: Stored data is erased after an hour, and the API endpoint is rate limited (currently to 5 requests per minute). It has one output, which is the data stored under its webhook endpoint. In this example usage, the pipe combines two webhook feeds and limits the length to the last 10 items. Textinput Use this block to let other users fill the input of connected blocks. This can make a pipe dynamic. For example, you could have a generic pipe that works with many sites, and this way other users could point the pipe to the site they want to use it on. The first user input is the name of the input. It will be shown to the user in the UI and is also the name of the GET parameter that controls this block when calling the pipe feed. The second user input is the default value that will be used when the parameter is not set. It has one output, which is the text that will be used by the connected input. In this example usage, the pipe filters a user-choosable feed for a search term the user selects as well. The default feed is the one from the New York Times, and the default search term is Trump. That pipe can be found here. ForEach Use this block to repeat the action of a download or feed block for every item of the input feed. Those blocks normally work with the user provided text input, to download a specific url for example. With the ForEach block, instead of acting on a single manually provided URL you could download all the URLs you extract from somewhere else. This can for example be useful to create full text RSS feeds. To control the action of the block, drop a download or feed block on it. The block takes the items it is supposed to act upon as input, usually provided by an extraxt block. It outputs the downloaded URLs (as items in an RSS feed) or fetched feeds (with all the items appended), depending on its configuration. In this example usage the block is configured as download block. The output will be the first two pages linked in the RSS feed of onli-blogging.de. Tweets (legacy) As there is no trustworthy model for further access to the twitter API, the twitter block is now defunct. Integrations Pipes has blocks to fetch data comfortably from some other sites. Those service specific blocks usually take a user or channel name or an url. They then return a normalized RSS feed with updates specific to the target site. Currently supported sites are: Twitter, see here Vimeo Dailymotion Periscope UStream Mixcloud SVT Play Speedrun.com Youtube, integrated into the Feed block Pipes does not have a special agreement with those sites. It just accesses (hidden) existing RSS feeds or uses APIs to create them. For some of those blocks Pipes calls RSS Box to do the feed discovery and creation work.",
    "commentLink": "https://news.ycombinator.com/item?id=40841980",
    "commentBody": "Pipes: A spiritual successor to Yahoo Pipes (pipes.digital)382 points by sea-gold 16 hours agohidepastfavorite109 comments onli 12 hours agoThis is my project :) Nice to see this here again! If there are questions, I'm ready to answer (I have a rather young baby here though that wanted attention while writing this, but she sleeps now, so I think we are good). Pipes saw some internal updates recently. It was fighting with some instability. My first measure was an internal re-architeture, on a small scale. Before, the data was transported as text from block to block. (Almost) Every block parsed the input, created an RSS feed object and then worked with that on the data, only to then output the created RSS feed as string. This was changed to just work with the RSS object directly and move that from block to block. That had some consequences for some existing pipes, mainly because it also changed when the input feeds gets normalized. As that did not help, next step was a server upgrade. I'm not sure whether it was directly the additional processing capacity or the dependency upgrades, but that one helped so far. Knock on wood. Also reconfigured the amount of threads and puma workers, that might have solved a bottleneck somewhere. I sadly never pinpointed exactly what made the server process stop before. I had played with a proper split between web frontend and pipes processing, but at least my approach was not viable. Worked nicely while developing it, immediately crashed with the production workload. And Pipes is not all that huge... I'll have to try again some time. reply Gys 11 hours agoparentWhat I am missing is a block to manipulate contents. For example, I might want to add an AI generated summary to certain feeds. Or if there is no image, add an alternative image. For those cases it could be useful to have a block that can post a feed item (or one selecteble field) to my custom url and add or replace that item with the result. Then I can make my own feed item manipulators for certain tasks. Thinking bigger, you might offer a (third party?) store of custom manipulators: 'add image', 'add summary', etc. reply onli 11 hours agorootparentI like that. I think it would have to be all items that get POSTed one by one to a url to work with the current flow, but that should be equally useful, no? Pipes does have some building blocks to prepare that operation or work with the result, there is an extract block and an insert block (that works by xpath). But I was always missing a model for the request block that I remember from Yahoo Pipes, that's why there is only a feed and a generic download block so far. Post somewhere, replace the item (or the item's content?) with the answer, that might be it. Edit: Thinking a bit more about this, don't forget that you can already chain pipes. A service you can POST to can probably also consume an RSS feed, download it by itself. You can then have one pipe that creates the feed up to the point where you would want to have it be treated by something else, let that thing output its result, and in a second pipe download the output and create the final feed. I still think the request block to POST items is a good idea, and there are probably services that can't store the output for the second pipe as they'd just react directly to a POST, but that chainability might be worth to keep in mind. reply Gys 9 hours agorootparentOk, I understand the concept of connecting pipes and that will work as well. Still, POSTing items is more separation of concerns. The remote service only has to process one item (no need to read feeds, worry about formatting, etc). reply RyLuke 3 hours agoprevI see a lot of folks here asking about what happened to the original Yahoo Pipes. We had the same question, so we went and talked with a lot of the original team and wrote up the story[0]. We also made a fun mini-site that contains a lot of easter eggs (e.g. if you click on the \"Memory Pipes\" folder on the desktop, you'll see a bunch of candid photos of the original team circa 2007) [0] https://retool.com/pipes reply jjordan 3 hours agoparentThose shiny gumdrop buttons hit hard with the nostalgia.Because at the end, it's all just graphs with vertices and edges. That's not the end. At the end there are just objects and morphisms between them. Graphs (with their edges and vertices) are just a special category of... Well, have a look at \"Category Theory\". reply ninetyninenine 14 hours agorootparentprevWell programming itself with text doesn't have boxes and lines. So there's other isomorphisms to represent this concept. reply TeMPOraL 13 hours agorootparentOf course it is. At a low level, AST is lines and boxes. So is a trace of control flow over time. Code is downstream of thinking, which makes it downstream of lines and boxed. reply ninetyninenine 5 hours agorootparentThe ast is just the result of a technique for parsing one style of programming involving text. Visual programming with say boxes and lines at its core should not involve an ast. You need to think outside the box. Your thinking of textual programming as the core and the visual programming as an abstraction above it which is how the industry has been focused since its inception. reply sen 11 hours agorootparentprevProgramming is the lines, and the boxes is the data you’re working with (generally). reply DonHopkins 11 hours agorootparentRight, generally but not necessarily. There are many different kinds of visual programming languages, with vastly different pure and hybrid models, including boxes and lines like flow charts, interlocking puzzle-piece blocks without lines like Snap!, Scratch, and Nassi-Shneiderman diagrams, and grids of adjacent cells like spreadsheets and cellular automata. https://en.wikipedia.org/wiki/Nassi%E2%80%93Shneiderman_diag... Some visual programming languages use lines to represent control flow, others use them to represent data flow, and others use a mixture of both. Additionally, different kinds of control and data flow can operate at different frequencies within the same system. Snap!, which is essentially a visual block based version of Scheme, allows you to pass functions or closures, by wrapping blocks in gray insulating \"gaskets\" like lambda expressions that delay evaluation, and even supports macros, special forms, continuations, user defined control structures, and threading, just like Scheme. https://snap.berkeley.edu Another example is Max/MSP, which primarily uses lines to represent control flow and data flow. However, it also distinguishes between data flow at \"simulation tick frequency\" and a much higher \"signal processing frequency\". This means thousands of audio samples can flow along one line at every simulation tick, while only a single piece of data or signal (like a pure data-less control flow \"bang\") may flow along other lines at a slower simulation tick frequency. https://en.wikipedia.org/wiki/Max_(software) In data flow visual programming languages, a data flow box can emit any number of data outputs at once in parallel. A control flow box typically emits only one control flow output at a time, unless it acts like a \"fork\" operator. Fork operators, as seen in Petri nets, support concurrent processes by allowing multiple control flow outputs. https://en.wikipedia.org/wiki/Petri_net A data flow conditional works like a relay with three inputs (A, B, and Select) and one output. The Select input determines whether A or B is the output. A control flow conditional, like a traditional flowchart \"if\", has one or more control flow inputs, a Select data input or embedded expression, and multiple control flow outputs. The Select input or expression chooses which control flow output the \"program counter\" branches to next. Pure data flow networks do not have a single explicit \"program counter.\" Instead, they typically evaluate nodes in partial dependency order, which may include loops (introducing a one-cycle feedback delay). Petri nets have multiple concurrent control flow \"tokens\" that flow between boxes along the lines in parallel. Another example is Body Electric aka Bounce, which is a data flow visual programming system with relay-like data flow conditionals, but also each box has an implicit \"enable\" input that you can use to switch it on and off (like a power supply), so when it's turned off, the last calculated outputs are latched and buffered, and can be read by downstream dependencies, but the values are not recalculated during simulation frames when it's not enabled. https://donhopkins.medium.com/bounce-stuff-8310551a96e3 In Blender geometry nodes, data flows from left to right, while functions can be passed and applied in a way that evaluates right to left against the data flow. That is, functions are passed on the left, but the data is then processed through the function, either once or iteratively. https://docs.blender.org/manual/en/latest/modeling/geometry_... New Blender 4.0 Loops! https://www.youtube.com/watch?v=mr_nQBoJPXw Functions or operations can be encapsulated within nodes and passed along the data flow. For example, a \"Subdivide\" node contains a function to subdivide geometry, and this function is applied to the geometry data passed into the node. Nodes can pass functions as parameters to other nodes. For example, a \"Function Input\" node can be used to define a custom function that can be passed into another node, such as a \"Map Range\" node, which applies the function to its input data. Functions are applied to the data as it flows through the nodes. For example, a \"Set Position\" node can apply a function that modifies vertex positions based on certain criteria (e.g., noise texture values). Some nodes, like \"Attribute Math\" or \"Attribute Vector Math,\" apply mathematical operations to attributes of the geometry, effectively using these operations as functions that transform the data. Nodes can be configured to apply functions iteratively or conditionally. For example, a \"Repeat\" node can apply a function multiple times to achieve iterative processing, such as repeated subdivision or transformation. Conditional nodes, like \"Switch\" or \"Boolean Math,\" allow functions to be applied based on specific conditions, enabling selective processing of data based on attributes or other criteria. While data flows left to right, some nodes can evaluate data in a right-to-left manner when applying functions. For instance, a \"Function Output\" node can send data back up through connected nodes for additional processing before final output. This evaluation allows for more complex operations where data may need to be processed in multiple stages or cycles. reply kevthecoder 9 hours agorootparentAnother example of 'control flow' is the new behavior graph from Khronos (glTF Interactivity Specification). They did a survey of existing visual programming langauges and are trying to making a standard. It's just been released for public comment: https://www.khronos.org/blog/gltf-interactivity-specificatio... reply mejutoco 13 hours agorootparentprevOne can also think as an assembly instruction as origin and destination (line) and an operation (instruction). reply yunohn 13 hours agorootparentprevProgramming, especially imperative, is essentially boxes (statements, function calls) and lines (control flow). That’s why algorithms can be represented as flowcharts. reply ninetyninenine 13 hours agorootparentRight. I call it an isomorphism. You can translate one into the other and back. Whatever \"other\" form of programming exists it can and should be translatable to boxes and diagrams. But this \"other\" form on first glance can look very different. There are no boxes and lines in C++ or python for example. reply yunohn 8 hours agorootparentI think you misread my post. Why would C++/Python not fit my description? Which languages do you consider? reply ninetyninenine 4 hours agorootparentNo. I did not misread. I believe you misunderstood. You’re saying c++ and python fit into a category of programs that is foundation-ally really just boxes and lines. I’m saying that’s not the right way to look at it. Because you can make the opposing statement. You can say that programs that are boxes and lines are foundation-ally just text programs. So because both contradictory statements can be made both aren’t really correct. There is no hierarchy. Boxes and lines is one concept and textual programming is another peer concept standing on equal footing. Both are isomorphic and thus translatable to each other. The point of my post is to suggest that there are other concepts that occupy this equivalency space. Imagine a category of interchangeable interpretations of programming that are all translate-able between each other with no hierarchy. It’s similar to the space of human languages. All human languages don’t occupy a hierarchy yet all are isomorphic. reply high_5 13 hours agoparentprevBecause programming is essentially about receiving the input data, transforming it through a process and in the end outputting it in some other form? reply astrange 11 hours agorootparentSometimes it's about searching a preexisting database or set of constraints. In that case there may or may not be input but it's not as important. reply ninetyninenine 13 hours agorootparentprevBut then how come we can represent this concept in a form without boxes and lines? See python, C++, javascript, etc. reply tgv 12 hours agorootparentAs other people have said, boxes and lines is another way of representing order, or sequential steps. You can also do that in text. But you're right that there are other ways. Scratch is an example. But most programming languages are relatively close to math notation, and that appears to be convenient for us. If you want to see a programming language that works entirely differently, yet still has a graphical representation alternative in boxes and lines, look at CSound. It's an (old) language for generating sound, but its representation is different. reply SquareWheel 12 hours agorootparentprevLines of code are essentially doing the same thing as boxes and arrows. It's just a different representation of the same idea. You can see pretty direct translations in programming languages like Scratch, Unreal Blueprints, and Godot 3's visual scripting. As well as logical implementations in tools like Davinci Resolve Fusion. I think most programmers find this logic easier to type out than to \"draw\" in a graph, but it's conceptually the same thing. reply theshrike79 13 hours agorootparentprevThe functions and objects are the boxes, them calling each other are the lines =) reply scott_w 12 hours agorootparentprevBecause the boxes and lines are just an abstraction. reply ninetyninenine 12 hours agorootparentNo the text is the abstraction. Or perhaps neither is the abstraction and they are just equivalent concepts like how uno is the same thing as one. Uno is spanish, one is English… there is no hierarchy where one concept is an abstraction over the other. The main point of my post is to suggest that there are other equivalent concepts between text and box/line. reply x-complexity 15 hours agoparentprev> every form of visual programming I've ever seen is a singular style of some of a box that does processing and a line that connects that box to another box. Mainly because it's the most straightforward method to: 1) Visualize a contained piece of code / function (box) 2) Show where the output of X goes to (line) There may be other styles, but this style is the easiest for a newbie to wrap their head around, and is the cleanest method of visualization. The box method can then be used to further wrap a bunch of boxes into a larger box when needed. reply chefandy 14 hours agorootparentThere are other advantages to this method: easily showing which wires are active while debugging, giving new programmers an intuitive sense of what's happening in control structures like for loops, using color, symbols and line weights to intuitively convey things about data/object types, giving users a complete list of potential methods and parameters without having to look for it, being able to use more complex code shapes to navigate complex classes, people can learn the basics of coding logic without needing to memorize syntax simultaneously... There are obviously tons of shortcomings to visual code, but I do think a lot of the guff they get is unwarranted. reply onion2k 12 hours agoparentprevIn the Scratch language blocks connect to each other like jigsaw pieces for internal function logic. reply TeMPOraL 12 hours agorootparentScratch is Lisp, you're literally assembling the AST :). reply DonHopkins 10 hours agorootparentScratch is much simpler and less powerful than Lisp, missing many of Lisp's most important features, like first class functions, complex data structures, and other first class objects. You can't even pass lists around as parameters, or include lists in other lists, and it's missing homoiconicity (representing code as data), reflection, macros, special forms, or even user defined blocks and control structures, etc. But Snap! goes way beyond Scratch, since it was inspired by and visually similar to Scratch, but is just as powerful as Scheme, including functions and everything else as first class objects, lexical closures, continuations, user defined blocks, macros, special forms, etc. It's much more powerful than Scratch, but uses the same visual interlocking block syntax. https://snap.berkeley.edu reply EclipseMantis 5 hours agorootparentScratch does support user-created blocks. reply wild_egg 15 hours agoparentprevAnyone know of an example of any other style? Would love to see any half-baked experiments even. Seems like a space that could really use some innovation reply __MatrixMan__ 14 hours agorootparentI don't have any code for this, but I always thought it would be better if the boxes represented data types and the arrows represented functions. Then you could use the arrow as a progress bar, and you'd have something that's a bit more focused on the inputs and outputs. Also, the data types could show up as a stack of boxes (like a deck of cards) so you can see when you've calculated a novel dataset (highlight new card on top the deck) or when you've recalculated a previous dataset (highlight the nth card). Such a view would resemble a commutative diagram (from category theory) and if you let the cards pile up for a while you could use their sizes to reason about which functions are bijective, which are good candidates for memorization, etc. When you want to know why a certain datum is the way it is, you can use the same view to trace it back to it's inputs (in the boxes-as-tasks mode, you end up zooming in on some imperative artifact... logs usually... And discerning the inputs is left as an exercise to the reader). I think this goes by the buzzword \"data lineage\" and it's usually this extra thing but I think it should be the main thing. Which is why I want to make the nodes into edges and the edges into nodes. reply andyferris 13 hours agorootparentThis is interesting. How do you visually identify a function with multiple inputs and outputs? I tend to imagine it as a bi-graph - with boxes for values and for functions, and arrows in between. The stack idea is interesting and can work here too. I could get behind saying \"all functions have exactly one input value and one output value\" but you really need to be able to construct and deconstruct structs/records/tuples to be able to e.g. pass two values to one function. reply __MatrixMan__ 4 hours agorootparentAh, yeah, you caught me. It's not so simple as a transform from nodes to edges. What I actually have in mind is a bit stranger, but the node/edge swap was my stepping stone towards it, which is why I mentioned it. The structs/records/tuples approach would work (I think this would correspond with \"currying\"), but I have this weird idea that certain types of reasoning have a \"shape\" and that we might be able to more heavily lean on analagous reasoning if we could easily show that shape in correspondence with explanations of our reasoning. I worry that currying everything would force us further away from the \"natural\" shape (if there even is such a thing). I've been working on something called plibs (simPlical mad LIBS, since really we're showing a https://en.m.wikipedia.org/wiki/Simplicial_complex here). The functions are represented as tuples of the same arity, and described via sentences with blanks in them (e.g. when you run ____ code in ____ environment you get ____ from stdout). One with three blanks would then appear like a triangle and then I'd fill it in with a gradient indicating which sides are inputs and which are outputs. Functions sharing a mad lib would appear as a regular polygon in the same color (one edge for each blank). Or maybe they're sort of starfish shaped so that each leg can reach out an touch whatever the data representation ends up being. One could imagine algorithms that generate patchwork quilts or 3D structures (like how protein folding is represented) out of such things. These could be computed only in one direction (from inputs towards outputs) but they could be traversed in any direction if one wanted to explore the relationships. I'm hoping it will be a nice way of citing your computational sources. You would attach them to any computational result (alongside a scientific paper, perhaps) as a way if saying \"here's how I came up with this, rerun it to verify my result\". Aside from the computational pathway that yielded the result that's being scrutinized, you could explore adjacent plibs to understand why certain inputs were chosen or to gain other contextual hints. Now that I'm describing it it sounds a bit like that awful zooming-through-towers-of-data visualization in \"Hackers\" the movie. So the \"edge\" that I was using as a progress bar in my previous description is really a series of paths through a space made of these things. The details of the functions being called (and of the datasets being generated) are attached to the polygonal regions that the paths connect. The paths may merge or branch. It could get unwieldy, so the user would have to be explicit about which plibs to show and which to highlight as a chain of computation. As for how the represent the intermediate data and their types, I keep changing my mind. Some days I like the stacks of cards thing, some days not. Hopefully something will sick soon, then I'll try to build it. reply ninetyninenine 14 hours agorootparentprevText itself is a form of visual programming. We really have only two styles of visual programming: 1. Some sort language formed by left to right based alphabetic symbols in some pattern called \"grammar\". There's an almost infinite amount of possible grammars 2. Boxes and lines. Where boxes represent state or function and lines connect the boxes and state together. There's less formalization in this area so the axioms are vague but it can form something of a \"grammar\" similar to the above. That's it. It's not a space that could really use innovation. It's the fact that the entire space has never innovated. We've just been drilling down on a very specific and biased way of seeing the universe. The concept of a compiler literally illustrates how deep it goes and how tied the idea of a program is to a \"language\". reply DonHopkins 10 hours agorootparentI disagree that \"It's not a space that could really use innovation.\" There's quite a lot of innovation and experimentation in the visual programming space, and many vastly different approaches to visual programming. You forgot about block based visual programming languages like Scratch, Snap!, and Nassi-Shneiderman diagrams, and also the most ubiquitous, widely used, universally known, commonly taught, extremely powerful, and easily accessible style of visual programming language, which the worldwide economy depends on and would collapse if millions of people didn't regularly use them every day: spreadsheets. reply viraptor 14 hours agorootparentprevDepends how literally you mean programming and processing. For example UML is a programming diagram. If you don't have the definitions anywhere else and always merge the diagram with specific methods implementation when compiling, is that an good example? What about ERD? If you materialise that into schema, is that programming? I think you'll run only into those two categories - boxes represent literal code you join together, or boxes represent something declarative that gets compiled into code but not necessarily in a straightforward way. I guess visual reporting like in MS Access could be thought of as a separate category? Depending on whether prose is programming, there's also https://visar.app/ And then there's CUBE https://marc.najork.org/papers/vl1992.pdf reply Ringz 12 hours agorootparentThank you for the very interesting links. I used UML extensively during my studies in the nineties, back then on Unix machines. Interestingly, even then, it was possible to work together on a UML concept. I'm not sure anymore if the novelty of working with a graphical UML program was the motivation, or the opportunity to program in something other than Modula. In the end, after my studies, I still had to program only in C++. Once accustomed to the relative directness, it is difficult to embrace new innovative concepts. But I would welcome it. reply jes5199 13 hours agorootparentprevI’ve seen a demo - cannot immediately remember where - where instead of data flowing through lines, it actually had a physics engine where the data was shot out of the boxes in various directions, and did computation with whatever box it intersected with next reply NaOH 14 hours agorootparentprevKeyboard Maestro foregoes the lines between boxes, but it still presents the same general format. https://www.keyboardmaestro.com/img/v11/overview-dark@2x.png reply jdougan 15 hours agorootparentprevScratch is arguably a style of visual programming reply viraptor 14 hours agorootparentScratch is boxes and arrows with arrows of zero length ; ) reply philsnow 12 hours agorootparentBlockly (the visual editor used by Scratch) elegantly shows hierarchy of control flow. I've never seen a boxes-and-lines visual editor where loops are as easily understood as Blockly pieces. reply jdougan 13 hours agorootparentprevI know that's a joke, but in that case every screen is a grid of (v. small) boxes with arrows of zero length. reply CannisterFlux 11 hours agorootparentprevThe only other thing I can think of is GB Studio - https://www.gbstudio.dev/ - but the visual scripting there is like regular coding, but with visual blocks. It doesn't use boxes and lines. Instead, the script elements are more like blocks within blocks. reply okennedy 14 hours agorootparentprevSpreadsheets. reply micheljansen 11 hours agoparentprevApple's Shortcuts seems heavily inspired by the visual programming language that Scratch uses: https://support.apple.com/guide/shortcuts-mac/intro-to-how-s... reply Tijdreiziger 2 hours agorootparentMore like Automator: https://support.apple.com/guide/automator/welcome/mac (although [per Wikipedia] the first Scratch prototype does seem to predate Automator by 1.5 years, so I suppose it’s possible there’s a connection) reply micheljansen 1 hour agorootparentShortcuts is indeed the (more than spiritual) successor to Automator. Scratch was pretty famous in interface design circles in the early 2000s (it was a pretty high profile MIT Media Lab project). I'm sure the Automator developers were aware of it, but it may also have been a case of both having a rich neighbour [1]. Agentsheets, for example, predates both by over 10 years [2] [1] https://folklore.org/A_Rich_Neighbor_Named_Xerox.html [2] https://en.wikipedia.org/wiki/AgentSheets reply smusamashah 9 hours agoparentprevTurns out there is another form of visual programming called Spatial Programming https://www.youtube.com/watch?v=eQgxFuw8f1U reply HumblyTossed 5 hours agoparentprevThere is already a diagram that would capture it so much better - flowcharts. reply DonHopkins 11 hours agoparentprevBlock based visual programming languages don't use lines connecting boxes to each other, which has been widely used for a long time with popular visual programming languages like Snap!, Scratch, Squeak eToys, Blockly, etc. Snap!: https://snap.berkeley.edu Scratch: https://scratch.mit.edu/ Squeak eToys: http://www.squeakland.org/ Blockly: https://developers.google.com/blockly Also, spreadsheets are an extremely popular form of visual programming that has been around for a long time, which don't use lines to connect boxes either. VisiCalc: https://en.wikipedia.org/wiki/VisiCalc Lotus 1-2-3: https://en.wikipedia.org/wiki/Lotus_1-2-3 Microsoft Excel: https://en.wikipedia.org/wiki/Microsoft_Excel Brad Myers' paper answers the age-old argument about whether or not spreadsheets are visual programming languages: https://news.ycombinator.com/item?id=26061576 reply BaudouinVH 4 hours agoprevtiny bit of feedback : I tried to sign-in with a non gmail address and did not receive the confirmation email. Switched to a gmail address and everything was smooth in the sign-in process. reply ReadCarlBarks 15 hours agoprevIt is on GitHub: https://github.com/pipes-digital/pipes. reply jumploops 12 hours agoprev> The default output format of a pipe is RSS. Do people still use RSS feeds? I work on a similar product[0], conceptually at least, to Pipes, and many of our users have asked for RSS feeds as output. Until now I’ve rationalized that as low priority, but maybe I’m wrong? [0] https://magicloops.dev reply onion2k 12 hours agoparentmany of our users have asked for RSS feeds as output If your users are asking for it then it's locally popular regardless of how much use it gets on the general internet, and therefore it should be important to you. Local popularity (eg the market for your app) is significantly more important to product design than general popularity. reply JoshTriplett 11 hours agorootparentExactly. The user base for \"people who want Pipes\" is likely to disproportionately want RSS as a way of generating data they can consume. reply rtpg 12 hours agoparentprevpeople totally use RSS feeds still. So many systems just make it by default, and I (and many other people) just use RSS readers for following blogs and whatnot. It's not a complicated thing to get setup! There's a bunch of details you can flub but at the end of the day it's a big thing of markup. The \"good enough\" thing is easy. reply InsideOutSanta 11 hours agoparentprevIs there anyone who doesn't use RSS, whether it is as infrastructure behind the scenes (e.g. for a podcast or some kind of notification system) or explicitly in a feed reader? reply stareatgoats 6 hours agoparentprev> Do people still use RSS feeds? Another data point from me: I recently started unsubscribing from as many newsletters as possible, and have organized them into hierarchical tree structures in my RSS feed reader instead, according to priority and topics. With many hundred sources by now, some of them extremely prolific posters, I rarely make it past the top priority ones, but for the rest, there is the occasional browse, and a full-text search that I can use on specific topics. This setup allows me to consume newsletters and blogposts much more efficiently, and the mailbox gets mainly used for correspondence, not general stuff. Suits me just fine, and as an added bonus: I now more seldom have the feeling my every move-move is monitored and stored by who knows who, who knows where, for who knows what purpose. reply stareatgoats 3 hours agorootparentedit: > move-move = mouse-move reply onli 11 hours agoparentprevI can't judge the economic impact of that for you. But with a service like this, it's just so easy to do that you might want to prioritize it :) If your model is pull based, you create a second endpoint that puts the data you'd normally output directly (otionally minus the JSON dressing) into the content field of RSS items. If it's push based, you could even write into an XML file on your server somewhere and let users access that. In the end, RSS is just some XML dressing around your data, and can be as easily implemented as a html template. Storing your old data (a RSS feed should not contain only one item) might be the biggest hurdle, if you don't do that already. Or you just send your users to Pipes, they could use a download and \"build feed\" block to create a feed on their own ;) Edit: Though I noticed that the endpoint of your loops can be specific actions, like a text message, that wouldn't translate of course. More a \"put it into the feed\" target action then. reply gumby 12 hours agoparentprevRSS is my main interface to the web reply Pufferbo 10 hours agoparentprevI exclusively consume YouTube via RSS. reply 8organicbits 4 hours agorootparentCan you share more, what reader do you use? I think YouTube may be the service with the most RSS feeds, as each channel has one. I could see RSS working well for YouTube, especially if the ads still play. reply shocks 2 hours agorootparentI use https://miniflux.app/ to consume YT. Works great. reply Pawka 7 hours agoparentprev> Do people still use RSS feeds? Daily. For me RSS feed is the only option to pick what I want to read instead of reading what promoter wants me to read. If there are other ways - I would be happy to know. reply rcarmo 11 hours agoparentprevI only read HN via RSS, and it is one of around two hundred feeds I filter, summarize and read every morning… reply freeqaz 12 hours agoprevThis is the code for a YC startup I ran in this space. Very similar. Build on AWS Lambda: https://github.com/refinery-labs/refinery I need to fix the docs because they are dead: https://web.archive.org/web/20200819221527/https://docs.refi... Yahoo Pipes was an inspiration though. It was similar to Heroku in many ways. reply harinijan 12 hours agoprevInteresting to see the evolution of Visual programming across many tools. We are working on something similar[0] for low-code/no-code devs focusing on creation of APIs, backend tasks, and AI workflows. [0] https://buildship.com reply anothername12 3 hours agoprevBack in the day I had hundreds of pipes, sub-pipes for all kinds of stuff from porn to price gathering, comparisons, to banking. When Yahoo Pipes died so did my will to port all that to something else. If I had to do it all again in 2024, what’s a robust self hosted project like this? reply jzemeocala 15 hours agoprevI wonder if we could pull off some wackiness with custom RSS feeds. Might have some utility in keeping an LLM constantly up to date as well. reply dbacar 12 hours agoprevYou know, there is also Apache Nifi. reply ozten 13 hours agoprevYahoo pipes was amazing! My vague memory of how this ended last time was processing cost and lack of a business model. reply _gtly 15 hours agoprevI want to support this idea, though If you use their service it's free to play around with 3 feeds, but then it gets more pricey. Maybe worth it for some?https://www.pipes.digital/pricing reply yreg 11 hours agoparentThree free pipes sound reasonable for me. If you have more than three use cases in your life, then it's perhaps worth it to pay a few bucks. reply cranberryturkey 10 hours agoprevYahoo pipes was awesome. Way ahead of its time. I don’t know why yahoo shut it down. reply nhggfu 14 hours agoprevthe twitter example link goes nowhere. reply onli 12 hours agoparentThe twitter integration did not survive their api changes and price hikes. I removed the block in the editor a while ago. Did I miss a link in the docs? reply mariusor 10 hours agorootparentI saw some shared pipes that were using twitter. reply onli 10 hours agorootparentOh, right, one of those might have been mine. I marked it as private now. Thank you. reply eitland 12 hours agoparentprevThe docs says Twitter integration is discontinued. reply nsonha 11 hours agoprevkinda related but does anyone know anything similar to yql, in the sense that you can query everything using the same syntax? reply yalogin 13 hours agoprev [–] I think this has some value potentially now, but they should not use “pipes” to describe it. Yahoo pipes is dead and there is no point associating one self with that brand anymore. The concept is easy enough to explain without using that word. reply Towaway69 6 hours agoparentUnix pipes came before yahoo pipes (in fact they took their name from unix pipes). Unix pipes are alive and well, or are they also an association with a dead product? reply scq 13 hours agoparentprevWhat's wrong with the word \"pipes\"? reply ReadCarlBarks 13 hours agoparentprev [–] Pipes was created back when people were looking up alternatives for the Yahoo service. It isn't a new project. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Pipes is a visual programming editor for feeds, allowing users to fetch, create, and manipulate feeds using blocks, similar to Yahoo! Pipes.",
      "It supports various input formats, including RSS, Atom, JSON, HTML, and text files, and offers a range of blocks for different feed operations like filtering, merging, and extracting content.",
      "Pipes CE is a free and open-source software (FOSS) under the AGPL license, available on Github, and supports integrations with popular sites like Twitter, YouTube, and Vimeo."
    ],
    "commentSummary": [
      "Pipes, a project inspired by Yahoo Pipes, recently underwent updates to improve stability, including a shift from text to RSS objects for data transport between blocks.",
      "Server upgrades and reconfiguration of threads and puma workers were implemented to address issues and bottlenecks.",
      "A user suggestion to add a block for AI-generated summaries or images via POST requests is being considered, with some foundational blocks already in place."
    ],
    "points": 382,
    "commentCount": 109,
    "retryCount": 0,
    "time": 1719799417
  },
  {
    "id": 40843848,
    "title": "My finetuned models beat OpenAI's GPT-4",
    "originLink": "https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation.html",
    "originBody": "My last post outlined the kinds of evaluation I need and want to understand how well my finetuned LLM is performing in the task of structured data extraction from press releases. Let’s start with the core metric I’m interested in, accuracy, and then later we can dive into some of the other evaluation metrics as well. TL;DR The headline for this post could well have been: finetuned models beat OpenAI, but evals were a bit painful to implement. There’s a lot of hidden code here in this post and it was slow to run. This step was the first time during the work for the finetuning course where I felt the pain and tradeoffs around the choice to finetune. I can see that without a system of some kind to handle this, the complexity of maintaining it all will start to mount up. But more on that at the end! This is a long post with lots of detail. I’ve tried to minimise the amount of code you see, but if you want to see how the charts or evals were done, expand the ‘code’ sections. If you’re interested in cutting straight to the aggregate results, click here to go to the end of this post. (To see the rest of the blog posts about this project, please click here. Some context: I’m doing some finetuning as part of the Hamel Husain / Dan Becker Finetuning course on Maven using some data I collected and labeled a few years back that makes for a cool little test of how good it works for structured data extraction.) Loading the datasets The data is all available on the Hugging Face Hub in a public repository, and for the purposes of these evaluations I want to use the test split of the dataset since none of our models have seen that data yet so it’s good for determining how well our model performs with new data. Code test_dataset Dataset({ features: ['name', 'eventrefnumber', 'text', 'StartDate', 'eventtype', 'province', 'citydistrict', 'village', 'targetgroup', 'commander', 'position', 'minkilled', 'mincaptured', 'capturedcharacterisation', 'killedcharacterisation', 'killq', 'captureq', 'killcaptureraid', 'airstrike', 'noshotsfired', 'dataprocessed', 'flagged', 'glossarymeta', 'minleaderskilled', 'minfacilitatorskilled', 'minleaderscaptured', 'minfacilitatorscaptured', 'leaderq'], num_rows: 724 }) We’ll first add an extra column to our DataFrame and then make a prediction for each and every row in the dataset. We’ll store a copy of the prediction to the column so as to make sure we don’t have to do this compute-intensive step repeatedly. But first we’ll assemple the data as Pydantic objects so as to handle validation and other quality of life features. Code Here’s what a couple of examples of our training data looks like as Pydantic models when we pass them in: Code [ IsafEvent( name='5', text='2013-01-S-025KABUL, Afghanistan (Jan. 25, 2013)During a security operation in Andar district, Ghazni province, yesterday, an Afghan and coalition force killed the Taliban leader, Alaudin. Alaudin oversaw a group of insurgents responsible for conducting remote-controlled improvised explosive device and small-arms fire attacks against Afghan and coalition forces. Prior to his death, Alaudin was planning attacks against Afghan National Police in Ghazni province.', start_date=datetime.date(2013, 1, 24), event_type={'insurgentskilled'}, province={'ghazni'}, target_group={'taliban'}, min_killed=1, min_captured=0, killq=True, captureq=False, killcaptureraid=False, airstrike=False, noshotsfired=False, min_leaders_killed=1, min_leaders_captured=0, predictions={} ), IsafEvent( name='2', text='2011-11-S-034ISAF Joint Command - AfghanistanFor Immediate ReleaseKABUL, Afghanistan (Nov. 20, 2011)A coalition security force detained numerous suspected insurgents during an operation in Marjeh district, Helmand province, yesterday. The force conducted the operation after receiving information that a group of insurgents were at a compound in the area. After calling for the men inside to come out peacefully, the insurgents emerged and were detained without incident.', start_date=datetime.date(2011, 11, 19), event_type={'detention'}, province={'helmand'}, target_group={''}, min_killed=0, min_captured=4, killq=False, captureq=True, killcaptureraid=True, airstrike=False, noshotsfired=False, min_leaders_killed=0, min_leaders_captured=0, predictions={} ) ] So when we’re making the prediction we’re hoping to get a JSON string like this out from the model: json_str = events[0].model_dump_json(exclude={\"text\", \"predictions\"}) print(json_str) {\"name\":\"5\",\"start_date\":\"2013-01-24\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"ghazni\"],\"target_group\":[\"tali ban\"],\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":false,\"nosh otsfired\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0} I’m starting with full evaluations using the GPT models and I’ll need a slightly more elaborate prompt in order to get decent results. I can’t pass in the exact same prompt as the one I used for the finetuned model since the GPT models haven’t been trained or finetuned to respond to those specific prompts. This is sort of an interesting problem to have: how much effort do we put into the GPT prompts to try to get the same level of accuracy as the finetuned model? Or in other words, is there even a way to really compare like to like between models that must accept different prompts? Let’s try this out for OpenAI GPT-4o and GPT-4 Turbo and see how we get on. You’ll note how long the prompt has to be to give the GPT models a fighting chance against the finetuned models. Ideally I’d stuff in even more examples into the context, but I also don’t want to explode the number of tokens I’m using. from openai import OpenAI from rich import print import json import os def query_openai(article_text: str, model: str) -> str: query = ( f\"The following is a press release issued by ISAF (formerly operating in Afghanistan):{article_text}\" \"## Extraction request\" \"Please extract the following information from the press release:\" \"- The name of the event (summarising the event / text as a headline)\" \"- The start date of the event\" \"- The event type(s)\" \"- The province(s) in which the event occurred\" \"- The target group(s) of the event\" \"- The minimum number of people killed during the event\" \"- The minimum number of people captured during the event\" \"- Whether someone was killed or not during the event\" \"- Whether someone was captured or not during the event\" \"- Whether the event was a so-called 'kill-capture raid'\" \"- Whether an airstrike was used during the event\" \"- Whether no shots were fired during the event\" \"- The minimum number of leaders killed during the event\" \"- The minimum number of leaders captured during the event\" \"## Annotation notes:\" \"- A 'faciliator' is not a leader.\" \"- If a press release states that 'insurgents' were detained without further \" \"details, assign a minimum number of two detained. Interpret 'a couple' as \" \"two. Interpret 'several' as at least three, even though it may sometimes \" \"refer to seven or eight. Classify the terms 'a few', 'some', 'a group', 'a \" \"small group', and 'multiple' as denoting at least three, even if they \" \"sometimes refer to larger numbers. Choose the smaller number if no other \" \"information is available in the press release to come up with a minimally \" \"acceptable figure. Interpret 'numerous' and 'a handful' as at least four, \" \"and 'a large number' as at least five.\" \"## Example:\" \"Article text: 'ISAF Joint Command Evening Operational Update Feb. 19, 2011ISAF Joint Command - \" \"Afghanistan\\u20282011-02-S-143\\u2028For Immediate Release \\u2028\\u2028KABUL, Afghanistan (Feb. 19)\\u2028\\u2028ISAF \" \"service members at a compound in Sangin district, Helmand province observed numerous insurgents north and south of \" \"their position talking on radios today. After gaining positive identification of the insurgent positions, the \" \"coalition troops engaged, killing several insurgents. Later, the ISAF troops observed more insurgents positioning \" \"in the area with weapons. After positive identification, coalition forces continued firing on the various insurgent \" \"positions, resulting in several more insurgents being killed.'\" 'Output: `{\"name\":\"Several insurgents killed in ' 'Helmand\",\"start_date\":\"2011-02-18\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"helmand\"],\"target_group\":[\"\"],\"mi' 'n_killed\":6,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":false,\"noshotsfired\"' ':false,\"min_leaders_killed\":0,\"min_leaders_captured\":0}`' ) # set up the prediction harness client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\")) response = client.chat.completions.create( model=model, response_format={\"type\": \"json_object\"}, messages=[ { \"role\": \"system\", \"content\": \"You are an expert at identifying events in a press release. You are precise \" \"and always make sure you are correct, drawing inference from the text of the \" \"press release. You always return a JSON string with the following schema: \" \"## JSON Schema details\" \"Here is some of the schema for the JSON output string you \" \"should make use of: event_types = ['airstrike', 'detention', \" \"'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], \" \"provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', \" \"'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', \" \"'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', \" \"'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', \" \"'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', \" \"'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', \" \"'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']\", }, {\"role\": \"user\", \"content\": query}, ], temperature=1, ) return response.choices[0].message.content We can make sure this function works with a quick example: json_str = query_openai(events[0].text, \"gpt-4o\") print(json.loads(json_str)) { 'name': 'Taliban leader Alaudin killed in Ghazni', 'start_date': '2013-01-24', 'event_type': ['insurgentskilled'], 'province': ['ghazni'], 'target_group': ['taliban'], 'min_killed': 1, 'min_captured': 0, 'killq': True, 'captureq': False, 'killcaptureraid': True, 'airstrike': False, 'noshotsfired': False, 'min_leaders_killed': 1, 'min_leaders_captured': 0 } Our model is working (as expected) and we’re also getting a JSON string back. Let’s assemble something that will iterate through all of our test data, get predictions, and then store those predictions on our Pydantic object. For the bulk predictions, we’ll make sure to do this async, since there are lots of events and we don’t want to waiting all day. You’ll see I also had to add some retries to the function to account for rate limiting on the GPT-3.5-turbo model. Code So as you can now see, we have three predictions attached to each event. print(events[0]) IsafEvent( name='5', text='2013-01-S-025KABUL, Afghanistan (Jan. 25, 2013)During a security operation in Andar district, Ghazni province, yesterday, an Afghan and coalition force killed the Taliban leader, Alaudin. Alaudin oversaw a group of insurgents responsible for conducting remote-controlled improvised explosive device and small-arms fire attacks against Afghan and coalition forces. Prior to his death, Alaudin was planning attacks against Afghan National Police in Ghazni province.', start_date=datetime.date(2013, 1, 24), event_type={'insurgentskilled'}, province={'ghazni'}, target_group={'taliban'}, min_killed=1, min_captured=0, killq=True, captureq=False, killcaptureraid=False, airstrike=False, noshotsfired=False, min_leaders_killed=1, min_leaders_captured=0, predictions={ 'gpt-4o': '{ \"name\": \"Taliban leader Alaudin killed in Ghazni\", \"start_date\": \"2013-01-24\", \"event_type\": [\"insurgentskilled\", \"captureandkill\"], \"province\": [\"ghazni\"], \"target_group\": [\"taliban\"], \"min_killed\": 1, \"min_captured\": 0, \"killq\": true, \"captureq\": false, \"killcaptureraid\": true, \"airstrike\": false, \"noshotsfired\": false, \"min_leaders_killed\": 1, \"min_leaders_captured\": 0}', 'gpt-4-turbo': '{ \"name\": \"Taliban leader Alaudin killed in Ghazni\", \"start_date\": \"2013-01-24\", \"event_type\": [\"captureandkill\"], \"province\": [\"ghazni\"], \"target_group\": [\"taliban\"], \"min_killed\": 1, \"min_captured\": 0, \"killq\": true, \"captureq\": false, \"killcaptureraid\": true, \"airstrike\": false, \"noshotsfired\": false, \"min_leaders_killed\": 1, \"min_leaders_captured\": 0}', 'gpt-3.5-turbo': '{ \"name\": \"Taliban leader Alaudin killed in Ghazni province\", \"start_date\": \"2013-01-24\", \"event_type\": [\"captureandkill\"], \"province\": [\"ghazni\"], \"target_group\": [\"taliban\"], \"min_killed\": 1, \"min_captured\": 0, \"killq\": true, \"captureq\": false, \"killcaptureraid\": false, \"airstrike\": false, \"noshotsfired\": false, \"min_leaders_killed\": 1, \"min_leaders_captured\": 0}' } ) I have all these predictions living in memory right now so it’s probably a good time to commit these to a dataset and push them to the Hugging Face Hub in case the notebook crashes or my local machine shuts down or something else unexpected. I’ll create a function to handle this as we’ll be repeating this process for the other models as well. It’s a bit verbose but I thought it preferable so you can see what’s going on. Code A more concise and abstract version of the convert_to_dataset function could be something like: def convert_to_dataset(data: List[BaseModel]) -> Dataset: dataset_dict = {} for field_name, field_value in data[0].__fields__.items(): field_type = field_value.outer_type_ if field_type in [str, int, float, bool, date]: dataset_dict[field_name] = [getattr(item, field_name) for item in data] elif field_type == set: dataset_dict[field_name] = [list(getattr(item, field_name)) for item in data] elif issubclass(field_type, BaseModel): dataset_dict[field_name] = [getattr(item, field_name).dict() for item in data] else: dataset_dict[field_name] = [getattr(item, field_name) for item in data] dataset = Dataset.from_dict(dataset_dict) return dataset But for now let’s just push our data to the Hub. convert_and_push_dataset(events, \"isafpressreleases_with_preds\", split_name=\"test\") Adding predictions from our finetuned models We’ve added some baseline OpenAI models, so let’s now add the models we previously finetuned. This includes a mix of local models as well as models hosted by some one-click finetuning providers. I’ll hide a bunch of the code with folding arrows so you can see it if you’re interested but there isn’t actually much of interest or new there. Reloading the predictions dataset Let’s start by loading our dataset and then we can get into adding some local model predictions: from datasets import load_dataset preds_test_data = load_dataset(\"strickvl/isafpressreleases_with_preds\")[ \"test\" ].to_list() We trained some local models, so let’s add those predictions to the dataset. Finetuned TinyLlama predictions Code Now if we inspect we’ll see that the new model predictions have been saved into the dataset: from rich import print print(preds_test_data[0]) { 'name': '5', 'text': '2013-01-S-025KABUL, Afghanistan (Jan. 25, 2013)During a security operation in Andar district, Ghazni province, yesterday, an Afghan and coalition force killed the Taliban leader, Alaudin. Alaudin oversaw a group of insurgents responsible for conducting remote-controlled improvised explosive device and small-arms fire attacks against Afghan and coalition forces. Prior to his death, Alaudin was planning attacks against Afghan National Police in Ghazni province.', 'predictions': { 'gpt-3.5-turbo': '{ \"name\": \"Taliban leader Alaudin killed in Ghazni province\", \"start_date\": \"2013-01-24\", \"event_type\": [\"captureandkill\"], \"province\": [\"ghazni\"], \"target_group\": [\"taliban\"], \"min_killed\": 1, \"min_captured\": 0, \"killq\": true, \"captureq\": false, \"killcaptureraid\": false, \"airstrike\": false, \"noshotsfired\": false, \"min_leaders_killed\": 1, \"min_leaders_captured\": 0}', 'gpt-4-turbo': '{ \"name\": \"Taliban leader Alaudin killed in Ghazni\", \"start_date\": \"2013-01-24\", \"event_type\": [\"captureandkill\"], \"province\": [\"ghazni\"], \"target_group\": [\"taliban\"], \"min_killed\": 1, \"min_captured\": 0, \"killq\": true, \"captureq\": false, \"killcaptureraid\": true, \"airstrike\": false, \"noshotsfired\": false, \"min_leaders_killed\": 1, \"min_leaders_captured\": 0}', 'gpt-4o': '{ \"name\": \"Taliban leader Alaudin killed in Ghazni\", \"start_date\": \"2013-01-24\", \"event_type\": [\"insurgentskilled\", \"captureandkill\"], \"province\": [\"ghazni\"], \"target_group\": [\"taliban\"], \"min_killed\": 1, \"min_captured\": 0, \"killq\": true, \"captureq\": false, \"killcaptureraid\": true, \"airstrike\": false, \"noshotsfired\": false, \"min_leaders_killed\": 1, \"min_leaders_captured\": 0}', 'tinyllama-templatefree': '{\"name\":\"Taliban leader killed in Ghazni\",\"start_date\":\"2013-01-24\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"ghazni\"],\"target_group\":[\"taliban\" ],\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":false,\"noshotsf ired\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}', 'tinyllama-sharegpt': '{\"name\":\"2\",\"start_date\":\"2013-01-24\",\"event_type\":[\"airstrike\"],\"province\":[\"ghazni\"],\"target_group\":[\"taliban\"], \"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":true,\"noshotsfire d\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}' }, 'start_date': datetime.date(2013, 1, 24), 'province': ['ghazni'], 'target_group': ['taliban'], 'event_type': ['insurgentskilled'], 'min_killed': 1, 'min_captured': 0, 'killq': True, 'captureq': False, 'killcaptureraid': False, 'airstrike': False, 'noshotsfired': False, 'min_leaders_killed': 1, 'min_leaders_captured': 0 } Finetuned Mistral predictions As I noted previously, it was impossible to get the finetuned Mistral model working locally so I did the inference over on Modal where I could spin up a juicy A100 to make the predictions. You’ll see below that the model didn’t perform very well, failing almost all of the evaluations. This is the mistral-lora-templatefree model you’ll see in the charts. Finetuned OpenAI predictions I used OpenAI’s one-click finetuning service to finetune the gpt-3.5-turbo-1106 model. I iterated over my dataset to generate predictions using that finetuned model using the OpenAI SDK. Code Finetuned Mistral models (via OpenPipe) I finetuned Mistral 7B and Mistral 8x7B models using OpenPipe so as to have something reasonable to compare the other models to. As always, OpenPipe makes it pretty easy to spin up a finetuning job and get predictions. Code Finetuned Solar LLM (via Predibase) Predibase announced a new best-in-class model for finetuning, the Solar LLM from Upstage, a week or so ago so I thought I’d try it out. The advantage of this model is that it’s trained to be good at the kinds of tasks people commonly finetune models for, like structured data extraction. As you’ll see below, it did pretty well! The base model is this one, I think, on the Hugging Face Hub so it’s available for you all to use as well. Code Finetuned Llama3 predictions (via OpenPipe) My locally finetuned Llama3 model hadn’t really worked well, but on OpenPipe the outputs seemed to look ok, so I used these predictions for the final evaluation. Code By the end of this process you can see we have a bunch of predictions attached to each entry in our dataset. You can view all of these in the public dataset I published on the Hugging Face Hub. from rich import print print(preds_test_data[0]) { 'name': '5', 'text': '2013-01-S-025KABUL, Afghanistan (Jan. 25, 2013)During a security operation in Andar district, Ghazni province, yesterday, an Afghan and coalition force killed the Taliban leader, Alaudin. Alaudin oversaw a group of insurgents responsible for conducting remote-controlled improvised explosive device and small-arms fire attacks against Afghan and coalition forces. Prior to his death, Alaudin was planning attacks against Afghan National Police in Ghazni province.', 'predictions': { 'finetuned-llama3-7b-32k-openpipe': '{\"name\":\"1\",\"start_date\":\"2013-01-24\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"ghazni\"],\"target_group\":[\"tal iban\"],\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":true,\"airstrike\":false,\"nosh otsfired\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}', 'finetuned-mistral-7b-optimised-openpipe': '{\"name\":\"1\",\"start_date\":\"2013-01-24\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"ghazni\"],\"target_group\":[\"tal iban\"],\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":true,\"airstrike\":false,\"nosh otsfired\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}', 'finetuned-openai-gpt-3.5-turbo-1106': '{\"name\":\"4\",\"start_date\":\"2013-01-24\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"ghazni\"],\"target_group\":[\"tal iban\"],\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":true,\"airstrike\":false,\"nosh otsfired\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}', 'gpt-3.5-turbo': '{ \"name\": \"Taliban leader Alaudin killed in Ghazni province\", \"start_date\": \"2013-01-24\", \"event_type\": [\"captureandkill\"], \"province\": [\"ghazni\"], \"target_group\": [\"taliban\"], \"min_killed\": 1, \"min_captured\": 0, \"killq\": true, \"captureq\": false, \"killcaptureraid\": false, \"airstrike\": false, \"noshotsfired\": false, \"min_leaders_killed\": 1, \"min_leaders_captured\": 0}', 'gpt-4-turbo': '{ \"name\": \"Taliban leader Alaudin killed in Ghazni\", \"start_date\": \"2013-01-24\", \"event_type\": [\"captureandkill\"], \"province\": [\"ghazni\"], \"target_group\": [\"taliban\"], \"min_killed\": 1, \"min_captured\": 0, \"killq\": true, \"captureq\": false, \"killcaptureraid\": true, \"airstrike\": false, \"noshotsfired\": false, \"min_leaders_killed\": 1, \"min_leaders_captured\": 0}', 'gpt-4o': '{ \"name\": \"Taliban leader Alaudin killed in Ghazni\", \"start_date\": \"2013-01-24\", \"event_type\": [\"insurgentskilled\", \"captureandkill\"], \"province\": [\"ghazni\"], \"target_group\": [\"taliban\"], \"min_killed\": 1, \"min_captured\": 0, \"killq\": true, \"captureq\": false, \"killcaptureraid\": true, \"airstrike\": false, \"noshotsfired\": false, \"min_leaders_killed\": 1, \"min_leaders_captured\": 0}', 'mistral-lora-templatefree': '1', 'tinyllama-sharegpt': '{\"name\":\"2\",\"start_date\":\"2013-01-24\",\"event_type\":[\"airstrike\"],\"province\":[\"ghazni\"],\"target_group\":[\"taliban\"], \"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":true,\"noshotsfire d\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}', 'tinyllama-templatefree': '{\"name\":\"Taliban leader killed in Ghazni\",\"start_date\":\"2013-01-24\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"ghazni\"],\"target_group\":[\"taliban\" ],\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":false,\"noshotsf ired\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}', 'ft-solar-1-mini-chat-240612-predibase': '{\"name\":\"2\",\"start_date\":\"2013-01-24\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"ghazni\"],\"target_group\":[ \"taliban\"],\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":true,\"airstrike\":false,\" noshotsfired\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}' }, 'start_date': datetime.date(2013, 1, 24), 'province': ['ghazni'], 'target_group': ['taliban'], 'event_type': ['insurgentskilled'], 'min_killed': 1, 'min_captured': 0, 'killq': True, 'captureq': False, 'killcaptureraid': False, 'airstrike': False, 'noshotsfired': False, 'min_leaders_killed': 1, 'min_leaders_captured': 0 } Unfortunately the Qwen2 inference on Predibase is still not working so I’ll skip that finetuned model for the moment. Now that we have predictions from seven finetuned models and three OpenAI models (to compare against), we can run our evaluations. I’ll start with a simple check to see what proportion of the predictions are even valid JSON. JSON Validity Test from datasets import load_dataset dataset_with_preds = load_dataset(\"strickvl/isafpressreleases_test_predictions\")[ \"train\" ].to_list() Code It’s already instructive to see the difference between the templatefree and the sharegpt template’s ability to generate valid JSON for the TinyLlama finetune. The OpenAI models generate valid JSON every single time, as does the finetuned Mistral and Llama3 models. While writing the code to evaluate the models, I noticed that some entries were blank or had no predictions at all, so I looked into that next. # find out how many of the predictions are None values or empty strings missing_values = { \"gpt-4o\": 0, \"gpt-4-turbo\": 0, \"gpt-3.5-turbo\": 0, \"tinyllama-templatefree\": 0, \"tinyllama-sharegpt\": 0, \"finetuned-openai-gpt-3.5-turbo-1106\": 0, \"finetuned-llama3-7b-32k-openpipe\": 0, \"mistral-lora-templatefree\": 0, \"finetuned-mistral-7b-optimised-openpipe\": 0, \"ft-solar-1-mini-chat-240612-predibase\": 0, } for row in dataset_with_preds: for model in row[\"predictions\"]: if row[\"predictions\"][model] is None or row[\"predictions\"][model] == \"\": missing_values[model] += 1 print(missing_values) { 'gpt-4o': 0, 'gpt-4-turbo': 0, 'gpt-3.5-turbo': 0, 'tinyllama-templatefree': 0, 'tinyllama-sharegpt': 38, 'finetuned-openai-gpt-3.5-turbo-1106': 0, 'finetuned-llama3-7b-32k-openpipe': 0, 'mistral-lora-templatefree': 0, 'finetuned-mistral-7b-optimised-openpipe': 0, 'ft-solar-1-mini-chat-240612-predibase': 0 } So were it not for the missing values, the tinyllama-sharegpt model would have had all 724 predictions, and valid JSON as well. Now we can get into what we’re really interested in: accuracy. I’ll calculate scores for all the properties where it makes sense for us to have a score and then show the results comparing the models. These are: start_date province target_group event_type min_killed min_captured killq captureq killcaptureraid airstrike noshotsfired min_leaders_killed min_leaders_captured Important note, for all these charts that follow, the total number of tasks was 724, so the numbers are out of a total of 724. Start Date Accuracy Code { 'gpt-4o': 527, 'gpt-4-turbo': 522, 'gpt-3.5-turbo': 492, 'tinyllama-templatefree': 231, 'tinyllama-sharegpt': 479, 'finetuned-openai-gpt-3.5-turbo-1106': 646, 'finetuned-llama3-7b-32k-openpipe': 585, 'mistral-lora-templatefree': 0, 'finetuned-mistral-7b-optimised-openpipe': 636, 'ft-solar-1-mini-chat-240612-predibase': 649 } Code Both Solar and our finetuned GPT3.5 model performed best on predicting which date the event took place. I’m surprised how poorly the OpenAI models did here, actually. And even our best model still got 75 of the dates wrong. This feels like something that I’d want to improve on. Possibly synthetic data could help, or maybe just an improvement in the finetuning prompt as well. Province Accuracy Code { 'gpt-4o': 649, 'gpt-4-turbo': 645, 'gpt-3.5-turbo': 595, 'tinyllama-templatefree': 335, 'tinyllama-sharegpt': 660, 'finetuned-openai-gpt-3.5-turbo-1106': 704, 'finetuned-llama3-7b-32k-openpipe': 707, 'mistral-lora-templatefree': 0, 'finetuned-mistral-7b-optimised-openpipe': 711, 'ft-solar-1-mini-chat-240612-predibase': 704 } Code In what will become a theme, the finetuned models actually outperform the OpenAI models, only making a few mistakes. Once again I’m surprised how poorly GPT3.5 did on this task. Target Group Accuracy Here there are potentially multiple groups mentioned as target group so I’ll give a score out of 1 of how many of the groups the model predicted were correct. Code Finetuned models doing significantly better than OpenAI for target group identification. I suspect that this would degrade if we added some new groups who weren’t in the training data (as I’d written about in my last post.) Event Type Accuracy Code Event type is actually one of the hardest categories since there are actually some semantically overlapping categories and it’s sometimes even hard for a human annotator, but once again the finetuned models do pretty well. Accuracy for min_killed Code For these number estimations, suddenly the playing fields gets leveled between the finetuned and the OpenAI models. Mistral still comes out on top, but not by much! And it’s impressive how the OpenAI models do really well at this. I suspect this is because of the whole section in the prompt which explained the rubric that was used for annotating the examples: Annotation notes: A ‘faciliator’ is not a leader. If a press release states that ‘insurgents’ were detained without further details, assign a minimum number of two detained. Interpret ‘a couple’ as two. Interpret ‘several’ as at least three, even though it may sometimes refer to seven or eight. Classify the terms ‘a few’, ‘some’, ‘a group’, ‘a small group’, and ‘multiple’ as denoting at least three, even if they sometimes refer to larger numbers. Choose the smaller number if no other information is available in the press release to come up with a minimally acceptable figure. Interpret ‘numerous’ and ‘a handful’ as at least four, and ‘a large number’ as at least five. Accuracy for min_captured Code Accuracy for killq Code I’d expect really high accuracy for these boolean attributes, and basically almost all the models were able to give this. Still, our finetuned Mistral still beats out GPT-4o best score. Accuracy for captureq Code Accuracy for killcaptureraid Code This is another attribute where it’s clear the lack of signposting in the prompts to the OpenAI models put them at a disadvantage. The term ‘kill-capture raid’ is a term of art and it was used in a specific way for the labelling. OpenAI knows nothing about how I made those calls, which explains why they performed so poorly here. Accuracy for airstrike Code Accuracy for noshotsfired Code I’m not quite sure why the OpenAI models are performing in the reverse order of what you’d expect. Recall that the noshotsfired attribute refers to whether the press release states that no shots were fired during a particular raid / event. (For a certain period the press releases were keen to mention this and it was a metric that was particularly useful for ISAF as a public relations gimmick.) I can think of some semi-anthropomorphizing ways to explain this around how the GPT-4 class of models were ‘overthinking’ the label, but more investigation would be needed to really understand this. Accuracy for min_leaders_killed Code We often hear about how LLMs are bad with numbers, how they default to certain values and so on, so I was surprised to see such high scores across the board for this task. I imagine this is something that everyone has been trying to improve and it shows. Still, though, our finetuned models do best. Accuracy for min_leaders_captured Code Final aggregate scores for the models Let’s add all these individual competency scores up, average them out and get final scores for how well our models do on accuracy. Code Surprising even me, the finetuned models beat out the GPT-class models from OpenAI. Actually even TinyLlama beats out GPT 3.5 Turbo! Our top performer was Mistral-7B (finetuned on OpenPipe), very closely followed by Solar LLM and Llama3-7B. Just looking at the scores above it seems like anyone finetuning models for structured data extraction would do well to start of with Mistral-7B, Solar 7B or Llama3-7B and then see which performs best, though with the caveat that they might all be more or less the same for accuracy. There are probably different tradeoffs when it comes to serving the model and the efficiency and latency there, but still these three do really well. I think if I were to stuff a few more examples into the prompt (as well a bit more explanation and rules) I could get the OpenAI models to perform even better, but at a certain point you have to remember all the things that having your own finetuned model brings you: data privacy (by not sending your confidential information to OpenAI) smaller models most likely means better performance (though I still have to test and prove that out) more control overall cost improvements On the cost, it’s a bit hard to make that comparison or claim right now, especially given the economies of scale that the large cloud providers can rely on, but in a real-world use case where you were building this model for repeated inference over a long-term time frame, then you would have a better chance of having the cost argument make sense, particularly since the only way to make the OpenAI inference calls better to stuff them full of examples and extra explanation, significantly bloating the cost-per-query. That said, there are some real tradeoffs that come up when finetuning a model, and I’ll get to some of those in my concluding thoughts. Finetuning works a charm, but… First off I’m so pleased that the oft-repeated “finetune your model and get better performance than with GPT-4” actually turned out to be true! And not only was it true, but it was true with relatively little tweaks and adaptations. Remember all the above models are the first finetunes I made with the data I brought. I basically used just the default values for everything and so it worked out of the box. For any further work I’ll focus on the Solar, Llama3 and Mistral 7B models which performed best. I used cloud finetuning services to finetune the best performing versions of those models, so I’ll want to get that all working locally as well. Evals were a pain (this time round)… Most of the evaluation work is represented here in this notebook, and that was perhaps the seeds of my own misfortune. I had some models that worked locally, and then a bunch of other models deployed in different environments and with different services. Not only that, but it was pretty slow to iterate through the 724 row so my test data (which the models hadn’t seen during finetuning, just to be clear) since I implemented it fairly naively. If I were to now make some updates to the models, or get them working locally, I’d really want to make sure that I have a way to run these evals locally as well. Moreover, I’d want a way to run a subset of the evals (i.e. on a slice of the data) and then at some point switch that out so that they could run across all the data. All of this is completely within the realm of possible, but for this round I was more focused on getting the results than I was about making the process repeatable and/or efficient. I know I can’t run all the models concurrently on the same machine, so maybe the way forward is simply to have a reliable cloud GPU provider like Modal where I can farm out these evaluations. I had a really good experience with them when I used them, so that’s probably the way forward there. In general, it was also painful having the models living in different places. I had to remember so many things. In any ideal world, you want a standard interface for inference to all your models, especially if they’re for the same use case or project. It’s convenient that my finetuned GPT3.5 is automatically deployed and served by OpenAI, and the same goes for Llama3 and Solar or Mistral, but I want a single place where I can see them all. Until now I hadn’t really seen this project or problem as being so much about MLOps, but when you have multiple models in play and you’re finetuning and updating them and data is changing all the time, then you’ll need a way of managing all this. This is funny to me since I work at an MLOps company – we build an open-source MLOps framework that helps you set up a platform – but I hadn’t anticipated it’d reach this point where I’d need something like a ZenML so soon. This is, of course, one of the major tradeoffs of finetuning LLMs, in that you have to manage all this stuff in order to make it work reliably and repeatably. Even at this early stage of my project, it’s clear that you need a way to keep everything straight without making mistakes. …but evals give me a way to know if I’m making progress Even though the evaluations were somewhat painful to implement (at least in the form of this Jupyter notebook), they have given me an amazing gift in that I now have a task-specific way to know whether any of the improvements or refinements to either the training data or to the model are helping move me forward. Without this I’d essentially be flying blind. Next Steps I had originally thought and suggested that I’d want to train multiple models to be super-specialists in their field, so for example to have one model that was really good at estimating how many people were captured in a particular event. Seeing the performance of my models, I’m not sure that’s the obvious next step for this project, or if I’d really be able to boost the accuracy by a significant amount by taking that approach. This project is all about accuracy, so it’s possible that I might want to try that out, but for now I’m still exploring all the different phases of the LLM finetuning process so I’ll put the submodels idea on the backburner. The first obvious next step is to run some evaluations for the non-accuracy-related tests mentioned in my last blog. For example, I’d like to see how it performs with out of domain data (i.e. completely made up data about something completely different). The other next step is to get into some of the details around model serving. I’d like to take my top three performers and dive into how LLM model serving is done. I’m familiar with non-LLM model serving and some of the ways people do that through my work, but LLM serving has it’s own tricks, tradeoffs and tools and I’m eager to learn more about those. If this was a problem that I was deeply invested in solving beyond these already excellent results, I’d probably also want to dive into the areas where my LLMs struggled. So I’d take all the places where my LLMs failed to get the answer correct, load them up into some kind of web interface like Lilac or Argilla and really inspect my data further. Understanding the failure scenarios will probably do more for the accuracy than any tweaking of the finetuning parameters or the like. For now, I’m just happy the finetuned models beat GPT-4!",
    "commentLink": "https://news.ycombinator.com/item?id=40843848",
    "commentBody": "My finetuned models beat OpenAI's GPT-4 (mlops.systems)300 points by majc2 10 hours agohidepastfavorite77 comments kcorbitt 5 hours ago(Disclaimer: I'm the founder of OpenPipe, one of the fine-tuning services OP tried and ultimately the one that produced the highest performing model, it appears.) Data extraction is a use case that fine-tuned models are fantastic at, so I'm not surprised that OP got good results. That said, I've also found it's pretty easy to beat GPT-4 across many task types if you have a way of getting strong training data. We published some research[1] a week ago where we found that across 4 example tasks spanning creative summarization, question answering, data extraction and classification a fine-tuned Llama 3 8B was able to outperform GPT-4 on 3 of them. The key was to create a repeatable way of generating high-quality training data, which is also addressed in the post. [1]: https://openpipe.ai/blog/mixture-of-agents reply GlassOwAter 2 hours agoparentIs this something, as a tech enthusiast that's no expert, I can easily fine tune are run? My use case would be fine tuning on technical docs. Specific news, 2 years of blog posts, primary source material, and Twitter explainer thread. I want to gather all the niche information of a topic from the last two years, dump it into this and have an LLM that is a subject-matter expert. reply babelfish 1 hour agoparentprevIs using model responses to train a new model against the ToS for the major LLM providers (OpenAI, Anthropic, etc)? reply yreg 1 hour agorootparentThere doesn't seem to be any restriction like that in OpenAI terms. reply zepton 1 hour agorootparentThere is: \"you may not... Use Output to develop models that compete with OpenAI\" (from https://openai.com/policies/terms-of-use/) reply yreg 59 minutes agorootparentThanks, I've missed that. I suppose the Output could be washed by publishing it on the web and having another entity crawl it. OpenAI doesn't treat anyone else's content any differently, acting like it's a fair game, so why should we care. reply babelfish 40 minutes agorootparentIt seems like you do not work for OpenPipe (OP), so it probably doesn't matter for you, but it could (should) matter a whole lot for OpenPipe and/or their customers reply colordrops 3 hours agoparentprevWhy isn't someone providing a \"meta model\" that uses an LLM to choose between various fine tuned models depending on the question to get overall better results than gpt4? reply billmalarky 3 hours agorootparentFounding AI Engineer at OpenPipe here, using a fine tuned \"router LLM\" to route between various specialized (inc fine tuned but not necessarily) applied models depending on the input is becoming a common pattern in more modern \"graph like\" LLM applications. See LangGraph's \"conditional edges\" concept here: https://langchain-ai.github.io/langgraph/concepts/low_level/... You can see how that \"routing function\" could include a call to a \"Router LLM.\" And yes, fine tuning is a great method to better improve the routing intelligence of said Router LLM. Great question btw! reply bashfulpup 2 hours agorootparentprevAlready a big thing. See the constellation architecture used here: https://arxiv.org/html/2403.13313v1 reply sheepscreek 2 hours agorootparentprevVery loosely, isn’t this what is happening inside most LLMs that have a “multi-head” mechanism? reply gillesjacobs 8 hours agoprevThis is entirely unsurprising and in-line with the finding that even small specialized models do better in information extraction and text classification. So no wonder finetuned large LMs do good too. Personally, my PhD did fine grained ACE-like event and sentiment extraction and \"small\" specialized finetuned transformers outperformed prompting LLMs like BERT and Roberta-large. Would love to see an inclusion of small model scores with some sota pipelines. This is great work anyway even if it replicates known results! reply renegade-otter 3 hours agoparentThe caveat here is that if you don't know how to create good specialized models - you are just wasting everyone't time and money: https://www.threads.net/@ethan_mollick/post/C46AfItO8RS?hl=e... reply gillesjacobs 1 hour agorootparentExactly, BloombergGPT performed worse on financial sentiment analysis then much smaller fine-tuned Bert-based models. For many extractive tasks BloombergGPT was quite disappointing. A 5-10% performance hit with much larger inference cost compared to smaller models is not desirable. But the research investment for Bloomberg makes sense to take the risk: a do-it-all generative model can mean significant complexity reduction in maintenance and deployment overhead. It didn't directly pay off for many extractive tasks, but I bet they're iterating. Bloomberg has the data moat and the business needs in their core products to make it worthwhile. reply pandatigox 8 hours agoparentprevYour thesis sounds interesting! Do you have a link to it by any chance? reply gillesjacobs 6 hours agorootparentrovr beat me to it below. Here are more links: https://jacobsgill.es/phdobtained (fun fact: because my thesis contains published papers, I am in breach of a few journal's copyright by uploading my own thesis pdf, but fuck'em). LLM approaches were evaluated on my own time and but published (I left research after obtaining my PhD). reply pandatigox 5 hours agorootparentThank you for the link! And congratulations on obtaining your PhD I have skimmed through it and it's truly amazing how good annotation of the dataset can lead to impressive results. I apologise in advance if the question seems ignorant: The blog post talked about fine-tuning models online. Given that BERT models can run comfortably on even iPhone hardware, were you able to finetune your models locally or did you have to do it online too? If so, are there any products that you recommend? reply gillesjacobs 4 hours agorootparentThanks! The fine-tunes where done in 2019-21 on a 4xV100 server with hyperparameter search, so thousands of individual fine-tuned models were trained in the end. I used weights and biased for experiment dashboarding the hyperparam search, but the hardware was our own GPU server (no cloud service used). I doubt you can fine-tune BERT-large on a phone. A quantized, inference optimised pipeline can be leaps and bounds more efficient and is not comparable with the huggingface training pipelines on full models I did at the time. For non-adapter based training you're going to need GPUs ideally. reply uolmir 2 hours agorootparentprevI don't know your circumstances but often you retain the right to distribute a \"post print\", ie the final text as published but absent journal formatting. A dissertation should fit that definition. reply gillesjacobs 1 hour agorootparentThis is indeed often the case, however, my university reviews each thesis, and deemed it can only change to open access in 2026 (+5 years from defense). I think this is default policy for thesis based on publication agreements here. In any case, I am not too worried. reply Mockapapella 3 hours agorootparentprevThis is really cool -- thanks for posting it! I'll have to skim through it at some point since a lot of my work is in classifications models and mirrors the results you've seen reply SpaceManNabs 3 hours agorootparentprev> because my thesis contains published papers, ..., but f 'em Excluding the part in the middle because I don't wanna repost potential issues for you. I just wanted to comment that that is terrible. People often talk about the siloed nature of research in industry, without considering that academia supports the draconian publishing system. I understand IP protection, but IP protection doesn't have to mean no access. This is such a huge issue in the bio- world (biostats, genetics, etc). reply rovr138 6 hours agorootparentprevCheck https://www.researchgate.net/publication/356873749_Extractin... reply wuschel 7 hours agorootparentprevSeconded! Any URI to your PhD? reply rovr138 6 hours agorootparentCheck https://www.researchgate.net/publication/356873749_Extractin... reply dimask 7 hours agoprevThanks for putting all this work and sharing it in such a detail! Data extraction/structuring data is the only serious application of LLMs I have actually engaged in for real work and found useful. I had to extract data from experience sampling reports which I could not share online, thus chatgpt etc was out of question. There were sentences describing onsets and offsets of events and descriptions of what went on. I ran models through llama.cpp to turn these into csv format with 4 columns (onset, offset, description, plus one for whether a specific condition was met in that event or not which had to interpreted through the description). Giving some examples of how I want it all structured in the prompt, was enough for many different models to do it right. Mixtral 8x7b was my favourite because it ran the fastest in that quality level on my laptop. I am pretty sure that a finetuned smaller model would be better and faster for this task. It would be great to start finetuning and sharing such smaller models: they do not really have to be really better than commercial LLMs that run online, as long as they are not at least worse. They are already much faster and cheaper, which is a big advantage for this purpose. There is already need for these tasks to be offline when one cannot share the data with openai and the like. Higher speed and lower cost also allow for more experimentation with more specific finetuning and prompts, with less care about token lengths of prompts and cost. This is an application where smaller, locally run, finetunable models can shine. reply hubraumhugo 6 hours agoparent> Data extraction/structuring data is the only serious application of LLMs I fully agree. I realized this early on when experimenting with GPT-3 for web data extraction. After posting the first prototype on Reddit and HN, we started seeing a lot of demand for automating rule-based web scraping stacks (lots of maintenance, hard to scale). This eventually led to the creation of our startup (https://kadoa.com) focused on automating this \"boring and hard\" problem. It comes down to such relatively unexciting use cases where AI adds the most value. AI won't eliminate our jobs, but it will automate tedious, repetitive work such as web scraping, form filling, and data entry. reply furyofantares 3 hours agorootparentThe way you cut that quote turns it into an assertion that doesn't exist in parent post. They didn't make the (incorrect) statement that no other serious, useful application exists. But that's how it reads when you cut off before \"I have actually engaged in for real work and found useful\" reply jappgar 1 hour agorootparentTo be fair the original sentence could still be implying the same thing. The second half of the sentence just sounds like a hedge. reply strickvl 6 hours agoparentprevThanks! Yes one 'next step' that I'd like to do (probably around the work on deployment / inference that I'm turning to now) will be to see just how small I can get the model. Spacy have been pushing this kind of workflow (models in the order of tens of MB) for years and it's nice that there's a bit more attention to it. As you say, ideally I'd want lots of these tiny models that were super specialists at what they do, small in size and speedy in inference time. As I hinted towards the end of the post, however, keeping all that updated starts to get unwieldy at a certain point if you don't set it all up in the right way. reply scosman 9 hours agoprevAnd that’s the point of fine tuning models. Still good to see someone walk through their fine tuning process, with a mix of hosted and local options. reply scosman 7 hours agoparentOn that note: is there a good service for “here’s my dataset”, please fine tune these 9 models and give me evaluation stats? reply strickvl 7 hours agorootparentOpenpPipe - https://openpipe.ai/ - is probably the service that most closely resembles what you’re asking for, but I found the evals weren’t really what I wanted — i.e. following my custom evaluation criteria — so you probably will end up having to do that yourself anyway. But for the finetuning, they’re all somewhat the same. Predibase and OpenPipe are two good options for that. Predibase has more base models for you to finetune, but it’s a bit more unwieldy to work with. I wrote about that in a previous post here -- https://mlops.systems/posts/2024-06-17-one-click-finetuning..... reply scosman 1 hour agorootparentWild to see them advertising collecting GPT4 responses for training other models. That’s definitely not allowed by TOS. I suspect many do, but front page advertising is another thing entirely. reply kcorbitt 4 hours agorootparentprev(Disclaimer: founder of OpenPipe). Thanks for the shout-out. Note that we're actively working on improved evaluations that will let you add more specific criteria as well as more evaluation types, like comparing field values to that of a golden dataset. This is definitely something that customers are asking for! reply tucnak 7 hours agorootparentprevTogether.AI is a good starting point. Even though I'm not sure what fine-tuning method they're using, the results are REALLY good. reply geokon 6 hours agoparentprevAs I understood the point was not that they fine tuned a model and it got better They use a much simpler model, fine tune it, and manage to beat a way more advanced model reply scosman 48 minutes agorootparentThat’s still the point. That model now does exactly one thing, and because of that can do better than a model 50x the size that tries to do everything. It will crush it in instruction following and consistency. A fine tuned 500b parameter model would probably beat the fine tuned 7b model, but only by a bit (depending on task obviously). A lot of that capacity is being used for knowledge, and isn’t needed for extraction/classification tasks. Fine tuning isn’t touching most of those weights. The smaller models need to focus on more general language skills, not answering “describe the evolution of France’s economy in the 1800s”. reply wongarsu 5 hours agorootparentprevWhen jumping from 7B parameters to 70B to 400B (or whatever GPT-4 uses) most of the additional neurons seem to go towards a better world model and better reasoning (or whatever you want to call the inference of new information from known information). There doesn't seem to be any major improvements in basic language skills past 7B, and even 1B and 3B models do pretty well on that front. In that sense it's not that surprising that on a pure text extraction task with little \"thinking\" required a 7B model does well and outperforms other models after fine tuning. In the \"noshotsfired\" label GPT-4 is even accused of overthinking it. It is interesting how finetuned mistral-7b and llama3-7b outperform finetuned gpt3.5-turbo. I would tend to attribute that to those models being newer and \"more advanced\" despite their low parameter count, but maybe that's interpreting too much into a small score difference. reply scosman 43 minutes agorootparentRe: 7b models vs gpt-3.5, I’m guessing different fine tuning parameters can account for the difference. The OpenAI fine tuning is a black box. reply mewpmewp2 5 hours agoprev1. It would be nice to see examples where GPT-4o was inaccurate, but best performing models were accurate. 2. It would be nice to try again with 0 temperature, as I do a lot of structured data extraction. In my experience 0 temperature should always be used, and it can make a huge difference. Temperature of 1 essentially means that it will start to pick tokens with lower probability of being accurate... reply jrm4 4 hours agoprevAt the risk of sounding like an old head; Seems to me then, priority one should be \"free and open source all the models as hard as possible, so that EVERYONE can fine-tune.\" (This being a subset of the idea of, free / open source is generally preferable for both freedom and quality) reply klabb3 4 hours agoparentIt seems to me this means whoever has hoarded and declared ownership of the most personal data will make the best products. Kinda like how some people liked their targeted ads because they’re more “relevant”, only now it’s not just ads but useful products. Another winner is of course platform owners like Apple and Microsoft who can scrape your data off their apps and products, even locally. This is a much bigger edge than being 3-6 months ahead in model quality. I despise the centralization of this tech as well, and while it’s hopeful that smaller fine tuned models are better, they won’t win (or barely stand a chance) out of the virtue of openness and privacy alone. Best we can hope for is proliferation in the small-medium sized business service space - that OpenAI tokens are not worth the extra expense if open models are commoditized and effective. This was probably Zuck's plan all along – to prevent centralized gate keepers in tech that’s mainly benefiting his rivals. But the enemy of my enemy is my friend, so his actions may be the best he’s ever done for the public good. reply jrm4 10 minutes agorootparentYour end point I think is exactly right. I think your first one is getting downvoted hard because your first sentence is not at all how any of this works. Sucking down personal data isn't JUST a bad idea for privacy, it's actually also bad for \"making the best products,\" I think you're overstating the extent to which all that data that is stolen and sold to the highest bidder actually helps the company buying it? reply simonw 2 hours agoprevI'd be interested to see how well these fine-tuned models compare to Claude 3 Haiku (or one of the more expensive Claude models) with a larger set of examples. The Claude models all have a 200,000 token limit and respond _really_ well to examples - you can feed them in as chat JSON message pairs of user input / ideal assistant output. Haiku is dirt cheap for this kind of thing and with 200,000 tokens you can probably provide a dozen or so examples. reply denhaus 7 hours agoprevFor anyone interested, we wrote a paper on a similar topic: https://www.nature.com/articles/s41467-024-45563-x reply mewpmewp2 5 hours agoprevI took a look at a random row to try to find why mistakes were happening. Why is this one labelled with start_date: 2011-02-07? > Afghan, Coalition Forces Clear Northern Kandahar ISAF Joint Command - Afghanistan 2011-02-D-081 For Immediate Release KABUL, Afghanistan (Feb. 12) – Afghan and coalition forces set out to provide security and assist the local population during a clearing operation in a remote village in Shah Wali Kot district, Kandahar province, Feb. 8. District Chief of Police Bacha Khan, and his policemen; Afghan commandos from 2nd Company, 3rd Commando Kandak, along with U.S. service members from Special Operations Task Force – South, searched the village throughout the day and detained 20 suspected insurgents. Also found were 80 pounds (36 kilograms) of homemade explosives and various improvised explosive device-making materials. Leading a squad during the operation was Afghan commando Sgt. Hafiz Rahman, who said this operation has shown him progress. “The people are respecting us,” Rahman said. “They ask us if we want tea, or ‘do we want bread?’ They are thankful for the security.” Children during the operation brought commandos blankets in the evening and offered them food throughout the day. Trying to find the source, I'm also not seeing any indication of Feb 7. https://www.dvidshub.net/news/65238/afghan-police-commandos-... --------------- And why is this labelled as Mar 6, GPT-4o and I personally find Mar 7 to be logical. ISAF Joint Command Morning Operational Update, March 8, 2011 ISAF Joint Command - Afghanistan 2011-03-S-022 For Immediate Release KABUL, Afghanistan (March 8, 2011) Afghan and coalition forces targeted a Taliban district chief, killed one insurgent and detained several others during an operation in Burkah district, Baghlan province, yesterday. The Taliban district chief maintains ties to Taliban senior leadership throughout Kunduz, Baghlan, and Takhar provinces. He is involved in purchasing weapons and IEDs. Intelligence reports led the security force to the targeted compound in the city, where Afghan forces called for all occupants to exit the buildings peacefully before conducting a search. During that time, an armed individual threatened the security force and the force returned fire, killing him. Several suspected insurgents were detained after initial questioning at the scene. But despite that the \"finetuned\" model also gets Mar 6. How does the finetuned model get Mar 6? reply courseofaction 8 hours agoprevReally interesting. Could the potentially controversial content of the target news article have an effect on ChatGPT's ability to summarize it? reply gillesjacobs 6 hours agoparentI use LLM information extraction for financial news articles with OpenAI Azure and it is a huge problem for me. 404 Content moderation response in 4% of articles. This is just financial news text. It is a prime reason we are considering open models. reply strickvl 8 hours agoparentprevI think not. Normally if you get those kinds of errors you wouldn’t get any output at all. In the blog I show that all 724 of the test cases got proper JSON output etc for the queries so I don’t think this was an issue. I think these kinds of topics would have been well covered in the training data, and probably the OSS models would have used similar data so I don’t even think there’s a disparity to be found between proprietary vs OSS models here. reply resource_waste 7 hours agorootparent>Normally if you get those kinds of errors you wouldn’t get any output at all I am not sure. I disagree. If there is a pro-chatGPT user, I'm probably it. Ive often seen it give significantly less effort to answer the question. reply strickvl 7 hours agorootparentInteresting. I can maybe try finetuning one or two of the so-called 'uncensored' open models and see if that makes a difference. A bit harder to switch out the dataset completely, as that's really what I'm interested in :) I think the general point that finetuning a model for some custom task works is fairly uncontroversial, but if OpenAI's poor performance was on account of these kinds of guardrails it'd be yet another reason someone might want to finetune their own models I guess. reply animanoir 30 minutes agoprevAnything beats GPT-4 nowdays to be honest. reply toisanji 4 hours agoprevI'm most excited about getting a faster model. A model like GPT4 can be overkill because its too slow. What are the smallest fine tuned models that could beat a gpt4 model? Is it 7b or could a 3b model like phi3 do well for tasks like classification and summarization? reply Tiberium 6 hours agoprevDid you release the dataset and the code for testing? It would be interesting to check how 3.5 Sonnet performs on this task. reply mewpmewp2 5 hours agoparentThe dataset is there: https://huggingface.co/datasets/strickvl/isafpressreleases_t... but when looking for rows where GPT-4o was deemed inaccurate then to me it seems the label was wrong or at least it wasn't possible to infer that certain label from the input text. But finetuned model was able to predict it. Which makes me wonder whether the finetuned models are poisoned with eval data... See this one: > ISAF Joint Command Morning Operational Update, March 8, 2011 ISAF Joint Command - Afghanistan 2011-03-S-022 For Immediate Release KABUL, Afghanistan (March 8, 2011) Afghan and coalition forces targeted a Taliban district chief, killed one insurgent and detained several others during an operation in Burkah district, Baghlan province, yesterday. The Taliban district chief maintains ties to Taliban senior leadership throughout Kunduz, Baghlan, and Takhar provinces. He is involved in purchasing weapons and IEDs. Intelligence reports led the security force to the targeted compound in the city, where Afghan forces called for all occupants to exit the buildings peacefully before conducting a search. During that time, an armed individual threatened the security force and the force returned fire, killing him. Several suspected insurgents were detained after initial questioning at the scene. It claims \"Yesterday\" on March 8, so you would assume March 7 is correct start_date, but it's labelled Mar 6, and finetuned models get it \"right\", while GPT says Mar 7. reply alach11 56 minutes agorootparentProps to the author for releasing the data. My instinct is also to immediately suspect data leakage. It's super easy for this to happen. For example the original dataset could contain multiple articles about the same event. reply wrsh07 4 hours agorootparentprevI was wondering if there was some info in the bizarrely formatted date, but I think 022 is just the issue number: https://www.dvidshub.net/news/66703/correction-isaf-joint-co... reply mewpmewp2 4 hours agorootparentAlso a lot of the time the dates are wrong seems to be due to only having those formats, which does make me wonder again how do fine tuned get this right unless they have been fine tuned using eval data... reply botro 7 hours agoprevThanks for sharing this, It's well written and informative. I noticed you used 'temperature=1' in the GPT test for the example in the post. Is this best practice for a task requiring structured output? Have you tested other temperature settings? My casual understanding was that a temperature of 0 is best for these types of workloads while higher temperatures would be more effective for more 'creative' workloads. reply strickvl 7 hours agoparentI followed whatever the guidance was for a specific model. Some of the LLM finetuning providers did indeed set the temperature to 0 and I followed that, but others suggested 1. I could probably iterate a bit to see what is best for each model, and I might well do that for the one that I choose as the one I’ll be doubling down on in subsequent iterations / finetunes. Thanks for the suggestion! reply mewpmewp2 5 hours agorootparentFor GPT, I would really urge to try again with 0. 1 kind of starts to force it to fail. I would say this actually invalidates the whole thing. reply Tiberium 6 hours agorootparentprevGPT models shouldn't be used at temp 1 unless you only care about creative writing. They get much worse at factual stuff and code than with lower temperatures. And yes, 3.5 Turbo is less affected by this, which might be the reason why the models performed for you in reverse. reply bongodongobob 4 hours agorootparentprevYou never use 1 for stuff like this. 1 is for poetry and creative writing. You need to redo this with temp=0 imo. reply visarga 8 hours agoprevWhat is a good fine-tuning script for Mistral and LLaMA3 on an A100? reply strickvl 8 hours agoparentDepends a bit where you’re running etc. This works for Modal, e.g., but they’re just using axolotl under the hood so you can just connect to whatever cloud provider of choice you’re using and then run axolotl straight. I did my finetunes across local GPUs, but it would have been just as easy to do it in a cloud environment using the same axolotl config. reply swalsh 6 hours agoparentprevUnsloth is a great tool, super fast. reply strickvl 6 hours agorootparentBut still only single GPU for now. I also heard great things about it, but wanted to make the maximum use of my multi-GPU local setup. reply uptownfunk 3 hours agoprevRemember folks there is no free lunch :) reply pcwelder 7 hours agoprevHere are some test data samples and corresponding closest train data rows to give you an idea of the task complexity. --- Test 1: KABUL, Afghanistan (Jan. 25, 2013) During a security operation in Andar district, Ghazni province, yesterday, an Afghan and coalition force killed the Taliban leader, Alaudin. Alaudin oversaw a group of insurgents responsible for conducting remote-controlled improvised explosive device and small-arms fire attacks against Afghan and coalition forces. Prior to his death, Alaudin was planning attacks against Afghan National Police in Ghazni province. Train: KABUL, Afghanistan (Jan. 8, 2013) – During a security operation in Washer district, Helmand province, yesterday, an Afghan and coalition force killed the Taliban leader, Mohammad Sayed, and one other insurgent. Mohammad Sayed distributed weapons and ammunition to Taliban fighters. Prior to his death, Sayed was attempting to acquire rockets for attacks targeting Afghan government officials in the province. --- Test 2: For Immediate Release KABUL, Afghanistan (Aug. 6, 2012) Afghan and coalition forces conducted a security operation in search of a Haqqani leader in Tsamkani district, Paktiya province, yesterday. During the operation the security force engaged a group of insurgents with a precision airstrike. After the strike, the Afghan and coalition security force conducted a follow-on assessment and confirmed several insurgents had been killed in the strike. They also confirmed the strike had not injured any civilians or damaged any civilian property. Train: For Immediate Release KABUL, Afghanistan (July 22, 2012) — Afghan and coalition forces conducted a security operation in Muhammad Aghah district, Logar province, Saturday. During the operation, a group of armed insurgents were engaged with a precision airstrike. After the strike, the Afghan and coalition force conducted a follow-on assessment and confirmed multiple insurgents had been killed. The security force also confirmed the airstrike had not injured any civilians or damaged civilian property. --- Test 3: ISAF Joint Command Morning Operational Update March 24, 2011 ISAF Joint Command - Afghanistan 2011-03-S-081 For Immediate Release KABUL, Afghanistan (March 24, 2011) A separate Afghan and coalition security force targeted a Taliban IED cell leader in Kandahar today. The leader is responsible for planning, preparing and executing explosive-device attacks on Afghan civilians, Afghan and coalition security forces. The joint security force targeted the leader’s suspected compound in Kandahar City based on tips from citizens. The security team contained the area and detained several suspected insurgents. There were no shots fired and no damage done to the targeted compound. Train: ISAF Joint Command Operational Update Dec. 22 ISAF Joint Command - Afghanistan 2010-12-S-267 2699, 2935, 3022, 3078 For Immediate Release Download PDF KABUL, Afghanistan (Dec. 22) – Several insurgents were killed by Afghan National Security and International Security Assistance Forces in separate clearing operations in southern Afghanistan over the last 24 hours. An Afghan Army and ISAF patrol spotted some insurgents emplacing an improvised explosive device in Sangin district, Helmand province today. After gaining positive identification, combined forces engaged the enemy position, killing two insurgents. reply blackice_cowboy 5 hours agoprev. reply sva_ 5 hours agoprevClickbait headline reply XiphiasX 7 hours agoprev [–] 1) beat at what? 2) do they beat Claude 3.5 Sonnet? reply input_sh 7 hours agoparentHave you tried clicking on the link and finding out? reply singularity2001 6 hours agoparentprevJust in the task of structured data extraction So very misleading title reply furyofantares 3 hours agorootparent> So very misleading title Eh, I can see that, but to me \"finetuned model\" pretty strongly implies some specific task reply freehorse 7 hours agoparentprev [–] Did you read the article or just the title? It is all explained there. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post discusses the evaluation metrics for assessing the performance of finetuned language models (LLMs) in extracting structured data from press releases, with a focus on accuracy.",
      "Finetuned models, including TinyLlama, Mistral, and Solar LLM, generally outperformed OpenAI's GPT-4 and GPT-4 Turbo in accuracy, despite the complexity and slow pace of evaluations.",
      "The evaluations highlighted the need for a better system to manage the complexity and maintenance, with future steps including non-accuracy-related tests and exploring model serving."
    ],
    "commentSummary": [
      "Fine-tuned models can outperform general models like OpenAI's GPT-4 in specific tasks, such as data extraction, creative summarization, question answering, and classification.",
      "The success of fine-tuned models hinges on high-quality training data, making them effective for niche information extraction and accessible to tech enthusiasts.",
      "Fine-tuning smaller models, such as Llama 3 8B, can be more efficient and cost-effective, but using model responses to train new models may violate terms of service for major LLM providers."
    ],
    "points": 300,
    "commentCount": 77,
    "retryCount": 0,
    "time": 1719823984
  },
  {
    "id": 40843720,
    "title": "Cities need more trees",
    "originLink": "https://herman.bearblog.dev/cities-need-more-trees/",
    "originBody": "Cities need more trees 01 Jul, 2024 I grew up just outside of Johannesburg, which is touted to be \"the greenest city in the world\". During the summer, when seen from above, it's a verdant, green landscape, with the tops of buildings popping through the canopy. Yes, there's a full city beneath those trees. However, this is not what the land originally looked like. To cut a long story short, Johannesburg was established following the discovery of gold in 1886. It's known colloquially as eGoli, or \"The City of Gold\". Gold extraction here is vastly different to other parts of the world. We don't have gold veins running through the rock like in old Western movies. Instead it's mineralised into the rock itself which requires significant processing to extract. This form of extraction leads to the crushing of millions of tons of rock, which is then dumped creating large, dusty hills around the city. These hills are (especially to the communities surrounding them) toxic and radioactive1, and when the wind picks up yellow dust clouds used to envelop the city. One of the proposed solutions to suppressing this omnipresent dust was the planting of trees (1.2 millions as of 2024) on sidewalks and pavement, and millions more planted privately in back-yards and plots. Due to Apartheid in South Africa, which economically and socially disadvantaged the majority of the population based on race, these planting efforts were not equitably distributed. You can still see a stark difference between rich and poor neighbourhoods to this day based purely on tree cover. But I'm not here to talk about the socioeconomic problems facing South Africa. Instead I'd like to focus on how the planting of so many trees affected the city itself. Since these trees were originally planted to manage dust, they are generally big and leafy. This has the benefit of creating a lot of shade throughout the city, mitigating a lot of the \"heat island\" effect which is pervasive in any city since asphalt and concrete are great at absorbing visible spectrum light and radiating it as heat2. Walking down a shady street on a hot summer day is such a pleasant experience when compared to being out in the blistering sun. Trees are not just great dust sinks and heat shields, but great sound barriers as well3. Having tree lined streets not only reduces the noise of traffic (and the dust kicked up from their tyres), but also protects pedestrians and infrastructure on the sidewalks from stray vehicles. On top of that it just looks better. I'm certain humans have genetic biophilia4, which is why we love being in nature or taking walks in the forest. Having trees around us, teeming with birds and other life just feels good. Speaking of birds, trees increase the biodiversity of insects and other small critters in urban environments. It also gives birds a safe-haven from the deadliest hunter of all: the humble house cat5. I've been to many cities, all over the world. Some that do trees well, and others that fail miserably. And I'm always shocked at the contrast. Given two cities with a similar climate, I'm significantly more likely to do things like walk to a local coffee shop and browse corner stores, or sit on a park bench. But only if the walk is pleasant and shady. Planting trees as a form of environmentalism or carbon capture is generally a hit-or-miss strategy. The vast majority of planted trees never reach maturity6, and depending on where the trees are planted it could have a net heating instead of a net cooling effect due to the change in the environment's albedo7. However, planting trees in cities is pretty much all upside with almost no downside (except that birds tend to shit on my car). So the next time you're enjoying a walk down a lovely shady street, take a look up and appreciate the trees. Footnotes Radioactive city: how Johannesburg’s townships are paying for its mining past↩ Urban head islands↩ How do trees reduce noise pollution?↩ Biophilia hypothesis: The idea that humans possess an innate tendency to seek connections with nature and other forms of life.↩ The impact of free-ranging domestic cats on wildlife↩ Phantom Forests: Why Ambitious Tree Planting Projects Are Failing↩ Could tree planting warm Earth? Science behind the albedo effect↩ 33",
    "commentLink": "https://news.ycombinator.com/item?id=40843720",
    "commentBody": "Cities need more trees (herman.bearblog.dev)273 points by HermanMartinus 10 hours agohidepastfavorite139 comments jurmous 9 hours agoHere in Utrecht the Netherlands they are trying to greenify the city to reduce heat stress during the hot months. They try to plant as much trees and plants as possible, they try not to mow grass often and let it grow, they encourage homes to remove tiled gardens and add green, they have programs to turn roofs into green roofs. I like it, the city feels nicer to life in. https://healthyurbanliving.utrecht.nl/fileadmin/_processed_/... https://healthyurbanliving.utrecht.nl/our-vision-for-utrecht... https://aiph.org/floraculture/news/utrecht-is-crowned-the-ne... reply config_yml 8 hours agoparentSame in Zurich, where the city tries to establish a “Schwammstadt”. But the tree growth is actually negative, because on private properties people are chopping down their trees to build lucrative housing. reply BSDobelix 8 hours agorootparentIf you look at bellevue i personally think it's disgusting, that loveless massive sechseleutenplatz, lot's of cars always using the horn, what a waste of that iconic place constricted between traemli and a always busy street, tbh even Delhi is less stressful, let alone Vienna. EDIT: Do you mean Schwanstadt right? That lovely smell of birdshit in the sommer plus the massive stink of piss and puke after (and up to 10 days) streetparade is the pure soul of Zurich ;) reply Xylakant 8 hours agorootparentNo, he means \"schwammstadt\" (sponge city), the concept idea that you need to create the capacity to soak up excess rainfall and store it like a sponge instead of relying on drainage entirely. https://en.wikipedia.org/wiki/Sponge_city reply BSDobelix 7 hours agorootparentI'm so old I can still remember when Sechseleuten Square was green grass, now replaced with searing heat assemblers of stone....so even the city is working against that sponge thing, but yeah it sounds good from a politician's mouth. reply Xylakant 6 hours agorootparentIt's a relatively novel concept, though the problems of replacing open surfaces with stone slabs and asphalt could have been obvious decades ago :( reply danaris 5 hours agorootparentAh, but you see, stone slabs and asphalt cost nothing to maintain. /s (...Even this is, of course, untrue, but it seems true when you're only paying attention to the next year or so.) reply BSDobelix 4 hours agorootparentJust 17 milion swiss franks made out of \"Valser Quarzit\" and a \"fountain\" that needs constant maintenance because it has to be drinking quality instead of lake-water, just look at that sh*t: https://aquatransform.ch/projects/sechselaeutenplatz-opernha... EDIT: But please don't waste energy otherwise (your politician) reply jajko 7 hours agorootparentprevWhich isn't a bad idea generally, but for cities like Zurich (and its sister city Geneva which is the same also in this regard) there isn't much need - it sits at the end of a massive lake, most of the city center isn't much higher than lake itself. Effect of the lake subsides somewhat with distance form it, but it definitely creates its own micro-climate thats always colder and more humid than further away from it. To have more high trees that prevent all that tarmac, concrete and stone from heating up during day is definitely a plus, but these cities are not some scorched Phoenix, AZ equivalent. reply Xylakant 6 hours agorootparentBerlin, one of the leading cities in Germany on those concepts is located on two rivers, plenty of canals and between a few lakes. Hamburg is another very active place and it's close to the seaside, with a huge river and plenty of water in the city. A lot of concepts refer back to city planning that Kopenhagen started after a massive rainfall event in 2011 - a city which is fairly flat, has a large river front and plenty of canals. Still, there's need to evaluate such city planning concepts because the lakes and rivers do not solve all of the problems associated with heavy rainfalls and drought cycles. They do nothing to handle the runoff if you get massive rainfalls in a short period of time because the water management gets overwhelmed. Also, city trees do not benefit substantially from the high ground water level and still suffer in drought. So while not all of the concepts ideas may be applicable to Berlin, Hamburg, Kopenhagen, Zurich or Geneva, concepts like green roofs, local water storage etc. are still required to respond to the incread frequency of high rainfall events and increasing summer temperatures. reply unglaublich 7 hours agorootparentprevZürich needs Lärmblitzer against pointless revving and honking. reply BSDobelix 7 hours agorootparentOr a bridge/tunnel before bellvue, and make the whole bellvue+niederdoerfli up to hauptbahnhof carfree...just imagine that! But yeah you are right with those Lärmblitzer too. reply ctenb 8 hours agoparentprevI live there as well, and while I'm sure it could be worse, it's nothing compared to Johannesburg. I would like to see much more green throughout the city. Hopefully they will keep up the trend. reply jurmous 7 hours agorootparentWell they are still in transition. My street is next. They will plant quite a bit more trees and plant areas. Looking forward to them :) reply nxobject 6 hours agoparentprevPortland, OR, has similar green roof code mandates, as well as tree replacement laws as well... although there is pressure from \"developer-friendly\" electeds to suspend these mandates. Boo for being short-sighted. reply agumonkey 7 hours agoparentprevI live at the boundary between city and forests and in summer the temperature drop is really 'incredible'. The amount of energy wasted by urban areas is as much so. But at the same time it's nice because we can restore more trees for shade easily reply jajko 7 hours agorootparentI live in foothills of a lower range of hills (European Jura), and it has numerous effects. The air is normally blowing down the mountains towards us in the evening (katabatic wind), so it doesn't matter how hot the day was, evenings are pleasantly cool and night gets colder than expected. This wind also cools hot surfaces pretty effectively. Even just entering a dense natural forest during the day, one can sense drop in temperature around 3-4C during summer, and increase in humidity during dry periods. reply fergonco 5 hours agorootparentLived in French Jura for 5 years. It is so beautiful. And I am not the only one to have noticed. The Last Man or Frankenstein have also the Jura mountains as landscape. I miss that place... (except its prices) reply agumonkey 3 hours agorootparentprev> katabatic wind I've noticed this while doing evening walk, I didn't know it was a named phenomenon. reply ctrlMarcio 8 hours agoparentprevthat made me want to live there lol reply huevosabio 9 hours agoprevOne of my biggest complains for American cities is the risk adversity with respect to trees. In SF, the city went in a rampage to prune and tear down trees (mostly ficus) because of the risk of the branches falling. There are lots of rules for where you can and can't plant trees based on road visibility, signage, electric cables etc. Result is that you have a lot of tree-less spaces in a city where basically anything grows. In contrast, Mexico City has an almost anarchist version of urban greenery. Trees overflow streets and side walks. Yes, there are issues from dealing with the urban greenery, but the city is incredibly pleasant to walk in. Also, despite being an incredibly noisy city, trees and buildings mute out a lot of the noise. reply michael_vo 7 hours agoparent\"You cannot see the wood for trees\" I'd argue there is so much good that the tradeoff of trees killing a few humans is worth it. The biological diversity that returns - birds, carbon soil. The air quality. Less chance that the heat will kill our senior citizens. Trees prevent floods. From NotJustBikes channel, trees and bushes can be used to obscure road visibility, which naturally forces drivers to slow down at a curve, which makes streets safer for pedestrians (43k deaths a year in USA from cars) reply robcohen 8 hours agoparentprevFor an opposing view, look at what happened during the freeze in Austin, Texas for an example of policies that allow for minimal tree pruning. It was a significant issue that caused the city an incredible amount of damage. Electric was shut down for days, cars were destroyed, houses had trees fall on them, etc. Might still be worth it, but an interesting data point nonetheless. reply MrGilbert 8 hours agorootparentAlbeit being a valid datapoint, the conclusion could as well be that it might be better to put electric power down in the ground, get better insurance, don't park under a tree, and so on… reply JoshGG 8 hours agorootparentThis is the standard for many cities in the northeast. Power lines are underground. reply kjkjadksj 3 hours agorootparentI’ve only seen it done for suburban tract development, never retrofitted to an existing large city. Do you have examples that aren’t a suburban tract? Just curious how it was implemented as the expenses are usually quoted to be astronomical reply XorNot 8 hours agorootparentprevRoot penetration of buried services is a serious problem. While roots generally chase water, they will surround and crush any services which get in the way. Trenching and conduiting is also a lot more expensive. Which is to say: you're looking at a huge amount of cost for what could be described as a very marginal gain because it's already a city. reply david-gpu 7 hours agorootparentWalkable green streets are not a marginal gain in my book. It is time we make our cities pleasant to live in. reply jajko 6 hours agorootparentprevThis is a non-issue in all of Europe maybe apart from south, even poor countries can afford this and root damage is negligent. Its not a serious-enough problem to change your city planning around. reply kjkjadksj 3 hours agorootparentChances are they did it decades ago when labor was far cheaper and continued to expand the system the entire time. reply CuriouslyC 7 hours agorootparentprevTell you what, you enjoy your Mumbai style wire canopy, I'll pay more to live in a place where they bury the lines and tree branches and sky are the only things over my head. reply ochre-ogre 4 hours agorootparentMumbai has mostly underground utilities, only broadband lines tend to be strung from building to building. reply worthless-trash 7 hours agorootparentprevRoot penetration is a minimal problem in most cases, sure it does break things sometimes but its probably not as bad as you're making out. reply detourdog 7 hours agorootparentprevCenter City Philadelphia has a series of squares fro green space and they are laid out in such a way that one can chart a course through the squares to navigate on foot. reply AidenVennis 9 hours agoparentprevStreetview shows this very nicely for Mexico City. I wonder if having so many trees in the city would make chopping down of few trees for some new public development less frowned upon by residents? reply rambambram 6 hours agorootparentThanks, now I just spent two hours streetview surfing through Mexico City. reply benfortuna 4 hours agoparentprevIn Australia power companies will butcher any tree that gets close to overhead powerlines. Understandable, but result is an ugly streetscape and very little opportunity for green spaces outside parks. reply inferiorhuman 8 hours agoparentprevKind of an odd flex given how walkable San Francisco is, but the \"risk adversity\" is because people actually die. Last year, five people died from fallen trees in a single storm (two in the city). reply FactolSarin 8 hours agorootparentMexico City is one of the largest cities in the world. Five people out of 21 million is nothing. Sure, every death is tragic, but there are a lot more efficient ways to spend time and money preventing deaths than tree trimming. reply inferiorhuman 7 hours agorootparentFive deaths in the Bay Area from one storm, two in San Francisco (with a population around 800,000). There were other tree related deaths last year. More to the point, the trees that the city is actively removing are old, mature ficus trees that present a variety of risks (e.g. sidewalks, sewer and other underground utilities), overhead (e.g. public transit) wires. reply kjkjadksj 3 hours agorootparentToo bad ficus are easily the densest canopy tree I’ve seen in california reply varius 7 hours agorootparentprevOn the other hand, trees cool down the area so having lot of them should lead to fewer heat-strokes. Last week we hit 95F in my area and I wouldn't be able to walk my dog if not for the trees. reply pvaldes 1 hour agorootparentIs not an easy decision. Politicians can be sued if a branch falls over somebody, but can get rid easily of the ten thousands of kills a year by contamination on air and water. Of course somebody being crushed by a tree is a real tragedy, but if the same people would have a heart attack because nitrogen emissions are out of the charts, or they get ill by asthma and can't work, would not reach the news. An interesting question is: Have people trees because is rich, or they are richer because they had trees? (and this will save a lot of money each year on energy bills, food and even psychologists). Trees are a tool to fight poverty. The problem is how to made the poor people respect them. reply have_faith 9 hours agoprevWhere I live, Sheffield in the UK, the council was nearly toppled because they chopped down some street trees. They still haven't fully recovered from it: https://en.wikipedia.org/wiki/Sheffield_tree_felling_protest... reply rob74 9 hours agoparentI'm also against chopping down trees - but sometimes tree protection turns into a pretext for NIMBYism, especially if it's against projects that are also good for the environment, such as public transport (felling 189 trees for a bus lane sounds like a lot, but if the trees shown in this photo https://proarbmagazine.com/controversial-sheffield-bus-lane-... are representative, those look more like shrubs) reply benoliver999 8 hours agorootparentIt was a bigger scale. The plan was to cut down 17,500 of 35,500 street trees, all due to a total mis-reading of a report. Instead of backing down the council ploughed on, got people arrested and even tried to jail one of its own councillors for protesting. It was a huge fiasco. reply buro9 9 hours agorootparentprevThere are a lot of species that may be misinterpreted as a shrub, i.e. a Hazel tree https://www.woodlandtrust.org.uk/trees-woods-and-wildlife/br... is dense, and forms many smaller straighter branches shooting up in parallel... could easily be mistaken as a shrub as this next pic shows https://cotswoldtrees.com/hazel-corylus-avellana/ . Still a tree though, and all trees are good trees. reply pvaldes 8 hours agorootparentHazelnut on cities are mainly from two different species. Corylus avellana is clearly a shrub, even if can grow 6m high with time, and will recover fast from being chopped, but the other species is a tree. reply pastage 8 hours agorootparentprev\"one more lane\" a buss lane means more hard and hot surfaces in a city. As long as you just add hard surfaces you are still making things worse. With that said buss lanes are more important than car lanes. reply PaulRobinson 7 hours agoprevSpent last week in South California, visiting family. Flying into LAX made me feel like I was flying into a hell scape. From the air, no green was visible for miles. Compared to my home (London, originally Manchester), I thought it not just odd but barely liveable. You could almost the heat off the concrete by looking at it from the air. Driving around as far south as Orange County for the next week I was happily surprised when I saw any indication of nature at all. Perhaps it’s just me and what I’m used to, but the only spot I visited that felt really relaxing was up around Griffith Observatory. reply CuriouslyC 7 hours agoparentNext time you visit family get them to rent a house in Santa Barbara. Technically it's \"central\" California but it's only 90 minutes from LA and it's probably the coolest and most livable place in the whole state. reply sebstefan 6 hours agorootparent\"only\" 90 minutes reply surfingdino 6 hours agorootparentprevMight as well rent a small plane. reply badpun 7 hours agoparentprevLos Angeles is located in semi-arid climate, so there's not going to be a lot of vegetation there. reply PaulRobinson 7 hours agorootparentThe Hollywood Hills show that doesn’t have to be the case. reply infecto 7 hours agorootparentThe Hollywood Hills are pretty barren no? There are trees but still very arid and not lush forest. reply tomjakubowski 4 hours agorootparentGriffith Park is full of trees. reply Kon-Peki 33 minutes agorootparentWhat the modern world thinks of as Los Angeles has always been terrible, treeless land. You have to go inland, and up the mountains, to get to the good land. From the ~1830s: https://www.gutenberg.org/ebooks/4277 > Leaving Santa Barbara, we coasted along down, the country appearing level or moderately uneven, and, for the most part, sandy and treeless; until, doubling a high sandy point, we let go our anchor at a distance of three or three and a half miles from shore. It was like a vessel bound to St. John's, Newfoundland, coming to anchor on the Grand Banks; for the shore, being low, appeared to be at a greater distance than it actually was, and we thought we might as well have stayed at Santa Barbara, and sent our boat down for the hides. The land was of a clayey quality, and, as far as the eye could reach, entirely bare of trees and even shrubs; and there was no sign of a town,— not even a house to be seen. What brought us into such a place, we could not conceive. > Leaving the boat, and picking our way barefooted over these, we came to what is called the landing-place, at high-water mark. The soil was, at it appeared at first, loose and clayey, and, except the stalks of the mustard plant, there was no vegetation. Just in front of the landing, and immediately over it, was a small hill, which, from its being not more than thirty or forty feet high, we had not perceived from our anchorage. > I also learned, to my surprise, that the desolate-looking place we were in furnished more hides than any port on the coast. It was the only port for a distance of eighty miles, and about thirty miles in the interior was a fine plane country, filled with herds of cattle, in the centre of which was the Pueblo de los Angeles,— the largest town in California,— and several of the wealthiest missions; to all of which San Pedro was the seaport. reply nurple 6 hours agoprevOne thing I'm very frustrated by is my city's(Salt Lake) push for water conservation to the point they're paying residential owners to xeriscape their property. We've spent over a century terraforming the desert into a beautiful green canopy, and every day they're building more concrete and asphalt jungles with nearly no greenery while existing properties are tearing out greenery and replacing it with rocks. We even had one politician try to say we needed to cut the trees in the canyon down to save the Salt Lake, because they're absorbing too much water. In the state, residential water usage is almost a single-digit percentage and unmetered secondary water systems have been a standard feature of neighborhoods built over farmland. They're now going around putting meters on the secondary water systems and no new developments even have them at all. I'm really worried as the city is beginning to resemble hellscapes like Las Vegas and LA. The developer-captured legislature is just pushing shit through without much thought for their livability or scalability. Couple legislative sessions ago they removed the requirement for them to review referenda brought by concerned residents, which is one of the only methods we still had to push back against overdevelopment. reply yen223 9 hours agoprev> You can still see a stark difference between rich and poor neighbourhoods to this day based purely on tree cover. You can see that here in Sydney. The poorer Western Sydney suburbs have a noticeable lack of tree coverage when compared with the richer inner west, northern, and eastern suburbs. I don't know which direction cause and effect goes. It is plausible that affluent suburbs can afford to plant trees, but it is also plausible, especially here in Australia, that trees lead to nicer climates that are more appealing to folks who can afford it. reply cjs_ac 9 hours agoparentThis is such a long-standing difference that Sydney newspapers will use the word 'leafy' to describe a suburb as having significant social capital. The local governments in the 'leafy' areas are extremely protective of their trees, for example, have a look at Hornsby Shire Council's regulations on trees[0]. They will fine you significant amounts of money if you kill or significantly prune a tree on your own property without permission. [0] https://www.hornsby.nsw.gov.au/environment/flora-and-fauna/t... reply lmpdev 9 hours agoparentprevWest Sydney was rural fields until it was developed during the later half of the 20th century I’m fairly sure the issue stems from insanely dense urban packing with a refusal to develop almost anything other than single family homes They try to maximise house size on small blocks leading to an almost impossible block to plant a tree on ——— Also pointing out Greater Western Sydney is one of the worst urban places in the world for tree cover given the climate I could never afford to live in Sydney, but my late father’s upbringing in the then “poor” North Sydney is starkly different to the quality of life available to the youth in “poor” areas of 21st century Sydney reply djrobstep 8 hours agorootparentBut it’s not just the block sizes, it’s also the streets. Often Western Sydney streets have no trees at all (and even no footpaths) reply kjkjadksj 3 hours agoparentprevIts super expensive to put in trees and maintain them. Most slumlords cut the trees down and do things like pave the entire plot. Rich people don’t move for trees. They create rich neighborhoods where there aren’t any trees all the time, then they bring in trees from nurseries and hire landscapers. reply partomniscient 7 hours agoparentprevAgreed, although I still don't get the fact we continually plant trees that grow taller than the overhead power lines and then the council has to continually prune them into weird shapes to prevent interference. e.g. https://www.abc.net.au/news/2022-10-18/pruning-street-trees-... A lesson on how not to do things. I'm all for appropriate trees though. reply agumonkey 9 hours agoparentprevMostly urban density ideas were incompatible with green space in the decades prior. The more you want to fit people to \"lower\" rent, the less you can allocate for grass or trees. reply wongarsu 8 hours agorootparentI don't know. When thinking about \"urban density in decades prior\" the first thing that comes to my mind are the \"commie blocks\" of the 50s-80s. The thinking of those often explicitly included green space: put people in denser higher buildings and you have space for green spaces in between. The execution wasn't always stellar, but it was in no way incompatible with trees. reply agumonkey 3 hours agorootparentThere were a lot of open spaces in the 60s and 70s, but over the years, \"every\" space has been converted into utility or new buildings. reply kjkjadksj 3 hours agorootparentI think you misunderstand the commie block. The entire greenspace is part of the property as well. See peter cooper village in nyc. reply agumonkey 1 hour agorootparentThings like Cooper village seems to be the exception where I live. reply jen729w 7 hours agoparentprevVisit Canberra! We have a remarkable amount of trees, it’s beautiful. https://www.nca.gov.au/education/canberras-history/charles-w... reply nxobject 6 hours agorootparentThe ACT is full of really thoughtful planning in general. A reminder that experiment and sanity coexisting sometimes prevails when planning in the very long term for development :) reply Jarmsy 9 hours agoparentprevI'd guess both. Rich places have the money and influence to improve their area, leading to increased property value, in a cycle. reply f6v 9 hours agoprevWhat I found incredibly uncomfortable when moving from Eastern to Western Europe is a lack of shade in residential areas. Yes, there’re parks, but the buildings often don’t have enough shade (anecdotal evidence). With the rising temperatures, many homes are exposed to sun whole day. I live in a city that’s been growing really fast and none of the newer residential houses have any trees around them. Mind you it takes many many years to grow a proper tree. reply mantas 9 hours agoparentDon't worry, newer developments in eastern europe suck at tree coverage too. reply earthnail 8 hours agorootparentNewer Western European developments tend to focus a lot on trees again in my experience. So there’s hope. reply nope1000 6 hours agorootparentYes, I live in a new apartment in Munich and when I look out of the window, I feel like I'm in nature. Really nice. reply mglz 9 hours agoprevLots of chopping trees seems to come from a laziness of thinking, where certain people in the city adminsitration are just used to the idea \"trees cause costs\". They never think any further about benefits beyond finances. reply stuaxo 9 hours agoparentOr, as in Sheffield in the UK misaligned incentives: the company that was paid to manage the trees realised it was cheaper for them to cut them down, so started labelling trees as diseased that weren't. reply pferde 8 hours agorootparentDoesn't that count as plain old fraud? reply pvaldes 8 hours agorootparentAnd steal of wood. One cubic meter of Cedrus is like 1000 euro in the market. Most owners don't realize that trees had value, and are often tricked to get rid of their \"diseased\" old cedars with a convenient \"free removal\" by part of the company that will chop it. If the owner can't be persuaded, a common trick deployed is some \"allergic\" woman calling authorities and requiring to chop that tree because it makes her uncomfortable. Then some good Samaritans ring the bell offering their services. Extra bonus if you can made the owner to pay for the operation. reply lucioperca 9 hours agorootparentprevPrivatisation gone wrong! reply wongarsu 8 hours agorootparentSomething the UK has a lot of experience with reply lopis 8 hours agoprev> However, planting trees in cities is pretty much all upside with almost no downside (except that birds tend to shit on my car). > So the next time you're enjoying a walk down a lovely shady street, take a look up and appreciate the trees. Author seems to be contradicting themselves (: reply blackhawkC17 8 hours agoparentI’ll trade a rare chance of bird poop to live in a pleasant leafy area :) A bird actually once pooped directly on my shirt, and it’s one of the funniest things I can remember. reply sriram_malhar 9 hours agoprevI think the department in charge of maintaining roads should also be put in charge of maintaining trees. Any time a road needs to be widened, make provisions for planting (or better) transplanting trees, including watering infrastructure. reply tetris11 9 hours agoprevI understand that tall trees a threat to capital and maybe also to human life when one falls over near a house, but do we really need to chop them down? Can we not add metal struts between a house and a tall tree, to reinforce its strength, rather than preemptively cut it every time? reply stuaxo 9 hours agoparentThe tree just needs to be managed, the whole thing doesn't need to be cut down. I'm in London on a tree lined street, every year they come and cut branches. If trees are rotten then they are cut (we have a lot of non-native trees). In this bit of London there's a target to plant much more trees, so any cut down are replaced with a native tree. Property developers are a different matter, I'm pretty sure the huge tree behind the flat I used to live in was illegally cut down; if the council had been aware it would probably have had a tree protection order put on it. reply silon42 9 hours agoparentprevSimpler/better to plant a new tree. reply pvaldes 8 hours agorootparentPlant a new tree and then pay air conditioner for 20 years until the tree grows again. Simpler yes; better not so much, if you take in mind all the hidden costs. Of course sometimes the tree is rotten or will fall over your home, and it must go before that happens. reply Hikikomori 9 hours agoparentprevToo much KSP? reply Joel_Mckay 9 hours agoparentprevIt is a $10k fine to cut down any tree over 6\" in diameter without a permit in our city. We have many cherry trees that form a pink-petal snow every year, that were planted after hostilities ceased with Japan in WWII: Accolade, Afterglow, Akebono, Ama-no-gawa, Atsumori, Autumnalis Rosea, Avium Plena, Beni-shidare, Birch Bark Cherry, Choshu-hizakura, Fudan-zakura, Gyoiko, Hosokawa-nioi, Ichihara-tora-no-o, Ichiyo, Ito-kukuri, Ito-zakura, Jo-nioi, Jugatsu-zakura, Kanzan, Kiku-shidare-zakura, Kiku-zakura, Korean Hill Cherry, Mikuruma-gaeshi, O-yama-zakura, Ojochin, Okame, Oshima-zakura, Pandora, Pink Perfection, Rancho, Sargent Hybrid Cherry, Schmitt Cherry, Sendai-shidare, Shiro-fugen, Shirotae, Shogetsu, Shosar, Shujaku, Snofozam, Snow Goose, Somei-yoshino, Spire, Star Cherry, Tai-haku, Takasago, Taki-nioi, Ukon, Umineko, Washi-no-o, Whitcomb, Yae-beni-shidare, Yama-zakura, Yokihi Fruit trees consume a lot of water, thus many cities should consider drought resistant options to reduce sun baked roads. Some don't like the trees due to superstitions, but most enjoy the reminder Spring has arrived. =3 reply short_sells_poo 9 hours agoparentprevUnfortunately the decision is often made based on costs, and chopping them down is cheaper than employing an arborist (?) to make sure the tree is reinforced properly. Please don't get me wrong, I agree with you. In many historic cities, there are trees that are hundreds of years old. There are trees that have been around since before the invention of the telegraph, lived through 2 world wars and are still going strong! There are efforts to keep these alive, but I feel newly planted trees won't get the chance to live that long :( reply lukan 8 hours agorootparent\"There are trees that have been around since before the invention of the telegraph\" There are trees, that are allmost 5000 years old, so around the time humans started building cities. https://en.m.wikipedia.org/wiki/List_of_oldest_trees The oldest tree in a city might be this one, 2300 years old, living history. https://en.m.wikipedia.org/wiki/Jaya_Sri_Maha_Bodhi Around here, they also plan to chop down a lot of trees that are around 200 years old, because the road repairs are cheaper with them gone. I hope it does not get to it as resistance is forming, but I might consider climbing and sitting there to block it. reply lippihom 5 hours agoprevWhen I tell people I live in Berlin, they often make a face and disparage it for being an \"ugly\" city - and to a certain extent I agree with them... if they've only visited the tourist heavy central areas (Alexanderplatz, Friedrichstrasse, and much of Mitte), where there is a notable lack of greenery. What I try to remind them of, is that there are more parks, gardens, and forest here than any other capital city, and that those areas will go toe-to-toe beauty wise with anywhere. Additionally, as temperatures rise (and there's a lack of indoor refuges due to AC being uncommon), finding respite among the greenery is one of the only foolproof ways to cut the heat here. reply arthurofbabylon 9 hours agoprevDoes anyone have a \"greenery index\" of world cities? reply grvbck 6 hours agoparentNot really a universal index, but interesting nevertheless: MIT Treepedia has indexed some cities based on tree canopies from Google street view. Some (to me) surprising finds, like Paris (8.8%) vs NY (13.5%). https://senseable.mit.edu/treepedia reply wodenokoto 9 hours agoprevI recently visited Tbilisi in Georgia which is also almost a city within a forest. It is very nice and something I would wish upon any city. However, it does take it toll on buildings and infrastructure. I wonder what Johannesburg spends per tree in not just cutting and leaf collection but also in addition road and pavement repairs and I’m sure there is more than one foundation that has been damaged by roots. reply openrisk 5 hours agoprevDense urban-type settlements attracted billions (and continue to do so worldwide) as they offer hard-to-beat advantages. But as these gains are booked and largely taken for granted, people long for the paradise lost, the walkability, the green spaces etc. Anecdotaly the pandemic may have hastened this realisation, as people resorted to walking in larger numbers. Walking around tree-lined streets with lots of natural looking grass and flower beds certainly beats doing the same in a car infested concrete jungle. reply z3phyr 8 hours agoprevWhen I lived in Delhi, my home was near the ridge area of North Campus. Its a lush forest with shrubs, ferns, and subtropical hard trees with a few lakes. The temperature difference, the coolness in air and the vibe difference from the rest of the city was like night and day. reply littlelady 7 hours agoparentI also know this feeling! I think if more people knew did, they would be more supportive of urban green spaces. It's always remarkable to ride my bike through a paved residential area and then cut through this green alleyway-- the temperature and air is so fresh and feels 5 degrees cooler than the street. reply robbyiq999 8 hours agoprevWould love to see them tear up highways and build forests and not the other way around reply littlelady 7 hours agoparentIt's not quite what you mean, but a woman in Ireland has planted over 1000 trees on her 3-acre plot, which was previously overrun with rushes and thought to be barren. Here are her before and after pictures, maybe it will inspire you too. https://bealtainecottage.com/before-permaculture/ reply langcss 7 hours agoprevCanberra is heavenly in terms of number of trees. Acres of parkland, farmland or wild bush near every part of the city. As well as treelined streets. reply notjustanymike 6 hours agoprevLiving in Brooklyn, I always prefer to explore east/west instead of north/south. This is because the streets have trees, while the avenues are exposed. In the hot summer months you’ll have a delightful walk down Bergen, then absolutely bake on Franklin or Nostrand. reply lordgrenville 8 hours agoprevI also grew up in Johannesburg, and really miss those trees. When you drive a little bit out of the city you see a completely different landscape. However I'm sceptical about the author's claim that there's no downside. I would assume that most of the trees are not native, and in any case definitely don't grow natively at that density. Feels like there's no free lunch in ecology: is mass tree-planting an exception? reply HermanMartinus 4 hours agoparentAuthor here :) You're correct that very few of them are native. That being said, when comparing to a city with no trees vs a city with non-native trees, I think the latter is still superior. reply hahamrfunnyguy 7 hours agoprevWe have the same issue in my city with respect to areas lacking trees. The trees in the more affluent areas tend to be better maintained by property owners, and replacement trees are planted when trees are lost in ice and wind storms. The city's forestry department is working on a long-term solution to this problem but it takes time to grow trees. reply b3ing 7 hours agoprevI just wish they wouldn’t plant acorn trees anywhere near a road, squirrels just end up in a kill zone reply 11235813213455 5 hours agoprevSimply remove cars, 40% more space reply nosrepa 7 hours agoprevEAB is definitely making sure we cut down more than ever! reply short_sells_poo 9 hours agoprevThe photo in the article looks amazing! You basically don't see a city, you see a vast woodland! An occasional rooftop peeking through here and there are the only signs of civilization. I'd love to live in a place like that. London is a very green city by many standards, but it is nowhere close to appearing like the unbroken forest in the article. In fact, the thing I miss the most after having moved to the UK are the forests. Most of it is private and fenced off, and it's just tiny patches of woods anyway. It's interesting how different it is compared to Switzerland for example, where one can roam freely (at risk of being chased by an occasional herd of curious cows) and there are plenty of forests where one can escape civilization. reply obscurette 9 hours agoparentI also happen to live in such area. It's nice, but there are also downsides – solar roofs for example don't make sense here. reply short_sells_poo 9 hours agorootparentThat's fair criticism, but perhaps it'd be much more effective to have a few but large solar farms outside the city, no? reply palata 7 hours agorootparentOr a nuclear plant. reply shiroiushi 9 hours agoparentprevHuh? I've read multiple times that in the UK, private land can't be \"fenced off\" and the general public has the right to roam on it, as long as they aren't causing a nuisance or getting too close to peoples' homes. Is that only for pastures or something? reply helsinkiandrew 9 hours agorootparent> and the general public has the right to roam on it There's no right to freely roam in the UK unlike Finland and other countries [1], although there are public footpaths/rights of way that cross private land [2] [1] https://en.wikipedia.org/wiki/Freedom_to_roam [2] https://www.gov.uk/right-of-way-open-access-land/use-public-... reply riknox 7 hours agorootparentThat applies outside of Scotland as far as I'm aware, whereas in Scotland there is a right to roam on all land as long as you're not disturbing housing/farm activities etc. reply seszett 9 hours agorootparentprevGenerally, only undeveloped land is covered by freedom to roam. Forests, cultivated land and gardens aren't necessarily freely accessible by law, if they are privately owned they can be fenced. There is much more expansive freedom to roam in Scotland, but then most of the land there is undeveloped anyway. reply globular-toast 8 hours agorootparentprevOnly in Scotland. reply pvaldes 8 hours agorootparentsheep dictatorship basically reply highhedgehog 8 hours agoprevPlease tell this to mediaval towns in Italy. reply floppiplopp 5 hours agoprevTrees in cities, just taking up valuable space, handing out shade and oxygen equally and for free, like them there communisms. Their leaves might turn red in autumn, but they are red inside all year, these woody marxist menaces! reply kkfx 4 hours agoprevCities have too much thermal mass exposed to the Sun, that's is. As a result they makes heat domes between buildings and you can't reduce much the thermal mass with trees because you still have all the buildings. A solution is new buildings \"coated\" with light air-gapped panels where air between them and the core structure of the building is free to slowly climb and going outside in the atmosphere. Of course you can't do so on glass facade walls. And it's hard to do on most existing buildings. Also you can't do much for the large asphalted area. Like it or not, cities and global heating are incompatible, as cities and many other aspects of the modernity. reply randomcarbloke 9 hours agoprevmy city is the largest urban forest. https://www.theguardian.com/commentisfree/2015/dec/07/london... reply andsoitis 8 hours agoparentJohannesburg is the largest man-made urban forest and Tijuca forest in Rio is the largest urban forest. https://en.wikipedia.org/wiki/Urban_forest reply randomcarbloke 8 hours agorootparentOkay, by some people's argument it is and since this is not an exact science comparing Tijuca's 3,953 hectares to London's 14,000 even with percentage coverage it would suggest otherwise. That said lots of cities have a claim: Oslo, Joberg(as you indicate), some places in China. Is it most trees, densest coverage, or simply largest area that still satisfies the definition of Urban Forest? You can pick a different answer for whichever metric qualifies. reply classified 9 hours agoprev [–] Trees are nothing but obstacles to traffic. If you take a good look at cities, you will see that they were built for cars, and cars alone. Anything that's not a car has no place in a city. reply oblio 9 hours agoparentEven for the US, I think the worst part is that they're not an obstacle to traffic, but they're an extra expense plus they block those huge ads from roadside businesses. Americans are making many of their roads and towns/cities hell just to penny pinch. reply lopis 8 hours agoparentprevEven if that was true, sounds like a net gain if trees make cities more inhospitable for cars. reply classified 6 hours agorootparentBut then, where would all those poor cars live? reply f6v 9 hours agoparentprevNot in Europe though. reply ffgjgf1 9 hours agorootparentDepends, people keep saying stuff about “Europe” as if it’s some single monolithic place. In any case very few people live in old town houses built prior to the late 19th century (in those places have their own issues) and there was plenty of car centric development during the 1950s and subsequent decades. reply morsch 9 hours agorootparentprevMostly better, still awful. reply self_awareness 9 hours agoparentprev [–] > Anything that's not a car has no place in a city. Erm, I thought cities are for people, not for cars reply nottorp 9 hours agorootparent [–] Erm, I think HN could use a \"+1, well done sarcasm\" upvote. reply Mordisquitos 8 hours agorootparent [–] HN commenters should definitely use the '/s' tag, as it is essential to correctly label the intention of sarcastic comments. /s reply lopis 8 hours agorootparent [–] Well, this isn't Reddit, so many we could have less sarcasm? Sarcasm isn't accessible even with text tags and doesn't help move the discussion forward. reply nottorp 6 hours agorootparent [–] Yes, HN is as hostile to humour as StackOverflow is to 'don't give ready to paste code, teach the man how to fish instead' answers. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Johannesburg, once barren, transformed into the \"greenest city in the world\" after planting millions of trees to combat dust from gold extraction.",
      "Tree planting in Johannesburg was unevenly distributed due to Apartheid, highlighting socioeconomic disparities.",
      "Urban trees provide significant benefits, including reducing the \"heat island\" effect, acting as sound barriers, enhancing aesthetics, increasing biodiversity, and encouraging outdoor activities."
    ],
    "commentSummary": [
      "Cities are increasingly planting trees and promoting green roofs to combat heat stress and improve urban livability.",
      "Utrecht, Netherlands, and Zurich, Switzerland, are leading examples, while U.S. cities like Portland, OR, have green mandates, and Salt Lake City is exploring xeriscaping.",
      "Trees offer significant benefits, such as cooling urban areas, improving air quality, and enhancing overall livability, despite challenges like private property development leading to tree removal."
    ],
    "points": 273,
    "commentCount": 139,
    "retryCount": 0,
    "time": 1719822718
  },
  {
    "id": 40845951,
    "title": "Welcome to Ladybird",
    "originLink": "https://ladybird.org/",
    "originBody": "About News Get Involved Sponsors Donate Welcome to Ladybird, a truly independent web browser. We are building a brand-new browser from scratch, backed by a non-profit. Get Involved About Ladybird Ladybird is a brand-new browser & web engine. Driven by a web standards first approach, Ladybird aims to render the modern web with good performance, stability and security. From its humble beginnings as an HTML viewer for the SerenityOS hobby operating system project, Ladybird has since grown into a cross-platform browser supporting Linux, macOS, and other Unix-like systems. Ladybird is currently in heavy development. We are targeting a first Alpha release for early adopters in 2026. What makes Ladybird unique Truly independent No code from other browsers. We're building a new engine, based on web standards. Singular focus We are focused on one thing: the web browser. No monetization No \"default search deals\", crypto tokens, or other forms of user monetization, ever. News & Announcements Announcement The Ladybird Browser Initiative Announcing our 501(c)(3) non-profit. Thoughts Why we need Ladybird Co-founder Chris Wanstrath shares his thoughts. Announcement Ladybird forks from SerenityOS Read the full announcement. Get Involved Ladybird is currently in heavy development, and there's work to be done in all areas of the browser. We're welcoming new developers every week. The main community hub is our Discord server. All the code is hosted on GitHub. Clone it, build it, and join our Discord if you want to collaborate on it! We're looking forward to seeing you there. Join Discord Get the code Sponsors Platinum Silver Bronze Become a Ladybird supporter Ladybird is funded entirely by sponsorships and donations from people and companies who care about the open web. We accept one-time and recurring monthly donations via Donorbox. If you or your company would like to make a large donation, we would be happy to display your logo on this website! Please contact us about becoming a sponsor. Frequently Asked Questions ? When is it coming? We are targeting Summer 2026 for a first Alpha version on Linux and macOS. This will be aimed at developers and early adopters. ? How many people are working on the browser today? We currently have 4 paid full-time engineers working on Ladybird. There is also a large community of volunteer contributors. ? What's the hiring plan? We have 3 new full-time engineers starting soon. Going forward, we would like to grow the team at a reasonable pace. Building the right team is more important than building it quickly. ? What does \"No code from other browsers\" really mean? The focus of the Ladybird project is to build a new browser engine from the ground up. We don't use code from Blink, WebKit, Gecko, or any other browser engine. For historical reasons, the browser uses various libraries from the SerenityOS project, which has a strong culture of writing everything from scratch. Now that Ladybird has forked from SerenityOS, it is no longer bound by this culture, and we will be making use of 3rd party libraries for common functionality (e.g image/audio/video formats, encryption, graphics, etc.) We are already using some of the same 3rd party libraries that other browsers use, but we will never adopt another browser engine instead of building our own. ? Will Ladybird work on Windows? We don't have anyone actively working on Windows support, and there are considerable changes required to make it work well outside a Unix-like environment. We would like to do Windows eventually, but it's not a priority at the moment. ? Will Ladybird work on mobile devices? We don't have anyone actively working on an Android or iOS port. More effort will be put into mobile once we have the desktop versions in a good state. While there is the start of an Android port in the project repository, mobile is not a priority at the moment. ? What are the sponsor tiers? Platinum USD $100,000 Gold USD $50,000 Silver USD $10,000 Bronze USD $5,000 Sponsors will have their logos displayed on our website, and will be thanked in updates / on social media. Please contact us if you are interested in sponsorship. ? How can you be \"independent\" if you have sponsors? All sponsorships are in the form of unrestricted donations. Board seats and other forms of influence are not for sale. ? Why build a new browser in C++ when safer and more modern languages are available? Ladybird started as a component of the SerenityOS hobby project, which only allows C++. The choice of language was not so much a technical decision, but more one of personal convenience. Andreas was most comfortable with C++ when creating SerenityOS, and now we have almost half a million lines of modern C++ to maintain. However, now that Ladybird has forked and become its own independent project, all constraints previously imposed by SerenityOS are no longer in effect. We are actively evaluating a number of alternatives and will be adding a mature successor language to the project in the near future. This process is already quite far along, and prototypes exist in multiple languages. Sign up for our newsletter We'll e-mail you once a month with updates about Ladybird. About News Get Involved Sponsors Contact Us © 2024. All rights reserved.",
    "commentLink": "https://news.ycombinator.com/item?id=40845951",
    "commentBody": "Welcome to Ladybird (ladybird.org)246 points by skilled 4 hours agohidepastfavorite62 comments jwells89 3 hours agoThis is great to see either way, but does anybody know if Ladybird's engine is being designed with embedded use in mind? One of the big missteps that led to Blink/Chromium and to a lesser extent WebKit overtaking Gecko was Mozilla's choice to remove embedding support from Gecko and effectively make Firefox the only modern Gecko browser. It'd be great to get a new browser but it'd be even better to get a new web engine to embed that's not the hulking behemoth that is Blink. reply catapart 3 hours agoparentI've been waiting for an embedable, rust-based, browser library broken down into modules like \"rendering\", \"scripting\", \"network\", \"encoding\", etc, for over a decade now. At certain points, it felt like a foregone conclusions. These days, it feels like no one else even wants something like that (or at least, no one is willing to work on it in the open). All I want to do is write apps that have native performance using the world's most popular document format and styling language, and so far the only response that even remotely looks like interest in that from the OS designers has been \"use a webview\" because they apparently, inexplicably understand the first clause of this sentence as 'I would like to embed a full browser stack to accommodate visuals'. reply fabrice_d 2 hours agorootparentCheck what is happening in Servo (https://servo.org). Some active members also want something very modular, and this is helped by work being actively done to use the engine in various contexts. reply lpribis 2 hours agoparentprevAs I understand, Ladybird was originally just the embedded HTML renderer for SerenityOS known as LibWeb. \"Ladybird\" is just the cross platform UI skin for an embedded LibWeb. reply quux 4 hours agoprevVideo announcement by Chris Wanstrath (GitHub co-founder) of the 501(c) non-profit and $1,000,000 donation: https://www.youtube.com/watch?v=k9edTqPMX_k reply vimsee 3 hours agoparentRecent events really have affected how I look at the Ladybird browser. A year ago (or some) before Andreas Kling announced the large sponsorships this all felt like a hobby project with Andreas himself even admitting that he would not expect too much of it. However, now they are actually trying to build a web-browser that will handle modern websites and web-apps. I am truly impressed by the community and also very stoked to see how this will develop going into the next two years. reply aestetix 3 hours agoparentprevThe Loompanics Principia Discordia on the shelf. Hail Eris! reply logicprog 3 hours agoprevThis is really awesome. I deeply admire what they're doing and hope it works out. What I'd really love to see with this is some kind of monetization model outside of just relying on sponsors to a non-profit. Although I'm not a huge fan of their specific license, I think maybe the FUTO model might be something to look at: The user is fully free to inspect, modify, and redistribute copies of or modifications of the source code of the application, and the full features of the application are available at all times to all users, but there is a way to \"pay for the application if you like it\" built into the interface that is easy to access and convenient to use, that gives people some kind of \"lifetime license\" that just adds a rewarding cosmetic thing to their account or something. Maybe with a one-time notification reminding the user to pay after a certain amount of time using the application that can be permanently dismissed with a button that's part of the notification. reply roughly 3 hours agoparentAgree with this. Similar to something like Frame.work, this kind of product will never have universal interest and is unlikely to take more than a couple percent of the market, but that’s Fine. Pick your audience, build them the product they want, and charge them money for it. I’ll happily pay for an open-source standards compliant browser with no conflict about who it’s building for. reply logicprog 2 hours agorootparentYes, I think that's at least one thing that the FUTO model shows, which is that there is actually a substantial proportion of people willing to pay for goods software that respects them as users. Whether that's enough to break even with development costs is another question though, but I certainly hope that would be the case. reply fabrice_d 3 hours agoparentprevThe real FUTO model is that a billionaire is funding the dev though... They are likely not making bank with the \"pay what you want\" model. Elementary OS also has a \"pay what you want\" option, but I could not find how much this is bringing them compared to their other funding sources. reply logicprog 2 hours agorootparent> The real FUTO model is that a billionaire is funding the dev though... They are likely not making bank with the \"pay what you want\" model. That's fair, but Ladybird can emulate that through the sponsors of its non-profit foundation. So it can be partially funded through shareware-like stuff and mostly funded through sponsors keeping it funded. Maybe I'm just crazy here, though. I really don't know what would make a viable funding model. A funding model that I really think would be great and I wish people would use more is only providing source code and the tools and documentation needed to modify it and build it yourself for free and charging money for access to pre-built or packaged binaries. I judt don't tend to go around suggesting it because I feel like a lot of people would be pretty resentful of this with our current culture around software, even though I think it's probably the best model in our ideal world. reply imiric 2 hours agoparentprevI think that cryptocurrencies get an undeserved level of hate, and that it would be perfect for monetizing a project like a web browser. Yes, there are scam coins, but there are also legitimate use cases for it. IMO the Basic Attention Token used by Brave is a good way to do it. Users have the option to earn BAT by watching \"privacy-friendly\" ads, or exchange fiat into BAT and skip seeing ads. They can then use the funds to support websites they visit, or the browser developer. I'm not a Brave user, but this always seemed to me like the best way to transition away from the intrusive and hostile ad-based business models, and into one that eliminates the middle-man and allows users to support content creators directly. reply blitzar 2 hours agorootparent> perfect for monetizing a project like a web browser They could also try regular old fashioned money for monetization. It has worked for ~3,000 years pretty well and you can buy things like food and shelter with it in pretty much every country on earth. reply deckiedan 52 minutes agorootparentAlso rich patrons supporting artists was how many of the greatest artworks of many civilisations were commissioned for thousands of years... Is writing software closer to growing potatoes or designing the Sagrada de Familia, or writing the Clarinet Concerto? I'd argue that writing a web browser is a lot closer in scope to writing a symphony than building a house - at least in audience and durability. In a house's life time, maybe 100 people live in it, and perhaps 2000 people visit it, but a browser, or a symphony, will have an audience of millions. The market is massively smaller. The impact massively larger. reply imiric 1 hour agorootparentprevAssuming you mean the electronic form of fiat, it's not a good fit for microtransactions because of the fee requirements. A digital currency can be used for much smaller and frequent transactions. Not Bitcoin and most major ones, but there are currencies optimized for real-time transactions with minimal fees, so it is possible. It's a shame that the cryptocurrency stigma doesn't allow legitimate uses of the technology to enable novel business models and user experiences. But keep downvoting me because you disagree. :) reply ksec 3 hours agoprevThis is exciting. Reminded me of the launch of Firefox and the New York Times ad with all the names from those of us who donated at the time. I wonder if Ladybird could do the same thing again. I also cant believe 20+ years later we have to fight another Browser war. reply hitekker 4 hours agoprevThis new homepage/logo lost the late 90s, early 00s charm that originally attracted me to Serenity. Instead, it looks looks not too different from hyped up, short-lived products that I've learned to avoid. On a side note, I wonder if the constraints they've lost when decoupling from Serenity may impinge direction and, by definition, velocity. 2026 is a long way away for an Alpha. reply circl_lastname 3 hours agoparentLadybird is for the modern web, not so the late 90s, early 00s anymore. I don't see a worry of integrating domain-specific third party libraries, we were able to drop thousands of lines of non-Web code that people would otherwise have to maintain. I'm guessing 2026 was chosen as people would be quickly discouraged by its slow rendering performance right now. The average person would just see it as just a slower browser rather than a serious contender. reply thiht 2 hours agoparentprev> the late 90s, early 00s charm Aka outdated, clueless, and a little bit sad. Making a cleaner design like they did doesn’t cost a lot. Don’t let nostalgia turn you into the \"it was better before\" crowd. reply logicprog 3 hours agoparentprevIf they had kept the late 90s or early 2000s aesthetic, then most people would've assumed that it was sti one of those hobby projects more focused on a nostalgic (but not necessarily better in any actual way) technological aesthetic that certain tech people get really attached to, and not something that was meant for serious use and to be pleasant to use in the modern decade for people who prefer modern interface paradigms. Having a clean and well-designed website that's nice to look at and doesn't feel horribly outdated is a good move in my opinion — to me, it signaled that the project really has shifted its mentality and goals in a substantial way and is now looking towards the future instead of the past. As soon as I saw the new website design, I got excited because it confirmed for me that their mentality has changed and that this could be something that could really go somewhere. Maybe you've learned to associate a nicely designed website that's pleasant to look at with short lived projects that are worth avoiding, but I think that's more your problem: Maybe you should realize that most new things don't last very long and so of course websites that are designed in a new and modern way will often be for projects that don't live very long simply because new projects are more likely to use a modern website design, and they are also likely for unrelated reasons to only live a short time. And that these exact same pressures and considerations would have applied back in the 2000s or 1990s, such that back in those days oldheads might have looked at something with a 2000s or 1990s design and thought that was clear evidence that it wouldn't last very long, because most new projects made back then would have been made with contemporary design styles, and would have also not lasted very long simply because they were new, same as in the modern day. And that your association of long-lived products with 1990s and 2000s design aesthetics might simply be survivor bias: most long-lived software projects today 1st started in the 90s or early 2000s, at the very least, because that's just the first set of decades that are long enough ago for projects that were started then and still survived today to be considered long-lived. And so, of course, most long-lived projects you are familiar with in the current decade will have aesthetics from decades ago, because that's simply how longevity works. And so you've learned to associate longevity with an old aesthetic, but there's nothing really about the aesthetic that talks about or portrays a mentality that makes projects live longer. It's simply that longer-lived projects will be from an older era of aesthetics. But there were plenty of projects that were designed with 1990s and 2000s aesthetic sensibilities (and whatever magical philosophy you attribute underneath those) that didn't survive long at all, just the same as projects that were designed with modern sensibilities that don't last long, you're just not aware of them because they didn't last and so they didn't show up in your sample. Edit: It occurs to me that this comment probably comes off as overly hostile and confrontational for no real reason, so I do want to apologize for that. I'm in a very grumpy mood today. I think my reasoning is correct, but I do want to clarify that you're totally allowed to prefer 1990s and 2000s aesthetics. Personally, I like both modern and 1990s aesthetics (in websites specifically) roughly equally and for their own unique reasons. ,the reason I made this comment is just that I find the cargo culting of ancient technical aesthetics[1], as if they are not only indicators of good software in and of themselves, but also of some moral purity and improved moral fibre on the part of those who use them, very tiring. There are many things wrong with our modern technical aesthetics to be sure, but I think there is an almost equal amount wrong with the sort of older technical aesthetics that people tend to worship, like the Unix philosophy and its associated tools and development environment, and things like that. They are just different kinds of wrong. [1]: https://prog21.dadgum.com/74.html reply chad1n 3 hours agoprevThe old logo looked cute, the current one looks like a meta copycat. I wish they went for something with more personality, not this type of corporate logo. reply JBits 3 hours agoprevPersonally, I think it would've been better to keep Serenity OS support but drop the no 3rd party code rule. While I realise it would've gone against Serenity OS's write everything philosophy, it would've kept the browser as a rallying focus for the OS. Thinking about what actually happened, if the Serenity OS now treats Ladybird as a port, then I think the result will be the same minus the social effect. reply doruk101 4 hours agoprev> Singular focus > We are focused on one thing: the web browser. It is great that this was put first reply superidiot1932 4 hours agoparentGreat jab at the Motzilla foundation that focuses on \"Pocket\", \"Mozilla VPN\", millions of dollars in CEO compensations, AI investments, political deplatforming and other things (maybe Firefox). reply mariusor 3 hours agorootparentI think you're projecting. What it means is that now the browser is Andreas' main focus after he announced earlier last month that he will step down from Serenity OS's team. reply jdpedrie 2 hours agorootparent\"No 'default search deals'\" doesn't sound like projecting, it sounds like a direct shot. reply fabrice_d 3 hours agorootparentprevAlso, it is easy to make bold claims early on, and at some point reality shows its ugly face. Remember that they already moved from \"we re-implement everything\" to \"let's re-use the same libraries that other web runtimes use (and sometimes maintain)\". reply squidbeak 3 hours agoprevI'm glad to see this and hope it succeeds. A proper challenge to Chrome's dominance is long overdue, and though I love and appreciate Firefox, it doesn't look as if it will come from that quarter under the current management's drift. reply AnonC 2 hours agoprev> Ladybird is currently in heavy development. We are targeting a first Alpha release for early adopters in 2026. It kinda sucks that it takes this long for an alpha release (I know that browsers are extremely complex). On the other hand, it also seems like this is quite quick for an alpha release of a browser that does not use code from any other browser before it. I can’t wait for 2026. In a way, this reminds me of the early days of Phoenix (which is now Firefox). reply MaximilianEmel 4 hours agoprevAndreas Kling has been making videos about its development for some time now: https://www.youtube.com/@awesomekling reply stefanos82 3 hours agoprevFinally! Congrats to Andreas and the dev team that made it happen! I hope a portion of the funds to go in Jakt's [1] development as it's a very promising language that looks both clean and elegant AND it's a memory-safe systems programming language. [1] https://github.com/SerenityOS/jakt/ reply hypeatei 3 hours agoparentI highly doubt it, Jakt is a hobby project that is basically dead. Writing a web browser is enough work already but also building your own language on top of that seems... costly. Using proven and widely used technologies in a very hostile environment seems like a better choice than rolling your own language which will need a ton of scrutiny to be considered serious. reply pbronez 3 hours agoparentprevLooks like they're evaluating several alternatives to C++ but haven't chosen one yet: \"We are actively evaluating a number of alternatives [to C++] and will be adding a mature successor language to the project in the near future. This process is already quite far along, and prototypes exist in multiple languages.\" reply nalinidash 3 hours agoprevThis is the last working archive of ladybird old website: https://web.archive.org/web/20240630172605/https://ladybird.... reply ChrisArchitect 2 hours agoprevAnnoucement post: https://ladybird.org/announcement.html (https://news.ycombinator.com/item?id=40845954) reply LightBug1 2 hours agoprev\"Backed by non-profit\" .... shudders I'm being unfair, but am I? Have been stung a few too many times by those words reply toomuchtodo 2 hours agoparentEffective governance is a never ending need, but non profits are arguably the best entity type for such efforts. It also allows for tax efficiency for the spend involved. reply warpech 3 hours agoprevAmazing news! It would be great to read how did it come to attract such great sponsorships. Was that actively searched for by Andreas? reply FlyingSnake 2 hours agoprevI have never built a browser so pardon my ignorance, but is the Ladybird browser team blazing a new trail in browser tech instead of following the same old path of KHTML/WebKit/Gecko? The constraints of 1990s are no longer there in 2024, and many new novel breakthroughs are available that might not have existed back then. reply MaximilianEmel 4 hours agoprevI miss the old, cute logo :( reply xnx 4 hours agoparentCurrent logo seems very inline with the current style used by Meta and Apple Intelligence. reply chubot 2 hours agorootparentNormally I'm puzzled when people say \"this word reminds me of this other bad word\" or \"this looks like that\" (my brain doesn't really work like that), but in this case, I have to say that it does have the same feeling. Designing our new company brand: Meta - https://design.facebook.com/stories/designing-our-new-compan... Apple AI logo is intended to look unthreatening, and non-anthropomorphic - https://9to5mac.com/2024/06/17/apple-ai-logo/ I hate to be a peanut gallery person, but I really hope they reconsider the logo! The new logo doesn't connote \"ladybird\" or \"ladybug\" at all ... it seems like there's obvious room to make it more distinct with that kind of association reply circl_lastname 4 hours agoparentprevIt was AI-generated reply runjake 3 hours agoprev> Question: When is it coming? > > We are targeting Summer 2026 for a first Alpha version on Linux and macOS. > This will be aimed at developers and early adopters. From watching awesomekling's livecoding videos, this seems rather far out on the timelines, but godspeed. We need something outside the scope of the current browser models. reply whamlastxmas 55 minutes agoprevWill ladybird be fundamentally different enough that my browser can’t be fingerprinted? reply nashashmi 2 hours agoprevSingle file packaged web browser will be just what the internet needs. Like QTWeb which is no longer supported. reply shmerl 3 hours agoprevI personally wait for Servo to mature and power a decent browser. Especially with Vulkan renderer. reply sampullman 3 hours agoparentI'm not sure it will ever mature, since it was dropped by Mozilla it seems like there are only a few people working on it in their spare time, though I could be mistaken. Ladybird seems to be gaining momentum, so I'd guess it will be generally usable sooner than Servo. reply shmerl 1 hour agorootparentServo started earlier and besides, Rust looks more promising than another C++ engine, however long it takes. reply gamblor956 1 hour agoprevSounds interesting, but the lack of planned Windows or mobile support makes this DOA. There are already Linux browsers with their own independent engines (for example, Konqueror uses KHTML), so what about this makes it special beyond having a Daddy Warbucks to fund its development? reply hofo 1 hour agoprevI’m confused. One paragraph says it’s grownn into a cross platform browser. Next that it is in heavy development with an alpha release in about a year and a half from now reply ohmyiv 22 minutes agoparentThose aren't conflicting statements. It started off as a project made solely for SerenityOS (as in no relation to other OSes). Now they switched and have been working on Linux/MacOS/Unix versions. The development thing is that they're not ready yet for any release. reply riskable 4 hours agoprev [–] I was very excited about this but I must say I lost a lot of that excitement when I found out it's written in a non-memory-safe language (C++). Browsers are one of the few things we use every day that really need to be as safe and secure as possible so it was a huge disappointment. The past 20 years of seemingly endless browser vulnerabilities due to memory management issues has left me... Skeptical. We're still seeing vulnerabilities based on memory management bugs in Chrome and the non-Rust parts of Firefox! The sooner we stop making stuff like browsers in unsafe languages the better. reply quux 4 hours agoparentAndreas has looked deeply into moving both SerenityOS and Ladybird to a successor language but at the time didn't find anything out there that met their requirements so he started a new language named Jakt which unfortunately never got enough traction to move to. One of the issues preventing them from adopting a language like Rust or Swift was the large number of 3rd party dependencies they would bring with them, this went against the \"we build everything from scratch\" ethos of SerenityOS. Now that Ladybird is forked from Serenity I hope they can revisit this decision. It wouldn't surprise me if they adopt Swift. Andreas really liked it and in the time since it was last evaluated Swift's C++ interop has improved a lot. reply MaximilianEmel 4 hours agoparentprev> However, now that Ladybird has forked and become its own independent project, all constraints previously imposed by SerenityOS are no longer in effect. We are actively evaluating a number of alternatives and will be adding a mature successor language to the project in the near future. This process is already quite far along, and prototypes exist in multiple languages. reply lufte 3 hours agoparentprevIsn't it possible to write memory-safe code in modern C++? reply indrora 2 hours agorootparentVery. In fact it’s becoming harder to write non-safe c++. Stuff like nullptr has been in the spec for a while, unique_pointer and other stuff has worked its way in too. And Rust has plenty of ways to turn memory into Swiss cheese. Anyone who does not actively acknowledge unsafe and the fact you can embed C into rust is delusional. reply raphyjake 1 hour agorootparent> It's becoming harder to write non-safe c++. It's becoming harder if you understand those features and use them correctly, on top of your knowledge of unsafe C++. Even then those are opt in, and it still takes a tremendous effort. I don't trust anyone using C++ correctly. It's just hubris at this point. You can use unsafe and call C but that's a smaller surface area to audit. Most big companies are transitioning to Rust for a reason. Some big companies were trying to replace C++ even before Rust was a thing. Why make a C++ browser now? What's the point? reply oldpersonintx 4 hours agoparentprevpart of the reason Ladybird split from SerenityOS was to break the \"no dependencies\" requirement...Ladybird will now be integrating with best-of-breed third party libraries. These will be primarily C/C++ authored I assume, so you are getting C/C++ no matter what. reply zogrodea 4 hours agoparentprev [3 more] [flagged] whs 4 hours agorootparentIn Rust most function take a reference to its argument, which means the ownership is shared. However, you can also pass non-reference to it which indicate that you wish to transfer ownership to the function (of course the caller must be the previous owner). Once the function take ownership there's no way to take it out unless they returned it, so it achieve the effect you're looking for. I heard some HTTP library used that to expose a state machine, so functions that cause HTTP header to be sent will take ownership and return a new value which no longer expose HTTP headers setter methods. For freeing memory or resources, the recommended way to do it is in RAII-style, with custom Drop implementation that will get called when the ownership is dropped instead of explicit close() or free() reply steveklabnik 4 hours agorootparentprev [–] I am not sure I really agree with your first paragraph, but the function you’re talking about is https://doc.rust-lang.org/std/mem/fn.drop.html I am also not sure what that has to do with anything, to be honest with you, but I’d be happy to elaborate if you will! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Ladybird is an independent web browser developed by a non-profit, focusing on performance, stability, and security, with an Alpha release planned for 2026.",
      "Initially an HTML viewer for SerenityOS, it now supports Linux, macOS, and other Unix-like systems, and is built entirely from scratch without using code from other browsers.",
      "The project is funded by sponsorships and donations, with no ads or user monetization, and is currently developed by a team of four full-time engineers."
    ],
    "commentSummary": [
      "Ladybird, initially an HTML renderer for SerenityOS, is now evolving into a cross-platform browser project.",
      "The project has received a notable $1,000,000 donation from GitHub co-founder Chris Wanstrath, indicating strong financial backing.",
      "The team plans to release an alpha version by 2026, emphasizing modularity and adherence to modern web standards."
    ],
    "points": 246,
    "commentCount": 62,
    "retryCount": 0,
    "time": 1719842693
  },
  {
    "id": 40839418,
    "title": "Postzegelcode",
    "originLink": "https://en.wikipedia.org/wiki/Postzegelcode",
    "originBody": "Toggle the table of contents Postzegelcode 1 language Nederlands Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Special pages Permanent link Page information Cite this page Get shortened URL Download QR code Wikidata item Print/export Download as PDF Printable version Appearance move to sidebar hide From Wikipedia, the free encyclopedia Example of a postzegelcode A postzegelcode is a hand-written method of franking in the Netherlands. It consists of a code containing nine numbers and letters that customers can purchase online from PostNL and write directly on their piece of mail within five days as proof of payment in place of a postage stamp.[1] For mail within the Netherlands, the nine letters and numbers are written in a three-by-three grid. For international mail there is a fourth additional row that contains P, N, L. The system was started in 2013.[2] Initially the postzegelcode was more expensive than a stamp because additional handling systems were required. Then for a while[when?] the postzegelcode was cheaper. Eventually the rates were set to the same price.[citation needed] In December 2020, 590,000 people sent cards with postzegelcodes.[3] Safety[edit] Since the codes are valid for only five days, the chance that someone would guess a recently purchased code is quite low. Assuming 26 letters and 9 digits (the zero is not used to avoid confusion with the letter O), there are 359 (78.8 trillion) possibilities. Even if a postzegelcode were used for all mail items in the Netherlands, the probability is about 1 in 2 million that any stamp code has been sold in the past five days.[citation needed] References[edit] ^ \"PostNL - Postzegelcode\". ^ \"Postzegel niet meer nodig met app\". nos.nl (in Dutch). 2013-05-23. Retrieved 2023-08-29. ^ \"Gebruik postzegelcodes PostNL bijna verdrievoudigd\". RTL Nieuws (in Dutch). 2020-12-29. Retrieved 2023-08-29. External links[edit] PostNL, postzegelcode Retrieved from \"https://en.wikipedia.org/w/index.php?title=Postzegelcode&oldid=1231977167\" Categories: Postal systems Postal markings Hidden categories: CS1 Dutch-language sources (nl) Articles with short description Short description matches Wikidata All articles with vague or ambiguous time Vague or ambiguous time from July 2024 All articles with unsourced statements Articles with unsourced statements from July 2024",
    "commentLink": "https://news.ycombinator.com/item?id=40839418",
    "commentBody": "Postzegelcode (wikipedia.org)228 points by tdeck 23 hours agohidepastfavorite95 comments gglnx 22 hours agoGermany has this too: #PORTO and the code. https://www.deutschepost.de/de/m/mobile-briefmarke.html (and valid for 3 years after you bought it) reply Abimelex 12 hours agoparentIt used to be also a squared code consisting of 12 digits, you could receive via SMS to 22122, but this method is deprecated or already even terminated. Has anybody tested it recently? https://www.giga.de/tipp/briefmarke-per-sms-an-22122-erhalte... reply lucb1e 20 hours agoparentprevIt is only purchasable if you install their software, can't just buy such a stamp on the website if I'm reading the page correctly? I seem to remember Deutsche Post also had a thing you could just write on the letter, or maybe it was DHL's parcel service instead. Those were purchased simply online like any other product/service and, after paying, it told you what to write in the corner. Or maybe I misremember and they only have this system where you can take the pdf to a parcel shop and have them print your label. Edit: found the \"print in store\" option, it's not for letters but for parcels https://www.dhl.de/de/privatkunden/pakete-versenden/angebot-... \"Don't have a printer? Let your stamp be printed at [places].\" reply cedilla 20 hours agorootparentThere used to be a premium number you could send an SMS to. It was more expensive than a normal stamp to cover for the expensive billing. \"Handyporto\" replaced this and that's probably also why you can't buy it on the website. Deutsche Post probably can't conceive of people who don't own a printer so they heavily advertise \"Internetmarke\" which requires one. reply illiac786 12 hours agorootparentActually the one you write yourself (#PORTO) and the one to print (InternetMarke) are both equally prominent in the app. reply ahartmetz 21 hours agoparentprevCool, I didn't know that. I still have a few self-printed QR code-type stamps. #PORTO is a nice option. reply kseistrup 11 hours agoparentprevDenmark has had this for years. The code is 3 lines, each with 4 alfadigits. The code is valid for 180 days. reply intothemild 21 hours agoparentprevSame here in Norway. reply skrebbel 23 hours agoprevIt works real well for people like me who don’t send physical mail often. Just pay online and write the code onto an envelope with a pen. I love the simplicity. reply berkes 22 hours agoparentFor me too. I used to have a sheet of stamps lying around. Often only one was used. Then those would \"expire\" after years, in the sense that (yearly?) tariff-increasments would invalidate the \"now too cheap\" ones. I then had to buy additional special ones to compensate for the price. Digital is the way forward and this is a perfect example of simple digital to analog. It works fantastic. reply consp 22 hours agorootparentThe old stamps with a monetary value you would have to \"add\" value by adding a stamp if the cost increased (and people were adding 5ct coins as a result). The new ones (1/2 numbered) do not expire and keep their relative value. [1] 1. https://www.postnl.nl/klantenservice/tarieven-postzegels/gel... reply sigio 6 hours agorootparentWith the massive price-hike on stamps, buying a 50/100 roll of these many many years ago, this got a great return on investment ;) Though I'll still have enough stamps to last until I die ;) reply ProllyInfamous 4 hours agorootparentI never thought those ten $50 rolls of 100 first class (USA) stamps would ever pay for themselves... but last I checked FCM stamps are now 36% more expensive. Still have five rolls, which is: >enough stamps to last until I die ;) BUT the hand-written OP stamps are a great idea! reply berkes 12 hours agorootparentprevI was indeed talking about these old ones. reply Sharlin 22 hours agorootparentprevGenerally there are also non-denominated \"forever\" stamps available whose real value will remain the same despite changing rates. reply saghm 22 hours agorootparentI assume you're talking about the US? I thought that nowadays all US postal stamps are \"forever\" ones as of several years ago, but maybe it's just the post offices around where I've lived that don't bother selling the \"temporary\" ones. reply Sharlin 22 hours agorootparentI’m talking worldwide (I only have first-hand experience about Finland, but they seem common according to Wikipedia) reply signal11 8 hours agorootparentprevThe most common UK First- and second-class stamps (the ones without a price on them, just a \"1st\" or \"2nd\") don't expire. Of course you don't need a stamp, you can purchase postage online. reply OJFord 7 hours agorootparentThe monetary ones (1p/2p/50p et al.) don't expire either, you just need to meet the cost at the time you use them. (Exactly like cash in that sense: loses value with inflation, but you can still use what you have, just for less.) reply ohmyiv 21 hours agorootparentprev> maybe it's just the post offices around where I've lived that don't bother selling the \"temporary\" ones. Are we talking about US? If so, it might be your office. The one near me tries to sell me all kinds when I actually go in to do stuff. USPS still sells varying rate stamps ranging 1 cent additional postage to $30 priority mail express stamps. reply theginger 22 hours agoparentprevOk now I get it. Somehow despite all the detail the Wikipedia article doesn't seem to capture the point of it. This does lead me to wonder what happens when something gets held up and it's an older code (sir) but it checks out. reply timvdalen 21 hours agorootparentYou (if you wrote a return address) or the receiver (if you didn't) get a bill that you can either pay or ignore reply illiac786 12 hours agoparentprevVery true. Most of all because stamps required value for a letter change often and I always ended up with lots of old stamps which were not enough for a letter anymore, had to buy lots of stamps with very little value to complete, it was anarchy. (2012, 2016, 2019 and 2022 saw price increases for German post for example…) reply olooney 19 hours agoprevThis would make a pretty good programming interview question: \"Design a service with two endpoints, one to create a postzegelcode and another to redeem it. A postzegelcode is a nine character, case insensitive alphanumeric code conventionally displayed in all upper case in a 3x3 grid. Postzegelcodes are valid for 5 days after issue and can only be redeemed once. Authentication and payment are out of scope.\" reply kevmo314 16 hours agoparentInsta-hire to the candidate that realizes the number of requests is so small that a plain-text file would suffice. reply OrderlyTiamat 9 hours agorootparentWith a million requests per month (~1k a minute average, non uniform distribution ofc), why not just use something like sqlite? I wouldn't want to work with csv files, and development time is the same. It's a more standard setup, and enables better logging and later changes. reply kazinator 5 hours agorootparentIf they expire in 5 days, that's 1/6 of a million. reply paulmd 14 hours agorootparentprevSQLite, sure, but I don’t want a coworker who chooses the hand-crafted full-scan on artisanal csv libraries or w/e reply jzwinck 12 hours agorootparentYou don't need a full scan or a giant CSV file. You can do it with a single small file in a single directory. The file contains the state of a pseudo random number generator. On each request, generate a number, save the new PRNG state into a new temp file in a subdirectory, and rename the temp file over top of the original file. If renaming fails, try the whole process a few more times. This allows multiple processes to serve requests concurrently. Exercise for the reader: avoid re-reading the PRNG state file on retries. reply Mo3 10 hours agorootparentAnd how would you invalidate the generated codes after 5 days? And is this even scalable to more than one Postzegelcode in circulation? reply paulmd 7 hours agorootparentprevthat sounds like a badly home-rolled uuid4 with extra steps. How about instead we just generate a uuid4, insert it into a sqlite file or Postgres database, and then the postzegelcode value is threefry(SECRET,uuid)? reply TacticalCoder 6 hours agoprev> Assuming 26 letters and 9 digits (the zero is not used to avoid confusion with the letter O) That is cute. But what about 'G' and '6', 'B' and '8' and the myriad of things that can go SNAFU when handwriting is involved? Heck, even on the official picture here: https://www.postnl.nl/versturen/postzegels/postzegels-kopen/... The handwriting '1' looks like a 'L'. Does the system allow for one mistake? Does it automatically try to replace '1' with 'L' and see if it matches a valid code? I wonder the actual failure rate is. If you decided to not use '0' to avoid confusion, you've accepted that there shall be confusion. And confusion doesn't just happen with '0' and 'O'. P.S: as a sidenote I know for a fact that at least 30 years ago there were post offices in some EU countries already doing OCR on handwriting, but the ones I've seen were doing it on digits-only, that had to be written on envelopes with pre-printed empty rectangles. It was done for the cities' postcode, to sort the mail automatically. reply kazinator 5 hours agoparentA well-formed, hand-written O is indistinguishable from a well-formed, hand-written zero. Literally, no amount of effort will make it clear which one is intended, short of some agreed-upon convention to use a different glyph like a slashed O for zero. This is not true of G and 6 or B and 8. If these are well-formed, they are completely distinct. People who paid for their post code are motivated to write the code clearly, for fear of nondelivery; but no amount of motivation will fix 0 versus O. > The handwriting '1' looks like a 'L'. It certainly does not. Rather, the lower case l is easily confused for the digit 1. This postzegelcode doesn't use lower case. I don't see it mentioned that I is avoided because of 1. The likely reason is that Europeans don't write 1 just as a stick. It starts with an angled upstroke. That upstroke makes it possible to confuse the European 1 with an American 7; but the European 7 has a horizontal stroke across its stem which thwarts that problem. (I addition, a cursive 7's upper horizontal stroke is actually wavy, like a tilde.) Confusion of I, 1 and 7 in European handwriting is next to nil. reply wodenokoto 2 hours agorootparent> The likely reason is that Europeans don't write 1 just as a stick. It starts with an angled upstroke. I'm European, and that's how I write it, but if you look at the link, that's not how they write 1's in the promotional material - Those 1's follow pretty much the stroke I would do for an L, only the foot is shorter and slightly more pointed upward. reply banish-m4 21 hours agoprevAmericans ought to petition USPS to do this. It sounds extremely convenient because it doesn't require a Pitney Bowes printer or printing anything from Endicia. PS: Are those tiny, cheap, desktop laser engravers powerful enough to \"print\" on paper without starting a fire? reply tichiian 21 hours agoparent> PS: Are those tiny, cheap, desktop laser engravers powerful enough to \"print\" on paper without starting a fire? Yes, you can modulate their power. It usually takes a few experiments if the manual doesn't have a material table. reply m463 11 hours agoparentprevIt sounds like the opposite of Forever Stamps. Maybe Ephemeral Stamps? Momentary Stamps? Whatever Stamps? reply thih9 23 hours agoprevWhat happens when I share my code with others? I.e. when there are multiple letters with the same code found in an outgoing post box? reply molf 23 hours agoparentFor all mail except the first, the same thing that happens if you send a letter with no postage: the sender will receive a request to pay after delivery. If the sender is unknown the recipient will receive a request to pay (which can be appealed). reply miunau 22 hours agorootparentI mail a fair amount of physical media using the postzegelcode. If you put a return address, PostNL just returns the letter with a sticker on it saying the postage was not enough. They won't deliver it. reply lancebeet 21 hours agorootparentIf they deliver the letter to the return address, what prevents you from writing the target address as the return address and getting it delivered for free? reply hananova 21 hours agorootparentThey know approximately the area the return address should be at, and they also know who used the app to buy the code. reply cedilla 20 hours agorootparentIt's also some kind of fraud in most jurisdictions, or subject to an extra fee if discovered. Germany's new stamps aren't stamped any more - a QR code gets scanned and logged instead - so people thought they were lucky when they received unstamped letters and decided to re-use the stamp. They got hit with a fine. reply thih9 12 hours agorootparentWhat if the postal system has a bug - and one of these people used a newly bought stamp? Rare but not unheard of[1]. [1]: https://amp.cnn.com/cnn/2024/01/13/business/uk-post-office-f... reply NikkiA 12 hours agoparentprevI would guess that the system generates more digits - lets say 3 when the postal system checks that PZC on the mail, if there is nothing written by the PZC then it's assumed to be fresh, and the 3 extra digits are looked up via a postal-service-only gateway, then placed on the mail by the PZC for future lookups, possibly along with a date. If the PZC has the extra digits, then it can be looked up and confirmed as redeemed with those extra digits, and possibly the stamped date. The extra digits could be stamped or handwritten, possibly with the postmark date used as the redemption date manually entered from the mail as it passes through the sorting system the first time, OR stamped from the date it's registered as redemption, either would work. Such a system would relatively guarantee that fraud would be difficult, since only one person could have a PZC without the check digits go into the system, and sharing would be useless. reply Boltgolt 23 hours agoparentprevIIRC the first envelope scanned at a sorting facility invalidates the postzegelcode immediately, any envelopes using it after that will be handled like they had no postzegel at all (receiver has to pay) reply OptionOfT 21 hours agorootparentWhat if you mail 2 and they are scanned at the same time and the database isn't up to date yet? reply dc96 21 hours agorootparentSeems strange they would design a system that could scan faster than they could process the DB, no? I assume there's some queue in between. But the real answer is probably just: multiple scans happen for the same piece of mail. Eventually, one or both of them will be marked as due for payment. The question then becomes, was it worth saving the postage cost for what is potentially a much costlier penalty? reply zoky 18 hours agorootparentprevThis seems fairly unlikely to ever occur. The database would have to be decentralized, and the two items would have to be mailed from separate locations and both be scanned more or less simultaneously—or within however many minutes it takes for database changes to propagate, which can’t be all that long since you’re able to buy a code and use it on the same day. Given how remote the probability of this happening is, simply ignoring it and delivering both pieces probably isn’t unreasonable. Or flag it for further investigation and maybe bill the sender or recipient for an additional piece of mail. reply berkes 12 hours agorootparentAnd even if for some reason it does occur more often, a message queue is a common, and rather simple solution for this. reply contravariant 20 hours agorootparentprevYou may find they will refuse service to you for unspecified reasons. reply consp 23 hours agoparentprevOnly the first one would be legal and the recipients would have to pay for the postal cost of the next one. If you put the origin (postal code plus home number is enough) on the back you get a payment proposal in your mailbox. reply ragebol 14 hours agoparentprevThe typical outgoing mail box, where you put your mail in for it to be delivered, doesn't allow peering into it. So a physical measure to avoid that. But if it's out in the open: first come first serve, like the other comments describe reply thih9 11 hours agorootparent1. Buy an apartment next to a well lit outgoing post box. 2. Install a system of mirrors to peek at the letters being inserted into the box 3. Reuse one code for a 50% chance of saving 1 euro and delivering your mail for free. 4. Send each letter 7 times, each with a different duplicate code, for a 99% chance of at least one free delivery. Not worth the effort and I’m sure this would be tracked and stopped immediately. But it’s cool that in theory this seems possible. reply 6510 22 hours agoparentprevIn my experience it will be delivered and the sender will be politely asked to pay for it. They don't have to pay for it but it's rude not to. reply miunau 22 hours agorootparentHmm. When was this? I've had the code read wrong a few times and they just have returned the letter. Then I've gotten a refund for the postage through their customer service. reply dmurray 22 hours agorootparentprev> They don't have to pay for it but it's rude not to. Given that this is the Netherlands, that sounds like \"they won't pay for it\". reply 6510 4 hours agorootparentMany people think they have to pay. reply vitplister 22 hours agoprevThis is available in Sweden as well https://kopfrakt.postnord.se/service/digitalt-frimarke reply fhackenberger 22 hours agoprevThere's a very similar system in Germany with a hashtag code from Deutsche Post since 2020, called #PORTO: https://www.deutschepost.de/de/m/mobile-briefmarke.html reply filt 12 hours agoprevSweden has this too: https://www.postnord.se/en/private/sending/letters-and-parce... reply pacifika 22 hours agoprevI wonder why they wouldn’t take the address at the time of payment so it’s not needed on the actual letter. That seems more convenient. Backwards compatibility? reply dfox 20 hours agoparentWhen we started our own parcel service in 2015 we thought that everything that is needed on the actual parcel is the unique ID of the parcel (generated by a system reminiscent of twitter's snowflake and intentionally printed with digits shuffled around as to increase the chance of the prefix being unique). Pretty quickly we found out that various operational concerns need additional data on the label, with routing info (for manual pre-sorting on the depots) and recipients phone number (the courier locates the parcel in the trunk by that) being pretty important. Also, cool design of the label is one thing, but on both laser and thermal printers the resolution repeatability is much better in the direction perpendicular to the paper travel, so you do not want to do cute design things with vertical barcodes. And you want to have a huge margin below the barcode if it is aligned to the bottom of the label as some printers will occasionally get misaligned and part of the barcode will get on the next label… and the imaging scanners in Zebra terminals will happily read half a milimetre high code-128 barcode instead of the correct one on the bottom of the label. reply inopinatus 19 hours agorootparentThis isn’t a minor detail. As with stored-value cards vs credit cards, the location of the master record(s) has far-reaching consequences for dependant business processes. Any field activity depending on on-demand access to a centralised API contains the seeds of its own failure. reply dfox 18 hours agorootparentThe availability of centralized API was not the issue. The issue was that the operators were significantly more efficient when performing the process “by eye” than when supported by some kind of computerized system. For example the on-depot sorting process that we settled on was two-phase: first you sort the parcels either manually by routing codes on the labels or through automated sorting line with the current data and then in second phase the operator scans everything that is supposed to go out of the depot and in that process moves around the ~5% of parcels that got sorted incorrectly, which is incredibly more effective than doing that correctly in one pass. reply inopinatus 18 hours agorootparent> The availability of centralized API was not the issue. The issue was that the operators were significantly more efficient when performing the process “by eye” than when supported by some kind of computerized system. If you think these are different considerations, then you are splitting the wrong hairs. The processes landed on to resolve the conflict are not surprising, but also doesn’t sound like they addressed the underlying architectural conceit. reply dfox 18 hours agorootparentAt a large enough scale you can do most of these processes by automation, except the actual courier delivery. And well, at least UPS has actual identified locations in their vans, so just going by location+barcode might be somewhat practical for them. reply inopinatus 18 hours agorootparentBeware of the line of operations research thinking that has latterly tanked Boeing’s engineering reputation, something the rest of us designing layered business processes can all learn from. Complexity is always best handled pushed down to the leaves of the tree. The ideal central body performs reporting and audit, not command and control. Although for some, I suppose the Boeing takeaway might be that the bottom line improved and no-one went to prison. reply ale42 21 hours agoparentprevNot sure about NL, but in Switzerland you can buy those codes with an SMS. I guess many people want to do it quickly, and they might have already written the address of the recipient. So just getting a code it much faster, you don't have to enter the address somewhere. reply Beldin 18 hours agorootparentI think the main issue is that regular post has addrrss info of the recipient. So the whole logistics chain is geared towards that. If you remove the need for address info for a (small) subset, you'd have to have two logistics chains somewhat intertwined with each other... reply inopinatus 19 hours agoparentprevIf the delivery model can’t survive an apocalypse, collapse of civilisation, or interruption to the rule of law/network connectivity, then you have no business running a postal service. reply zoky 18 hours agorootparentBut then how would Kevin Costner be able to make a movie out of it??? reply inopinatus 17 hours agorootparentOne may jest but the subtext of The Postman (particularly the David Brin novel, which I can recommend) is analysing which pre-apocalyptic institutional operating models are predisposed (or otherwise) to being rebooted/reconstituted even after the loss of all staff and centralised records, which is a definition of business continuity and process resilience I certainly intended to be in scope of the remark above. Which is to say, Kevin Costner did make a movie about it. Just not a good one. In practice though you can simulate/stimulate most of the failure mode conditions of central control dependencies by parking a delivery truck underground, or addressing something to a farm. reply sodality2 22 hours agoparentprevIf mail gets lost for longer than 5 days, it won’t be known who it goes to, since these codes are reused. reply pacifika 22 hours agorootparentThen you can write any code down and wait 5 days to send it reply t0mas88 21 hours agorootparentYou can, but an invalid code is treated as if there was no postage stamp on it at all. So your letter will be returned or the receiver is asked to pay. A code that's past the validity date is probably considered an invalid code as well. reply afro88 21 hours agorootparentprevProbably the sender has to be linked to the current code for it to be accepted reply Kiala 22 hours agoparentprevOr, you know, the mail man? reply Sharlin 22 hours agorootparentClearly mailmen would just be expected to use their phones to decode the addresses on demand, because everything is supposed to require a phone and an internet connection these days. A bit more realistically, the address could be printed on a sticker by the sorting machine, the way address redirections are handled. reply dfox 21 hours agorootparentFor a mail/parcel carrier relabeling anything is (somewhat surprisingly) a significant added cost (and there is not that much of margin), so they will do everything in their power to not do that unless absolutely necessary. reply Denvercoder9 20 hours agorootparentPostNL relabels approximately all parcels addressed to me (I almost always have to change either the delivery date or location to one convenient to me), completely free of charge. reply dfox 18 hours agorootparentFree of charge to you and free of charge to the sender. Which is exactly my point, for PostNL it is a sunk cost and if that will happen for significant amount of their consignments it would be an issue. reply alkonaut 23 hours agoprevI wonder if any of the 9 characters are checksum (the example math indicates none is), or if the validation scan allows a search without one of the characters e.g AB1?3XYZ in case of sloppy writing. If not, then the system relies on people being able to write 9 consecutive legible characters. With postal codes and addresses it’s less fragile because the context helps disambiguate (a last name helps a postman tell if a street address has a 7 or 1) reply molf 22 hours agoparentIn the case of sloppy writing, the letter will be ejected from the sorting machine and reviewed by a human (same process as for illegible addresses). If a human cannot find a match then the sender will receive a request to pay for the missing postage (presuming the code is invalid). If the sender is unknown the recipient will receive a payment request (which can be appealed). Speaking from experience you would normally want the letter to arrive at its destination so I take care to write the code very clearly. I imagine this is true for most people. reply kr2 21 hours agorootparentYou massively underestimate the care or thought some Americans give to anything including legibly writing an address; very cool look into how USPS handles that situation (link free to read without subscription) https://www.nytimes.com/2013/05/04/us/where-mail-with-illegi... reply Caligatio 13 hours agorootparentAnd like almost all things that are interesting, here's a Ton Scott video about the same topic: https://www.youtube.com/watch?v=XxCha4Kez9c reply hprotagonist 19 hours agorootparentprev“where MNIST comes from” reply dark-star 22 hours agoparentprevSince there is already a framework for returning letters with invalid/unreadable/damaged addresses, I'd guess the overhead of returning wrongly-written postzegelcodes is negligible. reply timvdalen 22 hours agoprevWorks pretty well, definitely less work than having to go to a store to buy a single stamp whenever I need to send a letter reply rtpg 18 hours agoprevIs there any context into why this was put into place at the time it was? I would like to understand why this was set up. reply jorams 9 hours agoparentI found an article[1] from before it was launched. Translated: > The most important reason for the introduction of the Zegelcode is that about 20% of our customers indicates they don't have a stamp on hand when they want to send something. [1]: https://www.internetkassa.nu/postnl-verwacht-digitaal-franke... reply fullspectrumdev 21 hours agoprevA similar system (that I have not used yet) has been adopted in Ireland, and apparently it works really well? reply tdeck 19 hours agoparentI found an article about it: https://www.rte.ie/news/business/2022/1012/1328565-digital-s... reply jcul 9 hours agoparentprevYeah, I got a flyer about it in the post recently. Seems like a great idea, I seldom post anything, but when I do it's a pain to have to go buy stamps. It's a bit stingy though that they are charging so much more for a digital stamp. reply atemerev 21 hours agoprev [–] Also works in Switzerland. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A postzegelcode is a nine-character code used in the Netherlands as a handwritten alternative to postage stamps, purchased online from PostNL.",
      "Introduced in 2013, the code must be written on mail within five days and has 78.8 trillion possible combinations, making it secure against guessing.",
      "As of December 2020, 590,000 people used postzegelcodes, which now cost the same as traditional stamps."
    ],
    "commentSummary": [
      "The discussion revolves around the use of digital postage codes, known as \"postzegelcode,\" which can be written on envelopes instead of using traditional stamps.",
      "Various countries, including Germany, Denmark, Norway, and Sweden, have implemented similar systems, allowing users to purchase postage online and write a code on their mail.",
      "The system is praised for its convenience, especially for those who do not frequently send physical mail, as it eliminates the need for physical stamps and adapts to changing postal rates."
    ],
    "points": 228,
    "commentCount": 95,
    "retryCount": 0,
    "time": 1719775062
  },
  {
    "id": 40847963,
    "title": "Supreme Court rules ex-presidents have immunity for official acts",
    "originLink": "https://apnews.com/article/supreme-court-trump-capitol-riot-immunity-2dc0d1c2368d404adc0054151490f542",
    "originBody": "The Supreme Court extended the delay in the criminal case against Donald Trump on charges he plotted to overturn the 2020 election, reducing the chance that Trump could be tried before the November election. Read More Photos 8 By MARK SHERMAN Updated 5:11 PM UTC, July 1, 2024 Share Share Copy Link copied Email Facebook X Reddit LinkedIn Pinterest Flipboard Print Live updates: Follow AP’s coverage of the US Supreme Court. WASHINGTON (AP) — The Supreme Court on Monday ruled for the first time that former presidents have broad immunity from prosecution, extending the delay in the Washington criminal case against Donald Trump on charges he plotted to overturn his 2020 presidential election loss and all but ending prospects the former president could be tried before the November election. In a historic 6-3 ruling, the court’s conservative majority, including the three justices appointed by Trump, narrowed the case against him and returned it to the trial court to determine what is left of special counsel Jack Smith’s indictment. The court’s decision in a second major Trump case this term, along with its ruling rejecting efforts to bar him from the ballot because of his actions following the 2020 election, underscores the role the justices are playing in the November election. The court last week also limited an obstruction charge faced by Trump and used against hundreds of his supporters who stormed the Capitol on Jan. 6, 2021. Related coverage Justices keep on hold Florida and Texas efforts to regulate social media platforms The Supreme Court rules for a North Dakota truck stop in a new blow to federal regulations Supreme Court makes it harder to charge Capitol riot defendants with obstruction, charge Trump faces “Under our constitutional structure of separated powers, the nature of presidential power entitles a former president to absolute immunity from criminal prosecution for actions within his conclusive and preclusive constitutional authority,” Chief Justice John Roberts wrote for the court. “And he is entitled to at least presumptive immunity from prosecution for all his official acts. There is no immunity for unofficial acts.” Roberts insisted that the president “is not above the law.” But in a fiery dissent for the court’s three liberals, Justice Sonia Sotomayor wrote, “In every use of official power, the President is now a king above the law.” Reading from her opinion in the courtroom, Sotomayor said, “Because our Constitution does not shield a former president from answering for criminal and treasonous acts, I dissent.” Sotomayor said the decision “makes a mockery of the principle, foundational to our Constitution and system of government, that no man is above the law.” The protection afforded presidents by the court, she said, “is just as bad as it sounds, and it is baseless.” Trump posted in all capital letters on his social media network shortly after the decision was released: “BIG WIN FOR OUR CONSTITUTION AND DEMOCRACY. PROUD TO BE AN AMERICAN!” Smith’s office declined to comment on the ruling. President Joe Biden’s campaign said in a statement that the Supreme Court’s immunity ruling “doesn’t change the facts” about the events of Jan. 6. Senate Majority Leader Chuck Schumer denounced the ruling as “a disgraceful decision,” made with the help of the three justices that Trump appointed. “It undermines SCOTUS’s credibility and suggests political influence trumps all in our courts today,” the New York Democrat said on X. The justices knocked out one aspect of the indictment. The opinion found Trump is “absolutely immune” from prosecution for alleged conduct involving discussions with the Justice Department. Trump is also “at least presumptively immune” from allegations that he tried to pressure Vice President Mike Pence to reject certification of Democrat Joe Biden’s electoral vote win on Jan. 6, 2021. Prosecutors can try to make the case that Trump’s pressure on Pence still can be part of the case against him, Roberts wrote. The court directed a fact-finding analysis on one of the more striking allegations in the indictment -- that Trump participated in a scheme to enlist fake electors in battleground states won by Biden who would falsely assert that Trump had won. Both sides had dramatically different interpretations as to whether that effort could be construed as official, and the conservative justices said determining which side is correct would require additional analysis at the trial court level. Roberts’ opinion further restricted prosecutors by prohibiting them from using any official acts as evidence in trying to prove a president’s unofficial actions violated the law. One example not relevant to this case but which came up in arguments was the hypothetical payment of a bribe in return for an ambassadorial appointment. Under Monday’s decision, a former president could be prosecuted for accepting a bribe, but prosecutors could not mention the official act, the appointment, in their case. Justice Amy Coney Barrett, who joined the rest of Roberts’ opinion, parted company on this point. “The Constitution does not require blinding juries to the circumstances surrounding conduct for which Presidents can be held liable,” Barrett wrote. The work of figuring out how to proceed will fall to U.S. District Judge Tanya Chutkan, who would preside over Trump’s trial. Trump still could face a trial, said Notre Dame law professor Derek Muller. “But the fact remains that it is almost impossible to happen before the election.” David Becker, an election law expert and the executive director of the nonprofit Center for Election Innovation and Research, called the breadth of immunity granted to Trump “incredibly broad” and “deeply disturbing.” “Almost anything that a president does with the executive branch is characterized as an official act,” he said on a call with reporters following the ruling. He said that “for any unscrupulous individual holding the seat of the Oval Office who might lose an election, the way I read this opinion is it could be a roadmap for them seeking to stay in power.” The ruling was the last of the term, and it came more than two months after the court heard arguments, far slower than in other epic high court cases involving the presidency, including the Watergate tapes case. The Republican former president has denied doing anything wrong and has said this prosecution and three others are politically motivated to try to keep him from returning to the White House. In May, Trump became the first former president to be convicted of a felony, in a New York court. He was found guilty of falsifying business records to cover up a hush money payment made during the 2016 presidential election to a porn actor who says she had sex with him, which he denies. He still faces three other indictments. Smith is leading the two federal probes of the former president, both of which have led to criminal charges. The Washington case focuses on Trump’s alleged efforts to overturn the 2020 election after he lost to Biden. The case in Florida revolves around the mishandling of classified documents. A separate case, in Georgia, also turns on Trump’s actions after his defeat in 2020. If Trump’s Washington trial does not take place before the 2024 election and he is not given another four years in the White House, he presumably would stand trial soon thereafter. But if he wins, he could appoint an attorney general who would seek the dismissal of this case and the other federal prosecution he faces. He could also attempt to pardon himself if he reclaims the White House. He could not pardon himself for the conviction in state court in New York. Justice Clarence Thomas wrote a separate opinion saying that he believed Smith’s appointment as special counsel was illegitimate. No other justice signed onto that opinion, but the question took center stage in recent arguments in the Florida case over classified documents. The Supreme Court that heard the case included three justices appointed by Trump — Neil Gorsuch, Brett Kavanaugh and Barrett — and two justices who opted not to step aside after questions were raised about their impartiality. Thomas’ wife, Ginni, attended the rally near the White House where Trump spoke on Jan. 6, 2021, though she did not go the Capitol when a mob of Trump supporters attacked it soon after. Following the 2020 election, she called the outcome a “heist” and exchanged messages with then-White House chief of staff Mark Meadows, urging him to stand firm with Trump as he falsely claimed that there was widespread election fraud. Justice Samuel Alito said there was no reason for him to step aside from the cases following reports by The New York Times that said flags similar to those carried by the Jan. 6 rioters flew above his homes in Virginia and on the New Jersey shore. His wife, Martha-Ann Alito, was responsible for flying both the inverted American flag in January 2021 and the “Appeal to Heaven” banner in the summer of 2023, he said in letters to Democratic lawmakers responding to their recusal demands. Trump’s trial had been scheduled to begin March 4, but that was before he sought court-sanctioned delays and a full review of the issue by the nation’s highest court. Before the Supreme Court got involved, a trial judge and a three-judge appellate panel had ruled unanimously that Trump could be prosecuted for actions undertaken while in the White House and in the run-up to Jan. 6. “For the purpose of this criminal case, former President Trump has become citizen Trump, with all of the defenses of any other criminal defendant,” the appeals court wrote in February. “But any executive immunity that may have protected him while he served as President no longer protects him against this prosecution.” Chutkan ruled against Trump’s immunity claim in December. In her ruling, Chutkan said the office of the president “does not confer a lifelong ‘get-out-of-jail-free’ pass.” ___ Associated Press writers Lindsay Whitehurst, Alanna Durkin Richer, Eric Tucker, Stephen Groves, Farnoush Amiri, Michelle Price and Ali Swenson contributed to this report.",
    "commentLink": "https://news.ycombinator.com/item?id=40847963",
    "commentBody": "Supreme Court rules ex-presidents have immunity for official acts (apnews.com)223 points by _rend 1 hour agohidepastfavorite324 comments treeFall 51 minutes ago>The President of the United States is the most powerful person in the country, and possibly the world. When he uses his official powers in any way, under the majority's reasoning, he now will be insulated from criminal prosecution. Orders the Navy's Seal Team 6 to assassinate a political rival? Immune. I would consider this an extreme knee jerk take, but it's Sotomayor saying it. https://x.com/mikedebonis/status/1807787300375445993 reply lolinder 28 minutes agoparentTo give the majority opinion its own voice: > The President enjoys no immunity for his unofficial acts, and not everything the President does is official. The President is not above the law. But Congress may not criminalize the President’s conduct in carrying out the responsibilities of the Executive Branch under the Constitution. And the system of separated powers designed by the Framers has always demanded an energetic, independent Executive. The President therefore may not be prosecuted for exercising his core constitutional powers, and he is entitled, at a minimum, to a presumptive immunity from prosecution for all his official acts. https://www.supremecourt.gov/opinions/23pdf/23-939_e2pg.pdf EDIT: OP left a reply to this comment that I think was unfairly flagged, visible here with showdead: https://news.ycombinator.com/item?id=40848860 EDIT 2: It's been restored, but I'm leaving the above note because there are now replies referencing it. reply Buttons840 5 minutes agorootparent> \"The President is not above the law.\" What does that even mean if it's impossible to prosecute the President? What does that even mean? reply abduhl 3 minutes agorootparentThere is a distinction to be drawn between The President (the office) and the President (the person). The latter is not above the law with respect to criminal prosecutions. The former is. reply Buttons840 0 minutes agorootparentSo the person of the President can be prosecuted? That's good to know because when the President breaks the law I want the person to be prosecuted, not the office. treeFall 25 minutes agorootparentprevThen it sounds like Sotomayor is being extreme by conflating domestic political assassinations as an \"official act.\" Does she not understand the difference? reply Goronmon 5 minutes agorootparentDoes she not understand the difference? Explain the difference. reply treeFall 0 minutes agorootparentDepriving someone of their constitutional rights cannot be an official act by definition. Arguing that it can be is just word games in a world where words stop mattering. jwithington 2 minutes agorootparentprevcouldn't be it reasonably construed as an official act if the president believed that person was a member of a terrorist group? the post 9/11 Authorization of Use for Military Force grants the president the use of all \"necessary and appropriate force\" in prosecuting terrorists. this act is still in effect. reply buildbot 4 minutes agorootparentprevThis was argued specially by Trumps defense team, it’s not a hypothetical https://www.msnbc.com/rachel-maddow-show/maddowblog/pressed-... reply lolinder 1 minute agorootparentRather, the prosecution asked it as a hypothetical and the defense refused to rule out that it might be an official act. downWidOutaFite 5 minutes agorootparentprevThe ruling is extremely wishy washy about what is official vs unofficial, and keeps saying that it is very hard to determine, and even prohibits prosecutors from using certain legal tactics to determine if something is official or unofficial. reply brokencode 5 minutes agorootparentprevJust call it necessary for national security with some BS reasoning and thin or made up evidence and suddenly it’s an official act. There are so many ways a president could twist pretty much anything into an official act. reply chx 12 minutes agorootparentprevThe problem is that official vs unofficial designation does not exist. The Supreme Court just invented it out of thin air. Further https://www.thenation.com/article/society/trump-immunity-sup... > the court has left nearly no sphere in which the president can be said to be acting “unofficially.” And more importantly, the court has left virtually no vector of evidence that can be deployed against a president to prove that their acts were “unofficial.” If trying to overthrow the government is “official,” then what isn’t? And if we can’t use the evidence of what the president says or does, because communications with their advisers, other government officials, and the public is “official,” then how can we ever show that an act was taken “unofficially?” reply inglor_cz 6 minutes agorootparentDoesn't this difference exist de facto? Trump murdering his business partner at a dinner because they had a fallout is pretty clearly unofficial, while Trump ordering assassination of the Tyrant of Ruritania is official, albeit probably immoral and/or dangerous to boot. Of course the grey zone between those two poles is going to be pretty wide. reply robertoandred 0 minutes agorootparentUh, you're missing the fact that his business partner is a danger to national security. The motives are irrelevant. sowut 13 minutes agorootparentprevnext [2 more] [flagged] ffgjgf1 7 minutes agorootparentBecause there isn’t? Just frame the assassination as fighting domestic terrorists or some other nonsense. reply alphabetatheta 36 minutes agoparentprevThis is a terrible take. The Supreme Court case severely limits even the use of evidence to prosecute a President. The majority ruling says that as long as something is done in \"official\" capacity the intentions don't matter. EDIT: This ruling by the way retroactively clears Nixon of all wrongdoing for Watergate. reply danielmarkbruce 39 minutes agoparentprevIt's extreme to the point of silliness. If a court decides that would fall under \"official acts\", we are already doomed. reply OgsyedIE 36 minutes agorootparentIn practice the court will say whatever POTUS tells it to say, lest an 'official act' remove some of the members of the court and their loved ones. reply TylerE 28 minutes agorootparentOnly if the potus has an -R. This court has been a caricature of siding with evil whenever given the opportunity. It's composed of the three honest judges and six partisan Federalist Society hacks who will make up whatever terrible \"justification\" to rule for the powerful and rich. reply belorn 11 minutes agorootparentprevUnder the same line of thought, soldiers in the military will always follow what their commander say, least order to comes to remove a soldier and their loved ones. reply bcrosby95 18 minutes agorootparentprevOfficial acts are still official regardless of the underlying reason and according to this case courts aren't even allowed to examine those reasons. reply danielmarkbruce 10 minutes agorootparent\"official acts\" have yet to be defined. reply doctorpangloss 18 minutes agorootparentprev> If a court decides that would fall under \"official acts\", we are already doomed. Everyone thinks lines don't get crossed, until they do. reply lenerdenator 11 minutes agorootparentAnd if the last nine years have proven anything, most of the system will say \"well, that wasn't technically a line, just something we've always done a certain way that was up for change at any moment's notice should one person decide to do so.\" reply MPSimmons 34 minutes agorootparentprevWhat does \"Enemies, foreign and domestic\" mean, really? reply o11c 20 minutes agorootparentThat phrasing is so passé. It has been \"enemies, real and imaginary\" for over 2 decades now. reply jcranmer 22 minutes agorootparentprevThe president telling any member of his cabinet to do anything is presumptively an official act, and the evidence of him doing so is categorically forbidden from being used as evidence. (Hell, that's more extreme than even Trump's lawyers asked for! This basically overturns US v Nixon in its quest to elevate This is an opinion that might make sense if we were being asked if a president ordering drone assassinations makes him liable for murder. In the context of the president trying to instigate a coup for not being reelected, to the point that his own government is threatening mass resignation if he carries it out in protest at the sheer unconstitutionality of it... this is the kind of question that almost begs SCOTUS to say \"make a narrow ruling as to whether or not this specific instance is permissible\" and SCOTUS decides instead to make a grand, sweeping proclamation for all ages and circumstances and neglect to look at the specific facts in this case and leave it unanswered here. Roberts, let this case be your Dred Scott decision, your Korematsu decision. You've certainly done more to torpedo the credibility of the court in one decision than any other case in the past few decades... and that's saying quite a bit. reply skhunted 26 minutes agorootparentprevBut this Court has essentially made it clear that Trump isn’t going to be prosecuted. He gets to try to co-opt the political process and attempt a coup with impunity. We are doomed. We were doomed on January 7 when Trump wasn’t arrested. The Republic is over. It is only a matter of time before some vile President uses this ruling for evil ends. We now live in a country in which a few select people have absolute immunity when committing crimes when exercising their constitutional powers. It’s absurd. reply downWidOutaFite 2 minutes agorootparentprevWe were doomed when Republicans decided that impeachment was a partisan tool and could not be used against Trump. They broke the system by abandoning their constitutional duties so we are now looking into the abyss. reply shadowgovt 35 minutes agorootparentprevI think that's rather her point. We are already doomed. Think about how the Executive handled rival political movements in the past. The Black Panthers. CPUSA circa 1919. Now imagine Fred Hampton had been running for President when he was killed. The line between \"political opponent\" and \"enemy of the State\" can become pretty blurry. This ruling gives the Executive broad power to act in its own interests when it can claim the line is blurry. reply brendoelfrendo 28 minutes agorootparentprevIt is, frankly, moon logic. The President is commander-in-chief, ergo, they are immune from prosecution when issuing an order to the military, even if the order is illegal? Because the Constitution says the President can issue orders and doesn't say anything about whether those orders need to be legitimate or justifiable in any sort of national context? Repeat for the Justice Department, or Immigration, or any of the many offices that fall under the executive branch. Apparently if the President is insane, or corrupt, or treasonous... that's a problem for all of us, but not necessarily a problem for the President. reply jameshart 11 minutes agorootparentReally does a number on the ‘unlawful order’ doctrine for military accountability. Has SCOTUS made ‘just following orders’ a valid legal defense? They’ve also made much of the fact that the presidential authority to pardon is constitutionally unreviewable, so even if the president orders someone to commit a crime, he can pardon them preemptively. His appointment power is similarly in the constitution, so he can also fire and replace them until he finds someone willing to do it. Are we really left with ‘if the president were to issue illegal orders to his staff, Congress would definitely impeach him’? reply enragedcacti 17 minutes agorootparentprevYou know the majority can read the dissent right? If they felt it so silly they could have addressed it. Rather than contesting the claim they dismiss it because they feel its not very likely: > The dissents’ positions in the end boil down to ignoring the Constitution’s separation of powers and the Court’s precedent and instead fear mongering on the basis of extreme hypotheticals about a future where the President “feels empowered to violate federal criminal law.” [...] The dissents overlook the more likely prospect of an Executive Branch that cannibalizes itself... Also important to note is that the hypothetical didn't sprout from thin air, Trump's lawyers (who's arguments SCOTUS broadly accepted) acknowledged that extrajudicial assassinations would fall under official acts. https://thehill.com/regulation/court-battles/4398223-trump-t... reply lolinder 13 minutes agorootparentThat's not what they say, you cut off their actual response: > The dissents overlook the more likely prospect of an Executive Branch that cannibalizes itself, with each successive President free to prosecute his predecessors, yet unable to boldly and fearlessly carry out his duties for fear that he may be next. ... Virtually every President is criticized for insufficiently enforcing some aspect of federal law (such as drug, gun, immigration, or environmental laws). An enterprising prosecutor in a new administration may assert that a previous President violated that broad statute. Without immunity, such types of prosecutions of ex-Presidents could quickly become routine. Their argument isn't that it's not likely, it's that there's another failure mode that is even more likely. reply lenerdenator 7 minutes agorootparentI wouldn't call that a failure mode. The US has a long history of relying purely on executive latitude and good faith to get things done. We now have a bad faith actor who wants back in the Oval Office with far fewer restrictions on his power. A lawsuit or criminal charge would absolutely be a good remedy in that situation. Furthermore, it's the implementation of a system. What the justices of the majority said today is \"there is no system\". When there is no system to deal with problems, then humans make them up ad-hoc, which usually results in violence, if history is any indicator - and it is. reply blue_dragon 18 minutes agoparentprevMy naive assumption is that ordering Seal Team 6 to assassinate a political rival is not an official nor constitutionally authorized power, and thus would be prosecutable. reply saucetenuto 16 minutes agorootparentGuess again! Roberts explicitly calls out orders to the military as covered by absolute immunity. EDIT: and motive is explicitly barred from review too. reply thih9 5 minutes agorootparentDo you have a source? I didn’t find this in the article, could you add a quote - or link if it’s from elsewhere? reply ceejayoz 2 minutes agorootparenthttps://www.supremecourt.gov/opinions/23pdf/23-939_e2pg.pdf Page 14 notes that the President's official responsibilities \"include, for instance, commanding the Armed Forces of the United States; granting reprieves and pardons for offenses against the United States; and ap- pointing public ministers and consuls, the Justices of this Court, and Officers of the United States.\" Page 17 states \"We thus conclude that the President is absolutely immune from criminal prosecution for conduct within his exclusive sphere of constitutional authority.\" Page 26 states \"In dividing official from unofficial conduct, courts may not inquire into the President’s motives.\" ceejayoz 12 minutes agorootparentprevThe President is the Commander in Chief; issuing orders to the military is very much an official act. \"But not for this! This would be clearly corrupt!\" you may say, but the decision addresses that as well; the President's motive for the \"official act\" cannot be introduced as evidence! > In dividing official from unofficial conduct, courts may not inquire into the President’s motives. reply MOARDONGZPLZ 13 minutes agorootparentprevArticle II of the constitution specifically gives the POTUS the authority to command the armed forces. The limit is declaring war, which is vested in Congress. So it seems reasonable that commanding Seal Team 6 is specifically a constitutionally authorized power and within an official duty. reply treeFall 8 minutes agorootparentBeing killed by the government without due process of law (ie: death penalty for convicted criminals) is a clear violation of your constitutional rights though, and violating someone's constitutional rights (read: going against the constitution) can't be an official act by definition. reply MOARDONGZPLZ 4 minutes agorootparentThis is explicitly about the POTUS being immune from prosecution for official acts like commanding ST6 to assassinate someone. Fortunately in some cases, unfortunately in others, semantic wordplay akin to “could god microwave a burrito so hot even they couldn’t eat it” has little effect in the court system. That is to say, semantic gotchas like “but actually it couldn’t be an official act because the very act is against the constitution” have no sway. reply Miner49er 4 minutes agorootparentprevIt happened under Obama and nobody cared. I think it is definitely something that could be considered an \"official act\". reply ffgjgf1 5 minutes agorootparentprevAnd you’re are free to sue the government in that case. The DOJ won’t prosecute the president for ordering something like that even after he leaves office.. reply sf_rob 4 minutes agorootparentprevAnd every war since Vietnam has been a police action or military operation. reply InTheArena 9 minutes agorootparentprevcommanding seal team 6 to assassinate a sitting head of state is a act of war, and only congress can declare war. reply ceejayoz 6 minutes agorootparentWe've made it very clear for decades that the President can commit acts of war without declaring it. Syria, Lybia, Iraq, Afghanistan, and dozens of other hotspots around the world. reply vlovich123 0 minutes agorootparentprevBut assasinating private citizens a ok? So Biden can order the assassination of Trump? Even if you claim as someone else that you can’t violate someone’s constitutional rights within an official act, we have already killed US citizens abroad via drone strikes. So if Trump is in some foreign country we can drop an a-bomb and claim it was to kill some terrorist & that’s a ok? The minority points out that official acts was already a defense. The majority opinion here claims that it’s an absolute immunity and even motivation is impervious to this immunity claim. This is a serious undermining of the rule of law in a substantial way and paves the way for the US to have a dictatorship at some point in the future. gortok 6 minutes agorootparentprevThat hasn't stopped any president in the last 60 years from using the Armed Forces to conduct a war -- or \"police action\" -- if you prefer. reply lenerdenator 13 minutes agorootparentprevMaybe, maybe not. What does the judge who reviews the case think? That's literally the only thing preventing that scenario from playing out. reply crazygringo 9 minutes agoparentprevI see a lot of people here in the comments claiming that this is still knee-jerk, or silly, or obviously that would not be an \"official act\". To the contrary -- this is an explicit example that came up during oral arguments, where a Trump lawyer specifically claimed that indeed, Trump could not be convicted criminally of this (unless he had first been impeached and convicted). Nowhere in the majority opinion does it try to draw some kind of line against this. And indeed, the President is constitutionally \"commander in chief of the Army and Navy of the United States\", and the opinion states this authority is \"conclusive and preclusive\". Quite simply, according to this decision, anything the president commands the Navy to do, including a Navy Seal, is an official act because it a power explicitly granted by the constitution, and thus immune from prosecution. To repeat: this specific scenario was brought up during oral arguments, indeed as one of the main arguments that was also widely reported. This is not a far-flung wacko example Sotomayor came up with herself -- it's the very heart of the case. The fact that the opinion does not even attempt to explain why this would still be considered criminal, and the fact the Sotomayor is confirming why it would be allowed, is not a misreading or a mistake. It is clearly intentional and genuinely scary. reply throwawaymaths 47 minutes agoparentprevin practice, not really much of a change. Did FDR stand trial for interning Japanese people? Ok, ok he died too soon. Would he have? reply vundercind 45 minutes agorootparentEven if he had—he might have been acquitted! Even under a relatively fair trial! The dissent notes that official acts as a defense is already A Thing. What’s changed is upgrading that to immunity, which means they can’t be tried in the first place, no defense needed. The law is simply held not to apply. reply skhunted 41 minutes agorootparentprevThere’s a huge difference between “I think I can get away with this” and “The Supreme Court says I have absolute immunity”. reply danielmarkbruce 38 minutes agorootparentExcept the ruling doesn't say that. reply redserk 32 minutes agorootparentWhat’s exactly said and what are the practical impacts are two completely different things. It will take many, many cases to elaborate on what defines an official act and what exactly decides immunity, and in the process we could see a lot of potential what-I’d-say is overreach. It is far too early to declare that there will be zero side effects from this — as with literally any Supreme Court ruling. We’ve already seen what qualified immunity gets us, and I’d bet many people didn’t expect it to go that way. reply lenerdenator 25 minutes agorootparentprevIt absolutely says that. A President, if they act in an \"official\" capacity, cannot later be charged with a crime for something they did in that official capacity. Not \"can offer being an official as a defense\", \"cannot be charged\". What is an official capacity? Well there's the traditional stuff like vetoes, appointments, and the like, but there's a lot of stuff that is far more nebulous. Does the President act in an official capacity when telling the Vice President what to do with regards to certifying an electoral college result? Well, if that's official, Trump has to be more-or-less let go for what happened on January 6th. We are now basically saying that it's up to a judge - who may or may not have been appointed by the President in question - to decide whether something was an official act. If it is? Welp, sorry the President's wanton order violated your Constitutional rights, but no trial. reply danielmarkbruce 5 minutes agorootparentOfficial act isn't defined, so it doesn't say that. Your last paragraph is closer to accurate, except a president doesn't just \"appoint\" a judge. They have to be voted in by the senate. So, if the president is a crook and nominates a crook and the senate is full of crooks and vote yes to have said crook become a judge, then yes really bad stuff can happen. But if the president is a crook and the majority of the senate are crooks, it's already all over. willcipriano 9 minutes agorootparentprevActs that are unconstitutional probably aren't official, by definition they are outside the scope of official duties, throw every living president into prison? Charge every act of violence ordered in places war is undeclared as war crimes? If not, the things they have on Trump seem like really small potatoes to zero in on. reply skhunted 33 minutes agorootparentprevWhile a president has total immunity for exercising “core constitutional powers,” a sitting or former president also has “presumptive immunity” for all official acts. That immunity, wrote Chief Justice John Roberts in the majority opinion, “extends to the outer perimeter of the President’s official responsibilities, covering actions so long as they are not manifestly or palpably beyond his authority.” reply flyingpenguin 9 minutes agorootparentprevHe very likely should have at least been tried (and probably acquitted). I don't understand this modern take so many people have that \"Freedom to act\" somehow means \"act without consequences\". reply dooglius 40 minutes agorootparentprevhttps://en.wikipedia.org/wiki/Korematsu_v._United_States?use... reply nashashmi 45 minutes agorootparentprevFDR did a lot of wrong, including paying farmers to not work. reply SoftTalker 20 minutes agorootparentA lot of presidents did a lot of wrong. The ones who did not would be a shorter list. reply jhp123 17 minutes agoparentprevThis example came up during oral arguments, Trump's lawyer agreed that assassination of a political rival could be a protected official act[0] [0] https://abcnews.go.com/Politics/seal-team-6-assassination-hy... reply Aunche 17 minutes agoparentprevI'll believe it when Sotomayor actually authors an opinion that defends the assassination of a political rival as an official act. Sotomayor is the most bad faith Justice after Thomas. reply kardianos 45 minutes agoparentprevThat's the minority opinion. Maybe people should read the majority opinion first? reply ahmeneeroe-v2 43 minutes agorootparent6 of 9 justices presumably felt they weren't enabling an executive with no limits reply 7thaccount 35 minutes agorootparentSeveral of those are absolutely corrupt at this point and bought off by the corporatocracy that now rules the country. reply mandmandam 4 minutes agorootparentConveniently, buying off SC judges was also made legal this week. reply bitlax 37 minutes agoparentprev> I would consider this an extreme knee jerk take, but it's Sotomayor saying it. Sotomayor is prone to extreme knee-jerk takes. https://apnews.com/article/fact-checking-052172757066 reply rafram 27 minutes agorootparentAn incorrect off-the-cuff comment during oral arguments != an incorrect assertion in a published decision. reply squidbeak 14 minutes agorootparentprevShe's a Supreme Court Justice so her cautions shouldn't be lightly dismissed. reply grotorea 4 minutes agoprevIt's interesting that even in the Roman Republic the immunity ended after the end of your term, and you could be prosecuted for official acts taken during it. And even let to Caesar fighting to keep himself in office at all times to avoid inevitable prosecution. https://theconversation.com/from-caesar-to-trump-immunity-is... reply zeroonetwothree 1 hour agoprevImmunity for things they do as part of their official duties. I suppose it’s reasonable but the question will now turn to what is actually an official duty. The opposite holding, where they are liable for everything, would be untenable. Could Obama be prosecuted for ordering drone strikes that unintentionally killed two Americans? It seems like that world would hamstring the president far too much. I don’t know if they struck the right balance here (and we not know until the next time it comes up), but at least we have slightly more clarity. reply lapcat 1 hour agoparent> Could Obama be prosecuted for ordering drone strikes that unintentionally killed two Americans? It seems like that world would hamstring the president far too much. The President shouldn't have the legal authority to conduct any drone strikes without a declaration of war from Congress. We've been ignoring the Constitution for a very long time. reply margalabargala 54 minutes agorootparent> The President shouldn't have the legal authority to conduct any drone strikes without a declaration of war from Congress. We've been ignoring the Constitution for a very long time. What part of the Constitution are we ignoring? According to the Constitution, the President is the Commander in Chief of the armed forces. The Constitution does not say that war must be declared for the armed forces to operate. Thus, ordering a drone strike without Congress' input would seem well within the scope of the President's powers. reply hartator 45 minutes agorootparent> The Constitution does not say that war must be declared for the armed forces to operate. It does. Armed forces killing people is a state of war. If not, it's policing then it's under the judiciary authority, not the president. reply margalabargala 14 minutes agorootparent> > The Constitution does not say that war must be declared for the armed forces to operate. > It does. Since the Constitution is a publicly available document, would you please point me to the spot where it says that? Thanks. reply janalsncm 19 minutes agorootparentprevCongress has the power to declare war. But Congress has also passed the War Powers Act which states the President can use the military for short periods of time. Are you claiming the War Powers Act is unconstitutional? reply InTheArena 3 minutes agorootparentThe War Powers Act is exactly the type of law that the Supreme Court has been going after. Bills that give power that was meant for one branch of government to a different branch, or abrogating that power. psunavy03 29 minutes agorootparentprev> Armed forces killing people is a state of war. Real life is rarely this black and white. reply clob 19 minutes agorootparentWhich is why we invented laws and legal systems to codify such things. Did you commit a crime, yes/no? Let's bring it to trial. The circumstances are messy in all real world instances, but the outcome is binary. Refusing to even put a president on trial is more black and white, because it's a clear \"no, not illegal\" without even discussion. reply OgsyedIE 40 minutes agorootparentprevArt. 1 sec. 8 does not say that and no other part of the U.S. constitution mentions war. reply BobaFloutist 24 minutes agorootparentprev>If not, it's policing then it's under the judiciary authority, not the president. Wait, what? Executive branch pretty explicitly encompasses enforcement. The Justice Department is part of the Executive branch. Unless by \"Judiciary authority\" you meant the Justice Department, and not the Judiciary branch of the federal government, and by \"not the president\" you were very specifically discussing the semi-independence the Justice department has from the president. reply nradov 42 minutes agorootparentprevHuh? Federal law enforcement falls under the executive branch, not judicial. Have you even read the Constitution? reply nostrademons 22 minutes agorootparentFederal (and state, and local) law enforcement are not empowered to kill. Their job is to bring defendants to trial, where the judicial system determines innocence or guilt and then determines the sentence. If they were, the whole cop-killers, BLM, excessive use of force issue would not be an issue. The law would just say \"Okay, a cop killed someone, that's their job, get over it.\" You can argue that the criminal justice system is too light on cops, but the fact that the criminal justice system is even involved means that killing people is not part of a cop's official job duties. reply saghm 48 minutes agorootparentprev> What part of the Constitution are we ignoring? There are a number of amendments that could be pretty reasonable argued to give citizens the right not to get killed by drone strikes. reply margalabargala 12 minutes agorootparentCitizens, sure. That's a much weaker claim than what the parent said, which is \"The President should not be able to conduct drone strikes without an act of Congress\". Drone striking (or otherwise killing) citizens without due process seems unconstitutional. Drone striking foreign buildings targets does not have those constitutional protections. reply golergka 45 minutes agorootparentprevKilled or targeted? There's a huge difference between ordering a drone strike on said citizens and ordering strike on legitimate military target that said citizens just happen to be in a vicinity of. reply InitialLastName 38 minutes agorootparentIn the case in question, the former. https://en.wikipedia.org/wiki/Anwar_al-Awlaki reply bergen 39 minutes agorootparentprevIs there? One is actively killing citizens the other is still a reckless disregard for human life in general. reply Buttons840 38 minutes agorootparentprevIs there a difference? Legally? Both would be official acts, right? reply jachee 38 minutes agorootparentprevWhy are actions being taken against military targets outside of a declaration of war? reply saghm 34 minutes agorootparentprevCan you give an example of a time when a \"legitimate military target\" needed to be killed so immediately that a unilateral presidential decision that also killed nearby citizens was warranted? My presumption is that there are very few circumstances where the threat of a military target is so immediate that killing nearby innocent civilians is justified over waiting to try to find a better time to attack the target. If anything, immediate threats that require action are generally threats to the nearby innocent civilians themselves, which would make such decisive action not particularly useful; I don't think anyone would suggest something like blowing up a bank where people are being held hostage, which seems like the closest hypothetical I can think of. reply JamesBarney 24 minutes agorootparentEnemies are not static they can react to our policies. If our policies are never to kill anyone who is near a civilian, enemies would just always be near civilians. reply TylerE 25 minutes agorootparentprevOne might question how that standard applies to what Israel is doing in Gaza. reply Exoristos 34 minutes agorootparentprevThe Constitution forbade the US even to have a standing army. A standing navy was instituted, however. reply rtkwe 56 minutes agorootparentprevThere is the 2001 Authorization for Use of Military Force that a lot of the expanded War on Terror activity are nominally authorized under according to the Executive. Challenging that is up to Congress as afaik there's no standing for a random person to sue. reply BobaFloutist 23 minutes agorootparentAnd Congress notably passed the dang thing and has pointedly refused to come back and limit it or curtail the admittedly expansive interpretations subsequent presidents have made of it, so I very much doubt they're going to ding a given president now. reply rtkwe 11 minutes agorootparentCorrect there's been several attempts to revoke or limit the 2001 AUMF and Congress has decided not to each time. It's pretty clear what is being done under the auspices of it too so failing to address the interpretation becomes a tacit endorsement. Personally I think it's been stretched to breaking but the fix is pretty simple and up to Congress. reply IamLoading 8 minutes agorootparentprevThen, what are american solidiers doing in Jordan dying out there. The reality is that we are sending troops using loopholes, and that's costing such unnecessary complexity. reply haroldp 50 minutes agorootparentprev> We've been ignoring the Constitution for a very long time. Since Thomas Jefferson sent Navy & Marines after the Barbary Pirates without consulting Congress. reply jkic47 41 minutes agorootparentprevI take your point, but quick action is occasionally necessary. Presidents knowing that their actions could be second guessed are more likely to use the minimum force needed to achieve an objective. reply Buttons840 35 minutes agorootparentWe ask every soldier to put their life on the line. But do we dare ask our President to subject himself to our laws? I mean, if Obama went to prison for the drone strikes he ordered, his sacrifice would be less than many common soldiers have made. I want Presidents willing to put their lives on the line and their actions under the law. Is that too much to ask? reply InTheArena 0 minutes agorootparentyes, because throughout history this has been abused. This was the reason that Caesar crossed the Rubicon - his opponents were all queued up to sue him into oblivious on his term as counsel ended. JamesBarney 22 minutes agorootparentprevNot less than the average sacrifice a soldier made. The average soldier would not sign up if they had a significant chance of going to prison afterwards. Yes it's too much to ask every president to significantly risk jail time for being president. reply lenerdenator 1 hour agoparentprevThe problem is, the people who make the call of what is or is not an official duty are often in thrall to the President in one way or another. This is a disaster for rule of law. reply MagicMoonlight 1 hour agorootparentWell obviously… otherwise a politician paid by putin could remove the ability for the president to fire the nuclear weapons. You can’t have randos deciding what the leader can do. The question is, will everyone surrounding a president allow the president to commit mass cullings or nuke California. And the answer is clearly not, outside of delusional fantasy scenarios. Trump wasn’t even allowed to build a wall, and you think his VP would have let him commit genocide? reply lenerdenator 57 minutes agorootparent> You can’t have randos deciding what the leader can do. These aren't randos; they're federal prosecutors that are hired by the sitting President of the United States. Let's say that even if there is a decision that Trump's acts aren't related to his official duties and he somehow gets punished for those charges (I wouldn't hold my breath). How does this not give Biden and future Presidents a way to seriously abuse their power with a hope that they're not held accountable for it? Before it was just a hypothetical. Now we've crossed the Rubicon and established that there are scenarios in which a President can have unlimited power. reply throwaway173738 39 minutes agorootparentIt’s also likely that the project 2025 people will have almost every federal employee declared a political appointee which would give any sitting president the power to fire any prosecutor prosecuting him. This was one of the OMB changes Trump made at the end of his last presidency. reply jkestner 24 minutes agorootparentIt may be a saving grace that little will get done when all the people with institutional knowledge are fired. reply lenerdenator 35 minutes agorootparentprevThere are some already drawing up list of those employees for targeting during a potential second Trump term, using a grant from the Heritage Foundation, IIRC. reply toast0 30 minutes agorootparentprevI mean, this isn't outside the norm. Most government employees and officials either elected, appointed, or hired generally don't have personal liability for discretionary acts of their office under the principle of qualified immunity. reply ffsm8 46 minutes agorootparentprevYour looking at it from today's perspective, but these kinds of rules need to be looked at from the perspective of worst case scenario - don't limit it to the current candidates, think about what a future president in... Say 30yrs is gonna do. The weihmaher republik was a pretty good democracy back in the day. It just gave the leader certain rights, and suddenly Hitler became the dictator, creating Nazi Germany. Everyone needs a limit to their power, otherwise they'll be able to essentially flip the board and declare themselves emperor. Please don't project what I said here onto trump, Biden or Obama. None of them are on that level. It's just possible that a future president is that morally bankrupt, especially if social issues continue to accrue/inequality keeps growing unchecked. reply bonzini 40 minutes agorootparent> Please don't project what I said here onto trump, Biden or Obama Technically the US is in this predicament because one of them wanted his vice president to not name a successor, after mentioning several times during his presidency that he wanted a third term. Just because he's a bad wannabe dictator, it doesn't mean he's not a wannabe dictator. He even said so himself, \"just for one day\". reply golergka 42 minutes agorootparentprev> The weihmaher republik was a pretty good democracy back in the day. No it wasn't. It had armed gangs, both right and left-wing, fighting in the streets, completely failed monetary policy and sky-high level of corruption. The republic just started to kind of getting back to normal for a few years at the end of 1920s, and then was wrecked with Great Depression. reply lenerdenator 37 minutes agorootparent> It had armed gangs, both right and left-wing, fighting in the streets See: 2020 in the US. > completely failed monetary policy See printing off a lot of money to deal with an emergency because the US hasn't had a meaningful conversation about revenue since 1993 when George HW Bush went back on \"read my lips\" > sky-high level of corruption You have people on this court accepting vacations and gifts from people who are wishing to push a certain political viewpoint on the court. They just ruled this was okay, too, by neutering a federal anti-bribery statute. Money is considered protected political speech. There are a lot of parallels between Weimar Germany and the current state of the USA. More than anyone should feel comfortable with. reply enragedcacti 45 minutes agoparentprevIts more like \"immunity for any action within the realm of presidential power, regardless of motive or criminality\". Accepting a bribe in exchange for a pardon would be A-ok on the basis that pardoning is a “conclusive and preclusive” authority of the president. > The opposite holding, where they are liable for everything, would be untenable. Literally no one was arguing for this and there are much more reasonable interpretations of presidential immunity you could compare this one to. reply ameister14 31 minutes agorootparentNo, the act undertaken in exchange for the bribe would be under the immunity umbrella but the act of taking the bribe would not. reply enragedcacti 5 minutes agorootparent> The President’s authority to pardon, in other words, is “conclusive and preclusive,” “disabling the Congress from acting upon the subject.” > Congress cannot act on, and courts cannot examine, the President’s actions on subjects within his “conclusive and preclusive” constitutional authority. It follows that an Act of Congress—either a specific one targeted at the President or a generally applicable one—may not criminalize the President’s actions within his exclusive constitutional power. Neither may the courts adjudicate a criminal prosecution that examines such Presidential actions. We thus conclude that the President is absolutely immune from criminal prosecution for conduct within his exclusive sphere of constitutional authority. Further evidence is that Sotomayors makes the claim in no uncertain terms and that the majority makes zero effort to dispute it. reply ameister14 2 minutes agorootparentYes, I agree - the President is immune for the official act they undertake, the existence of which allows for a the charge of bribery. They are not immune for the non-official act of soliciting or accepting bribes in exchange for an official act, which is the charge of bribery. So if the President took a bribe in order to make someone ambassador to Sweden, they could not be prosecuted for making that person the ambassador, but they could be prosecuted for taking the bribe. runjake 51 minutes agoparentprev> Could Obama be prosecuted for ordering drone strikes that unintentionally killed two Americans? If you're referring to Anwar Al-Awlaki and his son, both US citizens, it was intentional. If you're not referring to this, Obama's already done this without getting charged. Being POTUS is an unenviable job. https://en.wikipedia.org/wiki/Anwar_al-Awlaki Edit: Since people are assuming my views on this topic, I'll say that they're seriously conflicted and I'm not trying to imply any particular viewpoint, only the facts. reply itsdavesanders 31 minutes agorootparentI keep seeing this argument pop up about “should Obama be prosecuted” like it’s some sort of “gotcha liberal!” And as a liberal I think “hell YES he should be prosecuted!” The government shouldn’t just go around killing citizens without due process. I don’t care what letter is by their name. reply JamesBarney 20 minutes agorootparentI think that is 99% true, with the exception of civilians who take up arms against the US. Imagine the civil war if the union couldn't kill confederates. reply vundercind 7 minutes agorootparentprevI hear the same shit a lot. IRL, and in right wing media. “They don’t seem to get that if they can prosecute Trump, we can prosecute Biden and Hillary for [any of several shaky charges]” No, no: we do get that. We just think that’s a good thing. If you have the evidence (you don’t, or we’d have seen, like, any of it at all) then by all means, prosecute the absolute shit out of them! reply chasd00 46 minutes agorootparentprevon a tangent, was the drone operator charged? iirc \"just following orders\" only goes so far. reply haroldp 5 minutes agorootparentNo. It seems unlikely that the drone pilot would have all of that information beyond where the target was, that would be necessary to make an informed decision about participating in an extrajudicial execution of an American citizen. reply squidbeak 56 minutes agoparentprev> The opposite holding, where they are liable for everything, would be untenable Only where their actions were already criminal under the law. This is the problem. No-one should be above the law. reply doctorpangloss 16 minutes agoparentprev> Could Obama be prosecuted for ordering drone strikes that unintentionally killed two Americans? It seems like that world would hamstring the president far too much. I feel like you'd have a different opinion if it were you getting unintentionally killed! reply IIAOPSW 1 hour agoparentprev>I suppose it’s reasonable but the question will now turn to what is actually an official duty. I'm fairly sure there's a full and complete list of these is explicitly in the Constitution. reply throw0101b 1 hour agorootparent> I'm fairly sure there's a full and complete list of these is explicitly in the Constitution. Roberts disagrees in the decision (p. 17): > Distinguishing the President’s official actions from his unofficial ones can be difficult. When the President acts pursuant to “constitutional and statutory authority,” he takes official action to perform the functions of his office. Fitzgerald, 457 U. S., at 757. Determining whether an ac- tion is covered by immunity thus begins with assessing the President’s authority to take that action. > But the breadth of the President’s “discretionary respon- sibilities” under the Constitution and laws of the United States “in a broad variety of areas, many of them highly sensitive,” frequently makes it “difficult to determine which of [his] innumerable ‘functions’ encompassed a particular action.” Id., at 756. And some Presidential conduct—for example, speaking to and on behalf of the American people, see Trump v. Hawaii, 585 U. S. 667, 701 (2018)—certainly can qualify as official even when not obviously connected to a particular constitutional or statutory provision. For those reasons, the immunity we have recognized extends to the “outer perimeter” of the President’s official responsibilities, covering actions so long as they are “not manifestly or pal- pably beyond [his] authority.” Blassingame v. Trump, 87F. 4th 1, 13 (CADC 2023) (internal quotation marks omit- ted); see Fitzgerald, 457 U. S., at 755–756 (noting that we have “refused to draw functional lines finer than history and reason would support”). > In dividing official from unofficial conduct, courts may not inquire into the President’s motives. Such an inquiry would risk exposing even the most obvious instances of of- ficial conduct to judicial examination on the mere allegation of improper purpose, thereby intruding on the Article II in- terests that immunity seeks to protect. Indeed, “[i]t would seriously cripple the proper and effective administration of public affairs as entrusted to the executive branch of the government” if “[i]n exercising the functions of his office,” the President was “under an apprehension that the motives that control his official conduct may, at any time, become the subject of inquiry.” […] > Nor may courts deem an action unofficial merely because it allegedly violates a generally applicable law. For in- stance, when Fitzgerald contended that his dismissal vio- lated various congressional statutes and thus rendered his discharge “outside the outer perimeter of [Nixon’s] duties,” we rejected that contention. 457 U. S., at 756. Otherwise, Presidents would be subject to trial on “every allegation that an action was unlawful,” depriving immunity of its intended effect. Ibid. * https://www.supremecourt.gov/opinions/23pdf/23-939_e2pg.pdf reply lenerdenator 1 hour agorootparentThat last point is bothersome, because if you're looking at \"color of law\" as a defense, when does that end? A good example is that killer cop in Minneapolis, I don't remember his name or care to fill my brain with it. He was acting officially when George Floyd died under his care; he was responding to a 911 call that Floyd was the subject of. The cop was convicted of murder, but let's say that POTUS does something abhorrent (and this is likely to occur now that this is case law) under color of law and someone wanted to charge him or her because of it. Does that get somehow pulled back as it did for the cop? reply klyrs 52 minutes agorootparentTrump's legal team argued, I think in this case, that a president should be immune from prosecution up to and including calling a hit on a political opponent. Which, given all the consternation about Biden persecuting his political opponent, sounded like a bit of an invitation... reply rtkwe 42 minutes agorootparentVery much in the air and down to courts to decide, ordering the military around is certainly within the official acts but doing it on US soil not directly pursuant to external boundaries like the border generally wouldn't be. Ultimately I think the issue with this is we're trying to address a deep systematic issue through the system it's infected. Often those issues can't be solved that way and have to go around the system itself somehow. You're unlikely to be able to sue your way out of a fascist coup when it's successful for example, and similarly disadvantaged when the avenue is carrying water for a failed coup. reply lenerdenator 48 minutes agorootparentprevWell, SCOTUS didn't give POTUS license to do that, but they did give license to do something awful and then have their defense team argue that the act was a part of official duties, with a chance that a judge, possibly appointed by the President, agrees. The thing about Biden, since you brought him up, is he's unlikely to test this in any meaningful way with regards to Trump. Could he theoretically now have the USSS detail in charge of babysitting Trump indefinitely detain him, without consequence? Maybe. But he still has at least some belief in the process and won't do that. This gives me flashbacks to lessons about von Hindenburg. An old guard who had belief in the process going by it even when dealing with someone who has obvious and sneering contempt for that process. reply treis 32 minutes agorootparentprevOn the other hand, if Trump marches on Washington at the head of an army surely Biden can take him out with an airstrike. Extreme scenarios often lead to bad law. reply pessimizer 58 minutes agorootparentprevThat's what happens when the unitary Executive and the Commerce Clause expand to devour the entire constitution. If we wanted to reduce the expansiveness of executive power (the Bush Doctrine), Obama was given the mandate to do it, but instead went out of his way to codify the practices which a lot of people thought had been illegal. To borrow from a similar claim: with the combination of the \"Due Process\" interpretation that backed the al-Awlaki killing (where Holder explained that due process was simply a term designating whatever process that they do, and could entirely occur in one's own head), and this judgement, Trump could have stood in the middle of 5th Avenue and shot somebody and could not be charged for it. So can Biden. ----- edit: Ten Years after the al-Awlaki Killing: A Reckoning for the United States’ Drones Wars Awaits https://mwi.westpoint.edu/ten-years-after-the-al-awlaki-kill... > From the Magna Carta to the US Constitution, citizens have sought ways to protect themselves from the arbitrary exercise of sovereign power. The drone strike on al-Awlaki reversed this historical process with an executive process. From the so-called “Terror Tuesday” or “targeting Tuesday” meetings where President Obama personally approved targets for drone strikes to the drafting of the legal logic justifying the extrajudicial killing of an American citizen, the strike on al-Awlaki was the result of decision making within the executive branch. Completely absent from the proceedings was the judiciary, which acts as a crucial buffer and neutral arbiter between the citizen and the executive. > In its own defense, the Obama administration argued that due process was not the same thing as judicial process and presented the test that it used to justify the targeted killing. While some observers have emphasized the narrowness of the legal standard, it was crafted specifically to target al-Awlaki, therefore reinforcing just how discretionary this exercise of executive power was. Furthermore, it was conceived of and adjudged constitutionally sufficient by attorneys who had previously opposed executive overreach during the Bush administration. [It's interesting that the al-Awlaki killing was over speech, too. So while the executive branch is not allowed to exercise prior restraint over speech, Presidents are now constitutionally immunized for murdering people to keep them from speaking.] reply brightball 13 minutes agoparentprevPresident's can still be impeached as a result of their official acts. It seems that is intended to be the outlet for prosecuting the Executive Branch. Am I wrong there? reply AdamJacobMuller 2 minutes agoparentprev> Could Obama be prosecuted for ordering drone strikes that unintentionally killed two Americans? How about the one which intentionally killed an American child (16)? Part of me wouldn't mind seeing that, but, I would have to agree that was an official act, regardless of how despicable it was. reply WalterBright 18 minutes agoparentprevObama ordered Bin Laden killed. Should he be prosecuted for that? Remember that the President can still be impeached for \"high crimes and misdemeanors\". reply squidbeak 11 minutes agorootparentI'm unclear why ordering the death of one of his nation's worst foes, whose network was still at war with the US, could ever be illegal? reply giantg2 58 minutes agoparentprevThis is really just the formal ruling that qualified immunity is applied to the presidency. Just like with other qualified immunity, it'll be messy to figure out what should be covered an what is not. reply haroldp 56 minutes agoparentprevObama ordered drone strikes that intentionally killed at least two Americans. One he copped to, one he denied was intentional. https://en.wikipedia.org/wiki/Anwar_al-Awlaki https://en.wikipedia.org/wiki/Killing_of_Abdulrahman_al-Awla... To my mind, that is exactly the sort of thing I'd like to be hamstrung by a fear of prosecution. (Trump ordered drone a drone strike that also killed Abdulrahman's eight-year-old American sister.) reply klyrs 38 minutes agorootparentYeah, this pearl-clutching about presidents being tried for criminal acts is loathsome. I think quite a lot of people believe that quite a few past presidents are guilty of war crimes, and that there should be real consequences for those crimes. My knowledge of history in this regard more or less begins with the Vietnam War: since 1955, I am not aware of a single president who hasn't ordered acts that are crimes against humanity. And throughout that time period, our crimes against humanity have brought ruin to much of the world. Perhaps presidents should fear the law. reply lupusreal 55 minutes agoparentprev> Could Obama be prosecuted for ordering drone strikes that unintentionally killed two Americans? Unintentionally? American citizens were the intended targets of some of his drone strikes. They weren't in warzones either, it was effectively murder. reply ein0p 32 minutes agoparentprevWhat do you mean, “unintentionally”? Obama deliberately and knowingly ordered an extrajudicial killing of a US citizen. reply kibwen 1 hour ago [flagged]parentprevnext [3 more] > the question will now turn to what is actually an official duty Fortunately, the past rulings of this court have made the jurisprudence abundantly clear: If Biden orders Trump to be assassinated, that's unofficial, and he can be prosecuted. If Trump orders Biden to be assassinated, that's official, and he has complete immunity. Easy, right? reply dang 1 hour agorootparentPlease don't take HN threads into political flamewar hell, or any other flamewar hell. That's what we're trying to avoid here. https://news.ycombinator.com/newsguidelines.html reply TylerE 51 minutes agorootparentThis feels likely highly selected be encorcement. Either let people vent or nuke the whole thread reply w10-1 36 minutes agoprevThe essence, from the majority opinion https://www.supremecourt.gov/opinions/23pdf/23-939_e2pg.pdf > nature of Presidential power entitles a former President to absolute immunity from criminal prosecution for actions within his conclusive and preclusive constitutional authority. > And he is entitled to at least presumptive immunity from prosecution for all his official acts. > Testimony or private records of the President or his advisers probing such conduct may not be admitted as evidence at trial > The Constitution does not tolerate such impediments to “the effective functioning of government” [as when] the possibility of an extended proceeding alone may render [the President] “unduly cautious in the discharge of his official duties.” > The immunity the Court has recognized therefore extends to the “outer perimeter” of the President’s official responsibilities, covering actions so long as they are “not manifestly or palpably beyond [his] authority.” > In dividing official from unofficial conduct, courts may not inquire into the President’s motives. > Nor may courts deem an action unofficial merely because it allegedly violates a generally applicable law > Enduring separation of powers principles guide our decision in this case Supreme Court history has no broader grant of immunity based on principles less definitive. reply janalsncm 9 minutes agoprevMaybe we can stop acting like the Founding Fathers were political geniuses. They created a system where the only real recourse against a president is political, and a political system where political recourse is essentially impossible. A two party system is the logical conclusion of a first past the post voting system, which they have created. It is a bug, and fixing it is also effectively impossible. reply injidup 32 minutes agoprevPerhaps the simple solution is that all presidents should serve 15 years in jail after serving their term. Then only extremely socially minded people would dare to do the job. There was a similar sci fi story I read. At the end of a war the rule was that all allied ( not enemy ) generals would be executed. The idea was that war was such a horrible concept that to lead one would require extreme sacrifice and social consciousness on the part of the leaders. War was legal and to be fought without limit however on conclusion all leaders would be put to death. I don't remember the author or the story name. reply lucianbr 7 minutes agoparentOne of the alien species in https://en.wikipedia.org/wiki/The_Garden_of_Rama has this rule. The octospiders. reply dh2022 6 minutes agoparentprevRe: war analogy. Executing all allied generals at the end of the war would enable opponents who elevate winning generals at the end of the war. Guess which side would win? reply imoverclocked 15 minutes agoparentprev… so wars, one started, may never end. reply ajuc 10 minutes agoparentprevIf there's no downside you just do your worst. Look at it from the perspective of the president near the end of their term. reply leotravis10 43 minutes agoprevFolks, the blueprint for a American dictatorship has been created and you'll be a fool and a idiot to think otherwise. reply koonsolo 30 minutes agoparentIf Trump gets elected, he will never give away his power easily. reply citizen_friend 24 minutes agoparentprevRidiculous alarmism. Try reading the majority opinion first. reply ChildOfEru 1 minute agorootparentIt has been 3 years and 6 months since Trumps election interference with essentially no progress being made in the courts. If and when Trump gets elected how long do you think it will take the Supreme Court to differentiate between an 'official' act and a 'non-official' act when Trump acts illegally in 2025? Throw in the recent ruling increasing the difficulty of proving bribery and things are looking grim ( IMHO ). reply vundercind 2 minutes agorootparentprevWe had an attempted coup already. That this case was directly related to. I don’t even mean the very public storming of Congress during the certification of electors, but multiple attempts by the then-President to subvert the election process. The guy who did it’s probably about to be President again and just got a lot more legal cover. Folks need to read up on how democracies fail. Shit’s getting real iffy here. reply leotravis10 19 minutes agorootparentprevRead this and get back to me: https://www.vox.com/scotus/358292/supreme-court-trump-immuni... reply citizen_friend 6 minutes agorootparentNo I do not buy the vox interpretation. This is a completely reasonable measure. We can’t stop the country every time we think the president broke a law and have a trial. He is leading armies, etc. Impeachment is the process to stop a bad president. True for Biden, Obama, trump, etc. reply whatever1 34 minutes agoprevI understand why during their presidency the president needs to be immune (so that they can focus on their executive duties instead of spending their day in the court). But AFTER the end of their (last) term why not be held accountable for their actions? reply afavour 1 hour agoprevI'm dismayed by this ruling but I'm curious: can someone defend it? I'm able to understand the counter-perspectives to my own on many hot-button issues (2nd amendment, abortion bans) but this one seems very nakedly bad. But maybe I'm just not seeing the counterpoint? reply pyuser583 1 hour agoparentSure. The Founders envisioned an extremely weak criminal justice system, especially for \"their class of people.\" Defendants were given extremely strong protections, and convictions were the exception, not the rule. The Founders were more concerned about facing a duel than a criminal conviction. So they added other mechanisms for presidential accountability: impeachment, elections, and the weakness of the office. These other mechanisms have become weaker and weaker, while the criminal justice system has become stronger and stronger. Impeachment's happen, but not Senate convictions. The political parties have created a duopoloy on power which allow them to run weak candidates. Congress is less and less willing to hold presidents of their own party accountable. Dueling is prohibited not just criminally, but constitionally in most states. At the same time the criminal justice system is becoming more and more powerful. Convictions are in the high 90%. Juries are very weak and at the mercy of powerful prosecutors. The Constitution simply didn't envision a situation where the criminal justice system is more likley to hold someone accountable than an election or Congress. Impeachment, elections, and duels no longer deter bad conduct. Convictions do. So we have an edge case: a system that can only hold an ex-President accountable via a criminal charge. Edge cases are weird. They create \"sometimes it works and sometimes it doesn't\" situations. And that's where we are now. reply chadash 54 minutes agorootparent> Convictions are in the high 90%. Juries are very weak and at the mercy of powerful prosecutors. This is a bit misleading. DAs have latitude about what to prosecute and if they don't think they can win, they dont have to bring it to court. reply BobaFloutist 13 minutes agorootparentAnd even if they do think they can probably win, they'll usually leverage that into a plea deal. reply alistairSH 1 hour agoparentprevThis is like qualified immunity (usually for police), but applied to POTUS. You want public servants to be able to do their jobs without fear of time-wasting litigation. But, if you grant blanket immunity, the poilice (or POTUS) are free to do whatever they want. We're quite clearly tilted way over into \"do what they want\" territory. reply Clubber 12 minutes agorootparentI believe qualified immunity only protects from direct civil litigation but not criminal. reply alistairSH 5 minutes agorootparentYep, it's a rough analogy, not a perfect match. And, given the state of policing and justice in the US, civil litigation is often the only way to get any relief for over-zealous policing. And QI makes that bar even higher than it should be. reply Waterluvian 37 minutes agorootparentprevI think that’s the argument, yes. I think it’s facially a bad one when you consider all the examples of functional police that lack QI. Makes me skeptical that it’s a good faith argument. reply alistairSH 17 minutes agorootparent100% agree. AFAIK, QI no longer exists in England (not sure about Scotland or N.I.). And Germany never had it (civil cases for damages would be against the state, not the state representative). reply dagmx 1 hour agoparentprevI can’t defend the ruling based on the actual effects, because it puts someone above the law. However one could argue that this is the logical extension of qualified immunity for police officers which is precedent the court has already set. To be clear though, I think qualified immunity of any sort shouldn’t be allowed. It’s a subversion of any kind of fair justice. reply zeroonetwothree 1 hour agoparentprevIf presidents could be prosecuted for their official acts then the next time the other party takes over they will just immediately find various crimes their predecessor “committed” (there are probably 10s of 1000s of them). reply kibwen 1 hour agorootparentIf presidents break the law, they should be prosecuted. I don't care who you are or what party you're with. reply jjk166 38 minutes agorootparentThe issue is that presidents can be prosecuted even if they likely didn't break the law. It would be possible to just keep throwing bullshit charges at a former president either until one sticks or until their resources to fight off an unending string of legal battles is exhausted. It does make sense that the bar to prosecute someone should be higher when they are particularly likely to be the subject of malicious prosecution and the fear of such would interfere with them carrying out their duties. The issue is that blanket immunity for official acts is not just raising the bar, it's launching it into orbit. Not only can a president not be prosecuted for questionable decisions or on scant evidence, they can not be prosecuted when their crimes are heinous and obvious so long as it is plausibly within their domain. reply mminer237 16 minutes agorootparentYou can still prosecute someone for whatever the prosecutor wants. This doesn't change that. If someone is acting in bad faith, they can do so regardless of the law. reply mrkeen 4 minutes agorootparentprev> It would be possible to just keep throwing bullshit charges at a former president either until one sticks or until their resources to fight off an unending string of legal battles is exhausted. I can't make up my mind if this is \"movie plot threat\" or not. Has it happened to other important figures before? And can the prosecution can be punished for this kind of behaviour? Then again, I guess if you can shop around for (politically-appointed) judges, it wouldn't be too hard to find a judge to indulge some bullshit prosecution. reply readthenotes1 1 hour agorootparentprevFirst, I agree . Second, the first step in that is the impeachment process. reply jkaplowitz 53 minutes agorootparentImpeachment is not required in order to allow prosecuting a president, and even Chief Justice’s majority opinion which granted presidents significant immunity explicitly rejected the idea that impeachment is such a prerequisite. Impeachment is only a prerequisite to the Senate possibly convicting in the political rather than criminal trial and removing the person from office, and then possibly disqualifying them from future federal office. It has no bearing on whatever criminal procedures are not blocked by immunity. reply vundercind 50 minutes agorootparentprevIt is super-duper clear that’s not intended by the authors of the constitution, judging from their writings and the records of the debate over the constitution, and from the very limited relevant text in the constitution itself. This is laid out clearly in the dissent, complete with references for further reading. Meanwhile the majority’s argument is “lack of immunity (which has, so far, not existed!!!) would make the president too timid, so we’re adding immunity”. reply mrkeen 3 minutes agorootparentprevI think this was tried somewhat recently. The counter-argument was \"actually let's not impeach. Let's just wait for the next election and see what the voters think. Voting is the real impeachment.\" reply falcolas 38 minutes agorootparentprevThe impeachment process only applies to a sitting president. Once they're out of office, the threat provided by an impeachment is gone. More specifically, the role of the house impeachment and senate hearing is removal someone from office. That's it. There's no potential for punitive action (fines, jail, etc). reply toast0 18 minutes agorootparent> The impeachment process only applies to a sitting president. Once they're out of office, the threat provided by an impeachment is gone. > More specifically, the role of the house impeachment and senate hearing is removal someone from office. That's it. There's no potential for punitive action (fines, jail, etc). Judgement in impeachment can extend to \"disqualification to hold and enjoy any Office of honor, Trust or Profit under the United States\" [1] But, judgement in impeachment is political, and if you could get enough votes to impeach a former President in order to disqualify them from running again, it seems pretty unlikely that they'd be able to get nominated and elected in a future election; so it's not a big threat IMHO; at least assuming a two-thirds majority is required to remove, then a majority to disqualify. The constitution is not clear that removal from office and disqualification are linked, although in practice, disqualification has happened only after a removal, and the Senate has determined simple majority for removal is sufficient. [2] So, it might be possible to do a disqualification as a simple majority, without a removal. [1] https://constitution.congress.gov/constitution/article-1/#ar... [2] https://law.justia.com/constitution/us/article-2/49-judgment... reply klyrs 30 minutes agorootparentprev> the first step in that is the impeachment process Only, the requirement of a supermajority means that a minority can prevent a president from being convicted of blatantly criminal acts. This system is demonstrably weak against corruption, to which the USSC has given its full-throated approval. reply zarzavat 40 minutes agorootparentprevIndeed. The US constitution envisages impeachment by Congress as the first step to a criminal conviction. Trump was acquitted by the Senate for his conduct on Jan 6. It’s OK to disagree with the acquittal but it did in fact happen. Ultimately it’s really hard to design a constitution that is effective in opposing half of the population. You can’t solve mass social issues with laws. reply ApolloFortyNine 6 minutes agorootparentprevPeople forget that Obama actually did some questionable things too. He greenlit drone strikes that killed at least 2 Americans, not to mention civilian casualties. And he was the president that the 'holding camps' at the border (not sure what to call them) were started. No one (that I know of) seriously thought he should go to prison for those acts, but honestly the argument seems pretty easy to be made without some sort of immunity. reply afavour 1 hour agorootparentprevBut isn't that the status quo? Why didn't that happen when Obama left office? It's not like the Republicans were lacking a desire for retribution against him. \"there are probably 10s of 1000s of them\" also feels a little lacking to me. Do we have concrete examples? reply throw0101b 58 minutes agorootparent> Why didn't that happen when Obama left office? What illegal acts did Obama do, while POTUS, yet outside of his 'official duties', what would warrant prosecution? Remember the context for this decision: Trump's participation in the events of January 6: * https://en.wikipedia.org/wiki/January_6_United_States_Capito... You know, the insurrection in which people have been found guilty of seditious action: * https://en.wikipedia.org/wiki/Criminal_proceedings_in_the_Ja... Were his January 6 actions part of his official duties? reply lupusreal 50 minutes agorootparentprevObama didn't prosecute Bush, so the Republicans didn't try to prosecute Obama. Professional courtesy. reply chasd00 1 hour agorootparentprevi think it was just a \"gentleman's agreement\" to not prosecute former presidents or rivals. I remember in the 2016 debates when Trump said he would appoint a special prosecutor to look into Hillary her eyes got real big about how ignorant Trump was to the way things are and have been. That's one of the downsides to a political outsider a lot of formally unasked questions start needing answers. reply throw0101b 54 minutes agorootparent> i think it was just a \"gentleman's agreement\" to not prosecute former presidents or rivals. How many former presidents or rivals tried to prevent the transfer of power? * https://en.wikipedia.org/wiki/Self-coup There's a specific reason why Trump is being investigated. We're not talking about jay-walking here. And there's also intent with action, using yet another case: Biden had classified documents in his home residence, but he handed them back to the government with minimal fuss. Trump had classified documents and moved them around even after being subpoenaed to return them: * https://www.theguardian.com/us-news/2022/oct/12/donald-trump... * https://apnews.com/article/trump-justice-department-indictme... reply sickofparadox 1 hour agorootparentprevI would imagine that Obama is quite happy with his presidential immunity for ordering the extrajudicial killing of an American citizen via drone strike[1]. [1] https://mwi.westpoint.edu/ten-years-after-the-al-awlaki-kill... reply afavour 1 hour agorootparentYes I'm quite sure Obama is happy with it. Should the rest of us be? Would a court case over the legality of such an action not be a positive thing? reply ceejayoz 52 minutes agorootparentprevI mean, is that the out for Biden? A drone strike on Mar-a-Lago as an \"official act\" for which he enjoys absolute immunity? reply jshier 1 hour agorootparentprevCorrupt admins will already do that, so giving blanket immunity doesn't actually help anyone. All they need to then corruptly prosecute anyone is a court to agree that some act was \"unofficial\". reply pessimizer 55 minutes agorootparentIt has never happened before now. It isn't because past presidents have never been criminals, it was convention. It's a way for your country not to turn into Haiti or Zimbabwe. reply jkaplowitz 51 minutes agorootparentNixon was very probably criminal, but Ford’s pardon prevented a prosecution without having to settle the question of whether Nixon would have been immune in the absence of a pardon. reply danielmarkbruce 56 minutes agoparentprevIt's worth reading the actual ruling rather than the reporting on it, which is downright awful. reply squidbeak 46 minutes agorootparentHere's a sample of Sotomayor's dissent: > The President of the United States is the most powerful person in the country, and possibly the world. When he uses his official powers in any way, under the majority’s reasoning, he now will be insulated from criminal prosecution. Orders the Navy’s Seal Team 6 to assassinate a political rival? Immune. Organizes a military coup to hold onto power? Immune. Takes a bribe in exchange for a pardon? Immune. Immune, immune, immune. > Let the President violate the law, let him exploit the trappings of his office for personal gain, let him use his official power for evil ends. Because if he knew that he may one day face liability for breaking the law, he might not be as bold and fearless as we would like him to be. That is the majority’s message today. reply danielmarkbruce 33 minutes agorootparentYep, and it's speculation at best. She's making statements she knows to be nonsense - like, those assume the court will find anything to be an official act, which is nonsensical. Read the whole thing rather than the hot takes. https://prod-i.a.dj.com/public/resources/documents/SCOTUSTRU... reply ameister14 9 minutes agorootparentWell, to be fair, the first two examples she uses are very likely official acts and the third requires an official act to be bribery in the first place. reply fzeroracer 27 minutes agorootparentprevDo you really want this kind of thing to be speculated to be possible? The court should be putting the squash on this kind of thing full stop, not giving any wriggle room at all. The fact that you have to argue that a supreme court justice is speculating on the downstream effects of a monstrous decision should tell you something is very wrong. Step back and think for one second. reply danielmarkbruce 1 minute agorootparentIt's been speculated on for ages. Humans tend to deal with things as they come up. The entire system is built that way. We don't write legislation that deals with every single possibility, we deal with it in the courts as it happens. It seems to basically work. bbddg 19 minutes agorootparentprevA president who is willing to do those things, and has a military willing to carry those orders out, isn't likely to be stopped by the court telling them it's illegal. reply ahmeneeroe-v2 38 minutes agoparentprevDo you mean beyond what's written in the decision itself? Do you have a specific criticism of what was written by the justices themselves? reply dustincoates 1 hour agoparentprevSure. You want a President to be able to carry out the roles of the office without concern that his or her political opponents will use the courts to try to punish those actions. There are reasonable disagreements on where Presidential authority begins or ends on many topics, and you want the limits to be either through separation of powers (e.g., the Judicial Branch can bring an end to actions, the Legislative can impeach and remove the President) or through the ballot box. This does not mean, including from the majority opinion, that anything the President does is immune from challenges. If the President directs a cover-up for his campaign (Nixon) or directs a Governor to find enough ballots for him to win or directs \"alternate electors\" via fraud (Trump), this is not an official action. Trump's lawyers admitted as much in the oral arguments. reply mlinhares 1 hour agorootparentThe court never defined what are or aren’t official acts and it should be straight forward for the president to say that spying on his enemies, that are also the enemies of the state/government, is an official act, as they are protecting the constitution from those that would do it harm. Presidents are effectively kings now, won’t be long until one declares himself one for good. reply blue_dragon 8 minutes agorootparent> it should be straight forward for the president to say that spying on his enemies, that are also the enemies of the state/government, is an official act This would be challenged in court by the victims of the President's spying, and the court would ultimately decide whether or not the spying constituted an \"official act\". reply afavour 1 hour agorootparentprev> Sure. You want a President to be able to carry out the roles of the office without concern that his or her political opponents will use the courts to try to punish those actions. Hrm. Surely the implication you're outlining here is that the President will be breaking the law while carrying out \"the roles of the office\"? Is that not a concern? reply dustincoates 1 hour agorootparentThe next sentence addresses that—there are reasonable disagreements to the extent that some actions are legal or not. reply afavour 1 hour agorootparentWhy is a court the wrong place to settle those disagreements? That is to say, why is it preferable for a President to face the possibility of impeachment by Congress over a jury of everyday citizens? Seems considerably easier to rig the former than the latter. reply dustincoates 1 hour agorootparentLet's take Biden's student loan forgiveness as an example. Some say that he overstepped his authority. The courts should decide if that act is legally permitted, but it would be a tough pull to think that he should be personally responsible for that, including potential jail time. Now egregious cases that fall outside of the reasonable expectations of the role of the President. Sure, those should go to the courts. But I don't think this case prevents that. Most of what Trump did after the election, IMO, was clearly as a candidate, not a President. reply steadfastbeef 57 minutes agorootparentprevBecause the people are stupid? reply afavour 46 minutes agorootparentWhy do we even let them vote at all? Hello, slippery slope. reply lesuorac 1 hour agorootparentprevW.r.t. SCOTUS's ruling, you'd claim the main problem is that \"Both the District Court and the D. C. Circuit declined to decide whether the indicted conduct involved official acts.\". If the lower courts had claimed the conduct wasn't official when they denied the motion to dismiss then SCOTUS wouldn't have vacated the lower rulings? reply mlinhares 1 hour agoparentprevA common thing in empires is that the behavior that was perpetrated in the colonies eventually comes back to the mainland to be applied to the, previously, sheltered population. Out of the US, US presidents have always had effective immunity from any consequences, committing war crimes, disrespecting local laws and eschewing even the UN. Kinda like an expected consequence of everything else. reply legitster 28 minutes agoparentprevThe ruling says that 3 of the 4 indictments against Trump can proceed so long as prosecutors make a case that the President was acting outside of his duties. A president being incompetent or immoral in his line of duty is an issue for voters or congress to decide on. But a White House bogged down in lawsuits or petty criminal charges would cease to function. reply nprateem 13 minutes agorootparentWhich they can't because official duties haven't been defined. Trump can now continually appeal his actions were official, delaying charges until after November when, if he wins the election, he can instruct the DoJ to drop the case. reply ninininino 1 hour agoparentprevI believe the argument conservatives have been trying to make that aligns with the court's ruling is mostly about some very specific fear that an incoming or current president could prosecute former presidents and therefore crush dissent. So basically you have the right scared of former presidents being unjustly targeted in a way that threatens the democratic process, and then you have the left scared that the immunity will itself threaten the democratic process/enable dictators and corruption. Unjust use of prosecution as a political weapon vs just plain corruption being shielded. It sure seems to me like it would be better if these matters could be handled on a case by case basis rather than in some black and white \"former presidents can\" vs \"former presidents can't\" be prosecuted way, but perhaps that is what will end up happening, not a legal expert. reply dustincoates 1 hour agorootparent> if these matters could be handled on a case by case basis rather than in some black and white \"former presidents can\" vs \"former presidents can't\" be prosecuted way That's largely what's happening here. The President _can_ be prosecuted for things that fall outside of the official role as President. This is not a blanket immunity. reply haswell 56 minutes agorootparentDefining what is considered “official” and more importantly, what is absolutely not official is now at issue and where things get sticky. Certainly a president carrying out actions that call for prosecution would make the claim that those actions were either official or required to carry out the official duties of the office. Any hope of justice now depends entirely on being able to draw that line and agree about where it’s drawn. reply coffeemug 55 minutes agoparentprevYes. Ever since Bill Clinton (and probably before that, I was too young) the President and a non-trivial number of presidential candidates were either under an investigation of some sort, or a threat of such an investigation. Obviously Bill, Hillary, constant threats of investigation of George W Bush and Obama, special counsel investigating Biden, and all the Trump cases. Notably nothing ever comes out of these. This is ridiculous! It's blatantly political and both parties are guilty of this. The justice system is meant to hold people accountable for breaking the law, not as an additional political mechanism for checks and balances. I haven't looked into the case and don't know the legal precedent SCOTUS used for this decision, but from a consequentialist standpoint this seems to me an obviously good outcome. reply jauntywundrkind 30 minutes agorootparentYeah but the Republicans keep doing war crimes & the Democrats... Got a blow job? Passed Romneycare/Obamacare? One of these sets of prosecution is not like the others. reply MagicMoonlight 56 minutes agoparentprevYou have a whole congress and senate to hold them accountable. If they haven’t, then why should a random prosecutor in a city court be able to issue an arrest warrant and prosecute the leader of the country? Without immunity you’ll get the kind of shit they’ve done with trump, but against sitting presidents. Imagine if Obama had been arrested every time he entered Texas because the locals just feel like prosecuting him to send a message. reply heleninboodler 2 minutes agoprevCan't help but wondering if Joe is contemplating Seal Team Six's summer plans right about now. reply suid 2 minutes agoprevTime for \"Will nobody rid me of this troublesome priest?\" reply mperham 1 hour agoprevWould this ruling make Nixon’s actions in Watergate legal too? reply kibwen 1 hour agoparentIt would make it illegal to use the tapes as evidence against him. So it doesn't matter if it makes it legal or not, because it makes the illegality impossible to prove in a court of law by denying evidence to the prosecution. reply legitster 22 minutes agorootparentThis is not an accurate reading. In the decision, gathering of evidence is not protected by immunity. People are jumping on language in the decision that says discussions or probings about the criminal nature of the crime would not be admissible. So Nixon ordering Watergate would still be admissible - Nixon discussing with his legal team or cabinet after Watergate broke would not be. reply tuna74 1 hour agorootparentprevCould you explain this a bit further? reply DeRock 55 minutes agorootparentFrom the official ruling on https://www.supremecourt.gov/opinions/23pdf/23-939_e2pg.pdf: > (3) Presidents cannot be indicted based on conduct for which they are immune from prosecution. On remand, the District Court must carefully analyze the indictment’s remaining allegations to determine whether they too involve conduct for which a President must be im- mune from prosecution. And the parties and the District Court must ensure that sufficient allegations support the indictment’s charges without such conduct. Testimony or private records of the President or his advisers probing such conduct may not be admitted as evidence at trial. reply vundercind 34 minutes agorootparentE.v.e.r.y.o.n.e should just go read the decision and dissents. It’s not that long or hard to follow. Then probably go read all of the Federalist Papers, if they haven’t already, or one of the Constitutional Debate readers that are readily available and may include much of Federalist. The primary source here is plenty accessible, and free. And alarming. reply legitster 19 minutes agorootparentprev> And the prosecutor may admit evidence of what the President allegedly demanded, received, accepted, or agreed to receive or accept in return for being influenced in the performance of the act. See 18 U. S. C. §201(b)(2). What the prosecutor may not do, however, is admit testimony or private records of the President or his advisers probing the official act itself. The argument is not that all recordings are off limits, but if the President asks his lawyer \"what is a bribe?\" that can't be used as evidence he took a bribe. reply stefan_ 46 minutes agorootparentprev\"If official conduct for which the President is immune may be scrutinized to help secure his conviction, even on charges that purport to be based only on his unofficial conduct, the “intended effect” of immunity would be defeated.\" reply dustincoates 1 hour agoparentprev> Roberts explained in his 43-page ruling, presidents have absolute immunity for their official acts when those acts relate to the core powers granted to them by the Constitution – for example, the power to issue pardons, veto legislation, recognize ambassadors, and make appointments. Most likely not. Watergate was a result of an election campaign, not official acts as President. reply camel_Snake 21 minutes agorootparentI'm not so sure about that. From this ruling: > Testimony or private records of the President or his advisers probing such conduct may not be admitted as evidence at trial. And the 'smoking gun' implicating Nixon: > Nixon then released the tapes six days later. On one tape was the so-called \"smoking gun,\" showing that six days after the break-in Nixon had tried to use the CIA to block the FBI investigation of the burglary. IANAL, but my understanding is under this ruling those tapes would have never been made permissible evidence in court. Giving orders to the CIA is certainly an official act, as much as granting pardons is, and this court has established the examination of said motives is out-of-scope: > In dividing official from unofficial conduct, courts may not inquire into the President’s motives. Such a “highly intrusive” inquiry would risk exposing even the most obvious instances of official conduct to judicial examination on the mere allegation of improper purpose. Fitzgerald, 457 U. S., at 756. Nor may courts deem an action unofficial merely because it allegedly violates a generally applicable law. reply jshier 1 hour agorootparentprevYeah, Nixon should've ordered his secretary to do it, as all executive communications are now protected against criminal investigation. reply newprint 56 minutes agoparentprevI'm actually curious about this as well. Would like to hear some opinion. reply citizen_friend 22 minutes agoparentprevWell you can always impeach a president. reply cbxyp 7 minutes agoprevNixon confirmed not a Crook! \"When a president does it, it's not illegal\" - Richard Nixon (1979) reply BadHumans 1 minute agoprevIn all good faith, I don't see how people don't see the Nazi Germany happening in front of them. How far are people going to push goal post saying \"they aren't going to go that far\" and then they do before they realize what is happening? reply yongjik 43 minutes agoprevIANAL so could someone explain to me - does this ruling apply to the \"porn actress hush money\" trial or is it a separate issue? (I'd like to think that there's no way it's an \"official act\" of a president, but again, IANAL.) reply sjs382 6 minutes agoparentKind of. From CNN: https://www.cnn.com/politics/live-news/trump-immunity-suprem... > Donald Trump’s legal team will likely use Monday’s SCOTUS opinion as they challenge the New York hush money criminal verdict itself on appeal, a source familiar with their thinking tells CNN. > Trump’s team thinks the SCOTUS option could be used to challenge portions of Hope Hicks testimony as well as some of the tweets entered in as evidence, according to the source familiar. > CNN earlier reported the Trump team sees the opinion as “a major victory” because in addition to using it to try to get charges tossed, they can also use this opinion to get evidence related to official acts tossed in all cases — not just federal — which can hurt prosecutors’ ability to prove what charges are left. reply legitster 17 minutes agoparentprevThis is relating to 4 counts of election interference. The court is granting him blanket immunity on one count, and the rest the prosecutors need to clarify that the crimes were outside of the duty of his office. reply kadoban 36 minutes agoparentprevNo, Trump was a candidate, not President at the time of most (all?) of those crimes. They also yeah probably can't be considered official acts even by _this_ Court. reply ahmeneeroe-v2 41 minutes agoparentprevHonestly the decision is written in a fairly accessible style. You could try to read it reply danielmarkbruce 28 minutes agorootparenthttps://prod-i.a.dj.com/public/resources/documents/SCOTUSTRU... reply 71 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Supreme Court extended the delay in Donald Trump's criminal case regarding the 2020 election, reducing the likelihood of a trial before the November election.",
      "In a 6-3 ruling, the court's conservative majority granted former presidents broad immunity from prosecution for official acts, complicating the prosecution's case and requiring further analysis at the trial court level.",
      "The decision underscores the court's significant influence on the upcoming election, with Chief Justice John Roberts emphasizing immunity for official acts and Justice Sonia Sotomayor dissenting, arguing it undermines the principle that no one is above the law."
    ],
    "commentSummary": [
      "The Supreme Court has ruled that ex-presidents have immunity for official acts, sparking debate over potential abuses of power versus the need for presidential protection.",
      "The ruling specifies that immunity applies to actions within constitutional authority but not to unofficial acts, raising concerns about accountability for serious crimes.",
      "Critics, including Justice Sotomayor, worry this decision could impact ongoing and future legal cases involving former presidents."
    ],
    "points": 226,
    "commentCount": 325,
    "retryCount": 0,
    "time": 1719854392
  },
  {
    "id": 40840396,
    "title": "Convolutions, Fast Fourier Transform and polynomials (2022)",
    "originLink": "https://www.alvarorevuelta.com/posts/fft-polynomials",
    "originBody": "Alvaro Revuelta Posts Github ← BACK Convolutions, Fast Fourier Transform and Polynomials You may remember from high school what a polynomial is. If so, you may also remember how to multiply two of them. But what if I told you that the method you were taught is slow as F? In this post we will connect polynomials with the Fourier Transform and convolutions, and show you how to multiply polynomials with $O(nlogn)$ complexity instead of $O(n^2)$, being the latter the method that’s taught in high school. Polynomials: A Quick Recap Let's do a quick recap on what polynomials are. Formally defined, a polynomial $P(x)$ is a sum of terms where each one is an indeterminate variable $x$ with some exponent $k$ multiplied by a coefficient $a_k$. \\[P(x) = \\sum_{k=0}^{n} a_k x^k\\] This is an example of a polynomial. Note that we say it has a degree $2$ since it is its maximum exponent. It can be expressed as a vector like [5, 2, 9] or [9, 2, 5] depending on the notation we use. \\[P(x) = 5x^2 + 2x + 9\\] Let $P(x)$ and $Q(x)$ be two polynomials, we can perform different operations with them, like adding or subtracting them. In both cases, the result is trivially computed by summing (or subtracting) each term individually. In Python, we can compute the result as follows. Note that this works with p and q having the same degree. You can use zip_longest for different degrees. # a + b [a + b for a, b in zip(p, q)] # a - b [a - b for a, b in zip(p, q)] On the other hand, we can also multiply them. And here is where things get interesting. Multiplying $Y(x) = P(x) * Q(x)$ is a bit more complex than adding or subtracting. Let’s see an example: \\[\\begin{align*} Y(x) & = (2x^2 + 3x + 4) \\times (5x^2 + 6x + 7) \\\\ & = (2x^2 \\times 5x^2) + (2x^2 \\times 6x) + (2x^2 \\times 7) \\\\ & \\quad + (3x \\times 5x^2) + (3x \\times 6x) + (3x \\times 7) \\\\ & \\quad + (4 \\times 5x^2) + (4 \\times 6x) + (4 \\times 7) \\\\ & = 10x^4 + 12x^3 + 14x^2 \\\\ & \\quad + 15x^3 + 18x^2 + 21x \\\\ & \\quad + 20x^2 + 24x + 28 \\\\ & = 10x^4 + (12x^3 + 15x^3) + (14x^2 + 18x^2 + 20x^2) + (21x + 24x) + 28 \\\\ & = 10x^4 + 27x^3 + 52x^2 + 45x + 28 \\end{align*}\\] As you can see, we have a bunch of multiplications here. And well, the higher the degree of $P(x)$ and $Q(x)$ the more multiplications we will have. More specifically, the complexity is $O(n^2)$ which is not good. Can we do it better? Convolutions Before answering the question if we can do better than $O(n^2)$ to multiply polynomials, let's introduce the concept of convolution, since it will come in handy to understand what’s next. Convolution is defined both in continuous and discrete domains. The continuous domain might be cool for mathematicians, but we engineers who do real things prefer the discrete domain. We can define the convolution in the discrete domain of two discrete signals $p$ and $q$ as: \\[y[n] = \\sum_{k=-\\infty}^{\\infty} p[k] \\cdot q[n - k]\\] The operation is rather simple. You just need to flip $q$ and sum the product of all elements that overlap with $p$. Trust me, it's easier than it looks. Let's say we have two discrete signals p and q represented as vectors: p = [2, 3, 4] q = [5, 6, 7] Let's calculate the convolution of both $p * q$. Note that $*$ here is the convolution of both signals. We just need to flip q and move it from left to right until we are done. Note that we don’t have to do it from $-\\infty$ to $+\\infty$ since we have a discrete signal of just 3 samples. The maths are not difficult but lets try to visualize it step by step. \\[\\begin{matrix} &&&2&3&4 \\\\ \\times &7&6&5&& \\\\ \\hline \\end{matrix}\\\\ {2}\\times{5} = 10\\] Then we move the flipped version of q one element to the right. \\[\\begin{matrix} &&&2&3&4 \\\\ \\times &&7&6&5& \\\\ \\hline \\end{matrix}\\\\ 2\\times6+3\\times5=27\\] And one more. \\[\\begin{matrix} &&&2&3&4 \\\\ \\times &&&7&6&5 \\\\ \\hline \\end{matrix}\\\\ 2\\times7+3\\times6+4\\times5=52\\] And one more. \\[\\begin{matrix} &&&2&3&4 \\\\ \\times &&&&7&6&5 \\\\ \\hline \\end{matrix}\\\\ {3}\\times{7}+4\\times6 = 45\\] And one last more. \\[\\begin{matrix} &&&2&3&4 \\\\ \\times &&&&&7&6&5 \\\\ \\hline \\end{matrix}\\\\ {4}\\times{7} = 28\\] Each element represents a value of our new signal y which is p * q. We can see that it has a total length of 5 elements. We can express y as follows: y = [10, 27, 52, 45, 28] If you remember $Y(x)$ from the previous section, you will have noticed that its the same. The p and q we’ve just used are the vector representations of $P(x)$ and $Q(x)$. As a reminder this is $Y(x)$ from before: \\[10x^4 + 27x^3 + 52x^2 + 45x + 28\\] So multiplying two polynomials can be expressed as a convolution. As you can see, the result is the same. Both using the convolution and the polynomial multiplication method seen above, which most likely you studied in high school. In the next section, you will understand why convolutions are cool, and how the FFT can help us to multiply polynomials faster. Fourier Transform, and the FFT The Fourier Transform is a very powerful transformation that allows converting a signal from time domain to frequency domain. It's some kind of base change, where instead of looking at the signal from a time perspective, we look at it from a frequency point of view. Instead of saying that the signal has $x_1$ value at $t_1$ time, and $x_2$ at $t_2$, etc, we say that the signal is made of different oscillating frequencies at different rates. These oscillating frequencies are represented with sines and cosines and have a coefficient and a phase attached to them. Every signal that you can imagine, can be expressed as the sum of sines and cosines. The only problem is to know which sines/cosines represent that signal. Let's see the simplest example. Imagine a pure sinusoidal signal of frequency 5 Hz. import numpy as np import matplotlib.pyplot as plt fs = 1000 # Sampling frequency (Hz) t_end = 1 # Duration in seconds f0 = 5 # Frequency of the sine wave (Hz) t = np.linspace(0, t_end, int(fs * t_end), endpoint=False) y = np.sin(2 * np.pi * f0 * t) Y = np.fft.fft(y) freq = np.fft.fftfreq(len(y), d=1/fs) If we take the FFT of this signal, it will look as follows in the frequency domain. # Prepare the figure and axes fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5)) # Plot the sine wave ax1.plot(t, y, label=f'Sine Wave {f0} Hz') ax1.set_title('Time Domain') ax1.set_xlabel('Time (seconds)') ax1.set_ylabel('Amplitude') # Plot the magnitude of the Fourier Transform ax2.plot(freq, np.abs(Y), label='Magnitude of FFT') ax2.set_title('Frequency Domain') ax2.set_xlabel('Frequency (Hz)') ax2.set_ylabel('Magnitude') plt.tight_layout() plt.show() In the frequency domain, it looks like a delta, exactly at the frequency 5. This means that the signal on the left side (time domain) can be expressed as one sinus with frequency 5. Different ways of referring to the same signal. Before continuing, let's clarify the concepts. They all refer to the same thing but have some minor differences: Fourier Transform (FT): Fourier Transform defined in continuous domain. Discrete Fourier Transform (DFT): Fourier Transform defined for discrete signals. In practice, this is what matters, since nowadays all signals are digital, hence discrete. Fast Fourier Transform (FFT): It is an algorithm that allows to compute the Discrete Fourier Transform efficiently, $O(nlogn)$ instead of $O(n^2)$. Finally, let me introduce you to the equation you have been waiting for. The Discrete Fourier Transform. \\[X[k] = \\sum_{n=0}^{N-1} x[n] \\cdot e^{-i 2\\pi k n / N}\\] It takes a discrete signal in time domain $x[n]$ and converts it to the frequency domain $X[k]$. Note that what it does is quite simple. Each sample of $X[k] is calculated by taking each sample of $x$ and multiplying it with a complex number, that represents a given frequency. It's some kind of projection, where each element of the input signal is projected into sines, and then added up together. But let's go back to the main goal of this post. We wanted to multiply polynomials faster. One of the advantages of the DFT and working in the frequency domain, is that it converts a convolution into a simple multiplication. In other words, performing the convolution of two signals in time domain is equivalent to multiplying them in the frequency domain. And as you can imagine, multiplication is way faster than convolution. But we are still missing one piece. In order to benefit from this, we have to convert first from time domain to frequency domain. And well, this takes time. The good news is that the FFT allows us to convert from $x[n]$ (time domain) to $X[k]$ (frequency domain) with $O(nlogn)$ complexity. Multiplying Polynomials Faster As a quick recap, the way to multiply polynomials that you learned in high school is very inefficient and has a $O(n^2)$ complexity, where $n$ is the degree of the polynomials. This means that multiplying two polynomials or order 100 is way way more difficult than order 20. This doesn’t scale for huge polynomials. But we have an alternative: Convert our polynomials to frequency domain. Complexity $O(nlogn)$ using the FFT. Multiply them, which is cheaper than convolution. Complexity $O(n)$, simple element-wise multiplication. Convert the resulting polynomial back to time domain. Complexity $O(nlogn)$ using the IFFT. It may seem like a lot of operations, but for large polynomials this is way faster than the naive method that you were taught in high school. Let's see an example in Python and benchmark both approaches. Let's define multiply_naive which multiplies polynomials as you were taught in high school with $O(n^2)$ complexity. import numpy as np import random import timeit import matplotlib.pyplot as plt def multiply_naive(p, q): result_size = len(p) + len(q) - 1 result = [0] * result_size for i in range(len(p)): for j in range(len(q)): result[i + j] += p[i] * q[j] return result And multiply_fft which uses the FFT/IFFT and does a multiplication in the frequency domain, with $O(nlogn)$ complexity. def multiply_fft(p, q): length = 2 ** np.ceil(np.log2(len(p) + len(q) - 1)).astype(int) f_padded = np.pad(p, (0, length - len(p))) g_padded = np.pad(p2, (0, length - len(q))) # Calculate FFT and multiply Y = np.fft.fft(f_padded) * np.fft.fft(g_padded) result_coefficients = np.round(np.fft.ifft(Y).real).astype(int) return np.trim_zeros(result_coefficients, 'b').tolist() If we multiply the previous polynomials p and q, we can see how the result is the same as before. p = [2, 3, 4] q = [5, 6, 7] print(multiply_fft(p, q)) print(multiply_naive(p, q)) # [10, 27, 52, 45, 28] # [10, 27, 52, 45, 28] However, these polynomials have a low degree. Let's try to multiply polynomials up to degree 500 and see the time it takes using each method. We can do so with the following snippet: n_runs = 10 tiempos_naive, tiempos_fft = [], [] for grado in range(1, 500, 10): p = [random.randint(1, 100) for i in range(grado)] q = [random.randint(1, 100) for i in range(grado)] tiempos_naive.append(timeit.timeit('multiply_naive(p, q)', number=n_runs, globals=globals())/n_runs) tiempos_fft.append(timeit.timeit('multiply_fft(p, q)', number=n_runs, globals=globals())/n_runs) plt.plot(tiempos_naive, 'bo', label=\"naive\") plt.plot(tiempos_fft, 'ro', label=\"fft\") plt.title(\"Polynomial Multiplication: naive vs fft\") plt.ylabel(\"Time (s)\") plt.xlabel(\"Polynomial Degree\") plt.legend() plt.show() And we get the following times. As you can see, for low-degree polynomials it may not make sense to use the FFT approach, since the FFT/IFFT back-and-forth conversions take more time than the naive approach. However, as the degree increases, we can observe how the FFT approach is way more efficient. Summary Let’s summarise what we have seen: Multiplying polynomials using the naive method has $O(n^2)$ complexity. Polynomial multiplication can be seen as a convolution. A convolution in time domain is equivalent to a simple multiplication in frequency domain. We can convert polynomials to frequency domain with $O(nlogn)$ complexity using the FFT. Adding all together, we can multiply polynomials in frequency domain in $O(nlogn)$ which is faster than what you were taught in high school. © 2022 • Contents under CC-BY-NC • Credits",
    "commentLink": "https://news.ycombinator.com/item?id=40840396",
    "commentBody": "Convolutions, Fast Fourier Transform and polynomials (2022) (alvarorevuelta.com)211 points by clearprop 21 hours agohidepastfavorite53 comments programjames 20 hours agoSomething that always bothers me about these explanations is they usually forget about numerical errors. You can't just abstract away multiplying coefficients as \"constant time\". You may as well abstract away the entire multiplication to begin with! If you take into account numerical precision, it's closer to O(n (log n)^3) [1]. [1]: http://numbers.computation.free.fr/Constants/Algorithms/fft.... reply cperciva 19 hours agoparentThe error bound cited in that article is wildly pessimistic. The latest edition of Knuth has the correct bound (because I gave it to him). reply dataflow 19 hours agorootparentWould you mind just sharing it here...? reply cperciva 15 hours agorootparentShort answer is that FFTs are about as well behaved as anything can possibly be, because they're rotations in C^n. Explicit bound is in https://www.daemonology.net/papers/fft.pdf reply eranation 15 hours agorootparentThis is why I keep coming back to HN. You read an interesting article, a little proud you understand half of it, read a question that already makes you feel like the stupidest person in the room, then read a clarifying answer by someone who probably got a Knuth reward check for correcting an errata in the art of computer programming. reply cperciva 14 hours agorootparentKnuth judged that it wasn't an erratum, since the bound he included was correct and he never claimed it was optimal. :-/ reply nextaccountic 10 hours agorootparentDid he decide to include your better bound in future editions? reply cperciva 10 hours agorootparentYes. I believe proving the strict bound is one of the exercises now. reply Randor 13 hours agorootparentprevThanks, not often we see Knuth erratas. :) reply cperciva 11 hours agorootparentYou never see \"erratas\", since \"errata\" is already the plural (of \"erratum\"). reply tverbeure 4 hours agorootparentThe ensemble of errata of multiple books are erratas… probably. reply guyomes 6 hours agorootparentprevFor FFT with floating-point numbers, another paper from Arnold Schönhage in 1982 [1] already gives the bound in Psi(n l) operations, where n is the number of coefficients, and l is the desired precision (typically 53 for double precision). Psi(m) is the time to multiply two integers with m digits, which is known since 2021 to be O(m log m) [2]. So the current bound is O(nl log(nl)). [1]: https://doi.org/10.1007/3-540-11607-9_1 [2]: https://www.texmacs.org/joris/nlogn/nlogn-abs.html reply teleforce 13 hours agoparentprevIt will be great if we can minimize the multiplication errors and perhaps do away with the errors altogether by utilizing quaternion based operations describes in the OP article [1],[2],[3]. [1] One-Dimensional Quaternion Discrete Fourier Transform and an Approach to Its Fast Computation: https://www.mdpi.com/2079-9292/12/24/4974 [2] Convolution Theorems for Quaternion Fourier Transform: Properties and Applications: https://onlinelibrary.wiley.com/doi/10.1155/2013/162769 [3] On the Matrix Form of the Quaternion Fourier Transform and Quaternion Convolution: https://arxiv.org/abs/2307.01836 reply e-khadem 19 hours agoparentprevBut if the coefficients are integers, you can use NTT with a big enough modulus and get exact results and a boost (esp. in hardware) in multiplication time. reply xphos 19 hours agorootparentHad no clue what NTT was but found this as a reference https://codeforces.com/blog/entry/48798#:~:text=NTT%20(Numbe.... reply programjames 19 hours agorootparentI prefer this reference: https://cp-algorithms.com/algebra/fft.html#number-theoretic-... reply dataflow 19 hours agorootparentprevSee chapter 26 of the \"FXT book\". I just shared it here: https://news.ycombinator.com/item?id=40841355 reply programjames 19 hours agorootparentprevIs there a way to find groups with easy generators/primitive roots? I imagine you'd want a small root of unity, but also be able to choose a bigger modulus for extra big multiplications. Also, afaik it's discrete-logarithm level of difficulty to even find a generator if you choose a random modulus, though I don't know if it's easier to find a modulus after you choose the generator. reply adgjlsfhk1 17 hours agorootparentsince the groups you're looking at are is size log(n), you can do a lot of work without issue. as long as you do experimental it less work, it doesn't affect the runtime. reply uoaei 18 hours agoparentprevHence the distinction between computer science and software engineering. :) reply rjeli 3 hours agoprevNote that the FFT has this property of “convolution is pointwise multiplication” for any cyclic multiplicative group, see https://www.sciencedirect.com/science/article/pii/S002200007... for a more algebraic derivation. Some call this a “harmonic” fft, and there are also non-harmonic FFTs: - the “additive NTT” of [LCH14] on GF(2^n) - the circle fft on the unit circle X^2+Y^2=1 of a finite field [HLP24] - the ecfft on a sequence of elliptic curve isogenies [BCKL21] [LCH14]: https://arxiv.org/abs/1404.3458 [HLP24]: https://eprint.iacr.org/2024/278 [BCKL21]: https://arxiv.org/pdf/2107.08473 reply nickcw 11 hours agoprevYou can use this method to multiply long numbers together. The key insight you need is that polynomial multiplication is the same as normal long number multiplication without doing carrying. So suppose you have a 1000 digit number. You then take each digit and use it as the coefficient of a 1000 element polynomial. You can then multiply these polynomials together using the fft method as described in the article. To convert the result back to a number you need to do the carries. So if an element is bigger than 10 you carry the excess to the next digit. You then take the coefficients and turn them into numbers. That's the basic idea. There is some subtlety I've glossed over due to precision needed for the carries and to be sure that rounding the fft result to the nearest integer is correct. That is the way big number multiplication is done in GMP which is the leading Library for this sort of thing. reply clearprop 10 hours agoparentThanks for the comment. That makes sense, since as you are saying, a base10 number can be expressed as a polynomial where x=10. Eg: 983 = 9x^2 + 8x + 3 aka [9, 8, 3]. Wondering how big the number has to be to make sense, and where this is used in practice. reply Kubuxu 9 hours agorootparentFFT is used extensively in curve based zero-knowledge cryptography, not for the purpose of splitting a single large number into smaller ones, but to interpolate and evaluate very large polynomials (for example of the degree 2^27). All of this happens in a field of an elliptic curve so the complexity reduction is greatly appreciated. reply nickcw 8 hours agorootparentprevIn libgmp I think the numbers have to be around 100,000 bits or 30,000 digits for FFT to be faster. https://gmplib.org/manual/Multiplication-Algorithms GMP has lots of other methods in between schoolbook multiplication and FFT multiplication. A nice one is Karatsuba multiplication which is very easy to understand and delivers O(n^1.58) rather than O(n^2) performance. Python uses this method for multiplying large numbers together https://en.wikipedia.org/wiki/Karatsuba_algorithm reply kurlberg 4 hours agorootparentThere is a nice picture of the \"best\" for different ranges of sizes of numbers to be multiplied at http://gmplib.org/devel/log.i7.1024.png More context and explanation can be found at: http://gmplib.org/devel/ BTW, I like Bernstein's survey of different multiplication algorithms at https://cr.yp.to/papers/m3.pdf (there is a unifying theme about using ring isomorphisms to explain many of the \"standard\" routines.) reply kurlberg 4 hours agorootparentPS: if you're interested in multiplying \"ludicrously large numbers\", Harvey and van der Hoeven had a nice breakthrough and got multiplication down to \"FFT speed\" (n*log(n)), see https://hal.science/hal-02070778v2/document A pop-sci description can be found at https://theconversation.com/weve-found-a-quicker-way-to-mult... reply pfdietz 5 hours agorootparentprevThe Karatsuba algorithm idea can also be used for multiplication of polynomials. reply sigil 18 hours agoprevWho was the first person to propose FFTs for faster polynomial multiplication? Got curious about this recently. I’m not great at citation tracing, but did make it back to this 1995 paper by David Eppstein [0] where he uses it to efficiently solve Subset Sum after an incremental update. Surely Knuth’s TAOCP had it even earlier? The fact that FFT polynomial multiplication also lets you solve Exact Subset Sum with Repetition in sub-exponential time came as a real shock to me. [1] Crucially, this algo is O(N log N) where N = the maximum element, not N = the set size, so it isn’t a P ≠ NP counterexample or anything. [0] https://escholarship.org/content/qt6sd695gn/qt6sd695gn.pdf [1] https://x.com/festivitymn/status/1788362552998580473?s=46&t=... reply pbsd 17 hours agoparentPollard [1], Nicholson [2], and Schonhage-Strassen [3] seem to have come up with it independently around the same time, using different approaches. Strassen is said to have discovered the Pollard approach in 1968 but there is no (written) record of it. It should also be noted that, while it was not exactly the birth of the FFT, Cooley-Tukey's 1965 paper [4] on it was what kickstarted research on FFT and its applications. This was just a few years after that. [1] https://doi.org/10.1090/S0025-5718-1971-0301966-0 [2] https://doi.org/10.1016/S0022-0000(71)80014-4 [3] https://doi.org/10.1007/BF02242355 [4] https://doi.org/10.1090/S0025-5718-1965-0178586-1 reply sigil 12 hours agorootparentThank you! reply npalli 17 hours agoparentprevEarliest maybe Gentleman and Sande from 1966 and a kickass title (for '66) - \"Fast Fourier Transforms: for fun and profit\" https://www.cis.rit.edu/class/simg716/FFT_Fun_Profit.pdf reply cornstalks 18 hours agoparentprevThe Schönhage–Strassen algorithm from 1971 is basically a polynomial multiplication using FFTs: https://en.m.wikipedia.org/wiki/Sch%C3%B6nhage%E2%80%93Stras... reply froh 2 hours agoprevI'd like the article even more if it used numpy.convolve in the benchmark. comparing pure python and numpy fft feels not right to me. the result will be similar, sure, but the effect might then only show for much larger inputs. reply tverbeure 4 hours agoprevIf you haven’t seen it yet, you should watch this video: https://youtu.be/h7apO7q16V0?si=bmgUEMTQSqU3flIv It derives the FFT algorithm from polynomial multiplication and it’s fantastic. I rewatch it every 6 months or so. reply keepamovin 4 hours agoprevSo integer factoring is a discrete deconvolution? I wonder if juxtaposing the FFT representation (inverse pointwise multiplication) and the tableax (regular long multiplication / carry add) could break the symmetry and get enough information for a fast algorithm? reply sva_ 5 hours agoprevGood article, but I think the multiply_naive should've used numpy instead for a more fair benchmark comparison. reply eranation 15 hours agoprevThe key ”trick” of the operation seems to be this revelation: > In other words, performing the convolution of two signals in time domain is equivalent to multiplying them in the frequency domain. Great article, as it breaks down a complex idea into much smaller steps that even my math challenged mind can somehow grasp, but did I miss a step? Or is it left as an exercise to the reader to look up? I was already stretching my math ability to that point, but it felt a little bit like - “and then, draw the rest of the F-ing owl” to me. Is it just me? Great writing otherwise. reply clearprop 10 hours agoparentThanks! In case it helps: * Multiplying two polynomials as taught in school is in reality a convolution. * \"performing the convolution of two signals in the time domain is equivalent to multiplying them in the frequency domain\" * FFT allows us to convert from time domain to frequency domain * We use FFT to convert our polynomial to frequency domain. * If we are now in the frequency domain, we just need to multiply. Faster than a convolution. Does it clarify the missing step? Happy to update the post with what's missing. reply eranation 3 hours agorootparentYep, it definitely helps! What I'm struggling to understand is why \"performing the convolution of two signals in the time domain is equivalent to multiplying them in the frequency domain\" (I'm going to google/gpt it, this is more of an exercise left for me to dive into, your post is perfect as it is) reply bobmcnamara 14 hours agoprevAnother FFT trick applies well here: the middle vector multiplication can be folded into the last stage of the FFT and first stage of the IFFT. This saves writing these two stages out two stages to memory. reply joe_the_user 15 hours agoprevSure, the naive method of multiplying polynomials is slow in the degree of the polynomial. But when does one have to deal with two degree 100 polynomials? My impression is that this sort of method isn't used by computer algebra system for this reason. reply empath75 10 minutes agoparentExtremely common in error correction and signal processing. https://www.youtube.com/watch?v=CcZf_7Fb4Us https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_cor... is one example. reply programjames 14 hours agoparentprevComputer algebra systems (such as chebfun in Matlab) convert arbitrary functions to 100-degree+ polynomials to make it easier to find roots, optima, etc. reply 8051Enthusiast 5 hours agoparentprevWhen I wanted to reverse engineer some CRC checksum parameters for larger files, I made a program[1] that converts the files into some million degree GF(2) polynomials and calculates their GCD, which is only possible in reasonable time with FFT-based multiplication. [1]: https://github.com/8051enthusiast/delsum reply imjonse 14 hours agoparentprevThis convolutional view and fast GPU kernels for FFT were used in some pre-Mamba state space models for long sequence modeling where the polynomial is the input sequence. The Hazy Research blog posts from 2020-2023 have a lot of information on this approach. reply lkuty 7 hours agoparentprevSee https://news.ycombinator.com/item?id=40306339 \"(...) or my physics research I have worked with expressions that was just shy of a terabyte long and had > 100M terms\" reply citizen_friend 15 hours agoparentprevNow do 1000. That’s kind of the whole point. Quadratic works fine until it sneaks up on you. reply Kubuxu 9 hours agoparentprevZero-knowledge cryptography frequently deals with polynomials of degree 2^20 - 2^27. reply bjornsing 21 hours agoprevFunny feeling: Looking at the title I felt a bit puzzled. Then scrolling the article the concepts and their (well known) connections came back to me. reply adamnemecek 20 hours agoprev [–] I think that all machine learning is solving a convolutional equation. This paper talks about it in the context of RL https://arxiv.org/abs/1712.06115 but most approaches fit within that paradigm. reply salty_biscuits 19 hours agoparent [–] Basically kernel methods right? reply adamnemecek 19 hours agorootparent [–] Yes, it is there. There might be more. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Traditional polynomial multiplication has \\(O(n^2)\\) complexity, making it inefficient for large polynomials.",
      "The Fast Fourier Transform (FFT) reduces polynomial multiplication complexity to \\(O(n \\log n)\\) by converting the problem to the frequency domain.",
      "The FFT-based method involves converting polynomials to the frequency domain, multiplying them, and converting the result back, significantly improving efficiency for high-degree polynomials."
    ],
    "commentSummary": [
      "The discussion centers around the use of Fast Fourier Transform (FFT) for polynomial multiplication, highlighting its efficiency compared to naive methods.",
      "Key insights include the importance of numerical precision in FFT calculations and the historical context of FFT's development for polynomial multiplication.",
      "The conversation also touches on practical applications, such as error correction, signal processing, and zero-knowledge cryptography, where FFT-based methods are particularly beneficial."
    ],
    "points": 211,
    "commentCount": 53,
    "retryCount": 0,
    "time": 1719781723
  },
  {
    "id": 40845304,
    "title": "My Python code is a neural network",
    "originLink": "https://blog.gabornyeki.com/2024-07-my-python-code-is-a-neural-network/",
    "originBody": "My Python code is a neural network July 1, 2024 TL;DR: Many programs that we write can be embedded in recurrent neural networks (RNNs). For such programs, a trained RNN can perform better than if we write the algorithm by hand, refining it via trial and error. I walk through an example in detail. Is this Python code or a neural network? They are one and the same. Contents Introduction Detecting program code in messages Ideas for decision rules A hand-written algorithm Neural networks to the rescue The general idea Our Python code as math Training the network to discover better algorithms Trainable activation functions Architectures with more efficient implementations Architectures with more numerically stable gradients Data-driven discipline Introduction Humans are bad at managing spaghetti code. Of course, we should try and avoid writing spaghetti code if we can. But there are problems that are so ill-specified that any serious attempt to solve them results in just that. Let me make this concrete. In research projects, we often write programs that extract information from raw data. The data may have idiosyncrasies and follow no clear specification. Some examples: Identify mentions of corporations and their officers in news articles. Label public procurement contracts by what kind of service the firms supply. Determine if a message exchanged between engineers contains program code. If we want the output to be perfect—at least in the limit—then we need to exhaustively examine each observation. We can go one step further to reassure ourselves that the program produces the correct output by picking representative examples and writing unit tests for them. Almost every programming language has libraries for this, including R and Python. But this approach won’t work well if getting the correct output requires that we specify complicated decision rules. In the examples above, corporate officers might be mentioned in the news by nickname, contracts might describe the procured services using synonyms, and it might not always be trivial to tell apart program code from English. In such situations, we might be better off training a neural network. Algorithms that train neural networks thrive on spaghetti. Detecting program code in messages In this post, the problem that I’ll walk through solving is this: how can we detect if a message sent during code review explicitly refers to program code? Let’s suppose that the code base that we observe is written in C, and that the following examples are representative of the messages that engineers send: LGTM with render_ipa_alloc(). If the FTPSACK flag is set, then use a prespecified value. AFAICT there is nothing else to check (unless you can think of something). Actually, debug_error() doesn’t return NULL, so we should use IS_ERROR() here. This fails to build on aarch64 even though it works without issue on amd64. I’ve added if (err) goto cleanup; but the code still leaks. The highlighted expressions are program-code references that we would like to detect. Ideas for decision rules We take a straightforward approach and look for decision rules that can tell apart program code from ordinary English. Here are some simple ideas: # Rule True positive False positive False negative 1 Word followed by parentheses is code. A and D – B and F 2 All-caps word is code. B and D C A and F 3 Non-English word is code. A, B, and D C, E, and F – None of these rules is perfect: each has false positives or false negatives or both. Rule 1 is easy to implement but it misses obvious positive cases like if (err) goto cleanup;. Rule 2 mistakenly classifies capitalized acronyms as program code. To make this rule effective, we need to complement it with an extensive list of acronyms and abbreviations that we might encounter (e.g., AFAICT, LGTM, USD, COVID). Rule 3 mistakenly classifies engineering jargon as program code. To make this rule work, we need an even longer word list than for Rule 2. We can start with a canonical list of English words, but we need to complement it both with acronyms and with words that commonly appear in software engineering, like aarch64 and amd64. A hand-written algorithm Nevertheless, we reason that a simple algorithm might do well enough. So we set out to write a program that implements Rule 1. Our program will decide in two steps whether a message contains code: Preprocessing: convert the message into a sequence of tokens. The tokens are chosen so that they capture syntactic elements of C program code. This is the information that we need to apply Rule 1. Inference: apply a function to the token sequence to check if the sequence satisfies the rule. If it does, then we conclude that the message contains valid C code. Let’s take the first example sentence from before. We could encode it as a sequence of eight tokens:1 Rule 1 says that the token sequence underscore_identifier–open_paren–close_paren indicates the presence of program code in the sentence. Let’s write a classifier in Python that detects a sequence like this. First, we set up the types. For simplicity, we model tokens as strings. In order to keep track of the tokens that we have seen in the sequence, we also define a data class called State: 1 from dataclasses import dataclass 23 Token = str # E.g., \"all_caps_identifier\". 45 @dataclass 6 class State: 7previous_was_identifier: bool = False 8previous_was_open_paren: bool = False 9previous_previous_was_identifier: bool = False 10seen_code: bool = False To classify a message, we write a function called contains_code that returns True if the message satisfies Rule 1. The function decides this by iterating over the token sequence and checking at each element whether the rule applies. Since applying the rule means checking three consecutive tokens, not just one token, we need to keep a memory of what tokens we have seen earlier in the sequence. The function contains_code keeps this memory inside an instance of State: 12 from typing import Iterable 1314 def contains_code(tokens: Iterable[Token]) -> bool: 15state = State() 1617for token in tokens: 18state = process(state, token) 1920return state.seen_code Now we are ready to implement process. This is where we check if the rule applies and maintain the state: 22 def process(state: State, token: Token) -> State: 23\"\"\"Process one element of the token sequence, taking into account 24what we have seen earlier in the sequence.\"\"\" 2526if state.seen_code: 27return state 2829# Rule 1: an identifier followed by opening and closing parens is 30# program code. 31# 32if (token == \"close_paren\" 33and state.previous_was_open_paren 34and state.previous_previous_was_identifier): 35state.seen_code = True 36return state 3738# Housekeeping: take note if we see identifiers and opening parens 39# in case the next token will be a closing paren. 40# 4142state.previous_previous_was_identifier = ( 43state.previous_was_identifier 44) 4546state.previous_was_identifier = token in ( 47\"all_caps_identifier\", 48\"underscore_identifier\", 49\"misc_identifier\", 50) 51state.previous_was_open_paren = token == \"open_paren\" 5253return state And with that, we have a classifier that implements Rule 1. Let’s see how well it performs by looking at its precision and recall. The good news: The code is simple. It has no false positives. It has a precision of 100 percent on our (completely made up) examples. The bad news is that this classifier has a high false negative rate which leads to a recall of only 50 percent. If we want to increase recall and add Rule 2 to the algorithm, then the code becomes more complex: we need to add more fields to the State class, and more if/elif/else statements and related housekeeping to process. If we want to further refine any of the rules, then we need to add yet more complexity. We see that tweaking the algorithm by hand to find improvements can get out of control. It can easily result in spaghetti code that will be a struggle to maintain. How can we do better? Neural networks to the rescue Mired in our predicament, we reflect on the fact that contains_code and process are a state machine. This fact is notable because state machines can be encoded by recurrent neural networks (RNNs).2 So rather than tweaking the algorithm by hand, we could find a better algorithm by training an RNN on our examples. The general idea At a high level, an RNN is an approximation of the conditional probability \\[\\Pr({\\rm MessageContainsCode} = 1 \\mid {\\rm Token}_1 = x_1, \\ldots, {\\rm Token}_T = x_T).\\] This approximation is calculated by processing the token sequence element by element. For each token, we calculate a vector that does what State did in our Python code. This is usually called a hidden state because it is only an interim variable and appears in neither the input nor the output of the model. We get it by taking into account the previous state: \\[\\begin{aligned} {\\rm State}_0 &:= 0 \\in \\mathbb{R}^S \\\\ {\\rm State}_1 &:= f({\\rm Token}_1, {\\rm State}_0) \\\\ {\\rm State}_2 &:= f({\\rm Token}_2, {\\rm State}_1) \\\\ &\\;\\;\\vdots \\\\ {\\rm State}_T &:= f({\\rm Token}_T, {\\rm State}_{T-1}) \\\\ \\end{aligned}\\] where the function \\(f\\) represents the hidden layers of the recurrent network. The message is classified based on the final state: \\[\\widehat{\\Pr}({\\rm MessageContainsCode} = 1 \\mid {\\rm Token}_1, \\ldots) := g({\\rm State}_T)\\] where \\(g\\) is what is called the output layer of the network. How should we parameterize the functions \\(f\\) and \\(g\\)? Our Python code as math Let’s think about what a network that encodes Rule 1 looks like. Returning to the previous example sentence, let’s simplify the math that ensues by using fewer types of tokens: Each token in the sentence is represented as a binary vector, so the input data that we get looks like this (with the zeros erased to make the table easier to read):\\(x_1\\) \\(x_2\\) \\(x_3\\) \\(x_4\\) \\(x_5\\) \\(x_6\\) \\(x_7\\) \\(x_8\\) identifier \\(1\\)\\(1\\)\\(1\\)open_paren\\(1\\) close_paren \\(1\\)unknown\\(1\\)\\(1\\)\\(1\\) To model the state, we need to add three hidden layers to the network. When we are processing the \\(t\\)-th token in the sequence, we calculate each of the three layers, one after the other. Let’s denote the first layer by \\(h_t^1\\), the second layer by \\(h_t^2\\), and the third by \\(h_t^3\\). The first two layers hold intermediate calculations while the third layer holds our final state after processing the token. The third layer, \\(h_t^3\\), corresponds to \\({\\rm State}_t\\) in the schematic presentation above, and the initial state, \\(h_0^3\\), corresponds to \\({\\rm State}_0\\). This table shows how the hidden state evolves:\\(h_0^3\\) \\(h_1^3\\) \\(h_2^3\\) \\(h_3^3\\) \\(h_4^3\\) \\(h_5^3\\) \\(h_6^3\\) \\(h_7^3\\) \\(h_8^3\\) seen_code\\(1\\) \\(1\\) identifier\\(1\\)\\(1\\)\\(1\\)open_paren \\(1\\) p_identifier \\(1\\)\\(1\\)\\(1\\) To encode the hand-written algorithm, we use the binary indicator function, \\(\\mathbb{1}\\{\\cdot > 0\\}\\), which evaluates to 1 if its input is positive and to 0 otherwise.3 Of course, this would be an outstandingly bad choice if we wanted to train an RNN because its derivative is zero almost everywhere which breaks gradient descent. But for now, we merely want to specify an RNN that mimics the hand-written algorithm. The State class that we used is effectively a binary vector, and the binary indicator function ensures that the RNN’s hidden layers will be binary as well. The key calculation is done in the second and third hidden layers. The second layer checks if the token sequence \\((x_{t-2}, x_{t-1}, x_t)\\) satisfies Rule 1. We don’t do this by referencing the three tokens directly. Instead, we store information about them in the first hidden layer, and we reference that: \\[\\begin{aligned} h_{t,\\tt this\\_is\\_code}^2 &:= \\mathbb{1}\\left\\{ 2 h_{t,\\tt pp\\_identifier}^1 h_{t,\\tt p\\_open\\_paren}^1 h_{t,\\tt close\\_paren}^1 - 1 > 0 \\right\\} \\\\ \\end{aligned}\\] The third layer combines this check, \\(h_{t,\\tt this\\_is\\_code}^2\\), with a memory of whether any earlier part of the sequence has satisfied the rule: \\[\\begin{aligned} h_{t,\\tt seen\\_code}^3 &:= \\mathbb{1}\\left\\{ 2 \\left( h_{t, \\tt this\\_is\\_code}^2 + h_{t, \\tt p\\_seen\\_code}^2 \\right) - 1 > 0 \\right\\} \\\\ \\end{aligned}\\] Once we have processed all \\(T\\) tokens, the output is calculated using the final hidden layer: \\[y_T := \\mathbb{1}\\left\\{ 2 h_{T,\\tt seen\\_code}^3 - 1 > 0 \\right\\}\\] The formula for \\(h_{t,\\tt this\\_is\\_code}^2\\) looks simple enough but it contains a multiplication between \\(h_{t,\\tt pp\\_identifier}^1\\), \\(h_{t,\\tt p\\_open\\_paren}^1\\), and \\(h_{t,\\tt close\\_paren}^1\\). If we wanted to include this multiplicative term, we would need a higher-order RNN, similar to the second-order RNN that Giles et al. (1992) used to discover state machines. Fortunately, with binary hidden layers, we can stay within the framework of a first-order RNN by replacing the multiplication with a simple sum: \\[\\begin{aligned} h_{t,\\tt this\\_is\\_code}^2 &:= \\mathbb{1}\\left\\{ h_{t,\\tt pp\\_identifier}^1 + h_{t,\\tt p\\_open\\_paren}^1 + h_{t,\\tt close\\_paren}^1 - 2 > 0 \\right\\} \\\\ \\end{aligned}\\] (And when we will change the activation function in the next section so that we can train the network, we will see that a sum still works.) Now to wrap things up, we also get some auxiliary calculations done that are needed for \\(h_{t,\\tt this\\_is\\_code}^2\\) and \\(h_{t,\\tt seen\\_code}^3\\). In the first hidden layer, we take note of the token that we are processing and copy the previous state: \\[\\begin{aligned} h_{t,\\tt identifier}^1 &:= \\mathbb{1}\\{ 2 x_{t,\\tt identifier} - 1 > 0 \\} \\\\ h_{t,\\tt open\\_paren}^1 &:= \\mathbb{1}\\{ 2 x_{t,\\tt open\\_paren} - 1 > 0 \\} \\\\ h_{t,\\tt close\\_paren}^1 &:= \\mathbb{1}\\{ 2 x_{t,\\tt close\\_paren} - 1 > 0 \\} \\\\ h_{t,\\tt p\\_seen\\_code}^1 &:= \\mathbb{1}\\{ 2 h_{t-1,\\tt seen\\_code}^3 - 1 > 0 \\} \\\\ h_{t,\\tt p\\_identifier}^1 &:= \\mathbb{1}\\{ 2 h_{t-1,\\tt identifier}^3 - 1 > 0 \\} \\\\ h_{t,\\tt p\\_open\\_paren}^1 &:= \\mathbb{1}\\{ 2 h_{t-1,\\tt open\\_paren}^3 - 1 > 0 \\} \\\\ h_{t,\\tt pp\\_identifier}^1 &:= \\mathbb{1}\\{ 2 h_{t-1,\\tt p\\_identifier}^3 - 1 > 0 \\} \\\\ \\end{aligned}\\] In the second layer, we copy some values that will be used in the third: \\[\\begin{aligned} h_{t,\\tt identifier}^2 &:= \\mathbb{1}\\left\\{ 2 h_{t,\\tt identifier}^1 - 1 > 0 \\right\\} \\\\ h_{t,\\tt open\\_paren}^2 &:= \\mathbb{1}\\left\\{ 2 h_{t,\\tt open\\_paren}^1 - 1 > 0 \\right\\} \\\\ h_{t,\\tt p\\_seen\\_code}^2 &:= \\mathbb{1}\\left\\{ 2 h_{t,\\tt p\\_seen\\_code}^1 - 1 > 0 \\right\\} \\\\ h_{t,\\tt p\\_identifier}^2 &:= \\mathbb{1}\\left\\{ 2 h_{t,\\tt p\\_identifier}^1 - 1 > 0 \\right\\} \\\\ \\end{aligned}\\] And lastly, in the third layer, we maintain a memory of tokens that we can use later when we process the next token, \\(x_{t+1}\\): \\[\\begin{aligned} h_{t,\\tt identifier}^3 &:= \\mathbb{1}\\left\\{ 2 h_{t,\\tt identifier}^2 - 1 > 0 \\right\\} \\\\ h_{t,\\tt open\\_paren}^3 &:= \\mathbb{1}\\left\\{ 2 h_{t,\\tt open\\_paren}^2 - 1 > 0 \\right\\} \\\\ h_{t,\\tt p\\_identifier}^3 &:= \\mathbb{1}\\left\\{ 2 h_{t,\\tt p\\_identifier}^2 - 1 > 0 \\right\\} \\\\ \\end{aligned}\\] The illustration at the beginning of the post shows how this network classifies the example sentence. I won’t do it in this post, but we could set it up in PyTorch, too, and verify that it classifies messages exactly as our hand-written algorithm does. Training the network to discover better algorithms If we have a recurrent network, then we should be able to train it. But the network as we have parameterized it so far is not amenable to training. Gradient descent, the algorithm that is commonly used to train neural networks, will be stuck because the binary indicator function has a zero slope almost everywhere. Trainable activation functions We overcome this hurdle by switching to an activation function called the rectified linear unit (ReLU), defined as \\([x]^+ := x \\mathbb{1}\\{ x > 0\\}\\). The numerical constants, called weights and biases, also need to be replaced, as these are the parameters that gradient descent will estimate. We end up with \\[\\begin{aligned} h_{t,\\tt this\\_is\\_code}^2 &:= \\big[ w_{\\tt pp\\_identifier}^2 h_{t,\\tt pp\\_identifier}^1 + \\\\ &\\qquad\\quad+ w_{\\tt p\\_open\\_paren}^2 h_{t,\\tt p\\_open\\_paren}^1 + \\\\ &\\qquad\\quad+ w_{\\tt close\\_paren}^2 h_{t,\\tt close\\_paren}^1 + b_{\\tt this\\_is\\_code}^2 \\big]^+ \\\\ \\end{aligned}\\] and so on for the hidden layers. For the output layer, we use a sigmoid activation function: \\[y_T := \\frac{1}{ 1 + \\exp\\left( - w_{\\tt seen\\_code}^y h_{T,\\tt seen\\_code}^3 - b_{\\tt seen\\_code}^y \\right) }.\\] And we are ready to plug this into PyTorch and train it. However, what we will find if we do that is that while it is possible to get this network to learn something from the data, its performance is far from exceptional. We can tackle the code detection problem more effectively if we use some of the other options offered by PyTorch. Architectures with more efficient implementations One reason for the lackluster performance of our recurrent network is that its architecture is somewhat atypical. As a consequence, more of our training procedure is executed in Python glue code and less of it in PyTorch’s C++ library. An alternative that is provided off the shelf by PyTorch, and that thus has a more efficient implementation, is the Elman RNN. Our architecture differs from Elman’s in that in our network, each hidden layer takes only the previous layer as input: the first layer for token \\(t\\) takes the third layer for token \\(t-1\\), the second layer takes the first, and the third layer takes the second. In vector notation: \\[\\begin{aligned} h_t^1 &:= \\left[ w^1 \\left( x_t', {h_{t-1}^3}' \\right)' + b^1 \\right]^+ \\\\ h_t^2 &:= \\left[ w^2 h_t^1 + b^2 \\right]^+ \\\\ h_t^3 &:= \\left[ w^3 h_t^2 + b^3 \\right]^+ \\\\ \\end{aligned}\\] If we followed Elman’s architecture, the layers would be different in two ways. First, each hidden layer for token \\(t\\) would also take the same layer for token \\(t-1\\) as input. Second, the first hidden layer would not take the final layer for the previous taken as input. This becomes clearer when we compare the formulas shown above with those of the Elman RNN: \\[\\begin{aligned} h_t^1 &:= \\left[ w^1 \\left( x_t', {h_{t-1}^1}' \\right)' + b^1 \\right]^+ \\\\ h_t^2 &:= \\left[ w^2 \\left( {h_t^1}', {h_{t-1}^2}' \\right)' + b^2 \\right]^+ \\\\ h_t^3 &:= \\left[ w^3 \\left( {h_t^2}', {h_{t-1}^3}' \\right)' + b^3 \\right]^+ \\\\ \\end{aligned}\\] Architectures with more numerically stable gradients Another reason for the subpar performance of our recurrent network is that in real-world code review, engineers often send longer messages. Longer messages translate into longer token sequences which can throw off the gradient descent algorithm. Gradient descent works in theory but not always in practice: the gradients, obtained by diligently applying the chain rule, can get too close to zero which causes problems with numerical stability. While Elman’s architecture is susceptible, too, other architectures attempt to mitigate this. So we might well find that a gated recurrent unit or a long short term memory network performs better for our code detection task. Data-driven discipline Recurrent neural networks have handled our inevitable spaghetti code better than we could ourselves. But they have done more than that: they have imposed a certain data-driven discipline on us that we would probably not follow if we wrote the algorithm by hand. To train a network, we need to select training and validation data sets, we need to prelabel them, and we need to specify a loss function that makes it explicit what we want the classifier to achieve and what we don’t. This discipline forces us to clarify our thinking, as we will inevitably find gray areas that we didn’t expect. And it turns out that this discipline is useful even for those problems that we decide to solve by hand. The first word, “LGTM,” is a commonly used abbreviation of “looks good to me.” Why is it coded as all_caps_identifier rather than something else? Because in idiomatic C, constants are given all-uppercase names, so without a list of common abbreviations at hand, we err on the side of assuming that “LGTM” could be a constant in some program code.↩︎ Marvin Minsky’s 1967 book, Computation: Finite and Infinite Machines, already pointed out that recurrent neural networks can encode state machines. Later, in the late 1980s and through the 1990s, there was a burgeoning literature on how to discover state machines by training appropriately specified recurrent networks (e.g., Giles et al., 1992).↩︎ As a little-known trivia, this is also occasionally called the Heaviside step function or the Heaviside activation function.↩︎",
    "commentLink": "https://news.ycombinator.com/item?id=40845304",
    "commentBody": "My Python code is a neural network (gabornyeki.com)193 points by gnyeki 5 hours agohidepastfavorite43 comments skybrian 1 hour agoThis article doesn't talk much about testing or getting training data. It seems like that part is key. For code that you think you understand, it's because you've informally proven to yourself that it has some properties that generalize to all inputs. For example, a sort algorithm will sort any list, not just the ones you tested. The thing we're uncertain about for a neural network is that we don't know how it will generalize; there are no properties that we think are guaranteed for unseen input, even if it's slightly different input. It might be because we have an ill-specified problem and we don't know how to mathematically specify what properties we want. If you can actually specify a property well enough to write a property-based test (like QuickCheck) then you can generate large amounts of tests / training data though randomization. Start with one example of what you want, then write tests that generate every possible version of both positive and negative examples. It's not a proof, but it's a start. At least you know what you would prove, if you could. If you have such a thing, relying on spaghetti code or a neural network seem kind of similar? If you want another property to hold, you can write another property-based test for it. I suppose with the neural network you can train it instead of doing the edits yourself, but then again we have AI assistance for code fixes. I think I'd still trust code more. At least you can debug it. reply scotchmi_st 4 hours agoprevThis is an interesting article if you read it like a howto for constructing a neural network for performing a practical task. But if you take it at face-value, and follow a similar method the next time you need to parse some input, then, well, I don't know what to say really. The author takes a hard problem (parsing arbitrary input for loosely-defined patterns), and correctly argues that this is likely to produce hard-to-read 'spaghetti' code. They then suggest replacing that with code that is so hard to read that there is still active research into how it works, (i.e a neural net). Don't over-index something that's inscrutable versus something that you can understand but is 'ugly'. Sometimes, _maybe_, a ML model is what you want for a task. But a lot of the time, something that you can read and see why it's doing what it's doing, even if that takes some effort, is better than something that's impossible. reply thoughtlede 1 hour agoparentI think the mention of 'spaghetti code' is a red herring from the author. If the output from an algorithm cannot be defined precisely as a function of the input, but you have some examples to show, that's where machine learning (ML) is useful. In the end, ML provides one more option to choose from. Whether it works or not for you depends on evaluations and how deterministic and explainability you need from the chosen algorithm/option. The thing that struck me is if RNN is the right choice given that it would need to be trained and we need a lot of examples than what we might have. That said, maybe based on known 'rules', we can produce synthetic data for both +ve and -ve cases. reply pakl 4 hours agoprevThere exists the Universal (Function) Approximation Theorem for neural networks — which states that they can represent/encode any function to a desired level of accuracy[0]. However there does not exist a theorem stating that those approximations can be learned (or how). [0] https://en.m.wikipedia.org/wiki/Universal_approximation_theo... reply montebicyclelo 21 minutes agoparentPeople throw that proof around all the time; but all it does is show that a neural net is equivalent to a lookup table; and a lookup table with enough memory can approximate any function. It's miles away from explaining how real world, useful, neural nets, like conv-nets, transformers, LSTMs, etc. actually work. reply jb1991 3 hours agoparentprevFYI, there are actually many algorithms going back longer than the neural network algorithm that have been proven to be a universal function approximator. Neural networks are certainly not the only and not the first to do so. There are quite a few that are actually much more appropriate for many cases than a neural network. reply derangedHorse 3 hours agorootparentWhat other algorithms can do this and which situations would they be more useful than neural networks? reply gnyeki 2 hours agorootparentThis area is covered by non-parametric statistics more generally. There are many other methods to non-parametrically estimate functions (that satisfy some regularity conditions). Tree-based methods are one family of such methods, and the consensus still seems to be that they perform better than neural networks on tabular data. For example: https://arxiv.org/abs/2106.03253 reply someoneontenet 2 hours agorootparentprevNewtons Method approximates square roots. Its useful if you want to approximate something like that without pulling in the computational power required of NN. reply astrobe_ 1 hour agorootparentI think the problem to solve is more like : given a set of inputs and outputs, find a function that gives the expected output for each input [1]. This is like Newton's method on a higher order ;-). One can find such a tool in Squeak or Pharo Smalltalk, IIRC. [1] https://stackoverflow.com/questions/1539286/create-a-functio... reply kristjansson 1 hour agorootparentprevNewton's method related to universal function approximation in the same way a natural oil seep is related to a modern IC engine... reply visarga 2 hours agoparentprevThey can model only continuous functions, more specifically any continuous function on compact subsets of ℝⁿ. They can approximate functions to an arbitrary level of accuracy, given sufficient neurons reply richrichie 3 hours agoparentprevNot any function though. There are restrictions on type of functions \"universal\" approximation theorem is applicable for. Interestingly, the theorem is about a single layer network. In practice, that does not work as well as having many layers. reply arketyp 4 hours agoparentprevMakes you wonder what is meant by learning... reply dekhn 4 hours agorootparentLearning is using observations to create/update a model that makes predictions which are more accurate than chance. At some point the model ends up having generalizability beyond the domain. reply ryjo 4 hours agoprevReally awesome. Thanks for this thorough write-up. I don't totally understand the deeper math concepts mentioned in this article around RNNs, but it's sparked some of my own thoughts. It feels similar to things I've been exploring lately-- that is: building your app interwoven with forward chaining algorithms. In your case, you're using RNNs, and in mine, I'm building into the Rete algorithm. You also touch on something in this article that I've found quite powerful: putting things in terms of digesting an input string character-by-character. Then, we offload all of the reasoning logic to our algorithm. We write very thin i/o logic, and then the algorithm does the rest. reply jlturner 4 hours agoprevIf this interests you, it’s worth taking a look at Genetic Programming. I find it to be a simpler approach at the same problem, no math required. It simply recombines programs by their AST, and given some heuristic, optimizes the program for it. The magic is in your heuristic function, where you can choose what you want to optimize for (ie. Speed, program length, minimize complex constructs or function calls, network efficiency, some combination therein, etc). https://youtu.be/tTMpKrKkYXo reply nickpsecurity 3 hours agoparentI’ll add the Humies Awards that highlight human-competitive results. One can learn a lot about what can or can’t be done in this field by just skimming across all the submitted papers. https://www.human-competitive.org/ reply PixelN0va 3 hours agoparentprevhmm thanks for the link reply Fripplebubby 4 hours agoprevLove this post! Gets into the details of what it _really_ means to take some function and turn it into an RNN, and comparing that to the \"batteries included\" RNNs included in PyTorch, as a learning experience. Question: > To model the state, we need to add three hidden layers to the network. How did you determine that it would be three hidden layers? Is it a consequence of the particular rule you were implementing, or is that generally how many layers you would use to implement a rule of this shape (using your architecture rather than Elman's - could we use fewer layers with Elman's?)? reply gnyeki 2 hours agoparentI'm glad you found it valuable! Both are good questions and I haven't gone far enough mapping the code to Elman's architecture to know the answer to the second. For your first question, using three hidden layers makes it a little clearer what the network does. Each layer performs one step of the calculation. The first layer collects what is known from the current token and what we knew after the calculation for the previous token. The second layer decides whether the current token looks like program code, by checking if it satisfies the decision rule. The third layer compares the decision with what we decided for previous tokens. I think that this could be compressed into a single hidden layer, too. A ReLU should be good enough at capturing non-linearities so this should work. reply Fripplebubby 2 hours agorootparentAh, that makes sense. So, we consider two hidden layers more as \"memory\" or \"buffers\", and actually the rule is implemented in just one layer, at least for a single token. reply dekhn 4 hours agoprevAre RNNs completely subsumed by transformers? IE, can I forget about learning anything about how to work with RNNs, and instead focus on transformers? reply Fripplebubby 2 hours agoparentTo further problematize this question (which I don't feel like I can actually answer), consider this paper: \"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\" - https://arxiv.org/pdf/2006.16236 What this shows is that actually a specific narrow definition of transformer (a transformer with \"causal masking\" - see paper) is equivalent to an RNN, and vice versa. Similarly Mamba (https://arxiv.org/abs/2312.00752), the other hot architecture at the moment, has an equivalent unit to a gated RNN. For performance reasons, I believe they use an equivalent CNN during training and an RNN during inference! reply visarga 2 hours agorootparentThere still are important distinctions. RNNs have constant memory while transformers expand their memory with each new token. They are related, but one could in theory process an unbounded sequence while the other cannot because of growing memory usage. reply Fripplebubby 1 hour agorootparentprevTo be more concrete: you might decide not to learn about RNNs, but still find them lurking in the things you did learn about! reply toxik 2 hours agoparentprevTransformers have finite context, RNNs don’t. In practice the RNN gradient signal is limited by back propagation through time, it decays. This is in fact the whole selling point of transformers; association is not harder or easier in near/short distance. But in theory a RNN can remember infinitely far away. reply Voloskaya 4 hours agoparentprevNot if you want to be a PhD/Researcher in ML, yes otherwise. Source: Working on ML/LLMs as a research engineer for the past 7 years, including for one of the FAANG's research lab, always wanted to take time to learn about RNN but never did and never needed to. reply rolisz 3 hours agorootparentOh, I'm sure plenty of recent PhDs don't know about RNNs. They've been dropped like a hot potato in the last 4-5 years. reply derangedHorse 3 hours agorootparentprevI haven’t read it in a while but I remember this post giving a good rundown of rnns https://dennybritz.com/posts/wildml/recurrent-neural-network... reply ultra_nick 58 minutes agoprevI feel like neural networks are increasingly going to look like code. The next big innovation will be whoever figures out how to convert MOE style models into something like function calls. reply alsxnt 4 hours agoprevRecurrent neural networks can be used for arbitrary computations, the equivalence to Turing machines has been proven. However, they are utterly impractical for the task. This seems to be a state machine that is somehow learned. The article could benefit from a longer synopsis and \"Python\" does not appear to be relevant at all. Learning real Python semantics would prove quite difficult due to the nature of the language (no standard, just do as CPython does). reply danans 4 hours agoparent> Recurrent neural networks can be used for arbitrary computations, the equivalence to Turing machines has been proven. However, they are utterly impractical for the task. Karpathy's 2015 RNN article [1] demonstrated that RNNs trained character-wise on Shakespeare's works could produce Shakespeare-esque text (albeit without the narrative coherence of LLMs). Given that, why wouldn't they be able to handle natural language as formulaic as code review comments? In that case inference was run with randomized inputs in order to generate random \"Shakespeare\", but the structure of the language and style was still learned by the RNN. Perhaps it could be used for classification also. 1. https://karpathy.github.io/2015/05/21/rnn-effectiveness/ reply vidarh 3 hours agorootparentFor RNN abilities, RWKV is worth a look[1] It's billed as \"an RNN with GPT-level LLM performance\". [1] https://www.rwkv.com/ reply fnord77 3 hours agoprev> To model the state, we need to add three hidden layers to the network Why 3? And why use \"h\" for layer names? reply danans 4 hours agoprevI'd like to see a cost vs precision/recall comparison of using a RNN vs an LLM (local or API) for a problem like this. reply danans 16 minutes agoparentHah... Ask, and HN (sorta) delivers. Not RNN, but self-finetuned LLM cost vs performance compared to GPT-4. https://openpipe.ai/blog/mixture-of-agents reply sdwr 4 hours agoprevThis is new to me, and therefore bad and scary. It's great that you know NN well enough to fold it into regular work. But think of all us poor regular developers! Who now have to grapple with: - an unfamiliar architecture - uncertainty / effectively non-deterministic results in program flow reply sva_ 4 hours agoparentNN are in principle deterministic (unless you add randomness to it such as is the case with LLM top p/k temperature). Uncertainty is probably the better word of the two, but I feel like there should be a different term. reply dinobones 1 hour agoprevThis article was going decently and then it just falls off a cliff. The article basically says: 1) Here’s this complex problem 2) Here’s some hand written heuristics 3) Here’s a shitty neural net 4) Here’s another neural net with some guys last name from the PyTorch library 5) Here are the constraints with adopting neural nets You can see why this is so unsatisfying, the leaps in logic become more and more generous. What I would have loved to see, is a comparison of a spaghetti code implementation vs a neural net implementation on a large dataset/codebase, then show examples in the validation set that maybe the neural net generalizes to, or fails at, but the heuristic fails at, and so on. This would demonstrate the value of neural nets, if for example, there’s a novel example that the neural net finds that the spaghetti heuristic can’t. Show tangible results, show some comparison, show something, giving some rough numbers on the performance of each in aggregate would be really useful. reply thih9 3 hours agoprev> Of course, we should try and avoid writing spaghetti code if we can. But there are problems that are so ill-specified that any serious attempt to solve them results in just that. Can you elaborate or do you have an example? Based on just the above, I disagree - I'd say it's the job of the programmer to make sure that the problem is well-specified and that they can write maintainable code. reply godelski 55 minutes agoprev> Humans are bad at managing spaghetti code. Of course, we should try and avoid writing spaghetti code if we can. But there are problems that are so ill-specified that any serious attempt to solve them results in just that. Sounds like a skill issue. But seriously, how many programmers do you know that reach for the documents or help pages (man pages?) instead of just looking for the first SO post with a similar question? That's how you start programming because you're just trying to figure out how to do anything in the first place, but not where you should be years later. If you've been programming in a language for years you should have read a good portion of the docs in that time (in addition to SO posts), blogs, and so much more. Because the things change too, so you have to be keeping up, and the truth is that this will never happen if you just read SO posts to answer your one question (and the next, and the next) because it will always lag behind what tools exist and even more likely will significantly lag because more recent posts have less time to gain upvotes. It kinda reminds me of the meme \"how to exit vim.\" And how people state that it is so hard to learn. Not only does just typing `vim` into the terminal literally tell you how to quit, but there's a built in `vimtutor` that'll tell you how to use it and doesn't take very long to use. I've seen people go through this and be better than people that have \"used\" vim for years. And even then, how many people write `:help someFunction` into vim itself? Because it is FAR better than googling your question and you'll actually end up learning how the whole thing fits together because it is giving you context. The same is true for literally any programming language. You should also be writing docs to your code because if you have spaghetti code, there's a puzzle you haven't solved yet. And guess what, documenting is not too different from the rubber ducky method. Here's the procedure: write code to make shit work, write docs and edit your code as you realize you can make things better, go on and repeat but not revisit functions as you fuck them up with another function. It's not nearly as much work as it sounds and the investments compound. But quality takes time and nothing worth doing is easy. It takes time to learn any habit and skill. If you always look for the quickest solution to \"just get it done\" and you never come back, then you probably haven't learned anything, you've just parroted someone else. Moving fast and breaking things is great, but once you have done that you got to clean up your mess. You don't clean your kitchen by breaking your dining room table. And your house isn't clean if all your dishes are on the table! You might have to temporarily move stuff around, but eventually you need to clean shit up. And code is exactly the same way. If you regularly clean your house, it stays clean and is easy to keep clean. But if you do it once a year it is a herculean effort that you'll dread. reply lawlessone 4 hours agoprev [–] Edit: ok i see it detects code. I thought it was replacing bits of ANN with custom python functions. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post discusses embedding programs in recurrent neural networks (RNNs) and how trained RNNs can outperform hand-written algorithms.",
      "It provides a detailed example of detecting program code in messages, comparing simple decision rules, a hand-written algorithm, and an RNN-based approach.",
      "The post highlights the advantages of RNNs, such as encoding state machines, using trainable activation functions, and handling complex tasks with data-driven discipline."
    ],
    "commentSummary": [
      "The article discusses the construction of a neural network using Python but lacks details on testing and obtaining training data, which are crucial for ensuring the model's generalization to unseen inputs.",
      "The discussion highlights the Universal Approximation Theorem, which states that neural networks can represent any function to a desired level of accuracy, but emphasizes that learning these approximations is not guaranteed.",
      "There is a debate on whether Recurrent Neural Networks (RNNs) are being replaced by transformers, with some arguing that RNNs still have unique advantages, such as constant memory usage, which transformers lack."
    ],
    "points": 193,
    "commentCount": 43,
    "retryCount": 0,
    "time": 1719838058
  },
  {
    "id": 40846428,
    "title": "Who is hiring? (July 2024)",
    "originLink": "https://news.ycombinator.com/item?id=40846428",
    "originBody": "Please state the location and include REMOTE, INTERNS and&#x2F;or VISA when that sort of candidate is welcome. When remote work is not an option, include ONSITE.Please only post if you personally are part of the hiring company—no recruiting firms or job boards. One post per company. If it isn&#x27;t a household name, explain what your company does.Commenters: please don&#x27;t reply to job posts to complain about something. It&#x27;s off topic here.Readers: please only email if you are personally interested in the job.Searchers: try https:&#x2F;&#x2F;hnresumetojobs.com, https:&#x2F;&#x2F;hnhired.fly.dev, https:&#x2F;&#x2F;kennytilton.github.io&#x2F;whoishiring&#x2F;, https:&#x2F;&#x2F;hnjobs.emilburzo.com.Don&#x27;t miss these other fine threads:Who wants to be hired? https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40846426Freelancer? Seeking freelancer? https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=40846427",
    "commentLink": "https://news.ycombinator.com/item?id=40846428",
    "commentBody": "Who is hiring? (July 2024)191 points by whoishiring 3 hours agohidepastfavorite171 comments Please state the location and include REMOTE, INTERNS and/or VISA when that sort of candidate is welcome. When remote work is not an option, include ONSITE. Please only post if you personally are part of the hiring company—no recruiting firms or job boards. One post per company. If it isn't a household name, explain what your company does. Commenters: please don't reply to job posts to complain about something. It's off topic here. Readers: please only email if you are personally interested in the job. Searchers: try https://hnresumetojobs.com, https://hnhired.fly.dev, https://kennytilton.github.io/whoishiring/, https://hnjobs.emilburzo.com. Don't miss these other fine threads: Who wants to be hired? https://news.ycombinator.com/item?id=40846426 Freelancer? Seeking freelancer? https://news.ycombinator.com/item?id=40846427 applehire 2 minutes agoAppleSenior Software EngineerAustin & CupertinoFull-timeOnsite The ASE Data Infrastructure team is seeking an experienced senior software engineer to contribute to the development of our next-generation object storage infrastructure. As a key member of our team, you will play a critical role in designing and implementing solutions that enable seamless collaboration across Apple engineering teams. Requirements: - In-depth experience with object storage implementations: S3, GCS, Azure Blob Storage, MinIO, and Ceph - we are looking for candidates who have hands-on expertise with these technologies. - Proficiency in Rust: You will be working extensively with the Rust programming language to build high-performance systems. - Expertise in debugging and performance analysis: Experience driving performance analysis of end-to-end distributed systems is essential. We need someone who can quickly identify and resolve issues at scale. - Micro-services architecture and container orchestration expertise: You have experience working with containers (e.g., Docker) and orchestrated them with tools like Kubernetes or similar. This knowledge is critical to our system's scalability and reliability. - Relational and non-relational database expertise: PostgreSQL, Cassandra, and other databases are your area of specialization. You know how to design and implement efficient data storage and retrieval systems. - Experience in data migration, disaster recovery, and capacity planning: We're talking about large-scale data management here. Your experience in these areas will be invaluable. Responsibilities: Review and provide constructive feedback on pull requests and designs, fostering a culture of continuous learning and knowledge sharing. Collaborate with other senior team members across multiple sites to define high-quality and reliable standards for our solutions. Location: Austin & Cupertino (onsite work required) If you're passionate about building innovative software and pushing the boundaries of data storage, we want to hear from you! You can either apply directly (https://jobs.apple.com/en-us/details/200556764/senior-softwa...) or send me an email at mansur.ashraf@.com reply marvinkennis 7 minutes agoprevRectangleSoftware engineerFull-timeChicago USAOnsitehttps://rectanglehq.com The global supply chain has a collaboration problem. It's a mess of TMSs, ERPs, WhatsApp, emails, and spreadsheets. For the last 50 years, it's been solved with brittle EDI integrations to bridge walled gardens. We're taking a different approach. We aim to reshape how the supply chain collaborates and as a result redefine how trillions of dollars of goods get moved around the world. We’re a small product team who have worked on anything from foundational technology at Flexport, self-driving cars, to core AI experiences and products at Google. We’re looking to bring a high level of craft to an industry that runs on incredibly outdated software. If you're interested in learning more, send us an email at hn@rectanglehq.com. reply beastawakens 7 minutes agoprevSenior Frontend Engineer @ SmileIDhttps://www.usesmileid.com/Full-time & REMOTE Smile ID is Africa’s leading digital identity verification, fraud detection, anti-money laundering, and KYC compliance solution for businesses scaling across the continent. We're growing and scaling fast and looking for more talented people to join our team. Find out more and apply here: https://wellfound.com/recruit/jobs/2929597 reply nullindividual 35 minutes agoprevFor those posting REMOTE (US) or Washington State-based jobs, please provide salary ranges in accordance with RCW 49.58.110. The civil penalties for not doing so can be significant. reply zegerjan 18 minutes agoparentI'd not be surprised if that would lead to job with REMOTE (US EXCEPT WA) soon. reply cde-v 9 minutes agorootparentIt would have to be \"REMOTE (US EXCEPT CA, CO, CT, MD, NV, NY, RI, WA)\" unless there is something different about the WA transparency law that I don't know. reply justicz 13 minutes agoprevCharge RoboticsSenior Software Engineer (Full-Stack)Full-timeOnsite / HybridSF Bay Area Hi HN! We’re a YC-backed startup building robots that build large-scale solar farms. We just got back from the first commercial deployment of our system on a real-world solar construction site. Check out a video of our system here! https://youtu.be/ZZ2fP1Y5Z2E It turns out that construction companies literally can't build solar fast enough, so what we're doing is a crucial part of switching the grid over to renewable generation. We're looking to bring another talented full-stack software engineer onto our small team. You’ll be a key contributor, helping bring our next generation of mobile solar factories to life. Your work will involve architecting, developing, and maintaining high-quality code and documentation for a web application that controls our fleet of fully-automated factories Come work with us if you love: * Greenfield web application development * Making a positive climate impact through your work * Hacking on massive construction equipment! Apply here: https://www.workatastartup.com/jobs/67459 reply freeqaz 23 minutes agoprevFigmaSenior/Staff Security EngineerRemote, or in-office SF/NYCFull-time$149k - $350k The security team at Figma spans a bunch of areas (AppSec, Infra, AI/ML, etc). I am currently working on securing our recently announced AI features -- how do you deal with model security? Especially when some of those models include private data? Not a lot of prior art here, so it's an interesting learning experience! We're really just looking for smart people that want to find problems to chip at. The company is growing fast and there is no shortage of interesting work. (I wrapped up my YC startup to join the team!) Generic job req is here: https://boards.greenhouse.io/figma/jobs/4214880004 Feel free to email me at fwortley at .com reply burnaway 20 minutes agoprevIVPNSenior Infrastructure EngineerRemote (UTC-1 to UTC+3)Full-timehttps://www.ivpn.net IVPN is a privacy-focused consumer VPN service in operation since 2010. We have high ethical standards, regular security audits and a stellar reputation among security and privacy analysts. We are looking for a Senior Infrastructure Engineer to help: -- maintain our existing infrastructure -- implement a new server architecture for our VPN gateways that we have been working on for the past year that is open, immutable, diskless, secure and built on bare-metal servers The position is fully remote with high level of autonomy. Requirements are experience with: -- infosec, specifically with regards to cryptographic controls used on servers e.g. Public key crypto, reproducible builds, TPM -- TCP/IP, UDP, SSL/TLS and other related Internet protocols -- configuration management tools e.g. Ansible, Puppet, Salt -- managing Linux servers, including bare-metal -- Python, Go or other scripting languages You can email me if you have any questions about the role: viktor at ivpn.net. If you are ready, it's better to apply here: https://ivpn.recruitee.com/o/senior-infrastructure-engineer-... Note: target budget for this role is 55k to 70K EUR pa. depending on experience. reply mrroryflint 4 minutes agoprevLondon - Tipalti (Fintech) We're hiring two hybrid mobile engineers (React Native) - one senior, one mid. Also hiring one senior QA. Send me a message (email is on rory.codes) and I can get the ball rolling very quickly. reply EgregiousAI 16 minutes agoprevEgregious AIFullstack EngineerLondonfull-timeONSITEhttps://egregious.ai/ Egregious is making the internet a better place by building AI to detect scammy and deceptive accounts on social media. We use maths, ML and tonnes of code to map & simulate the emergence and spread of harmful content. We're the smallest big data company you've never heard of and we're looking for an exceptional, mission-oriented engineer to join us as employee #3 in our London office. You'll need * Exceptional React, Vite, Javascript, CSS & Tailwinds skills * Solid understanding of asynchronous request handling and RESTful APIs * Proficiency with Git, Docker, Kubernetes and Azure/AWS/GCloud * Experience with PostgreSQL, MySQL, ElasticSearch, MongoDB or similar * Experience with Python is a plus Unfortunately we can't sponsor Visa holders. Candidates must be EU/UK passport holders and resident in the UK for the last 5 years. Please send your resume to: rupert at egregious dot ai. reply jeswin 12 minutes agoprevInfrastructure and DevOps EngineerFull-timeIndia CredCore is a well-funded startup building solutions for the US debt sector. Our product suite uses AI to make sense of financial data (such as Credit Agreements), and aggregate the information into reports and dashboards. We're looking for people who can help us in the following areas: - Augment our Security Team. We deal with sensitive data, and take security seriously. The potential candidate needs to be a highly-qualified, proven expert on Web/Cloud Application Security. - DevOps Engineering (Azure, AWS, GitOps etc) - Azure / Windows IT Pros who can set up best practices for Windows (and Microsoft Apps) on the Cloud. If you have one or more of these skills, please reach out to me at jeswin@credcore.com reply themanmaran 14 minutes agoprevOmniAI (YC W24)https://getomni.aiFounding EngineersSF In-PersonFull Time$125k – $175k • 0.50% – 01.50% Omni is building an ETL for unstructured data. Tech stack is TypeScriptPostgresDockerReactand about 5 different LLMs The main things we spend our time on: 1. Wrangling LLMs into providing predictable outputs 2. Running data transforms at scale 3. Processing multimodal data (audio, documents, images, etc.) All of these problems are hard, especially in conjunction with each other. If you’ve had any experience with structured LLM output, we’d love to chat. If you're interested in learning more, drop me an email (my name at the url) or apply at https://www.workatastartup.com/jobs/67540 reply iliketrains 2 hours agoprevMaFi GamesSenior SWE/game devContract or full-time$70-110kRemoteC# I’m the co-founder of MaFi Games – an indie studio behind the game Captain of Industry. We are a small but passionate team who gave up their jobs at Google/Nvidia to pursue building the best factory simulation game possible, and we need more hands! We are looking for an experienced software engineer to grow the team and accelerate our progress. We strongly prefer candidates with a background in game development or with experience in desktop UI, 3D graphics, and performance optimizations. Some reasons you’d enjoy working with us: * A multicultural, collaborative, and innovative work environment where your voice is heard. * Fully remote job with flexible working hours and vacation schedule. * High quality C# code base, code reviews, tests. * High work satisfaction, work on a popular video game with a wonderful community. As an example of our technical work see https://www.captain-of-industry.com/post/cd-31. If interested, please see the detailed info and requirements at https://www.captain-of-industry.com/jobs, thanks! Note that this is a fully remote job and we are happy to consider candidates from any country around the world! reply visviva 3 hours agoprevUmbraMultiple rolesOnsite & Hybrid (Santa Barbara, Austin, Washington DC)https://umbra.space/ Umbra builds next-generation space systems that observe the Earth in unprecedented fidelity. Our mission: Deliver global omniscience. To stay ahead of climate change, geopolitical risk, and other major crises, we need a global understanding of what is changing, where, and how fast. Umbra is built around the idea that by providing easy access to the highest quality commercial satellite data available, we can become an indispensable tool for the growing number of organizations monitoring the Earth. We empower our customers with the ability to create the solutions that inform, inspire, and address our planet’s most pressing needs. We’re helping to create a brand new industry that has never meaningfully existed before. To make our vision a reality, we're gonna need some help. We're looking to fill a variety of mid- to senior-level roles, including openings in Spacecraft Operations, Mission Systems Architecture, Embedded Software Engineering, and Corporate Counsel. If you're interested in helping us build, please apply at https://umbralab.bamboohr.com/careers reply jjwarmerdam 3 hours agoprevMake Waveshttps://makewaves.fmLead developer & Marketing ManagerFULLTIMEONSITE / HYBRID (>= one day in the office a week)Amsterdam, NL Make Waves was founded in 2021 by two brothers with a track record in the music and tech industry. We’re building a kinder, more helpful platform for independent artists to release their music and build a following. By providing artists with easy-to-understand tools and insights in a notoriously over-complicated industry, we want to empower DIY artists to control their careers without making concessions. In the last years we’ve built the platform from the ground up and are now entering a phase where we’re focusing on growth and maturity. We’ve also secured funding for the next three to four years, so we can provide a stable job with great growth potential. We are actively looking for a lead developer (with a hope you can transition into a CTO role in the near future) and a marketing manager. Unfortunately we can’t offer relocation or visa sponsorship and are only considering candidates that are able to join us in the office weekly. Check out https://kb.makewaves.fm/general/careers for more details. reply mi3law 2 hours agoprevAO LabsBuilding an alternative to backpropagationhttps://www.aolabs.ai/Berkeley, CA + remote AI systems struggle with edge cases and understanding local context despite increasing model sizes. From our research at UC Berkeley into the evolution of intelligence from simple organisms, we’ve discovered the missing link is continuous learning (deep learning is pre-trained by design). Models built with our framework learn through customizable parameters similar to animal instincts, allowing for AI grounded with built-in memory and reasoning. We're a community of 160+ developers and researchers building general intelligence from the bottom-up from places like Berkeley, NYU, Imperial College, and Google. We're building way outside of the current paradigm and we're looking for collaborators at all levels --hackers, contributors, the curious-- as we'll be making our first hires soon. Email with \"HN Hiring\" in subject line to: ali at aolabs.ai or chat with us in our discord: https://discord.gg/Zg9bHPYss5 This post is near identical to mine from last month; if you reached out then, please know that I'll respond to you soon (I've been busy wrapping up a fundraise). reply lakrikor1 4 minutes agoparentCan you fix the FAQ boxes on your website to be clickable...or are they not supposed to be clickable? I would like to learn more about your mission. reply nfriedly 5 minutes agoprevFullstoryAtlanta or Remote (USA)https://www.fullstory.com/ Fullstory provides session replay and analytics for websites and mobile apps. We have a couple of engineering roles open: * Senior Security Engineer - https://www.fullstory.com/careers/jobs/915356a7-05a3-4ff7-b9... * Senior Data Engineer - https://www.fullstory.com/careers/jobs/580a69d0-05c6-4ed1-85... ...and a number of other roles in sales, finance, marketing, and customer support. See https://www.fullstory.com/careers/ for the full list and some general info. I'm not sure what any of the pay ranges are, but I can say that as a Staff Software Engineer on the mobile team, I make a bit over $200k base, in addition to bonus and equity. All the usual benefits: healthcare, 401k matching (Vanguard), unlimited PTO (I take about 5 weeks a year), etc. Feel free to reply here or email nathan@[company site] if you have any questions. (To apply, please submit your info on the website.) reply nikhilalmeida 37 minutes agoprevKharonDataREMOTE (Madrid/EU/US)Full-timehttps://kharon.com/ Kharon navigates risk at the intersection of global security threats + international commerce. We turn complex data into actionable insights, enabling top financial institutions to make informed decisions. Why Kharon? - Impact: Work on major global crisis events. - Tech Stack: AWS, Python, PySpark, Kubernetes, Docker, PostgreSQL, Graph DBs, Elasticsearch. Access to a unique dataset with home grown propriety technologies to analyze it. - Team: 2M MAUs, massive transaction volume) that developers love. We've raised $26M from Sequoia & Paradigm. We ship constantly - multiple production releases per week. Reach out at join [at] privy.io - these emails go to me. https://jobs.ashbyhq.com/privy reply jasperstory 50 minutes agoprevYarn (YC W24)https://www.yarn.soFounding EngineersNYC In-PersonFull Time Yarn is Figma for video. We're taking a novel approach in the video space. We think making videos is reasoning-heavy, so we're focusing on using LLMs and interfaces to orchestrate the best domain-specific models. We want to help people create high-end output, not programmatic AI slop. We believe in small, in-person, talent-dense teams. We're looking for smart generalists that enjoy picking up new challenges and learning new tech. If you're interested in learning more, get in touch jasper @ yarn.so or apply at https://www.workatastartup.com/jobs/66928 reply amacneil 54 minutes agoprevFoxgloveRemoteFull Time or Contracthttps://foxglove.dev/ Foxglove is the leading observability platform for robotics developers. We help robotics and autonomous vehicle companies log, ingest, organize, and visualize multimodal data. We're well funded (Series A, ~20 people), with an experienced and fast-moving team. Seeking like-minded people to join us! - Senior / Staff Backend Engineer (Remote, US time zone, mostly TypeScript / Node.js, bonus if you have experience with any of Rust, C++, Go, Python) - Senior / Staff Rust Engineer (Remote, Australia / New Zealand time zone) - Sales / Solutions Engineer (Boston MA, ideally experienced with cloud + robotics) https://foxglove.dev/careers reply alcaide-mor 48 minutes agoparentAt the Careers page it says \"Remote - Americas\" and then in the actual position's page it says \"For this position, we're looking for someone in North America.\" So it's North America, not Americas. reply slimjimrick 3 hours agoprevEigenFull-Timeproduct engineers, ex-technical founder engineers, and ai engineersREMOTE (all remote)Hiring GMT-8 to GMT+2 Eigen helps companies build world-class AI products, enabled by the cutting edge of AI and LLMs. Products span spaces from edtech to leadgen to finance. * we are a real business — profitable with $XM in revenue this year. * we need: product engineers, ex-technical founders, and ai engineers to build products. Experience with typescript stacks and/or python is fantastic. * smart team. ex. stanford ai lab, harvard, uber, tinder, etc. * if you're a fullstack ts dev and are interested in growing into AI, this role is perfect for you. * if you like learning about new industries and working on different kinds of projects regularly, this role is perfect for you. send an email to careers [at] eigen (.) net with a resume/site link/whatever is indicative of you. No need to write long email, just a \"hi\" is great :) reply joshsenseiag 1 hour agoprevSensei Aghttps://www.sensei.ag/Senior Data Pipeline EngineerRemote within USFull-time Are you ready to harness technology for a profound environmental impact? Be a part of a pioneering movement in food production as a Software Engineer at Sensei Ag, a leader in indoor farming technology co-founded by visionaries Larry Ellison and Dr. David Agus. Our mission is not just to transform farming but to do so sustainably—optimizing indoor spaces to yield fresher, more sustainable produce year-round. Within Sensei Ag, the mission of the Data Insights group is to foster a data-driven organization by providing and supporting robust, organized wise use of data to gain insights and make decisions. This role, as part of the Data Insights group, supports the development of data processing pipelines for Sensei Ag. Day-to-day tasks would include monitoring of data processing, troubleshooting and debugging data processing tasks, and development of new data flows for existing platforms. This role will also provide basic support to the development of new data processing technologies including setup and administration of new tools and engineering support to new data software systems. The role offers multiple growth opportunities including work in data science, data visualization, and sensing/control systems. An ideal candidate will have a passion for helping build systems which will empower Sensei Ag to grow crops more efficiently and sustainably. Apply now to join our team and be part of the revolution in sustainable farming: https://boards.greenhouse.io/senseiag/jobs/6016099003 reply mertens 1 hour agoprevCrazyGameshttps://about.crazygames.com/REMOTEFull-timeMultiple roles With recent technologies such as WebGPU and WebAssembly, the browser is quickly becoming a powerful gaming platform. High-quality 3D graphics and near-native level performance are becoming possible without the need for downloads, apps, or platform-specific development. Our browser games platform is already reaching more than 35 million people per month. We are self-funded, profitable, remote-first, and fast-growing. We are currently looking for multiple people on the product and engineering side to bring our product to the next level. Please note that we're looking for people who can work on a European timezone. * Product Engineer: https://crazygames.recruitee.com/o/remote-product-engineer-b * Data scientist: https://crazygames.recruitee.com/o/data-scientist * Front-end engineer: https://crazygames.recruitee.com/o/remote-senior-front-end-e... * Performance marketing specialist: https://crazygames.recruitee.com/o/fully-remote-ua-manager We also have 1 role in Belgium specifically: * Software Engineer: https://crazygames.recruitee.com/o/software-engineer reply mangecoeur 1 hour agoprevCompany: Planeto (early stage cleantech startup) Planeto develops next generation software for design and operation of clean district heating and cooling networks, to accelerate adoption of renewable energies for zero emissions cities. Hiring a Senior Software Engineer, REMOTE (EU/UK/CH, head office in Geneva, Switzerland) https://cord.co/u/planeto-/jobs/125274-senior-software-engin... reply shulu 2 hours agoprevLooploop.comMarketing/Product/Design/EngineeringOn-site San Francisco, CA, Chicago, ILH1B OK Loop is on a mission to unlock profits trapped in the supply chain (https://loop.com/article/unlock-profit-trapped-in-your-suppl...) and lower costs for consumers. Bad data and inefficient workflows create friction that limits working capital and raises costs for every supply chain stakeholder. Loop’s modern audit and pay platform uses our domain-driven AI to harness the complexity of supply chain data and documentation. We improve transportation spend visibility so companies can control their costs and power profit. That is why industry leaders like J.P. Morgan Chase, Great Dane, Emerge, and Loadsmart work with Loop. A recent WSJ piece on Loop - on.wsj.com/3PE357W 1. Raised $65m from JPM GEP, Founders Fund, 8VC, Susa Ventures, Flexport, Index, and Expa. 2. 35 paying enterprise customers with multiple-year contracts; 60+ customers in the pipeline. 3. High-caliber team of engineers and operators from Google, Scale AI, Flexport, Uber, Rakuten, Square, Meta, Stanford, Brown, Princeton, and Yale. 4. 5+ years cash runway Fullstack Engineer San Francisco - https://boards.greenhouse.io/loop/jobs/4102236004 Fullstack Engineer Chicago - https://boards.greenhouse.io/loop/jobs/4830548004 Senior Product Designer San Francisco - https://boards.greenhouse.io/loop/jobs/4126037004 reply mousetree 2 hours agoprevSeen Financehttps://seen.comFull timeBerlin, ChicagoHybrid on-site At Seen, we believe that everyone deserves a fair opportunity to (re)build their credit. We understand how tough it can be to qualify for a credit card when your credit score isn't where you want it to be. Seen is a new fintech company, started in 2023 following the acquisition of Sable (YC S19) by Snap Finance. We've recently launched a credit card in the US for people with less than ideal credit. We plan to offer other novel credit products in the future. It's a great time to join as the team is small (15 engineers) and we're growing fast. We're a startup but we're also lucky to have the financial support and customer base from our parent company. We're hiring for: * Senior and Staff Software Engineers (Berlin) * Senior and Staff Data Engineers (Chicago) Our mobile app, web app, and backends are written in Typescript. Our data stack uses Python and SQL. If you're interested, just email us your resume or LinkedIn page: join-berlin@seen.com (for both Berlin and Chicago roles) reply mjulian 1 hour agoprevStealthSan Francisco, CA (hybrid: M-W-F ONSITE)Full-timeBackend Engineer We're building a product in the cost management space (yes, another one), but this one is being built by the founders of The Duckbil Group (Corey Quinn & Mike Julian), an AWS-focused cost management consultancy. We're looking for someone to join as second engineer to help build out the product. Mostly looking for backend expertise. Past experience with AWS billing or Clickhouse are a huge bonuses. Tech stack: Python (Flask), React, Typescript, Postgres, Clickhouse, AWS We can sponsor the transfer of an existing visa but cannot sponsor the creation of a new visa. You can apply here: https://duckbillgroup.typeform.com/to/LSRU50mB reply getrecharge 42 minutes agoprevRechargeEngineering, DevOpsREMOTEFull-timehttps://getrecharge.com/ Recharge is an e-commerce and subscription management platform that powers the growth and retention strategies of more than 20,000 brands serving over 100 million subscribers. We're looking to expand some of our primarily backend engineering teams to support new feature development and product reliability. Tech stack: Python, Flask, MySQL, Redis, Elasticsearch, React, Vue, GCP, Kubernetes, etc. We're hiring: - Senior DevOps Engineer: https://grnh.se/431b341b2us - Senior Backend Engineer (Core Commerce): https://grnh.se/f265a47c2us - Senior Software Engineer (Merchant Experience): https://grnh.se/d9fbc82f2us The above are all referral links for current jobs, but you can also see a full list of jobs and apply online at https://getrecharge.com/careers/ reply ck_hiring 1 hour agoprevChurnkeyTechnical Support EngineerRemote (preferably in North or South American time zones)$80k to $110k per yearchurnkey.co Churnkey is a rapidly expanding customer-centric subscriptions company actively looking for a Senior Full-Stack Engineer. As an early engineer at Churnkey, you’ll work directly with our CTO and product team in architecting and developing new features and product lines. To get a sense of what we’ve been building lately, check out our launch log(https://docs.churnkey.co/launch-log) We’re looking for: We are looking for a Technical Support Engineer to serve as a key player in both supporting our clients and contributing to our product development. This hybrid role is designed for individuals passionate about technology and customer satisfaction. Your responsibilities will not only involve addressing technical queries and issues, but also participating in feature development and product enhancements. Desired skills to be successful: * Technical Skills: Strong problem-solving skills with a foundation in software development, preferably in VueJS and NodeJS. A knack for troubleshooting and debugging technical issues. * Customer Support Experience: Experience with customer support systems like Linear, Intercom, etc., and an ability to manage customer interactions effectively. * Product Familiarity: Willingness to deeply understand our product suite, our technology stack, and the payment platforms we integrate with, such as Stripe and Chargebee. If you or someone you know is a strong fit, please apply here: https://churnkey.co/careers reply j_kauf 1 hour agoprevSitewireFull Stack Rails EngineerFull-TimeREMOTE (US-only)https://sitewire.co Learn more or apply at https://wellfound.com/l/2zf9K9 - I personally review every application, so just mention you found us on HN. Sitewire improves cash flow for real estate developers and members of the skilled trades. Our customers rely on us for project budgeting, virtual inspections, and draw management. The mobile-first, self-service draw process empowers workers with instant payments while combating fraud. We’re looking for an experienced and entrepreneurial Full-Stack Rails Engineer (senior or above) to help us achieve an ambitious growth plan. You will be a core member of the engineering team, working alongside peers with the support of a mature founding team expert in construction, mobile, and IoT. Technologies: Ruby on Rails, Hotwire (Turbo / Stimulus), Tailwind, Postgres, Sidekiq Experience with 3D graphics, machine learning, mobile, or React are all pluses. reply konz 2 hours agoprevML6Machine Learning Engineer, Data EngineerPython, TensorFlow, PyTorch, GCP, AWS, AzureFull-timeAmsterdam, Berlin, Ghent (EU) On-site/hybrid We are a Machine Learning consulting company that builds end-to-end Machine Learning solutions. By applying the latest AI research, we keep our clients at the forefront of innovation. If you are interested check out: https://www.ml6.eu/resources/resource-library and https://www.ml6.eu/client-cases Work on innovative projects for the biggest clients across Europe such as Randstad, ASML, FUNKE, and many more! Whether it’s about leveraging LLMs to improve customer support, building data lakes on cloud platforms to improve storage or implementing models using sensor data for quality control. You can find it all at ML6. You will mostly work with Python and a range of ML frameworks such as TensorFlow, PyTorch, or HuggingFace Transformers to solve hard Machine Learning tasks and help bring these application into production by building data pipelines and cloud infrastructure on all of the major cloud providers (GCP, AWS, Azure). We are looking for: • (Senior) Data Engineer • (Senior) Software Engineer • (Senior) Machine Learning Engineer • Alliance Manager – Azure • AI Team Lead • AI Client Executive • AI Client Partner • AI Project Manager • Talent Partner Apply at: https://ml6.eu/join-us reply edavidson-LA 1 hour agoprevLeagueAppsOffice in NYC, or Remote almost anywhere in USFull-timehttps://leagueapps.com/ LeagueApps is a youth sports management SaaS. We help youth sports organizers run their business, providing the tools they need to manage registration, scheduling, payments, and more. We believe that every kid should have the opportunity to play, and we provide 1% of our revenue to youth sports accessibility. We're hiring: - Product Manager https://grnh.se/592d27243us - Senior Fullstack Engineer https://grnh.se/cb8ca0143us - Business Development Representative (New York Only) https://grnh.se/b2a469af3us - Growth Marketing Manager (New York Only) https://grnh.se/3bb6fb983us Find other roles on our Job Board: https://grnh.se/0b718ab43us You can also use this link to join our talent community, and our recruiters will reach out when new positions become available: https://grnh.se/b1bedbd53us These links are all referral links, but you can also apply via https://careers.leagueapps.com/ reply jayfk 2 hours agoprevJUPUS GmbHSenior Software Engineer / Emerging LeaderCologneREMOTE We are JUPUS, one of Germany's fastest-growing legal tech startups, and we are looking for a talented and experienced Senior Software Engineer - Emerging Leader to join our team. This position offers an exciting opportunity to grow into a leadership role, providing technical guidance and mentorship to our team. Our stack is Python/Django in the backend and TypeScript/Vue.js in the frontend. Looking for someone who is full stack and can assist leading a team of 6 engineers. Salary is up to 84k/year. Apply here https://join.com/companies/jupus/11486463 or send me an email at jannis.gebauer@jupus.de reply Jreys 2 hours agoprevVaarstUK, Bristol - HybridFull or part timecan't sponsor visas Vaarst focuses on the energy transition space, we're currently developing autonomy solutions for offshore wind where we're working on subsea autonomous visual inspection combined with our data and ML platform and SubSLAM technology. We are growing quickly and looking for people who enjoy working on challenging projects in fast paced environments - think deploying autonomous robots to autonomously survey subsea structures. Robotics, Senior and Principal Engineers - £100-150k Software, Senior and Principal Engineers (C++, Python, Computer vision, SLAM) - £100-150k Unreal Engineers, Senior and Principal UX Designers Product Managers https://careers.vaarst.com/#jobs reply bgrgndzz 1 hour agoprevHockeyStackCracked Backend/Infra EngineerONSITE (San Francisco)VISA sponsorship available$180k-$250k + 0.2-0.4% stock option HockeyStack is creating a single source of truth and reporting for marketing, sales, and all customer data. We came out of YC S23, and raised $4.3M from General Catalyst, Madrona, et al. We went from 0 to millions in ARR in a really short time and are in a DEEPLY URGENT need of really good engineers to help scale infra and backend. The product collects data from CRM, marketing automation, ad platforms and data warehouses. The data then gets cleaned and transformed into a unified format. Out of the unified data format, customers can create any type of report the want using the UI. We have a dynamic query engine that transforms UI inputs into a query. Because of the flexibility and how much data we process, keeping integrations up, transforming data accurately, reporting accurately and performantly is a huge technical challenge. Please email me at bugra @ hockeystack.com if you want to take a crack at some of these problems. Worth noting that we are fully in-person in a beautiful office near Oracle Park in SF! reply uxhacker 1 hour agoprevRemote Generative AI Enthusiast for Luxury Fashion Content Creation We’re a startup team of five, blending top talent from technology and luxury brands. Our mission: revolutionize content creation for luxury brands using generative AI. Our CTO, on his third successful startup journey, brings a wealth of experience from diverse industries. Position: • Role: Generative AI Enthusiast • Location: Remote, with occasional travel for team meet-ups • Experience: Enthusiasm for generative AI and some experience with Python. Direct AI experience is a plus but not required. About Us: • Team: A blend of technology experts and creatives, including a member known for iconic fashion brand content. • Current Locations: Poland and Rome, but we’re open to anyone working from anywhere. • Industry: Luxury fashion content powered by AI. • Current Stage: Early-stage startup, crafting our identity and product. Why Join Us: • Collaborate with industry leaders. • Be at the forefront of AI innovation in luxury fashion. • Enjoy a flexible remote work environment. How to Apply: Send your resume and a brief cover letter to sjloydpage at gmail dot com reply ilaksh 5 minutes agoparentI sent you an email, seeing a problem with that address: \"Your message wasn't delivered to [the email address you wrote] because the address couldn't be found, or is unable to receive mail.\" I can deliver my message via this GitHub gist: https://gist.githubusercontent.com/runvnc/5c33d935c2d96a7d24... reply cuchoi 2 hours agoprevEnveritas (YC S18, Non-Profit)Data ScientistRemote / Globalhttps://enveritas.org/jobs/ Enveritas is a 501(c)3 non-profit working on sustainability issues facing coffee farmers around the globe. We provide sustainability assurance for the coffee industry. We visit smallholder coffee farms around the world to understand their social, economic, and environmental practices. In 2024, we will visit over 70,000 farms across more than 25 countries in Asia, Africa, and Latin America. We work with leading coffee roasters to understand the sustainability issues in their supply chain based on our sustainability standards. * Backend Software Engineer - $130-$150k — http://enveritas.org/jobs/backend-software-eng/#10d7adef8us (worldwide remote) reply kisamoto 2 hours agoparentThat is very cool. If I wasn't working on my own project I would applying for this (I love coffee and the journey of coffee). Keep up the great work. reply kevinconroy 2 hours agoparentprevI highly recommend Enveritas to anyone looking! reply brown 2 hours agoprevMedplum (https://www.medplum.com/)Founding Developer Experience EngineerSFFull time Medplum (YC S22) is an open source, API first, healthcare developer platform. \"Headless EHR\", we take care of the security, compliance, and regulatory burdens of healthcare software development. Well funded and growing fast. We're hiring an amazing Dev-Ex / Dev-Rel engineer to delight customers, build sample apps, and promote the Medplum platform. Tech stack: TypeScript, React, Node.js, AWS Learn more: https://www.medplum.com/careers/devex-engineer reply jakespencer 2 hours agoprev76 Software Engineering GroupOklahoma City, OKFULL-TIMEONSITEU.S. CITIZENSHIP REQUIRED 76 SWEG is a civilian software engineering organization operating under the United States Air Force. We are hundreds of (civilian) scientists and engineers that provide software, hardware, and engineering support solutions to a variety of Air Force and military platforms. We are located on Tinker Air Force Base in Oklahoma City, OK. We often operate like a contractor to other parts of the military and federal government by providing independent engineering services without seeking a profit. We have dozens of active projects using C, C++, C#, Java, Python, JavaScript, LabVIEW, Visual Basic, Assembly, Ada, Fortran, and other more esoteric languages. We have immediate opportunities available to hire candidates with degrees in Computer Science, Computer Engineering, Electrical Engineering, or closely-related fields. If you are interested in learning more, please e-mail 76SMXG.Tinker.Careers@us.af.mil and tell them Jake sent you. reply PanMan 3 hours agoprevQuatt.ioAmsterdam, NetherlandsFull-timeHybrid/ONSITEhttps://quatt.ioclimate tech I'm head of Software at Quatt, a quickly growing startup/scaleup building heatpumps to help fix climate change. Heating and cooling is 50% of all energy used in the EU. Heat pumps save 10 times more CO2 for each Euro spent on them compared to electric cars. We're building the most accessible and smartest heatpump on the market. Our product is live, we have thousands of customers, tons of data, and I really like the impact we're having. I'm currently looking for a few roles for my department, as we believe having the best software will allow us to have the best product. Our backend and frontend is Typescript. * Medior QA / Test automation engineer * Senior app developer (React native) * Senior Backend developer (Typescript) * Medior Full-stack developer (Typescript) * DevOps Engineer Now is a great time to join, as the software team is still small but growing quickly. These and other vacancies are on our careers page: https://www.quatt.io/working-at-quatt Email me directly ( my-hacker-news-username@quatt.io ) for questions or apply via the career page. Unfortunately, at this time, you have to be allowed to work in the EU: we're not able to sponsor Visa reply lukan 3 hours agoparent\"Hybrid/ONSITE\" What exactly does that mean? Part time remote from within the EU is possible, with regulary travelling there, or does it mean, some days of the week remote is possible but otherwise office time is required (and therefore relocating)? reply PanMan 1 hour agorootparentIt depends a bit on the role, we're considering more remote options, but for now we mostly hire people that are in the office part of the week. reply benhamner 2 hours agoprevSumblesumble.comData/Engineering + first GTMRemote-only North-America timezones Sumble's applying language models (both small and large) to build high-quality datasets. Near-term our focus is company-related data that we integrate with customer's Salesforce data to make go-to-market operations more efficient. Long-term we want to become the first place you go to find high-quality data that lives outside your organization. Currently a team of 7 engineers, including Kaggle/Google/Primer/Stack Overflow/Meta alumni. Our tech stack includes Python, FastAPI, React/Typescript, GCP (postgres + alloydb + cloud run), regular expressions, and Pytorch/Gemma/Mistral. Challenges we face include: - Transforming noisy datasets + noisy models into high quality data products - Making expensive analytics computations run efficiently - Managing the complexity of an increasing number of data sources, models, and operations on top of these If this excites you, get in touch with us by emailing me at ben at sumble. reply corlinp 1 hour agoprevParaFi Technologies LLCU.S. remote possible, San Diego preferredfull-timeInfrastructure Engineer ParaFi Technologies is a small team operating some of the highest-performing blockchain validators and node infrastructure. We are looking for a passionate, junior- or mid-level engineer to develop software and infrastructure (Python, Go, Terraform, Docker, etc.) and be an integral part of ParaFi Technologies' growth. https://docs.google.com/document/d/e/2PACX-1vQUBpZCVa8fmfJiO... You can apply by sending a thoughtful email to careers@parafi.tech reply aketchum 3 hours agoprevVIVA FinanceAtlanta, GAJunior/Senior Back-End and React Developers, Project Manager VIVA is a Fintech Startup based in Atlanta, GA with the mission to build a more inclusive financial system. VIVA offers unsecured personal loans to subprime customers who have traditionally been excluded and taken advantage of by the legacy financial institutions. The VIVA difference is to underwrite heavily on employment history and set up repayments through voluntary direct deposit payments from the borrower paycheck. We are a VC backed company and have been in business for 5 years. We hit cash-flow positive in May and are now default alive with our ongoing growth series B likely to be our final round. We are growing the team to further support our personal lending product, in addition to getting the staff to support building new products to diversify our revenue streams. Our tech stack for the back-end is fully on AWS, using Lambda and ECS for compute and Typescript as the language, but experience with this specific stack is not necessary for talented candidates. Preference is given to candidates who can come 4 days a week to our new office on the Atlanta Beltline (next to Krog Street Market) but remote positions are available for strong candidates able to work hours in eastern timezone. Send me an introduction (alex at viva-finance.com) with a resume and we will be in touch! reply niall00c 3 hours agoprevMatterworks.aiSoftware & Data EngineeringHybrid (Boston/Somerville MA) At Matterworks we are building AI tools to extract insights from the ever-growing corpora of biological data and to unlock opportunities in therapeutic discovery, development, and manufacturing. We are building large-scale deep learning models of biological data to predict the phenotype and behavior of biological systems. Throughout the interview process you can expect the following during your time with us: - Phone call to review with hiring manager - Virtual pairing interview (system architecture and design) - Hopefully a quick decision and offer! Principal Software Engineer, Full Stack - https://jobs.lever.co/matterworks/94bec32b-7dfc-4307-9338-ce... Principal Data Engineer - https://jobs.lever.co/matterworks/ed4cdf51-7e5d-4a0f-9d42-b5... reply tonyjin 1 hour agoprevWormhole LabsMultiple PositionsRemote (US ET work hours)https://wormholelabs.xyz/$150-225K + token grant + discretionary performance bonus Wormhole Labs is a software company that specializes in building open source blockchain technology. We contribute to the Wormhole cross-chain messaging protocol and other protocols interested in expanding to different ecosystems. We are looking to hire a Senior Frontend Engineer and a Smart Contract Engineer. The Senior Frontend Engineer would contribute to the Wormhole TS SDK (https://github.com/wormhole-foundation/wormhole-sdk-ts) and Wormhole Connect (https://github.com/wormhole-foundation/wormhole-connect). The Smart Contract Engineer would contribute to smart contracts throughout Wormhole’s ecosystem and build reference implementations for new protocols. Our team isIf you are uncomfortable answering any or all of the questions, then please click the option \"prefer not to say\". You're missing the option not to say in one question, and the option \"Not sure/don't know\" in several. reply continua 3 hours agoprevContinua AIML and Systems EngineersNYC, SF Bay Area, Seattlehttp://continua.ai Continua was founded in April 2023 by a Distinguished Engineer from Google Research to give everyone in the world an agent that uses their real-time context and personal memory to deliver actionable and timely assistance. Our MVP is currently in limited alpha testing, and we’re immensely excited by the feedback we’re getting from early users - they’re finding that the Continua agent helps them in all manner of surprisingly useful ways. This is an opportunity to be a part of setting the product and technical direction of this early stage company! ML engineers at Continua work on new approaches for knowledge representation, multimodality, and personalization under computational resource constraints. Systems engineers at Continua work on a wide variety of secure and reliable backend services and data pipelines to power our products. We’re backed by tier-1 VCs, and we’ve already assembled a team of seven engineers with backgrounds at Google, Slack, CourseHero, Stash, and Amazon. We have a limited number of additional spots available on the team for engineers who are excited about building innovative products that leverage the latest advances in ML, including Transformers / Mamba, RLHF, LoRA/QLoRA, at-edge (on-device) training and inference, privacy-first ML, and more. If that sounds like you, we would love to hear from you. If you are passionate about building innovative new products, and you’re eager to work in a dynamic startup environment, we would love to hear from you. Please apply via https://www.continua.ai/careers, and let's embark on this exciting journey together! reply iskeletorr 2 hours agoparenthi. since https://www.continua.ai/careers link is not opening, you should change the link with https://jobs.ashbyhq.com/continua/ reply 0x6164616d 3 hours agoparentprevError \"page not found\" on careers link! reply wizerno 2 hours agorootparentI believe it should be https://jobs.ashbyhq.com/continua reply kcartmell 3 hours agoprevSnout https://www.snoutplans.com/Principal Software Engineer, Product Manager, Product MarketingRemote USFull Time Join us at Snout on our mission to ensure no one ever has to make a health decision for their pet based on the cash in their back account. Snout plans pay for 100% of routine veterinary care, unlimited visits, and additional member benefits - think pet insurance, that you will actually use every year. Principal Software Engineer - You will be working on our core wellness product, launched late-2022. Work includes the addition of new wellness plan features and capabilities, enhancement of user experience, testing and operation of the wellness platform, and special projects that arise from time to time. We frequently work cross-functionally and you can expect to write code and perform technical operations for our marketing, sales, and support efforts, as well. Our tech stack includes Node.js, React, PostgreSQL, AWS, and Tailwind. We use both JavaScript and TypeScript heavily. To apply for the engineering role, please email kyle@snoutplans.com. Our typical process is a one hour technical interview followed by a casual 30 minute meeting with our whole team. We're a small startup team consisting of fewer than ten people. Successful team members are comfortable participating in spirited and detailed debates to establish product and technical plans, and then taking initiative and ownership to deliver on those plans quickly and effectively. Our team is collaborative. You can expect to meet with the team on a daily basis, pair regularly, and participate in slack discussions throughout the day. We keep pacific work hours and expect the team to be available during 9a-5p at a minimum. If you're looking for a team where you can carve out your area of responsibility, work with experienced partners who have your back, grow your role alongside the growth of the company, and take a product to the moon, then we should talk. reply futo_hn 1 hour agoprevFUTOAustin, TXOn-site / Hybrid / RemoteFull Time / Hourly FUTO is an organization dedicated to developing, both through in-house engineering and investment, technologies that frustrate centralization and industry consolidation. Through a combination of in-house engineering projects, targeted investments, generous grants, and multi-media public education efforts, we will free technology from the control of the few and recreate the spirit of freedom, innovation, and self-reliance that underpinned the American tech industry only a few decades ago. Our principles are here: https://futo.org/about/what-is-futo/ FUTO is hiring for two Quality Assurance roles. QA Lead - We are looking for a full-time, in-person QA Lead to drive our quality assurance efforts across FUTO’s projects, with a focus on our Android applications. The ideal candidate is aligned with our principles, has a strong sense of software craftsmanship and will be highly active in dogfooding our products. https://futo.org/jobs/qa-lead/ QA Tester - We are seeking an experienced QA Tester to join our team on a part-time or contract basis. The ideal candidate will have a keen eye for detail, a systematic approach to testing, and a strong ability to document and communicate findings. The position will be responsible for ensuring our applications meet the highest standards of quality. https://futo.org/jobs/qa-tester/ To apply, send your resume and cover letter to jobs at futo dot org with the subject of the position you're interested in. reply PlantingSpace 3 hours agoprevPlantingSpaceFull-timeRemote (EU time zone) with quarterly gatheringshttps://planting.space We are building an AI system that can accurately represent knowledge and handle uncertainty, to enable the discovery of insights and solve problems based on explainable reasoning. Our technology is not based on a GPT framework but on a novel approach to structured knowledge representation. We envision applications to automate analysis and speed up research in domains such as Finance, Strategy Consulting, Engineering, Material Sciences, and more. We are continuously looking for strong software engineers who are up for a challenge and a steep learning curve. You’ll be exposed to cutting edge research in Bayesian statistics, dynamical systems, information theory, symbolic computing, and more. Current openings at PlantingSpace: - Senior NLP Engineer: build bridges between neural and symbolic representations within our system. - Software Engineer: contributing to core system development, production backend engineering, implementing and analysing algorithms. - Applied Category Theorist (Knowledge Representation): research category theoretic representations of real world phenomena to inspire our development. To see a full list of openings, and to apply, check out our Join Us page: https://planting.space/joinus/ We’re excited to also share some example tasks that will give you a taste of the work we do here: https://planting.space/examples/ reply hudson-trading 3 hours agoprevHudson River TradingHybridFull-time We’re a quantitative trading firm based in NYC that trades hundreds of millions of shares each day on over 200 markets worldwide. We use math and technology in everything we do; our talented developers, engineers, and programmers build complex models and systems that allow us to make automated trading decisions on global markets. We’re looking for: Technical Product ManagerNYChttps://www.hudsonrivertrading.com/careers/job/?gh_jid=54008... Data Production EngineerNYC, Londonhttps://www.hudsonrivertrading.com/careers/?q=data+productio... Cloud Solutions ArchitectLondonhttps://www.hudsonrivertrading.com/careers/job/?gh_jid=58809... Low Level Software Engineer (C++)Singaporehttps://www.hudsonrivertrading.com/careers/job/?gh_jid=12290... And more! For more information about our benefits, check out Life at HRT: https://www.hudsonrivertrading.com/life-at-hrt/ reply jeffreyw128 2 hours agoprevExaSan FranciscoIn personFull time$130K-180K Jeff, cofounder of Exa here. LLMs represent a brand new opportunity to organize humanity's knowledge, in a way that hasn't been done before. We're an AI research lab focused on AI-powered search algorithms (using embeddings), currently applied to vast swaths of the web (we make our money as a search API). We're hiring pretty broadly across engineering - AI research, high performance Rust (e.g., we build an in-house vector DB), and full stack. If the mission of organizing the Internet motivates you, it's a good fit :) https://exa.ai/careers reply beaconai 3 hours agoprevBeacon AIhttps://beaconai.coFull-TimeSF Bay Area Hybrid Beacon AI builds R2-D2 for pilots and an aircraft operator data platform. We use AI and technology to augment pilots to help them fly safer and more efficiently. Our pilot assistant, Murdock, and Lighthouse platforms are designed for commercial and defense aviation operations. ** We recently secured additional investments and contracts and anticipate growing our team about 25-30 new team members over the next year! ** Our team has unique, directly relevant industry expertise in aviation, AI, and autonomy. Join a growing engineering team supported by a world-class advisor team, talented developers, and amazing investors, including Sam Altman, Scout Ventures, JetBlue Ventures, Zach Perret (Plaid), Countdown Capital, and many others. Learn (a little bit) more at https://www.beaconai.co *We do not work with outside talent agencies. Please do not message from one, thank you.* We are looking for Senior I, Senior II, Staff, Tech Lead, or Engineering Manager level candidates in the following: * Advanced Pilot Assistant Software (C++, Python, AI/Autonomy/Flight Software/Feature Engineer) This role builds the brains for Murdock. * iOS Software This role builds the primary pilot interface for Murdock * Senior React Web Application Engineer This role builds our web application, Lighthouse. Preferably full-stack (AWS infra). * Cloud Infrastructure Software * Technical Sourcer & Recruiter Apply directly at https://beaconai.co/careers and mention that you found this post on HN! reply tevon 2 hours agoprevBruinenbruinen.co(Senior) Software Engin-person / remote San Francisco Bruinen is working on infrastructure technology for edge environments - we're starting with the database. Our database, Delta, is local-first and distributed. Each device (AV, robot, drone, sensor) can continue working without connectivity, and merge it's changes (conflict free) whenever it comes back online. We target use-cases in the harshest environments - where connectivity is expected to drop: battlefields, at sea, in the air, and in the wilderness. We're written completely in rust (though its not a requirement for this position, come learn!). If interested, email me at tevon at bruinen dot co reply jvanderbot 2 hours agoprevShield.ai (https://shield.ai)San Diego, Texas (ONSITE mostly, some REMOTE positions)All Full-time We're hiring a people/tech manager for our planning and controls stack, in particular. We also have a huge number of open roles across software, foundations, cloud, embedded, and autonomy for software engineers. Link for SD jobs: https://jobs.lever.co/shieldai?location=San%20Diego%20Metro%... Shield is a well funded deftech startup experiencing rapid growth in their autonomy software and medium/large segment aircraft sales. reply Cixelyn 2 hours agoprevSpellbrushAnime AI researcher, Software Engineer, or Unity EngineerSan Francisco OR TokyoSponsors Visas Spellbrush (YCW18) builds state-of-the-art AI foundational models for anime and video games. We probably spends 10x more compute on anime than anyone else in the industry - we think our results over at nijijourney.com speak for themselves. Looking to both expand our game development team and our AI research and data processing teams. JAX and TPU experiences are a plus. Otaku looking for a job in AI are welcome to ping over name of best waifu or husband to jobs@spellbrush.com reply matt_akkio 3 hours agoprevAkkioFully RemoteFull-timeFrontend / Full Stack Engineers (Junior and Senior)$120k-$180k + equity Use Akkio to help digital marketing agencies build & deploy AutoML pipelines, using Generative AI for data exploration and cleansing, and other marketing-specific feature sets We're looking for solid and enthusiastic frontend/full stack engineers to help us build and scale our data platform. Vue.js, Node.js, TypeScript, JavaScript, Firebase Must be authorized to work in the US and have timezone overlap with the US https://www.akkio.com/jobs/frontend-engineer and https://www.akkio.com/jobs/senior-frontend-engineer reply pydeveloper22 3 hours agoparentHi Matt, do you have an email address to reach you at? Thanks reply mmyller20 2 hours agoprevKoddiCurrently seeking FT employees in the following locations: Ann Arbor, MIFort Worth, TXAustin, TXNew York, NY Open roles: Senior Software Engineers (Go, Java, C, C++, PhP); Integration Engineer; Integration Engineering Manager; ML Engineer (must have adtech experience), Director of Data Science (must have adtech experience), Platform Engineer, Director of Infrastructure. Must be US Citizen or Green Card holder and physically located in the US. Passionate about development in leading technologies? Looking to become a major player on a diverse team? Want to make a big impact on an engineer-driven roadmap in your next career adventure? Koddi Engineers drive innovation by embracing challenges and deploying emerging technologies to solve complex problems in software development. Koddi is a global technology company with software and services that help top digital marketplaces effectively monetize their first-party audiences through industry-leading commerce media technology and strategy. Our enterprise platforms leverage first-party data to drive marketplace revenue and profit by improving user experience and target shoppers throughout the purchase path. Koddi’s platforms enable any advertiser, any marketplace, in any industry to increase awareness, generate demand, and drive revenue. Review all open roles at www.koddi.com/careers and apply directly, or send your resume to matthew.myller@koddi.com. reply wsdookadr 1 hour agoprev40% of the posts in this thread are fake reply wwweston 1 hour agoparentHow did you determine this and how could someone else verify it? reply swozey 57 minutes agorootparentI don't know if I'd outright call them fake but I've gone through https://hnhiring.com/ which aggregates these posts a few times with an absolutely impeccable and perfect resume for some companies in a fairly niche role, I'm a Staff level SRE, so way less common than a SWE, less job postings, and I've never received a reply to any of them when I can absolutely guarantee you that me as a hiring manager seeing my resume would at least give me a phone call. For instance Kubernetes came out in 2015 (alpha/beta, 1.0) and I've worked on it since then and I'll apply to lead K8s eng jobs from these and not hear a word. I've also worked for companies like Rancher and a few CDNs that literally make K8s distros. I've worked with companies like Google, Meta, Nasa, the USA gov, etc launching production k8s clusters across the world. Like I've seen it all including in cell towers, fast food drive thrus, military use, etc. I hire for the same roles on my team and if I saw my resume I would have a heart attack and get them in immediately. I know how rare my LOE with distsys is, I build platform teams. 99% of the people I interview that \"know k8s\" don't know k8s, they know Helm or yaml, whereas I have a literal github portfolio full of 7 year old gcloud/k8s/aws tools I provided the community that's also listed in my resume. Could just be luck of the draw, of course. But my initial \"oh this person will know me from HN and look at my portfolio!\" is gone from these posts and I just send resumes like a normal job that won't call me back. When I was a kid (teens/20s) I got a bunch of my first sysadmin/neteng jobs off of IRC and it doesn't feel the same at all. reply theideaofcoffee 12 minutes agorootparentThis has been my experience too having been a long time in the SRE niche, and I think it's a factor of a couple of things: the market being over-saturated both from the huge numbers of layoffs happening in the recent past and the hangers-on/bootcamp attendees expecting outsized comp with little experience so postings get absolutely flooded with applicants. I spoke with a hiring manager just within the last few weeks and they said for a senior-ish position, they received almost 1000 CVs. Were they all qualified? Who knows, it seems like it would be almost impossible to find the true gems in a pile of shit like that. A lot of these companies also seem to be reaching beyond their grasp as well. I don't think a lot of them necessarily have the vision, nor the wisdom on their staff to recognize a well-rounded background, especially if there isn't the blaring siren of a FAANG on someone's resume. They're simply not equipped to dig in. It's probably also a matter of raw number of years, they're afraid that if you join, you'll get bored then they'll be back to square one (been there). reply mistidoi 3 hours agoprevRelevant HealthcareDirector of EngineeringRemote (USA + Canada) Relevant (https://relevant.healthcare) is a small and experienced team building an analytics and population health platform for non-profit clinics called Community Health Centers*. We're bootstrapped, fully remote, and hiring a director of engineering! Read more about the role here: https://relevant.healthcare/jobs/director-of-engineering/ Read more about Community Health Centers here: https://relevant.healthcare/jobs/ reply beautiful-ai 3 hours agoprevBeautiful.aiFully RemoteUSA/CanadaFull-time Series B16M FundingAI IndustryB2B and B2C SaaS 1. Lead Software Engineer - $175k - $250k Base Salary + Equity https://boards.greenhouse.io/beautifulai/jobs/4231312007 2. Senior Information Security Manager - $150k — $165k USD Base Salary + Equity https://boards.greenhouse.io/beautifulai/jobs/4413899007 3. VP of Marketing - $225k - $300k Base Salary + Equity https://boards.greenhouse.io/beautifulai/jobs/4323511007 reply gciguy 2 hours agoprevGCI (https://www.gci.com)Anchorage, AK / REMOTESenior Data Engineer We're looking for experienced Data Engineers familiar with working in Azure and Databricks. We build pipelines to ingest data for our analysts and data scientists and integrate with other platforms. We also take care of the platform and work to improve it for all teams. IT Data Engineer IV: https://edqv.fa.us2.oraclecloud.com/hcmUI/CandidateExperienc... Senior IT Data Engineer: https://edqv.fa.us2.oraclecloud.com/hcmUI/CandidateExperienc... About GCI: Headquartered in Alaska with additional locations throughout the U.S., GCI has worked for more than 40 years to deliver communication and technology services to some of the most remote communities and in some of the most challenging conditions in North America. GCI is a pioneer in its field, bringing telemedicine and online education capabilities to communities across the state and continuing efforts to connect the Arctic globally as well as providing strong services to consumer and business markets. GCI’s introduction of 1 GIG internet speeds in the state as well as its innovative partnership with Apple are among the countless ways the company has transformed communication and quality of life for Alaskans. reply amcrouch 3 hours agoprevHealth Partners GroupFully Remote (UK ONLY, NO VISAS)Tech Leads, Developers, Data Analysts & QA Lead Join us in shaping the future of occupational health, mental health and employee wellbeing services. At Health Partners Group, we stand at the forefront of occupational health, merging expert advice, clinical excellence, and cutting-edge technology. Our mission? To forge health programmes that not only enhance wellbeing but also boost performance across workforces. Who You Are You're passionate about leveraging technology to transform healthcare. Your drive is matched by your technical expertise and you're looking to make a real impact within a dynamic team. You understand that your role is more than a job; it's about enhancing the lives of millions. Roles We’re Hiring For: Tech Lead’s to lead squads of six product, development and QA engineers. Senior .Net Developers to join one of our squads to work with product and QA engineers to deliver key features and functionality. .Net Developers to join one of our squads to work with product and QA engineers to deliver key features and functionality. Data Analysts to join our MI & Data team who use Power BI, SQL Server and various tooling to generate the insight that we provide our clients. Automation QA Lead to manage the QA team on a day-to-day basis while being an individual contributor, guiding the development of our automation test framework. What We Offer: A collaborative environment that thrives on innovation and new ideas. A competitive salary with a comprehensive benefits package. Opportunities for personal and professional growth. A flexible working policy to support your work-life balance. How to Apply: Send your CV, along with a cover letter that tells us your story and why you're the perfect fit for our team, to itrecruitment@healthpartnersgroup.com. Please specify the role you're applying for in the subject line and confirm you have the right to work in the UK. We do not provide sponsorship currently. Health Partners Group is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. reply k1w1 3 hours agoprevAha! (https://www.aha.io)Rails / ReactREMOTE Aha! is the #1 tool for product managers to plan strategy and roadmaps. We serve more than a million users worldwide. We are looking for: * Experienced full-stack Rails and security engineers to work on the Aha! product. Our application is built in Ruby on Rails, with React on the frontend for rich client-side experiences. Aha! is profitable, you can work from anywhere in North America, South America or New Zealand, and we offer excellent benefits. We use our own product to manage our work (which is especially rewarding) and we deploy continuously. Our entire team has always been 100% remote - in North American timezones so we can collaborate during the work day. reply digital_sawzall 1 hour agoparentI have seen your security engineer job ad consistently up since I was looking for my last job in 2021. Loking through my linkedin 'top job picks for you' I see this job role first come up June 15th, 2020. How can you not have found a someone since then, or does everyone quit? I had seen this ad so often I was starting to suspect it was part of a information gathering scheme for information security professionals. reply sosodev 1 hour agoparentprevGiven the volume of resume spam these days, do you have any tips for getting eyes on my application? reply _false 1 hour agoprevrecurse.mlML Researcher, Founding EngineerLondon On-Site We're a seed-stage, code generation startup, working to automate the boring tasks in large codebases. If you'd like to push the boundaries of code generation and deploy your work in some of the largest software engineering teams orgs in the world, shoot me an email (it's in the Notion job descriptions). https://www.notion.so/cerebral-af/Working-at-Recurse-ML-1d3a... reply tracebit 3 hours agoprevTracebithttps://tracebit.comMultiple rolesLondon, UKFull-TimeOn-site (5 days) Tracebit lets security teams implement ‘assume breach’ with automated cloud based honeypots or canaries. Off the back of a successful seed fundraise from tier 1 VCs, we are actively hiring for smart people who get things done in the following positions: - Founding Engineer£70-100k + equityhttps://tracebit.com/jobs/founding-engineer On-site roles (5 days a week) in Central London. Learn more and apply: https://tracebit.com/careers reply poooogles 2 hours agoparent>We think 9am-6pm will bring a great cadence to work Any reason why you think this given that studies show knowledge workers can't be productive over that long a period? reply ghostpepper 26 minutes agoparentprevHow many founding engineers do you have / plan to hire? reply entangled_atom 1 hour agoprevqBraidCloud DevOps EngineerFull-time~$100K (dependent on locale) + equityRemote or In-Person (Chicago, IL) We are qBraid (https://www.qbraid.com) – a leading platform for accessing quantum software and hardware. We are building a cloud-based IDE optimized for quantum computing, and we're looking for an experienced Cloud DevOps Engineer to join our passionate team! We are seeking someone with expertise in EKS/GKE/self-hosted k8s, load/network balancing, and scaling cloud infrastructure. If you have experience with quantum computing, that's a huge plus, but it's not necessarily required. *Some reasons you’d enjoy working with us:* * A collaborative, innovative work environment where your voice is heard. * Opportunities to work on pioneering quantum computing projects. * Competitive salary with equity-based compensation. * Professional development opportunities, health care, and paid lunches. *Basic Qualifications:* * Experience developing applications with JavaScript, TypeScript, Python. * Experience deploying/managing containerized applications with Kubernetes and Docker. * Strong experience using AWS and GCP cloud services. * Experience with NoSQL databases (e.g., MongoDB, DynamoDB). * Experience with Node.js runtime environments and REST APIs. * Experience with Git, CI/CD workflows, and event-driven automation. * Experience building cloud-based solutions and scaling cloud infrastructures. * Self-motivated and comfortable working both independently and collaboratively on complex projects in a fast-paced environment. * Excellent written and verbal communication skills. *Preferred Qualifications:* * Enthusiasm for quantum computing. * Familiarity with the Jupyter stack (i.e., JupyterHub, JupyterLab, Jupyter Notebook). *If interested, please send your resume and a little blurb about why you are interested in qBraid to qbraid_hiring@proton.me* reply koenbok 2 hours agoprevFramer (https://framer.com)Remote (EU)Senior+ Product Engineers The best professional web builder for your entire dotcom. Used by Perplexity, Twingate, ByteDance, Superhuman and many more. TypeScript / React, Go / AWS. koen at framer dot com reply minaguib 2 hours agoprevHivestack by PerionMontreal, QC (Hybrid Office/Remote)Full Time Industry: AdTech, specializing in Digital Out Of Home I'm hiring for two engineering lead positions (senior/staff level): * Data Discipline Lead - this includes driving the architecture and technical roadmap, and the implementation of various technologies to support adtech-scale ingestion, transport, processing, warehousing, and reporting. * Edge Discipline Lead (Ad Bidding & Delivery): This position leads the team responsible for the high-volume low-latency ad bidding and delivery systems - a particularly good fit for someone who enjoys systems performance engineering, architecture, while still being hands-on Full job descriptions: https://www.hivestack.com/careers/ If you're interested, get in touch :) (contact info in bio) reply Zaheer 2 hours agoprevLevels.fyiSenior React Native Frontend EngineerRemote (India, Open to other countries)Full-timehttps://www.levels.fyi Levels.fyi's mission is to help every professional build a better career through the most accurate insights and services. We're building the future of compensation & hiring by centering ourselves around professionals. You'll be joining a close-knit team of 2 engineers to work across our product verticals. You'll have the opportunity to lead and own new projects / initiatives from idea to production end-to-end (architecture to deployment). We move fast and have come an incredibly long way on a tiny team. We're looking for a self-starter and resourceful engineer with strong communication skills and experience building things from scratch. Apply at: https://docs.google.com/forms/d/e/1FAIpQLSf8CxjgiSYo7Po2xrDX... reply simguy 3 hours agoprevVenture Global LNGData Scientist, Data EngineerArlington, VA ONSITEVISA Venture Global LNG is a natural gas company. Primarily, we liquify natural gas for export, but are expanding into other parts of the industry. Our data analytics team is like a startup: informal, small, growing rapidly. We need 2 DS and 2 DE who know Spark and can autonomously design and execute complex work for high-level stakeholders. Requirements for both: - 5 years professional, non-academic experience - Excellent communications - Strong Spark skills - Technical skills and ambition well above the average \"good\" candidate Go to our jobs page and scroll to Information Technology / Data Engineer or Data Scientist to apply. Direct links: https://venturegloballng.com/careers/?gh_jid=7470846002 https://venturegloballng.com/careers/?gh_jid=7470493002 reply bill_02109 3 hours agoprev40GRID - Full-time RemoteSenior Angular Frontend Engineer Our mission is to empower field-service companies to grow by modernizing and automating their business operations. Every company we work with has unrealized potentials — our task is to build the platform that empowers growth and helps them unlock opportunities. Email: jobs [at] 40grid.com (no recruiters or agencies, thank you). Tech: Django / PythonAngular (Typescript) reply wskemper 1 hour agoprevSaildroneSoftware Engineer(s), SecurityHybrid in Alameda, CA or Washington, DCUS only, no visas Saildrone is on a mission to help scientists and sailors better understand the ocean and everything in it, to preserve a healthy ocean and a safe, sustainable planet. We make autonomous sailing robots that travel the oceans, measuring almost everything they can find along the way. Think of them as floating maritime tricorders. Saildrones have discovered new underwater mountains[1], circumnavigated Antarctica[2], and even traveled right through hurricanes[3]! We are expanding our Security team with two software engineers to help cover a wide array of needs: building security services, helping other teams assess the security of their own work, running pentests and tabletops, teaching security classes, and working with auditors to prove we do all the above. You read that right - we do SWENG, APPSEC, OFFSEC, SECED, and GRC all at the same time! I can guarantee you'll never get bored. I've listed two positions, but we're flexible on levels for the right candidates: * Software Engineer: https://careers.jobscore.com/careers/saildrone/jobs/software... * Senior Software Engineer: https://careers.jobscore.com/careers/saildrone/jobs/senior-s... [1] https://www.sfchronicle.com/climate/article/underwater-mount... [2] https://web.archive.org/web/20190809011553/https://www.bloom... [3] https://web.archive.org/web/20230609092913/https://www.nytim... and https://www.pmel.noaa.gov/saildrone-hurricane2021/index.html reply omgJustTest 3 hours agoprevGamma Reality Inc Imaging Scientist Hardware / Electronics Engineer https://www.indeed.com/jobs?q=gamma+reality&l=Richmond%2C+CA... Gammareality.com to learn more about how GRI is changing radiation detection. reply louiskw 3 hours agoprevbloop (YC S21)Product Engineer (Rust, AI)Onsite London, UK We're combining LLMs and transpilers to translate COBOL into Java. If you’re getting into (or already work in) Rust, and have a strong product mindset, this role could be a great fit. Come and build LLM pipelines and agents, that help the largest companies modernise their legacy codebases. Please email join [at] bloop [dot] ai with \"HN Product Engineer\" in the subject line. More details - https://www.ycombinator.com/companies/bloop/jobs/iCrEllp-pro... reply xGrill 2 hours agoprevPMGSoftware EngineersONSITE (Dallas) We are a digital marketing technology company that wants to make adtech and martech significantly more data driven. We are looking for folks who want to transition their careers from Software Engineers to AI Engineers https://www.pmg.com/careers/job-openings?department=Data%20%... reply powertoolstech 3 hours agoprevPowertools TechnologiesJunior/Senior EngineerLisbon, PortugalFull-timeONSITE > Looking for a senior engineer for work on software related to Electronic Design Automation. Candidate should have some experience with EDA software (Cadence Virtuoso, Siemens Calibre, Synopsys Design Compiler, etc), ideally including plugin development. Experienced Software Developers are more than welcome to apply. > Looking for a junior engineer for work on software related to Electronic Design Automation and/or Software Development. Candidate should at least have (or graduate shortly) a 3 year university degree in engineering. Most suitably Electronic/Computer Engineering or Informatics. Software Developers are more than welcome to apply. Site: https://www.powertools-tech.com . Growing a small experienced team with international industrial and academic track, willing to train new hire in fairly uncommon skill set. Candidate should be capable of quality detail work, and have good communication abilities, to provide support to international design teams in fabless semiconductor companies. Email your interest and CV to hr@powertools-tech.com, please. reply jgnatch 2 hours agoprevTimelineLondon, United KingdomSenior Full Stack Engineer (full-time, contract)Remote We're looking for experienced Senior Software engineers! TL;DR: Our tech stack is Elixir, Elm and Rust. We are a rapidly growing company (100% YoY since launch). We work remotely. We work asynchronously and in four-week cycles using the Shape Up methodology. About Timeline: We are disrupting the financial planning and portfolio management software for advisers in the UK. We manage over £6B of clients assets on behalf of financial advisers. Apply here: https://timelineapp.freshteam.com/jobs/bmZYCi_Breyu/full-sta... reply malvikajain_ 2 hours agoprevshmood.ioHybridNYC (in a beautiful office)Full-timeFull-Stack & Back-End DevelopersB2B and B2C SaaS Shmood is a VC backed company on a mission to eliminate design alignment churn. We’re building Pinterest for design enterprises — to allow designers and their teams instantly align with clients of any background, on a project of any scale. To do this, we’re integrating a semantic data layer that takes multimodal design intent and communicates it, refines it, and turns it into actionable advice. https://www.shmood.io/careers (or email malvika@shmood.io with an intro and resume and we will be in touch!) reply julian-datable 2 hours agoprevDatableSenior Software Engineer, ReliabilityBay Area (3 days hybrid)Full-time$160k-$220k + equity Do you find metrics, logs, and traces fascinating and infuriating in equal measure? We are sick of the limitations and costs of observability, so we’ve decided to do something about it! Datable is a building a streaming data pipeline to help customers manage their observability data. We are a seed stealth startup based in San Francisco. Our six person team has a lot of experience in this space, and we’re looking for an engineer passionate about reliability to build out our customer platform. Reliability for reliability? Neat! You’ll be working in Terraform, Helm, and NodeJS. We strongly believe in the strength of Product, Design, and Engineering collaborating together to build better products. You’ll have a lot of autonomy to try new things along the way, and will have a huge impact on building our product. Our work is heavily based on best practices from OpenTelemetry and years of experience in the observability world. Our stack is primarily Typescript, React, NodeJS, Postgres, and AWS. Email ben@gahlsdorftalent.com reply ot1138 2 hours agoprevValley View TradingDeveloper/TraderOnsite, Chicago IL Calling all elite developers who are ready to make their mark in the markets: Valley View trading is not your typical trading firm. We're a small team of successful trader/engineers looking for an inventive, entrepreneurial and detail-oriented developer who wants to master the equity options markets. If you're a self-starter with serious C++ chops, a track record of moving fast and a hunger to excel in the ultra-competitive world of options trading - let's talk. This is a unique chance to reap the rewards of your talent. The right candidate thrives as an individual contributor, has 5+ years of experience creating high-octane, low latency code and a clean background. You'll hit the ground running and have 60 days to ace the Series 57 exam on us. If you're ready to trade Silicon Valley politics for the chance at Silicon Valley paydays, this is your opportunity. Show us what you've got - reach out with cover letter and resume at valleyviewoptionsgmail. reply kdumont 3 hours agoprevAllSpicehttps://allspice.io/Boston, MA / SF or Remote/HybridFull timeSr. - Principalgolang, Vuejs, rust At AllSpice, we're building the future of hardware development and collaboration, applying modern software design principles to the hardware industry with revision control, design review, and automated test (think GitHub/Bitbucket for hardware). AllSpice is unlocking the next generation of smart vehicles, IOT devices, rockets, medical devices, robotics, and much more. https://techcrunch.com/2023/12/05/git-allspice-enterprise-6m... We have a highly-capable, tight-knit, remote-first team with a flex office in Somerville, MA and competitive benefits. We strongly value continuous communication and personal development. See more on our careers page [https://allspice.notion.site/AllSpice-Careers-3173d0cd518b42...] We’re hiring primarily for: Principal / Lead Fullstack SW Engineer - Take a pivotal role on our incredible engineering team by helping coordinate our application architecture and implement new product features. You'll work on optimizing 2d graphics rendering, scaling application code, and testing/planning architecture to support new feature development of our git hosting platform. More info here: https://www.notion.so/allspice/Principal-Lead-Fullstack-Engi... Mid / Sr. Frontend Software Engineer - You'll be the subject matter expert for our front-end testing (Vue, JS, Golang, Playwright, jest), and working with open-source projects to extend our testing capability. https://allspice.notion.site/Mid-Sr-Front-End-Software-Engin... Director of Sales - We're also hiring for a director of sales with expertise in enterprise B2B software. It's a phenomenal opportunity to streamline our sales process & coach our capable and growing sales team. https://allspice.notion.site/Director-of-Sales-48528b4a68344... Tech Stack: Docker, Terraform, GoLang, Rust, Python, Vue See our careers page: https://allspice.notion.site/AllSpice-Careers-3173d0cd518b42... Apply by emailing us at jobsallspice[dt]io ( -> @, [dt] -> .) with [HN] in the title and a link to your GitHub/GitLab profile and/or resume. reply ellisd 55 minutes agoprevNuna (https://www.nuna.com)San Francisco & Remote friendly (US only)| Full-timeVisa Transfer In the US, we spend an average of over $12,500 per person each year on healthcare -- that’s almost twice what other developed countries spend. Healthcare in the US costs a staggering $4 trillion dollars per year, almost 1/5 of our Nation’s entire economy. Yet with all this resource, our healthcare outcomes are poorer than other countries, people still can’t afford their healthcare, and our healthcare providers are burnt out. Moreover, our healthcare is systemically unequal. People of color, lower income, and LGBTQ+ have demonstrably worse healthcare outcomes, a disparity grimly highlighted by the pandemic we’re living through now where people of color are three-times more likely to die from COVID-19. How can this be, and how can we change it? Nuna is tackling one of the most hardest problems in healthcare underlying the negative outcomes and disparities we see: how healthcare gets paid. Today, hospitals only get paid when they do more-- more visits, more tests, more meds, more surgeries. Hopefully this helps patients get better, but regardless, the system gets paid. In fact, doing more is the only way to stay afloat. But -- is this really the right set of incentives? Shouldn’t everyone get rewarded not just by doing more, but by when patients actually get better? Shouldn’t everyone have access to affordable, high quality care, and shouldn’t hospitals be rewarded when they deliver this care? And shouldn’t insurance companies get rewarded when they help ALL their patients get better? Absolutely, yes. This concept is called Value-Based Care. In fact, healthcare as a whole has been trying to move in this direction for years, but making it all reality is deeply complex -- it is after all our healthcare. Nuna’s technology platform, our software apps, our vision, and our exceptionally talented team are collectively accelerating the healthcare system’s ability to make value-based care available to everyone. In 2022, Nuna will power over $70B of healthcare payments for over 6.5M patients. We also leverage our data science and platform to direct patients to the best, culturally-matched, and accessible care providers for them. Additionally, we make it transparent and easy for both hospitals and insurers to see how they are performing in value-based care by spotlighting the patients or areas where they need to pay extra attention so that they can provide good care to all their patients and get rewarded. Nuna is unique - we have brought together an exceptional team of over 200 people. We are the industry’s best in healthcare data, analytics, engineering, clinicians, and value based healthcare experts. We have joined forces to create a more equitable health system for everyone. Our dreams and ambitions to change healthcare as we know it are big. If yours are too, we want to work with you. Open positions include: * Engineering Manager, Data Platform * Sr Software Engineer, Full Stack - Provider * Senior Data Scientist, LLM * Lead Software Engineer, Rewards AI/ML * Lead Software Engineer, Data Platform * Lead Software Engineer, Program Engine * Lead Software Engineer, Developer Infrastructure Jobs Board: https://bit.ly/nuna-job-board Frontend: React, Typescript, Flutter (Android and iOS) Backend: Django, Python, Kotlin, Scala Cloud: AWS Questions? Email: recruiting+hn@nuna.com reply sails 1 hour agoprevsea.devFull Stack Product EngineerLondon, UKHybrid or REMOTE in UKFull-timeFintech sea.dev is looking to hire our first engineer. We are looking for a “Full Stack Product Engineer” to help build our product. We are building technology that enables financial institutions to flexibly embed LLM capabilities into their workflows and product offerings. The founding team has worked at world-leading financial and research institutions, and brings together decades of experience in data technology, graphs, and finance. We have extensive prior experience building data teams and launching data products from scratch. FAQ: Pre-seed. VC-backed. Team of three. Fully remote but centred around London For more details see https://join.sea.dev reply zerojames 3 hours agoprevRoboflowField Engineer + ML EngineerFull-time (Remote, SF, NYC)https://roboflow.com/careers?ref=whoishiring0724 Roboflow is the fastest way to use computer vision in production. We help developers give their software the sense of sight. Our end-to-end platform[1] provides tooling for image collection, annotation, dataset exploration and curation, training, and deployment. Over 250k engineers (including engineers from 2/3 Fortune 100 companies) build with Roboflow. We now host the largest collection of open source computer vision datasets and pre-trained models[2]. We are pushing forward the CV ecosystem with open source projects like Autodistill[3] and Supervision[4]. And we've built one of the most comprehensive resources for software engineers to learn to use computer vision with our popular blog[5] and YouTube channel[6]. We have several openings available but are primarily looking for strong technical generalists who want to help us democratize computer vision and like to wear many hats and have an outsized impact. Our engineering culture is built on a foundation of autonomy & we don't consider an engineer fully ramped until they can \"choose their own loss function\". At Roboflow, engineers aren't just responsible for building things but also for helping us figure out what we should build next. We're builders & problem solvers; not just coders. (For this reason we also especially love hiring past and future founders.) We're currently hiring full-stack engineers for our ML and web platform teams, a web developer to bridge our product and marketing teams, several technical roles on the sales & field engineering teams, and our first applied machine learning researcher to help push forward the state of the art in computer vision. [1]: https://roboflow.com/?ref=whoishiring0724 [2]: https://roboflow.com/universe?ref=whoishiring0724 [3]: https://github.com/autodistill/autodistill [4]: https://github.com/roboflow/supervision [5]: https://blog.roboflow.com/?ref=whoishiring0724 [6]: https://www.youtube.com/@Roboflow reply Redeker_15_Beep 3 hours agoprevTemporal TechnologiesMultiple positions in United States - WORK FROM HOMEFULL-TIMETemporal offers an entirely new way to build scalable and reliable applications. Temporal enables developers to focus on writing important business logic, and not on managing state or worrying about the underlying infrastructure. Sequoia Capital led our last round of funding and our team has experience from start-ups and larger companies like Microsoft, Google, Amazon, Uber, and more. Temporal Investors Expand Funding: https://temporal.io/news/temporal-investors-expand-funding-w... Temporal in 7 minutes: https://temporal.io/tldr We're looking for senior level engineers for multiple roles - see here - https://www.temporal.io/careers FEATURED ROLES: Senior Developer Success Engineer → https://grnh.se/ca70567c7us Staff Product Manager - Governance & Security → https://grnh.se/ef4098687us Staff Software Engineer - Control Plane Core → https://grnh.se/9612b42f7us Senior Software Engineer - Cloud Infrastructure → https://grnh.se/7e95c2ed7us Senior Manager, Commercial Sales → https://grnh.se/0b3d09097us US benefits include: Unlimited PTO, 12 Holidays + 2 Floating Holidays, 100% Premiums Coverage for Medical, Dental, and Vision, AD&D, LT & ST Disability and Life Insurance , Empower 401K Plan, Additional Perks for Learning & Development, Lifestyle Spending, In-Home Office Setup, Professional Memberships, WFH Meals, Internet Stipend and more! Benefits outside the United States vary by country. Apply here https://www.temporal.io/careers/ reply swozey 1 hour agoprevThe amount of on-site only job listings now in 2024 is insane. I cannot believe you people will work for these companies and managers that require on-site and not fight back. We're losing our power. I hope nobody applies to these jobs. Datadog, on-site only in NYC, Paris, Boston and Tel Aviv.. lmao. I live in one of the USA HCOL cities (not those, I'm Denver/Austin) and I wouldn't go near those places rent wise. $3500/month for a 2 bedroom is high enough for me to sit on my computer and write go. I've worked at 4 companies in the last decade moving off of Datadog to Grafana and they require on-site in NYC to write log chutes and charging an insane amount for their product while paying their NYC engineers $300k+. Wild. reply angoragoats 1 minute agoparentI'll never apply to a non-remote software engineering job again, and I'm willing to retire or switch careers before I compromise on that. There is absolutely no reason for the majority of software jobs to not be fully remote in 2024, and the quality of life allowed by the switch was transformative for me. reply allywilson 1 hour agoparentprevUpsetting isn't it? reply FrankStanley12 3 hours agoprevStartupSoftwareRemote (USA + Canada preferred)$100k + equity We're looking for a person who thinks C/C++ is fun. You'll be writing a lot of code from scratch. We prefer someone with 10+years of experience but if you're talented we'll accept a junior Please send your resume and a hello world program that doesn't use printf or iostreams FrankStanley12@proton.me reply Cheezmeister 1 hour agoparentCool application process. Do the team have a name, or at least a code name? reply sandovn 33 minutes agoprevMeta is currently hiring across the world across different engineering levels and roles. reply standardmodel 3 hours agoprevStandard Model BiomedicineFounding Engineering Roles We are building the standard model for biomedicine. This is the hardest and most important problem in life science. We are solving it. One of the following (founders are filling both roles currently): AI: you are 5x faster than average at implementing, training, fine-tuning sota. e.g. you can write an ACL accepted paper in 1 week from concept to arxiv. Given a choice between openai and working with us, you would choose us. Data science: you are 3x faster and 10x better than average at deciding what to evaluate, how to evaluate it, and how to communicate it. You are comfortable learning biomedical jargon and humble but confident about testing that with any biomedical expert in the world. Given a choice between leading analytics at the top ranked medical institution in a given field and us, you would choose us. kevin@standardmodel.bio reply delqn 2 hours agoprevOpenAISoftware Engineer, Cloud InfrastructureSan Francisco, CAONSITE Join the Cloud Infrastructure team at OpenAI Applied and build the foundational platform for ChatGPT and the OpenAI API. For more info and to apply: https://openai.com/careers/software-engineer-cloud-infrastru... reply angoragoats 2 hours agoparentCan you please add ONSITE to your post (assuming remote work is not allowed), as requested in the parent post? reply irtefa 3 hours agoprev [–] Jam.devStaff Fullstack Engineer & AI Product EngineerTypescript/ReactRemote (+ in person in SF, Austin, NYC)Full-time Dev tools company with 125,000+ users in less than 2 years. $10M in funding from Vercel CEO, GitHub CTO, Cloudflare CEO, etc. We’re building a flight recorder for web apps – so anyone can report issues to engineers in a way that's actually debuggable (w/ console, network, websockets debugger, etc). Small, senior team – several ex-engineering directors turned ICs (mostly ex-early Cloudflare). Looking for staff-level engineers with experience building highly performant front-end apps. Stack: React/Typescript and MobX (MST) on the frontend, and Node/GraphQL across our backend. The challenge ahead: Scaling. Usage 10x’ed last year, and our users are in 176 countries, on all sorts of devices, network conditions, etc. Our bar for quality is high. As a dev tool, developers at Jam are directly connected and involved with the product. Your usage of the product will directly inform the direction of Jam’s future. Apply here (we read and respond to every submission): https://jam.dev/careers reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Various companies are hiring for multiple roles, including remote, onsite, and hybrid positions, across different locations and industries.",
      "Notable companies include Apple, Figma, Charge Robotics, and SmileID, offering positions such as Senior Software Engineer, Senior/Staff Security Engineer, and Senior Frontend Engineer.",
      "Opportunities span across the globe, with some companies offering visa sponsorship and roles in emerging technologies like AI, machine learning, and full-stack development."
    ],
    "points": 191,
    "commentCount": 171,
    "retryCount": 0,
    "time": 1719846109
  },
  {
    "id": 40842867,
    "title": "Programmers Should Never Trust Anyone, Not Even Themselves",
    "originLink": "https://carbon-steel.github.io/jekyll/update/2024/06/19/abstractions.html",
    "originBody": "Programmers Should Never Trust Anyone, Not Even Themselves 19 Jun 2024 By Sung Kim Programmers should be paranoid. “I double checked the code” “The code passes the tests” “The reviewer approved my code” “So is my code correct?” Writing code correctly is hard and verifying code correctness is impossible. Here are some reasons why: Generality: Even if your code behaves correctly once, will it do so for all cases, for all machines, for all times? False Pass: Failing tests indicate the presence of bugs, but passing tests do not promise their absence. Lack of certainty: You could write a formal proof for your code’s correctness but now you must wonder if the proof is correct. You would need to prove the proof. This chain of verifying the verification would never end. It is folly to pursue certainty of your code’s correctness. A bug may be hiding in a dependency that you’ll never find. Yet we should not despair. We can still decrease the risk of bugs via greater understanding and due diligence. Abstractions What is “greater understanding”? Let’s focus on one facet of understanding which comes up frequently among programmers: abstractions. Abstractions are… mental models of how stuff works when we treat thing A as if it were thing B metaphorically… the result of the data compression that occurs in your brain seeing the forest for the trees used all the time in day-to-day life The word “abstraction” has many meanings. In programming, it can also refer to layers of code that hide complexity. This post will only be talking about abstractions in the cognitive sense. Examples of abstraction: We treat our bank deposits as if the bank simply stores that money for us. In reality, the bank does not just store the money we deposit. It loans away/invests most of the money that people deposit. Our money does not sit idle in a large pile in a vault. The abstraction works because banks still keep enough cash on hand to handle most withdrawals. We treat time as if it passes at the same rate for everyone. Time dilation slightly changes each person/object’s flow of time based on their speed and how much gravity they’re under. GPS satellites orbiting the Earth have to adjust their clocks by ~38 microseconds per day to account for time dilation (Source). The abstraction works because the effect of time dilation is too miniscule to notice unless you are doing extremely precise engineering. One way to form abstractions is by removing details (creating a simplified view of something complex). For example, most people that drive do not know much about the inner workings of their car. Their perspective of the car can be boiled down to: Ignition turns on car Accelerator makes car go Brake makes car stop Wheel turns car Car needs gas/diesel Knowing the above abstraction makes it unnecessary to understand the inner workings of car engines. Most drivers have only this working knowledge of cars and they can drive where they need to go. When we use a programming language, it provides abstractions that allow us to operate computers without understanding their inner workings. Basic language features (such as loops, if-conditionals, functions, statements, and expressions) are all abstractions which hide: Hardware-level details: CPU instructions, registers, flags, and details specific to CPU architecture, … OS-level details: call stack management, memory management, … Portability: Languages abstract away the need to concern ourselves with the differences between different machines. Any compiled Java program (eg, a jar file) should be able to run on any machine that has a Java Runtime Environment (ie, JVM) on it. A Python script should be able to run on any machine with a Python interpreter. A C program should be able to compile and run on any machine if the machine has a C compiler. Abstractions fail Unfortunately, abstractions fail. Language abstractions are not enough if you care about code performance. To speed up your code, you need to know hardware-level and OS-level details. Porting programs that have external dependencies such as dynamic libraries or network requirements is not as simple. They cannot simply be moved to another machine and run. Extra setup and knowledge are required. Car owners who only know the bare minimum can end up stuck in a broken down car. If the driver doesn’t regularly change their car’s lubricant/oil, they’ll shorten the engine’s lifespan. The driver’s abstraction works well in the short term (for a single drive), but fails in the long term (many years). Joel Spolsky describes such failing abstractions as “leaky” and put forth the Law of Leaky Abstractions: All non-trivial abstractions, to some degree, are leaky. Which is similar to the maxim from statistics: All models are wrong, but some are useful. When we write code, we use leaky abstractions all the time. Here are some random examples: Garbage collection takes away the burden of worrying about memory management (unless we care about latency jitters) C++ smart pointers make memory safe (as long as you don’t store any raw pointers from it) Hashtables are fast because they have O(1) operations (but arrays are faster for smaller sizes). Passing by reference is faster than passing by value (except for cases of copy elision and values which fit in CPU registers like ints) Fortunately, many leaky abstractions crash your code when they fail, so they’re easy to address. However, some may just produce undefined behavior or performance degradation which are harder to identify and fix. Press X to doubt So if abstractions can be problematic, then should we try to understand a topic without abstractions (to know cars as they really are)? No. When you dig beneath abstractions, you just find more abstractions. It’s turtles all the way down. Underpinning our abstraction of cars, there is an understanding of each component’s purpose. Beneath that, the chemistry of combustion and the mechanical engineering of the engine Beneath that, the mathematics/physics that model the forces of our universe These layers of abstractions go down until we hit our most basic axioms about logic and reality. As programmers, we should see our knowledge as a house of cards made of leaky abstractions and assumptions. We should have a healthy amount of skepticism of everything and everyone, including ourselves. Trust, but verify A programmer should have a “trust, but verify” policy. Here are some examples: Trust the information that people tell you, but verify it with what the documentation says Test your beliefs by trying to disprove them. You wrote tests for your code change and they pass on the first try. Try running the tests without your changes and see if they still pass. They may have a bug that makes them always pass. You’ve refactored the code which should be a no-op. All the tests still pass. Check to make sure there actually are any tests that run the code you refactored. You optimized your service and you see the expected reduction in resource utilization. Check to make sure your service is not just handling fewer requests at the moment. You’ve submitted a code change and you see no problems in the service the next day. Check to make sure a rollout occurred that day and your code was included in it. Always measure impact when optimizing code. Code changes that appear to be “theoretically” faster can end up being slower due to factors revealed in lower layers of abstraction. Beware of unknown unknowns The scariest epistemological issue for programmers is the “unknown unknown”. There are… things you know (ie, “knowns”) things you know you don’t know (“known unknowns”) things you don’t even know that you don’t know (“unknown unknowns”) These unknown unknowns are the root of abstraction failures (and the reason why programmers can never accurately predict how long a project will take). You may have never heard of… Sanitizing user inputs If you use user-provided strings as part of a SQL query, your service can be hacked via SQL injections. Character encodings Any text data your code processes must be using the character encoding (eg, ASCII, UTF-8, UTF-32, etc) your code expects/supports. Random access of a character in a text buffer could take constant time (for ASCII) or linear time (for UTF-8) depending on the character encoding. You may output incomprehensible characters if you try to read text data using the wrong character encoding. Java heap size Your program may be slowed down due to a lack of heap memory. You could fix this issue if you knew to configure a larger max heap size for your Java program. If you have not heard of these topics before, you may not even know that you fell into their pitfalls. There is no sure fire way to catch unknown unknowns when they are around but we should check under at least one layer of abstraction to look for them. Especially when a project requires learning something new, you should always learn more than you need to. Doing so will mitigate the risk of being surprised by abstraction failures. When learning/working with an unfamiliar platform/language/tool/library/technology: Read more documentation than just the bare minimum you need Watch videos Conference presentations have the highest quality in my experience Read blog posts Read the source code Grow your understanding of abstractions you have to work with Learn about features recently added in to your programming language Read through all of the public functions of libraries, not just the ones you’re using Skim through all of the flags of a CLI tool’s man page Learn at least one layer of abstraction lower than you need Learn about your compiler’s optimizations If you’re running a service, learn about your orchestration platform (ex: kubernetes) If you’re working in Java, learn about the JVM If you’re working in Python, learn about the Python interpreter. Conclusion Abstractions are necessary as they allow us to think efficiently but they are treacherous as they can make it appear that we know “enough”. Programmers who learn superficially will not succeed on difficult projects that come without known solutions and involve multiple domains of expertise. That said, the ideal put forth by this blog post needs to be balanced against reality. Obviously, we can’t take time to learn every little thing when we’re in a rush. Also, beginners can’t be expected to be so thorough. Ideals should be balanced against real-world considerations. And real-world considerations should be balanced against ideals. We should be willing to pay some short-term costs for thorough learning and verifications. Not just for writing correct code, but as part of our long-term journey of becoming capable and principled engineers.",
    "commentLink": "https://news.ycombinator.com/item?id=40842867",
    "commentBody": "Programmers Should Never Trust Anyone, Not Even Themselves (carbon-steel.github.io)172 points by kiyanwang 13 hours agohidepastfavorite138 comments agentultra 3 hours agoTo me, this is the argument for formal verification. I don't want to hear a hand-waving explanation that this algorithm will always complete. If the algorithm is sufficiently complex I want proof. Otherwise, why would I believe you? Abstractions, in the mathematical sense, always hold (unless there is a flaw in the definition itself). Axioms in any sense are always going to throw a wrench in things. Thank Godel. But that shouldn't mean we cannot make progress. Do the work, show your proof! Think hard! Although sometimes all you need are a few unit tests. They key is to develop the wisdom to know when unit tests aren't sufficient for the task at hand. reply atrus 1 hour agoparent> Abstractions, in the mathematical sense, always hold (unless there is a flaw in the definition itself). Axioms in any sense are always going to throw a wrench in things. Thank Godel. But that shouldn't mean we cannot make progress. But this seems like an argument against formal verification. Formal verification is 100x harder than writing tests...and still doesn't guarantee correctness? Those axiom wrenches are still there, those flaws in the definition are still there, not to mention the flaws in the proof writing. All that extra effort for what gain? reply agentultra 1 hour agorootparentIt guarantees correctness vis. the axioms chosen. That's a much more powerful statement and guarantee than a unit test which only exercises a single example. A formal proof that an algorithm makes progress, doesn't require a lock, or whatever property the proof is arguing is 100% guaranteed for every case. For example, a simple function over the set of integers. A unit test can only test individual elements of the set. A proof demands more: the property must hold over all elements of the set. The Incompleteness Theorem puts a limit on the provability. That hasn't stopped mathematicians from pursuing the formalization of mathematics. It shouldn't stop computer scientists and programmers either. In fact it tends to make us more honest about the limits and capabilities of our systems. reply mbonnet 1 hour agoparentprevFormal verification is nice, but requires crazy skilled people at high cost. It's not always practical/feasible to have as part of your SWE process. reply agentultra 20 minutes agorootparentAnd it's not necessary for every project. It's nice to have when it is though. The cost is coming down. Proof automation is incredible today and rapidly improving. This is the part that does the tedious parts of a proof for you so that you can focus on the theorems that matter. The languages and proof systems themselves are easier than ever to pick up and use which is bringing the skill cost down. I don't think most software projects need a huge, dedicated team of specialists to benefit from formal software verification. reply bee_rider 1 hour agoparentprevI dunno. Even in some fairly math-y situations, line solving sparse linear systems, we get stuff like BiCGStab which often will work well and converge in cases where we can’t prove that it must. CS seems mostly good for proving that everything is hopeless in the rigorous general case and, hey, here are some heuristics to give you hope again. reply kryptiskt 8 hours agoprevI think \"trust, but verify\" (as mentioned in the article) is a much more useful motto than \"never trust anyone\". The latter isn't an useful attitude, if you took it seriously you would have carefully check or rewrite everything from the ground up. And then you'd either have to trust the hardware anyway or enlist in a course on VLSI design. \"Trust, but verify\" is much more practicable, at least if you don't feel the need to verify absolutely everything[0], but is content with doing spot checks of all the features. So, good article with a misleading title. Don't be paranoid. [0] I don't consider 100% test coverage as anywhere near close enough for that. reply fatnoah 4 hours agoparent> \"Trust, but verify\" Even though my entire career has been software, I was an Electrical Engineering major, so I have taken VLSI design (and even designed an 8-bit ALU). My first job was writing embedded software, and would frequently \"trust, but verify\" the hardware through the use of a logic analyzer. When I pulled out printouts from the analyzer to show the hardware team that the hardware had a bug, the surprised and incredulous look on their faces was priceless. The blow was somewhat softened by the fact that I'd also found a software bug. The constant blame shifting between SW and HW teams is one reason I left that job after less than a year. reply Spivak 3 hours agorootparentThis is why blameless postmortems are such good thing. Going one step further having a culture where finding a bug or defect in your own code/design is rewarded makes it so people aren't afraid but excited to talk about them. reply fatnoah 1 hour agorootparent> This is why blameless postmortems are such good thing. 100%. I'm very much a fan of asking _what_ happened and then figuring out how to prevent it from happening ahead. Look back to inform the future, not to find blame. Ultimately, if 1 person can blow things up, it's a bigger issue at play. reply ImHereToVote 3 hours agorootparentprev\"Blame is for small children and God\" reply danielmarkbruce 1 hour agoparentprevIn some sense you are saying \"i can't stand making people uncomfortable so I contort reality\". If you have to verify, you don't trust. Google \"trust definition\". Here is the first result: \"firm belief in the reliability, truth, ability, or strength of someone or something\". There is no reasonably sized body of code I ever wrote where I'd have a \"firm belief\" it was error free. There are many situations where we shouldn't believe someone's work is error free. It's fine. Anyone who has ever worked in a field where it can be shown that some work has an error knows how many errors humans make. Anyone who is honest with themselves in the software business knows just how easy it is to make an error. If you need a pithy phrase: \"Assume good intent and capability, but verify work\". reply bigstrat2003 3 hours agoparentprevIf you have to check up on someone to make sure they aren't screwing up (or misleading you), then you don't actually trust them. Thus, \"trust but verify\" is not trust at all. reply bitwize 3 hours agoparentprev\"Trust, but verify\" comes from the Soviet Union and is an example of Russian humor. It helps that the Russian words for \"trust\" and \"verify\" rhyme. It really means something like \"act like you trust the person, but secretly, don't trust them and double check that they've fulfilled their commitments\". Putting on a smiling face and acting as if everybody was acting in good faith, while secretly expecting the stab in the back because you would do the same, was key to diplomacy in the USSR, even between departments of the same government. reply mistermann 2 hours agoparentprev> I think \"trust, but verify\" (as mentioned in the article) is a much more useful motto than \"never trust anyone\". The latter isn't an useful attitude, if you took it seriously you would have carefully check or rewrite everything from the ground up. Not actually. You're describing an abstraction: your opinion/perception of what must/can be done. It is possible to be comfortable with uncertainty and the unknown (everyone already is, but only in certain, intuitive (in large part due to cultural conditioning, which comes in a variety of forms) ways), it's mainly just counter-culture and counter-intuitive, thus needs strategies, and practice(!) (plus some non-trivial multi-level, multi-dimensional recursion....this is what us HN folks are good at, and love though, right? Right?[1]). We've all been through the hard work at least once, in a certain (mostly) shared way. There are other ways though. > \"Trust, but verify\" is much more practical How do you verify your verification in complex scenarios though? I bet I know: trust/contentment (in your verification skills), though this layer typically is not revealed to us, so causes no psychological unrest (\"all is well\"), because it does not exist. > Don't be paranoid. What do you think your reaction would be if you discovered this is not just wrong, but backwards? [1] Alternatively: maybe we are only good at it, and only love it, sometimes? But then, \"we\" is a complex and deep set, into which we have little insight, but also plenty of hallucinated \"insight\". reply atoav 1 hour agoprevAs an electronics guy this is really ingrained. Not only could you easily waste days on a problem if you assume things rather than check them, in some cases you might also get a painful experience or depending on what you're working on that mistake might even be your last, burn down a house, kill others or what not. Checking your priors is one thing, ensuring your stuff fails gracefully if they are abnormal another. Meanwhile most software won't even handle a network disconnect gracefully. As more and more stuff™ is moving from hardware into software, for totally understandable reasons, the absolute number of software that could absolutely ruin human lifes is growing. This calls for higher standards when it comes to the whole field of software engineering. reply skydhash 1 hour agoparentHardware fails because of physical rules, software is more abstract and more flexible. I agree that higher standards should be required, but business people won't do it as it will make it as slow as hardware engineering. I'd love to see formal verification for at least the business core of any application. That should incentivize product managers to write better specifications. reply nottorp 4 hours agoprevMy favourite advice to give out is: \"If you feel very smart after writing a particularly intricate piece of code, it's time to rewrite it to be more clear.\" Speaking of not trusting yourself. reply will1am 4 minutes agoparentClear and maintainable code is valuable reply bruce511 11 hours agoprev>> Random access of a character in a text buffer could take constant time (for ASCII) or linear time (for UTF-8) depending on the character encoding This is true, but incomplete. All unicode encodings take linear time, not just utf-8. That's because a character can contain multiple code points. Utf-32 allows for random access of code points in constant time, but not characters. Utf-16 has variable length code points so it is the same as utf-8 in that regard. reply darby_nine 1 hour agoparentI've come to the conclusion that \"character\" is worth abandoning as a coherent concept. Words and bytes are much more useful and easier to define and aren't dependent on which runtime you're using, and the number of times it's worth slicing a word into characters is actually pretty low. One exception to this might be chinese (and I'm sure others) where differentiating words is not a straightforward task of splitting by whitespace, but they also have much more straightforward glyph rendering bypassing most of unicode's nastiness. Random access strings in chinese are actually super straightforward once you abandon run length encoding to heavy users of ascii and emoji. reply LegionMammal978 5 hours agoparentprevReally, it's not even random access per se that takes linear time, but random access indexed by code points. You can access the middle of a UTF-8 string just fine if you index by byte position. (Or more generally, by code units for UTF-16.) reply mjevans 10 hours agoparentprevIMO within a small range iterating isn't too painful. It's probably safe, maybe even close enough to optimal for typical use, to have an array or list of bytestrings for each line. Or maybe more complex Line (of text) objects that record the byte-length, 'codepoint'-length, and '(display)character'-length. There might even be special cases built in for typical and massive documents (number of lines) and overly long lines. reply bluGill 4 hours agorootparentIn most real world cases N is small and so a linear search for data beats a binary search - the binary search is going to stall the pipeline with cache misses all the time, while the linear search will prefetch everything into the cache before you need it thus resulting in not pipeline stalls. Of course different computers (CPU, memory configuration... they all matter) have different characteristics so where N is large enough is different for each. However in general linear search is fast enough these days. Where it isn't you can look at a profile to verify you hotspot. reply Zamicol 6 hours agoparentprevPedantically, UTF-16 has some code points out of order. The way I've always seen it implemented results in more operations spent on some points than others. See this example C code: https://icu-project.org/docs/papers/utf16_code_point_order.h... UTF-8 doesn't suffer from this design flaw. reply myworkinisgood 11 hours agoparentprevYeah, but the author was talking about ASCII. ASCII takes constant time. reply bruce511 8 hours agorootparentThe author differentiated I the statement between ASCII and utf-8. The reference to the specific encoding (utf-8) was misleading because the reference should have been to Unicode (the mapping) as the issue applies to all Unicode encodings, and is not limited to utf-8. ASCII does indeed take constant time - which I agreed with. reply atribecalledqst 6 hours agoprev> Failing tests indicate the presence of bugs, but passing tests do not promise their absence. As somebody who primarily lives on the testing side of the house, I've definitely run into cases where the developer promises that their unit tests will make a new feature less buggy, then about 5 minutes later I either find a mistake in the test or I find a bug in something that the developer didn't think to test at all. I've also seen instances where tests are written too early, using a data structure that gets changed in development, and then causes churn in the unit tests since now they have to be fixed too. I've generally come to think that unit tests should be used to baseline something after it ships, but aren't that useful before that point (and could even be a waste of time if they take a long time to write). I don't think I'll ever be able to convince anybody at my company about this though lol reply gchamonlive 5 hours agoparentTests are akin to scientific experiments. They test hypothesis and try to falsify claims. They shouldn't be seen as ground truth, but ways to gain information about what the system claims to be doing. In this sense it makes sense that tests will become obsolete or evolve with the system, because the model and domain upon which the system is based also evolves and changes with time. reply randomdata 5 hours agorootparentThis is why I'm not sure more languages don't instil the idea of \"public\" and \"private\" tests like Go does. Your \"public\" tests should document the API for future programmers. This is the concrete contract that should never change, no matter what happens to the implementation. If these tests break, you've done something wrong. Your \"private\" tests are experiments that future programmers know can be removed if they no longer fit the direction of the application. reply marcosdumay 4 hours agoparentprev> I've also seen instances where tests are written too early, using a data structure that gets changed in development That's as clear a signal that they are testing the wrong interface as you can get. Unfortunately, developers think of tests as testing code, not interfaces. As a natural consequence, they migrate towards testing the most complex break-down of their code as they can; it increases the ratio of code coverage / number of tests... at the cost of functionality coverage. reply Spivak 3 hours agorootparentThere's two kinds of tests I think developers are trying to write and I think both of them have merits. Writing a test for the code is actually totally fine so long as the reason that you're doing it is because you want to be able to depend on that and you need something to scream if ever changes. I think starting with a test that the code does what the code does is actually a pretty good starting point because it's mechanical. And if you never end up revising that code, it can just live there forever, but if you do end up revising the code, those tests will slowly morph overtime into testing the interface. When you actually do your revision, you get a very clear signal that like hey this test that is now failing as a result of the change, clearly that can't be the important part. Waiting for the change to see what stays the same I think is often more accurate than trying to guess what the invariants are ahead of time. reply marcosdumay 3 hours agorootparentI get the impression you are talking about taking over legacy code. Well, the invariants you want to test are what people want the software to do. If you create those test from the beginning, there's nothing to guess. But of course, if people write a bunch of code and throw that knowledge away, you have to recover it somehow, and it will necessarily involve a lot of guessing. reply fendy3002 6 hours agoparentprevUnpopular opinion, but I always say that unit tests are contracts for the API. If you don't want / don't need to make contract, don't do unit tests. Unit tests main purpose is not to improve code or reduce bug, it's main purpose is to verify the code to work against the contract that's defined in unit tests. Code improvement or bug reduction are added benefit, if any. reply dartos 6 hours agorootparent> Unpopular opinion, but I always say that unit tests are contracts for the API You’re talking about integration tests or e2e tests. Those don’t sound like unit tests. reply bluGill 5 hours agorootparentUnit tests testing the API - if there is no API there then the level of unit it probably too low. The purposes of all tests is to say \"No matter what this will never change\" - while never is a bit to strong as you are allowed to make changes any API that your unit tests covers will be painful to change, both because the tests will also have to change and so you have nothing to guide you, and also because odds are you don't have good coverage from other tests (integration tests would catch issues, but you rarely have all cases covered). Or to put it a different way, your unit tests should cover units that have a good boundary to the rest of the system. This should sound like a module, but there is reason to have module as a larger thing than your unit (most of the time there shouldn't be, but once in a while this is useful), and so while there is overlap it is often useful to consider them different. Integration tests cover the API, but they do not test the API (well they often use some API as well, but the won't cover all your internal APIs.) reply matt_j 6 hours agorootparentprevAPI doesn't imply integration. Consider any module or package to have an API exposed to the user of the package. Unit tests should assert that the package behaves as expected. reply bluGill 5 hours agorootparentBy users I assume you mean other developers who use the API (including you next week). It would be better to use a different term as often user means \"end user\" or \"customer\" and not internal users. reply matt_j 5 hours agorootparentYeah, I'm not sure what the better word is. As a programmer, I use APIs. \"interact with\", \"code to/against\". I think \"user\" is OK. We're all users at different levels of abstraction. reply randomdata 5 hours agorootparentprevThat's exactly what Beck described when he originally coined the term. reply thfuran 6 hours agorootparentprevYour units don't have some interface through which they interact with other units? reply bobthepanda 1 hour agorootparentAPI is accesible public interface, I feel like I’ve seen it used to talk about accessible “to the public/other teams” from a service standpoint but not to describe any sundry public methods of a file. reply HankB99 5 hours agoparentprev> something that the developer didn't think to test at all. Raising hand, guilty as charged. I test for things based in my concept of how the system works, but those darn users may have other ideas! Actually, when I found a user who seemed to have a knack for finding bugs, they were gold and I let them know I appreciated their efforts. > unit tests should be used to baseline something after it ships That has not been my experience. I found that unit tests let me get the pieces working properly so that when assembled, the chances that everything worked as expected were much improved. reply willcipriano 5 hours agorootparent> those darn users may have other ideas Getting those ideas in front of you is the job of the product team. It isn't your fault if the guy writing the ticket refuses to talk to the users/and or/you. \"Ticket meets acceptance criteria, please submit a new request for this new feature.\" reply sameoldtune 5 hours agoparentprevMy gift to any team I work with is an integration test harness that usually has some kind of DSL for setting up state. This looks wildly different depending on the project. But my theory is that if tests are easy to write then it is easy to make more of them. So it is worth it to write some ugly code one time under the hood to make this happen. If every test requires copy pasting a bunch of sql statements and creating a new user and data, my experience is the team will have 3-4 of these kinds of tests. But if the test set-up code looks like `newUser().withFriends(3).withTextPost(“foo”).withMediaPost().sharingDisabled().with…` then the team is enabled to make a new integration test any time they think of an edge case. reply williamdclt 3 hours agorootparentwe have _exactly_ that (down to the `withXXX` syntax), and indeed I found it great. Downside is that test setup can be a bit slow: each `withXXX` can create more data than really necessary (eg a \"withPost()\" might update some \"Timeline\" objects, even though you really don't care about timelines for your test). Upside is that it's a lot closer to what happens in reality, regularly finding bugs as side-effect. And also you align incentives: you make your tests faster my making your application faster. reply packetlost 5 hours agoparentprev> I've generally come to think that unit tests should be used to baseline something after it ships, but aren't that useful before that point I disagree, but not entirely. I think there's a balance. Tests can be a great way to execute code in isolation with expected inputs/outputs and can help dramatically in absence of other ways to execute the code. But in general, I mostly agree. Tests are mostly valuable as a way of ensuring you don't break something that was previously working, but they are still valuable for validating assumptions as you go. reply dartos 6 hours agoparentprevI’ve been preaching this for a while. When a codebase gets too big, and devs gets too clever with their tests, the whole test suite becomes complicated. If your test suite is approaching the complexity of the actual codebase (what with layers of mocks and fixtures that are subtly interdependent,) how could you be expected to trust a test you wrote more than the code you wrote. reply bloopernova 3 hours agorootparentI (sysadmin/devops) am writing some nodejs and the complexity of the tests is confusing to me. I'm a nodejs beginner to be sure, and I'm not experienced enough to verify what copilot gives me. All those mocks, and other Jest code, all seem overly complicated but I don't know of anything \"better\". reply randomdata 5 hours agoparentprev> I've generally come to think that unit tests should be used to baseline something after it ships In theory, but I've never seen anyone successfully write tests after something ships. By that time much of the context that should be documented in tests is forgotten. reply bluGill 4 hours agorootparentI've done a handful of tests over the years. Once in a while the original didn't have any tests and I had no confidence I could change it without writing tests. Once in a while the original was buggy and after getting tired of going back top fix bugs I wrote a few tests. Once I had a case where the fix for bug A introduced by B, which the obvious fix was revert this code that made no sense thus bringing back bug A - when someone realized this was happening every year we write a few tests just to stop that pattern. The above those is a very rare exception. The general rule is once code is shipped management doesn't allow you time to make it better. reply englishspot 6 hours agoparentprev> I don't think I'll ever be able to convince anybody at my company about this though lol especially if the company uses 100% code coverage as a metric for success reply jagged-chisel 6 hours agoparentprev> ... instances where tests are written too early ... omg yes. However, reading this makes me wonder how the TDD people handle this. reply stonemetal12 5 hours agorootparentIt is just the cost of good quality. This is like suggesting you shouldn't write error handling code, because the code might change and have different errors that need to be handled. Also if the interface doesn't change but your unit tests fail on a data structure change then perhaps your tests are too coupled. reply marcosdumay 4 hours agorootparentprevWhy would anybody doing TDD have tests highly coupled with the implementation? They shouldn't have this problem at all. The GP diagnostic isn't good here. Those are bad tests, not tests written too early. What doesn't mean you shouldn't wait for your functionality to be accepted before testing it; some times you should; but this happens for completely different reasons. reply randomdata 5 hours agorootparentprevBy using TDD, which promotes isolating the changeable surface area to a small area during discovery. That way you don't have to introduce the complexities of API changes across the rest of the application surface area, avoiding the churn spoken of earlier. reply bluGill 5 hours agorootparentprevMost of the time I already know what I'm going to write. Thus most of the time I can start with a simple test and it isn't too early. It is rare to be presenting with a problem in code that you don't know if it is solvable or how to solve it and jump right into code (as opposed to research, or white board discussions), which then gets enough in place that it isn't too early. reply nullserver 5 hours agoparentprevI inherited an ancient project that had literally tens of thousands of test. I reviewed hundreds of them, tried rewriting dozens of them. Eventually, realize that essentially all of the tests were just testing that mock data being manually manipulated by the test gave the result of test expected. Absolutely nothing useful was actually being tested. Some team spent a couple of years writing an unholy number of tests as a complete waste of time. Basically just checking off a box that code had tests. reply randomdata 5 hours agorootparentThe first box on the testing checklist states that the test should first fail. I wonder how they managed to not test anything while seeing the tests transition from failure to success. reply bluGill 4 hours agorootparentFrom the GP: mock data being manually manipulated by the test gave the result of test expected. These tests are easy to write - your mock returns something, and then you verify that the API does nothing (thus the test fails), then returns whatever the mock does and the test passes. These tests are easy to write and they do fail until code is written. However they are of negative value - you cannot refactor anything as the code only calls a mock, and returns some data from the mock. reply randomdata 4 hours agorootparentI can't imagine writing a function that is nothing more than an identity function would be easy to write (unless it was explicitly intended to be an identity function, I suppose). There must be some terrible gut wrenching feeling that goes along with it, if nothing else? Frankly, I don't understand how this situation is possible in practice, but perhaps I misunderstand what is written? reply drewcoo 45 minutes agorootparentprevI've never seen that but I've heard the claim before. So . . . is the team of devs who spent years writing and maintaining those tests incompetent or is it the new dev with the complaint? If it was the whole team, how did that happen? reply drewcoo 53 minutes agoparentprevTests don't find bugs. They find things a developer needs to investigate. Most tests I've seen aren't aids to diagnosability, so if there is a bug, the developer is still needed to find it. > tests are written too early, using a data structure that gets changed in development I wouldn't call this \"too early,\" but \"testing the wrong thing\" if they were testing internal particulars instead of behaviors. reply tm11zz 10 hours agoprevThis is a really useful mindset as a programmer, but backfires in real-life as it makes you an anxious person. reply will1am 3 minutes agoparentWhile achieving absolute certainty in code correctness is often impossible reply eru 10 hours agoparentprevI guess you need to compartmentalise into different kinds of 'trust'. The not 'trusting' you do with a computer is different from the trusting you do with fellow humans in daily life. They just happen to use the same word in English. reply blitzar 8 hours agorootparentI trust the computer far more than the fellow humans. The computer will generally give a predictable output for a given input. \"fellow humans in daily life\" .... not so much. reply will1am 0 minutes agorootparentHuman variability and unpredictability can also be a source of creativity. But I understand you completely eru 6 hours agorootparentprevI hope you never have to cross a street, or get anywhere near a car with a human behind the wheel. reply blitzar 6 hours agorootparentIts terrifying; sometimes when I am crossing the street people accelerate, sometimes they slow down, sometimes they stop at red lights, sometimes they drive through them. reply Zambyte 5 hours agorootparenthttps://www.youtube.com/watch?v=3mnG_Gbxf_w reply blitzar 5 hours agorootparentIf it is a tesla computer you can be certain it wont stop. reply Zambyte 7 hours agoparentprevFunnily enough, I program in real life and have also been anxious lately. reply mistermann 1 hour agoparentprev> but backfires in real-life as it makes you an anxious person. If I was to point out this is an approximation, or a tautology (it is only true to the degree that it is true, which is not (necessarily[1]) 100% of the time), would it make you anxious? And if so, do you think it wouldn't be possible for you to learn [1] a new approach so it does not make you anxious? reply pcwelder 11 hours agoprevHigh quality article with some new advices. > Read more documentation than just the bare minimum you need I wish I practiced this before, I'd be as good and quick as some of my brilliant colleagues. reply skydhash 1 hour agoparentI love it when projects gave single HTML documentation or a PDF. In the worst case, I'd take a nice site like Laravel's documentation. Any piece of software I use, I try to do a speed reading of the whole documentation or a significant part if it's big. A mental model of all the offered features is a nice help when solving problems. reply protomolecule 4 hours agoparentprevI usually do, but not a long time ago after I've read docs on boost::spirit, gpt4 suggested a trick with which I could've come up myself but I failed to connect two different pieces of documentation. This left me wondering if reading the docs thoroughly is still a good investment. On the other hand, having some level of knowledge of a library saves me some time since I can often write things faster by myself than explaining gpt4 in more and more details what I need. reply vzaliva 3 hours agoprevAs someone who routinely formally verifies code, I can confirm that you should not trust yourself. I keep finding bugs and hidden assumptions in the most trivial code or even in code covered with unit tests. In my opinion, formal verification is the only way to truly trust some code. reply simonw 2 hours agoprevThis is why I try to have my code commits bundle tests along with any corresponding implementation changes. The job of the test is to PROVE that the updated implementation code did what it was supposed to do - both now and into the future. If the test doesn't do that - if it still passes even if you revert the implementation - then the test isn't doing its job. reply jll29 8 hours agoprevThis is a good post, and I agree with the \"paranoid porgrammer\" attitude being useful. The post could also talk about yet another desirable paranoia, namely \"Am I building the right thing?\" - so talking to customers/clients/stakeholders/users is arguably one of the most important steps when seeking re-affirmation and controlling one's work. Nothing worse than people who think they understand a technical problem, but it's not the one that the customer wants solved... reply BD103 3 hours agoprevI love the link to the article on time dilation in this article, fascinating! (https://pilotswhoaskwhy.com/2021/03/14/gnss-vs-time-dilation...) reply megamix 5 hours agoprevThere's a leaky reasoning about the abstraction hierarchy I think. Beyond, or until a certain layer - I know that I can trust the process. I'm fine baking, I trust the physical process. reply Joel_Mckay 10 hours agoprevAfter many years, I settled on a constraint based design philosophy: 1. type checking, data marshaling, sanity checks, and object signatures 2. user rate-limits and quota enforcement for access, actions, and API interfaces 3. expected runtime limit-check with watchdog timers (every thread has a time limit check, and failure mode handler) 4. controlled runtime periodic restarts (prevents slow leaks from shared libs, or python pinning all your cores because reasons etc.) 5. regression testing boundary conditions becomes the system auditor post-deployment 6. disable multi-core support in favor of n core-bound instances of programs consuming the same queue/channel (there is a long explanation why this makes sense for our use-cases) 7. Documentation is often out of date, but if the v.r.x.y API is still permuting on x or y than avoid the project like old fish left in the hot sun. Bloat is one thing, but chaotic interfaces are a huge warning sign to avoid the chaos. 8. The \"small modular programs that do one thing well\" advice from the *nix crowd also makes absolute sense for large infrastructure. Sure a monolith will be easier in the beginning, but no one person can keep track of millions of lines of commits. 9. Never trust the user (including yourself), and automate as much as possible. 10. \"Dead man's switch\" that temporarily locks interfaces if certain rules are violated (i.e. host health, code health, or unexpected reboot in a colo.) As a side note, assuming one could cover the ecosystem of library changes in a large monolith is silly. Good code in my opinion, is something so reliable you don't have to touch it again for 5 years. Such designs should not require human maintenance to remain operational. There is a strange beauty in simple efficient designs. Rather than staring at something that obviously forgot its original purpose: https://en.wikipedia.org/wiki/File:Giant_Knife_1.jpg https://en.wikipedia.org/wiki/Second-system_effect Good luck, and have a wonderful day =3 reply keepworking 9 hours agoprevI think is kinds of \"Need for cognitive closure\". It is diffrent with \"Never trust anyone\" and \"Everyone can be wrong\". the NTA is make us can not to move forward. But allow the Everyone can be wrong makes we can move forward. reply throwitaway222 3 hours agoprev-Programmers- > People reply dwighttk 7 hours agoprevThe radical trust of turning on a computer reply leecommamichael 10 hours agoprevWhat does it do to the psyche, to not be able to trust anyone? reply DoctorDabadedoo 9 hours agoparentCynism and burnout, but if you are in a dysfunctional it's hard to move away from that either way. reply amelius 10 hours agoparentprevProgrammer syndrome. reply Barrin92 9 hours agoparentprevNothing if you don't take it personally. There's 'not trusting' in the sense of recognizing that most processes or people are unreliable, including yourself and there's 'not trusting' in the sense of thinking everyone's out to get ya or not living up to your expectations. The second one largely has to do with ego and is the one that creates insecure people who usually hold others to different standards than themselves, the first is just a realistic view of the world and thus very useful reply psychoslave 10 hours agoprev> verifying code correctness is impossible Somehow, taking into account the state of our industry, yes. But this is not an absolute truth. I mean, we do have the theoretical frameworks and even tools to come with solutions that allow to proof that code is correct. It’s just that mapping this \"know how\" with the \"how to deal with the expected flow rate feature\" is very uncommon. reply n4r9 9 hours agoparent> tools to come with solutions that allow to proof that code is correct I may be misunderstanding, but isn't part of the problem that these tools are themselves written in code and therefore subject to bugs? reply agentultra 3 hours agorootparentThis is addressed by the de Bruijn Criterion. The essential idea is that a small number of \"trusted\" rules should be enough to satisfy even the largest proofs. You have to keep the number of rules small enough that they can be reviewed and understood by humans so that you can trust the proofs verified by the kernel. reply n4r9 3 hours agorootparentThat probably helps to reduce the occurrence of issues, but I feel you are still ultimately relying on the correctness of proof assistants like Coq. And I am sure that bugs are occasionally found in Coq! reply agentultra 2 hours agorootparentIndeed it does happen and it's unavoidable. You have to pick your axioms from somewhere and sometimes we pick the wrong ones or we find errors in our definitions. There's no \"complete\" or \"perfect\" system. reply superidiot1932 3 hours agorootparentprevThere are verified compilers such as the compcert compiler and also ways to verify that a given binary does in fact correctly implement an specification reply tpm 10 hours agoparentprevIt is an absolute truth in the sense that it is true for all code that was not specifically written to be formally proven correct. So, given an arbitrary piece of code, it's impossible to verify it (because of the halting problem). reply skydhash 1 hour agorootparentIt's not impossible to verify it, it's impossible to automatically verify it with another program. Other issues are the underlying abstract machine (which can have its own bugs) and the surrounding environment (you either need to assert or/and sanitize your inputs). reply cryptica 4 hours agoprev> Programmers Should Never Trust Anyone, Not Even Themselves Yep. Can confirm, the best programmers are often paranoid. I guess over time your brain becomes more logical and you start to notice inconsistencies everywhere around you... Then before you know it, it feels like you're living on planet of the apes. reply mistermann 2 hours agoprev> So if abstractions can be problematic, then should we try to understand a topic without abstractions (to know cars as they really are)? No. When you dig beneath abstractions, you just find more abstractions. Demonstrating that language and colloquial \"logic\" are also abstractions. > It’s turtles all the way down. Memes and catch phrases are abstractions. > These layers of abstractions go down until we hit our most basic axioms about logic and reality. Reality too is an abstraction. Luckily all humans run Faith, and it runs invisibly, otherwise I suspect we'd have not made it this far. Though, it now seems like that which saved us may now take us down (climate change, nuclear weapons, other/unknown). > Trust, but verify. Haha...of course, just use logic and critical thinking! reply mewpmewp2 11 hours agoprevBut then you say \"trust, but verify\" in the post. reply fragmede 11 hours agoparenttrust but verify translates to trust no one but don't be a dick about it reply nine_k 11 hours agorootparentTo me, it's rather a postcondition instead of precondition. Trust that a person will do the right thing, but verify that the right thing has actually been done. reply stavros 10 hours agorootparentWhich is a contradiction, because trusting someone means not needing to verify what they say or do. reply laserlight 10 hours agorootparentTo the contrary, trust arises from being able to verify. Otherwise, you never know whether a failure is because of ill intentions or errors. That's why I very much dislike “trust, but verify”. I prefer “trust, and verify”. reply stavros 10 hours agorootparentWhat's the meaning of \"I trust you that the amount of cash you've given me is correct\" when you then proceed to count it? If you're verifying, why do you need to say \"I trust you\"? A trustworthy and an untrustworthy person will get equally verified. reply dncornholio 9 hours agorootparentHas nothing to do with trust. I trust that you will give me the money. I also trust that you counted the right amount. But I still want to make sure you did not make a mistake. reply robertlagrant 8 hours agorootparentI suppose the question is: why bother with trust? Why not just \"verify\"? reply t_mahmood 6 hours agorootparentI trust you, about your intention and effort. But I verify because there are too many moving pieces in the world that can go wrong, that you may have no control over reply dncornholio 2 hours agorootparentprevWell the thing is, that’s how trust already works isn’t it? Trust doesn’t require any effort. Trust means I won’t put in effort. So I just verify, because I trust. reply worble 8 hours agorootparentprev> postcondition instead of precondition. How can you verify as a precondition instead of a postcondition? You can't verify anything until the act has been completed. reply ramesh31 3 hours agoprevYou do need a certain level of self confidence to actually get anything done though, and not get lost in analysis paralysis. The mantra goes \"strong opinions, loosely held\". Be prepared to vigorously defend your position, while simultaeneously being willing to immediately toss it aside in the face of overwhelming evidence. reply m0llusk 10 hours agoprevAlso from 8 days ago https://news.ycombinator.com/item?id=40764826 and 9 days ago https://news.ycombinator.com/item?id=40760885. The \"A Python script should be able to run on any machine with a Python interpreter.\" remark is amusing. Recently ended up installing a whole new distribution version just to get Python 3.11 and the new library versions that the script I was running depended upon. Giving a program finish and polish does have this kind of unlimited depth, but it is critical to remember that comes after the initial coding which is always rough with many gaps since that is always where things start. And then we make many choices. Hearing people commit to strong typing because docs are always out of date is another chuckle. Maybe if the docs were kept right from the start, maybe with some use of automatically generated reference pages, then there wouldn't be such frequent problems getting types right in the first place? To each their own, but strong typing hype is just another currently popular method among many. Strong typing has its value, but like every other methodology cannot be absolutely trusted to save programmers from error. reply globular-toast 11 hours agoprev> In reality, the bank does not just store the money we deposit. It loans away/invests most of the money that people deposit. Our money does not sit idle in a large pile in a vault. Nope. In reality the money doesn't exist. Amazing how many people think that somewhere a cartload of money is physically moving every month when they get paid. When was the last time you deposited anything in a bank? The abstraction is even higher than that. It's just numbers in a computer system. The abstractions work because banks are \"too big to fail\". reply xyzzy123 10 hours agoparentThis is going to sound weird but even if it was actual coins (say) would that make the whole thing any more \"real\"? Either way it's a symbolic system and it doesn't really matter if you execute it on an abacus, or in a digital computer, or by writing in a book and carting bags of coins around. I think the parts that are missing from most people's mental model are things like the role of the central bank, and the fact that lending limits are influenced but not determined by deposits. reply JonChesterfield 10 hours agorootparentYes, physical coins or grams of gold or whatever are more real. Primarily because they add inertia to changing the float, it takes time to find more gold or make more coins. You can't 10x the supply overnight. Things changing more slowly damps out oscillations. reply globular-toast 9 hours agorootparentprev> This is going to sound weird but even if it was actual coins (say) would that make the whole thing any more \"real\"? Yes, because then there would be a physical limit on what banks can do. They would have to invent cloning machines or alchemy or whatever to do what they currently do. In other words, it would force bank to actually do the model most people (including the author) have in their heads, namely a bank stores cash and lends it out (called fractional reserve banking). In reality, without any such physical constraints, the banks are essentially free to \"create\" money out of thin air by issuing loans. That's how the money supply became 99% \"bank money\" and only ~1% physical money. reply jampekka 10 hours agoparentprevIndeed banks don't loan/invest deposits. Banks conjure money out of thin air when a loan is made. Some amount of deposits (a small fraction of the loans) is sometimes needed, although e.g. USA has scrapped fractional reserve demands. I guess people have hard time accepting this because it seems too absurd to be true. Also it's common to confuse money to be a scarce resource/commodity because that's how it looks like for most individuals (and that's the story usually pushed for us plebs). reply ttoinou 10 hours agoparentprevAgreed. But a bank failing doesn’t mean the financial system would be down though. We need to get rid of bad apples. Free banking reply Vecr 10 hours agoparentprevFractional reserve backing is old school, zero reserve banking is where it's at. reply IshKebab 10 hours agoparentprevI don't think the author was under the impression that banks physically store money. They meant the digital currency does not sit idle on the bank's asset list. They're just using normal human language. reply globular-toast 9 hours agorootparentThe deposits aren't assets to the bank, they are liabilities. The loans are their assets (your liability is their asset). They don't need to \"do anything\" with your deposit because it doesn't exist. If I increment the number 2 to the number 4, have I brought something into existence, in particular 2 \"things\"? What is that? Why can't I just bring 3 things into existence without incrementing the initial number? If this sounds silly it's because it is. I thoroughly recommend anyone who is confused to a) do their own accounts and b) invent a silly currency inside your accounts and start a bank for your imaginary friends. Just add trust and government support to your bank and you'll be like any other bank. reply IshKebab 44 minutes agorootparent> They don't need to \"do anything\" with your deposit because it doesn't exist. Uhm yeah they do. If they take my money and then just do nothing with it then they won't make any money from it. Banks invest your money. (I don't think that's the main way they make money - it's probably mostly from credit card interest, but they definitely do it.) The money you pay into banks absolutely exists in every sense. Banks can create money when they issue loans, which I suppose you could argue doesn't exist. But they aren't allowed to create unlimited money. I'd say it exists as much as any other money exists. reply mulmboy 11 hours agoprev> Failing tests indicate the presence of bugs, but passing tests do not promise their absence. If only :) Far too often I find myself working with tests that patch one too many implementation details, putting me in a refactoring pickle reply yxhuvud 10 hours agoparentOr even worse, tests that test implementation details that doesn't matter for the actual outcome. reply hyfgfh 10 hours agorootparentIf I had a dollar for each frontend test that actually don't actually test anything I would be able to retire by now! reply orwin 7 hours agorootparentTests that don't test anything come in at least two categories for me: - test that were useless, are still useless and will always be useless - tests that are currently useless but were used in the \"wtf should i write\" phase of coding (templating/TDD/ whatever you want to call it). I'm partial towards the seconds, and i like when they're not removed, because often you understand how the API/algorithm was coded thanks to them (and its often unit tests). But ideally, both should be out of a codebase. reply bdjsiqoocwk 10 hours agorootparentprevCargo cult testing. Some people don't understand the point of testing so they just go thru the motions and end up with something that makes no sense. reply throwawaysleep 8 hours agorootparentOnly needs to be management that misunderstands. I’ve written plenty of do nothing tests in my time to be sure that management regularly got a report of tests being added. reply blitzar 8 hours agorootparentprev\"It passed all the tests so it must be working, it must be something you are doing wrong\" reply inopinatus 9 hours agorootparentprevUsed to be (perhaps still is) a nasty habit of Rails apps to have vast test suites covering every Active Record query they ever used (with fixed seeds to boot), rarely straying from giving the bog-standard and already very thoroughly tested and battle-scarred AR predicate builder a wholly unneeded workout; but none of their own front-end code because writing for selenium was too hard. But look! Thousands of tests and they all pass! Taste the quality! reply yxhuvud 8 hours agorootparent> but none of their own front-end code because writing for selenium was too hard. I've also seen plenty of tests that test if a template was rendered rather than if whatever thing it actually outputs was in the output. It is just calcifying the impementation making it hard to test. But it is a tradeoff, and a hard one as well, because if you do all things all the time, combining all variations of database with all variations of the views, then you end up with a test suite that take forever to run. Finding the right tradeoff there has not shown itself to be very obvious, sadly. reply germandiago 8 hours agorootparentprevOne thing I do sometimes is to start part of an API in a TDD style. Everything starts very \"basic\", which adds a lot of relatively trivial test cases. When done with that phase and my API looks relatively functional, I remove all relatively trivial tests and I write bigger ones, often randomized and property-based. This works decently well and you do not have an army of useless tests there hanging after the process is done. reply AlienRobot 6 hours agorootparentprevBut if you don't test those, you may break someone's workflow! https://xkcd.com/1172/ reply quectophoton 8 hours agoparentprev> refactoring pickle Been there. Change one tiny thing, and 20 tests fail all over the place. But hey, at least we had ~95% test coverage! /s The more time some piece of code has survived in production, the more \"trusted\" it becomes, approaching but never reaching 100% \"trust\" (I can't think of a more precise word at the moment). For tests it's similar; the longer they had remained unchanged while also proving useful (e.g. catching stuff before a merge), the more trusted they become. So when any code changes, its \"trust level\" resets to zero at that point, whether it's runtime code or test code. The only exception might be if the test code reads from a list of inputs and expected outputs, and the only change is adding a new input/output to that list, without modifying the test code itself. Tests that change too frequently can't be trusted, and chances are those tests are at the wrong level of abstraction. That's how I see it at least. reply olivierduval 7 hours agorootparentJust been beaten by a bug in a production system... hidden in code silently for more than 10 years ! It just mean that for 10 years, this codepath has not been taken (The conditions for this specific error case was not met for 10 years) :-( Actually, it would be a good monitoring information to know which path are \"hot\" (almost always taken since the beginning), \"warm\" (from time to time) or \"cold\" (never executed). It could help build a targetd trust. I guess that it might be possible for VM languages (like based on JVM) because the VM could monitor this... but it might be harder for machine code reply aunderscored 6 hours agorootparentThis could be interesting. Unfortunately it'd be a performance hog to do. Some kinds of things do work with this (see performance guided optimisation in compilers) reply lenerdenator 5 hours agoprev [–] You sort of have to trust yourself, but verify it against what others expect. That's the point of code reviews. Trust me; I'm an expert on never trusting myself. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Programmers should maintain a healthy level of skepticism, as writing and verifying code correctness is inherently challenging and often impossible.",
      "Abstractions, while simplifying complex systems, can fail and lead to issues like performance degradation or undefined behavior, as highlighted by Joel Spolsky’s Law of Leaky Abstractions.",
      "To mitigate unknown issues, programmers should verify information, test beliefs, and measure the impact of code changes, while continuously learning about new platforms, languages, tools, and technologies."
    ],
    "commentSummary": [
      "The discussion centers on the importance of formal verification in programming, emphasizing that programmers should not trust anyone, including themselves, without proof.",
      "Formal verification, though complex and costly, provides stronger guarantees of correctness compared to unit tests, which only cover specific examples.",
      "The debate highlights the trade-offs between the rigor of formal verification and the practicality of unit tests, suggesting that the choice depends on the project's requirements and resources."
    ],
    "points": 172,
    "commentCount": 139,
    "retryCount": 0,
    "time": 1719812397
  },
  {
    "id": 40840315,
    "title": "Unification in Elixir",
    "originLink": "https://www.ericpfahl.com/from-pattern-matching-to-unification/",
    "originBody": "Unification in Elixir Eric Pfahl Feb 25, 2023 • 13 min read Pattern matching is a pervasive and powerful tool in Elixir. This isn't too surprising if you know a little about the history of Elixir's parent language, Erlang. Erlang was originally inspired by and written in Prolog, a logic programming language where pattern matching has first-class support. In Elixir, you might see an expression like this: [x, [2, y]] = [1, [2, 3]] Copy When this expression is evaluated, the variables x and y on the left are assigned to values 1 and 3, respectively. The symbol =, called the match operator, attempts to associate variables with values that allow the two sides be equal. When a unique match is possible, variables are assigned and made available to the runtime. When a match isn't possible, Elixir raises a MatchError: [x, [2, y]] = [1, 2] ** (MatchError) no match of right hand side value: [1, 2] Copy Elixir allows one-sided pattern matching, where variables can appear only on the left side of the match operator. There are probably very good practical reasons for this choice, but there is no logical reason why variables couldn't appear on both sides. To help you get a better appreciation for how pattern matching works, and to tease the potential of more general logic programming, I'll walk through the implementation of two-sided pattern matching, also known as unification. Unification is an algorithm for solving symbolic equations. Described below are the ingredients and steps to make unification seem relatively intuitive and to allow you play with it yourself. What is unification? Like Elixir's pattern matching, unification is an assertion of equality, followed by variable assignment when the assertion is true. Unification operates on a pair of terms. These terms may be variables or atomic values (numbers, strings, atoms) or a list of terms. Successful unification produces a mapping from variables to values (other terms), so that the original assertion is satisfied. This mapping is a substitution. Unlike Elixir's pattern matching, the substitution that results from successful unification may contain variables with only partially known values. This allowance for uncertainty is a powerful feature. New information may come along that further refines existing variables, adds new variables to the substitution, or refutes the substitution and causes unification to fail. Here are some notional examples of successful unification, along with the resulting substitutions: #1 unify(1, 1) identity; empty substitution --- #2 unify(x, x) identity; empty substitution --- #3 unify(x, 1) x => 1 --- #4 unify(x, [1, y]) x => [1, y] --- #5 unify([x, [2, y]], [1, [2, [x, 4]]]) x => 1 y => [x, 4] Copy This interface is slightly different than the implementation shown below, but the spirit is the same. The first two example are identity relations: true statements that don't provide any new information. Partial knowledge is shown in the 4th example: x is assigned to a list, but y is unknown. In example 5, the substitution says that variable y is assigned to a list that contains variable x. But since x is assigned to 1, the value of y is actually [1, 4]. This kind of indirection in substitutions is common. You'll see below how to simplify variable assignments in cases like this. These pairs of terms fail to unify: #1 unify(\"one\", \"two\") --- #2 unify([x, 2], :a) --- #3 unify([x, x], [1, 2]) Copy Two different atomic values aren't equal, and a list can never be equal to an atomic value. The 3rd case says that a list of two identical elements can't be equal to a list with two different elements. If all of this still seems a bit murky, don't panic. The ideas are pretty straightforward, and once you see the code and put it to use, I think things will fall into place. Next, I'll define more precisely what is meant by term, variable, and substitution. What is a term? A term is a collective label for the kinds of values that can be unified. To distinguish from the broader class of Elixir terms, I'll use the type uterm when discussing unification. A uterm can be either an atomic value, a variable, or a list of uterm values. For this discussion, a list will be the only collection type in the definition of uterm. As an Elixir typespec, a uterm can be defined as @type atomic :: numberatombinary @type uterm :: atomicvariable[uterm] Copy (The variable type will be defined shortly.) Notice that the definition of uterm is recursive. If x, y, and z are variables, then examples of valid terms include 1, x, \"term\", [1, 2], [x, :a, 1, \"one\"], and [[x, y], [[1, [2, z]]]. What is a variable? A variable is placeholder for a value. That value can be any term, including another variable or a list that contains variables. Variables with different names, like x and y, may hold different values. If variable x appears more than once in the terms being unified, every instance of x must have the same value. And once the value of a variable is set, it can't have any other value. The term [x, y] might successfully unify with both [1, 2] and [1, 1]. The term [x, [1, x]] would successfully unify with [:a, [1, :a]], but not with [3, [1, 2]]. There are a number of ways to define a variable in code. Whatever the approach, it must be possible to uniquely identify different variables, and a variable must be distinct from a number, binary, atom, or list. Since we're working in Elixir, an idiomatic way to represent a variable is as a struct, like so defmodule Var do alias Var @type t :: %Var{id: any} defstruct [:id] @spec new(any) :: Var.t() def new(id), do: %Var{id: id} end Copy This defines a type Var.t and a constructor Var.new to be used outside the module. The variable type in uterm can alias the struct type: @type variable :: Var.t() Copy This struct representation makes it convenient to use Elixir pattern matching when working with variables. When constructing a variable, there doesn't need to be any relationship between the identifier and the Elixir variable name. It's valid to type x = Var.new(1). Just be careful to use different identifiers for different variables. As a convention, I'll define variables with an atom identifier that corresponds to the Elixir variable name, as in x = Var.new(:x) and y = Var.new(:y). What is a substitution? As already stated, a substitution is a mapping from variables to values, where a value can be any term. Unsurprisingly, an Elixir Map is an excellent choice to represent a substitution. This type definition serves as handy documentation: @type substitution :: %{}%{variable => uterm} Copy Because the substitution is represented as an ordinary map, standard functions like Map.fetch and Map.put can be used to query and update a substitution. A substitution will often have indirect relationships, where one variable is associated with other variables, and those variables depend on other variables, and so on. To resolve the value of a term, it may be necessary to hop among associated variables–to walk the web of relationships. Two kinds of walks will be used later, one shallow and one complete. The shallow version, which, according to convention, I just call walk, is used in the implementation of unification. The complete walk, which I call simplify, is used to present the final result of unification in the simplest form possible. A walk of a term is performed against the current substitution. In walk, the term is first looked up as a key in the substitution. If the key is present, the walk is applied recursively to its associated value. If, at some point in the recursion, the key is absent, the walk returns the term. Here's an implementation of the shallow walk: @spec walk(substitution, uterm) :: uterm def walk(sub, term) do case Map.fetch(sub, term) do {:ok, t} -> walk(sub, t) :error -> term end end Copy For the substitution %{x => y, y => 1}, a shallow walk on x first finds the term y, then the term 1, and then returns 1. If x is walked on %{x => y, y => [1, z]}, the walk stops when the list term [1, z] is reached, because a list can't be a key in the substitution; only variables can be keys. The function simplify starts with a shallow walk, but then descends into list terms and recursively applies simplify to their contents. Here's the code: @spec simplify(substitution, uterm) :: uterm def simplify(sub, term) do case walk(sub, term) do [ht] -> [simplify(sub, h)| simplify(sub, t)] term -> term end end Copy If the the shallow walk ends at a list, a new list is formed by applying simplify to the head and tail. Otherwise, the result of walk is returned. When x is simplified in %{x => y, y => [1, z], z => 2}, the result is [1, 2]. Putting it all together At this point, all the data structures and basic operations have been discussed. Now I'll step through the implementation of the unification algorithm. The public function for unification is @spec unify(substitution, uterm, uterm) :: {:ok, substitution}:error def unify(sub, term1, term2) do ... end Copy When unify succeeds, it returns an :ok tuple with an updated substitution; otherwise, :error is returned. When unify is first called, it will often be given an empty substitution, which is the same as saying that there's no prior knowledge about the variables. The assertion that the two terms are equivalent constrains the relationships between variables present in the terms. Information about these constraints is captured as an updated substitution. The substitution acts like a working memory as unification proceeds through a series of relatively simple steps. Step 1: Walk If one or both terms are variables, the values of those variables are needed before equivalence can be tested. Variable values are obtained by a shallow walk with the given substitution. These walked values are passed as arguments to do_unify, a private function that does the heavy lifting: def unify(sub, term1, term2) do do_unify(sub, walk(sub, term1), walk(sub, term2)) end Copy With a shallow walk as the first step, unification is able to navigate variable relationships incrementally, testing for equivalence and new information at each stage of the descent. Remember that walk returns the first value that isn't a key in the substitution. This could be a variable for which there is not yet a value, a list, or an atomic value. Each of these possibilities is dealt with, in turn, using multiple clauses of do_unify. Step 2: Equivalence First, test for the equivalence of the two walked terms. If the two terms passed to do_unify have equivalent values, then stop and return {:ok, sub}: defp do_unify(sub, term1, term2) when term1 == term2 do {:ok, sub} end Copy When the terms are lists, unification succeeds if the lists are equivalent. This is true even if the lists contain variables. Note that [x, 2] == [x, 2] is true for all values of x. Steps 3 and 4: Variable If the first term in do_unify is a variable (a Var struct), update the substitution by associating the variable with the second term. If the first term isn't a variable, perform the same test on the second term: defp do_unify(sub, %Var{} = var, term) do extend(sub, var, term) end defp do_unify(sub, term, %Var{} = var) do extend(sub, var, term) end Copy where extend is: @spec extend(substitution, variable, uterm) :: {:ok, substitution} defp extend(sub, var, term) do {:ok, Map.put(sub, var, term)} end Copy This is the step where new knowledge is recorded in the substitution. When unify(%{}, x, 1) is called, there's an assertion that x == 1. Since this assertion doesn't conflict with the initially empty substitution, the substitution is updated to %{x => 1}. The same result is obtained from unify(%{}, 1, x) because of the symmetry enforced by the pair of do_unify clauses that test for a Var struct. Step 5: List If both terms are non-empty lists, first unify the heads of the two lists. If the heads successfully unify, return the unification of the tails. It looks like this: defp do_unify(sub, [h1t1], [h2t2]) do with {:ok, sub}y, y => [1, z], z => 2} and asked to reduce the variables to their simplest possible forms. That means walking the relationships in the substitution as far as possible. This is the job of simplify that was introduced earlier. Here's a usage example: x = Var.new(:x) y = Var.new(:y) z = Var.new(:z) sub = %{x => y, y => [1, z], z => 2} simplify(sub, [x, y, z]) Copy This application of simplify returns [[1, 2], [1, 2], 2]. Notice that a new term, [x, y, z], was created to collect the simplified values. Examples This can all seem a bit dry and technical without meaningful examples. Here are two simple examples that hint at more general possibilities. Yes, and Here are three lists, each with variables: [x, y, z] = for i1, y => 2}, holds information about x and y, but not z. Feed this substitution into the unification of l2 and l3: {:ok, s23} = unify(s12, l2, l3) Copy This yields the substitution s23 = %{x => 1, y => 2, z => 3}. All the variables now have atomic values. Unification of l1 and l3 using the previous substitution will work just fine, but it doesn't add any new information. All of these steps can be captured in one block of code that looks like a pipeline, where the substitution from the previous step is fed to unify in the next step: with {:ok, s12}:left}, %{d1 => :right}] Copy The list of substitutions exhibits the two choices for d1. Another way to write this, for reasons you'll see very soon, is d1 = Var.new(:d1) [sl, sr] = [%{}] |> Enum.flat_map(fn sub -> {:ok, sl} = unify(sub, d1, :left) {:ok, sr} = unify(sub, d1, :right) [sl, sr] end) Copy Convince yourself that this returns the same result (see the docs for Enum.flat_map). What's the situation after two steps? The choices are the same (:left or :right), but each choice for the second step is compounded with the choice from the first step. The first step is either :left or :right, and the second step is either :left or :right. This is a conjunction of two disjunctions. The result from the first step is chained with the result from the second step. Here's what that looks like in code: d2 = Var.new(:d2) [sl, sr] |> Enum.flat_map(fn sub -> {:ok, sl} = unify(sub, d2, :left) {:ok, sr} = unify(sub, d2, :right) [sl, sr] end) Copy The result from the first step is piped to flat_map. This has the same structure as the code from the first step. The two-step result is [ %{d1 => :left, d2 => :left}, %{d1 => :left, d2 => :right}, %{d1 => :right, d2 => :left}, %{d1 => :right, d2 => :right} ] Copy There are four substitutions, each displaying a pair of binary choices. A third step will produce eight substitutions. Can you see how to pipe the previous result into a similar flat_map expression that contains a variable d3? The pattern can certainly be leveraged to create a more ergonomic way to generate the substitutions for any number of steps. That's left to you as an exercise! Wrapping up and looking ahead You now have in your toolbox a working Elixir implementation of unification. My hope is that you also understand how unification works, perhaps well enough to read about other implementations or to write your own version in a different language. If your understanding is still a little hazy, have some fun experimenting with the code. Create your own examples. Tweak the algorithm and see what happens. Add some helper functions to streamline tedious steps. Even on its own, unification, like Elixir's pattern matching, is a powerful tool. But things get so much more interesting when new operations are layered on top of unification. The examples above hint at how conjunction and disjunction might be implemented more generally. With only equality (via unification), conjunction, and disjunction, it's possible to create a very capable logic programming language. To read more about this, take a look at some of the sources that inspired this post: microKanren: A Minimal Functional Core for Relational Programming Relational Programming in miniKanren: Techniques, Applications, and Implementations The Reasoned Schemer All of these references use a Lisp dialect as the host language. For an Elixir-flavored miniKanren, see Logos, a library I wrote that is still very much a work-in-progress. In a follow-up post, I'll go into some of the core operations defined in microKanren and how they can be leveraged to build more powerful logical abstractions. Have fun and logic on!",
    "commentLink": "https://news.ycombinator.com/item?id=40840315",
    "commentBody": "Unification in Elixir (ericpfahl.com)155 points by weatherlight 15 hours agohidepastfavorite26 comments contificate 7 hours agoIt is worth noting that lots of applications of unification do not reify explicit substitutions in their implementations. You often see introductory type inference articles (usually focused on Hindley-Milner) use algorithm W, which uses a unification procedure that returns substitutions (which must then be composed with other substitutions). All of this is less efficient and arguably more error-prone and more complicated than the destructive unification implied by algorithm J (using the union-find data structure, often implemented invasively into the type representation to imply a union-find forest). To this end, good coverage of decent (destructive) unification algorithms can be found in any simple resource on type inference (https://okmij.org/ftp/ML/generalization/sound_eager.ml) or the Warren Abstract Machine (https://github.com/a-yiorgos/wambook/blob/master/wambook.pdf). Of course, there are times where you would want to reify substitutions as a data structure to compose and apply, but most of the time you just want to immediately apply them in a pervasive way. Despite what another comment says, unification is a valid - and rather convenient - way to implement pattern matching (as in the style of ML) in an interpreter: much like how you rewrite type variables with types you are unifying them with, you can rewrite the pattern variables to refer to the parts of the (evaluated) value you are matching against (which you then use to extend the environment with when evaluating the right hand side). reply ReleaseCandidat 7 hours agoparent> Despite what another comment says, unification is a valid - and rather convenient - way to implement pattern matching The article shows that it is a valid and rather convenient way. That's why it is important to tell people that it is not necessary to use it for the \"usual\" pattern matching. You do (again: for the \"usual\" pattern matching without \"variables\", as in free variables, on the right side) _not_ want to use unification but \"compile\" the pattern matching. reply contificate 6 hours agorootparentIf we are talking about the context of a compiler, I agree: you should compile pattern matching to decision trees (DAGs), by way of something like Pettersson's algorithm. In my comment, I specified \"in an interpreter\", as I think it's the natural way to implement an SML-like match construct in that context. EDIT: Perhaps I am missing Elixir-specific context and I apologise if this is the case. It was the unification that made me click this post. reply ReleaseCandidat 6 hours agorootparentWell, I'd do that with \"real\" interpreters (using bytecode) too. But on the other hand also _especially_ for \"toy\" tree walking interpreters, to learn how to build the pattern matching tree -- as unification is needed for the type checker anyways ;). Although I would start by reading Wadler's version (that I linked in the other post), it's IMHO a bit more accessible than Peterson's https://www.classes.cs.uchicago.edu/archive/2011/spring/2262... But please don't get me wrong: I'm just saying that there exist other, specialised and (well, generally) more performant algorithms for pattern matching and people should have at least heard about them. If somebody decides to use unification for pattern matching, that still could be changed if performance or better error messages or ... matters. reply contificate 5 hours agorootparentAh, I thought your username was familiar: you recommended Wadler's approach on a previous HN thread concerning my blog post about Pettersson/Maranget's algorithm (https://news.ycombinator.com/item?id=39241776). Yeah, I admit that Pettersson's telling of the algorithm is a bit turgid (probably why Maranget's paper - which is the same technique as its core, at least to my understanding - is cited more often). reply ReleaseCandidat 8 hours agoprevWhile I'm at (free) books: a great example implementation of unification (in Lisp) is in chapter 11 \"Logic Programming\" of Peter Norvig's \"Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp\". https://github.com/norvig/paip-lisp There is also an implementation in Python, but I have never tested it, just googled it now: https://github.com/dhconnelly/paip-python/blob/master/prolog... https://dhconnelly.github.io/paip-python/docs/prolog.html reply ReleaseCandidat 10 hours agoprev [–] Type checking (type inference) is the usual application of unification in programming languages (compilers or interpreters). Pattern matching does not need unification, because, as mentioned in the beginning of post, the \"matching\" is done one the left side only (no variables to \"match\" on the right side). reply YeGoblynQueenne 10 hours agoparent [–] The one-sided pattern matching seems to be a peculiarity of Elixir. The post above discusses an implementation of full unification without that restriction. To clarify, unification is not generally restricted to one-sided matching only. Is unification unnecessary for pattern matching? I think that depends on what you meany by \"pattern matching\". In programming cirlces \"pattern matching\" usually means matching strings. Now, unification is Turing-complete pattern matching, meaning that it goes well beyond e.g. regular expressions that can only match the set of strings recognised by regular automata. For instance, in Prolog, you can unify a variable with an entire Prolog program and call the program at runtime, etc. So if you need \"pattern matching\" only to match strings then you probably don't need the full power of unification. If I understand correctly (not an expert) Hindley-Milner type checking, the type checking that uses unification, relies exactly on that ability of unification to match entire programs. Anyway that kind of type checking is indeed pattern matching by unification. So it's not different than pattern matching on strings with regular expressions. The difference is only in what kinds of strings can be matched. reply gergo_barany 13 minutes agorootparent> Now, unification is Turing-complete pattern matching No. First-order syntactic unification is not Turing complete. The combination of unification and resolution is Turing complete. > For instance, in Prolog, you can unify a variable with an entire Prolog program and call the program at runtime, etc. Lots of languages allow you to do this. Lisp had \"eval\" ten years before Prolog existed. Unification is not necessary for this. Prolog is great, unification is great. Some other things are great too. reply ReleaseCandidat 8 hours agorootparentprev> I think that depends on what you meany by \"pattern matching\" Oh, yes, that's true. \"Pattern matching\" as that what normally is used in \"functional programming languages\". This kind of pattern matching does not need unification. The IMHO best (correct, but understandable and with all needed definitions) explanation of it is in chapters 4 \"STRUCTURED TYPES AND THE SEMANTICS OF PATTERN-MATCHING\" and 5 \"EFFICIENT COMPILATION OF PATTERN-MATCHING\" of Peyton Jones et al., \"The implementation of functional programming languages\". https://simon.peytonjones.org/slpj-book-1987/ > In programming cirlces \"pattern matching\" usually means matching strings. Depends on the \"circle\" ;) https://en.wikipedia.org/wiki/Pattern_matching reply YeGoblynQueenne 6 hours agorootparentThanks, I don't really know much about Functional Programming. My expertise is in logic programming which btw is a different paradigm. I've heard it said that they're similar and it seems to me the reason was the support for pattern matching in functional languages. I agree that what \"pattern matching\" means depends on a programmer's background. It's interesting that the wikipedia article that you link to does not mention \"unification\" at all and only has one reference in Prolog as one of a group of example languages with a \"pattern matching construct\"; which is not how I would describe the use of the unification algorithm in Prolog. To be honest, I am not sure what is meant by \"pattern matching\". It seems to be a loosely defined term, like \"Finite State Machine\" (as opposed to \"Finite State Automaton\"). I expect that the book you linked to has a more formal treating, perhaps, but from what I can tell from a quick look the \"pattern matching\" here is based on types. In Prolog, unification is used to bind values to logic variables, which do not have types. So a variable, X, in Prolog can unify, i.e. \"match with\" absolutely anything. A \"pattern\" then can be formed by constructing terms (the only data structure in Prolog) with \"holes\" like p(X, a, b, Y,[1,2,3|Tail], f(Z)) etc. where capital letters are variables. One use of this ability is to manipulate singly-linked lists to insert in or remove elements from their tail, where normally only the head of such a list can be accessed directly. What I mean is that in Prolog a list is denoted by the special syntax [H|T] where H is the head of a list (and can be any term, including another list) and T is the tail of the list, another list. Normally one appends to a list in Prolog using the append/3 predicate: append([], L, L). append([H|T], L, [H|R]) :- append(T, L, R). This works on ordinary lists, but it is also possible to define a \"difference list\" as a term, e.g. like: [a,b,c,d|T]-T Where T is a variable bound to the end of the tail of the list. Two difference lists like that one can be appended with the following predicate: append_dl(Xs-Ys, Ys-Zs, Xs-Zs). For example: ?- Xs = [a,b,c,d|T1]-T1, Ys = [e,f,g,h|T2]-T2, append_dl(Xs,Ys,Zs). Xs = [a,b,c,d,e,f,g,h|T2]-[e,f,g,h|T2], T1 = [e,f,g,h|T2], Ys = [e,f,g,h|T2]-T2, Zs = [a,b,c,d,e,f,g,h|T2]-T2. The result of appending is in Zs, but you can see the result of the intermediary unification steps in the other variables echoed in the Prolog REPL (the \"listener\"). Of course, because this is Prolog you can also use the same predicate to split a list: ?- Xs = [a,b,c,d|T1]-T1, Zs = [a,b,c,d,e,f,g,h|T2]-T2, append_dl(Xs,Ys,Zs). Xs = [a,b,c,d,e,f,g,h|T2]-[e,f,g,h|T2], T1 = [e,f,g,h|T2], Zs = [a,b,c,d,e,f,g,h|T2]-T2, Ys = [e,f,g,h|T2]-T2. Where Ys is what remains if we split Xs from Zs. Appending or splitting lists like this is preferred to append/3 because it appends the two lists in a single operation so it can be used to economically put a list together in a loop. The same technique is used with Prolog's Definite Clause Grammars where the \"difference\" in a difference list is the start and end of a string accepted by a grammar, acting as a parser. https://en.wikipedia.org/wiki/Definite_clause_grammar Sterling and Shapiro's \"The Craft of Prolog\" has a nice chapter on \"Incomplete Data Structures\" using difference lists and patterns with \"holes\" but I don't think there's a free version online, unfortunately. reply ReleaseCandidat 6 hours agorootparent> To be honest, I am not sure what is meant by \"pattern matching\". I'd say that \"pattern matching\" is unification, where there are no free variables on the right side and the left side is the \"pattern\" to match. As Prolog code, something like ?- [X|Xs] = [a, b, c] where X = a and Xs = [b, c] would be \"pattern matching\" the head and the tail of the list [a, b, c], but ?- [X|[b, c]] = [a|Xs] where X = a and Xs = [b, c] would be \"real\" unification, if [X|[b, c]] would be valid syntax. Oh, and both unification for type checking (inference) and pattern matching are compile time actions (or done before evaluating the generated functions). A (simple) pattern match like match list withhead :: tail -> do_something(head, tail)[] -> do_something_else() is \"compiled\" to something like if list != [] then do_something(head(list), tail(list)) else do_something_else() reply gergo_barany 6 minutes agorootparent> I'd say that \"pattern matching\" is unification, where there are no free variables on the right side and the left side is the \"pattern\" to match. Plus the restriction that the left hand side must be linear, that is, must not contain any variable more than once. So you usually can't write a pattern match like match list with[x, y, x] -> ... but languages with pattern matching usually allow you to add side conditions like this: match list with[x, y, x'] if x = x' -> ... reply YeGoblynQueenne 6 hours agorootparentprevThat sounds right. I guess then \"pattern matching\" as in Elixir et al. is really just one-sided unification. Which makes me wonder- why don't those languages go all the way? At a guess, that's something to do with the difficulty to implement unification efficiently. The first description of Unification, by J. Alan Robinson (who first described Resolution) had exponential time complexity and it took a while to find efficient unification algorithms. Perhaps the designers of functional languages simply don't need the hassle and can make do with something more efficient, less complete. That would make good, practical sense in languages that do not use Resolution as an interpreter- it's Resolution that absolutely needs unification (there's ground Resolution, without the need for unification, but it's restricted to propositional terms without first-order variables). reply rdtsc 3 hours agorootparentPattern matching as is in Erlang, or how it is in most modern languages, was not there at some point and was considered novel and exotic. Now most everyone can do [X, Y_] = [1, [2], 3, 4] and it's not a seen as something advanced. I am not sure if it was ever called \"unification\" in Erlang, it was always called \"pattern matching\". But the equal sign did retain its name as the \"match\" operator: https://www.erlang.org/doc/system/expressions.html#the-match.... And that is a legacy of Prolog. I never read X=1 as \"make X equal to 1\", in my head I always read it as \"match X to 1\". At some point, I also wondered: why didn't they go all the way and just bring in Prolog's full unification. They obviously liked Prolog; Erlang was developed in Prolog initially, even. Well, it turns out with unification we'd have to carry the match \"hole\" around until we might a match. But Erlang has lightweight processes with isolated heaps which communicate via messages, so now we'd need to pass these \"holes\" around from process to process until say one of the processes somewhere manages to unify the value, and then have it propagate across the system to original query. That breaks isolation. So you see how the two neat features came into a conflict and they picked one over the other, and rightly so it seems. reply jpt4 2 hours agorootparentprevYeGoblynQueene, would you be amenable to discussing certain questions of logic programming via e-mail? I am reading about Jean-Yves Girard's \"Stellar Resolution\" model of computation, and would greatly appreciate a knowledgeable counterpart in the investigation. reply troupo 9 hours agorootparentprev [–] > The one-sided pattern matching seems to be a peculiarity of Elixir. And Erlang. And Haskell. And OCaml. And F#. And... As far as I understand, only Prolog has unification (out of more-or-less used/known languages). And in languages that require (rather awkward and verbose) workarounds for this functionality, it's hard to see the immediate value of unification. reply weatherlight 1 hour agorootparentits not quite the same. you have to write quiet a bit of OCaml or Haskell to do the following. [foo, [2, 3, bar], baz, 5] = [1,[2,3,4], 5] If there's a match, the bindings \"foo\", \"bar\", \"baz\" will be bound to the values on the right hand side. if not an exception will be thrown (You are right, though, Erlang and Elixir have the same semantics, both being on the BEAM) reply throwawaymaths 6 hours agorootparentprev>And Erlang. And Haskell. And OCaml. And F#. And... JavaScript (maps), Rust, Ruby, Python... reply weatherlight 1 hour agorootparentIt's not the same. > [a, 2, 2, b] = [1, 2, 2, 3] [1, 2, 2, 3] > a 1 > b 3 > [foo, 2, bar] = [3,3,3] %MatchError{term: [3, 3, 3]} > foo %CompileError{description: \"undefined function foo/0 (there is no such import)\", file: \"nofile\", line: 1} Ruby does something similar but the variables are bound in the scope of the case block. reply gergo_barany 2 minutes agorootparentWhat's not the same as what? Based on your examples it looks like you're saying that unification is not the same as pattern matching. But nobody said it was. __jonas 3 hours agorootparentprevWhat do you mean by this? Do maps in JavaScript have some kind of pattern matching? reply munificent 3 hours agorootparentJavaScript supports destructuring lists and objects (sort of like maps): https://developer.mozilla.org/en-US/docs/Web/JavaScript/Refe... It's not full pattern matching because as far as I know they don't support refutable patterns, but it handles destructuring and binding to variables. reply YeGoblynQueenne 6 hours agorootparentprev [–] >> And Erlang. And Haskell. And OCaml. And F#. And.. Thanks, I didn't know that. I took the article to mean that this kind of one-sided pattern matching is unique in Elixir. My misreading. From what I've seen it's true that most languages that have support for pattern matching have special constructs for it, whereas in Prolog it's baked in to the only data structure in the language, the term, which can be a single variable (a logic variable that can unify with ... anything). I don't know how useful or not unification would be in a language other than Prolog to be honest. In Prolog it's an integral component of the Resolution-based theorem prover used as an interpreter of Prolog programs: resolution proceeds by unifying Horn goals to the heads of definite clauses, to produce new goals, recursively. More over, when one runs a Prolog \"query\" the result of the query itself is the substitution of the variables in the query, i.e. the substitution that makes the query true [1]; a substitution derived and propagated by unification. I like how George W. Lloyd describes the use of unification in Prolog. Apologies in advance for the long-form quote: The idea that first order logic, or at least substantial subsets of it, could be used as a programming language was revolutionary, because, until 1972, logic had only ever been used as a specification or declarative language in computer science. However, what [R. Kowalski, Predicate Logic as a Programming Language, 1974] shows is that logic has a procedural interpretation, which makes it very effective as a programming language. Briefly, a program clause A <- B1, ..., Bn is regarded as a procedure definition. If <- C1, ..., Ck is a goal, then each Cj is regarded as a procedure call. A program is run by giving it an initial goal. If the current goal is <- C1, ..., Ck, a step in the computation involves unifying some Cj with the head A of a program clause A <- B1, ... Bn and thus reducing the current goal to the goal <-(C1, ..., Cj-1, B1, ..., Bn, Cj+1, ..., Ck)θ, where θ is the unifying substitution. Unification thus becomes a uniform mechanism for parameter passing, data selection and data construction. The computation terminates when the empty goal is produced. And that's first-order SLD-Resolution, with unification, in a nutshell! From George Lloyd: Foundations of Logic Programming, second Ed. 1993. There's a copy of the book here: https://archive.org/details/foundationsoflog0000lloy_o7l2 But it's a limited preview :/ ________ [1] ... well, the substitution that makes the Horn goal that we call a \"query\" false, in reality, because Prolog proves things by refutation but we fudge it a bit. reply jerf 5 hours agorootparentWhen you're using pattern matching to also create variables, it's just really confusing to be able to create variables with both sides of the expression. It can get confusing enough as it is. Haskell has some extensions to pattern matching that AIUI are generally considered just too \"powerful\" to generally use, as it makes it very difficult to read, for instance. Plus to a first approximation you can always just swap the binding from the right to the left. A deeper analysis may reveal there's some case you can't do that, there's a lot to pattern matching and I'm not going to try to sit here and construct a case where you can't, but certainly the most common cases (by far) would just require a simple swap, so it's not an onerous requirement. And if you did construct a case where you can't, you could make a very solid software engineering case that you just shouldn't do that and should do it it two parts. To the extent this doesn't apply to prolog, it would be when prolog is functioning directly as a logic language rather than a programming language, and to be honest I'd still advise against a prolog programmer binding variables on both sides of an expression unless they're forced to for some reason just because if nothing else, you may have to understand your own code a week later. Note this specifically applies to binding variables; I'm not bothered by two already-bound variables being matched against each other, that's just prolog functioning as designed. reply jpt4 2 hours agorootparentprev [–] The Curry-Howard Correspondence is more of an observation than a theorem, but insofar as it describes the general concept of relating logics and programming languages, what is the CHC for logic programming languages? If strikes me that LP directly instantiates and manipulates logical expressions, in a \"self-dual\" sense; or, is it that the various \"proof procedures\" of the logic side map to \"proof search\" algorithms on the computing side? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Unification in Elixir extends pattern matching by allowing variables on both sides of an equation, solving symbolic equations and producing substitution mappings.",
      "Unlike pattern matching, unification can handle partially known values, making it a powerful tool for logic programming in Elixir.",
      "The unification algorithm involves walking terms, testing equivalence, handling variables, and recursively unifying list elements, simplifying variable assignments through substitution."
    ],
    "commentSummary": [
      "The article compares the efficiency of unification algorithms in type inference, focusing on algorithm W and algorithm J.",
      "Algorithm W, used in Hindley-Milner type inference, is less efficient and more error-prone due to the need for composing substitutions.",
      "Algorithm J, which employs a union-find data structure for destructive unification, is simpler and more efficient, with additional insights on unification in pattern matching and compiled pattern matching using decision trees."
    ],
    "points": 155,
    "commentCount": 26,
    "retryCount": 0,
    "time": 1719781112
  },
  {
    "id": 40840699,
    "title": "Google Arts and Culture site I didn't know existed",
    "originLink": "https://artsandculture.google.com/",
    "originBody": "Home Explore Nearby Profile Achievements Collections Themes Experiments Artists Mediums Art movements Historical events Historical figures Places About Settings View activity Send feedback Privacy & Terms • Generative AI Terms Home Explore Play Nearby Favorites Sign in Where do you want to go? Discover with the Van Gogh Museum, Château de Chantilly, and more Taiwan Tour in 9 stops Explore Van Gogh's Library What did he read? Explore Into a Flower Garden Learn their language Explore Morocco with Delacroix On tour with the painter Explore Today's top picks 5 Museums That Broke Records From the biggest dinosaur to the longest model railway 7 Couples Who Changed the World These unions made an impact in their own ways Explore the Kiyomizu-dera Temple Transport yourself to East Kyoto in 40 seconds Artists Exploring the Environment Explore with the Bangkok Art and Culture Centre Can You Find 'American Gothic'? Hunt through the Art Institute of Chicago Fun on a Monday K-Pop Dance Challenge Learn to dance like a K-Pop pro in this choreographed dance challenge. A collaboration between the V&A and Google Arts & Culture Lab. Play now BTS x Street Art Un-Dough! Haiku Imagined More culture games Pick a place to experience art A Street Gallery A Modern Art Gallery A Virtual Gallery A Museum At Home Discover over 13k artists Artwork of the day IMAGE The Fair Toxophilites William Powell Frith Discover RAMM Project the painting Follow Us on Instagram Keep up with the latest from arts and culture Recommended for you Discover popular artworks and hidden gems from around the world Rudolf II of Habsburg as Vertumnus Giuseppe Arcimboldo A Face of Fruit Composed Related story The Librarian By same artist Four Seasons in One Head Visually similar work The Victor of Prague Depicts same location The First Count in Sweden Uses same medium A Young Bride Depicts same object Philip III of Spain From same collection Lucky dip! Inside 12 Famous Food Markets Follow your taste buds from Toronto to Taipei Meet Vermeer in 3D All of the artist’s paintings together for the first time ONLINE EXHIBIT 7 Wonders of the Natural World From the Grand Canyon to Mount Everest São Paulo Botanical Gardens Take a virtual walk in the great outdoors Choose your adventure Game time 3D tour Travel guide Art explorer From writing plays to poetry Discover culture through familiar faces You Know Shakespeare... ...But What About Ada Negri Choose your interactive experience Play with art, games, and more Which Artwork Looks Like You? Play with Art Selfie Follow the Blobs to Mexico Play with Blob Opera Turn Yourself Into 'The Scream' Play with Art Filter The red feathered hat This tapestry represents Louis XIV’s visit to the Gobelins' manufacture. The king is easily recognisable by his red feathered hat. Silverware Here we can see people carrying the silverware created for Louis XIV by the goldsmiths of Les Gobelins and the Louvre. Vase This richly decorated vase may be the great dragon vase, delivered by the goldsmith Alexis Loir in 1683. Duke of Orleans Louis XIV is standing next to his brother Philippe, Duke of Orleans. But who is that peeking through? The King’s Visit to the Gobelins, Charles Le Brun, 1673/1679 Click on the dots to zoom into the details Explore Explore Pocket Galleries Step inside a gallery with Augmented Reality The Art of Color Explore artworks from around the world in AR From Africa to Japan Explore contemporary art the Jean Pigozzi Collection Life in Miniature Explore Indian miniatures in Augmented Reality Brushes with the World An art tour of the world in sound J. Paul Getty Museum Discover European paintings at home Meet Vermeer All of the artist’s paintings together for the first time Kandinsky Discover Kandinsky's work in Augmented Reality MUSIC + ART Discover Shadow Art to Music Explore 'Hanuman' from the Unit Pengelola Museum Seni Watch Crossword of the day Test your knowledge Cultural Crosswords Get cultured through word puzzles Play now More games Uncover artifacts Tour museums Browse artworks Explore places Explore with Street View From backstage at the Paris Opera to the top of the Taj Mahal EXPLORE Alberobello Italy EXPLORE Great Pyramid of Giza El Giza, Egypt EXPLORE Sher Shah Suri's Tomb New Delhi, India EXPLORE Angkor Wat Temple Cambodia View All Take a tour through nature Discover the beauty of French National Parks Explore the Northern Vosges Nature Park Sandstone, rich forests, and a UNESCO protected home Explore Mercantour National Park Take a tour of the Alpine peaks and valleys Explore Haut Jura Regional Park See the region's luscious landscapes from home Explore the Landes de Gascogne Discover the wonders of the French Regional National Park Explore Ballons des Vosges Nature Park Discover wooded slopes, vineyards, and a thousand ponds Explore from home The State Tretyakov Gallery History is the national treasury of Russian fine art and one of the greatest museums in the world. It is located in one of the oldest directs of Moscow - Zamoskvorechye, not far from the Kremlin. See the collection Explore spectacular street art collections See colorful streets in Berlin, Brazil, Amsterdam, and more East Side Gallery Germany São Paulo Street Art Brazil Street Art Museum Amsterdam Netherlands Global Street Art Foundation United Kingdom View All Journey through the animal kingdom Meet artisan animals made in various Mexican communities Discover artists around the world Find your new favorite Artemisia Gentileschi 30 items K. H. Ara 19 items Hokusai 956 items Lee Krasner 27 items Clementine Hunter 27 items View All Step inside iconic homes Explore where famous figures lived and worked Anne Frank's Family Home Discover where she lived just before going into hiding Take a Tour Through Tolstoy's Home Zoom Into Van Gogh's bedroom Discover in 360° Experience culture from all angles Watch West Side Story in 360˚ Experience \"Cool\" at Carnegie Hall The Kyoto Costume Institute in 360° How did Japanese design redefine beauty? Explore a Buddhist Cave in 360° Discover the pilgrimage site of Kawgun Cave in Myanmar View All Explore a museum from home The Rijksmuseum in the Netherlands holds world-famous masterpieces from the Dutch Golden Age including the 'Milkmaid' by Vermeer and Rembrandt's 'Night Watch'. See the collection Explore in high definition The details you might have missed More artworks How Does Picasso Connect to Penguins? Discover connections between culture with Machine Learning Start finding connections Keep exploring... Dive into culture from around the world Exploring the Maya World A journey into the past Explore Meet the People of Kenya From the cradle of mankind Explore Wonders of Pakistan Discover the cultural treasures Explore Wayang Shadow Puppets Style, epic stories, and surprises Explore Cuban Arts & Culture Artworks, archives, and stories Explore A Century of Polish Art The inspirations and impact of artists Explore Meet Lee Ungno A new look at the pioneering artist Explore Journey Around Mexico Unique landscapes and ancient heritage Explore",
    "commentLink": "https://news.ycombinator.com/item?id=40840699",
    "commentBody": "Google Arts and Culture site I didn't know existed (artsandculture.google.com)154 points by andrei-akopian 21 hours agohidepastfavorite42 comments filleduchaos 20 hours agoI first stumbled on this site over half a decade ago, and it's nice to see it's still going. There's genuine effort to collaborate with locals to produce curated content - see for example the city showcase of Lagos, Nigeria: https://artsandculture.google.com/project/creative-lagos I unironically think it's Google's greatest contribution to media in the 2010s. reply virgulino 18 hours agoparentI agree. I'm still amazed by the quality and depth of the Brazilian content (I'm Brazilian). The page about Portinari's huge \"War and Peace\" paintings at the UN headquarters is a striking example. https://artsandculture.google.com/project/wonders-of-brazil https://artsandculture.google.com/story/dAVh9lXSqG6hJA reply doctorhandshake 18 hours agoprevThis site is great but am I wrong in thinking the title of this post is editorialized against HN policy? reply automatoney 16 hours agoparentTaken as a whole, the title guidelines feel more focused on titles of articles, rather than whole website titles. I can understand the complaint, but in my opinion this title is fine. The title just feels a little... bare without the note. reply andrei-akopian 17 hours agoparentprevYou could be right, after reading through the guidelines it does seem on the edge. Which exact policy do you think it is violating? I was trying to relay that it is an enormous resource by Google but somehow nobody talks about it. (At least I haven't heard about it, which is odd considering its size.) The title does sound a bit clickbaity, please tell me what title I should have used instead. reply 43920 17 hours agorootparentI think the typical approach is to use the regular site name (so just \"Google Arts and Culture\") and let people click on it assuming there's something interesting about it to justifying being posted. That doesn't work super well when the original title is vague, like this one, but I think it does correspond to this guideline: > Please don't do things to make titles stand out, like using uppercase or exclamation points, or saying how great an article is. It's implicit in submitting something that you think it's important. reply stevage 16 hours agorootparentprev\"I didn't know existed\" is clearly editorial. You could just make it: \"Google Arts and Culture site\" reply mbivert 20 hours agoprevThere's a handy tool to download images from there: https://dezoomify.ophir.dev/ reply jd3 20 hours agoparentI've mostly used Google's arts and culture site to download hi-res images of famous pieces of art for a side project of mine. reply mbivert 19 hours agorootparentYou mean, directly? I remember seeing a few download links for some images, but many of them aren't directly available (random example: [0]) But otherwise yes, it's one of the best online archive. Wikimedia & some museums's websites are good sources of high-quality material as well. [0]: https://artsandculture.google.com/asset/nicolaas-rubens-wear... reply Palomides 19 hours agorootparentmany of the high res images on wikipedia/wikimedia of notable paintings come from this google project (the category for it has ~25,000 https://commons.wikimedia.org/wiki/Category:Files_from_Googl... ) reply Daub 18 hours agorootparentThe super high resolution photos are taken with a special rig. Details: https://www.dpreview.com/news/1947958268/google-art-camera-u... From what I recal, the software used for stitching is more like Excel than Photoshop. reply ghghgfdfgh 15 hours agoparentprevDoes the Arts and Culture site have a harsh ratelimit? I can already think of so many things you can do with all that data if I had it.... reply Daub 18 hours agoparentprevMany thanks! Book marking now, reply wrs 20 hours agoprevIt’s been around since 2011. I think it was one of the earliest examples of “infinite zooming” tiled image websites. (See https://ghostarchive.org/archive/Igdyv) reply Daub 20 hours agoparentI use this feature in my painting class all the time. The ability to zoom in so close to a painting is incredible. In many ways, it offers a better learning experience for a young painter than seeing the paintings in real life. reply drewda 15 hours agoparentprevOr an earlier example was Seadragon, which was acquired by Microsoft and renamed Deep Zoom https://en.m.wikipedia.org/wiki/Seadragon_Software https://en.wikipedia.org/wiki/Deep_Zoom reply gxt 15 hours agoprevBlob opera: https://artsandculture.google.com/experiment/blob-opera/AAHW... Für Elise performed by Blob opera: https://youtu.be/0t4BEpbw5jQ reply chairhairair 20 hours agoprevThe app is genuinely great: https://apps.apple.com/us/app/google-arts-culture/id10509705... reply raincole 20 hours agoprevI've been using this intensively for art references since Pinterest got flooded by AI stuff. reply renegat0x0 12 hours agoprevOh google has many pages: - https://arvr.google.com/ - https://bughunters.google.com/ - https://design.google.com/ - https://opensource.google.com/ - https://code.google.com/ - https://ai.google.com/ - https://mapsplatform.google.com/ - https://programmablesearchengine.google.com/ - https://privacy.google.com/ - https://lens.google.com/ - http://labs.google.com/ - https://jigsaw.google.com/ - https://datastudio.google.com/ - https://tagmanager.google.com/ - https://script.google.com/ There are some more scraped by me https://github.com/rumca-js/Internet-Places-Database reply andrei-akopian 5 hours agoparentI just went to Google products Wikipedia page: https://en.m.wikipedia.org/wiki/List_of_Google_products There is lots of other cool stuff like a Santa Tracker: https://santatracker.google.com/ I initially went through google subdomain lists on github, but many of them just redirected to the main search engine. With how many things Google has killed, its hard to remember how many are still alive. And thanks for sharing Internet Places DB. reply rwbt 20 hours agoprevSeriously asking this - How did it survive so far? reply Daub 20 hours agoparentIn my opinion, it's usability has decreased over the years. Predictably this is a result of Google adding new features like pre-curated collections. reply refulgentis 19 hours agoparentprevCan mostly survive on autopilot, & it's s0o0o0o much easier to shut something down for not having > X0 million users, than it is to shut it down a clear charity case because it doesn't make money. (it's not supposed to!) After 7 years there, the greatest lesson I walked away with is motivated reasoning really matters. reply sulam 18 hours agorootparentVery very little at Google survives on autopilot. There’s a constant drumbeat of required changes, whether they be accessibility, regulatory (DMA, hurrah), internally motivated changes (material design++) etc. For anything to survive there must be someone who cares and is willing to invest in it. This is a large part of why stuff gets killed, the default there is for things to die. Yes there are strategy changes too, but very often things die because the person who could champion a thing is no longer there to do so, or has changed roles significantly enough that they can’t plausibly do so anymore. reply refulgentis 18 hours agorootparent(context: Xoogler, knew people on the team, am partially culpable for the last material++) This is autopilot at BigCo. No? There's still a pilot in the cabin. Low stakes. No big investment required. No politicking. Unaffected by latest $X trend. No release sprints. No release slogs. reply buildbot 18 hours agorootparentprevIt seems heavy than maintaining something like Reader, but perhaps that's incorrect. reply lupire 18 hours agorootparentReader wasn't unmaintainable. It was cancelled because it competed with Google+. reply refulgentis 18 hours agorootparentprevI would wager so, it's not significantly changed much in years. reply shermantanktop 20 hours agoprevSeattle’s “nearby” feature lists usual suspects but also the Rubber Chicken museum at Archie McPhee’s, a defunct “Bad Art Museum,” a selfie museum, an NFT museum, a coffee museum… I’m no fine art snob but this looks like it’s powered by a keyword search result. reply buildbot 18 hours agoparentYeah but that's okay! They are still museums! Plus, I never would have discovered this extremely specific museum - https://www.nwkidney.org/about-us/dialysis-museum/ reply tmsh 13 hours agoprevI really like https://artsandculture.google.com/story/QQUBlr8x6YW7Jw and https://artsandculture.google.com/nearby. Thanks for posting! reply mig39 20 hours agoprevThe mobile app used to have a feature where you could upload a selfie and it would search for a painting that looked like you. It did a really good job. Not sure if you can do that on the website. reply skissane 19 hours agoparent> The mobile app used to have a feature where you could upload a selfie > Not sure if you can do that on the website. You absolutely can: https://developer.mozilla.org/en-US/docs/Web/API/Media_Captu... All major current browsers support it: https://caniuse.com/stream reply skissane 14 hours agorootparentDearest downvoters, I can understand why you did it - the person I was replying to was talking about whether Google’s web app actually has that feature, whereas I misinterpreted them as talking about whether it technically could have that feature. Mea culpa. Still, I think my reply does demonstrate - there is nothing technically stopping Google from adding this feature to the web version, if they felt sufficiently motivated to do so reply KTibow 19 hours agoparentprevThe app still does. reply buildbot 18 hours agoprevI weirdly remember this being announced when I was an undergrad in 2011, and thinking it would get killed super fast like Wave, which was given to Apache in 2010 - amazing it still exists! reply eek2121 17 hours agoprevMy only advice beyond what others have said would be to try and archive it. Google has show themselves to be anti-consumer unless they benefit, so I expect they will kill this as well. Also, in case someone wants to give me crap, If someone gave me traffic numbers and such so that I know what to expect, I would absolutely be willing to mirror this provided I can find it, and unlike Google, for me, fund != profit. If you work at Google and are confused, please just do a search on Duck Duck Go for the Google graveyard and find a sane job after you realize your current job will end soon. reply TheAceOfHearts 20 hours agoprevThanks for the reminder, I'd forgotten about this website. My favorite post on there is \"Exploring the Ripley Scroll: A recipe for making the Philosopher's Stone\" [0]. Now I'm feeling motivated to make a painting inspired by the Ripley Scroll. There's something so whimsical and delightful about it. I really wonder what the creator was thinking when they were making it. I love all these arcane magic systems. [0] https://artsandculture.google.com/story/OwXBNEuJqIJzLg reply chris_wot 19 hours agoprevSo at what point will Google remove this? reply adolph 15 hours agoprev [–] Yeah, Reader was too much to support. Good think they kept this instead. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Google Arts & Culture offers a wide range of virtual tours and interactive experiences, allowing users to explore art, history, and culture from around the world.",
      "Highlights include virtual tours of famous museums, augmented reality (AR) experiences, and interactive games that make learning about art and culture engaging and accessible.",
      "Special features include the ability to explore Van Gogh's library, take a 3D tour of Vermeer's paintings, and participate in a K-Pop dance challenge in collaboration with the V&A Museum."
    ],
    "commentSummary": [
      "Google Arts and Culture is a lesser-known but significant project by Google, offering curated cultural content from around the world.",
      "Users appreciate its high-resolution images and unique features, such as the ability to zoom in closely on artworks and explore various cultural projects.",
      "The platform has been active since 2011 and continues to be a valuable resource for art and culture enthusiasts, despite concerns about the longevity of Google projects."
    ],
    "points": 154,
    "commentCount": 42,
    "retryCount": 0,
    "time": 1719784504
  },
  {
    "id": 40839830,
    "title": "Newswire: A large-scale structured database of a century of historical news",
    "originLink": "https://arxiv.org/abs/2406.09490",
    "originBody": "Computer Science > Computation and Language arXiv:2406.09490 (cs) [Submitted on 13 Jun 2024] Title:Newswire: A Large-Scale Structured Database of a Century of Historical News Authors:Emily Silcock, Abhishek Arora, Luca D'Amico-Wong, Melissa Dell View PDF HTML (experimental) Abstract:In the U.S. historically, local newspapers drew their content largely from newswires like the Associated Press. Historians argue that newswires played a pivotal role in creating a national identity and shared understanding of the world, but there is no comprehensive archive of the content sent over newswires. We reconstruct such an archive by applying a customized deep learning pipeline to hundreds of terabytes of raw image scans from thousands of local newspapers. The resulting dataset contains 2.7 million unique public domain U.S. newswire articles, written between 1878 and 1977. Locations in these articles are georeferenced, topics are tagged using customized neural topic classification, named entities are recognized, and individuals are disambiguated to Wikipedia using a novel entity disambiguation model. To construct the Newswire dataset, we first recognize newspaper layouts and transcribe around 138 millions structured article texts from raw image scans. We then use a customized neural bi-encoder model to de-duplicate reproduced articles, in the presence of considerable abridgement and noise, quantifying how widely each article was reproduced. A text classifier is used to ensure that we only include newswire articles, which historically are in the public domain. The structured data that accompany the texts provide rich information about the who (disambiguated individuals), what (topics), and where (georeferencing) of the news that millions of Americans read over the course of a century. We also include Library of Congress metadata information about the newspapers that ran the articles on their front pages. The Newswire dataset is useful both for large language modeling - expanding training data beyond what is available from modern web texts - and for studying a diversity of questions in computational linguistics, social science, and the digital humanities. Comments: arXiv admin note: text overlap with arXiv:2306.17810, arXiv:2308.12477 Subjects: Computation and Language (cs.CL); General Economics (econ.GN) Cite as: arXiv:2406.09490 [cs.CL](or arXiv:2406.09490v1 [cs.CL] for this version)https://doi.org/10.48550/arXiv.2406.09490 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Melissa Dell [view email] [v1] Thu, 13 Jun 2024 16:20:05 UTC (3,050 KB) Full-text links: Access Paper: View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CLnewrecent2024-06 Change to browse by: cs econ econ.GN q-fin q-fin.EC References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=40839830",
    "commentBody": "Newswire: A large-scale structured database of a century of historical news (arxiv.org)150 points by h2odragon 19 hours agohidepastfavorite32 comments rileyphone 17 hours agoAccording to the paper, the dataset goes up to 1978 because that's when copyright law was updated to automatically apply to newswires. It's unfortunate that we got into the situation where academia has to play by the rules wrt copyright while big private labs flaunt it. reply saulpw 15 hours agoparentThey don't have to play by the rules either. They can also be sued by the NYTimes. reply tivert 2 hours agorootparent> They don't have to play by the rules either. They can also be sued by the NYTimes. I understand a lot of other outlets have been folding and making deals with OpenAI, because they're too weak to sue and desperate for the revenue. Which if true is really sad. It's like taking out a payday loan: solve your short-term problem by giving yourself a bigger one in the future. reply m-p-3 6 hours agorootparentprevJustice at the service of whoever has the bigger wallet. :/ reply fritzo 14 hours agoparentprev*flaut reply devindotcom 14 hours agorootparent*flout reply gcanyon 6 hours agorootparent*flute :-) reply runningmike 12 hours agoprevIn the text is stated’ All code used to create Newswire is available at our public Github repository. ’ But a check learned “(This repository is empty.” So not yet open science.. reply huijzer 12 hours agoparentYet another result of incentives in academia being misaligned. reply tunesmith 13 hours agoprevI've wondered for a while if a new kind of news could be fashioned from the events of the world. Basically, if I were to try and define what items are most newsworthy, it'd be along the lines of what (according to my pop-culture understanding) Claude Shannon described, of the items that are most \"surprising\". But while we normally look at surprise as a subjective thing that impacts only one person, like an item that most conflicts with my current understanding and requires me to update my own model the most, we'd instead apply that to very large groups of people, or whatever group the \"news\" is being tuned to. So, what items of news cause the most \"change\" (or surprise) to that group of people. All our understandings and notions of truth are based off of large chains of reasoning that are based off of premises and values. When a new event happens that changes a premise our understandings are based on, which events cause the largest changes in those dags? We're regularly subjected to \"news\" that doesn't change anything very much, while subtle events of deep impact are regularly missed. Maybe it would be a way to surface those things. I wonder if someone smarter than me could analyze a data set such as this and come up with a revised set of headlines and articles over the years that do the best job of communicating the most important changes. reply mistermann 3 hours agoparentIt's an interesting idea...most news outlets are pretty similar, but with variance in a few (of the same) dimensions. A radical change could be to vary along new and more dimensions, and in new ways, but also to introduce variance (or novelty) to the shared axioms it all sits on top of (~\"the reality\"). If it was all powered by AI I think you could get some really interesting results. I bet it would invoke extremely strong emotions in Humans, they tend to not like their \"reality\" being messed with (well...in ways other than they have become accustomed to - these ways they seem extremely fond of, and defend them passionately). Another angle no one's run with in any serious way would be a sort of meta-journalism, again with the techniques described above. I think a well done implementation could steal/borrow 50% of Trump's base, and 30% of the Democrats. Actually, with different variations of parameters I'd think you could get a wide range of outcomes, it is often hard to know in advance what will strike a chord with people, some of the weirdest things work like a charm. reply donohoe 16 hours agoprevJust randomly searched for a term and the article data appears full of typos and OCR mistakes in the sample I used. Makes me wonder if this is a bigger problem. reply chazeon 3 hours agoparentYes, the OCR problem is also very interesting to me. I have been looking into the OCR/labeling field recently, and it seems to still be an actively researched field. There is a surya-ocr that was recently posted to YC, transformer-based, but it is still expensive if running on a really large dataset like 100 years of newspaper. Tesseract doesn't seem to handle this kind of thing very well. In the paper, they just mention it was an active-learning type of method. reply jakeogh 16 hours agoparentprevExciting work. I hope they consider releasing the 138 million scans. The FAQ at the end is a bit confusing, because it says: Was the “raw” data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the “raw” data. All data is in the dataset. reply defrost 15 hours agorootparentThe raw scans are in the Linrary of Congress digital collection. (^- oops, example transcription error) The \"end of described process\" dataset descrption is at https://huggingface.co/datasets/dell-research-harvard/newswi... and for each record there's \"newspaper_metadata\", \"year\", \"date\", and \"article\" fields that link back to _a_ LoC newspaper scan. I stress _a_ singular as much is made of their process to identify articles with multiple reprints and multiple scans across multiple newspapers as these repititions of content (to a degree mitigated by local sub editors) with varying layouts are used to robust the conversion of the scans. I haven't investigated whether every duplicate article has a seperate record to a distinct scan source .. reply h2odragon 19 hours agoprevhttps://huggingface.co/datasets/dell-research-harvard/newswi... reply dbglog 18 hours agoprevThis seems like a serious scholarly work and a contribution no matter how you cut it. Thanks to the team that puts this out. reply zX41ZdbW 16 hours agoprevFirst look at the data: https://pastila.nl/?05ee30a0/be7f1715c7de106b95cccd9385a6c2e... TLDR: it makes sense :) reply wumms 8 hours agoparentSeems to correlate nicely [0]: > Prime Minister of the United Kingdom, from 1940 to 1945 during the Second World War, and 1951 to 1955 > Died 24 January 1965 [0] https://en.wikipedia.org/wiki/Winston_Churchill reply zX41ZdbW 16 hours agoparentprevAlso uploaded it to the public playground for queries: https://play.clickhouse.com/play?user=play#U0VMRUNUIHllYXIsI... reply zX41ZdbW 16 hours agorootparentBut the scan quality is subpar. Example: > For belter safekecping Russta’s $2¢4,000,000 collection of crown jewels, probably (he finesl array of gems ever assem- bled at one tle https://play.clickhouse.com/play?user=play#U0VMRUNUICogRlJPT... reply whistle650 15 hours agorootparenthttps://chatgpt.com/share/13f553a8-5cff-42a1-be95-4a9d33cd10... May also be easy to correct a lot of it: “For better safekeeping, Russia’s $24,000,000 collection of crown jewels, probably the finest array of gems ever assembled at one time,” reply notachatbot1234 5 hours agorootparent\"$2¢4,000,000\" should be \"$204,000,000\" rather than ChatGPT's \"$24,000,000\". reply djhn 13 hours agorootparentprevAre you aware of any models that perform as well as an LLM on this task at lower cost? reply bbarnett 11 hours agorootparentprevBut are you correcting the OICR or miscorrecting the originals? I want original text, including misspellings, and original regional / historical spellings, including slang (which may look like another word, but is not, and isn't in a dictionary). You cannot fix OCR text wirhout lioking at the original. reply brabel 8 hours agorootparentWith the spelling having been fixed, even if imperfectly, you could much more easily search for content and find relevant results, and then go on to look at the originals. What you want is still possible, unless you unreasonably make it a requirement that the transcriptions should be perfect. reply bbarnett 7 hours agorootparentProper transcription to digital is to do so with accuracy, not \"close enough\". reply DemocracyFTW2 1 hour agorootparentto quote myself, \"every interesting data set will have inaccuracies in it\" reply dr_kiszonka 12 hours agoparentprevIn case it wasn't intentional, you may be doxxing yourself. reply cs702 17 hours agoprevLooks like fantastic work. It's so nice and refreshing to see something like this, instead of the common \"we tweaked this and that thing and got better results on this and that benchmark.\" Thank you and congratulations to the authors! reply 1propionyl 17 hours agoprevReally good stuff. On the ground, in the weeds data collection and collation like this is sadly often thankless (and poorly funded) work, but it's a huge force multiplier to downstream research and should always be commended. reply KaiserPro 11 hours agoprev [–] Wait, so AP didn't/doesn't have an archive? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Researchers have created a comprehensive archive of U.S. newswire content from 1878 to 1977 using a deep learning pipeline on image scans from local newspapers.",
      "The dataset includes 2.7 million unique public domain articles, georeferenced, tagged by topic, and linked to Wikipedia, providing valuable information for computational linguistics, social science, and digital humanities research.",
      "The project involved transcribing 138 million structured article texts and using a neural bi-encoder model to de-duplicate articles, ensuring only public domain content was included."
    ],
    "commentSummary": [
      "A comprehensive database of historical news up to 1978 has been created, reflecting changes in copyright laws, and is available on GitHub, though currently empty.",
      "Users have identified OCR (Optical Character Recognition) errors in the data, underscoring persistent challenges in digitizing historical texts.",
      "The project, despite its issues, is lauded for its scholarly value, with raw scans accessible through the Library of Congress digital collection."
    ],
    "points": 150,
    "commentCount": 32,
    "retryCount": 0,
    "time": 1719777526
  }
]
