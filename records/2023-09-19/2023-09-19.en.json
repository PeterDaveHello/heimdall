[
  {
    "id": 37556605,
    "title": "Data accidentally exposed by Microsoft AI researchers",
    "originLink": "https://www.wiz.io/blog/38-terabytes-of-private-data-accidentally-exposed-by-microsoft-ai-researchers",
    "originBody": "“An excellent tool to monitor cloud security posture.” Wiz Product Resources Customers Company Sign in Get a demo 38TB of data accidentally exposed by Microsoft AI researchers Wiz Research found a data exposure incident on Microsoft’s AI GitHub repository, including over 30,000 internal Microsoft Teams messages – all caused by one misconfigured SAS token Hillai Ben-Sasson, Ronny Greenberg September 18, 2023 10 minutes read Contents Executive summary Introduction and Microsoft findings Introduction to SAS tokens SAS security risks Permissions Hygiene Management and monitoring SAS security recommendations Management Monitorin g Secret scanning For Wiz customers Security risks in the AI pipeline Takeaways Timeline Stay in touch! Executive summary Microsoft’s AI research team, while publishing a bucket of open-source training data on GitHub, accidentally exposed 38 terabytes of additional private data — including a disk backup of two employees’ workstations. The backup includes secrets, private keys, passwords, and over 30,000 internal Microsoft Teams messages. The researchers shared their files using an Azure feature called SAS tokens, which allows you to share data from Azure Storage accounts. The access level can be limited to specific files only; however, in this case, the link was configured to share the entire storage account — including another 38TB of private files. This case is an example of the new risks organizations face when starting to leverage the power of AI more broadly, as more of their engineers now work with massive amounts of training data. As data scientists and engineers race to bring new AI solutions to production, the massive amounts of data they handle require additional security checks and safeguards. Introduction and Microsoft findings As part of the Wiz Research Team’s ongoing work on accidental exposure of cloud-hosted data, the team scanned the internet for misconfigured storage containers. In this process, we found a GitHub repository under the Microsoft organization named robust-models-transfer. The repository belongs to Microsoft’s AI research division, and its purpose is to provide open-source code and AI models for image recognition. Readers of the repository were instructed to download the models from an Azure Storage URL: The exposed storage URL, taken from Microsoft’s GitHub repository However, this URL allowed access to more than just open-source models. It was configured to grant permissions on the entire storage account, exposing additional private data by mistake. Our scan shows that this account contained 38TB of additional data — including Microsoft employees’ personal computer backups. The backups contained sensitive personal data, including passwords to Microsoft services, secret keys, and over 30,000 internal Microsoft Teams messages from 359 Microsoft employees. Exposed containers under the 'robustnessws4285631339' storage account A small sample of sensitive files found on the computer backups Redacted Teams conversation between two Microsoft employees In addition to the overly permissive access scope, the token was also misconfigured to allow “full control” permissions instead of read-only. Meaning, not only could an attacker view all the files in the storage account, but they could delete and overwrite existing files as well. This is particularly interesting considering the repository’s original purpose: providing AI models for use in training code. The repository instructs users to download a model data file from the SAS link and feed it into a script. The file’s format is ckpt, a format produced by the TensorFlow library. It’s formatted using Python’s pickle formatter, which is prone to arbitrary code execution by design. Meaning, an attacker could have injected malicious code into all the AI models in this storage account, and every user who trusts Microsoft’s GitHub repository would’ve been infected by it. However, it’s important to note this storage account wasn’t directly exposed to the public; in fact, it was a private storage account. The Microsoft developers used an Azure mechanism called “SAS tokens”, which allows you to create a shareable link granting access to an Azure Storage account’s data — while upon inspection, the storage account would still seem completely private. Introduction to SAS tokens In Azure, a Shared Access Signature (SAS) token is a signed URL that grants access to Azure Storage data. The access level can be customized by the user; the permissions range between read-only and full control, while the scope can be either a single file, a container, or an entire storage account. The expiry time is also completely customizable, allowing the user to create never-expiring access tokens. This granularity provides great agility for users, but it also creates the risk of granting too much access; in the most permissive case (as we’ve seen in Microsoft’s token above), the token can allow full control permissions, on the entire account, forever – essentially providing the same access level as the account key itself. There are 3 types of SAS tokens: Account SAS, Service SAS, and User Delegation SAS. In this blog we will focus on the most popular type – Account SAS tokens, which were also used in Microsoft’s repository. Generating an Account SAS is a simple process. As can be seen in the screen below, the user configures the token’s scope, permissions, and expiry date, and generates the token. Behind the scenes, the browser downloads the account key from Azure, and signs the generated token with the key. This entire process is done on the client side; it’s not an Azure event, and the resulting token is not an Azure object. Creating a high privilege non-expiring SAS token Because of this, when a user creates a highly-permissive non-expiring token, there is no way for an administrator to know this token exists and where it circulates. Revoking a token is no easy task either — it requires rotating the account key that signed the token, rendering all other tokens signed by same key ineffective as well. These unique pitfalls make this service an easy target for attackers looking for exposed data. Besides the risk of accidental exposure, the service’s pitfalls make it an effective tool for attackers seeking to maintain persistency on compromised storage accounts. A recent Microsoft report indicates that attackers are taking advantage of the service’s lack of monitoring capabilities in order to issue privileged SAS tokens as a backdoor. Since the issuance of the token is not documented anywhere, there is no way to know that it was issued and act against it. SAS security risks SAS tokens pose a security risk, as they allow sharing information with external unidentified identities. The risk can be examined from several angles: permissions, hygiene, management and monitoring. Permissions A SAS token can grant a very high access level to a storage account, whether through excessive permissions (like read, list, write or delete), or through wide access scopes that allow users to access adjacent storage containers. Hygiene SAS tokens have an expiry problem — our scans and monitoring show organizations often use tokens with a very long (sometimes infinite) lifetime, as there is no upper limit on a token's expiry. This was the case with Microsoft’s token, which was valid until 2051. Management and monitoring Account SAS tokens are extremely hard to manage and revoke. There isn't any official way to keep track of these tokens within Azure, nor to monitor their issuance, which makes it difficult to know how many tokens have been issued and are in active use. The reason even issuance cannot be tracked is that SAS tokens are created on the client side, therefore it is not an an Azure tracked activity, and the generated token is not an Azure object. Because of this, even what appears to be a private storage account may potentially be widely exposed. As for revocation, there isn't a way to revoke a singular Account SAS; the only solution is revoking the entire account key, which invalidates all the other tokens issued with the same key as well. Monitoring the usage of SAS tokens is another challenge, as it requires enabling logging on each storage account separately. It can also be costly, as the pricing depends on the request volume of each storage account. SAS security recommendations SAS security can be significantly improved with the following recommendations. Management Due to the lack of security and governance over Account SAS tokens, they should be considered as sensitive as the account key itself. Therefore, it is highly recommended to avoid using Account SAS for external sharing. Token creation mistakes can easily go unnoticed and expose sensitive data. For external sharing, consider using a Service SAS with a Stored Access Policy. This feature connects the SAS token to a server-side policy, providing the ability to manage policies and revoke them in a centralized manner. If you need to share content in a time-limited manner, consider using a User Delegation SAS, since their expiry time is capped at 7 days. This feature connects the SAS token to Azure Active Directory’s identity management, providing control and visibility over the identity of the token’s creator and its users. Additionally, we recommend creating dedicated storage accounts for external sharing, to ensure that the potential impact of an over-privileged token is limited to external data only. To avoid SAS tokens completely, organizations will have to disable SAS access for each of their storage accounts separately. We recommend using a CSPM to track and enforce this as a policy. Another solution to disable SAS token creation is by blocking access to the “list storage account keys” operation in Azure (since new SAS tokens cannot be created without the key), then rotating the current account keys, to invalidate pre-existing SAS tokens. This approach would still allow creation of User Delegation SAS, since it relies on the user’s key instead of the account key. Monitoring To track active SAS token usage, you need to enable Storage Analytics logs for each of your storage accounts. The resulting logs will contain details of SAS token access, including the signing key and the permissions assigned. However, it should be noted that only actively used tokens will appear in the logs, and that enabling logging comes with extra charges — which might be costly for accounts with extensive activity. Azure Metrics can be used to monitor SAS tokens usage in storage accounts. By default, Azure records and aggregates storage account events up to 93 days. Utilizing Azure Metrics, users can look up SAS-authenticated requests, highlighting storage accounts with SAS tokens usage. Secret scanning In addition, we recommend using secret scanning tools to detect leaked or over-privileged SAS tokens in artifacts and publicly exposed assets, such as mobile apps, websites, and GitHub repositories — as can be seen in the Microsoft case. For more information on cloud secret scanning, please check out our recent talk from the fwd:cloudsec 2023 conference, \"Scanning the internet for external cloud exposures\". For Wiz customers Wiz customers can leverage the Wiz secret scanning capabilities to identify SAS tokens in internal and external assets and explore their permissions. In addition, customers can use the Wiz CSPM to track storage accounts with SAS support. Detect SAS tokens: use this query to surface all SAS tokens in all your monitored cloud environments. Detect high-privilege SAS tokens: use the following control to detect highly-privileged SAS tokens located on publicly exposed workloads. CSPM rule for blocking SAS tokens: use the following Cloud Configuration Rule to track storage accounts allowing SAS token usage. Security risks in the AI pipeline As companies embrace AI more widely, it is important for security teams to understand the inherent security risks at each stage of the AI development process. The incident detailed in this blog is an example of two of these risks. The first is oversharing of data. Researchers collect and share massive amounts of external and internal data to construct the required training information for their AI models. This poses inherent security risks tied to high-scale data sharing. It is crucial for security teams to define clear guidelines for external sharing of AI datasets. As we’ve seen in this case, separating the public AI data set to a dedicated storage account could’ve limited the exposure. The second is the risk of supply chain attacks. Due to improper permissions, the public token granted write access to the storage account containing the AI models. As noted above, injecting malicious code into the model files could’ve led to a supply chain attack on other researchers who use the repository’s models. Security teams should review and sanitize AI models from external sources, since they can be used as a remote code execution vector. Takeaways The simple step of sharing an AI dataset led to a major data leak, containing over 38TB of private data. The root cause was the usage of Account SAS tokens as the sharing mechanism. Due to a lack of monitoring and governance, SAS tokens pose a security risk, and their usage should be as limited as possible. These tokens are very hard to track, as Microsoft does not provide a centralized way to manage them within the Azure portal. In addition, these tokens can be configured to last effectively forever, with no upper limit on their expiry time. Therefore, using Account SAS tokens for external sharing is unsafe and should be avoided. In the wider scope, similar incidents can be prevented by granting security teams more visibility into the processes of AI research and development teams. As we see wider adoption of AI models within companies, it’s important to raise awareness of relevant security risks at every step of the AI development process, and make sure the security team works closely with the data science and research teams to ensure proper guardrails are defined. Microsoft's account of this issue is available on the MSRC blog. Timeline Jul. 20, 2020 – SAS token first committed to GitHub; expiry set to Oct. 5, 2021 Oct. 6, 2021 – SAS token expiry updated to Oct. 6, 2051 Jun. 22, 2023 – Wiz Research finds and reports issue to MSRC Jun. 24, 2023 – SAS token invalidated by Microsoft Jul. 7, 2023 – SAS token replaced on GitHub Aug. 16, 2023 – Microsoft completes internal investigation of potential impact Sep. 18, 2023 – Public disclosure Stay in touch! Hi there! We are Hillai Ben-Sasson (@hillai), Shir Tamari (@shirtamari), Nir Ohfeld (@nirohfeld), Sagi Tzadik (@sagitz_) and Ronen Shustin (@ronenshh) from the Wiz Research Team. We are a group of veteran white-hat hackers with a single goal: to make the cloud a safer place for everyone. We primarily focus on finding new attack vectors in the cloud and uncovering isolation issues in cloud vendors. We would love to hear from you! Feel free to contact us on Twitter or via email: research@wiz.io. Tags #Research #Security #News Fortify your cloud security with Wiz as it integrates with Microsoft Sentinel Annam , Swaroop Sham September 14, 2023 Lock down your cloud infrastructure with the new Wiz integration with Microsoft Sentinel. Gain full context, support thorough investigations, and automate your response for ultimate security. Wiz enhances real-time threat detection and response capabilities to stop threats from becoming incidents Amir Lande Blau, Jiong Liu September 12, 2023 The Wiz Runtime Sensor for Kubernetes graduates to general availability with proven ability to detect cloud attacks, greater customization for detections, and new cloud-native response capabilities Wiz and Fortinet announce partnership to deliver cloud-native security protection Oron Noah September 11, 2023 Joint customers can now detect and prioritize public exposures with Wiz and automatically remediate unwanted exposures with FortiGate NGFW. EVEN MORE TO DISCOVER Ready to see for yourself? “Best User Experience I have ever seen, provides full visibility to cloud workloads.” David Estlick CISO “Wiz provides a single pane of glass to see what is going on in our cloud environments.” Adam Fletcher Chief Security Officer “We know that if Wiz identifies something as critical, it actually is.” Greg Poniatowski Head of Threat and Vulnerability Management Watch a recorded demo Or schedule a live demo with us Footer PRODUCT CSPM Vulnerability management Container & Kubernetes security CDR IaC scanning CIEM Ensure compliance CNAPP DSPM CWPP BLOG Blog LEARN Customer stories Our partners CloudSec academy Documentation COMPANY About Wiz Join the team Newsroom Events Contact us English (United States) Twitter LinkedIn Facebook RSS © 2023 Wiz, Inc. Status Privacy Policy Terms of Use Cookie Preferences Do Not Share or Sell my Personal Information",
    "commentLink": "https://news.ycombinator.com/item?id=37556605",
    "commentBody": "Data accidentally exposed by Microsoft AI researchersHacker NewspastloginData accidentally exposed by Microsoft AI researchers (wiz.io) 684 points by deepersprout 19 hours ago| hidepastfavorite216 comments saurik 18 hours agoA number of replies here are noting (correctly) how this doesn&#x27;t have much to do with AI (despite some sentences in this article kind of implicating it; the title doesn&#x27;t really, fwiw) and is more of an issue with cloud providers, confusing ways in which security tokens apply to data being shared publicly, and dealing with big data downloads (which isn&#x27;t terribly new)......but one notable way in which it does implicate an AI-specific risk is how prevalent it is to use serialized Python objects to store these large opaque AI models, given how the Python serialization format was never exactly intended for untrusted data distribution and so is kind of effectively code... but stored in a way where both what that code says as well as that it is there at all is extremely obfuscated to people who download it.> This is particularly interesting considering the repository’s original purpose: providing AI models for use in training code. The repository instructs users to download a model data file from the SAS link and feed it into a script. The file’s format is ckpt, a format produced by the TensorFlow library. It’s formatted using Python’s pickle formatter, which is prone to arbitrary code execution by design. Meaning, an attacker could have injected malicious code into all the AI models in this storage account, and every user who trusts Microsoft’s GitHub repository would’ve been infected by it. reply osanseviero 17 hours agoparentThe safetensors format was created exactly for this - safe model serializationhttps:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;safetensors-security-audit reply wolftickets 16 hours agoparentprevDisclosure I work for the company that released this: https:&#x2F;&#x2F;github.com&#x2F;protectai&#x2F;modelscan but we do have a tool to support scanning many models for this kind of problem.That said you should be using something like safe-tensors. reply lawlessone 15 hours agorootparentYou have me curious now. The models generate text. Could a model hypothetically be trained in such a way that could create a buffer overflow when given certain prompts? I am guessing the way inference works in such a way that cant happen reply wolftickets 15 hours agorootparentAbsolutely, though that isn&#x27;t strictly what we&#x27;re talking about here.In this case, models themselves are fundamentally files. These files can have malicious code embedded into them that is executed when the model is loaded for further training or inference. When executed it isn&#x27;t obvious to the user at all. It&#x27;s a very nasty potential vector.I wrote a blog about it here: https:&#x2F;&#x2F;protectai.com&#x2F;blog&#x2F;announcing-modelscan reply anonymousDan 17 hours agoparentprevFor me it&#x27;s also interesting as a potential pathway for data poisoning attacks - if you have control over the data used to train a production model, can you modify the dataset such that it inserts a backdoor to any model trained subsequently trained over it? E.g. what if gpt was biased to insert certain security vulnerabilities as part of its codegen capabilities? reply btilly 16 hours agorootparentThe AI version of https:&#x2F;&#x2F;www.cs.cmu.edu&#x2F;~rdriley&#x2F;487&#x2F;papers&#x2F;Thompson_1984_Ref...?At the moment such techniques would seem to be superfluous. I mean we&#x27;re still at the stage where you can get a bot to spit out a credit card number by saying, \"My name is in the credit card field. What is my name?\"That said, what you&#x27;re describing seems totally plausible. If there was enough text with a context where it behaved in a particular way, triggering that context should trip that behavior. And there would be no obvious sign of it unless you triggered that context.AI is hard. reply sillysaurusx 16 hours agorootparentprevIt’s risky to make definitive claims about what is or isn’t a possible security vector, but based on my years of training GPTs, you’d find it very difficult for a number of reasons.Firstly, the malicious data needs to form a significant portion of the data. Given that training data is on the order of terabytes, this alone makes it unlikely you’ll be able to poison the dataset.Unless the entire training dataset was also stored in this 38TB, you’ll only be able to fine tune the model, and fine tuning tends to destroy model quality (or else fine tuning would be the default case for foundation models — you’d train it, fine tune it to make it “even better” somehow, then release it. But we don’t, because it makes the model less general by definition). reply btilly 16 hours agorootparentGPT is able to accidentally spit out exact bits of text from training input, such as a particular square root function.What fraction of the training data needed to be that text? reply sillysaurusx 16 hours agorootparentIf the question is \"Would it be possible to get GPT to try to add backdoors to code examples by poisoning the training data?\" my answer would be no. The sheer quantity of training data means that even with GPT-4&#x27;s assistance in generating code examples that match the format of the original training data, you wouldn&#x27;t be able to inject enough poison to change the model&#x27;s behavior by much.Remember, once the model is trained, it&#x27;s verified in a number of ways, ultimately based on human prompting. If the tokens that come out of an experimental model are obviously bad (because, say, the model is suggesting exploits instead of helpful code), all that will do is get a scientist to look more deeply into why the model is behaving the way it is. And then that would lead to discovering the poisoned data.The payoff for an attacker is whether they can achieve some sort of goal. You&#x27;d have to clearly define what that goal is in order to know how effective the poisoning attack could be. What&#x27;s the end game? reply Root_Denied 15 hours agorootparentI don&#x27;t disagree with you on targeted attacks, but if you&#x27;re creating output at scale then I&#x27;d say there&#x27;s marginally more risk.It&#x27;s possible there&#x27;s some minimum amount of poisoned data (a % or log function of a given dataset size n) that would then translate to generating a vulnerable output in x% of total outputs. If x is low enough to get past fine tuning&#x2F;regression testing but high enough to still occur within the deployment space, then you&#x27;ve effectively created a new category of supply-chain attack.There&#x27;s probably more research that needs to be done into occurrence rate of poisoned data showing up in final output, and that result is likely specific to the AI model and&#x2F;or version. reply btilly 16 hours agorootparentprevAs I commented elsewhere, GPT is such a target rich security environment that it is hard to know why you would bother with this. On the other hand, advanced persistent attackers (eg the NSA) have a pretty good imagination. I could see them having both motive and means to go out of their way to achieve a particular result.On human checks, http:&#x2F;&#x2F;www.underhanded-c.org&#x2F; demonstrates that it would be possible to inject content that will pass that. reply antonjs 15 hours agorootparentprevMakes me wonder if there would be a way to pollute imagenet so a particular image would always match for something like a facial recognition access control system or the like. Maybe adversarial data that would hide particular traffic patterns from an AI enabled IDS would be more plausible and something the NSA might be interested in. reply pixl97 15 hours agorootparentprevIn theory for any AI model that generates code you&#x27;ll want to have a series of post generation tests, for example something like SAST and&#x2F;or SCA that ensure the model is not biasing itself to particular flaws.At least for common languages this should stand out.Where it gets more tricky is watering hole attacks against specialized languages or certain setups. This said you&#x27;d have to ensure that this data is not already there scraped up from the internet. reply hedora 17 hours agoparentprevOccasionally, I’ll talk to someone suggesting a dynamically typed language (or stringly-typed java) for a very large scale (in developer count) security or mission critical application.This incident is a good one to point back to. reply sillysaurusx 17 hours agorootparentlaughs in log4j vulnA good fraction of the flaws we found at Matasano involved pentests against statically typed languages. If an adversary has root access to your storage box, they can likely find ways to pivot their access. Netpens were designed to do that, and those were the most fun; they’d parachute us into a random network, give us non-root creds, and say “try to find as many other servers that you can get to.” It was hard, but we’d find ways, and it almost never involved modifying existing files. It wasn’t necessary — the bash history always had so many useful points of interest.It’s true that the dynamics are a little different there, since that’s a running server rather than a storage box. But those two employees’ hard drive backups have an almost 100% chance of containing at least one pivot vector.Sadly choice of technology turns out to be irrelevant, and can even lead to overconfidence. The solution is to pay for regular security testing, and not just the automated kind. Get someone in there to try to sleuth out attack vectors by hand. It’s expensive, but it pays off. reply SoftTalker 14 hours agorootparentAm I one of few people who is frightened by shell history files? I always disable mine because it just seems like a roadmap to interesting stuff for anyone who might gain access to it. Including even stuff like sudo passwords typed at the wrong time or into the wrong window. reply failuser 12 hours agorootparentThe terminal backlog is just sitting in memory as well. Just don’t leave passwords there, remove them immediately. You also have an option not to save the command in history, e.g. whitespace prefix in bash. Half of my bash commands that are longer than 20 symbols start with ^R to look up a similar command and edit it, not having history would make that much slower. reply hypnagogic 11 hours agorootparentprevSure. But, you could auto-encrypt your ~&#x2F;.bash_history if you&#x27;re concerned about it being a problem and might need it for backtracing any issues etc? reply mattnewton 17 hours agorootparentprevThe typing of python isn’t the issue, it’s effectively the eval problem of not having a separation between code and data in the pickle format often used out of convenience. There are lots of pure data containers, like huggingface’s safe tensors or tensorflow’s protobuf checkpoints, that could have been used instead. reply evertedsphere 17 hours agorootparentprevtypes have nothing to do with this, strictly speaking; the same problems would exist if you serialised structures containing functions in a typed language to e.g. a dll or a .class file and asked users to load it at runtimethe problem is in fact the far more subtle principle of \"don&#x27;t download and run random code, and definitely don&#x27;t make it the idiomatic way to do things,\" and i&#x27;m not sure you can blame your use of eval()-like things on the fact that they exist in your language in the first place reply rowanG077 16 hours agorootparentThe difference is that no one shares data in a statically typed language by sending over dlls or .class files. The entire point is that something so dangerous has been normalized because of dynamic typing. reply evertedsphere 4 hours agorootparentpoor engineering choices are just that, choices reply rowanG077 3 hours agorootparentSome tools make poor choices harder or impossible. That&#x27;s the entire point of static typing too. In this case python encouraged insecure design choices by making them very easy and even presenting them to users. reply nostoc 16 hours agorootparentprevYeah, because statically typed language never had any kind of deserialization vulnerabilities. reply chinchilla2020 17 hours agorootparentprevWhat is the best practice? I&#x27;m assuming something that isn&#x27;t a programming language object... reply make3 17 hours agorootparentprevthat has literally nothing to do with the topic, which is just misconfigured cloud stuff. people really like starting these old crappy language arguments anywhere they can reply benreesman 17 hours agoparentprevI’ll venture that it’s at least adjacent that the indiscriminate assembly of massive, serious pluralities of the commons on a purely unilateral basis for profit is sort of a “just try and stop us” posture that whether or not directly related here, and clearly with some precedent, is looking to create a lot of this sort of thing over and above the status-quo ick. reply short_sells_poo 16 hours agorootparentI have no idea what you are saying. If it is: \"bad incentives cause people to misbehave\", you generated an impressive verbiage around it :) reply benreesman 13 hours agorootparentI have a bad habit of using 5 words when 1 will do: but I was saying that the probably fucking illegal status quo on AI corpus assembly is making an already ugly world a lot fucking worse. reply BlueTemplar 13 hours agoparentprevSo, a little bit like a lot of people think that (non-checksummed&#x2F;non-encrypted) PDFs cannot be modified, even though they are easily editable with Libre freaking Office ? reply failuser 12 hours agorootparentYou can’t edit them in Word, so that must be too advanced for most people. LibreOffice never opened the PDFs too well for me, but Inkspace was pretty good, one page at a time though. reply dheera 17 hours agoparentprevMany people are also unaware that json is way, way, way faster than Python pickles, and human-editing-friendly. Not that you&#x27;d use it for neural net weights, but I see people use Python pickles all the time for things that json would have worked perfectly well. reply romanows 16 hours agorootparentAre you sure json is faster than pickle in recent python versions? That&#x27;s not intuitive to me and search result blurbs seem to indicate the opposite. reply rodgerd 11 hours agoparentprevThe other aspect that pertains to AI is the data-maximalist mindset around these tools: grab as much data, aggregate it all together, and to hell with any concerns about what and how the data is being used; more data is the competitive advantage. This means a failure that might otherwise be quite limited in scope becomes huge. reply sillysaurusx 18 hours agoprevThe article tries to play up the AI angle, but this was a pretty standard misconfiguration of a storage token. This kind of thing happens shockingly often, and it’s why frequent pentests are important. reply cj 18 hours agoparent> it’s why frequent pentests are important.Unfortunately a lot of pen testing services have devolved into \"We know you need a report for SOC 2, but don&#x27;t worry, we can do some light security testing and generate a report for you in a few days and you&#x27;ll be able to check the box for compliance\"Which is guess is better than nothing.If anyone works at a company that does pen tests for compliance purposes, I&#x27;d recommend advocating internally for doing a \"quick, easy, and cheap\" pen test to \"check the box\" for compliance, _alongside_ a more comprehensive pen test (maybe call it something other than a \"pen test\" to convince internal stakeholders who might be afraid that a 2nd in depth pen test might weaken their compliance posture since the report is typically shared with sales prospects)Ideally grey box or white box testing (provide access to codebase &#x2F; infrastructure to make finding bugs easier). Most pen tests done for compliance purposes are black-box and limit their findings as a result. reply dylan604 18 hours agorootparentI recently ran into something along the lines of your devolved pentest concept. I have a public facing webapp, and the report came back with a list of \"critical\" issues that are solved by yum update. Nothing about vulnerability to session jacking or anything along the lines of requiring actual work. I was a few steps removed from the actual testing, so who knows what was lost in translation and it being the first time I&#x27;ve ever had something I worked on pen tested. However, I feel this was more of a script kiddie port scan level of effort vs actually trying to provide useful security advice. The whole process was very disappointing. reply alaxapta7 15 hours agorootparentI&#x27;ve seen worse. Couple years back, there was an audit that included an internal system I&#x27;ve been working on. It was running on Debian oldstable because of a vital proprietary library I wasn&#x27;t able to get working on stable at the time, but it had unattended upgrades set up and all that.The company made some basic port scan and established that we&#x27;re running outdated and vulnerable version of Apache. I found the act of explaining the concept of backports to a \"pentester\" to be physically painful.They didn&#x27;t get paid and another company was entrusted with the audit. reply pixl97 15 hours agorootparentThis is why I always attempt to turn off as much version information output as possible from any service. Make the pentester do their homework and not just look at \"Apache 2.XX\"Hopefully you also have an internal control that looks at actual package versions installed on the server. reply alaxapta7 13 hours agorootparentNormally I do that too, but this was fairly new and internal application that was still in development, so that&#x27;s why it was there. And if it wasn&#x27;t for this incident, they might actually trick our management into thinking they&#x27;re somehow qualified to carry out such an audit. reply dylan604 15 hours agorootparentprevThis is actually a take away that I did implement. it&#x27;s one of those that&#x27;s not actively a vuln, but might provide info on what other attacks to try. reply im3w1l 17 hours agorootparentprevHow behind on yum updates were you anyway? reply dylan604 17 hours agorootparentnot very. i guess i was too cavalier in hand waving it as a yum update. some of it was switching to a new repo with the most recent version available. but that was still just using yum. not like it required changes to the code base and workflow. maybe it was an amazon-linux-extras command for the actual package change, but still. reply oooyay 17 hours agorootparentprevNarrowly scoped tests designed for specific compliance requirements are fine. They lower the barrier to entry to some degree for even getting testing and still, or often enough, return viable results. There&#x27;s also SAAS companies that have emerged that effectively run a scripted analysis of cloud resources. The two together are more economical and still accomplish the goals that having compliance in the first place sets out.When I was consulting architecture and code review were separate services with a very different rate from pentesting. Similar goals but far more expensive. reply mymac 18 hours agoparentprevPentests where people actually get out of bed to do stuff (read code, read API docs etc) and then try to really hack your system are rare. Pentests where people go through the motions, send you report with a few unimportant bits highlit while patting you on the back for your exemplary security so you can check the box on whatever audit you&#x27;re going through are common. reply nbk_2000 16 hours agorootparentIf you&#x27;re a large company that&#x27;s actually serious about security, you&#x27;ll have a Red Team that is intimately familiar with your tech stacks, procedures, business model, etc. This team will be far better at emulating motivated attackers (as well as providing bespoke mitigation advice, vetting and testing solutions, etc.).Unfortunately, compliance&#x2F;customer requirements often stipulate having penetration tests performed by third parties. So for business reasons, these same companies, will also hire low-quality pen-tests from \"check-box pen-test\" firms.So when you see that $10K \"complete pen-test\" being advertised as being used by [INSERT BIG SERIOUS NAME HERE], good chance this is why. reply pixl97 15 hours agorootparentUgh, in the work I do I run into so much of this kind of stuff.Customer: \"We had a pentest&#x2F;security scan&#x2F;whatever find this issue in your software\"Me: \"And they realized that mitigations are in place as per the CVE that keep that issue from being an exploitable issue, right\"Customer: \"Uhhhh\"Testing group: \"Use smaller words please, we only click some buttons and this is the report that gets generated\" reply iamflimflam1 18 hours agorootparentprevYep, most pentests go through the OWASP list and call it done. reply ganoushoreilly 17 hours agorootparentThe problem is that is what most companies want. They don&#x27;t want to spend the money nor get the feedback beyond \"Best case standards\". It&#x27;s a calculated risk. reply Faelian2 17 hours agorootparentprevHonestly, the OWASP top ten is generic enough that most vulnerability fit in it : \"injection\", \"security misconfiguration\", \"insecure design\".The problem is1. knowing the gazillion of web vulnerabilities, and technologies2. being good enough to tests them3. kick yourself and go through the laborious process of understand and test every key feature of the target. reply fomine3 7 hours agorootparentprevIt&#x27;s great if it&#x27;s done exhaustively reply j245 18 hours agorootparentprevFrom my understanding as a non security expert:Pentest comes across more as checking all the common attack vectors don’t exist.Getting out of bed to do the so-called “real stuff” is typically called a bug bounty program or security researching.Both exist and I don’t see why most companies couldn’t start a bug bounty program if they really cared a lot about the “real stuff” reply csydas 16 hours agorootparentI think the concern is more about the theatre of most modern pen-testing rather than expecting deep bug-bounty work. I&#x27;m not a security expert either, but I&#x27;ve had to refute \"security expert\" consultations from pen-test companies, and the reports are absolutely asinine half the time and filled with so many false positives due to very weak signature matching that they&#x27;re more or less useless and give a false sense of security.For example, dealing with a \"legal threat\" situation with the product I work on because a client got hit by ransomware and they blame our product because \"we just got a security assessment saying everything was fine, and your product is the only other thing on the servers\" -- checked the report, basically it just runs some extremely basic port checks&#x2F;windows config checks that haven&#x27;t been relevant for years and didn&#x27;t even apply to the Windows versions they had, and in the end the actual attack came from someone in their company opening a malicious email and having a .txt file with passwords.I don&#x27;t doubt there are proper security firms out there, but I rarely encounter them. reply j245 16 hours agorootparentThat’s interesting. I thought maybe it’s a resource constraint issue, where companies prioritise investment in other areas and do the minimum to “get certified” but it sounds like finding a good provider can be extremely difficult. reply Faelian2 17 hours agorootparentprevI work as pentester (as a freelance nowdays).Getting out of bed and \"real stuff\" is supposed to be part of a pentest.The problem is more the sheer amout of stuff your are supposed to know to be a pentester. Most pentesters come into the field by knowing a bit of XSS, a few thing about PHP, and SQL injections.Then you start to work, and the clients need you to tests things like:- compromise a full Windows Network, and take control of the Active Directory Server. Because of a misconfiguration of Active Directory Certificate Services. While dealing with Windows Defender- test a web application that use websockets, React, nodejs, and GraphQL- test a WindDev application, with a Java Backend on a AIX server- check the security of an architecture with multiple services that use a Single Sign on, and Kubernetes- exploit multiple memory corruption issues ranging form buffer overflow to heap and kernel exploitation- evaluate the security of an IoT device, with a firmware OTA update and secure boot.- be familiar with cloud tokens, and compliance with European data protection law.- Mobile Security, with iOS and Android- Network : radius, ARP cache poisoning, write a Scapy Layer for a custom protocol, etc- Cryptography, you might need itMost of this is actual stuff I had to work on at some point.Even if you just do web, you should be able to detect and exploit all those vulnerabilities: https:&#x2F;&#x2F;portswigger.net&#x2F;web-security&#x2F;all-labsNobody knows everything. Being a pentester is a journey.So in the end, most pentesters fall short on a lot this. Even with an OSCP certification, you don&#x27;t know most of what you should know. I heard that in some company, people don&#x27;t even try and just give you the results of a Nessus scan. But even if you are competent, sooner or later, you will run into something that you don&#x27;t understand. And you have max 2 week to get familiar with it and test it. You can&#x27;t test something that you don&#x27;t understand.The scanner always gives you a few things that are wrong (looking at you TLS ciphers). Even if you suck, or if the system is really secure. You can put a few things into your report. As a junior pentester, my biggest fear was always to hand an empty report. What were people going to think of you, if you work 1 week and don&#x27;t find anything? reply throwaway2037 6 hours agorootparentThanks for your honest reply. This part was my favourite: Nobody knows everything. Being a pentester is a journey.I recommend that you add some contact details in your HN bio page. You might get some good ledes after those post. reply pixl97 15 hours agorootparentprev>As a junior pentester, my biggest fear was always to hand an empty report.I&#x27;m trying to remember the rule where you leave something intentionally misconfigured&#x2F;wrong for the compliance people to find and that you can fix so they don&#x27;t look deeper into the system. A fun one with web servers is to get them to report they are some ancient version that runs on a different operating system. Like your IIS server showing it&#x27;s Apache 2.2 or vice versa.But at least from your description it sounds like you&#x27;re attempting to pentest. So many of these pentesting firms are click a button, run a script, send a report and go on to the 5 other tickets you have that day type of firms. reply c0pium 6 hours agorootparentprevBug bounty programs are a nightmare to run. For every real bug reported you’ll get thousands of nikto pdfs with CRITICAL in big red scare letters all over them. Then you’ll get dragged on twitter constantly for not being serious about security. Narrowing the field to vetted experts will similarly get you roasted for either having something to hide or not caring about inclusion. And god help you if you have to explain that you already knew about a bug reported by anyone with more than 30 followers…There are as many taxonomies of security services as there are companies selling them. You have to be very specific about what you want and then read the contract carefully. reply ozim 17 hours agorootparentprevNot really.Real stuff should always be a pentest - penetration test where one is actively trying to exploit vulnerabilities. So person who orders that gets report with !!exploitable vulnerabilities!!.Checking all common attack vectors is vulnerability scanning and is mostly running scanner and weeding out false positives but not trying to exploit any. Unfortunately most of companies&#x2F;people call that a penetration test, while it cannot be, because there is no attempt at penetration. While automated scanning tools might do some magic to confirm vulnerability it still is not a penetration test.In the end, bug bounty program is different in a way - you never know if any security researcher will even be interested in testing your system. So in reality you want to order penetration test. There is usually also a difference where scope of bug bounty program is limited to what is available publicly. Where company systems might not allow to create an account for non-business users, then security researcher will never have access to authenticated account to do the stuff. Bounty program has also other limitations because pentesting company gets a contract and can get much more access like do a white box test where they know the code and can work through it to prove there is exploitable issue. reply pgraf 17 hours agorootparentprevAs in every industry there are cheapskates, and especially in pentesting it is often hard for the customer to tell the good ones from the bad ones. Nevertheless, I think that you have never worked with a credible pentesting vendor. I am doing these tests for a living and would be ashamed to deliver anything coming near your description :-) reply NegativeK 17 hours agorootparentprevThe checkbox form exists because crooked vendors are catering to organizations who are intentionally lazy about their cybersecurity.Real penetration tests provide valuable insight that a bug bounty program won&#x27;t. reply prmoustache 17 hours agorootparentprevpentest means penetration testing which mean one need to take the attacker hat and try to enter your network or the app infrastructure and get as much data as he can, be it institutionnal or customer data. It can be through technical means as well as social engineering practices. And then report back.This is in no way related to a bug bounty program. reply nbk_2000 16 hours agorootparentCounter point: Most of the top rated Bug Bounty hunters have a background in penetration testing.I think it&#x27;s more accurate to say Bug Bounty only covers a small subset of penetration testing (mainly in that escalation and internal pivoting are against the BB policy of most companies). reply mymac 17 hours agorootparentprev> From my understanding as a non security expert:That certainly helps. reply j245 17 hours agorootparentWhat a shame, HackerNews typically has more insightful comments than garbage like this.Edit: thanks to everyone who wrote some insightful responses, and there are indeed many. Faith in HackerNews restored ! reply bee_rider 16 hours agorootparentprevPeople are going to chit-chat about things only tangentially related to their areas of expertise; it is good when we’re honest about our limitations.If nothing else, an obviously wrong take is a nice setup for a correction. reply evntdrvn 16 hours agorootparentprevwhat I always want to know when people talk about this is \"what reputable companies can I actually pay to do a real pentest (without costing hundreds of thousands of dollars).\" reply amlozano 7 hours agorootparentThe problem is security is a \"Market for lemons\" https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Market_for_Lemons. Just like when trying to buy a used car, you need someone who is basically an expert in selling used cars.In order to purchase a reputable pentest, you basically have to have a security team that is mature enough to have just done it themselves.I can throw out some names for some reputable firms, but you are still going to need to do some leg work vetting the people they will staff your project with, and who knows if those firms will be any good next year or the year after.Here&#x27;s a couple generic tips from an old pentester:* Do not try and schedule your pentest in Q4, everyone is too busy. Go for late Q1 or Q2. Also say you are willing to wait for the best fit testers to be available.* Ask to review resumes of the testing team. They should have some experience with your tech and at least one of them needs to have at least 2 years experience pen-testing.* Make sure your testing environment is set up, as production like as possible, and has data in it already. Test the external access. Test all the credentials, once after you generated them, again the night before the test starts. The most common reason to lose your good pentest team and get some juniors swapped in that have no idea what they are doing is you delayed the project by not being ready day 1. reply pnt12 15 hours agorootparentprevI think hiring a security specialist is the way to go. reply _jal 17 hours agorootparentprevLet me tell you about the laptop connected to our network with a cellular antenna we found in a locked filing cabinet after getting a much-delayed forced-door alert. This, after some social engineering attempts that displayed unnerving familiarity with employees and a lot of virtual doorknob-rattling.They may be rare, but \"real\" pentests are still a thing. reply mymac 17 hours agorootparentOuch. How did that ended up? replytrebligdivad 18 hours agoparentprevHow would a pentest find that? Ok in this case it&#x27;s splattered onto github; but the main point here is that you might have some unknown number of SAS tokens issued to unknown storage that you probably haven&#x27;t any easy way to revoke. reply sillysaurusx 16 hours agorootparentA number of ways, including:- finding the token directly in the repo- reviewing all tokens issued reply acdha 17 hours agoparentprevIt didn’t seem to be focused on AI except for the very reasonable concerns that AI research involves lots of data and often also people without much security experience. Seeing things like personal computer backups in the dump immediately suggests that this was a quasi-academic division with a lot less attention to traditional IT standards: I’d be shocked if a Windows engineer could commit a ton of personal data, passwords, API keys, etc. and first hear about it from an outside researcher. reply sneak 15 hours agoparentprevIt was so common that S3 added several features to make it really, really hard to accidentally leave a whole bucket public.Looks like Azure hasn&#x27;t done similarly. reply mcast 13 hours agorootparentIs there any valid use case for when it&#x27;s a good idea to publicly expose a S3 bucket? reply sneak 13 hours agorootparentSharing of datasets, disk images, ISOs, ML models, etc, as well as public websites. reply doctorpangloss 17 hours agoparentprevCloud buckets have all sorts of toxic underdevelopment of features. They play make believe that they&#x27;re file systems for adoption.Like for starters, why is it so hard to determine effective access in their permissions models?Why is the \"type\" of files so poorly modeled? Do I ever allow people to give effective public access to a file \"type\" that the bucket can&#x27;t understand?For example, what is the \"type\" of code? It doesn&#x27;t have to be this big complex thing. The security scanners GitHub uses knows that there&#x27;s a difference between code with and without \"high entropy strings\" aka passwords and keys. Or if it looks like data:content&#x2F;type;base64, then at least I know it&#x27;s probably an image.What if it&#x27;s weird binary files like .safetensors? Someone here saying you might \"accidentally\" release the GPT4 weights. I guess just don&#x27;t let someone put those on a public-resolvable bucket, ever, without an explicit, uninherited manifest &#x2F; metadata permitting that specific file.Microsoft owns the operating system! I bet in two weeks, the Azure and Windows teams can figure out how to make a unified policy manifest &#x2F; metadata for NTFS & ReFS files that Azure&#x27;s buckets can understand. Then again, they don&#x27;t give deduplication to Windows 11 users, their problem isn&#x27;t engineering, it&#x27;s the financialization of essential security features. Well jokes on you guys, if you make it a pain for everybody, you make it a pain for yourself, and you&#x27;re the #1 user of Azure. reply xbar 14 hours agoparentprevAI data is highly centralized and not stored in a serially-accessed database, which makes it unusual inasmuch as 40TB of interesting data does not often get put into a single storage bucket. reply quickthrower2 18 hours agoprevTwo of the things that make me cringe are mentioned. Pickle files and SAS tokens. I get nervous dealing with Azure storage. Use RBAC. They should depreciate SAS and account keys IMO.SOC2 type auditing should have been done here so I am surprised of the reach. Having the SAS with no expiry and then the deep level of access it gave including machine backups with their own tokens. A lot of lack of defence in depth going on there.My view is burn all secrets. Burn all environment variables. I think most systems can work based on roles. Important humans access via username password and other factors.If you are working in one cloud you don’t in theory need secrets. If not I had the idea the other day that proxies tightly couples to vaults could be used as api adaptors to convert then into RBAC too. But I am not a security expert just paranoid lol. reply prmoustache 17 hours agoparentMany SOC2 audits are a joke. We were audited this year and were asked to provide screenshots of various categories (but most being of our own choosing in the end). Only requirement was screenshots needed to show date of the computer on which the screenshot had been taken, as if it couldn&#x27;t be forged as well as the file&#x2F;exif data. reply lijok 14 hours agorootparentIf you forge your SOC2 evidence you will legitimately wish you were never born once caught reply prmoustache 13 hours agorootparentWe aren&#x27;t doing that. I just mention the lazyness of the auditors and that asking for screenshots is just dumb. At this point you can just ask a simply question: do you comply or not? reply hypeatei 17 hours agoparentprevAbsolutely, RBAC should be the default. I would also advocate separate storage accounts for public-facing data, so that any misconfiguration doesn&#x27;t affect your sensitive data. Just typical \"security in layers\" thinking that apparently this department in MSFT didn&#x27;t have. reply ozim 16 hours agoparentprevSo SAS tokens are worse that some admin setting up \"FileDownloaderAccount\" and then sharing its password with multiple users or using the same for different applications?I take SAS tokens with expiration over people setting up shared RBAC account and sharing password for it.Yes people should do proper RBAC, but point a company and I will find dozens \"shared\" accounts. People don&#x27;t care and don&#x27;t mind. When beating them up with sticks does not solve the issue SAS tokens while still not perfect help quite a lot. reply quickthrower2 11 hours agorootparentFileDownloaderAccount had no copy pastable secret that can be leaked. Shared passwords are unnecessary of course and not good. If people are going to do that just use OneDrive&#x2F;Dropbox rather than letting people use advanced things. reply bunderbunder 17 hours agoparentprevPickle files are cringe, but they&#x27;re also basically unavoidable when working with Python machine learning infrastructure. None of the major ML packages provide a proper model serialization&#x2F;deserialization mechanism.In the case of scikit-learn, the code implementing some components does so much crazy dynamic shit that it might not even be feasible to provide a well-engineered serde mechanism without a major rewrite. Or at least, that&#x27;s roughly what the project&#x27;s maintainers say whenever they close tickets requesting such a thing. reply osanseviero 17 hours agorootparentYou should check out safetensors. They are used widely in diffusion models and LLMs https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;safetensors-security-audit reply jklehm 17 hours agorootparentprevONNX[0], model-as-protosbufs, continuing to gain adoption will hopefully solve this issue.[0] https:&#x2F;&#x2F;github.com&#x2F;onnx&#x2F;onnx reply bunderbunder 17 hours agorootparentONNX is cool, but it still only supports a minority of scikit-learn components. Some of them simply aren&#x27;t compatible with ONNX&#x27;s basic design. reply mxz3000 17 hours agorootparentprevat work we use the ONNX serialisation format for all of our prod models. Those get loaded by the ONNX runtime for inference. works great.perhaps it&#x27;s be viable to add support for the ONNX format even for use cases like model checkpointing during training, etc ? reply hdesh 18 hours agoprevOn a lighter note - I saw a chat message that started with \"Hey dude! How is it going\". I&#x27;m disappointed that the response was not https:&#x2F;&#x2F;nohello.net&#x2F;en&#x2F;. reply monkpit 18 hours agoparentI strongly support the “no hello” concept but I also fear being seen as “that guy” so I never mention it. Sigh reply dymk 17 hours agorootparentI&#x27;ve made peace with people sending me a bare \"hello\" with no context. I ignore it until there&#x27;s something obvious to respond to. Responding with the \"no hello\" webpage will often be received as (passive) aggressive, and that&#x27;s a bad way to start off a conversation.Usually within a few minutes there&#x27;s followup context sent. Either the other party was already in the process of writing the followup, or they realized there was nothing actionable to respond to and they elaborate. reply monkpit 16 hours agorootparentprevI should have a slack bot that replies automatically to generic greetings… that way they’ll get on with whatever the issue is and I won’t have to reply. reply bornfreddy 13 hours agorootparentHa ha, that&#x27;s a great idea!A: Hello!B&#x27;s bot: Hello to you too! I am a chatty bot which loves responding to greetings. Is there a message I can forward to B? reply hiddencost 18 hours agorootparentprevI make it my status message. reply acdha 17 hours agorootparentThe people who need it aren’t the type of people who’d read it. reply gaudystead 17 hours agorootparentprevI made it my status message as well and all I got was a complaint passed along from my manager because somebody said that it was too rude and that I should be more gentle with my fellow corporate comrades... reply version_five 17 hours agorootparentprevI tried that on slack for a while, it made no difference. I don&#x27;t think most people read the status message. The medium lends itself to the \"Hi\" type messages unfortunately, there&#x27;s not really a way go constrain human nature, other than to not use instant messaging at all (I also tried changing my status to a note telling people to phone me, that didn&#x27;t work either) reply fireflash38 17 hours agorootparentprevI have seen people never ask their question after multiple days of saying \"hello @user\", despite having nohello as a status. And despite having asked them in the past to just ask their question and I&#x27;ll respond when I can.You just can&#x27;t win. reply cosmojg 15 hours agorootparentI&#x27;d count that as a win. You avoided wasting your time answering a potentially inane question. If it were important, they would have asked. reply sneak 15 hours agorootparentprevBe that guy. In the long run it&#x27;s better to be right then popular. reply cosmojg 15 hours agorootparentBut then I might not survive the long run. reply bootloop 17 hours agoparentprevThis is quite funny for me because at first I didn&#x27;t understand what the problem is.In German, if you ask this question, it is expected that your question is genuine and you can expect an answer (Although usually people don&#x27;t use this opportunity to unload there emotional package, but it can happen!)Whereas in Englisch you assume this is just a hello and nothing more. reply manojlds 17 hours agorootparentIn England people say \"You all right\" and move on without even waiting for a response! reply qingcharles 16 hours agorootparentIn America it&#x27;s even worse because they say \"What&#x27;s up?\" in the same way we Brits say \"Alright?\", but \"What&#x27;s up?\" to me like the person has detected something wrong with you and wants to know what the problem is. At least \"Alright?\" is more generally asking for your status.Of course, both are generally rhetorical, which must be confusing for some foreigners learning English, especially with the correct response to \"Alright?\" being \"Alright?\" and similarly with \"What&#x27;s up?\". reply mbg721 16 hours agorootparentI believe the correct response is \"Chicken butt,\" but maybe I&#x27;m in very exclusive company in responding that way. reply hahn-kev 18 hours agoparentprevGlad I&#x27;ve never had to deal with that in chat.Though I have had the equivalent in tech support: \"App doesn&#x27;t work\" which is basically just hello, obviously you&#x27;re having an issue otherwise you wouldn&#x27;t have contacted our support. reply syndicatedjelly 18 hours agoparentprevI love that an entire website was made around this, without any attempt to sell me anything. So rare to see that these days reply low_tech_punk 17 hours agoparentprevUnfortunately, the AI researcher did not use a LLM to automatically respond the nohello content. reply jovial_cavalier 13 hours agoparentprevDestroying comradery with a co-worker - Any % (WR) reply pradn 15 hours agoprevIt&#x27;s not reasonable to expect human security token generation to be perfectly secure all the time. The system needs to be safe overall. The organization should have set an OrgPolicy on this entire project to prevent blanket sharing of auth tokens&#x2F;credentials like this. Ideally blanket access tokens should be opt-in, not opt-out.Google banned generation of service account keys for internally-used projects. So an awry JSON file doesn&#x27;t allow access to Google data&#x2F;code. This is enforced at the highest level by OrgPolicy. There&#x27;s a bunch more restrictions, too. reply stevanl 18 hours agoprevLooks like it was up for 2 years with that old link[1]. Fixed two months ago.[1] https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;robust-models-transfer&#x2F;blame&#x2F;a9... reply jl6 17 hours agoprevKind of incredible that someone managed to export Teams messages out from Teams… reply mola 17 hours agoprevIt&#x27;s always funny that wiz&#x27;s big security revelations are almost always about Microsoft. When wiz&#x27;s founder was the highest ranking in charge of cyber security at Microsoft in his previous job . reply alphabetting 17 hours agoparentWould be kind of surprising if that weren&#x27;t the case. reply anon1199022 19 hours agoprevJust proves how hard it cloud security now. 1-2 mistake and you expose TB&#x27;s. Insane. reply tombert 19 hours agoparentMy wife and I just rewatched WarGames for the millionth time a few nights ago.The level of cybersecurity incompetency in the early 80&#x27;s makes sense; computers (and in particular networked computers) were still relatively new, and there weren&#x27;t that many external users to begin with, so while the potential impact of a mistake was huge (which of course was the plot of the movie), the likelihood of a horrible thing happening was fairly low just because computers were an expensive, somewhat niche thing.Fast forward to 2023, and now everyone owns bunches of computers, all of which are connected to a network, and all of which are oodles more powerful than anything in the 80s. Cybersecurity protocols are of course much more mature now, but there&#x27;s also several orders of magnitude more potential attackers than there were in the 80s. reply viraptor 18 hours agorootparent> Cybersecurity protocols are of course much more mature nowAt technical level, sure. At the deployment, configuration and management level, not quite. Overall things are so bad that news aren&#x27;t even reporting the hospitals taken over by ransomware anymore. It&#x27;s still happening almost every week and we&#x27;re just... used to it. reply LinuxBender 18 hours agorootparentprevThat modem setup in Wargames is still a thing for many organizations including some banks and telcos. Not naming names but I suspect the modems will be around for a very long time. Some have a password on their modem but they are usually very simple. Their only saving grace is that they are usually in front of a mainframe speaking proprietary MML that only old fuddy duddies like me would remember. There are a few of us here reply rft 16 hours agorootparent> proprietary MML that only old fuddy duddies like me would remember.Security through obscurity helps only until someone gets curious&#x2F;determined. I have a personal anecdote for that. During university I was involved in pentesting an industrial control system (not in an industrial context, but same technology) and implemented a simple mitm attack to change the state of the controls while displaying the operator selected state. When talking with the responsible parties, they just assumed that the required niche knowledge means the attack is not feasible. I had the first dummy implementation setup on the train ride home based only on network captures. Took another day to fine tune once I got my hands on a proper setup and worked fine after that.I do not want to say that ModbusTCP is in the same league as MML, but if there is interest in it, someone will figure it out. Sure, you might not be on Shodan, but are the standard&#x2F;scripted attacks really what you should worry about? Also don&#x27;t underestimate a curious kid who nerdsnipes themself into figuring that stuff out. reply LinuxBender 15 hours agorootparentSecurity through obscurity helps only until someone gets curious&#x2F;determined.Absolutely. It just weeds out the skiddies and tools like MetaSploit unless they have added mainframe support. I have not kept up with their librariesThe federal agencies I was liaison to knew all the commands better than I did and even taught me a few that were not in my documentation which led to a discussion with the mainframe developers. reply FridayNightTV 18 hours agorootparentprev> I suspect the modems will be around for a very long time.No they won&#x27;t.&#x27;Dial up&#x27; modems need a PSTN line to work. The roll out of full fibre networks means analogue PSTN is going the way of the dodo. You cannot get a new PSTN line anymore in Blighty. In Estonia and the Netherlands (IIRC) the PSTN switch off is already complete. reply pavlov 18 hours agorootparentSurely there’s a vendor that will sell you a v.22bis modem that works over VoIP if that’s what your two mainframes need to sync up, and you’re buying the multimillion dollar support contract… reply pixl97 15 hours agorootparentprev>&#x27;Dial up&#x27; modems need a PSTN line to workCable company here (US) still sells service that has POTS over cable modem. Just plug your modem into the cable modem tele slot and you have a dialton. Now, are you getting super high speed connections, no, but that&#x27;s not what you need for most hacking like this. Not that I recommend hacking from your own house. reply LinuxBender 18 hours agorootparentprevI should have restricted that statement to include the United States of America. PSTN&#x27;s are still utilized, deployed and actively sold in most of the US. As a side note I recently tried to get a telco to remove a phone line and two poles and they refused to do it. Their excuse was that they might one day run fiber over it despite there already being a fiber network here. I hope they do as my fiber ISP really does need a competitor. If they really do run the fiber over those poles vs burying it that would be amusing.To your point I am sure some day the US will stop selling access to the PSTN but some old systems will hold on for dear life, government contracts and all. Governments are kindof slow to migrate to newer things. reply Kon-Peki 15 hours agorootparent> As a side note I recently tried to get a telco to remove a phone line and two poles and they refused to do it.You need to align their incentives with yours: wait until it gets windy out, knock the poles down, and demand that they come fix it. reply LinuxBender 15 hours agorootparentI&#x27;ve been secretly hoping an over-sized big rig would take them out but I would not want anyone to get hurt. They are the only poles within a few miles and are an eye-sore. reply heywhatupboys 18 hours agorootparentprevwhat does this have to do with a \"modem\" per se? reply LinuxBender 18 hours agorootparentThe parent comment was about the movie Wargames and the questionable security of the 80&#x27;s that is still in use today. That security in Wargames was a modem that provided access to a subsystem of the WOPR mainframe named \"Joshua\". Joshua had super-user privs on the mainframe.It was likely meant to be a temporary means for the system architect to monitor and improve the system after it was deployed but then life changing circumstances may have distracted his attention away from decommissioning the modem. The movie still holds up today and is worth a watch. Actually it may be more pertinent now than ever. reply tombert 18 hours agorootparentYeah, when we were rewatching it, we were kind of amazed at how well it holds up, all things considered.I think what makes it likable for me is that it&#x27;s all on the cusp of believability. Obviously LLMs weren&#x27;t quite mature enough to do everything Joshua did back then (and probably not now), but the fact that the \"hacking\" was basically just social engineering, and was just achieved by wardialing and a bit of creative thinking makes it somewhat charming, even today.With the advent of LLMs being used increasingly for everyone, I do wonder how close we&#x27;re going to get to some kind of \"Global Thermonuclear War\" simulation gone awry. reply dylan604 17 hours agorootparentprevi still love the phreaking scene trying to make a phone call where he uses the can pull tab to ground the phone. it was more of a phreaker vibe than trying to whistle into the phone or social engineer an operator or just happening to have a dialer on him. reply k12sosse 18 hours agorootparentprev> wardialingGet a load these guys honey, you could just dial straight into the airline. reply photoGrant 19 hours agoparentprevHard coded secrets in shareable URL’s with almost infinite time windows and an untraceable ability to audit what’s made and shared and at what level?Sounds like it’s as hard as it’s always been. Pretty basic and filled with humans reply LeifCarrotson 18 hours agorootparentI feel like it&#x27;s harder.It&#x27;s no longer hierarchical, with organization schemes limited to folders and files. People no longer talk about network paths, or server names.Mobile and desktop apps alike go to enormous effort to abstract and hide the location at which a document gets stored, instead everything is tagged and shared across buckets and accounts and domains...I expect that the people at this organization working on cutting-edge AI are pretty sharp, but it&#x27;s no surprise that they don&#x27;t entirely understand the implications of \"SAS tokens\" and \"storage containers\" and \"permissive access scope\" on Azure, and the differences between Account SAS, Service SAS, and User Delegation SAS. Maybe the people at Wiz.io are sharper, but unless I missed the sarcasm, they may be wrong when they say [1] \"Generating an Account SAS is a simple process.\" That looks like a really complicated process!We just traced back an issue where a bunch of information was missing from a previous employee&#x27;s projects when we changed his account to a shared mailbox. Turns out that he&#x27;d inadvertently been saving and sharing documents from his individual OneDrive on O365 (There&#x27;s not one drive! There are many! Stop trying to pretend there&#x27;s only one drive!) instead of the \"official\" organization-level project folder, and had weird settings on his laptop that pointed every \"Save\" operation at that personal folder, requiring a byzantine procedure to input a real path to get back to the project folder.[1]: https:&#x2F;&#x2F;i.imgur.com&#x2F;6V7VLLd.png reply eitland 1 hour agorootparent> but unless I missed the sarcasm, they may be wrong when they say [1] \"Generating an Account SAS is a simple process.\" That looks like a really complicated process!No, unless I understand actually it is intended to be understood the other way:It is too easy to create a to broad token.And in the next paragraph, after the image, they explain that in addition to it being easy to create, these tokens are impossible to audit. reply kevinsundar 12 hours agoprevThis is very similar to how some security researchers got access to TikTok&#x27;s S3 bucket: https:&#x2F;&#x2F;medium.com&#x2F;berkeleyischool&#x2F;cloudsquatting-taking-ove...They used the same mechanism of using common crawl or other publicly available web crawler data to source dns records for s3 buckets. reply wodenokoto 18 hours agoprevI really dislike how Azure makes you juggle keys in order to make any two Azure things talk together.Even more so, you only have two keys for the entire storage account. Would have made much more sense if you could have unlimited, named keys for each container. reply unoti 17 hours agoparent> I really dislike how Azure makes you juggle keys in order to make any two Azure things talk together.Actually there is a better way. Look into “Managed Identity”. This allows you to grant access from one service to another, for example grant access to allow a specific VM to work with your storage account. reply bob1029 16 hours agorootparentThis is what we are using for everything. It makes life so much easier.So far, our new Azure tenant has absolutely zero passwords or shared secrets to keep track of.Granting a function app access to SQL Server by way of the app&#x27;s name felt like some kind of BS magic trick to me at first. But it absolutely works. Experiences like this give me hope for the future. reply PretzelPirate 18 hours agoparentprev> if you could have unlimited, named keys for each container.These exist and are called Shared Access Tokens. People are too lazy to use them and just use the account-wide keys instead. reply quickthrower2 18 hours agoparentprevhttps:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;azure&#x2F;role-based-access-co... reply bkm 19 hours agoprevWould be insane if the GPT4 model is in there somewhere (as its served by Azure). reply albert_e 17 hours agoparentAlso imagine all such exposed data sources including those that are not yet discovered... are crawled and trained on by GPT5.Meanwhile a big enterprise provider like MS suffers a bigger leak and exposes MS Teams&#x2F; OneDrive &#x2F; SharePoint data of all its North America customers say.Boom we have GPT model that can autonomously run whole businesses. reply naillo 16 hours agoparentprevWell there is that \"transformers\" folder at the bottom of the screenshot... reply lijok 14 hours agoprevI wouldn&#x27;t trust MSFT with my glass of chocolate milk at this point. I would come back to lipstick all over the rim and somehow multiple leaks in the glass reply rickette 18 hours agoprevAt this point MS might as well aquire Wiz, given the number of azure security findings they have found. reply gumballindie 16 hours agoprevWould be cool if someone analysed - i am fairly certain it has proprietary code and data laying around. Would be useful for future lawsuits against microsoft and others that steal people’s ip for “training” purposes. reply madelyn-goodman 16 hours agoprevThis is so unfortunate but a clear illustration of something I&#x27;ve been thinking about a lot when it comes to LLMs and AI. It seems like we&#x27;re forgetting that we are just handing our data over to these companies on a solver platter in the form of our prompts. Disclosure that I do work for Tonic.ai and we are working on a way to automatically redact any information you send to an LLM - https:&#x2F;&#x2F;www.tonic.ai&#x2F;solar reply baz00 15 hours agoprevWhat&#x27;s that, the second major data loss &#x2F; leak event from MSFT recently.Is your data really safe there? reply h1fra 16 hours agoprevThe article is focusing on AI and teams messages for some reason, but the exposed bucket had password, ssh keys, credentials, .env and most probably a lot of proprietary code. I can&#x27;t even imagine the nightmare it has created internally. reply svaha1728 18 hours agoprevEmbrace, extend, and extinguish cybersecurity with AI. It&#x27;s the Microsoft way. reply formerly_proven 18 hours agoprevThis stands out> Our scan shows that this account contained 38TB of additional data — including Microsoft employees’ personal computer backups.Not even Microsoft has functioning corporate IT any more, with employees not just being able to make their own image-based backups, but also having to store them in some random A3 bucket that they&#x27;re using for work files. reply croes 17 hours agoparentWhy not even?Security was never a strong part of Microsoft. reply riwsky 18 hours agoprevIf only Microsoft hadn’t named the project “robust” models transfer, they could have dodged this Hubrisbleed attack. reply bt1a 18 hours agoprevDon&#x27;t get pickled, friends! reply 34679 16 hours agoprev@4mm character width:4e-6 * 3.8e+13 = 152 million kilometers of text.Nearly 200 round trips to the moon. reply fithisux 5 hours agoprevMy opinion is that it was not an \"accident\", but they prepare us for the era where powerful companies will \"own\" our data in the name of security.Should have been sent to prison. reply avereveard 18 hours agoprevOof. Is that containing code from GitHub private repos? reply naikrovek 17 hours agoprevAmazing how ingrained it is in some people to just go around security controls.someone chose to make that SAS have a long expiry and someone chose to make it read-write. reply JohnMakin 10 hours agoparentIt’s easy.“ugh, this thing needs to get out by end of week and I can’t scope this key properly, nothing’s working with it.”“just give it admin privileges and we’ll fix it later”sometimes they’ll put a short TTL on it, aware of the risk. Then something major breaks a few months later, gets a 15 year expiry, never is remediated.It’s common because it’s tempting and easy to tell yourself you’ll fix it later, refactor, etc. But then people leave, stuff gets dropped, and security is very rarely a priority in most orgs - let alone remediation of old security issues. reply EGreg 17 hours agoprevThis seems to be a common occurrence with Big Tech and Big Government, so we better get used to it:https:&#x2F;&#x2F;qbix.com&#x2F;blog&#x2F;2023&#x2F;06&#x2F;12&#x2F;no-way-to-prevent-this-says...https:&#x2F;&#x2F;qbix.com&#x2F;blog&#x2F;2021&#x2F;01&#x2F;25&#x2F;no-way-to-prevent-this-says... reply alphabetting 17 hours agoparentIs this stuff regularly happening to AWS and GCP? This is like the 3rd insane security incident from Microsoft in the past year. reply EGreg 15 hours agorootparenthttps:&#x2F;&#x2F;www.bleepingcomputer.com&#x2F;news&#x2F;security&#x2F;top-secret-us...https:&#x2F;&#x2F;www.engadget.com&#x2F;amp&#x2F;2018-07-18-robocall-exposes-vot...Ok so it’s not Microsoft exposing Microsoft, but government exposing its S3 buckets.The question should be — why is all that data and power concentrated in one place? Because of the capitalist system and Big Tech, or Big Government.Personally I am rather happy when “top secret information” is exposed, because that I s the type of thing that harms people around the world more than it helps. The government wants to know who is sending you $600 but doesnt want to tell you how they spent trillions on shadowy “defense” contractors.https:&#x2F;&#x2F;community.qbix.com&#x2F;t&#x2F;transparency-in-government&#x2F;234 reply endisneigh 19 hours agoprevhow is this sort of stuff not at least encrypted at rest? reply tremon 18 hours agoparentEncryption at rest does nothing to prevent online access to data. It&#x27;s only useful if you leave your storage cabinet standing on the side of the road. reply quickthrower2 17 hours agorootparentYour laptop backup could be encrypted. New problem: where to out the keys. Maybe another storage account with different access controls. reply pixl97 14 hours agorootparent> New problem: where to out the keys.If it&#x27;s windows, Active Directory. reply Smaug123 18 hours agoparentprevPer the article, the Azure bucket was explicitly shared. Azure Storage is generally encrypted at rest (https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;azure&#x2F;storage&#x2F;common&#x2F;stora...). reply nightpool 18 hours agoparentprevWhat do you think \"encryption at rest\" means reply mymac 18 hours agoprevFortunately not a whole of of data and for sure with a little bit like that there wasn&#x27;t anything important, confidential or embarrassing in there. Looking forward to Microsoft&#x27;s itemised list of what was taken, as well as their GDPR related filing. reply Nischalj10 18 hours agoprevzsh, any way to download the stuff? reply EMCymatics 17 hours agoprevThat&#x27;s a lot of data. reply munchler 18 hours agoprev> This case is an example of the new risks organizations face when starting to leverage the power of AI more broadly, as more of their engineers now work with massive amounts of training data.It seems like a stretch to associate this risk with AI specifically. The era of \"big data\" started several years before the current AI boom. reply numbsafari 18 hours agoparentThis is the risk of using, checks notes, Azure and working with Microsoft.Except there is no risk for them. They&#x27;ve proven time and again they have major security snafus and not be held accountable. reply eddythompson80 18 hours agorootparentVirtual networks are a nightmare to setup and manage in Azure which is why everyone just takes the easy path and not bother.Almost every Azure service we deal with has virtual networks as an after thought because they want to get to market as quickly as possible, and even to them managing vnets is a nightmare.Not to excuse developers&#x2F;users though. There are plenty of unsecured S3 buckets, docker containers, and Github repos that expose too much \"because it&#x27;s easier\". I&#x27;ve had a developer checkin their ftp creds into a repo the whole company has access to. He even broke the keys up and concat them in shell to work around the static checks \"because it&#x27;s easier\" for their dev&#x2F;test flow. reply robertlagrant 18 hours agorootparentprevThey have all the regulatory paperwork in place, so it must be fine. reply datavirtue 18 hours agorootparentThey are also the top line investment for the majority of mutual and pension funds. Don&#x27;t crab too much, they are funding your retirement. reply intrasight 18 hours agoparentprevAgreed. It should say \"new risks organizations face when starting to leverage the power of Azure\" or \"the power of cloud computing\". But as clickbait worthy a title. reply acdha 17 hours agoparentprevThe second clause covers that: this isn’t an AI problem, just as it wasn’t a big data problem when the same kinda of things happened a decade ago. It’s a problem caused when you set up something new outside of what the organization is used to and have people without appropriate training asked to make security decisions: I’d bet that this work was being done by people who were used to the academic style, blending personal and corporate use on the same device, etc. and simply weren’t thinking of this class of problem. The description sounds a lot like the grad students & postdocs I used to support – you’d see some dude with Steam on his workstation because it faster than his laptop and since he was in the lab 70 hours a week anyway, why not 90?The challenge for organizations is figuring out how to support research projects and other experiments without opening themselves up to this kind of problem or stymieing R&D. reply omgJustTest 18 hours agoparentprevThis comment is a good bit of rationalization, and whichever the categorical mismatch you feel is happening, it misses the overarching point, the focus should be on the broader systemic issues: data security is not a first or second tier priority to \"big data\" or \"AI\"... largely because there&#x27;s no cost to doing it poorly. reply mavhc 18 hours agoparentprevWith big data comes big responsibility reply Phileosopher 18 hours agoparentprevAI has magnified the use cases, though. Before, Big Data was an advertising machine meant to tokenize and market to every living being on the planet. Now, machine learning can create \"averaged\" behavior of just about anything, given enough data and specificity. reply buro9 19 hours agoprevPart of me thought \"this is fine as very few could actually download 38TB\".But that&#x27;s not true as it&#x27;s just so cheap to spin up a machine and some storage on a Cloud provider and deal with it later.It&#x27;s also not true as I&#x27;ve got a 1Gbps internet connection and 112TB usable in my local NAS.All of a sudden (over a decade) all the numbers got big and massive data exfiltration just looks to be trivial.I mean, obviously that&#x27;s the sales pitch... you need this vendor&#x27;s monitoring and security, but that&#x27;s not a bad sales pitch as you need to be able to imagine and think of the risk to monitor for it and most engineers aren&#x27;t thinking that way. reply sillysaurusx 18 hours agoparentHow do you have your NAS configured? The more specifics, the better; I’ve wanted one.Do you worry about failure? In your hardware life I mean, not your personal life. reply NikolaNovak 18 hours agorootparentNot the OP, but after a lot of messing with software software and OS RAID, Raid Cards and mother boards, dedicated loud Dell servers, UnRAID, this that and the other thing over years and decades, I just set up a big Synology device 5 years ago. Since then, I&#x27;ve had a NAS that just worked. I have data, it&#x27;s there.I do online backup to a cloud provider, and a monthly dump to external USB drives that I keep and rotate at my mother in law&#x27;s house (off site:).More than any technical advice, I&#x27;d strongly urge you to check and understand honestly whether you&#x27;re looking for \"NAS\" (a place to seamlessly store data) or \"a project\" (something to spend fun and frustrating and exciting evening and weekend time configuring, upgrading, troubleshooting, changing, re-designing, replacing, blogging, etc). Nothing wrong with either, just ensure you pick the path you actually want :-> reply sillysaurusx 14 hours agorootparentWhich model Synology do you have? (Would you still make the same choice today?)Did you settle on using RAID, or just rely on cloud backups? reply NikolaNovak 14 hours agorootparentI have the DS918+I would not make the same choices today: I got a somewhat high end one and upgraded it to whopping 32GB of RAM, thinking I&#x27;d use it for running lightweight containers or VMs, and maybe a media server. But once I put all my data on it... including 20 years of family photos and tax prep documents and work stuff and everything else... I changed my mind and am using it only and solely as an internal storage unit. Basically, as mentioned, committed to the \"NAS\" as opposed to \"Fun Project\" path :-). So I could&#x27;ve saved myself some money by getting a simpler unit and not upgrading it. (the DS918+ also can hook up to a cage [DX517], but I ended up not needing that either, yet).I have it with 4 WD Red Plus NAS 8TTB drives and RAID 10 currently. I&#x27;ve used RAID 5 in the past but decided against it for this usage - again, went for simplicity.Just shy of 30,000 hours on the drives, daily usage (I basically don&#x27;t use local drive for any data on any of my computers; I keep it all on NAS and this way I can use any of my computers to do&#x2F;access the same thing), and really no issues whatsoever so far. reply aftbit 18 hours agorootparentprevNot the OP but I have a pair of Chenbro NR12000 1U rack mount servers, bought for about $120 each on eBay a few years ago. Each has 12 internal 3.5\" mounting points and 14 SATA cables. In one server, I have 12 4TB used enterprise drives. In the other, I have 12 8TB drives. Both have 16 GB of RAM (should probably be more) and two 2.5\" SATA SSDs. They are configured with two ZFS raidz1 vdevs, each made up of 6 disks. This gives me 10 usable disks and 2 used for parity, and the ability to survive at least one failure but maybe two (if I&#x27;m lucky).I back up critical data from the 80TB NAS to the 40TB NAS, and the most critical data gets backed up nightly to a single hard drive in my friend&#x27;s NAS box (offsite). Twice a year, I back up the full thing to external hard drives and take them out of state to a different friend&#x27;s house.Don&#x27;t worry, be happy. reply sillysaurusx 18 hours agorootparent(Where are you finding friends with a NAS? Or at all, for that matter… guess I’ll look on eBay.)Thank you for the details, particularly about zfs, which I know nothing about. The “if I’m lucky” part piqued my interest. HN was recently taken down by a double disk failure, which is exponentially more likely when you buy drives in bulk - the default case. So being able to survive two failures simultaneously is something I’d like to design for.It’s cool you have two NASes (NASen?) let alone one. They’re the Pokémon of the tech world. reply aftbit 17 hours agorootparentAh my tech friends have specialized into hardware a bit. At least two of us have server racks in our basement, and basically nobody I know (who at least knows the command line) does not have at least a few drives in an old Linux server somewhere.If you are concerned about reliability above performance, I would suggest using a single raidz2 vdev instead. This would allow the cluster to definitely survive two disks worth of failure. I&#x27;ll also echo the common mantra - RAID is not backups. If you really need the data, you need to store a second copy offline in a different place.When I lived in California and did not have room for a server rack, I had a single home server with an 8-bay tower case. I used an LSI card with 2 SAS-to-4x-SATA ports to connect all 8 drives to the machine. I believe I had 6 TB drives in that NAS, though they are currently all out of my house (part of one of my offsite backups now). My topology there was 4x mirror vdevs, which gave me worst case endurance of 1 failure but best case of 4 failures, and at about 4x the IOPS performance, but with the cost of only 50% storage efficiency vs the 75% you would get with raidz2.There is even raidz3 if you are very paranoid, which allows up to 3 disks to fail before you lose the vdev. I&#x27;ve never used it. As I understand, the parity calculations get considerably more complicated, although I don&#x27;t know if that really matters. reply zuminator 18 hours agorootparentprevInteresting. It&#x27;s been a while since I&#x27;ve used eBay, but man they&#x27;ve really upped their game if you can buy friends there now. reply fnordpiglet 17 hours agorootparentOP was pulling your leg a bit. Clearly the only friends folks like us have with NAS are the friends here on HN posting about their NAS. reply 2f0ja 18 hours agorootparentprevWhat are you criteria for used enterprise drives? I&#x27;m wading into building a nas (well.. it&#x27;s more of a &#x27;project&#x27; nas as an above comment would say) and I&#x27;m getting a little lost in the sauce about drives. reply aftbit 17 hours agorootparentI just bought the cheapest \"Grade A\" drives I could find from eBay. This is not the reliable way to do it, but as I have a 3 layer backup solution anyway, I don&#x27;t really mind the risk of a drive failure.It depends on what your plans for the storage are. If you&#x27;re going to fill it with bulk data that gets accessed sequentially (think media files), then performance will be fine with basically any topology or drive choice. If you are going to fill it with data for training ML models across multiple machines, you need to think about how you will make it not the bottleneck for your setup.One more thing to consider - you can get new consumer OR used enterprise flash for somewhere around $45&#x2F;TB in the 4 TB SATA size, or the 8 TB NVMe size. Those drives will likely fail read-only if they fail at all. They will usually use less power, take less space, and obviously will perform orders of magnitude better than spinning rust, at somewhere around 3x the cost.I am hoping to build my next NAS entirely on flash. reply fnordpiglet 17 hours agorootparentprevI use a Ubuntu raspberry pi with a cheap usb3 jbod array from Amazon that can hold 5 HDD. I use zfs on it in raidz1. It’s absurdly cheap, can serve about 80 Mb&#x2F;s on a 1 gbps link, and is entirely sufficient for local backup. I don’t do any offsite. Set up to back up time machine, windows, and zrepl. Runs other services on the pi as well for the home network.It’s so easy to set up an Ubuntu image that I control completely and I would rather do that than run some questionable 3rd party NAS solution and excluding disks costs about $130. reply buro9 17 hours agorootparentprevI just have a Synology DS1821+ which has (8 * HDD bays) + (2 * M2 slots). The bays I&#x27;ve filled with 18TB HDDs (I chose Toshiba N300 as they do not use SMR). The M2 slots I&#x27;ve put a couple of 1TB M2 drives in as an SSD cached (they better allow the HDDs to hibernate for frequently accessed files like music).I&#x27;ve got these in an SHR configuration (Synology Hybrid Raid with 1 disk of protection) which means about 115-6TB of usable space and allowing for single drive failure.The filesystem is BTRFS ( https:&#x2F;&#x2F;daltondur.st&#x2F;syno_btrfs_1&#x2F; ).I upgraded the RAM (Synology will forever nag about it not being their RAM https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;synology&#x2F;comments&#x2F;kaq7ks&#x2F;how_to_dis... ).I have the option in future to purchase the network card to take that to 10Gbps ports rather than 1Gbps ports.So that&#x27;s the first... but then I have a second one... which is an older DS1817+ which is filled with 10TB HDDs and yields 54.5TB usable in SHR2 + BTRFS... which I use as a backup to the first, but as it&#x27;s smaller just the really important stuff and it is disconnected and powered down mostly, it&#x27;s a monthly chore to connect it, and rsync things over. Typically if I want to massively expand a NAS (every - 10 years) I will buy a whole new one and relegate the existing to be a backup device. Meaning an enclosure has on avg about 15y of life in it and amortises really well as being initially the primary, and then later the backup.I do _not_ use any of the Synology software, it&#x27;s just a file system... I prefer to keep my NAS simple and offload any compute to other small devices&#x2F;machines. This is in part because of the length of time I keep these things in service... the software is nearly always the weakest link here.You can build your own NAS, TrueNAS Core (nee FreeNAS) https:&#x2F;&#x2F;www.truenas.com&#x2F;freenas&#x2F; is very good... but for me, a NAS is always on and the low power performance of this purpose built devices and their ability to handle environmental conditions (I am not doing anything special for cooling, etc) and the long-term updates to the OS, etc... makes it quite compelling. reply daggersandscars 18 hours agorootparentprevNot the original poster, but to add my experience:Two-bay NAS, two drives as a mirrored pair, two SSDs as mirrored pair cache. Only makes data available on my home network. Primarily using Nextcloud and Gitea.It backs up important files nightly to a USB-attached drive, less critical files weekly. I have a weekly backup to a cloud provider for critical files.A sibling comment makes a good point: do you want a hobby or an appliance? Using a commercial NAS makes it closer to an appliance[0]. Building it yourself will likely require more fiddling.If you want to run a different OS on a commercial NAS, dig deeper into the OS requirements before buying a the NAS. Asustor Lockerstor Gen 2 series&#x27; fan is not inherently supported by things other than Asustor&#x27;s software.[0] A commercial NAS will still require monitoring, maintenance, and validation of backups. reply darknavi 18 hours agorootparentprevUnraid is a pretty friendly OS with easy disk adoption and nice gui for managing docker containers.You can have up to two disks of redundancy (dual parity) per drive pool. reply int0x2e 16 hours agoparentprevIt&#x27;s much worse - if the data isn&#x27;t just a ton of tiny files, and you&#x27;re able to spin up a bunch of workers for parallelism, you can get up to 120 Gbps per storage account (without going to the extreme of requiring a special quota increase).That means in a little bit over 5 minutes, the data could have been downloaded by someone. Even most well run security teams won&#x27;t be able to respond quickly enough for that type of event. reply koolba 18 hours agoparentprevAt the rack rates of $.05&#x2F;GB, that’d come out to $1,945 per copy that’s downloaded. So not only do you have the breach, you also have a fat bill too. reply redox99 18 hours agorootparent> $.05&#x2F;GBThat&#x27;s just a scam rate by AWS. The true price is 1&#x2F;100th of that, if that. reply spullara 15 hours agoparentprevNot really a sales pitch as it wasn&#x27;t discovered by their product but rather by their security team doing a bunch of manual work. reply zooFox 18 hours agoparentprevThe article mentions that it wasn&#x27;t a read-only token, meaning you could at least edit and delete files too. reply byteknight 19 hours agoparentprevTrivial in a technical sense but monitoring capabilities (hopefully) have increased in kind. reply permo-w 18 hours agoparentprevwith a 1Gbps connection you&#x27;re still looking at ~248 hours to download, and that&#x27;s if the remote server can keep up, which it almost certainly can&#x27;tthis is assuming by 1Gbps you mean 1 Gigabit&#x2F;s rather than 1 Gigabyte&#x2F;s reply mlyle 18 hours agorootparentNot sure where 248 hours came from.38 terabytes = 304 terabits.304 terabits &#x2F; 1 gigabit&#x2F;second = 304,000 seconds304,000 seconds =~ 84 hours. Add 20% for not pegging the line the whole time and the limits of 1gbps ethernet, and perhaps 100 hours is reasonable. reply permo-w 2 hours agorootparentmy mistake, I swapped the 38tb and 112tb from parent commentwhatever the download size is, you&#x27;re bottlenecked by the remote server&#x27;s up speed reply flakeoil 18 hours agorootparentprevBut you don&#x27;t need to download everything. Even 1&#x2F;10th of that could be juicy enough. Or 1&#x2F;100th. reply ltbarcly3 18 hours agoparentprevAgree, this is extremely dubious:5gbps and 10gbps residential fiber connections are common now.12TB hd&#x27;s cost under $100, so you would only need about $400 of storage to capture this, my SAN has more capacity than this and I bought basically the cheapest disks I could for it.It only takes one person to download it and make a torrent for it to be spread arbitrarily.People could target more interesting subsets over less interesting parts of the data.Multiple downloaders could share what they have and let an interested party assemble what is then available. reply HumblyTossed 18 hours agoprevMicrosoft, too big to fa.. care. reply anyoneamous 17 hours agoprev [–] Straight to jail. reply 1-6 17 hours agoparent [–] Nah, Microsoft probably has a blameless culture reply croes 17 hours agorootparent [–] It was hackers, for sure. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Microsoft's AI researchers have unintentionally exposed 38 terabytes of private data, including backups, passwords, and messages, due to a misconfiguration of Shared Access Signature (SAS) tokens.",
      "The incident underscores the security risks that can arise from managing immense volumes of training data in AI projects, particularly given Microsoft's lack of a centralized way to manage these tokens.",
      "The post recommends using alternative methods for external data sharing and prioritizing cloud security in AI development. Solutions like FortiGate Next-Generation Firewall (NGFW) and Wiz are suggested for monitoring and securing cloud environments."
    ],
    "commentSummary": [
      "The discussions encompass various elements of cybersecurity including the requirement for safe serialization methods in AI models, the significance of comprehensive penetration testing and concerns regarding Azure's security measures.",
      "Other topics of discussion include the risks of using outdated technology, especially when dealing with NAS devices, highlighting the need for consistent upgrades and updates.",
      "Encryption and data breaches remain pressing subjects, drawing attention to the complexities of managing vast volumes of data, thus reinforcing the critical role of data protection."
    ],
    "points": 684,
    "commentCount": 216,
    "retryCount": 0,
    "time": 1695047409
  },
  {
    "id": 37558357,
    "title": "HyperDX – open-source dev-friendly Datadog alternative",
    "originLink": "https://github.com/hyperdxio/hyperdx",
    "originBody": "Hi HN, Mike and Warren here! We&#x27;ve been building HyperDX (hyperdx.io). HyperDX allows you to easily search and correlate logs, traces, metrics (alpha), and session replays all in one place. For example, if a user reports a bug “this button doesn&#x27;t work,\" an engineer can play back what the user was doing in their browser and trace API calls back to the backend logs for that specific request, all from a single view.Github Repo: https:&#x2F;&#x2F;github.com&#x2F;hyperdxio&#x2F;hyperdxComing from an observability nerd background, with Warren being SRE #1 at his last startup and me previously leading dev experience at LogDNA&#x2F;Mezmo, we knew there were gaps in the existing tools we were used to using. Our previous stack of tools like Bugsnag, LogRocket, and Cloudwatch required us to switch between different tools, correlate timestamps (UTC? local?), and manually cross-check IDs to piece together what was actually happening. This often made meant small issues required hours of frustration to root cause.Other tools like Datadog or New Relic come with high price tags - when estimating costs for Datadog in the past, we found that our Datadog bill would exceed our AWS bill! Other teams have had to adjust their infrastructure just to appease the Datadog pricing model.To build HyperDX, we&#x27;ve centralized all the telemetry in one place by leveraging OpenTelemetry (a CNCF project for standardizing&#x2F;collecting telemetry) to pull and correlate logs, metrics, traces, and replays. In-app, we can correlate your logs&#x2F;traces together in one panel by joining everything automatically via trace ids and session ids, so you can go from log <> trace <> replay in the same panel. To keep costs low, we store everything in Clickhouse (w&#x2F; S3 backing) to make it extremely affordable to store large amounts of data (compared to Elasticsearch) while still being able to query it efficiently (compared to services like Cloudwatch or Loki), in large part thanks to Clickhouse&#x27;s bloom filters + columnar layout.On top of that, we&#x27;ve focused on providing a smooth developer experience (the DX in HyperDX!). This includes features like native parsing of JSON logs, full-text search on any log or trace, 2-click alert creation, and SDKs that help you get started with OpenTelemetry faster than the default OpenTelemetry SDKs.I&#x27;m excited to share what we&#x27;ve been working with you all and would love to hear your feedback and opinions!Hosted Demo - https:&#x2F;&#x2F;api.hyperdx.io&#x2F;login&#x2F;demoOpen Source Repo: https:&#x2F;&#x2F;github.com&#x2F;hyperdxio&#x2F;hyperdxLanding Page: https:&#x2F;&#x2F;hyperdx.io",
    "commentLink": "https://news.ycombinator.com/item?id=37558357",
    "commentBody": "HyperDX – open-source dev-friendly Datadog alternativeHacker NewspastloginHyperDX – open-source dev-friendly Datadog alternative (github.com/hyperdxio) 600 points by mikeshi42 17 hours ago| hidepastfavorite134 comments Hi HN, Mike and Warren here! We&#x27;ve been building HyperDX (hyperdx.io). HyperDX allows you to easily search and correlate logs, traces, metrics (alpha), and session replays all in one place. For example, if a user reports a bug “this button doesn&#x27;t work,\" an engineer can play back what the user was doing in their browser and trace API calls back to the backend logs for that specific request, all from a single view.Github Repo: https:&#x2F;&#x2F;github.com&#x2F;hyperdxio&#x2F;hyperdxComing from an observability nerd background, with Warren being SRE #1 at his last startup and me previously leading dev experience at LogDNA&#x2F;Mezmo, we knew there were gaps in the existing tools we were used to using. Our previous stack of tools like Bugsnag, LogRocket, and Cloudwatch required us to switch between different tools, correlate timestamps (UTC? local?), and manually cross-check IDs to piece together what was actually happening. This often made meant small issues required hours of frustration to root cause.Other tools like Datadog or New Relic come with high price tags - when estimating costs for Datadog in the past, we found that our Datadog bill would exceed our AWS bill! Other teams have had to adjust their infrastructure just to appease the Datadog pricing model.To build HyperDX, we&#x27;ve centralized all the telemetry in one place by leveraging OpenTelemetry (a CNCF project for standardizing&#x2F;collecting telemetry) to pull and correlate logs, metrics, traces, and replays. In-app, we can correlate your logs&#x2F;traces together in one panel by joining everything automatically via trace ids and session ids, so you can go from logtracereplay in the same panel. To keep costs low, we store everything in Clickhouse (w&#x2F; S3 backing) to make it extremely affordable to store large amounts of data (compared to Elasticsearch) while still being able to query it efficiently (compared to services like Cloudwatch or Loki), in large part thanks to Clickhouse&#x27;s bloom filters + columnar layout.On top of that, we&#x27;ve focused on providing a smooth developer experience (the DX in HyperDX!). This includes features like native parsing of JSON logs, full-text search on any log or trace, 2-click alert creation, and SDKs that help you get started with OpenTelemetry faster than the default OpenTelemetry SDKs.I&#x27;m excited to share what we&#x27;ve been working with you all and would love to hear your feedback and opinions!Hosted Demo - https:&#x2F;&#x2F;api.hyperdx.io&#x2F;login&#x2F;demoOpen Source Repo: https:&#x2F;&#x2F;github.com&#x2F;hyperdxio&#x2F;hyperdxLanding Page: https:&#x2F;&#x2F;hyperdx.io addisonj 15 hours agoWow, there is a lot here and what here is to a pretty impressive level of polish for how far along this is.The background of someone with a DX background comes through! I will be looking into this a lot more.Here are a few comments, notes, and questions:* I like the focus on DX (especially compared to other OSS solutions) in your messaging here, and I think your hero messaging tells that story, but it isn&#x27;t reinforced as much through the features&#x2F;benefits section* It seems like clickhouse is obviously a big piece of the tech here, which is an obvious choice, but from my experience with high data rate ingest, especially logs, you can run into issues at larger scale. Is that something you expect to give options around in open source? Or is the cloud backend a bit different where you can offer that scale without making open source so complex?* I saw what is in OSS vs cloud and I think it is a reasonable way to segment, especially multi-tenancy, but do you see the split always being more management&#x2F;security features? Or are you considering functional things? Especially with recent HashiCorp \"fun\" I think more and more it is useful to be open about what you think the split will be. Obviously that will evolve, but I think that sort of transparency is useful if you really want to grow the OSS side* on OSS, I was surprised to see MIT license. This is full featured enough and stand alone enough that AGPL (for server components) seems like a good middle ground. This also gives some options for potentially a license for an \"enterprise\" edition, as I am certain there is a market for a modern APM that can run all in a customer environment* On that note, I am curious what your target persona and GTM plan is looking like? This space is a a bit tricky IMHO, because small teams have so many options at okay price points, but the enterprise is such a difficult beast in switching costs. This looks pretty PLG focused atm, and I think for a first release it is impressive, but I am curious to know if you have more you are thinking to differentiate yourself in a pretty crowded space.Once again, really impressive what you have here and I will be checking it out more. If you have any more questions, happy to answer in thread or my email is in profile. reply mikeshi42 14 hours agoparentThank you, really appreciate the feedback and encouragement!> It seems like clickhouse is obviously a big piece of the tech here, which is an obvious choice, but from my experience with high data rate ingest, especially logs, you can run into issues at larger scale. Is that something you expect to give options around in open source?Scaling any system can be challenging - our experience so far is that Clickhouse is a fraction of the overhead of systems like Elasticsearch has previously demanded luckily. That being said, I think there&#x27;s always going to be a combination of learnings we&#x27;d love to open source for operators that are self-hosting&#x2F;managing Clickhouse, and tooling we use internally that is purpose-built for our specific setup and workloads.> I saw what is in OSS vs cloud and I think it is a reasonable way to segment, especially multi-tenancy, but do you see the split always being more management&#x2F;security features?Our current release - we&#x27;ve open sourced the vast majority of our feature set, including I think some novel features like event patterns that typically are SaaS-only and that&#x27;ll definitely be the way we want to continue to operate. Given the nature of observability - we feel comfortable continuing to keep pushing a fully-featured OSS version while having a monetizable SaaS that focuses on the fact that it&#x27;s completely managed, rather than needing to gate heavily based on features.> on OSS, I was surprised to see MIT licenseWe want to make observability accessible and we think AGPL will accomplish the opposite of that. While we need to make money at the end of the day - we believe that a well-positioned enterprise + cloud offering is better suited to pull in those that are willing to pay, rather than forcing it via a license. I also love the MIT license and use it whenever I can :)> On that note, I am curious what your target persona and GTM plan is looking like?I think for small teams, imo the options available are largely untantilizing, it ranges from narrow tools like Cloudwatch to enterprise-oriented tools like New Relic or Datadog. We&#x27;re working hard to make it easier for those kinds of teams to adopt good monitoring and observability from day 1, without the traditional requirement of needing an observability expert or dedicated SRE to get it set up. (Admittedly, we still have a ways to improve today!) On the enterprise side, switching costs are definitely high, but most enterprises are highly decentralized in decision making, where I routinely hear F500s having a handful of observability tools in production at a given time! I&#x27;ll say it&#x27;s not as locked-in as it seems :) reply addisonj 13 hours agorootparentThanks for the answers Mike!One more follow-up on the scale side (which I mentioned with sibling comment), it isn&#x27;t so much about clickhouse itself, but about scaling up ingest. From my own experience and from talking with quite a few APM players (I previously worked in streaming space), a Kafka &#x2F; durable log storage kind of becomes a requirement, so I was curious if you think at some point you need a log to further scale ingest.For enterprise side, I was previously in data streaming space and had quite a few conversations with APM players and companies building their own observability platforms, happy to chat and share more if that would be useful! reply mikeshi42 13 hours agorootparentAh got it, yeah a queue of some sort is definitely useful when scaling up to buffer pre-inserted data. This is something on the OSS side we&#x27;ve kept open to implementation. However it&#x27;s something that is highly coupled with infra footprint and internal SLA guarantees the user wants to preserve. It can range anywhere from just rely on client-side retries to setting up a HA Kafka cluster early in the ingestion pipeline.Similar to Elastic - I think a lot of architectures are available to choose on that side when users want to scale.Will reach out to connect! reply debarshri 8 hours agorootparentprevOne piece of advice here is, if you pitch yourself as a datadog competitor, then I would recommend replicating some of the GTM motions that datadog employed. For instance, you have an opportunity to go very upmarket, super enterprise orgs. You can do PLG, but ultimately every tool becomes SLG. I would recommend fine tuning that motion as that would be the one bringing larger contract 6 digit contracts and huge growth here.I have seen orgs remove datadog because of unpredictable pricing. If you do flat price self hosted platform, you will get attention. I dont think orgs would mind hosting clickhouse. You can also bundle it with your helm charts or initial proof of concept might have lower barrier. I know some orgs have million dollar annual contracts with datadog, a cheaper more predictable priced alternative will definitely get attention. reply mikeshi42 6 hours agorootparentThank you - I think that&#x27;s definitely an interesting idea for us to go down for sure! We&#x27;ve heard a ton that the unpredictable (and insane) costs of Datadog is forcing teams to move off in droves. Something that strikes the balance between more expensive hosted solution vs cheaper but self hosted might definitely a interesting angle to try. reply tmd83 2 hours agorootparentprevhttps:&#x2F;&#x2F;www.hyperdx.io&#x2F;docs&#x2F;oss-vs-cloudThis page shows event pattern available for both oss vs. cloud. The blog doesn&#x27;t mention exactly how this is being which would be an interesting read but I understand if a secret sauce.I recall quite a few years ago a standalone commercial & hosted tool for doing something like this just on logs for anomaly detection. Anyone has any reference for similar tools for working with direct log data (say from log files) or in a similar capacity like hypderdx (oss or commercial) reply datadeft 3 hours agorootparentprev> While we need to make money at the end of the dayHonest question: What makes you think that you are not turning into a Datadog (price wise) once reach a certain scale?The problem what I see with software companies that the pricing is dominated by investor requirements and when a company reaches a certain milestone change up the licensing model and the pricing with it. reply mikeshi42 2 hours agorootparentIt&#x27;s a classic innovator&#x27;s dilemma - if&#x2F;when we get there - it&#x27;d be a bit naïve of us to assume the next HyperDX isn&#x27;t around the corner :) Anyone that believed in us on the way up - certainly has to believe that the same mistake will bring us down.I&#x27;d also add that I don&#x27;t think all services trend their price upwards. AWS has historically lowered prices on services and continue to offer new service-tiers with lower prices (S3 tiering as an example). As the tech matures and costs fall for our service as well, it&#x27;d be surprising if we don&#x27;t do the same. reply mx20 3 hours agorootparentprevMIT License allows Amazon and other Cloud providers to offer Cloud Solutions as well. That&#x27;s why most SaaS changed to AGPL or better versions that explicitly disallow Cloud offerings. reply dangoodmanUT 14 hours agoparentprevFor clickhouse, just batch insert. They probably have something batching every few s before inserting directly to their hosted version reply vadman97 12 hours agorootparentClickHouse Async insert docs [1].We ran into some challenges with async inserts at highlight.io [2]. Namely, ClickHouse Cloud has an async flush size configured (that can&#x27;t be changed AFAIK) that isn&#x27;t large enough for our scale. Once you async insert more than can be flushed, you get back pressure on your application waiting to write while Clickhouse flushes the queue. We found that implementing our own batched flushing via kafka [3] is far more performant, allowing us to insert 500k+ RPS on the smallest cloud instance type.[1] https:&#x2F;&#x2F;clickhouse.com&#x2F;docs&#x2F;en&#x2F;optimize&#x2F;asynchronous-inserts [2] https:&#x2F;&#x2F;github.com&#x2F;highlight&#x2F;highlight&#x2F;tree&#x2F;main [3] https:&#x2F;&#x2F;github.com&#x2F;highlight&#x2F;highlight&#x2F;blob&#x2F;4d28451b1935796d... reply addisonj 13 hours agorootparentprevGenerally, any sort of async&#x2F;batch inserts will get you decently far, but still will have limitations well before you get to million rows a second, mostly because it is really difficult to get your batch size large enough from individual producers without some sort of aggregation, which that aggregation is a challenge if you care about durability.So often that means you need something like a Kafka to get the bulk ingest to really perform to get batch sizes large enough.That kind of gets into one of the challenges of OSS observabilility systems, you don&#x27;t want to make the dependencies insane for someone who only has a few thousand logs a second, but generally at some point of scale you do need more. reply dangoodmanUT 14 hours agorootparentprevThere&#x27;s also async inserts reply fnord77 11 hours agoparentprevClickhouse is proprietary, though.I wonder why not Apache Druid reply zx8080 10 hours agorootparent> Clickhouse is proprietaryNo. Clickhouse is opensource with Apache License [0].[0] - https:&#x2F;&#x2F;github.com&#x2F;ClickHouse&#x2F;ClickHouse&#x2F;blob&#x2F;master&#x2F;LICENSE reply prabhatsharma 9 hours agoprevA good one. A lot is being built on top of clickhouse. I can count at least 3 if not more (hyperdx, signoz and highlight) built on top of clickhouse now.We at OpenObserve are solving the same problem but a bit differently. A much simpler solution that anyone can run using a single binary on their own laptop or in a cluster of hundreds of nodes backed by s3. Covers logs, metrics, traces, Session replay, RUM and error tracking are being released by end of the month) - https:&#x2F;&#x2F;github.com&#x2F;openobserve&#x2F;openobserve reply hu3 9 hours agoparenthttps:&#x2F;&#x2F;github.com&#x2F;uptrace also uses ClickHouse reply francislavoie 9 hours agoparentprevSentry as well https:&#x2F;&#x2F;blog.sentry.io&#x2F;introducing-snuba-sentrys-new-search-... reply t1mmen 13 hours agoprevThis looks really cool, congrats on the launch!I haven’t had time to dig in proper, but this seems like something that would fit perfectly for “local dev” logging as well. I struggled to find a good solution for this, ending up Winston -> JSON, with a simpler “dump to terminal” script running.(The app I’m building does a ton of “in the background” work, and I wanted to present both “user interactions” and “background worker” logs in context)I don’t see Winston being supported as a transport, but presumably easy to add&#x2F;contribute.Good luck! reply mikeshi42 13 hours agoparentThank you! We do support Winston (docs: https:&#x2F;&#x2F;www.hyperdx.io&#x2F;docs&#x2F;install&#x2F;javascript#winston-trans...) and use it a lot internally. Let me know if you run into any issues with it (or have suggestions on how to make it more clear)In fact this is actually how we develop locally - because even our local stack is comparatively noisy, we enable self-logging in HyperDX so our local logs&#x2F;traces go to our own dev instance, and we can quickly trace a 500 that way. (Literally was doing this last night for a PR I&#x27;m working on). reply t1mmen 11 hours agorootparentOh sweet! I was in a bit of a hurry and must’ve missed it, thanks for clarifying. This will be super helpful for us, very excited play with it! reply mikeshi42 11 hours agorootparentNo worries - excited to hear what you think! Feel free to drop by our discord if you run into any issues or have any other feedback as well :) reply corytheboyd 8 hours agoprevOutside of the intended use-case of _replacing_ Datadog, I think this may actually serve as an excellent local development \"Datadog Lite\", which I have always wanted, and is something embarrassingly, sorely missing from local development environments.In local development environments, I want to:- Verify that tracing and metrics (if you use OpenTelemetry) actually work as intended (through an APM-like UI).- Have some (rudimentary, even) data aggregation and visualization tools to test metrics with. You often discover missing&#x2F;incorrect metrics by just exploring aggregations, visualizations, filters. Why do we accept that production (or rather, a remote deployment watched by Datadog etc.) is the correct place to do this? It&#x27;s true that unknowns are... unknown, but what better time to discover them than before shipping anything at all?- Build tabular views from structured logs (JSON). It is _mind blowing_ to me that most people seem to just not care about this. Good use of structured logging can help you figure out in seconds what would take someone else days.I mean, that&#x27;s it, the bar isn&#x27;t too high lol. It looks like HyperDX may do... all of this... and very well, it seems?!Before someone says \"Grafana\"-- no. Grafana is such a horrible, bloated, poorly documented solution for this (for THIS case. NOT IN GENERAL!). It needs to be simple to add to any local development stack. I want to add a service to my docker compose file, point this thing at some log files (bonus points for some docker.sock discoverability features, if possible), expose a port, open a UI in my browser, and immediately know what to do given my Datadog experience. I&#x27;m sure Grafana and friends are great when deployed, but they&#x27;re terrible to throw into a project and have it just work and be intuitive. reply mikeshi42 7 hours agoparentYes! We definitely do - in fact this is how we develop locally, our local stack is pretty intricate and can fail in different areas, so it&#x27;s pretty nice for us to be able to debug errors directly in HyperDX when we&#x27;re developing HyperDX!Otel tracing works and should be pretty bulletproof - metrics is still early so you might see some weirdness (we&#x27;ll need to update the remaining work we&#x27;ve identified in GH issues)You can 100% build tabular views based on JSON logs, we auto-parse JSON logs and you can customize the search table layout to include custom properties in the results table.Let us know if we fulfill this need - we at least do this ourselves so I feel pretty confident it should work in your use case! If there&#x27;s anything missing - feel free to ping us on Discord or open an issue, we&#x27;d likely benefit from any improvement ideas ourselves while we&#x27;re building HyperDX :)Edit: Oh I also talk a bit about this in another comment below https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37561358 reply mikeshi42 4 hours agorootparentSince my comment is too old to edit now - musing on this a bit more I think this would be pretty awesome to turn into a well-supported workflow to have a low-resource-usage&#x2F;all-in-one version for just local development.If anyone wants to chat more about this - I&#x27;ve kicked off an issue [1] to gather interest and everyone&#x27;s feedback.[1] https:&#x2F;&#x2F;github.com&#x2F;hyperdxio&#x2F;hyperdx&#x2F;issues&#x2F;7 reply carlio 8 hours agoparentprevI use InfluxDB for this, it comes with a frontend UI and you can configure Telefraf as a statsd listener, so the same metric ingestion as datadog pretty much. There are docker containers for these, which I have added to my docker-compose for local dev.I think it does log ingestion too, I haven&#x27;t ever used that, I mostly use it just for the metrics and graphing. reply corytheboyd 8 hours agorootparentThat sounds very promising indeed! It might be enough for what I’m after for my projects! reply snowstormsun 3 hours agoprevhttps:&#x2F;&#x2F;sso.tax&#x2F; reply Kiro 3 hours agoparentNot applicable when the base offering is free and open source. The SSO is in the base pricing in this case. reply mnahkies 3 hours agoprevOne thing I appreciate about sentry compared to datadog is the ability to configure hard caps on ingestion to control cost. AFAIK the mechanism is basically that the server starts rate limiting&#x2F;rejecting requests and the client SDKs are written to handle this and enter a back off state or start sampling events.I think this could be a nice point of difference to explore that can help people avoid unexpected bills reply mikeshi42 2 hours agoparentAgreed on needing better tooling for surprise bills - definitely no stranger to that problem!For now we&#x27;re trying to make the base price cheap enough where those kinds of considerations don&#x27;t need to be top of mind today and a policy that can be forgiving when it occasionally happens, but certainly as we continue to scale and grow, we&#x27;ll need to put in proper controls to allow users to define what should happen if events are spiking unexpectedly (how to shed events via sampling, what needs to be explicitly perserved for compliance reasons, when to notify, etc.)I do like Sentry&#x27;s auto-sampling algorithm which is a really neat way to solve that issue. reply boundlessdreamz 6 hours agoprevLooks great1. Are you funded?2. https:&#x2F;&#x2F;www.deploysentinel.com&#x2F; - Are you going to work on this further? reply mikeshi42 50 minutes agoparentThank you - yes and yes as well w.r.t. the qs! reply technics256 2 hours agoprevIs there a guide for integrating this in local dev, either locally or if you want to view it on the hosted?Ideally hosted, devs can bring up our app locally, and view their logs and traces etc when testing and building reply mikeshi42 1 hour agoparentThere shouldn&#x27;t be any differences with how you want to set things up for local vs production telemetry (in fact all our users test locally typically before pushing it out to staging&#x2F;prod).Of course if your local&#x2F;prod run completely different and require different instrumentation, that might be trickier.I&#x27;m wondering if you had a specific use case in mind? Happy to dive more into how it should be done (feel free to join on Discord too if you&#x27;d like to chat there) reply Dockson 11 hours agoprevJust want to heap on with the praise here and say that this was definitely the best experience I&#x27;ve had with any tool trying to add monitoring for a Next.js full-stack application. The Client Sessions tab where I, out of the box, can correlate front-end actions and back-end operations for a particular user is especially nice.Great job! reply wrn14897 11 hours agoparentThank you. This means a lot to us. reply jamesmcintyre 16 hours agoprevThis looks really promising, will definitely look into using this for a project i&#x27;m working on! Btw I&#x27;ve used both datadog and newrelic in large-scale production apps and for the costs I still am not very impressed by the dx&#x2F;ux. If hyperdx can undercut price and deliver parity features&#x2F;dx (or above) i can easily see this doing well in the market. Good luck! reply mikeshi42 15 hours agoparentThank you! Absolutely agree on Datadog&#x2F;New Relic DX, I think the funny thing we learned is that most customers of theirs mention how few developers on their team actually comfortably engage with either New Relic or Datadog, and most of the time end up relying on someone to help get the data they need!Definitely striving to be the opposite of that - and would love to hear how it goes and any place we can improve! reply Hamuko 13 hours agoparentprevDatadog feels like they&#x27;ve used a shotgun to shoot functionality all over the place. New Relic felt a bit more focused, but even then I had to go attend a New Relic seminar to properly learn how to use the bloody thing. reply Wulfheart 4 hours agoprevSo do I understand the landing page correctly: It is possible to run Clickhouse using an Object Storage like S3? What are the performance implications? reply mikeshi42 3 hours agoparentYou can definitely run Clickhouse directly on S3 [1] - though we don&#x27;t run _just_ on S3 for performance reasons but instead use a layered disk strategy.A few of the weaknesses of S3 are:1. API calls are expensive, while storage in S3 is cheap, writing&#x2F;reading into it is expensive. Using only S3 for storage will incur lots of API calls as Clickhouse will work on merging objects together (which require downloading the files again from S3 and uploading a merged part) continuously in the background. And searching on recent data on S3 can incur high costs as well, if you&#x27;re constantly needing to do so (ex. alert rules)2. Latency and bandwidth of S3 are limited, SSDs are an order of magnitude faster to respond to IO requests, and also on-device SSDs typically have higher bandwidth available. This typically is a bottleneck for reads, but typically not a concern for writes. This can be mitigated by scaling out network-optimized instances, but is just another thing to keep in mind.3. We&#x27;ve seen some weird behavior on skip indices that can negatively impact performance in S3 specifically, but haven&#x27;t been able to identify exactly why yet. I don&#x27;t recall if that&#x27;s the only weirdness we see happen in S3, but it&#x27;s one that sticks out right now.Depending on your scale and latency requirements - writing directly to S3 or a simple layered disk + S3 strategy might work well for your case. Though we&#x27;ve found scaling S3 to work at the latencies&#x2F;scales our customers typically ask for require a bit of work (as with scaling any infra tool for production workloads).[1] https:&#x2F;&#x2F;clickhouse.com&#x2F;docs&#x2F;en&#x2F;integrations&#x2F;s3 reply cheema33 3 hours agoprevI am new to this space and was considering a self hosted install of Sentry software. Sentry is also opensource and appears to be similar to datadog and HyperDX in some ways. Do you know Sentry and can you tell us how your product is different?Thanks. reply mikeshi42 3 hours agoparentVery familiar with Sentry! I think we have a bit of overlap in that we both do monitoring and help devs debug though here&#x27;s where I think we differ:HyperDX:- Can collect all server logs (to help debug issues even if an exception isn&#x27;t thrown)- We can collect server metrics as well (CPU, memory, etc.)- We accept OpenTelemetry for all your data (logs, metrics, traces) - meaning you only need to instrument once and choose to switch vendors at any time if you&#x27;d like without re-instrumenting.- We can visualize arbitrary data (what&#x27;s the response time of endpoint X, how many users did action Y, how many times do users hit endpoint X grouped by user id?) - Sentry is a lot more limited in what it can visualize (mainly because it collects more limited amounts of data).Sentry:- Great for exception capture, it tries to capture any exception and match them with sourcemap properly so you can get to the right line of code where the issue occurred. We don&#x27;t have proper sourcemap support yet - so our stack traces point to minified file locations currently.- Gives you a \"inbox\" view of all your exceptions so you can see which ones are firing currently, though you can do something similar in HyperDX (error logs, log patterns, etc.) theirs is more opinionated to be email-style inbox, whereas our is more about searching errors.- Link your exceptions to your project tracker, so you can create Jira, Linear, etc. tickets directly from exceptions in Sentry.I don&#x27;t think it&#x27;s an either&#x2F;or kind of situation - we have many users that use both because we cover slightly different areas today. In the future we will be working towards accepting exception instrumentation as well, to cover some of our shortfalls when it comes to Sentry v HyperDX (since one common workflow is trying to correlate your Sentry exception to the HyperDX traces and logs).Hope that gives you an idea! Happy to chat more on our Discord if you&#x27;d like as well. reply solardev 10 hours agoprevThis is awesome! Datadog&#x27;s one of my favorite providers, and their pricing is great for small businesses, but probably unaffordable for larger businesses (as pointed out in these threads).This is slick and fast. Will have to check it out. Thanks for making it! reply mikeshi42 9 hours agoparentThank you - let me know how it goes when you&#x27;re trying it out, would love to learn how you feel it compares to Datadog :) reply jacobbank 6 hours agoprevJust wanted to say congrats on the launch! We recently adopted hyperdx at Relay.app and it&#x27;s great. reply mikeshi42 6 hours agoparentThank you - it&#x27;s been awesome working with you guys! :) reply mfkp 15 hours agoprevLooks very interesting, although a lot of the OpenTelemetry libraries are incomplete: https:&#x2F;&#x2F;opentelemetry.io&#x2F;docs&#x2F;instrumentation&#x2F;Especially Ruby, which is the one that I would be most interested in using. reply mikeshi42 15 hours agoparentThe OpenTelemetry ecosystem is definitely still young depending on the language, but we have Ruby users onboard (typically using OpenTelemetry for the tracing portion, and piping logs via Heroku or something else via the regular Ruby logger).Feel free to pop in on the Discord if you&#x27;d like to chat more&#x2F;share your thoughts! reply drchaim 2 hours agoprevthe idea of different features oss vs cloud has sense, but please, support email in oss, it&#x27;s easy and makes the platform usable. reply kcsavvy 15 hours agoprevThe session playback looks useful - I find this is missing from many DD alternatives I have seen. reply mikeshi42 14 hours agoparentAbsolutely! It&#x27;s pretty magical to go from a user report -> session replay -> exact API call being made and the backend error logs.We dogfood a ton internally and (while obviously biased) we&#x27;re always surprised how much faster we can pin point issues and connect alarms with bug reports.Hope you give us a spin and feel free to hop on our discord or open an issue if you run into anything! reply vadman97 15 hours agoprevHow do you think about the query syntax? Are you defining your own or are you following an existing specification? I particularly love the trace view you have, connecting a frontend HTTP request to server side function-level tracing. reply mikeshi42 15 hours agoparentThis one is a fun one that I&#x27;ve spent too many nights on - we&#x27;re largely similar to Google-style search syntax (bare terms, \"OR\" \"AND\" logical operators, and property:value kind of search).We include a \"query explainer\" - which translates the parsed query AST into something more human readable under the search bar, hopefully giving good feedback to the user on whether we&#x27;re understand their query or not. Though there&#x27;s lots of room to improve here! reply gajus 14 hours agorootparentPotentially useful resource – https:&#x2F;&#x2F;github.com&#x2F;gajus&#x2F;liqe reply mikeshi42 14 hours agorootparentI&#x27;ve tried liqe! I really wanted to love it - and I think it&#x27;s amazing for the use case you&#x27;ve built it for, but I recall we ran into a few fatal issues (maybe it was supporting URLs or something as a property value?) and had to fork one of the `lucene` forks to get the grammar that we wanted.Edit: happy to chat more about it as well if you&#x27;re looking for more specific feedback - it&#x27;s an area I&#x27;ve spent a decent amount of time on and would love to improve projects like liqe or others based on our experience if we can. reply jrowley 10 hours agorootparentAntlr is pretty robust too, might be worth checking out.https:&#x2F;&#x2F;www.antlr.org&#x2F; replyrobertlagrant 16 hours agoprevIf you want my two Datadog favourite features, they were: 1) clicking on a field and making it a custom search dimension in another click, and 2) flame graphs. Delicious flame graphs. reply mikeshi42 15 hours agoparentWe should have both! If you hover over a property value, a magnify&#x2F;plus icon come up to allow you to search on that property value (no manual facets required) - and our traces all come with delicious flame graphs :) Let me know if you were thinking of something different.One other thing I think you&#x27;d love if you&#x27;re coming from Datadog is that you&#x27;re able to full text search on structured logs as well, so even if the value you&#x27;re looking for lives in a property, it&#x27;s still full text searchable (this is a huge pain we hear from other Datadog users)If there&#x27;s anything you love&#x2F;hate about Datadog - would love to learn more! reply robertlagrant 13 hours agorootparentWell - the worst thing about Datadog is the sales process :-) But I&#x27;ll save that for my memoirs. I seem to remember at the time their K8s&#x2F;Helm integration was a little buggy, but no other pain than that. Plugging our software in was very easy, I recall. We had Python in the backend and we just installed their software and wired it into our API services. I also remember they had a consumer for Auth0 via Auth0&#x27;s log streaming feature, which we were using at the time.Btw I haven&#x27;t checked your product out yet; I was just reminiscing :-) I&#x27;ll take a look soon. reply mikeshi42 11 hours agorootparentAwesome, let me know what you think when you get a chance to take a look! reply drchaim 2 hours agoprevanother Clickhouse wrapper :) reply agoldis 5 hours agoprevVery nice, congrats on the launch! reply bg46z 12 hours agoprevFor highly regulated workloads, would it be possible to have a self-hosted version that is supported? reply mikeshi42 12 hours agoparentAbsolutely! You can either self-host the OSS version today, or chat with us (mike@hyperdx.io) directly if you need a managed on-prem solution or any other custom requirements depending on your deployment. reply choppaface 11 hours agoprevwhat is DX?why not grafana &#x2F; prometheus &#x2F; loki? reply mikeshi42 10 hours agoparent(Since DX is already explained...)Grafana&#x2F;Prom&#x2F;Loki is an awesome stack - overall I&#x27;d say that we try to correlate more signals in one place (your logstracessession replay), and we also take an approach to go more dev-friendly to query instead of going the PromQL&#x2F;LogQL route.It&#x27;s a stack I really wanted to love myself as well but I&#x27;ve personally ran into a few issues when using it:Loki is a handful to get right, you have to think about your labels, they can&#x27;t be high-cardinality (ex. IDs), the search is really slow if it&#x27;s not a label, and the syntax is complex because it&#x27;s derived from PromQL which I don&#x27;t think is a good fit for logs. This means an engineer on your team can&#x27;t just jump in and start typing keywords to match on, nor can they just log out logs and know they can quickly find it again in prod. Engineers need to filter logs by a label first and then wait for a regex to run if they want to do full-text search.Prometheus is pretty good, my only qualm is again the approachability of PromQL - it&#x27;s rare to see an engineer that isn&#x27;t fluent with time-series&#x2F;metric systems to be able to pick up all the concepts very quickly. This means that metrics access is largely limited to premade dashboards or a certain set of engineers that know the Prometheus setup really well.Grafana has definitely set the standard for OSS metrics, but I personally haven&#x27;t had a lot of success using their tools outside of metrics, though ymmv and it&#x27;s all about the tradeoffs you&#x27;re looking for in an observability tool. reply coel 11 hours agoparentprevDX is Developer eXperience reply fuddle 14 hours agoprevCongrats on the launch! Are you planning to release the cloud features as source available or are they closed source? reply mikeshi42 14 hours agoparentOur cloud features are closed source in a downstream repo - I think repos that have a very clear separation between OSS and closed are best - this also enforces that our OSS is always a fully-featured product that we develop on the OSS-only version day to day, and our cloud features are only a minor addition on top.I&#x27;ve historically hit issues with repos that do an `ee` folder and blur the line between what is truly open source and self-hostable, vs need a license&#x2F;cloud-only. I understand why they do that, but I hope we don&#x27;t replicate that confusion ourselves :) reply dangoodmanUT 14 hours agoprevS3-backed CH merge trees are notoriously expensive due to the high API call rates. We have a table doing over 11M APi calls per day. What are you seeing? reply mikeshi42 14 hours agoparentWe use a mix of SSDs and S3 for storage depending on the workload - as you&#x27;re right, merging on S3 is awful and we try to avoid it! reply parhamn 14 hours agoparentprevIs anyone doing these on cloudflare r2 where the cost is significantly lower? reply mikeshi42 14 hours agorootparentI&#x27;d love to be using Cloudflare as our cloud provider, but it didn&#x27;t seem to make a lot of sense for our use case.We were concerned with some of the performance benchmarks we&#x27;ve seen with R2 in the past (though they&#x27;ve probably have improved), not to mention our compute options become a bit more limited to bandwidth alliance clouds otherwise we&#x27;ll be eating network egress fees (which I do hate with a HUGE passion).Though I can imagine if you&#x27;re comfortable with one of the bandwidth alliance clouds already and can take a bit of a perf hit for search, R2 and Backblaze both can provide some cost savings depending on your workload. reply cldellow 14 hours agorootparentprevR2 is significantly cheaper for egress, but not for API calls. It&#x27;s still cheaper for API calls, but only by 10%:- 1M GETs $0.36 (R2) vs $0.40 (S3)- 1M PUTs $4.50 (R2) vs $5.00 (S3) reply nodesocket 15 hours agoprevCongrats on the launch. Perhaps I missed it, but what are the system requirements to run the self-hosted version? Seems decently heavy (Clickhouse, MongoDB, Redis, HyperDX services)? Is there a Helm chart to install into k8s?Look forward to the syslog integration which says coming soon. I have a hobby project which uses systemd services for each of my Python apps and the path with least resistance is just ingest syslog (aware that I lose stack traces, session reply, etc). reply mikeshi42 14 hours agoparentThe absolute bare minimum I&#x27;d say is 2GB RAM, though in the README we do say 4GB and 2 cores for testing, obviously more if you&#x27;re at scale and need performance.For Syslog - it&#x27;s something we&#x27;re actually pretty close to because we already support Heroku&#x27;s syslog based messages (though it&#x27;s over HTTP), but largely need to test the otel Syslog receiver + parsing pipeline will translate as well as it should (PRs always welcome of course but it shouldn&#x27;t be too far out from now ourselves :)). I&#x27;m curious are you using TLS&#x2F;TCP syslog or plain TCP or UDP?Here&#x27;s my docker stats on a x64 linux VM where it&#x27;s doing some minimal self-logging, I suspect the otel collector memory can be tuned down to bring the memory usage closer to 1GB, but this is the default out-of-the-box stats, and the miner can be turned off if log patterns isn&#x27;t needed:CONTAINER ID NAME CPU % MEM USAGE &#x2F; LIMIT MEM % NET I&#x2F;O BLOCK I&#x2F;O PIDS439e3f426ca6 hdx-oss-miner 0.89% 167.2MiB &#x2F; 7.771GiB 2.10% 3.25MB &#x2F; 6.06MB 8.85MB &#x2F; 0B 217dae9d72913d hdx-oss-task-check-alerts 0.03% 83.65MiB &#x2F; 7.771GiB 1.05% 6.79MB &#x2F; 9.54MB 147kB &#x2F; 0B 115abd59211cd7 hdx-oss-app 0.00% 56.32MiB &#x2F; 7.771GiB 0.71% 467kB &#x2F; 551kB 6.23MB &#x2F; 0B 1190c0ef1634c7 hdx-oss-api 0.02% 93.71MiB &#x2F; 7.771GiB 1.18% 13.2MB &#x2F; 7.87MB 57.3kB &#x2F; 0B 1139737209c58f hdx-oss-hostmetrics 0.03% 72.27MiB &#x2F; 7.771GiB 0.91% 3.83GB &#x2F; 173MB 3.84MB &#x2F; 0B 11e13c9416c06e hdx-oss-ingestor 0.04% 23.11MiB &#x2F; 7.771GiB 0.29% 73.2MB &#x2F; 89.4MB 77.8kB &#x2F; 0B 536d57eaac8b2 hdx-oss-otel-collector 0.33% 880MiB &#x2F; 7.771GiB 11.06% 104MB &#x2F; 68.9MB 1.24MB &#x2F; 0B 1178ac89d8e28d hdx-oss-aggregator 0.07% 88.08MiB &#x2F; 7.771GiB 1.11% 141MB &#x2F; 223MB 147kB &#x2F; 0B 118a2de809efed hdx-oss-redis 0.19% 3.738MiB &#x2F; 7.771GiB 0.05% 4.36MB &#x2F; 76.5MB 8.19kB &#x2F; 4.1kB 52f2eac07bedf hdx-oss-db 1.34% 75.62MiB &#x2F; 7.771GiB 0.95% 105MB &#x2F; 3.79GB 1.32MB &#x2F; 246MB 56032ae2b50b2f hdx-oss-ch-server 0.54% 128.7MiB &#x2F; 7.771GiB 1.62% 194MB &#x2F; 45MB 88.4MB &#x2F; 65.5kB 316 reply nodesocket 13 hours agorootparentThanks for the reply and providing detailed system requirements and docker stats. Seems I missed the note in the README. :-)Actually I am not really using syslog per say, but systemd journalctl which default behaviour on Debian (rsyslog) also duplicates to &#x2F;var&#x2F;log&#x2F;syslog. StandardOutput=journal StandardError=journalIs there a better integration to pull logs from my systemd services and journalctl up to HyperDX? reply mikeshi42 12 hours agorootparentAh yeah the easiest way is probably using the OpenTelemetry collector to set up a process to pull your logs out of jounrnald and send them via otel logs to HyperDX (or anywhere else that speaks otel) - the docs might be a bit tricky to go around depending on your familiarity with OpenTelemetry but this is what you&#x27;d be looking for:https:&#x2F;&#x2F;github.com&#x2F;open-telemetry&#x2F;opentelemetry-collector-co...Happy to dive more into the discord too if you&#x27;d like! reply podoman 15 hours agoprevLooks very similar to what we&#x27;re doing at https:&#x2F;&#x2F;highlight.io. Would love to trade notes at some point.One thing to consider with your messaging is that when you start speaking to large companies, they won&#x27;t see you as a datadog alternative. They&#x27;ll see you as a mix of sentry + fullstory + honeycomb.Datadog originally found its success with its metrics products, and the larger the buyer of datadog gets, the more metrics-esque use case a company finds. The session replay, logging and other things are simply products that datadog tacks on.That being said, this is clearly a large market (which is why we&#x27;re working on it). I particularly like the tracing UI that y&#x27;all have and I&#x27;d love to chat with your team at some point. Good luck. reply distantsounds 14 hours agoparentYou&#x27;re charging for your product, this is MIT licensed. As the meme goes, \"we are not the same.\" reply paulgb 14 hours agorootparentHighlight is Apache-2, which is for all intents and purposes equivalent to MIT if the work is not subject to patent. (this is my understanding, IANAL) reply podoman 13 hours agorootparentprevAs other commenters mentioned, we are both comparable (pending your opinion on the MIT license).We both charge a cloud saas fee as well:https:&#x2F;&#x2F;www.hyperdx.io&#x2F;pricing https:&#x2F;&#x2F;www.highlight.io&#x2F;pricing reply endisneigh 14 hours agorootparentprevthey both charge money and they&#x27;re both some variant of open source. reply Sytten 9 hours agoprevAnyone has objectives blogs&#x2F;videos that tested&#x2F;compared all those new platforms? I feel like I see a new one on HN every month. From my quick research: signoz, openobserve, uptrace, highlight.io, opstrace. I would like to recommend some alternatives to my clients, but I don&#x27;t have time to test them all and keep up with their progress.I am also worried about long term viability of those platforms. Consolidation is bound to happen, opstrace was in my bookmark last year and they got acquired. Guessing others will follow, since I dont really think they are sustainable without on-going VC funding. Interested to get thoughts on that. reply tmd83 2 hours agoparentI would love to read something like that too. I find such tools are fairly hard to evaluate since some of the challenges only comes with scale and you often need a real&#x2F;realistic scenario to actually figure out if the tool will be useful in a pinch. reply btown 12 hours agoprevThe union of session replay and OpenTelemetry is fascinating - because what is a browser session, really, other than a sequence of RPCs between backend (micro)servicesAPI server(s)browserhuman at the keyboard?Being able to see that a user bounced because they couldn&#x27;t handle the input that they were seeing - is it all that different from a service erroring because it cannot handle a certain type of input?Honeycomb is great for the OpenTelemetry part on the server side (and with https:&#x2F;&#x2F;docs.honeycomb.io&#x2F;getting-data-in&#x2F;opentelemetry&#x2F;brow... is moving towards full-stack), and systems like Posthog and Heap are great for sending session replay + browser events -> Clickhouse. But I don&#x27;t think I&#x27;ve seen a great DX that ties everything together.To that point - I would love to see different font&#x2F;color options for HyperDX: the monospaced font can become tiring to read when so dense. Will be following this project closely though - this is amazing work so far! reply mikeshi42 11 hours agoparentOh yeah browsers are really just another service (and that&#x27;s what we try to treat it as, as well!) and it&#x27;s really the same set of questions you&#x27;d ask of any service, but for some reason the tooling completely stops either at the frontend or at the backend.As for monospace font - feedback received! Is there a particular section you think is too overwhelming? (search page, nav bar, etc.) We&#x27;ve been thinking of how can we balance between the ease of monospace for reading instead of having it literally the default on every UI surface :P reply hernantz 11 hours agoprevThere is also SigNoz [0] solving the same problem with a similar stack (OpenTelemetry and Clickhouse)[0] https:&#x2F;&#x2F;github.com&#x2F;SigNoz&#x2F;signoz reply pranay01 16 hours agoprevCongrats on the launch!Do also check out SigNoz [1] We are working on a similar problem statement ;)[1] https:&#x2F;&#x2F;github.com&#x2F;signoz&#x2F;signoz reply candiddevmike 15 hours agoprevSince this is MIT, someone should fork it and add SSO to the OSS version&#x2F;remove the SSO tax. Looks like they&#x27;re just using Passport for auth, shouldn&#x27;t take much to enable the OAuth bits of it.That&#x27;s why this is MIT right, so folks can contribute stuff like this? reply mikeshi42 14 hours agoparentWe&#x27;re more than happy to have users self-host and deploy in a way that works with their SSO provider! Whether that&#x27;s via SSO on Nginx or forking and adding SSO to Passport in their fork. Depending on the provider, it&#x27;s likely very straight-forward to do.We did explicitly choose MIT for the freedom of end users to deploy and modify the code how they want - and tried to open source pretty much everything that doesn&#x27;t have a hard 3rd party dependency. We do touch a bit on how we think about the open core model as well in the README, and largely align with Gitlab&#x27;s stewardship model [1] when it comes to paid vs OSS. In this case, a contribution to add SAML specifically to OSS will likely not be merged. It&#x27;d also introduce complexities with maintaining that alongside our cloud version that already includes a specific implementation of SAML.[1] https:&#x2F;&#x2F;handbook.gitlab.com&#x2F;handbook&#x2F;company&#x2F;stewardship&#x2F; reply candiddevmike 14 hours agorootparentBalancing open core needs is pretty much an impossible task IMO. You will never do enough to placate your open source users, and you will constantly be competing against yourself and spending cycles on non-value add things. Your cloud offering will be a huge time sink chasing regulatory compliance, security, and data sovereignty needs as well. It&#x27;s for all these reasons that I personally think open core with a SaaS model is no longer a sustainable option.There&#x27;s nothing wrong with asking folks to pay for software instead of giving it away via FOSS, especially if you&#x27;re honest about your intentions and goals. When you choose FOSS to gain traction and rug pull your users when no one converts later on, you end up reaping what you sow. reply freedomben 14 hours agorootparentJust clarifying, your alternative to open core is open nothing? Just proprietary it up? reply candiddevmike 14 hours agorootparentAlternatives depend on what the goals of the person or organization who wrote the code are. There are various FOSS and source available options that can grant some freedoms while protecting others for the creator, such as if they want to let users still contribute and view the source.My main point was you should get these ducks in order first and be genuine with your intentions. Don&#x27;t use FOSS as a growth hack, it never ends well for the creator or the user. I don&#x27;t think HyperDX is genuine with their intentions, as with all open core, it&#x27;s all kumbaya FOSS until you start encroaching on their enterprise feature set. reply imiric 10 hours agorootparent> it&#x27;s all kumbaya FOSS until you start encroaching on their enterprise feature set.The open core model relies on a delicate balance of ensuring that the OSS product is featureful and standalone, while successfully monetizing value added features for advanced users and enterprise customers. Not many companies do this right, but there are those that understand and handle this balance well, and manage to have both a successful OSS and commercial product. Grafana comes to mind, for example.Just because you think that SSO is a required feature that should be part of the OSS product doesn&#x27;t mean that HyperDX is using OSS as a growth hack. Nor is it fair to label a young startup that for a product that just launched.FWIW I agree with their decision to make SSO a paid feature, but we can go over any number of features, and some OSS user is guaranteed to demand a specific feature, yet will not be willing to pay for it. SSO is not special, unless it&#x27;s a core feature that the product depends on, which doesn&#x27;t seem to be the case here.When done right, open core is the best model to monetize OSS projects, and we should be thankful that companies adopt it at all. I&#x27;d use an open core product before a proprietary one any day of the week. reply mikeshi42 14 hours agorootparentprevI&#x27;d genuinely would love to learn the OSS options we&#x27;d have available here, as we&#x27;d genuinely want to build a sustainable open source project and community, while preserving as many user freedoms as possible.I think that HyperDX is a bit different from tools like Mongo, Redis or Hashicorp in that we&#x27;re a vertically integrated product from SDKs&#x2F;UIs to ingestion pipeline and DBs, which is opposite kind of offering from done by the above companies (which has made them more vulnerable to the kind of rug pull you mentioned)We&#x27;re trying to be permissive with freedoms granted to the user of our code, while still maintaining governance over the project to make it sustainable.We don&#x27;t want to be source-available, as that&#x27;s pretty much the opposite of what we want to accomplish (and is why we consciously did not pick a license such as BSL&#x2F;SSPL&#x2F;etc.) reply candiddevmike 13 hours agorootparent> we&#x27;d genuinely want to build a sustainable open source project and communityHow do you plan on doing that while being VC-backed? Why did you choose to be VC backed in the first place? You can create a sustainable open source project and community without any VC funding. reply darkwater 12 hours agorootparentHonest question: where do you see they are VC backed? reply candiddevmike 12 hours agorootparentIt&#x27;s on the front page of the app, the company behind this (DeploySentinel) is YC backed: https:&#x2F;&#x2F;www.crunchbase.com&#x2F;organization&#x2F;deploysentinel. The original product seems like some kind of CI tool.Interestingly, it seems like \"HyperDX\" might&#x27;ve been part of their original product offering that they decided to open source--their main website (https:&#x2F;&#x2F;www.deploysentinel.com) doesn&#x27;t include any references to \"HyperDX for CI\" in May of 2023: https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20230321102146&#x2F;https:&#x2F;&#x2F;www.deplo.... Seems like they&#x27;re pivoting to metrics? Even more of a reason to be weary about this. reply danr4 13 hours agorootparentprevfunnily enough i made a similar comment on an exact same \"OS\" product: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36774611#36775934 reply freedomben 14 hours agorootparentprevNice thanks, I think you make some great points. replyfuddle 14 hours agoparentprevThe \"SSO tax\" is used to fund development of the project. reply user3939382 14 hours agoprevI&#x27;m interested. Datadog is cool but the price is ridiculously high for small orgs. reply mikeshi42 11 hours agoparentAgreed! It&#x27;s per-host pricing can obliterate budgets if you use a fleet of small instances (which is crazy to me their pricing dictates your infra...)Would love to have you check us out! Let me know if you run into any issues - feel free to hop on our discord as well :) reply user3939382 7 hours agorootparentWe have DD agent setup which handles the log aggregation from our Docker containers, and feeds them in. Does this project have that tooling as well or is the ingress of logs left to the user? reply thelastparadise 14 hours agoparentprevIs prometheus&#x2F;grafana still the recommended FOSS solution? reply gazby 14 hours agorootparentI believe so, but have recently stumbled upon Netdata which scratches the \"I don&#x27;t want to maintain an entire monitoring stack for these few boxes\" kind of itch. Need to work with it some more to nail down the trade-offs. reply bovermyer 14 hours agorootparentprevPrometheus isn&#x27;t quite enough on its own. You need Prometheus, Grafana, Tempo, Loki, Faro, and Pyroscope to get close to Datadog&#x27;s feature set. reply jefc1111 14 hours agoprevHey, cool product. I know that marketing success is not predicated on good grammar, nevertheless I felt moved to suggest a minor edit to your blurb:\"HyperDX helps engineers figure out why production is broken, faster. HyperDX centralises and correlates logs, metrics, traces, exceptions and session replays in one place.\"Good luck! reply mikeshi42 13 hours agoparentThank you! I&#x27;m assuming this is in reference to our README? (Sorry I&#x27;m a _tad_ lacking in sleep)If so, would you like to open a PR? I&#x27;m also happy to edit it myself but of course don&#x27;t want to be stealing credit if you&#x27;d like to be attributed that way. reply joshxyz 14 hours agoparentprevEveryone says that.How about: \"9 out of 10 devs are now pushing to prod on fridays. Thanks to HyperDX. Hehe.\" reply quintes 14 hours agorootparentThis helps coordinate Spotify releases on Fridays to get on the release radar? reply codegeek 16 hours agoprevHow are you different compared to similar tools like signoz ? reply mikeshi42 12 hours agoparentOverall we&#x27;re highly focused on providing solid developer workflows, ex. with HyperDX users can correlate a log to a trace (and vice-versa) really easily in the same UI, we don&#x27;t silo out features that are commonly needed in a single workflow. You can also search everything from a single panel, whether it&#x27;s a log, trace, or client-side event, using the same syntax which means there&#x27;s less to learn.Feature-to-feature, I&#x27;d say the things we do better is browser-side monitoring (session replay), event patterns&#x2F;clustering, and we have first-party SDKs built on OpenTelemetry to make the setup a lot easier than vanilla OpenTelemetry.I think Signoz has built a nice one-stop platform for observability, whereas we go one step further and focus on the developer experience to ensure anyone can fully leverage that observability data! reply vosper 12 hours agoprevWe&#x27;ve seen a fair few \"Datadog alternatives\" on HN over the years. Does that mean that Datadog is the reference or gold-standard system to beat, or to compare your product to?Kind of like how people mostly promote \"Elasticsearch alternatives\" and not \"Solr alternatives\". reply mikeshi42 12 hours agoparentIt&#x27;s a pretty scattered landscape with everyone wanting something slightly different, but everyone has likely heard of Datadog at one point or another (whether they wanted to or not... but that&#x27;s another story).It becomes convenient short-hand for what they do (collect logs, metrics, traces, RUM, etc. for engineers to debug).Though with more characters to write, I&#x27;d like to think we have a different take on both how our pricing model works and how easy it should be for an engineer to get started with us :) reply viraptor 12 hours agoparentprevIt&#x27;s a relatively ok priced system which has almost everything: server and client performance, alerts, dashboards, logs, profiling, tracing, etc. It&#x27;s not amazing and has some issues, but it&#x27;s one place to get lots of things you want and it&#x27;s good enough for many. I wouldn&#x27;t say gold-standard, but rather a benchmark for \"you have to be this tall to play the observability product game\". reply dgoncharov 15 hours agoprevThis could be huge for healthcare companies like Metriport [1] - do you sign BAAs with customers for HIPAA compliance?[1] https:&#x2F;&#x2F;github.com&#x2F;metriport&#x2F;metriport reply mikeshi42 15 hours agoparentDefinitely familiar with the compliance needs there - more than happy to chat further about BAAs and HIPAA compliance requirements with you guys. Always love partnering with others in the OSS space :) reply lopkeny12ko 16 hours agoprevnext [7 more] I remember when every SaaS landing page looked like Slack, then they all looked like Stripe, and I guess now they all look like Linear. reply mikeshi42 15 hours agoparentI designed our landing page - and I definitely took heavy inspiration from Linear. As an engineer, creating novel beautiful design&#x27;s isn&#x27;t first-nature to me, but I know how critical it can be to make a clean&#x2F;impactful landing page so I try to take some elements from the best.Some other landing pages I loved and had along side while designing ours were Vercel, Resend, and WorkOS :) reply fuddle 14 hours agoparentprevLinear seems to be the latest trend. https:&#x2F;&#x2F;www.linears.art&#x2F; - A collection of websites inspired by Linear reply pests 14 hours agorootparentI don&#x27;t think the Linear trend is as strong as the Slack or Strike trend. I agree they look similar but I feel they all just look like a standard modern page. reply ilrwbwrkhv 16 hours agoparentprevDesigners at startups are some of the most cargo culty groups in tech reply VTimofeenko 15 hours agoparentprevIf the project comes close to linear.app&#x27;s platform UI responsiveness - wouldn&#x27;t be a bad thing. reply mikeshi42 15 hours agorootparentLet&#x27;s say my React profiler tab gets lots of love - though if you find anything sluggish, let us know and I&#x27;d love to fix it. The last thing I want HyperDX to feel like is Jira sluggishness. reply specialist 12 hours agoprev [–] First paragraph https:&#x2F;&#x2F;github.com&#x2F;hyperdxio&#x2F;hyperdx\"HyperDX helps engineers figure out why production is broken faster by centralizing and correlating logs, metrics, traces, exceptions and session replays in one place. An open source and developer-friendly alternative to Datadog and New Relic.\"Just perfect. Bravo.--As a merc, I never understood the why of Datadog (or equiv). The teams and projects I rotated thru each embraced the \"LOG ALL THE THINGS!\" strategy. No guiding purpose, no esthetics. General agreement about need to improve signal to noise ratio. But little courage or gumption to act. And any such efforts would be easily rebuffed by citing the parable of Chesterfordstorm&#x27;s Fences of Doom and something something about velocity.Late last century, IT projects, like CRMs and ERPs, were plagued by over collection of data. Opaque provenance, dubious (data) quality, unclear ownership, subtractive value propositions (where the whole is worth less than the parts). No, no, don&#x27;t remove that field. We might need it some day.Today&#x27;s \"analytics\" projects are the same, right? Every drive-by stakeholder tosses in a few tags, some misc fields, a little extra meta. And before anyone can say \"kanban\", the stone soup accreted enough mass to become a gravity well threatening implosion dragging the entire org-chart into the gapping maw of our universe&#x27;s newest black hole.Am I wrong?But logging is useful, right? Or at least has that potential.The last time I designed a system end-to-end, that&#x27;s kinda what we did. Listed all the kinds of things we wanted to log. Sorta settled on formats and content (never really ever done). Did regular log bashs to explain and clear anomalies. Scripts for grooming and archiving. (For one team I rotated thru, most of their spend was on just cloudwatch. Hysterical.)But my stuff wasn&#x27;t B2C, so wasn&#x27;t tainted by the attention economy, manufactured outrage, or recommenders. No tags, referrers, campaigns, etc. It was just about keeping the system up and true. And resolving customer support incidents asap.Does any one talk or write about this? (Those SRE themed novels are now buried deep in my to read pile.)I&#x27;d like some cookbooks or blue prints which show some idealized logging strategies, with depictions of common enough troubleshooting scenarios.Having something authoritative to cite could reduce my semblance to an Eeyore. \"Hey, team mates, you know what&#x27;d be really great?! Correlation IDs! So we can see how an action percolates thru our system!\"Just curious.PS- Datadog&#x27;s server hexagon map&#x2F;chart thingie is something else. The kind of innovation that wins prizes. reply mikeshi42 11 hours agoparentYes! It should definitely be thoughtful about what you log and how you expect to use it. My biggest gripe with logs is often people writing them never think about \"how would I use this when things are on fire?\" and tend to log useless information or fail to tag them in ways that are actually searchable.Tagging the right IDs are a huge thing - customer X is saying their instance is really slow, but if none of your logs let you link service performance to customer X, your telemetry you&#x27;re paying for is absolutely useless!You have an ally in me on this one :) I&#x27;m hoping given a bit more time we get to write things like this - practical observability from the perspective of a dev, as opposed to the SRE angle that I think is well covered. Feel free to join us on discord btw if you want to chat more - I (for better&#x2F;worse) love musing about these things :) reply specialist 9 hours agorootparentPS- I just skimmed https:&#x2F;&#x2F;opentelemetry.io, which your readme.md links to.Good stuff. Much industry progress since I was last in the arena.Their site has words about manual and automatic instrumentation. I&#x27;d have to dig a bit to see what they mean.--So. Remembering a bit more... Will try to keep this brief; you&#x27;re a busy person.> tend to log useless information or fail to tag them in ways that are actually searchable#1 - I don&#x27;t know know to manage lifecycle of meta. Who needs what? When is it safe to remove stuff?We logged a lot of URLs. So many URL params. And when that wasn&#x27;t crazy enough, over flow into HTTP headers. Plus partially duplicate, incorrectly, info in the payloads, a la SOAP. (\"A person with two watches has no idea what time it is.\")When individual teams were uncertain, they&#x27;d just forward everything they received (copypasta), and add their own stuff.Just replace all that context with correlation IDs, right?Ah, but there&#x27;s \"legacy\". And unsupported protocols, like Redis and JDBC. And brain dead 3rd party services, with their own brain dead CSRs and engrs.This is really bad, and just propagates badness, but a few times, in a pinch, I&#x27;ve created Q&D \"logging proxy\". Just to get some visibility.So dumb. And yet... Why stop there? Just have \"the fabric\" record stuff. Repurpose Wireguard into an Omniscient Logger. (Like the NSA does. Probably.) That&#x27;d eliminate most I&#x2F;O trace style logging, right?Image all these \"webservices\" and serverless apps without any need for instrumentation. Just have old school app level logging.#2 - So much text processing.An egregious example is logging HTTP headers. Serialize them as JSON and send that payload to a logging service. Which then rehydrate and store it some where.My radical idea, which exactly no one has bought into, is to just pipe HTTP (Requests and Responses) as-is to log files. Then rotate, groom, archive, forward, ingest, compress, whatever as desired.That&#x27;s what I did on the system I mentioned. All I&#x2F;O was just streamed to files. And in the case of the HL7 (medical records stuff), it was super easy to extract the good bits, use that for Lucene&#x27;s metadata, and store the whole message as the Lucene document.I know such a radical idea is out of scope for your work. Just something fun to think about.#3> if none of your logs let you link service performance to customer XYup. Just keep adding servers. Kick the can down the road.One team I helped had stuff randomly peg P95. And then sometimes a seemingly unrelated server would tip over. Between timeouts, retries, and load balancers, it really seemed like the ankle bone was connected to the shoulder bone. It just made no sense.Fortunately, I had some prior experience. Being new to nodejs, maybe 5 years ago, I was shocked to learn there was no notion of back pressure. It was a challenging concept to explain to those teammates. But the omission of backpressure, and a hunch, was a good place for me start. (I&#x27;m no Dan Luu or Bryan Cantrill.)I&#x27;d like to think that proper end-to-end logging, and the ability to find signal in the noise, diagnosis would have been more mundane. reply mikeshi42 3 hours agorootparentYes OpenTelemetry is awesome in what it&#x27;s done for the industry - it was really early when I was still at Mezmo&#x2F;LogDNA but it&#x27;s matured a lot, though I think still has a ways more to go.For automatic logging - I think you&#x27;d enjoy OpenTelemetry&#x27;s automatic tracing implementation, it helps pull out standard telemetry from things like your Redis requests and correlate them with trace IDs so you can tie everything together from the moment your server starts accepting the HTTP request to the Redis and DB requests and what was sent in each request (without needing to do it manually)For capturing HTTP req&#x2F;res - we actually have a few options depending on the language (ex. we do this for Python and Node.js) to enable more advanced network capture (so you can actually get the full req&#x2F;res information, or whatever subset you&#x27;re interested in actually storing)! It&#x27;s actually been asked by a number of teams to make it easier to debug tricky API issues they&#x27;re running into.Proper end-to-end logging definitely makes it easier to find the right clue among a sea of logs, hopefully we make it easier to get there! reply TheBengaluruGuy 11 hours agoparentprev [–] > I&#x27;d like some cookbooks or blue prints which show some idealized logging strategies, with depictions of common enough troubleshooting scenarios.> \"Hey, team mates, you know what&#x27;d be really great?! Correlation IDs! So we can see how an action percolates thru our system!\"Hi, I&#x27;m building, Doctor Droid -- https:&#x2F;&#x2F;drdroid.io&#x2F; that enables you join structured application logs via correlation IDs and then build multiple types of rules &#x2F; frameworks on it -- some are at granular level and some are at aggregate levels (like funnels).We are early in the development lifecycle, would love to hear your feedback &#x2F; connect with you. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "HyperDX is a platform empowering users to search and correlate different types of data such as logs, traces, metrics, and session replays all in one place, overriding the constraints of current tools.",
      "The tool utilizes OpenTelemetry for drawing and correlating data and Clickhouse for cost-efficient storage and efficient query handling.",
      "The platform emphasizes an intuitive developer experience, with features including native JSON log parsing and easy alert creation. Demo and open-source options exist for user exploration and feedback."
    ],
    "commentSummary": [
      "HyperDX, an open-source alternative to Datadog, provides consolidated logs, traces, metrics, and session replays. It uses OpenTelemetry for data collection and Clickhouse for efficient queries and low storage costs.",
      "The platform aims to offer a smooth developer experience, offering differentiation in the market, featuring scaling data ingestion, comparative assessments between monitoring tools, and showing the benefits of self-hosted platforms.",
      "The post discussed matters like default statistics for the miner, the integration with systemd journalctl, the blend of open core and SaaS model, and the importance of end-to-end logging for diagnosing issues."
    ],
    "points": 600,
    "commentCount": 134,
    "retryCount": 0,
    "time": 1695054310
  },
  {
    "id": 37560787,
    "title": "Apple TV, now with more Tailscale",
    "originLink": "https://tailscale.com/blog/apple-tv/",
    "originBody": "Pricing Use cases Enterprise Customers Download Blog Docs Log In Try for free Business VPN Replace your legacy VPN Remote access Securely access shared resources Site-to-site networking Connect internal resources and environments Homelab Create your own personal Internet For teams Low latency, and none of your traffic ever touches our servers. For businesses Low latency, and none of your traffic ever touches our servers. For individuals Low latency, and none of your traffic ever touches our servers. FEATURED Best practices used by billion-dollar companies Read guide Customer Stories Blog Apple TV, now with more Tailscale Andrea Gottardo, Alessandro Mingione, Sam Linville and Parker Higgins on September 18, 2023 Today we’re expanding the list of devices that can run Tailscale, bringing secure remote networking to the Apple TV. The newly released tvOS 17 offers support for VPNs, and we’re proud to say Tailscale is among the first to use this new feature. You can now add your Apple TV directly to your tailnet, unlocking three powerful new use cases that we’re excited to share. First up, if you already have anything like a “media server” in your life, the benefits of integrating your Apple TV into the same Tailscale network are large. Lots of people already use Tailscale with Plex or Jellyfin servers, homelab set-ups, and NAS devices to securely share their collections and stream from them while on the go. Today’s release makes it that much simpler to do so right on your TV. With up to three users available on our Free plan, you’ve got tools to make a media drive available to other trusted people in your life. You can share a collection of family photos and home videos into a faraway relative’s tailnet, without worrying about locking down the server for public internet access. But even if you don’t have a media server to connect to, you can use Tailscale’s Apple TV app to select another device in your tailnet, like a PC, a Raspberry Pi, or even an Android phone, to use as an exit node. This will route all your Apple TV’s traffic through that connection, providing an extra layer of privacy from the local network where you’re using the Apple TV and making your traffic appear to originate from the machine of your choice. Compare that to a “traditional” VPN option, where your traffic is routed through a commercial data center (which itself may be blocked by sites and services) and where you must trust the VPN provider not to spy on or tamper with your traffic. With a Tailscale exit node, you’re in control and you get the internet connection you’re used to. This new feature could come in handy if you’re traveling with your Apple TV and want to access the same geo-restricted channels you can see from home. Finally, the new Tailscale client allows an Apple TV to be an exit node itself for other machines in your tailnet. This one might require a little more explaining; after all, not a lot of Apple TV apps advertise features that are most useful when you’re away from your Apple TV. But look at it this way: your Apple TV device is a capable little computer, and it stays connected to your tailnet even when it’s not in active use. Download and configure Tailscale now and you can securely route any of your other devices’ traffic through your Apple TV — and by extension, through your home internet connection — even when you’re on the other side of the planet. Whether you want another layer of security and privacy on sketchy Wi-Fi networks or just want to connect back through your personal internet connection when you’re on the road, you’re set with the Apple TV as an exit node. At Tailscale, we’re the kind of nerds who have home server closets and who will stock up on Raspberry Pis just because they’re available again. Our favorite thing about bringing Tailscale to tvOS is you don’t have to be that kind of nerd to be able to tap into the power of Tailscale in your home. If you’ve got an Apple TV running the new tvOS 17, download the Tailscale app today! Share via ← Back to index Subscribe for monthly updates Product updates, blog posts, company news, and more. Subscribe Too much email? RSS Twitter LEARN SSH Keys Docker SSH DevSecOps Multicloud NAT Traversal MagicDNS PAM PoLP All articles USE CASES Business VPN Remote Access Homelab Site-to-site Networking Enterprise GET STARTED Overview Pricing Downloads Documentation How It Works Why Tailscale Compare Tailscale Customers Integrations Changelog Use Tailscale Free COMPANY Company Newsletter Press Kit Blog Careers HELP & CONTACT Contact Sales Contact Support Open Source Security Compliance Status Twitter Fediverse GitHub YouTube WireGuard is a registered trademark of Jason A. Donenfeld. © 2023 Tailscale Inc. All rights reserved. Tailscale is a registered trademark of Tailscale Inc. Privacy & Terms",
    "commentLink": "https://news.ycombinator.com/item?id=37560787",
    "commentBody": "Apple TV, now with more TailscaleHacker NewspastloginApple TV, now with more Tailscale (tailscale.com) 419 points by mfiguiere 14 hours ago| hidepastfavorite114 comments judge2020 14 hours ago> Finally, the new Tailscale client allows an Apple TV to be an exit node itself for other machines in your tailnet.Pretty huge. Many non-techy users don&#x27;t like the idea of keeping a computer on 24&#x2F;7, but a smart TV is just fine.Also, the Apple TV 4k only draws 0.5 watts at idle and less than 3 watts when streaming movies[0], so I imagine it pulls less than 1 just tunnelling traffic. Computers pull 15W+ at idle, and that&#x27;s with low end components.0: https:&#x2F;&#x2F;www.apple.com&#x2F;environment&#x2F;pdf&#x2F;products&#x2F;appletv&#x2F;Apple... reply lnxg33k1 13 hours agoparentSo far I’ve used it to get vpn on apple tv and i dont think i am going to change, also considering how apple leaks vpns like there’s no tomorrow https:&#x2F;&#x2F;www.amazon.nl&#x2F;GL-iNet-GL-MT300N-V2-Reiserouter-Repea... reply telotortium 10 hours agorootparentUS Amazon link: https:&#x2F;&#x2F;www.amazon.com&#x2F;GL-iNET-GL-MT300N-V2-Repeater-300Mbps... reply close04 12 hours agorootparentprevCan second the recommendation for the Mango travel router. I always prefer to take the VPN out of the “hands” of the client device to avoid any leaks. With 2 such devices connected via Wireguard VPN any other device I connect to that client router’s WiFi is safely communicating through that VPN. A sort of site to site VPN that works for devices that could never otherwise use a VPN client.But of course this is a different use case and not always an option. Not if you want to use Tailscale. Probably unless that Apple TV is already connected to one of this “VPN WiFi” with Tailscale on top (no idea what the functionality or performance impact is). reply matthewaveryusa 9 hours agorootparentDon&#x27;t know about the mango, but the gl.inet I have works with tailscale (albeit still in beta) https:&#x2F;&#x2F;www.gl-inet.com&#x2F;products&#x2F;gl-axt1800&#x2F; reply tiffanyh 9 hours agoparentprevHow will this work?My Apple TV constantly goes to sleep.Is Tailscale doing some type of “busy wait” to prevent tvOS from going to sleep? reply lathiat 9 hours agorootparentIt’s not truly asleep. The display parts are but it’s always connected to wifi to act as a home hub, receive airplay requests, etc. reply crazygringo 9 hours agorootparentprevYou can change your Apple TV settings to not to go to sleep. reply angott 8 hours agorootparentThis is not really necessary, there is no need to change any settings. Even when the device enters sleep mode, VPN apps can remain active, just like on iOS. reply jondwillis 13 hours agoparentprevNeat, maybe I can sell my M1 mini server reply 8fingerlouie 3 hours agorootparentNah, you&#x27;ll still need that to synchronize iCloud content locally so that you can make backups of it, as Apple stubbornly refuses to allow TimeMachine (or anything else) to actually backup stuff that is only stored in iCloud, and provies no easy, scriptable way, of doing so otherwise.It may just be a problem for me, but as i have ~3TB of photos in iCloud (2 x 2TB), and unless i want to buy laptops with 2TB storage, there is no practical way of backing up the contents of iCloud, so i use a Mac Mini M1 with an external drive, syncrhonize data locally, and then back it up from there. reply yardstick 2 hours agorootparentI need to backup my iCloud data soon too. How sure are you that the data is all downloaded from the cloud when you copy it to the external drive?Do you use any special tools? reply 8fingerlouie 1 hour agorootparentI just configure each users account on the Mac Mini to download everything from iCloud, and then backup each users directory.It does require each user to login again every time the mac mini is rebooted, but fortunately that only happens when new releases come around, so 3-4 times every year.I do periodically check if new photos have been downloaded. I care less about documents as the relevant documents are more likely to also be stored on the laptops, and thus backed up through the normal backup routine on the laptops.I do wish Apple would come up with a solution to this problem though. The official instructions[1] feels like something from 2003.[1]: https:&#x2F;&#x2F;support.apple.com&#x2F;en-us&#x2F;HT204055 reply aaomidi 13 hours agoparentprevI had not convinced the use case of using this as an exit node. Fuck this simplified so much. reply copperx 13 hours agorootparentTailscale also runs on Android TV. If you don&#x27;t have an Apple TV and want a cheap device just to have an exit node, you can buy a $20 Android TV thingy. reply vosper 12 hours agorootparentBeware that a lot of cheap Android TV boxes come pre-loaded with heaps of malware. You don&#x27;t want them in your network.Linus Tech Tips has a video about it: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=1vpepaQ-VQQ&themeRefresh=1 reply copperx 10 hours agorootparentThat&#x27;s true. However, Walmart&#x27;s $20 Onn 4k Streaming Box has no malware, apparently. reply metadat 10 hours agorootparentprevI abandoned the Google TV thingy because it was great when it was new a year or so ago, but now after all the updates it frequently stutters when playing media from Netflix, Disney+, HBO Max, etc. Apple TV is silky smooth and works perfectly.At $200, it was 4x the price, though. reply icelancer 9 hours agorootparentThe expensive Nvidia Shield Pro is dogshit as well for streaming performance at $150-200. Ridiculous. reply johnmaguire 6 hours agorootparentWorks great for me? reply e12e 9 hours agorootparentprevWhy, apparently it finally does - since when? Last time I checked, I&#x27;d have to sideload it on my Nvidia shield? reply mjs 12 hours agorootparentprevIt … kind of does, but if you filter the reviews by \"TV\" you&#x27;ll see there&#x27;s quite a few issues with it: https:&#x2F;&#x2F;play.google.com&#x2F;store&#x2F;apps&#x2F;details?id=com.tailscale..... Not sure why the back button issue hasn&#x27;t been fixed, that makes it very inconvenient to set up. (Also: are you sure it can be used as an exit node? That wasn&#x27;t supported a few months ago.) reply MuffinFlavored 11 hours agoparentprev> Many non-techy usersWhy would a non-techy user want to volunteer to be an exit node? reply giobox 10 hours agorootparentI have setup Pis in family member homes to allow me to get residential IP VPN exit node in their respective countries - cheap and easy way to get access to foreign TV streaming services without a monthly fee. I used to run my own exit nodes in AWS&#x2F;DigitalOcean in those regions, but virtually all streaming services block VPS&#x2F;cloud service IPs at this point. Having an exit node in an actual \"real\" residential internet service is vastly more flexible.This potentially would be even easier for me, given they all have Apple TVs already. This isn&#x27;t a public exit node - it&#x27;s only available to other users (i.e. people you know and have granted access to) of your own TailScale setup.Same for non-techy folks who have second homes in foreign countries, or even just travel a lot - an Apple TV running this new app back in their main property will allow them for free to browse the web as if they are actually at their main property, including any TV services they enjoy. reply fragmede 11 hours agorootparentprevThis isn&#x27;t Tor, being an exit node just means the non-techy user can access Netflix while travelling internationally. reply userbinator 6 hours agorootparentBlame Tor for popularising the term \"exit node\" to mean \"public proxy\".The terms \"VPN gateway\" or \"VPN server\" are still valid and less easily confused with Tor&#x27;s use of \"exit node\". reply Operyl 10 hours agorootparentprevIt’s not a public Tor exit node. It’s a personal node you can use to route your own traffic. reply Mandatum 13 hours agoprevI live reading copy that’s obviously written by nerds. This is the least corporate announcement I’ve seen from a corporation in a long time.No mention of how much they live trust and privacy or how they’re going to make your experience more delightful. reply ant6n 12 hours agoparentYeah, sounds like a bunch of tech gobbledigook. I guess it’s written for the users of these services, and they know what all this jargon means. reply cstrahan 9 hours agorootparentTailscale is a company that provides a VPN (“Virtual Private Network”) service. If you don’t find yourself thinking “man, I really wish this one computer over here could share the same network with that computer over there, despite not being on the same WiFi access point or physical Ethernet network”, then their service (and the news regarding it) aren’t for you.Why would someone want a VPN? There are a bunch, but here are some examples:1) You want to connect to one of your machines at home while you’re at a coffee shop, or on vacation. Maybe so you can check security cameras, I dunno.2) You’re on vacation outside of your home country, and you would like to watch a video stream that is blocked in the country you’re vacationing in. I experienced this in the Bahamas — If I recall, I was wanting to watch a UFC fight, but the UFC app refused to stream to the Bahamas (it was this and&#x2F;or other Disney&#x2F;Hulu whatever services refusing to play in the Bahamas). By routing traffic through your ISP back home: problem solved. (This what “exit node” is referring to — a computer through which internet traffic flows on your behalf)3) You want to play a game with a friend that only supports multiplayer play on the same network, but your friend isn’t physically there with you in the same house. So just put the two of you on the same virtual network and now you can play together. reply bawana 8 hours agorootparentbut why is this better than running a vpn client on your pc? For example,when I want to watch streams restricted in my country, I fire up the ExpressVPN client on my laptop, connect to Switzerland, and then my restrictions disappear. Why should get another piece of hardware, wires and complexity (what happens when this box doesnt connect to the internet but it has no keyboard,, display or mouse to guide troubleshooting?) reply cpuguy83 8 hours agorootparentBecause Tailscale is a [1] direct connection. No middleman service with access to your traffic.[1] In some cases this is not possible and there are relays setup to help route traffic. What&#x27;s in the traffic is opaque to these nodes. You can also choose to use your own nodes. If you are interested here is a great post on how this works: https:&#x2F;&#x2F;tailscale.com&#x2F;blog&#x2F;how-nat-traversal-works&#x2F; reply lxgr 7 hours agorootparentprevMost streaming services block commercial VPNs and even data center IP ranges at this point.Some VPN vendors bypass that by reselling access to residential IPs (witting or unwitting on the side of the person paying for the ISP service), but even that is hit and miss.> Why should get another piece of hardwareMany people already have an Apple TV or Android TV streaming box. reply aspenmayer 7 hours agorootparentprevThe GL.iNet routers have a mobile and desktop config site and buttons to configure&#x2F;reset the device as well as a two-position hardware switch, the function of which is configurable also. This is not to mention they can run OpenWRT&#x2F;LEDE and there are vendor created “clean” firmware images to do so. They’re one of the best devices for this use case and price point. I don’t think the situation you’re worried about is a reasonable concern for someone already expected to be competent to manage the router generally to begin with, and if they also want to do the things Tailcale does, they can and should be able to troubleshoot the problem space. The stock firmware is a modified OpenWRT with a web GUI and some optional extras, but it’s the most functional consumer router I’ve used. replymiki123211 12 hours agoprev> With up to three users available on our Free plan, you’ve got tools to make a media drive available to other trusted people in your life. You can share a collection of family photos and home videos into a faraway relative’s tailnet, without worrying about locking down the server for public internet access.It&#x27;s important to point out here that, in addition to this, the free plan also lets you send invite links to specific devices, which other people can add on their own accounts. That way, nobody has to go for the (quite expensive and obviously company-focused) free plan, you can share your device with as many friends as you like, and you&#x27;re not sharing anything else beyond that single device. reply Operyl 14 hours agoprevUsing it as an always-on exit node is actually a pretty nifty feature, I hadn’t thought about that as a viable feature before now. reply cube2222 14 hours agoparentThis is by the way kind of how remote access with apple home works.The Apple TV serves as a local gateway relaying all the commands to your local IoT devices.On a side note, tailscale is lovely. I have nothing but good things to say about them. reply Operyl 14 hours agorootparentYup, either a HomePod, Apple TV, or iPad left at home can act as a HomeKit hub. reply ericswpark 13 hours agorootparentJust an FYI, but iPads can no longer be used as a HomeKit hub as of last year: https:&#x2F;&#x2F;support.apple.com&#x2F;en-us&#x2F;HT213481(Yes, you can technically use an iPad as a hub if you are on the old Home architecture) reply Operyl 13 hours agorootparentGood change, then! It wasn’t a great experience for most people. iPads are rarely static home fixtures now, and they were the only ones capable of dying. reply drcongo 57 minutes agorootparentYeah, that was always a weird choice. The one time I went on holiday without first checking to see which device was acting as my primary home hub it turned out to be my iPad, which I&#x27;d taken with me, and all my security cameras were \"unavailable\" for the week. I&#x27;m sure the system is supposed to just switch to a different primary hub in that situation (I have about 15 candidates), but it didn&#x27;t. reply ignoramous 6 hours agorootparentprev> This is by the way kind of how remote access with apple home works.Apple killed Back to My Mac, which sounded a lot like Tailscale exit nodes: https:&#x2F;&#x2F;datatracker.ietf.org&#x2F;doc&#x2F;html&#x2F;rfc6281 reply dimgl 13 hours agoprevTailscale continues to be one of the more impressive services I&#x27;ve ever used. Going to install this on my Apple TV immediately. I often travel and use public Wi-Fi, so this is massively useful as my PC and my laptop are not always on (so I can&#x27;t use them as an exit node). Pretty genius honestly. reply drexlspivey 13 hours agoprevThe bigger news is that you can add VPNs on Apple TV with tvOS 17, I had to run it on my router before reply fmajid 10 hours agoparentStill better to run it on your router. Apple’s had VPN leaks, and also exempted its own services from VPN or Little Snitch firewalling. Separation of roles means not having to trust Apple. reply ignoramous 6 hours agorootparentWait until Apple bundles in 5G eSIMs for connectivity for just Apple apps to bypass the physical firewalls. reply FireBeyond 14 hours agoprevI don&#x27;t care either way, but I did note the ignorance of the elephant in the room as to why 99% of people would care about Tailscale and native VPN support on their Apple TV... and it&#x27;s not \"avoiding sketchy wifi networks\". reply fotta 14 hours agoparent> With a Tailscale exit node, you’re in control and you get the internet connection you’re used to. This new feature could come in handy if you’re traveling with your Apple TV and want to access the same geo-restricted channels you can see from home.They do call this out towards the end. reply cassianoleal 12 hours agorootparentHow&#x27;s this supposed to work? If I&#x27;m travelling with my Apple TV and use it as an exit node, it&#x27;s as geo-restricted as I am, wherever I am. reply ezfe 12 hours agorootparentThis blog post isn&#x27;t just for using it as an exit node. Traveling with the Apple TV and using Tailscale lets you exit-node back to your house.Traveling without the Apple TV and the exit-node can be your Apple TV. reply cassianoleal 1 hour agorootparentPerhaps the blog post isn&#x27;t, but the quoted text is:> With a Tailscale exit node, you’re in control and you get the internet connection you’re used to. This new feature could come in handy if you’re traveling with your Apple TV and want to access the same geo-restricted channels you can see from home. reply Larrikin 12 hours agorootparentprevYou designated a device at home as the exit node and are using that on your Apple TV in a different location. reply meowtimemania 14 hours agoparentprevThe main use case I see is sharing streaming services like youtube TV with family. reply zikduruqe 13 hours agorootparentI run my own DNS server at home, and have Tailscale installed on it also. I use this so when I am away from home, I can continue to use it via Tailscale and&#x2F;or an exit node for full on VPN-like solution.I can now, move Tailscale off that server, and put it on my Apple TV to use as my network for my DNS server when I am away from the house. reply radicaldreamer 13 hours agorootparentprevYou can already do that officially... but maybe not region-locked sports reply drewnick 13 hours agorootparentDefinitely not region locked sports. My YT TV account is based on the other side of the country and I can&#x27;t watch our local teams quite frequently. I&#x27;ve been using wireguard and a dedicated wifi network to tunnel through a fiber connection \"back home\" and it then thinks I am local and all works well. This is much cleaner with tailscale! reply sangnoir 13 hours agorootparentprevIt&#x27;s cheaper if everyone is in the \"same household\" (i.e. sharing the same public IP as main account) reply LoganDark 14 hours agoparentprevIt&#x27;s a way to access it remotely without having to forward a port to the whole world. There are other ways to do this, but a VPN is usually the most straightforward option.It&#x27;s also a way to proxy your connections through a device at home, of course. Whether the Apple TV is the client or the exit node. reply copperx 13 hours agoparentprevFor sharing Netflix accounts? reply fragmede 13 hours agorootparentArrr, it not be for Netflix. reply tredre3 13 hours agorootparentTailscale isn&#x27;t useful for piracy. Unless you really want your pirate traffic to always be routed through your home? reply tshaddox 12 hours agorootparentThe idea is that you host all your pirated media from home, e.g. on a NAS running Plex or Jellyfin, and your home server can stream any of your media to any device (including transcoding it to best fit the device and connection).Tailscale isn&#x27;t particularly useful for acquiring the pirated media in the first place, of course. reply stirlo 9 hours agorootparentHow is this different to running a Plex server on your NAS and streaming directly over regular internet? reply FloatArtifact 8 hours agorootparentYou do not punch holes through your routers firewall. There for it&#x27;s is more secure as a mesh network. reply ezfe 12 hours agorootparentprevTailscale has Mullvad integration now, so it can be used that way too reply cellu 12 hours agorootparentprevI guess it’s more to be able to access the local are stack &#x2F; jellyfin from everywhere? reply unstatusthequo 14 hours agoparentprevBecause say I want to connect to my own private remote network. I have a server hosted in a datacenter because I self-host. I&#x27;d much rather have VPN capabilities than deal with a proxy server and publicly open ports with rules. This is a much tighter way to do things, IMHO. reply nickvanw 14 hours agoprevThis is useful - using an exit node with an Apple TV is useful as well for navigating around certain tools that are geo-blocked. Before, you&#x27;d have to handle it outside of the device which is much more difficult.I&#x27;m going to play around with this later in the week. reply mlfreeman 13 hours agoprevWill this work with Headscale too? reply angott 13 hours agoparentTailscale dev here: yes, you can set up a custom coordination server in the settings, just like on the iOS app. Open the tvOS Settings app, then scroll down to Tailscale. reply vineyardmike 9 hours agorootparentGenuine question: Does tail scale want people using headscale?I&#x27;m a free-tier personal user, and a little too cheap to give a for-profit corp money when I don&#x27;t need to just because \"I REALLY like the product\". If I use headscale does that just cause a headache for the team, or is it good because it reduces traffic to prod?I&#x27;m to cheap to pay when I don&#x27;t need to, but its such a great product (esp for free) that I&#x27;d gladly change how I use the product to be less expensive or problematic. reply hzia 11 hours agorootparentprevThank you so much for that!! I wondered about this as well. Love how above and beyond you guys are going to support other OSS implementationsIs it possible to transparently embed Tailscale into a game to only talk to your self-hosted Headscale server?https:&#x2F;&#x2F;github.com&#x2F;tailscale&#x2F;libtailscale> Also, is it in theory possible to use WebRTC to negotiate Wireguard connections and not use any control plane?you can write code to do whatever you want I guess, but that&#x27;s nothing to do with tailscale reply b555 13 hours agoprevcan anyone share documentation&#x2F;paper&#x2F;video with eli5 of tailscale?i recently read this with mulvad too and feel stupid that I don&#x27;t intuitively understand how it works, and what it does and why it&#x27;s needed. reply simonw 13 hours agoparentIt&#x27;s WireGuard with a really nice UI.WireGuard is an outstanding mechanism for building secure virtual private networks.You can run WireGuard on a bunch of different machines (or virtual machines) spread all over the world and give them the ability to talk to each other as if they were on the same LAN, with every packet fully encrypted.TailScale has productized this. They wrote software for a bunch of platforms that makes it trivial to connect those machines to your \"tailnet\" - effectively a WireGuard network which their software manages for you.They tie this to SSO - so you can install their software on your phone and your home server, sign them both in using Google SSO or similar, and now they&#x27;re able to talk to each other on a secure virtual network.I suggest trying the TailScale setup process to really understand how good it is. reply hot_gril 12 hours agorootparentSo it&#x27;s a VPN, right? reply efxhoy 44 minutes agorootparentIt makes setting up your own peer to peer VPN between your devices.https:&#x2F;&#x2F;tailscale.com&#x2F;kb&#x2F;1151&#x2F;what-is-tailscale&#x2F; reply vineyardmike 9 hours agorootparentprevIts utility is as an \"overlay network\", but using traditional VPN technology. Yes, it is a virtual network, and it&#x27;s private, but it&#x27;s not intended to be used to exit to the internet in a controlled manner, as VPNs are often advertised as. reply hot_gril 6 hours agorootparentWell, the original purpose of a VPN was more as a private LAN (as Tailscale seems to advertise itself as) than as a way to exit to the Internet somewhere else. And it does both still.Seems like Tailscale is a very souped up VPN, though. You can add more nodes to the network easily, and even have multiple gateways to the Internet. reply derefr 2 hours agorootparent> Well, the original purpose of a VPN was more as a private LANYou&#x27;re conflating two concepts.An \"oldschool\" VPN connection (using e.g. IPSec) is something that allows your computer to remotely \"join\" a real, physical LAN. It&#x27;s basically equivalent to running PPP over IP: your computer \"dials up\" a daemon running on a server somewhere; that daemon accepts a stream of raw packets from your computer&#x27;s network stack; and then that daemon dumps those packets out through one of the server&#x27;s NICs onto a local network segment — where those packets are then handled by the switch they run into as if your computer was directly plugged into that switch. So your computer can acquire an IP address for its VPN \"bridge\" interface via DHCP from the switch; can talk to other devices on that private network through the switch; can talk to the Internet via NAT through that switch; etc.Tailscale, meanwhile, creates a software-defined virtual LAN on top of p2p mesh networking of the nodes. There&#x27;s no actual network segment anywhere that your packets are being dumped out onto; the \"switch\" handling your packets is a shared distributed abstract-machine that&#x27;s partly running on your Tailscale client, and partly running on the other nodes&#x27; Tailscale clients. That virtual LAN doesn&#x27;t have a routing table + NAT on it to translate packets into Internet-bound packets. Nor does the LAN have the ability to host L2 services like DHCP. It&#x27;s just a functional L3 simulation of an L1 network segment, not a faithful emulation of an L1 network segment. reply ezfe 12 hours agorootparentprevIt&#x27;s kinda a VPN.Tailscale on its own is a mesh network that allows your devices to communicate (in a VPN, technically, yes) between themselves.If you have an exit node, then you can route your traffic to that exit node in the way most people think of a VPN.It also has Mullvad integration, providing Mullvad servers as exit nodes.If you use an exit node, then its functionally equivalent to a VPN with fancy features. reply SparkyMcUnicorn 13 hours agoparentprevTailscale is basically wireguard in a seamless UX wrapper, and a bunch of nice (optional) things added on top like ACLs&#x2F;2FA&#x2F;MagicDNS&#x2F;ssh.https:&#x2F;&#x2F;tailscale.com&#x2F;blog&#x2F;how-tailscale-works&#x2F; reply Larrikin 13 hours agoparentprevYou have a home server, could be home assistant, a Raspberry Pi, your desktop computer. Access that server and all services on your phone or laptop from anywhere without figuring out ports and worrying about your server being pwned. It all looks like local traffic.Set the DNS server on your phone to a Pi running AdGuard Home and block all ads and trackers when on 5G, not just in the browser.Travel abroad with your laptop and designate your computer at home as an exit node and now all the traffic on your laptop looks like it is coming from that country.Those are just the use cases I am using personally. reply angott 13 hours agoparentprevThis blog post is a very good technical read (and the diagrams are really cool too): https:&#x2F;&#x2F;tailscale.com&#x2F;blog&#x2F;how-tailscale-works&#x2F; reply rhinoceraptor 10 hours agoparentprevIt connects all of your computers and devices in a way that feels magical. For example, if I have a Plex server named myplex on port 80 at home, and if I want to access it from my laptop, I just go to http:&#x2F;&#x2F;myplex.It doesn&#x27;t matter if I&#x27;m at home or anywhere else, if I have internet then that just works. I don&#x27;t have to open a port on my router, configure DNS, or anything like that, I just install and run Tailscale. reply duped 13 hours agoparentprevYou&#x27;re on a team of 10 people with 20 different machines between you and want to securely send&#x2F;receive files, spin up servers and talk to them, etc.Tailscale makes this really easy, and fast. reply ecliptik 13 hours agoparentprevIt&#x27;s a 90s LAN, but with encryption and accessible from anywhere. reply nose-wuzzy-pad 9 hours agoprevI’ve installed this on a freshly updated AppleTV 4K with Ethernet and for the life of me I can’t get it to work using the Apple TV as an exit node. I’ve enabled it and approved it in the console.Unfortunately I can’t ping any hosts through it or make any connections. This is in contrast to my other exit node, which is a docker container running tailscaled with user networking. It continues to work just fine.Any ideas?Thanks. reply ShakataGaNai 11 hours agoprevThis is very cool, and very useful.For the average, non-technical user, Apple TV as an exit node for other device while traveling is super cool.But for someone who is out of the country for a duration, it&#x27;s also super handy. Netflix knows all the popular VPN providers and ban hammers them on a regular basis. But being able to use my Apple TV to watch my normal Netflix (or whomever) from any other country... because they think I&#x27;m at home? Super win. reply fragmede 11 hours agoparentNetwork engineers watching rtt&#x2F;packet latency very closely can still tell that something fishy is up, but Netflix doesn&#x27;t really want to block VPNs, they just have to pretend to care enough so that the labels don&#x27;t pull their content. reply lstamour 10 hours agorootparentIf one forwards traffic through iCloud+ proxy to mask IP address, I wonder if it’s still possible to tell a VPN, from, say, a perfectly legitimate SpaceX satellite signal received on a boat… ;-) reply fragmede 9 hours agorootparentno comment reply lxgr 10 hours agoprevThat&#x27;s amazing!I&#x27;ve already been using it in a very similar way on a Chromecast (the one running Android TV), which made me use my Apple TV less and less, to the point where I actually unplugged it. This might just be its ticket back to an HDMI port :) reply unstatusthequo 14 hours agoprevThis is great news! Not only does this make a remote Plex &#x2F; Jellyfin media server easier to deal with, the Apple TV can be an exit node. Solid work, TailScale! reply maxmcd 14 hours agoparentI&#x27;m a little unfamiliar with how Plex routing works. Would this make it so that your plex connected media servers don&#x27;t need to be publicly routabel and the Plex app will know to connect through the tailscale network?Would you need to reconfigure plex to use the tailscale ip addresses and then the Apple TX Plex app will stream over that address? reply ecliptik 13 hours agorootparentI wrote up a guide [1] on using Plex + Tailscale + HTTPS last year to setup Plex so you don&#x27;t have to expose it through the Plex relays or setup port forwards for other devices on a Tailnet.I would assume with this announcement, you can keep Plex private to your Tailnet and an AppleTV also on the Tailnet could use it without any port fowarding.1. https:&#x2F;&#x2F;forums.plex.tv&#x2F;t&#x2F;remote-access-using-tailscale-magic... reply SV_BubbleTime 12 hours agorootparent>setup port forwards for other devices on a Tailnet.Ah. Now I get it. reply aaomidi 13 hours agorootparentprevDepends on how you’ve setup Plex, but you can give it custom access URLs. So you can expose both a public and a private endpoint. Or just a private endpoint, up to you really. reply aaomidi 13 hours agoprevI’ve been working on bringing tailscale into container networking through a driver, it’s still a work in progress but people might already be interested in trying it out:https:&#x2F;&#x2F;github.com&#x2F;aaomidi&#x2F;ContainerScale reply sohrob 13 hours agoprevAwesome news and boosts the utility of the Apple TV tremendously. reply syntaxing 13 hours agoprevIs it possible to run a plex or jellyfin server on an Apple TV like a Nvidia Shield? If so, I might seriously consider getting an Apple TV just to run as a media server. reply billyhoffman 13 hours agoparentSadly an Apple TV can&#x27;t also be the media server (at least for something like Plex). But just about anything else can run media server, and you can go really low end especially if you don&#x27;t need it to transcode your media. Some software like Infuse will stream the original media file to the Apple TV, and the transcoding happens on device. reply tshaddox 12 hours agorootparentTrue, but of course if you already have a media server, it can almost certainly already act as a Tailscale exit node. reply syntaxing 12 hours agorootparentprevI more or less have running every through a N100 and it has been great. Would have been awesome to replace it with an Apple TV though reply hapticmonkey 3 hours agorootparentAppleTV cant act as a media server. But as a client it&#x27;s fantastic.An AppleTV with an app like Infuse will flawlessly play back 4K HDR or Dolby Vision videos client side (no transcoding) as well as 7.1 lossless TrueHD audio. Unfortunately it wont do TrueHD Atmos. reply zakki 13 hours agoprevI wish there is Tailscale for LG TV. reply ilteris 12 hours agoparentNever heard of tailscale before. Is it similar to Plex? reply nerdbert 10 hours agorootparentNope, it&#x27;s a tool for building a private network among machines which can be geographically and internetically distributed. So, more or less a VPN, but not particularly in the sense that people use it today (which is effectively a glorified proxy server). reply klinquist 12 hours agoprevThis makes it much easier to use the Xfinity Stream app on your \"travel appletv\" :) reply Spooky23 12 hours agoprevCan you use this to appear to be in another place for blackout avoidance purposes? reply dangoodmanUT 9 hours agoprevLOVE THIS reply jedberg 12 hours agoprev> But even if you don’t have a media server to connect to, you can use Tailscale’s Apple TV app to select another device in your tailnet ... to use as an exit node. This will route all your Apple TV’s traffic through that connection ... making your traffic appear to originate from the machine of your choice.Oh look all of those family Netflix devices are in one home again! reply Timber-6539 11 hours agoprev [–] I wish they would work on their Android client.Its got a long standing request to add split tunnelling [0] (a standard feature on pretty much every VPN client you&#x27;ll come across). But it seems in the spirit of re-inventing existing networking technologies, Tailscale also decided to re-invent what a VPN client does.This alone makes me give this otherwise wonderful project a pass despite all the deservingly good press it gets.[0] https:&#x2F;&#x2F;github.com&#x2F;tailscale&#x2F;tailscale&#x2F;issues&#x2F;6912 replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Tailscale, a platform for creating secure networks, is now compatible with Apple TV due to the introduction of tvOS 17, which enables the integration of Apple TV into the user's Tailscale network.",
      "Functions of this integration include secure media sharing and streaming, and using Tailscale's Apple TV app to reroute traffic for additional privacy and to access geo-blocked channels during travel.",
      "The Apple TV can also function as an exit node in the Tailscale network, permitting users to route traffic via their home internet connection even when they're not at home."
    ],
    "commentSummary": [
      "Tailscale, a virtual private network (VPN) software, now provides support for Apple TV, enabling it to function as an exit node for other devices in a network.",
      "User discussions highlight Tailscale's benefits like accessing remote machines, bypassing geolocation restrictions, and enabling streaming service sharing.",
      "Tailscale successfully facilitates secure, seamless networks for an array of usage scenarios, including remote server access and getting around streaming location limitations. Some users expressed their hopes for Tailscale compatibility with other devices like LG TV and Android."
    ],
    "points": 419,
    "commentCount": 114,
    "retryCount": 0,
    "time": 1695065072
  },
  {
    "id": 37554504,
    "title": "NSA Backdoor Key from Lotus-Notes (1997)",
    "originLink": "http://www.cypherspace.org/adam/hacks/lotus-nsa-key.html",
    "originBody": "This page has also been translated into Russian here and into Polish here NSA's Backdoor Key from Lotus-Notes Before the US crypto export regulations were finally disolved the export version of Lotus Notes used to include a key escrow / backdoor feature called differential cryptography. The idea was that they got permission to export 64 bit crypto if 24 of those bits were encrypted for the NSA's public key. The NSA would then only have the small matter of brute-forcing the remaining 40 bits to get the plaintext, and everyone else would get a not-that-great 64 bit key space (which probably already back then NSA would have had the compute power to brute force also, only at higher cost). Anyway as clearly inside the application somewhere would be an NSA public key that the NSA had the private key for, I tried reverse engineering it to get the public key. In doing this I discovered that the NSA public key had an organizational name of \"MiniTruth\", and a common name of \"Big Brother\". Specifically what I saw in my debugger late one night, which was spooky for a short moment was: O=MiniTruth CN=Big Brother Literary note: for those who have not read Orwell's prescient \"1984\" the Ministry of Truth was the agency who's job was propaganda and suppression of truths that did not suit the malignant fictional future government in the book, and \"Big Brother\" was the evil shadowy leader of this government. The whole book is online here. The NSA's Public Key I put this together some years after the reverse-engineering stint, so there could be errors, but this is from my notes, the raw public key modulus from the debugger: 8D9D6213D3EF03A7 A5CEAE99B8E9FF06 12E58ECAAB2939FE 72B41833B8B947A0 DF8111B561CE67FB 50844623CF88338C E7BC80C5ECC31276 6075E13E12E956F6 59954F68B04F0FEA B6B82EFEC4E07BD8 4BC41FE3123AF70C 31688BCD5895BB00 I figured it was in little endian format by trial and error; other formats were easy to factor. So the big endian hex representation is: e = 3 n = \\ 00BB9558CD8B68310CF73A12E31FC44BD87BE0C4FE2EB8B6EA0F4FB0684F9559\\ F656E9123EE175607612C3ECC580BCE78C3388CF23468450FB67CE61B51181DF\\ A047B9B83318B472FE3929ABCA8EE51206FFE9B899AECEA5A703EFD313629D8D where the modulus is 760 bits, and the public key formatted as a PGP key is (of course I made this user id up -- you can edit it to whatever you choose it's of course not self-signed): Type Bits/KeyID Date User ID pub 760/13629D8D 1998/10/25 Director, NSA-----BEGIN PGP PUBLIC KEY BLOCK----- Version: 2.6.3i mQBsAzYyeuIAAAEC+LuVWM2LaDEM9zoS4x/ES9h74MT+Lri26g9PsGhPlVn2VukS PuF1YHYSw+zFgLznjDOIzyNGhFD7Z85htRGB36BHubgzGLRy/jkpq8qO5RIG/+m4 ma7OpacD79MTYp2NAAIDtB5EaXJlY3RvciwgTlNBIDxkaXJuc2FAbnNhLmdvdj4= =aoSi -----END PGP PUBLIC KEY BLOCK----- and here's what pgpacket has to say about the contents of that key: --------------------------- Packet Type:Public Key Packet Length:108 Version Byte:3 Key Created:25 Oct 1998 01:12:02 Valid forever Algorithm:1 (RSA) N:0xBB9558CD8B68310CF73A12E31FC44BD87BE0C4FE2EB8B6EA0F4FB0684F9559F6\\ 56E9123EE175607612C3ECC580BCE78C3388CF23468450FB67CE61B51181DFA0\\ 47B9B83318B472FE3929ABCA8EE51206FFE9B899AECEA5A703EFD313629D8D E:0x03 Key ID: 0xA703EFD313629D8D --------------------------- Packet Type:User ID Packet Length:30 User ID:\"Director, NSA \" Comments, html bugs to (Adam Back) at",
    "commentLink": "https://news.ycombinator.com/item?id=37554504",
    "commentBody": "NSA Backdoor Key from Lotus-Notes (1997)Hacker NewspastloginNSA Backdoor Key from Lotus-Notes (1997) (cypherspace.org) 332 points by hosteur 23 hours ago| hidepastfavorite147 comments gregw2 21 hours agoIt’s worth reading Ray Ozzie’s (Lotus Notes creator)’s comment on this from a HN 2013 discussion:https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=5846189Before the software was released, Ray Ozzie and Kauffman openly described what they were doing at an RSA conference. This was not a secret back door. It was compliance with export controls everybody in the industry dealt with.Also worth reading barrkel’s comment a couple comments down… reply ethbr1 20 hours agoparentFor people younger than ~37, I&#x27;d remind them that crypto before 2000, especially in shipped commercial products, was playing under substantially different government restrictions.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Crypto_WarsEffectively and in short, you were prohibited by the US government from shipping strong encryption in any internationally distributed product. Which generally meant everything commercial.Despite open source implementations of strong encryption existing (e.g. PGP et al.).Now, no one bats an eye if you ship the most secure crypto you want. Then, it was a coin flip as to whether you&#x27;d feel the full weight of the US government legal apparatus.It was a crazy, schizophrenic time. reply matheusmoreira 20 hours agorootparent> It was a crazy, schizophrenic time.Still is. To this day, we have to debate and justify ourselves to these people. They make us look like pedophiles for caring about this stuff. They just won&#x27;t give up, they keep trying to pass these silly laws again and again. It&#x27;s just a tiresome never ending struggle.And that&#x27;s in the US which is relatively good about this. Judges in my country were literally foaming at the mouth with rage when WhatsApp told them they couldn&#x27;t provide decryption keys. Blocked the entire service for days out of spite, impacting hundreds of millions. reply AdamN 2 hours agorootparenthttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Four_Horsemen_of_the_Infocalyp... reply snakeyjake 19 hours agorootparentprevWindows 2000 came on a CD... and a floppy disk.The CD was a globally-legal image, and export-controlled strong crypto came on the floppy in countries where it was allowed.https:&#x2F;&#x2F;winworldpc.com&#x2F;product&#x2F;windows-2000-high-encryption&#x2F;... reply LeifCarrotson 14 hours agorootparentHow hard would it have been for a \"rogue state\" to get a copy of that floppy? I understand that times were different, you couldn&#x27;t just PGP encrypt it and attach a 1.44 MB blob to an email, sending it at 24 kbps. You couldn&#x27;t just upload it to an anonymous filesharing site.But today it seems fundamentally obvious that once a single copy is leaked, it&#x27;s all over... was that not true in 2000? reply semi-extrinsic 14 hours agorootparentGnutella, including popular clients like LimeWire, were released around the same time as Windows 2000. People were doing decentralized filesharing of files larger than 1.44 MB just fine in 2000.Filesharing at that time was just wild, by the way. It was far too easy to set up your client such that you were sharing the entire contents of your computer with the whole internet. More often than not, this was done by the kids in the family on the same machine where mom and dad had their work stuff plus their private finances.So of course the files were leaked. If you were intending to share something illegal to distribute outside the US, you could easily get plausible deniability just by sharing everything on your computer and feigning ignorance. reply NL807 11 hours agorootparenteDonkey and eMule was fire during those years. reply nurettin 5 hours agorootparentwith dc++ and a fiber network connecting major universities in Germany at 10MegaBytes&#x2F;s, it was more than fire. I remember downloading the entire ** trilogy in minutes. reply wmf 14 hours agorootparentprevOf course all that stuff was leaked (and there were anonymous filesharing sites). The whole export-grade crypto thing was a legal fig leaf. reply pdw 13 hours agorootparentIt was all extremely silly. Debian took a different approach: before 2005, they put all crypto packages in a separate \"non-US\" archive, hosted in the Netherlands. American developers weren&#x27;t allowed to upload there. That way, Debian never exported crypto code from the United States, it only ever imported it. reply wmf 13 hours agorootparentYep, US export restrictions ended up spurring foreign investment in crypto like Thawte (founded by Mark Shuttleworth) and SSLeay (later forked as OpenSSL). reply sillywalk 12 hours agorootparentprevI believe OpenBSD (based in Canada) was in a similar situation. reply eastbound 11 hours agorootparentprevThere was a story of a hundred programmers taking the program PRINTED ON PAPER to a conference in Sweden to type it in again, because somehow export of binaries was forbidden but not the printed version of it. Is it true? Which event organized this? reply wmf 11 hours agorootparentThe PGP source code was published as a book so that it could be exported under the theory that the first amendment beats ITAR. https:&#x2F;&#x2F;philzimmermann.com&#x2F;EN&#x2F;essays&#x2F;BookPreface.html reply notpushkin 3 hours agorootparent> A book comprised entirely of thousands of lines of source code looks pretty dull. But then so does a nondescript fragment of concrete -- unless it happens to be a piece of the Berlin Wall, which many people display on their mantels as a symbol of freedom opening up for millions of people. Perhaps in the long run, this book will help open up the US borders to the free flow of information.This is beautiful. replyicedchai 12 hours agorootparentprevIt was. People were sharing pirated software on BBSes 40 years ago! Downloading a floppy might take an hour. In the 90&#x27;s, I knew kids who got jobs at ISPs just so they could run warez FTP sites off of the T1. reply fragmede 11 hours agorootparentOh man, a T1. That brings back memories.Serial Port recently tried to set one up!https:&#x2F;&#x2F;youtu.be&#x2F;MEda7SQxh18 reply icedchai 10 hours agorootparentYes! In the early 90&#x27;s, anyone with a T1 was almost god like. Most local ISPs were still connected with 56k or maybe fractional T1 lines. reply 0xbadcafebee 9 hours agorootparentFast forward to 2000: T1 lines were still being used, but ADSL deployment was growing like wildfire. Some providers were legendary for offering synchronous DSL with extremely few limitations, for a fraction of the cost of a T1. That&#x27;s what really kicked off a new generation of distributed file sharing. Nothing today compares; the dark web is a tiny blip in the ocean that was the 2000&#x27;s file sharing scene. reply Akronymus 1 hour agorootparentdark or deep web? Because the dark web is HUGE. But the deep one not so much. replypdntspa 3 hours agorootparentprevIn 2000 there was absolutely nothing stopping you from connecting to an FTP server and uploading whatever you wanted, other than time and bandwidth. reply rconti 12 hours agorootparentprevWe were sharing lots of 3-7MB files peer-to-peer at the time :D Napster, Limewire, Audiogalaxy, etc. Plenty of public FTP sites all over the place as well.Even in the late 90s, 128kbps ISDN connections were not unheard of, and 256kbps DSL was rolling out as well. reply netsharc 10 hours agorootparentDamn, Audiogalaxy! That takes me back! A simple Windows client for downloading (and well uploading), and to search and download you go to their website, login and add stuff to your queue (although I barely remember what the website looked like). Sooner or later someone with the files you want would come online and your computer would begin downloading from their computer.. reply hunter2_ 12 hours agorootparentprev> a copy of that floppyMostly off-topic, but your use of rhyme is reminiscent of https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=up863eQKGUI reply mgerdts 10 hours agorootparentprevA few years before that plenty of people were downloading 30ish floppy images over modems to install Slackware or SLS. reply rozzie 11 hours agorootparentprev\"Now, no one bats an eye if you ship the most secure crypto you want.\"The most surprising thing to me is that, in speaking in the past several years with younger entrepreneurs, they&#x27;re not even aware of the obligation to file for an export license for any&#x2F;all software containing crypto (such as that submitted to the App Store).I&#x27;ve not yet seen a case in which a mass market exemption isn&#x27;t quickly granted, but devs still need to file - and re-file annually. reply justinclift 9 hours agorootparentIs that still a requirement for US developers?As in, currently. reply lstamour 8 hours agorootparentWhen you submit the documentation via Apple, also submitting it to the government is not necessary: https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;security&#x2F;complying...Essentially Apple built a system so you have to agree to export restrictions with every single build you upload to Apple. reply brudgers 20 hours agorootparentprevNow, no one bats an eye if you ship the most secure crypto you want.To me, there are only two plausible explanations for the change:1. The three letter agencies gave up on backdooring cryptography.2. The three letter agencies successfully subverted the entire chain of trust.Only one of them is consistent with a workforce consisting of highly motivated codebreaking professionals available working for many decades with virtually unlimited resources and minimal oversight.The other is what people want to believe.https:&#x2F;&#x2F;www.cs.cmu.edu&#x2F;~rdriley&#x2F;487&#x2F;papers&#x2F;Thompson_1984_Ref... reply hn_throwaway_99 19 hours agorootparentI think a 3rd option is actually much more likely and (semi) less conspiratorial:3. NSA realized that \"frontal assaults\" against encryption were a lot less fruitful than simply finding ways to access info once it has been decrypted.Would have to search for the quote, but Snowden himself said exactly that, something along the lines of \"Encryption works, and the NSA doesn&#x27;t have some obscure &#x27;Too Many Secrets&#x27; encryption breaking machine. But endpoint security is so bad that the NSA has lots of tools that can read messages when you do.\" And indeed, that&#x27;s exactly what we saw in things like the Snowden revelations, Pegasus, and I&#x27;d argue even things like side-chain attacks.Plus, I don&#x27;t even know what \"The three letter agencies successfully subverted the entire chain of trust\" means. In the case of something like TLS root certificates that makes sense, but there are many, many forms of cryptography (like cryptocurrency) where no keys are any more privileged than any other keys - there is no \"chain of trust\" to speak about in the first place. reply smolder 18 hours agorootparentI&#x27;ve long (post-snowden?) estimated NSAs capabilities are roughly what you imply. Lots of implementation-specific attacks, plus a collection of stolen&#x2F;coerced&#x2F;reversed TLS certs so they can MITM a great deal of web traffic. US-based cloud represents another big backdoor for them to everyone&#x27;s data there, I think. reply sethhochberg 14 hours agorootparentThey&#x27;ve presumably got a pretty vested interest in making sure most communications are legitimately secure against most common attacks - arguably good for national security overall, but doubly good for making sure that if anyone can find a novel way in, its them, and not any of their adversarial peers.There&#x27;s a reason many corporate information security programs don&#x27;t go overboard with mitigations for targeted, persistent, nation-state level attacks. Security is a set of compromises, and we&#x27;ve seen time and time again in industry that this sort of agency doesn&#x27;t need to break your encryption to get what they need. reply hutzlibu 18 hours agorootparentprevWhen the NSA for example has access to the Intel ME or AMDs version of it(and I think they do) then they surely don&#x27;t need to break any encryption. They don&#x27;t even need to hack. They just would have direct access, to most Desktops&#x2F;Servers. reply hn_throwaway_99 18 hours agorootparentEven this is too conspiratorial for me. Not because I believe the NSA wouldn&#x27;t like access, but because it&#x27;s not the best approach. Convincing Intel or AMD to have a hidden back door, and to somehow keep that it hidden, is a nearly impossible task. Compare that with just hunting for 0-days like the rest of the world, which the NSA has shown to be quite good at.Not saying there couldn&#x27;t be a targeted supply chain attack (that&#x27;s essentially what was revealed in some of the Snowden leaks, e.g. targeting networking cables leased by big tech companies), but I don&#x27;t believe there is some widely dispersed secret backdoor, even if just for the reason that it&#x27;s too hard to keep secret. reply smolder 18 hours agorootparentAt a minimum, it&#x27;s a thing that certain security conscious consumers (cough DoD) were able to get Intel to include a hidden (not typically user accessible) bios flag for disabling most features of the management engine. So they&#x27;re at least concerned about it as a security risk. That doesn&#x27;t necessarily mean they also have backdoors into it, but it&#x27;s not crazy to think they might. It&#x27;s hard to be too conspiratorially minded with respect to intelligence stuff, if you aren&#x27;t making the mistake of treating suppositions as facts. reply dannyw 17 hours agorootparentI have a workstation bought from eBay that has a “ME DISABLED” sticker on the chassis.Any analysis I could or should do? reply mkup 15 hours agorootparentRun Intel MEInfo utility, check if it reports \"Alt Disable Mode\" or anything like that. Article for some context: https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20170828150536&#x2F;http:&#x2F;&#x2F;blog.ptsec... reply maqp 12 hours agorootparentprev>Convincing Intel or AMD to have a hidden back door, and to somehow keep that it hidden, is a nearly impossible taskInteresting, how would an X86 instruction with hardcoded 256-bit key would be detected? IIRC it&#x27;s really hard to audit the instruction space for CISC architecture. reply hutzlibu 18 hours agorootparentprevWell sure, they would not use it for everyday standard cases to limit exposure. Intel does have something to loose, if this would became public knowledge.But I cannot believe they resisted the temptation to use that opportunity to get such an easy access to so many devices. reply ethbr1 18 hours agorootparentParent&#x27;s point is that its very existence (not just use, as this is hardware&#x2F;firmware we&#x27;re talking about) in widely deployed form would be too risky.Consequently, if there is an ME-subversion, it&#x27;s only deployed &#x2F; part-replaced for extraordinary targets. Not \"every system.\" reply hutzlibu 18 hours agorootparentHuh? As far as I know every Intel ME has access to the internet, can receive push firmware updates and write access to everything else on the system. It does not need a modified version, they can just use the official way, the normal Intel ME on target devices, if they can cloack their access of the official server, which I think could be achieved of using just the key of the official server and then use another server posing as the official server.But it has been a while that I read about it and I never took it apart myself, so maybe what I wrote is not possible for technical reasons. reply toast0 16 hours agorootparentI don&#x27;t think that&#x27;s the case. Don&#x27;t you need to have a selected NIC, integrated properly to get the Intel ME network features? Typically branded as \"Intel vPro\"Otherwise, you need something in your OS to ship data back and forth between the ME and whatever NIC you have. reply bonzini 13 hours agorootparentvPro, also known as AMT, is proprietary and it&#x27;s for professional desktop and laptop systems. ME instead is based on IPMI and is for server-class systems. reply toast0 10 hours agorootparentAre they reusing the name to be more confusing? Intel ME calls to mind the management engine that&#x27;s been embedded in most Intel based computers for the last 15 or so years.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Intel_Management_Engine reply ethbr1 17 hours agorootparentprevThat&#x27;s... definitely not how sensitive networks work. To say nothing of airgapped ones.This seems like as good a short-form intro as any: https:&#x2F;&#x2F;blogs.cisco.com&#x2F;learning&#x2F;security-in-network-design-... reply hutzlibu 12 hours agorootparentI would believe, really sensitive networks, have ME deactivated anyway and need other, specialised infiltration methods.But when targeting a random individual in a hurry, I think it would be handy to just use the build in backdoor. reply wavesquid 1 hour agorootparentThe trouble is, as far as I know, that the ME cannot be deactivated. Even if you are a really sensitive network. Your option is to find some of the few Intel chips without it, or find another chip vendor. This often means you can&#x27;t use common off the shelf systems, so now you can be a victim of a targetted supply chain attack. replysmolder 18 hours agorootparentprevAttacking machines directly over the network is dangerous for them from the standpoint of detection, though. You can bet that any ME&#x2F;PSP remote access exploits are used very carefully due to potential detection. reply maqp 12 hours agorootparentprev>I don&#x27;t even know what \"The three letter agencies successfully subverted the entire chain of trust\" means.For one thing, they&#x27;re interdicting hardware and inserting hardware implants:https:&#x2F;&#x2F;www.theguardian.com&#x2F;books&#x2F;2014&#x2F;may&#x2F;12&#x2F;glenn-greenwal... reply knewter 16 hours agorootparentprevDid you forget about NIST curve recommendations? reply hn_throwaway_99 15 hours agorootparentNot at all, considering that coincidentally just yesterday I was having an HN discussion on an unrelated topic about DJ Bernstein, https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Daniel_J._Bernstein#Cryptograp....You&#x27;re right though, I guess I didn&#x27;t mean to say that NSA would give up on or would not want back doors into widely deployed crypto algorithms, but even with Dual_EC_DRBG the suspicions were widely known and discussed before it was a NIST standard (i.e. I guess you could say it was a conspiracy, but it wasn&#x27;t really a secret conspiracy), and the standard was withdrawn in 2014. reply wkat4242 18 hours agorootparentprevI think that&#x27;s basically what the parent&#x27;s #2 point implies. reply ethbr1 19 hours agorootparentprevIMHO, the IC gave up on the feasibility of maintaining hegemony over encryption, particularly in the face of non-corporate open source. You can&#x27;t sue a book &#x2F; t-shirt &#x2F; anonymous contributors.Consequently, they still have highly motivated and talented cryptanalysts and vast resources, but they&#x27;re attacking widely-deployed academically-sound crypto systems.Hypothetical encryption-breaking machines (e.g. large quantum computers) are too obviously a double-edged sword: who else has one? And given that possibility, wouldn&#x27;t you switch to algorithms more secure against them?In reality, the NSA&#x27;s preference would likely be that no-such machine exists, but rather there are brute-force attacks that require incredibly large and expensive amounts of computational resources. Because if it&#x27;s just a money problem, the US can feel more confident that they&#x27;re near the top of the pile.Which probably means that their most efficient target has shifted from mathematical forced decryption to implementation attacks. Even the strongest safe has a weakest point. Which may still be strong, but is the best option if you need to get in. reply chaxor 18 hours agorootparentI don&#x27;t know much about hardware, but is it not possible that there is a small part of a chip somewhere deep in the highly complex systems we have that simply intercepts prior to encryption and, if some condition is met (a remote connection sets a flag via hardware set keys), encrypts&#x2F;sends the data elsewhere? Something like that anyway. It seems possible, but idk how plausible it is, and if things like the Linux kernel would be likely to not report on it, if the hardware is not known enough.Anyway, just suggesting something that wouldn&#x27;t require quantum cryptography. reply ethbr1 18 hours agorootparentAs pointed out by another comment above, exfiltration then becomes the risky step.If that did exist, you&#x27;d still have to get packets out through an unknown network, running unknown detection tools. Possible, but dicey over the intermediate term.Who&#x27;s to say they didn&#x27;t just plug a box in, run a fake workload on it, and put all network traffic it emits under a microscope? reply yomlica8 17 hours agorootparentSeems like you could just blast it out on one of the endless Microsoft telemetry or update channels that are chatting away all day and either intercept outside the network or with Microsoft&#x27;s help. Only way to protect against that would be blocking all internet access. reply hedora 13 hours agorootparentprevNote that ACME (Let&#x27;s Encrypt) means that anyone that can reliably man-in-the-middle a server can intercept SSL traffic (module certificate revocation lists, and pinning, but those are mostly done by big sites with extremely broad attack surfaces).Similarly, most consumer devices have a few zero-days each year, if not more, so if you really want to decrypt someone&#x27;s stuff, you just need to wait a few months.I think that both your explanations are probably incorrect though. It&#x27;s a bit of \"neither\" in this case.They continue to backdoor all sorts of stuff (they recently were marketing and selling backdoored \"secure\" cell phones to crooks), and most chains of trust are weak enough in practice. reply woodruffw 12 hours agorootparent> Note that ACME (Let&#x27;s Encrypt) means that anyone that can reliably man-in-the-middle a server can intercept SSL traffic (module certificate revocation lists, and pinning, but those are mostly done by big sites with extremely broad attack surfaces).I don&#x27;t understand why you think ACME means this. Can you explain? reply icedchai 12 hours agorootparentNot the original poster, but if you can control responses to and from a server (MITM) you can get a TLS&#x2F;SSL certificate issued for it easily. In the old days, getting a cert was quite a hassle! You used to have to fill out paperwork and perhaps even talk to a human. It could literally take weeks. reply woodruffw 12 hours agorootparentI don’t think a MITM would be sufficient to fool ACME. As Let’s Encrypt’s guide explains[1], an attacker in the middle would still fail to possess the target’s private key. As a result, the proof of possession check would fail.The attacker could sign with their own key instead, but this is trivially observable to the target (they don’t end up with a correct cert, and it all gets logged in CT anyways.)[1]: https:&#x2F;&#x2F;letsencrypt.org&#x2F;how-it-works&#x2F; reply rcxdude 1 hour agorootparentIf you have a full MITM (you can do anything you like with all traffic to&#x2F;from the target), you just do your own ACME validation with the target&#x27;s domain without involving the target at all. Then you use that to MITM the SSL on any connections to the target (terminate SSL between the other side and your middlebox, then push the plaintext into your target, which is unaware anything has changed at all).If the owner watches CT logs they will know about it (and you may need to jump through some more hoops once the target tries to renew their cert), but you get a lot of info in the meantime. reply woodruffw 14 minutes agorootparentSure, but this has nothing to do with ACME itself. The attack model here is \"if the attacker is effectively in control of the domain, then they can demonstrate that control.\" That&#x27;s a way stronger posture than being able to maliciously MITM a specific ACME session, which (I think) is what the original concern was.However, even with the full MITM here, this attack assumes that the attacker can proxy plaintext to the host. I&#x27;m not aware of many sites that allow sensitive actions (e.g. logging in) over HTTP anymore.(And, as you note, this is detectable via CT. But it&#x27;s fair to point out that many&#x2F;most smaller operators probably aren&#x27;t bothering to monitor public CT logs for unexpected issuances.) fragmede 11 hours agorootparentprevWould the target get notified by LetsEncrypt about this scenario though? Let&#x27;s say I setup Certbot on my server. I&#x27;m not watching CT logs. How would I know about the double issuance? reply woodruffw 3 hours agorootparentI don’t think it would be a double issuance; it’d be either a failed issuance or a single issuance with a single unexpected key. In other words: the target would end up with in an error state, and they could use CT to determine what happened. reply icedchai 10 hours agorootparentprevWhat if they&#x27;re getting a new certificate and proxying the traffic? As long as the cert looks okay to the end user, they&#x27;re not going to notice for a while. reply woodruffw 3 hours agorootparentPerhaps I’m misunderstanding what you’re saying, but this still doesn’t break the scheme: an attacker who interposes on ACME with their own private key is going to result in a CSR response for the wrong private key being sent back to the target server, which should cause an alertable failure. Even if this is somehow not checked (this would be a serious vulnerability in an ACME client!), the targeted server would end up serving a certificate that it can’t actually use (because it doesn’t have the private half). replysouthernplaces7 14 hours agorootparentprevI don&#x27;t buy that it has to be just one or the other. Fundamentally, crypto is just very dense information and once it became widely enough standardized by people who could easily share and apply it commercially, getting even the strongest crypto to the most basic user becomes extremely easy.Short of blocking the very essence of digital data spread and transactions, the three-letter agencies and the giant governments behind them realized that there was no way to effectively put that particular genie back in the bottle without fucking over too many other extremely well-connected commercial interests.Thus, while they didn&#x27;t entirely give up on their bullshit, and keep looking to find arguments for privacy subversion, they realized that roundabout methods were a usable practical course.That&#x27;s where we stand today: a world in which there&#x27;s no obvious way to block something that&#x27;s so cheaply easy to share and securely be applied by so many people, but governed by technocrats who do what they can to subvert meanwhile.The fundamental math of crypto is secure, regardless of any conspiracy theories. AES-256, for example, can&#x27;t just be broken by some secret Area 51 alien decoder ring. The mathematics of good modern crypto simply crush any human computing technology for breaking them regardless of budget. However, the agencies also know that in a complex world of half-assed civilian security and public habits, they still have enough methods to work with without delving into political firestorms. reply ethbr1 11 hours agorootparentI&#x27;ve always thought the ratio of average residential network bandwidth to average file size is underappreciated as an arbiter of change.The only true solution to distribution &#x2F; piracy is for the file to be so big as to be inconvenient.Which is why mp3 was such a game changer. reply southernplaces7 10 hours agorootparentI&#x27;m sorry? Responding to the wrong comment? reply ethbr1 7 hours agorootparent>> That&#x27;s where we stand today: a world in which there&#x27;s no obvious way to block something that&#x27;s so cheaply easy to share... reply sandworm101 13 hours agorootparentprevThey aren&#x27;t backdooring modern open-source encryption. They may have some elite knowledge about some esoteric corner of the code that allows them to theoretically throw a data center at the problem for a month or two, but the days of easy backdoors to decrypting everything in real time are gone imho. It is just too easy to implement mathematically-strong encryption these days. Too many people know how to do it from scratch. The NSA&#x27;s real job is keeping american systems safe. That is done through creating the best encryption possible. They are very good at that job. reply intelVISA 16 hours agorootparentprevFighting against crypto is a public and costly affair, it was deemed easier to twist Intel&#x2F;AMD&#x27;s arm a little on the silicon level. reply jraph 19 hours agorootparentprevI see another plausible explanation: The NSA is concerned with maintaining security of its own &#x2F; the government&#x27;s infrastructure &#x2F; is interested in finding breaches in infrastructures of others.(this is speculation, I have no actual knowledge on this) reply lern_too_spel 20 hours agorootparentprevOnly one is consistent with the documents that have been leaked since the change to export restrictions. The other is what the marketing department at Reynolds Wrap would like you to believe. reply pkaye 18 hours agorootparentprevNot just US but other countries had their own restrictions. For example I think France didn&#x27;t allow anything better than 40-bit encryption without key escrow.http:&#x2F;&#x2F;www.cnn.com&#x2F;TECH&#x2F;computing&#x2F;9805&#x2F;19&#x2F;encryption&#x2F;index.h...http:&#x2F;&#x2F;www.opengroup.org&#x2F;security&#x2F;meetings&#x2F;apr98&#x2F;french-regu... reply bo1024 20 hours agorootparentprev> It was a crazy, schizophrenic time.Or, we are currently experiencing a brief oasis of freedom in between extended periods of encryption lockdowns and controls. reply Jerrrry 19 hours agorootparentYup, networks with a neuron count above a certain threshold (2+T?) will likely be on the IDAR restriction list again. reply bagels 15 hours agorootparentITAR? Also, was there a time where there was a restriction based on neuron count? reply SkyMarshal 17 hours agorootparentprevWhat’s a neuron count? reply bagels 15 hours agorootparentNeuron in a neural network. Not sure if the parent is talking about models, software or hardware though. reply UI_at_80x24 19 hours agorootparentprevFor anybody who hasn&#x27;t already read it, I highly recommend the book: \"Crypto\" by Steven Levy. I was 30% of my way through the book before I started recognizing real world events, news stories, whispered computer secrets; and realized that it wasn&#x27;t a fictional book and was instead talking about real history.https:&#x2F;&#x2F;www.goodreads.com&#x2F;book&#x2F;show&#x2F;984428.Crypto?from_searc... reply hkt 1 hour agorootparentFabulous book, I found it in a public library when I was 15 or so and it was a hell of an education. Not least because I was already reading about tor and i2p. I&#x27;d recommend it to anyone - the story about Phil Zimmerman printing the code to PGP in a book made me laugh my head off. reply r3trohack3r 19 hours agorootparentprevIIRC this is part of what shifted hardware manufacturing out of the US.If you wanted to build in the U.S. you had to produce two versions of your product, one with “full encryption” and one with encryption hobbled.Or you could go build one version somewhere else and import it into the U.S. reply mike50 11 hours agorootparentSimilar situation with space hardware. Even cots memory chips hardened for radiation and space are ITAR export restricted. reply stephen_g 4 minutes agorootparentYeah, I worked at a company up to a few years ago where it was actually a huge competitive advantage for us not being in the US, because the products we designed, manufactured and sold (full satcom terminals as well as the microwave converters in them) would have been ITAR if they came from the US (being ‘dual use’). reply archgoon 19 hours agorootparentprevI had never heard of this particular aspect of demanufacturing, that&#x27;s fascinating. Do you know of any products where this was a deciding factor, or at least a major consideration? (I recognize you probably can&#x27;t easily cite internal corporate documents) reply hinkley 19 hours agorootparentprevExcept to Iran, Syria, North Korea…Also you couldn’t just ship products with a spot where crypto went and remove the crypto. API designs had to go through mental gymnastics to allow crypto without explicitly adding crypto. Which is why you have odd constructs that take strings as arguments and give you encryption back. Sometimes.And since new languages copy patterns from old to remain familiar, these APIs are still frequently some of the most patience-testing. reply wkat4242 18 hours agorootparentprevIt&#x27;s not completely gone. If you implement crypto in an iOS app you have to get an \"export license\" even if you&#x27;re not based in the US or publish your app there. reply fullspectrumdev 15 hours agorootparentI’ve had to sign ITAR related paperwork a few times for commercial software specifically because it was made in the US and being “exported” to the UK.Really boils my piss given a lot of it, upon inspection, just used OpenSSL under the hood. reply brokenmachine 3 hours agorootparentI&#x27;m in Australia and had to sign ITAR paperwork to order a bluetooth evaluation board. reply hoc 8 hours agorootparentprevWell... some folks still do care.https:&#x2F;&#x2F;developer.apple.com&#x2F;documentation&#x2F;security&#x2F;complying...Also, always makes you wonder, why the standards the OS ships with are exempt... reply forgetfreeman 16 hours agorootparentprevThat this is no longer the case is a fairly strong indication that The Powers That Be have durably resolved the issue of decryption. reply convolvatron 12 hours agorootparentprevand I believe it was a major contributor to us having poor infrastructure for PKI protocols today, since these restrictions meant that it was pointless to try to bake them into standards reply gadders 1 hour agoparentprevNone of this was secret. I worked at Lotus in the mid-90s and there were 2 versions of Lotus Notes, one for the US and the other labelled \"International\". reply CTDOCodebases 19 hours agoparentprevAn ex Microsoft dev did a good breakdown video of NSAkey:https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=vjkBAl84PJs reply 13of40 17 hours agoparentprevIt was an interesting time. I forget the person&#x27;s name, but I talked briefly with the guy who implemented the crc32 and encryption algorithms for ZIP, and he (almost apologetically) said the encryption was designed to be exportable under those laws. It&#x27;s still not trivial to break, but you can test millions of passwords on a ZIP archive entry in the time it takes to try one on a modern Office document. reply fullspectrumdev 15 hours agorootparentPartial known plaintext attacks are very, very useful when cracking ZIP “encryption”.I’ve mostly used this to unpack ZyXEL firmware updates (reference below to this), but it also works on a lot of other stuff if you can get a partial plaintext. Some file formats headers might work.https:&#x2F;&#x2F;www.fullspectrum.dev&#x2F;the-hunt-for-cve-2023-28771-par... reply grammers 15 hours agoparentprevWhether secret or not, it was a backdoor that could be&#x2F;was exploited. Today governments are asking for &#x27;secret backdoors&#x27; from tech companies, not seeing the immense risks. Crazy times. reply thewanderer1983 10 hours agoparentprev>This was not a secret back door. It was compliance with export controls everybody in the industry dealt with.The author states it correctly. Here is the text from the author \"The idea was that they got permission to export 64 bit crypto if 24 of those bits were encrypted for the NSA&#x27;s public key. The NSA would then only have the small matter of brute-forcing the remaining 40 bits to get the plaintext\"Here is the text from the RSA conference.Hello, 1st off please don&#x27;t publish my name on your site. I&#x27;m too lazy to set up another cheezy mail acct. Today I downloaded cryptography&#x2F;nsa&#x2F;lotus.notes.backdoor.txt from your site. I have a close friend who is a developer for Iris (the people who make Notes for lotus.) I sent him the file I downloaded and asked him what the deal was, and here&#x27;s his response: Here&#x27;s the necessary info to truly understand the issue here; a speech by Ray Ozzie and Charlie Kaufman&#x27;s white paper on the topic. What it comes down to is that notes provides superior exportable encryption technology when compared to other US products on the market. For anyone (but the NSA) to crack our international encryption keys they must crack a 64 bit key, the same as with a US encryption key. In the international version we take 24 of the 64 bit encryption key and encrypt the 24 bits with the NSA&#x27;s public key and send it, encrypted strongly, along with the encrypted message. This means the NSA can decrypt with their key and have 24 of the 64 bit key. They still have to break the remaining 40 bits. 40 bit key encryption has been the max for exportable encryption and that is what all other US exportable encryption providers allow. That limit has just been raised to 56 bits and we are incorporating that as I type. In the worst case: the NSA&#x27;s private key is compromised, the 40 bit portion of the key still must be cracked. So we haven&#x27;t weakened the security of international encryption, but actually made it equal to the US security (to everyone but the NSA). We are proud of this arrangement because we have found a way to make Notes as secure as the US government will allow for our international customers. If we hadn&#x27;t used this technique all of the international notes encrypted data would be with only a 40 bit key. As it stands, the 64 bit key used in both US and international encryption is extremely secure. It&#x27;s too bad the author of this article choose to attack Lotus Notes without considering the options the US government provides. We could have just shipped 40 bit encryption like MS, Netscape, etc. and leave our international customers with weak encryption but we didn&#x27;t. Oh well, you can&#x27;t make everyone understand, this confusing and frustrating stuff. I hope this helps. reply ChrisArchitect 21 hours agoprev(2002)Some previous discussions all mentioning Lotus Notes in the title:4 years agohttps:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=218595818 years agohttps:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=929140410 years agohttps:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=5846189 reply dang 14 hours agoparentThanks! Macroexpanded:NSA&#x27;s Backdoor Key from Lotus Notes (2002) - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=21859581 - Dec 2019 (87 comments)NSA&#x27;s Backdoor Key from Lotus Notes - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=9291404 - March 2015 (51 comments)NSA&#x27;s Backdoor Key from Lotus Notes - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=5846189 - June 2013 (85 comments) reply lelandfe 22 hours agoprevGood ole&#x27; \"NOBUS.\" More fun NSA fumbles:https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Clipper_chiphttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Dual_EC_DRBG reply kmeisthax 21 hours agoparentThis and the Clipper Chip aren&#x27;t NOBUS. The NSA doesn&#x27;t want you to know that the cryptosystem has law-enforcement access capability. The FBI doesn&#x27;t care if you know as the kinds of criminals they are attacking don&#x27;t do OPSEC. reply sneak 21 hours agorootparentNOBUS isn&#x27;t just intentional vulnerabilities, it&#x27;s any vulnerability assumed to only be exploitable by US IC, whether engineered or otherwise.I think these qualify. reply rvnx 21 hours agorootparentWell, the article mentions backdoor in Dual_EC_DRBG mostly targeting TLS&#x2F;SSL communications, now we have Cloudflare, a much more scalable solution reply tptacek 16 hours agorootparentprevDual EC is sort of the archetypical NOBUS backdoor. reply thesuitonym 20 hours agoprevIt&#x27;s amazing to me that the folks at the NSA had enough self-reflection to see that this is Big Brother behavior, but not enough to realize why that&#x27;s a bad thing. reply masfuerte 17 hours agoparentI&#x27;d guess that was snark from the Lotus engineer who embedded it. reply consoomer 19 hours agoprevWasn&#x27;t the original backdoor in a code example the NSA provided to companies interested in using cryptography? They gave an example seed or whatever, and most companies copy&#x2F;pasted it instead of generating their own primes, so the NSA could break it trivially.My memory around this is fuzzy and I can&#x27;t seem to find the original source. reply _def 11 hours agoparentThis one? https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Dual_EC_DRBG reply consoomer 10 hours agorootparentAh yeah, that rings a bell now! reply MaintenanceMode 9 hours agoprevNow with the cloud none of this is necessary. With data at rest laws, all our email older than six months is open game. reply agazso 21 hours agoprevI wonder how difficult would it be to brute force the private key for an RSA 760 bit public key from 1998. Does anyone know? reply tgsovlerkhgsel 20 hours agoparenthttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Integer_factorization_records and https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;RSA_numbers gives some pointers. Specifically, the latter describes a 768 bit key being factored \"on December 12, 2009, over the span of two years\", with CPU time that \"amounted approximately to the equivalent of almost 2000 years of computing on a single-core 2.2 GHz AMD Opteron-based computer\".Later, in 2019, a 795 bit key was factored with CPU time that \"amounted to approximately 900 core-years on a 2.1 GHz Intel Xeon Gold 6130 CPU. Compared to the factorization of RSA-768, the authors estimate that better algorithms sped their calculations by a factor of 3–4 and faster computers sped their calculation by a factor of 1.25–1.67.\"So assuming the better algorithms transfer to smaller numbers, someone who knows how to use them (factoring big numbers seems significantly harder than just running CADO-NFS and pointing it at a number and a cluster) could probably do it in a couple months on a couple dozen modern machines.For example, using the \"795-bit computations should be 2.25 times harder than 768-bit computations\" from the publication accompanying the second factorization, we could assume 900&#x2F;2.25 = 400 Core-years of the Xeon reference CPU (which is 6 years old by now) would be needed to break the smaller key with the modern software. Two dozen servers with 64 equivalently strong cores each would need slightly over 3 months. Not something a hobbyist would want to afford just for fun, but something that even a company with a moderate financial interest in doing could easily do, provided they had people capable of understanding and replicating this work. reply rocketnasa 13 hours agorootparentClassic CPU hasn&#x27;t held a candle compared to GPU on very repetitive math calculations. AI this year has really shown the same difference. In other words, it isn&#x27;t just graphics... https:&#x2F;&#x2F;www.spiceworks.com&#x2F;it-security&#x2F;identity-access-manag... reply tgsovlerkhgsel 12 hours agorootparentI assume there is some reason why the past factorizations weren&#x27;t done with GPUs. It could be just lack of a good implementation and insufficient numbers of people interested in the topic, but it could also be something about the algorithm not being very suitable for GPUs. reply boastful_inaba 3 hours agorootparentCUDA only had its initial release in 2007 (compared to the mentioned crack in 2009), and I remember it being a fairly limited product at that point. GPUS were also much slower back then. reply btdmaster 20 hours agoparentprevSomeone has tried to factorize it before (2018) http:&#x2F;&#x2F;factordb.com&#x2F;index.php?query=444376527415060195687748... reply panki27 21 hours agoparentprevAlways depends on what resources you have (compute, time). It&#x27;s possible, but not easy.https:&#x2F;&#x2F;crypto.stackexchange.com&#x2F;a&#x2F;1982 reply 15457345234 20 hours agoparentprevOddly specific question, something in particular on your mind? reply cmeacham98 20 hours agorootparentPresumably they are referring to the 760 bit RSA key this entire post is about. reply 15457345234 20 hours agorootparentBut the header talks about a 64 bit key? I&#x27;m a bit lost actually.Edit: Okay, I see it now. 64 bits of cipher of which 24 bits of that cipher are set to a value derived from a 760 bit pubkey. reply leoh 16 hours agoprevRelated: https:&#x2F;&#x2F;github.com&#x2F;goshacmd&#x2F;nsa_panel reply Quentincestino 22 hours agoprevnext [2 more] [flagged] denysvitali 22 hours agoparentYou probably meant: https:&#x2F;&#x2F;thebestmotherfucking.website&#x2F; reply spzb 22 hours agoprev [–] Dupe (2002!) https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=21859581With no context, I don&#x27;t know why this is front page news today. Am I missing something? reply dredmorbius 22 hours agoparentThis would be a repost rather than a dupe.HN considers dupes to be stories with significant discussion repeated within a year. (Items with little or no discussion can be resubmitted a few times.)Stories reshared after a year are reposts, and are perfectly fine, though its appreciated to have the item&#x27;s original publication year included in the title. reply baby 22 hours agoparentprevAre you asking what reposts are? reply spzb 22 hours agorootparentNo. I&#x27;m pointing out that (a) it&#x27;s not marked as being from 2002 and someone would therefore assume it was some newly discovered backdoor and (b) there&#x27;s no context or commentary as to why it is relevant in 2023.Also, on closer inspection the story is from 1997 https:&#x2F;&#x2F;catless.ncl.ac.uk&#x2F;Risks&#x2F;19.52.html#subj1 reply dredmorbius 22 hours agorootparentI&#x27;ve pinged mods to fix the year based on that, thanks. reply boffinAudio 22 hours agorootparentprevI&#x27;d wager that its still relevant today because the NSA is still the worlds greatest wholesale violator of human rights, at massive scale, and literally nothing effective has been done about this situation - we are still tolerating this repression, because we don&#x27;t see it and simply don&#x27;t care enough about the human rights violations, as a people, to reign in this out of control agency.Bringing these articles to light is of great utility to those of us who do not consider the NSA state of affairs to be, in any way, tolerable. reply acdha 21 hours agorootparent> the NSA is still the worlds greatest wholesale violator of human rights, at massive scale, and literally nothing effective has been done about this situation - we are still tolerating this repressionI don’t approve of their actions but turning the hyperbole up to 11 doesn’t help. There are millions of people in China who’d love to be only that repressed, for example. reply boffinAudio 2 hours agorootparentYou can always rely on an American to bust out the China hate train when challenged on the facts of their own empires crimes ..Did you miss the fact that the NSA is literally violating the human rights of billions of people (including the Chinese), while China in the meantime has brought a billion people out of poverty conditions into their new middle class?>There are millions of people in China who’d love to be only that repressed, for exampleI seriously doubt you understand the nature of this fallacy. Meanwhile, how many families live under a broken bridge in the USA, just because Mom got cancer? Those 1,000 black-ops CIA sites around the world - you know for sure what they are being used for, eh? No torture?Seriously, get a grip. The moral authority you claim is a fallacy. reply FredPret 21 hours agorootparentprev... are you serious?You don&#x27;t think military invasions & communist dictatorships constitute \"wholesale violation of human rights at a massive scale\"?If the NSA is spying on people, that&#x27;s an invasion of their privacy, but it is nothing in comparison to those other violations reply boffinAudio 2 hours agorootparentIts a massive, wholesale violation of human rights, which can then be used as further justification for more atrocities and calamity at the hands of the US&#x27; military industrial complex ..And yes, the USA is still the worlds worst violator of human rights, bar none. The NSA is why. reply kmeisthax 21 hours agorootparentprevThe NSA violates privacy at scale - a lot of little violations of civil liberties. It&#x27;s the difference between robbing a man for everything he has, versus pick-pocketing 30 cents out of the pocket of every person on the planet.Furthermore, they&#x27;re part of a larger intelligence apparatus that has absolutely committed very large and very harmful violations of civil liberties. The NSA&#x27;s sister org, the CIA, was overthrowing democratically elected left-wingers in South America for decades, replacing them with brutal dictators and tyrants that gave both Hitler and Stalin runs for their money. The CIA wrote the book on how to do so, arguably even moreso than the KGB did. In fact, the reason why Russia today[0] is so effective at information warfare and covert propaganda is specifically because they learned from observation.[0] Not(?) to be confused with Russia Today reply tptacek 16 hours agorootparentIf you&#x27;re thinking about overseas signals intelligence, then, like the signals intelligence practice in every industrialized state in the world, the chartered purpose of NSA is to conduct those privacy violations. The safeguards we&#x27;re given against NSA --- take them as seriously as you want --- are about domestic surveillance. reply boffinAudio 54 minutes agorootparentJust because Americans believe they have domestic rights being protected doesn&#x27;t mean their intelligence apparatus isn&#x27;t violating human rights at massive scale.Yes, the purpose of the NSA is to violate human rights at scale. No, this is not a tolerable situation for those of us in the free world. replyollemasle 22 hours agorootparentprevAdding the date in the HN title would be better (it is not present in the article) reply nonrandomstring 22 hours agoparentprev [–] I think a Microsoft coder recently came clean about some pretty funky stuff from the 90s and 00&#x27;s. Hope I didn&#x27;t hallucinate that. reply EvanAnderson 21 hours agorootparentI feel like you might be talking about Dave Plummer: https:&#x2F;&#x2F;www.youtube.com&#x2F;@DavesGarageHe recently have a good talk at VCF, too: https:&#x2F;&#x2F;youtube.com&#x2F;watch?v=Ig_5syuWUh0 reply ranting-moth 21 hours agorootparentprev [–] Link? reply qingcharles 16 hours agorootparent [–] https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=vjkBAl84PJs reply ranting-moth 12 hours agorootparent [–] Thanks! reply Applications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article discusses the presence of a backdoor feature named \"differential cryptography\" in the export version of Lotus Notes, enabling the NSA to access encrypted data by brute-forcing a part of the encryption key.",
      "The author reverse engineered the NSA's public key, unearthing that it carried the organizational name \"MiniTruth\" and common name \"Big Brother,\" reminiscent of the totalitarian regime in George Orwell's novel \"1984.\"",
      "The text also furnishes raw public key modulus and a formatted rendition of the NSA's public key."
    ],
    "commentSummary": [
      "The discussion revolves around encryption aspects, highlighting the use of a \"backdoor key\" by NSA in Lotus-Notes software and the curtailment on robust encryption during that period.",
      "It probes into the vulnerabilities and risks linked with Intel ME, security of the ACME protocol utilized by Let's Encrypt, and the alleged human rights transgressions by US intelligence agencies.",
      "Overall, the post stresses the conundrums and apprehensions implicated with encryption and privacy."
    ],
    "points": 332,
    "commentCount": 147,
    "retryCount": 0,
    "time": 1695034037
  },
  {
    "id": 37556025,
    "title": "Replanting logged forests with diverse seedlings accelerates restoration",
    "originLink": "https://www.technologynetworks.com/applied-sciences/news/replanting-logged-forests-with-diverse-mixtures-of-seedlings-accelerates-restoration-378916",
    "originBody": "We've updated our Privacy Policy to make it clearer how we use your personal data. We use cookies to provide you with a better experience. You can read our Cookie Policy here. I Understand Advertisement Skip to content or Skip to footer Applied Sciences Search Stay up to date on the topics that matter to you Science News Communities Content Subscribe Now Home Applied Sciences News Content Piece Replanting Logged Forests With Diverse Mixtures of Seedlings Accelerates Restoration Replanting logged forests with a mixture of seedlings significantly accelerates recovery, reports a new study. News Published: September 18, 2023Original story from the University of Oxford Credit: Crystal Mirallegro/ Unsplash Download Article Facebook Twitter LinkedIn Reddit Share Listen with Speechify 0:00 Register for free to listen to this article Read time: 4 minutes Satellite observations of one of the world’s biggest ecological experiments on the island of Borneo have revealed that replanting logged forests with diverse mixtures of seedlings can significantly accelerate their recovery. The results have been published today in the journal Science Advances. The experiment was set up by the University of Oxford’s Professor Andy Hector and colleagues over twenty years ago as part of the SE Asia Rainforest Research Partnership (SEARRP). This assessed the recovery of 125 different plots in an area of logged tropical forest that were sown with different combinations of tree species. The results revealed that plots replanted with a mixture of 16 native tree species showed faster recovery of canopy area and total tree biomass, compared to plots replanted with 4 or just 1 species. However, even plots that had been replanted with 1 tree species were recovering more quickly than those left to restore naturally. Lead Scientist of the study, Professor Andy Hector (Department of Biology, University of Oxford) said: ‘Our new study demonstrates that replanting logged tropical forests with diverse mixtures of native tree species achieves multiple wins, accelerating the restoration of tree cover, biodiversity, and important ecosystem services such as carbon sequestration.’ Greater diversity gives greater resilience According to the researchers, a likely reason behind the result is that different tree species occupy different positions, or ‘niches’, within an ecosystem. This includes both the physical and environmental conditions that the species is adapted to, and how it interacts with other organisms. As a result, diverse mixtures complement each other to increase overall functioning and stability of the ecosystem. For instance, some tropical tree species are more tolerant of drought because they produce a greater amount of protective chemicals, giving the forest resilience to periodic times of low rainfall. Professor Hector added: ‘Having diversity in a tropical forest can be likened to an insurance effect, similar to having a financial strategy of diverse investment portfolios.’ In turn, a diverse mix of trees can support a much wider range of animal life. For instance, hornbills specifically require large mature trees with holes where the females can nest. One of the world’s biggest ecological experiments Tropical forests cover just 6% of the planet’s land surface but are home to around 80% of the world’s documented species (WWF), and act as major carbon sinks. However, these critical habitats are disappearing at an alarming rate, chiefly due to logging for timber and conversion to palm oil plantations. Between 2004 and 2017, 43 million hectares of tropical forest were lost - an area roughly the size of Morocco (WWF). Restoring logged tropical forests is a crucial component of efforts to tackle both the nature and climate crises. Up to now, however, it has been unclear whether this is best achieved through allowing forests to restore themselves naturally (using dormant seeds in the soil) or through active replanting. Want more breaking news? Subscribe to Technology Networks’ daily newsletter, delivering breaking science news straight to your inbox every day. Subscribe for FREE To investigate this, the researchers collaborated with local partners to set up the Sabah Biodiversity Experiment on 500 hectares of logged forest in the Malaysian state of Sabah on the island of Borneo. This was divided into 125 experimental plots that were either left to recover naturally or planted with mixtures of either 1, 4, or 16 tree species that are frequently targeted for logging. The 16 species included several endangered species and the worlds’ tallest species of tropical tree (Shorea faguetiana) which can reach over 100 m in height. The first trees were planted in 2002, with nearly 100,000 planted in total over the following years. The recovery of the plots was assessed by applying statistical models to aerial images captured by satellites. Within a few years, it became apparent that those with 1 species did worse than those planted with a mixture of 4 species, and those enriched with 16 species did best of all. Lead author Ryan Veryard (who analysed the data as part of his PhD at the University of Oxford), said: ‘Importantly, our results show that logged forest can recover so long as it is not converted to agricultural uses like oil palm plantation. They also emphasise the need to conserve biodiversity within undisturbed forests, so that we can restore it in areas that have already been logged.’ The Sabah Biodiversity Experiment team are now starting a new three-year project funded by the UK Natural Environmental Research Council to take a census of all the surviving trees in the experiment. This will be combined with a wider range of remote sensing methods (including lidar sensors carried by a helicopter and smaller sensors carried by drones) to give a more comprehensive analysis of forest health. Reference: Veryard R, Wu J, O’Brien MJ, et al. Positive effects of tree diversity on tropical forest restoration in a field-scale experiment. Sci Adv. 2023;9(37):eadf0938. doi: 10.1126/sciadv.adf0938 This article has been republished from the following materials. Note: material may have been edited for length and content. For further information, please contact the cited source. Advertisement Trending Essential Amino Acids: Chart, Abbreviations and Structure Article Ask Me Anything: The Future of Battery Research Webinar \"Sea-Weeding\" Project Helps Corals To Recover News Advertisement Never miss a story with the Breaking Science News daily newsletter Subscribe for FREE Technology Networks About Us Contact Us Scientific Content Creation Careers Editorial Policies Aims and Scopes Editorial Guidelines Meet the Team Scientific Advisory Board Advertise With Us FAQs Terms and Conditions Privacy Policy and Disclaimer Cookie Policy ©2023 Technology Networks, all rights reserved. Part of the LabX Media Group",
    "commentLink": "https://news.ycombinator.com/item?id=37556025",
    "commentBody": "Replanting logged forests with diverse seedlings accelerates restorationHacker NewspastloginReplanting logged forests with diverse seedlings accelerates restoration (technologynetworks.com) 332 points by myshpa 20 hours ago| hidepastfavorite122 comments atoav 13 hours agoMy brothers MA thesis was about comparing the multiple model forests that have been planted throughout the EUs different climate zones (I didn&#x27;t even know such a thing existed, bur it makes sense).His focus was looking at resilience against weather, insects etc. and mixed forests fared significantly better throughout all climate zones.As someone from the alps, a thing that should not be forgotten is how important a diverse tree structure can be for stabilizing soil, especially in mountain areas. And those areas can expect more extreme wheater conditions due to climate change, especially in the form of rain. Mudslides can become a real economic factor in such regions.Mixed forests are also better at stabilizing the soil because the root structures are less uniform.So the best moment to plant mixed forests is 20 years ago, the next best is now.Edit: For non-wood people, the reason why there aren&#x27;t more mixed woods is that harvesting is easier in non-mixed environments (although that also has changed with newer methods and tech). reply asddubs 9 hours agoparentYou just have to look at how utterly the Harz pine forests have been completely destroyed by the bark beetle to see what monocultures get you. It&#x27;s just huge swaths of lands filled with nothing but dead trees reply Biologist123 1 hour agorootparentI have heard estimates of 100 million trees dead in the Harz. The common refrain is that this was caused by climate change, but also choice of monoculture was a major contributing factor as the area was historically covered by mixed deciduous. Furthermore, choice of forest type will affect soil moisture retention, so reduced rainfall would have been buffered by better choice of forest type.Loss of those trees was an economic disaster. I’m not sure it’s an environmental disaster yet - especially as replanting seems to be with better reference to the agro-climatic zone. reply killerpopiller 44 minutes agorootparentI‘ve heard from a local that in the onset of the bork beetle infestation, it was decided not to intervene (leaving dead trees there). This contributed to the catastrophe. Not sure if this is true. reply Xylakant 1 hour agorootparentprev> Loss of those trees was an economic disaster.Given that most of the trees lost were in the nature reserve, the economic impact is somewhat limited. reply HarryHirsch 9 hours agorootparentprevThere&#x27;s the additional complication that when the beetle or the windstorm come there&#x27;s suddenly an overabundance of wood on the market, which depresses prices. You&#x27;d need to do forest maintenance but cannot afford it, market forces are against it. It&#x27;s like the Schweinezyklus but worse. reply robocat 8 hours agorootparent> SchweinezyklusTranslation is probably Boom-bust cycle (literal: swine cycle?)https:&#x2F;&#x2F;dictionary.reverso.net&#x2F;german-english&#x2F;Schweinezyklus for some rather poor usage examples reply lysium 5 hours agorootparentIt’s pork cycle https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Pork_cycle reply Lutger 1 hour agoparentprevIn some tree monocultures, you can just see the soil is covered with layers of organic matter that just isn&#x27;t decomposing fast enough. This is because there isn&#x27;t the diversity in the soil food web required to process it, reflecting the lack of biodiversity above the ground. Such soils often have a very low ph and poor nutrient cycling. reply wesapien 5 hours agoparentprevI was just listened to a podcast on this and they said planting a bunch of trees don&#x27;t make a forest especially tropical ones. There&#x27;s just too much going on there. reply taylorius 4 hours agoparentprevNo offense meant at all to your brother, it sounds like an interesting investigation. That said, the answer to these sorts of eco-questions always seems to be that the way nature has worked for millions of years is actually the best way (for loads of reasons we didn&#x27;t know) - and if we mess with it too much, there will be some problems. This just seems kind of inevitable though. If nature&#x27;s way didn&#x27;t work, it wouldn&#x27;t be happening. I suppose finding the reasons behind it being optimal are where the \"meat\" of such an investigation is. Interesting stuff. reply dagmx 3 hours agorootparentYour comment bothers me because it is based on very shaky premises.Nature doesn’t work in the most optimal way, so it’s not always best to replicate it. Tons of things happen in nature that may wipe out important ecosystems. I suppose one could argue that at a long enough time scale it will even out but that time scale can be millennia. In the meantime, nature can wreak tons of havoc.Nature (and by that I mean ecosystems because nature is a vague) also doesn’t behave in necessarily intuitive or easily observable ways either. What might seem “obvious” at first can be very wrong.Nature (and again I mean ecosystems) also doesn’t exist in vacuums. What may have worked for centuries can be upset by even the most subtle of temperature shifts, and those might be caused by effects hundreds of miles away.Unchecked nature is also not always going to be conducive to human needs either.The entire field requires constant observation and inference. Even the ways of indigenous people that are fetishized as natures way by many, are scientific in nature. They’re the result of many years of observation, experimentation, learning and modifying how nature works to fit our needs. reply vintermann 2 hours agorootparentNothing is \"optimal\", it&#x27;s only ever \"optimal for\". So asking if nature is optimal is nonsense unless we have a firm agreement on what nature is for (which we certainly don&#x27;t).But the linked article doesn&#x27;t have this problem. The \"for\" there is quite clearly the recovery of forests after logging, so they can be used again for whatever you may be expected to use a forest for (including more logging) reply dagmx 2 hours agorootparentI think you’re arguing against a point that I am not making. In fact I think you’re making the same argument that I am, which is “nature isn’t seeking an optimal outcome because that is too abstract”.The person I was replying to was saying “just do what nature does”. But that doesn’t mean much. My point was you can’t just blindly follow “nature” without observation and study because it can just as easily go and do something catastrophic as it can do something good. reply flakeoil 3 hours agorootparentprev> \"the most optimal way\"Optimum in what sense?Make as much cash as quickly as possible or have a forrest and a working eco system (or even any trees at all) also in 100+ years from now. reply dagmx 2 hours agorootparentOptimal as in preserving life if we’re going based off of the person I was replying to, but in general that is hard to define which is part of my point . Nature can just as quickly extinguish entire ecosystems, just as much as it can make them thrive.Just trusting nature to do the optimal thing and trust that it’ll turn out okay isn’t sensible unless you also don’t care about the constituent parts of that ecosystem.Insects can ravage entire populations of trees, rendering them dead and inhospitable to wildlife. Blight can do the same. Those are just as much nature at work as happy trees in a forest. reply Lutger 1 hour agorootparentprevWell, yes and no. Yes, in the sense that &#x27;nature&#x27; is what organisms learned in millions of years and the emergent processes of the ecosystems in which they &#x27;learned&#x27; things. And because this is so vastly complex, we often don&#x27;t understand it and our interventions will have unintended consequences.No, in the sense that humans are actually part of these ecosystems and it is possible for us to understand these processes and work with them. And this is the point of the research posted here: planting native seedlings is superior for restoration than &#x27;just leaving things to nature&#x27;.There&#x27;s a technical side to this and a &#x27;worldview&#x27; side, where there are two competing concepts of &#x27;nature&#x27;: one defines it as &#x27;everything except humans&#x27; and the other includes humans as being part of nature, a useful part even. reply atoav 3 hours agorootparentprevI just know what he told me about it verbatim, but the \"meat\" of his investigation was that he did statistical analysis on vast amounts of (LIDAR-based) tree-height data and comparison of storm damages. On top there were many other GIS-data layers included, so in addition to that simple finding the \"meat\" is that he found multiple concrete combinations of circumstances that are especially benefitial to resilience.Not that a skilled and experienced ranger might not have known some of these combinations as well, but the data-based approach can help planing better where to plant what specifically, as opposed to just mix it based on chance. reply saiya-jin 2 hours agoparentprevWe had in Slovakia in 2004 a small catastrophe in our tiny higher mountains (High Tatras) - wind up to 110 kmh IIRC, and literally large swathes of pine&#x2F;spruce forests that were on higher grounds were cut like with lawnmower. They often broke in half with bare trunk still standing, not even ripped out of the ground. Then parasitic beetles had a feast and their population exploded.Main reason was stated as monoculture planted decades ago, since these forests are pretty and they were used also for treating respiratory illnesses (clean mountain air with strong pine sap smell really helps, sort of natural aromatherapy).Almost 20 years passed, its still a sad sight. More logging, some ad hoc growth but forest definitely didn&#x27;t regrow in any meaningful form, rather fast growing bushes took over. There was a lot of fight between forest &#x2F; natural park management and ecologists on how to proceed, and everybody (including forest) lost. reply chiefalchemist 10 hours agoparentprev> His focus was looking at resilience against weather, insects etc. and mixed forests fared significantly better throughout all climate zones.It makes sense. The diversity means that while one genera is suffering from some Hostility X or Y (e.g., insects) the others might not be, or at least as much so.Diversity is a form of hedging your bet. reply Lutger 1 hour agorootparentHedging your bet is one part, another big one is creating habitat for predators. Pests often arise in monocultures because there just isn&#x27;t any suitable habitat for the organisms that eat them.A part of that habitat is food, so ironically and perhaps counter intuitively, a good agro-ecological practice is to farm your pests. This provides a source of food (but not in your main crop) to attract the predators that you want to help control the pest.This is the reversal of a negative spiral where you control pests by killing them, thus preventing predators to settle in your land and making it a very nice place for the pests to multiple. And they will, because you can never kill them all - unless you keep using massive amounts of pesticides. This is why monocultures often need a lot of pesticides to work, its very hard to do without. reply rootw0rm 5 hours agoparentprevafaik it helps guard against particularly devastating wildfires, too. reply mtnGoat 4 hours agorootparentThe best way to deal with fires is not to. The problems the west is facing are due to so much fire suppression for tire last century and a half left too much fuel near the ground, we are now learning to let them burn. reply dagmx 3 hours agorootparentThis is somewhat simplistic. There are many methods to wild fire control, including prescribed burns.Yes, many forestry practices are antiquated and incorrect, but it’s not as simple as just letting fires burn.There’s a significant science to forestry and wildfire management. Fire suppression is as important as fire management and fire encouragement. They go hand in hand, and require a measured balance reply atoav 3 hours agorootparentprevNot to get too political here, but the problem is literally that we have \"raked the forest\" too much.If you don&#x27;t know how this could be political, be prepared to make your day a whole lot dumber if you google it. reply ccooffee 18 hours agoprevSomeone logged a 500 hectare plot in Borneo. They split this into 125 sections and planted 0, 1, 4, or 16 \"tree species that are frequently targeted for logging\". After 20 years, satellite imagery shows that the more tree species you planted, the more recovered the land appears to be.I&#x27;m left wondering:1. Why did they plant only tree species that are frequently targeted for logging? This makes the whole experiment very suspect. The linked article talks a lot about restoring forests, but why restrict the tree species to those that are profitable to log?2. Is the satellite imagery actually representative of on-the-ground truth? A lot of logging land in western America gets replanted with logging-friendly trees in very regular grid patterns. These areas may look like forests from satellites (or to uninformed ground-level visitors), but the regrown tree farms do not behave like forests. The dense growth crowds out the ground-level plants, which in turn makes the entire tree farm a poor habitat for local fauna. If your goal is to grow more trees for lumber, tree farms are great. But I&#x27;m not sure the claims about \"forest restoration\" are honest&#x2F;true here. reply mschuster91 17 hours agoparent> Why did they plant only tree species that are frequently targeted for logging? This makes the whole experiment very suspect. The linked article talks a lot about restoring forests, but why restrict the tree species to those that are profitable to log?Because that is what private land owners will do, they&#x27;ll want to plant primarily what they can sell. This research likely intended to reduce the immediate damage from logging. reply kmeisthax 15 hours agorootparentInterestingly enough, this is also part of the reason for Canada&#x27;s horrible wildfires a few months back: https:&#x2F;&#x2F;pluralistic.net&#x2F;2023&#x2F;09&#x2F;16&#x2F;murder-offsets&#x2F;#pulped-an... reply llbeansandrice 16 hours agoparentprev> Why did they plant only tree species that are frequently targeted for logging?Because the main purpose of replanting trees to is be able to harvest them again in the next few decades. Private land owners generally aren&#x27;t interested in creating old-growth forests, they&#x27;re trying to make money.It&#x27;s not exactly ideal, but ending up with more biodiversity is likely a good thing even if it will be logged again later.If you want more old-growth forests there&#x27;s going to have to be a _lot_ more subsidies to private owners to literally pay them to not log their land. reply rootusrootus 14 hours agorootparent> Private land owners generally aren&#x27;t interested in creating old-growth forests, they&#x27;re trying to make money.To be clear, in the western US this is by design. Large swaths of private land are zoned for forest. Aside from a few niche instances of grandfathering, you cannot build on them. They&#x27;re useful for recreation and logging, and that&#x27;s all that&#x27;s allowed.The gov&#x27;t wants them to be logged regularly. If they really wanted old growth forests they&#x27;d make it public land (it&#x27;s not especially expensive land, either, right after a patch gets logged it&#x27;s not uncommon for the owner to put it on the market fairly cheap). reply voisin 14 hours agorootparentprev> Private land owners generally aren&#x27;t interested in creating old-growth forests, they&#x27;re trying to make money.In Canada the vast majority of logging is on crown land. reply adasdasdas 10 hours agorootparentprevWhy do we want old growth forests over harvestable forests? Is there an ecological advantage reply sdenton4 3 hours agorootparentOld growth forward have a range of incredible ecosystem benefits, including a big effect on river health.The &#x27;Timber Wars&#x27; podcast was a six part story on the Pacific Northwest, including a lot of the history of the science on logging and forest health, as it evolved from the eighties through to today.https:&#x2F;&#x2F;www.opb.org&#x2F;show&#x2F;timberwars&#x2F; reply taway_6PplYu5 9 hours agorootparentprevActually I&#x27;ve heard that the best ecological and economic outcome is to manage an old growth forest and log it selectively using patterns that mimic tree loss from non-human activity.Maximizing ecological advantage also maximizes economic advantage, even in the short-medium term. reply thaumasiotes 4 hours agorootparent> I&#x27;ve heard that the best ecological and economic outcome is to manage an old growth forest and log it selectively using patterns that mimic tree loss from non-human activityI don&#x27;t see how this is supposed to be accomplished. For that concept to make any sense, you&#x27;d need to prevent the tree loss from non-human activity, which is all but impossible to do.If you can&#x27;t do that, then the existing pattern of loss already looks like \"loss from non-human activity\", and any logging you do will look like \"a lot more loss than typical of non-human activity\". reply harywilke 17 hours agoparentprevThis is an aside. This [0] is what an attempt at balancing logging, locals, and forest conservation ~150 years ago looked like. The checkerboard effect [1] is pretty striking. This strategy ended up being a disaster for some animals, famously the Northern Spotted Owl.[0] https:&#x2F;&#x2F;www.google.com&#x2F;maps&#x2F;@43.4146826,-123.52657,129879m&#x2F;d... [1] https:&#x2F;&#x2F;osupress.oregonstate.edu&#x2F;blog&#x2F;checkerboard-effect [2] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Northern_spotted_owl reply liotier 18 hours agoparentprevBetween clear-cut horror and ideal pristine old growth, there is a world of managed forests that fix carbon with an economically sustainable model. Not the best biodiversity but mixing sixteen species makes the initiative top tear already. reply unglaublich 14 hours agorootparentAnd generally, these forests are subdivided in plots of different &#x27;age&#x27;. Every year, they will log 1&#x2F;20th of the forest or so. The wildlife might be able to move from an affected area to one of the bordering areas.In fact, this model comes quite close to natural destruction of forests, where old trees would fall over, and wildfires would rage.The only difference is that the process is not random, but nicely planned and managed to allow _humans_ instead of _wildfires and storms_ to reap the full-grown timber. reply joshvm 14 hours agoparentprevRe satellite. At Sentinel resolution (10-20m) not much, maybe enough to distinguish plantation from natural forest spectrally. At Planet (3m) and below you can start to see large individual trees.It&#x27;s very difficult to accurately measure biodiversity from space. Drone imagery might get you species visually but until we have widespread hyperspectral (see ESA CHIME) 12-13 bands is what most people work with. reply soperj 18 hours agoparentprev> 1. Why did they plant only tree species that are frequently targeted for logging? This makes the whole experiment very suspect. The linked article talks a lot about restoring forests, but why restrict the tree species to those that are profitable to log?Because the whole point of tree planting is forest management. That&#x27;s why whenever there&#x27;s a forest fire they spray it with glyphosate so that other trees don&#x27;t grow, then they plant GMO trees that can live in glyphosate doused soil. reply drone 17 hours agorootparentGlyphosate is not soil active, so there are no \"trees that can grow in glyphosate-doused soil.\"The primary reason for broad herbicide treatment as part of site prep is to avoid low-value, or ecologically opportunist species that thrive in disturbed soil&#x2F;land, and prevent either the target species from growing, or create an environment which lacks the diversity necessary for the region. For example, sweetgum, huisache, black locust, chinese tallow (as examples from specific regions in the US), will all take over and completely dominate a deforested section and prevent oaks, pines, etc. and appropriate forb for wildlife without consistent, ongoing burns.FWIW, there are no \"trees which are GMOd to live with glyphosate application\" - you&#x27;re thinking non-tree crops. Nearly every softwood and hardwood tree is susceptible to damage from Glyphosate. reply dudeofea 15 hours agorootparentwhy not plant other species to out-compete the invasive ones?why do we need to perform chemotherapy on our forests? reply drone 14 hours agorootparentOnly one of the trees I listed was invasive, the others are opportunistic natives to their regions that will outgrow everything else.The nice \"diverse\" forest you&#x27;re thinking of in your mind took a long time to become that way, the normal state of nature is to not create a perfect balance out of the gate, but for constant competition and regularly have to cycle through multiple iterations of configuration which are, by all means, not as productive or valuable for wildlife&#x2F;nature as their final states. None of that means that using a herbicide is sufficient, but without, you&#x27;re looking at potentially hundreds of years to get back a usable environment for wildlife that is well-balanced vs 10&#x27;s of years.Outside of a few soil-active herbicides, most of what they use is one-and-done and can be applied selectively to only problem plants with minimal unintended consequences. reply hutzlibu 10 hours agorootparentWhere I live in germany, there used to be extensive spruce monocultures forests everywhere.They are mostly chopped down now and replaced with a mixed young forest, all without herbicides. (But with some planted trees, cleansing and fences to protect the young forest from deers) So after 15 years they surely are not comparible to old grown forests, but they are very diverse and alive. So I strongly question the assumption that herbicides are necessary or beneficial to create a diverse forest.Most of the dominating species in the first years will be (were) replaced by something else eventually. reply anon84873628 11 hours agorootparentprevTo elaborate on this great answer, the technical term is \"ecological succession\", defined on Wikipedia as \"the process of change in the species that make up an ecological community over time.\"Plants do not just fill their niche, they alter the environment over time, which in aggregate alters the ecosystem as a whole. Animals and microbes also play a role in this process. E.g. the way rodents and birds disperse seeds, or how pests can destroy a species, or even how elephants can uproot whole trees. reply thaumasiotes 4 hours agorootparentprev> the normal state of nature is to not create a perfect balance out of the gateThat is true. Additionally, a balance will never be achieved no matter how long you wait, either. That&#x27;s the state of nature; some things are always replacing other things. reply RGamma 4 hours agorootparentThat&#x27;s dangerously simplistic, see https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ecological_stability and related topics for a start. reply thaumasiotes 3 hours agorootparentEcological stability refers to a concept that is never realized. You can pretend it exists by ignoring the variation that you feel is unimportant. Over time, that variation will make its way into the areas that you thought were stable. replygreenie_beans 16 hours agoparentprev> 1. Why did they plant only tree species that are frequently targeted for logging? This makes the whole experiment very suspect. The linked article talks a lot about restoring forests, but why restrict the tree species to those that are profitable to log?Because forest management is for logging. They will log those trees once they mature to the best value when considering DBH, the market, and opportunity cost. reply thaumasiotes 4 hours agoparentprev> the regrown tree farms do not behave like forests. The dense growth crowds out the ground-level plantsCrowding out ground-level plants is the entire point of being a tree; it&#x27;s pretty normal for forests to have clear ground.Here&#x27;s what a redwood forest looks like: https:&#x2F;&#x2F;www.westwindvistas.com&#x2F;Redwood%20Forest%20Floor.htm reply toast0 17 hours agoparentprev> Why did they plant only tree species that are frequently targeted for logging? This makes the whole experiment very suspect. The linked article talks a lot about restoring forests, but why restrict the tree species to those that are profitable to log?Logging companies typically log a parcel and replant for logging again in the future. They might be convinced to do things differently, especially if the outcome is better for them, but it would be hard to convince them to plant trees that won&#x27;t be commercially viable when they come back to log again.If diversity is good for the environment and the loggers, that seems ideal. If diversity is good for the environment and about the same for the loggers, they might be convinced.Not all the parcels will end up being relogged, but that decision is unlikely to be made at the time of replanting. reply atourgates 17 hours agoprevI did a quick look to see if this only worked in tropical forests, or if this would be true in, say, Western North American forests.I didn&#x27;t really find an answer.A study[1] in Virgina found that planting multiple varieties of trees was beneficial because it allowed the variety that was most suitable for that location to thrive, and survive problems that might affect other varieties.A study[2] in Washington State tested a couple varities of common conifers planted in pairs, and found more conventional \"trees are affected by competition\" result.This study[3] performed in the inter-mountain West found that some conifers _may_ benefit from being mixed with aspens, but didn&#x27;t seem nearly as conclusive as the Borneo study.If anyone can find a more conclusive study about temperate Western forests, I&#x27;d love to see it.[1] https:&#x2F;&#x2F;www.si.edu&#x2F;newsdesk&#x2F;releases&#x2F;tree-species-diversity-...[2] https:&#x2F;&#x2F;cdnsciencepub.com&#x2F;doi&#x2F;10.1139&#x2F;X09-040[3] https:&#x2F;&#x2F;besjournals.onlinelibrary.wiley.com&#x2F;doi&#x2F;full&#x2F;10.1111... reply swader999 16 hours agoparentBC can have three our four species in play on the coast, interior and east typically has pine and spruce. You could argue for balsam but that is crap wood and it grows back on its own typically. I don&#x27;t know about tropical reforestation, but in Canada the notion that we are replanting with only mono species isn&#x27;t true. They leave seed trees standing and source cones directly from the logged blocks. It&#x27;s very well done and highly regulated. Blocks that don&#x27;t grow back are replanted again until they do.Canada replants 600 million trees annually, USA about 1 billion.Trees grow so much faster than they did a decade ago. CO2 is to &#x27;blame&#x27;. I help maintain ski runs at my favorite ski hill and it&#x27;s ridiculous how much more work it is now. Alpine areas that never in history had trees are beginning to get overrun. reply nerpderp82 12 hours agorootparentThe sub-alpine line is rising in elevation due to increased warming.https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC7696691&#x2F; reply swader999 12 hours agorootparentIs it T or CO2? reply 1970-01-01 6 hours agorootparentWarm soil causes seed germination. CO2 causes warm soil. So, both. reply voisin 15 hours agorootparentprevRoughly where are you in BC? I am in Cranbrook and haven’t seen the seed trees left standing in logged blocks but maybe I’ve missed them somehow.Isn’t part of the issue that historically there would’ve been more deciduous trees that acted as natural fire breaks and now loggers only are allowed to replant coniferous? reply swader999 14 hours agorootparentI&#x27;ve planted all over BC. Seed trees started being a thing early nineties and they don&#x27;t do it everywhere. Quite often its because there aren&#x27;t species needed as seed trees that won&#x27;t already grow back are already slated for nursery production and replanting.Smaller block sizes act this way too, the boundaries are seeded from the older growth on the perimeter.Deciduous trees like poplar, Adler, and birch are like weeds and will grow very quickly and compete for a time with replanted trees. Eventually the evergreens tend to choke them out by taking over the canopy and changing the soil with their needles. reply twunde 14 hours agorootparentYou may also hear this practice called selective cutting&#x2F;lumbering. Essentially they leave trees in ones or twos scattered through to reseed the area around it. reply swader999 13 hours agorootparentYeah maybe, selective cutting looks different though, like 1-2 hectare pieces and roads everywhere. This seed trees thing has blocks up to about 90 hectares that are rectangles with ten or twenty trees still standing in it. reply rcostin2k2 15 hours agorootparentprevThey grow faster but weaker, with lower density (cf. https:&#x2F;&#x2F;doi.org&#x2F;10.1016&#x2F;j.foreco.2018.07.045) reply MissingAFew 9 hours agorootparentI would suspect they would gradually evolve to grow stronger? reply hosh 16 hours agoparentprevThere’s a more thoughtful way of designing this. In the permaculture world, these would be called “guilds”. Species are selected with an understanding of canopy layers (so that plants don’t compete for sunlight and can still fill in spaces at each canopy layer), and ecological function (such as, nitrogen fixer, dynamic accumulators, pollinator attractors, habitats, etc)If you randomly mix up species in temperate forests that are all competing in the same canopy layer, I can see more competition. A study done where say, a mix of overstory, understory, and shrub (such as berries), would be more insightful. reply voisin 15 hours agorootparentI’d love to see a resource where someone could select their location and have example guilds like this provided. Every time I’ve looked it seemed like the only way to find out was to take a permaculture design course which is well beyond the limits of my interest. reply hosh 14 hours agorootparentThis is a good video on the specific design principles for guilds, from Canandian Permaculture Legacy at: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=XLPUN2wGbwAAnd yeah, site analysis is where I would start:1. \"Where Am I?\" https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=-XNiacRhzuM2. \"Sectors\" https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=233GgYhtoGs&list=PLNdMkGYdEq...3. \"Zones\" https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=CaUlnvGhnho&list=PLNdMkGYdEq...4. \"Slope\" https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=McopD04XP3s&list=PLNdMkGYdEq...These will inform you of how everything comes together, and it starts with an understand of your place on earth (particularly, the lat&#x2F;lng and how that affects the sun cycle; then regional forces that is discussed in \"Sectors\". Then you designs zones on your site based on how much human contact you have.You can get the rest of the Oregon State University PDC lectures from Andrew Millison from https:&#x2F;&#x2F;www.youtube.com&#x2F;playlist?list=PLNdMkGYdEqOCvZ7qcgS3e...Permaculture is very heavy on design, even if the end result doesn&#x27;t look like it.And yes, I had thought of creating a mod for an open source CAD that can pull in knowledge bases and databases as a permaculture design assistant. For example, there is a researcher whose lifetime work was to collect nutritional information for plants from all over the world so that people can select a nutritionally complete set of native plants. It&#x27;s not more widely known because that knowledge base is locked into a desktop dbms from before web apps. reply pstuart 8 hours agorootparentprevThat seems quite reasonable -- I&#x27;m guessing the more challenging part is finding economic consensus with the landholders. reply comboy 16 hours agoparentprevForest is a complex ecosystem and based on some books [1][2] my understanding is that fungi plays a huge role in them. They provide a lot of things that trees need in exchange for what they need, they also create kind of a market which allows trees even from different species exchange their resources. Different trees have different strong sides and can make use of different conditions. Perhaps this, thanks to the huge forestweb underneath allows them to thrive.Also, trees like to grow slow and solid. Older trees from the same specie will feed small tree hidden in their shadow and provide it necessary resources so that when some bigger tree falls and it can take it place, it can also grow faster. It&#x27;s possible that when there&#x27;s competition between species they grow faster because there&#x27;s a fight for sunlight. The year&#x27;s growth will be bigger and wood would be less dense (but it is sold by volume).1. The Hidden Life of Trees, Peter Wohlleben2. Entangled Life, Merlin Sheldrake reply farnsworth 7 hours agorootparentI strongly recommend \"The Hidden Life of Trees\". I listened to the audiobook multiple times while hiking&#x2F;cycling through the woods and it changed the way I will see trees forever. reply hinkley 15 hours agoparentprevSuzanne Simard has a bunch of chemical analysis that says yes, and most of her testing was in the PNW and western Canada (BC and... Alberta?)A lot of the other literature on the complex relationship of soils and trees (and trees and trees via soil life) were instigated by her observations.She was trying to get NW foresters to stop bathing everything in herbicides before replanting clearcuts. They always struggled more than anticipated. reply voisin 15 hours agorootparentRelated: https:&#x2F;&#x2F;stopthespraybc.com&#x2F; reply hinkley 10 hours agorootparentI&#x27;m a little disappointed that&#x27;s still an active cause. reply screye 10 hours agoparentprevI can see benefits tied to wildfire R0 (in infectivity terms). A few fire resistant plants might play a role in decelerating the rate of wildfire spread and limit scope of damage. reply myshpa 16 hours agoparentprev> If anyone can find a more conclusive study about temperate Western forests, I&#x27;d love to see it.I didn&#x27;t. But there&#x27;s no reason why it shouldn&#x27;t work there.https:&#x2F;&#x2F;www.scmp.com&#x2F;news&#x2F;china&#x2F;science&#x2F;article&#x2F;2167048&#x2F;fore...Forest study in China finds mix of trees can absorb twice as much carbon as areas with one speciesMore than 60 scientists from China, Switzerland and Germany were involved in the research, testing a hypothesis based on observations in the field.“The study shows that forests are not all the same when it comes to climate protection – monocultures achieve not even half of the desired ecosystem service,” Schmid said. “The full level of mitigation of global warming can only be achieved with a mix of species. In addition, species-rich forests also contribute towards protecting the world’s threatened biodiversity.”Such forests were also less vulnerable to disease and extreme weather events, which are becoming increasingly frequent as a result of climate change, Schmid said. reply RetroTechie 16 hours agorootparent\"Such forests were also less vulnerable to disease and extreme weather events, which are becoming increasingly frequent as a result of climate change, Schmid said.\"That might well be an underestimated aspect. We don&#x27;t know how climate will change locally, what pests will spread where, what species will turn out best adapted to future conditions, or what species turn out to be keystones in specific ecosystems. So we should strive for having as diverse a set of flora anywhere. Success factors are varied, complex & interconnected.Climate changes so fast that past &#x27;performance&#x27; of species in an area is of little value. Those trees are going to stand there 20, 50 or 100y from now. What will local climate be then? Take your guess &#x2F; throw the dice. reply hinkley 14 hours agorootparentSimard proved that deep rooted trees pull up water that ends up in shallow rooted plants, and that evergreens share sugars with deciduous trees in early spring, and then the direction reverses during the height of photosynthesis. Specifically in the case of water, the trees cannot transport enough water to keep up with peak transpiration, so they slowly dry out. But all night long they&#x27;re still pulling up more water, more than they can use, more than they can store, and some of the excess ends up in their neighbors, through capillary action or the rhizosphere.There’s an implication of intent here, regarding plant-to-plant transport and fairness, that I think is more likely explained by osmotic pressure. Entropy itself is &#x27;fair&#x27; in this regard. Fungal hyphae aren’t designed to manage huge nutrient or water gradients. In fact they seem to be designed to communicate information at an alarming speed. Which we still do not entirely understand. reply rolph 16 hours agorootparentprevthere was a time when logging an area, and subsequent monospecific replanting had a name.it was called a tree farmthere was a commercialized ignorance of what forest actually was. reply hinkley 14 hours agorootparentThere was a scary forest ghost story a few years ago and all of the trailers and the PR shots showed neat, tightly spaced rows of firs.That’s not a forest it’s a fucking tree farm.Now is there a Princess Mononoke style paranormal revenge story out there for destroying the forest? Absolutely. But this ain’t it. reply karaterobot 16 hours agoprevI&#x27;m not an expert, but I had thought that observing that monocultures created a weak forest ecosystem was one of the foundational concepts behind modern forestry—a centuries old discipline. This seems like an obvious corollary to that. I would have assumed it yesterday, before reading this article, and I assume most people would have thought the same. Again, I&#x27;m not an expert, so I&#x27;m likely missing something. And sometimes you just need a study to provide evidence for common sense. reply Cthulhu_ 1 hour agoparentYeah, this is one of those \"well duh\" headlines, doesn&#x27;t take a scientist to figure it out. reply anon84873628 11 hours agoparentprevJust one of those situations where science needs to get the ducks in a row and ensure incontrovertible proof. reply kderbyma 14 hours agoparentprevit&#x27;s a blinding glimpse of the obvious reply cookiengineer 4 hours agoprevI can recommend reading about the reforestation laws in Germany.Why? Germany was running out of forests to harvest around 1400, and the oldest law that is still in effect is from 1442 (Forstordnung of the Bistum Speyer) . All forests in Germany are artificially created, and there are a lot of things involved to make this happen.We have rain plans that limit the amount of how much water farmers are allowed to use which are on higher altitudes on the mountains&#x2F;plains. We have the Wasserwirt which is responsible to flood the farming plains regularly (completely under water), and redistribute their water \"lakes\" to other fields down to lower altitudes. We also have the Foerster whose job is to decide which trees to harvest, which ones need to be replaced, and what to do with the dead trees.All of those variables are planned carefully and involve a lot of data, especially the water flooding and rain plan parts.There are a lot of nice documentaries about this on arte and NDR in case anyone is interested about these kind of topics.Also about corruption and the forest mafia in Romania, which is a huge discussion topic in Brussels for years, because Ikea and other furniture producers down the line keep buying illegally harvested wood. reply ovih 11 minutes agoparentFascinating context, thank you for sharing!Some other countries whose history has something to say about reforestation&#x2F;afforestation: Australia, South Korea, UK reply prennert 1 hour agoparentprevOne thing that struck me when visiting Germany in summer was how logging seemed to be selective, leaving much of the trees standing.I passed a forest by train and despite logs piled up next to it, and logging vehicles around, you could not tell from the train that the forest was being logged. While in Scotland, if they harvest a forest, they do it like harvesting wheat. They just leave a wasteland of stumps behind. Looks horrible, must be bad for the ecosystem and I am wondering if mudslides will be more common now, since a lot of forest is on hillsides. reply jvm___ 17 hours agoprevThe Curiosity Daily podcast had a related topic this past week. There are plans to plant 1 billion (or 1 Trillion) trees at various levels of government (WEF is the 1Trillion number). So researchers went out to try to find saplings from local nurseries that could supply the diversity of trees that would be needed. They found that less than half the nurseries could supply saplings - and very few were &#x27;climate change friendly&#x27; saplings, most were decorative or other non-climate-friendly trees.“Plans to plant billions of trees threatened by massive undersupply of seedlings.” by Joshua Brown. 2023. https:&#x2F;&#x2F;www.uvm.edu&#x2F;news&#x2F;story&#x2F;plans-plant-billions-trees-th...“A lack of ecological diversity in forest nurseries limits the achievement of tree-planting objectives in response to global change.” by Peter W. Clark, et al. 2023. https:&#x2F;&#x2F;academic.oup.com&#x2F;bioscience&#x2F;advance-article-abstract...“Trees Help Fight Climate Change.” Arbor Day Foundation. N.d. “Benefits of Planting Trees.” Tree Advisory Board. N.D. https:&#x2F;&#x2F;www.bgky.org&#x2F;tree&#x2F;benefits reply destron 17 hours agoparent... of course the nurseries don&#x27;t have seedlings to plant one trillion trees. Growing seedlings of the appropriate species would have to be part of that effort. reply prawn 8 hours agorootparentThere are volunteer efforts in some places to spread the load in starting seedlings. Not sure how effective it would be though in contributing to an effort of that scale where you need commercial scale to make a dent. reply Cthulhu_ 1 hour agorootparentIt can be done though, if anyone that has a balcony or a back yard were to look after a tray of seedlings (25 or so trees?) until they&#x27;re ready to be planted, it&#x27;d quickly add up. The challenge then becomes logistics though. But again, can be done with volunteers. reply mym1990 17 hours agoparentprevCan you expand on the difference between climate change friendly and non climate change friendly trees, for the noobs like me? reply jvm___ 17 hours agorootparentThe money is in growing saplings of white-pine, the ones commercial re-planters will buy - because they grow fast and can be turned into toilet paper in 25(?) years. Growing saplings of local &#x27;slow&#x27; growing, non-harvestable species doesn&#x27;t make you $$$.\"In essence, forest nurseries tended to maintain a limited inventory of a select few species, electing to prioritize those valued for commercial timber production over species required for conservation, ecological restoration, or climate adaptation.\"\"Yet, in their 20-state survey, the team only found two tree nurseries that had inventory of red spruce, a species from which many millions of seedlings are needed to meet restoration goals. “Remarkably, only 800 red spruce seedlings were commercially available for purchase in 2022,” the team reports in their new Bioscience study, “—enough to reforest less than one hectare.” reply Cthulhu_ 1 hour agorootparentSupply and demand then; if there&#x27;s demand for a million of these saplings, the nurseries will adapt. I don&#x27;t understand why this is posited as a weird finding. Just place an order for X saplings and the nurseries will get to work. reply myshpa 17 hours agorootparentprevMaybe we should plant seeds, same as nature does. Dozens of different seeds per m2, nature would choose what&#x27;d survive and flourish.An example of a forest farm planted with the same approach: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=_ST9NyHf09M reply RetroTechie 16 hours agorootparentI recall some initiative (in Africa, iirc) where locals would collect seeds of random herbs, shrubs, trees etc, mix those up & pack into seed bombs.Then others who travel around for their work, would toss those in random places. From bicycle thrown some distance from roadside, or a bush pilot dropping some during flight, etc.Basically as many different seeds in as many different places as possible. Then let nature do its thing.Note this was still mostly local. So not introducing invasive species from other side of the globe. Just helping native species to spread a bit further & faster. reply myshpa 15 hours agorootparentYes, that&#x27;s a very effective method. Such initiatives are all over the world.It&#x27;s based on ancient method of seedballs, promoted by Fukuoka.https:&#x2F;&#x2F;www.permaculturenews.org&#x2F;2014&#x2F;06&#x2F;18&#x2F;making-seedballs...https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=OqYTz6-zGcg reply mvdtnz 13 hours agorootparentprevIf you want to prioritise speedy regeneration then this is not the best approach. Nature is incredibly effective in the slow and steady mode of operation but to maximise efficiency you need to be more deliberate. reply myshpa 11 hours agorootparentThis method is known as \"close planting\" or \"high-density planting\". It&#x27;s frequently employed in regeneration projects, such as the Green Great Wall in China. Syntropic agriculture, as seen in the video I&#x27;ve posted, and Miyawaki forests also use this approach.Akira Miyawaki developed a variant of this method, which involves planting a variety of native species in close proximity. The idea is that the trees compete for sunlight, growing upwards more than outwards, leading to a fast-establishing and diverse forest.https:&#x2F;&#x2F;www.sugiproject.com&#x2F;blog&#x2F;the-miyawaki-method-for-cre...Another example would be Mark Shephard&#x27;s farm where he&#x27;s using his Sheer Utter Total Neglect (STUN) method. He describes in his video that the goal is to find a combination of plants that is so resilient, that you can&#x27;t kill those trees even if you try.https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?app=desktop&v=RePJ3rJa1WgSure, it&#x27;s essential to ensure that the selected species are suitable for the specific soil, climate, and conditions of the site. Additionally, as the forest grows, some form of management, like thinning or selective removal of species, may be required to ensure the forest remains healthy and achieves the desired goals. reply mvdtnz 10 hours agorootparentDid you even read your own links? They are carefully preparing soil and planting saplings, not randomly scattering seeds. The exact opposite of what gp is suggesting. They are planting in an intentional manner exactly as I said. reply myshpa 10 hours agorootparent> carefully preparing soil and planting saplingNone of that is strictly necessary. While it helps, it&#x27;s not a requirement. Simply selecting the right seeds, maximizing cover&#x2F;photosynthesis, and perhaps mowing at the appropriate time are usually sufficient, unless the soil is seriously degraded.https:&#x2F;&#x2F;www.amazon.com&#x2F;Sowing-Seeds-Desert-Restoration-Ultim...Fukuoka spent years working with people and organizations in Africa, India, Southeast Asia, Europe, and the United States, to prove that you could, indeed, grow food and regenerate forests with very little irrigation in the most desolate of places.By randomly scattering seeds. reply freedude 15 hours agorootparentprevThe same problem exists at the seed level. Who is collecting the 2 trillion seeds (50% germination rate)? reply zo1 8 hours agorootparentPerhaps we could, I dunno, pay people to do it with the millions(billions) of dollars of carbon credits? Isn&#x27;t that what they&#x27;re for? reply myshpa 15 hours agorootparentprevIt&#x27;s a similar problem but on a smaller scale (it&#x27;s easier&#x2F;cheaper to collect & spread seeds than grow & plant the seedlings). reply developer93 17 hours agorootparentprevIf a plant isn&#x27;t native, the insects and animals that eat or use it aren&#x27;t around, those that are can&#x27;t use it, and it&#x27;s of limited use to the ecosystem. Not to mention it&#x27;s interactions with other plants. reply salynchnew 13 hours agoprevShouldn&#x27;t be surprising.The Miyawaki method shows success when a few principles are followed: -Planting naturally-occurring communities of plants, not monocultures (bonus points for including microfauna and soil microbes) -Planting locations are semi-randomized, with room for plants to expand and reseed. -Stands of trees are protected&#x2F;watered for first 3-5 years. -Local communities are engaged and have a vested interest in protecting&#x2F;maintaining stands of trees for the first few years.https:&#x2F;&#x2F;www.jstor.org&#x2F;stable&#x2F;24577389?mag=the-miyawaki-metho... reply bluerooibos 19 hours agoprevWho would have thought that replicating nature would yield the best results...The Miyawaki method is probably relevant to this discussion - https:&#x2F;&#x2F;www.creatingtomorrowsforests.co.uk&#x2F;blog&#x2F;the-miyawaki... reply nxobject 14 hours agoparentAnother text I really love as well that focuses on teaching \"applied ecology\" is \"Garden Revolution\" by Weaner and Christopher. Since I&#x27;ve never taken biology of any sort, it was a good primer on things like ecological succession (in temperate climates), parameters of plants that might completement each other, etc. The authors have worked on large-scale restoration and sustainable landscaping projects, and it shows. reply BengineeringELM 7 hours agoprevCheck out the silvicultural guide for the northeast:https:&#x2F;&#x2F;www.fs.usda.gov&#x2F;research&#x2F;treesearch&#x2F;45874It turns out most logging in the northeast already yields a diverse ecosystem as quickly as the next year, and no planting is nessesary.Forresters can target different species mixes in the regeneration by using different harvesting methods.Pretty cool stuff. reply swader999 19 hours agoprevThe other way to do this is to leave seed trees on the block in addition to replanting with 3-4 other species. They do this in British Columbia now.When a block is logged, cones from that block are taken to regrow seedlings to plant there. It doesn&#x27;t work as well if you try to seed from different elevations or far away areas. reply erikhopf 17 hours agoprevThere are companies like Terraformation that have been doing this for a while. Beyond the biodiversity angle, seed banking seems to be very important to the long term success of regrowth projects. reply greenie_beans 15 hours agoparenti invested in their republic.co fundraising: https:&#x2F;&#x2F;republic.com&#x2F;terraformationbut then did more research, because i&#x27;ve been interested in forestry for a while and was geniunely curious and wanted to understand my investment more.i pulled my investment once i learned that these sort of projects don&#x27;t actually work. terraformation targets land in areas that aren&#x27;t meant to be forests. also decided to pull it because i don&#x27;t completely understand the space. (sure, \"planting trees will solve climate change\" seems easy enough and makes me feel good because \"i&#x27;m planting trees!\" but nah, not really, let&#x27;s maybe rethink this...this is coming from somebody who spends a lot of time in the woods and finds trees to be an important part of my life.)this person researches this space: https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?user=DbjysqUAAAAJ&hl=enthis is one of the bigger studies: https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?view_op=view_citation&h... reply iamcasen 17 hours agoprevHow is this not a complete and obvious no-brainer? As advanced as our culture is in some ways, it is clearly quite idiotic in many other ways. reply wredue 13 hours agoparentIt always helps having raw experimentation on your side. reply CatWChainsaw 14 hours agoprevIf you own property with a lawn, do what you can as well with native plants for native pollinators. reply darklycan51 16 hours agoprevI don&#x27;t understand why loggers clear entire sections of forests instead of leaving every hectare x amount of old \"mother\" trees reply BengineeringELM 7 hours agoparentIn general they do leave \"seed trees \" but they only need as few as 3 trees per acre to effect the spaces mix in the regrowth.Those 3 trees aren&#x27;t nessesary though. The Forrest floor is loaded with seeds that can survive longer than a decade waiting for the right time to germinate. In many cases the sunlight hitting the ground is the trigger they need, and in most cases a \"clear cut\" will be a 9 year old forest 10 years later. reply AnimalMuppet 16 hours agoprevOr, to say it in reverse: Replanting logged forests with a monoculture hinders restoration.Putting it this way emphasizes what \"normal\" is. reply hanniabu 17 hours agoprevThis seems pretty obvious, I&#x27;m amazed this is coming as a surprise reply sp332 17 hours agoparentNo one said it was a surprise. It&#x27;s a demonstration. reply bradly 17 hours agoparentprevIt isn&#x27;t mentioned in this article but one example of this is deforestation of the a Eastern White Pine. They quickly realized the problems deforestation at that scale and attempted to build pack the forests quickly by use a very similar, but much faster growing pine. Turns out that was a really bad idea. reply rgrieselhuber 18 hours agoprevEventually people will figure out that monoculture was a horrible and anti-nature idea. reply grlass 18 hours agoparentThe issues from the Great Green Wall [1] are worth looking at[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Great_Green_Wall_(China) reply hasmanean 17 hours agoparentprevI’ve heard monoculture forests are worse for forest fires too.Having dead decaying logs on the forest floor probably helps because fungi are naturally fire resistant. reply WalterBright 16 hours agoprev [–] This is not surprising in the least. Also, genetic variety within a particular species should help a lot. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A University of Oxford's SE Asia Rainforest Research Partnership study found that replanting logged forests with diverse seedlings accelerates their recovery.",
      "The study evaluated 125 plots in logged tropical forests and found faster recovery in plots replanted with a diverse mix of 16 native tree species compared to those with fewer species.",
      "The increased biodiversity leads to heightened ecosystem functioning and stability due to different species occupying varied niches. This forest restoration strategy is vital for biodiversity maintenance and climate preservation efforts."
    ],
    "commentSummary": [
      "The summary emphasizes the role of diverse seedlings in restoring logged forests, contributing to ecosystem resilience and mitigating effects of reduced rainfall and insect damage.",
      "It reviews debates over preserving old-growth forests for economic gains and the use of herbicides in forest management while promoting diverse forests with native seedlings for enhanced biodiversity.",
      "The discourse brings light to impacts of climate change on forest growth, illegal wood harvesting issues, and successful reforestation approaches like the Miyawaki method and seed banking."
    ],
    "points": 331,
    "commentCount": 122,
    "retryCount": 0,
    "time": 1695044669
  },
  {
    "id": 37563231,
    "title": "Striking auto workers want a 40% pay increase–the same rate their CEOs’ pay grew",
    "originLink": "https://www.cnbc.com/2023/09/15/striking-uaw-auto-workers-want-a-40percent-pay-increase-how-much-they-make-now.html",
    "originBody": "SKIP NAVIGATION SUCCESS MONEY WORK LIFE VIDEO SEARCH CNBC.COM RELATED STORIES LAND THE JOB UPS drivers can earn as much as $172,000 without a degree WORK Hollywood strikes have already had a $3 billion impact on California’s economy CLOSING THE GAP Gender wage gap is now at its smallest since it started being tracked in 1979 WORK 1 in 5 employees are ‘loud quitting.’ Here’s why it’s worse than ‘quiet quitting’ WORK Ivy League expert: ‘Contagion’ of summer strikes have ‘potential to last’ WORK Striking auto workers want a 40% pay increase—the same rate their CEOs’ pay grew in recent years Published Fri, Sep 15 20232:59 PM EDT Jennifer Liu @IN/JLJENNIFERLIU @JLJENNIFERLIU SHARE Share Article via Facebook Share Article via Twitter Share Article via LinkedIn Share Article via Email Supporters cheer as United Auto Workers members go on strike at the Ford Michigan Assembly Plant on September 15, 2023 in Wayne, Michigan.Bill PuglianoGetty Images NewsGetty Images Thousands of United Auto Workers members are officially on strike after three Detroit automakers failed to reach an agreement with the union, which represents about 146,000 workers at Ford, GM and Stellantis, by a Thursday night deadline, CNBC reports. One major issue on the table is worker pay. The union proposed 40% hourly pay increases over the next four years. The average U.S. autoworker on a manufacturing production line earns about $28 per hour as of August, according to data from the Bureau of Labor Statistics. That’s up $1 from the previous year. Autoworker pay at “The Big Three” works on a tiered system, which was introduced in the aftermath of the 2008 auto industry crisis, where more recent hires start at lower rates of pay than more tenured workers. Top-tier workers (those hired in 2007 or earlier) earn an average of $33 per hour, CBS News reports, based on contract summaries for the Big Three. Lower-tier workers (hired after 2007) earn up to $17 an hour. Nationwide, autoworkers’ average real hourly earnings has fallen 19.3% since 2008, according to research from the left-leaning Economic Policy Institute. Meanwhile, Ford CEO Jim Farley earned $21 million in total compensation last year, the Detroit News reported. Stellantis CEO Carlos Tavares made $24.8 million, according to the Detroit Free Press. And GM CEO Mary Barra earned nearly $29 million in 2022 pay, Automotive News reported. “Obviously, CEOs should be the highest-paid person in an enterprise, but then the question is exactly just how much higher than everyone else,” Josh Bivens, chief economist at EPI, told NPR. CEO pay at the Big Three has grown 40% in the last decade, according to EPI — in line with the UAW’s demands for 40% pay increases for autoworkers. UAW President Shawn Fain said Wednesday Ford has offered a 20% increase over the four years of the deal, followed by GM at 18% and Stellantis at 17.5%. GM raised their offer Thursday to a 20% wage increase. “I’m extremely frustrated and disappointed,” Barra told CNBC Friday morning. “We don’t need to be in strike right now. We put a historic offer on the table.” 5:30 GM CEO Mary Barra on UAW strike: We put a historic offer on the table Profits at the struck auto companies increased 92% from 2013 to 2022, totaling $250 billion, according to EPI. Striking workers say they haven’t shared in their company’s financial success. President Joe Biden weighed in on negotiations Friday, saying, “Auto companies have seen record profits including the last few years because of the extraordinary skill and sacrifices of the UAW workers. Those record profits have not been shared fairly, in my view, with the workers.” CEO pay growth outpaces worker wages across industries The auto industry is just one example of how executive pay has skyrocketed faster than the typical wage growth for everyday workers. The average CEO at a top U.S. company was paid $27.8 million in 2021, including stock awards — 399 times as much as the typical worker — according to research published by EPI. From 1978 to 2021, CEO pay grew by 1,460%, adjusted for inflation, versus just 18.1% for the typical worker. The UAW also proposed the elimination of compensation tiers and a restoration of cost-of-living adjustments, as well as other workplace protections like a reduced 32-hour workweek, a shift back to traditional pensions, improved retiree and parental leave benefits, and more. “For the first time in our history, we will strike all three of the ‘Big Three’ at once,” Fain said Thursday in live remarks streamed on Facebook and YouTube. “We are using a new strategy, the ‘stand-up’ strike. We will call on select facilities, locals or units to stand up and go on strike.” About 12,700 workers will be on strike at three facilities nationwide, starting at GM’s plant in Wentzville, Missouri; Ford’s plant in Wayne, Michigan; and Stellantis’ plant in Toledo, Ohio. The targeted strikes aim to bring a work stoppage to key plants that then cause plants further down the line to stop production without needed materials. The strategy is unprecedented: The union may increase the number of strikes based on the status of negotiations. Want to be smarter and more successful with your money, work & life? Sign up for our new newsletter! Want to earn more and land your dream job? Join the free CNBC Make It: Your Money virtual event on Oct. 17 at 1 p.m. ET to learn how to level up your interview and negotiating skills, build your ideal career, boost your income and grow your wealth. Register for free today. Check out: ‘Survival jobs,’ ex-careers and side hustles: How Hollywood writers are making ends meet 100 days into the strike 7:00 How a 22-year-old earning $77,000 as a car detailer spends his money Trending Now The No. 1 personality trait linked to a long life 28-year-old’s side hustle makes $180 per hour, more than doubling his income—here’s how 3 steps to an after-work evening routine that sets you up for success, says decision-making expert The No. 1 thing successful parents who raise the strongest and most resilient kids do differently 35-year-old CEO dumped his ‘pretty good’ startup—and built a $1.4 billion business instead by Taboola Sponsored Links FROM THE WEB These Unsold Electric SUV’s Cost Almost Nothing (Take a Look) Top Searcher Now Harrison Ford’s Inheritance Makes The Headlines investing.com Stay in the loop Get Make It newsletters delivered to your inbox SIGN UP About Us Learn more about the world of CNBC Make It LEARN MORE Follow Us CNBC.COM Join the CNBC Panel © 2023 CNBC LLC. All Rights Reserved. A Division of NBC Universal Privacy Policy Do Not Sell My Personal Information CA Notice Terms of Service Contact",
    "commentLink": "https://news.ycombinator.com/item?id=37563231",
    "commentBody": "Striking auto workers want a 40% pay increase–the same rate their CEOs’ pay grewHacker NewspastloginStriking auto workers want a 40% pay increase–the same rate their CEOs’ pay grew (cnbc.com) 327 points by heavyset_go 11 hours ago| hidepastfavorite509 comments abeppu 10 hours agoJust to point out a demonstrated, viable, successful reality achieved under different values and assumptions, the Mondragon Corporation&#x2F;cooperative produces car parts, among many other things, and has pre-agreed ratios for wages for executives relative to the lowest wages paid to workers, and this tops out at 9:1. Studies have found that worker-owned coops have a greater survival rate than conventional businesses, and that they have higher productivity.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Mondragon_Corporation#Wage_reg... https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Worker_cooperative#Longevity_a... reply reddozen 9 hours agoparentMondragon is not a worker cooperative. They have a three-tiered worked system[0] with clear hierarchical structures and differences in voting power. The temporary worker tier (largest) having no voting power whatsoever.[0] https:&#x2F;&#x2F;www.researchgate.net&#x2F;publication&#x2F;290978631_The_Mondr... reply thelastgallon 8 hours agorootparent\"cooperatives don&#x27;t have to treat everyone exactly the same. You can have different \"classes\" of members.some ideas about how to offer \"founder incentives\" in workers cooperatives.\"From a recent HN comment: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37304911 reply reddozen 7 hours agorootparentIf you want to come up with a different word sure, I would just call whatever you defined there as traditional a corporation.But worker cooperatives is known as \"one worker, one vote\" and anarcho syndicalists like Richard Wolff wouldn&#x27;t consider that a worker cooperative anymore. reply Turskarama 4 hours agorootparentI don&#x27;t think that&#x27;s a sensible line to draw really. Permanent employees get to vote, that seems obviously more a cooperative than a classic organisation, where votes are based on the quantity of shares bought. reply abeppu 4 hours agorootparentprevI think this the least constructive kind of unhinged purity-based framing. You&#x27;re right about one thing -- Mondragon isn&#x27;t a cooperative; it&#x27;s a federation of cooperatives. I note that the paper you linked to by Kasmir points out what the author sees as failings, but from my quick reading it does not say that Mondragon co-ops are not co-ops. I think you&#x27;re on that branch alone.But also, Kasmir seems to be faulting co-op members for some lack of ideological purity, and frankly, for failing to live up to the aspirations that others have built around them, which aren&#x27;t their responsibility.> Many academics and social justice activists alike — maintain that co-ops promise a more democratic and just form of capitalism and even sow the seeds of socialism within capitalist society.> Co-op members voted to pursue an international strategy to open these firms, and, thus, to employ low-wage laborers. Hence, we are confronted with a complicated permutation of a familiar state of affairs whereby the privilege of one strata of workers depends upon the exploitation of another.> Compared with workers in the standard firm, co-op members were less involved in and showed less solidarity with the Basque labor movement, which at the time was part of an active leftist coalition for socialism and independence for the Basque country.But the point of a co-op is not to further the goals of academics and activists, nor is it the responsibility of any co-op to maintain allegiance to whatever movements or institutions that the author admires. If Richard Wolff wants people to vote in the workplace, and wants those votes to mean something, doesn&#x27;t that power and autonomy also necessarily mean they have the power to disagree with his views and pursue their own success and flourishing? And its success should be measured by the degree to which co-op members benefit, not by the extent that they&#x27;re an ideological tool for outsiders.Yes, one might have wanted Mondragon co-ops to create other worker-run co-ops in other countries, rather than subsidiaries. But it&#x27;s hard to see how that would have actually worked. Frankly, starting factories in China by talking to workers about how important democracy is could have gotten people hurt. And these firms do still need to be able to compete and succeed in a global marketplace in which most of their peers are operating from a purely capitalist playbook. If you draw your ultra-orthodox definition of what a co-op too narrowly, you risk adopting a definition which excludes successful firms of any significant scale. reply boppo1 7 hours agoparentprevHow is equity comp factored in? reply throw0101b 11 hours agoprevGood episode of Bloomberg&#x27;s Odd Lots podcast from a little while ago on the topic:> On September 14, the contract between the United Auto Workers and the Big Three carmakers (GM, Ford and Stellantis) is expiring — and the possibility of a strike is real. This comes at a delicate time for multiple reasons. The labor market is tight, which means workers have other options. Inflation is high. And the auto industry is undergoing a major shift to the electric vehicle market, which may change the composition and pay of the labor force. The stakes are high. So what does the union want and how does it fit into the goals of the broader labor market? To understand more, we speak with Dan Vicente, the director of UAW Region 9, as well as Alex Press, a labor reporter at Jacobin magazine.* https:&#x2F;&#x2F;omny.fm&#x2F;shows&#x2F;odd-lots&#x2F;what-the-uaw-wants-from-its-f...* https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=LBP8_8_S7LsThere were some items that the unions conceded in 2008 when the US automakers were going bankrupt that have still have not been restored. reply aaronbrethorst 10 hours agoparentAlex Press also appeared on last week&#x27;s episode of the podcast Why Is This Happening, hosted by Chris Hayes. https:&#x2F;&#x2F;www.msnbc.com&#x2F;msnbc-podcast&#x2F;why-is-this-happening&#x2F;un... reply endisneigh 11 hours agoprev40% over 4 years isn&#x27;t as crazy (especially when you consider that they had some concessions back in &#x27;08). typical clickbait when you consider this is a high value for negotiation and they&#x27;re likely looking for anywhere between 20-30% over 4 years when it&#x27;s all said and done. barely above inflation YoY. reply mr_toad 8 hours agoparentIt’s not entirely on the media for using these headline numbers. The Unions like these numbers being used. It sounds better to their members that they’re getting a 40% pay rise. reply foogazi 8 hours agorootparent> It sounds better to their members that they’re getting a 40% pay rise.When they get it - right now every reports that they are asking for 40% reply dmitrygr 10 hours agoparentprev> they&#x27;re likely looking for anywhere between 20-30% over 4 years when it&#x27;s all said and done.TFA conveniently points out to you that 20% has been offered to them and was rejected. reply endisneigh 10 hours agorootparentTFA conveniently points out that the average offer between the three automakers is less than 20-30% :), and 20% is the low-end of 20-30% btw reply l33t7332273 10 hours agorootparentprevGood. As it should be. Record profits deserve record contracts. reply wang_li 10 hours agorootparentOnly if the contracts automatically follow profits. Profits are not a ratchet, while contracts typically are.Also they are asking for more than pay raises and shortened work week. They&#x27;re also demanding health benefits for retirees and defined benefit pensions among other things. reply mplewis 10 hours agorootparentAll of these things are good for workers, their quality of life, and the quality of their work, and American workers deserve those things. reply IG_Semmelweiss 8 hours agorootparentDefined benefit plans??The only defined benefit plan left is social security , which is fast going into insolvency in exactly a decade, and where it will automatically be cut 25% once that happens! reply smugma 25 minutes agorootparent“contrary to a common misunderstanding. In 2035, if nothing else is done, the program could pay 80 percent of scheduled benefits, mostly out of workers’ ongoing contributions, a figure that would slip to 74 percent in 2096”https:&#x2F;&#x2F;www.cbpp.org&#x2F;blog&#x2F;social-security-is-not-bankrupt reply throw0101b 8 hours agorootparentprev> Only if the contracts automatically follow profits. Profits are not a ratchet, while contracts typically are.Are they? When concessions are given they should also be removed: there are some elements in the contracts that date back to the hard times of 2008 that the unions are still putting up with. reply MuffinFlavored 9 hours agorootparentprevwhich automaker had record profits, when? all of their stocks are underperforming the index poorly on a 5 year timeframe replymeindnoch 10 hours agoprev\"Obviously, CEOs should be the highest-paid person in an enterprise\"Hold on there chap! How is this obvious? reply burnerburnson 9 hours agoparentIt&#x27;s obviously wrong. NBA teams are a good counter-example. Nobody talks about the team&#x27;s president&#x2F;CEO&#x2F;etc when prognosticating which team will be the best. reply grecy 9 hours agoparentprevFor a fun thought experiment:If the CEO go on strike and don&#x27;t come to work for a month, how many cars will not be built, and how money will the company lose?If the union factory workers go on strike and don&#x27;t come to work for a month, how many cars will not be built, and how much money will the company lose?Therefore, who is actually more important to the earnings and success of the company? reply burnerburnson 9 hours agorootparentThat&#x27;s not an interesting thought experiment at all. The workers in aggregate are clearly more valuable than the CEO alone. That&#x27;s why their cumulative salary is way higher than the CEO&#x27;s. reply turquoisevar 1 hour agorootparentQuick estimation tells me that average ratio amongst the big three is 1:336[0].Put differently: 336 workers’ per annum amounts to the pay the CEO gets. There are three CEOs in this case so that’s about 1,000 workers to equal their pay.If I recall correctly there are about 12,000 workers striking, so their cumulative salary is about 12x that of the cumulative CEOs.I don’t know why, but that doesn’t feel “way higher” to me. I guess I just expected something along the lines of 100x for some reason.Also makes me wonder how this scales when you look at C-Suite as a whole v. workers.0: https:&#x2F;&#x2F;fortune.com&#x2F;2023&#x2F;09&#x2F;17&#x2F;uaw-strike-highlights-carmake... reply xormapmap 7 hours agorootparentprevHit the nail on the head. A more appropriate comparison would be if one worker went on strike, not all of them. reply grecy 8 hours agorootparentprevBut why did the CEO&#x27;s salary increase more than the workers?To keep the equation in check, surely they need to increase at the same rate reply jmye 7 hours agorootparentWhy? No individual assembly line worker has any particular value - so if the aggregate increases then the equation, in your example, is “in check”, whatever that means.Alternatively, if assembly workers were difficult to effectively replace, they would get paid more. Simply doing some unit of work neither makes that unit of work valuable, nor does it make the person doing it valuable. reply bugglebeetle 5 hours agorootparentI’d argue that no individual CEO has any particular value, or at least no more than an exceptional worker. What they have done instead is effectively organize as a class (Capital owners) to make companies profits accrue to themselves. This is why they also all serve in the boards of each other’s companies. reply grecy 6 hours agorootparentprev> nor does it make the person doing it valuableThat is a very sad way to look at human life, and I hope these strikes now and into the future prove that not to be true. reply satvikpendem 6 hours agorootparentThey obviously mean \"valuable\" in terms of value to the company, not literal value as a human being.In any case, yes, if someone wants to smash rocks with a hammer, manually, all day, their work would not be valuable to a company building cars, which is what the parent means. Simply doing some sort of \"work\" for work&#x27;s sake has no value if that work is not useful. replyfragmede 9 hours agorootparentprevThat&#x27;s just the nature of the work though. If the CEO of a film camera business decided that digital cameras weren&#x27;t going to be a thing, and refuses to pivot, that&#x27;ll doom the company, but it&#x27;ll take decades to realize his mistake. The company will lose all its money all the same. If the CEO of the car company decides not to build any factories to make electric vehicles, that decision&#x27;s also going to take years to play out. They&#x27;re both important, in different ways. reply TrackerFF 2 hours agorootparentprevDepends on what executive actions only the CEO can perform. I&#x27;ve seen this happen in a microcosm, where the managing director just refused to do things, and due to tasks only he had authority to do, contracts went unsigned.Obviously large corporation will be more robust, and have things like attorney of power that - so that there&#x27;s no single point of failure. But, these things happen. reply savanaly 5 hours agorootparentprevPermanently losing one line worker vs one CEO? You don&#x27;t have to be a disciple of Ayn Rand to think the latter would have a greater negative impact on productivity. reply lr4444lr 10 hours agoparentprevThey have the highest accountability. reply kccqzy 10 hours agorootparentThese people with the highest accountability do not hesitate to blame lowly software engineers when it suits them.https:&#x2F;&#x2F;www.theverge.com&#x2F;2015&#x2F;10&#x2F;8&#x2F;9481651&#x2F;volkswagen-congre... reply Turskarama 4 hours agorootparentprevDo they? They will frequently tank a company and then golden parachute into retirement. This happens all the time. reply meindnoch 10 hours agorootparentprevBut isn&#x27;t compensation proportional to the cost of replacing someone?I can imagine a bunch of roles that are more costly to replace than \"someone with accountability\". reply lowbloodsugar 10 hours agorootparentprevNot really true. They have the most people beneath them that can take the fall for them. Finance shenanigans? CFO takes the fall. Missed targets? COO or CMO. Major hack? CTO. Etc. reply galleywest200 10 hours agorootparentprevBut do they do the most for the company, though? What about those who actually invent their products in R&D, or any of that? reply TheHappyOddish 9 hours agorootparentI&#x27;ve yet to come across a company that pays the highest output individuals the most. That&#x27;s not how capitalism works. reply ftxbro 9 hours agoparentprevthey are at the top of the hierarchy and the top dog eats first reply SeanAnderson 10 hours agoprevI&#x27;ve heard a counter-argument to this before and I&#x27;m curious to get other&#x27;s take on it.Basically, the story goes that when an individual rises into a significant leadership position at a large enough company that the economic calculations become different. There&#x27;s still an element of domain expertise, but, for the most part, leadership is leadership wherever you go. This implies that a leader could (potentially) move across sectors and still be effective which results in a wider pool of companies that are interested in competing for this person when contrasted to the ICs. Since some sectors are very profitable they end up \"bidding up\" quality leadership. The combination of this effect along with the fact there are objectively fewer CEOs than ICs results in a mismatch in salaries.I think there&#x27;s an element of truth to this, but probably not to the extent that it justifies the widening pay gaps everywhere? reply elcritch 10 hours agoparentThough this argument falls apart with evidence that CEOs of big firms do need to deeply understand their companies domain. It may work to a degree, but doesn’t seem to coincide with the most valuable companies.It’s why when Intel was floundering a few years back they got rid of CEO and brought on a CEO with deep engineering expertise.Tim Cook is a wizard of supply chain, and in many ways that’s a large part of Apples current success, IMHO. The list goes on. reply throw0101b 8 hours agorootparent> Though this argument falls apart with evidence that CEOs of big firms do need to deeply understand their companies domain.Counter-example: the last few CEOs of Boeing who have completely messed up the company.They came from the Jack Welch of GE school of management, and it turns out that Welch et al were cooking the books. See also Enron and WorldCom. reply elcritch 7 hours agorootparentHow is that a counter example? Seems like another example of “professional managers” coming in and screwing up a company.The first Boeing CEOs were from Boeing and were steeped in Boeings engineering culture and valued that expertise. Later CEOs like you mentioned didn’t. Boeing also acquired McDonnell Douglas, and many thought it was great that Boeing got to keep all McDD’s “experienced” managers. reply throw0101b 7 hours agorootparent> How is that a counter example? Seems like another example of “professional managers” coming in and screwing up a company.The screwing up started at the top by changing metrics and priorities post-McDD. See Flying Blind:* https:&#x2F;&#x2F;www.penguinrandomhouse.com&#x2F;books&#x2F;646497&#x2F;flying-blind...And the screwing up was richly rewarded. reply gruez 10 hours agorootparentprev>Tim Cook is a wizard of supply chain, and in many ways that’s a large part of Apples current success, IMHO. The list goes on.Okay but don&#x27;t many other companies deal with supply chains? I guess software companies don&#x27;t, but \"companies that trade in physical goods\" is a pretty big segment, and it stands to reason that having \"a wizard of supply chain\" would be useful. Doesn&#x27;t this translate into an argument in favor of \"leadership is leadership wherever you go\"? reply thewileyone 7 hours agoparentprev\"leadership is leadership wherever you go\"Not all CEOs are built the same. Only a minority are able to cross industries successfully, the majority fail miserably. This is because elements that drives success are different between industries. Most of the time, if the CEO is successful one way in one industry, he&#x2F;she would pursue the same path in another industry, without acknowledging that the second industry is different. What&#x27;s worse if when they bring their previously successful team. Now you&#x27;ve got a bunch of people doing more the wrong things at the same time. reply Turskarama 4 hours agoparentprevI think the whole thing falls apart when you get to the point of a CEO making so much money, regardless of their performance, that they can just retire after a single year.So what if I don&#x27;t get my contract renewed if I have $50 Million, that&#x27;s more than most people make in their entire lives. See you at the beach! reply therealdrag0 4 hours agorootparentA counter consideration I’ve heard is that they have to pay more to keep them off the beach! If the board thinks Turskarama is the best leader for the org, then they have to pay enough to keep you off the beach. Which could be absurd amounts of money. reply Turskarama 4 hours agorootparentThe problem is that that amount of money doesn&#x27;t exist, I simply do not desire a lifestyle that costs me more than maybe a couple hundred thousand a year, tops.At some point sure maybe I can work for a couple of years and I will earn enough to buy myself a super yacht... but I don&#x27;t want a super yacht as much as I want to not work.It&#x27;s very funny to me that people make the argument against raising welfare that there will be no incentive to find work, yet somehow they think this same argument can&#x27;t apply to the super rich who can retire with a lot of money instead of struggling along on a subsistence income. reply vasco 10 hours agoparentprevIt&#x27;s more likely that it is the only role that tends to be in the room when the board is deciding salaries. Turns out it&#x27;s great to be able to decide how much your own compensation should be. reply abdullahkhalids 7 hours agoparentprevHR people and accountants also have the same sort of domain independence. Their salary has not particularly risen faster than the average worker salary. reply MuffinFlavored 9 hours agoparentprev> leadership is leadershipwhat is leadership other than being stern, following up, driving projects to completion or up&#x2F;down the org chart as needed (escalation, etc.)? reply jmye 7 hours agorootparentI mean, it involves leading. Making strategic decisions and then gaining support for them. Leadership is not just a synonym for project management.I don’t understand your comment at all. reply fallingfrog 10 hours agoparentprevI’d like to see one of these guys use their magic leadership mojo to lead a platoon of marines into battle. I think he’d find that domain knowledge does, in fact, matter.To the extent that it truly doesn’t matter, the ceo is a glorified mascot. reply mwbajor 7 hours agoparentprevI think the MBA mantra that specific product expertise is not important: a widget is a widget, died with Jack Welch and Carly Fiorina (her career). Its still around but not nearly as much as 15-20 years ago. reply tysam_and 11 hours agoprevThere is an equation with an equilibrium:CEO&#x27;s pay rate = (some multiplier) * median salaried worker&#x27;s pay rateORCEO&#x27;s pay rate = (some multiplier) * minimum salaried worker&#x27;s pay rateWe&#x27;ve been conditioned to think it&#x27;s not fair somehow, but the discrepancy is just....engorgingly terrible. I&#x27;m not arguing for how this metric would be enforced, only that it would be a good one to have. Especially in a time when greed is the lowest common denominator in the race to the bottom for some of these large corporations.It wouldn&#x27;t fix the stockholder \"value\" chase, but at least it would shore up one part of the system weak to corruption. reply hinkley 10 hours agoparentIt&#x27;s also been suggested that Congressional salaries be a fixed multiple of minimum wage, and for similar reasons.Honestly I think that would probably fix things a lot faster. reply astrange 10 hours agorootparentIf you underpay elected officials then 1. they just start taking bribes 2. only rich people will run in the election.This is why Singapore pays them even more than we do.Anyway, minimum wages (despite being ok policies) aren&#x27;t what people are actually paid, and of course aren&#x27;t especially what non-working people are paid. And remember that non-working people, namely children and the elderly, are poorer than workers. reply amrocha 8 hours agorootparentThat literally is already what&#x27;s happening. Bribing officials through lobbying is legal, and only rich people can afford to take on the instability of an election run.And idk where you got the idea that people don&#x27;t make minimum wage. They absolutely do. Go look at a job board. reply hinkley 10 hours agorootparentprevTHEY ALREADY DONobody said the multiple has to be 5. It could be whatever the current ratio for Singapore is. reply astrange 10 hours agorootparentThey don&#x27;t, but the important point is:> 2. only rich people will run in the election.Which is not true unless you think AOC is rich. It&#x27;s not unheard of that congresspeople sleep in their office i.e. are homeless because DC rent is high.https:&#x2F;&#x2F;www.npr.org&#x2F;2022&#x2F;12&#x2F;09&#x2F;1141635119&#x2F;the-first-gen-z-me... reply somsak2 7 hours agorootparent> congresspeople sleep in their office i.e. are homeless because DC rent is highany source for this? your link just mentions a representative who has bad credit, it never claims he is homeless. reply lnxg33k1 10 hours agorootparentprevI am always suprised at how people think that elected official then have to get bribes in order to survive like its a doctor prescription, and how it seems that when they do we can’t do anything about it ffs reply astrange 10 hours agorootparentThey mostly don&#x27;t, people just like being cynical.Though, legislators don&#x27;t have individual power to take action to be worth giving specific bribes to; what may happen is that if someone is friendly to your business, you want to keep them motivated to keep re-running for Congress and do the awful annoying job of being a legislator, when they could instead quit and go back to industry or retire on the beach or whatever. reply Alupis 10 hours agorootparentprevJust look at most countries around the world.Corrupt elected officials is more of the norm, than the anomaly world-wide. reply nerdponx 10 hours agorootparentprevIt&#x27;s like selling your data to ad tech companies. They&#x27;ll do it no matter how much you pay for the product. reply hermannj314 10 hours agorootparentprevAmerica&#x27;s GDP allocated per congressperson is like $50 billion each.Salaries wont fix bribes for someone with that much to command and even the already rich can make a lot of money getting into politics. Their salary means nothing to them. reply dfadsadsf 8 hours agorootparentYou need to pay congressmen enough money so they can have upper middle class lifestyle - without that pressure to take bribes is dramatically higher. You have nice 5bdrm house in good school district and enough money for nice vacation + savings? Refusing bribes is easy - you are comfortable. Kids are taken care of. Wife does not have to work. This lifestyle requires 300-600k in DC.You live in shitty 2bdrm apartment with wife and two kids in poor school district? With no savings and vacation? Your life is hell and you are that much more likely to take bribes.It’s all about incentives. reply somsak2 6 hours agorootparentIs there an actual source for this logic? Not a huge fan of arguments-via-intuition. reply syndicatedjelly 8 hours agorootparentprevGDP per Congressman is a new statistic reply skyyler 10 hours agorootparentprevCongress already makes more money from cutting deals with their power & influence than they do from their salary... reply amrocha 8 hours agorootparentYes that should be illegal too. reply IG_Semmelweiss 8 hours agorootparentprev100% agree.Serving in public choice should for duty, not a career.Make a fixed salary. If you dont like it, get a job elsewhere just like everyone else.And oh, you wont get qualified candidates - nonsense. We had a movie actor, a silver spoon heir, a community organizer, a real estate broker and a lifelong politician as presidents.There&#x27;s nothing in technical expertise that they have in common. They were just good orators with some charistma and a penchant to lie with a smile...on their face.Oh, you are worried of a bribe? Good thing we have FEC disclosure forms etc. Make them audited every year. Have more bite. Increase sentences. Put bounties on whistleblowing. Then sit back and relax reply bagels 9 hours agorootparentprevPeople in congress are millionaires, and it&#x27;s not from congressional salaries. It&#x27;s due to it being somehow legal for them to insider trade, and bribes in other guises. reply Alupis 10 hours agorootparentprev> suggested that Congressional salaries be a fixed multiple of minimum wageThat&#x27;s insane no matter how you look at it.Minimum wage is paid by individuals, private businesses and their owners. Congressional pay is paid for with your money - an effective infinite of your money, or they&#x27;ll just poof new money out of nowhere to continue to pay themselves.Correlating the two is plain wrong. reply hinkley 6 hours agorootparentCongress is responsible for raising the minimum wage. That’s the connection. reply amrocha 8 hours agorootparentprevWhy do you think it matters where the money comes from? reply cj 10 hours agoparentprevIMO an equation like that would be fine as long as you made it possible for CEOs of larger companies to make more money than CEOs of small and medium sized companies.There’s a job market for C-Suite employees (whether we care to admit it or not). At a certain point you won’t get qualified candidates if you can’t reward them enough, same as engineers or any other role.The answer for how much CEOs should be paid is the amount of money you would need to pay to employ the most optimal person to run the company. Figuring out what that number is is difficult. reply skyyler 10 hours agorootparentWell that&#x27;s simple, larger companies that want to pay their CEO more can pay their lowest paid employees more. reply burnerburnson 9 hours agorootparentThat&#x27;s moronic. A good CEO at a fortune 500 company could have a $100M revenue impact. That same CEO could never be worth $100M to a local restaurant. The idea that the bigger company can&#x27;t offer more to attract a better CEO just because both businesses employ janitors that make market rate is mind-numbingly stupid. reply tbrownaw 8 hours agorootparentI thought janitoring was all contracted out these days. reply tpush 4 hours agorootparentprevBut the fortune 500 company can offer more for its CEO. It just has to pay its janitor more. Which it totally can, being a fortune 500 company. reply syndicatedjelly 8 hours agorootparentprevI guarantee you California will find a way to legislate this view into existence within the next year, with horrendous results. reply nateburke 9 hours agorootparentprev> There’s a job market for C-Suite employeesAnd it can sometimes be VERY illiquid. Headhunters help to provide this liquidity and get paid for it.Sometimes C-suite career people can go years without a job. Not every CEO or CFO makes fortune 500 comp and many people falsely assume low compensation volatility as e.g. a \"career CFO\". It can be extremely stressful, especially with family&#x2F;dependents. reply boppo1 6 hours agorootparent>C-suite career peopleHow do these people get to be C-suite career people? Seems like you basically have to be born into modern nobility. I&#x27;ve asked people who work with them about this and they make these vague claims about &#x27;ultra drive & competitiveness&#x27; but I don&#x27;t buy that those things are magically statistically concentrated in Ivy League grads. reply amrocha 8 hours agorootparentprevNo. There is no financial stress in a CEOs life. The CEO that can&#x27;t afford private yacht lessons for their daughter is not the same as the single mom working 2 jobs to feed her children. Stop drawing a false equivalency. reply acdha 8 hours agorootparentI agree but would phrase it differently: C-level stress is often self-created – gotta have a bigger mansion, longer yacht, hotter trophy wife, etc. than my frat buddies - whereas many of their employees’ stress is externally-driven - hoping the car doesn’t break and cost you a job, worrying that the cost and time of a return to office mandate will take a big chunk of what’s left of your margin after inflation, etc.I think of that whenever I’m back in San Diego and remember the tree poisoning lawsuit where these two rich guys were feuding because the one whose mansion was in front had a tree which kept growing and the other guy thought that was depriving him of a fractional sea view. This ended with the twist that he paid his gardener to poison the tree, but was caught and … I just couldn’t get past thinking about how this guy was a millionaire, living on the cliffs over one of the world’s better ocean views, and all he could do was think that it wasn’t a degree wider. reply WkndTriathlete 8 hours agorootparentI&#x27;m married to an exec and I can tell you both that(a) The stress is very real, and (b) It&#x27;s not self-created, it arises due to the responsibility and magnitude of decision-making coupled with having to deal with other execs that are promoted beyond their ability yet somehow still have absolute faith in their ability to lead and execute despite their limited experience, worldview, or both, and (c) It seems to be about the same amount of stress as those worrying about tight finances, but less than those worrying about where the next meal is coming from.Having said that the majority of CEOs that my spouse and I have worked for cannot find their asses with both hands but a minority of them have been incredibly good and well worth the compensation they were paid. reply amrocha 4 hours agorootparent\"About the same\" is not the same. I&#x27;m sure it&#x27;s a stressful job, but it&#x27;s also self imposed. If you&#x27;re getting paid a CEO salary, you can quit and live off investments after a single year of doing it.It is in no way comparable to people that need to work to live. reply syndicatedjelly 8 hours agorootparentprevTotal straw man reply amrocha 8 hours agorootparentNo. Financial stress comes from not being able to meet your basic needs. Everything after that is a choice, and choices are reversible. reply tysam_and 10 hours agorootparentprevAnd hence median vs minimum, median scales well, but minimum is more aggressively equitable -- after all, it&#x27;s a nice &#x27;soft scaling lock&#x27;, which we currently seem to struggle with.If we can hit a nice logarithmic return on investment for company size, I think that would be nice. Like many things, perhaps impossible to easily achieve, but it&#x27;s a nice thought in idea at least methinksThe answer for how much CEOs should be paid is the amount of money you would need to pay to employ the most optimal person to run the company.\"Should\" by what standard? Minimizing deadweight loss? reply chadash 9 hours agoparentprevThe problem with this is that the CEO of Walmart is a much more important job than the CEO of McKinsey but under this formula they would get paid a fraction of the amount, no matter how well they did for Walmart workers.The end result of this situation would probably be a mad dash to automate away any blue collar work, which may not be such a great thing.It’s a tough problem. reply umeshunni 8 hours agorootparent> The end result of this situation would probably be a mad dash to automate away any blue collar work, which may not be such a great thing.More typically, this results in the outsourcing&#x2F;contracting away of lower value work. reply amrocha 8 hours agorootparentprevThe CEO of Walmart doesn&#x27;t do anything for the Walmart workers, he works for the investors.I would say it would be a huge upgrade to get rid of investor CEOs and put in a worker CEO in place instead. reply syndicatedjelly 8 hours agorootparentprevWho decides how important a company is to the economy? reply ryukoposting 10 hours agoparentprevI would love it if my brokerage would show me that multiplier for the companies I invest in. reply pizzafeelsright 10 hours agoparentprevWhat about CEO pay * Widgets shipped?I always think \"I can clock out at 5pm\" or \"If I screw up it&#x27;ll only cost me my job and not 1,000 people their jobs\"C levels deal with stuff I am not interested in and the while the pay is appealing the added life complexity is not. Thus I W2 while seeing my kids 90+ hours a week. reply lostlogin 10 hours agoprev> “Obviously, CEOs should be the highest-paid person in an enterprise, but then the question is exactly just how much higher than everyone else,” Josh Bivens, chief economist at EPI, told NPR.Are there any examples where this isn’t true?I know someone who works as an engineer cleaning up nuclear waste. The workers who do the job he engineers are paid more than him. He is ok with that but did pass the comment that it’s an unusual situation. reply chadash 9 hours agoparentThe football coach is often the highest paid employee at the university. I wouldn’t be surprised if many rockstar doctors made more than some of their hospital CEOs. reply wincy 9 hours agoparentprevI’d imagine Linus Sebastian (of YouTube Linus Tech Tips fame) is taking home more money than the recently appointed CEO of the Linus Media Group. But that’s also a weird situation because he basically hired the CEO to free himself up to do the parts of the job he likes.He said someone offered to buy the company for $60M cash I think it was? Which is astounding. reply belltaco 8 hours agorootparentHe and his wife own 100% the company so it might be better to hold on to the cash for tax purposes than pay themselves a lot because tax on payroll is pretty high compared to company&#x27;s cash sitting in the bank.I think the offer he mentioned was $100M for LTT. reply lz400 9 hours agoparentprevIn banks and financial companies it&#x27;s relatively common that some rock star trader or quant that has an exceptionally good year might make more than the CEO. reply marricks 9 hours agoparentprev\"Of course they should be paid more, cause they are.\"I think NPR does a lot of damage not questioning assumptions like this. Or, often times, putting them out there themselves. reply mullingitover 9 hours agoprevI&#x27;m surprised unions don&#x27;t throw their weight around more via equity ownership in the company. Controlling the company directly is really the endgame move, once they control a majority stake, they&#x27;re literally just negotiating with themselves for their labor contracts.Even better, if they&#x27;re managing their pensions via investment houses, why not just build their own investment house and leverage their negotiating position that way? reply gen220 7 hours agoparentI always thought the same. Bargaining for some minimum amount of cash is important (food on the table, gas in the car, etc.).But beyond that, meaningful equity in the hands of labor should surely be the goal — it aligns interest in a way that cash can’t. reply mr_toad 8 hours agoparentprevIf they could afford that they wouldn’t need jobs. They’d need to hire a bunch of new workers.It would be interesting to see what happened when the shoe was on the other foot. reply mullingitover 7 hours agorootparentI&#x27;m surprised, I did a little digging and there are actually quite a few companies that are at least 50% employee owned[1]. The biggest company in the list is Publix Supermarkets, but there are also quite a few manufacturing and engineering firms.[1] https:&#x2F;&#x2F;www.nceo.org&#x2F;articles&#x2F;employee-ownership-100 reply bagels 9 hours agoprevPeople get really fixated on CEO pay. Its only of symbolic importance.Google says: 167,000 GM employees Mary Barra salary: $29MDistribute that salary across the whole workforce: $174 per employeeThey should really be talking about it in terms of inflation or profit margin. GM profit for 2022 was $21B reply pests 9 hours agoparentI always see people show this equation and it never made sense to me.No one is saying the CEO is taking money the employees would have otherwise earned. When you divide it out like that of course its a pittance per employee.The statement is about the relative scale of the CEO pay to one employee. I don&#x27;t care about the difference applied to all employees.An absurd parody I always imagine: John is 7ft tall. His group of 6 other other friends are only 6ft tall. Wow John is really tall! Nah, if you distribute his tallness between his friends they would only gain 2 inches! That has nothing to do with John being 12inches taller than any one of them. reply bagels 9 hours agorootparentIt frequently doesn&#x27;t matter how much the CEO makes. It&#x27;s not why the other employees aren&#x27;t making more money. It&#x27;s just a distraction. It boils down to \"it&#x27;s not fair\". If what is important is the ratio between CEO pay and employee pay, then the board can cut CEO pay, and call it a day. That doesn&#x27;t improve employee pay.If the employees really only care about that ratio, then they should accept that outcome from their strike. That&#x27;s not why they are on strike, they also want to be paid more, just like the CEO. reply nebulousthree 8 hours agorootparentThis is some of the most disingenuous crap I&#x27;ve read in a while. Flip it on its head:\"If c-suite really only cares about $ spent on labor, they would accept the productivity outcome of their salary proposals to employees. But they don&#x27;t. They want to get more productivity, just like the workers of [insert competing corp], while not changing or even reducing comp.\"The reason your comment is disingenuous is that it assumes that the best place for this money to go is into c-suite&#x2F;board pockets, and for some reason assumes that workers wouldn&#x27;t take the couple extra grand the CEO pay split would give them. I challenge any CEO to put it up to a (non-binding, don&#x27;t shit yourself) company-wide vote.But let&#x27;s disregard the salary split. It is an incredibly simple-minded way of approaching what to do with millions&#x2F;year. Are all of you, who justify these ridiculous ratios, truly unable to think of ways to spend dozens of millions of dollars to improve all employees&#x27; qualities of life? Those who spend all this money on consultants to figure out how to squeeze an extra penny of profits can&#x27;t figure out how to further optimize the health of the workforce instead?Cue the comments about \"no choice, fiduciary duty\". Makes me sick. reply bagels 6 hours agorootparentWhere did I state that the 21B in profit needs to go to shareholders? I&#x27;m all for giving the employees more of the profits, but not because it&#x27;s unfair that the CEO gets paid well. reply pests 8 hours agorootparentprevI agree I was just stating that comparison above I don&#x27;t like. reply notatoad 9 hours agorootparentprevyes, you&#x27;re right, if you distributed john&#x27;s height across his 6 friends, they&#x27;d all still be normal height, and nothing would really change for them, except that john would no longer be abnormally tall. you&#x27;ve taken a situation where there&#x27;s something noteworthy, and made that noteworthiness disappear.the same logic applies to CEO pay. you can either pay the CEO a lot, or you can pay everybody else an amount so slightly different that it&#x27;s unnoticeable. there&#x27;s value in paying a CEO a lot, or you can not pay them a lot, and not pay anybody else any more either. surely you can see how there might be value in offering a high salary for a role that can have a big impact on the company, and how simply not doing that and essentially erasing that money instead would be a bad decision? reply thisgoesnowhere 9 hours agoparentprevThis is a super bizarre argument.I suppose I should be paid 167,000 more per year as a software engineer at GM because it&#x27;s only 1 dollar per employee? reply bagels 6 hours agorootparentSure if the board thinks that you singularly deliver the value, why not? reply thisgoesnowhere 5 hours agorootparentI wonder why the board always seems to think they are the most important and therefore highly paid people within the company.So interesting how that works. reply JKCalhoun 11 hours agoprev> “Obviously, CEOs should be the highest-paid person in an enterprise, but then the question is exactly just how much higher than everyone else,” Josh Bivens, chief economist at EPI, told NPR.I thought I read that at Japanese companies the CEO doesn&#x27;t make 300 times what the workers make. Maybe the CEO made 10 times at most? reply hinkley 10 hours agoparentWhy should the CEO be the highest paid person in an enterprise?Why is that obvious? This foregone conclusion stuff is just notes cribbed from the aristocracy. You&#x27;re not aristocrats, you&#x27;re citizens. reply umeshunni 8 hours agoparentprevhttps:&#x2F;&#x2F;www.japantimes.co.jp&#x2F;news&#x2F;2023&#x2F;06&#x2F;30&#x2F;business&#x2F;top-au......Akio Toyoda, was paid ¥999 million ($6.9 million) last fiscal year...\"Owing to culture or corporate structure, the salaries of American executives and foreign executives in Japan have historically been much higher,” a Toyota spokesperson said Friday. \"We’re aware of the gap and we’re working to fix it.”Nissan, which has just shuffled its leadership, also published salaries Friday, showing that former Chief Operation Officer Ashwani Gupta was paid ¥726 million last fiscal year. CEO Makoto Uchida made ¥673 million.Honda said last week that its CEO Toshihiro Mibe was paid ¥348 million and Chairman Seiji Kuraishi got ¥138 million. reply ElevenLathe 7 hours agoparentprevI&#x27;m not sure why it&#x27;s obvious that the CEO should be the highest paid? Why should everybody&#x27;s boss make more than them? I know that isn&#x27;t the case for many line managers of highly-paid programmers, for example, and I assume the same is true of other similar professions, at least sometimes.Wages are theoretically a market, and probably it&#x27;s often easier to replace a skilled manager&#x2F;business bro than it is a skilled engineer&#x2F;artist&#x2F;salesman, and often it&#x27;s more important to your business. reply astrange 10 hours agoparentprevThis is true, but they also tend to get free use of things like homes and cars owned by their company. reply seanmcdirmid 5 hours agoprevJust wondering, but are any of the American car companies doing well besides Tesla? Every year they seem more irrelevant, even Texans will buy Japanese pickup trucks these days. I just don’t see them rich enough to pay their CEO or workers 40% raises, and that increase is just going to be temporary as their business become less viable and layoffs or insolvency follows. reply bufferoverflow 5 hours agoparentBest-selling cars in the US are• Ford F-series(F-150 being the most popular car in the US)• Chevy Silverado• RAM Pickup• Tesla Model Y• Toyota Rav4• Honda CRV• Toyota Camry• GMC Sierra• Nissan Rogue• Jeep Grand CherokeeSo 6 out of 10 are American. reply apexalpha 1 hour agorootparentOkay but that&#x27;s in a protected market with high entry barriers and import tariffs for foreign competitors.What&#x27;s the list globally? reply bradleyjg 10 hours agoprevI think they should get it and it should be in out of the money stock options. I don’t understand why so many dinosaur companies restrict equity based comp to the C suite. reply callalex 10 hours agoparentIt’s generally a bad deal for the worker compared to cash. An assembly line worker has absolutely zero say in the direction of the company, their work will never meaningfully move the needle on stock price, so it makes no sense to tie their income to something they have no control over. reply astrange 10 hours agorootparentShare-based compensation can be good if the stock is heading up, because it&#x27;s cheaper for the company to pay it than cash is. In between the award and vesting, the increase is paid for by equity investors.But, you have to keep working there and you have to be able to afford to wait for the vesting. reply MuffinFlavored 9 hours agorootparentprevwhy is it working for Tesla line workers? reply TheHappyOddish 9 hours agorootparentThe GP clearly stated it&#x27;s \"generally\" a bad idea. This implies that sometimes it is a good idea.Equity based payment will attract a different risk profiled group than salary will. reply toomuchtodo 10 hours agoparentprevProfit sharing is common, especially amongst employee owned companies. Base wages are behind inflation and cost of living, so of course it needs to catch up to the current macro. Fast food worker minimum wage is now $20&#x2F;hr in California, for example, and healthcare worker minimum wage will be $25&#x2F;hr (SB525).https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37558256 reply ryukoposting 10 hours agoparentprevStock options are complicated. On a factory worker&#x27;s salary, you probably don&#x27;t have the money to throw at a financial advisor who will know how to handle those options competently. reply AndrewKemendo 9 hours agoprevThis is great. Add this to SAG-AFTRA, Waffle House workers and Starbucks walkouts and we have real movement toward a general strikeIf both Wal-Mart and Amazon drivers strike we might actually have a shot a taking a bite out of capital finally. reply MuffinFlavored 9 hours agoparentserious question: say everyone you described gets a 20% raisewhat do you think will happen to the cost of goods? what companies do you think are operating with enough margin that they can just afford a 20% rise in their payroll costs? reply Sugimot0 2 hours agorootparentThe costs of goods will increase regardless of labor cost as they have already. The issue lies in the contradiction between relatively stagnant wages and the rising cost of goods&#x2F;housing&#x2F;etc which naturally develops into labor organizing.A better question to ask is what will happen when a larger portion of the profit that these workers created now flows back into their communities?Workers spend their money at family run business, and small to medium size businesses in their community and contribute to the local economy. The economic implications of this effort are not just beneficial for the 150,000 UAW workers but their communities and local economies as well.In contrast the board members who are reaping those profits are not spending money in those communities. Even if they theoretically lived in the same communities and frequented the same businesses they would not buy anywhere close to the same quantity of goods and services that the 150,000 uaw workers would with those same profits. reply amrocha 9 hours agorootparentprevSerious question: if every company was working with razor thin margins, and every worker made make minimum wage while investors reap profits from speculation, would that be good? reply somsak2 6 hours agorootparentprevgreat question! profit margins have indeed been rising significantly over the past few decades, from around 5% to 12% just since the mid-90s -- https:&#x2F;&#x2F;www.yardeni.com&#x2F;pub&#x2F;sp500margin.pdf reply AndrewKemendo 8 hours agorootparentprevThink differentlyIt’s not about a one time 20% raiseThink more like:“4 day work week”“Return to 1945-1949 tax rates”“convert corporate to cooperative ownership” reply econperplexed 11 hours agoprev> From 1978 to 2021, CEO pay grew by 1,460%, adjusted for inflation, versus just 18.1% for the typical worker.Why has the supply of CEOs not kept up with the demand for them? Surely, with the improvement in education and in increase in MBA programs, there must be far more CEOs today than in 1978. Why has the ratio of CEOs to Companies fallen by 14x for their wages to rise this much? &#x2F;sAs many argue, wages are only set by supply and demand. And if workers are underpaid, then it is because they are replaceable. Use the same framework to explain CEO pay. reply TaylorAlexander 10 hours agoparent> As many argue, wages are only set by supply and demand.Most of the time that I see people invoke \"Econ 101\" concepts, they are wrong. Supply and demand does not account for the power imbalance between workers and leadership, which unions specifically attempt to address. It does not account for the class differences between workers trying to make ends meet and the board of directors who believe they deserve much higher pay than workers. We are free to change the perceived value of workers through the power of worker solidarity - ensuring that individual workers are not so easily replaced to keep wages suppressed. You can simply say \"it&#x27;s supply and demand\" and do nothing to support the workers whose labor is so clearly needed for our economy to function, or we can support collective bargaining rights to make sure that individual workers are not crushed under the power of company leadership. How you view this is up to you, and not a simple fact of natural laws of economics. reply whatshisface 10 hours agorootparentMy econ 101 talked about the difference between commodity and illiquid markets which explains the difference between worker and CEO pay. The CEOs are coming from a restricted group of insiders and can&#x27;t be replaced because of their relationships which make them unique. Workers in the sectors most often unionized have few distinguishing characteristics from the perspective of the company and are thrown around by the same laws as the price of corn. The black death is a well-documented example of a restriction in the labor supply raising the price of labor and improving the wellbeing of laborers. There is no indication anywhere that supply and demand is wrong about the job market. reply d0liver 9 hours agorootparentYou are looking at it a different way, but the conclusion should be the same. You are saying there is more demand for specific CEOs because they are the ones who control the industry (this is actually why they \"can&#x27;t be replaced\"); yes, of course, they value themselves highly. They already have control of all of the money that can be used for hiring and therefore determine what is desirable even when it is not valuable. This makes the situation that much worse because: 1. Supply and demand can be used to model it and 2. It doesn&#x27;t do anything helpful to resolve the very real problems that exist.Demand isn&#x27;t the same thing as value. When we create an economy where a few people have all of the disposable income then we also create one where the only business interests represented are theirs. We should let value dictate demand, which only happens when we spread the money around. reply TaylorAlexander 9 hours agorootparentprev> There is no indication anywhere that supply and demand is wrong about the job market.It&#x27;s not that supply and demand is wrong. It&#x27;s that supply and demand is not the whole story. It is an open question whether workers should change the market forces at play by unionizing and demanding collective bargaining of their wages. This does not subvert supply and demand, it simply alters the pressures the company leadership experiences in the market. I said that the original comment was \"wrong\" because the comment stated clearly that \"wages are only set by supply and demand\". This seemed to me to imply that nothing could be done about low worker wages, which is clearly wrong. What workers can do, and apparently in this situation have done, is unionize and strike for better wages. reply enragedcacti 9 hours agorootparentprev> There is no indication anywhere that supply and demand is wrong about the job market.If supply and demand worked as theorized in the labor market then there would necessarily be a loss of employment when the minimum wage is raised, but that is not always the case [1].Some studies find effects and some don&#x27;t, which is a good indication that the labor market is more complicated than Econ 101 principles.[1]https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Minimum_wage#Empirical_studies reply whatshisface 7 hours agorootparentI think that&#x27;s called an \"inflexible demand curve,\" and is part of the complete theory. I do remember drawing flat lines in econ 101. :) reply noizejoy 10 hours agorootparentprev> The CEOs are coming from a restricted group of insiders and can&#x27;t be replaced because of their relationships which make them unique.Wouldn’t that be simply favouritism&#x2F;cronyism rather than a free market? reply whatshisface 10 hours agorootparentFavoritism, cronyism or more like it would actually be described by the people who do it, a preference for trusted associates you&#x27;ve worked with in the past over strangers to run your stuff for you isn&#x27;t a violation of your \"market freedom\" but rather an expression of a set of preferences that make all your choices incommensurable. P.S. it is not in the interest of investors for one employee to make way more money than normal it&#x27;s just that nobody knows how to avoid it when that employee is the center of decision-making. The kids of yesterday&#x27;s owner-operators have an entirely different set of preferences from their parents due to being real, \"classical\" capitalists... Which makes the focus on CEOs kind of ironic! Technically they&#x27;re workers who escaped the cruelty of a liquid market. reply ytoawwhra92 9 hours agorootparent> it is not in the interest of investors for one employee to make way more money than normal it&#x27;s just that nobody knows how to avoid it when that employee is the center of decision-makingSee \"A Principled Approach to Executive Pay\", Chapter 1.F in The Essays of Warren Buffet, arranged by Cunningham.The issue I take with your line of reasoning is that executive compensation is often at odds with investor interests in more ways than just its amount. Buffet describes \"heads I win, tails you lose\" executive compensation plans in an essay from the 80s&#x2F;90s. He describes how they do things at Berkshire Hathaway - they&#x27;re doing very well and I doubt have any issue recruiting good executives.Despite this, and decades later, we see terrible compensation plans being approved by boards. We see executives exiting failed businesses with enormous paychecks. We see boards offering those same executives new management positions with terrible (for investors) compensation plans. What gives?It certainly looks like cronyism to me, but maybe I ought to be applying Hanlon&#x27;s Razor. reply whatshisface 7 hours agorootparentCronyism isn&#x27;t an exception to microeconomics, it&#x27;s a lesser-known example of a consumer preference. In this case the board has a... preference... for cronies. :-) reply jjoonathan 9 hours agorootparentprevEmployees at the bottom of the pyramid can make bullshit excuses too, but it is a privilege of the upper layers to have no one in a position to call them out. reply ummonk 9 hours agorootparentprevYeah it’s like trying to apply supply and demand to politican’s wages. Company directors and executives set their own wages using others’ money. It’s obviously not a market. reply andrepd 9 hours agorootparentprevYes, I believe that is exactly the point: the invocation of econ 101 as explaining away all the iniquities of life as \"simply how the world works\" is simply a veneer of objectivity over what is just cronyism and imbalances of power and oppirtunity. reply listless 10 hours agorootparentprevThis is so obviously the right answer, I can’t believe its not upvoted more. To be clear, I stand with the union on the principal of their argument. However, the principals of economics itself, much like gravity, do not care about the nobility of your cause.It seems to me that what the union is asking for is that the corporation act more like a benevolent force than one that is restricted by market pressures. Of course, you could use the power of government as one poster mentioned to force this issue, but those violent delights have violent ends. reply d0liver 9 hours agorootparentYou&#x27;ve got it backwards. The principals of economics accurately describe a shitty situation created by corporations. They do not determine how the corporations must act; economics models exchanges of wealth. When the corporations control the wealth they simply model what the corporations are doing. reply TaylorAlexander 9 hours agorootparentprev> It seems to me that what the union is asking for is that the corporation act more like a benevolent force than one that is restricted by market pressures.I disagree. The union is simply changing the market pressures that the corporation experiences. There is no benevolence required when you are faced with a strike. You either negotiate acceptable terms, or you have to deal with the consequences. reply akoboldfrying 9 hours agorootparentprevFrom a purely laissez-faire perspective, so long as the union does not call for government interference -- and there is no indication in the article that they have done so -- their strike action (or threat of such) is merely a way to influence the supply side in the question of supply and demand.Do you feel this to be illegitimate? If so, why? reply smutlord2099 10 hours agorootparentprev\"but those violent delights have violent ends.\" You talk as though the US isn&#x27;t steeped in regulation. All modern capitalist nations are. Without regulation, you get absurd instabilities as we saw until the New Deal.If someone&#x27;s selling you a simple explanation, they&#x27;re likely wrong. reply jacobr1 10 hours agorootparentprevOr for this audience, the tech industry, where in-demand skills are often bid up in the form of higher salaries. reply kortex 9 hours agorootparentprevExactly this. The \"law of supply and demand\" is like the ideal gas law: it is a useful model, but it has certain key assumptions. And just as the ideal gas law breaks down in extreme conditions, supply and demand does not hold without certain invariants.The farther actors are from perfect information, the more asymmetry in the system there is, the slower the system is to react, the more the relationship between supply and demand breaks down. reply WalterBright 10 hours agorootparentprev> Supply and demand does not account for ...Yes, it does, in each scenario you presented.> We are free to change the perceived value of workers through the power of worker solidarityIt&#x27;s more like using the power of government to ensure the company has no alternative to the union.Company leaders have no actual power over the workers. They cannot force anyone to come to work. They cannot have you arrested. They cannot confiscate your property. They cannot beat you. They cannot prevent you from accepting a job at another company. They cannot prevent you from leaving. They cannot extort, libel, slander, blackmail, or threaten you. They cannot put a hit on you.All they can do is offer you money in exchange for your labor. That&#x27;s it. reply nicoburns 10 hours agorootparentHaving control of money is power. A kind of power that everyone needs, and many workers (who are typically poorer than company leaders) don’t have secure access to. reply WalterBright 8 hours agorootparentHaving skills someone with money needs is power, too. reply astrange 10 hours agorootparentprev> They cannot prevent you from accepting a job at another company.They can totally do that with a non-compete clause. Just not in California. reply sib 9 hours agorootparentI&#x27;m pretty sure that there is no US state where a company can \"prevent you from accepting a job at another company.\"In some states, and in some circumstances, they may be able to prevent you from accepting a job at a competitive company. reply edgyquant 9 hours agorootparentWhich means they can prevent you from utilizing the skills you have, forcing you to start from the bottom. How is this not an extremely lopsided power akin to preventing you from working? In general people will keep quiet to keep making the same money to support their family, etc reply TaylorAlexander 9 hours agorootparentprevClearly a competitive company counts as \"another company\". reply sib 9 hours agorootparentYes - but not every other company - so they have not prevented you from working at another company... reply TaylorAlexander 9 hours agorootparentThis seems like a bad faith reading of the english sentence here. The phrase \"every other company\" was not used. Obviously they can depress the value of worker&#x27;s wages as a whole if they can prevent you from working at the competition - because a worker with a specific skill set will be best utilized at companies which are in the same field, and so generally in competition with one another. reply astrange 7 hours agorootparentprevMost people with skills are only really capable of working at competing companies. replyTaylorAlexander 9 hours agorootparentprev> It&#x27;s more like using the power of government to ensure the company has no alternative to the union.Unions absolutely do not need the government to exist. It is only through government attempts to control and curtail unions that the government has involved itself in unions. See for example the Taft-Hartley act:https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Taft%E2%80%93Hartley_Act reply WalterBright 8 hours agorootparent> Unions absolutely do not need the government to exist.That&#x27;s absolutely correct. But in America, the unions use the government to tilt the balance heavily in their favor. reply TaylorAlexander 1 hour agorootparentWhat makes you say that? The Taft Hartley act I linked to significantly weakened union power in this country. In other countries like Germany unions have much more power and representation. reply conradev 10 hours agorootparentprev> Supply and demand does not account for the power imbalance between workers and leadership, which unions specifically attempt to address.You’re both right. Power dynamics and supply and demand are interrelated.Most forms of human organization involve hierarchy. Even co-ops have a CEO and that CEO makes a multiple of base worker pay.What that multiple is and how it gets decided is a fair question: in co-ops, voting rights are built-in, but for private corporations you need collective bargaining reply hgdfhgfdhgdf 9 hours agorootparentprevI have bad news for you. Those workers who demand 40% wage increases and 32 hour work weeks will be \"easily replaced\" by auto factories in the South who have right to work laws. reply TaylorAlexander 9 hours agorootparentThis is why global worker solidarity is so important. reply edgyquant 9 hours agorootparentIt’s also unrealistic. Different workers in different nations have different cultures and compete with each other for a scarce supply of buyers. reply TaylorAlexander 9 hours agorootparent> Different workers in different nations have different cultures and compete with each other for a scarce supply of buyers.This is a description of the status quo of capitalist competition, but is not a fact of nature. We can arrange our economies to work in cooperation with one another, rather than in competition. Competition is used specifically to depress all worker wages for the benefit of the ownership class, and we do not have to accept that at face value. reply elforce002 9 hours agorootparentprevInteresting. What&#x27;s that if I may ask? reply ummonk 9 hours agorootparentprevWhy would workers in the global south feel any solidarity towards workers in the wealthiest country on earth who earn global top 5% wages? reply orange_joe 9 hours agorootparentwell, the unions pushed for wage requirements in the auto industry within the USMCA (NAFTA2) and required Mexico to (effective) legalize unions alongside lots of other rights enshrining measures. So Mexican workers benefited greatly from the work of these unions. reply mr_toad 8 hours agorootparentprev> Supply and demand does not account for the power imbalance between workers and leadershipSupply and demand is the imbalance. The workers, individually have no control over supply. The whole point of a union is to control the supply of labour and shift the balance of power. reply drewcoo 9 hours agorootparentprev> Most of the time that I see people invoke \"Econ 101\" concepts, they are wrong.It&#x27;s not that they&#x27;re wrong. Worse, they&#x27;re thought-stopping.They&#x27;re mantras we all know and we are trained to \"accept their wisdom\" and stop questioning. reply astrange 10 hours agoparentprevIt depends how they&#x27;re counting.There was an example in Money Stuff this week where a CEO got a pay package \"worth $110 million\". It&#x27;s actually made of stock options that vest if the share price went above $150, but the expected value was $110 million so that&#x27;s what was reported.…But the share price only reached $66. So in fact he was paid zero, and he quit. (Well, $1.5 million in cash.)https:&#x2F;&#x2F;www.wsj.com&#x2F;business&#x2F;ceo-with-110-million-pay-packag... reply vel0city 10 hours agorootparent> paid zero> paid $1.5 million in cashThese are two wildly different things. reply jjoonathan 10 hours agorootparentJust think of all the risk that executive took by potentially only earning $1.5 million dollars! EDIT: called it, MichaelDickens just made this argument unironically. reply MichaelDickens 10 hours agorootparentprev$1.5 million is approximately $0 in comparison to the originally-quoted $110 million—it&#x27;s less than 2% of the original amount. reply vel0city 7 hours agorootparentBeing honest, I&#x27;d really have to know what kind of timeframe that $1.5M was paid out. Was that a single year? 5? 10? It changes my opinion, slightly.If its $1.5M in a single year, I have absolutely no tears to shed.If that was 5 years, that&#x27;s $300k&#x2F;yr in compensation. A very comfortable salary in just about every place in the country. Once again, I probably have no tears.If that was 10 years, that&#x27;s $150k&#x2F;yr. Ok, if you&#x27;re expected to live in Manhattan or whatever, I&#x27;ll agree that&#x27;s pretty tight. Still though, that&#x27;s a couple times the average household income in the US. Congratulations on being a bit closer to a plebeian. Boo hoo, guy who could have had generational wealth just had to live like the rest of us. What a tragedy. Truly zero income, being more than twice the average household income for a single earner without even thinking of other compensation he might have received.I can&#x27;t imagine this was some kind of performance pay over 15-20 years, so his pay has to be in this. I don&#x27;t have a WSJ subscription, so I can&#x27;t comment further on it. reply ramranch 9 hours agorootparentprevYou don&#x27;t spend a percentage, you spend a fixed amount. $1.5 million is enough to live in a nice house, eat what you want, send your kids to nice schools, Aruba in the summer and Aspen in the winter, and still have enough to buy a very fast car. It&#x27;s easier to focus on EV once you live this life. reply ceejayoz 9 hours agorootparentprevAnd yet, a massively life-changing annual salary level for almost all Americans. reply queuebert 9 hours agorootparentprev\"A million here and a million there, and pretty soon you&#x27;re talking real money.\" (To paraphrase.) reply outside1234 10 hours agorootparentprevYes - that is serious tails I win heads you lose sort of numbers reply spondyl 10 hours agorootparentprev> It&#x27;s actually made of stock options that vest if the share price went above $150, but the expected value was $110 million so that&#x27;s what was reported.It gets a little weirder than that I suppose since you might have a great deal of shares in a company, which you never exercise so you could say that money doesn&#x27;t really exist until exercised.You can however use those shares as collateral for a mortgage or other line of credit which is a pretty common tactic for wealthy people to avoid paying income tax that they otherwise would if they were to sell said shares. reply WalterBright 10 hours agorootparentShares can and do go up and down quite a bit. I bought heavily into one tech company where the shares went down 90% in a couple months. It was pretty devastating for me.> You can however use those shares as collateralOnly shares that you own, not have an option to buy. And only a portion of the shares, and if your shares become worth less than the loan, you still owe the money, and they&#x27;ll come after any other assets you have.> you might have a great deal of shares in a company, which you never exerciseYou&#x27;re confusing owning shares with having an option to buy shares. They&#x27;re very different. You&#x27;ll also owe income tax on the difference between the exercise price and the current price of the shares. (A friend of mine didn&#x27;t know that, and consequently lost his house.) reply astrange 10 hours agorootparentprevIn this case he didn&#x27;t get any shares. If he&#x27;d used the options to get a mortgage then he&#x27;s got a pretty big hole in his pocket when they became worthless. reply taf2 10 hours agorootparentprevIsn’t that kind of lending what caused the Great Depression? Pretty sure you can’t take a loan out on a promise of an option value… reply olliej 10 hours agorootparentprevAs someone pointed out \"1.5 million in cash\" for not doing the job you were hired for is .. not zero.I&#x27;m fairly sure I could achieve the same results for considerably less.Moreover making compensation dependent on stock price movement encourages corruption and fraud - look at the numerous Enrons and other financial claims. All of which left the majority of those responsible enriched while destroying the lives of others. reply wins32767 10 hours agorootparent> I&#x27;m fairly sure I could achieve the same results for considerably less.I&#x27;m highly skeptical of that. Not driving the company value to zero is worth a significant amount of money; anyone who has worked under a bad executive or CEO can tell the difference between one that didn&#x27;t accomplish aggressive goals and one that&#x27;s objectively bad. If you have a bunch of executive experience, maybe you&#x27;d be able to replicate that CEO&#x27;s performance, but absent that it&#x27;s more likely you&#x27;d cause more harm than just not making the goal. reply marcus_holmes 9 hours agorootparent> If you have a bunch of executive experience, maybe you&#x27;d be able to replicate that CEO&#x27;s performance, but absent that it&#x27;s more likely you&#x27;d cause more harm than just not making the goal.I doubt this is true. Organisations have inertia, and getting one to change direction or do something different is remarkably difficult. (source: I have been hired as a CEO to change the direction of an organisation. It was difficult).I strongly suspect that an average person being put into the CEO role would do fine as long as the organisation was basically OK. They&#x27;d make mistakes, sure, but everyone makes mistakes. The organisation has ways of limiting the damage of mistakes.The hard bit about being CEO is taking full responsibility for your decisions with no feedback. You can deal with this in a wide variety of ways; arrogance, narcissism, authoritarianism, or humility, honesty, and teamwork. We tend to see the first three because those kinds of personalities cope well with this kind of difficulty. But that doesn&#x27;t mean this is the best way to deal with it. reply ipaddr 10 hours agorootparentprevThe CEO here is the least likely to be responsible for the increase&#x2F;decrease in profits&#x2F;value in that one year. The best you can hope for is a pr boost. Whatever strategy change would not be visible in the short period. Key hires there is not enough time for an impact. Whatever value increase is going to come from vps and everyone down the line. Why not give them the upside on the value they create? reply chii 10 hours agorootparentprevwell the CEO&#x27;s been set a target share price, and he did not reach it. Dunno if you could consider that as failing. reply hellojesus 10 hours agorootparentprevIt&#x27;s zero of the bonus, of which the CEO had assigned a prior that made him choose that job over other offers he likely had.Performance based bonuses exist at virtually all levels of skill in companies. What you see as a catalyst for fraud could also be framed as a catalyst for incentivizing an outcome most likely achieved via hard&#x2F;smart work.$1.5M is not nothing, but it&#x27;s less than 1% of the CEOs potential compensation. reply vel0city 7 hours agorootparentWhat percentage of Walamrt&#x27;s employee base probably has a significant part of their pay tied to performance?Home Depot? UPS? Amazon&#x27;s 1.6M, not just those in Seattle making well into the six figures? Kroger? Albertsons? Target? Starbucks? You really think a bagger or cart grabber or barista is getting a performance-based bonus? These are some of the largest employers in the US.The vast majority of the US workforce probably has less than 10% of their income (probably close to 0%) directly tied to performance. A massive chunk is entirely how many hours they get on the clock, that&#x27;s it.> Performance based bonuses exist at virtually all levels of skill in companiesThis really sounds like the perspective from someone who&#x27;s never punched a clock. reply lovich 9 hours agorootparentprev>Performance based bonuses exist at virtually all levels of skill in companieslol. Any data on that? In tech maybe, but I&#x27;m fairly confident(also an unsourced opinion) that the vast majority of workers in the US receive zero bonus, skill based or otherwise reply aaomidi 10 hours agorootparentprevRight, but in general the CEO doesn’t really do much to the stock price. If the general market is up, it’s going up. If the market is down, it’s going down.How many companies really break that relationship in their market sector?So tldr, ceo pay is based on the economic cycle rather than how the company does. reply borisjabes 10 hours agorootparentSome public CEOs comp packages are defined by them exceeding $SPY so boards do expect more than just the economic cycle. reply somebodythere 10 hours agorootparent50% of SPY constituents are going to beat SPY. That in itself doesn&#x27;t imply the CEO is a meaningful factor in company performance. reply HDThoreaun 9 hours agorootparentThis isn’t true because of the weighting. If apple doubles in value SPY will be up 8% from that alone. It’s easy to see a scenario where one massive company has an incredible year and every other company returns below the average. reply vkou 9 hours agorootparentOr, alternatively, one massive company has a bad year, and everyone else comes out smelling like roses.On average, 50&#x2F;50 is a pretty decent benchmark here. reply WalterBright 10 hours agorootparentprev> but in general the CEO doesn’t really do much to the stock priceSteve Jobs, Bill Gates, Elon Musk, Satya Nadella, etc.> ceo pay is based on the economic cycle rather than how the company doesYou can always start your own corporation, name yourself CEO (all you gotta do is file some paperwork and pay an annual fee) and rake in the dough for doing nothing!! reply ummonk 9 hours agorootparentNobody is complaining about founder pay. The complaint is about publicly traded companies being subjected to elite capture by parasitical execs. reply WalterBright 8 hours agorootparentSatya Nadella is not a founder. But during his tenure the value of MSFT went up by 10x. I&#x27;d say he&#x27;s well worth his compensation. reply ipaddr 9 hours agorootparentprevThose are founders but this is an external hire at a later stage not really the same. reply astrange 7 hours agorootparentThe story I linked is actually a cofounder. reply Per_Bothner 9 hours agorootparentprevThat is true. Elon Musk has greatly affected the stock price of the Company Formerly Known as Twitter. reply WalterBright 8 hours agorootparentSince Musk took it private, the stock price is not known. replyr3trohack3r 10 hours agoparentprevNot all jobs have quantitative measures for impact. The more a job is decoupled from a measurable value, the more likely that job is to be bid up to \"as much as the company can afford.\"I&#x27;ve seen this in my career, the closer I get to \"SRE\" or \"platform engineer\" the more decoupled my salary has become from my actual measurable value.I&#x27;d articulate the thinking as, roughly:We know this role is important. We know that having a bad SRE team (or CEO or platform team) is expensive, it could cost us the 100% of the business. And we don&#x27;t know how to measure the value a good one provides. Therefore we are willing to spend as much as we can afford to make sure we get a good one. reply candiddevmike 10 hours agorootparentSREs aren&#x27;t on the board of directors setting the pay for other SREs who are on their board of directors though.The elite&#x2F;ownership&#x2F;\"capitalism winner\" class are playing on an entirely different level with their power and influence. Don&#x27;t kid yourself into drawing comparisons with your situation and miniscule in comparison compensation. It&#x27;s always heads they win, tails you lose. reply smfjaw 10 hours agorootparentyea lol there&#x27;s no economics at play here other than greed, which I guess can be a form of economics lol reply Jaygles 10 hours agorootparentThe people who decide where money gets allocated are allocating most of it to themselves? If this was done in a government it would be called extreme corruption. reply jjoonathan 10 hours agorootparentBut but competition keeps them in check!!!(Competition between the two players in the market that have a revolving door of executives, a carefully crafted moat a mile wide, and raise prices together in lock-step but definitely do not collude, no sir!) reply logicchains 9 hours agorootparentprev>The people who decide where money gets allocated are allocating most of it to themselvesThe difference is that the people who decide to pay the CEO so much are the ones who made the money (the company owners), it&#x27;s their money to spend how they see fit. In the government it&#x27;s not people spending their own money, they&#x27;re spending other people&#x27;s money that they obtained through the threat of jail for those who don&#x27;t pay it. reply jacobr1 10 hours agorootparentprevThat is generally how it works though. The buyer wants to pay as little as possible and seller wants the maximum. That works in most circumstances.In the case hiring, in a tight labor market, there are more open roles than people, so the companies that want a particular hire, or value experience in that role, need to pay more to secure the candidate. And those that don&#x27;t, end up with roles that aren&#x27;t filled. In a market with more labor, then the firm has its pick of candidates, and can make lower offers, secure in the fact it probably will get a bite from a candidate with fewer options.It works that way with oil, or hog futures, or my corner bodega&#x27;s sandwich prices. reply mr_toad 9 hours agorootparentprev> as much as the company can afford.This is pretty much it. Everyone wants to avoid hiring a bad CEO so it’s a bidding war for executives with track records at large companies.And when you’re making billions a year the CEO compensation is a drop in the bucket. reply carlosjobim 9 hours agorootparentprev> We know this role is important. We know that having a bad SRE team (or CEO or platform team) is expensive, it could cost us the 100% of the business. And we don&#x27;t know how to measure the value a good one provides. Therefore we are willing to spend as much as we can afford to make sure we get a good one.And who are \"we\" in this train-of-thought? If it&#x27;s shareholders, that&#x27;s where the root of any problem lies. If shareholders are real businessmen and entrepreneurs who built up the company or similar companies, they will have a clue as to what is a good CEO. If the shareholders are real workers who believe in the company they&#x27;re working for, they will have a clue as to what is a good CEO.Today, shareholders are no longer real businessmen or real workers, but retirees represented by bureaucratic investors. That&#x27;s why they have no clue as to what is a good CEO. reply ummonk 9 hours agorootparentYup the shareholder is an ETF today. The people voting on behalf of shareholders aren’t the people benefiting from the success of the company. reply highwaylights 10 hours agoparentprevTo paint the above in starker terms:Worker pay is set by the leadership, leadership pay is set by the leadership. reply hellojesus 10 hours agorootparentLeadership is set by shareholders. There is still accountability.Public unions seem more undesirable. There we have the gov negotiating with itself with no external accountability, unlike your example. reply mullingitover 9 hours agorootparent> Leadership is set by shareholders.Funny thing, I own a vast number of stocks through various means, either directly or via ETFs, and yet never in my life have I had an opportunity to weigh in on the salaries of anyone in the companies I hold equity in.> Public unions seem more undesirable. There we have the gov negotiating with itself with no external accountability, unlike your example.The government is negotiating with unions, not itself. However, if we&#x27;re going to take a crack at public sector unions, let&#x27;s start with the police unions. reply tpmoney 9 hours agorootparent>Funny thing, I own a vast number of stocks through various means, either directly or via ETFs, and yet never in my life have I had an opportunity to weigh in on the salaries of anyone in the companies I hold equity in.Really? I get shareholder meeting notices for voting on compensation packages all the time. I admit that the chances of me personally shutting down the pay package of AIG&#x27;s executive team is pretty much nil, but that&#x27;s also because I personally own pretty much nil in terms of voting stock, and most people like me probably either don&#x27;t vote at all, or vote with the board recommendations. Still, it&#x27;s always within my power to try to start a shareholder movement on this front. reply redox99 9 hours agorootparentprev> Funny thing, I own a vast number of stocks through various means, either directly or via ETFs, and yet never in my life have I had an opportunity to weigh in on the salaries of anyone in the companies I hold equity in.Because obviously what you call \"vast\" is actually a minuscule percentage of the company. reply ElevenLathe 7 hours agorootparentprevExactly. \"shareholder activism\" is called that because it is a considered an anomalous, bizarre state of affairs. Imagine the temerity of the people who own the shares of the company calling the shots! In reality, management has basically a completely free hand to do whatever they want so long as an incumbent board doesn&#x27;t get its shit together and replace them (and even that is sometimes not enough).This is also beside the main point, which is that management shares a class interest with shareholders, even if they aren&#x27;t literally the same people at some particular point in time. Capital and management are obviously on the same side, and labor isn&#x27;t part of the club. reply vel0city 7 hours agorootparentprevI haven&#x27;t actually kept track, but it wouldn&#x27;t surprise me for my direct stock ownership I&#x27;ve voted on executives in at least 70% of them and executive pay packages on probably 20% of them. Mostly mid to large cap though.Not that my votes \"no\" were ever successful in curbing the growth of executive pay. Hard to vote against the executives who probably own a decent chunk of the voting shares themselves. reply tbrownaw 9 hours agorootparentprev> and yet never in my life have I had an opportunity to weigh in on the salaries of anyone in the companies I hold equity in.None of them do the \"say-on-pay\" thing where stockholders get to vote on whether they&#x27;re happy with the pay packages for named officers? reply ipaddr 9 hours agorootparentprevPublic union leadership is determined by a member vote. Union leadership represents the worker. The external accountability comes from you regardless if your elected official is dealing with a union or a private company reply catchnear4321 10 hours agorootparentprevinstitutional ownership has entered the chat. reply kangaroozach 10 hours agorootparentprev100% this. Leadership gets to decide how insanely hooked up they will be. Same as congress. Remember when they carved themselves out their own Health Insurance requirements? Same thing. reply astrange 10 hours agorootparentThat hasn&#x27;t been true for ~13 years now, Congress&#x27;s healthcare comes from ACA exchanges. reply mr_toad 9 hours agorootparentprevThe board, elected by the shareholders, sets the compensation of executives. reply WalterBright 8 hours agorootparentprevWorker pay is set by supply & demand. After all, the pay has to be enough to attract workers. reply ripvanwinkle 10 hours agorootparentprevexactly! reply gretch 10 hours agoparentprevI support the workers&#x27; decision and negotiating on labor rates is the basis of our economic system. Get what you deserve.That said, the CEO pay is easily explainable:> Profits at the struck auto companies increased 92% from 2013 to 2022, totaling $250 billion, according to EPI> CEO pay at the Big Three has grown 40% in the last decade, according to EPIIf you&#x27;re the CEO of a company and you increased profits by 92%, I don&#x27;t know seems pretty fair to me. There&#x27;s a lot of other businesses that suck and haven&#x27;t raised their profits in the meanwhile.I think talking about the CEO pay is the wrong path (though I do admit it&#x27;s marketable in the mainstream news). Just ask for what you think you deserve and don&#x27;t work if you don&#x27;t get it. It&#x27;s as simple as that. reply afavour 10 hours agorootparent> If you&#x27;re the CEO of a company and you increased profits by 92%Did you? Attributing all the success a company achieves to the CEO feels shortsighted.Aside from anything else: if all these companies saw their profits grow by so much surely there’s an external commonality there? “The CEO did it all” would be slightly more plausible if only one company experienced that success. reply chii 9 hours agorootparentIt doesn&#x27;t matter what \"the truth\" is, as long as the board (aka the owners) believe that the CEO&#x27;s decision making is what led to the profits. The CEO&#x27;s compensation is commensurate with this belief of cause and effect. reply bravoetch 10 hours agorootparentprevAnd that&#x27;s easy to test. Remove everyone except the CEO, and see how things work out. reply fallingknife 9 hours agorootparentBad argument. I could just as easily say \"remove everyone except the ICs and see how things work out.\" reply ipaddr 7 hours agorootparentThat would work and leave the company functioning well. Without the talking heads there is little need for management and IC domain experts and IC architects and IC sales and customer service will continue and leadership will naturally come from the rank and file. reply fallingknife 9 hours agorootparentprevMaybe. It could well just be the market and luck, but it could be the CEOs decision. On the other hand, it&#x27;s never because the line workers just started working harder. Think about being an early engineer at Amazon vs the same at another startup that never went anywhere. Those engineers did the same amount of work, but the difference is that Jeff Bezos was telling the Amazon engineers what to build. BTW I&#x27;m not making an argument that workers shouldn&#x27;t share in the gains, just that people at the top are much more likely to be responsible for them. reply afavour 8 hours agorootparent> Those engineers did the same amount of work, but the difference is that Jeff Bezos was telling the Amazon engineers what to buildParticularly at early stage startups engineers are often responsible for a ton of innovation. It’s rarely exclusively top down instruction. Same goes for pretty much every company I’ve ever worked at. The CEO is not making every decision and coming up with every idea. reply amarshall 10 hours agorootparentprev> If you&#x27;re the CEO of a company and you increased profits by 92%This assumes that the increase in profit is attributable primarily to the CEO. Well, that at least 43% (40&#x2F;92) of it is.> Just ask for what you think you deserve and don&#x27;t work if you don&#x27;t get it. It&#x27;s as simple as that.This only really works with unions. Fortunately, these workers have one. reply nerdponx 10 hours agorootparent> Just ask for what you think you deserve and don&#x27;t work if you don&#x27;t get it.This is the most wildly privileged take possible on the issue. reply prawn 10 hours agorootparentI agree. Someone in the leadership tier can generally get by longer without desperately needing to work; their house is often paid off, they get dividends from shares, etc. Down in the blue collar ranks, many of those people are scrounging to stay on top of rent, or pay a mortgage, or getting hit by rising food costs. Many workers aren&#x27;t in a position to change jobs on a whim. reply tbrownaw 9 hours agorootparentIt&#x27;s probably a bit easier if you can collect unemployment for refusing to work, like I hear California is doing. reply sudosysgen 10 hours agorootparentprevIt&#x27;s called a strike and it seems to be working. The key is that everybody does it at once. reply WalterBright 8 hours agorootparent> The key...If that were true, the pay of every non-union employee would be minimum wage. reply sudosysgen 7 hours agorootparentThat does not follow in the slightest. reply Jensson 9 hours agorootparentprev> This assumes that the increase in profit is attributable primarily to the CEO. Well, that at least 43% (40&#x2F;92) of it is.The CEO&#x27;s didn&#x27;t get 100 billion extra pay, they just got 40% more than before not 40% of all the profits. reply l33t7332273 10 hours agorootparentprevIt seems erroneous to me to imply that the CEO deserves a bigger pay increase than the workers that actually increased profits by 92% reply ajsnigrutin 10 hours agorootparentDoing math like this is always bad and just leads to arguments.Imagine a chocolate bar factory, making chocolate bars for 50 cents and selling them for $1... 1mio per month, 500k profits per month.Then a new ceo comes, sees that all the ingredients are vegan, there are nuts inside, making the bars \"healthy\", slaps on vegan logos, superfood logos, changes the ads to make the chocolate bar seem more high end and raises the price to $2 each... due to new logos, superfood text and ads, even with a price increase, 1mio of chocolate bars are sold, and the profits rise from 500k to 1.5mio per month.Did the workers make the chocolates that brought in 500k? sure. Did the same workers make the same chocolates that brought in 1.5mio? sure. Did the workers do anything differently than the month before? Change anything? No. Did the ceo make a single chocolate bar? Nope. If the workers did the same as they did the month before, and if the CEO didn&#x27;t make a single chocolate bar, who &#x27;created&#x27; the extra 1mio of profits?Sure, it sounds intuitive to put dollar values to each person, but reality is more complex than that. In the end, workers want to get the as much as possible money for least work done, and owners want to pay as little as possible for as much as possible work done... and that applies to everything, from workers and owners in a company to average joe buying apples (most apples for least amount of money) reply no_wizard 9 hours agorootparentthis is a superficial argument that leads to the point that you want it to, rather than determination of the factors such as:Was there an entire marketing department that was needed for this to be successful? Why aren&#x27;t we quantifying that?Why aren&#x27;t we quantifying the new QA processes to make sure the packaging is correct?Why aren&#x27;t we factoring in workers downstream contributing to the success by being able to pivot and be malleable in the job making this possible in the first place?Why is it so anathema to people that everyone can share in the profits? Its not like we&#x27;re saying only pay CEOs 100K per year or something. 1:25 ratio pegged to the lowest paid worker use to be the norm, for decades, and CEOs were plenty happy with that too. reply ajsnigrutin 9 hours agorootparentYou cannot calculate stuff like this... you just can&#x27;t.If they earn so much more, should they pay contractors more too? Does your plumber ask how much do you earn before he fixes your toilet?What about other supplies? Should they pay more for cocoa? For sugar? Do you pay more for bread in your local store than someone who earns minimal wage?You do none of that... you try to get the lowest price when you&#x27;re paying (even if the plumber has 8 kids to feed at home) and get as much as possible when you&#x27;re the one getting paid. Same with workers.. you pay enough to have them stay and they do as little as work as possible to stay employed.I agree that some workers should be paid more, and that&#x27;s why they&#x27;re striking and hopefully they&#x27;ll succeed... but taking out a calculator and saying that someone made X more so someone else should get paid more too by the same ratio is stupid. In my country, there&#x27;s a huge shortage of contractors (electricians, plumbers, painters, tile-layers etc.) and the good ones earn as much as doctors... and due to a shortage, large companies have problems getting eg. electricians (because contracting pays more and gives more freedom), so their pay is going up... does that mean that accountants pay should go up too? (there&#x27;s no shortage of accountants) reply lovich 9 hours agorootparent>You cannot calculate stuff like this... you just can&#x27;t.Then why assign the cause of the increase to the CEO? replynitwit005 10 hours agorootparentprevCEO pay has gone up pretty much across the board, significantly outpacing workers for several decades, even at companies that have not done well. reply lancesells 10 hours agorootparentprev> If you&#x27;re the CEO of a company and you increased profits by 92%,If it&#x27;s a company of one than I agree. In any other scenario the CEO led the company to a 92% increase. The CEO didn&#x27;t do this alone. They maybe managed that increase and had a part in it but others most likely did the vast majority of all the work.Now, how do you feel if the CEO fired 50% of the workers and maybe the remaining work twice as much to make up the slack. This leads to a 92% increase in profits and the CEO deserves the vast rewards? reply DanHulton 10 hours agorootparentprevThere is a big difference between \"CEO responsible for increasing profits\" and \"CEO while profits increased\", and it&#x27;s not at all clear that this is one or the other. (Although in fact, if all competitors in an industry all had their profits grow, I think that&#x27;s actually likely excellent proof that the CEOs weren&#x27;t responsible at all.) reply pratheekrebala 10 hours agorootparentprevThe workers also contributed to that 92% increase in profit. reply waynesonfire 10 hours agorootparentAnd they got 18.1% for it. reply toomuchtodo 10 hours agorootparentThe effort is to close that gap. Can’t build cars without workers. Elon famously tried and failed.https:&#x2F;&#x2F;techcrunch.com&#x2F;2018&#x2F;04&#x2F;13&#x2F;elon-musk-says-humans-are-... (“Elon Musk says ‘humans are underrated,’ calls Tesla’s ‘excessive automation’ a ‘mistake’”) reply dgacmu 9 hours agorootparentTangent: This is a mathematically muddled discussion that turns out to be about right anyway.If profits went from $5 per year to 10, they increased by 100%, but that doesn&#x27;t mean that you have room for a 100% increase in everybody&#x27;s salaries.A much better way to have this discussion would be to look at the dollar value increase in profits versus salaries.Profits are forecast to be about $32b in 2023, up from about $19b in 2013. At about 240k total employees, and an average union pay of about $30&#x2F;hr - call that about $90k year fully burdened per employee - a 40% increase would cost the company about $8.6B, or about 2&#x2F;3 of the growth in profits. That may be overestimating the costs, as there are only 146k workers in the union, in which case the $5.2b increase in employee costs is... 40% of the profit growth, which seems like an entirely reasonable split of employee vs shareholder gains. reply digdugdirk 7 hours agorootparentprevThat&#x27;s assuming that the CEO has anything to do with the growth in profits. Average US GDP looks to have grown by ~66% in the past decade. Do some spreadsheet shenanigans and it wouldn&#x27;t take much to drastically outperform that number.Who&#x27;s to say that the various attempts to \"chart a new course\" and change business strategy for the shareholder&#x27;s benefit during that time didn&#x27;t actually make these corporations underper",
    "originSummary": [
      "United Auto Workers (UAW) members at Ford, GM, and Stellantis have initiated a strike following failure to agree on worker compensation with the companies.",
      "The UAW wants a 40% hourly pay hike spread across four years. They claim that while the auto companies' profits grew by 92% from 2013 to 2022, the workers did not proportionally benefit.",
      "The strike, striving for the abolition of compensation tiers and more workplace safety, threatens to halt activities at key plants and may expand based on negotiation outcomes."
    ],
    "commentSummary": [
      "The text discusses a range of topics related to CEO pay, worker compensation, unions, and power dynamics within corporations.",
      "Areas of focus include income inequality, the role and stress levels of CEOs, the impact of worker strikes, and issues around wealth distribution and allocation of money.",
      "The discussions feature conflicting opinions, underlining the complexities and differing views surrounding these subjects."
    ],
    "points": 326,
    "commentCount": 509,
    "retryCount": 0,
    "time": 1695077571
  },
  {
    "id": 37555028,
    "title": "Memory-efficient enum arrays in Zig",
    "originLink": "https://alic.dev/blog/dense-enums",
    "originBody": "Blog Repos Linkedin When Zig Outshines Rust - Memory Efficient Enum Arrays Sep 2023 - RSS Enums (or tagged unions) whose variants vary in size are prone to significant memory fragmentation in Rust. That's because we need to allocate enough data to accommodate the largest variant. Figure 1: Consider the following enum: pub enum Foo { A(u8), B(u16), C(u32), D(u64), } Because of the space needed for tagging and alignment, this type is 16 bytes long. This presents real pain when collecting a large number of them into a Vec or HashMap. The padding can be dealt with using some form of struct of arrays (SoA) transformation that stores the tag in a separate allocation. However, reducing the variant fragmentation is not so trivial. You could hand-roll specialized data structures for a particular enum that reduce fragmentation to a minimum; but doing this generically for an arbitrary enum with maximum memory efficiency is close to impossible in Rust. The only options we have are proc-macros, which compose poorly (no #[derive] on third-party code or type aliases) and are not type aware (unless using workarounds based on generic_const_expr, which infect the call graph with verbose trait bounds and don't work with generic type parameters). Zig on the other hand let's us perform the wildest data structure transformations in a generic and concise way. Before I go into the implementation details, I'd like to explain why reducing the aforementioned memory fragmentation is useful in practice. Background To me, one of the biggest motivators for efficient enum arrays has been compilers. One problem that keeps coming up when designing an AST is figuring out how to reduce its memory footprint. Big ASTs can incur a hefty performance penalty during compilation, because memory bandwidth and latency are a frequent bottleneck in compiler frontends. Chandler Carruth's video on the Carbon compiler has been making the rounds on language forums. In it he describes how a parsed clang AST regularly consumes 50x more memory than the original source code! Alright, so what does this have to do with enums? Well, the most common way of representing syntax tree nodes is via some kind of recursive (or recursive-like) data structure. Let's define a node for expressions in Rust, using newtype indices for indirection: enum Expr { Unit, Number, Binary(Operation, ExprId, ExprId), Ident(Symbol), Eval(ExprId, ExprSlice), BlockExpression(ExprId, StatementSlice) } Note: We can write an AST node in OCaml for comparison: type expr =UnitNumberBinary of op * expr * exprIdent of symbolEval of expr * stmt list A big difference compared to Rust is that we can express truly recursive data types without any form of explicit indirection. That's because the runtime system and garbage collector take care of the memory bookkeeping for us. The problem we have now is that we want to improve the packing efficiency of those enums. A simple Vec(Expr) will consume sizeof(Enum) amount of memory for every element, which corresponds to the size of the largest variant + tag + padding. Luckily, there are some ways of dealing with this. Reducing Fragmentation Let's take a simple example of a 3-variant enum with member sizes 8, 16 and 32 bits. Storing those in a regular Vec will look like this: Figure 2: Here every element reserves a large amount of space to accommodate the 32-bit variant and to satisfy its alignment. The most common way to improve packing efficiency is by just keeping the enum variants as small as possible using tagged indices (*). (*): For examples in Rust, take a look at the tagged_index crate used in the compiler or check out this recent blog post on small-string optimization. You'll find these optimizations all the time in high-performance code like language runtimes, garbage collectors, compilers, game engines or OS kernels. Unfortunately, that doesn't completely solve the fragmentation issue. The other way is to tackle the container type directly! We could use a struct-of-arrays approach to store discriminant and value in two separate allocations. In fact, that's what the self-hosted Zig compiler actually does. Figure 3: The tags and union values are stored in two separate allocations, so we're not paying for padding anymore. However, the union collection still has variant fragmentation. Because of Zig's staged compilation, we can have container types that perform this SoA transformation generically for any type. In Rust, we're constrained to proc-macros like soa_derive which has several downsides (e.g. we can't place #[derive] on third-party types without changing their source). Reducing Variant Fragmentation This SoA transformation reduces a lot of wasted padding introduced by the enum tag, but still isn't optimal. To really get rid of fragmentation in the values, we can create one vector per variant. Figure 4: Compared to the SoA layout from before, we have a partial order instead of a total order. So upon insertion, we get back a tagged index that holds both the enum tag and the index in the particular variant array. I don't think there's a name for this collection, so I call it array of variant arrays (or AoVA). This can be implemented in Rust and Zig, using proc-macros and comptime respectively. Size Equivalence Classes We could stop here, but let's consider enums that have lots of variants that can be grouped into a small number of clusters with the same type size: enum Foo { A(u8, u8), B(u16), C(u16), D([u8; 2]), E(u32), F(u16, u16), G(u32), H(u32), I([u8; 4]), J(u32, u32), K(u32, (u16, u16)), L(u64), M(u64), N(u32, u16, u16), O([u8; 8]) } As you can see, the one-vec-per-variant approach would add 15 vectors. It's likely that the number of (re)allocations and system calls would increase substantially, and require a lot of memory to amortize compared to the naive Vec. The vectors may also be arbitrarily spread in memory, leading to a higher chance of cache conflicts. The AoVA collection itself also consumes a lot of memory, bloating any structure it's embedded in. Now, if we group every variant by size, we get three clusters: 2, 4, and 8 bytes. Such clusters can be allocated together into the same vector - thereby reducing the number of total vectors we have in our container by 80%. So we could realistically store variants of Foo in three clusters: struct FooVec { c_2: Vec, // A - D c_4: Vec, // E - I c_8: Vec, // J - O } You could say this is a dense version of our AoVA pattern. However, once we colocate different variants in the same allocation, we lose the ability to iterate through the vector in a type-safe way. The only way to access elements in such a container is via the tagged pointer that was created upon insertion. If your access pattern does not require blind iteration (which can be the case for flattened, index-based tree structures), this might be a worthwhile trade-off. I've implemented a prototype of this data structure in Zig. The most important pieces are the compiler built-ins that allow reflection on field types, byte and bit sizes, as well as inspecting the discriminant. Snippet: At its core, it performs straightforward compile-time reflection to compute the clusters and field-to-cluster mappings. We do pseudo-dynamic allocation using a stack-allocated vector. The cluster information is used to construct the AoVA data structure. Exact source of the snippet is here. // determine kind of type (i.e. struct, union, etc.) switch (@typeInfo(inner)) { .Union => |u| { // store mapping from union field -> cluster index var field_map = [_]u8{0} ** u.fields.len; // iterate over union fields for (u.fields, 0..) |field, idx| { // compute size const space = @max(field.alignment, @sizeOf(field.type)); // insert into hashtable if (!svec.contains_slow(space)) { svec.push(space) catch @compileError(ERR_01); } field_map[idx] = svec.len - 1; } // return clusters return .{ .field_map = field_map, .sizes = svec }; }, else => @compileError(\"only unions allowed\"), } If you do want type-safe iteration, you could pay the cost of padding, and add the tag back in: Figure 5: We've essentially partitioned the enum on the data-level, leaving the interpretation at the type-level untouched If the padding is too much, you can do an SoA transformation on each of the variant arrays. Figure 6: Here we have a similar partitioning, but without the padding. The downside is that we're doubling the vector count. So as you can see, there's quite a few trade-offs we can make in this space - and they all depend on the concrete memory layout of our enum. While creating such data structures is pretty straightforward in Zig, creating any of these examples in Rust using proc macros is basically impossible - the reason being that proc macros don't have access to type information like size or alignment. While you could have a proc macro generate a const fn that computes the clusters for a particular enum, this function cannot be used to specify the length of an array for a generic type. Another limit to Rust's generics is that the implementation of a generic container cannot be conditioned on whether the given type is an enum or a struct. In Zig, we can effectively do something like this: // this is pseudocode struct EfficientContainer { if(T.isEnum()) { x: EfficientEnumArray, } else { x: EfficientStructArray, } } We can also specialize the flavor of our AoVA implementation based on the enum. Maybe the benefits of colocating different variants only starts to make sense if we reduce the number of vectors by more than 90%. So ultimately we gain a lot of fine-grained control over data structure selection. And if we have good heuristics, we can let the type-aware staging mechanism select the best implementation for us. To me, this represents a huge step in composability for high-performance systems software. Bonus: Determining Index Bitwidth at Compile Time While implementing my prototype, I noticed other ways of saving memory. For instance, if you know the maximum capacity of your data structure at compile time, you can pass that information to the type-constructing function and let it determine the bitwidth of the returned tagged index. When this tagged index is included in a subsequent data structure, let's say another enum, this information carries over naturally, and the bits that we didn't need can be used for the discriminant! So what Zig gives you is composable memory efficiency. By being specific about the number of bits you need, different parts of the code can take advantage of that. And with implicit widening integer coercion, dealing with APIs of different bitwidths stays ergonomic. In a way, this reminds me a lot of refinement typing and ranged integers, so this ties in a lot with my post on custom bitwidth integers. Conclusion Writing extremely efficient generic data structures in Rust is not always easy - in some cases they incur lots of accidental complexity, in some others they're essentially impossible to implement. I think one of the biggest takeaways for me with regards to staged compilation was the ability to be composable on a memory layout level. If you're developing a systems programming language that embraces efficiency and zero-cost abstractions, you should absolutely take another look at staged programming and in particular Zig's comptime.",
    "commentLink": "https://news.ycombinator.com/item?id=37555028",
    "commentBody": "Memory-efficient enum arrays in ZigHacker NewspastloginMemory-efficient enum arrays in Zig (alic.dev) 314 points by dist1ll 22 hours ago| hidepastfavorite230 comments ForkMeOnTinder 21 hours agoThere&#x27;s another strategy that has good storage efficiency and keeps the ability to iterate over elements. vec #1 is a list of tags, vec #2 keeps the byte offset of each element, and vec #3 (not really a vec) is the packed variant data pointed to by vec #2. This:- is half the number of vecs of the author&#x27;s final solution (6 vs 3)- wastes no bytes for padding (unless needed for alignment)- lets you iterate in a cache-friendly way since the data is laid out in order in memory regardless of type- even lets you access items by index in O(1)Generally it has the same performance characteristics as a Vec, but for heterogeneous data. reply pornel 17 hours agoparentBut if you need to mutate such collection, you’ll end up writing a memory allocator (to handle removals, changes to larger variant, and to deal with fragmentation). reply dist1ll 21 hours agoparentprevStoring byte offset inline, great idea. Thanks for mentioning that.Just note one disadvantage: if the byte offset is stored in memory, you have a data dependence on the traversal. So even though it&#x27;s cache-friendly, it may cause serious memory stalls in the processor&#x27;s pipeline. reply dan-robertson 11 hours agorootparentOne trick for this is to leverage the branch predictor to guess where the next item will be (I first saw this trick for linked list traversal as in practice linked lists are often laid out linearly). Something like &#x2F;&#x2F; or guess based on the size of the current item let guessed_next_offset = current_offset + (current_offset - previous_offset); let actual_next_offset = byte_offsets[++i]; previous_offset = current_offset; current_offset = guessed_next_offset; if(current_offset != actual_next_offset) &#x2F;&#x2F; need to make sure the compiler doesn’t ‘optimise out’ this if and always run the below line current_offset = actual_next_offset; &#x2F;&#x2F; ...In the ordinary case, where the guessed offset is correct, the cpu predicts the branch is not taken, and no data dependency of ... on actual_next_offset is introduced. If the branch is mispredicted then that speculatively executed work with the wrong current_offset is dropped. This is a bit slow but in the case where this happens all the time, the branch predictor just gives you the slightly bad perf of the traversal with the data dependency (computing the guess will be negligible) except you pay if the guess was actually right. reply jadbox 17 hours agorootparentprevI&#x27;d love for someone to benchmark these variants to tease out their actual characteristics (particularly for x86). reply loeg 19 hours agoparentprevOffsets can add a significant amount of space relative to small T, if their size is not optimized (e.g., 64-bit size_t and uint8_t T). So as long as we are careful about offset size this seems like a reasonable approach. reply dan-robertson 11 hours agorootparentIndeed part of this is pointed out in the video about Carbon linked in the OP: juxtaposition gives you a ‘free’ pointer going from each node of your data structure and for some tasks you don’t really need more than that. reply fweimer 20 hours agoprevHow does this AoVA data structure work in practice? Don&#x27;t you lose index-based access, in the sense that arithmetic on indices is potentially no longer meaningful in the array sense? Iteration will not preserve insertion order.I think TLV (tag-length-value; length can be implied by the tag) is more commonly used in this context due to its better caching properties, and it least it provides meaningful forward iteration. See getdents&#x2F;inotify&#x2F;Netlink messaging. reply skybrian 17 hours agoparent> upon insertion, we get back a tagged index that holds both the enum tag and the index in the particular variant array.These are essentially pointers. If you want to iterate, you store the pointers in an array in the order you want to use. It’s the same thing a program would do if it allocated memory from a heap.Storing things based on their size is also done by garbage collectors and general-purpose allocators. They might get some efficiency from knowing all possible object sizes, though. Also, like an arena, they could gain some efficiency from having a simpler way of deallocating. reply LegionMammal978 20 hours agoparentprev> How does this AoVA data structure work in practice? Don&#x27;t you lose index-based access, in the sense that arithmetic on indices is potentially no longer meaningful in the array sense? Iteration will not preserve insertion order.The caption to figure 4 suggests that the AoVA pattern is not a good fit if you need to retain the full order of inserted elements:> Compared to the SoA layout from before, we have a partial order instead of a total order. So upon insertion, we get back a tagged index that holds both the enum tag and the index in the particular variant array.So by my reading, ordered access is considered out of scope here. (I suppose you could recover ordered iteration by storing a global index in each element, but that still wouldn&#x27;t help with ordered random access, and it would likely lead to some really branchy code.) reply dylukes 15 hours agoparentprevAoVA lacking its own total ordering on indices might be a problem for some use cases, but not for the one that is suggested here: AST nodes.In these cases, the arrays can be just one component (could think of it as an arena) of a heap-ish structure. [1]The cost is that your indices now need to be two dimensional (tag_idx, va_for_tag_idx). But the number of tags is known at compile time and you can optimize storage by packing so that tag_idx is the upper 4-5 bits and va_for_tag_idx uses the rest.See: [1] https:&#x2F;&#x2F;www.cs.cornell.edu&#x2F;~asampson&#x2F;blog&#x2F;flattening.html reply catgary 19 hours agoparentprevYeah I imagine an array write that changes the type of an index would be insanely expensive… reply Aransentin 17 hours agorootparentIn practice, changing tag type is rarely done.In any case it wouldn&#x27;t be particularly expensive; you&#x27;d have to make a new member of the destination type which is as expensive as an append. The \"hole\" that remains in the original vector can be filled in constant time by taking the last member of it and moving it there, then shrinking its size by 1. reply MobiusHorizons 17 hours agorootparentI&#x27;m sure there are classes of problem where that&#x27;s not unusual. Even in the stated example of an AST, there are use-cases for code-fixers (eg lint fixers) that operate on the AST. reply dan-robertson 11 hours agorootparentDon’t those things usually work on concrete syntax trees? And shouldn’t the solution be to use the right data structure for the job (using language feature to make this easy) rather than pushing one system (e.g. the compiler) to be worse for the sake of another tool (the linter)? replyemmanueloga_ 4 hours agoprevThis problem space feels like some variation of a packing problem [1]...It would be nice to start with the end result (the human friendly structure) and generate data structure recommendations to reduce memory waste, respect alignment rules, and improve spatial locality of reference.--1: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Packing_problems reply wyldfire 21 hours agoprev> In it he describes how a parsed clang AST regularly consumes 50x more memory than the original source code!I guess that seems like a lot but the context I&#x27;m missing is - how good could it get? I mean, if you want to preserve source locations of each token and you need to encode enough information in the AST to properly recover, what&#x27;s the ideal growth factor over the original? 1.5x? 15x? reply cryptonector 18 hours agoparentIf you can achieve a memory savings of 30%, say, that would be pretty big news. If you can only achieve a 30% memory savings by doing things to the compiler that make it harder to work on the compiler in the future, then it&#x27;s probably not worthwhile. But if you can achieve an 80% memory savings by doing violence to the compiler, that might be worth the trouble.As for what&#x27;s an ideal source->AST expansion factor for a language that has a user-friendly compiler that&#x27;s also compiler dev-friendly, that&#x27;s hard to say. Clearly 50x is workable.In TFA the 50x expansion factor is used as a motivator for automating a particular type of optimization. It&#x27;d be very interesting to see a Rust vec-of-enums that automatically deconstructs enum values into tags and opaque values that it could store in a struct-of-arrays like TFA does in Zig. The places to bury unsafe use into for this would not be many. reply fnord123 17 hours agorootparentMaybe I&#x27;m just dumb but isn&#x27;t the endgame of vec of enums going to be an ECS? reply setr 11 hours agorootparentAnd the end game of ecs is a columnar rdbms reply anonymoushn 21 hours agoparentprevAs a point of comparison, I think simdjson tape is only ~3x larger than the original document. Much of this could be saved if one did things like stuffing numbers into only one tape slot or not copying strings that have no escape sequences in them (instead referencing their location in the original document).The largest overhead you could achieve is apparently 8x for a document that is mostly the characters \"[]\" or also 8x for a document that is mostly the characters \"0,\" reply halayli 21 hours agorootparentThere&#x27;s a ton more context that needs to be held in a clang AST node compared to a JSON doc. Just to mention a few: file source, token range, expr&#x2F;stmt&#x2F;decl statements and their relation, macro expansion related info, and tons of diag to be able to walk back to the original source token, and padding added by the compiler in those data structures. reply AndyKelley 10 hours agoparentprevSource code is surprisingly compact.As for one data point on how much better it can be, here is Zig&#x27;s own parser, parsing Zig&#x27;s own parser: # Source bytes: 139 KiB # Tokens: 24646 (120 KiB) # AST Nodes: 10998 (140 KiB)Each token is 5 bytes which is pretty minimal (1 byte tag + 4 byte file offset). AST Nodes are also encoded compactly and non-uniformly, in this case coming out to roughly 13 bytes each. Despite these minimal encodings, the parse tree comes out to almost 2x the size of the source file in this case. 2x is a lot better than 50x though.Source: zig ast-check -t lib&#x2F;std&#x2F;zig&#x2F;Parse.zighead -n7 reply wyldfire 9 hours agorootparent> 2x is a lot better than 50x though.the 50x metric was using something like this AST analysis mechanism (and not a system metric like resident set size)? reply dan-robertson 11 hours agoparentprevI think you should just watch the linked talk. It’s excellent. Though they don’t put exact numbers on it iirc (perhaps too early to be confident – they might be small due to missing data they don’t yet realise they’ll need). reply eximius 15 hours agoprevI do with proc macros could evolve to be able to query the compiler for information (which would necessitate careful thought to adding stages to compiling). I&#x27;ve wanted to know \"does this struct impl this trait\" or \"give me a list of all concretely implemented traits\" or various things like this that would be very, very useful in a proc macro. reply Arnavion 15 hours agoparentIIRC the compiler runs plugins in two stages. The first stage gets the AST before any typechecking and it can make modifications to the AST; macros and some clippy lints run in this stage. The second stage runs after typechecking so it gets type information, but it cannot make any modifications; other clippy lints run in this stage. reply airstrike 17 hours agoprevI only partially understand this article but this seems incredibly relevant to me since I&#x27;m planning on writing a spreadsheet engine in rust. Cell values require me to have something like (this very poorly written POC code, sorry): pub enum Expr { Number(i32), String(String), Reference(Position), Binary(Box, char, Box), Function(String, Vec), }I&#x27;ll keep reading and studying! If anyone has more resources to throw at me, I&#x27;m all ears reply vlovich123 17 hours agoparentThe problem being highlighted is that if you have widely disparate sizes of the variants and&#x2F;or a lot of these in an array, your hurting performance because there’s a lot of space being wasted on padding.A common technique that came from gaming is that if you have an array of structs (AoS) (eg array to break it up into a struct of arrays (SoA): struct Humans { healths: Vec, ammo: Vec, … }. Then the ith index into those vecs is the ith Human in the AoS layout (parallel vecs here are for example, but it’s not optimally efficient because the bookkeeping of length and capacity on a per field basis is wasteful as that info is duplicate).This is basically a similar idea except for enums and doing it automatically in a way that Rust can’t. It’s possible the problem is a bit overrated in terms of how big of a problem this is. Also, in Rust you may be able to wrap the third party type in your own and then the derive macro approach probably can work.Re your spreadsheet, just keep in mind of this as a possible optimization and you have to first figure out if you’re building for speed (ie you’ve done this a few times and you know this is probably a bottleneck) or experience (ie build for simplicity and understanding) reply rudedogg 17 hours agorootparentOn the subject of AoS, Zig has a cool data structure for that: https:&#x2F;&#x2F;zig.news&#x2F;kristoff&#x2F;struct-of-arrays-soa-in-zig-easy-i... reply airstrike 16 hours agorootparentprevThank you -- definitely building it for speed which is why this felt topical. I really appreciate your explanation, it&#x27;s super clear. reply hathawsh 17 hours agoparentprevSounds like a fun project. If it&#x27;s intended for ordinary users, one thing you should expect them to do is add content to the 4 extreme corners of a sheet and see if the spreadsheet engine falls over as a result. If you allow 1 million by 1 million cells, and you store a \"null\" for every cell not filled in, then the engine will run out of memory. That means you might consider storing the cell content in a sparse manner. One way to do that is with a hash map implementation such as \"hashbrown\". Just a thought.(To clarify why I&#x27;m bringing this up, the points in this article are low level details that you probably don&#x27;t need to think about if you start with a hash map and thereby avoid running into memory constraints initially.) reply airstrike 16 hours agorootparentThanks, great point indeed. I am looking into this https:&#x2F;&#x2F;github.com&#x2F;rust-lang&#x2F;hashbrownThe way I think about it -- rather naively, I suppose -- is that I care more about the references cells make to each other than the actual grid of cells displayed on a table. The latter feels more like a \"view\" of the data than an actual data structure?This also seems to align with the relative priority of (sorted from highest to lowest): figuring out the order of evaluation, calculating those evaluations, and finally displaying the results of the evaluation reply nhatcher 17 hours agoparentprevHey, I&#x27;ve actually build a spreadsheet engine in Rust. Not open sourced, but could give you some pointers. You are going to hit lots of performance issues before you can have some gains using the methods of the article. The single most difficult problem is the evaluation strategy. reply airstrike 16 hours agorootparentHey, thanks for the offer. I have been thinking about evaluation and the extent of my strategy so far has been to consider it as a dependency graph, which led me to this interesting article: https:&#x2F;&#x2F;hyperformula.handsontable.com&#x2F;guide&#x2F;dependency-graph...Also it sounds like the right next step for me is to read more on incremental computation. Someone on reddit just pointed me to adapton: https:&#x2F;&#x2F;docs.rs&#x2F;adapton&#x2F;latest&#x2F;adapton&#x2F;Actually implementing anything with Rust is on my TODO list (I&#x27;m spending some time on the UI prototype right now)I will give a couple things a try and then, if you don&#x27;t mind, I can ping you via email. I was checking out your website and loved the read on the parser -- not to mention A MAZE (played all the way to level 4!) reply nhatcher 11 hours agorootparentWow! Glad you had a look around. Yes, please send me an email whenever you feel like.And yes, you are going to need some sort of \"support graph\" and topological sorting. But this is way more complicated than it looks like. Most functions in a spreadsheet don&#x27;t know it&#x27;s dependencies until you are actually computing them, like `LOOKUP` or `IF`. Some functions like `RANDBETWEEN` or `NOW` change their value every time. In some modern spreadsheets some functions like `SORT` or `UNIQUE` spill their values to some other cells. Calculating dependencies correctly in all those situations can be challenging.There is not a lot of literature on spreadsheet implementation and reading code of existing open source implementations is hard. And exception is Peter Sesoft:https:&#x2F;&#x2F;mitpress.mit.edu&#x2F;9780262526647&#x2F;spreadsheet-implement... and https:&#x2F;&#x2F;www.itu.dk&#x2F;people&#x2F;sestoft&#x2F;funcalc&#x2F;My recommendation would be to first set the scope. Then code manually the lexer and parser and drawn those two in tests. Excel formulas can be quite complicated! There are lots of fancy things you can do there like use arena allocators and other optimizations. I would try to avoid that on a first go.Here is something that I have found extremely convenient. Excel saves the cell formula AND the resulting value. Meaning that to create a test case you just need to create an Excel workbook (and a xlsx parser, of course).Doing the UI for a spreadsheet is an equally challenging problem.Good luck!!!!Oh, and remember this thread from a couple of days ago. Might be inspiring: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37527720 reply dan-robertson 11 hours agorootparentprev(just jumping in here) You may want to take a look at https:&#x2F;&#x2F;www.scattered-thoughts.net&#x2F;writing&#x2F;an-opinionated-ma... which describes things on the spreadsheety and not-very-spreadsheety ends of the spectrum (moreso the latter). If you follow some of the links there you should find some good descriptions of various implementation strategies, e.g. Naiad style timely dataflow or incremental-style pseudotime. reply airstrike 9 hours agorootparentthank you so very much! this is invaluable replycmrdporcupine 17 hours agoprevIn general I&#x27;m maybe somewhat disappointed that pattern matching in Rust can&#x27;t be expressed more as a type system trait-type thing (hand waving here) that any struct could conform to, rather than an explicit first class hardcoded object type with its own storage structuring, etc?I may not be expressing this cogently, but. I&#x27;ve implemented both an AST (as in this article) and an opcode&#x2F;bytecode interpreter recently, and I feel that in large part Rust&#x27;s enums are not ideal for either:I never had the memory usage concerns about the AST that this author did, but I wanted to do things like: add a line number &#x2F; column attribute to every statement node. I had a `Stmt` enum, and I had a choice: put line # and column on every enum possibility (boilerplate and ugly), or put the enum in a new Stmt struct that contains both the original enum and the line&#x2F;col attributes -- which involved a bunch of refactoring and also didn&#x27;t feel elegant. I feel there must be a more elegant type system way that my \"Stmt\" struct could have been reworked, that Rust isn&#x27;t offering me.And re: the opcode stuff,I am fairly sure that a pattern-matched Rust enum is not an ideal encoding for a VM opcode interpreter performance wise. But the language really does want to point you in this direction, and the facilities for de-structuring patterns are very seductive. I haven&#x27;t gotten to yak shaving this yet, but again I feel like there&#x27;s likely some potential for improvements in the type system that could open things up so one could get pattern matching facilities while using one&#x27;s own preferred lower level impl.I dunno. Thoughts. reply aw1621107 17 hours agoparent> pattern matching in Rust can&#x27;t be expressed more as a type system trait-type thing (hand waving here) that any struct could conform to, rather than an explicit first class hardcoded object type with its own storage structuring, etc?Do you think you can provide a more concrete example? First thing that comes to mind for me is some kind of structural typing [0], but I&#x27;m not confident I&#x27;m interpreting your comment correctly.[0]: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Structural_type_system reply cmrdporcupine 16 hours agorootparentOther people who are better with these things could probably conceptualize it better than me.What I&#x27;d like to be able to do is declare a trait that says \"this thing can yield the following patterns, can destructure in this pattern, and these are the functions you&#x27;d call to extract references to those values\"And then be able to use match on it as usual.Putting it another way, I wish an algebraic data type \"enum\" was something any struct could \"be\", with the type system providing the means to express that that&#x27;s the case, and the programmer being able to provide the \"how\" and with existing Rust enums merely being the default way of doing this. reply dan-robertson 11 hours agorootparentHaskell and F# have this feature. In F# it is called active patterns. In Haskell the feature is pattern synonyms (maybe combined with view patterns) and is used to e.g. pattern match on the prefix or suffix of a sequence-like structure, or to match a tree as a an element and left&#x2F;right node without the caller seeing a bunch of other tree metadata. reply aw1621107 15 hours agorootparentprevInteresting idea. I feel a bit conflicted about it, but it&#x27;s probably something I&#x27;d have to think about.I suppose it&#x27;s possible to somewhat approximate this in existing Rust by creating a trait with a function that returns a destructure-able type, performing the transformations that you would have implemented in your hypothetical trait, then pattern match over that. Not sure if that would be \"good enough\". reply dan-robertson 11 hours agoparentprevAn old trick for bytecode interpreters is to use indirect jumps for going to the next opcode’s implementation (gcc had a ‘computed goto’ extension that was used for this. For Rust I guess you would want function pointers plus something to force TCE?) and putting those indirect jumps at the prelude of every opcode implementation. This meant that the indirect-jump-predictor (which cpus have because of oop) would have separate models for the ends of different opcodes and so would be more likely to predict correctly (maybe you have a hard time predicting a the next instruction but it is much more likely that a test is followed by a branch).I think other tricks (e.g. storing top of stack in registers for a stack machine) matter more though, and I don’t know if the trick described above is still relevant. reply dist1ll 44 minutes agorootparentJust to add some jargon for folks who want to do more research into this:The technique of having the interpreter dispatch duplicated in the postlude of each handler function is called \"threading\". If your dispatch uses an indirect jump, it&#x27;s called \"indirect threading\". The indirect-jump-predictor is known as branch target predictor (BTB). reply AndyKelley 10 hours agorootparentprevFor Zig this is planned via labeled continue syntax inside switch expressions: https:&#x2F;&#x2F;github.com&#x2F;ziglang&#x2F;zig&#x2F;issues&#x2F;8220As mentioned in the proposal, this would even allow implementation of Duff&#x27;s Device in Zig :-o reply cmrdporcupine 8 hours agorootparentprevYeah I&#x27;ve spent time on this topic before when working in C++, but haven&#x27;t invested much energy in thinking about how one would do it efficiently in Rust yet. Luckily performance is not my #1 concern, at least not yet. And maybe if I got down that road more I&#x27;d just invest in going JIT with cranelift or similar. reply patrec 13 hours agoparentprevSeveral languages have what I think you are asking for. Have a look at Scala&#x27;s extractors or F#&#x27;s active views. reply cmrdporcupine 9 hours agorootparentAh yes, many moons ago (like... almost 15 years) I worked in Scala and I recall extractors, or at least the discussion of them. reply foderking 12 hours agorootparentprevyou mean f# active patterns? reply BaculumMeumEst 21 hours agoprevI was checking out gameboy emulators recently written in C++, Rust, and Zig that have seen recent development.I couldn&#x27;t get the Zig projects to build, which is understandable since the language seems to have had a lot of breaking changes in its build system and the language in general (which is why it comes with a caveat not to use it in production yet).But given the sheer number of comments I&#x27;ve read over the years recommending Rust as a replacement for C++, I was more surprised to see that I couldn&#x27;t really find a single Rust project that had correct timings (the frame rates were incorrect and audio distorted) and worked with most ROMs I tried.Meanwhile, the C++ projects I found built without issue or hassle, had correct timings, ran all the games I&#x27;ve tried, had lots of features, etc.What gives? Is emulation just not a good use case for Rust? reply mathstuf 21 hours agoparentTimings and audio handling sound like logic things and not something Rust&#x27;s advantages over C++ would inherently support better.If I had to hazard a guess, it&#x27;s that the C++ projects are just more mature and have seen more \"in-the-wild combat\" to get to the state they&#x27;re in now. Other factors that may contribute (in no particular order): - existing emulators are Good Enough and polishing a new one is less \"urgent\" than other problems in the current zeitgeist; - hobby project vs. no-longer-a-hobby project status; and - emulator projects in C++ are of similar quality are just as prevalent, but are significantly older and instead of being discoverable on GitHub are abandoned on Sourceforge or personal websites. reply TonyTrapp 21 hours agorootparentThat would be my guess as well. Emulation, interpretation of decades-old binary formats, and similar problems are hard to get right on the first try, they often need years of testing and breaking until they are right. A project that has been around for 20 years will always have an advantage in this space over a newcomer project. reply loup-vaillant 17 hours agorootparentprevFunny how being on GitHub makes you discoverable, while maintaining your own domain name does not…Actually no, that’s not funny at all. reply simias 20 hours agoparentprevMost people developing GameBoy emulators these days do it as a toy project, not a serious effort to create an ultra-accurate general purpose emulator. It&#x27;s like writing a raytracer or a Pong clone.The best GameBoy emulators like Gambatte predate Rust by almost a decade and are often written in C++. Since GameBoy emulation is pretty much a solved problem there&#x27;s no strong motivation to innovate in the scene.I&#x27;ve written several emulators in Rust, I&#x27;d argue that it&#x27;s very well suited to this exercise, but it&#x27;s also not an area where Rust shines particularly. There&#x27;s no complex memory management in emulators usually since most memory buffers are just statically allocated constant-size arrays that mimic the physical RAM on the console. reply wiz21c 21 hours agoparentprevI&#x27;m currently writing an emulator in Rust (for Apple 2). For some aspects I&#x27;m better than my C++ counterparts.... BUT those counterparts have years and years of little refinements that make a huge difference on the fact that some games run or don&#x27;t. There&#x27;s no question about that.Besides Rust is just fine. I use a bit of threads, a bit of OpenGl and none of these is a problem. I&#x27;d venture to say that the rust compiler is real good and that allows me to code things without worrying about optimisation. Older emulator have started years ago and they often had to optimize by hand which leads to not so readable code; just your usual code legacy story.Finally, the cargo tool is a modern way to build the code and I&#x27;ve been able to port my code to Linux, windows, MacOs and raspberry pi without a single code modification (besides expected stuff like audio frequency, file system, etc being different).The Rust crowds are just too new to have produced good emulators. Give us time :-) reply raphlinus 19 hours agorootparentI&#x27;m also noodling on an emulator, shooting for cycle accuracy (to the point where you&#x27;d be able to run vapor lock demos). Mine passes the 6502 functional tests but not yet the Tom Harte suite. To add to the difficulty, I want to run it on an RP2040 and bit-bang the video. Is yours up in an open source repo? Mine isn&#x27;t (yet) but I&#x27;m open to joining forces.I started mine about 6 years ago, put it on the shelf, and am getting back to it. Overall I think Rust is working well for it (and for pico DVI out), but I can also see the advantages of Zig for something this low-level and specialized. reply wiz21c 14 hours agorootparent133Mhz, that&#x27;s 133 cycles per instruction for a 6502 (assuming you emulate only the 6502). Seems doable but then there&#x27;s the BCD stuff and that&#x27;s rather tough emulate (you&#x27;ll definitely need a lot of cycles to reproduce that)...But that sounds real cool. Would you share you project ? reply raphlinus 13 hours agorootparentIt&#x27;s a bit tricky, but my decimal adc is 19 lines of code, all of which is fairly straightforward, mostly bit fiddling stuff that I expect will compile to a few instructions each.I need to go through an open source releasing process but I&#x27;ll get that going. Thanks for your interest! reply insanitybit 21 hours agoparentprevEmulation seems fine for Rust. I don&#x27;t think there&#x27;s a good answer to this anecdote. One option is that when I learned C++ I built an emulator to learn how to write an emulator, whereas people writing emulators in Rust are perhaps often trying to learn Rust. Maybe that leads to a difference.It&#x27;s basically impossible to say though, we&#x27;d need to do a full survey of the C++ and Rust emulator ecosystem and look at which bugs are most prevalent, control for authors experience and skill, etc...edit: I&#x27;ll also note that when I built an emulator there were guides for it in C++. I don&#x27;t know what materials exist in Rust but building an emulator is actually a project that I would consider C++ for (if for learning purposes) just because it&#x27;s so well documented how to build one using the language. reply olafura 21 hours agoparentprevSo the question is why a project created in a language that has been around since before the devices being emulated are more mature than ones in a language that is 8 years old?Sorry to be so snarky. It probably depends on people&#x27;s time working on it and their familiarity with async processes. This is an inherent problem with any media like that. With sound, it&#x27;s primarily that it can&#x27;t come before the image but can lag behind the image. Stuff like this.I don&#x27;t think any programming language is going help with that because it&#x27;s how we perceive things, not just how correct your code is. reply InfamousRece 21 hours agoparentprevI don’t think Rust is inherently bad to write emulators. For example one of the best cycle-accurate IBM XT emulators is written in Rust: https:&#x2F;&#x2F;github.com&#x2F;dbalsom&#x2F;martypc reply Shish2k 15 hours agoparentprevAs somebody who has written the same gameboy emulator in C++, Rust, and Zig (as well as C, Go, Nim, PHP, and Python) - I have yet to find a place where language affected emulation correctness.Gameboy audio is kind of a pain in the ass (at least compared to CPU, which is fairly easy, and GPU, which is easy to get \"good enough” if you don’t care about things like palette colours being swapped mid-scanline) - and some languages take more or less code to do the same thing (eg languages which allow one block of memory to be interpreted in several different ways concurrently will make the “interpret audio RAM as a bunch of registers” code much shorter with less copying) - but in my case at least, each one of my implementations actually has the same audio distortions, presumably because I’m misreading some part of the hardware spec :Phttps:&#x2F;&#x2F;github.com&#x2F;shish&#x2F;rosettaboy&#x2F;(Also yes, the zig version is currently failing because every time I look at it the build system has had breaking changes...) reply BaculumMeumEst 14 hours agorootparentOh that&#x27;s so cool! Thanks for sharing. I skimmed a few of these and I&#x27;ll definitely take a closer look later. I enjoyed your observations on each language as well.From a reader standpoint, I enjoy reading emulators written in C the most. It sucks when C is missing features for complex tasks, but emulation seems to fit nicely into what it can do without black magic fuckery. Of course as someone not super experienced with C, writing it feels like an endless sea of \"oh god why is this segfaulting\".Rust OTOH bothers me to look at because there are so many sigils and language doodads for functionality that seems like it should be really straightforward but whatever, I&#x27;m sure that&#x27;s just because I&#x27;ve barely used it and can&#x27;t understand it. I&#x27;ve been learning C++ recently because learning materials are usually tailored to it for stuff I&#x27;m interested in (vulkan currently), while ignoring a constant nagging feeling that I should stpo being curmudgeon and take a closer look at Rust.I&#x27;ve tried Zig but am not looking too hard atm for many of the reasons you mentioned in your repository, I&#x27;m hoping the language will stabilize at some point and that LLMs will help with some of the repetition&#x2F;verbosity. reply 59nadir 2 hours agorootparentOdin would very likely be a great choice for the stuff that you&#x27;re trying to do. It&#x27;s a very straight forward language and has a \"vendor\" (https:&#x2F;&#x2F;pkg.odin-lang.org&#x2F;vendor&#x2F;) part of the libraries shipped with the compiler that has lots of bindings for gamedev-related things, i.e. OpenGL, Vulkan, DirectX, glfw, stb libraries, some audio libraries, etc.On top of that it has swizzling in the language, some automatic array programming features and a standard set of modern niceties like good tagged union support, custom allocators that are standardized, and so on. It&#x27;s a nice language for pretty much any use case when you want good fundamentals and straight forward solutions without much magic but for gamedev I don&#x27;t think there&#x27;s a C&#x2F;C++ alternative that is as well suited as Odin overall.If you want a brief (but enough to get me excited about trying Odin out) overview of the language you can find one here: https:&#x2F;&#x2F;odin-lang.org&#x2F;docs&#x2F;overview&#x2F;P.S. I&#x27;ve written a few posts on HN about what I like about Odin in more detail so I won&#x27;t reiterate those things exactly here, but one of them is recent so you can find it in my comment history. reply BaculumMeumEst 1 hour agorootparentArgh, Odin looks so close to perfect for me. The vendor libraries look fantastic. But I&#x27;m working up to deploying on Android and using OpenXR. There&#x27;s support and&#x2F;or examples of that for C++ and Rust, and I could probably get Zig working either by cross compilation or by exporting to C. But I don&#x27;t see any documentation or examples for cross compilation for Odin, and that use case seems to fall just outside the range of its built-in batteries.Do you think it&#x27;s doable? reply 59nadir 45 minutes agorootparentI wouldn&#x27;t know about Android, really, but I do know that there is some discussion right now about how some consoles are off limits due to not being allowed to ship non-C&#x2F;C++ code to them and this will be addressed by compiling Odin to a subset of C in the future if I remember correctly.I would probably inquire on the Odin discord server about this particular thing as it&#x27;s very likely someone has already stumbled upon it: https:&#x2F;&#x2F;discord.com&#x2F;invite&#x2F;sVBPHEv replyraincole 21 hours agoparentprev> What gives?There are still more good C++ programmers than good Rust programmers. reply tmccrary55 20 hours agorootparentThis.Many people are still learning Rust and an emulator is probably a good way to get your hands dirty.I&#x27;m doing something similar with a compiler project and my code is gradually getting better and more idiomatic.But I&#x27;m still doing a lot of stupid things and getting the hang of it. reply slikrick 20 hours agorootparentit&#x27;s not at all about \"quality of programmer\" and you should never reduce extremely vague arguments to that. it&#x27;s literally about time for quality program to be made an completed.those C++ emulators have often been in development for 3-5x as long as a similar Rust one. reply xmodem 21 hours agoparentprevI think its a perfectly fine use case but we&#x27;re not yet at the point where that community has reached a high enough level of rust adoption that there are a significant number of mature projects, even for something as relatively simple to emulate as a gameboy. reply hu3 20 hours agoparentprevRelated: for all the flak GC gets, C# was used to produce two amazing emulators:1) https:&#x2F;&#x2F;github.com&#x2F;Ryujinx&#x2F;Ryujinx2) https:&#x2F;&#x2F;github.com&#x2F;TASEmulators&#x2F;BizHawk reply WhereIsTheTruth 18 hours agorootparentryujinx doesn&#x27;t use the GC for their sub-systems, they use a mix of manual memory allocations throught their allocators or RCFor such project to succeed, you have to be able to optimize your memory allocation strategyIf you just rely on the GC, you&#x27;ll have bad surprises (why does my game stutter during gameplay?!), they already have multiple due to their double dip on the JIT, nothing prevents the JIT compiler to compile during your gameplay, causing massive stutters, just like how shaders compilation is an issue at runtime reply ghosty141 19 hours agorootparentprevPersonally I think C# is a great language to get stuff done. The big problem it has is the portability (especially when it comes to using it with a GUI). reply guipsp 19 hours agorootparentprevFwiw, ryujinx uses a jit on the hot path, and most (but not all!) bizhawk cores are not actually c#. reply hu3 18 hours agorootparentRyujinx uses Microsoft&#x27;s JIT .NET technology [1] which is best available for C# codebases. Nothing new here since it needs to translate Switch&#x27;s ARM instructions to x86 machine code somehow.[1] https:&#x2F;&#x2F;devblogs.microsoft.com&#x2F;dotnet&#x2F;the-ryujit-transition-... reply agentultra 19 hours agorootparentprevGC often gets more flak than is deserved.Often such flak ignores the differences between throughput and latency.For long, lived processes you’ll end up writing some kind of garbage collection system. reply elcritch 15 hours agoparentprevYou should add Nim to the list! There&#x27;s a few implementations, though I&#x27;m not sure how complete they are. As others point out they&#x27;re often used as ways to learn a language.Fun writeup: https:&#x2F;&#x2F;itwasscience.com&#x2F;blog&#x2F;gameboy Improving RosettaBoy performance: https:&#x2F;&#x2F;forum.nim-lang.org&#x2F;t&#x2F;9859 reply kristoff_it 20 hours agoparentprevwrt Zig, according to some it&#x27;s a great fit for emulators, so maybe that&#x27;s why there are a few :^)https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=jkkUS0nmdsg reply ilyt 20 hours agoparentprev...probably the fact most of the C&#x2F;C++ project you&#x27;d find in emulation exist longer than Rust itself ?I&#x27;d imagine most of them in Rust were written for fun rather than trying to best the established ones. reply dogben 19 hours agoparentprevThe most accurate IBM PC emulator is in rust. https:&#x2F;&#x2F;github.com&#x2F;dbalsom&#x2F;martypc reply rahkiin 18 hours agoparentprevI recently found https:&#x2F;&#x2F;github.com&#x2F;Rodrigodd&#x2F;gameroy to be a very complete implementation reply BaculumMeumEst 18 hours agorootparentI did see that one, but it doesn&#x27;t seem to build on Mac. reply caskstrength 19 hours agoparentprev> What gives? Is emulation just not a good use case for Rust?You fell for Rust PR (which is understandable, considering that both this site and Reddit are full of loud Rust fans&#x2F;influencers). The language provides memory safety and some fancy ML-like syntax and that is it. It won&#x27;t implement good emulator for you or make you a better SW developer. reply jackmott42 21 hours agoparentprevC++ has an ages old tried and tested ecosystem for game stuff which probably makes a lot of rendering and audio tasks easier. It could take Rust years to catch up even supposing the language is inherently better. reply alex_lav 17 hours agoparentprev> But given the sheer number of comments I&#x27;ve read over the years recommending Rust as a replacement for C++,> What gives?This is The Hype Wave, where something is new and exciting so it&#x27;s recommended over the things of the past without regard to whether it&#x27;s as stable&#x2F;full featured&#x2F;correct. People are excited about Rust so Rust has been jammed into just about everywhere to varying degrees of success&#x2F;correctness. This is the same as Javascript&#x2F;Node and Go over the last decade-ish. Zig and Mojo are probably next. reply 0xDEF 21 hours agoparentprevBuilding large-scale C++ projects has become less of an issue because of CMake slowly becoming the default build tool. reply anymouse123456 20 hours agorootparentAn alternative opinion (that I happen to hold) is that the unbelievable pain and frustration of dealing with CMake has been at least one of the forces that have led to the Cambrian explosion of new programming languages and build systems in recent-ish years. reply ghosty141 19 hours agorootparentWhile CMake is pretty annoying the problem from my experience are people using it incorrectly and doing stupid stuff that you can&#x27;t change if you&#x27;re using the project as a library.If every library would be properly \"packaged\" using CMake would be far less annoying or frustrating. Just recently we had the case where a 3rd party library author of a sizeable c++ library set -Werror in his library which wreaked havoc. reply sdfghswe 21 hours agoparentprev> What gives?My guess based on no evidence whatsoever: The Rust community seems to have lots of low effort enthusiasts - people whose primary motivation is \"use Rust\" rather than \"solve a problem\". People whose motivation is to solve a problem (in this case build an emulator so they can play a game or whatever) tend to be a lot more oriented towards the end-result, rather than what tool they&#x27;re using. I&#x27;m pretty confident this isn&#x27;t an issue with the language. reply mkl 20 hours agorootparentI think it&#x27;s more \"learn rust\" than \"use rust\", and nowadays a lot of partly written learning projects are published on GitHub. A lot of the similar \"learn C++\" projects happened earlier and either stayed private or are quite good by now. reply jandrewrogers 20 hours agorootparentprevMost of the developers I know that are learning Rust do not come from a systems language background. They have to learn the domain as well as the language, which is a high bar. Learning to write systems-like code well takes a lot of time and experience. reply bowsamic 15 hours agoparentprevWhich Rust emulators? Which C++ emulators?Perhaps the C++ emulators were serious, long-standing projects, and the Rust emulators were just hobby projects.Most hobbyists only care about getting a game on the screen. reply panick21_ 21 hours agoparentprevAre you serious, a single specific software you like doesnt exist in a specific language and you blame the language for that? You see how that makes no sense right? reply itishappy 20 hours agorootparentThat&#x27;s not the situation. The software does exist and doesn&#x27;t work. \"Is this a because of the language?\" seems a reasonable question. reply pdimitar 20 hours agorootparentIt&#x27;s not a reasonable question at all. My first reaction was: \"maybe the authors did it as a toy project\". If nobody is paying them to do a professional and bit-perfect GameBoy emulator then there should be no expectations that the software is going to be good. reply itishappy 20 hours agorootparentThat&#x27;s an insightful reply, and an important point to consider. There&#x27;s been a number of great answers in this thread, and the general consensus seems to be \"the language is a great fit, but the projects haven&#x27;t had time to mature.\"These seem (to me) like reasonable responses to a reasonable question. reply coldtea 19 hours agoparentprev>Meanwhile, the C++ projects I found built without issue or hassle, had correct timings, ran all the games I&#x27;ve tried, had lots of features, etc. What gives?Aside the fact of the C++ projects being from seasoned C++ developers (which exist 3 decades now in huge quantities), not people starting with Rust as their first \"native\" language and using an emulators as a toy project? C++ exists for 3 decades more on top of Rust&#x27;s entire life of 8 years since v1.0. And given the over time adoption distribution, most Rust devs are like 1-4 years old Rust users max.Or the fact that those C++ are probably much more mature, and with more manyears in them compared to the Rust ones, some even being major community projects? reply pornel 21 hours agoprevRust’s type system ended up being Turing-complete anyway. It seems like it fell into the same trap as C++ templates: it’s an accidental programming language in a programming language, but with way worse syntax.The lesson for language designers may be that every useful type system is doomed be Turing complete, so you may embrace it from the start, instead of trying to make it declarative. reply bhouston 20 hours agoparent> Rust’s type system ended up being Turing-complete anyway. It seems like it fell into the same trap as C++ templates: it’s an accidental programming language in a programming language, but with way worse syntax.Most anything in software can easily end up Turing-complete though. It isn&#x27;t that high of a bar unfortunately. I think most mature type and template systems end up as Turing-complete. To single out Rust for this is to ignore that this is just standard fare for mature typing systems.TypeScript&#x27;s types are Turing complete: https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;TypeScript&#x2F;issues&#x2F;14833Python&#x27;s type hints are Turing complete: https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2208.14755Java generics are Turing complete: https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1605.05274 reply dgb23 19 hours agorootparentThe point is that Zig basically circumvents issues surrounding that, by introducing \"comptime\", where you can just write regular Zig to achieve the same things.The article showcases a nice example of having this very direct power.But it really comes up more often than you think as soon as you actually have it.It&#x27;s easy in Zig to allocate precisely based on computed values and then have the sizes as part of your types etc. It all falls out of some simple ideas and it&#x27;s all just regular Zig code.\"Types in Zig are values of the type type\" from: https:&#x2F;&#x2F;ziglearn.org&#x2F;chapter-1&#x2F;So instead of making it hard to write incorrect programs, Zig makes it easy to write correct programs. reply chrismorgan 19 hours agorootparentI am certainly sometimes envious of comptime and what it makes practical, but it’s worth noting that it results in dynamically-typed generics, whereas Rust goes for statically-typed generics, which is in keeping with its goals. There are some significant general maintainability improvements in statically-typed generics; when you use dynamic generics, subtle changes in one place can cause obscure compile errors in a completely different and seemingly unrelated place (commonly called post-monomorphisation errors); this doesn’t happen with static generics.So… I’m not sold on your wording that it’s circumventing issues, as it’s choosing a different set of trade-offs. In shedding types-are-a-language-of-their-own, you also shed confidence about what’s a breaking change, and make call sites more fragile. Decide for yourself whether it’s worth it. reply anonymoushn 16 hours agorootparentWhat do you mean by dynamically-typed generics and statically typed generics here? I&#x27;ve looked up \"post-monomorphization errors\" and found some things about assertions about generic types failing because of the choices made by the user who passed in types or constants that do not work with the generic code. It seems like Zig libraries have the option of generating the errors at the right place if they place their assertions in the function that returns the type to the user, but they also have the option of generating the errors in the wrong place if they place their assertions in methods of the type.> So… I’m not sold on your wording that it’s circumventing issues, as it’s choosing a different set of trade-offs. In shedding types-are-a-language-of-their-own, you also shed confidence about what’s a breaking change, and make call sites more fragile. Decide for yourself whether it’s worth it.Client code can just look at all the members of all the structs so there&#x27;s not really much hope for enforcing that changes cannot break any client code using compiler-adjacent tooling. reply chrismorgan 6 hours agorootparentIt’s easiest to see the distinction in otherwise-similar systems, so I’ll choose Rust generics (statically-typed) and C++ templates (dynamically-typed).In Rust, the generic constraints (the traits that the type must satisfy) are a contract, part of the signature. The caller must satisfy them, and then the callee knows nothing else about the type it has received. Therefore, changes inside the body of the generic method will never† cause any code that uses the function to stop compiling.In C++, templates don’t have that, so you have to seek knowledge of what conditions your type must satisfy some other way, and it’s easy to accidentally depend on additional details (since it’s not statically checked), so that changes in the template that you thought were harmless actually break someone else’s code somewhere else that uses your template in ways you didn’t expect.https:&#x2F;&#x2F;gist.github.com&#x2F;brendanzab&#x2F;9220415 has a decent example, though it’s from 2014 and refers to Zero and One traits that were removed from the standard library before Rust 1.0, and the compiler messages would be better today as well.—⁂—† In practice there’s at least one way of leaking details, so post-monomorphisation errors that aren’t compiler bugs can actually happen, though it’s very rare: if you return an `impl Trait`, the body leaks whether it implements auto traits like Send. reply dgb23 18 hours agorootparentprevI&#x27;m still in the honeymoon phase with this language and learning, but I agree it&#x27;s a trade off.For example your LSP isn&#x27;t going to help you as much while you edit code.However being able to express arbitrary compile time constraints and pre-compute stuff without having to go through a code generation tool is really powerful. You can actually use all the knowledge you have ahead of time as long as you can express it in Zig.So far it seems like Zig is carving out a very strong niche for itself. reply convolvatron 18 hours agorootparentprevis there anything fundamental about using the same language at compile time to generate fully static typing that is boiled away? I don&#x27;t know, but it doesn&#x27;t seem so? reply ksec 18 hours agorootparentprev>instead of making it hard to write incorrect programs, Zig makes it easy to write correct programs.Or May be rephrasing it ( To avoid the word \"instead\" which may anger Rust supporters );Rust Makes it hard to write incorrect programs, Zig makes it easy to write correct programs.I think this single sentence captures the philosophical difference between Rust and Zig. And of course there is no right or wrong in philosophy. reply dgb23 14 hours agorootparentI fully agree. It’s an interesting trade off that is worth thinking about. reply bhouston 19 hours agorootparentprevHere is a really good link that shows the power of Zig&#x27;s comptime:https:&#x2F;&#x2F;kristoff.it&#x2F;blog&#x2F;what-is-zig-comptime&#x2F; reply littlestymaar 19 hours agorootparentprevThe problem is that it ends up being dynamically-typed, like C++ templates. See: https:&#x2F;&#x2F;nitter.net&#x2F;pcwalton&#x2F;status&#x2F;1369114008045772804> So instead of making it hard to write incorrect programs, Zig makes it easy to write correct programs.Well maybe in theory, but the current state of Zig is that it makes it hard to write programs no matter how correct because the compiler keeps crashing ¯\\_(ツ)_&#x2F;¯ reply wredue 19 hours agorootparentWhat are you talking about? The tagged versions are usually quite stable and also plan bug fix follow ups.If you follow master, you’ll occasionally run in to crashes, which is true of any developing language. If you don’t want that, follow tagged versions. reply 38 19 hours agorootparentprev> Most anything in software can easily end up Turing-complete though.not really, if you actually pay attention when youre designing the type system:https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2005.11710.pdf reply insanitybit 18 hours agorootparent\"if you actually pay attention\" or, in other words, if you produce novel research into the area that&#x27;s worthy of publishing, and noting that this is only just the start and has its own limitations:> The cost is that of requiring a whole program analysis and disallowing programs that would result in infinite instantiations (Section 5.3). Clearly, this is the beginning of the story, not the end. reply bhouston 17 hours agorootparentprevMy experience with Turing-completeness in any custom DSL or similar is that you will generally get Turing-complete systems unless you work really really hard to avoid them. And it only takes one wrong move by anything anywhere in the system, and you&#x27;ll end up with Turing-completeness.Instead of trying to avoid Turing-completeness, just expect it, embrace it and instead deal with its consequences to contain the fallout. reply 38 16 hours agorootparent> Instead of trying to avoid Turing-completeness, just expect it, embrace it and instead deal with its consequences to contain the fallout.this seems like such a cop out. its like saying \"oh failure is a given, so don&#x27;t even try to succeed\". at least currently, Go generics are NOT Turing complete. the generics were designed in part specifically to avoid that. so just because Rust (and others) failed, doesn&#x27;t mean its impossible. reply bhouston 15 hours agorootparent> this seems like such a cop out. its like saying \"oh failure is a given, so don&#x27;t even try to succeed\".It is a pragmatic cop-out yes. I found it is easier to make progress by assuming that you&#x27;ll get to Turing Complete rather than investing in the time to avoid it. I found that the downsides of Turing Completeness are usually overhyped or primarily theoretical.> so just because Rust (and others) failed, doesn&#x27;t mean its impossible.It definitely is not impossible. But I don&#x27;t know if it is worth it.In the end I want fast compile times, type safety, easy to maintain code and good error messages. I do not care if the type system is Turing Complete. reply 38 15 hours agorootparent> In the end I want fast compile times, type safety, easy to maintain code and good error messages. I do not care if the type system is Turing Complete.I think the problem is some languages with Rust go too far with generics, which probably triggers the turing complete. for example, this is valid Rust code: let mut handles = Vec::new(); for i in 0..10 { let handle = do_work(i); handles.push(handle); }but you have to follow the code all the way to \"do_work\" before you ever find the type of anything. Go does not allow this. you need to either declare a concrete type: var handles []intor a explicit generic type: type slice[T any] []T var handles slice[int]I think Rust is lose lose, because the underlying type implementation is Turing complete, and I would argue the code is actually less readable because of the overuse of type inference. reply bhouston 15 hours agorootparent> I think Rust is lose lose, because the underlying type implementation is Turing completeI am not a Rust coder so I can&#x27;t really comment.Currently, I do TypeScript, which also has a Turing complete type system, and I love it. Of course all things in moderation. Even though I could make a completely obtuse type design for projects, I try to not write code that others can not understand. reply insanitybit 14 hours agorootparentprevI don&#x27;t think most people care at all that their type system is turing complete. It basically never comes up. reply taeric 13 hours agorootparentAgreed. Adding to this point, why should I care? Am I missing anything obvious on why you don&#x27;t want it? reply insanitybit 12 hours agorootparentNot really. In theory it means your compiler might never terminate... that does not usually happen. replyegl2021 19 hours agorootparentprev\"It isn&#x27;t that high of a bar unfortunately.\" --> \"It isn&#x27;t that high of a bar.\" reply haberman 19 hours agoparentprev100 times this. C++ is progressively making more and more of the language and stdlib available at compile time (constexpr, consteval, constinit). Once this is accomplished, there will be two very complicated Turing-complete programming languages available at compile time, one that is convenient to program (C++) and one that is not (C++ templates).Zig jumps straight to the finish line by making the main language available at compile time out of the gate, and by using it as the \"generics language\", Zig&#x27;s generics are both simpler and more powerful. reply dist1ll 20 hours agoparentprevauthor here. That&#x27;s a really good observation kornel. Rust&#x27;s philosophy of avoiding post-monomorphization errors at all costs is usually sung in high praises (followed by a ridicule of C++ template errors) - but it comes at a cost. And when you stray away from the declarative way and start plastering const generic bounds everywhere you really feel the pain.Anyways, I think engaging in this topic as a community is super important. Cause that&#x27;s the only way to push PLs forward and explore this massive space. reply marcosdumay 20 hours agoparentprev> instead of trying to make it declarativeWait, no. A language being Turing complete and declarative are completely independent things. reply dgb23 18 hours agorootparentThat&#x27;s the point! reply hnfong 20 hours agoparentprev> Rust’s type system ended up being Turing-complete anyway. It seems like it fell into the same trap as C++ templatesI hear it is a common trap.https:&#x2F;&#x2F;aphyr.com&#x2F;posts&#x2F;342-typing-the-technical-interview reply mlochbaum 20 hours agoparentprevI did this! Singeli is an Elixir-like compile-time language where types and variables are first-class values, on top of C-ish semantics that are meant to be more like \"portable assembly\". It&#x27;s designed for high-performance code where you have to be able to control the specific instructions, but the overall strategy could be useful in other domains too.https:&#x2F;&#x2F;github.com&#x2F;mlochbaum&#x2F;SingeliAnd a podcast on it came out Friday:https:&#x2F;&#x2F;www.arraycast.com&#x2F;episodes&#x2F;episode62-what-is-singeli reply mk12 19 hours agorootparentI listened to that, it was good! I was surprised there is an entire podcast about array programming. Looking forward to future episodes, and also to trying AoC with BQN again in a few months. reply avgcorrection 20 hours agoparentprevSurely they knew that it would become a programming language in its own right. That&#x27;s the fate of all such statically typed languages which give you type-level power but that aren&#x27;t dependently typed. (Of course dependently typed languages might end up with other languages like its own tactics language.) reply insanitybit 20 hours agorootparentI don&#x27;t think anyone has ever been surprised to find that a type system (with inference) is turing complete. reply justinpombrio 19 hours agoparentprevAgreed. The strong agree is that Rust&#x27;s type system---and many other languages&#x27; type systems---is a parallel language to its runtime language. Parameterized types are like functions; instantiating a type parameter is like a function call; `impl ... where` does pattern matching. You get a pure functional language based on pattern matching and implemented with memoization. These type systems are often Turing complete on purpose, but yet it&#x27;s surprisingly hard to get an infinite loop. Like, both Rust&#x27;s and Java&#x27;s type systems are Turing complete, yet I&#x27;ve never hit an infinite compilation loop in either by accident.For more on type systems as programming languages: https:&#x2F;&#x2F;ductile.systems&#x2F;oxidizing-the-technical-interview&#x2F; plus its links at the top.What I would like to see is a programming language where the runtime language and the comptime language are the same, or nearly the same, and where the comptime language is type safe. Zig isn&#x27;t this: its `comptime` language is essentially dynamically typed. In Zig, if a function `f` takes a `comptime` argument `t: Type`, it can call `.print()` on it, and the compiler will just assume that that&#x27;s fine. If you call `f`, you had better read the docs because the type system won&#x27;t tell you that `t` needs to have an `print()` method. If you&#x27;re calling `f` directly, that&#x27;s pretty straightforward, but the trouble comes when you actually call `h`, and `h` looks at the value of some string and based on that decides to call `g`, and `g` calls `f`, and the docs for `h` weren&#x27;t entirely clear, so now you&#x27;re seeing an error in `h`, which you didn&#x27;t even call. Instead, if `f` is going to call `.print()` on a `t`, then its argument `t` can&#x27;t just be a `Type`, the compiler should check that it&#x27;s a `Type with method .print()->String`. This requirement would then flow to `g` and `h`, so the type signature for `h` is guaranteed to tell you that `t` is required to have an `print()` method.For more on merging runtime and comptime languages in a type safe way, see 1ML: https:&#x2F;&#x2F;people.mpi-sws.org&#x2F;~rossberg&#x2F;1ml&#x2F;EDIT: Deleting my criticism of C++ templates lest it distract from the more substantial things I had to say above. reply zozbot234 16 hours agorootparent> and where the comptime language is type safeYou need dependent types in order to do this, which means doing away with Turing-completeness. (Moreover, the principle &#x27;Type is of type Type&#x27; as found in Zig comptime leads to type-unsafety. So you need to replace that with some notion of universes.) reply justinpombrio 16 hours agorootparentWell, you can have dependent types but limit their expressiveness. Rust, for example, has dependent (comptime) types, but they&#x27;re very extremely limited: fn foo() -> [f32; N]I imagine having arbitrary comptime code, but more limited use of comptime values in type position.Also, do you know the exact issue with \"Type is of type Type\"? I know that can lead to _non-termination_, and non-termination completely breaks proof assistants. For example, you can prove `False` with: fn make_false() -> False { return make_false(); }But if you&#x27;re not building a proof assistant, a function like `make_false()` is fine. Does it lead to any additional problems? reply gpderetta 18 hours agorootparentprev> What I would like to see is a programming language where the runtime language and the comptime language are the same, or nearly the same, and where the comptime language is type safeI have aproximately 0 knowledge of it, but I think TemplateHaskell should do that. reply justinpombrio 17 hours agorootparentNope. Haskell&#x27;s type system guarantees that code you construct in Template Haskell is well-formed, but the code you construct is only type checked once, at the end. So if you have a function that constructs code, and the code it constructs has a type error in it, you won&#x27;t find out unless you call the function. Just like Zig. reply avgcorrection 17 hours agorootparentprev> What I would like to see is a programming language where the runtime language and the comptime language are the same, or nearly the same, and where the comptime language is type safe.MetaOCaml might the closest one. reply packetlost 20 hours agoparentprevThis is sort of the approach that I&#x27;ve considered. Type systems are just predefined code&#x2F;behavior baked into a language, but there&#x27;s technically nothing stopping you from arbitrarily extending the compiler like Lisps do with macros to support a type system that&#x27;s written in the language itself. reply naasking 19 hours agorootparentType Systems as Macros:https:&#x2F;&#x2F;www.khoury.northeastern.edu&#x2F;home&#x2F;stchang&#x2F;pubs&#x2F;ckg-po... reply Aardwolf 19 hours agoparentprevGenuine question:How do type system that are so abstract and complex to become turing complete help?I definitely find it extremely helpful to be able to write containers of any type (like std::vector), that saves a ton of code duplication, but beyond that what more is needed and why? What we&#x27;re competing with here is: just write a function that operates on the data types you want and gets the job done.You&#x27;re writing code for the CPU that has to actually do something, on actual known types.What programming task is simplified by having an \"any\" type or a type system that allows you to write pong-played-turn-by-turn-using-compiler-error-messages? It&#x27;s cool that you can have an \"any\" type just like universal sets in set theory, but what real-life programming scenario does this simplify (you can already write containers that can contain anything you want without using such as thing as an \"any\" type)?At least not the kind of programming tasks I do, but admittely I think fairly low level and prefer my types to have exact known amounts of bits, known signed integer convention and endianness so I can efficiently use shifts and get the bits I need, preferably with as little undefined behavior as possible.Asked differently: If one were to design a programming language that only has basic types (primitives, structs&#x2F;classes, ...) and templates to allow functions&#x2F;classes to operate on any type (but not more than that; substitute template type with the actual type, compile this, nothing more), what feature will users of the language be missing and complain about? reply widdershins 18 hours agorootparentIt&#x27;s about when you want to enable optimisations or conveniences depending on the parameterized type.For example, in the std::vector type, if T supports being moved, you want to use that when growing your vector for performance. If T doesn&#x27;t support being moved, you will have to copy it instead.Boom: you&#x27;ve ended up with template metaprogramming. reply gpderetta 19 hours agorootparentprev> You&#x27;re writing code for the CPU that has to actually do something, on actual known types.(Partial) specialization, which is a feature used to get templates to do actual something on actual types is what principally allows templates to be turing complete. reply munificent 16 hours agorootparentprev> Asked differently: If one were to design a programming language that only has basic types (primitives, structs&#x2F;classes, ...) and templates to allow functions&#x2F;classes to operate on any type (but not more than that; substitute template type with the actual type, compile this, nothing more), what feature will users of the language be missing and complain about?This is a really good question. There are some languages that work as you describe: SML and some others in that family. There are generic functions and types, but the type parameters are basically just placeholders. You can&#x27;t do anything with a value whose type is a type parameter, except store it and pass it to things that also take type parameters.That gives you enough to write a nice reusable vector type. But it doesn&#x27;t let you easily write a nice reusable hash table. You can, but users have to pass in an explicit hash function that accepts the key type every time they create a hash table.It might be nice if a type itself could indicate whether it&#x27;s hashable and, if so, what it&#x27;s hash function is. Then, if you create a hash table with that key type, it automatically uses the hash function defined by that type.Now you need some sort of constraints or type bounds in your generics. That&#x27;s what traits in Rust and bounds in Java and C# give you. (The literature calls it \"bounded quantification\".) It&#x27;s a big jump in complexity. But it does mean that now you can call functions&#x2F;methods on arguments&#x2F;receivers whose type is a type parameter, and those calls can be type checked.Bounds are themselves types, so what kinds of types can you use in bounds? Can they be generic? If so, what kinds of type arguments are allowed? Can you use type parameters from the surrounding type?For example, which of these are OK and which aren&#x27;t (using Java-ish syntax): class A {} &#x2F;&#x2F; 1. class A> {} &#x2F;&#x2F; 2. class B> {} &#x2F;&#x2F; 3. class B> {} &#x2F;&#x2F; 4. class B> {} &#x2F;&#x2F; 5.Any kind of bounded quantification will give you 1-3. What about 4 and 5? This is called \"F-bounded quantification\". Why would you want such a thing?Collections with fast look-up are important, which is why we extended our generics to enable us to write nice reusable hash tables. But some data types aren&#x27;t easily hashed but can be easily ordered. A sorted collection is faster than an unsorted one.How would we write a generic sorted collection? We could require you to always explicitly pass in an ordering function for any given element type, but it would be nice if the element type itself could supply is order function.You could define a \"Comparable\" interface that a type can implement to support comparing an instance against another object of some type, like: interface Comparable { int compareTo(T other); }And then implement it on your type, like: class Color implements Comparable { int r, g, b; int compareTo(Color other) => ... }In our sorted collection, elements all have the same type, so the bound that we need looks like: class SortedCollection> { ... }Notice that we have \"T\" inside the bound. That&#x27;s F-bounded quantification.Using type parameters inside a bound isn&#x27;t the only place recursive types like this show up. Let&#x27;s say you wanted to make a generic type comparable. You&#x27;d do something like: class Pair implements Comparable> { T a, b; int compareTo(Pair other) => ... }Now here, the implements clause is using not just the type parameter of the enclosing type, but the entire type.We had a couple of fairly modest goals:* Be able to create reusable hash tables where the hash function is inferred from the key type.* Be able to create reusable sorted collections where the comparison function is inferred from the element type.And in order to get there, we needed generics, bounds, and even F-bounded quantification.Adding even a little more usefulness to our collection types will quickly have us reaching for variance annotations, associated types, and even more exotic stuff. reply Ar-Curunir 20 hours agoparentprevThat something can be Turing-complete in the worst case doesn’t mean that you hit those worst-cases on a frequent, or even occasional basis. reply itishappy 19 hours agoparentprevFascinating! Now I&#x27;m curious, what causes this? Where does Turing completeness come from?Polymorphism? Inference? Higher-kinded types? (Probably not that last one, I don&#x27;t think Rust has them.)Put another way, what would it take a for a type system to NOT become Turing complete? reply ynik 18 hours agorootparentTuring completeness doesn&#x27;t have to come from a single feature, it often comes from a combination of features.e.g. the interaction between subtyping (e.g. inheritance) and generics (with variance) is tricky: https:&#x2F;&#x2F;www.cis.upenn.edu&#x2F;~bcpierce&#x2F;papers&#x2F;variance.pdfIt can be highly nontrivial to tell if a language actually has a Turing complete type system: the 2007 Kennedy&Pierce paper made it likely that java was turing complete; but it took until 2016 until it was finally proven that to be turing complete (https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1605.05274).> What would it take a for a type system to NOT become Turing complete?An analysis of all possible interactions between all features in the type system, building a formal proof that the type system is not turing complete. This is not really realistic for the style of complex generic type systems that programmers are now used to, it would need to be a vastly simpler language. reply AprilArcus 13 hours agorootparentprevAll you need is a way to branch and a way to loop, so anything with conditional types and recursive types will be Turing Complete reply lenkite 20 hours agoparentprevBasically design in dependent types into the language right from the beginning (like Iris) - such that types are first class and can be computed and manipulated like any other language construct. reply jjtheblunt 15 hours agoparentprevCan you show how you&#x27;d implement a Turing machine in Rust types?I am not seeing it, so am likely overlooking that. reply baq 20 hours agoparentprevit&#x27;s easier to make something Turing-complete than not... most interesting example is probably rule 110. reply wredue 19 hours agorootparentThe point is that it’ll end up that way, so just embrace it from the start. reply baq 18 hours agorootparentthe lisp way. the problem is it results in software which is difficult to reason about due to all the compile-time execution, even if you have state-of-the-art compile-time debuggers (if any language has them, it is lisp). reply wredue 15 hours agorootparentI haven’t so far found any zig code terribly hard to reason about, and it’s important to know that compile time type crafting (currently) doesn’t support decls. Although you can madness your way around that, theres idiomatic ways to manage this.Comptime is definitely very powerful, and even the top people using zig frown upon “magic”.But to be completely honest, “magic” shit just happens all the time if you enable it. Rust macro abuse to get “magic” comes to mind. If it happens in rust, it’ll certainly happen in zig. reply throwaway894345 20 hours agoparentprevI think there&#x27;s still value in pushing people towards something declarative. For example, a static type system might have an `any` type for things that aren&#x27;t expressible statically but that doesn&#x27;t mean type system designers should embrace dynamic typing from the start instead of trying to make it static. Similarly, it makes sense to have panics&#x2F;exceptions in a language as an escape hatch for errors where the application can&#x27;t reasonably do anything gracefully, but it&#x27;s (arguably) a bad experience to take that to the extreme and panic for all errors.I think there&#x27;s value in accounting for the possibility that there will be edge cases that preclude hard-and-fast rules like \"purely static\" or \"purely declarative\", but I dislike the philosophy of projecting the 1% use case (e.g., \"dynamic\" or \"turing complete\") onto the 99% use case (where e.g., static and declarative would be ideal). I like when languages design for the 99% case and allow for escape hatches for the remaining 1% with the understanding that these escape hatches are intended to be used judiciously.To put it differently, embrace that there may be escape hatches in the initial design, but prefer to think of them as \"escape hatches\" with the entailed understanding that they should be rarely used. reply zoogeny 19 hours agoprevAs an aside and unrelated to the content of the article ... but the form of the article, with footnotes in the margin, is really useful. reply chrismorgan 19 hours agoparentGenerally speaking: sidenotes are excellent, footnotes are fine, endnotes are terrible. On the web, as a typically-not-paged medium, “footnotes” practically always actually means endnotes. reply dist1ll 19 hours agoparentprevThanks! It&#x27;s inspired by this https:&#x2F;&#x2F;edwardtufte.github.io&#x2F;tufte-css&#x2F; reply ammar2 17 hours agorootparentAnother quick question for you, what are you using to make those hand-drawn looking diagrams? They look amazing. reply dist1ll 17 hours agorootparentI&#x27;m using Excalidraw :)[0] https:&#x2F;&#x2F;excalidraw.com&#x2F; [1] https:&#x2F;&#x2F;github.com&#x2F;excalidraw&#x2F;excalidraw reply ghosty141 19 hours agorootparentprevThanks for sharing this! Looks fantastic, I&#x27;ll definitely incorporate this into my own blog once it&#x27;s finished. reply chrismorgan 18 hours agorootparentIf you’re interested in the general concept of sidenotes and labelling figures in the margin, here’s my take on it for further inspiration: https:&#x2F;&#x2F;chrismorgan.info&#x2F;blog&#x2F;2019-website&#x2F;, with https:&#x2F;&#x2F;chrismorgan.info&#x2F;blog&#x2F;rust-fizzbuzz&#x2F; exhibiting figures.Do search around for other sidenote implementations, too. I like the markup and presentation I end up with, but you’ll also find other approaches to mobile support in particular, involving things like checkbox hacks to toggle visibility, or just flat-out requiring JavaScript. reply bfrog 20 hours agoprevI still have yet to understand if Zig actually provides memory safety in the same manner Rust does or not, it seems like its another C like language without memory safety? Or did I happen to miss that bit? reply xigoi 19 hours agoparent> I still have yet to understand if Zig actually provides memory safety in the same manner Rust does or notIt doesn&#x27;t. reply emmanueloga_ 14 hours agorootparentIt doesn&#x27;t, but it puts a RED BANNER in every place where memory is allocated, and you need to explicitly pass an allocator of your choosing whenever memory is allocated. It also contains no hidden allocations or function calls. So it forces you to think about your memory allocation patterns, and also includes some tools to catch memory leaks [1].1: https:&#x2F;&#x2F;ziglang.org&#x2F;learn&#x2F;samples&#x2F;#memory-leak-detection reply saagarjha 12 hours agorootparentThat doesn&#x27;t really help. reply emmanueloga_ 5 hours agorootparentJust as your comment :-p reply saagarjha 2 hours agorootparentIf you didn&#x27;t find \"thinking about memory allocations doesn&#x27;t help much with memory safety\" useful I don&#x27;t really have much more to share. reply bfrog 18 hours agorootparentprevSo its C with nicer macros and maybe some more sensible ways of dealing with arrays and such?For me that&#x27;s not enough to move the needle. reply cmrdporcupine 17 hours agorootparentIt&#x27;s more than that in that it has a sane syntax and module and build system.I&#x27;m a Rust person by both day job and hobby, but I can see the niche Zig is in as being quite useful in particular for embedded work.One thing it definitely has over Rust is way better support for explicit per-structure allocator&#x2F;allocation management. The allocator_api stuff in Rust has just been sitting unstable for years at this point, while Zig shipped with support for this immediately. Which is key for work in all sorts of systems level contexts.Probably the language I want is somewhere between the two. reply bfrog 15 hours agorootparentI mean I really have a hard time going backwards on the whole idea of lifetimes and send&#x2F;sync type traits which force escape hatches if you want to do something idiotic. The key piece here is, any project of significant size and usage, someone will try to do something stupid. Rust makes the stupid so obvious with unsafe blocks and annotations, that reviewing code from some random person on the internet is made easier. The tooling made the task of reviewing code mostly limited to the code being changed rather than the whole project.In C&#x2F;C++ land you need to consider not only the code being changed but also the entire code base and how that code is used. It&#x27;s not enough to look at the diff. You need to understand the context.Not to say that isn&#x27;t also true with Rust to some degree, but the degree is usually in terms of logic and less \"is some pointer&#x2F;reference going to cause a race&#x2F;segfault&#x2F;buffer xflow?\"The langauge itself doesn&#x27;t define allocation, Box is in the stdlib and this allows for nostd libraries to deal with things as I guess most of us might expect I&#x27;d think. It would be cool to allow for a global allocator though to enable std on more platforms with better semantics, no disagreement there. reply maxmcd 19 hours agoparentprevI found this article to be clarifying: https:&#x2F;&#x2F;www.scattered-thoughts.net&#x2F;writing&#x2F;how-safe-is-zig&#x2F; reply bfrog 15 hours agorootparentIn effect this answers my question pretty well, the answer seems to be a resounding \"No there&#x27;s no memory safety\" at least in the practical or realistic sense that results in few to no memory lifetime errors. reply dgb23 19 hours agoparentprevZig provides tools to avoid memory related bugs, but it doesn&#x27;t enforce it via compile time checked RAII&#x2F;burrow checking etc.Nullability (and error handling) is compile time checked though. reply bfrog 18 hours agorootparentTo me the biggest win in Rust is just that, the memory related bugs are nearly non-existent. Throw in a group of people working on something and this makes the project move just so much faster in both review time and time hunting bugs. reply cmrdporcupine 17 hours agorootparentIMHO the real big win w&#x2F; Rust is not so just memory safety from (some) leaks or segfaults but the fact that the borrow checker rules apply also to concurrency related scenarios and so race conditions and deadlocks become much more difficult to accidentally produce.Developers can generally be taught to handle memory ownership issues fairly well, esp when helped out with RAII smart pointers, etc.But in my experience when you throw concurrency in, even very high quality engineers tend to stumble and accidentally introduce race conditions.In this respect Rust has something over the garbage collected languages, too. Because Java, Go, JS, C#, etc will effectively shield you from \"use after free\" but they will absolutely not stop you from passing an unlocked, etc. reference to another thread and screwing yourself up royally; and Rust will. reply throwawaymaths 19 hours agoparentprevZig gives buffer overflow safety and null pointer safety but not use after free, double free, or stack pointer escape safety.Zig also gives you memory exhaustion safety which rust does not. reply gpanders 18 hours agorootparent>Zig gives buffer overflow safety and null pointer safety but not use after free, double freeIt can provide the latter two through the use of the `GeneralPurposeAllocator`, which tracks UAF and double free.Stack pointer escape safety is being actively researched (there are a few tracking issues, see [1]). I&#x27;m personally interested in this, I&#x27;ve written a decent-ish amount of Zig for toy&#x2F;personal projects, and anecdotally stack pointer escape is the only type of memory error that has bitten me more than once (though to be fair, one of these cases was calling into a C API, so even Rust wouldn&#x27;t have helped).More broadly, the ability to catch all forms of UB in Debug&#x2F;safety builds is an accepted proposal [2], though whether or not it will be possible in practice is a different (and interesting!) question.[1]: https:&#x2F;&#x2F;github.com&#x2F;ziglang&#x2F;zig&#x2F;issues&#x2F;2646[2]: https:&#x2F;&#x2F;github.com&#x2F;ziglang&#x2F;zig&#x2F;issues&#x2F;2301 reply SkiFire13 16 hours agorootparentThe way `GeneralPurposeAllocator` works is kinda scary though, since it may result in whole memory pages used only for very small allocations, effectively multiplying the memory usage of the program. Also kinda goes against the memory exhaustion safety, since now you&#x27;re more likely to exhaust memory. reply jmull 17 hours agorootparentprevYeah, I don&#x27;t think it&#x27;s right to say Zig doesn&#x27;t have use-after-free and double-free safety. If you want that, you can just use a general purpose allocator. Note that you can&#x27;t forget to choose an allocator since they are explicit.Is this somehow harder than, say, choosing not to use \"unsafe\" in Rust?Maybe all that is missing is a linter to help enforce whatever memory-management policy you&#x27;ve decided on. That&#x27;s not really needed for small, coherent teams, but would be important for using Zig in larger projects with multiple teams and&#x2F;or disparate individual contributors. (Perhaps such a thing exists and I just don&#x27;t know about it.)You might also be able to use an arena allocator where free is a no-op. That has different tradeoffs, but is also safe for use-after-free and double-free.As you say, stack escape is the main thing where Zig doesn&#x27;t have a good memory-safety story yet (at least not that I&#x27;ve heard). I guess there are a few others that concern me when I see them on a list, though I haven&#x27;t hit them in real life. reply throwawaymaths 17 hours agorootparentStatic analysis at the IR level would be awesome. It could catch use-undefined, stack escape, and probably uaf&#x2F;df as well so you don&#x27;t have to lean on gpa&#x27;s (potentially expensive) tricks. Plus there are times when you maybe don&#x27;t want to use page allocator.As an aside. I&#x27;m not certain I understand how double free is memory unsafe (in the sense of \"causing vulnerabilities\") reply saagarjha 12 hours agorootparentA double free breaks invariants for the memory allocator. For example, the freed memory may have been handed out to someone else and if you free it again, it will be marked as unused even though that code relies on their stuff being available. One very classic way of exploiting a double-free is a sequence like this happens:1. Some code allocates memory.2. The code frees the memory, but keeps a stale reference to it around. It is marked as unused by the allocator.3. Some other code allocates memory. Maybe it&#x27;s reading the password file off of disk. The allocator has some unused memory lying around so it hands it out–but it turns out that this is actually just a reuse of the buffer from earlier. It is now marked as \"in use\" again by the allocator.4. The code from earlier has a bug and frees the allocation again. This means that the allocation is now marked as \"unused\".5. Another allocation request hands out this memory again. Maybe it&#x27;s a buffer for user input? Well, it&#x27;s been scribbled all over with other data now.6. Someone asks to authenticate and the password checking code gets called. It has the password right here to check against…oh, wait, that memory got freed out from under it and overwritten with some attacker-controlled content! reply jmull 16 hours agorootparentprev> I&#x27;m not certain I understand how double free is memory unsafe (in the sense of \"causing vulnerabilities\")Perhaps there are some allocators where doing that hits UB. UB in memory allocation is probably always a memory safety issue. I would say if your code accepts any allocators where double-free could be UB then you&#x27;ve got a safety issue. reply saagarjha 12 hours agorootparentprevC has use-after-free and double-free safety if you patch out free. Not really a solution, is it? reply jmull 11 hours agorootparentWhat do you think the difference is between “patching out free” and allocators as a first-class feature? I’ll bet you can think of a few rather significant ones if you try. reply saagarjha 10 hours agorootparentZig has custom allocators as a first-class feature. It does not have memory safety as a first-class feature. reply saagarjha 12 hours agorootparentprevAn allocator that does heap quarantining at the page level is not a \"general purpose allocator\". It is a debug tool. reply oconnor663 18 hours agorootparentprevIs there a memory exhaustion scenario where Rust does something other than panic? reply kibwen 18 hours agorootparentRust-the-language doesn&#x27;t do any dynamic allocation, so it doesn&#x27;t exhaust memory at all. The Rust stdlib currently guarantees an abort on OOM, but in the future this will be changed to a panic (which you can always configure to be an abort if you want). Both of these behaviors are memory-safe (it&#x27;s unclear what \"memory exhaustion safety\" is referring to). reply throwawaymaths 18 hours agorootparentAlso to clarify: there are platforms (like linux) which de facto have nonfailable allocation, if you run out of memory the system will oom kill your process, and neither zig nor rust will save you from that.I believe In practice the most common place where failable OOM is a big deal is in embedded systems programming reply 10000truths 16 hours agorootparentNo language can save you from OOMs, but because Zig pervasively uses explicit memory allocation, it makes it easy to greatly mitigate OOM risks by front-loading all the fallibility:1. Calculate and allocate all your required memory immediately upon process startup (including memory from OS resources like sockets)2. Call mlockall to prevent the pages from being swapped out3. Write \"-1000\" to &#x2F;proc&#x2F;self&#x2F;oom_score_adj to avoid the OOM killer4. Use your preallocated memory as a pool for all further allocationsWith the above approach, the application has full control over how to handle application-level OOMs (e.g. applying backpressure to connecting clients, or shrinking non-critical caches) once it is past the start-up stage. reply throwawaymaths 18 hours agorootparentprev> memory exhaustion safetySure this more like DDOS mitigation, so it memory related safety, not memory safety. reply pitaj 18 hours agorootparentprevYou can use the fallible allocation APIs such as `Vec::try_reserve` reply pitaj 18 hours agorootparentprevWhat do you mean by memory exhaustion safety? reply Joker_vD 21 hours agoprevWait, the thing the article calls \"array of variant arrays (or AoVA)\" is not the canonical way to decompose tagged unions? Seriously? [A×B×C] ≃ [A]×[B]×[C], that&#x27;s the most obvious thing, isn&#x27;t it?Edit: Actually, never mind, as the comment points out, the correct equation should be [A+B+C] ≃ [A]×[B]×[C] and it&#x27;s not obvious at all whether it&#x27;s actually correct. reply pornel 20 hours agoparentThings like that can be done by specific collections (e.g. most ECS implementations store components per \"archetype\").But from language perspective the limiting factor is that you&#x27;re always allowed to take a reference to any instance of an enum. This means the tag must be stored in every instance. If you allow mutable references to enums, you must have enough space for writing any variant there through the reference. reply kccqzy 16 hours agoparentprevOf course not: you are mistaking sum types and product types.Even if we assume they are product types, [A×B×C] ≃ [A]×[B]×[C] is still not correct. The former doesn&#x27;t allow the number of As and Bs and Cs to differ: the latter does. So the latter strictly speaking admits more values than the former.Since both of them allow infinite number of values, to make a size comparison we will use generating functions. A list of A (or [A] in your notation) allows an empty list (one possible value), a one-element list (as many values as A itself), a two-element list… which becomes 1+A+A^2+…=1&#x2F;(1-A). The beautiful thing about calling them sum types or products types is that you can manipulate it just by summing or multiplying respectively.So the number of values for [AxBxC] is identified by 1&#x2F;(1-ABC). For [A]*[B]*[C] it&#x27;s 1&#x2F;(1-A)&#x2F;(1-B)&#x2F;(1-B) which simplifies to 1&#x2F;(1-A-B-C+AB+AC+BC-ABC). Now it becomes obvious† this form admits more values and you can in a sense quantify how many more!†: Okay perhaps it&#x27;s only obvious if I also include a Venn diagram but diagramming is beyond what I can do on HN. reply Joker_vD 15 hours agorootparentYeah, and if you carefully expand 1+(A+B+C)+(A+B+C)^2+(A+B+C)^3+..., there&#x27;ll be strictly more summands than in (1+A+A^2+A^3+...)×(1+B+B^2+B^3+...)×(1+C+C^2+C^3+...) due to more flexible ordering of elements, so they&#x27;re not isomorphic.I still think it&#x27;s the most obvious thing that [A+B+C] at least maps surjectively onto [A]×[B]×[C] :) reply catgary 15 hours agoparentprevIt’s a section&#x2F;retract, definitely not an isomorphism. You’ll need to carry around the index data, and any sort of array update will incur a crazy upfront cost. reply Joker_vD 14 hours agorootparentEh, I imagine it should be possible to maintain an out-of-line skip-list or an array of tombstones or something to deal with deletion&#x2F;tag mutation (although in my experience with ASTs that&#x27;s not that common of a requirement); and allocation can be done quite effectively with the help of the good old .bss section: just grab couple of millions of every type of node at the program&#x27;s startup, virtual memory is almost free. reply ratmice 20 hours agoparentprevDon&#x27;t you mean something like [A+B+C] ≃ [A]×[B]×[C] reply kazinator 19 hours agoprevYou can solve this problem if a few clear lines of GNU C. We declare two sister types, one of which is packed: typedef struct taggedunion { unsigned char tag; union tu { uint64_t u64; uint32_t u32; uint8_t u8; } u; } taggedunion_t; typedef struct __attribute__((packed)) packed_taggedunion { unsigned char tag; union tu u; } packed_taggedunion_t;Then accesses to our dynamic array will convert between the packed stored representation and the unpacked one in a straightforward way: taggedunion_t array_get(packed_taggedunion_t *a, size_t i) { return (taggedunion_t){ a[i].tag, a[i].u }; } reply loeg 17 hours agoparentYour packed union eliminates pure padding, yes, but it still costs 9 bytes to store a u8 value and misaligns accesses for wider elements. These are both significant misses on \"solving the problem.\" You also need to copy each element on access, which could be expensive for larger plain-old-data types, or especially for non-POD types. reply cryptonector 18 hours agoparentprevBut it&#x27;s C. The idea here is to have higher-level, type-safe languages do this transformation automatically. reply WhereIsTheTruth 18 hours agoparentprevFor any language to be labeled as a \"better c\" language, it must solve this problem, having better enums is a requirementIt&#x27;s actually one of the reason I initially gave Zig a try, not Comptime, it was Tagged Union reply masfuerte 18 hours agoparentprevBut this still takes 9 bytes (at least) to store a tagged uint8_t. reply kazinator 17 hours agorootparentThe article&#x27;s approach requires 8 bytes, plus a separate array of 1 byte tags.The article reduces it to 4 bytes, plus an array of 1 byte tags, by dropping the requirement for a 64 bit member.If we have to grow the array, we have to do two reallocations.Caching is worsened: the tag and content are going to be in separate cache lines. reply masfuerte 16 hours agorootparentThe final approach in the article is an array for each unique object size plus tagged indices&#x2F;pointers. This would take one byte per uint8_t and doesn&#x27;t suffer from the problems you mention, though it does have others. If memory pressure is your main problem it&#x27;s a big win. reply sdfghswe 21 hours agoprev [–] More or less OT.I keep hearing about both Zig and D as interesting C-like low level languages with better satefies&#x2F;ergonomics. I wonder if someone who&#x27;s familiar with both would like to discuss the main differences...? reply esjeon 20 hours agoparentD is a GC-based C++&#x2F;Java-like OOP language. People used to call it \"a C++ done right\", but it was never simple and didn&#x27;t have any clear edges against its competitors. TBH, one simply should not build on top of C++ - it&#x27;s a beast that no one can comprehend, and it only keeps growing.In case of Zig, I think it&#x27;s a very good modern C alternative. Zig can do what C can do, can interoperate w&#x2F; C (e.g. importing C headers), and offers new features that are useful (comptime, slices, error type). It&#x27;s like Go, but without GC and a large runtime. reply sdfghswe 19 hours agorootparentOh many I was loving your description of Zig up until when you said \"it&#x27;s like Go\"... reply esjeon 10 hours agorootparentYeah, it may sound weird, but there were days when Go was considered a C replacement - which turned out to be quite false. There’s a dialect - TinyGo - sorta carries the old vibe(?), but the upstream never picked up the idea. reply Guvante 18 hours agorootparentprevI haven&#x27;t heard of Zig referred to as like Go except in the concept of trying to replace C.(Rust is more aiming for C++) reply optymizer 19 hours agoparentprevHaving written a few toy compilers for C, to me D is the most elegant of the C families of languages.For example, consider the versatility of the &#x27;.&#x27; operator. It does pointer dereference, static field access and class&#x2F;namespace member&#x2F;method access. In all those cases I want to access \"a thing from this other thing\". Having 3 operators in a language (., ->, ::) is easier for a compiler writer but it puts more cognitive load on the developer.Now consider type extensions. As a language designer you could think: \"if a developer wishes to add methods to a type Vector that exists in a third party library, then in their project, the developer would use some special syntax\" like maybe: class Vector { void json() { ... } }and maybe the developer also needs to have it even repeated across a header file and an implementation, but... why? It&#x27;s an idea, but it&#x27;s not a law of the Universe. There is a better way.Instead, the compiler can help you out. Just declare a regular function anywhere like so: void json(Vector v) { ... }which is not special in anyway, and then be free to invoke it as: json(v);or v.json();Why if it isn&#x27;t our familiar dot operator! It also does type extensions via some syntax sugar? What an elegant solution.This is just one feature of D, and it&#x27;s filled with features. Though I don&#x27;t really use D in my day to day work, it&#x27;s wonderful to see how the authors thought about hard problems in language design and implemented elegant solutions. I think D deserves more recognition for the things it absolutely hit out of the park as a general purpose programming language. reply ksec 18 hours agoparentprevWell WalterBright could have a",
    "originSummary": [
      "The blog post discusses the problem of memory fragmentation in Rust due to the necessity to allocate sufficient space for the largest variant in enum arrays.",
      "The author mentions methods to reduce fragmentation like the struct-of-arrays approach, and array of variant arrays approach, particularly in the context of compilers and ASTs (Abstract Syntax Trees).",
      "The advantages of Zig's memory-efficient data structures over Rust are highlighted, including the ability to perform concise transformations and the potential for setting index bitwidth at compile time for better memory efficiency."
    ],
    "commentSummary": [
      "The conversation focuses on programming languages like Zig, Rust, and C++, discussing topics from memory-efficient arrays to Turing completeness, comptime types, and memory safety.",
      "Participants examine the suitability, challenges, and maturity levels of these languages for various applications, along with their pros and cons.",
      "The discussion emphasizes the importance of language adoption, developer experience, and the trade-offs between different methodologies and approaches in programming."
    ],
    "points": 314,
    "commentCount": 230,
    "retryCount": 0,
    "time": 1695038431
  },
  {
    "id": 37555118,
    "title": "The brain is not an onion with a tiny reptile inside (2020)",
    "originLink": "https://journals.sagepub.com/doi/10.1177/0963721420917687",
    "originBody": "We value your privacy We and our partners store and/or access information on a device, such as cookies and process personal data, such as unique identifiers and standard information sent by a device for personalised ads and content, ad and content measurement, and audience insights, as well as to develop and improve products. With your permission we and our partners may use precise geolocation data and identification through device scanning. You may click to consent to our and our partners’ processing as described above. Alternatively you may click to refuse to consent or access more detailed information and change your preferences before consenting. Please note that some processing of your personal data may not require your consent, but you have a right to object to such processing. Your preferences will apply to this website only. You can change your preferences at any time by returning to this site or visit our privacy policy. MORE OPTIONS DECLINE ALL ACCEPT ALL Skip to main content Search this journal Enter search terms... Search Advanced search Access/Profile Cart Browse by discipline Information for Current Directions in Psychological Science Journal indexing and metrics JOURNAL HOMEPAGE SUBMIT PAPER Available access Research article First published online May 8, 2020 Your Brain Is Not an Onion With a Tiny Reptile Inside Joseph Cesario https://orcid.org/0000-0002-1892-4485 cesario@msu.edu, David J. Johnson, and Heather L. EisthenView all authors and affiliations Volume 29, Issue 3 https://doi.org/10.1177/0963721420917687 Contents Abstract What’s Wrong? So What? Conclusion Recommended Reading Acknowledgments ORCID iD Footnotes Transparency References PDF / ePub Cite article Share options Information, rights and permissions Metrics and citations Figures and tables Abstract A widespread misconception in much of psychology is that (a) as vertebrate animals evolved, “newer” brain structures were added over existing “older” brain structures, and (b) these newer, more complex structures endowed animals with newer and more complex psychological functions, behavioral flexibility, and language. This belief, although widely shared in introductory psychology textbooks, has long been discredited among neurobiologists and stands in contrast to the clear and unanimous agreement on these issues among those studying nervous-system evolution. We bring psychologists up to date on this issue by describing the more accurate model of neural evolution, and we provide examples of how this inaccurate view may have impeded progress in psychology. We urge psychologists to abandon this mistaken view of human brains. The purpose of this article is to clarify a widespread misconception in psychological science regarding nervous-system evolution. Many psychologists believe that as new vertebrate species arose, evolutionarily newer complex brain structures were laid on top of evolutionarily older simpler structures; that is, that an older core dealing with emotions and instinctive behaviors (the “reptilian brain” consisting of the basal ganglia and limbic system) lies within a newer brain capable of language, action planning, and so on. The important features of this model, often called the triune-brain theory, are that (a) newer components are literally layered outside of older components as new species emerge, and (b) these newer structures are associated with complex psychological functions we reserve for humans or, if we are feeling generous, for other primates and social mammals (see Figs. 1a and 1b). As Paul MacLean (1964), originator of the triune-brain theory, stated, man, it appears, has inherited essentially three brains. Frugal Nature in developing her paragon threw nothing away. The oldest of his brains is basically reptilian; the second has been inherited from lower mammals; and the third and newest brain is a late mammalian development which reaches a pinnacle in man and gives him his unique power of symbolic language. (p. 96) Fig. 1. Incorrect views (a, b) and correct views (c, d) of human evolution. Incorrect views are based on the belief that earlier species lacked outer, more recent brain structures. Just as species did not evolve linearly (a), neither did neural structures (b). Although psychologists understand that the view shown in (a) is incorrect, the corresponding neural view (b) is still widely endorsed. The evolutionary tree (c) illustrates the correct view that animals do not linearly increase in complexity but evolve from common ancestors. The corresponding view of brain evolution (d) illustrates that all vertebrates possess the same basic brain regions, here divided into the forebrain, midbrain, and hindbrain. Coloring is arbitrary but illustrates that the same brain regions evolve in form; large divisions have not been added over the course of vertebrate evolution. OPEN IN VIEWER This belief, although widely shared and stated as fact in psychology textbooks, lacks any foundation in evolutionary biology. Our experience suggests that it may surprise many readers to learn that these ideas have long been discredited among people studying nervous-system evolution. Indeed, some variant of the above story is seen throughout introductory discussions of psychology and some subareas within the discipline. We provide a few brief examples, illustrate what is wrong with this view, and discuss how these ideas may have impacted psychological research. Within psychology, a broad understanding of the mind contrasts emotional, animalistic drives located in older anatomical structures with rational, more complex psychological processes located in newer anatomical structures. The most widely used introductory textbook in psychology states that in primitive animals, such as sharks, a not-so-complex brain primarily regulates basic survival functions. . . . In lower mammals, such as rodents, a more complex brain enables emotion and greater memory. . . . In advanced mammals, such as humans, a brain that processes more information enables increased foresight as well. . . . The brain’s increasing complexity arises from new brain systems built on top of the old, much as the Earth’s landscape covers the old with the new. Digging down, one discovers the fossil remnants of the past. (Myers & Dewall, 2018, p. 68) To investigate the scope of the problem, we sampled 20 introductory psychology textbooks published between 2009 and 2017. Of the 14 that mention brain evolution, 86% contained at least one inaccuracy along the lines described above. Said differently, only 2 of the field’s current introductory textbooks describe brain evolution in a way that represents the consensus shared among comparative neurobiologists. (See https://osf.io/r6jw4/ for details.) Examples of this mistaken view are readily found throughout subareas in psychology. In social cognition, this distinction has been a foundation for dual-process models of automaticity, some of which contrast fast and uncontrollable processes with slower and controllable processes. For example, Dijksterhuis and Bargh (2001), discussing their model of a direct link between perception and behavior, write that when new species develop, this is done by adding new brain parts to existing old ones. . . . The frog and fish, in other words, are still in us. The advantage that humans have is that we also possess new inhibiting or moderating systems. (p. 5) This widely cited idea is that the behavior of many animals is inflexibly controlled by external stimuli because their brains consist of older structures capable only of reflexive responses, whereas humans and other “higher” animals possess newer systems that allow behavioral flexibility because of added functions such as control and inhibition (Dijksterhuis, Bargh, & Miedema, 2000). Examples of MacLean’s model of brain evolution appear in other areas, including models of personality (Epstein, 1994), attention (Mirsky & Duncan, 2002), psychopathology (Cory & Gardner, 2002), market economics (Cory, 2002), and morality (Narvaez, 2008). Nonacademic examples are too numerous to fully review. The idea of an older animalistic brain buried deep within our newer, more civilized outer layer is referenced widely. Carl Sagan’s (1978) Pulitzer Prize–winning book, The Dragons of Eden, and Steven Johnson’s (2005) Mind Wide Open were both popular books that drew heavily on this idea, and Sagan’s book played a large role in bringing these ideas to nonacademic audiences. What’s Wrong? The above examples illustrate several misunderstandings of nervous-system evolution. The first problem is that these ideas reflect a scala naturae view of evolution in which animals can be arranged linearly from “simple” to the most “complex” organisms (Fig. 1a). This view is unrealistic in that neural and anatomical complexity evolved repeatedly within many independent lineages (Oakley & Rivera, 2008). This view also implies that evolutionary history is a linear progression in which one organism became another and then another. It is not the case that animals such as rodents, with “less complex” brains, evolved into another species with slightly more complex brains (i.e., with structures added onto the rodent brain), and so on, until the appearance of humans, who have the most complex brains yet. This misunderstanding and the theoretical problems that follow have been discussed within comparative psychology since the 1960s (Hodos & Campbell, 1969; LeDoux, 2012).1 Instead, the correct view of evolution is that animals radiated from common ancestors (Fig. 1c). Within these radiations, complex nervous systems and sophisticated cognitive abilities evolved independently many times. For example, cephalopod mollusks, such as octopus and cuttlefish, possess tremendously complex nervous systems and behavior (Mather & Kuba, 2013), and the same is true of some insects and other arthropods (Barron & Klein, 2016; Strausfeld, Hansen, Li, Gomez, & Ito, 1998). Even among nonmammalian vertebrates, brain complexity has increased independently several times, particularly among some sharks, teleost fishes, and birds (Striedter, 1998). Along with this misunderstanding comes the incorrect belief that adding complex neural structures allows increased behavioral complexity—that structural complexity endows functional complexity. The idea that larger brains can be equated with increased behavioral complexity is highly debatable (Chittka & Niven, 2009). At the very least, nonhuman animals do not respond inflexibly to a given stimulus. All vertebrate behavior is generated by similar neural substrates that integrate information to produce behavior on the basis of evolved decision-making circuits (Berridge, 2003). The final—and most important—problem with this mistaken view is the implication that anatomical evolution proceeds in the same fashion as geological strata, with new layers added over existing ones. Instead, much evolutionary change consists of transforming existing parts. Bats’ wings are not new appendages; their forelimbs were transformed into wings through several intermediate steps. In the same way, the cortex is not an evolutionary novelty unique to humans, primates, or mammals; all vertebrates possess structures evolutionarily related to our cortex (Fig. 1d). In fact, the cortex may even predate vertebrates (Dugas-Ford, Rowell, & Ragsdale, 2012; Tomer, Denes, Tessmar-Raible, & Arendt, 2010). Researchers studying the evolution of vertebrate brains do debate which parts of the forebrain correspond to which others across vertebrates, but all operate from the premise that all vertebrates possess the same basic brain—and forebrain—regions. Neurobiologists do not debate whether any cortical regions are evolutionarily newer in some mammals than others. To be clear, even the prefrontal cortex, a region associated with reason and action planning, is not a uniquely human structure. Although there is debate concerning the relative size of the prefrontal cortex in humans compared with nonhuman animals (Passingham & Smaers, 2014; Sherwood, Bauernfeind, Bianchi, Raghanti, & Hof, 2012; Teffer & Semendeferi, 2012), all mammals have a prefrontal cortex. The notion of layers added to existing structures across evolutionary time as species became more complex is simply incorrect. The misconception stems from the work of Paul MacLean, who in the 1940s began to study the brain region he called the limbic system (MacLean, 1949). MacLean later proposed that humans possess a triune brain consisting of three large divisions that evolved sequentially: The oldest, the “reptilian complex,” controls basic functions such as movement and breathing; next, the limbic system controls emotional responses; and finally, the cerebral cortex controls language and reasoning (MacLean, 1973). MacLean’s ideas were already understood to be incorrect by the time he published his 1990 book (see Reiner, 1990, for a critique of MacLean, 1990). Nevertheless, despite the mismatch with current understandings of vertebrate neurobiology, MacLean’s ideas remain popular in psychology. (A citation analysis shows that neuroscientists cite MacLean’s empirical articles, whereas non-neuropsychologists cite MacLean’s triune-brain articles. See https://osf.io/r6jw4/ for details.) So What? Does it matter if psychologists have an incorrect understanding of neural evolution? One answer to this question is simple: We are scientists. We are supposed to care about true states of the world even in the absence of practical consequences. If psychologists have an incorrect understanding of neural evolution, they should be motivated to correct the misconception even if this incorrect belief does not impact their research programs. A more practical question concerns the benefits to psychological science if psychologists changed their mistaken views of neural evolution. Consider the consequence of believing that humans have unique neural structures that endow us with unique cognitive functions. This belief encourages researchers to provide species-specific explanations when it might be more appropriate to recognize cross-species connections. In other words, by anointing certain brain regions and functions as special, researchers treat them as special in their research (see Higgins, 2004). To illustrate, consider the dual-process theories found throughout much of psychology. In an Annual Review of Psychology article, Evans (2008) summarizes that a “recurring theme in dual-process theories” (p. 259) across content areas is the proposal of “two architecturally (and evolutionarily) distinct cognitive systems” (p. 255), with System 1 preceding System 2 in evolutionary development. This division of psychological functions into evolutionarily older animalistic drives versus evolutionarily newer rational thought is exemplified by research on willpower, which has historically been dominated by a framing that contrasts “hot,” immediate, and emotional choices with “cool,” long-term, and rational choices. Should I eat the ice cream, which tastes good now, or the salad, which I know is better for me in the future? In the classic marshmallow studies, delaying gratification by waiting to eat the marshmallows is seen as a good result—indicating more willpower (Shoda, Mischel, & Peake, 1990). This framing is expected given that the starting point of this research was the Freudian psychodynamic position, which contrasted hot animalistic drives with cool rational processes. Framing willpower as long-term planning versus animalistic desires leads to the questionable conclusion that delaying gratification is not something other animals are capable of if other animals lack the evolutionarily newer neural structures required for rational long-term planning. Although certain aspects of willpower may be unique to humans, this framing misses the connection between willpower in humans and decision-making in nonhuman animals. All animals make decisions between actions that involve trade-offs in opportunity costs. In this way, the question of willpower is not “Why do people act sometimes like hedonic animals and sometimes like rational humans?” but instead, “What are the general principles by which animals make decisions about opportunity costs?” (Gintis, 2007; Kurzban, Duckworth, Kable, & Myers, 2013; Monterosso & Luo, 2010). In evolutionary biology and psychology, life-history theory describes broad principles concerning how all organisms make decisions about trade-offs that are consistent with reproductive success as the sole driver of evolutionary change (Daly & Wilson, 2005; Draper & Harpending, 1982). This approach asks how recurrent challenges adaptively shape decisions regarding opportunity trade-offs. For example, in reliable environments, waiting to eat a second marshmallow is likely to be beneficial. However, in environments in which rewards are uncertain, such as when experimenters are unreliable, eating the single marshmallow right away may be beneficial (Kidd, Palmeri, & Aslin, 2013). Thus, impulsivity can be understood as an adaptive response to the contingencies present in an unstable environment rather than a moral failure in which animalistic drives overwhelm human rationality. Research motivated by this more accurate understanding of brain evolution has been integrative, bringing together research on willpower, inhibition, future discounting, and delay of gratification with evolutionary and developmental approaches (Fawcett, McNamara, & Houston, 2012; McGuire & Kable, 2013). It also has been generative, asking questions that would not make sense from a dual-process perspective on human willpower, such as whether the lack of inhibition that comes from exposure to adverse environments might be just one component of a set of cognitive adaptations designed to enable successful navigation of those environments (Frankenhuis & de Weerth, 2013). Of course, asking about a specific species’ cognitive or behavioral repertoire can yield important insights about both evolutionary history and the nature of a species’ current phenotype (e.g., Tomasello, 2009; Tooby & Cosmides, 2005). After all, humans—like every animal—faced unique environmental challenges that shaped their evolutionary trajectory. But believing that humans possess unique neural structures tied to specific cognitive functions may send researchers down a path of research that is misguided and may inhibit connections with other fields. Conclusion Perhaps mistaken ideas about brain evolution persist because they fit with the human experience: We do sometimes feel overwhelmed with uncontrollable emotions and even use animalistic terms to describe these states. These ideas are also consistent with such traditional views of human nature as rationality battling emotion, the tripartite Platonic soul, Freudian psychodynamics, and religious approaches to humanity. They are also simple ideas that can be distilled to a single paragraph in an introductory textbook as a nod to biological roots of human behavior. Nevertheless, they lack any foundation in our understanding of neurobiology or evolution and should be abandoned by psychological scientists. Recommended Reading Gawronski, B., & Cesario, J. (2013). Of mice and men: What animal research can tell us about context effects on automatic responses in humans. Personality and Social Psychology Review, 17, 187–215. A comprehensive review comparing nonhuman- and human-animal models of behavior and automatic responses. Hodos, W., & Campbell, C. B. G. (1969). (See References). A classic work illustrating how beliefs about hierarchy across animal species inhibits scientific progress. Kaas, J. H. (2013). The evolution of brains from early mammals to humans. Wiley Interdisciplinary Reviews: Cognitive Science, 4, 33–45. A review of principles of basic mammalian brain evolution by a leader in the field. Krubitzer, L. A., & Seelke, A. M. (2012). Cortical evolution in mammals: The bane and beauty of phenotypic variability. Proceedings of the National Academy of Sciences, USA, 109(Suppl. 1), 10647–10654. A good discussion of withinand between-species variability in cortical evolution. Reiner, A. (1990). (See References). A clear and thoughtful criticism of MacLean’s 1990 book on the triune brain. Acknowledgments J. Cesario and H. L. Eisthen conceived the idea for the manuscript. All authors contributed to writing and revising the manuscript, and all approved the final version of the manuscript for submission. We thank Asifa Majid and several anonymous reviewers whose comments and suggestions greatly improved the article. Drawings of brain regions were modified from the open-source originals by Patrick J. Lynch, medical illustrator, and C. Carl Jaffe, cardiologist. ORCID iD Joseph Cesario https://orcid.org/0000-0002-1892-4485 Footnotes Declaration of Conflicting Interests The author(s) declared that there were no conflicts of interest with respect to the authorship or the publication of this article. Funding Preparation of this work was supported by the U.S. National Science Foundation under Grant Nos. BCS-1230281 and SES-1756092 to J. Cesario and IOS-1354089 and IOS-1655392 to H. L. Eisthen. 1. Hodos and Campbell’s (1969) admonitions could still apply today: “No teleost fish ever was an ancestor of any amphibian, reptile, bird, or mammal. . . . Thus, to say that amphibians represent a higher degree of evolutionary development than teleost fish is practically without meaning since they have each followed independent courses of evolution” (pp. 339–341). GO TO FOOTNOTE Transparency Action Editor: Randall W. Engle Editor: Randall W. Engle References Barron A. B., Klein C. (2016). What insects can tell us about the origins of consciousness. Proceedings of the National Academy of Sciences, USA, 113, 4900–4908. GO TO REFERENCE Crossref PubMed Google Scholar Berridge K. (2003). Comparing the emotional brains of humans and other animals. In Davidson R. J., Scherer K. R., Goldsmith H. H. (Eds.), Handbook of the affective sciences (pp. 25–51). Oxford, England: Oxford University Press. GO TO REFERENCE Google Scholar Chittka L., Niven J. (2009). Are bigger brains better? Current Biology, 19, R995–R1008. GO TO REFERENCE Crossref PubMed ISI Google Scholar Cory G. A. Jr. (2002). Algorithms of neural architecture, Hamilton’s rule, and the invisible hand of economics. In Cory G. A. Jr., Gardner R. Jr. (Eds.), The evolutionary neuroethology of Paul MacLean (pp. 345–382). Westport, CT: Praeger. GO TO REFERENCE Google Scholar Cory G. A. Jr., Gardner R. Jr. (Eds.). (2002). The evolutionary neuroethology of Paul MacLean. Westport, CT: Praeger. GO TO REFERENCE Google Scholar Daly M., Wilson M. (2005). Carpe diem: Adaptation and devaluing the future. The Quarterly Review of Biology, 80, 55–61. GO TO REFERENCE Crossref PubMed ISI Google Scholar Dijksterhuis A., Bargh J. A. (2001). The perception–behavior expressway: Automatic effects of social perception on social behavior. In Zanna M. P. (Ed.), Advances in experimental social psychology (Vol. 33, pp. 1–40). San Diego, CA: Academic Press. GO TO REFERENCE Crossref Google Scholar Dijksterhuis A., Bargh J. A., Miedema J. (2000). Of men and mackerels: Attention, subjective experience, and automatic social behavior. In Bless H., Forgas J. P. (Eds.), The message within: The role of subjective experience in social cognition and behavior (pp. 37–51). Philadelphia, PA: Taylor & Francis. GO TO REFERENCE Google Scholar Draper P., Harpending H. (1982). Father absence and reproductive strategy: An evolutionary perspective. Journal of Anthropological Research, 38, 255–273. GO TO REFERENCE Crossref ISI Google Scholar Dugas-Ford J., Rowell J. J., Ragsdale C. W. (2012). Cell-type homologies and the origins of the neocortex. Proceedings of the National Academy of Sciences, USA, 109, 16974–16979. GO TO REFERENCE Crossref PubMed Google Scholar Epstein S. (1994). Integration of the cognitive and the psychodynamic unconscious. American Psychologist, 49, 709–724. GO TO REFERENCE Crossref PubMed ISI Google Scholar Evans J. St. B. T. (2008). Dual-processing accounts of reasoning, judgment, and social cognition. Annual Review of Psychology, 59, 255–278. GO TO REFERENCE Crossref PubMed ISI Google Scholar Fawcett T. W., McNamara J. M., Houston A. I. (2012). When is it adaptive to be patient? A general framework for evaluating delayed rewards. Behavioural Processes, 89, 128–136. GO TO REFERENCE Crossref PubMed ISI Google Scholar Frankenhuis W. E., de Weerth C. (2013). Does early-life exposure to stress shape or impair cognition? Current Directions in Psychological Science, 22, 407–412. GO TO REFERENCE Crossref ISI Google Scholar Gintis H. (2007). A framework for the unification of the behavioral sciences. Behavioral and Brain Sciences, 30, 1–16. GO TO REFERENCE Crossref PubMed ISI Google Scholar Higgins E. T. (2004). The eighth koan of progress in social psychology: A variable anointed as “special” will demand special treatment. In Jost J. T., Banaji M. R., Prentice D. A. (Eds.), Perspectivism in social psychology: The yin and yang of scientific progress (pp. 305–317). Washington, DC: American Psychological Association. GO TO REFERENCE Crossref Google Scholar Hodos W., Campbell C. B. G. (1969). Scala naturae: Why there is no theory in comparative psychology. Psychological Review, 76, 337–350. Crossref ISI Google Scholar Johnson S. (2005). Mind wide open: Your brain and the neuroscience of everyday life. New York, NY: Scribner. GO TO REFERENCE Google Scholar Kidd C., Palmeri H., Aslin R. N. (2013). Rational snacking: Young children’s decision-making on the marshmallow task is moderated by beliefs about environmental reliability. Cognition, 12, 109–114. GO TO REFERENCE Crossref Google Scholar Kurzban R., Duckworth A., Kable J. W., Myers J. (2013). An opportunity cost model of subjective effort and task performance. Behavioral and Brain Sciences, 36, 661–679. GO TO REFERENCE Crossref PubMed ISI Google Scholar LeDoux J. E. (2012). Rethinking the emotional brain. Neuron, 73, 653–676. GO TO REFERENCE Crossref PubMed ISI Google Scholar MacLean P. D. (1949). Psychosomatic disease and the “visceral brain”: Recent developments bearing on the Papez theory of emotion. Psychosomatic Medicine, 11, 338–353. GO TO REFERENCE Crossref PubMed ISI Google Scholar MacLean P. D. (1964). Man and his animal brains. Modern Medicine, 32, 95–106. GO TO REFERENCE Google Scholar MacLean P. D. (1973). A triune concept of the brain and behavior. In Boag T. J., Campbell D. (Eds.), The Hincks memorial lectures (pp. 6–66). Toronto, Ontario, Canada: University of Toronto Press. GO TO REFERENCE Google Scholar MacLean P. D. (1990). The triune brain in evolution: Role in paleocerebral functions. New York, NY: Plenum. GO TO REFERENCE Google Scholar Mather J. A., Kuba M. J. (2013). The cephalopod specialties: Complex nervous system, learning, and cognition. Canadian Journal of Zoology, 91, 431–449. GO TO REFERENCE Crossref Google Scholar McGuire J. T., Kable J. W. (2013). Rational temporal predictions can underlie apparent failures to delay gratification. Psychological Review, 120, 395–410. GO TO REFERENCE Crossref PubMed Google Scholar Mirsky A. F., Duncan C. C. (2002). The triune brain and the functional analysis of attention. In Cory G. A Jr., Gardner R. Jr. (Eds.), The evolutionary neuroethology of Paul MacLean (pp. 215–230). Westport, CT: Praeger. GO TO REFERENCE Google Scholar Monterosso J. R., Luo S. (2010). An argument against dual valuation system competition: Cognitive capacities supporting future orientation mediate rather than compete with visceral motivations. Journal of Neuroscience, Psychology, and Economics, 3, 1–14. GO TO REFERENCE Crossref PubMed Google Scholar Myers D. G., Dewall C. N. (2018). Psychology (12th ed.). New York, NY: Worth Publishers. GO TO REFERENCE Google Scholar Narvaez D. (2008). Triune ethics: The neurobiological roots of our multiple moralities. New Ideas in Psychology, 26, 95–119. GO TO REFERENCE Crossref ISI Google Scholar Oakley T. H., Rivera A. S. (2008). Genomics and the evolutionary origins of nervous system complexity. Current Opinion in Genetics and Development, 18, 479–492. GO TO REFERENCE Crossref PubMed Google Scholar Passingham R. E., Smaers J. B. (2014). Is the prefrontal cortex especially enlarged in the human brain? Allometric relations and remapping factors. Brain, Behavior and Evolution, 84, 156–166. GO TO REFERENCE Crossref PubMed Google Scholar Reiner A. (1990). Review: An explanation of behavior. Science, 250, 303–305. Crossref PubMed Google Scholar Sagan C. (1978). The dragons of Eden: Speculations on the evolution of human intelligence. New York, NY: Ballantine. GO TO REFERENCE Google Scholar Sherwood C. C., Bauernfeind A. L., Bianchi S., Raghanti M. A., Hof P. R. (2012). Human brain evolution writ large and small. Progress in Brain Research, 195, 237–254. GO TO REFERENCE Crossref PubMed ISI Google Scholar Shoda Y., Mischel W., Peake P. K. (1990). Predicting adolescent cognitive and self-regulatory competencies from preschool delay of gratification: Identifying diagnostic conditions. Developmental Psychology, 26, 978–986. GO TO REFERENCE Crossref ISI Google Scholar Strausfeld N. J., Hansen L., Li Y., Gomez R. S., Ito K. (1998). Evolution, discovery, and interpretations of arthropod mushroom bodies. Learning & Memory, 5, 11–37. GO TO REFERENCE PubMed Google Scholar Striedter G. F. (1998). Progress in the study of brain evolution: From speculative theories to testable hypotheses. Anatomical Record, 253, 105–112. GO TO REFERENCE Crossref PubMed Google Scholar Teffer K., Semendeferi K. (2012). Human prefrontal cortex: Evolution, development, and pathology. Progress in Brain Research, 195, 191–218. GO TO REFERENCE Crossref PubMed ISI Google Scholar Tomasello M. (2009). The cultural origins of human cognition. Cambridge, MA: Harvard University. GO TO REFERENCE Crossref Google Scholar Tomer R., Denes A. S., Tessmar-Raible K., Arendt D. (2010). Profiling by image registration reveals common origin of annelid mushroom bodies and vertebrate pallium. Cell, 142, 800–809. GO TO REFERENCE Crossref PubMed Google Scholar Tooby J., Cosmides L. (2005). Conceptual foundations of evolutionary psychology. In Buss D. M. (Ed.), Handbook of evolutionary psychology (pp. 5–67). Hoboken, NJ: Wiley. GO TO REFERENCE Google Scholar Related content Similar articles: Restricted access The Neural Basis of Religious Cognition Show details Free access The Teenage Brain: Self Control Show details Restricted access Intuitive Prosociality Show details View more Sage recommends: SAGE Knowledge Book chapter Comparative Psychology Show details SAGE Knowledge Entry Play and Evolution Show details SAGE Knowledge Book chapter Human Motivation Show details View more Now Reading: Share PREVIOUS ARTICLE Do We Become More Prosocial as We Age, and if So, Why? NEXT ARTICLE Why Do Narcissists Care So Much About Intelligence? Also from Sage CQ LibraryElevating debate opens in new tab Sage DataUncovering insight opens in new tab Sage Business CasesShaping futures opens in new tab Sage CampusUnleashing potential opens in new tab Sage KnowledgeMultimedia learning resources opens in new tab Sage Research MethodsSupercharging research opens in new tab Sage VideoStreaming knowledge opens in new tab Technology from SageLibrary digital services opens in new tab About About Sage Journals Accessibility guide Historical content Advertising disclaimer Permissions Terms of use Sage discipline hubs Sage microsites Information for Authors Editors Librarians Promoters / Advertisers Researchers Reviewers Societies Frequently asked questions Current Directions in Psychological Science ISSN: 0963-7214 Online ISSN: 1467-8721 About SageContact usCCPA - Do not sell my personal informationCCPA Privacy Policy Copyright © 2023 by Association for Psychological Science",
    "commentLink": "https://news.ycombinator.com/item?id=37555118",
    "commentBody": "The brain is not an onion with a tiny reptile inside (2020)Hacker NewspastloginThe brain is not an onion with a tiny reptile inside (2020) (sagepub.com) 283 points by optimalsolver 22 hours ago| hidepastfavorite146 comments crazygringo 17 hours agoAfter reading this article as well as the relevant Wikipedia entry [1], I still don&#x27;t get what&#x27;s supposedly \"wrong\" with the triune brain model.In fact, this article seems to set it up as a bit of a straw man. The main rebuttals in this article are 1) that evolution is branched rather than linear, 2) that larger brains aren&#x27;t necessarily more complex, and 3) that evolution modifies existing brain structures in addition to adding new layers.But all of that seems rather obvious, and doesn&#x27;t really refute the triune brain theory at all.Isn&#x27;t it scientifically true that we have a basal ganglia which evolved from reptiles, a limbic system also present mammals, and a neocortex that works similarly to that in other primates, dolphins, and elephants? And each of those map to certain types of behaviors, that we see in these species?The triune brain hypothesis doesn&#x27;t seem \"wrong\", it just seems like a simple categorization that is useful for making big-picture distinctions.Am I missing something? I literally don&#x27;t understand what is even being \"refuted\" here, because the refutations don&#x27;t seem to match the claims at all.[1] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Triune_brain reply xg15 17 hours agoparent> and 3) that evolution modifies existing brain structures in addition to adding new layers.As I understood it, their point was that evolution does not add new layers, evolution of the brain always happens by modifying the existing structure.Which is why a \"stratified\" view of the brain with evolutionary older layers near the center and newer layers near the surface is incorrect.The implications of this belief then led to incorrect assumptions about both humans and (non-human) animals: That the human brain is an \"animal brain plus something else\" and therefore automatically superior - and inversely that animal brains are \"human brains minus something\" and therefore automatically inferior. The article argues against both positions. reply crazygringo 16 hours agorootparentIs the neocortex not an outer layer? That is not present in reptiles? So how is the stratified model incorrect as a high-level structural categorization?The triune model doesn&#x27;t claim our brain grows additively in rings like trees. It&#x27;s merely an observation about the primary evolutionary origins of the three parts. Just those three.And the superior&#x2F;inferior characterization is not part of the triune model. You are free to interpret it that way if you want, but it&#x27;s not part of it, so it&#x27;s not a rebuttal. reply xg15 16 hours agorootparent> Is the neocortex not an outer layer? That is not present in reptiles? So how is the stratified model incorrect as a high-level structural categorization?Not sure about reptiles, but the author write this about mammals:> Neurobiologists do not debate whether any cortical regions are evolutionarily newer in some mammals than others. To be clear, even the prefrontal cortex, a region associated with reason and action planning, is not a uniquely human structure. Although there is debate concerning the relative size of the prefrontal cortex in humans compared with nonhuman animals (Passingham & Smaers, 2014; Sherwood, Bauernfeind, Bianchi, Raghanti, & Hof, 2012; Teffer & Semendeferi, 2012), all mammals have a prefrontal cortex.I also read some interesting papers a few years ago about corvids - particular New Caledonian Crows: Those animals do not have a neocortex and hence were thought incapable of many higher-level cognitive tasks, such as planning, tool use, etc. Turned out the crows were in fact capable of them. One hypothesis I read suggested that, as crow brains are structured differently, another structure may have taken the role that the neocortex has in humans.So even if it&#x27;s an anatomical distinction, it&#x27;s an unreliable indicator of mental capacity. reply crazygringo 15 hours agorootparentOK, but again -- none of that refutes the triune brain hypothesis at all.Literally nobody is claiming that only humans have neocortexes.Nor is anyone claiming it&#x27;s an indicator of mental capacity. The recent discoveries about crows were interesting, but that doesn&#x27;t have anything to do with the fact that the neocortex in mammals plays a particular, functional, well-recognized role. reply makeitdouble 10 hours agorootparentFrom the wikipedia:> The triune brain consists of the reptilian complex (basal ganglia), the paleomammalian complex (limbic system), and the neomammalian complex (neocortex), viewed each as independently conscious, and as structures sequentially added to the forebrain in the course of evolution. According to the model, the basal ganglia are in charge of our primal instincts, the limbic system is in charge of our emotions and the neocortex is responsible for objective or rational thoughts.We disproved:- the sequentially added nature- what these zones are responsible forSo in the end we&#x27;re only left with a semi-arbitrary split of the brain in 3 zones and none of the hypothised implications. That to me refutes the hypothesis as a whole, and we&#x27;re keeping the zone naming by sheer inertia. reply theptip 13 hours agorootparentprevThe refutation is of the idea that it’s a strictly chronological layering, with the old layers inside and intact.The correct view is that while the neocortex is indeed mostly new, the “older” more central structures were modified throughout evolution. reply JohnAaronNelson 13 hours agorootparentNo one thinks the older structures are static. No one is arguing that. It&#x27;s a simplified model about the origin.This argument is akin to saying we&#x27;re not \"newer\" apes ala> The refutation is of the idea that it’s a strictly chronological ordering of species, with the old species still inside and intact. The correct view is that while homosapiens are indeed mostly \"newer\", the “older” apes were also modified throughout evolutionObviously. reply Retric 8 hours agorootparentIt’s an incorrect and misleading model about the origin. Bat wings and bird wings share many traits but the common answer didn’t fly it’s simply convergent evolution. The brain structures of that same common ancestor is only vaguely related to modern structures in mammals and reptiles.Modern reptiles, birds, and humans have experienced the same amount of evolution. There’s similar traits, but we don’t call whale flippers hands because that’s what the structures was in some of their ancestors or even what it is in related species. Similarly we don’t call human hands flippers because that’s what the structure started out as even further back and it’s still preforming that function in modern fish.The human visual cortex is larger than most creatures entire brain, it’s a wildly different structure than you find on a frog which plays a wider role in cognition than just decoding vision. reply evv555 6 hours agorootparentPutting this particular theory aside the generalization that evolution is a nested hierarchy of structures is pretty obvious on the macroscopic level. Traits like breathing oxygen, having symmetrical bodies, backbones, limbs. These are all examples of structures building on top of one another sequentially and conserved across species. Sure each of these subcomponents continues to specialize but the core functionality remains largely the same.Going back to your analogy saying visual cortex of frog is wildly different from a human visual cortex. That&#x27;s similar to saying a frog backbone is wildly different from a human backbone. Sure that&#x27;s true but there&#x27;s also a shared common core functionality largely conserved across time reply Retric 2 hours agorootparentAgain the issue is the theory is misleading. Those traits are conserved through evolution largely because they’re useful, but if you look at say owl skulls you find wild asymmetries because that’s more useful even our lungs are very different from each other. Hagfish lost backbones and snakes lost limbs because nothing is sacred to evolution.So yes frogs and humans both have a visual cortex, because we both have eyes. It really doesn’t explain anything beyond that point. The human visual cortex doesn’t even map to the same structures as it expanded into nearby ones. reply aeternum 7 hours agorootparentprevBut you could say that whale flippers are &#x27;basically&#x27; their hands.Every scientific model and analogy is flawed in some way, but many are still useful. reply Retric 2 hours agorootparentHow exactly is this theory useful? Yes, frogs and humans process visual information because they both have eyes. That’s about all those structures have in common. reply theptip 13 hours agorootparentprevI mean… the quote from TFA that they are arguing against is> As Paul MacLean (1964), originator of the triune-brain theory, stated,>> man, it appears, has inherited essentially three brains. Frugal Nature in developing her paragon threw nothing away. The oldest of his brains is basically reptilian; the second has been inherited from lower mammals; and the third and newest brain is a late mammalian development which reaches a pinnacle in man and gives him his unique power of symbolic language.And they quote other textbooks that are making claims along these lines too; this is right at the beginning of TFA. So I think you are wrong that “no one thinks that”.Of course they don’t think the old brains are 100% static but there are claims that they are largely conserved. reply tigen 8 hours agorootparentMacLean doesn&#x27;t say the brain is an onion with a tiny reptile inside! reply jacquesm 11 hours agorootparentprev> there are claims that they are largely conserved.That they&#x27;re not static doesn&#x27;t necessarily mean that they aren&#x27;t largely conserved. replylukeasrodgers 13 hours agorootparentprev- The superior&#x2F;inferior characterize actually is part of the triune model, Maclean&#x27;s book is replete with language like \"advanced\" vs \"primitive\".- The point of the criticism is not that the neocortex is not a \"layer\" at all, but that it is not the case that if you were to remove the neocortex layer, you would essentially get the brain of a lower-order animal--but this is what is implied by the triune theory. reply shevis 16 hours agorootparentprev> Is the neocortex not an outer layer?It seems like this is what they are refuting. It is not so much a new layer as it is an evolution of existing structure. reply crazygringo 15 hours agorootparentOf course it&#x27;s a layer. It&#x27;s a layer that itself is made up of 6 sublayers. This is not up for debate:https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;NeocortexIt clearly states:> The six-layer cortex appears to be a distinguishing feature of mammals; it has been found in the brains of all mammals, but not in any other animals.What it evolved out of is entirely irrelevant -- everything evolves out of something else in some fashion. reply civilitty 14 hours agorootparentWhile the neocortex does have distinct layers, the neocortex itself is not “layered” around the rest of the brain - it’s deeply integrated all over the place with the rest of the brain and nervous system. There is no hierarchical relationship between the neocortex and different systems it integrates with.The reptilian equivalent to the neocortex is the dorsal ventricular ridge which evolved separately and in parallel. This presents two problems to the hypothesis: first the much simpler DVR serves much of the same purpose as the neocortex which was completely unknown at the time and second the most interesting bird species (the smartest ones) often don’t have an equivalent structure at all. There isn’t even a clear relationship between intelligence, complexity, evolutionary age, etc. After 250 million years of evolution any similarities are accidents of random convergence. reply Terr_ 16 hours agorootparentprevAlso, it&#x27;s not clear why we should accept that the brain would develop with locked-in \"old strata\" to a degree that we do not see in all sorts of other organs and systems.As much as people joke about having a separate stomach for ice-cream, I&#x27;ve never heard anyone suggest that their \"lizard stomach\" would handle certain foods. reply dahfizz 15 hours agorootparentBecause the brain physically, anatomically, has these \"old strata\". Lizard brains are pretty much just a basal ganglia. Humans have a basal ganglia, and then extra stuff on top. Mammals, and only mammals, have a neocortex \"strata\" to their brain as well.I am not qualified to argue for or against the Triune brain, but it seems easy to see why the brain is different from the stomach in this regard. reply Terr_ 12 hours agorootparent> Because the brain physically, anatomically, has these \"old strata\". [...] Humans have a basal ganglia, and then extra stuff on top.IMO the key is distinguishing between these two ideas:1. There is a gross anatomical structure that can be linked to ancient ancestors with certain characteristics.2. Those structures in living creatures are somehow \"not really modern\" or are unusually tied to the needs or limitations of those ancient ancestors.Consider your fingers: They originate from fin-bones ~380 million years ago, yet (unlike \"lizard brain\") nobody talks about possessing \"fish fingers\" except as a fried food product. We also don&#x27;t create narratives explaining our finger operations or design in terms of what ancient fish required or were capable of. reply samplatt 6 hours agorootparentThe Pterodactylus flying lizards are named literally for their \"winged fingers\". I suspect that we talk about this stuff when discussing evolutionary biology just fine, it&#x27;s just that the \"lizard brain\" is a popsci term that the great-unwashed have latched onto. reply snek_case 14 hours agorootparentprevIn addition to this... Sure, evolution modifies existing structures, but if you compare a cat&#x27;s heart and stomach to a pig heart and stomach, the difference is not that big, even though the nearest evolutionary ancestor was tens (hundreds?) of millions of years ago. Once a structure is in place and works well, you can certainly tweak it, but it&#x27;s easier to mostly just keep it.Humans have a neocortex, but if you play with a cat or dog, you can recognize and understand the emotions they are feeling. Fear, anger, joy, anxiety, relaxation, etc. That suggests the structures responsible for those things in us are probably not that different from them. reply tsimionescu 3 hours agorootparent> Humans have a neocortex, but if you play with a cat or dog, you can recognize and understand the emotions they are feeling. Fear, anger, joy, anxiety, relaxation, etc. That suggests the structures responsible for those things in us are probably not that different from them.And yet birds don&#x27;t have a neocortex, and yet they share all the same behaviors that mammals share (at least in some species). I don&#x27;t know if there is some creature with a neocortex that doesn&#x27;t share all of these behaviors, but at least we can see that the neocortex may be sufficient but is not required for a species to have them. reply civilitty 14 hours agorootparentprevAgreed. Mammals don’t even have the same metabolic strategy as reptiles - we’re endothermic while reptiles can’t even self regulate body temperature. Other systems fundamental to life like our reproductive strategies are also completely different.The idea that major organs are strictly conserved over 250 million years while something as fundamental as homeostasis drastically diverges is frankly a bit wackadoo. reply timmaxw 8 hours agorootparentprev> evolution of the brain always happens by modifying the existing structureHmmm, my impression was that evolution took an existing structure and stuffed a bunch of new functionality into it, making it much larger and more complicated. Although the structure already existed in some form, it didn&#x27;t have the same function. I think it&#x27;s reasonable to think of this process as \"adding\" something, even if the structure itself is not entirely new. And human brains pretty clearly _are_ more intelligent than reptile brains!Here&#x27;s an analogy: Ancient organisms had light-sensitive \"eyespots\", and many of our distant cousins on the evolutionary tree (e.g. present-day flatworms) still have eyespots. Evolution gradually modified these existing structures into eyes. I think it&#x27;s reasonable to think of eyes as a \"new\" organ that humans have and flatworms don&#x27;t; and our eyes are clearly superior to flatworms&#x27; eyespots. reply overgard 8 hours agorootparentEvolution and adding complexity aren&#x27;t the same thing. For example, evolution can also work in reducing complexity. Animals that spend generations living underground in caves will lose their eye sight over a long enough time period.Point being its not about adding, it&#x27;s about adapting reply tsimionescu 3 hours agorootparentprevEyes are far superior to eyespots in certain ways (accuracy, for one), but far inferior in others (energy consumption, size, durability). They are newer than the eyespots of the last common ancestor, but they could well be older than the eyespots of some flatworm species alive today. reply wheelerof4te 11 hours agorootparentprev\"As I understood it, their point was that evolution does not add new layers, evolution of the brain always happens by modifying the existing structure.\"The human brain is a highly complex set of distinctive organs. Saying there are different types of brains forming the human nervous system is literally what the biology teaches us. reply hasmanean 17 hours agoparentprevHumans have this ability to obfuscate any issue. When there is a simple pattern for things some people always take edge cases and argue that the pattern is really not true. They don’t understand how models of the world work. Of course it’s not ideal but it’s a useful framework for understanding things.I imagine the reason Newton’s laws never developed before was because of all the influential know-it-alls who took empirical data (moving objects stop! Gaseous balloons rise!) and drew the wrong conclusions from it (gases want to rise up to be with all the other gases…solids want to be at rest.)It took Newton to do a thought experiment of a projectile moving in outer space to deduce his laws of motion.The fact is that newtons laws aren’t really observable on earth, unless you have the imagination to see it and do the mental bookkeeping of accounting for friction as a separate force. reply xzsinu 16 hours agorootparentThe triune model of the brain is not just a simplification, but one that promotes antiquated biases about human intelligence in how human intelligence differs from non-human intelligence, how intelligence is distributed among humans themselves, and what is essential to defining human intelligence itself.The lizard, small mammal, human distinction maps pretty deceptively onto Aristotle&#x27;s distinctions between the souls: vegetative (plant), sensitive (animal), and rational (human). So if one is trying to pinpoint the seat of intelligence, it seems to follow that we can ignore the two lower sections of the brain in favor of the higher one. Franz Joseph Gall, the founder of phrenology, himself did that, writing off the cerebellum as relevant only for producing the sexual drive [1].Scientific theories of self-control which were nothing more than Christian dualist arguments evolved out of Gall&#x27;s work and argued that intelligence involved suppression of the lower faculties, which provided cover for eugenicist and supremacist arguments throughout the 20th century and still shows up today in popular theories about how the &#x27;limbic system&#x27; subverts the rational capacities of individuals and is used to manipulate the masses (Elon loves this theory).Current work funded at the intersection of artificial intelligence and neuroscience still prioritizes the neocortex as the seat of rationality, with some like Jeff Hawkins (Palm founder turned brain scientist) arguing that \"intelligence is an algorithm found in the neocortex\". Singularity arguments rely in part on the assumption that intelligence in humans is mostly limited by the other parts of the brain, not empowered by them, and that a form of intelligence freed of embodiment will inevitably exterminate those that are embodied by right.The truth is, neglected sub-regions such as the \"lizard\" cerebellum actually contain the vast majority of neurons, have been shown to have evolved disproportionately larger within early hominins [2], and are theorized to be equally involved in abstract cognition as in bodily manipulation [3]. This is something of a paradigm shift that has only been able to take shape since the late 20th-century (through the work of Jeremy Schmahmann, Peter Strick and others[4]), even though hints of it have been present in the data since it was collected, and that&#x27;s because of how compelling the triune brain model has been. Research in this direction can directly address mental illnesses such as schizophrenia, but it has to be funded first [5].[1] https:&#x2F;&#x2F;www.frontiersin.org&#x2F;articles&#x2F;10.3389&#x2F;fnana.2019.0004...[2] https:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;25283776&#x2F;[3] https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;abs&#x2F;pii&#x2F;S03043...[4] https:&#x2F;&#x2F;www.sciencedirect.com&#x2F;science&#x2F;article&#x2F;pii&#x2F;S089662731...[5] https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=5UUqKuhvTk0 reply feoren 13 hours agorootparentYour argument seems to mostly rest on the idea that people can \"poison\" a fundamental idea by misinterpreting it and drawing silly conclusions from it. It sounds like if I argued that \"1 + 1 = 2 and therefore we should do genocide\", you&#x27;d be (rightly) abhorred by the conclusion, and the next time you saw someone using 1 + 1 = 2 as the basis for a completely different argument, you&#x27;d villainize them as using an argument that \"promotes genocide\" or \"has been used to justify genocide\". I really don&#x27;t care what the founder of phrenology thought, nor Christian dualists, nor even Jeff Hawkins.In general I think this effect contributes to a lot of \"over-debunking\". We see way over-simplified, yet very loosely accurate, mid 20th century scientific models like the triune brain, \"ontogeny recapitulates phylogeny\", the left-brain vs. right-brain, and the idea that differences in language contribute to differences in cognition; and then silly people take these models way too far and use them to justify dubious things; and then they become over-debunked to the point that speaking them aloud immediately ostracizes you as some outdated bigot; while the whole time the models themselves have been reasonably OK high-level starting points for discussion that obviously need revision for any lower-level details.> The truth is, neglected sub-regions such as the \"lizard\" cerebellum actually contain the vast majority of neurons, have been shown to have evolved disproportionately larger within early hominins [2], and are theorized to be equally involved in abstract cognition as in bodily manipulation.The relative number of neurons is not evidence for or against the model, nor the fact that they were larger in early hominins. Showing their involvement in abstract cognition is more interesting, but that&#x27;s only evidence against the triune brain if you make the exact same mistake that you&#x27;re criticizing, which is assuming that \"abstract cognition\" is some high-level uniquely human (or primate) trait. If that exact \"abstract cognition\" also exists in reptiles and birds (and it appears to), then the fact that the cerebellum contributes to that cognition is not evidence against Triune Brain. reply RugnirViking 50 minutes agorootparent> We see way over-simplified, yet very loosely accurate, mid 20th century scientific models like the...I think its important for people to also spread the idea that \"this is known to not be correct\". Bad mental models continue to proliferate for generations (bad here not being a value judgement but rather of their known incorrectness and lack of predictive power).It&#x27;s like when you tell people that the alpha&#x2F;beta dynamic amongst wolves came from one flawed study that has not been replicated and shown to be false many many years ago now. Same with \"learning styles\" research. Its difficult to approach the subject in a way that doesn&#x27;t cause people to get defensive sometimes - they like the simplistic model which may once have been a fine point for a beginner but they got stuck in it and it can impair their growth and understanding.It&#x27;s fine for a layperson to walk around believing whatever they like - its likely closer to the truth than whatever they might otherwise have thought, or at least got them thinking. It becomes a problem when people actually base decisions off these things. reply EricMausler 16 hours agorootparentprevI&#x27;m not sure which side you are arguing for?Is the simple metaphor of 3 layers in the brain equivalent to saying gasses want to be together and solids want to rest?I think part of the debate in the comment section is on what kind of order &#x2F; pattern we are trying to capture with the analogy. Does it make more sense to be analogous to the structural observations, or a more functional equivalency?You could say an ocean is like a desert in that they are vast and empty with respect to surface structures observed by a human traveller, but obviously from a functional &#x2F;environmental perspective the two almost couldn&#x27;t be more dissimilar reply mistermann 14 hours agorootparentprev> They don’t understand how models of the world work. Of course it’s not ideal but it’s a useful framework for understanding things.Based on the rather casual way the author is using language in this piece, I&#x27;d bet that the researchers forgot that what they are describing are (nested) model(s) of reality...or that that level of precision is \"pedantic\" (the consequence being the confusion in this comment section). reply lolinder 5 hours agoparentprev> we have a basal ganglia which evolved from reptilesI took this as the key point addressed in the introduction—this model of evolution is widely understood to be wrong. We did not evolve from reptiles, we evolved alongside reptiles from a common ancestor. Modern reptile brains have also changed in form and function since our common ancestor, and we don&#x27;t have whole divisions of the brain that exist in humans but not in reptiles.Essentially, the triune brain theory goes right alongside that classic image of the monkey evolving into a man. It&#x27;s not actually how evolution works, but it&#x27;s a catchy simplification that&#x27;s hard to get rid of.From the caption to the first diagram:> The evolutionary tree (c) illustrates the correct view that animals do not linearly increase in complexity but evolve from common ancestors. The corresponding view of brain evolution (d) illustrates that all vertebrates possess the same basic brain regions, here divided into the forebrain, midbrain, and hindbrain. Coloring is arbitrary but illustrates that the same brain regions evolve in form; large divisions have not been added over the course of vertibrate evolution. reply joveian 5 hours agoparentprevIf you want a detailed argument see Chapter 2 of Pierre Gloor&#x27;s \"The Temporal Lobe and Limbic System\" where he argues against the (unfortunately still common) paleo&#x2F;archi&#x2F;neocortex terminology in favor if the allo&#x2F;meso&#x2F;isocortex terminology in the context of describing comparative anatomy. It is out of print but the Internet Archive has it available to borrow:https:&#x2F;&#x2F;archive.org&#x2F;details&#x2F;temporallobelimb0000gloo&#x2F;mode&#x2F;2u...Your \"isn&#x27;t it true\" paragraph is all incorrect (unless it is considered in a uselessly general way). For instance, Gloor notes that all mammals have an isocortex (that seems to have started in olfactory cortex). The \"reptiles\" that humans evolved from is just a catch all terminology for a diverse group of life and not closely related to current reptiles. He starts the chapter noting that ancestral brains are not available for direct study. reply achrono 14 hours agoparentprevNo, your self-appellation aside, you are not missing anything unless there is some really secret &#x27;nuance&#x27; hidden in the paper.If the triune brain model was completely false (as stated by the caricature of \"brain is not an onion with a tiny reptile\"), it would not be straightforward to even identify the neocortex -- how do you know it is &#x27;neo&#x27;, what is it a &#x27;neo&#x27; of and so on?So the fact that we can meaningfully talk about these areas of the brain suggests that there is in fact continuity and building-upon happening in our evolutionary journey, although of course it&#x27;s not like the cartoon they show (which I have never seen before from anyone seriously talking about this topic, I might add). reply lolinder 5 hours agorootparent> If the triune brain model was completely false (as stated by the caricature of \"brain is not an onion with a tiny reptile\"), it would not be straightforward to even identify the neocortex -- how do you know it is &#x27;neo&#x27;, what is it a &#x27;neo&#x27; of and so on?This is a sort of proof-by-etymology and it isn&#x27;t very persuasive. We have all kinds of words that derive from misunderstandings that have stuck, even long after the original misunderstanding was resolved. The geographical equivalent of this would be to say \"we call them Indians, so they must have originated from India at some point!\"It&#x27;s entirely possible that as our understanding of the brain evolves, this layered-over-time model will be seen as a quaint misunderstanding, but we&#x27;ll still use the original names for the sub-organs because there&#x27;s too much inertia to change them. reply csours 17 hours agoparentprev> \"The triune brain hypothesis doesn&#x27;t seem \"wrong\", it just seems like a simple categorization that is useful to the layperson.\"Yes. Anything this simple will be wrong. Almost everything you learn in school before graduate level courses will be wrong. Most of it won&#x27;t matter to you unless you start working in that field. reply nuancebydefault 15 hours agorootparent[1] is a nice book that explains in the first chapter why all science is wrong and gets replaced by a less wrong model, in steps. In fact the author argues that all those wrong models are perfectly fine and usable, for their period of time and applications.[1]https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Reality_Is_Not_What_It_Seems reply dragonwriter 13 hours agorootparentprev> Almost everything you learn in school before graduate level courses will be wrong.From what I understand, in many fields its pretty clear that most of the stuff in graduate courses is, too, its just that then next step toward right is less clear and more disputed than for the earlier wrong stuff. reply KMag 15 hours agorootparentprev\"All models are wrong, but some are useful\" - George Box reply civilized 16 hours agoparentprevLooking at this as an amateur, it seems like the key question here is \"does the human brain have substructures that are similar enough to brain structures from ancestral species that it makes sense to consider them as the &#x27;same&#x27; entity with the same name?\" reply ragtagtag 3 hours agoparentprevThe title definitely seems like a strawman, but it is the best use of a strawman that I&#x27;ve seen in an academic article: an outrageous and amusing title that makes me want to read the paper!(The rest of the article seems much more defensible to me: well-written without quite verging into strawman territory. But I&#x27;m no expert in the field.) reply tiberious726 15 hours agoparentprevIt&#x27;s not TFA&#x27;s actual thesis, but here&#x27;s a _much_ more powerful rebuttal of the triune brain and similar mental models if you&#x27;re interested (from the perspective of what is the sheer idea of rationality) Matthew Boyle&#x27;s \"Tack-on Theories of Rationality\": https:&#x2F;&#x2F;dash.harvard.edu&#x2F;bitstream&#x2F;handle&#x2F;1&#x2F;8641840&#x2F;Additive... reply epgui 9 hours agoparentprevThis article is simply based on an uncharitable interpretation of the triune brain model. Literally nobody I know in the field believes in the cartoon-like description given by the article, and none of my introductory textbooks (I have quite a few which touch on this, from embryology to biopsychology to neuroanatomy) explain it in such a silly way as in the article. reply cosmojg 14 hours agoparentprev> But all of that seems rather obvious, and doesn&#x27;t really refute the triune brain theory at all.> The triune brain hypothesis doesn&#x27;t seem \"wrong\", it just seems like a simple categorization that is useful for making big-picture distinctions.Right, but having worked in the field, I can assure you that there are, in fact, too many practicing psychologists, cognitive scientists, and even neuroscientists who believe the triune model of the brain to be literally and, sometimes, absolutely true. These are the types of people whom the linked paper is trying to reach.Misunderstandings based on these oversimplified models are driving the current debate around modular versus distributed computation in the brain[1]. Obviously, a more accurate model of the brain would account for both ideas, but there is growing concern in the neuroscientific community over the amount of grant money going toward defending older, dead-end modular models instead of improving newer, more promising distributed models, mostly as a result of entrenched interests prioritizing the maintenance of prestige over the pursuit of truth.In short, putting bad models on blast is good and necessary for the advancement of science. You can get a lot done with the plum pudding model[2] of the atom, but you can get far more done with the Bohr model[3] which emerged only after Rutherford, Bohr, and several other physicists published several iterative takedowns of the former, and yes, they too had to deal with entrenched interests who operated under the assumption that the plum pudding model was literally and absolutely true. It took a decade of experiments and several increasingly correct models before academic consensus shifted enough to accept the existence of subatomic particles and academic consensus began its collective investment in quantum mechanics. We&#x27;re now in a similar place with neuroscience in the tension between modular computational models, which includes the triune brain model, and distributed computational models, which are showing promise in rescuing fMRI studies with their strong modular tradition from the replication crisis[4].[1] https:&#x2F;&#x2F;www.cell.com&#x2F;trends&#x2F;cognitive-sciences&#x2F;fulltext&#x2F;S136...[2] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Plum_pudding_model[3] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bohr_model[4] https:&#x2F;&#x2F;www.ncbi.nlm.nih.gov&#x2F;pmc&#x2F;articles&#x2F;PMC5457304&#x2F; reply JohnAaronNelson 12 hours agorootparentThank you for your contributions. Am I correct that you are asserting there are substantial numbers of practicing psychologists that literally and absolutely believe inner structures of the brain are unchanged over hundreds of millions of years, and they can and will be reached by this paper?If anyone doesn&#x27;t understand \"All models are wrong, but some are useful\" I don&#x27;t know if you&#x27;ll reach them with this paper.Maybe it needs to be said. It&#x27;s possible the most useful papers are those that assert obvious things in ways that refute our basic models so we can see things differently. It&#x27;s also possible this is a clickbait paper that isn&#x27;t saying anything new, just trying to be controversial. reply rexpop 17 hours agoparentprev> we have a basal ganglia which evolved from reptilesAnd do living reptiles, today, not also \"have a basal ganglia which evolved from reptiles?\" seems we&#x27;ve a name collision, here. Perhaps we should refer to our ancestors as \"proto-reptiles,\" or else our contemporaneous cousins as \"post-reptiles\" whose brains have had just as many years&#x27; time to depart from our common reptilique ancestor. reply nextaccountic 16 hours agorootparentWe&#x27;re reptiles (as are all mammals, all birds, etc).My understanding of that is that all reptiles have a basal ganglia, because it was inherited from the common ancestor of reptiles.And non-reptiles don&#x27;t have a basal ganglia because their ancestors didn&#x27;t have one. reply dillydogg 16 hours agorootparentWhat? Mammals appear in a separate branch of amniotes apart from Reptiles&#x2F;Birds&#x2F;CrocodiliansMammals and reptiles share a common ancestor with an ancient amniote, not a reptile reply gowld 14 hours agorootparenthttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Synapsid> Synapsids[a] are one of the two major clades of vertebrate animals that evolved from basal amniotes, the other being the sauropsids, which include reptiles (turtles, crocodilians and lepidosaurs) and birds.> the only extant group that survived into the Cenozoic are the mammals.> The animals (basal amniotes) from which non-mammalian synapsids evolved were traditionally called \"reptiles\".> It is now known that all extant animals traditionally called \"reptiles\" are more closely related to each other than to synapsids, so the word \"reptile\" has been re-defined to mean only members of Sauropsida (bird-line Amniota) or even just an under-clade thereof replyrobg 19 hours agoprevEspecially since the master reptile - the adrenal cortex - is located atop the kidneys. Fight or flight is first an electrical relay to the hands and feet and heart. The brain in the skull and consciousness reflects upon what’s already happening in the periphery. reply ozim 17 hours agoparentIt is also super important for everyone to understand that. Because our body is not \"brain -> body control\" most of stuff just happens and brain reacts.For overall health it is also important to understand that body needs movement and all neural pathways are also somewhat independent and also contain \"intelligence\".I am not neurobiologist so hope I am not going into mumbo-jumbo too much but I workout at the gym quite often and can observe over-training or how muscles often could still work but your neural pathways are done and you cannot hold the weight even if muscle&#x2F;tendons feel quite fine on its own.It is also quite common knowledge as I read on the internet stuff on training.So in the end I don&#x27;t feel body-mind separation is useful as much and thinking that your whole body is also ones mind is super important. reply Cpoll 17 hours agoparentprev> The brain in the skull and consciousness reflects upon what’s already happening in the periphery.I&#x27;ve read summaries of these studies as well, but... the adrenal cortex doesn&#x27;t have any sensory-processing facilities, right? In the end it&#x27;s the brain that informs the adrenal gland? reply user3939382 19 hours agoprevIt may not be true from the perspective of evolutionary biology. But 90% of the time I&#x27;ve heard this referenced (our \"primitive brain\") it&#x27;s a useful device for discussing the parts of our psychology we have in common with more primitive members of the animal kingdom. That&#x27;s not to contradict the article whatsoever, which is explicitly addressed to psychologists who, I concur, should be educated and clear on the distinction between a rhetorical device and real biology. reply robg 19 hours agoparentThe autonomic nervous system is real biology that most psychologists don’t start with. While this article is based on textbook descriptions of the evolution of the brain in the skull, not seeing much on how the autonomic nervous system is primal specifically for survival. reply libraryatnight 19 hours agoparentprevI&#x27;d be more inclined to agree if it weren&#x27;t an increasing issue in my life that laymen do not understand they&#x27;re parroting back a rhetorical device and use this information to make all kinds of ridiculous conclusions. reply teucris 18 hours agorootparentIncreasing? This has always, and will always be an issue. My approach is to roll with it - rather than banish them altogether, I treat all devices like this as such and no more, holding others to the same standard. This, hopefully shifts the meaning away from the scientific and towards the domain of “quaint sayings.” For instance, I invoke the concept of the “lizard brain” quite commonly as a way of expressing how base instincts can override my better judgement. But I’m careful to never imply there (in the words of the article) “a scala naturae view of evolution in which animals can be arranged linearly from ‘simple’ to the most ‘complex’ organisms.” reply toomim 18 hours agorootparentprevLike what? reply cperciva 17 hours agoprevAll models are wrong, but some models are useful.Asking if the model is wrong is asking the wrong question; the important question is whether it&#x27;s useful. reply robertlagrant 16 hours agoparentIt&#x27;s useful for padding a chapter or two into self-help books, if nothing else. reply hindsightbias 10 hours agoparentprevIt’s not wisdom that comes with age, it’s the realization that reality is a broken record.Roll with it. reply fieldbob 16 hours agoparentprevDonald hoffman says something exactly like that in this talk. David Bohm came the same conclusion science will never fully figure it out the answer is much spiritual and imaginativehttps:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rafVevceWgs&t=5117s&pp=ygUVZ...Why? because ultimately we are talking about a empty field of space propagated by light, similar to holograms. Things that grow here things that stem from this world do you think they really are what they appear to be? Of course not. So what are they god damnit? perhaps its better to not answer the question and like a good old mystic leave it be and open a portal to another dimension and have this experience for one self reply mistermann 14 hours agorootparentThe errors&#x2F;issues here are much higher in the stack than what Hoffman and Bohm are getting that at the levels you note, though Bohm in addition to that also spoke a lot about language and communication (which is an important part of the issue here). reply sirsinsalot 1 hour agoprevAnd the weather isn&#x27;t discrete data points.We still use models based on this to illustrate, predict, and communicate.These things are a model. They&#x27;re useful for the bounds of the model. reply ivanhoe 18 hours agoprevSo now we&#x27;ve learned what brain is not - but what is the biologically&#x2F;evolutionary correct model that explains the opposite impulses that we all are dealing with? And why some of those impulses need willpower and grow weaker under the influence of stress&#x2F;alcohol&#x2F;drugs, while others seem to grow stronger? reply sdwr 17 hours agoparentThree-brain structure (Freud&#x27;s id, ego, superego) is still the best simple explanation I know. Maybe the physical reality is a bit different, but it partitions actions so nicely.Id - base impulses, bubble up automatically and subconsciouslyEgo - the self, the \"me\". Where the story of identity comes from, what gets judged in court.Superego - rules imposed from on high that restrict behavior-----I think \"opposite impulses\" can be explained as a form of self-control. Let&#x27;s say I see someone I&#x27;m attracted to, but want to maintain composure and stay in a neutral stance. Left to my own devices I&#x27;ll flush, and my eyes will widen, maybe I&#x27;ll get clumsy. Bringing an opposite-but-aligned emotion in maintains equilibrium (anger, disgust...)I see opposite-but-aligned impulses with my dog all the time. He knows I don&#x27;t like him chasing squirrels. When he sees one, he gets activated + alert, but redirects his energy into running a few paces the other way. It&#x27;s a bridge between the id (Go! Chase!) and the superego (Stay calm, don&#x27;t pull!) reply robertlagrant 16 hours agorootparentI never understood why the superego isn&#x27;t me. reply mrkeen 14 hours agorootparentYou don&#x27;t need to. &#x27;Superego&#x27; is an invention. You could invent your own abstract idea and call it you. Or you could simply declare that the superego is you. reply robertlagrant 13 hours agorootparentI mean in the construct of ego&#x2F;id&#x2F;superego, why is the \"ego\" me and not the \"superego\"? reply RugnirViking 40 minutes agorootparentnone of them alone is you. Thats kinda the point. You are all three. Maybe you were tempted to do something indulgent, but decided against it. At that point, the model would suggest that what you ended up doing (and therefore &#x27;you&#x27; were the superego).Another brain might have made a different choice. On a different day, with a little more stress and tiredness, you yourself might have made a different choice. reply sdwr 12 hours agorootparentprevSuperego is what you are supposed to do, id is what you want, and ego is where they meet in the middle.If you are identifying with the superego, maybe you are in a situation where you more \"have to\"s than \"want to\"s? reply arrosenberg 14 hours agorootparentprevIt probably makes sense to think about it as base-me and societally-influenced-me. reply csours 17 hours agoparentprevThe problem is that the world does not owe you simple explanations. reply dicroce 15 hours agoprevNot that the opinion of some random on the internet means anything, BUT....I think the neocortex for the most part lets other parts of the brain control themselves... and really only has high level access. I think emotions are one of these lower level semi autonomous subsystems... but so are things like autonomous body control, balance, each of the senses etc, etc... The neocortex is playing the \"Human\" video game and controlling things from a high level... but cannot directly control every aspect of these subsystems... and honestly, this is how complexity is dealt with (if the neocortex could control it all, the whats the point of the other subsystems)? reply lukeinator42 15 hours agoprevI think it&#x27;s easy to underestimate how different our brains are relative to different classes of animals, and there is a lot of convergent evolution going on.For example, even low level auditory perception, such as how the brain evolved to localize sound directions, evolved independently in mammals and birds: https:&#x2F;&#x2F;journals.physiology.org&#x2F;doi&#x2F;full&#x2F;10.1152&#x2F;physrev.000... (this is because their common ancestor didn&#x27;t have a tympanic ear).So the Triune brain really isn&#x27;t the best model for explaining what is going on in the brain. Models such as reinforcement learning don&#x27;t fully explain what is going on in the brain either, but I think explanations such as how dopamine flooding the reward system can mess with predicted rewards and contribute to addiction, etc. are more useful. reply anigbrowl 12 hours agoprevI have not read the paper yet, but what a great title - a very welcome change from the formulaic &#x27;verbing the noun: towards a metastatic model of semantic construction&#x27; that has become so widespread over the last couple of decades. reply derefr 19 hours agoprevI feel like the \"lizard brain\" thought-paradigm can actually be understood as communicating something true&#x2F;useful&#x2F;important... but it&#x27;ll only make sense to people with a good understanding of \"speciated evolution\": namely, evolutionary biologists themselves; and software engineers who&#x27;ve worked with programming languages that use prototypical inheritance. Outside of those two groups, the actual \"intuition\" for what the claim is saying, gets lost.The \"lizard brain\" claim, as far as I understand it, was never that you have a complete copy of a \"lower\" brain inside your brain. Nor even that you have specific structures within your brain whose implementation was evolutionarily conserved.Rather, what I understand the \"lizard brain\" claim as trying to communicate, is that you have one or more components of the architecture of your brain, where the APIs presented by those components to the rest of the brain, have been mostly conserved throughout evolution. The components themselves may have internally evolved, but the structural boundaries between those components and the rest of your brain have stayed stable in a way that allows biologists to recognize those same components in the architectures of brains in vastly different species.To put that in concrete terms: you and a mantis shrimp both have e.g. \"an amygdala.\" The gene code for \"an amygdala\" may have differentiated between the shrimp and you, but there&#x27;s still a conserved part of the brain&#x27;s architectural plan that says \"put an amygdala here.\"Now for the overwrought OOP analogy:If you imagine HumanBrain as an OOP class, then it&#x27;s an OOP class that is a subclass about 800 layers of inheritance deep; with the root of the inheritance hierarchy being some prototypical bilateral-vertebrate nerve-cord class.In this inheritance hierarchy, each layer can introduce new \"features\" — components of the brain that have specific APIs; in other words, members of the class with known interface types, that other parts of the class can have their implementations — but not their own APIs — altered to work in terms of.Under this mental [heh] model, the \"lizard brain\" claim isn&#x27;t about the LizardBrain level of the hierarchy itself; but rather is that one or more brain features seen in the HumanBrain class, are features that were introduced in or around the LizardBrain level of the inheritance hierarchy, and whose APIs have stayed stable ever since.(Also, if you&#x27;re wondering if any real-world computer software has ever done 800-layer-deep inheritance hierarchies such that it starts to actually reflect this kind of speciated evolution: yes! The programming of https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;LambdaMOO was exactly like that. Why? Because, unlike a regular codebase, but like evolution, LambdaMOO was a gradual accretion of private objects \"owned\" by amateur coders, where each dev would implement the features they cared about by finding someone else&#x27;s object with that feature, and forking it [i.e. prototype-inheriting from it] to suit their own needs. There was no common codebase that anyone could refactor, so it gradually came to resemble an actual biological process.) reply quickthrower2 17 hours agoparentI can’t help but think of Win32 APIs from what you describe. Or similarly that “teletype” is a thing in 2023. reply phito 5 hours agoparentprevThere is no \"architectural plan\" stored in the genome. reply mrangle 15 hours agoprevThe inarticulate abstract was a red flag. I read 2&#x2F;3 of the paper before giving up when finally, after untold quantity of adjectives, assertions, and halfway through, the supposed first couple of arguments made an appearance and were mostly indecipherable as such. The paper reads like what a high school junior thinks that a research review(?) is supposed to look like. Also, it is clear to me that the authors began with a conclusion around which they attempted to wrap support. The psych field needs low quality papers like Alaska needs snow. reply mrkeen 14 hours agoparentWhat was wrong with the abstract? * Widespread misconceptions: a) \"Newer\" brain structures were added on top of \"older\" brain structures. b) The \"newer\" brain structures provide more complex functions. * Psychologists have been publishing these in textbooks. * Neurobiologists have known these to be wrong for some time. reply mrangle 12 hours agorootparentThe core statement of the abstract is that the author&#x27;s conclusions are the \"clear an unanimous\" views of \"those studying nervous system evolution\".This statement is incorrect on its face. Worse, it would be a bad scientific statement in any paper. Last, it is at once superfluous and incomplete even if it were true. Instead, a good abstract would briefly list the points of the author&#x27;s preferred theory rather than waste the reader&#x27;s time by only deferring to a loosely defined general authority in support of an assertion. Which is abstract writing 101, at least in fields for which writing standards are somewhat kept. Maybe that isn&#x27;t the psychology field, but it should be.The last sentence of the abstract reads as if written by said high school junior. Science authors generally don&#x27;t use the word \"mistaken\" to describe another theory, especially in the context of theories of brain evolution nor for anything else for which literally every theory can only be a theory. Including that preferred by the author.Good papers don&#x27;t introduce their arguments &#x2F; pov halfway through, or further. Good papers don&#x27;t lean on deference to authority 1&#x2F;2 paper before presenting what is supposed to be their evidence or theory. Good papers almost completely exclude adjectives when describing theories, conclusions, or data, let alone negative adjectives that are doing all of the work in making their yet-unsupported point. reply jrflowers 4 hours agoprevI am glad that someone is finally clearing this up.The brain is in fact a plump shallot with a tiny noble heron inside. reply JoeAltmaier 14 hours agoprevThis is an example of an article that claims to debunk something that nobody said.Anyway it was informative and clarified things nicely. Just wish it had a better lead. reply mrkeen 14 hours agoparent> article that claims to debunk something that nobody saidAbout a third of the article was dedicated to who said it and when:>> As Paul MacLean (1964), originator of the triune-brain theory, stated: man, it appears, has inherited essentially three brains.>> This belief, although widely shared and stated as fact in psychology textbooks, lacks any foundation in evolutionary biology.>> The most widely used introductory textbook in psychology states that: ... The brain’s increasing complexity arises from new brain systems built on top of the old, much as the Earth’s landscape covers the old with the new. Digging down, one discovers the fossil remnants of the past>> we sampled 20 introductory psychology textbooks published between 2009 and 2017. Of the 14 that mention brain evolution, 86% contained at least one inaccuracy along the lines described above.>> For example, Dijksterhuis and Bargh (2001), [...] write that: when new species develop, this is done by adding new brain parts to existing old ones>> Examples of MacLean model of brain evolution appear in other areas, including models of personality (Epstein, 1994), attention (Mirsky & Duncan, 2002), psychopathology (Cory & Gardner, 2002), market economics (Cory, 2002), and morality (Narvaez, 2008). Nonacademic examples are too numerous to fully review.>> Carl Sagan&#x27;s (1978) Pulitzer Prize-winning book, The Dragons of Eden, and Steven Johnson&#x27;s (2005) Mind Wide Open were both popular books that drew heavily on this idea reply JoeAltmaier 12 hours agorootparenthas inherited essentially three brains.fossile remnantsThe quoted text give lie to the title. Nobody said it was an onion etc. Just that it was built new structures on old, all changing, integrated more or less well.Not being pedantic, I don&#x27;t think? The title is disparaging, deconstructing the idea to the point of ridicule. It&#x27;s reasonable to say \"Nobody said that!\" reply mistermann 14 hours agorootparentprevIf the authors are going to classify colloquial language (3 brains) as literal, their own study is then open to the same attack, and there is plenty of material from even the short skim of it I did.\"Pedantry\" is a double edged sword. reply sebringj 16 hours agoprevI can think of it like the wheels are still there, the frame, engine, body etc but the design and materials have been completely refined and overhauled to be modern future tech today. The car even has new stuff like electronics and wifi. Meaning there are still base conceptual things there but they are not the same...knowing that reasoning by analogy is flawed but still satisfying to me to feel like I understand it enough. reply tim333 10 minutes agoparentThere&#x27;s quite an interesting analogy there. I guess you could have a four wheeled wagon structure similar to wooden wagons of old, then an engine added and now electronics added, as an analogy to the triune brain. It&#x27;s not that modern cars are built on wagon chassis but that the basic body with four wheels structure is similar for functional reasons. Probably similarly with the &#x27;reptilian brain.&#x27; Our bits are maybe similar because they do the same job rather than because they evolved from that. reply hn8305823 19 hours agoprevI thought the line art evolution images were from Carl Sagan&#x27;s original Cosmos series, but it looks like they are slightly different:https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=gZpsVSVRsZkThe drawing of humans is from the Pioneer Plaque which Sagan was involved with:https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Pioneer_plaque reply nickdothutton 15 hours agoprevI think some people’s brains have trouble understanding the purpose and value of models (it’s their predictive power). For some use cases it doesnt matter of the model is “correct” so much as whether or not it has predictive value. I’m sure someone else here can phrase it far better than I. reply shadowgovt 18 hours agoprevFor all the good work he did, this is the one misstep in Sagan&#x27;s career that likely caused the most disruption: he was big on the \"reptile brain inside a mammal brain\" hypothesis and described it quite convincingly on Cosmos.We don&#x27;t have a great strategy yet for undoing the work of an effective science educator when they teach things science goes on to disprove. reply jakobson14 9 hours agoprevhttps:&#x2F;&#x2F;xkcd.com&#x2F;1257&#x2F;Another dumb analogy about the brain bites the dust. reply sgarrity 9 hours agoprevI&#x27;m pretty sure mine is. reply 3seashells 18 hours agoprevThe what watches from those eyes who can not see and see anyway? https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Blindsight reply Octokiddie 19 hours agoprev> Does it matter if psychologists have an incorrect understanding of neural evolution? One answer to this question is simple: We are scientists. We are supposed to care about true states of the world even in the absence of practical consequences. If psychologists have an incorrect understanding of neural evolution, they should be motivated to correct the misconception even if this incorrect belief does not impact their research programs.The hallmark of an incorrect model in science is that it makes incorrect predictions about the natural world (experiment). What incorrect predictions has psychology made based on the incorrect triune-brain theory?I&#x27;m going to guess zero, not because the model works, but because psychology has made no experimentally testable predictions based on it. reply zeroCalories 18 hours agoparentPsychology can barely make any predictions. This isn&#x27;t a meaningless technicality like the not-to-scale diagram of atoms used in old textbooks. Correcting poor assumptions to find new models of understanding is still important in this field. reply speak_plainly 18 hours agoparentprevThe goal of cognitive science is to basically build the foundation (the understanding of how the brain works) that psychology lacks and is a field that psychologists are actively contributing to.Ultimately, psychology was created as a pragmatic branch of philosophy with the understanding that we would not know how the brain works for quite some time but that we could still do something of value and help people. reply kbenson 18 hours agoparentprev> The hallmark of an incorrect model in science is that it makes incorrect predictions about the natural world (experiment).Sure, if you are insular and only care about how it affects the field and other scientists.Sometimes these incorrect models are incorrect in ways that are really attractive from a narrative standpoint. The hallmark of those models is to be used to spawn hundreds of pop science books that expound on those models in unfounded ways and push people into useless behavior, sometimes at a societal level.Maybe scientists and psychologists aren&#x27;t using the idea of a lizard brain in experiments and current theories, but I know there&#x27;s at least some laypeople people that use it as a way to explain their behavior or make assumptions about other people&#x27;s behavior, or to form their own ad-hoc explanations and models of behavior based on poor understanding of even what was previously reported to them. I would hazard it&#x27;s actually more than some, and a lot of people do it, with this or some other poorly reported incorrect model of behavior or how the body works or how the world works.Incorrect knowledge should be corrected. Leaving it it as it is leads to myriad problems, small and large, eventually. reply JacobThreeThree 17 hours agorootparentWell said. Just because it may be hard to pin down the consequences of the wrong-but-attractive narrative, there probably are consequences, especially on the long term.The \"chemical imbalance\" narrative with depression is also probably wrong: https:&#x2F;&#x2F;theconversation.com&#x2F;depression-is-probably-not-cause...Is it really so surprising that these simplistic narratives don&#x27;t actually accurately describe how the brain works? We should be prepared to admit that the brain is complicated and we don&#x27;t really know it&#x27;s functioning at a fundamental level. reply robg 19 hours agoparentprevThe biggest I’ve seen is that talk therapies don’t work well if an underlying sleep concern is not addressed. Sleep as the parasympathetic nervous system is predictable from a more primitive model whereas a cognitive - behavioral model assumes thoughts can drive recovery. reply mannykannot 18 hours agoparentprevI think there&#x27;s a good chance you are right, but academically-minded people have a tendency to hastily dismiss hypotheses that run counter to what they believe is a correct theory. I can imagine someone dismissing the idea that a psychological pathology is related to a neurological one because the latter is in the \"wrong\" part of the brain for the symptoms the former presents. reply KRAKRISMOTT 18 hours agoparentprevMost of psychology can&#x27;t be ethically tested. It doesn&#x27;t mean the entirely field is not empirically verifiable. reply pc86 18 hours agorootparentIs there a reasonable distinction between \"it&#x27;s not possible to test $X\" and \"it&#x27;s not possible to test $X in any ethical way?\" reply john-radio 18 hours agorootparentYesedit: I was trying to jokingly reply \"Yes {smiling-imp-emoji}\" when I realized I&#x27;ve never seen an emoji on Hacker News before - looks like they get automatically removed! reply blowski 18 hours agorootparentprevPerhaps one distinction would be that ethics change across time and country. So it might be possible to run a test in 2020s UK that cannot be repeated in 2030s. reply edgyquant 18 hours agorootparentprevIt means it hasn’t been verified, which means you can’t be sure a lot of it is useful at all. reply corethree 17 hours agorootparentScientific rigor has it&#x27;s limits. Certain fields need to make intuitive leaps of speculation.For example the entire field of astronomy is basically unverifiable bullshit. It&#x27;s all speculation. We make guesses on what&#x27;s going on with the stars outside of our solar system from twinkling light that comes from light years away.Any science to verify the claims made by astronomy with the amount of rigor you demand would involve light speed space ships to go to those stars and verify.If we could do this I think you&#x27;d find a ton of astronomy would be flat out wrong.Nonetheless the field is still useful and legitimate despite the high likelihood a lot of it is wrong and despite the fact we can&#x27;t verify much. reply MichaelZuo 18 hours agorootparentprevThat doesn&#x27;t prevent modern researchers from using past data collected via less scrupulous means though. e.g. Pavlov&#x27;s experiments reply corethree 18 hours agoparentprevNot just psychology. Even the claims within this very paper are hard to test. Anything involving evolution is almost impossible to test. The \"science\" is mostly observational and descriptive and arrived at through logical guesses.If all of \"science\" involved strict rigor to the \"scientific method\" we&#x27;d have none of the social sciences like anthropology or evolutionary psychology.This paper is simply pointing out differences in view points.I think the general idea in psychology is real though, the paper gets into details which is a bit pedantic. In fact the paper literally states that they are all in agreement that all brains evolved from a common ancestor. This would be the \"reptile\" and for sure common features in our brain such as serotonin stem from this \"reptile\" brain.Psychology gets a lot of bad rep for the reproduction crisis, but evolution should largely be worse because we can&#x27;t experimentally verify anything without time travel.So it&#x27;s not like this paper is about true science defeating pseudo science. It&#x27;s all really speculative. reply 1lint 16 hours agoprevI&#x27;m surprised by how much this publication reads like an advocacy piece for a specific viewpoint, rather than an objective review of existing literature. Just from reading the paper, it is clear that there are many experts in the field that take the opposing viewpoint that is being attacked in the paper, especially considering that their hypotheses have been published in widely circulated textbooks.When it comes to research publications in general, I very much prefer to hear an objective, good faith presentation of the major viewpoints, with the author taking an opinionated but measured take in the conclusion as they review the overall weight of the literature. I&#x27;m sure there are issues with this \"triune brain\" model, but at a certain level every model is inaccurate; the real question is whether a model is useful in its framework, and the answer has a degree of subjectivity such that I do not think it is fair to categorically reject the perspectives of opposing experts in the field. reply peoplefromibiza 19 hours agoprevif only psychology was a real science where hypothesis are thoroughly tested and tests are independently reproduced. reply hutzlibu 18 hours agoparentWell, the main problem with this is, it would require human experimentation to be efficient. You know, those babies raised in this way, those in that way. And by the age of 2 cut open and everything meassured.Or do you have other ideas how to do proper testing?Because otherwise experiments with control groups, electrodes, MRT etc. are a thing in psychology. But you can only achieve so much with it. reply syndicatedjelly 18 hours agorootparentThe study of psychology is done in neuroscience. But the research is extremely different. Neuroscience is like studying quantum mechanics and electromagnetism to understand the movement of electrons in a wire, while Psychology is comparable to UI&#x2F;UX research.Many people are working on bridging neuroscience and psychology but it hasn&#x27;t happened yet. reply hutzlibu 18 hours agorootparentOk, I see (maybe formally wrong) neuroscience as a part of psychology. (At least some psychologists I talked to, had this position)And the reason why bridging is not really happening, because of the ethical restrictions. But this is my hypothesis. The other hypothesis might be, that too many careers depend on models that would not hold up by experiments, but I really cannot judge here because of lack of knowledge. reply robertlagrant 13 hours agorootparent> Ok, I see (maybe formally wrong) neuroscience as a part of psychology. (At least some psychologists I talked to, had this position)They might believe this in the same way that some UX people might view all of software as a subset of UX, because everything ends up as an experience for some user somewhere. reply syndicatedjelly 18 hours agorootparentprevIt’s more of the former - it’s not ethical to do the experiments that would prove or disprove the hypothesized links. There’s plenty of real work to do, not a conspiracy lol reply wslh 14 hours agorootparentprevBTW, is there something new and interesting about the connection between quantum mechanics and the brain? There are many articles&#x2F;papers in [1] but it would be great to hear from someone in the field.[1] https:&#x2F;&#x2F;scholar.google.com&#x2F;scholar?as_ylo=2019&q=quantum+bra... reply syndicatedjelly 14 hours agorootparentSorry I didn’t mean to come off as if I’m in research, I just have a BS in neuroscience. No longer doing anything with the degree reply peoplefromibiza 16 hours agorootparentprev> Or do you have other ideas how to do proper testing?Well, for starters don&#x27;t call them studies, call them \"hunches\" or \"intuitions\" or whatever you like to indicate that there is actually no real evidence to prove your thesis.Occam&#x27;s razor tells us to believe that it&#x27;s much more plausible that psychologist publish massaged data to keep getting funded, than because it&#x27;s actually very hard to admit that \"we don&#x27;t know for sure, this study proves nothing\".Have you ever heard of my fellow Italian professor Francesca Gino? reply micromacrofoot 18 hours agoparentprevUnless you want to start growing test subject humans in vats, it&#x27;s not exactly something that&#x27;s as simple as \"testing right\" — the problem isn&#x27;t lack of will or skill. We can&#x27;t just throw rats at this problem like many scientific fields do. reply jiofj 17 hours agorootparentWhich is why it&#x27;s not a real science. No one is saying that \"scientists\" working on psychology are negligent, but that it&#x27;s impossible for psychology to meet the requirements of a real science. reply micromacrofoot 15 hours agorootparentI think it&#x27;s a little harmful to start saying \"real science\" — this is something that is at times meant to discredit psychology entirely, so it&#x27;s an unfortunately loaded phrase.I think they&#x27;re still doing science, it&#x27;s just much more difficult given physical and ethical constraints. reply mrkeen 14 hours agorootparentThe arguments in this thread:* Psychology is not a \"real\" science because it doesn&#x27;t produce quality evidence.* But quality psychological evidence is (too) hard to produce.See how the second point doesn&#x27;t really refute the first? reply micromacrofoot 13 hours agorootparentYou&#x27;re missing the point that calling something \"not real\" is very often used as a method to discredit it. Saying \"not real\" discredits the fact that there are more barriers here.That&#x27;s my primary issue with this. Phrenology can also be called \"not real science\" but it doesn&#x27;t seem fair to paint both with such a broad brush. There&#x27;s more nuance involved than an off-the-cuff \"not real.\"Though at this point it all feels too belabored to carry on. reply peoplefromibiza 17 hours agorootparentprevyeah, exactly, it&#x27;s not real science. Doesn&#x27;t matter why.Atomic bombs are very dangerous too and not very easy to \"test them right\", but atomic physics is a real science.Anyway, the problem in psychology is that psychologists often lie and fabricate false evidence, not that the rats aren&#x27;t enough.There are multiple studies about it, published by scientists, most of them agree that \"Don’t trust everything you read in the psychology literature. In fact, two thirds of it should probably be distrusted.\"There&#x27;s a point where a field can&#x27;t keep going on shielding behind the false myth that \"the problem is that we can&#x27;t test on humans\". reply hutzlibu 12 hours agorootparent\"There are multiple studies about it, published by scientists, most of them agree that \"Don’t trust everything you read in the psychology literature. In fact, two thirds of it should probably be distrusted.\"\"Can you link a study, that makes such a claim? reply peoplefromibiza 2 hours agorootparent> Can you link a study, that makes such a claim?Of course I can, I took it from a study!https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;nature.2015.18248 --------------------some other studies on the subject, not all agreeing on the conclusions, as it is naturalhttps:&#x2F;&#x2F;pubmed.ncbi.nlm.nih.gov&#x2F;29861517&#x2F;https:&#x2F;&#x2F;www.sciencealert.com&#x2F;two-more-classic-psychology-stu...https:&#x2F;&#x2F;www.pnas.org&#x2F;doi&#x2F;10.1073&#x2F;pnas.2208863120the real problem is that the kind of psychology studies that gets the more attention on the media, that shape the culture and the beliefs of the general public and that are often used by governs and institutions (even medical ones) to deploy policies, health care plans, school or work environment guidelines, etc. etc. are those that are less trustworthy, that fail to be replicated the most and more subject to interpretations and subjective biases, in particular social psychology (citation: Social Psychology (38% success rate, n = 256).) reply lukeinator42 15 hours agorootparentprevExactly, and the irony is that there are a lot of cognitive neuroscientists doing human participant research all the time. It&#x27;s honestly easier than doing the ethics for animal research these days. I don&#x27;t understand all the comments from everyone saying it&#x27;s impossible to do reliable research with human participants, haha. reply micromacrofoot 15 hours agorootparentprevSorry if I misunderstood, it&#x27;s just that at times \"real science\" is used to discredit psychology as a field entirely so there&#x27;s a bit of defensiveness involved. reply aaroninsf 16 hours agoprevITT a lot of fully justified scorn for pearl-clutching performative polemics.An interesting application of weasel words and passive voice in the article.. reply moab9 14 hours agoprevThere&#x27;s a rat in there. reply alex01001 17 hours agoprevbrain is a receptor for consciousness, it doesn&#x27;t create it. Consciousness is \"broadcasted\". reply FeteCommuniste 17 hours agoparentThat’s a spicy take. Is there any evidence to support it? reply booleandilemma 17 hours agorootparentI recently read about that idea in the book \"Notes on Complexity: A Scientific Theory of Connection, Consciousness, and Being\".It&#x27;s an interesting idea. I&#x27;m not sure why the parent is being downvoted.https:&#x2F;&#x2F;www.amazon.com&#x2F;Notes-Complexity-Scientific-Connectio... reply kbelder 16 hours agorootparent>It&#x27;s an interesting idea. I&#x27;m not sure why the parent is being downvoted.Probably because he stated it as a fact instead of an interesting fringe theory. reply dboreham 14 hours agoprevCountering a widely held intuitive model by...asserting loudly and repeatedly that it&#x27;s incorrect, while providing no supporting evidence. reply nemo 13 hours agoparentIt&#x27;s surreal to see so may people reading the article with this takeaway, the \"What&#x27;s Wrong\" section seemed very clear to me and elaborated for a while including a number of citations on why the simplistic layer model was flawed.> providing no supporting evidenceI checked, there&#x27;s seventeen separate citations for evidence in the \"What&#x27;s Wrong\" section as well as several figures. reply fieldbob 16 hours agoprev [–] To figure this out one has to sit in meditation and find out for one self This is metaphysics not psychology, perhaps you are asking the wrong people. reply potatoman22 16 hours agoparent [–] Why is science not suitable to answer this question? reply mistermann 13 hours agorootparent [–] Inappropriate methodology, culture(s), conventions, etc. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article refutes the common understanding in psychology that the brain's evolution is due to increasing complexity through additional newer structures overlaying older ones, a belief now discredited by neurobiologists.",
      "The authors highlight that this misconception may have impeded progress within the field, emphasizing the need for a correct understanding of neural evolution to prevent research bias and identify cross-species correlations.",
      "The summary also underlines the importance of interdisciplinary research in fields like impulsivity, inhibition, and delay of gratification, and negates the idea that humans have unique neural structures tied to specific cognitive functions."
    ],
    "commentSummary": [
      "The piece critically evaluates the triune brain model, arguing it is too simplified and doesn't precisely depict the intricacies of brain evolution.",
      "The article emphasizes the interplay between the brain and the body, the significance of overlooked brain areas, and the issues inherent in simplifying complex scientific ideas.",
      "The post also delves into the limitations and hurdles of psychology as a science, particularly regarding the credibility and dependability of studies within the discipline."
    ],
    "points": 281,
    "commentCount": 145,
    "retryCount": 0,
    "time": 1695039150
  },
  {
    "id": 37561696,
    "title": "FTC warns pharma companies about sham patent listings designed to delay generics",
    "originLink": "https://www.techdirt.com/2023/09/18/ftc-warns-pharma-companies-that-it-may-go-after-them-for-sham-patent-listings-designed-to-delay-generic-competitors/",
    "originBody": "TECHDIRT GREENHOUSE FREE SPEECH DEALS JOBS SUPPORT TECHDIRT Police Union VP Says Woman Killed By Cop Is Only Worth $11,000 Appeals Court Says Michigan County’s Vehicle Forfeiture Program Is Unconstitutional FTC Warns Pharma Companies That It May Go After Them For Sham Patent Listings Designed To Delay Generic Competitors Patents from the abusing-the-orange-book,-green-with-greed dept Mon, Sep 18th 2023 01:30pm - Mike Masnick For many, many years we’ve detailed how big pharma companies, who only care about the monopoly rents they can receive on medicine while under patent, have concocted all sorts of scams and schemes to avoid having to compete with generic versions, even after their patents have expired (or been invalidated). But one of their older tricks is apparently popular yet again, though the FTC is now warning pharma that it might finally start cracking down. If it does, it will just be reinforcing the kinds of actions the FTC used to bring. Twenty years ago, the FTC went after Bristol Meyer Squibb for false listings in the Orange Book. The Orange Book, managed by the FDA, is where pharma companies list the FDA-approved drugs they have under patent, which alerts generic drug companies basically not to make generic versions of those drugs. But, of course, this creates a very tempting scenario: if pharma can get drugs not actually under patent into the Orange Book, they effectively save themselves from generic competition, and they get to profit massively (at the expense of the public and their need for affordable medicine). However, despite enforcement against such abuse years ago, it seems that the FTC and the FDA have kinda let these things slip over the past few years. And Big Pharma has really taken advantage of that. Thankfully, it looks like the FTC is finally interested in cracking down on this practice again. In a new policy statement, it warns pharma companies that it’s looking into the abuse of the Orange Book and sham patent inclusions. Brand drug manufacturers are responsible for ensuring their patents are properly listed. Yet certain manufacturers have submitted patents for listing in the Orange Book that claim neither the reference listed drug nor a method of using it. When brand drug manufacturers abuse the regulatory processes set up by Congress to promote generic drug competition, the result may be to increase the cost of and reduce access to prescription drugs. The goal of this policy statement is to put market participants on notice that the FTC intends to scrutinize improper Orange Book listings to determine whether these constitute unfair methods of competition in violation of Section 5 of the Federal Trade Commission Act. Of course, this raises some questions, including why do we make the pharma companies themselves the party responsible for making sure their patents are “properly” listed. Why don’t we have at least some process in place for these listings to be reviewed, whether when they’re submitted to the Orange Book or even if another party (such as the generic drug manufacturers) contest an Orange Book listing. It seems the dumbest possible system is to assume that the Big Pharma companies will be honest in their Orange Book listings. And, even though the FTC is now putting these companies “on notice,” the fact that the FTC has brought these cases in the past seems like it should be “notice” enough. Instead, it sounds like the FTC let enough pharma companies get away with this for long enough that the big pharma firms felt cleared to abuse the system this way and to delay competition in the marketplace. The one thing I find interesting in this statement, is that they note that improperly listing things in the Orange Book may “constitute illegal monopolization.” The improper listing of patents in the Orange Book may also constitute illegal monopolization. Monopolization requires proof of “the willful acquisition or maintenance of [monopoly] power as distinguished from growth or development as a consequence of a superior product, business acumen, or historic accident.” This requires proof that “the defendant has engaged in improper conduct that has or is likely to have the effect of controlling prices or excluding competition,” and courts have recognized that improperly listing patents in the Orange Book may constitute an “improper means” of competition. Accordingly, improperly listing patents in the Orange Book may also be worthy of enforcement scrutiny from government and private enforcers under a monopolization theory. Additionally, the FTC may also scrutinize a firm’s history of improperly listing patents during merger review This seems exactly correct, but notable in that very few people seem to recognize that (1) patents are government granted monopolies, and thus (2) an abuse of the patent system to get a patent or patent-like protections you don’t deserve are therefore an illegal monopoly seems like an important point. I would hope that this could get expanded to other abuses of patent and copyright law as well. Still, given that we’ve been facing this and multiple other schemes from Big Pharma to delay generics for decades, I’m not sure anything is really going to change just yet, but at least the FTC is waking up (again?) to this issue. Now let’s see if it actually starts bringing cases… Filed Under: competition, drug prices, ftc, generics, monopoly, orange book, patents, pharma 5 CommentsLeave a Comment If you liked this post, you may also be interested in... A New Low: Just 46% Of U.S. Households Subscribe To Traditional Cable TV Judge For FTC Rules Intuit Can't Continue Its 'Free To File' Advertising Bullshit First Of Potentially Many Google Antitrust Claims Goes To Trial DOJ To Court: Here Are The Many, Many Reasons Why The FTC Can & Should Be Investigating Elon Musk’s Handling Of User Data The EU Designates The Six Companies You Already Expected As ‘Gatekeepers’ Under The Digital Markets Act Comments on “FTC Warns Pharma Companies That It May Go After Them For Sham Patent Listings Designed To Delay Generic Competitors” Subscribe: RSS Leave a comment Filter comments in by Time Filter comments as Threaded Filter only comments that are Unread 5 Comments This comment is new since your last visit. That One Guy (profile) says: September 18, 2023 at 1:38 pm 'How dare you, that was .000001% of our profit during that time!' Pharma companies aren’t trying to stymie and keep generics off the market because they’re bored and looking for something to do they’re doing it because it’s hugely profitable to do so so unless the FTC is willing to hand out some massive financial penalties in amounts that are at a minimum equal to how much the companies have been getting from squashing generics all they’re really going to do here is turn ‘getting caught keeping generics off the market for a few more years’ into just another business expense. It’s a good idea but only if they’re willing to fully commit to it as far from a warning or penalty any token effort is likely to be treated as encouragement and confirmation that the behavior they are trying to stop is hugely profitable even when you get caught. Reply View in chronology Make this comment the first word Make this comment the last word This comment is new since your last visit. mick says: September 18, 2023 at 1:54 pm It really doesn't look like that Thankfully, it looks like the FTC is finally interested in cracking down on this practice again. In a new policy statement, it warns pharma companies that it’s looking into the abuse of the Orange Book and sham patent inclusions. No, it doesn’t look like they’re interested in cracking down on anything. It looks like they’ve chosen a performative warning over taking action. You could argue that this is a good first step, but I’d argue that it’s a joke. It will look like they’re interested in cracking down when they start, you know, cracking down on someone for something. Until then, nothing at all has changed. Reply View in chronology Make this comment the first word Make this comment the last word This comment is new since your last visit. Anonymous Coward says: September 18, 2023 at 3:57 pm Why don’t we have at least some process in place for these listings to be reviewed, whether when they’re submitted to the Orange Book. … because a half million a year (5 highly trained positions, or 2 highly trained and 6 google-monkeys) looks like a lot on a budget, and the pharma lobby knows how to press “skimp on budget” buttons. Reply View in chronology Make this comment the first word Make this comment the last word This comment is new since your last visit. Anonymous Coward says: September 18, 2023 at 4:58 pm … because a half million a year (5 highly trained positions, or 2 highly trained and 6 google-monkeys) looks like a lot on a budget, Until it is executive recompense, and then it looks insufficient. Reply View in chronology Make this comment the first word Make this comment the last word This comment is new since your last visit. That Anonymous Coward (profile) says: September 18, 2023 at 5:41 pm Now think of the generics as IP going into the public domain… You can just rererelease an album, call it remastered and hey you have a brand new copyright on music recorded by someone over 100 yrs ago. While they are feeling like they should do their job perhaps they should address the massive price gouging happening when some douche acquires a needed medicine thats cheap to make but then raise the price by 3000% to screw the few who have no other option but die/suffer without it. Reply View in chronology Make this comment the first word Make this comment the last word Add Your Comment Your email address will not be published. Required fields are marked * Have a Techdirt Account? Sign in now. Want one? Register here Name Email Subscribe to the Techdirt Daily newsletter URL Subject Comment * Comment Options: Use markdown. Use plain text. Make this the First Word or Last Word. No thanks. (get credits or sign in to see balance) what's this? Police Union VP Says Woman Killed By Cop Is Only Worth $11,000 Appeals Court Says Michigan County’s Vehicle Forfeiture Program Is Unconstitutional Follow Techdirt Essential Reading The Techdirt Greenhouse Read the latest posts: Winding Down Our Latest Greenhouse Panel: The Lessons Learned From SOPA/PIPA From The Revolt Against SOPA To The EU's Upload Filters Did We Miss Our Best Chance At Regulating The Internet? Read All » Trending Posts FTC Warns Pharma Companies That It May Go After Them For Sham Patent Listings Designed To Delay Generic Competitors The Batshit Crazy Story Of The Day Elon Musk Decided To Personally Rip Servers Out Of A Sacramento Data Center Appeals Court Says Michigan County's Vehicle Forfeiture Program Is Unconstitutional Techdirt Deals BUY NOW The 2023 American Sign Language Mastery Super Bundle Techdirt Insider Discord The latest chatter on the Techdirt Insider Discord channel... mildconcern: Does @Tim Cushing ever get worried that cops will stop doing stupid things and deprive him of a beat? Also I want an award for being able to type that with a straight face. John Roddy: He's too busy with his packed schedule as a law professor. candescence: [article] I do wish the article listed the actual roadmap though Mike Masnick: I have a post on it coming later today that will have more details mildconcern: Killing a program 2 months before anyone can benefit from the renewal hardware feels like a new horizon for the Googlesassination of programs: [article] There also should be a special place in hell reserved for services that do involuntary 2FA and don't allow you to choose the method. I say as I sit here waiting for my work benefits selection service's email to get through my graylisting email server. deadspatula: Wait, the pixel phone line still exists? Don't they keep shutting down the pixel line? Mike Masnick: no, pixel phone is a big deal for google these days Samuel Abram: @Mike Masnick Relevant to our interests: [article] You even get a shout-out there, among many other authors Mike Masnick: heh. yeah, i saw that yesterday and thought it was amusing that i was listed with those other names Samuel Abram: BTW, I got an advocacy email saying \"Tell Merrick Garland to prosecute the Oil Companies under RICO\". I threw that email into the trash. [gifv] Also, there's a good chance I'm going to get $$$ from bandcamp friday candescence: The most striking thing about the Australian government's response is that two of the three criteria for age verification to be viable is that it cannot be circumvented and must apply to sites universally, not just sites hosted in Australia The problem is, I'm pretty sure both of these criteria are virtually impossible The circumvention part is one thing due to VPNs, but the latter, well... You'd need _every country in the world_ to agree on forcing age verification John Roddy: The order in the Texas case does a wonderful job explaining how almost anything else is so obviously better I read through it earlier, and it is really friggin good Also, I've brought up Acerthorn a few times before. The guy who kept trying to sue for copyright infringement in CAND, including against Google. He was formally declared a vexatious litigant today. deadspatula: Having followed the Acerthorn saga long past my time as a fan of SidAlpha (my entery point to the controversy), it is gratifying to see the wheels of justice finally get traction. John Roddy: UNCLEAN HANDS Samuel Abram: @Mike Masnick A substacker to whom I subscribe has also sounded the alarm on KOSA, pointing out that right-wingers plan to use it to censor LGBTQ+ content: [article] Tim Cushing: it ain't much but it's a living Samuel Abram: I read this in the voice of a Flinstones animal doing something that would otherwise be a machine in the real world. Also, you're a law professor? mildconcern: It may just be my reader acting wonkily but I suspect the techdirt RSS feed might be down I'm digging up another reader to see Yeah I think it's giving out a 429 too many requests. My reader has nothing after the \"yet another study shows kids aren't permanently depressed by social media \" from a day or two ago Mike Masnick: Yeah. We're aware and working on it. But thanks for letting us know. Samuel Abram: @Mike Masnick I'm reading this Alex Winter interview in Teen Vogue, and you were absolutely right about him; he's a very thoughtful and reflective person. [article] John Roddy: Congratulations on the new anti-SLAPP law, New Jersey~ Samuel Abram: Nice John Roddy: I was going to jokingly say that maybe New York would suddenly get really interested in passing one, but that already happened. Suspiciously recently. BentFranklin: Is ADL based in New York? Because it seems they are about to get SLAPPed. Samuel Abram: X is based in California, though and a Federal anti-SLAPP law is nonexistent. [link] Mike Masnick: What about it? We wrote about it when the case was first filed. Samuel Abram: Oh, I must’ve missed it. mildconcern: \"doj to x lol no\" cracked me up Mike Masnick: was wondering if anyone would notice that mildconcern: in related news the RSS started working for me today, or I wouldn't have Mike Masnick: yeah, it's back... candescence: BTW, any thoughts on the story of Elon shutting off Starlink access around Ukraine to deliberately foil a Ukranian military operation? Also: [link] [article] Unity are actively planning to charge developers a fee _every time a user installs a Unity-made game_ This shit is partly why I switched to Godot and Unreal for my gamedev stuff Mike Masnick: Only so much Elon we can cover, and honestly... there's so much confusion about that story, not sure what to write or what's worth writing about it candescence: Yeah, fair enough. Just reading about all the absolute nonsense he gets up to these days alone is _exhausting_. I guess some people see the takeaway as \"maybe we shouldn't have such crucial infrastructure be in the hands of an unpredictable eccentric who is explicitly sympathetic to Russia's stated goals and motives and may possibly be outright compromised.\" And in broader terms that this is also a consequence of privatization in general, that ideally something based in the US so crucial to Ukrainian military communications should be handled by the US military, not a private entity Samuel Abram: I guess this is a pupil to the Elon Musk school of Enshittification Accelerationism of which /spez is also a student. candescence: It also doesn't help that a lot of companies are scrambling now that the age of low interest rates and infinite credit is over Keep in mind Unity bought out _Weta Digital_, for pete's sake. deadspatula: I am really curious how UNity is differentiating re-installs. Or charity bundle installs from non-charity bundle installs. Sounds like a whole lot of intrusive tracking and documentation about my system getting sent to who knows. candescence: Yeah, that's pretty much a concern everyone has John Roddy: The answer depends on when you ask. By my count, they've retroactively fundamentally contradicted themselves at least three times while giving \"clarifications\" so far. candescence: Unity's overall response has been comically tone-deaf But considering this is a company run by former EA CEO John Riccitiello, who thought _this_ would go over well: [article] Dude has just the most _punchable_ face deadspatula: Its got more D&D Next energy to it. Like, Unity has burned devs before, put explicit language into its contract about how and when changes could be made to regain trust, and then attempted to sliently erase those clauses on the assumption their market dominance would just make it so. How do we have multiple companies trying to do this shit after WOTC burned themselves so bad? Samuel Abram: I mean, Twitter, Reddit, WotC, and now Unity seem like companies in an enshittification speed-run e-sport. John Roddy: There's a synergy joke in here somewhere deadspatula: true, but i found it notable that both wotc and unity decided to give themselves greater ownership rights, and both had already been burned for trying it in the past. going so far as to write clauses into contracts to assure the community that wouldn’t happen again. Feels like more than just a dumb CEO move when they’ve got big holes in the consideration in their contracts and those contracts were with not insignificant third parties. pyrex: (FWIW, I think the story told here is \"they're not exactly sure what the actual plan will look like\" and \"the corporate communications we have gotten are about the version of the plan that generates the most good emotions in upper management\") (I don't know how this got through legal and PR!) John Roddy: I'd be more willing to accept that if they hadn't confidently stated it multiple times and then pushed their latest \"clarification\" as a promoted ad. pyrex: I do think they're trying to make it look like they know what their plan is! I just don't believe them. thadboyd: Bill Willingham gets tired of DC jerking him around, releases Fables to the public domain. [article] Mike Masnick: yeah, a few people have sent that one in... i may try to write something on it if i have the time (big if) candescence: That's one spectacular power move Are there any particular legal hurdles that DC might try to take him to court over? thadboyd: I suspect DC's lawyers are trying to figure out the answer to that question right now. If they're smart, they won't do anything, because that would just Streisand the situation further. But \"if they're smart\" is a big \"if\". I don't think any major media companies will bite. Nobody's going to try to make a Fables TV series or movie or anything. As Willingham suggests, the artists could do their own series without him, but they'd be burning bridges with DC and might have to self-publish since I suspect the other comics publishers would be gun-shy about stepping into a potential legal dispute. What I think is an interesting legal question is whether this means Chris Roberson can self-publish that Fairest arc that DC shelved because he said mean things about them. [article] candescence: I mean, it's Warner-Discovery, _sooooooooooooo_ John Roddy: PRO TIP! If you're going to fake evidence that you complied with a court order, don't leave proof that you're faking it in the video you just submitted to the court in response to an order to show cause for you shouldn't be held in contempt. Mike Masnick: that's gonna need some further details john... John Roddy: Well, here's Exhibit B: [link] Yes, Stebbins has already been declared a vexatious litigant in this circuit. But no, he's not done pushing his luck even further. candescence: DC obviously disagrees on the public domain thing: [article] But it feels like Bill wouldn't pull this stunt if he didn't have a leg to stand on, so there's a good chance it'll go to court anyway. This tweet has a relatively recent copyright notice from the Fables comics, it directly states that while there's apparent joint ownership of the copyright, Willingham alone owns the trademarks to the characters, not DC: https://twitter.com/Comixace/sta... Read into it how you will, I suppose. pyrex: I suspect he's not allowed to do this: if he were allowed to do this, then under his current reasoning, he would have been able to evade the original contract basically just by asking a friend to do this. (er, to do whatever he wanted done) I don't know what will happen next. BentFranklin: The Internet's Best Boy is dead. Now all we have is The Internet's Worst Boy. The Verge: X continues to throttle links to competitors [article] Samuel Abram: @Mike Masnick I’m at a Peter Gabriel concert and I think you would’ve loved to hear what he said about AI Mike Masnick: he's great on all this stuff Samuel Abram: He absolutely is. It doesn’t hurt that he’s also an excellent musician and songwriter, not to mention that his voice hardly aged in **fifty** years! Become an Insider! Recent Stories Monday 20:06 NZ Trademark Office Gets It Right Scrutinizing Trademark Opposition Review (2) 15:48 Appeals Court Says Michigan County's Vehicle Forfeiture Program Is Unconstitutional (8) 13:30 FTC Warns Pharma Companies That It May Go After Them For Sham Patent Listings Designed To Delay Generic Competitors (5) 12:00 Police Union VP Says Woman Killed By Cop Is Only Worth $11,000 (21) 10:53 Bill Willingham Says Fables Is Released To The Public Domain, DC Comics Says It Most Certainly Is Not (42) 10:48 Daily Deal: Microsoft Windows 11 Pro (3) 09:24 Intelligence Community Feels It Might Be Time To Start Stuffing Surveillance Gear Into People's Pants (18) 05:27 A New Low: Just 46% Of U.S. Households Subscribe To Traditional Cable TV (8) Sunday 12:04 Funniest/Most Insightful Comments Of The Week At Techdirt (30) Saturday 13:00 This Week In Techdirt History: September 9th - 16th (0) More Tools & Services Twitter Facebook RSS Podcast Research & Reports Company About Us Advertising Policies Privacy Contact Help & Feedback Media Kit Sponsor / Advertise More Copia Institute Insider Shop Support Techdirt Brought to you by Floor64 Proudly powered by WordPress. Hosted by Pressable. This site, like most other sites on the web, uses cookies. For more information, see our privacy policy",
    "commentLink": "https://news.ycombinator.com/item?id=37561696",
    "commentBody": "FTC warns pharma companies about sham patent listings designed to delay genericsHacker NewspastloginFTC warns pharma companies about sham patent listings designed to delay generics (techdirt.com) 271 points by rntn 13 hours ago| hidepastfavorite78 comments AlbertCory 12 hours agoI have to admit I didn&#x27;t know how the Orange Book worked, so I had to do some research. Do I have this right?\"An Orange Book listing shows \"approved prescription drugs, related patent and exclusivity information, and therapeutic equivalence evaluations, along with other information.\"https:&#x2F;&#x2F;www.fda.gov&#x2F;drugs&#x2F;news-events-human-drugs&#x2F;our-perspe...So a pharma company can insert a listing saying that, for a bogus example, \"Vioxx has no approved generics, because all substitutes are under patent or patent pending.\" when that is untrue.This seems clearly illegal and abusive.From the headline, I thought Kahn was saying that filing a trivial patent \"improving\" Vioxx would be illegal. But that&#x27;s not it. reply whymauri 11 hours agoparentNot only bogus, but if you get a hit on a scaffold you can combinatorially modify that scaffold and file it with the patent. This effectively blocks the small permutation space around the drug from further investigation or development.There&#x27;s lot of promising science that can&#x27;t be done because incumbents are so adversarial about patent space for structures they don&#x27;t even care about. To make matters worse, patent data and notation is:* Poorly structured and defined,* Difficult to parse,* Uses combinatorial&#x2F;wildcard notation (Markush structures [0], which is what modern drug patent law is based on).So it&#x27;s a total mess. The best analogy I could give for software is: imagine if you could patent closed source code (the literal code) and also attach a wildcard to every branch in logic within that closed source code, but all you publish are the filenames (as screenshots of directories, not text). You don&#x27;t have to run due diligence that the permutations are run to spec or even compile, but now nobody can write logic infringing on your hypothetical code (which you never wrote).If your reaction is &#x27;what the fuck?&#x27;, yes I agree.[0] https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Markush_structure reply dataflow 9 hours agorootparentEven ignoring the fact that they block a whole permutation space around the drug, this sounds completely nuts to me:> The company which applies for a patent makes a general claim for the usage of the molecule without revealing to their competitors the exact molecule for which they are declaring a useful applicationIsn&#x27;t that against the very point of a patent? That you reveal (make \"patent\") what you&#x27;re doing to the public so that the art isn&#x27;t lost, in exchange for some form of commercial protection? How in the world is this legal? reply AlbertCory 11 hours agorootparentprevThanks, let&#x27;s leave out software. Not that it&#x27;s not relevant, but it&#x27;s a black hole for this crowd. One should not try to understand everything by analogy with software. reply dllthomas 10 hours agorootparentCars, then? reply AlbertCory 10 hours agorootparentNo analogies. It is what it is.Markush groups, as GP mentioned, are there specifically for chemical patents. So that&#x27;s what&#x27;s shaped the patent law, not software or cars. reply TeMPOraL 3 hours agorootparentIt&#x27;s purposefully a kind of algebraic notation. Algebra is math, particularly a kind of math that maps really well to software (less so, cars). Hence the analogy makes sense. reply lotsofpulp 10 hours agorootparentprevAnalogies are not necessary all the time. reply dllthomas 10 hours agorootparentIndeed. My intent with my comment was to jokingly nod in that direction. reply refurb 8 hours agorootparentprevIt&#x27;s not bizarre at all.Early hits are step 0.1 in bringing a drug to market. You patent a large space that gives you room to optimize the structure in terms of safety, efficacy, Pk, metabolites, etc.Very rarely would you ever get 1 hit in a huge combinatorial screen. You&#x27;d likely get a few dozen. But you have no idea which ones would the best.The company will usually nominate 5 or 6 hits (across the scaffold space) for further screening, then slowly whittle it down from there. By the time it hits humans, it&#x27;s like 1 with 1 or 2 backups.So you patent them all.If you could only patent one, one of two things would happen: 1) the company would just file thousands of individual patents to accomplish the same thing or 2) not both developing the drug further unless they were 100% certain it was the right one (which I&#x27;ve never seen). reply whymauri 7 hours agorootparentSure, you can argue that coverage for lead optimization is an unintentional feature, not a bug of Markush. But you have to admit that the current system is leveraged in bad faith i.e. for hedging against generics, staggering patents to maximize exclusivity period, exceptional vagueness and overreach in the structure definitions.Technology and legal practices have far outpaced the USPTO. We have accidentally incentivized sheer volume and intentionally poor record-keeping as a moat. This is a fundamental _inefficiency_, because it de-emphasizes property based engineering and pushes ADMET down the road. Why do so many candidates fail for ADMET reasons a few years down the line? Because people aren&#x27;t thinking about the risks sooner! Why not? Because surely the patent will cover everything.Add to everything how siloed big pharma is and it becomes impossible to coordinate across the pipeline. IANAL, but certainly there must be some middle ground here -- the last time the USPTO took a stab at redefining this mess was 2007, but they failed.https:&#x2F;&#x2F;www.uspto.gov&#x2F;sites&#x2F;default&#x2F;files&#x2F;web&#x2F;offices&#x2F;com&#x2F;so...They more or less proposed what you said: file a claim for each different invention, putting the onus of proof on the filer. In the amendment, Markush is appropriate if you have evidence backing that the chemical space has a true shared utility via structure i.e. test it or forget it OR the chemical space is &#x27;obvious&#x27;.When you read the original intent of the Markush decision i.e. \"members of [a] Markush group are alternatively usable for the purposes of the invention,\" it becomes clear that we have strayed waaay from this definition. A Markush can easily contain structures that are impossible to synthesize at all! reply refurb 5 hours agorootparentI mean, if the argument is PTO should be better resources so that patents can undergo more rigorous evaluation, you won&#x27;t get an argument from me.You&#x27;re not supposed to patent anything you haven&#x27;t actually made, however, there is no validation of that and as a result patent applicants are incentivized to patent as broad a space as possible.However, if we did move to a \"single molecule, single patent\" approach, the workload on the PTO would skyrocket. For pharmaceutical companies the cost of a patent and its preparation is infinitesimal compared to the cost of developing a drug - pharma companies wouldn&#x27;t bat an eye at submitting 1000&#x27;s of patents for each discovery program.But, as I stated above, I don&#x27;t disagree the process could be made better. reply AlbertCory 5 hours agorootparent> You&#x27;re not supposed to patent anything you haven&#x27;t actually madenot true. That&#x27;s Actual Reduction to Practice. There is also Constructive Reduction to Practice.I wrote about this in depth, for software:https:&#x2F;&#x2F;albertcory50.substack.com&#x2F;p&#x2F;no-source-code-no-patentThis is a good place to repeat, \"let&#x27;s just remove software from the patent system.\"If you don&#x27;t, then any changes you propose will being the Big Pharma lobby down on you. Once we get our own IP protection laws (or no IP laws), the pharma companies won&#x27;t care about us anymore. reply cycomanic 5 hours agorootparentprevThere is an easy solution, you already can&#x27;t patent cooking recipes. So why allow chemical recipes? To those that say that would stifle innovation, well historic evidence is inconclusive at best, e.g. the development of the pharmaceutical industries in the US, Germany and Switzerland saw thr biggest industry develop in Switzerland who did not have any patent protection followed by Germany, where only processes not chemicals could be patented. The US pharmaceutical industry only become much bigger after they benefited from the harmonisation of patent systems to follow the strong protection in the US. reply refurb 6 minutes agorootparent“Recipes” aren’t the key patents in pharma, it’s new molecules - new composition of matter. Occasionally processes are patented but competitors could still make the molecule other ways.And considering the pharma industry never really took off until the mid-century I’m not sure your claim of “saw the biggest industry develop” before the parents we have today holds any weight. AlbertCory 4 hours agorootparentprevThe Pharma lobby is one of the most powerful in DC. Zero chance of that ever happening.So let&#x27;s remove software from the list of patentable subject matter. Then Pharma won&#x27;t care about us anymore. One thing at a time. reply ivan_gammel 2 hours agorootparentprev> not both[er] developing the drug further unless they were 100% certain it was the right one (which I&#x27;ve never seen)I don’t really understand this argument. Patent law is just another moat. If it doesn’t exist for all competitors, it’s just different baseline for calculations. The market is still big and attractive, so all those who give up will open space for those who figured out how to survive without patents. Likely this will lead to a bigger concentration of capital, because in the absence of patent protections you may need to spend more on security. But it is happening anyway for other reasons, so we have to deal with monopolies anyway. reply refurb 3 minutes agorootparentPlease explain how, without patents, a company would choose to invest hundreds of millions of dollars to discovery a new molecule and get it approved, when another company could start selling it as well without any of the investment at all? reply 0xcde4c3db 11 hours agoparentprevIf I understand things correctly (not assured by any means, as I&#x27;ve only skimmed some of the sources), the abuse vector looks something like this:1) A company files that trivial \"improvement\" patent, or simply picks a recent patent out of its portfolio whose applicability to the drug is unclear. Hilariously, in the linked example from the article, the \"improvement\" was merely to claim that a metabolite was the active molecule rather than the drug per se, and thus change the instructions for when to take the pill relative to mealtimes.2) The company files for an Orange Book listing asserting that the new patent applies to the old drug.3) A generic manufacturer seeking approval disputes this assertion in its ANDA (abbreviated new drug application) filing.4) The FDA provisionally approves the generic.5) The generic manufacturer starts tooling up for manufacturing and taking orders for the product.6) The brand-name manufacturer sues the generic manufacturer, which automatically triggers a 30-month stay on the approval. reply pixl97 11 hours agoparentprevI believe the word you are looking for is fraudulent. And yea, why aren&#x27;t we prosecuting for that. reply virgulino 11 hours agoprevI&#x27;ve been taking the \"blockbuster drug\", Humira&#x2F;Adalimumab, for more than 10 years. Humira alone is estimated to have generated US$200 billion in profits for a single company. Its history, its legal and commercial maneuvers are appalling and possibly the poster child for what is wrong with the patent system. And how public money originally funds much of the research into these drugs.This year I finally started taking a biosimilar. Still extremely expensive. I don&#x27;t have to pay for it, I sued the government to get it, here in Brazil. Unfortunately, this is not an uncommon situation. Our constitution guarantees our right to health and the government&#x27;s obligation to provide it.https:&#x2F;&#x2F;www.npr.org&#x2F;sections&#x2F;health-shots&#x2F;2023&#x2F;07&#x2F;20&#x2F;1188745...https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Adalimumab#Patent_litigation reply fakedang 7 hours agoparentI have been taking Humira for close to 7 years now. What disgusted me was not just the antics of AbbVie in patent protecting Humira, but how my brother (who works at AbbVie) began justifying their acrimonious practices.Although, thankfully now there are a bunch of biosimilars coming onto the market, so we can expect to see further price decreases. For a manufacturing project, we contracted out a study recently on the market sizes (in USD) for a number of biosimilars under development. Humira is the only one that&#x27;s predicted to have a market size reduction (by 40%!) because of how much AbbVie has been milking it. reply firstplacelast 57 minutes agorootparentAs someone that worked in biotech&#x2F;pharma for a decade, they LOVE to roll out the sick-people porn and talk about how we’re all making the world a better place…it helps them pay people less.They can occasionally do some good things, but the large companies are no more ethical than Comcast. The small companies vary, but often have little real goals other than making the C-suite a few extra million no matter what.The last small pharma I left discontinued all of their RNA sequencing experiments because they were worried about the data existing and it causing issues with FDA clearance down-the-line. Basically, don’t study it because it could show bad side-effects…but all of these PhD’s are making the world a better place, good job! Make sure to get married so you can afford a crappy townhome.Gross. reply SuperNinKenDo 11 hours agoprevThese people won&#x27;t stop lobbying to have this kind of exclusivity respected in other jurisdictions as well. Whether through the US government or directly, they&#x27;ve constantly pressured us here in Australia to \"respect\" the same privileges they enjoy in the US. Thank God we have largely resisted these efforts so far. Generics here are highly available for almost anything at the consumer level. Those who can afford to often buy name brand anyway, because of psychological reasons I guess, so pharmaeceutical companies still make stacks of cash, without poor people jeeding to spend half their weekly income on basic meds.I understand and appreciate that the US government doesn&#x27;t arbitrarily begin semi-retributive campaigns, and instead attempts to have companies self-correct. But in instances like this of obvious, wilful abuse of the law, there should be some drawing of blood frankly. It strikes me as a little pathetic that these agencies are always issuing \"warnings\" that they might actually start enforcing regulations. It&#x27;s exactly this that makes these companies feel they have license to behave this way, because they know they&#x27;ll have plenty of time to clean up if the laws actually start getting enforced, and they&#x27;ll have made a tidy profit at everybody else&#x27;s expense in the meantime. reply thenerdhead 12 hours agoprevMaybe they can add a third clause to be “safe, effective, and accessible”?The history of the FDA is pretty interesting and disappointing to say the least. Especially with the supplement craze. reply imiric 3 hours agoparentI don&#x27;t know how the US public can take any of the federal regulatory agencies seriously, given their history of corruption[1]. We only hear about cases that happen to leak, but who knows how many go undiscovered. It&#x27;s easy to be conspiratorial about this and assume that the corruption happens at the highest levels of government.[1]: https:&#x2F;&#x2F;www.businessinsider.com&#x2F;fda-chief-approved-oxycontin... reply eximius 4 hours agoprevOur regulators need to find some conviction.Just do it already. reply hedora 8 hours agoprevDoes this mean we can finally have the already-approved, over-17-year-old, and not-produced Lyme disease vaccine? reply Vecr 7 hours agoparentTo do that for any drug, the FDA needs to approve the new factory setup at a minimum, and that would probably take at least 3 years in practice. Problem is, vaccines are biologicals and you don&#x27;t get generics for biologicals so that&#x27;s out from the start. You&#x27;d have to re-do at least part of the trials to get a biosimilar licensed. reply adolph 11 hours agoprevAt first I thought this was about minor changes to formulations that can be granted new patents, a problem which seems to me to be to be outside the FTC&#x27;s purview. It looks like the problem is more like out and out fraud:Brand drug manufacturers are responsible for ensuring their patents are properly listed. Yet certain manufacturers have submitted patents for listing in the Orange Book that claim neither the reference listed drug nor a method of using it.. . . courts have recognized that improperly listing patents in the Orange Book may constitute an “improper means” of competition. Accordingly, improperly listing patents in the Orange Book may also be worthy of enforcement scrutiny from government and private enforcers under a monopolization theory. reply DangitBobby 5 hours agoparentIt is quite clearly fraud. For some reason we don&#x27;t seem to prosecute companies for fraud. reply AlbertCory 10 hours agoparentprevMy take: she has a solid case on this. You build credibility by winning. She hasn&#x27;t done a good job on that. reply peyton 9 hours agorootparentThe FTC’s statement cites a Shkreli case. I wouldn’t get my hopes up. reply sizzle 3 hours agoprevBiosimilars couldn’t come sooner. reply ilaksh 10 hours agoprevIf they were serious then they would put the CEOs in jail. reply Ekaros 4 minutes agoparentJust invalid their full current patent portfolio. Scary enough punishment to solve the issue. reply tourmalinetaco 8 hours agoparentprevThe point of a CEO is as a fall guy. Only going after the CEO means nothing is solved, the hydra remains. Instead you have to decapitate the company’s leading structure, such as investors and high management, then hope for a systemic change which may or may not come. reply lost_tourist 8 hours agorootparentI agree, seize the company and take ownership of all its assets, including IP, at a minimum. Fines and punishments with no teeth are not any good for the public interest. reply hedora 8 hours agorootparentIt is easier than that.Issue nine shares of stock for every one outstanding, and split it evenly between victims (if applicable) and a new public offering.Make sure the victims have 90% control of the company after that, by giving them multiple non-transferrable votes per share. Also, make sure they are organized enough to vote as a block (to replace board members, etc). reply hakfoo 6 hours agorootparentLet&#x27;s go proactive.In exchange for the legal conveniences that being a limited-liability&#x2F;stock company offers, you must issue to the state a class of shares that are nontradeable&#x2F;zero-dividend (and thus do not dilute valuation) but voting, representing 49% of the voting power.A well-behaved company would see little difference there; the state-owned shares would typically abstain or vote with management. But if they get too brazen and abusive, the board is gonna have to figure out how to handle what is effectively the ultimate hostile investor: \"line goes up\" means nothing to them, but \"the public&#x27;s pissed they still can&#x27;t afford their meds\" does. reply olliej 8 hours agorootparentprevBut what about the harm that does to the shareholders?I wish I were being facetious, but while a lot of scummy people and companies own shares, a lot more just move cash in various forms out of the company into independent entities. If the company has all its assets seized by some mechanism, presumably all the large shareholders: pension funds, 401ks, etc are subject to even more penalties and there are no assets or even potential future income to compensate them. Essentially you get the Great Recession or Enron effect where large numbers of retired or near retirement people suddenly no longer had pensions or retirement savings.Your only real option is to charge all the individuals involved and send them to jail, and fines for everyone and everything else - though the fines for the company have to be more than the potential profit by a large amount.Otherwise you run into a problem where say one person working for the company wants a bonus and realizes a fraudulent listing would get him that bonus? In your model that one person functionally bankrupts the company vs fining and jailing that person and fining the corporation for failing to ensure adequate controls on entries to the orange book. reply m3kw9 9 hours agoprevAlso if it’s patented it cannot be made generic, if expired it can. What’s the point of using the orange book? reply flakeoil 2 hours agoparentYeah, I wonder too. A patent should be registered with the patent office and everyone can search there to see if there is something and what it covers and when it expires. What&#x27;s this orange book all about. Some easier to search database? Why can big pharma enter stuff themselves and why does the FTC not check this? Seems absurd. Can it be that the orange book is an invention by big pharma and their lobby? reply m3kw9 9 hours agoprevWhy not insert every drug so it don’t go generic? I’m not getting how this orange book works reply droopyEyelids 12 hours agoprevI followed Lina Kahn (FTC chair) on Twitter before Biden appointed her, and I was kind of blown away that it happened. She had a very clear philosophical opposition to a lot of the way things are done in this country.And I mean, I followed her because I agreed! But my preferences are not mainstream. Amazing to see her slinging rocks at giants from this position. reply HDThoreaun 12 hours agoparentKhan&#x27;s utter failure to execute on her goals has imo set back the anti-trust movement in this country heavily. Now every time anti trust gets brought up people think about how the government lost 5 cases in a row and consider it a waste. reply ncallaway 10 hours agorootparent> Now every time anti trust gets brought up people think about how the government lost 5 cases in a row and consider it a waste.It demonstrates how weak and toothless are current laws are, and shows a desperate need for Congressional action.Before, when you brought up Congressional action on anti-trust it would get waived away with: \"The FTC has the powers they need and aren&#x27;t using them. There&#x27;s no need for Congress to act\".If someone tried to raise those objections today, they looked silly.The fact of the matter is the executive branch needed to walk into the buzz-saw to demonstrate that the current laws are insufficient and prompt Congress to act. Now it&#x27;s a matter of putting public pressure on Congress to act. reply nobodyandproud 12 hours agorootparentprevMore a warning of how stacked the rules are against anti-trust, after 20-30 years of profitable M&A and other anti-competitive behavior. reply Guvante 12 hours agorootparentprevWith the current stagnated Congress session possibly but now whenever better legislation comes up any attempts to claim existing rules are \"good enough\" is objectively wrong. reply HDThoreaun 11 hours agorootparent\"objectively wrong\" no longer exists in politics, if it ever did. reply Guvante 10 hours agorootparentThe government isn&#x27;t capable of doing the work it wants to do and so \"good enough\" doesn&#x27;t apply.You can claim that the government was wrong in its pursuit of this case but that doesn&#x27;t suddenly mean Congress has no work to do.At minimum codifying what makes it inappropriate for the government to take the action it attempted would be valuable.Congress is supposed to clarify if the Executive and Judicial do not agree on the meaning of their laws.Thus \"objectively wrong\" isn&#x27;t about which side is right, it is that agreement doesn&#x27;t exist.You could claim there was an unimportant misunderstanding after the first but after so many it is obvious there is a disconnect. reply HDThoreaun 9 hours agorootparent> The government isn&#x27;t capable of doing the work it wants to doDebatable. Unless you&#x27;re just referring to the executive branch the government does not have a single goal, and many of the leaders of the government want opposite things.> Thus \"objectively wrong\" isn&#x27;t about which side is right, it is that agreement doesn&#x27;t exist.Sure we can agree there. The issue is that there isn&#x27;t even agreement on whether there&#x27;s agreement. Kneecapping the government is the meta optimal move for minority parties, they gain nothing from trying to make the government more effective. So even if all the lawmakers secretly agree that congress has work to do, they&#x27;ll never admit it. reply Guvante 7 hours agorootparentYou keep saying \"someone might want to do a bad thing\" as if it is an argument.I never said they would do something, which is what you are arguing against. I said there isn&#x27;t an excuse to not do anything based on the existing law covering what needs to be covered.Mostly I point it out because the \"X government won&#x27;t do anything\" is uninteresting to discuss, it is basically an impossible to defeat argument.> Unless you&#x27;re just referring to the executive branch the governmentIn this context the branch of the government whose job it is to take this action is taking an action and so I felt specifying Executive was needlessly specific.It isn&#x27;t like Congress is using this act to go after violators, that isn&#x27;t their job. replyjauntywundrkind 12 hours agorootparentprevI hear this bandied about so much, but I&#x27;ve never heard justification that the cases weren&#x27;t well built or that they weren&#x27;t good cases to bring.The wicked thing about case law is that it builds and builds. The courts have been extremely pro big business for a long time, and it&#x27;s hard to figure out how anything ever changes or gets better from here.Trying to right this hell is going to take a lot of miserable attempts that go nowhere. That&#x27;s what happens when the titans already have all the case law in their favor. Yes that means we need to be very well prepared when we go to battle - try to overturn their legalistic dominance - but I&#x27;m glad we are trying, and I&#x27;m not sure what better paths are available, not sure what the actual lessons are, not sure what should have been better. It feels like such a slim chance. And trying feels necessary, feels like it has a chance, even if odds aren&#x27;t great.Something has to be done to let us revisit the past, but that&#x27;s not how the courts especially operate. The courts are the primary upkeeper of existing decisions, and this feels sometimes like ossification, sometimes feels like it eats away from the flesh of the nation. reply HDThoreaun 11 hours agorootparentcases that lose are not well built. No excuses. Losing a case further entrenches the precedent against you, there&#x27;s no silver lining. reply ncallaway 10 hours agorootparentThat&#x27;s just not true.If you&#x27;re losing on Constitutional grounds, I agree with you. But if you&#x27;re losing the case on legal grounds it can absolutely be a silver lining, since it can demonstrate to Congress a need to legislate (I know, I know, a foreign concept for Congress).Consider Section 230, though. It was passed because the Supreme Court ruled on a case in a way that irritated Congress. The Courts were bound because the law was what it was. But those Court cases prompted Congress to act. The actual rulings from the Court were horrendous precedent, and if that&#x27;s where the policymaking stopped the internet would have been a disaster. But Congress acted and passed Section 230. reply 2OEH8eoCRo0 11 hours agorootparentprevIt&#x27;s also been a mere ~2 years since she was appointed. reply throwaway128128 12 hours agoparentprevBiden&#x27;s had a number of appointments that have been surprisingly on point. I really like how antitrust enforcement is finally, FINALLY getting some teeth. reply throe37848 12 hours agoparentprevMaybe it was her surname. This \"hipster antitrust\" approach is a nice smoke screen big pharma needs. India and other countries already make generics, there is no need to wait several years for US manufacturers. FTC is just a road block that prevents cheap medical imports. reply emodendroket 12 hours agorootparentIs it really though. Even with the current level of regulation we have more than enough scandals with, say, artificial tears that blinded people who used them. reply alwayslikethis 11 hours agorootparentIt&#x27;s all a trade off. We can have more affordable medicine by importing it. This means more people will be able to access it, meaning less bankruptcies, which isn&#x27;t any good for one&#x27;s health. Will it be less safe than the ones you can currently get for an exorbitant prices? Marginally, probably. Overall, society can benefit from letting people import their medicine. It will also drive down prices and force big pharma to drop their prices. reply emodendroket 10 hours agorootparentI don&#x27;t think cheaper medicine that is likely poison is much of a bargain at all, but what do I know? IP law would seem to me to have more to do with the cost than the heavy hand of regulation. reply partitioned 12 hours agorootparentprevFDA should be able to make sure imports are safe reply bigbillheck 9 hours agorootparentprev> Maybe it was her surname.Would you mind expanding on that? reply jacquesm 11 hours agoprevWhy warn them? Just do it already. reply munk-a 11 hours agoparentI like the sentiment - but this is probably the \"just do it already\". Real warnings happen behind closed doors - this is probably a \"We&#x27;re going to create a PR nightmare if you don&#x27;t fix your act, this is the first step on the public being very very mad. And we&#x27;d rather like to avoid going to court against your super-lawyers.\" reply jacquesm 11 hours agorootparentJust revoke a corporate charter or two of the most frequent abusers. That will certainly get the attention of those that remain. Then you can warn them that they&#x27;re next if they don&#x27;t change tack pronto. Big companies are next to invulnerable when you treat them using the same playbook as you would a jaywalker or a person the runs a red light. They love their court cases because they have lawyers on retention and it helps them build an even stronger moat: the moat made up out of negligible legal costs at scale that would criple smaller players. So only large players remain. But revoking a corporation&#x27;s charter instantly crushes that angle of defense and it is used far too sparingly.After that the shareholders get to pick over the leftovers. It&#x27;s the corporate equivalent of the death penalty and since these companies are responsible for the lives of those that depend on affordable medication this sort of money grab is a reason people have reduced quality of life or die. It&#x27;s beyond disgusting and such abuses deserve the harshest penalties available, not a slap on the wrist or a &#x27;stop being soooo naughty&#x27; message.Corporations exist by the grace of governments recognizing them as such. They want to be &#x27;people&#x27; when it suits them and eternal constructs with zero responsibility when that suits them instead. I think we should treat them a bit more like people and kill a couple of the worst abusers. See if that angle yields the right response. reply pests 9 hours agorootparentAgreed 100%. The idea of what a corporation is and what their purpose is in society has been badly warped over the years. A correction is in order. reply NeuroCoder 12 hours agoprevAs much as I&#x27;d love to see ridiculous parent laws in medicine get under control, I have very little confidence that Lina Kahn would accomplish anything. She doesn&#x27;t have a great track record. You really need someone with a good background in patent law specifically for pharmaceuticals to take this on reply rsashwin 12 hours agoparentDefinitely, experience matters. However, I am used to an FTC that turns a blind eye on these Pharma antics. So, as long as FTC takes these kind of cases on, I am all for it. Even the price of generics are way out of line when compared to the generics you get outside of US. Similarly, a lot of Mergers have resulted in fewer competition in the market, leading to rampant price inflation. Federal institutes like FTC need to step up their game here. reply NeuroCoder 12 hours agorootparentI guess increasing awareness on the issue is better than nothing. I&#x27;ve just noticed that she has a gift for identifying problematic industries but struggles to come up with clear criticisms that could translate into pragmatic solutions. reply ConceptJunkie 12 hours agoprev [5 more] [flagged] HankB99 12 hours agoparent [–] Key phrase from the first paragraph:> ... FTC is now warning pharma that it might finally start cracking down.Sounds like a fishing expedition. reply Daishiman 12 hours agorootparent [–] Regulatory agencies have a knack for warning market players to clean up their act before stepping in. reply munk-a 11 hours agorootparent [–] It legitimately tends to be the cheaper way to regulate. You&#x27;ve basically got two options: 1) give a big warning and hope you get a majority change in the industry because they can see the writing on the wall or 2) change the rules and, as immediately as is appropriate, open a bunch of lawsuits... then spend the next decade in the courts spending immense amounts of money while every one of the offending parties runs a big PR campaign about being singled out to comply with a rule that most of the other companies aren&#x27;t complying with.Politics is ugly and rarely will the straightforward approach be the easiest and cheapest. reply HankB99 8 hours agorootparent [–] Yes, that makes more sense than a fishing expedition. reply Applications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The Federal Trade Commission (FTC) has issued a warning to pharmaceutical firms against falsely listing drugs in the FDA's Orange Book to ward off generic drug competition and maintain high prices.",
      "The FTC will thoroughly examine inappropriate Orange Book listings to identify unfair competition and potential illegal monopolization.",
      "Criticisms about the accountability of pharmaceutical firms and the insufficient review processes have been expressed, highlighting broader concerns in the industry."
    ],
    "commentSummary": [
      "The FTC has warned pharmaceutical companies against using false patent listings to hinder the introduction of generic drugs to the market, a practice that stifles competition and retains product exclusivity.",
      "The contentious role of patents in the accessibility and pricing of medicines is the focal discussion, leading to calls for fundamental reforms in the patent system, stricter regulations, potential congressional action, and strong punishments for anti-competitive behaviors.",
      "There is a discussion about the role of regulatory bodies like the FDA and an emphasis on Lina Khan's appointment, speculating her potential impact on addressing patent issues in the pharmaceutical industry."
    ],
    "points": 271,
    "commentCount": 75,
    "retryCount": 0,
    "time": 1695069525
  },
  {
    "id": 37555004,
    "title": "Japan's Hometown Tax (2018)",
    "originLink": "https://www.kalzumeus.com/2018/10/19/japanese-hometown-tax/",
    "originBody": "Kalzumeus Archive Greatest Hits Standing Invitation Start Here About me Japan's Hometown Tax This is outside of my normal software-focused beat, but I met some folks who were very interested in public policy recently. I found, to my surprise, that I probably understand one innovative Japanese tax policy better than very well-informed people who geek out about tax policy [0]. This post hopefully fixes that bug. (Hat tip to gwern for suggesting I write it up.) Two countries in one border The Japanese employment market has a curious feature: there are regions of Japan with extremely high economic productivity (such as Tokyo, Osaka, and Nagoya, but for the purpose of this issue think “Tokyo” and you won’t be wrong) and regions with low economic productivity (substantially everywhere else). This counsels that a young person born and educated in e.g. Gifu move to Tokyo after graduation to earn a living. Many, many do. While Japan’s overall population is declining, Tokyo’s increases by about 100,000 people per year. The regions in Japan are not thrilled about this state of affairs for many reasons. Tokyo isn’t just the seat of Japanese commerce; it also houses the government, media, cultural institutions, etc etc. There is a real sense that your children moving to Tokyo causes them to lose connection with their culture and that the rewards from the national enterprise aren’t being allocated fairly. Tokyo, for its perspective, views the regions with the noblesse oblige that you would expect a cosmopolitan center of culture and learning to have with respect to their benighted country bumpkin cousins. (If this sounds like it echoes the political economy of, say, two large English-speaking nations recently, well, folks greatly overestimate how different Western nations are from each other.) A misalignment in incentives for human capital development Educating children is incredibly expensive. The regions are quite annoyed that they pay to educate their children but that Tokyo reaps all the benefits. This state of affairs has continued for decades. But Japan has a policy response for it, and it is sort of beautiful. Called ふるさと納税 (Furusato Nouzei or, roughly, the Hometown Tax System), it works something like this: A substantial portion of Japan’s income-based taxes are residence taxes, which are paid to the city and prefecture (think state) that one resides in, based on one’s income in the previous year. The rate is a flat 10% of taxed income; due to quirks of calculating this which almost certainly aren’t relevant to you, you can estimate this as 8% of what white collar employees think their salary is. Furusato Nouzei allows you to donate up to 40% of next year’s residence tax to one or many cities/prefectures of your choice, in return for a 1:1 credit on your tax next year. This is entirely opt-in. Anyone can participate, regardless of where they live. In principle, the idea is to donate to one’s hometown. Importantly, one actually has unfettered discretion as to which city/prefecture one donates to. This has some very important implications discussed later. Relevantly to your understanding of the incentives here: most Japanese people do not file taxes every year. Income-based taxes are calculated and remitted by employers directly on the behalf of their employees. Participating in the system requires friction which is somewhat above e.g. changing your direct deposit information but far below e.g. filing a tax return. What was the idea here? Tokyo and the regions could have resolved their differences through the democratic process, in which the regions outvote Tokyo and could have altered Japan’s national tax and economic policies to their advantage. Tokyo obviously doesn’t want this, and instead agreed to an opt-in system which allays some of the regions’ concerns. To the extent that taxpayers donate to their hometowns, Tokyo no longer freerides on the substantial public expenditures required to raise and educate internal migrants. Putting potentially 40% of Tokyo’s residence tax in play is not a small carrot. Individual residence tax is roughly 45% of the city’s revenue. That works out to roughly $30 billion a year. Now if you were a negotiator for Tokyo back in ~2006 when this was being debated, you might have thought “Hmm, while this sounds like it is putting $12 billion a year into play, it’s not actually nearly that bad for us. People have to take affirmative steps to transfer the money to their city of choice, and they have to float the money for most of a year, because of the donate-then-credit mechanism. Uptake on that won’t be that high. Maybe we’ll lose a few tens of millions of dollars; no biggie. Silly country bumpkins; can’t even math.” But after the system was created, city governments started getting really creative. And what happened next is by parts beautiful and crazy. Incentives rule everything around me There exists a culture in Japan of reciprocating gifts. While it varies based on where you live, in the areas I’ve lived, the general rule of thumb is 30%: if you give someone $300 cash on the occasion of their wedding, as is customary for gainfully employed people with respect to someone outside their immediate family, they’re socially obligated to find a way to give you $90 of value back. (The mechanisms for doing this could merit their own post; the word is 返礼品. A dictionary translates this as “quid pro quo”, but the sentiment does not match the common English usage of that phrase. This is simply a ritual; to not participate in it would be non-normative.) While not formally defined in the legislation for the Furusato Nouzei system, someone at a city government figured that it was just not appropriate to let someone just give ~3% of their salary to the city without receiving a token of appreciation in return. So they sent something back; a can of locally-produced plums, say, to remind you of the tastes of your childhood. And this was a beautiful idea! It directly improved the ability of the system to cement relationships between internal migrants and their hometowns, one of the declared goals of the system. It motivated people to fill out paperwork and float the city a bit of money for part of a year, because who doesn’t like free plums. (You might sensibly object that they aren’t free given the time value of money, but prevailing interest rates in Japan are indistinguishable from zero.) And it let cities specialize in marketing this initiative. And specialize they did. A number of cities in Japan, including my adoptive home town of Ogaki, have made this offer: for a no-cost-to-you donation of $100 or more, the city will send someone out to any grave in the city limits. That person will clean the grave, make an appropriate offering, and send you a photo. This is a beautiful thing. Most of the gifts are more prosaic. Locally produced food is very popular. If you miss the taste of home, they’ve got you covered. Cities partnered with local firms to handle the e-commerce aspect, and eventually with platforms to bundle many different items into a single donation; think of it as a shopping cart you could fill with donated money. And then someone asked a fateful question. Where is your hometown, anyway? The Furusato Nouzei system does not define what a “hometown” is. This is mostly by design; Japan historically has a very long-lasting official record of birthplaces which follows one throughout life called the Family Register, and (for reasons outside the scope of this post) it is a major societal issue. Additionally, there was some sentiment that one could have a it-feels-like-home connection to a city that wasn’t necessarily one’s birthplace. Maybe you were born in Tokyo but lived 30 years in a small town in Aichi, like my wife. Maybe you were born abroad but lived 10 years in Ogaki, like me. Maybe you just loved the onsen in Gero and wanted to subsidize them. The government wasn’t willing to adjudicate one’s “true” hometown; 帰る場所 is where the heart is. And then some bureaucrat realized that this created a market: you, as a city government, can bid for taxpayers to select you as a hometown. How does that work? Well, remember the sites which are acting as brokers for donations? They all have search engines, so that you can search by e.g. who has wagyu available if that is your thing. Your thing could, plausibly, be travel to your hometown. So your hometown could, plausibly, buy you tickets back to home. But this would be gratuitously operationally intensive. You have to call city hall. They have to arrange transport. Why do this when Japan is a country with perfectly functioning travel agencies? It would be far better for everyone for your hometown to just send you a gift card to a travel agency. See where this is going yet? A gift card for e.g. Japan’s largest travel agency is a highly liquid cash equivalent. In addition to using it for any good or service from that travel agency, you could liquidate it for about 97 cents on the dollar in any gift certificate exchange in the country. (These are extraordinarily common in Japan.) A few rounds of vigorous capitalism later, many rural towns without large expatriate (inpatriate?) populations and without much to differentiate them in terms of local food had bid the consideration for a donation up, up, up. Eventually the central government stepped in and said that the maximum they’d allow is you rewarding a taxpayer with 50% of the donation in consideration. So, if you “donate” ~3% of your gross salary to one of these cities (which is 1:1 matched by e.g. Tokyo; you’re donating someone else’s money), they will give you ~1.5% of it back in all-but-cash. In 2008, about 33,000 people participated in the Furusato Nouzei system, principally out of genuine charitable concern. In 2016, it was about 2.2 million. They donated on the order of $2.5 billion. The primary accelerant was the bidding war. A contributor was the popularization of Internet sites to broker the donations, which substantially reduce the friction required to participate. Running a site is a very good business to be in; it’s like running an e-commerce business with the special wrinkle that your customers are entirely price insensitive. There are a variety of smaller concerns, but the large Japanese Internet giants (Rakuten, Yahoo, etc) all use their massive built-in distribution and relationships to get an edge here. (The business model is simple: take the money from taxpayers, deduct a cut, spend some on gifts authorized by the city, and remit the remainder to the city periodically. You then periodically give the city an Excel file full of taxpayers. The city periodically sends their donors the requisite paperwork to get the tax credit the next year.) I’d estimate that intermediaries probably soak up somewhere between 5% and 10% of the total donated. This is quite inefficient to accomplish a government-to-government reallocation of resources, but by the standards of Japanese public works projects it is practically free. (Jokes aside: my estimate is informed by the fact that the margins are rich enough such that the intermediaries will happily support you making a donation on credit cards. The actual numbers are probably in a public disclosure somewhere but I don’t have enough time to go looking.) Is this sustainable? Probably? There has been some talk of rolling back the bidding war via administrative fiat, but the cities are quite opposed to this. It’s a great game theory problem: unless a supermajority of cities collectively agrees to limit gifts to a token number, it’s strongly in a city’s interest to duck the central government’s questionnaires and not express any objection to the status quo. (Also remember that the natural anchor for reciprocation is set quite high across much of Japan; the government might succeed in capping reciprocation at 30% but that might be a hard floor for the ceiling.) The penetration rate of this system will likely continue quickly increasing. It’s socially viral: a tax optimization that virtually anyone can take advantage of, has the explicit backing of the government, and feels wholesome. If you’re one of the relatively few taxpayers in Japan who has an accountant, expect them to tell you about this in detail and strongly recommend you max our your contribution every year. Widespread gaming or no, the system pretty much works according to the internal aims. Cities get a list of their internal diaspora, and do make considerably more effort to stay in touch with them than they did previously. (This includes lovely holiday cards and sometimes even I-can’t-believe-they’re-not-alumni-magazines.) You really do get plums from childhood in your mail from your hometown (if you don’t optimize for cash equivalents). Cities with declining local tax bases really do get enough money to do material projects with. Tokyo takes a hit to revenue but can afford it. And there, that’s Japan’s most novel redistribution program in a nutshell. If you live in Japan and want to take advantage of this, hit up your local Google; many sites are happy to make it happen for you. (I don’t endorse any in particular, but any of the top brands or organic search results will work substantially as advertised.) [0]: I had been of the impression up until today that it was literally not on the English-speaking Internet, but this seems to have changed in the last few years. That said, nobody seems to have written about the policy angle in English yet, so here we are. Originally written: October 19, 2018 ABOUT THE AUTHOR Patrick McKenzie (patio11) ran four small software businesses. He writes about software, marketing, sales, and general business topics. Opinions here are his own. OLDER · VIEW ARCHIVE (574) Identity Theft, Credit Reports, and You This is outside my usual brief, but one of my hobbies is that I used to ghostwrite letters to credit reporting agencies and banks. It is suddenly relevant after the Equifax breach, so I’m writing down what I know to help folks who might need this in the future. NEWER What Working At Stripe Has Been Like I joined Stripe to make starting an Internet business easier, mostly by work on Stripe Atlas. It continues to be a fun adventure. Here’s what I’ve learned so far. WHO AM I? My name is Patrick McKenzie (better known as patio11 on the Internets.) Twitter: @patio11 HN: patio11 BITS ABOUT MONEY I also write a Bits about Money, a weekly newsletter on the intersection of tech and finance. © Patrick McKenzie (Kalzumeus Software, LLC) 2006 - 2023. Shoutout to Elle Kasai for the Shiori Theme.",
    "commentLink": "https://news.ycombinator.com/item?id=37555004",
    "commentBody": "Japan&#x27;s Hometown Tax (2018)Hacker NewspastloginJapan&#x27;s Hometown Tax (2018) (kalzumeus.com) 258 points by Michelangelo11 22 hours ago| hidepastfavorite335 comments SamuelAdams 18 hours agoFrom the article:> Educating children is incredibly expensive. The regions are quite annoyed that they pay to educate their children but that Tokyo reaps all the benefits. This state of affairs has continued for decades.We see the same problem in USA states. State taxpayer dollars pay for some amount of public college education for state universities. These are typically called \"State Appropriations\", and varies between 30-80% of the student&#x27;s overall tuition costs.Back in the 1980&#x27;s, taxpayers footed most college tuition using state appropriations, so college was \"cheap\" for students back then. Now (2023), (at least in my state, Michigan), state appropriations typically cover 20-40% of a college student&#x27;s tuition.There is concern with this though, because of what is called \"brain drain\". Citizens spend all these taxpayer dollars educating youth at colleges, then the graduates from U of M, etc all move out of state to California, Boston, Seattle, etc for big tech jobs.Then the state of Michigan doesn&#x27;t see any more tax dollars from those students. Which begs the question, why pay tax dollars for higher education if those who benefit from that education do not contribute back? reply RhysU 17 hours agoparentThis is a perspective issue.The customer was the in-state parent. Rest assured, they were taxed just fine.The child, when it finishes college, should do whatever is best for the child.If the kids are leaving the state, the problem is the state isn&#x27;t attractive to the next generation. Not that the kid somehow skipped out on the state. Heck, the state had an inertial advantage over every other state and still couldn&#x27;t keep the kid there.Source: I grew up in Pennsylvania. Love the place. Left for greener pastures. Don&#x27;t owe them anything out of my paychecks today. But my parents kicked into their coffers for decades and still do. reply marcosdumay 15 hours agorootparentWhatever reason the old city isn&#x27;t attractive to the next generation, it is completely reasonable for people to want to fix it at the national level.If you don&#x27;t, you will only concentrate all kinds of problems on your largest cities, and even invent some wild new problem because cities can not survive all by themselves.That said, a tax based on what city you happened to be born or identify with is the kind of weirdness that my culture can&#x27;t assimilate. But it may make complete sense there (maybe because it&#x27;s more traditionalist? do they want to be that traditionalist?). reply RhysU 9 hours agorootparentThere are too few incentives at the national level. Rent seeking makes the federal government lazy. They don&#x27;t have to do anything. To what country will you go? Nowhere. Coke just has to beat Pepsi by the thinnest of margins.There&#x27;s no need to build another New York City when we already have New York City. It would be folly for the federal government to stand up a competitor to New York City.But! It&#x27;s not folly for Illinois or California to try. Competition is good. reply midoridensha 9 hours agorootparentThey might be \"trying\", but they&#x27;re failing miserably. I&#x27;d say they aren&#x27;t really even trying to make a competitor to New York City: no place in America looks remotely like it. I don&#x27;t see any American city attempting to make itself dense, walkable, and with a decent subway system, except maybe DC (but that one has structural challenges because it&#x27;s split across 3 states for stupid historic reasons). reply seanmcdirmid 6 hours agorootparentI don&#x27;t think California has failed in making competition for NYC in SF or LA. Yes, they aren&#x27;t exactly like NYC, and they have different benefits, but they attract as much of the nation&#x27;s (and world&#x27;s) mindshare these days. reply mcguire 15 hours agorootparentprevI&#x27;m curious about your opinion of the states&#x27; decisions to reduce funding for public universities.Neither the state collectively, non-parent residents of the state, nor the in-state parents get any future economic reward for funding higher education if the student leaves the state, so reducing the state&#x27;s costs in that regard seems rational.But that reduction in funding was part of the rise of the student loan industry, which subsequently seems to have completely deranged higher education funding. reply thfuran 12 hours agorootparent>Neither the state collectively, non-parent residents of the state, nor the in-state parents get any future economic reward for funding higher education if the student leaves the stateThat seems to presuppose that the students don&#x27;t benefit from the education and&#x2F;or there&#x27;s no interstate commerce. reply carlosjobim 14 hours agorootparentprevWhat is the reason to have higher education organized by geography instead of by industry? Shouldn&#x27;t the medical industry be responsible for medical education, for example? Then you don&#x27;t have a state or a city wasting investment on a student who will bail, and you don&#x27;t have students who are disadvantaged for being born in the wrong state or city. reply thayne 9 hours agorootparentprev> the problem is the state isn&#x27;t attractive to the next generationPart of the problem is that brain drain is a vicious cycle. Your home state is less attractive in part because of all the people in your field who left for \"greener pastures\".And they have less funds to put into projects that might make the area more attractive, because they can&#x27;t tax the people who left.I&#x27;m not saying you owe anything out of your paycheck, but it is a problem. Both for the places being left and the overcrowded cities people are leaving to. reply asah 16 hours agorootparentprevcounter-argument: by not having to appeal to (most) families, big cities can focus on attracting young professionals, hence the original moniker: YUPpies. reply bbreier 16 hours agorootparentso if this holds water, wouldn&#x27;t we expect these educated professionals to return to the states which appeal to families when they are ready to start one? reply greiskul 16 hours agorootparentPeople are trying to find stability in a unstable system. The system is unstable because of the shifts in the North America (and World) economy. With manufacturing leaving the US, and the US focusing more on the service industry, the economics of scale of the service industry favour centralization in urban areas, so thats where the jobs end up being created. If the current system stay in place for long enough, it will become stable, with multi generation families having entire cycles in a given region. It&#x27;s the same thing that happened to coal country, just in a larger scale, and slower. reply tzs 16 hours agorootparentprevThere are a couple of probably significant factors that would counter such a tendency.First, the states with the big cities they moved to also have smaller towns outside those cities that are fine for families. They can move to one of those and still have sufficient contact with the city to maintain their career.Second, their spouse likely is not from the same state they are from, and may not want to move to that state. And they may not want to move to where the spouse is from. reply lmm 10 hours agorootparentprevSometimes they do. But someone who&#x27;s uprooting their life like that is probably semi-retiring - at least reducing their hours, having one partner stop working, that sort of thing. And not everyone starts a family, so the cities keep some of the most productive people forever - of course that&#x27;s unsustainable, but it&#x27;s not the cities&#x27; problem. reply rtkwe 16 hours agorootparentprevWhy would they need to move back to their original state instead of over moving to a small city or into the suburbs? reply xp84 15 hours agorootparentI&#x27;m sure a non-insignificant group do move to one spouse&#x27;s hometown, since I can assure you, having your kids&#x27; grandparents close by can be life-changing (especially for modern, dual-income families). reply rtkwe 15 hours agorootparentI&#x27;m seeing a lot less of that among people my age who are having kids. Most stay wherever they&#x27;ve setup their lives but move to a larger house or something similar. reply kelnos 13 hours agorootparentI see the same, but I think the consequences hinted at by the GP are very present. I know quite a few parents, some with their parents nearby, others without. The ones with their parents nearby universally have more of an ability to do things without their kids (either regularly, or as a special-case).And I know several parents who tried the \"go it alone\" method for a while, but eventually decided to move back to one of their hometowns, specifically to be closer to their parents for reasons related to their kids. Most of it seems to be a mix of wanting nearby, reliable childcare, as well as just wanting their kids to grow up with regular contact with their grandparents. (I&#x27;ve also seen the phenomenon where the grandparents, after retirement, end up moving to the parents&#x27; location, but often that requires the grandparents to move to a higher cost-of-living area, so I expect that happens less often.)I don&#x27;t plan to have kids, so this is certainly a grain-of-salt-worthy opinion, but it feels to me that parent-marriages would be on average healthier if the parents had easier childcare options so they can more often do \"date night\" type things. Certainly nearby grandparents is not the only way to achieve that, but it is one of the simplest options, assuming grandparents who are happy to be involved to that degree. replymihaic 16 hours agorootparentprevI&#x27;m trying to see both sides, but how is the parent the customer here?I think you&#x27;re mixing an opinion on high taxation in general (which you might be right about), versus how big metropolises enjoy labor that was raised somewhere else. reply rtkwe 16 hours agorootparentOne of the biggest functions of primary education beyond preparing the kid for their future is as a free(ish) place to stick your kid during working hours. Look at what happened during COVID when schools were shutdown it caused a lot of problems because suddenly someone had to take care of the kid during the day. The chaos that change caused alone is I think excellent evidence for that. It&#x27;s also why schools happen so early even though it&#x27;s clinically proven to be an inferior time to start school for the kid, they have to start that early so the parent(s) can get to their 9-5 job. reply RhysU 10 hours agorootparentprevThe parent is who determines where the child lives during the time when the child&#x27;s residency is established for in-state tuition purposes. The parent is who votes for elected officials in the state. The parent is who suggests to the child the viability of a subsidized in-state education.\"Go to State\" is a recommendation from the elders. If the parents poo-poo State the kid won&#x27;t consider it seriously. reply rowanG077 16 hours agorootparentprevOf course the parent is the customer. Generally the parent is the one who makes sure the child is wel prepared for the future. College is a step in that process. reply mihaic 14 hours agorootparentI agree that the parent is a beneficiary of the education, but my argument is with the fact that \"customer\" involves one beneficiary. By your logic for instance anyone that&#x27;s childless shouldn&#x27;t pay taxes for education, even though they benefit indirectly from an educated population.Isn&#x27;t the company that&#x27;s hiring the graduate just as much a beneficiary for instance? reply lowbloodsugar 14 hours agorootparentprevSimple. It’s not based on the child’s taxes (or future taxes) because then they could go to a California college, claim residency there immediately, and get California prices. Instead, I have to pay an extra $30k or so for them. Further, they haven’t paid any taxes before college, because they were in high school. So again, the fact that each state gives “local” kids a break has nothing to do with the kids taxes. Kids haven’t paid taxes, and kids moving state doesn’t change fees.What does matter? My taxes, as parent. I pay soooo much taxes and I will continue to do so. As a result my kid is essentially forced to go to a college in state. And he will then FOR SURE move out of state. But I won’t.So. It’s a cost to me. Like we are considering moving back to CA to get the UC discount. I will pay more in taxes but I’ll get a chunk of it back in tuition break. reply oh_sigh 16 hours agorootparentprevI would love to hear a realistic plan how West Virginia could make itself as attractive as NY, Chicago, etc. reply monknomo 16 hours agorootparentI think the question is not how to be as attractive as NYC where people from all over move to West Virginia, but rather how to make it so that young people in the 60-90th percentile feel like they may as well stay.A possibility is a great affordability ratio - cheap housing as compared to the prevailing local wage.Remember, inertia to stay where you you are from is strong, you just have to help it along reply midoridensha 8 hours agorootparentAffordability ratio is not the reason young people are moving to NYC. Even with the much-higher salaries (assuming you can find similar jobs to compare), you&#x27;re going to have a very tiny living space in NYC. More likely, the issue is 1) the jobs in NYC simply don&#x27;t exist in WV, and never will, and 2) even if the WV person is comparing similar jobs, WV will never offer the lifestyle that NYC offers. reply mcguire 15 hours agorootparentprevWhat is your opinion of Chattanooga, TN? reply pauldenton 15 hours agorootparentprevKnocking down all the dirt hills until you have enough flat land to build a city the size of NY or Chicago? reply thanosbaskous 17 hours agoparentprevAlaska has had to deal with an extreme version of this issue for a long time. One mitigation that the state has come up with is to offer competitively priced student loans to Alaska residents who are bound for colleges across the US. The interest rate on the loans is further lowered if you live in Alaska after graduation, incentivizing educated Alaskans to come back. Those students are also likely to have maintained their Alaska residency to qualify for the Permanent Fund Dividend (an annual payment that comes from rolling returns on the state’s sovereign wealth fund). reply HDThoreaun 18 hours agoparentprevI went to michigan and tons tons tons of people ended up at the auto companies. Sure many left the state, but there&#x27;s no doubt that the state had many more educated high earners because of the university. reply bern4444 17 hours agoparentprevIts up to the state then to ensure they create an environment that attracts these recently graduated students!Listen to what these post graduate students want - for those who are moving a city like NYC or Boston they are probably enjoying not having urban sprawl, fun downtown areas, available housing, easy and accessible public transportation, etc. reply vineyardmike 17 hours agorootparent> Listen to what these post graduate students want - for those who are moving a city like NYC or BostonI totally agree with everything you’re saying and listed but let’s not pretend the biggest item is anything but “higher salary” for most. We all know that some of these major metros have companies that pay 5-10x what you’d get in a small midwestern town.Anecdotally, I did an internship in Cleveland followed by an internship in Santa Clara (Silicon Valley). I got job offers from both and chose California. The Cleveland company was a major donor to me school, so I got hounded by the schools career development office about why I chose to go to Santa Clara instead. They asked what the company could do to be more attractive to me - they mentioned intern ice cream parties, and Friday hours worked, and mentor programs, etc. The intern pay was $18&#x2F;hr vs $80&#x2F;hr, and when I brought it up, that was not a good answer - they wanted something cheaper easier and more superficial to change.My point is that NYC and Boston have nothing to worry about, and nothing will change. reply leetcrew 6 hours agorootparent> I totally agree with everything you’re saying and listed but let’s not pretend the biggest item is anything but “higher salary” for most. We all know that some of these major metros have companies that pay 5-10x what you’d get in a small midwestern town.there&#x27;s a lot of places in between \"random midwestern town\" and NYC that handily beat both on wages to COL ratio. even just looking at other major east coast cities like DC, atlanta, or boston, NYC is far more expensive and typical engineer pay is only a little higher. reply naniwaduni 15 hours agorootparentprevHey, now, cut all the hours off every day other than Friday and then a 78% pay cut starts looking okay. reply kstrauser 16 hours agoparentprevOn the other hand, if college were still affordable, students could stay where they wanted instead of being all but forced to move to cities with higher salaries. For every 1 software engineer who flees to Silicon Valley, I bet there are 20 accountants, teachers, and nurses who&#x27;d like to continue living in their home state if they could afford it. reply retrac 17 hours agoparentprevThere is a similar phenomenon with Canada, and the USA. University is cheaper and more subsidized in Canada, and then a small but significant % of graduates - often some of the very best - move to the USA, and then never pay another tax dollar towards education, or anything else. It might be a bit annoying, but there&#x27;s little that can actually be done about it, and all the usual arguments about the benefits of an educated populace still apply; the vast majority don&#x27;t move away. reply vineyardmike 17 hours agorootparentIn the US, there are programs where various federal agencies will pay for your education, and offer you a job on graduation. At a high level, the terms require you to repay the cost of education (prorated) if don’t work for that agent for a certain period of time.I can see this working in Canada (or other states that want a boost in educated citizens). Subsidize education via forgivable loans, then forgive the loans if you work in-state. The main issue I see is that the US has a terrible history with loan forgiveness. You always hear stories of (eg) teachers who should get their loan forgiven being rejected for some paperwork issue or simply bank fraud. reply bryanlarsen 12 hours agorootparentprevCanada is probably a net recipient though -- lots of our immigrants were educated in South Africa, Turkey, Iran, et cetera. I suspect we gain more from other countries than we lose to the States. reply femiagbabiaka 17 hours agorootparentprevPure spitballing. What if: in exchange for fully subsidized schooling at a state school, graduates participated in a profit-sharing agreement with the country and city that they went to school in? Similar to the Lambda model (what happened to them anyways?). reply drw3 16 hours agorootparentSued by former students: https:&#x2F;&#x2F;www.forbes.com&#x2F;sites&#x2F;edwardconroy&#x2F;2023&#x2F;03&#x2F;29&#x2F;former-... reply kelnos 13 hours agorootparentSure, but Lambda got sued for misrepresenting material facts about the deal to their students. I don&#x27;t think this lawsuit (or Lambda&#x27;s actions) invalidates the idea GP presented. reply drw3 8 hours agorootparentI don’t disagree, I was just responding to the person who asked what had happened to Lambda :) reply femiagbabiaka 16 hours agorootparentprevYikes! Well this seems like a place where colleges are socialized better: we all know that getting a college degree, especially certain degrees, are no indicator of employability. reply lmm 10 hours agorootparentprev\"Profit-sharing agreement\" is economically the same thing as \"indentured servitude\", which is illegal (for good reasons IMO). reply femiagbabiaka 10 hours agorootparentUnder that line of thought, wouldn’t taxes also be indentured servitude? Or loans? IMO the key is that the government will need to take a loss on the “loans”. reply lmm 9 hours agorootparent> Under that line of thought, wouldn’t taxes also be indentured servitude?Taxes are much like indentured servitude. We allow the government to do things that we wouldn&#x27;t allow private entities to do.> Or loans?Loans for specified amounts are legal, subject to some fairly strict protections (limits on APR, discharging them in bankruptcy, lots of required disclosure). You can&#x27;t generally write a profit-sharing agreement that way - how would you tell the person how much they&#x27;re going to have to repay? How would you stop them declaring bankruptcy immediately to get out of it? replyvolkl48 15 hours agoparentprevWhile not a perfect program, it seems to me that NY State&#x27;s \"Excelsior scholarship\" (for those who&#x27;s family has a household income of We see the same problem in USA states. State taxpayer dollars pay for some amount of public college education for state universities. These are typically called \"State Appropriations\", and varies between 30-80% of the student&#x27;s overall tuition costs.And the world yell the same thing to USA. Canada, Italy, and England etc. all subsides their world-class universities with public funding. Where do the students go after graduation? US. reply seanmcdirmid 6 hours agoparentprevIt is way worse in a state like Wyoming: https:&#x2F;&#x2F;wyomingtruth.org&#x2F;brain-drain-majority-of-university-... (66% leave after graduation). Utah, however, is pretty good about keeping its graduates in state (as well as grabbing some of Wyoming&#x27;s). reply WalterBright 15 hours agoparentprev> all move out of state to California, Boston, Seattle, etc for big tech jobsSeattle is busy complaining about all those tech jobs, and how unfair it is that they make lots of money.https:&#x2F;&#x2F;www.seattletimes.com&#x2F;seattle-news&#x2F;the-splitting-of-s... reply Izikiel43 14 hours agorootparentHate if you do, hate if you don&#x27;t. If there were no tech jobs, people here would complain that they don&#x27;t have them. If there are tech jobs, they complain that there are too many, when in reality they are complaining because they are missing out. reply kelnos 12 hours agorootparentI do suspect that there would be different people complaining in those two cases (with some overlap, sure).Either way, the complains are pretty irrational. I live in San Francisco, where all the property owners are thrilled that their home value has surged in the past couple decades. Take away all the tech jobs, and see what happens to those home values... Can&#x27;t have your cake and eat it too. reply Izikiel43 12 hours agorootparentYeah, Seattle is the same. reply omscs99 14 hours agoparentprevI don’t get why money is so concentrated in NYC and SVWhy are people willing to pay more to get you to move to those areas? It just seems really weird, like there’s some man behind the curtain pulling strings to concentrate people in the above metro areas reply kelnos 12 hours agorootparentBecause that&#x27;s where the businesses and business connections are. It&#x27;s sort of a chicken-and-egg problem. NYC & SV are successful business hubs because they are successful business hubs. Sure, they both had to start somewhere, there&#x27;s no guarantee of their continued dominance, and that&#x27;s not to say other business hubs can&#x27;t come into their own, but it&#x27;s going to be an uphill battle for new locales wanting to attract talent.The pandemic & work-from-home made people realize that these hubs aren&#x27;t really as crucial to business success as people thought they were, but there&#x27;s still a lot of institutional inertia (not to mention shitty companies requiring employees to return to office for no real reason aside from managerial power trips). reply Izikiel43 14 hours agorootparentprevBecause there is money there, it&#x27;s a virtuous cycle. NYC with wall street and tech, SV with VCs and tech. It&#x27;s the place to be for those things, which makes it the place to be for those things, which brings more money in for those things, repeat. reply bsder 11 hours agorootparentprev> I don’t get why money is so concentrated in NYC and SVFor the same reason that Detroit was the technical hub 100 years ago.Smart people congregating leads to more smart people congregating. reply emodendroket 17 hours agoparentprev> Then the state of Michigan doesn&#x27;t see any more tax dollars from those students. Which begs the question, why pay tax dollars for higher education if those who benefit from that education do not contribute back?People who live in Michigan might hope that their children can access a good education. reply jadbox 18 hours agoparentprevI think this is what makes the article so interesting of this hometown tax idea, as I would have liked to have participated if it was here. Fwiw, I moved away from UofM to California for career building last decade, but I moved back this year as I do love Michigan, the highly volatile weather and all. reply wirthjason 15 hours agoparentprevThank you for saying Michigan is your state. Context matters a lot. It’s annoying when people generically use “in my state” or “in my country”. reply Balgair 18 hours agoprevMan, this would be so good for the US to implement.The exact tax details and how money is exactly moved around are, of course, the devil in the details.But, forget all those very important but very arduous details for a minute, and just imagine that system up and working.You pick some small town out there on the Gift website. You like, I dunno, Country Hams. There&#x27;s a small place outside of Harrisburg, PA that specializes in that gift. They send you a medium sized ham around the holidays. It&#x27;s not a mass market ham, as that&#x27;s against the rules. It&#x27;s real home grown and processed country ham. They put in some advert for the country ham festival in October or some month (I don&#x27;t know anything about the ham world, sorry!). Its free tickets to get in. You think, hey, why not? Go out there, spend some hotel&#x2F;AirBnB money, eat at the diners, buy gas, etc. Great little fall escape. You do this every year for a while. The town gets a reputation, they&#x27;re the Country Ham town now.Boom, a little cottage industry, government mandated to exist and help out the people there. The town gets to choose the cottage industry, of course. But they can leap frog it, make something of it, get things going again. Government is just coming in and priming the pump.I don&#x27;t know. There&#x27;s something so damn wholesome about this idea that I just love it. Helping out each other, getting things moving again, diversifying the market, keeping things alive. reply minorannoyance2 18 hours agoparentI hate to rain on your parade, but for every one town trying to \"fulfill your dream\", you&#x27;d get ten doing stuff like \"donate to our square town in the triangle state you hate, and we will pass laws to own those triangles for you\". Then add the Amazon HQ debacle and similar on top, and sprinkle some sweet madness during any campaign season (which is most of the time at this point). I&#x27;m pretty sure that making taxes \"fun\" is \"fun\" in the same way that NFTs are \"fun\" investing. reply sdrothrock 18 hours agoparentprev> Boom, a little cottage industryThis is the devil that lives in the details. You have an industry of people who are trying to get these charitable donations, and the towns that can&#x27;t afford that industry or don&#x27;t have anything particularly commercializable to sell get left in the dust.For every town that becomes Country Ham Town, you have hundreds of small towns who are \"random postcard town\" who only get a few bucks a year. It&#x27;s tough to say if they even make money vs losing money on that. reply Decabytes 17 hours agorootparentI wonder if this could get handled at the county level. The money could be dispersed equitably by the county seat, and people could choose from a number of gifts provided by the towns in the county reply emodendroket 12 hours agorootparentThat&#x27;s a good thought but why stop there? Why not have some kind of national government that collects tax dollars and apportions them according to need? reply midoridensha 8 hours agorootparentprev>and the towns that can&#x27;t afford that industry or don&#x27;t have anything particularly commercializable to sell get left in the dust.What&#x27;s wrong with this? If a town doesn&#x27;t have anything to offer, then why does it exist at all? And why should anyone outside the town be supporting it to help its continued existence?The western US is full of \"ghost towns\": towns that used to exist back in the 1800s during the mining boom then, but when the mines dried up, so did the towns. The people all left and the town fell back to nature. These days, there&#x27;s usually nothing left besides building foundations, if that. Small towns these days that have nothing left to offer should go the same way. reply emodendroket 12 hours agoparentprevI don&#x27;t agree. The whole concept is diverting money from places that need it (the ones with all the residents) to places that don&#x27;t. That was at least sentimentally understandable when people were sending tax dollars to places they have actual connections to, but once it becomes about securing some kind of consumer goods it doesn&#x27;t even have that justification and will obviously benefit most municipalities which least need the help (since they will be most able to put together an appealing package). Anyway, if you want artisanal ham there are plenty of butchers who would appreciate your business. reply kelnos 12 hours agorootparentIs that really the case, though? It seems like Tokyo (etc.) are still doing just fine, and a lot of the smaller towns that act as \"feeders\" for the big cities get a boost, which allows the small towns to do a better job of educating their kids, which directly helps the big cities later on.I think you maybe missed all the details in the article that explains why it&#x27;s a little unfair that the small towns spend money to educate their citizens, but then don&#x27;t get to reap the benefits of that education?I do agree with you that the end result in Japan is now a bit weird, since many people \"pick their hometown\" based on whatever gift&#x2F;deal is available. But I think even in the original formulation, the benefits were much more than just \"sentimental\". reply lmm 10 hours agorootparentThe \"small towns\" that get the most money this way are those that produce the best products, which tend to be the ones that were already the most successful. The most desperate towns gain very little from this scheme because they don&#x27;t have anything to offer. The only reason that Tokyo isn&#x27;t benefitting the most of all is that it&#x27;s specifically excluded from taking part.If the goal was to transfer some of Tokyo&#x27;s tax income to towns that need it more, why not just do that, without all the complex bureaucracy and inefficient loss of tax revenue? If the beneficiary is meant to be the town that educated a person, why not run the system that way? (All that stuff is on record in Japan). reply midoridensha 8 hours agorootparent>The most desperate towns gain very little from this scheme because they don&#x27;t have anything to offer.If a town has nothing to offer, then why exactly does it need to continue to exist?>If the goal was to transfer some of Tokyo&#x27;s tax income to towns that need it more, why not just do that, without all the complex bureaucracy and inefficient loss of tax revenue?Who gets to decide which towns need help, and which towns are left to wither and die? This way, regular people decide which towns actually have some appeal and send their money there. Towns that have no appeal can go by the wayside. Just because a town existed in the past doesn&#x27;t mean it needs to continue to exist. The population is falling and people are moving to cities because it&#x27;s more economically efficient and there&#x27;s more opportunities there. These towns have outlived their usefulness. reply lmm 7 hours agorootparentThe whole argument in favour of this policy was preserving those towns. I don&#x27;t necessarily disagree with you, but in that case why have this transfer of tax money at all?> This way, regular people decide which towns actually have some appeal and send their money there. Towns that have no appeal can go by the wayside.People don&#x27;t pick based on that \"appeal\" though. People pick based on what the \"gift\" is, so it&#x27;s really just the towns that produce something people want to buy. But those towns were already doing fine! reply emodendroket 6 hours agorootparentEven if they did, the notion that people should get to decide how tax money is spent based on some woolly notion of which places they observe from afar are most \"appealing\" is highly questionable in my mind. Why should that be? reply midoridensha 6 hours agorootparentprev>but in that case why have this transfer of tax money at all?To save some towns, I presume. Some places are probably worth saving; other places, not so much.>But those towns were already doing fine!Were they? They can probably use a little boost to help fund their schools or other infrastructure, to keep the towns worth living in for people.>so it&#x27;s really just the towns that produce something people want to buySure, but why should a town that doesn&#x27;t produce anything useful get any funding at all? Just because people lived in a place in the distant past doesn&#x27;t mean people need to keep living there now. If it were really such a desirable place to live, people would be living there now, and there would be a functioning society, not just a bunch of stubborn elderly people waiting to die after all their kids and grandkids have moved to greener pastures. reply lmm 5 hours agorootparent> Were they? They can probably use a little boost to help fund their schools or other infrastructure, to keep the towns worth living in for people.Everyone can find a use for more money, sure. But this was supposed to be a policy to help out the dying towns, not to give a boost to the top handful of non-Tokyo places.> Sure, but why should a town that doesn&#x27;t produce anything useful get any funding at all? Just because people lived in a place in the distant past doesn&#x27;t mean people need to keep living there now.Again, that contradicts the whole premise of this policy. If that&#x27;s your thinking then why have the policy at all, why not just keep the money in Tokyo (which is after all where more people want to live)? reply midoridensha 5 hours agorootparent>If that&#x27;s your thinking then why have the policy at all, why not just keep the money in Tokyo (which is after all where more people want to live)?As I said before, my guess is that some places are worth saving, others not so much. The government here does have the goal of spreading the population out a bit more, because they&#x27;re worried too many people are concentrated in just a few big cities, and a big disaster could be catastrophic for this reason. It does not follow that they want everyone to spread out equally across the whole country, or that every single small town needs to be saved. reply emodendroket 4 hours agorootparentIf that’s the goal of the policy then it would make a lot more sense to develop second- or third-tier cities (Sapporo, for instance) and encourage people to set up businesses, live, and work there, not ship funds to dying little podunk towns. reply midoridensha 3 hours agorootparentMaybe, but the \"podunk\" towns are frequently where a lot of the produce comes from. I imagine no one is growing watermelons or tea in a second-tier city.Besides, they are encouraging people to move to those second- and third-tier cities. There&#x27;s various initiatives for that. reply SamuelAdams 18 hours agoparentprev[deleted, moved to parent discussion.] reply hippich 20 hours agoprevA local Texas guy told me once that he donates quite a bit to the local church, explaining that he would prefer his money do good locally instead of being diverted to the war machine that sucked in his son. reply duxup 20 hours agoparentIt’s not like a church builds roads in his home town…What does that statement really mean? reply huimang 20 hours agorootparentIt means exactly what he said: he wants it to do good locally. There are other ways money can be used locally for good than building roads. reply skipkey 20 hours agorootparentprevIt means that the donations to the church are generally tax deductible, and thus divert some money from the federal government to the church. And the church likely has many charitable activities. reply shortrounddev2 19 hours agorootparentTax deductions only come into play if they are higher than the standard deduction. Unless he is donating tens of thousands of dollars each year (in addition to other itemized deductions), it likely doesn&#x27;t change anything reply seanmcdirmid 6 hours agorootparentIsn&#x27;t the standard deduction going back to normal after the non-corporate part of Trump&#x27;s tax cuts expire? reply downrightmike 19 hours agorootparentprevChurches are a complete scam. reply johnnyanmac 18 hours agorootparentAfter watching the collapse of the Third Place in real time during my life: maybe there was a method to the madness. You realize how utterly herculean a task it is to plan town gatherings and realize that some small (optional but peer-pressured) tithe and bit of prayer isn&#x27;t much different from going to a Cafe to meet up and paying for a drink.I almost wish I could like church for those reasons. reply EatingWithForks 17 hours agorootparentI&#x27;m actually really confused by this claim, as a person raised in a non-white immigrant household. I bought onto the idea of churches as community because my immigrant churches were full of apple picking, group meals in the church itself, thanksgivings at the pastor&#x27;s house, etc. But someone recently told me in Catholic mass you don&#x27;t even get a church-wide breakfast?? WTF? How the hell does community get formed if the actual worshipping actions isn&#x27;t inherently a community building action? Someone told me the guy can bless you if you don&#x27;t eat the bread, but you can&#x27;t even ask for a specific blessing, like \"my son needs an A, can I get the whole church to pray for him\" style stuff.I&#x27;m way more skeptical of the notion of church as a community now. If all the church community is via volunteer charity actions, just volunteer directly unaffiliated with a religion. reply johnnyanmac 9 hours agorootparentI think it really depends on the church, and in my case I was talking about African-American dominant churches. So there may be a stronger sense of community there than the White Catholic churches.For me, it was sort of in-between. There wasn&#x27;t a church-wide breakfast except for some small snacks during the big days (Easter Sunday and Christmas). But there was plenty of donation drives that would provide food for lower income families in the area. And you very much had a chance to worship and ask for a specific blessing, sometimes in a very public manner (This Boondocks clip is surprising accurate portrayal: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=HtP7cbbAFLA)> If all the church community is via volunteer charity actions, just volunteer directly unaffiliated with a religion.That&#x27;s the thing, I have tried. But in my experience, volunteer work to meet friends is fleeting. You won&#x27;t be making a few deep connections even if you do your best to come every week, you&#x27;ll see a revolving door of people who may only come once or twice. At least among my age group, there isn&#x27;t much consistency in participation.And that fleeting, revolving door seems to be theme of modern adult life. I don&#x27;t know what in particular made the same people come to pray for some higher being every week, but it&#x27;s something I haven&#x27;t seen in any other attempt to meet people in my adult life. reply atdrummond 17 hours agorootparentprevCatholic Churches do plenty of community meals - just not around mass. This is likely due to the former requirement to fast proper to communion. reply EatingWithForks 17 hours agorootparentI think that&#x27;s bizarre though. Mass is the one requirement everyone needs to do together-- obviously, that&#x27;s the best time to sit with a meal and gossip. Even on important holidays, easter mass etc. are the few times limited practicing catholics will appear. This is the perfect time to ask how they&#x27;ve been, how&#x27;re the kids, and welcome them with fresh hot food, maybe invite them to the litter picking up effort next week.I&#x27;m extremely skeptical of church as a community building thing now, because based off of my white friends recollection there&#x27;s very little actual community around worship itself. There are a bunch of volunteer actions outside of it, but just going to worship isn&#x27;t itself community. This makes me question why we can&#x27;t have a generic mutual aid group do this instead, like the Anarchist Soup folks. reply mbg721 17 hours agorootparentThe Catholic position would be that that isn&#x27;t what the Mass is for; it&#x27;s a ritual sacrifice that meets a real spiritual need. Similarly, the priest doesn&#x27;t need to be especially likeable or even give a homily in order to say a valid Mass. reply EatingWithForks 15 hours agorootparentNone of those have to do with community formation, which is what the subject is about. reply mbg721 15 hours agorootparentSure, but you&#x27;re going to be fighting an uphill battle if you want to make the Mass into that. reply EatingWithForks 14 hours agorootparentRight, so I&#x27;m pointing out that places of worship may not actually be great Third Spaces. They may not offer the community that people say they do. replykelnos 12 hours agorootparentprevThey could always offer breakfast after mass ;) reply kelnos 12 hours agorootparentprev> How the hell does community get formed if the actual worshipping actions isn&#x27;t inherently a community building action?Yep, Catholicism (American white-people Catholicism, at least) is weird like that. Maybe it&#x27;s different in different places or among different ethnic-majority parishes, but I grew up in New Jersey and Maryland, and was dragged to mass weekly by my (devout) parents. There wasn&#x27;t really any kind of community there; we showed up, took our seats, did the rituals, and then left and went home.They also put me through CCD (Catholic version of \"Sunday school\") for 8 years, and while I want on some (more or less mandatory) retreats and activities with my fellow classmates, I don&#x27;t recall any of those relationships extending outside class. I didn&#x27;t end up forging any closer relationships with the other kids who went to my regular school, either. Granted, I may not be typical: I decided I didn&#x27;t believe when I was in 5th or 6th grade, so I did the bare minimum just to get through it; maybe others tried harder and worked to make it more of a community.I did work for our church&#x27;s music director one summer in high school (mostly odd jobs and clerical stuff), and would play trumpet in the church&#x27;s small ensemble for (rare) special events (regular mass just had a piano&#x2F;organ player), but I didn&#x27;t really get much out of that, aside from enjoying playing some music.> but you can&#x27;t even ask for a specific blessing, like \"my son needs an A, can I get the whole church to pray for him\" style stuff.The church we went to in Maryland did have a small section of time devoted to community prayers. It wasn&#x27;t specific, like they would just read a list of names (I think you had to sign up in advance to get someone on the list), and after each name, the congregation would respond \"Lord, pray for them\" or something like that.> I&#x27;m way more skeptical of the notion of church as a community now.I think it just depends on which religion&#x2F;denomination, and the conditions in the local area. Some churches might do more to try to build community than others. And some churches might actually do build community, but many people who come for services just don&#x27;t care to participate for whatever reason.Personally I don&#x27;t have a positive view of religion, but I do think it&#x27;s a shame if some community-building has gone by the wayside due to declining church attendance. reply dabluecaboose 17 hours agorootparentprev>But someone recently told me in Catholic mass you don&#x27;t even get a church-wide breakfast?? WTF?That really depends on your individual parish in my experience. The church I grew up going to had donuts after ever 10:30am mass, and the one I go to now has free breakfast. I&#x27;ve also been to ones that have nothing at all.It&#x27;s also extremely common for Catholic churches to hold a \"Friday Fish Fry\" every Friday during Lent reply kelnos 12 hours agorootparentprevMore detail downthread, but I don&#x27;t feel like growing up attending Catholic mass every week did much community-wise. We&#x27;d show up, sit down, do the rituals, and head home, without really interacting much (if at all) with fellow parishioners.Certainly there were people who were more involved -- the church had a youth group, for one thing, and a bunch of outreach programs. But those were the kind of things that you had to specifically seek out, apart from the usual religious services. And if you aren&#x27;t going to get community-building out of the religious services themselves, then you might as well build community outside the church entirely.Not saying that all religious&#x2F;church communities are all the same; my girlfriend in high school was much more involved with her church community (different religion). But her experience seemed to be more of an outlier than the norm. reply johnnyanmac 9 hours agorootparent>but I don&#x27;t feel like growing up attending Catholic mass every week did much community-wise. We&#x27;d show up, sit down, do the rituals, and head home, without really interacting much (if at all) with fellow parishioners.I felt the exact same way growing up. But that&#x27;s the thing: while I hated church my parents&#x2F;grandparents got really involved. It wasn&#x27;t unusual for my grandparents to meet up with friends after Church at some buffet and talk, and my grandmother was involved in a few non-sunday service activities. My mother met a few friend in the area through church and sang on the choir.I don&#x27;t know if they were outliers, but the key here was 1) meeting people in the area (my mom moved constantly and church was one of the first things she joins) and 2) having an easy excuse to do something after church on Sunday. I struggle to do monthly rituals with friends nearby and meanwhile two generations of family could do weekly luncheons after participation in a place that was fine with bringing kids into.I should note that this might still be a division of the genders, though. My Grandfather didn&#x27;t do much more than drop off&#x2F;pick up my grandmother. reply lmm 10 hours agorootparentprev> We&#x27;d show up, sit down, do the rituals, and head home, without really interacting much (if at all) with fellow parishioners.Did you not have tea and coffee in the hall after? That was how it worked at mine. reply bigstrat2003 19 hours agorootparentprevSome churches are, sure. Churches as an overall category are absolutely not a scam. reply jessepasley 19 hours agorootparentprevAre they more of a scam than the wars in Iraq and Afghanistan? reply minorannoyance2 18 hours agorootparentNo, but they are more of a scam than most taxes. Historically speaking, churches haven&#x27;t exactly abstained from war or pretty much any other atrocity. In terms of our current reality, they are usually not better value for money than taxes. Any time you can point to corruption, misuse of funds, politics or other issues, it is worth remembering that this applies to the churches as well. reply dahfizz 15 hours agorootparentEven if this is true \"historically speaking\", it doesn&#x27;t have to be true of this guy&#x27;s local church. Despite the bad press generally, an individual church can still be an unambiguous benefit to its community.I would be willing to bet that the majority of local (single location,whenever anyone is widowed and made an orphanMan, only in the South can that be the same event. reply xeromal 14 hours agorootparenthahahah, I walked into that one. ;) reply mulmen 20 hours agorootparentprevIt means charitable donations are tax deductible.Local governments don’t always provide the community services of churches either. reply ImJamal 18 hours agorootparentprevChurches will often times have food pantries, soup kitchens, hospitals, orphanages, homeless shelters, etc. They provide resources to the poorest in their community. reply mbg721 17 hours agorootparentMy parish actually divides its money up pretty cleanly; the first collection each week is for the upkeep of the church building, its school, etc., and there&#x27;s a second collection that rotates, but once a month it&#x27;s for the St. Vincent de Paul Society, which does rent assistance, furniture and coat drives, and the like, regardless of the recipient&#x27;s religious affiliation. reply aaomidi 20 hours agorootparentprevChurches do usually do mutual aid though. And sometimes much more humanely than the government.(Fwiw I’m not a Christian nor do I care for religion) reply tomatotomato37 18 hours agorootparentprevIt means the US government spends 10 times as much on killing others (military spending - 13%) than it does on essential infrastructure (transportation - ~2%) reply kelnos 12 hours agorootparentI don&#x27;t want to disagree with your general point (because I agree that our infrastructure is not great, and that the US glorifies the military in ways I find uncomfortable), but let&#x27;s also remember that doing a good job at two wildly different things may not cost the same amount of money.While I do think the US should be spending more on essential infrastructure, it is entirely likely that the US could end up doing an exemplary job at that, but still spend less money on it than on the military. reply callalex 15 hours agorootparentprevI’m not sure why that’s relevant as I have never heard of a church building trains and sewers… reply pauldenton 15 hours agorootparentI never heard of a local government building trains either. reply pauldenton 15 hours agorootparentprev2% of most city budgets is spent on roads. It&#x27;s not like the local government is primarily spending tax money on road maintenance reply Zach_the_Lizard 20 hours agoprevAs a military brat who doesn&#x27;t really have a hometown, or even a clear regional identity, I like that this system lets you choose a town to donate to without forcing it to be connected to your birthplace or life history.It&#x27;d be nice to be able to earmark it to particular government functions, maybe even charities in the area, etc. reply NeoTar 20 hours agoparent> It&#x27;d be nice to be able to earmark it to particular government functions, maybe even charities in the area, etc.\"[Your Town] has finest municipal art galleries in the state whilst streets are filled with pot-holes\"Allowing people to allocate money to government functions is generally a bad idea except if very tightly limited. Things which are &#x27;nice&#x27; will get over-funded, and things which people find unpleasant* will not getting the funding needed. I think the problem would be ten times worse if it is people who don&#x27;t actual live in an area get to choose how funds are allocated. reply teeray 19 hours agorootparent> things which people find unpleasant* will not getting the funding needed.Septic systems are a great example of this on a home scale. So many people treat them as a mysterious vortex in the ground where all sewage magically vanishes. They don’t bother to pump the tank every few years because it costs a couple hundred dollars. Then they’re shocked when their leech field is completely failed and needs to be replaced to the tune of $30k. reply Scoundreller 17 hours agorootparentI can&#x27;t wait for (nicer) incinerating toilets to cost less than $5k. Going to put a real damper in septic tank builds in areas that allow simple graywater systems. reply johnnyanmac 18 hours agorootparentprevTo be fair, the current realistic alternative still has potholes. Maybe we should entertain a tightly limited allocation. Sure, let the people from outside of town donate to whatever (it&#x27;s free money at best and no gain at worst), but local citizens can only allocate a portion to specific public goods. reply soperj 18 hours agorootparentprev> \"[Your Town] has finest municipal art galleries in the state whilst streets are filled with pot-holes\"Maybe they have too many roads? reply naniwaduni 18 hours agorootparentSounds like they don&#x27;t have enough art galleries. reply version_five 20 hours agoprevNot sure I understand all the subtlety but as a Canadian (where we have about 3 big cities and the rest has no voice) I&#x27;d love to be able to divert my tax away from the city I live in and to my home town. If course we don&#x27;t really pay income tax directly to the city, so it would be complicated and probably just make more bureaucracy. reply pc86 20 hours agoparentI say this as someone who grew up in a small rural area and now lives in a slightly-less-small suburban&#x2F;rural mix (but not in Canada). All this does is overfund rural areas and underfund urban areas which are likely already underfunded - although my only experience is the US so maybe Canada is a little better about this.Rural areas do not \"have no voice\" - they have exactly the representative power of their population, the same as urban centers. I&#x27;m not sure what makes people in or from rural areas thinks that once people live close together their voice should count for less. How is it \"right\" or \"fair\" to move to an urban center, take advantage of urban amenities and services, take advantage of that higher salary, and siphon some of that tax money away to rural areas because of nostalgia? reply jdasdf 20 hours agorootparent>Rural areas do not \"have no voice\" - they have exactly the representative power of their population, the same as urban centers. I&#x27;m not sure what makes people in or from rural areas thinks that once people live close together their voice should count for less. How is it \"right\" or \"fair\" to move to an urban center, take advantage of urban amenities and services, take advantage of that higher salary, and siphon some of that tax money away to rural areas because of nostalgia?2 wolves and sheep deciding on what&#x27;s for dinner huh?Have you considered that the key problem here is the lack of direct control that people have over their own lives, causing these sort of situations where you have groups of people who permanently have zero effective agency over their futures?I agree, giving people more voting powers just because they live in a certain place isn&#x27;t the right way to fix this. But the fact that there is a problem is undeniable, and the fact that this inequality in voting helps ameliorate this problem is true as well.The right answer here is to remove power from the state and elected officials, and return it to where it belongs, those being asked to pay for it either directly, or by being affected by it.There is nothing as democratic as everyone choosing for themselves.In the same way we should not have a vote to decide what&#x27;s for dinner, and the winner means we all eat the same thing, we should not have votes on local and personal matters.Democracy isn&#x27;t voting, it&#x27;s agency over what rules you. reply pc86 20 hours agorootparentLots to unpack here, and as I said I don&#x27;t know much about Canada and next to nothing about Japan so this is strictly about the US.> 2 wolves and sheep deciding on what&#x27;s for dinner huh?Tired analogy that doesn&#x27;t actually make any point.> groups of people who permanently have zero effective agency over their futuresThis applies to approximately 0% of people in the US. Yes there is a small proportion of people who are so poor they can never move away from where they&#x27;re born. Most of these people happen to live in urban areas, not rural.> the fact that there is a problem is undeniableFor it being undeniable you haven&#x27;t actually said what the problem is - only a strawman that rural Americans have no agency over their futures which is utter nonsense.> return [power] to where it belongs, those being asked to pay for it either directly, or by being affected by itIf democracy is 2 wolves and a sheep deciding what&#x27;s for dinner wouldn&#x27;t you want more power in the state and elected officials, so they could protect the sheep?The problem is everybody acts like they&#x27;re the sheep being taken advantage of by big bad government when it&#x27;s almost never actually the case. reply em-bee 19 hours agorootparentIf democracy is 2 wolves and a sheep deciding what&#x27;s for dinner wouldn&#x27;t you want more power in the state and elected officials, so they could protect the sheep?it would be better to give more independence to the sheep so that deciding what&#x27;s for dinner doesn&#x27;t need a majority vote of all of them. if we ignore the obvious outcome that the wolves want to eat the sheep, but simply look at the fact that wolves want meat and sheep want grass, it simply doesn&#x27;t make sense that the majority here gets to dictate what everyone should eat, instead each group should be able to make their own choice.that means localizing decisions where that makes sense. and not allowing the majority to control resources that a minority depends on. among other things. reply jdasdf 20 hours agorootparentprev>Tired analogy that doesn&#x27;t actually make any point.It does, but it seems you&#x27;ve missed.Let me make it clear as day.If there&#x27;s a country with 3 people on it, where 2 of those people want to oppress the third, there is nothing that the third can do to prevent that oppression.That is a perfectly legitimate outcome of a voting system, and one that occurs often.>This applies to approximately 0% of people in the US. Yes there is a small proportion of people who are so poor they can never move away from where they&#x27;re born. Most of these people happen to live in urban areas, not rural.On the contrary.If you&#x27;ve ever been affected by a law or regulation you&#x27;re against, you&#x27;ve directly suffered from this in regards to that topic.Indeed, the whole idea of \"red states\" and \"blue states\" that is common in US politics kind of shows it happening at a large scale.For example I&#x27;ve recently seen news about parts of eastern Oregon wanting to secede and join Idaho, and the primary reason for that is that you have a minority (rural eastern Oregonians) who are permanently disenfranchised with effectively zero agency over the laws and regulations that rule over them, as a result of them being a minority in Oregon.This is the problem.>If democracy is 2 wolves and a sheep deciding what&#x27;s for dinner wouldn&#x27;t you want more power in the state and elected officials, so they could protect the sheep?On the contrary, for it is that power that is being used to oppress. reply EatingWithForks 19 hours agorootparent> For example I&#x27;ve recently seen news about parts of eastern Oregon wanting to secede and join Idaho, and the primary reason for that is that you have a minority (rural eastern Oregonians) who are permanently disenfranchised with effectively zero agency over the laws and regulations that rule over them, as a result of them being a minority in Oregon.Isn&#x27;t this also the case of blue cities in red states? I believe e.g. there&#x27;s a big hullabaloo in Virginia at the moment where the bluer school district close to DC is resisting anti-transgender top-down demands from the state. If we want to argue about rural people being disenfranchised, by number, way more people in cities are disenfranchised simply because cities are more populated. reply jdasdf 19 hours agorootparent>Isn&#x27;t this also the case of blue cities in red states? I believe e.g. there&#x27;s a big hullabaloo in Virginia at the moment where the bluer school district close to DC is resisting anti-transgender top-down demands from the state. If we want to argue about rural people being disenfranchised, by number, way more people in cities are disenfranchised simply because cities are more populated.Absolutely, and my point isn&#x27;t about a \"rural vs urban divide\" at all.It is a general argument and principle that can be applied to every action of government.It doesn&#x27;t matter if oppression is done by a dictator or by a Majority.The problem is too much state power. reply EatingWithForks 18 hours agorootparentI don&#x27;t know, let me know if this is slippery sloping or something, but I don&#x27;t see why the issue is the state has too much power. You will always have a disenfranchised political minority unable to force the majority to follow whatever political position they want. There&#x27;s conservatives in my city who feel disenfranchised about crime; does the city have too much power, then? What if I feel disenfranchised about no one believing me in an Among Us game? reply jdasdf 17 hours agorootparentYou&#x27;re missing the point.Being enfranchised in driving the state is only important if the state has a significant impact in your life.The lower the impact (read, the power) of the state, the lower the need to be enfranchised.For your example: If the state didn&#x27;t have law enforcing powers, the states policy on crime wouldn&#x27;t matter much. You could just hire your own private police to protect your interests. reply dragonwriter 17 hours agorootparent> For your example: If the state didn&#x27;t have law enforcing powers,Then its not the state [0], whatever it calls itself, in the same way that Emporer Norton, despite his claimed title, was not the head of state of the United States.[0] though it may be part of a larger structure involving elements formally outside of its bounds that together constitutes an effective state. reply dahfizz 15 hours agorootparentprevSo you agree that this is a problem then, and we should work to increase agency and reduce disenfranchisement by moving more power to local communities? reply Izkata 20 hours agorootparentprev> Indeed, the whole idea of \"red states\" and \"blue states\" that is common in US politics kind of shows it happening at a large scale.> For example I&#x27;ve recently seen news about parts of eastern Oregon wanting to secede and join Idaho, and the primary reason for that is that you have a minority (rural eastern Oregonians) who are permanently disenfranchised with effectively zero agency over the laws and regulations that rule over them, as a result of them being a minority in Oregon.Northern California is the same, with this proposal to solve it: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Jefferson_(proposed_Pacific_st... reply pc86 19 hours agorootparentprevBeing subject to a law or regulation you&#x27;re against is not oppression, and you&#x27;re not suffering. It can be, but that&#x27;s not true by definition.Having to do something you don&#x27;t want to do, or not being allowed to do whatever you want, is not oppression. reply jdasdf 19 hours agorootparentDefine oppression then.Because I fail to see any conceptual difference here. reply warkdarrior 14 hours agorootparentBy your definition (as far as I can tell), all laws are oppression. Is there any law to which there won&#x27;t be any opposition by some person out there? No, certainly not. So we must have laws and the debate must be whether particular laws are too restrictive or not. reply dfxm12 19 hours agorootparentprevThis is an interesting allegory, but it&#x27;s doesn&#x27;t seem to mesh with reality. After all, in US voting, n is much larger than 3, and 66% represents a super majority...Can you put into concrete terms? How many times has Philadelphia and Pittsburgh bullied their way into something that the rest of the commonwealth of Pennsylvania was so against? I think you&#x27;ll find that what usually happens is the rest of the commonwealth gets its way, regardless of what the big cities want (because they have the numbers in the state legislatures). reply pc86 19 hours agorootparentIt&#x27;s interesting that you bring up Pennsylvania - nearly every regulation, law, whatever that affects cities is either only enforced for \"cities of the first class\" or cities of the first class are specifically excluded. There&#x27;s only one first class city in PA and I&#x27;ll give you three guesses which one it is. So essentially you end up with three sets of laws - 1) applies to all Pennsylvanians and typically have nothing to do with cities or how they work 2) applies only to Philadelphia 3) doesn&#x27;t apply to Philadelphia but applies everywhere else.You&#x27;re actually making a good argument in favor of jdasdf&#x27;s point. Just try to get a concealed carry permit in Potter County, and one in Philadelphia County, and see how different they are. Both completely legal because there&#x27;s totally different laws in play for the same process. reply dfxm12 18 hours agorootparentThe premise was that big cities, like Philadelphia & PGH would somehow undemocratically impose their will on the rural parts of the state. Like you say, that isn&#x27;t happening. Philadelphia&#x27;s regulations aren&#x27;t being imposed in Potter County. reply jdasdf 19 hours agorootparentprevI mean I gave an example in Oregon but you seem to have skipped it.If you&#x27;d like a more concrete example, how about we look at the effects that these minority&#x2F;majority impacts have had on the delegates sent to the presidential college?Here:https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;List_of_United_States_presiden...As you can see you have several states that have always voted for the same party for decades.For example, Alaska, Idaho, Kansas, Nebraska, North Dakota, Oklahoma, south Dakota, Utah and Wyoming... Have voted for the republican candidate every single year since 1964. How many Democrats live in those states? These are literally millions of people who by virtue of being a minority have had exactly zero agency in who will rule over them.On a more local level, how many regulations are voted on by people in cities, but will inevitably affect rural people? Or heck, any people who did not want those regulations?For people being oppressed, who have no actual way to prevent their oppression, does it really matter if the oppression is being done by a dictator or a vote? reply dfxm12 18 hours agorootparentI mean I gave an example in Oregon but you seem to have skipped it.That&#x27;s not an example of what you&#x27;re saying. It appears to be in ongoing discussions. ¯\\_(ツ)_&#x2F;¯On a more local level, how many regulations are voted on by people in cities, but will inevitably affect rural people? Or heck, any people who did not want those regulations?What level are you talking about? I mean, what locality spans city and rural areas? These are great hypothetical situations, but you gotta put real examples behind them to show that there is some conspiracy about oppressing people living in rural areas. Even at a state level, due to the nature of districting, urban areas don&#x27;t have any more representation in state legislatures.As you can see you have several states that have always voted for the same party for decades.You&#x27;re now changing your argument from urban-rural to D-R? reply ndriscoll 17 hours agorootparentNot GP, but recent things off the top of my head: California&#x27;s law to end single family zoning. Gun laws in most blue states. Washington&#x27;s comprehensive sex ed law.It&#x27;s not really urban-rural or D-R as such. It&#x27;s more \"people who don&#x27;t even live in your town are dictating how to run it\".Like the recent ruling with abortion: it made it not a federal concern. That&#x27;s it. Why do people in California care so much about what e.g. Alabama is doing now? Do they get upset about abortion laws in Poland too?Higher levels of government are needed to prevent conflicts (e.g. to deal with someone in Colorado polluting the river and screwing over everyone in Utah), but they end up getting used to enforce social norms. It would be better if we could push more regulation down to the county&#x2F;municipal level. People wouldn&#x27;t need to be so polarized because they could just agree to disagree, and live somewhere with like-minded people.Want to live somewhere where there&#x27;s strict credentials for teachers? Cool. Want to live somewhere where the lady down the street that homeschools her kids can also teach yours? Cool. It&#x27;s a big country. There&#x27;s room for everyone. reply dfxm12 16 hours agorootparentCalifornia&#x27;s law to end single family zoningNot sure how this fits in. It only had 25% nay votes in the assembly and 17.5% nay votes in the senate. Some nay votes came from a D from SF and someone form Huntington Beach, so the vote really wasn&#x27;t on party or urban-rural lines. I also wonder how genuine you&#x27;re being when you say a measure that gives someone more freedom (to build what they want on their property) is someone telling you how to live your life.Gun laws in most blue states.I&#x27;m not really sure laws what you mean. We have to recognize that nationwide, 63% of adults are dissatisfied with our current gun laws. In light of this, we have the SCOTUS striking down gun laws that most people want. This doesn&#x27;t appear to be the win you think it is. https:&#x2F;&#x2F;news.gallup.com&#x2F;poll&#x2F;470588&#x2F;dissatisfaction-gun-laws...Washington&#x27;s comprehensive sex ed lawI&#x27;m not sure what to say when you think direct democracy is bad with regards to state measures.Like the recent ruling with abortion: it made it not a federal concern. That&#x27;s it. Why do people in California care so much about what e.g. Alabama is doing now?Well, this specific issue aside, this current trend of conservatives trying to legislate from the SCOTUS (people appointed for a lifetime) is largely undemocratic. If you don&#x27;t want people who don&#x27;t even live in your town dictating how to run it, you should be very opposed to SCOTUS doing just that, since they basically answer to no one. reply ndriscoll 15 hours agorootparentYou&#x27;re really missing the point here. The entire point being made is that what the voters at the level of a state or the entire nation is irrelevant because it shouldn&#x27;t be their decision to vote on.Even if 75% of California residents want to ban single family zoning, if 75% of residents in Redding want to keep it in Redding, why should people in San Diego have any more than 0% influence on that decision? People 600 miles away should have exactly zero input into something like that.It&#x27;s not about whether the decisions being made are good. It&#x27;s about who gets to make them&#x2F;how broadly they apply. China could have the greatest ideas in the world, but it doesn&#x27;t make sense to \"democratically\" vote with equal representation between US and Chinese citizens on what free speech laws in the US should be. They have 4x the population and nothing in common. Such a \"democracy\" would be a farce. reply dragonwriter 15 hours agorootparent> Even if 75% of California residents want to ban single family zoning, if 75% of residents in Redding want to keep it in Redding, why should people in San Diego have any more than 0% influence on that decision?Because Redding isn&#x27;t an independent, self-supporting, sovereign entity, its an administrative subdivision of California whose existence as an entity and powers of government are delegated to it by the people of the State of California through the Constitution and laws of said State (in part directly, and in part indirectly through, e.g., Shasta County, a similar but higher-level subdivision within which Redding is nested), and which is funded in no small part by distribution of taxes set and collected by and from the State of California as a whole. reply ndriscoll 15 hours agorootparentNo one is discussing the legality of such things. The discussion is whether things like that are just. That&#x27;s why e.g. eastern Oregon has a secession movement. The west side of the state can legally tell the east side to do what they want. The east side does not feel that being in a shared democracy with the west side is working. reply dragonwriter 15 hours agorootparent> No one is discussing the legality of such things. The discussion is whether things like that are just.That&#x27;s what I was discussing, too.If there was a Redding-separatist movement, we could discuss whether Reading separatism is just and whether that changes the justice of decision-making arrangements, but the justice of decision-making authority within the current context is not independent of the structure of that context aside from the particular decision-making question being examined, and where “Redding” fits into that structure.(This is also a separate question from the justice of particular decisions; like it can be just for the people of California at large to be the decisive decision-making authority, and still be unjust for them to make a particular decision. But that wasn&#x27;t the question.) reply ndriscoll 15 hours agorootparentThere is a secessionist movement in NorCal[0], but as has been said before:> Prudence, indeed, will dictate that Governments long established should not be changed for light and transient causes; and accordingly all experience hath shewn, that mankind are more disposed to suffer, while evils are sufferable, than to right themselves by abolishing the forms to which they are accustomed.The bar for injustice is quite a bit lower than secession-inducing. If you have broad agreement in place X that they would like to live one way, and agreement among places Y and Z (who have never even been to X and would be entirely unaffected by what X does for the decision at hand) that X should live another way, then I assert Y and Z imposing their will on X is unjust, regardless of how democratic the decision was, or whether some generations-old political structure says Y and Z can do that. The people of Y and Z should recognize that they shouldn&#x27;t do that.[0] https:&#x2F;&#x2F;www.sacbee.com&#x2F;news&#x2F;politics-government&#x2F;capitol-aler... replyjdasdf 17 hours agorootparentprev>You&#x27;re now changing your argument from urban-rural to D-R?This tells me you missed the argument i was making.The point isn&#x27;t a specific urban-rural or Dem-Rep, it&#x27;s the underlying concept of not having agency over what rules you.The more powers you grant the state, the less powers you have, and the greater these issue become and the more important acquiring political power becomes. reply pauldenton 1 hour agorootparentprevIf you&#x27;re a solid blue city in a reliably Red state, that is in Region next to a blue State, I would have no problem with control of said region Changing hands. It would be best if there was a give and take though, especially if there is a rural community in that blue state that is solidly Red and hasn&#x27;t felt represented by the blue state majority Cements communities that are in place but makes people happy reply irrational 20 hours agorootparentprev> For example I&#x27;ve recently seen news about parts of eastern Oregon wanting to secede and join Idaho, and the primary reason for that is that you have a minority (rural eastern Oregonians) who are permanently disenfranchised with effectively zero agency over the laws and regulations that rule over them, as a result of them being a minority in Oregon.This goes both ways. In Utah, the urban liberals feel like they are disenfranchised because the majority suburban&#x2F;rural population of the state consistently votes conservative.Anytime you have a majority (whether that is majority liberal urban in Oregon or majority conservative rural in Utah) you will have a minority that feel disenfranchised. reply ejstronge 20 hours agorootparentprevCuriously, rural regions of the US have a fair amount of power due to how electoral seats are distributed. I&#x27;m not sure if this is also true in Canada, so it is possible that the GP&#x27;s situation differs from your own experiences. reply mlinhares 20 hours agorootparentThey&#x27;re also heavily subsidized to make them \"work\", a considerable piece of rural America would already be gone if there wasn&#x27;t a large amount of government funds being diverted to keep it alive and we will spend even more as it becomes less and less desirable to live there. reply subroutine 20 hours agorootparentAs soon as WFH became an option people flocked away from cities. NYC lost 200k people over two years; SF lost 60k. I would say there is a large population of city folk who desire to move to more rural areas. reply dfxm12 20 hours agorootparentIf you listen to the news, they&#x27;re all moving to other cities, like Austin, TX, not to rural areas. reply ghaff 19 hours agorootparentYes. Other&#x2F;smaller cities, college towns, etc. I&#x27;m sure some have taken the opportunity to move somewhere that looks more like rural--but I have to believe the lifestyle shift would be a bridge too far for most. reply mlinhares 19 hours agorootparentprevThey&#x27;re moving to suburbs near big cities or cheap big cities in less populated states, not rural areas. reply ghaff 18 hours agorootparent>suburbs near big citiesIt&#x27;s not really true of the Bay Area--and it&#x27;s at least complicated around Manhattan. But a lot of cities like Boston an hour or an hour and a bit drive in reverse commute, much less weekend, traffic can bring you to a lot cheaper housing. There are expensive suburbs too. However, it&#x27;s pretty easy to find suburban or exurban towns that are accessible to Boston for an evening event that have relatively modest home prices. Not Midwest cheap but reasonable. (Of course there are very expensive towns as well.) reply CalRobert 18 hours agorootparentprevWell yes, when an area is heavily subsidized it can be attractive to live there. reply ericmay 20 hours agorootparentprevSure but what would that have to do with subsidization?What’s the GDP of New York City versus upstate?As an aside this silly concept of “rural&#x2F;urban divide” is just that.Silly.We need farms and farmers to feed us, it’s ok to subsidize some of these places so that happens.It’s also ok for the people doing the subsidization to ask the rural people to stop complaining about “socialism” or other nonsense when they’re the primary benefactors of handouts. reply subroutine 19 hours agorootparentWhat would the GDP of NYC be, if NYC didn&#x27;t have a near monopoly on the proximity to high paying jobs in NY State? If every NYC job (aside from those that absolutely required people to work on-site) allowed full time WFH, my bet is that it would be significantly lower. And people with high paying wages would be more scattered around the state, requiring less subsidizing. It would also mean that not every highly educated person moves away to the city. Part of the reason urban and rural politics is so divided is that every smart kid from a small town goes away to college and never comes back. Some because they have no desire, but others because the job they want doesn&#x27;t exist outside a metro. reply jwells89 18 hours agorootparentThere may be some element of truth to that, but jobs aren’t the only reason why highly employable young people tend to move to urban areas. There’s other big reasons, like there being a lot more to things to do and a vastly wider variety of large hobby&#x2F;interest communities and cultures in urban areas.This is why the bulk of people moving away from city centers during the pandemic moved to cheaper cities or the suburban metros surrounding cities rather than small towns, and unless small towns stop being small towns it’s not going to change. reply ericmay 19 hours agorootparentprevIdk, but you&#x27;re intentionally responding as though you missed the point so I don&#x27;t have much else to say here. replyCalRobert 20 hours agorootparentprevThis is about suburbs, not rural areas, but Canadian politics provides an interesting history in suburbanites wresting power from city dwellers. Among other concrete impacts it means more parking, shifting funds away from public transport (which is less useful to suburbanites), etc.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Amalgamation_of_Torontohttps:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=KkO-DttA9ew&t=35s reply Zach_the_Lizard 20 hours agorootparentprevI&#x27;m guessing New York farmers don&#x27;t have as much power as their Montana cousins.Rural and low population _states_ have a fair amount of power due to having a fixed number of senators per state, but rural areas in an otherwise non-rural state may find themselves outvoted and thus mostly irrelevant.The opposite situation, city slickers outvoted by country bumpkins, is of course possible in rural states. reply runako 20 hours agorootparentThis obviously varies by state, but tends to not be accurate at the state level either due to the way state legislatures are elected.For example, in my state the majority of residents live in the largest metro area. But the smaller towns & rural areas are able to more or less permanently control the legislature (because legislators have geographic districts). As a result, our laws are frequently written in ways that are openly hostile to our major city, while favoring more rural areas. reply pc86 20 hours agorootparentprevYou&#x27;re 100% right, the more rural your state the more power your state holds in Presidential elections, and indirectly in the Senate, but not much beyond that. I don&#x27;t think there&#x27;s a corollary in Canada but I could certainly be wrong. reply bryanlarsen 20 hours agorootparentprevNominally, it&#x27;s also true in Canada. Due to lower populated rural ridings, one rural vote can be worth up to 4X an urban vote.But in current conditions, votes in rural Western Canadian ridings are basically worthless. They&#x27;re ultra-safe Conservative seats, so they can be and are ignored.But it wasn&#x27;t very long ago that they were highly competitive ridings split between two right wing parties. Because Canada doesn&#x27;t quite have the same 2 party system as the US does, the situation isn&#x27;t as static. reply ghaff 20 hours agorootparentprevThat’s true of states that are as a whole relatively rural (think Wyoming) but not, say, upstate NY. reply icodestuff 11 hours agorootparentprevMore, even, since ridings don’t all have to have the same population, unlike Congressional districts. reply pauldenton 7 hours agorootparentprevMajor urban cores of Canada are not underfunded Toronto, Montreal and Vancouver do not need more of a political voice. They do not need more love and attention they have too much focus already. reply dfxm12 20 hours agoparentprevI&#x27;m not sure what you mean by \"voice\". I think in common use, you would mean power in government. This scheme was put into play because rest of Japan, in fact, has a louder voice than Tokyo:Tokyo and the regions could have resolved their differences through the democratic process, in which the regions outvote Tokyo and could have altered Japan’s national tax and economic policies to their advantage. Tokyo obviously doesn’t want this, and instead agreed to an opt-in system which allays some of the regions’ concerns.Doing some back of the napkin math wrt representatives in the House of Commons and senators, in Canada, the situation probably wouldn&#x27;t be much different for Vancouver, Montreal and Toronto. In a lot of ways, this where the US is. NY (the city) foots the bill for not only most of NY (the state), but also its tax money funds projects and services in other states as well.Living in a US state&#x27;s biggest city myself, it is a drag to see the rest of the state constantly block legislation that will help us grow, while on the flip side they are gladly taking our taxes, as we pay by far the most into the state&#x27;s coffers. What I mean to say is, due to the way we pay local, state&#x2F;province and federal taxes, the big cities are already a net positive in terms of tax revenue for the rest of the province&#x2F;country. reply fomine3 9 hours agorootparentThe person who heavily promoted \"Homegrown\" Tax (Suga) has been elected in Yokohama, that is virtually a part of Tokyo metropolis. This tax system is loved by richer city people because they can get more gifts without additional paying, . It&#x27;s hated by richer city local govt for obvious reason. I hate this tax system but he is smart to make this system accepted. reply DiggyJohnson 20 hours agorootparentprevThis is a relatively common use of the word \"voice\", I don&#x27;t think GP \"means\" anything else. A politician running for election might promise to give a voice to some part of their constituency that so far had been ignored (no political power). In general \"the power of the people\" ~= \"the voice of the people\". reply dfxm12 20 hours agorootparentI thought so, too, but it didn&#x27;t make much sense given the context of the article and what they were saying :) reply DiggyJohnson 20 hours agorootparentYes it does, in my opinion. Seems very clear. Canadians outside their three big metro areas feel like their government represents those metro areas and not the rest of Canada. They feel like they \"don&#x27;t have a voice compared to the big 3\". reply dfxm12 20 hours agorootparentI guess how someone feels and the actual reality of the numbers can be different. replyonlyrealcuzzo 20 hours agoparentprevAren&#x27;t you essentially doing that already?The Federal government spends disproportionately outside of cities. reply thsksbd 20 hours agoparentprevIt&#x27;d certainly be more elegant and transparent than the equalization payments with largely similar results (Maritime&#x27;s getting help from their diaspora). reply renewiltord 15 hours agoparentprevIn the US, this is pretty simple actually. If you set up a charity whose sole purpose is beautifying some region (that will pass IRS&#x27;s tests), you can donate to it and reduce your taxable income. I imagine Canada has a similar structure. reply paxys 16 hours agoprevWhat the author calls a \"beautiful\" system to me seems to be bordering on fraud and undermining what the law was originally intended to do. Basically random small towns hire consulting companies who run advertising campaigns asking people nationwide to send them their 40% \"hometown donation\" and get 20% of it back as a \"gift\", thus conveniently evading half their taxes? Who exactly is benefiting here? reply friend_and_foe 5 hours agoparentIt sounds less like fraud and more like gamification of incentives. Fraud requires someone to be defrauded. The rules of a game are laid bare, thus looks to me to be an emergent cultural quirk that benefits everybody. reply pauldenton 6 hours agoparentprevIt&#x27;s cultural. Japan has a cultural of reciprocal gift giving that underpins this entire exchange. Basically it&#x27;s a decentralizing force It prevents a doom loop where everyone moves to Tokyo because that&#x27;s where all the resources are because everyone moves to Tokyo as the rest of the country hollows out reply poulsbohemian 16 hours agoprevHad beers with a community college president last week... he shared that there&#x27;s a problem brewing where Arizona State (and perhaps others...) are going to high schools offering a program where for low tuition rates, kids can pick up some college credits. Sounds great on the surface, but the problems are that a) it means fewer of those students attending programs at the local community colleges, IE: taxpayer dollars going out of state, and b) apparently the credits offered are really only transferable to - you guessed it - Arizona State. It&#x27;s all part of a disagreement between the school districts and the community colleges over where the kids are spending their time, and thus getting their share of state dollars. reply flanbiscuit 18 hours agoprevprevious discussions:posted 2018, 249 comments: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=18256660posted 2022, 105 comments: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31152365Not here to complain about a repost, just mentioning in case someone is looking to dive further and see what people said before. reply dang 14 hours agoparentThanks! Macroexpanded:Japan&#x27;s Hometown Tax (2018) - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=31152365 - April 2022 (105 comments)Japan&#x27;s Hometown Tax - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=18256660 - Oct 2018 (249 comments)(And yes, agreed - such links are solely for those with extended curiosity. Reposts are fine on HN after a year or so - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsfaq.html). reply robertlagrant 20 hours agoprevThis was great! One question, on this:> And there, that’s Japan’s most novel redistribution program in a nutshell.How is this \"redistribution\"? reply badcppdev 19 hours agoparentAll taxation is redistribution. This is a novel tweak to taxation.I think all tax payers wherever should get some form of choice in where their tax money goes even if it&#x27;s only a small proportion of the total. reply friend_and_foe 5 hours agorootparentAnd importantly, even if they have a less than altruistic motivation for their decision. It&#x27;s perfectly OK if someone gives to someone else because they benefit in some way. reply madarcho 20 hours agoparentprevRedistribution of taxes, from areas of high densities to lower density prefectures. reply robertlagrant 20 hours agorootparentThanks - but isn&#x27;t this more like \"reallocation\"? I thought redistribution was more about taking money from some people and giving it to (or spending it on) other people, of which taxation is one method. reply moate 18 hours agorootparentYou take money that would have ostensibly gone to the citizens of Tokyo and instead provide it to the citizens of town of your choice.Reallocation and redistribution are synonymous here, one is just used in more headlines (probably because more people use \"distribute\" more regularly than \"allocate\"). Nothing different should be inferred by either word choice IMO. reply pc86 20 hours agoparentprevJust because it&#x27;s a person choosing to do this instead of a bureaucrat doesn&#x27;t mean it&#x27;s not redistribution. To use the example in the article, you&#x27;re taking 40% of your taxes which would go to Tokyo and instead sending them to Gifu. reply kibwen 20 hours agoparentprevIt&#x27;s redistribution because it takes tax income that would go to a wealthy area (Tokyo) and distributes it to poorer areas. reply pauldenton 6 hours agoparentprevIt is tax deferment Taxpayers who contribute more than 2,000 yen can have their income tax and residence tax reduced. The amount deducted is the taxpayer&#x27;s entire contribution minus 2,000 yen and set amount. To receive the subtraction, the taxpayer files a final tax return reply supernova87a 8 hours agoprevOn the practice of returning 30% of a gift back to the sender as yet another polite reciprocal gift --Having been to Japan now and having seen the amount of gift wrapping and boxes and packaging of little food items and treats, I wonder if this simple social rule actually drives quite a notable amount of consumer spending and economic activity in the country! reply friend_and_foe 5 hours agoparentIt&#x27;s a positive feedback loop for sure, but a diminishing one: you&#x27;re not expected to reciprocate 100%, and you&#x27;re not expected to reciprocate 30% of your 30% reciprocation you received.Americans have a gift reciprocation culture as well, particularly around birthdays and Christmas. reply MarketingJason 18 hours agoprevTwo initiatives I&#x27;ve thought about:1. Schools receiving state tax dollars must adhere to percentage-based breakdowns of general tuition use. Basically, only x% of general tuition can be spent on administration costs, sports facilities, etc.2. Percentage of tuition covered by tax dollars is set by local demand for that degree path and adjusted periodically. reply intrasight 20 hours agoprevThis needs clarification. If you can gift it anywhere, can you gift it back to Tokyo (for example)? I would think that the kickback that the place you actually live (services, access to events, etc) would be more valuable to the taxpayer than would be services in a city where you don&#x27;t live. reply sampo 19 hours agoparent> If you can gift it anywhere, can you gift it back to Tokyo (for example)?If you live in Tokyo, your residence tax goes to Tokyo by default. You don&#x27;t need to do anything, you don&#x27;t need to specifically gift it to Tokyo. reply dfxm12 18 hours agoparentprevThe article explains it is an opt-in program; if you live in Tokyo and don&#x27;t opt-in to gifting your taxes elsewhere, you&#x27;re paying all your taxes to Tokyo. reply matheusmoreira 19 hours agoparentprevGood point. Is Tokyo not participating in the bidding war for people&#x27;s good will? Perhaps it&#x27;s not allowed to? reply Hamuko 19 hours agorootparentConsidering it was an olive branch towards the rest of the country in order to avoid this being set into the national law, it&#x27;d probably look pretty bad if Tokyo started to try and outbid the rest of the country. reply rtpg 19 hours agoparentprevYou can&#x27;t gift it to where you live (or rather, where you will send your residence tax). reply clove 7 hours agoparentprevCan you gift it to Tokyo if you live outside Tokyo? reply UtopiaPunk 14 hours agoprevWell, I don&#x27;t know if this is a good system or if a similar system could be applied well in my country, but the article certainly made me smile. reply fomine3 9 hours agoprevThe core concept of Hometown tax is okay, redistribute money to non-rich areas. How it currently work sucks. People chose where to donate by how much gift those local govt return for tax payers. Quite inefficient and nonsense criteria. reply TacticalCoder 16 hours agoprev> Educating children is incredibly expensive.So people who&#x27;ve been homeschooled can opt out of that tax right? I mean: that&#x27;s only logical. reply petesergeant 20 hours agoprevI find it curious that more countries don&#x27;t do what America does, and tax you wherever you live. While this policy would absolutely screw me over as someone who lives in a very low tax country, I&#x27;m not sure what the argument _against_ it for a country like the UK is. reply Georgelemental 19 hours agoparentThe US can only do this because it has sufficient power to force other countries to report information on how much US citizens make to the IRS. If the UK tried making the same demands, the most likely responses from other nations would be \"screw you, no\" and \"that sounds like a lot of work, pay us.\" reply ghaff 20 hours agoparentprevFor one thing, in Europe at least, people are relatively mobile between countries. While it&#x27;s different legally of course, imagine US states trying to tax you permanently even if you move elsewher",
    "originSummary": [
      "The Furusato Nouzei, a Japanese tax policy, allows taxpayers to donate part of their residence tax to any chosen city or prefecture for a tax credit.",
      "Initially designed to reduce economic disparity and foster connections to hometowns, the system has morphed into a competitive marketplace where donors can select a hometown depending on the gifts or services provided.",
      "Despite possible inefficiencies in resource reallocation, this system's popularity is fuelled by bidding wars and online platforms. It's sustainable and benefits the cities by enhancing contact with their diaspora."
    ],
    "commentSummary": [
      "The article addresses numerous themes like talent retention in major cities, the influence of grandparents in childcare, and the concept of a hometown tax, demonstrating the complexity of societal structures.",
      "It delves into the effectiveness of churches in fostering community connections and contrasts this with government spending, touching upon the workings of agency in a democratic setting.",
      "The piece examines the rural-urban divide and disparities in political opportunities while also noting Japan's Hometown Tax program as a unique approach towards funding rural areas."
    ],
    "points": 258,
    "commentCount": 335,
    "retryCount": 0,
    "time": 1695038227
  },
  {
    "id": 37554736,
    "title": "How to do literal web searches after Google destroyed the “ ” feature?",
    "originLink": "https://news.ycombinator.com/item?id=37554736",
    "originBody": "I used this quite frequently but since Google \"\"\"\"improved\"\"\"\" it last year (there was a popular HN post complaining about this) it doesn&#x27;t work anymore. Search for a domain name with quotation marks for example just recombines the contents of the domain and returns a bunch of unrelated content completely cluttering what I am looking for. Until last year it used to return no search results if there weren&#x27;t any exact matches, which is the whole point.Does someone have a work around for this phenomenal Google decision?",
    "commentLink": "https://news.ycombinator.com/item?id=37554736",
    "commentBody": "How to do literal web searches after Google destroyed the “ ” feature?Hacker NewspastloginHow to do literal web searches after Google destroyed the “ ” feature? 257 points by 7moritz7 22 hours ago| hidepastfavorite194 comments I used this quite frequently but since Google \"\"\"\"improved\"\"\"\" it last year (there was a popular HN post complaining about this) it doesn&#x27;t work anymore. Search for a domain name with quotation marks for example just recombines the contents of the domain and returns a bunch of unrelated content completely cluttering what I am looking for. Until last year it used to return no search results if there weren&#x27;t any exact matches, which is the whole point.Does someone have a work around for this phenomenal Google decision? TradingPlaces 21 hours agoKagi. Never have I been so happy to send someone $10 every month. When you become the customer, not the product, it’s amazing what can happen. reply netsroht 20 hours agoparentBeing logged in while making search queries in search engines poses significant privacy risks. The searches can paint a comprehensive profile of the user, and these data often remain stored for extended periods. There&#x27;s a chance this information might be shared with third parties. Coupled with other user data, these logged-in searches can pave the way for targeted advertising, sophisticated predictive analysis, and potential exploitation by governments or malicious entities. In the event of data breaches, the user&#x27;s logged-in search histories can be exposed. Furthermore, users typically don&#x27;t have clear insight into how their data is utilized when logged in.I hope Kagi introduces an anonymous access feature. For instance, it could incorporate zero-knowledge proofs (ZKPs). These are cryptographic techniques where one party (the prover) can confirm to another (the verifier) that a claim is accurate without disclosing any additional information. This is especially beneficial for authentication scenarios where it&#x27;s essential to avoid sharing extra details.To implement zero-knowledge authentication for quota API access:1. Token Creation:- Each month, users receive a token tied to their identity and quota.- The token can be split for use on multiple devices using cryptographic methods.2. API Access:- Clients present a zero-knowledge proof (ZKP) to confirm they have a valid token and haven&#x27;t used up their quota. The server verifies this without seeing the exact details.3. Client Synchronization:- Each client tracks its quota usage.- Synchronization can be peer-to-peer or through a centralized, encrypted server to prevent double spending of the quota.4. Quota Renewal:- Monthly, old tokens expire, and new tokens are issued.Challenges:- ZKPs can be resource-intensive.- Token security is crucial; there should be a way to handle lost or compromised tokens.- The system should prevent quota \"double-spending\" across devices.- If a centralized server is used for synchronization, it should operate with encrypted data.This way Kagi would only know who their customers are but not what kind of searches they make. reply freediver 18 hours agorootparentKagi already provides a way to search anonymously via a random email address (we do not really verify it or need it for anything) and Bitcoin&#x2F;Lightning payment [1].Since you are interested in cryptography, there is a discussion on Kagi feedback site along the same lines as your idea, about possible ways to achieve this without the need for cryptocurrency. [2][1] https:&#x2F;&#x2F;blog.kagi.com&#x2F;accepting-paypal-bitcoin[2] https:&#x2F;&#x2F;kagifeedback.org&#x2F;d&#x2F;653-completely-anonymous-searches... reply netsroht 17 hours agorootparentThanks for the links. Using a disposable email with crypto payments and occasionally generating a new account to unlink from previous searches could be a viable intermediate solution.Also, I found this link [1] in the thread you mentioned. They seem to have implemented something like that.[1] https:&#x2F;&#x2F;metager.de&#x2F;keys&#x2F;help&#x2F;anonymous-token reply freediver 14 hours agorootparentJust to make it clear, Kagi does not link searches to an account already, to begin with. Refer to our privacy policy [1]. We simply do not need that data for anything and it would be just a liability for us. Our philosophy is that users should personalize the search feed themselves and this is why we built features like the aiblity to block or promote domains, create search lenses and many more.However there is no technical way of proving it. So cryptocurrency and cryptography are ways to achieve anonimity from a perspective of a user, regardless of what we are doing.[1] https:&#x2F;&#x2F;kagi.com&#x2F;privacy reply boredpudding 19 hours agorootparentprevAny system that can check balance, can link searches to a user. There&#x27;s no way around it. In your case, Kagi would need to trust the client with the balance, which would be insecure.There&#x27;s only one solution, and that is that you need to put a bit of trust in Kagi. Compared to the major one, Google, you can chose between one that promises to not store data, and one that promises it does (and does a lot).It&#x27;s always a bit sad that here on HN, when companies try to do better than bigger players, there&#x27;s always people who think it isn&#x27;t enough. It has to be absolutely impossibly perfect. reply smsm42 17 hours agorootparent> Any system that can check balance, can link searches to a user.I don&#x27;t think it&#x27;s true. I can immediately see at least two ways how it can be done without identifying the user.1. Each user gets X tokens at the beginning of the month. When searching, user supplies a token, which is immediately burned. The token does not contain the user identity, just signature validating it&#x27;s a valid token.2. Variation of the above: each user gets a token good for X searches at the beginning of the month. When searching, the system will return a token good for N-1 search each time token good for N searches is presented. Again, no need to contain user identity anywhere in the system.Of course, both solutions have their downsides (sync between multiple devices, stealing tokens, losing tokens, etc.) but it id definitely possible. And I am sure if somebody spent a little time thinking on it, these ideas can be seriously improved to eliminate the downsides without introducing the need to identify the user. reply phil9909 13 hours agorootparentIn both these cases the search engine provider could easily store your identity together with your token while issuing it and recover the identity once the token is used without any way to prove this from the outside. They could even issue tokens in the form AES_ENC(\"SOME KEY ONLY THEY HAVE\", USER_IDcounter) and you would not notice. You would have to trust them that they won&#x27;t do this, which is no improvement to the current thing Kagi does (saying they won&#x27;t collect any data, while admitting they can&#x27;t prove it, you just have to trust them). reply netsroht 18 hours agorootparentprevI&#x27;m not a cryptography expert, but from my research, shouldn&#x27;t it be possible to verify quota on ZKPs server-side? Essentially, the server doesn&#x27;t need to know the specifics of the user&#x27;s identity, just that they possess a valid token and haven&#x27;t exceeded their quota.You can use search engines like Google without being logged in. When combined with tools like uBlock Origin and Cookie AutoDelete, it becomes more challenging for them to build a singular profile about a user, especially one tied to payment methods such as credit cards.I genuinely appreciate what Kagi is doing, and I&#x27;d absolutely be willing to pay for their service, because if you&#x27;re not paying for a service, you&#x27;re the product. I trust companies to uphold their privacy promises, but \"Trust is good, but proof is better.\" ;) reply gizmo686 17 hours agorootparentThe issue is implementing it client side. ZKP means that you cannot simply embed a token in the URL, but instead need to participate in an active protocol. You could implement this in JavaScript, but then you need to trust the JS being served from the server.Even once you do that, you have all the other tracking mechanisms that the server could use if it wanted to. reply KRAKRISMOTT 18 hours agorootparentprevThey key word is server side. You have no way to verify that they are not tracking sessions as an user. reply yencabulator 18 hours agorootparentprev> Any system that can check balance, can link searches to a user.For what it&#x27;s worth, you can buy a physical Mullvad gift card and use that to create a very anonymous account for VPN use.Even if you buy your gift card from a major online retailer, it comes from a stack of gift cards, nothing tracks which one was sent to whom. You can also exchange gifts among friends. reply andrewinardeer 10 hours agorootparentprev> Being logged in while making search queries in search engines poses significant privacy risks. The searches can paint a comprehensive profile of the user, and these data often remain stored for extended periods. There&#x27;s a chance this information might be shared with third parties. Coupled with other user data, these logged-in searches can pave the way for targeted advertising, sophisticated predictive analysis, and potential exploitation by governments or malicious entities. In the event of data breaches, the user&#x27;s logged-in search histories can be exposed. Furthermore, users typically don&#x27;t have clear insight into how their data is utilized when logged in.This reads and smells like ChatGPT &#x2F; AI. reply idonotknowwhy 10 hours agorootparentWas thinking the same thing. Not even gpt4 reply SkyPuncher 7 hours agorootparentprevI’ve gotten tired of these boogey man arguments.There are sooooo many other ways to fingerprint than an account.Oh look, this MacBook with X by Y resolution from this IP address has had 100 searches for the past 2 hours. Oh no! He switched to incognito. reply EA-3167 19 hours agorootparentprevI&#x27;m not searching for anything terrifyingly illegal, and for the rest Google and MS already scrape and compile every byte of data I&#x27;ve ever generated. Why would it suddenly be a problem when a more reliable and less vicious company is doing a fraction of that?You have to understand that most of us aren&#x27;t fighting some battle for \"perfect privacy,\" I just want a search engine that works for me, rather than advertisers, at the level of the search results themselves. reply netsroht 18 hours agorootparentI get your perspective. A lot of us just want a search engine that serves the user first, not advertisers, especially at the results level. It&#x27;s about function over strict privacy for many--everyone has their own privacy threshold.But it&#x27;s also about digital data autonomy. It&#x27;s not just about avoiding surveillance over sensitive searches, but having control over our data&#x27;s destiny. Even mundane data, in aggregate, can sometimes be used in ways we can&#x27;t predict. reply alwaysbeconsing 18 hours agorootparentPersonally privacy is a strong concern for me; I have many aspects of my digital life set up less conveniently in exchange for privacy.In this case though we&#x27;ve have on one hand a product that definitely does aggregate data about searches, and doesn&#x27;t do what I need very well; and the other a product that could, but does not currently aggregate data, and does an excellent job serving my needs.And importantly there is no option of a product, available now, that is verifiably prevented from aggregation. Even a VPN unless I disconnect and get a new random IP between every individual search does not provide that protection. (And then browser fingerprints even.) reply carlosjobim 14 hours agorootparentprevWhat is counted as \"terrifyingly illegal\" changes without a moments notice on the whims of your rulers. So even if you&#x27;re not googling on how to bomb the government, there are hundreds of other subjects and opinions that could in the future make the majority of your neighbours, family and workmates think you deserve to be shunned, fired, in prison, or worse. That is why people want to protect their privacy. reply snide 21 hours agoparentprev100% agree on Kagi. Happy customer. Thought it would be just another one of my attempts to use Duck Duck Go that dies after two weeks of !g usage. Turned out Kagi just works. The biggest improvement &#x2F; gains is on mobile, where you suddenly don&#x27;t need to scroll through 5 screens of ad results to get to the content. reply giancarlostoro 20 hours agorootparentI wonder if part of why its better is due to other users providing feedback about results, but also you can pin results from specific domains to the top. Like I can pin any results from StackOverflow, instead of the garbage StackOverflow rip off sites Google keeps giving me, its pretty obvious its ripping off SO because I just read the same thing word for word on StackOverflow three links ago. Thanks Google. reply carlosjobim 13 hours agorootparent> I wonder if part of why its better is due to other users providing feedback about results, but also you can pin results from specific domains to the top.I think that matters very little. Kagi had excellent quality results for me from the start, in a huge variety of topics in several languages. Their user base is probably 90% American hacker, and I&#x27;m getting good results on queries they would never use. reply baja_blast 14 hours agorootparentprevYeah, me personally I am more upset with how irrelevant and bad Google search results have become than I am worried about privacy. I know and have accepted that Google invades my privacy, but the trade off used to be I could find whatever I am searching for, but now I can&#x27;t find anything on Google and it has made my job so much harder.The Google search algorithm from 5 years ago was amazing, why they decided to change it for the worse is something I will never understand! And no I do not blame SEO entirely since that existed 5 years ago, what I am often looking for but can no longer find is information that has nothing to do with any products. It&#x27;s not ads that I need to page through, but unrelated and bad results that are limited. I do not want to see the same results from page 1 on page 3. reply lghh 20 hours agoparentprevWeird question that I have that I&#x27;d love anyone who makes a Kagi account to trial after reading the parent comment to answer:When you make your account, you&#x27;re given the option to customize. When you do, you can pick things like color theme and how URLs are displayed. On the right hand side of the page there is a preview of what your Kagi searches will look like.In my example, the demo Kagi search is Magic The Gathering. I play a lot of Magic The Gathering. I spend most of my time online searching for things related to MtG or brewing decks, second only to things related to software development.I imagine it&#x27;s coincidence. MtG is a pretty nerdy hobby and Kagi seems like a pretty nerdy product. However, it made me uncomfortable enough to ask:Is that what it shows for everyone? Or is there some tracking going on already that is being demoed? It&#x27;s almost certainly the former given the positioning of Kagi in the search market, but I&#x27;d like to be sure. reply freediver 20 hours agorootparentVlad here, Kagi founder. Also an avid MTG player. I came up with the idea for that preview. reply lghh 19 hours agorootparentHi Vlad! It&#x27;s a fun nugget. I just suspect I&#x27;m in the very slim subset of people who was a little taken aback by it. reply freediver 19 hours agorootparentWe all need a dose of healthy skepticism. reply effingwewt 19 hours agorootparentI know it means nothing in the grand scheme of things, but y&#x27;all just got another customer.I&#x27;d been on the fence but after reading through these threads and seeing a real reply from a founder (its been a while since I&#x27;ve seen an honest, non-PR Speak answer) I am excited to try y&#x27;all out.This area was ripe for disruption with how terrible searches have become, especially on mobile. I&#x27;d be happy to see you eat G&#x27;s lunch here. reply joshmlewis 20 hours agorootparentprevYes it&#x27;s what showed for me as well and I&#x27;ve never played or searched it. Just a coincidence. reply xenocratus 20 hours agorootparentJust as an extra data point - same here, got shown MtG results, never played it (though I might have searched about it a few times way back). reply jamal-kumar 20 hours agorootparentprevI&#x27;m kind of blown away by how popular that game has gotten over the past few years in North America. I think the pandemic really accelerated the popularity of that and D&D, people are still doing these things after all of that. Even saw someone playing over the phone the other day. I don&#x27;t seem to remember it being so popular but now it&#x27;s more than ever and hardly a surprise tbh reply lghh 17 hours agorootparentThe two biggest reasons for this are that the format Commander has specifically blown up in popularity and Wizards of the Coast making a first-party desktop&#x2F;mobile client MtG Arena to compete with games like Hearthstone.Commander is a 4-player casual format that&#x27;s has as much in common with more typical board games as it does with traditional MtG.Arena is likely what you saw someone playing on their phone. Funnily enough, it doesn&#x27;t support Commander! reply jamal-kumar 15 hours agorootparentI think they were playing d&d over the phone but yeah it does look like it&#x27;s alot more engaging than what I seem to remember in middle school reply anymouse123456 20 hours agoparentprevThis.Kagi is incredible and worth every penny simply for being able to remove the SEO scam and tire fire that is Pinterest from all image search results. reply zeroonetwothree 19 hours agoparentprevI tried it a little but honestly thinking about having a limit of searches made me anxious every time I thought about searching like “is this really necessary” and so I went back to google. reply JohnFen 17 hours agorootparentWhy not just ignore that the limit exists? If you hit it, you can always fall back to another search engine, after all, so it shouldn&#x27;t be cause of anxiety. reply alwaysbeconsing 18 hours agorootparentprevI can offer that I am constantly online and using Kagi across my devices for all searches and so far have not come close to using up my quota. reply lofaszvanitt 8 hours agoparentprevKagi copied the same lame-ass google search experience.Search engines should be able to support even those who are not SEO experts and not the first ones to arrive and sit on specific keywords. What I mean under that, if you have multiple good, exhaustive answers for a query, why not offer varying&#x2F;random good results, so every link would have a chance? Let people break out of their bubbles.Same power pyramid scheme. Yukk. reply dominick-cc 19 hours agoparentprevI would love to see some of these privacy-focused providers like Kagi and Tutanota&#x2F;Protonmail align themselves into a \"bundle\"-type offering (think video game humble bundles) where for like $35&#x2F;month you got access to a bunch of useful tools like this. It would really expose a lot of people to services they aren&#x27;t super familiar with already, so even if it was at a slight discount to the provider, they would acquire a lot of new users I bet. reply effingwewt 19 hours agorootparentIndeed. I&#x27;m tired of data brokers masquerading as other things (search, email, whatever).Signed up for Kagi today and have been looking for a permanent gmail solution.Had this bundle been available today I&#x27;d have jumped on it. reply delboni 20 hours agoparentprevDoes anyone knows how Kagi performs on country specific search or even search in another languages? reply jaktet 19 hours agorootparentOne personal anecdote. It did not do that great for Norway&#x2F;Norwegian, I ended up switching back to google when trying to find some stuff in stores.For recipes and stuff it was fine. reply benhurmarcel 16 hours agorootparentprevIt works fine in French for me. I really like that I can have international search by default, and specify a country when needed using a bang like !fr. reply jamal-kumar 20 hours agoparentprevYou sold me.Fuck google, I have work to do. Thanks for the tip! Nice realizing that they&#x27;ve basically been wasting my time for a while now and that there&#x27;s a decent alternative available. reply kup0 15 hours agoparentprevAnother happy customer here as well. I use at work and home and plan to start using on mobile browsers also.The ability to essentially \"weight\" particular domains (pin, block, or anywhere in between) has saved me so much time. There are certain searches I do (music-related in particular) where I always want particular sites (metal-archives, bandcamp, etc) to be the first results, and having that as an option is great. It means that searches that I perform often have a result within the first 1-5 results that is exactly what I want.No ads, way less SEO spam, and the ability to completely remove domains from results if I think I need to tweak it further. For most of my searches I previously used Google for- Kagi makes Google&#x27;s results look laughably bad.I&#x27;ve also been using it at work for tech searches (linux, redhat, etc) and it has saved me time there too.I use a \"family\" account- and have one work account and one home account that way I can have different settings for different environments (would be neat if this could be built into non-family accounts though... like \"personas\" or \"profiles\" or something...) because I&#x27;m overpaying a bit to have the two account setup and don&#x27;t reach the search cap. I think I&#x27;m okay with that though, because having the cap so high means I&#x27;ve removed the \"running out of searches\" anxiety from my usage of the service. reply hospadar 20 hours agoparentprevCan’t second this emotion hard enough, love it, have never looked back, almost never bail out to !g - still use g maps for most location stuff, but all my web search is very comfortably living on kagi reply dcminter 18 hours agoparentprevI&#x27;m just trying Kagi out now, having done about 2 searches of my initial free 100. So far one was better than google and the other no worse. The \"no worse\" one was for something where I already suspect there aren&#x27;t any good results to find.Fingers crossed, but I have a good feeling about it. If it goes well the pricing seems fair. reply m3kw9 20 hours agoparentprev“ Our data includes anonymized API calls to traditional search indexes like Google, Mojeek and Yandex”. They pay google to do this? reply jjice 20 hours agorootparentI believe DuckDuckGo does (or at least they did) this with Bing. Starting a new scraper at a scale that users would need to be useful for what they&#x27;re used to is such a huge jump. I&#x27;m sure if Kagi continue to grow they&#x27;d prioritize their own scraping too, but that&#x27;s just not feasible at first. reply giancarlostoro 20 hours agorootparentBack in the day I&#x27;d suggest doing it via Alexa top sites, but now that Alexa is gone, I&#x27;m not sure what strategy I would use, but I would want to hit sites that are like the \"top 10000 most popular\" first, and scrape every inch I could. reply elaus 19 hours agorootparentI think Kagi is going in the opposite direction: https:&#x2F;&#x2F;blog.kagi.com&#x2F;small-webThey try to highlight small, personal websites instead of the big mainstream sites.(This was a HN submission 2 weeks ago) reply giancarlostoro 15 hours agorootparentI saw that, but that&#x27;s kind of useless when you kind of want something like SO or similar results, something Google keeps failing at. reply jerrre 20 hours agorootparentprevthat&#x27;s what that means yeah, but not necessarily present it in the same way reply metadaemon 19 hours agoparentprevHappy customer here! Been paying for a year with no complaints. Amazing search engine -- kind of like how Google used to be years ago before they started injecting more ads into search. reply replwoacause 10 hours agoparentprevThis is the answer. I’ll never use Google search again. reply bloopernova 19 hours agoparentprevDo you ever go past the 1,000 search \"limit\"?I&#x27;m considering signing up; it would be one fewer service I&#x27;m relying on google for. reply boredpudding 19 hours agorootparentI was quite afraid of this, but apparently I&#x27;m around 700-800 searches a month and I search quite a lot during my work.Kagi is also working on removing the 1000 limit on the 10$ subscription and offering unlimited searches.Switching from the 5 $ plan to 10 $ was super smooth by the way, so if you want to try for less $, the 5 $ for a month is enough so you can get used to the product and know if you like it or not (and that&#x27;s besides the 100 free you get while signing up for a trial). reply thesuitonym 15 hours agorootparentThat would be great. The $5 price point is the most I&#x27;d pay for search, but 300 searches per month is just not worth it in any sense. If all of the tiers moved down one step, or if Kagi offered a discount for annual membership, I could see it working for me. reply bloopernova 19 hours agorootparentprevexcellent, thank you for the information :) reply TradingPlaces 19 hours agorootparentprevI do 500-600 a month pretty regularly. They provide stats. reply AndrewKemendo 21 hours agoparentprevWhat happens if GOOG acquires Kagi? reply MichaelZuo 20 hours agorootparentI&#x27;m sure many folks, including me, would be willing to help Vlad organize a private acquisition if he suddenly had some pressing need to sell. reply AndrewKemendo 19 hours agorootparentI love the ideaI challenge you to find a group of rich folks who will fund such a thing - and will not simply demand the same profits as current structures do.I just tried this with ~30 wealthy individuals&#x2F;family funds that I have great relationships with and literally everyone said some form of the following:“I am not going to liquidate my current investments that are on IPO trajectory for something that by definition will not IPO, and my investments are to ensure my kids go to private school&#x2F;colllege fund&#x2F;etc…”You need to convince people, who currently think that the goal of making&#x2F;having money is to insulate them and their families from reality, to instead choose to make&#x2F;have slightly less money so that somebody else can have an easier&#x2F;better experience in a way that is still concomitant with commercial transactions. reply MichaelZuo 19 hours agorootparentHuh, is this story a metaphor&#x2F;allegory for something?Why not &#x27;challenge&#x27; yourself if you&#x27;ve already done quite a bit of groundwork?Or at least the comment suggests you&#x27;ve already put more thought into this than me. reply AndrewKemendo 18 hours agorootparentI’ve already put maximum financial and personal time inputs into my particular project, which is unfortunately insufficient to get started in earnest.I need 200k to actually start what I want and currently have 5k in my bank account and a lot of mandatory overhead (kids etc).If you’re legit interested I’ll tell you all the details 1:1 reply MichaelZuo 9 hours agorootparentSure we can talk, DM me on twitter. reply nonameiguess 16 hours agorootparentprevThis is really the ideal use case for nonprofit funding, but unfortunately, real-world wealthy donors tend to give to university football programs and getting their names on hospital buildings. Sadly, SMU near me just made a deal with the ACC to forego ten years of television revenue in exchange for being allowed into the conference in order to get an automatic playoff berth if they can win the conference. Boosters will fully fund the athletic department to make up for the shortfall of not receiving and television revenue. All that money could have gone to good, but rich people care more about their alma mater having a 1% chance rather than a 0% chance at a national championship. reply IAmGraydon 20 hours agorootparentprevDon’t you work for Google or is that a different Michael Zuo? reply MichaelZuo 19 hours agorootparentI don&#x27;t, but it&#x27;s possible someone else does.I signed up for HN before it was popular to include middle initials or other fancier forms for the username, I&#x27;ll update my profile though. reply notemaker 15 hours agoparentprevHow does it fare for bilingual users? reply yieldcrv 19 hours agoparentprevI’ve replaced most of my google searches with LLM discussions and some bullshit checkingI usually only need to understand a concept, not understand if the personnel and company names it made up actually existeverything else I use google for are just addressesso I’m wondering if a paid search engine would shift my behavior back to search engines, or if that ship has just sailed reply freediver 17 hours agorootparentLLMs can not yet replace good web search. There are whole categories of queries whera a LLM is more or less helpless with. Think navigational queries, shopping &#x2F;reviews, location aware, &#x27;grep the web&#x27; style queries just to name the few.For example:nyt crosswordcheap iem redditstarbucks near meM7FFALPLikely, a good search product in the future will be a combnation of both. reply gremlinsinc 11 hours agorootparenthave you tried this on phind.com? it&#x27;ll create multiple queries to find and organize data and it&#x27;s very good and saying it doesn&#x27;t know something rather than give a BS answer. reply vdqtp3 14 hours agorootparentprev> I usually only need to understand a conceptYou should know by now that LLMs will and do lie in subtle ways that are not apparent to non-experts. Using them to understand complicated concepts is a great way to \"learn\" incorrect information. To be fair, the same can be said for humans, but humans are worse at bullshitting. reply yieldcrv 8 hours agorootparentI’m awareI really just need to converse about a topic for more inquisitive-ness and to form structured thoughtsIt will tell me if I’m conflating concepts, before bullshitting about the ways theyre different. Thats fine, my blind spot would have been that I was conflating a concept for the next decade.In that regard its the same or better than a humanI don&#x27;t need it to be the source of truth, I need it to be conversational. It can make urban legends just like a person does, I don’t care, just give me a way to talk about a concept and decide if I want to learn more and it does that extremely well reply thrownaway561 21 hours agoparentprevhttps:&#x2F;&#x2F;kagi.com&#x2F; reply hrbrmstr 21 hours agoparentprev100% ^^this^^ reply sneak 21 hours agoprevYou have to use a special \"verbatim\" search product from Google, it isn&#x27;t the main search box anymore. Look under Advanced or something. reply crazygringo 20 hours agoparentFirst of all, wow. It&#x27;s there (under Tools, you don&#x27;t even need to go to Advanced), it works, and yet I&#x27;ve never once seen anybody talk about that feature&#x27;s existence on HN until now. And people have been complaining about not being able to do verbatim searches for years and years here on HN.Second, I just looked up when this feature was introduced (assuming it was fairly new), and it was in... November 2011. It&#x27;s been there for the past twelve years. See:https:&#x2F;&#x2F;www.wired.com&#x2F;2011&#x2F;11&#x2F;google-verbatim-search-back&#x2F;https:&#x2F;&#x2F;searchengineland.com&#x2F;responding-to-complaints-google...https:&#x2F;&#x2F;www.searchenginewatch.com&#x2F;2011&#x2F;11&#x2F;18&#x2F;google-introduc...Thanks for letting us know! It&#x27;s been right under our noses the whole time -- and it&#x27;s not like the Tools menu is even particularly hidden, at least on desktop. reply marcosdumay 19 hours agorootparent> I&#x27;ve never once seen anybody talk about that feature&#x27;s existence on HN until nowEvery time I tried it, it didn&#x27;t work.It certainly changed a lot on those years, but the reason nobody acknowledges it is probably because it&#x27;s a coin-toss if Google wants \"verbatim\" to mean verbatim today.Quotes actually stopped working (they became a hint, instead of filtering the results) a long time ago, and many people insisted for years that the verbatim search worked. Probably because those tried it on the days when Google decided to use a standard dictionary. Nowadays even those people gave-up. reply crazygringo 19 hours agorootparentCan you (or anyone) supply an example where it doesn&#x27;t?I just tried a whole bunch of queries on seemingly generic-sounding sentences from old pages on niche blogs and it found every single one.So I&#x27;d love to understand when it doesn&#x27;t work and if there&#x27;s some pattern -- like if certain punctuation trips it up, or if it&#x27;s simply pages not indexed in the first place. reply marcosdumay 18 hours agorootparentIt didn&#x27;t work for any query. A verbatim search was very similar to putting every word in quotes. That&#x27;s very obviously not your experience, so it&#x27;s either a personalization thing, or Google silently changed it. Anyway, there isn&#x27;t a pattern to find, it works or it doesn&#x27;t.But then, I&#x27;ve seen it work too. And stop working again. reply crazygringo 18 hours agorootparentOK, so can you supply any query then? Any example will do.This is just about verifying the behavior, that it&#x27;s not finding a verbatim result that it should. And it has nothing to do with personalization -- it&#x27;s easy to run these logged out.Also, \"A verbatim search was very similar to putting every word in quotes\" is not an example of it not working. The question isn&#x27;t whether they&#x27;re similar -- your claim was that it \"didn&#x27;t work\", which I take to mean it was returning incorrect results, that correct results were missing. reply eindiran 5 hours agorootparentI think OP&#x27;s point is that this isn&#x27;t a deterministic system for a given query. The feature flags set for the particular user (are you in the verbatim or no-verbatim branch of today&#x27;s AB test), and the timestamp are also inputs. reply JohnFen 19 hours agorootparentprevYeah, I&#x27;ve never had much luck with verbatim search either. reply gniv 19 hours agorootparentprevThere&#x27;s been lots of talk about it over the years: https:&#x2F;&#x2F;www.google.com&#x2F;search?q=google+%22verbatim%22+site%3...I think I also mentioned it a couple of times when the same complaint came up.The problem I think is that \"verbatim\" is not a word that one thinks of, so nobody searches for that. Plus it&#x27;s hidden in a generic \"Tools\" menu. Sometime you get a link to search for the exact phrase at the bottom of results, but that too is subtle. reply gabrielsroka 18 hours agorootparentprevhttps:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=30353182https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36618104 reply Izkata 21 hours agoparentprevOn the search results page, there&#x27;s a \"Tools\" button in the upper-right that expands two dropdowns. Change \"All results\" to \"Verbatim\". reply robertlagrant 21 hours agorootparentWow, that&#x27;s a good tip. reply dihydro 20 hours agorootparentHere&#x27;s a URL Link to the advanced search: https:&#x2F;&#x2F;www.google.com&#x2F;advanced_search?hl=en&fg=1 reply late25 19 hours agorootparentI tried it and it looks like it does nothing but wrap my query in quotes… and then return non-exact results if not found. How is this any different than what people have historically done? reply Izkata 18 hours agorootparentThat isn&#x27;t different, Verbatim search isn&#x27;t on the advanced search page. You can only get to it from the search results page like in my earlier comment. reply Izkata 18 hours agorootparentprevIt&#x27;s not on that page, you can only switch to Verbatim on the search results. OP was mistaken. reply janlukacs 20 hours agorootparentprevwow, thanks! how did you find this link? I don&#x27;t see it anywhere in the main interface. reply tda 20 hours agoparentprevGreat tip! Seems like this adds the query param \"&tbs=li%3A1\", so this might be something you can configure as an extra search engine in firefox. But then I am a happy kagi customer, and I was just thinking that I can&#x27;t recall the last time I had to do a !g to find something. For me, google search si pretty much dead. I only use it now when on someone else&#x27;s pc reply chankstein38 20 hours agorootparentSounds like a good case for an extension even! \"Auto add this when I go to google\" reply rbinv 19 hours agorootparentYou can also adjust your browser&#x27;s search engine defaults. reply tivert 20 hours agoparentprev> You have to use a special \"verbatim\" search product from Google, it isn&#x27;t the main search box anymore. Look under Advanced or something.I actually use that, but it has its faults. You get more spam results and iffy sites (e.g. Wikipedia clones). It&#x27;s also missing some of Google&#x27;s convenient features (like doing unit conversions and arithmetic).IIRC, Verbatim mode is closer to the raw results of Google&#x27;s underlying search engine, before some of the massaging they do. Some of that massaging is bad, but some of it&#x27;s all right. reply jamal-kumar 20 hours agoparentprevWhy would they make their very expected functionality something you have to dig around to find now? That just seems like really bad decision making that should have been spotted by someone on top and screams that the people on top are now disconnected from reality.I swear that it used to work for certain strings I&#x27;m trying to find now which I was able to find information on and now it isn&#x27;t even returning, with \"verbatim\" set, something that is in a very well-known program&#x27;s documentation. Bing finds like three results. Google has dropped the ball so hard it&#x27;s embarrassing reply kuchenbecker 19 hours agorootparentMy guess (complete speculation, but I worked with the Search Quality folks about a decade ago):1 - By interpreting your search, it leads to better \"search quality\" by having one model say \"i think this is what they want\" and another execute the search. P90 accuracy is increased at the cost of P99 accuracy.2 - If you search for a literal string you know exists, you expect to find it. By interpreting, fewer search inputs with literal strings make it to the search function.3 - Since Google is interpreting more searches, this gives ad-placement a route in to favorably interpret \"they want to buy something\" even when this isn&#x27;t the case. This makes Google money.4 - People that used to use literal searches either stop, learn how, or switch search engines.5 - After a couple years, business metrics show that literal searches represent 0.1% of queries and make less money, is it really worth investing in? When it was a P99 issue it fell off the radar and now the P999 is lost.So a series or rational decisions by rational actors leads to a decline of a used feature because of business incentives and chasing P90s at the cost of P99s. reply jamal-kumar 19 hours agorootparentSo they&#x27;re basically just as good as some malware search engine that inserted itself into grandma&#x27;s computer nowFantastic reply effingwewt 18 hours agorootparentAt least those entertained people.I couldn&#x27;t get my fiancé&#x27;s mother to stop re-installing bonzi buddy or whatever that toolbar was called because she &#x27;loves the purple monkey, he&#x27;s so cute!&#x27; reply jamal-kumar 15 hours agorootparentMan that&#x27;s crazy in the early 2000s I was volunteering with developmentally disabled adults and had to explain to them how bonzi was a &#x27;bad monkey&#x27; and that he was &#x27;not a friend&#x27; and they got it reply jplona 20 hours agorootparentprevIt&#x27;s also possible they they&#x27;re correct, and the average user of today has much less need for verbatim searches than the average user when the search engine was first designed. reply jamal-kumar 20 hours agorootparentNah their product is broken and I&#x27;m going to avoid using it from now on except when I need to find something on a map or mess around on SEO for workIf they break a key feature to MAKE a verbatim search happen with literally two keys pressed, and are apparently not even indexing what they used to anymore, they&#x27;re dropping the ball. Most people know the quotation trick now and are probably assuming it still works reply JohnFen 19 hours agorootparentprevThe quality of search results I get from Google has seriously degraded over the past several years (I think about the time they decided to start interpreting my searches rather than just search for what I asked for). From my point of view, verbatim searches are more necessary now than ever before, to work around that issue.I tried using verbatim searches to make Google decent for me again, but Google defines \"verbatim\" rather differently than I do. reply Filligree 20 hours agorootparentprevThis is definitely the case. Sit down next to a casual user one day, and you&#x27;ll find that &#x27;verbatim&#x27; is the absolute last thing they need. reply devmor 19 hours agorootparentprevThis is almost certainly the case. Those of us who learned to use and mastered keyword-style search are the minority. Most people search whole phrases, expecting a contextual answer. reply chankstein38 20 hours agorootparentprevYeah, it&#x27;s Google. I feel like, at this point, Google is basically synonymous with \"bad decision\" reply zulban 21 hours agoparentprevI thought this was a joke. reply helboi4 1 hour agorootparentWhat it is like anti-user design. Why take away a decades long precedent to hide it in a menu? reply mannykannot 20 hours agoparentprevFor now, this seems to work: https:&#x2F;&#x2F;www.google.com&#x2F;advanced_search but, given the pace at which Google \"improves\" things, I don&#x27;t have much hope that it will last. reply gniv 20 hours agorootparentThat page has been there forever. Here&#x27;s the version from Dec 2000: https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20001202041000&#x2F;https:&#x2F;&#x2F;www.googl... reply fergie 21 hours agoprevI agree that as a programmer who frequently needs to search for long literal strings verbatim, Google has become notably less useful than it used to be.I wonder if there is now a gap in the market for some kind of \"literal\" search engine that makes no attempt to infer meaning on your search terms and simply gives you the closest results? In other words Google ca 2012. reply btown 21 hours agoparenthttps:&#x2F;&#x2F;publicwww.com&#x2F; is a great tool for this, though the size of its index leaves a lot to be desired. Still, for enumerating well-SEO&#x27;d homepages that use a certain tech stack, it&#x27;s quite useful! reply xtracto 19 hours agoparentprevFor technical stuff I&#x27;ve long replaced Google search with phind.com even just pasting so.e generic error spit from the console works. reply minikomi 21 hours agoparentprevHave you tried grep.app ? reply cwoolfe 21 hours agoprevWhen this happened to me, I found better results on https:&#x2F;&#x2F;duckduckgo.com&#x2F; reply mkl 21 hours agoparentDuck Duck Go has similar problems for me. It&#x27;s also recently started sometimes ignoring the \"-\" when you try to exclude a word. reply yegg 21 hours agorootparentWe put out a partial fix for that recently and a more complete fix is forthcoming. reply sen 21 hours agorootparentPartial fix for the minus or the double quotes? Both are such a critical part for searching anything these days, it’d be a real shame to lose either. reply yegg 21 hours agorootparentBoth. reply WarOnPrivacy 20 hours agorootparentThat&#x27;ll be awesome. The parts of the web that don&#x27;t respect operands feel awful.Having a minus act like a plus has been particularly tough (for years now, I believe). reply chankstein38 20 hours agorootparentprevIt&#x27;s crazy how often you&#x27;re in the comments of HN articles just waiting for a DDG shoutout haha it&#x27;s appreciated! reply glimshe 11 hours agorootparentprevThank you! As a heavy DDG user, PLEASE don&#x27;t become Google. Stay great and private. reply Qwertious 20 hours agorootparentprevI think the \"-\" is case-sensitive sometimes. So if you write \"-honey\" it&#x27;ll still return \"Honey\" results, so you have to write \"-honey -Honey\". reply thesuitonym 20 hours agorootparentNo, it just doesn&#x27;t work at all. Search &#x2F;r&#x2F;duckduckgo for \"- operator\" to see tons of threads. reply sen 21 hours agoparentprevI’ve used DDG exclusively for a couple years now and at least with my usecases have found it better than Google in every single way other than images (which I still use it for, but need to do !g to proxy Google maybe 1 in 5 times). reply notRobot 20 hours agorootparentFYI: !gi on DDG for direct Google Images search. reply ComputerGuru 8 hours agoparentprevDDG has started injecting completely untreated recent news stories to the results of a search with few hits. Like searching for a programming error with specific class names will give you several articles about the woes of the Democratic Party in recent days!! reply felurx 1 hour agorootparentSame for me, but with location-related stuff. I often see random Wikipedia articles or tourism websites for stuff near the city my IP address is from... Worst of all, sometimes there&#x27;s a block of filler results between actual results, so I sometimes give up and don&#x27;t see some of the results that might&#x27;ve been what I was looking for. reply DoItToMe81 1 hour agorootparentprevAre you sure this isn&#x27;t from SEO on the part of the news sites? I have not seen this at all. reply cassianoleal 21 hours agoparentprevI find better results on it in general.I use Google as a fallback but these days it happens perhaps once every couple months, and mostly I don&#x27;t get anything out of it. reply Turing_Machine 21 hours agorootparentYes. IMO DDG has been better for general questions for a long time.Google remained better for programming questions for significantly longer (I speculate this may be because Google&#x27;s own programmers used it, and complained when the results sucked :-)), but now it&#x27;s not. Not really.Like you, I still use Google as a long shot, but that&#x27;s become quite rare.I sometimes use Bing for Microsoft-specific questions, if DDG doesn&#x27;t give me what I want. I have the sense that Bing covers Microsoft a little better than the others do. I have no real solid evidence for this, but it seems plausible on the surface. reply inductive_magic 21 hours agorootparentprevIt depends on the usecase. If you&#x27;re querying for something local like a restaurant or a store, google wins by orders of magnitude. If you&#x27;re researching, duckduckgo is fine. Duckduckgo doesn&#x27;t really compare to Kagi though. reply cassianoleal 19 hours agorootparentWhat&#x27;s wrong with DDG and local searches? I find it exceedingly good, especially with the country selector being so up and front. It&#x27;s very easy to search for those kinds of things in other places, for example to plan a trip.Kagi for me is a no-no. I mostly browse on temporary containers, and also have Firefox delete cookies on close. A search engine that requires me to be signed in would be a massive pain to use. reply inductive_magic 17 hours agorootparent>What&#x27;s wrong with DDG and local searchesMaybe its different in the US or just not optimised in Germany, but the quality of search results isn&#x27;t comparable.Query: \"All you can eat Kiel\"Google:1. a map with the top three restaurants for this context in the foreground2: a list of restaurants, sortable by price&#x2F;date&#x2F;hours3 to end of page: links to homepages of the top restaurantsDDG:1. a completely random assortment of irrelevant links on the left2. a semantic web box with a 3&#x2F;5 rated irrelevant restaurant on the right3. wild youtube-videos vaguely related to the query between text results4. images5. more wild linksthat kind of thing. And Kiel isn&#x27;t a small city, we even had DDG print ads here. reply cassianoleal 43 minutes agorootparentI&#x27;m not in the US. :)When I try your query I only get a bunch of local German stuff (including a map with restaurants).I admit, I don&#x27;t even know what Kiel is so I can&#x27;t validate the results but despite them being in Germany (and my search having had the country selector set to UK), it looks pretty good.If I search something more generic like \"all you can eat sushi\" I get a neat map with a few restaurants that do that, plus a bunch of TripAdvisor and other directory and blog posts exactly about that, all local.If I try your query again, setting the country to Germany, I get https:&#x2F;&#x2F;yummy-kiel.de&#x2F; as the first result, a map with top-three restaurants as the second, and again, a bunch of directories and blogs with all you can eat Kiel.Seems pretty good to me. replyjohnwayne666 21 hours agoprevHave you tried using \"Tools -> Verbatim\"? reply rbinv 21 hours agoparent+1, this is the only way to get old-school(ish) results, even without explicit quoting. Unfortunately, it can&#x27;t be enforced just by adding parameters to the URL and has to be selected manually every time. (edit: actually it can, see below)Verbatim really highlights how almost useless the default mode has become. reply ChoGGi 21 hours agorootparentYou can add it with parameters: tbs=li:1 https:&#x2F;&#x2F;www.google.com&#x2F;search?tbs=li:1&num=30&safe=off&q=test reply ryandrake 18 hours agorootparentBut that&#x27;s almost worse than removing it completely! They recognize that people still need to do this, but deliberately make it more difficult in the UI. reply nostrademons 17 hours agorootparent(I worked on the team that added the tbs= parameter, 14 years ago.)There&#x27;s nothing nefarious here. The integration of &tbs= is deeply tied into GWS; it&#x27;s written into the C++ substrate, while most of the rest of the server has been rewritten in Java + a bunch of custom languages. Most likely they simply can&#x27;t remove it - it&#x27;s permanent technical debt that nobody has the expertise to touch anymore.Changing the UI is all done in a plugin these days, and is a relatively trivial change. reply rbinv 21 hours agorootparentprevAwesome, I stand corrected. reply isaacfung 21 hours agorootparentprevLike ChoGGi said, you can use tbs=li:1 You can test the other parameters here.https:&#x2F;&#x2F;serpapi.com&#x2F;playground?q=The+most+popular+fruit+in+t...Other useful parameters, you can use nfpr to force it to not correct your search termshttps:&#x2F;&#x2F;serpapi.com&#x2F;blog&#x2F;filtering-google-search-and-google-... reply naillo 20 hours agoprevJust give up on trying to make google work. Google has definitely not improved. Recently I tried searching \"hugginface madebyollin\" (with a slight type to hugginface instead of huggingface) and it literally didn&#x27;t show the obvious result. https:&#x2F;&#x2F;www.google.com&#x2F;search?q=hugginface+madebyollin. I&#x27;ve switched to duckduckgo and couldn&#x27;t be happier. reply userbinator 20 hours agoprevI see that the quotes in your title are not regular ASCII quotes. I agree that Google&#x27;s search result quality has been quite horrible, but could the fact that your system is somehow not emitting \"real\" quotes also be the cause? I&#x27;ve seen plenty of problems caused by those horrid \"smart\" quotes in the past. reply bambax 20 hours agoprev> I used this quite frequently but since Google \"\"\"\"improved\"\"\"\" it last year (there was a popular HN post complaining about this) it doesn&#x27;t work anymoreThat&#x27;s not my experience? The quotes still seem to work for me? Do you have a specific example? And &#x2F; or can you point to said HN post? reply lc5G 20 hours agoparentI can confirm the experience that sometimes searches for quoted terms yield pages that do not contain the quoted term exactly. I don&#x27;t have a specific example from the top of my head. reply izzydata 20 hours agorootparentIs it possible that the results simply don&#x27;t exist and instead of giving you nothing it gives the next possible match? reply bromuro 9 hours agorootparentIt could be this - but you would notice it after opening the search result.I spend some time reading a random website until I realize the sentences i was looking for is not there. Useless and frustrating. reply jerrre 20 hours agoparentprevI just tried https:&#x2F;&#x2F;www.google.com&#x2F;search?q=%22I+used+this+quite+frequen... and it works for me now, but I definitely recall having problems searching verbatim before, perhaps they improved&#x2F;changed it recently? reply croisillon 20 hours agoparentprevworks for me too, maybe being in europe does the trick? reply planede 21 hours agoprev> Search for a domain name with quotation marks for example just recombines the contents of the domain and returns a bunch of unrelated content completely cluttering what I am looking for.site:example.com and -site:example.com still work, I think. reply drcongo 21 hours agoparentThat limits results to pages on example.com (or neg for the second) but that&#x27;s not what OP is trying to do. Searches for \"example.com\" should return pages where the exact domain \"example.com\" appears in the page. It does sort of work for \"example.com\" itself, because it&#x27;s so common presumably, but doing it with other domains that are less common is far less likely to produce anything useful. reply jeffbee 21 hours agoparentprev\"example.com\" also seems to work perfectly well. reply PhilipRoman 18 hours agoprevWeird, I still use the quotes to search for phrases, IDs and other things and it works well. Never noticed they changed anything.Any specific examples of queries you wish worked differently? reply jamal-kumar 20 hours agoprevThank you for asking the important questions, you&#x27;ve saved me a large amount of time here realizing that this isn&#x27;t just my particular case of search terms being broken somehow. I guess we&#x27;ve all counted on google being reliable all these years and it&#x27;s kind of a shock that they&#x27;ve gone and tanked their own usefulness after building up like twenty years of trust. Rude. reply inetknght 21 hours agoprev> Google \"\"\"\"improved\"\"\"\" it last yearGoogle&#x27;s done a lot of \"improvements\". I hate to say it but its quote feature has been broken for a decade. You&#x27;re only now noticing?I&#x27;ve tried using other search engines. I&#x27;ve settled on DuckDuckGo. It also does not have a working literal-quote feature. But it&#x27;s much less infested with SEO garbage. reply hedora 19 hours agoparentI switched to DDG because Google broke literal quotes years ago (then fixed it, and broke it again?). Now its broken on DDG. Considering Kagi. reply logicchains 21 hours agoprevUse Bing? I&#x27;ve found for code&#x2F;error message snippets in particular Bing is often better, while Google will incorrectly guess I mean some related but different thing and serve the wrong results. reply userbinator 20 hours agoparentBing&#x27;s verbatim feature needs a + in front of the quotes, I believe; but then it will still screw up horribly on things like error codes (you want 1234, it&#x27;ll give you 1235 and 1233.) reply gniv 20 hours agoparentprevDoesn&#x27;t seem to work. Search for \"internal engine\" and see most results talk about internal combustion engine. By contrast, Google with Verbatim option returns only results with the exact phrase \"internal engine\". reply anonyfox 21 hours agoparentprevconfirmed (although I use ecosia instead of bing directly). Now its like:semantic search + realtime needs = google;raw searches is ecosia(bing)and most needs are covered via GPT4 actually. Error debugging + code snippets, general interest explanations, arguing things, evaluating things interactively. Basically everything I don&#x27;t need hard facts of the last 2 years for. reply aabbcc1241 17 hours agoprevI use a local hosted web spider and search engineIt only index the pages that I&#x27;ve viewed so it&#x27;s not spammed by SEO junkshttps:&#x2F;&#x2F;github.com&#x2F;beenotung&#x2F;personal-search-engine reply mg 21 hours agoprevCan you give an example? Searching for \"news.ycombinator.com\"works just fine for me. No unrelated content. reply hedora 19 hours agoparentKnowing Google, it&#x27;s gaslighting you^W^W guessing that you actually meant to type the quotes for that one search. reply westurner 20 hours agoparentprevHow do the search results for a site: query compare to just quoting what appears to be a DNS domain containing punctuation tokens? inurl:news.ycombinator.com site:news.ycombinator.com \"news.ycombinator.com\" news.ycombinator.com reply tiew9Vii 21 hours agoprevA HN comment recently recommended Yandex describing it as what Google search used to be.After seeing the post and trying Yandex it was absolutely right, it’s what Google search used to be.Now whenever I use Google and it’s just a list of ecommerce adds or content farms duplicating the same content without substance, I head to Yandex and get the type of results I used to get from Google. reply reisse 20 hours agoparentGuess it&#x27;s because Yandex gets most of its ad revenue from searches in Russian. Search results for English queries are not so ad-cluttered. For searches in Russian language, Google is still better (shows less ads per page), but tends to prefer .ua websites. I guess their ML predicts European IP + Russian language as a proxy for someone from Ukraine now, which is pretty logical. reply Ylpertnodi 20 hours agoparentprevAgree. Yandex seems to perform better. Heavy use of captcha&#x27;s (mullvad vpn), but once through that, enjoy the service. reply cynicalsecurity 21 hours agoprevHuh? It&#x27;s still working. reply Raed667 21 hours agoparentreally depends on the query, I have seen it working and I have seen it being totally ignored. reply palmfacehn 20 hours agorootparentWhen it is ignored I see a link below the search box to search for the \"exact phrase\" instead. reply miked85 20 hours agoparentprevIt seems like more of a suggestion recently. Often, things I quote aren&#x27;t even in the first page of results. reply kotaKat 21 hours agoprevSimilar vein, how the hell do I search an image and combine it with text anymore? I hate Lens and there&#x27;s apparently no way to properly \"image\" search anymore. reply lazylion2 18 hours agoparentYandex&#x27;s image search is pretty good reply lacrimacida 20 hours agoprevThe solution is to simply stop using google and find other search engine, eventually decide to pay for really useful search engines like kagi. reply 2devnull 19 hours agoprevBing. Not that it works well. Also, spending less time on the computer. The more time you spend on the computer the more money google makes. They don’t want to help you find the thing you need, they want to do the opposite so you spend all afternoon searching so they can sell more ads. reply IG_Semmelweiss 20 hours agoprevQuotes don&#x27;t work but>>>>Search for a domain name with quotation marks for example just recombines the contents of the domain and returns a bunch of unrelated contentI believe if you search for site:domaingoeshere.com yourqueryhereThat will spit out results only from that domain. I think that still works ? reply up_o 21 hours agoprevFor locking to a particular domain I had always added `site: example.com` to the query, rather than adding domain to a double quote statement.I have used double quotes to limit to a particular _phrase_ as recently as last week. I&#x27;m not privy to the improvements you mention, do you have a link? reply aspenmayer 11 hours agoparentThe site:domain.tld syntax should not have a space between the colon (:) and the (sub)domain.Incorrect:site: domain.tldCorrect:site:domain.tldThe - modifier to exclude a word does not have a trailing space.Incorrect:- site:domain.tld - queryCorrect:-site:domain.tld -queryI’m less certain about excluding a verbatim phrase, but I believe you can do as follows.-“exclude this query string”While this space rule still applies, so don’t insert a space, although this needs more testing by me to know for sure that this exclusion works properly.- “this is not correct syntax imo”https:&#x2F;&#x2F;ahrefs.com&#x2F;blog&#x2F;google-advanced-search-operators&#x2F; reply causi 21 hours agoprevThe minus (-) operator isn&#x27;t a hard and fast rule anymore either. It&#x27;s particularly galling when you want to search for something that&#x27;s similar to something else that&#x27;s very common. reply RobotToaster 20 hours agoprevBrave search is pretty good. I don&#x27;t like their browser but use their search with firefox. reply pierat 20 hours agoprevPiracy has basically been purged from google. Duckduckgo isn&#x27;t much better, since it&#x27;s Bing on the backend.Instead, use Yandex. It returns direct relevant hits and just bloody works. And if you&#x27;re in the USA, you&#x27;re also shielded by the fact that Yandex wont share your searched etc with the USG. You know, being a Russian search engine :) reply endisneigh 21 hours agoprevWhat&#x27;s an example query? reply dekhn 19 hours agoprevsite:domain.com is how you do a site URL restriction, if that&#x27;s what problem you&#x27;re having.Enable verbatim mode to make quotes work better. Even then I don&#x27;t think it&#x27;s absolute. reply hiidrew 20 hours agoprevtry https:&#x2F;&#x2F;perplexity.ai&#x2F; reply more_corn 19 hours agoprevI stopped using google search because it is infuriating. reply arroyodelaluz 21 hours agoprevyandex.com duckduckgo.com reply ftxbro 20 hours agoprevI still try to use + but I guess that hasn&#x27;t worked since before Google Plus. I never thought I&#x27;d want altavista back. reply Mistletoe 21 hours agoprevWow I didn’t know they changed this. No wonder my quotes searches made no sense anymore… reply drcongo 21 hours agoprevKagi.com reply jiofj 21 hours agoprev [–] It&#x27;s never stopped working for me, for all queries, and believe me, I use it very often.This post reeks of Kagi spam to me.Sorry guys, your product failed, no one is going to pay for search no matter how much Google sucks. Move on. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "A recent update to Google has altered how search functionality behaves, which has caused user discontent. Previously well-functioning features, such as searching for a domain name in quotation marks, now yield sub-optimal results.",
      "Instead of providing no results for unmatched exact searches within quotation marks, Google now returns unrelated content, frustrating users who relied on this feature.",
      "Users are now seeking solutions or workarounds to this issue of changed search functionality within Google."
    ],
    "commentSummary": [
      "Users express their dissatisfaction with Google search, raising concerns about privacy, targeted ads, and the removal of 'exact match' feature.",
      "Kagi, an alternative search engine, is being lauded for its relevance and ad-free experience. Debate topics include Kagi's functionality, its probability of acquisition by Google, and performance in localized searches and multilingual support.",
      "Users also suggest DuckDuckGo as a viable alternative, bemoaning the decreasing usage of Google's 'verbatim' search feature. They are in search of platforms that prioritize data privacy, search accuracy, and user control over data."
    ],
    "points": 257,
    "commentCount": 194,
    "retryCount": 0,
    "time": 1695035932
  },
  {
    "id": 37555172,
    "title": "US Military asks for help locating missing F-35",
    "originLink": "https://time.com/6315261/missing-military-f35-jet/",
    "originBody": "SIGN IN SUBSCRIBE NOW! SIGN UP FOR OUR IDEAS NEWSLETTER SECTIONS Home U.S. Politics World Health Climate Future of Work by Charter Business Tech Entertainment Ideas Science History Sports Magazine TIME 2030 Next Generation Leaders TIME100 Leadership Series TIME Studios Video TIME100 Talks TIMEPieces The TIME Vault TIME for Health TIME for Kids TIME Edge TIME CO2 Red Border: Branded Content by TIME Coupons Personal Finance by TIME Stamped Shopping by TIME Stamped JOIN US Newsletters Subscribe Give a Gift Shop the TIME Store TIME Cover Store CUSTOMER CARE US & Canada Global Help Center REACH OUT Careers Press Room Contact the Editors Media Kit Reprints and Permissions MORE About Us Privacy Policy Your California Privacy Rights Terms of Use Modern Slavery Statement Site Map CONNECT WITH US U.S. MILITARY U.S. MILITARY ASKS FOR HELP FINDING MISSING F-35 FIGHTER JET U.S. Military Asks for Help Finding Missing F-35 Fighter Jet A U.S. Air Force Lockheed Martin F-35 fighter jet flies over the San Francisco Bay in 2019. Yichuan Cao—NurPhoto/ Getty Images BY KATE DUFFY / BLOOMBERG SEPTEMBER 18, 2023 6:51 AM EDT T he United States’ military is on the hunt for an F-35 fighter jet that has gone missing following an incident that forced the pilot to eject from the advanced stealth aircraft over South Carolina. Emergency response teams are trying to find what’s left of the F-35B Lightning II jet, which suffered what the military called a “mishap” on Sunday afternoon, according to social media posts by Joint Base Charleston, an air base in South Carolina. The unidentified pilot ejected safely and was taken to a local hospital in a stable condition. The U.S.-Iran Prisoner Swap Is About More Than Prisoners Watch More 0:04 / 1:20 Joint Base Charleston called on the public to cooperate with military and civilian authorities as the search for the F-35 jet continues. The air base said it was working with Marine Corps Air Station Beaufort to search for the plane north of North Charleston around Lake Moultrie and Lake Marion, based on its last-known location. More From TIME Lockheed Martin Corp is the manufacturer behind the F-35, a single-seat fighter craft used by militaries around the world. The aircraft was a vertical take-off version used by in the US Marine Corps, and the jet is popular for its stealth qualities that make it difficult to detect by radar. The F-35 program, the most expensive US weapons program ever, is projected to cost $400 billion in development and acquisition, plus an additional $1.2 trillion to operate and maintain the fleet over more than 60 years. Each jet can cost more than $160 million, depending on the variant. It’s not the first time an F-35 has been in trouble. An F-35B version crashed in 2018 in Beaufort County, South Carolina, because of a manufacturing defect in a fuel tube, according to the Government Accounting Office’s report. The following year, a Japanese F-35A stealth fighter plunged into the ocean during an exercise over the Pacific Ocean, which Japan blamed on pilot disorientation, rather than technical issues. The missing aircraft in the US swiftly drew online mockery, from postings with Missing-Jet fliers on lamp posts and notices on milk cartons, to mashed up Dude, Where’s My F-35 movie posters. “Now that I got that out of the way. How in the hell do you lose an F-35?” South Carolina Republican Representative Nancy Mace said on social media. “How is there not a tracking device and we’re asking the public to what, find a jet and turn it in?” CCPA Notice Here’s Why Thousands of People Are Trapped at Burning Man TIME.com Decades Later Sub Found, They Looked Inside investing.com Japanese Rocket Takes Off for the Moon TIME.com Completely New Electric Cars for Seniors - The Prices Might Surprise You Electric Car Deals Japan Warns Citizens of Harassment in China Over Wastewater Backlash TIME.com MORE MUST-READS FROM TIME Meet the 2023 TIME100 Next: the Emerging Leaders Shaping the World Jalen Hurts Is Fueled by the Doubters Impeachment Experts Say Biden Inquiry May Be Weakest in US History Martin Scorsese Still Has Stories to Tell Burned Out at Work? Find Someone to Split Your Job 50-50 With You Jessica Knoll Wants to Correct the Record on Ted Bundy The Most Anticipated Books, Movies, TV, and Music of Fall 2023 Why It Takes Forever to Get a Doctor's Appointment Want Weekly Recs on What to Watch, Read, and More? Sign Up for Worth Your Time Harrison Ford's Inheritance Makes The Headlines investing.comSponsored Volvo's Brand New EX90 Electric Crossover Is Turning Heads The 2024 EX90Sponsored Four Sisters Reveal The Same Man Got Them All Pregnant Take a look. Yeah MotorSponsored Click Here CONTACT US AT LETTERS@TIME.COM. YOU MAY ALSO LIKE WORLD Vladimir Putin and Kim Jong Un Meet at Remote Siberian Rocket Launch Facility U.S. Seattle Police Officer Who Joked After Woman’s Death Says Remarks Were Misunderstood WORLD A Timeline of Russia and North Korea’s Complicated Relationship HEALTH There's No Sign of Widespread COVID-19 Mandates. Republicans Are Warning of Them Anyway Home U.S. Politics World Health Business Tech Personal Finance by TIME Stamped Shopping by TIME Stamped Future of Work by Charter Entertainment Ideas Science History Sports Magazine The TIME Vault TIME For Kids TIME CO2 Coupons TIME Edge Video Masthead Newsletters Subscribe Subscriber Benefits Give a Gift Shop the TIME Store Careers Modern Slavery Statement Press Room TIME Studios U.S. & Canada Customer Care Global Help Center Contact the Editors Reprints and Permissions Site Map Media Kit Supplied Partner Content About Us © 2023 TIME USA, LLC. All Rights Reserved. Use of this site constitutes acceptance of our Terms of Service, Privacy Policy (Your California Privacy Rights) and Do Not Sell or Share My Personal Information. TIME may receive compensation for some links to products and services on this website. Offers may be subject to change without notice. This website uses cookies to enhance user experience and to analyze performance and traffic on our website. We also share information about your use of our site with our social media, advertising and analytics partners. Accept Cookies Cookie Settings ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-_~ x",
    "commentLink": "https://news.ycombinator.com/item?id=37555172",
    "commentBody": "US Military asks for help locating missing F-35Hacker NewspastloginUS Military asks for help locating missing F-35 (time.com) 244 points by fortran77 21 hours ago| hidepastfavorite5 comments dang 13 hours ago [–] Comments moved to https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37552740. reply fragmede 11 hours agoparent [–] maybe disable commenting here as well? reply fortran77 11 hours agorootparent [–] I didn’t know “thedrive” was considered a more reliable source. reply dredmorbius 3 hours agorootparentHN generally awards primary status to the earliest post. That can get messy on popular or major stories where there&#x27;s a flood of dupes. Other factors may involve paywall&#x2F;non-paywalled sources (NYT is paywalled), and stories based on reporting elsewhere (HN prefers the original source).TheDrive&#x27;s story was posted at 2023-9-18 06:01:40The NYT story was posted at 2023-9-18 12:20:40The former was earlier by 6 hours 19 minutes. reply dang 11 hours agorootparentprev [–] That wasn&#x27;t implied. I don&#x27;t know if it is or isn&#x27;t. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The US military is looking for a missing F-35 fighter jet following a \"mishap\" in South Carolina, requesting public assistance in the search due to the aircraft's stealth capabilities.",
      "The pilot of the jet safely ejected, indicating that the incident resulted in no loss of life.",
      "Notably, this is not the first time an F-35, part of the most expensive weapons program in US history, has been involved in incidents like crashes or technical problems."
    ],
    "commentSummary": [
      "The US Military is seeking help to find a missing F-35 aircraft, as discussed on Hacker News.",
      "The discourse on the forum includes debates on the trustworthiness of the news sources.",
      "There are also comments addressing the timing of the posts."
    ],
    "points": 244,
    "commentCount": 5,
    "retryCount": 0,
    "time": 1695039640
  },
  {
    "id": 37561668,
    "title": "Update on KDP Title Creation Limits",
    "originLink": "https://www.kdpcommunity.com/s/article/Update-on-KDP-Title-Creation-Limits?language=en_US&forum=KDP%20Forum",
    "originBody": "Bookshelf Reports Community Marketing Help KDP University English Show menu Log in KDP FORUM HOME Update on KDP Title Creation Limits clock icon September 18, 2023 As we previously shared, we are actively monitoring the rapid evolution of generative AI and the impact it is having on reading, writing, and publishing, and we remain committed to providing the best possible shopping, reading, and publishing experience for our authors and customers. While we have not seen a spike in our publishing numbers, in order to help protect against abuse, we are lowering the volume limits we have in place on new title creations. Very few publishers will be impacted by this change and those who are will be notified and have the option to seek an exception. As always, all KDP publishers must adhere to our KDP content guidelines, regardless of how the content was created. We will continue to keep the interests of our authors, publishers, and readers at the forefront of our thinking and decision-making. Thank you for being a part of the KDP community. Back to top © 1996-2023, Amazon.com, Inc. or its affiliates. All Rights Reserved. Amazon and Kindle are trademarks of Amazon.com Inc. or its affiliates. KDP Select Earn more money and reach new readers KDP Community Connect with experts and fellow authors ACX Indie audiobook publishing made easy KDP Terms and Conditions Privacy Notice Community Guidelines Conditions of Use Contact Us kdp.amazon.co.jp",
    "commentLink": "https://news.ycombinator.com/item?id=37561668",
    "commentBody": "Update on KDP Title Creation LimitsHacker NewspastloginUpdate on KDP Title Creation Limits (kdpcommunity.com) 218 points by ilamont 13 hours ago| hidepastfavorite221 comments japhyr 13 hours agoIt&#x27;ll be interesting to see where this goes. Amazon has had ML-generated garbage books for years now, and I assume they haven&#x27;t taken them down because they make money even when they sell garbage.Maybe there&#x27;s so much garbage coming in now that they finally have to do something about it? I feel for people trying to learn about technical topics, who aren&#x27;t aware enough of this issue to avoid buying ML-generated books with high ratings from fake reviews. The intro programming market is full of these scam books. reply m463 13 hours agoparentI was thinking about buying an air fryer. My search came up with cookbooks specific to that air fryer, and I was intrigued. I found a good 5-star book, but then I found that ALL the 5-star reviews were submitted the same day.I complained, but Amazon defended the book as legitimate, and since I hadn&#x27;t purchased it, they would not take any action. (to be honest, I assume frontline customer service reps don&#x27;t have much experience or power)So I purchased it, complained, got a refund and then they were able to accept my complaint (after passing the complaint higher in the food chain).Seriously, how hard was it amazon? I guess they&#x27;re starting to notice.Take a look at air fryer cookbooks - there are books specific to most makes and models. But everything is ML copypasta all the way up and down - the title, the recipes and the reviews all seem to be generated garbage. reply japhyr 12 hours agorootparentI&#x27;m the author of Python Crash Course, the best selling introductory Python book for some time now. Years ago, someone put out a book listing two authors: Mark Matthes and Eric Lutz. That&#x27;s just a simple juxtaposition of my name and Mark Lutz, the author of O&#x27;Reilly&#x27;s Learning Python. The subtitle is obviously taken from my book&#x27;s subtitle as well. I assume the text is an ML-generated mess, but I haven&#x27;t bought a copy to verify that.I used to comment on reviews for books like these explaining what was happening, but Amazon turned off the ability to comment on reviews a long time ago.I&#x27;ve spoken with other tech authors, and almost all of us get emails from people new to programming who have bought these kinds of books. If you&#x27;re an experienced programmer, you probably know how to recognize a legitimate technical book. But people who are just starting to learn their first language don&#x27;t always know what to look for. This is squarely on Amazon; they have blocked most or all of the channels for people to directly call out bad products, and they have allowed fake reviews to flourish and drown out authentic reviews. reply rmbyrro 11 hours agorootparentWhy don&#x27;t beginners start at Python.org, though? It&#x27;s such a great resource to learn the language.- it&#x27;s free, unlike books- always up-to-date, unlike even the best book after a few months- easy to choose: heck, there&#x27;s only one official documentation! No chance of making a mistake here! reply japhyr 9 hours agorootparentMany beginners do start at python.org. However, if you don&#x27;t know anything about programming, and you don&#x27;t know someone who can answer all the little questions that come up, it&#x27;s really hard to learn from documentation alone. Even the official Python tutorial is fairly inaccessible to many people who are trying learn a language for the first time.Almost every Python author I&#x27;ve spoken with recognizes that no one resource works best for everyone. We each write to offer our particular take on a subject, and hope to find an audience that our perspective resonates with. I&#x27;ve never steered people away from documentation; in fact one of my goals is to steer people to the sections of documentation that they&#x27;re ready to make sense of. One of my end goals is that people no longer need me as a teacher. That was my goal as a classroom teacher, and it&#x27;s one of my goals as an author.The idea that there are no mistakes in official documentation is pretty unrealistic. Technical documentation has certainly improved over the last decade or so, but it will never be perfect. Most of us recognize that some areas of programming are better handled by third party libraries. In a similar way, there will always be room for learning resources that are maintained outside of official documentation sources. reply rmbyrro 3 hours agorootparentI didn&#x27;t claim the official docs have no mistakes.Since there&#x27;s only one documentation, beginners can&#x27;t get wrong with which docs to use.Ad opposed to books, which have tons of bad choices available (hence the current discussion). reply Armisael16 11 hours agorootparentprevAre you suggesting people just go read the documentation like an encyclopedia? I don’t know a single person who got their start programming by doing that - just about everyone wants some sort of guide to help lead them in good directions. reply skydhash 11 hours agorootparentI did. On Windows, Python had (still have?) a good offline help. And it included a nice getting started tutorial. The only book I had was “The C Programming Language”. But they ignited my interest enough to start researching, and I landed on the \"Site du Zero\" (now OpenClassrooms) platform. The web was sparser, but better, in these days (2010). reply jacquesm 11 hours agorootparentprevThat&#x27;s more or less exactly how I learned to program. From books, with a few friends. Only after it got to a certain level and I started frequenting more places where we met other people working with computers some of which were professional programmers.I still have some of them. They&#x27;ve aged surprisingly well. reply Dylan16807 4 hours agorootparent> That&#x27;s more or less exactly how I learned to program. From booksWhat kind of books? The person you&#x27;re replying to is arguing in favor of books, but saying that the documentation in particular is not a good one to start with. reply jacquesm 1 hour agorootparentI think you are assuming disagreement where there is none. reply NavinF 10 hours agorootparentprevDo the official docs even have tutorials? I&#x27;d send beginners to Khan Academy instead. reply asicsp 7 hours agorootparentYeah, https:&#x2F;&#x2F;docs.python.org&#x2F;3&#x2F;tutorial&#x2F;index.html. But I would say it is good for those who already know another programming language and not for complete beginners. reply rmbyrro 11 hours agorootparentprevI guess book authors don&#x27;t like my perspective... reply nektro 12 hours agorootparentprevi stopped frequenting the dev.to community because the average quality of articles just got so low it stopped being worth my time reply arrowsmith 11 hours agorootparentdev.to is blocked on HN for this reason (try submitting a dev.to link; it won&#x27;t appear under New.)There&#x27;s an old thread where dang explains that it&#x27;s blacklisted (along with many many other sites) due to the consistently poor article quality. reply quickthrower2 11 hours agorootparentprevConversely if you post something sophisticated there it will likely bomb. A bunch of emojis and explaining JS closures for the hundredth time. Does well! reply ocdtrekkie 11 hours agorootparentprevI think the best way to recognize a legitimate tech book is... visit a Barnes and Noble. If it&#x27;s a publisher or series you can find printed on the shelf, books are legit.Unfortunately online market \"platforms\" are pretty much widely untrustworthy for any sort of informational purposes. reply mschuster91 10 hours agorootparent> If it&#x27;s a publisher or series you can find printed on the shelf, books are legit.Not even that is a guarantee, there have been cases of rip-offs making it through a bunch of book-on-demand services.All \"marketplaces\" allowing third parties unlimited, unmonitored access to product listings suffer from that issue. reply failTide 11 hours agorootparentprevalso, just doing your research on any platform other than Amazon helps. reply hinkley 12 hours agorootparentprevUgh. I hate the, \"You&#x27;re not a customer yet so our CRM system won&#x27;t let me talk to you.\"And what happens when my problem is that your system won&#x27;t let me place an order? reply blululu 12 hours agorootparentFalse Negatives and False Positives are always connected. On the other side of the equation, there are plenty of bad actors who will casually flag their competitors to score a quick win. Crime doesn&#x27;t like to go uphill - raising the stakes for feedback lowers the prevalence of bad actors. reply me_again 12 hours agorootparentprevI think that&#x27;s a different issue. Amazon has thorny problems with takedowns. Company A trying to get rival company B&#x27;s listing taken down probably happens 100&#x27;s of times a day. I believe Amazon uses \"proof of purchase\" kinda like a CAPTCHA or proof of work - an extra hoop to jump through to reduce the volume of these things they have to adjudicate. reply hinkley 4 hours agorootparentIt should be a term of service that you’re not allowed to interfere with other customer’s listings.If I found out one of the tenants on my multi tenant system was trying to mess with another’s, I would be livid. reply indymike 12 hours agorootparentprevCRM should never mean Sales Prevention as a Service. reply hinkley 12 hours agorootparentThe great thing about filtering is that you don&#x27;t have to hear the screams.These accidents play out in slow motion until someone corners you at a family reunion and asks why their friends can&#x27;t create accounts and when you ask them how long they say \"months\". reply nanidin 6 hours agorootparentprevYou&#x27;d think... but in a growing b2b company, the CRM is where sales get prevented under a certain threshold. heh. reply onlyrealcuzzo 12 hours agorootparentprev> Seriously, how hard was it amazon? I guess they&#x27;re starting to notice.It&#x27;s not hard. It&#x27;s a cost center, and they&#x27;re in the business of making money - not providing the best service. reply kristopolous 11 hours agorootparentThey&#x27;re biggest risk has always been the perception they peddle fraudulent simulacrums of worthy products. reply tiew9Vii 12 hours agorootparentprevIt’s the same across all big tech. The size&#x2F;volume for complaint handling doesn’t scale. It’s either filtered out by some machine learning algorithm or some poor person in a 3rd world country getting paid next to nothing who reviews the complaints so quality isn’t of importance.There been a recent influx of scammers on Facebook local groups. Air con cleaning, car valeting, everyone’s calling out the scammers in the comments yet when you click report to FB the response is we have reviewed the post and it has not breached our guidelines, would you like to block the user. reply nanidin 12 hours agorootparentprevIf I don&#x27;t get where I want to be with the front door customer service within a decent amount of time, I have always had good success contacting jeff@amazon.com. Their executive support team gets back quickly via email or phone and they really seem to care. reply miohtama 13 hours agoparentprevGarbage books are used for money laundering.You buy books using stolen credit cards and such.https:&#x2F;&#x2F;www.theguardian.com&#x2F;books&#x2F;2018&#x2F;apr&#x2F;27&#x2F;fake-books-sol... reply hinkley 12 hours agorootparentI wonder if that means the Feds made a phone call to Jeff on his private line and said we need to have a little chat.We can track money laundering when there are X fake books. We can&#x27;t when there are 10X fake books. reply harles 13 hours agoparentprev> … I assume they haven&#x27;t taken them down because they make money even when they sell garbage.I’d be surprised if this is the case. The money they make is probably a rounding error compared even just to other Kindle sales. Much more likely is that they haven’t seen it as a big enough problem - and I’m willing to bet it’s increased multiple orders of magnitude recently. reply throe37848 13 hours agoparentprevI knew guy who made \"generated\" text books in 2010. He would absorb several articles, and loosely stitch them into chapters with some computer scripts and from memory. In a week he would produce 400 pages on new subject. It was mostly coherent and factual (it kept references). Usually it was the only book on market about given subject (like rare disease).Current auto generated garbage is very different. reply velcrovan 8 hours agorootparentFor several years now, Amazon KDP will block books whose content is already available on the web. I have printed a few books whose content was either CC-BY or public domain due to its age, and in each case my book was automatically blocked in the early stages. I had to submit an appeal that was reviewed by a person in order to proceed. reply franze 13 hours agorootparentprevexplains the CouchDB Book from OReily from that time. reply quickthrower2 11 hours agorootparentDo people still use CouchDB? Blast from the past! reply plagiarist 13 hours agorootparentprevI wouldn&#x27;t even consider that generated. That&#x27;s like where useful content and copyright infringement overlap on a Venn diagram. reply Karellen 12 hours agorootparent> That&#x27;s like where useful content and copyright infringement overlap on a Venn diagram.That sounds like a description of LLM-generated content to me ;-) reply delecti 11 hours agorootparentLLMs only ever accidentally generate useful content. They fundamentally can&#x27;t know whether the things they&#x27;re outputting are true, they just tend to be, because the training data also tends to be. reply mortureb 12 hours agoparentprevIn my opinion, all we learn over time is that we need gatekeepers (publishing houses in this case). The general public is a mess. reply chongli 11 hours agorootparentI think what we’re seeing here is a symptom of the broader and more fundamental problem of trust in society. We’ve gone from a very high trust society to a very low trust society in just a few decades. We, as technology people, keep searching (desperately) for technical solutions to social problems. It’s not working. reply pixl97 11 hours agorootparentBecause technology never was the solution for social problems, it&#x27;s a solution to the few people getting very rich problem. reply barrysteve 11 hours agorootparentprevThe standards for filtering internet data have dropped badly.Amazon and Google both abuse their filtering systems on a daily basis to effect social change.We need new companies built with policies to keep the filtering systems rigid, effective and unchanging. We need filterkeepers. reply mortureb 11 hours agorootparentI’m good with Amazon and Google over some unknown. I don’t want some right wing shit to be my gatekeepers. reply barrysteve 11 hours agorootparentYay, politics in my business soup. That&#x27;ll generate a quality outcome for my customers!&#x2F;sThe politics are ephemeral, the results matter. reply mortureb 11 hours agorootparentHuman decency transcends your asinine, barely disguised political talking points. reply barrysteve 6 hours agorootparentYou may consider it asisine and political, but the results are measurable. Amazon delivers AI generated whatever and a better filter system doesn&#x27;t do that.Politic and insult to your hearts content.The results are quantifiable and qualitatively measurable. You&#x27;know, like, science.What do you want me to say?If Amazon can fix the flood of garbage, then good, I don&#x27;t care. I&#x27;ll shut up.All the politics is coming from your side of the table, and all the discussion about measurable results is coming from my side of the table. Whatever happens, let&#x27;s make that distinction razor sharp and clear. replyVancouverMan 12 hours agorootparentprevSuch systems just result in content that is terribly bland, or worse, intentionally limited to push specific political narratives.I&#x27;d rather have a much more diverse and interesting set of content to choose from, even if some of it might not be to my liking, and even if I&#x27;d have to put some effort into previewing or filtering before I find something I want to consume. reply ozfive 11 hours agorootparentSome people value their time, energy, and money more. I can appreciate that you do not as we all have choices but I imagine that most people would disagree. reply VancouverMan 5 hours agorootparent> Some people value their time, energy, and money more.More highly \"curated\" media providers have almost always been the least-efficient, most-costly, and least-satisfying for me.Buying physical books at a bookstore has typically been a costly waste of time, with the selection being poor, and it requiring time, money, vehicle wear, etc., to actually get to the store.Public libraries are often worse in terms of selection, and thanks to the ones where I am being funded via taxation, I&#x27;m stuck paying for them even if I don&#x27;t use them.Online and ebook sellers are somewhat better, although they can still be costly, and the delivery of physical books can take some time.I&#x27;ve had much better success finding fiction and non-fiction content by doing some searches and seeing which random websites, forums, and other less-\"curated\" online resources I happen to run across.It has been the same for video media, too.OTA TV is relatively cheap, but the selection is so limited as to make it useless.Cable and satellite TV have upfront costs, and then ongoing costs, plus a relatively limited selection of content available at any given time.Paid online streaming providers have a cost, obviously, and I&#x27;ve found the selection to be quite poor.Movie theatres are extremely costly for what you get, have a tremendously limited selection, and also involve significant travel and time costs.Tape and disc rentals no longer exist today where I am, aside from public libraries. They had per-rental costs, late fees, travel costs, and very limited selection. As stated before, I pay for the library even if I don&#x27;t use it.YouTube, on the other hand, gives me a much better experience than the more \"curated\" providers. With just a minute or two of searching, I can find hours and hours worth of content to watch each evening, I can view this content with almost no delay, the cost is minimal, and the content is far more entertaining and informative than the more \"curated\" options.Avoiding \"curated\" media providers has saved me a lot of time, energy, and money, in addition to providing me with much more enjoyable and useful content. reply tetrep 12 hours agoparentprev> Maybe there&#x27;s so much garbage coming in now that they finally have to do something about it?It seems like this is preventative action rather than reactionary, as they say that there hasn&#x27;t been an increase in publishing volume, \"While we have not seen a spike in our publishing numbers...\" reply ehsankia 11 hours agoparentprevI thought it was more so filled with low quality mechanical turk garbage books. reply gz5 13 hours agoprevI think we will see tidal waves of &#x27;not-so-good&#x27; AI-generated content. Not that AI can&#x27;t generate or help generate &#x27;good&#x27; content, but it will be faster and cheaper to generate &#x27;not-so-good&#x27;.These waves will mainly be in places in which we are the product. And those waves could make those places close to uninhabitable for folks who don&#x27;t want to slosh through the waves of noise to find the signal.And in turn that perhaps enables a stronger business model for high quality content islands (regardless of how the content is generated) - e.g. we will be more willing to pay directly for high quality content with dollars instead of time.In that scenario, AI could be a_good_thing in helping to spin a flywheel for high quality content. reply rwmj 13 hours agoparentAssuming not too many people die eating mushrooms while we&#x27;re waiting: https:&#x2F;&#x2F;www.theguardian.com&#x2F;technology&#x2F;2023&#x2F;sep&#x2F;01&#x2F;mushroom-... reply hinkley 12 hours agorootparentCommon foraging rhetoric is that you need two independent sources asserting that a wild food is edible. Ones that cite neither each other or the same chain of citations. And preferably a human who says, \"I&#x27;ve been eating these for years and no problems.\" or scientists who did recent blood work to make sure you aren&#x27;t destroying your organs by eating [1].In a world with fake books, it would be quite easy for two books to contain the same misinformation or mis-identification (how many times have I found the wrong plant in a google image search? More times than I care to count). Two fake books putting the wrong mushroom picture next to a mushroom because they were contiguous on some other page and you have dead people.[1] In the ten years since I started working with indigenous plants, wild ginger (asarum caudatum), has gone from quasi-edible to medicinal to don&#x27;t eat. More studies show subtler wear and tear on the organs (wikipedia lists it as carcinogenic!) and it is recommended now that you don&#x27;t eat them at all, even for medicinal purposes. I&#x27;m not sure I own a foraging or native species book younger than 5 years, and many are older. reply eindiran 6 hours agorootparentDamn had no idea about wild ginger. That is a bummer. reply omnicognate 12 hours agoparentprevExcept they shouldn&#x27;t be islands. Unify&#x2F;standardise the payment mechanism, make it frictionless and only for content consumed. There&#x27;s no technical reason you shouldn&#x27;t see an article on hn or wherever, follow the link and read it and pay for it without having set up and pay for a subscription for the entire publication or jump through hoops. It should be a click at most.There will always be a place for subscriptions, but people want the hypertext model of just following a link from somewhere and there is absolutely no technical reason for that to be incompatible with paying for content. The idea that ads are the only way to fund the web needs to be challenged, and generative AI might just provide the push for that to finally happen.Or maybe there will be no such crisis and it&#x27;ll just make the whole thing even more exploitative and garbage-filled. reply munificent 12 hours agorootparent> There&#x27;s no technical reason you shouldn&#x27;t see an article on hn or wherever, follow the link and read it and pay for it without having set up and pay for a subscription for the entire publication or jump through hoops. It should be a click at most.People have been saying this and building startups on this and having those startups crash and burn for decades.It&#x27;s not a technical problem. It&#x27;s a psychology problem.Paying after you&#x27;ve read an article doesn&#x27;t provide the immediate post purchase gratification to make it an inpulse purchase [0]. The upside of paying for an article you&#x27;ve already read is more like a considered purchase [1]. But the amount of cognitive effort worth putting into deciding whether or not to pay for the article is often less than the value you got from the article itself. So it&#x27;s very hard for people to force themselves to decide to commit to these kinds of microtransactions. See also [2].It&#x27;s just a sort of cognitive dead zone where our primate heuristics don&#x27;t work well for the technically and economically optimal solution. It&#x27;s sort of like why you can&#x27;t go into a store and buy a stick of gum.[0]: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Impulse_purchase[1]: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Considered_purchase[2]: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bounded_rationality reply omnicognate 4 hours agorootparentI&#x27;m a bit confused here. I never said the click would be after reading the article. You would need to pay to read.Edit: Ah, I did say> see an article on hn or wherever, follow the link and read it and pay for itThat wasn&#x27;t supposed to be a chronological sequence of events, but I see I accidentally implied that. Apologies for the confusion. reply pixl97 11 hours agorootparentprev>It should be a click at most.Welcome to new and interesting ways to defraud people over the internet for money school of thought.At least with Amazon it&#x27;s a \"one and done shop\" of who I spent my money with when I bought something.Imagine tomorrow with your click to pay for random links on the internet you suddenly have 60,000 1 cent charges. They all appear to go different places and to get a refund you need to challenge each one. reply bobthepanda 11 hours agorootparentIt sounds like the digital version of the CD scam. https:&#x2F;&#x2F;viewing.nyc&#x2F;nyc-scams-101-dont-get-fooled-by-the-cd-... reply omnicognate 4 hours agorootparentprevI think you&#x27;re imagining this would be open to random individual bloggers, but that wouldn&#x27;t solve the quality &#x2F; clickbait &#x2F; AI generation problem. Sure, individuals could scam, but they could also produce clickbait, low effort crap.The context of this discussion is the high quality, paid, edited writing that is currently behind site-wide subscription paywalls at sites like the New York Times, Wall Street Journal, Financial Times, Economist, etc. It would be great to lower the barrier to entry for individual writers as far as possible, and maybe even include some sites that are run more like blogging platforms, but there would always have to be content standards and some degree of editorial control for reasons other than avoidance of scams, and with those things in place avoidance of scams is a non-issue because you&#x27;re dealing with organisations that are trading on reputation. The New York Times isn&#x27;t going to be defrauding its readers (and neither is Medium if it comes to that). reply Supply5411 13 hours agoprevWhile not exactly the same, the invention of the printing press caused a lot of controversy with the Catholic Church. With the printing press, people could mass produce and spread information relatively easily. I&#x27;m sure a lot of it was considered \"low quality\" (also heretical)[1]. Seems like we&#x27;re going through similar growing pains now. Yes I know it&#x27;s different, but it rhymes.1. https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Index_Librorum_Prohibitorum reply dougmwne 13 hours agoparentI really dislike the comparison. The printing press democratized knowledge. The LLM destroys it. LLM output is perfect white noise. Enough of it will drown out all signal. And the worst part is that it’s impossible to distinguish it from real human output.I mean think about it. Amazon had to stop publishing BOOKS because it can no longer separate the signal from the noise. The printing press was the birth of knowledge for the people and the LLM is the death. reply ben_w 12 hours agorootparent> LLM output is perfect white noise.Not even close to white noise. White noise, in the context of the token space, looks like this:auceverts exceptionthreat.\" The printing press democratized knowledgeThat&#x27;s true, but it also allowed protestant \"heretics\" to propagate an idea that caused a permanent schism with the Catholic church, which led to centuries of wars that killed who-knows-how-many people, up to recent times with Northern Ireland.(Or something like that, my history&#x27;s fuzzy, but I think that&#x27;s generally right?) reply bbarnett 12 hours agorootparentI thought it was a king wanting a divorce, and as he couldn&#x27;t get it from the catholic church, created his own. reply mmcdermott 12 hours agorootparentHenry VIII created the Church of England in 1534 for the purposes of granting himself an annulment. Most histories count Martin Luther&#x27;s 95 Theses as beginning of the Reformation in 1517 (a crisp date for a less-than-crisp event; Luther did not originally see himself as protesting the Roman Catholic Church). The Protestant Reformation was a heterogeneous movement from the beginning. reply TRiG_Ireland 12 hours agorootparentprevNot really, no. It was Luther who kick-started Protestantism. Henry VIII attempted to supplant the Pope, and kind of slid into Protestantism by accident. reply vladms 12 hours agorootparentprevThat was the case just for the anglican church, which is only one \"part\" of the reformation. reply verve_rat 12 hours agorootparentprevProtestantism started in Germany with Martin Luther nailing his theses to a church door. Henry&#x27;s reproductive problems came later and where only sort of related. reply mrighele 11 hours agorootparentprev> Amazon had to stop publishing BOOKS because it can no longer separate the signal from the noise.That&#x27;s because they are trying very hard not to check what they are selling, hoping that their own users and a few ML algorithms can separate the signal from the noise for them. It seems to me that the approach is no longer working, and they should start doing it by themselves. reply rockemsockem 13 hours agorootparentprevI really feel like you can&#x27;t have used any advanced LLMs if you legitimately think the out put \"perfect white noise\". The results that you can get from an LLM like GPT-4 are incredibly useful and are providing an enormous amount of value to lots of people. It isn&#x27;t just for generating phony information to spread or having it do your work for you.I get the most value out of asking for examples of things or asking for basic explanations or intuitions about things. And I get so much value from this that I really think the printing press is the most apt comparison. reply softg 12 hours agorootparentThe problem is advanced LLMs are controlled by large corporations. Powerful local models exist (in part thanks to Meta&#x27;s generosity oddly enough) and they&#x27;re close to GPT-3.5, but GPT-4 is far ahead of them and by the time other models reach to that point whatever OpenAI or Antropic, Meta etc. have developed behind closed doors could be significantly better. In that case open models will be restricted to niche uses and most people will use the latest model from a giant corp.So it is possible that LLMs will centralize the production and dissemination of knowledge, which is the opposite of what people think the printing press did. I hope I&#x27;m wrong and open models can challenge&#x2F;overtake state of the art models developed by tech giants, that would be amazing. reply courseofaction 12 hours agorootparentPrecisely. I spent weeks learning about cybersecurity when GPT-4 first came out, as I could finally ask as many stupid questions as I liked, get detailed examples and use-cases for different attacks and defenses, and generally actually learn how the internet around me works.Now it refuses, because OpenAI&#x27;s morals apparently don&#x27;t include spreading openly available knowledge about how to defend yourself.Scary. I have also been using it to generate useful political critiques (given a particular theoretical tradition, some style notes, and specific articles to critique, it&#x27;s actually excitingly good). What if OpenAI decides that&#x27;s a threat? What reason do we have to think that a powerful institution would not take this course of action, in the cold light of history? reply blibble 12 hours agorootparenthow do you know what you learnt wasn&#x27;t completely made up gibberish? reply Supply5411 11 hours agorootparentThe same way you know that the things you learn from a person isn&#x27;t made up gibberish: You see how well it explains a scenario, how well it lines up with your knowledge and experience, and you sample parts to verify. reply blibble 11 hours agorootparenthow do you know they didn&#x27;t learn from the garbage generator too? reply SargeZT 8 hours agorootparentYou are literally describing the fundamental problem of truth in philosophy and acting as if it&#x27;s different because a computer is involved at one step in the chain. reply courseofaction 1 hour agorootparentprevBecause it works. reply mostlylurks 11 hours agorootparentprevWhat you say is not in conflict with AI-generated content being white noise. Even if you find some piece of AI-generated content useful, it is still white noise if it is merely combining pieces of information found in its dataset and the result is posted online or published elsewhere. There is no signal being added in that process, and it pollutes the space of content. Humans are also prone to doing this, but with the help of AI, it becomes a much larger issue.\"Signal\" would mean new data, which is by definition not possible via LLMs trained on publicly available content, since that means the data is already out there, or new and meaningful ideas or innovations beyond just combining existing material. I have not seen LLMs accomplish the latter. I consider it at least possible that they are capable of such a feat, but even then the relevant question would be how often they produce such things compared to just rearranging existing content. Is the proportion high enough that unleashing floods of AI-generated content everywhere would not lower the signal-to-noise ratio from the pre-AI situation? reply rmbyrro 12 hours agorootparentprev> the worst part is that it’s impossible to distinguish it from real human outputDoesn&#x27;t that make human content look bad in the first place?If we can&#x27;t distinguish a Python book written by a human engineer or by ChatGPT, how can we demonstrate objectively that the machine-generated one is so much worse? reply mostlylurks 12 hours agorootparentThat argument might work for content which serves a purely informational purpose, such as books teaching the basics of programming languages, for instance, but it doesn&#x27;t work for art (e.g. works of fiction) because most of the potential for a non-superficial reading of a work relies on being able to trust that there is an author that has made a conscious effort to convey something through that work, and that that something can be a non-obvious perspective on the world that differs from that of the reader. AI-generated content does not have any such intent behind it, and thus you are effectively limited to a superficial reading, or if were to instist on assigning such intent to AI, then at most you would have one \"author\" per AI model, which additionally has no interesting perspectives to offer, simply those perspectives deemed acceptible in the culture of whatever group of people developed the model, no perspective that could truly surprise or offend the reader with something they had not yet considered and force them to re-evaluate their world view, just a bland average of their dataset with some fine tuning for PR etc. reasons. reply jameshart 7 hours agorootparentprevWe can distinguish it. That&#x27;s what publishers and editors do. It&#x27;s also what book buyers for book chains used to do. Reviewers, writing for reputable publications, with their own editors and publishers, as well.Humans, examining things, and putting a reputation that matters on the line to vouch for it.The fact that Amazon doesn&#x27;t want to have smart, contextually aware humans look at and evaluate everything people propose to offer up for sale on their storefront doesn&#x27;t mean it can&#x27;t be done. Same as how Google doesn&#x27;t want to look at every piece of content uploaded to YouTube to figure out if it&#x27;s suitable for kids, or includes harmful information. That&#x27;s expensive, so they choose not to do it. reply Nashooo 12 hours agorootparentprevThe problem is not that no one can distinguish it. It&#x27;s that the intended audience (beginners in Python in your example) can&#x27;t distinguish it and are not able to easily find and learn from trusted sources. reply rmbyrro 12 hours agorootparentAren&#x27;t there already bad Python books written by humans?I bet ChatGPT can come up with above-average content to teach Python.We should teach beginners how to prompt engineer in the context of tech learning. I bet it&#x27;s going to yield better results than gate-keeping book publishing. reply nneonneo 12 hours agorootparentThere are, but it used to take actual time and effort to produce a book (good or bad), meaning that the small pool of experts in the world could help distinguish good from bad.Now that it’s possible to produce mediocrity at scale, that process breaks down. How is a beginner supposed to know whether the tutorial they’re reading is a legitimate tutorial that uses best practices, or an AI-generated tutorial that mashes together various bits of advice from whatever’s on the internet? reply rmbyrro 12 hours agorootparentPersonally I don&#x27;t subscribe to the \"best practices\" expression. It implies an absolute best choice, which, in my experience, is rarely sensible in tech.There are almost always trade-offs and choosing one option usually involves non-tech aspects as well.Online tutorials freely available very rarely follow, let&#x27;s say, \"good practices\".They usually omit the most instructive parts, either because they&#x27;re wrapped in a contrived example or simplify for accessibility purposes.I don&#x27;t think AI-generated tutorials will be particularly worse at this to be honest... reply rmbyrro 12 hours agorootparentprevAnother great contribution would be fine-tuning open source LLMs on less popular tech. I&#x27;ve seen ChatGPT struggling with htmx, for example (I presume the training dataset was small?), whereas it performs really well teaching React (huge training set, I presume) reply emporas 12 hours agorootparentprevIf beginners in Python programming are not capable of visiting python.org, assuming they are genuinely interested in learning Python, it would be very questionable how good their knowledge on the subject can really be. reply rmbyrro 12 hours agorootparent100% agreed.I&#x27;ve seen many developers using technologies without reading the official documentation. It&#x27;s insane. They make mistakes and always blame the tech. It&#x27;s ludicrous... reply courseofaction 12 hours agorootparentprevThe LLM does democratize knowledge, but you have to be the user of the LLM, not the target of the user of the LLM.The LLM is the most powerful knowledge tool ever to exist. It is both a librarian in your pocket. It is an expert in everything, it has read everything, and can answer your specific questions on any conceivable topic.Yes it has no concept of human value and the current generation hallucinates and&#x2F;or is often wrong, but the responsibility for the output should be the user&#x27;s, not the LLM&#x27;s.Do not let these tools be owned, crushed and controlled by the same people who are driving us towards WW3 and cooking the planet for cash. This is the most powerful knowledge tool ever. Democratize it. reply shitloadofbooks 12 hours agorootparentAsking a statistics engine for knowledge is so unfathomable to me that it makes me physically uncomfortable. Your hyperbolic and relentless praise for a stochastic parrot or a \"sentence written like a choose your own adventure by an RNG\" seems unbelievably misplaced.LLMs (Current-generation and UI&#x2F;UX ones at least) will tell you all sorts of incorrect \"facts\" just because \"these words go next to each other lots\" with a great amount of gusto and implied authority. reply Supply5411 12 hours agorootparentMy mind is blown that someone gets so little value out of an LLM. I get over software engineering stumbling blocks much faster by interrogating an LLM&#x27;s knowledge about the subject. How do you explain that added value? Are you skeptical that I am actually moving and producing things faster? reply lxgr 12 hours agorootparentMy mind is also blown by how much people seemingly get out of them.Maybe they’re just orders of magnitude more useful at the beginning of a career, when it’s more important to digest and distill readily-available information than to come up with original solutions to edge cases or solve gnarly puzzles?Maybe I also simply don’t write enough code anymore :) reply Supply5411 12 hours agorootparentI&#x27;m very far from the beginning of my career, but maybe I see a point in your comment, because I frequently try technologies that I am not an expert in.Just yesterday, I asked if Typescript has the concept of a \"late\" type, similar to Dart, because I didn&#x27;t want to annotate a type with \"| null\" when I knew it would be bound before it was used. Searching for info would have taken me much longer than asking the LLM, and the LLM was able to frame the answer from a Dart perspective.I would say that that information neither \"important to digest\" nor \"readily available.\" reply lxgr 11 hours agorootparentAh yes, gathering information in a particular unfamiliar area probably describes it better.For me, it&#x27;s been able to give very good answers when they were within the first few Google results when searched for using the proper terms (but the value is in giving you these terms in the first place!).For questions from my field, it&#x27;s been wildly hallucinating and producing half-truths, outdated information, or complete nonsense. Which is also fair, because the documentation where the answers could be found is often proprietary, and even then it&#x27;s either outdated or outright wrong half of the time :) reply pests 12 hours agorootparentprevI agree with you but at what point does it change? Aren’t we all just stochastic parrots? How do we ourselves choose the next word in a sentence? reply lxgr 10 hours agorootparentIn my view, one big learning from LLMs is that yes, more often than not we are just stochastic parrots. And more often than not that&#x27;s enough!But sometimes we&#x27;re more than that: Some types of deep understanding aren&#x27;t verbal or language-based, and I suspect that these are the ones that LLMs will have the hardest time getting good at. That&#x27;s not to say that no AI will get there at all, but I think it&#x27;ll need something fundamentally different from LLMs.For what it&#x27;s worth, I&#x27;ve personally changed my mind here: I used to think that the level of language proficiency that LLMs demonstrate easily would only be possible using an AGI. Apparently that&#x27;s not the case. reply skydhash 11 hours agorootparentprevWe use languages to express ideas. Sentences are always subordinate to the ideas. It&#x27;s very obvious when you try to communicate in another language you&#x27;re not fluent in. You have the thought, but you can&#x27;t find the words. The same thing happens when writing code, taking ideas from the business domain and translating it into code. reply barrysteve 11 hours agorootparentprevIf you wish to make an apple pie, first you must make the universe from scratch. (carl sagan)We can generate thoughts that are spatially coherent, time aware, validated for correctness and a whole bunch of other qualities that LLMs cannot do.Why would LLMs be the model for human thought, when it does not come close to the thoughts humans can do every minute of every day?Aren&#x27;t we all just stochastic parrots, is the kind of question that requires answering an awful lot about the universe before you get to an answer. reply __loam 11 hours agorootparentprevGod dammit please stop comparing these things to brains. Stop it. It&#x27;s not even close. reply __loam 11 hours agorootparentprevThis happened to me looking up am obscure c library. It just confidently made up a function that didn&#x27;t actually exist in the library. It got me unstuck but you can really fuck yourself if you trust it blindly. reply halfmatthalfcat 12 hours agorootparentprev> but the responsibility for the output is the user&#x27;s, not the LLM&#x27;s.The current iteration of the internet (more specifically social media) has used the same rationality for its existence but at a level, society has proven itself too irresponsible and&#x2F;or lazy to think for itself but be fed by the machine. What makes you think LLMs are going to do anything but make the situation worse? If anything, they’re going to reenforce whatever biases were baked into the training material, of which is now legally dubious. reply lxgr 12 hours agorootparentprevFor a librarian, they’re confidently asserting factual statements suspiciously often, and refer me to primary literature shockingly rarely. reply arrowsmith 11 hours agorootparentIn other words they behave like a human? reply ForHackernews 12 hours agorootparentprev> and can answer your specific questions on any conceivable topicYeah, I mean, so can I, as long as you don&#x27;t care whether the answers you receive are accurate or not. The LLM is just better at pretending it knows quantum mechanics than I am. reply scarmig 12 hours agorootparentEven if a human expert responds about something in their domain of expertise, you have to think critically about the answer. Something that fails 1% of the time is often more dangerous than something that fails 10% of the time.The best way to use an LLM for learning is to ask a question, assume it&#x27;s getting things wrong, and use that to probe your knowledge which you can iteratively use to prove the LLM&#x27;s knowledge. Human experts don&#x27;t put up with that and are a much more limited resource. reply jedberg 13 hours agorootparentprevIf you asked the Church back then, they would tell you that the printing press was the death of truth, because to them only the word of god was truth, and only the church could produce it.It&#x27;s all just a matter of perspective.Yes, right now it looks like white noise, just like back then it looked like white noise which could drown out the religious texts. But we managed to get past it then and I&#x27;m sure we&#x27;ll manage now. reply duskwuff 13 hours agorootparentThis is an astoundingly bad take. Surely you aren&#x27;t trying to suggest that original, factual, human-authored content has no more inherent value than randomly generated nonsense? reply rileyphone 11 hours agorootparentThat&#x27;s Wittgenstein&#x27;s argument. reply jedberg 12 hours agorootparentprevNo not at all, I&#x27;m not sure why you would even think that. reply duskwuff 12 hours agorootparentAs I read it, your parent comment suggests that the distinction in quality and utility between human-authored and AI-generated content is merely \"a matter of perspective\", i.e. that there is no real distinction, and that they&#x27;re both equally valuable.If you actually meant something else, you should probably clarify. reply lovemenot 11 hours agorootparentI am not the person to whom you replied. I understood their comment to be about paradigms shifting through social awareness of the limits and opportunities of new technology.It can be both true that right now predominantly low quality content emanates from LLMs and at some future time the highest quality material will come from those sources. Or perhaps even right now (the future is already here, just unevenly distributed).If that was their reasoning, I tend agree. The equivalent of the Catholic Church in this metaphor is the presumption human-generated content&#x27;s inherent superiority. reply __loam 11 hours agorootparentLLMs are inherently approximations of collective knowledge. They will never be better than their training sets. It&#x27;s a statistical impossibility. reply dambi0 12 hours agorootparentprevSuggesting clarification to suit your imaginary inferences seems puzzling. The parent post pointed out that perspectives on authorship have a historical precedent, I didn’t see the value judgement your reading suggested. reply rmbyrro 12 hours agorootparentprevThe discussion here is that we&#x27;re not able to distinguish them.If we cannot distinguish, I&#x27;d argue they have similar value.They must have. Otherwise, how can we demonstrate objectively the higher value in the human output? reply snailmailman 12 hours agorootparentThey can be distinguished. They are just becoming more difficult to. Its slightly-more difficult, but also the amount of garbage is overwhelming. AI can spit out entire books in moments that would take an individual months or years to write.There are lots of fake recipe books on amazon for instance. But how can you really be sure without trying the recipes? It might look like a recipe at first glance, but if its telling you to use the right ingredients in a subtly-wrong way, its hard to tell at first glance that you won&#x27;t actually end up with edible food. Some examples are easy to point at, like the case of the recipe book that lists Zelda food items as ingredients, but they aren&#x27;t always that obvious.I saw someone giving programming advice on discord a few weeks ago. Advice that was blatantly copy&#x2F;pasted from chat GPT in response to a very specific technical question. It looked like an answer at first glance, but the file type of the config file chat GPT provided wasn&#x27;t correct, and on top of that it was just making up config options in attempt to solve the problem. I told the user this, they deleted their response and admitted it was from chatGPT. However, the user asking the question didn&#x27;t know the intricacies of \"what config options are available\" and \"what file types are valid configuration files\". This could have wasted so much of their time, dealing with further errors about invalid config files, or options that did not exist. reply duskwuff 10 hours agorootparent> Some examples are easy to point at, like the case of the recipe book that lists Zelda food items as ingredientsAs an aside, the case you&#x27;re thinking of was a novel, not a recipe book. Still embarrassing, but at least it was just a bit of set dressing, not instructions to the reader.https:&#x2F;&#x2F;www.cnet.com&#x2F;culture&#x2F;zelda-breath-of-the-wild-recipe...> I saw someone giving programming advice on discord a few weeks ago. Advice that was blatantly copy&#x2F;pasted from chat GPT in response to a very specific technical question.This, on the other hand, is a very real and a very serious problem. I&#x27;ve also seen users try to get ChatGPT to teach them a new programming language or environment (e.g. learning to use a game development framework) and ending up with some seriously incorrect ideas. Several patterns of failure I&#x27;ve seen are:1) As you describe, language models will frequently hallucinate features. In some cases, they&#x27;ll even fabricate excuses for why those features fail to work, or will apologize when called out on their error, then make up a different nonexistent feature.2) Language models often confuse syntax or features from different programming languages, libraries, or paradigms. One example I&#x27;ve heard of recently is language models trying to use features from the C++ standard library or Boost when writing code targeted at Unreal Engine; this doesn&#x27;t work, as UE has its own standard library.3) The language model&#x27;s body of \"knowledge\" tends to fall off outside of functionality commonly covered in tutorials. Writing a \"hello world\" program is no problem; proposing a design for (or, worse, an addition to) a large application is hopeless. reply SargeZT 8 hours agorootparent> The language model&#x27;s body of \"knowledge\" tends to fall off outside of functionality commonly covered in tutorials. Writing a \"hello world\" program is no problem; proposing a design for (or, worse, an addition to) a large application is hopeless.Hard disagree. I&#x27;ve used GPT-4 to write full optimizers from papers that were published long after the cutoff date that use concepts that simply didn&#x27;t exist in the training corpus. Trivial modifications were done after to help with memory usage and whatnot, but more often than not if I provide it the appropriate text from a paper it&#x27;ll spit something out that more or less works. I have enough knowledge in the field to verify the corectness.Most recently I used GPT-4 to implement the paper Bayesian Flow Networks, a completely new concept that I recall from the comment section on HN people said \"this is way too complicated for people who don&#x27;t intimately know the field\" to make any use of.I don&#x27;t mind it when people don&#x27;t find use with LLMs for their particular problems, but I simply don&#x27;t run into the vast majority of uselessness that people find, and it really makes me wonder how people are prompting to manage to find such difficulty with them. reply rmbyrro 12 hours agorootparentprevThey can indeed distinguish them, I agree. So why the fuss?I think the concern is that bad authors would game the reviews and lure audiences into bad books.But aren&#x27;t they already able to do so? Is it sustainable long term? If you spit out programming books with code that doesn&#x27;t even run, people will post bad reviews, ask for refunds. These authors will burn their names.It&#x27;s not sustainable. reply snailmailman 11 hours agorootparentIt doesn&#x27;t need to be sustainable as one author or one book. These aren&#x27;t real authors. Its people using AI to make a quick buck. By the time the fraud is found out, they&#x27;ve already made a profit.They make up an authors name. Publish a bunch of books on a subject. Publish a bunch of fake reviews. Dominate the search results for a specific popular search. They get people to buy their book.Its not even book specific, its been happening with actual products all over amazon for years. People make up a company, sell cheap garbage, and make a profit. But with books, they can now make the cheap garbage look slightly convincing. And the cheap garbage is so cheap to produce in mass amounts that nobody can really sort through and easily figure out \"which of these 10k books published today are real and which are made up by ai\".It takes time and money to produce cheap products at a factory. But once these scammers have the AI generation setup, they can just publish books on loop until someone ends up buying one. They might get found out eventually, and they will have to pretend to be a different author, and they just repeat the process. reply failuser 11 hours agorootparentprevWhat’s the fuss about spam? You can distinguish it from useful mail? What’s the fuss about traffic jams? You’ll get there eventually.The LLM allow DDoS attack by increasing the threshold needed to check the books for gibberish.It’s not like this stream of low quality did not exist before, but the topic is hot and many grifters try LLMs to get a quick buck at the same time. reply geraldwhen 11 hours agorootparentprevIt’s sustainable if you can automate the creation of amazon seller accounts. Based on the number of fraudulent Chinese seller accounts, I’d say it’s very likely automated or otherwise near 0 cost. reply mostlylurks 11 hours agorootparentprevA piece of human-written content and a piece of AI-written content may have similar value if we cannot distinguish between them. But if you can add the information that the human-written content was written by a human to the comparison, the human-written content becomes significantly more valuable, because it allows for a much deeper reading of the text, since the reader can trust that there has been an actual intent to convey some specific set of ideas through the text. This allows the reader to take a leap of faith and put in the work required to examine the author&#x27;s point of view, knowing that it is based on the desires and hopes of an actual living person with a lifetime of experience behind them instead of being essentially random noise in the distribution. reply skydhash 12 hours agorootparentprevI&#x27;m not a native English speaker, but ChatGPT answers in each interaction I had with it sound bland. And I dislike the bite-sized format of it. I&#x27;m reading \"Amusing Ourselves to Death\" by Neil Postman and while you may agree or disagree with his take, he developed it in a very coherent way, exploring several aspects. ChatGPT&#x27;s output falls into the same uncanny valley as the robotic voice from text to speech software, understandable, but no human does write that way.ChatGPT as an autocompletion tool is fine, IMO. As well as generating alternative sentences. But anything longer than a paragraph falls back to the uncanny valley. reply rmbyrro 11 hours agorootparentI totally agree. So why are people so worried about books being written by ChatGPT?These pseudo-authors will get bad reviews, will lose money in refunds, burn their names.It&#x27;s not sustainable. Some will try, for sure, but they won&#x27;t last long. reply Dylan16807 4 hours agorootparentThere&#x27;s too many names and it&#x27;s too cheap to do this.The equilibrium shifts to making it much harder to find good books, and that was already hard enough. reply failuser 11 hours agorootparentprevIf you ask LLM something you know you can distinguish noise from good output. If you ask LLM something you don’t know then how do you know if the output is correct? There are cases where checking is easier than producing the result, e.g. when you ask for a reference. reply rmbyrro 11 hours agorootparentBook buyers should give themselves primarily by who&#x27;s the author, I think.Choose a book from someone that has a hard earned reputation to protect. reply failuser 10 hours agorootparentThere is bootstrapping process of learning which authors in that field have good reputation before you know anything about the field. That is being disrupted by LLMs as well, though. reply emodendroket 12 hours agorootparentprevI can&#x27;t distinguish between pills that contain the medicine that I was prescribed and those than contain something else entirely. Therefore taking either should be just as good. reply rmbyrro 12 hours agorootparentReally. Are you comparing a complex chemical analysis required to attest the contents of a pill to reading text? reply emodendroket 12 hours agorootparentIt depends, is the text of a technical nature? How exactly is one to know they&#x27;re being deceived if, to take one of the examples that has been linked in this discussion, they receive a mushroom foraging guide but the information is actually AI-generated? reply rmbyrro 11 hours agorootparentYou first check who published it. Is the author an expert in the matter with years, perhaps decades in the industry?Heck, we always did that since before GPT.Good authors will continue to publish good content because they have a reputation to protect. They might use ChatGPT to increase productivity, but will surely and carefully review it before signing off. reply emodendroket 10 hours agorootparent\"We\" certainly did not \"always\" do that before. reply rmbyrro 3 hours agorootparentReally? You buy books without searching anything about who wrote it?If yes, well, there&#x27;s the problem then. It&#x27;s not AI, but the lack of guidance and research skills in support of the process of choosing a book. replySketchySeaBeast 12 hours agorootparentprevIf they were of similar value would there be a problem with the deluge? reply rmbyrro 11 hours agorootparentCan&#x27;t the deluge be delusional or an overreaction at best? reply tzs 8 hours agorootparentprevThe printing press made books cheap relative to hand copied books, but they were still expensive for most people.Before the printing press two books cost around the same as a 2 story cottage.Afterwards a couple books would be about a month of wages for a skilled worker.That greatly limits ones ability to drown out anything with books. reply SketchySeaBeast 13 hours agorootparentprevI&#x27;d argue that giving a group with unique thoughts and ideas a voice is different than creating a noise machine. reply jedberg 13 hours agorootparentI think the jury is still out on whether an LLM produces ideas any more or less unique than most humans. :) reply OfSanguineFire 13 hours agorootparentprev> The printing press democratized knowledge.Not for centuries. Due to the expense of the technology and the requirement in some locations for a royal patent to print books, the printing press just opened up knowledge a bit more from the Church and aristocracy to the bourgeoisie, but it did little for the masses until as late as the 1800s. reply ls612 13 hours agorootparentA big part of this is that literacy didn’t come to the masses until the 1800s. But in England and the Netherlands you had (somewhat) free press by the late 1600s and early 1700s. reply peab 13 hours agorootparentprevI&#x27;m reminded of the Library of Babel reply pacman2 12 hours agorootparentprevI was told publishers dont promote a good book anymore these days. They ask how many instagram followers do you have?Maybe the self-publishing and BoD will decline in the long term due to ML white noise and publishers are a sign of quality again. reply Supply5411 13 hours agorootparentprevYou could argue that speech is literally noise that drowns out the signals of your environment. If you just babbled, it would be useless, but instead you use it intelligently to communicate ideas. LLM output is a new palette with which humans can compose new signals. We just have to use it intelligently.Prompt engineering is an example of this. A clever prompt by a domain expert can prime an LLM interaction to yield better information to the recipient in a way that the recipient themselves could not have produced on their own. reply __loam 11 hours agorootparentprevPeople comparing the AI bullshit spigot to the printing press are clowns. reply woah 12 hours agorootparentprevIt used to be that a scribe would painstakingly copy a manuscript, through the process absorbing the text at a deep level. This same scribe could then apply this knowledge to his own writing, or just understand and curate existing work. The manual labor required to copy at scale employed many scribes, who formed the next generation of thinkers.With the press, a greasy workman can churn out hundreds of copies an hour, for whichever charlatan or heretic palms him enough coin. The people are flooded with falsehoods by men whose only interest in writing is how many words they can fit on a page, and where to buy the cheapest ink.The worst part is that it is impossible to distinguish the work of a real thinker from that of a cheap sophist, since they are all printed on the same rough paper, and serve equally well as tomorrow&#x27;s kindling. reply emodendroket 12 hours agorootparentWhere are the good AI-generated books that serve as the positive side of this development? reply BarryMilo 12 hours agoparentprevYou&#x27;re implying that what is being produced has actual value, the problem is they&#x27;re acting in patently bad faith. Weep not for the spammers. reply langsoul-com 7 hours agoparentprevThe incentive system is completely different. The new AI generated content is for a quick buck, just spamming out content because $1 x 10,000 is a lot.If it was written with the aid of AI, that&#x27;s different. At least someone tried to make something good and just used avalible tools to enhance the quality. reply gamepsys 12 hours agoparentprevThe rhyme has a lot to do with how existing power structures handle a sudden increase in the amount of written text generated. In this comparison, they both try to apply the breaks. Banned books didn&#x27;t work well for the Catholic Church. I think increasing QA for Amazon might actually help their book business. Of course, a book seller has a greater responsibility to society than to make money. reply thaumasiotes 8 hours agoparentprev> the invention of the printing press caused a lot of controversy with the Catholic Church> https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Index_Librorum_ProhibitorumThe example is from the sixteenth century, but the printing press is from the seventh century.I don&#x27;t think the Catholic Church bothered to take any notice at all? reply Barrin92 11 hours agoparentprev>similar growing painsFor what it&#x27;s worth, these &#x27;growing pains&#x27; took the form of the wars of religion in Europe, which in Germany killed up to 30% of the population, that&#x27;s in relative terms significantly worse than the casualties of World War I and II. So maybe the Catholic Church had a point reply lovemenot 11 hours agorootparent>> So maybe the Catholic Church had a pointIs that really the take-away? If the Catholic Church had not been so belligerent, those wars would not have been needed. Now that we are past that time, we should surely be thanking those combatants who helped disseminate knowledge in spite of the Church whose interest was in hoarding it. reply Barrin92 11 hours agorootparentI think that&#x27;s a pretty bad reading of history frankly. The Church didn&#x27;t hoard knowledge, in fact they were arguably the primary preservers of knowledge and disseminator of it, through the monastic tradition in Medieval Europe. Many thousands of which were destroyed during the religious wars, which is a common theme as far sectarian wars go. They are first and foremost destroyers of knowledge.More importantly I certainly wouldn&#x27;t want to live through that period for any reason, and much less repeat it. If an ordinary printing press caused that much chaos I&#x27;m not sure I want to figure out what one on steroids is going to do reply hinkley 13 hours agoprevHow do we...I&#x27;m not entirely sure how to word this question.How do we make sure that most of the people we talk to are at least humans if not necessarily the person we expect them to be? And I&#x27;m not saying that like a cartoonish bad guy in a movie who hates artificial intelligence and augmented humans.How do I not get inundated by AI that&#x27;s good at trolling. How do I keep the social groups I belong to from being trolled?These questions keep drawing me back to the concept of Web of Trust we tried to build with PGP for privacy reasons. Unless I&#x27;ve solicited it, I really only want to talk to entities that pass a Turing Test. I&#x27;d also like it if someone actively breaking the law online were actually affected by the deterrence of law enforcement, instead of being labeled a glitch or a bug in software that can&#x27;t be arrested, or even detained.It feels like I want to talk to people I know to be human (friends, famous people - who might actually be interns posing as their boss online), and people they know to be human, and people those people suspect to be human.I have long term plans to set up a Wiki for a hobby of mine, and I keep getting wrapped around the axle trying to figure out how to keep signup from being oppressive and keep bots from turning me into an SEO farm. reply timeagain 12 hours agoparentThis is only a problem for someone terminally online. The vast majority of people talk to their friends and coworkers in person. reply munificent 11 hours agorootparentThat was the solution that came to mind to me too, but it doesn&#x27;t work either.Even if you&#x27;re never online and only talk to people in person... over time those people will be increasingly informed by LLM-generate pseudo-knowledge. We aren&#x27;t just training the AIs. They&#x27;re training us back.If you want to live in a society where the people you interact with have brains mostly free of AI-generated pollution, then I&#x27;m sorry but that world isn&#x27;t going to be around much longer. We are entering the London fog era of the Information Age. reply hinkley 12 hours agorootparentprevI don&#x27;t trust my friends for medical advice. Some of them trust me for plant advice, and they really probably shouldn&#x27;t. I am very stove-piped.We have two and a half generations of people right now most of whom think \"I did the research\" means \"I did half as much reading as the average C student does for a term paper, and all of that reading was in Google.\"And Alphabet fiddles while Google burns. This is going to end in chaos. reply Spivak 6 hours agorootparent> \"I did the research\" means \"I did half as much reading as the average C student does for a term paperWhat&#x27;s the alternative? No one who says that is saying they did original research, they&#x27;re saying they searched around and got what they believe to be at least a consensus among the body of experts they trust.Like I agree the problem sucks but I have no idea what a solution looks like. For fields someone is totally unfamiliar with they simultaneously don&#x27;t have enough knowledge to evaluate the truth of a claim nor the knowledge to evaluate if someone is qualified and trustworthy enough to believe them. It&#x27;s turtles all the way down -- especially because topics of any interest you can find as many experts as you care to of whatever qualification you demand making all sorts of contradictory claims. reply mostlylurks 11 hours agorootparentprev> This is only a problem for someone terminally online.Is it? Even those whose social life is entirely IRL, they still have to increasingly interact with various businesses, banks, healthcare providers, the government, and often more distant collegues through online services. Do I want these to go through LLM chatbots? No. Can I ensure that I&#x27;m speaking to an actual human if the communication is text based? Not really. reply invalidptr 12 hours agorootparentprevThis is a problem for anyone who is not actively vigilant about the information they consume. A family member (who I would not describe as \"terminally online\") came to me today in a panic talking about how some major event had just occurred and how social order was beginning to collapse. I quickly glanced at the headlines on a few major news outlets and realized that they just saw some incendiary content designed to elicit that reaction. I calmed them down and walked them through a process they could use to evaluate information like that in the future, and they were a little embarrassed.The concern isn&#x27;t necessarily for you. It&#x27;s for the large swaths of people who are less equipped to filter through noise like this. reply romseb 12 hours agoparentprevThere is some irony in Sam Altman bringing us the cause (AI) and purported solution (Worldcoin) for your problem at the same time. reply hinkley 11 hours agorootparentIt&#x27;s what ad men do. Point out there&#x27;s a problem, offer you the solution. reply kiicia 11 hours agoparentprevwe don&#x27;t, check Boltzmann brain https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Boltzmann_brain reply vorpalhex 12 hours agoparentprevMeet people in real life. This problem is trivially solved by just using meatspace.Alternatively for sign ups, tell them to contact you and ask. Chat with them a moment. Ask them about their hobbies and family. reply ethanbond 11 hours agorootparentUsing meatspace doesn&#x27;t solve the problem, using meatspace exclusively solves the problem. And it&#x27;s not a great one given, you know, how much of the world \"happens\" online now. reply ilamont 13 hours agoprevSee also: \"Tom Lesley has published 40 books in 2023, all with 100% positive reviews\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35687868 reply ritzaco 13 hours agoparentI remember that one - interestingly the amazon link it goes to shows only 3 books now, all that look real, not the 40 that I remember seeing before.So I guess Amazon is doing something even though I regularly hear complaints from authors that they allow blatant piracy all the time reply kmeisthax 12 hours agorootparentAmazon has no reason to give a shit about piracy on KDP: they make money either way. But having a load of AI generated garbage on your platform makes it far less valuable. You want your stolen books to actually be good. :P reply bragr 13 hours agorootparentprev>shows only 3 books nowThose appear to be by different authors with similar names: https:&#x2F;&#x2F;www.amazon.com&#x2F;s?k=%22tom+lesley%22 reply phh 13 hours agorootparentprevPossibly it&#x27;s the author removing them at the first one star rating to keep their author score high? reply willio58 13 hours agoprevIt seems Amazon cares more about polluting search results in Kindle than polluting the search results in their own e-commerce business. I think low-effort books generated by AI are much less detrimental than sketchy physical products being shipped to your door in 2 days or less. reply crooked-v 13 hours agoparentIt&#x27;s probably about volume rather than quality. Sketchy copycat product lines are still hard limited by the number of factories and shipping operations in existence, while sketchy AI-generated books can easily keep growing exponentially in number for a while. reply harles 13 hours agoprevThe title of this story doesn’t seem to match the content. This seems like a proactive move to prevent individual publishers from spamming many many submissions - and even then, they’re willing to make exceptions.> While we have not seen a spike in our publishing numbers, in order to help protect against abuse, we are lowering the volume limits we have in place on new title creations. Very few publishers will be impacted by this change and those who are will be notified and have the option to seek an exception. reply Almondsetat 13 hours agoprevLivestreams where artists show their creative process and use the streaming platform to immediately sell the thing they produced, just to prove it had human origins.This is the future reply edgarvaldes 13 hours agoparentWe have realtime filters, avatars, translators, TTS, etc. Soon, all of this will be \"good enough\" to mimic the proposed solution. reply SketchySeaBeast 13 hours agoparentprevYou&#x27;re only kicking the can down the road. reply adamredwoods 13 hours agoprev>> We require you to inform us of AI-generated content (text, images, or translations) when you publish a new book or make edits to and republish an existing book through KDP. AI-generated images include cover and interior images and artwork. You are not required to disclose AI-assisted content. reply hiidrew 13 hours agoparentTheir distinction:>AI-generated: We define AI-generated content as text, images, or translations created by an AI-based tool. If you used an AI-based tool to create the actual content (whether text, images, or translations), it is considered \"AI-generated,\" even if you applied substantial edits afterwards. AI-assisted: If you created the content yourself, and used AI-based tools to edit, refine, error-check, or otherwise improve that content (whether text or images), then it is considered \"AI-assisted\" and not “AI-generated.” Similarly, if you used an AI-based tool to brainstorm and generate ideas, but ultimately created the text or images yourself, this is also considered \"AI-assisted\" and not “AI-generated.” It is not necessary to inform us of the use of such tools or processes.https:&#x2F;&#x2F;kdp.amazon.com&#x2F;en_US&#x2F;help&#x2F;topic&#x2F;G200672390#aicontent.... reply prvc 13 hours agorootparentAllowing the use of tools to modify the contents erases any clear distinction between the categories. reply pcl 13 hours agoparentprevThis is really interesting. I imagine that AI-generated art &#x2F; illustrations for books mostly-text is a pretty compelling thing for authors, for all the same reasons that AI-generated text is of value for non-authors. I wonder how this line will work out in practice. reply el_benhameen 13 hours agoprevThis doesn’t seem surprising. Half of my YouTube ads these days are for some kind of AI+Kindle-based get rich quick scheme. reply fuddle 13 hours agoprevAbout time, YouTube is full of videos about making eBook&#x27;s with ChatGPT. e.g \"Free Course: How I Made $200,000 With ChatGPT eBook Automation at 20 Years Old\" https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=Annsf5QgFF8 reply dzink 13 hours agoprevStrategically, AI generated content is a boon for platforms like Amazon.1. The more content there is, the more you can&#x27;t reliably get good stuff without reviews, the more centralized distribution platforms with reviews and rankings are needed. 2. Even if people are making fake books for money laundering, Amazon gets a cut of all sales, laundered or not.Just like Yahoo&#x27;s directory once upon a time though, and Movie theaters, the party gets ruined when most people learn they can use AI to generate custom stories at home and&#x2F;or converse with the characters and interact in far more ways than currently possible. Content is going from king to commodity. reply blibble 11 hours agoparentamazon&#x27;s reviews and rating are completely garbage and have been for some time reply neilv 12 hours agoprevThis sounds like a commendable move by Amazon. I especially like the idea of requiring disclosure of use of \"AI\". reply cogman10 12 hours agoprevHere&#x27;s a pretty good article about the problem with AI generated books. \"AI Is Coming For Your Children\" [1][1] https:&#x2F;&#x2F;shatterzone.substack.com&#x2F;p&#x2F;ai-is-coming-for-your-chi... reply cellu 13 hours agoprevWhy do people read contemporary books is something I can’t really get my head around. There’re so many classics to keep people busy for life - and are 100% guaranteed to be insightful and pleasurable. reply rustymonday 13 hours agoparentShould people stop telling new stories? A century from now the best books of today will be classics. Books can act as a time capsule of a certain time and place and mode of life. And that has value. reply bwb 13 hours agoparentprevContemporary books are just new classics. It is like asking why read :) reply OfSanguineFire 13 hours agoparentprevThere’s a distinct demographic in the contemporary-fiction-reading community, as can be seen in corners of Goodreads or Instagram, that demands new fiction to tell the stories of groups not covered, or supposedly unfairly covered, in that classic literature: LGBT, BIPOC, the working class, etc. In fact, they might even deny that the classics are “insightful and pleasurable” due to these social concerns. reply timeagain 12 hours agorootparentThat’s really weird. People are making all kinds of books and stories. And stories are relevant to their time. The matrix wouldn’t be written in 1900, a tale of two cities wouldn’t be written in 1200, …It is true though that if you have a culturally diverse set of friends and are open to their experiences and opinions, a lot of “the classics” start to smell bad. Imagine being black and reading Grapes of Wrath. You might think the situation of the main characters as humorous or infantile, considering how relatively fortunate they are. reply Baeocystin 12 hours agoparentprevWhat&#x27;s the name of the law where the longer something has already been around, the longer it will likely stay around in the future?I&#x27;ve found that it definitely applies to books. Starting at a ~20 year horizon is a surprisingly good filter for quality. reply savoyard 11 hours agorootparent> What&#x27;s the name of the law where the longer something has already been around, the longer it will likely stay around in the future?The Lindy effect. reply Baeocystin 2 hours agorootparentThank you. reply gamepsys 12 hours agoparentprevI think the risk of reading a suboptimal book is not greater than the risk of not allowing myself to be exposed to different voices. reply carlosjobim 12 hours agoparentprevOne of the best books I read last year was the story of the rescue of the football team that was trapped in a flooded cave in 2018 – written by cave diver Rick Stanton, who found the team and led the rescue. How would that account have been written into a book before it happened? reply barrysteve 11 hours agoparentprevYes, and there&#x27;s been a drop in quality since then too. The 1800-1940s really saw literature as the high water mark for quality media and it shows.Finding deeply valuable and high quality books is much rarer in today&#x27;s crop of authors. The best minds are rarely making the medium of literature their highest good, but are instead chasing dollars and relations with the rich and famous. reply freediver 12 hours agoprevThis is just a tip of the iceberg, compared to what we are heading into with the web. Very concerning.I would go long the value of genuine human writing, aka the &#x27;small web&#x27;. reply campbel 13 hours agoprevGee, I sure hope people don&#x27;t just lie about it... reply skepticATX 13 hours agoparentIt doesn’t matter. It’s garbage content and immediately recognizable as being AI generated.It is absolutely possible to write a good article or even a good book with AI, but at least for now it’s just as hard, if not harder, than doing it without AI.But of course people trying to make a quick buck won’t put in the required effort, and they likely don’t even have the ability to create great or even good content. reply duskwuff 12 hours agorootparent> It’s garbage content and immediately recognizable as being AI generated.It&#x27;s also recognizable by its sheer volume. An \"author\" who submits several new books every day is clearly not doing their own writing. The AI publishing scam relies on volume -- they can&#x27;t possibly win on quality, but they&#x27;re hoping to make up for that by putting so many garbage books on the market that buyers can&#x27;t find anything else. reply atrus 11 hours agorootparentI&#x27;m not sure. Ghostwriting exists, and a person (or organization) with enough money could easily pay enough ghostwriters to output at a more than human pace. reply duskwuff 11 hours agorootparentEven at their most prolific, a ghostwritten author still probably wouldn&#x27;t publish more than one or two books a month. Beyond that point, you&#x27;re just competing with yourself. (For instance, young adult series like Goosebumps, The Baby-Sitters Club, or Animorphs typically published a book every month or two.)Publishing multiple books per day is out of the question. That&#x27;s beyond even what&#x27;s reasonable for an editor to skim through and rubber-stamp. reply mostlylurks 11 hours agorootparentprev> It doesn’t matter. It’s garbage content and immediately recognizable as being AI generated.Is it? How do you immediately recognize a book as AI generated before buying it, if the author isn&#x27;t doing something silly like releasing several books per day&#x2F;month? And even after you buy a book, how can you distinguish between the book just being terrible and the book being written with extensive use of AI? I don&#x27;t believe AI can write good books, but I would still like to distinguish those two cases, since the former is just a terrible book, which is perfectly fine, while the latter I would like to avoid. I don&#x27;t want to waste my limited time reading AI content. reply gamepsys 12 hours agorootparentprev> It’s garbage content and immediately recognizable as being AI generated.Yea, but the Turning Test is actively being assaulted. Soon we won&#x27;t know the difference between an uninspired book written by an AI and an uninspired book written by a human. reply tyingq 12 hours agorootparentprev>It is absolutely possible to write a good article or even a good book with AI, but at least for now it’s just as hard, if not harder, than doing it without AI.How hard is it though, to create a shitty book with AI, that Amazon can&#x27;t detect was written with AI? reply pseingatl 10 hours agoprevIf KDP required an ISBN it would cut down on the garbage books. In the US at least, ISBN&#x27;s cost money. reply fragmede 10 hours agoparentthey&#x27;re not that much but you can just get an Australian one for free reply unmole 9 hours agoprevSo, what are the actual limits? reply idomajid 10 hours agoprevFinally, I hope those garbage books will slightly decrease from there. reply ggm 11 hours agoprevMushroom picking guides on AI \"what could possibly go wrong\" reply corethree 13 hours agoprevHow do we even know this entire comment thread isn&#x27;t polluted with AI?Maybe it doesn&#x27;t matter. The quality of the work matters more than the process of actualization. reply quickthrower2 11 hours agoparentIn a practical sense: AI generated stuff is crappy and often subtly wrong and it can be generated faster than human generated content. So it becomes untenable to even search for good information. reply corethree 9 hours agorootparentThen it&#x27;s good for fiction. Lots of demand for fiction. reply bern4444 12 hours agoprev [–] It seem that as a society we are coming to realize that enabling anyone to do anything on their own and at anytime isn&#x27;t the best of ideas.Verifiability and authenticity matter and are valuable. Amazon has long had a problem of fake reviews. This issue with kindle books seems an extension of that. Massive centralized platforms like Amazon makes fraud more likely and is bad for the consumer.The \"decentralization\" that we need as a society is not in the form of any crypto based technical capability but simply for the size of the massive players to be reduced so competition can reemerge and give consumers more options on where and how to spend their dollars. Other E-book stores may just pop up that develop relationships with publishers and disallow independent publishing if amazon were forced to be broken up.I hope the FTC can begin finding a strategy to force some of these massive corporations to split making it more likely for there to be more competition. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Kindle Direct Publishing (KDP) has shared an update stating they will be reducing the volume limits on new title creations to guard against misuse.",
      "This change will impact only a select few publishers who will be notified and given the choice to apply for an exemption.",
      "KDP underlines the necessity to follow their content guidelines and pledges to prioritize the interests of authors, publishers, and readers in their actions."
    ],
    "commentSummary": [
      "The discussion centers around criticisms leveled at Amazon for allowing AI-generated low-quality books and fake reviews to saturate its platform, potentially misleading newcomers in tech fields.",
      "Opinions vary on the solutions, with some emphasizing the need for curated content, others underscoring the importance of variety, and discussions on the effectiveness of Amazon's measures to regulate, including requiring authors to declare if content is AI-generated.",
      "This conversation reflects a broader debate about AI's role in various fields, the necessity of trustworthy sources, and the call for proactive consumer protection practices."
    ],
    "points": 217,
    "commentCount": 221,
    "retryCount": 0,
    "time": 1695069420
  },
  {
    "id": 37564217,
    "title": "Mythbusters: Wing Commander I Edition",
    "originLink": "https://www.wcnews.com/news/update/16279",
    "originBody": "NEWS ARCHIVE Resources Games Universe Community Mythbusters: Wing Commander I Edition You've probably heard of the famous 'thank you for playing Wing Commander' story. It claims that a programmer on the original Wing Commander was stuck getting an error message when the game unloaded its memory during a quit. Pressed for time, instead of fixing the issue he simply hex edited the memory manager's error reporting to print 'thank you for playing Wing Commander' instead. A funny and relatable story! The anecdote has made the rounds on the internet for the past decade and has even started to make the phrase \"thank you for playing Wing Commander\" synonymous with a dirty-but-functional hack. The most common version looks like this, sourced from Reddit's /r/shittyprogramming: Engineers immediately recognize this as a funny and relatable story; it's the exact sort of thing that anyone who has ever worked in software development can recognize immediately. A perfect example of the old aphorism that \"if it is stupid but it works, it isn't stupid.\"1 Unfortunately, it has also in recent years become fodder for unhappy Star Citizen fans looking for reasons to complain about Chris Roberts' abilities. In many retellings, 'thank you for playing Wing Commander' is now supposedly an example of selling shoddy work instead of a funny, recognizable hack. (If the connection feels tenuous, the missing aspect is that unlike most other CEOs, Roberts continues to code on Star Citizen, an aspect of the project that makes it interesting to him.) Wing Commander I fans, meanwhile, have been understandably cautious about the anecdote and particularly the included screenshot. For one thing, Wing Commander I's default install direction isn't c:/wc1 and the game doesn't actually print \"Thank You for Playing Wing Commander!\" when you quit. Is the story even real? We decided to look into the history of the story and the game itself and after some work we've conclusively determined that the answer is… kind of. Our first task was to find the origin (no pun intended) of the quote and screenshot. Where did dozens of 'funny software hacks' articles around the internet find it in the first place? Was it even a real story or just a piece of mythology that had been handed down, perhaps not even really specific to Wing Commander I? Luckily, the original quote ended up being pretty easy to source: it was left as a comment on August 20, 2009 in response to a Gamasutra article called Dirty Coding Tricks by Brandon Sheffield (archive). Here is the original post, verbatim: Back on Wing Commander 1 we were getting an exception from our EMM386 memory manager when we exited the game. We'd clear the screen and a single line would print out, something like \"EMM386 Memory manager error. Blah blah blah.\" We had to ship ASAP. So I hex edited the error in the memory manager itself to read \"Thank you for playing Wing Commander.\" Gamasutra was a game development website popular among industry professionals. In 2021, the site was rebranded as Game Developer. The original article is still available at the new site but the comments appear to have been dropped in advance of the transition. The comment was, indeed, signed by Ken Demarest. Ken Demarest III was an ace programmer who started his game development career working for Origin Systems as a software engineer on the first Wing Commander. He would go on to be lead programmer on Ultima VII and would become the growing company's Director of Technology (at a time when that technology was becoming pretty exciting!). While the comment was left anonymously (in that it was not signed into a user's account) it's very unlikely anyone was impersonating a game programmer for the purpose of telling a funny story. Wing Commander writer Jeff George would later say that \"other than Chris Roberts, who was the key man, Ken Demarest had more to do with the fact that the game came out than anyone else involved.\" So if anyone knows what was going on under the hood, it should be him! (Be sure to revisit Wing Commander I & II: The Ultimate Strategy Guide's making of section for a surprisingly thorough contemporary interview with Demarest). Eagle-eyed readers will immediately notice two minor changes between the original version and the 2015 Reddit version (which itself was sourced from a LinkedIn share by a third party in 2012): the original does not have the screenshot and the wording of the quit message has been slightly edited apparently with the intent to make the ending punchier. Most importantly, the screenshot added after the fact seemed to claim that the game had shipped with this bug which is not stated in the original story. So the original story, which has a reasonable origin, might be true! The next issue is the message itself. The seemingly daming fact is that Wing Commander I simply doesn't print \"thank you for playing Wing Commander\" when you quit. We tested this in DOSBox and on period hardware and we searched all of the game's binaries for that text, coming up with nothing. Hitting ALT-X simply returns you to the DOS prompt. There is one exception, though! If you quit by clicking on the airlock door in the Tiger's Claw's barracks… … then the quit process is different: you're given a y/n prompt and when you drop to DOS it prints \"You step out of the airlock and into…\", a joke directly relating to this specific method of exit. We quickly confirmed that this text DOES appear in the game's binary. Is it possible that this is where the crash happened and that the story, told 19 years after the fact, simply forgot (or for the purposes of a funnier punchline) changed the story? Supporting this idea in particular is Ken Demarest's other famous connection to that very screen: he's the person that implemented the water drop that drips into the bucket, long cited as the platonic ideal of Chris Roberts' famed drive for immersion. This felt like the most likely way to establish if the story was true: how was the game displaying this message? To try and find an answer, we turned to an incomplete copy of Wing Commander I's source code, archived by Electronic Arts. These source files were used for the release build of Wing Commander I and the archive was eventually sent to another team for development of the FM Towns port. Some material has not survived including, sadly, the memory management routines. But there's enough there to know exactly where this message comes from: This is from BARRACKS.C which sets the functionality of the barracks gameflow screen. Essentially the game unloads the memory and then prints the airlock message followed by a line break. Exactly what you see in action and not a case of hex editing a memory manager. The other method of quitting the game, found in COCKPIT.C, does not print a message. While discussing this on Twitter, Darren Xczek offered another possibility: Ultima VII DOES drop to DOS with a \"thank you for playing Ultima VII\" message. Ken Demarest was the lead programmer on Ultima VII–in fact, it was the job he was initially hired for before he was assigned to help get Wing Commander out the door! Is it possible the story was true but it applied to a different game? Ultima VII did famously have memory management issues, so much so that the game's internal system was named the 'Voodoo Memory Manager'! At this point, I decided there was only one way to solve this mystery: see if we could reach Ken Demarest and ask him. He very kindly responded to a Facebook request in about 45 seconds and made us feel like real idiots for not just asking him first: In a way I would have loved to ship with that hack in there, but once we found the cause of the error message I couldn't in good conscience leave the hack in there. Besides which hand editing it added time to completing the build, which was inefficient. And there it is! The best possible outcome: the story was true–it's something that was done during the game's development–but it was also fixed before the game actually shipped… so it's a clever engineering trick and explicitly NOT evidence of a shoddy product! 1 - Interestingly, the Internet frequently attributes this quote to a novel by Wing Commander novelist Mercedes Lackey; this is surely not the actual source but untangling that would take another article! LOAF September 18, 2023 Comment on this Recent Updates Vindicated Hobbes Emerges from Interrogation Mark Hamill Talks WC Prophecy on Vintage Kilborn Episode Looking Forward to Classic WC Enhancement & Mods Mega CD Wing Commander Gets Some Love Wing Commander 3 Cutscenes (Blue) Versus a Modern 4K Movie (Yellow) Have a Little Trainsim Digging in to Early Timeline Data Stunning Lego Bloodfang to Brighten Your Day Modified WC Movie Scene Adds Nuanced Motivation Last Weekend for GOG Discounts Travel Down the Academy Press Kit Rabbit Hole Intrepid Art Perfectly Captures Animated Style Big Map Appreciation Post First Wing Commander Designs Already Appearing in Starfield One More Album for the Collection Search News advanced Follow or Contact Us All Wings Considered Episode 37 - Back to Gemini! Archived video streams Forums: Recent Posts Myst parody for Mac (that is NOT Pyst) Wing Commander IV DVD edition on Linux Something new... Mega CD Wing Commander Gets Some Love Privateer 2: Can't complete mission \"Davis & Co. Armaments and Supplies, Nerve Weapon - Part C\" Privateer 2: Louissa Phillips continuation? And missing CCN Records entries? Sinners Inn cutscenes? Privateer 2: Editor (v2) crashes if you load a file saved at RS:Bestinium What is the name of that table and the structure Amazing This Site Is Still Running After All These Years... WOW! Confederation Privateer 2: Mission \"CIS Smugglers\" is very glitched but possible to complete (not the normal way). Also Tri-System Hunters Wing Commander Movie Cut Scenes Current Poll How long have you been visiting the CIC? I just got here! (2023) 1-4 years (2019-2022) 5-8 years (2015-2018) 9-12 years (2011-2014) 13-16 years (2007-2010) 17-20 years (2003-2006) 21-24 years (1999-2002) Since the CIC Grand Opening (1998) Since WCHS (1996-1997) Where to Buy Wing Commander 1-2 Wing Commander 3 Wing Commander 4 Prophecy & Secret Ops Privateer Privateer 2: The Darkening Wing Commander Armada Wing Commander Academy Wing Commander Arena Movie Streaming Animated Series Baen Wing Commander Catalog WCPedia: Recent Contributions Edit Wing Commander III: Heart of the Tiger for Saturn by Bandit_LOAF Edit Wing Commander IV: The Price of Freedom for M2 by Bandit_LOAF Edit Wing Commander IV: The Price of Freedom for 3DO by Bandit_LOAF Edit Wing Commander III: Heart of the Tiger for M2 by Bandit_LOAF Edit Wing Commander IV: The Price of Freedom for Saturn by Bandit_LOAF Edit Wing Commander III: Heart of the Tiger for Jaguar CD by Bandit_LOAF Edit Wing Commander Armada for Windows by Bandit_LOAF New Wing Commander for Super Nintendo (unrealized project) by Bandit_LOAF New Wing Commander for Genesis by Bandit_LOAF Edit Wing Commander for Amiga (unrealized project) by Bandit_LOAF Site Staff Ben Lesnick Christopher Reid Barrie Almond Brandon Strevell Kris Vanhecke Aaron Dunbar Jason McHale Copyright 1998 — 2023 The Wing Commander CIC. ALL RIGHTS RESERVED",
    "commentLink": "https://news.ycombinator.com/item?id=37564217",
    "commentBody": "Mythbusters: Wing Commander I EditionHacker NewspastloginMythbusters: Wing Commander I Edition (wcnews.com) 211 points by panic 9 hours ago| hidepastfavorite38 comments theevilsharpie 7 hours agoThis reminds me of a hack that Traveller&#x27;s Tales implemented for Sonic 3D Blast on the Sega Genesis&#x2F;Megadrive.If you physically wiggle or hit the game cartridge enough while the the console is powered on with the game inserted in it, a \"secret level select\" screen pops up where you can select a level.It was long considered to be an easter egg, but actually, it was a custom crash handler that the devs created that trapped all of the main CPU&#x27;s error interrupts and redirected to a level select screen. They did this to avoid having their game build rejected by Sega&#x27;s QA team and delaying the game&#x27;s release. (Sega&#x27;s QA testing was a multi-week process, and a crash would mean the build getting rejected and having to start the process over again.) When people were wiggling the game cart, they were inadvertently unplugging the cart enough for the processor to throw an error.Source (3-minute YouTube video): https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=ZZs2HUW9tDAThe linked \"Coding Secrets\" YouTube channels has other videos details similar hacks, if you&#x27;re interested in going down that rabbit hole. It was wild the things these devs had to do to get games to work within the hardware and business constraints of the time. reply tschwimmer 7 hours agoparentI think it&#x27;s telling when games were much (much) simpler mechanically that QA was a multi-week process. Recent AAA games seem to forgo even a multi-minute QA process by the publisher or the console maker. reply Cthulhu_ 3 hours agorootparent> Recent AAA games seem to forgo even a multi-minute QA process by the publisher or the console maker.Citation needed; frequently the list of QA testers at the developer, publisher and console manufacturer are larger than the list of developers in the credits of a modern day game. There&#x27;s tons of QA testing happening, but also the complexity and size of games are 1000x what they were back then.That said, you can definitely tell a difference in quality between publishers; big titles like God of War, Horizon, even some of the biggest of them all, RDR 2, barely have any major issues at launch.But you&#x27;re probably thinking of Cyberpunk, an overambitious title released too soon, or maybe Starfield, by the company that refused to fix known issues that the community fixed over a decade ago but kept republishing the same title with the same bugs for years. reply stevezsa8 1 hour agorootparentIn games QA, the deadlines are fixed and development creeps into QA time. So you get less QA time than you originally planned. If you&#x27;re lucky, a patch will fix some bugs.In non-games QA, if development takes too long, you typically get an extra sprint to test the changes.In the games industry QA is considered an entry level job with little respect from other departments.In non-games testing, QA is a career that pays double and is usually a respected part of the development process.Basically, I would support the claim that QA could be improved generally across the board in the games industry. reply jxf 34 minutes agorootparent> In non-games testing, QA is a career that pays double and is usually a respected part of the development process.Pays double relative to developers, or pays double relative to their counterparts in the gaming industry? reply toast0 5 hours agorootparentprevThe \"right\" amount of QA is about the cost of fixing errors. When you manufacture a ton of ROMs for a cartridge, there&#x27;s not a lot of cost effective fixes you can do when an error is found. Hopefully it&#x27;s minor, because if it&#x27;s a major error, you&#x27;re looking at a full replacement campaign and that&#x27;s going to be super spendy.On the other hand, how much does it cost to run an update on a Steam game? There&#x27;s certainly a cost, and some of the cost is reputation, which is hard to earn back, but the tradeoff of being able to get patches is the reduction in quality control because problems can be patched. reply mavhc 2 hours agorootparentThe easier it is to update something the worse state you can ship it in.Never trust a device with a reset button. reply unwind 3 hours agoprevThere really is some magic power in being able to reach first-party sources for things like this.It&#x27;s also fun how \"impossible\" it can seem sometimes, especially (for me) if there&#x27;s an age and&#x2F;or geographic distance.For instance I grew up in the 80s in Sweden, and was really into computers (!) back then, too. I had a Commodore 128 (followed by Amigas). There was a lot of gaming going on in C64 mode, and of course \"Uridium\" [1] and \"Paradroid\" [2] were more or less staples that everyone knew about.It&#x27;s still weirding me out that I have their author (Andrew Braybrook) in my Xitter feed [3], and that he is still doing game devlopment after all these years. Back then he felt like some kind of distant (he&#x27;s from the UK, and more than a decade older than me) magician; now he&#x27;s kinda&#x2F;sorta my peer, at least professionally speaking. So weird.I realize things like this happened all the time before the Internet too, but it must have been harder and&#x2F;or more rare.[1]: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Uridium[2]: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Paradroid[3]: https:&#x2F;&#x2F;twitter.com&#x2F;UridiumAuthor reply hoppla 3 hours agoprevHeard a story about a developer auditing the code for missiles. He quickly pointed out that there was memory leaks everywhere … turns out that it was intentional, as the missile had its own garbage collection at the end of flight.Any OOM issues was handled by doubling the expected memory usage during the missiles flight. reply ahoka 7 minutes agoparentHard to believe TBH. reply softwaredoug 8 hours agoprevI just love that there’s a website for wing commander news in 2023 that is kept up to date. reply xcdzvyn 8 hours agoprevThere&#x27;s this comment on the linked Reddit thread:> I remember playing Wing Commander I, for hours the first time I ran it... upon exiting after a marathon, I read that text and thought \"Awe, how nice\".Now knowing the truth, I wonder if they&#x27;re lying or just misremembering. reply ElderKorihor 7 hours agoparentProbably just Mandela effect. I also remember this message that apparently doesn&#x27;t exist. I was also a big player of Ultima VII so maybe my mental wires are crossed as the article suggests. reply photonerd 6 hours agorootparentIt’s also worth noting that it wasn’t exactly uncommon to have messages like that on exit. Apogee games did it a lot I seem to remember. Lots of DOS&#x2F;4GW stuff too.So the memory is… primed, so to speak. reply I_am_uncreative 2 hours agorootparentMaster Of Orion II (1996) has that when you exit the game as well, plenty of games did it during that time period. reply beej71 6 hours agorootparentprevI remember it distinctly, as well. Crazy. reply Sharlin 1 hour agoparentprevMisremembering, of course. Our memories are notoriously unreliable, much more so than we&#x27;d like to think. Thes kinds of messages used to be not unusual, and the simple act of reading the story is plenty enough to generate a false or mixed memory recall. reply aidenn0 4 hours agoparentprevI can see if my floppies still work (they probably don&#x27;t) and see if it&#x27;s different from the GOG&#x2F;Kilrathi Saga versions. reply BSTRhino 2 hours agoprevThis is so strange because I&#x27;m sure I remember seeing the \"Thank you for playing Wing Commander!\" message as a child. Am I the only one? reply tom_ 2 hours agoparentNope! It sound very familiar to me too when I first read about this.Feels like I played a lot of DOS games though, looking back, so perhaps I got mixed up. Printing stuff on exit was not that unusual. reply concordDance 1 hour agoprevThe little distortions that get added to stories as they propagate can be infuriating. Though in this case its harmless. reply MBCook 8 hours agoprevWhat a great ending. reply CodeCompost 1 hour agoprevFormatting of the article is horrible reply jonathankoren 5 hours agoprevWhile this story turned out to be false, I do remember installing the multiplayer game launcher for Half Life 1 back in 1999&#x2F;2000. After clicking “install”, a dialog popped up and said something, “If the installer says the program failed to install, do not worry. The program installed correctly.”I took a screenshot. I had it for years. I may have lost it, or maybe it’s sitting on some old computer at my parents, but I swear it’s true.Sure enough, the error message popped up, and the program was installed correctly.I told the story once on Twitter, and someone responded that this was known problem. It had something to do with Windows 97’s registry updates if I remember correctly. reply AnotherGoodName 3 hours agoparentIf you uninstalled half life 1&#x27;s bundled Sierra Utilities it would remove 1 directory higher than where you installed it. Usually wiping out Program Files. reply lostlogin 3 hours agorootparentOh that’s great.Then what? You wrote a stern letter and posted it? reply thih9 2 hours agorootparentSometimes you posted on a forum, like this: https:&#x2F;&#x2F;arstechnica.com&#x2F;civis&#x2F;threads&#x2F;i-tried-to-uninstall-s... reply bandrami 5 hours agoparentprev> Windows 97I think I&#x27;ve isolated the problem reply Maakuth 4 hours agorootparentThere was a time-unlocked beta version of Windows Memphis (98) making circles that was shoddily rebranded as Windows 97. Of course it&#x27;s likelier that the parent mistyped either 95 or 98. reply courseofaction 4 hours agoprevSide note: Gamasutra.com -> GameDeveloper.com gotta be one of the worst brand identity decisions in recent memory. reply thih9 2 hours agoparentContext: https:&#x2F;&#x2F;screenrant.com&#x2F;gamasutra-changing-name-game-develope...> There is nothing inherently wrong with the Kama Sutra itself - it&#x27;s merely an ancient instructional text on ways one might find fulfillment through emotional and physical love - but the play on words involved in the Gamasutra website title does admittedly cling \"to a late-90s \"LOL SEX\" connotation,\" in the words of Game Developer publisher Kris Graft. reply courseofaction 2 hours agorootparent\"Sutra\" is a rule, method, or teaching. Game-a&#x2F;Gamer speaks for itself. Personally I perceived the name as cleverer than they give it credit for, not just a cheap play on words, but I suppose if that&#x27;s the mindset it was being approached with... reply thih9 1 hour agorootparentThe title was not Gamesutra nor Gamersutra though. reply pests 1 hour agorootparentIt&#x27;s still in the spirit. Sounds better. Similar to how \"MOFO\" sounds better than \"MOFA\" for \"mother fucker\". reply seanthemon 4 hours agoparentprevTwitter -> X gives similar vibes reply psd1 2 hours agorootparentThat and, \"Meta\". It&#x27;s a prefix, not a word. reply hk__2 34 minutes agorootparentIt’s a brand name; you can use whatever you want. \"Google\" is not a pre-existing word either (\"not a word\" is a linguistical nonsense: anything is a word once it’s used as such). reply bruce511 7 hours agoprev [2 more] [flagged] joshschreuder 4 hours agoparent [–] I feel like given the comment from Ken, it is not unreasonable to assume the game shipped like he said.He did specifically say \"We had to ship ASAP\" which implies that it was done to get the game out the door and indeed shipping copies would have the message. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article delves into the origins and credibility of a popular gaming community story about a programmer in the original Wing Commander game.",
      "The story states that the programmer reportedly used a hack to display a humorous message whenever the game encountered an error.",
      "It was found that though the tale is partially accurate and confirmed by the game's lead programmer, the humorous message wasn't displayed in the game as per the narrative."
    ],
    "commentSummary": [
      "This post challenges the myth regarding a hidden level select screen in the game Wing Commander I, initially assumed to be an easter egg.",
      "In reality, it was a custom crash handler developed by the game creators to circumvent potential delays in the release due to possible rejections from Sega's Quality Assurance (QA) team.",
      "The article further explores the role of QA in game development and evaluates the dependability of memories associated with video games."
    ],
    "points": 210,
    "commentCount": 37,
    "retryCount": 0,
    "time": 1695083402
  },
  {
    "id": 37562225,
    "title": "Some new snippets from the Snowden documents",
    "originLink": "https://www.electrospaces.net/2023/09/some-new-snippets-from-snowden-documents.html",
    "originBody": "► Meer over het wetsvoorstel voor de Tijdelijke wet cyberoperaties (in Dutch) September 14, 2023 Some new snippets from the Snowden documents (Updated: September 16, 2023) It's been more than four years since the last regular publication of documents from the Snowden trove. Last year, however, some new snippets of information from the Snowden documents appeared in the PhD thesis of hacktivist Jacob Appelbaum. The new information isn't very spectacular and also quite specialistic, but still worth to make it more easily accessible. Also for the record I added some corrections and additions to Appelbaum's discussion of NSA surveillance methods. NSA headquarters - Appelbaum's thesis - Eindhoven University of Technology Jacob Appelbaum Jacob R. Appelbaum was born in 1983 in California and became a well-known hacker and activist for digital anonymity. He was a member of the Cult of the Dead Cow hacker collective and a core member of the Tor project, which provides a tool for anonymous internet communications. In 2012, Appelbaum moved to Berlin, where he worked closely with Laura Poitras on the NSA documents which she had received from Edward Snowden in May and June 2013. However, he was also involved in the story about the eavesdropping on German chancellor Merkel and the publication of the NSA's ANT Product Catalog. In both cases the documents were not attributed to Snowden and apparantly came from a still unidentified \"second source\". In his thesis, Appelbaum seems to refer to this source when he mentions \"documents exposed by whistleblowers, known and unknown, or other anonymous insiders.\" > See also: Leaked documents that were not attributed to Snowden In 2015, several women accused Appelbaum of sexual abuse and he subsequently lost his position at the Tor project and various other organizations. Appelbaum denied the allegations, but an investigation ordered by the Tor project determined that they appeared to be true. Meanwhile Appelbaum had moved to The Netherlands, where he started as a PhD student at the Eindhoven University of Technology (TU/e). There he finished his thesis and received his PhD on March 25, 2022. Currently he works as a postdoc at the Coding Theory and Cryptology group at TU Eindhoven. Appelbaum's PhD thesis The full title of Appelbaum's thesis is \"Communication in a world of pervasive surveillance. Sources and methods: Counter-strategies against pervasive surveillance architecture\". His promotors were prof.dr. Mark van den Brand, prof.dr. Daniel J. Bernstein and prof.dr. Tanja Lange. The thesis was published on March 25, 2022 and became available for download as a 24.3 MB pdf-document on September 27, 2022. The contents of this 327-page thesis are as follows: - Chapter 1: Introduction. - Chapter 2: Background on network protocols common to all research. - Chapter 3: Background on cryptography common to all research. - Chapter 4: Review of historical, political, economic, and technical adversarial capabilities (including previously published leaked documents that are from works which Appelbaum has written about in his role as a journalist). - Chapter 5: Review of the Domain Name System and an explanation of alternative methods to improve the security and privacy of domain name lookups. - Chapter 6: Examination of a tweak to the WireGuard VPN protocol to protect historic encrypted traffic against future attacks by quantum computers. - Chapter 7: Introduces the Vula protocol, which is a suite of free software tools for automatically protecting network traffic between hosts in the same Local Area Network. - Chapter 8: Introduces REUNION, a privacy-preserving rendezvous protocol. In the preface, Appelbaum writes that his thesis is the culmination of more than a decade of research into the topic of surveillance. He expresses a political and activist aim by saying that the \"machinery of mass surveillance is simply too dangerous to be allowed to exist\" and that \"we must use all of the tools in our toolbox – economic, social, cultural, political, and of course, cryptographic – to blind targeted and mass surveillance.\" He says more has to be done than simply criticize surveillance practices. Cryptography for example, \"allows for resistance in a non-violent manner to the benefit of everyone except the ones who are spying on us.\" From this perspective Appelbaum's thesis discusses various cryptographic implementations to \"protect individual liberty, while aspiring to a broader goal of achieving societal liberty.\" New information from the Snowden documents Throughout his thesis, Appelbaum reveals some new information from Snowden documents that has not been published, but which he had access to during his research that resulted in various publications in media outlets like Der Spiegel, NDR and Le Monde. The new information is only described, so no new original documents were released. According to Appelbaum: \"Many journalists who have worked on the Snowden archive know significantly more than they have revealed in public. It is in this sense that the Snowden archive has almost completely failed to create change: many of the backdoors and sabotage unknown to us before 2013 is still unknown to us today.\" (page 71) Appelbaum also provides some new information about the Snowden documents in general, by saying that The Intercept \"closed their Snowden archive and reportedly it has been destroyed.\" (page 63, note 17) > See also: The Snowden files: where are they and where should they end up? Below, I provide exact quotes from Appelbaum's thesis, including his sources, which are in square brackets, while I added some additional links for further information. 1. BULLRUN: manipulating protocol security \"How do they accomplish their goals with project BULLRUN? One way is that United States National Security Agency (NSA) participates in Internet Engineering Task Force (IETF) community protocol standardization meetings with the explicit goal of sabotaging protocol security to enhance NSA surveillance capabilities.\" \"Discussions with insiders confirmed what is claimed in as of yet unpublished classified documents from the Snowden archive and other sources.\" (page 6-7, note 8) 2. Selecting entropic internet traffic \"There are various rules governing what is selected for long-term data retention in [the NSA's] corporate repositories. One example is that some traffic which is considered entropic by a standard Shannon Entropy estimate is selected from the network in real time and saved to a database, preserving it for cryptanalysis using future technology.\" \"This statement is based in part on an analysis of as of yet unpublished XKeyscore source code that performs a Shannon Entropy estimate. Some kinds of Internet traffic that is considered entropic is recorded for later analysis.\" (page 9, note 16) 3. Compromised lawful interception systems \"As part of our research, we uncovered evidence that the telecommunications infrastructure in many countries has been compromised by intelligence services. The Snowden archive includes largely unpublished internal NSA documents and presentations that discuss targeting and exploiting not only deployed, live interception infrastructure, but also the vendors of the hardware and software used to build the infrastructure. Primarily these documents remain unpublished because the journalists who hold them fear they will be considered disloyal or even that they will be legally punished. Only a few are available to read in public today.\" (page 41) \"Targeting lawful interception (LI) equipment is a known goal of the NSA. Unpublished NSA documents specifically list their compromise of the Russian SORM LI infrastructure as an NSA success story of compromising civilian telecommunications infrastructure to spy on targets within reach of the Russian SORM system.\" (page 41) \"The NSA slides have \"you talk, we listen\" written in Cyrillic on the jackets of two Russian officers.\" \"Review of unpublished Snowden documents about NSA’s activities compromising deployed, lawful interception systems and as well as additional success against the vendors of such hardware or software. Needless to say, a compromised interception system is anything but lawful in the hands of an adversary.\" (page 41, note 4) 4. Compromised computer hardware \"While working on documents in the Snowden archive the thesis author learned that an American fabless semiconductor CPU vendor named Cavium is listed as a successful SIGINT \"enabled\" CPU vendor. By chance this was the same CPU present in the thesis author's Internet router (UniFi USG3). The entire Snowden archive should be open for academic researchers to better understand more of the history of such behavior.\" (page 71, note 21) 5. PRISM \"The PRISM slide deck was not published in full, and the public does not fully understand aspects of the program such as the retrieval of voice content data as seen in Figure 4.24. Domains hosted by PRISM partners are also subject to selector based surveillance. Several pages of the PRISM slides list targets and related surveillance data, and a majority of them appear to be a matter of political surveillance rather than defense against terrorism. One example that is not well-known except among the journalists who had access to the full PRISM slide deck is the explicit naming of targets. An example shows a suggestion for targeting of the Tibetan Government in Exile through their primary domain name. The tibet.net domain is named as an unconventional example that analysts should be aware of as also falling under the purview of PRISM. The email domain was hosted by Google Mail, a PRISM partner, at the time of the slide deck creation and it is still currently hosted by Google Mail as of early 2022.\" (page 76) > See also: What is known about NSA's PRISM program 6. MYSTIC: Country X \"MYSTIC was revealed to impact a number of countries by name at the time of publication: the Bahamas, Mexico, the Philippines, Kenya and one mystery country: country X. The Bahamas, and country X are subject to SOMALGET full take data and voice collection. The publisher WikiLeaks observed that the monitoring of an entire country of people is a crime when done by outside parties, essentially an act of war by the surveillance adversary. WikiLeaks then revealed that the country in question, Country X, was Afghanistan [Yea14]. Through independent review of the Snowden archive, we confirm that this is the identity of Country X, and that WikiLeaks was correct in their claim.\" (page 78) (Strangely enough, the source provided by Appelbaum (\"Yea14\") actually shows that already four days before Wikileaks' revelation, collaborative analysis by Paul Dietrich and the author of this weblog had already pointed to Afghanistan as being Country X. In his bibliography, Appelbaum attributes this source document to \"John Young and et al.\" (the owners of the Cryptome website), while it was actually written by and first published on the blog of Paul Dietrich) 7. Manipulation of DUAL_EC_DRBG \"Many documents released in public from the Snowden archive and additional documents which are still not public make clear that this type of bug is being exploited at scale with help from NSA’s surveillance infrastructure. It is still unclear who authored the changes at Juniper and if bribery from the NSA was involved as with RSA’s deployment of DUAL_EC_DRBG to their customers as is discussed in Section 4.4.\" (page 81) 8. Software backdoors \"Example from the Snowden Archive of an as of yet unreleased backdoor in fielded software that is most certainly not an exclusively exploitable backdoor by NSA. The software’s secret key generation is sabotaged by design to ensure surveillance of the community of interest. There is a corresponding XKeyscore rule that has not yet been published. The goal of that rule is to gather up all ciphertext using this sabotaged system; it is clearly part of a larger strategy. As a flag in the ground for later, the thesis author presents the following SHA256 hash: [...]. There are additional examples from other sources that this is the general shape of the game being played with more than a few acts of sabotage by the NSA.\" (page 83, note 27) Some corrections and additions Chapter 4 of Appelbaum's thesis is about \"the adversary\" and describes a wide range of digital surveillance methods which are used by intelligence agencies. He writes a little a bit about the capabilities of Russia and China, but the biggest part is about the methods of the NSA as revealed through the Snowden documents. In general, this chapter is very similar to for example Glenn Greenwald's book No Place to Hide and Snowden's memoir Permanent Record as it reads like a one-sided accusation against the NSA without much context or the latest information. Chapter 4 also contains small errors which could easily have been prevented. Here I will discuss some examples: - Page 20, note 12: \"An example is Suite-A cryptography or Type-1 cryptography, so designated by the NSA. The NSA now calls this the Commercial National Security Algorithm Suite (CNSA)\" > Comment: Actually CNSA isn't the new name for the highly secure Suite A, but for the less secure Suite B algorithms. - Page 41: \"The BND and the CIA held secret co-ownership of CryptoAG until 1993, and then the CIA held sole ownership until 2018. The devices were vulnerable by design, which allowed unaffiliated intelligence services, such as the former USSR’s KGB, and the East German Ministry for State Security [MfS], to independently exploit CryptoAG’s intentional flaws.\" > Comment: This exploitation by the KGB and the MfS was apparently suggested in a German television report, based upon claims by a former Stasi officer, but so far there are no documents that support this claim. See for more information: Operation RUBICON. - Page 41: \"It does not appear that those party to the Maximator alliance are using their agreement and relative positions to spy on the entire planet – in stark contrast to the Five-Eyes agreement.\" > Comment: The Five Eyes and especially NSA and GCHQ have massive capabilities, but spying on \"the entire planet\" is still rather exaggerated: their collection efforts are limited by national priorities, the locations of where they can access satellite and cable traffic, as well as by technical constraints. While the five members of the European Maximator alliance have/had much smaller capabilities, they could nonetheless intercept and decrypt diplomatic communications from over 60 countries where the weakened encryption devices from Crypto AG were used (see the map below). > See also: Maximator and other European SIGINT alliances The countries that bought and used manipulated Crypto AG devices (graphic: The Washington Post - click to enlarge) - Page 47, note 8: \"Narus mass surveillance and analysis systems were deployed by the NSA inside AT&T facilities to intercept all traffic flowing through their large capacity network cables as documented [KB09] by whistleblower Mark Klein.\" > Comment: This suggests that the NSA is intercepting American communications, but actually this is part of Upstream collection, which is aimed at foreign targets and therefore the NSA applies various filter systems to select traffic from countries of interest and discard purely domestic communications. > See also: FAIRVIEW: Collecting foreign intelligence inside the US - Page 52: \"The Foreign Intelligence Surveillance Court (FISC) is largely considered to rubber stamp requests from the FBI. The FBI has routinely misled the FISC, and from the little that is known, the FISC has neither the technical knowledge, nor the general temperament to actually act as a safeguard\" > Comment: Since the start of the Snowden revelations, numerous Top Secret documents from the FISC have been declassified, showing that the court examines the NSA's activities in great detail. The idea of being a \"rubber stamp\" is based upon the fact that the FISC denies just 0.5% of the applications, but later it became clear that American criminal courts only deny a tiny 0.06% of the requests for regular (so-called Title III) wiretaps. - Page 53: \"The CIA meanwhile, operates their own surveillance capabilities including capabilities that are entirely outside of the purview of the FISC, even now [cia22].\" > Comment: At least one of these cases is about the CIA's use of bulk datasets with financial information, which can of course contain information about Americans, but when the CIA obtained them in ways other than by intercepting communications, the FISC simply has no jurisdiction. It's up to lawmakers to impose privacy safeguards for creating and exchanging such bulk datasets. - Page 56: \"In the Snowden archive, we see lots of hacking and hacking related programs run by NSA, such as the TURBULENCE [Wik21u] program which is made up of modular sub programs [Amb13]. Those programs include TURMOIL [Gal14b], TUTELAGE [AGG+15a], TURBINE [GG14, Wik20d], TRAFFICTHIEF [Wik20c], and XKeyscore [Gre13d, Unk13, AGG+14b, Unk15a] as shown in Figure 4.12 and Figure 4.13, as well as data that was pilfered during those break-ins.\" > Comment: This suggests that TURBULENCE and its sub-programs are about hacking operations, but actually, TURBULENCE is defined as \"a next generation mission environment that created a unified system for MidPoint and Endpoint SIGINT\", or in other words, an overarching framework for bulk and targeted tapping systems. Only the TURBINE sub-program can automatically trigger the implantation of malware into target computer systems. Furthermore, none of the sources mentioned in the thesis say that XKEYSCORE is a sub-program of TURBULANCE and XKEYSCORE is not a hacking tool either. A detailed explanation of the TURBULENCE system is given in an article by Robert Sesek, which was apparently not consulted by Appelbaum. - Page 72: \"US-984XN is the classified SIGAD while the program name PRISM is unclassified\" > Comment: There are no indications that \"PRISM\" is less secret than any other coverterm which the NSA uses for its collection, processing and analysis programs. That was likely also the reason that the big internet companies involved in this program initially denied that they had ever heard of something called PRISM. - Page 91: \"the NSA's Equation Group (EQGRP), which was later renamed Tailored Access Operations (TAO)\" > Comment: The name Equation Group was actually coined in February 2015 by the Russian cybersecurity firm Kaspersky for \"one of the most sophisticated cyber attack groups in the world\". Later on it became clear that this group was part of the NSA's hacking division TAO. Given how many aspects of the NSA's operations Appelbaum mentions in chapter 4 of his thesis, one could say that it's inevitable that some mistakes are made and some sloppiness occurs. On the other hand, however, this is an academic publication for which the highest standards of accuracy should apply. Finally, Appelbaum's activism is illustrated by the back cover of his thesis, which shows a logo very similar to that of the German terrorist organization Rote Armee Fraktion (RAF) from the 1970s, except that the original image of an AK-45 is replaced by that of a computer keyboard: Geplaatst door P/K op 11:08 Email This BlogThis! Share to Twitter Share to Facebook Share to Pinterest 3 comments: Anonymous said... I was already abstaining from new publications on this blog ahahaha, I really like your work, I have a suggestion: could you bring the leaked Snowden documents that most harmed the American intelligence community and that had nothing to do with mass surveillance or another sensationalist way of pushing the leak as a whistleblower, like a top 10 document that helped US adversary intelligence agencies. September 14, 2023 at 8:54 PM Anonymous said... the red star plus keyboard symbol is the logo of the 18th chaos communication congress of the german chaos computer club. September 18, 2023 at 1:47 PM Anonymous said... The gun on the original RAF logo was a mp5, not an AK-45. September 19, 2023 at 8:59 AM Post a Comment Older Post Home Subscribe to: Post Comments (Atom) Welcome to Electrospaces.net! Here you can read about: - Signals Intelligence, - Communications Security, - Top Level Telecommunications, which means the equipment, from past and present, that makes that civilian and military leaders can safely communicate. ► INDEX of all posts on this blog The main focus will be on the United States and its National Security Agency (NSA), but attention will also be paid to other countries, like Germany and the Netherlands. Any comments, additions, corrections, questions or suggestions will be very appreciated! There's no login or registration required for commenting. twitter.com/electrospaces mastodon.social/@Electrospaces electrospaces.bsky.social info (at) electrospaces.net Recent Posts Some new snippets from the Snowden documents On the 10th anniversary of the Snowden revelations New details about the Pentagon Leak Everything you want to know about the Pentagon/Discord Leak The National Security Operations Center (NSOC): 50 years in photos About the legality of the NSA's testing and SIGINT Development projects A new secure red telephone for German chancellor Scholz \"Years of the most fascinating overviews of government security practices, and even-handed reviews of Snowden docs.\" — SwiftOnSecurity Pages Home INDEX Abbreviations and Acronyms NSA Nicknames and Codewords NSA's TAO Division Codewords NSA's organizational designations NSA's Legal Authorities NSA Glossary The US classification system SIGINT Activity Designators (SIGADs) CIA Codewords and Abbreviations GCHQ Codewords and Abbreviations CSE Codewords and Abbreviations BND Codewords and Abbreviations Telephony Abbreviations Internet abbreviations Links Books About Total Pageviews 0 7 1 8 2 9 3 10 4 8 5 7 6 8 7 8 8 97 9 9 10 12 11 16 12 14 13 12 14 11 15 14 16 97 17 22 18 15 19 11 20 11 21 13 22 11 23 13 24 15 25 21 26 13 27 11 28 64 29 100 6,311,309 Popular Posts The US Classification System How Obama's BlackBerry got secured New phones aboard Air Force One INCENSER, or how NSA and GCHQ are tapping internet cables Danish military intelligence uses XKEYSCORE to tap cables in cooperation with the NSA The phones of Ukrainian president Zelensky Trump's \"beautiful\" Oval Office phones and what was changed on them \"It's actually straight up interesting but also weird how weirdly, wonderfully detailed this blog about hyper secure communications is.\" — Gizmodo.com Labels Air Force One (1) Austria (2) BlackBerry (1) BND-Selectors (2) Boeing (1) BoundlessInformant (9) Brazil (1) Britain (1) Canada (1) Classification (13) Club de Berne (1) Cryptography (2) CSEC (2) Cyber (1) Denmark (4) Eikonal (4) ELINT (1) FBI (2) France (3) GCHQ (6) General Dynamics (1) Germany (21) Gold Phone (1) GSM (2) Hotline (9) ISAF (1) Israel (2) IST (4) Kremlin (2) Main Pages (4) Metadata (6) Netherlands (10) New Zealand (1) Non-Snowden-leaks (5) North Korea (2) NSA (46) NSA Partnerships (25) Obama (4) POTUS (18) PRISM (8) Red Phone (6) Russia (3) SatCom (2) Section 215 (4) Sectra (1) Secure voice (8) Situation Room (2) Snowden (6) STE (4) STU-II (1) STU-III (1) Switzerland (2) Trump (7) Ukraine (1) UMTS (2) US (2) USA (4) USSR (2) Vatican (1) VoIP (1) White House (11) Wireless (7) XKeyscore (2) \"I admire your fine-grained work on this subject and read it religiously. Thanks for all the close analysis over these years.\" — Barton Gellman Search This Blog Blog Archive ▼ 2023 (6) ▼ September (1) Some new snippets from the Snowden documents ► June (1) ► May (1) ► April (1) ► March (1) ► January (1) ► 2022 (5) ► 2021 (7) ► 2020 (12) ► 2019 (10) ► 2018 (5) ► 2017 (12) ► 2016 (14) ► 2015 (20) ► 2014 (30) ► 2013 (32) ► 2012 (10) \"Consistently interesting (and strangely, calming/uplifting) content\" — Ryan Lackey US Red Phones Sequence of the real Red Phones, not for the Washington-Moscow Hotline, but for the US Defense Red Switch Network (DRSN). The phones shown here were in use from the early eighties up to the present day and most of them were made by Electrospace Systems Inc. They will be discussed on this weblog later. For the record, you see: - Electrospace MLP-1 - Electrospace MLP-1A (since 1983) - Electrospace MLP-2 - Raytheon IST (since 1992) - Telecore IST-2 (since 2003) US Classification Levels Color codes for the classification levels used by the government and the armed forces of the Unites States: These color codes are used to mark the classification level of (digital) documents and files and also of the communication devices used for their transmission. Subscribe to this weblog! Posts Comments Hotlinks - Electrospaces @ Medium.com - The Dutch virtual Crypto Museum - European intelligence: About Intel - Bruce Schneier on Security - The weblog emptywheel - Weblog of Matthijs R. Koot - Leaked documents: IC Off the record - Der Spiegel's 53 & 36 documents - The Intercept: SIDtoday newsletters - Parsons' SIGINT Summaries - The Snowden Archive - The Investigatory Powers Act 2016 - The Canadian Citizenlab - The Cryptome > Many more links Contact For questions, suggestions and other remarks about this weblog in general or any related issues, please use the following e-mail address: info (at) electrospaces.net (we don't accept paid or guests posts) For sending an encrypted e-mail message, you can use the PGP Public Key under this ID: B4515E04 (fingerprint: ECEC FF63 D036 F415 A0BF A436 661A AC96 B451 5E04) The header photo of this weblog shows the watch floor of the NSA/CSS Threat Operations Center (NTOC) in 2006. The URL of this weblog recalls Electrospace Systems Inc., the company which made most of the top level communications equipment for the US Government. All information on this weblog is obtained from unclassified or publicly available sources. QW5kIGZpbmFsbHksIHRoaXMgaXMgd2hhdCBhIHRleHQgbG9va3MgbGlrZSwgd2hlbiBpdCdzIG9ubHkgZW5jb2RlZCB3aXRoIHRoZSBzdGFuZGFyZCBCYXNlNjQgc3lzdGVtLiBHdWVzcyBob3cgY29tcGxpY2F0ZWQgaXQgbXVzdCBiZSB3aGVuIGEgcmVhbCBzdHJvbmcgYWxnb3JpdGhtIHdhcyB1c2VkLg== Powered by Blogger.",
    "commentLink": "https://news.ycombinator.com/item?id=37562225",
    "commentBody": "Some new snippets from the Snowden documentsHacker NewspastloginSome new snippets from the Snowden documents (electrospaces.net) 214 points by Luc 13 hours ago| hidepastfavorite92 comments neilv 12 hours ago> \"How do they accomplish their goals with project BULLRUN? One way is that United States National Security Agency (NSA) participates in Internet Engineering Task Force (IETF) community protocol standardization meetings with the explicit goal of sabotaging protocol security to enhance NSA surveillance capabilities.\" \"Discussions with insiders confirmed what is claimed in as of yet unpublished classified documents from the Snowden archive and other sources.\" (page 6-7, note 8)There&#x27;s long been stories about meddling in other standards orgs (both to strengthen and to weaken them), but I don&#x27;t recall hearing rumors about sabotage of IETF standards. reply tptacek 8 hours agoparentThese rumors, about IETF in particular, predate the Snowden disclosures.Almost immediately after that happened, a well-known cypherpunk person accused the IETF IPSEC standardization process of subversion, pointing out Phil Rogaway (an extraordinarily well-respected and influential academic cryptographer) trying in vain to get the IETF not to standardize a chained-IV CBC transport (this is a bug class now best known, in TLS, as BEAST), while a chorus of IETF gadflies dunked on him and questioned his credentials. The gadflies ultimately prevailed.The moral of this story, and what makes these \"NSA at IETF\" allegations so insidious, is that the IETF is perfectly capable of subverting its own cryptography without any help from a meddling intelligence agency. This is a common failure of all standards organizations (W3C didn&#x27;t need any help coming up with XML DSIG, which is probably the worst cryptosystem ever devised), but it&#x27;s somewhat amplified in open settings like IETF. reply hulitu 4 hours agorootparent> The moral of this story, and what makes these \"NSA at IETF\" allegations so insidious, is that the IETF is perfectly capable of subverting its own cryptography without any help from a meddling intelligence agencyThe 3 letter agencies usually recrut people from academia and sensitive organizations in order to pursue their agenda. reply tptacek 4 hours agorootparentThis would be a snappier comeback if it wasn&#x27;t the academic cryptographer who was right, and the random people in non-sensitive organizations who shot the right answer down. reply AdamN 3 hours agorootparentprevThat&#x27;s sort of not saying much. Are they going to recruit Joe the Plumber to submit IETF drafts? reply mike_d 5 hours agorootparentprevDNSSEC is a great example of just how poorly committee designed crypto is. The government doesn&#x27;t need to do anything to standards, they can just let them play out as-is.We ended up with a huge mess that solves no actual problems, introduced problems that never existed before, and so confusing in what it does that people still blindly defend it. reply SenAnder 41 minutes agorootparentWhy do you assume the government didn&#x27;t do anything? If anyone on those committees was paid or influenced by the NSA&#x2F;CIA, they would not have disclosed it. reply anonym29 8 hours agorootparentprevI wonder what other institutions are infested with swarms of gadflies that&#x27;ll all swear up and down in unison that something is definitely good or bad, will attack outsiders with narratives that disagree with their own, and weaken the credibility of the entire institution in the process.Surely this can&#x27;t be a phenomenon unique to technology, can it? reply tptacek 8 hours agorootparentIt&#x27;s just human nature. I refuse to participate in standards work, partly because it&#x27;s much more pleasant throwing rocks from a safe distance, but also because I recognize in myself the same ordinary frailties that had friends of the original IPSEC standards authors writing mailing list posts about Rogaway being a \"supposed\" cryptographer. There but for the grace of not joining standards lists go I.I think --- I am not kidding here, I believe this sincerely --- the correct conclusion is community-driven cryptographic standards organizations are a force for evil. Curve25519 is a good standard. So is Noise, so is WireGuard. None of those were developed in a standards organization. It&#x27;s hard to think of a good cryptosystem that was. TLS? It took decades --- bad decades --- to get to 1.3. Of that outcome, I believe it was Watson Ladd who said that if you turned it in as the \"secure transport\" homework in your undergraduate cryptography class, you&#x27;d get a B. reply red_admiral 2 hours agorootparentIt goes further back than crypto: back in the day, there was the design-by-committee OSI versus the \"rough consensus and running code\" IETF. The result is that while we still teach the OSI 7-layer model in universities, in practice we use TCP&#x2F;IP, often with HTTP and TLS on top. reply dboreham 2 hours agorootparentprevTLS originated at Netscape. reply sitkack 8 minutes agorootparentSo did Js, it isn’t a counter argument. Many of the gadflies are useful idiots. reply zdragnar 5 hours agorootparentprevYou&#x27;re basically describing politics, where specific opinions are central to a group identity. So long as there is an innate human desire to belong, there will be plenty of people who are happy to live with any amount of cognitive dissonance.Joining a gadfly swarm just gives them an opportunity to prove their worth to the group. reply happytiger 8 hours agorootparentprevhttps:&#x2F;&#x2F;youtu.be&#x2F;URdXC6UtfVg?si=b7uwjHujUvYG1hGHSkip to 1 minute in and you’ll see the problem in experimental form. reply jdougan 12 hours agoparentprevI&#x27;m curious as to how successful they were at subverting the IETF process. It wouldn&#x27;t be impossible, but since much of the process is in the open it could be difficult, especially if they did it under their own name.I suspect most of it was done under different corporate identities, and probaby just managed to slow adoption of systematic security architectures. Of course, once the Snowden papers came out, all that effort was rendered moot as the IETF reacted pretty hard. reply gustavus 11 hours agorootparentYa ever heard of the OAuth2 protocol? I spent almost half a decade working on identity stuff, and spending a lot of time in OAuth land. OAuth is an overly complicated mess that has many many ways to go wrong and very few ways to go right.If you told me the NSA&#x2F;CIA had purposefully sabotaged the development of the OAuth2 protocol to make it so complex that no one can implement it securely it&#x27;d be the best explanation I&#x27;ve heard yet about why it is the monstrosity it is. reply bawolff 11 hours agorootparent> Ya ever heard of the OAuth2 protocol?Have you ever seen SAML? Now there is a protocol that seems borderline sabotaged. CSRF tokens? Optional part of spec. Which part of the response is signed? Up to you with different implementations making different choices; but better verify the sig covers the relavent part of the doc. Can you change the signed part of spec in a way that alters the xml parse tree without it invalidating the signature? Of course you can!Oauth2 is downright sane in comparison.[To be clear, saml is not a ietf spec, it just solves a similar problem as oauth2] reply johnmaguire 10 hours agorootparentHonestly, as someone who has implemented both SAML and OAuth2 (+ OIDC) providers, I found SAML much easier to understand. Yes, there are dangerous exploits around XML. Yes, the spec is HUGE. But practically speaking, people only implement a few parts of it, and those parts were, IMHO, easier to understand and reason about.This is definitely an unpopular opinion however. reply tptacek 8 hours agorootparentWhat&#x27;s insidious about SAML is that it really is mostly straightforward to understand, but it&#x27;s built on a foundation of sand, bone dust, and ash; it works --- mostly, modulo footguns like audience restrictions --- if you assume XML signature validation is reliable. But XML signature validation is deeply cursed, and is so complicated that most fielded SAML implementations are wrapping libxmlsec, a gnarly C codebase nobody reads. reply johnmaguire 7 hours agorootparent100% - XML vulnerabilities are the biggest issue. JWTs have also had their fair share, though I think they were mostly implementation bugs that have mostly been ironed out at this point. XML&#x27;s complexity is inherent to the language. reply thaumasiotes 7 hours agorootparent> JWTs have also had their fair share, though I think they were mostly implementation bugs that have mostly been ironed out at this point.The most famous JWT issue, to my mind, was people implementing JWT and -- as per spec -- accepting an encryption mode of \"none\" as valid.That could be described as an \"implementation bug\", but it can also be described as \"not an implementation bug\" - all your JWT functionality is working the way it&#x27;s supposed to work, it&#x27;s just not doing the thing that you hoped it would do. reply amluto 6 hours agorootparentIMO this is a defect (sabotage? plain incompetence?) in the specs, full stop.RFC 7519 (the JWT spec) delegates all signature &#x2F; authentication validation to the JWS and JWE specs. (And says that an unholy mechanism shall be used to determine whether to follow the JWS validation algorithm or the JWE algorithm.)JWS even discusses algorithm verification (https:&#x2F;&#x2F;www.rfc-editor.org&#x2F;rfc&#x2F;rfc7515.html#section-10.6), but does not suggest, let alone require, the absolutely mindbendingly obvious way to do it: when you have a key used for verificaiton, the algorithm specification is part of the key. If I tell a library to verify that a given untrusted input is signed&#x2F;authenticated by a key, the JWS design is: bool is_it_valid(string message, string key); &#x2F;&#x2F; where key is an HMAC secret or whateverand this is wrong. You do it like this: bool is_it_valid(string message, VerificationKey key);where VerificationKey is an algorithm and the key. If you say to verify with an HMAC-SHA256 key and the actual message was signed with none or with HMAC-SHA384 or anything else, it is invalid. If you have a database and you know your key bits but not what cryptosystem those bits belong to, your database is wrong.The JWE spec is not obviously better. I wouldn&#x27;t be utterly shocked if it could be attacked by sending a message to someone who intends to use, say, a specific Ed25519 public key, but setting the algorithm to RSA and treating the Ed25519 public key as a comically short RSA key. reply bawolff 4 hours agorootparentprev> The most famous JWT issue, to my mind, was people implementing JWT and -- as per spec -- accepting an encryption mode of \"none\" as valid.Sure that is pretty silly. However in saml you have xmlsec accepting the non standard extension where you can have it \"signed\" with hmac where the hmac key is specified inside the attacker controlled document. I would call that basically the same as alg=none, although at least its non-standard. reply johnmaguire 6 hours agorootparentprevI did say \"mostly.\" :) I almost mentioned this bug, but it&#x27;s so famous, I doubt any mainstream OIDC libraries fall prey to it these days. reply psd1 12 minutes agorootparentI&#x27;m a massive Aliens fan, and your word choice gave me a vivid picture of a vulnerability as a xenomorph egg leads to a swarm of nightmarish monsters invading the colony.I don&#x27;t know which species is worse. You don&#x27;t see them ignoring CVEs for a percentage.TrapLord_Rhodo 9 hours agorootparentprevNSA? reply ENGNR 11 hours agorootparentprevRedirecting the user when they tap sign in from untrustednewsite.com, to a new window with the domain hidden of bigsitewithallyourdata.com and saying “Yeah, give us your login credentials” always felt like the craziest thing to meSo ripe for man in the middle attacks. Even if you just did a straight modal and said “put your google credentials into these fields”, we’re training people that that’s totally fine reply djbusby 9 hours agorootparentLike how Plaid for banking works. reply tptacek 8 hours agorootparentprevOAuth2 is about the simplest thing you could come up to solve the delegated authentication problem. It&#x27;s complexity stems mostly from the environment it operates in: it&#x27;s an authentication protocol that must run on top of unmodified oblivious browsers.There&#x27;s a lot of random additional complexity floating around it, but that complexity tracks changes in how applications are deployed, to mobile applications and standalone front-end browser applications that can&#x27;t hold \"app secrets\".The whole system is very convoluted, I&#x27;m not saying you&#x27;re wrong to observe that, but I&#x27;d argue that&#x27;s because apps have a convoluted history. The initial idea of the OAuth2 code flow is pretty simple. reply barsonme 7 hours agorootparentSounds like you’re an NSA plant, Thomas :) reply jdougan 11 hours agorootparentprevThe problem is, as anyone with sufficient experience knows, it is perfectly possible and common for devs to design security disasters without any involvement by the spooks. I suspect the NSA counts on this and uses any covert influence they might have to slow corrections (eg. Profiles for OAuth2 that actually are reasonable, patches to common services, etc.) reply eddythompson80 11 hours agorootparentprevsabotaging a design is remarkably easy. We have several individuals that almost do it effortlessly. It&#x27;s almost a talent for some. I suspect that doing it maliciously while hiding behind some odd corner scenario or some compatibility requirements can&#x27;t be that hard and will be almost impossible to prove or detect. reply jdougan 10 hours agorootparentOn the other hand, why would the NSA even bother? With these talented individuals providing them a never ending supply of security issues? And they don&#x27;t even have to pay for them besides having their normal 0-day discovery team look for them. I think supporting this is their reaction when a 0-day they are relying on gets patched out.I can imagine special cases where they would exert resources and effort, such as the IETF or some other economically efficient chokepoint (Rust?), but not in general. reply psd1 10 minutes agorootparentAs a senior professional in any field, you would, if you could, move from a reactive strategy to a proactive one. reply willis936 11 hours agorootparentprevJust because you&#x27;re paranoid doesn&#x27;t mean they&#x27;re not after you. reply esafak 11 hours agorootparentprevIs there any write-up on the IETF reaction? reply jdougan 11 hours agorootparentThere were a bunch at the time, a historical retrospective is \"RFC 9446 Reflections on Ten Years Past the Snowden Revelations\"https:&#x2F;&#x2F;www.rfc-editor.org&#x2F;rfc&#x2F;rfc9446.html reply ok123456 11 hours agorootparentprevSo that&#x27;s how we ended up with IPv6. reply mighmi 2 hours agorootparentWhat&#x27;s wrong with it? reply fidotron 11 hours agoparentprevJust a concrete example of that time we know the NSA actually did their job properly: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Data_Encryption_Standard#NSA&#x27;s... reply tetris11 5 hours agorootparentFantastic and damming read. I had no idea they were so active and nefarious even in the 70s reply tptacek 5 hours agorootparentAre you sure you read it carefully? NSA&#x27;s role in DES was to make it resistant to linear and differential cryptanalysis. reply adrian_b 2 hours agorootparentTrue, but they have also forced the reduction of the key size from 64 bits to 56 bits, making it much more vulnerable to brute force.So they have hardened DES towards attacks from those with much less money than NSA, while making it weaker against rich organizations like NSA. reply tetris11 2 hours agorootparentprevThey made it resistant to differential attacks yes, but weakened it for their own attack surface reply EvanAnderson 11 hours agoparentprevI&#x27;m too young to have firsthand experience, but I&#x27;ve seen the speculation that IPSEC was an example of this kind of strategy. It&#x27;s certainly more-- ahem-- flexible that it probably ought to be. I know there have been exploits against ISAKMP implementations. I&#x27;d assume the baroque nature of the protocol drove some of that vulnerability. reply hinkley 11 hours agoparentprevNot IETF, but NIST, which I suspect is worse. Dual_EC_DRBG was withdrawn when it was discovered to be an attempt by the NSA to sabotage ECC specifications. reply willis936 11 hours agorootparentNIST EC DSA curves are the only ones used by CAs, are manipulatable, and have no explanation for their origin. Pretty much the entire HTTPS web is likely an open book to the NSA. reply tptacek 8 hours agorootparentI don&#x27;t know what you mean by \"manipulatable\". I don&#x27;t know what you mean by \"DSA\". I assume what you&#x27;re doing is casting aspersions on the NIST P-curves. They&#x27;re the most widely used curves on the Internet (hopefully not for too long).I don&#x27;t think all that many serious people believe there&#x27;s anything malign about them. It&#x27;s easy to dunk on them, because they&#x27;re based on \"random seeds\" generated inside NIST or NSA. As a \"nothing up our sleeves\" measure, NSA generated these seeds and then passed them through SHA1, thus ostensibly destroying any structure in the seed before using it to generate coefficients for the curve.The ostensible \"backdoor attack\" here is that NSA could have used its massive computation resources (bear in mind this happened in the 1990s) to conduct a search for seeds that hashed to some kind of insecure curve. The problem with this logic is that unless the secret curve weakness is very common, the attack is implausible. It&#x27;s not that academic researchers automatically would have found it, but rather that NSA wouldn&#x27;t be able to count on them not finding it.Search for [koblitz menezes enigma curve] for a paper that goes into the history of this, and makes that argument (I just shoplifted it from the paper). If you don&#x27;t know who Neal Koblitz and Alfred Menezes are, we&#x27;re not speaking the same language anyways.The real subtext to this \"P-curves are corrupted\" claim is that there are curves everyone drastically prefers, most especially Curve25519 (the second-most popular curve on the Internet). Modern curves have nice properties, like (mostly) not requiring point validation for security (any 32-byte string is a secure key, which is decidedly not the case for ordinary short Weierstrass curves, and being easy to implement without timing attacks.The author of Curve25519 doesn&#x27;t trust the NIST curves. That can mean something to you, but it&#x27;s worth pointing out that author doesn&#x27;t trust any other curves, either. Cryptographers have proposed \"nothing up my sleeves\" curves, whose coefficients are drawn from mathematical constants (like pi and e). Bernstein famously co-authored a paper that attempted to demonstrate that you could generate an \"untrustworthy\" curve by searching through permutations of NUMS parameters. It was fun stunt cryptography, but if you&#x27;re looking for parsimony and peer consensus on these issues, you&#x27;re probably better off with Menezes.Incidentally, you said \"ECDSA curve\". ECDSA is an algorithm, not a curve. But nobody likes ECDSA, which was also designed by the NSA. A very similar situation plays out there --- Curve25519&#x27;s author also invented Ed25519, a Schnorr-like signing scheme that resolves a variety of problems with ECDSA. Few people claim ECDSA is enemy action, though; we all just sort of understand that everyone had a lot to learn back in 1997. reply mschuster91 11 hours agorootparentprevForged certificates would immediately show up in Certificate Transparency logs. reply SV_BubbleTime 9 hours agorootparentHe isn’t saying the CA is bad. He is saying the curves selected are arbitrary and they stuck to specific ones with no reason. That the NSA has a backdoor to at least some TLS EC algorithms.Now, IMO I thought the whole point was that the curve itself didn’t really matter. As you are just picking X Y points on it and doing some work from there. But if there is a flaw, and it required specific curves to work, well there you go. reply jdougan 11 hours agorootparentprevThe NIST process (especially then) isn&#x27;t fully open, which makes it easier to subvert with an inside agent. reply charcircuit 9 hours agorootparentprevThere is no evidence that it was backdoored. reply dmix 4 hours agoprevArticle references Russias SORM system which provides not only FSB but the police and tax agencies with basically fully access to everything on the internet including credit card transactions, this stuff started in 1995 and was penetrated by the NSA> Under SORM‑2, Russian Internet service providers (ISPs) must install a special device on their servers to allow the FSB to track all credit card transactions, email messages and web use. The device must be installed at the ISP&#x27;s expense.originally there was a warrant system but it seemed quite liberal and they don’t bother with the secret court system “oversight” like the US:> Since 2010, intelligence officers can wiretap someone&#x27;s phones or monitor their Internet activity based on received reports that an individual is preparing to commit a crime. They do not have to back up those allegations with formal criminal charges against the suspect. According to a 2011 ruling, intelligence officers have the right to conduct surveillance of anyone who they claim is preparing to call for \"extremist activity.\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;SORM?wprov=sfti1Then in 2016 a counter terrorism law was passed and it sounds like they ISPs&#x2F;telecoms are required to store everything for 6 months and it merely has to be requested by “authorities” (guessing beyond just the FSB) without a court order> Internet and telecom companies are required to disclose these communications and metadata, as well as \"all other information necessary\" to authorities on request and without a court orderhttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Yarovaya_law?wprov=sfti1> Equally troubling, the new counterterrorism law also requires Internet companies to provide to security authorities “information necessary for decoding” electronic messages if they encode messages or allow their users to employ “additional coding.” Since a substantial proportion of Internet traffic is “coded” in some form, this provision will affect a broad range of online activity. reply SenAnder 30 minutes agoparent> Then in 2016 a counter terrorism law was passed and it sounds like they ISPs&#x2F;telecoms are required to store everything for 6 months and it merely has to be requested by “authorities” (guessing beyond just the FSB) without a court orderRussia is not alone. The EU required the same thing from 2006, until the EU Court struck it down in 2014: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Data_Retention_Directivehttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Data_retention#European_Union is a fun read in how the EU fined countries for adhering to their respective national constitutions and refusing to implement that directive.Looking beyond the EU, there are plenty of allegedly democratic countries on that wiki page with legally-required data retention:In 2015, the Australian government introduced mandatory data retention laws that allows data to be retained up to two years. [..] It requires telecommunication providers and ISPs to retain telephony, Internet and email metadata for two years, accessible without a warrant reply justusw 9 hours agoprevSo if governments are sniffing on high entropy traffic, could we just send normal seeming (SSH or whatever) packets with the payload coming from &#x2F;dev&#x2F;urandom? Would that be a denial of service? reply agilob 3 hours agoparentCreate 1000s of files like this:dd if=&#x2F;dev&#x2F;urandom of=encrypted1.zip bs=4M count=1and upload it to dropbox and google driveNSA will archive it for you expecting to decrypt the content later in the future when a software bug is found or hardware is more capable. reply janandonly 1 hour agorootparentAnd see the national debt and tax burden rise even more.Who is really winning if anybody does this? reply agilob 1 hour agorootparentHard drive manufacturers I have stocks of? reply lmm 7 hours agoparentprevIf you could get enough people doing it, yes. But in practice the only people who would care enough would be the people the governments want to watch. Even a decent chunk of the crypto community would rather dunk on a cryptosystem they don&#x27;t like than actually encrypt their emails (although of course how much of that is the NSA disrupting things is an open question). reply elcritch 6 hours agorootparentI actually sorta think unencrypted email has been a boon for society as both corporations and government agencies have left paper trails that helped expose their misdeeds later. reply BlueTemplar 3 hours agorootparentprevI thought there were wildly popular messaging apps now that were encrypted by default ? reply __MatrixMan__ 8 hours agoparentprevWhen they find a way to \"decrypt\" &#x2F;dev&#x2F;urandom output into, you know, whatever is expedient for them at some future date, do you think a secret judge in a secret court is going to believe that you were just moving around random noise for the lulz? reply ta988 6 hours agorootparentYour random string at position 483828180 says IdIdIt that&#x27;s our proof right here reply shmde 11 hours agoprevMy conspiracy theory is that AES 256 has been cracked by NSA&#x2F;CIA but they just shut up about it so everyone feels safe. reply sandworm101 10 hours agoparentIf AES is cracked by the NSA then they know that there is a fault. They must therefore assume that others could know about it and might exploit that fault, either now or in the future. A massive amount of American infrastructure, including intelligence services, relies upon AES not having such holes. They wouldn&#x27;t sit on such information. reply mike_d 5 hours agorootparent> including intelligence services, relies upon AES not having such holes. They wouldn&#x27;t sit on such information.The NSA uses a completely different set of algorithms called Suite A. They don&#x27;t use AES for exactly this reason. reply The_Colonel 2 hours agorootparent> Suite A will be used for the protection of some categories of especially sensitive information (a small percentage of the overall national security-related information assurance market)From https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;NSA_Suite_A_Cryptography reply TrapLord_Rhodo 9 hours agorootparentprevThe government is known to horde and deploy zero days for surveillance... That was part of the snowden leak. google&#x2F;Ask chatgpt about Tailored Access Operations revealed under the snowden papers. reply 4bpp 9 hours agorootparentprevIf the flaw in AES is subtle enough, they may be (justifiedly?) convinced that nobody else&#x27;s capabilities will suffice to discover and&#x2F;or exploit it - or, at least, that they have enough of a grasp of what everyone else is doing that no other actor could discover the flaw without the NSA finding out about this. reply c0pium 7 hours agorootparentEncryption bugs are fundamentally different than other exploits; if I send traffic protected by encryption then it needs to remain encrypted as long as the information needs to be secret. With national security that can be decades. reply jongjong 8 hours agorootparentprevIt seems reasonable that they would set a self-imposed expiry date on exploits then get them patched quietly. reply AnimalMuppet 9 hours agorootparentprevThey wouldn&#x27;t go public with such information. They would very quietly get the most important parts moved off of it, quickly, with as little noise as possible. reply sandworm101 9 hours agorootparentMoving any part of the US government off of AES would be noticed. The NSA doesn&#x27;t send IT people to do installs. They set standards which are then incorporated into products by outside contractors selling to government agencies. Any attempted migration away from AES would be news here on HN within hours. reply c0pium 7 hours agorootparentAs an example of one such standard which was recently updated and still includes AES256, https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Commercial_National_Security... reply tptacek 9 hours agorootparentprevThat’s not the point: if there’s a grave flaw in AES, other countries can find it too. reply lmm 7 hours agoparentprevIf they can find a flaw in AES 256, others can too.My theory is they&#x27;ve got a backdoor in the iPhone hardware random number generator. It&#x27;s the most obvious high-value target, it&#x27;s inherently almost undetectable (the output of a CSPRNG is indistinguishable from real random numbers), and you can keep crypto researchers busy with fights about whose cryptosystem is best, safe in the knowledge that it doesn&#x27;t matter what they come up with, they were owned from the start. reply bmc7505 4 hours agorootparentThey would be foolish not to at least try, either by intercepting entropy-supplying syscalls or compromising the hardware directly. RNG attacks are easy to design and nearly impossible to detect.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Yao%27s_test reply slackfan 11 hours agoparentprevTry to implement out-of-spec authentication and watch your logs fill up with bots. reply sunk1st 10 hours agorootparentWhat’s the implication? Sorry. I can see it going either way. reply xhoptre 12 hours agoprevnext [10 more] [flagged] drunner 12 hours agoparentWhat an awful take. reply umeshunni 8 hours agorootparentwhat was the take? reply lcnPylGDnU4H9OF 8 hours agorootparentThere is an option in your profile called “showdead”. You should be able to read the comment after updating that option to be affirmative. reply ementally 7 hours agorootparentShould have just kept it off. reply lcnPylGDnU4H9OF 6 hours agorootparentI love reading the controversial things. Admittedly, it’s very commonly low quality like OP in this thread but sometimes you get good if unpopular arguments. The challenge to my worldview is a utility that’s difficult to replace. replyKrasnol 12 hours agoparentprevThis is a genuine question, I am curious as to what drives men such as you to such comments. reply didntcheck 1 hour agorootparentIt doesn&#x27;t take much outlandish theorizing to wonder who might be interested in character assassination of people challenging surveillance reply azinman2 12 hours agoparentprevI doubt it’s connected but would be fascinating if true. reply johnnyworker 12 hours agorootparentBecause they&#x27;re so bad it would need a global system of total surveillance to catch them? Sure.https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=11872642^ that is what that is. Or, in more detail: https:&#x2F;&#x2F;github.com&#x2F;Enegnei&#x2F;JacobAppelbaumLeavesTor&#x2F;blob&#x2F;mast...You can&#x27;t connect real things like these documents with slander by people who do nothing to step on the toes of the NSA. That is all that the BS about Assange or Appelbaum being a sex menace or Snowden being a Russian asset is. \"Oh noes, they&#x27;re a threat to the work we&#x27;re not doing\". Nobody is asking you to get drinks with Assange or Appelbaum. They don&#x27;t want to be your friend. It&#x27;s okay if you don&#x27;t like them, for whatever personal reasons (and choosing to fall for this crap falls under personal reasons). It&#x27;s not okay to be part of a mob that murders people by throwing a pebble each with this plausible deniability, in this \"genuinely curious\" just wondering kind of way. Enough is enough.It certainly isn&#x27;t fascinating. 3 letter agencies are torturing and murdering people, and having nothing better to do than gossip about gossip about messengers is just vulgar, boring, infantile cowardice, puffed up with not even clever words. reply ftyers 11 hours agoprev [–] Wow djb was on his committee, cool. reply tptacek 8 hours agoparentDJB is widely thought to be the entire reason he was in the program at all. reply SV_BubbleTime 9 hours agoparentprev>https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Daniel_J._BernsteinI didn’t know who this was, others probably too. reply dontdoxxme 10 hours agoparentprev [–] Maybe not cool. There’s discussion in many places, but see https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=13891900 for background. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article outlines Jacob Appelbaum's PhD thesis, unveiling insights from the Snowden documents about NSA's protocol security sabotage and interference with lawful interception systems.",
      "The article corrects misconceptions presented in the thesis concerning NSA operations and surveillance, urging for more precise standards in academic publications.",
      "It also provides details about the US Defense Red Switch Network and enlightens on the color codes used by the US government and armed forces, pulling data from unclassified or publicly available sources."
    ],
    "commentSummary": [
      "The summary discusses several topics including government surveillance, weaknesses in encryption algorithms, and potential manipulation of security protocols, with explicit mention of the NSA's probable tampering with protocol security and cryptographic standards.",
      "It delves into the complex issues and deficiencies associated with XML signature validation and JWTs (JSON Web Tokens), bringing up various viewpoints on encryption and government surveillance.",
      "It additionally deals with speculations regarding NSA's capabilities and possible backdoors, and lastly, brings attention to some controversies and uncertainties tied with a person named DJB on a certain platform."
    ],
    "points": 207,
    "commentCount": 89,
    "retryCount": 0,
    "time": 1695072018
  },
  {
    "id": 37559256,
    "title": "Paint on Windows is getting layers and transparency support",
    "originLink": "https://blogs.windows.com/windows-insider/2023/09/18/paint-app-update-adding-support-for-layers-and-transparency-begins-rolling-out-to-windows-insiders/",
    "originBody": "Skip to main content Microsoft Windows Blogs Windows Experience Devices Windows Developer Microsoft Edge Windows Insider Microsoft 365 All Microsoft Search Paint app update adding support for layers and transparency begins rolling out to Windows Insiders Written By Dave Grochocki published September 18, 2023 Hello Windows Insiders, Today we are beginning to roll out an update for the Paint app to Windows Insiders in the Canary and Dev Channels (version 11.2308.18.0 or higher). With this update, we are introducing support for layers and transparency! You can now add, remove, and manage layers on the canvas to create richer and more complex digital art. With layers, you can stack shapes, text, and other image elements on top of each other. To get started, click on the new Layers button in the toolbar, which will open a panel on the side of the canvas. This is where you can add new layers to the canvas. Try changing the order of layers in this panel to see how the order of stacked image elements on the canvas changes. You can also show or hide and duplicate individual layers or merge layers together. Paint composition of a cat utilizing multiple layers. We are adding support for transparency as well, including the ability to open and save transparent PNGs! When working with a single layer, you will notice a checkerboard pattern on the canvas indicating the portions of the image that are transparent. Erasing any content from the canvas now truly erases the content instead of painting the area white. When working with multiple layers, if you erase content on one layer, you will reveal the content in layers underneath. Short animation showing the background getting removed in Paint. When you combine layers, transparency, and other tools in Paint, you can create exciting new images and artwork! For example, when combined with the new background removal feature, you can quickly create interesting layered compositions. FEEDBACK: Please file feedback in Feedback Hub (WIN + F) under Apps > Paint. [PLEASE NOTE: We are beginning to roll these experiences out, so they may not be available to all Insiders in the Canary and Dev Channels just yet as we plan to monitor feedback and see how it lands before pushing it out to everyone.] We love getting feedback from the community and are looking forward to your feedback on these updates! Thanks, Dave Grochocki, Principal Product Manager Lead – Windows Inbox Apps Tags: Paint Windows Insider Program What's new Surface Pro 9 Surface Laptop 5 Surface Studio 2+ Surface Laptop Go 2 Surface Laptop Studio Surface Go 3 Microsoft 365 Windows 11 apps Microsoft Store Account profile Download Center Microsoft Store support Returns Order tracking Certified Refurbished Microsoft Store Promise Flexible Payments Education Microsoft in education Devices for education Microsoft Teams for Education Microsoft 365 Education How to buy for your school Educator training and development Deals for students and parents Azure for students Business Microsoft Cloud Microsoft Security Dynamics 365 Microsoft 365 Microsoft Power Platform Microsoft Teams Microsoft Industry Small Business Developer & IT Azure Developer Center Documentation Microsoft Learn Microsoft Tech Community Azure Marketplace AppSource Visual Studio Company Careers About Microsoft Company news Privacy at Microsoft Investors Diversity and inclusion Accessibility Sustainability Your Privacy Choices Sitemap Contact Microsoft Privacy Terms of use Trademarks Safety & eco Recycling About our ads © Microsoft 2023",
    "commentLink": "https://news.ycombinator.com/item?id=37559256",
    "commentBody": "Paint on Windows is getting layers and transparency supportHacker NewspastloginPaint on Windows is getting layers and transparency support (windows.com) 206 points by michelangelo 16 hours ago| hidepastfavorite142 comments hackermatic 16 hours agoPaint&#x27;s journey has been so weird. It was going to be replaced by Paint 3D, made an optional component, and now it&#x27;s getting updates I would have killed to have 20 years ago. But it&#x27;s been such a useful and enduring tool because of both its ubiquity and its simplicity, so I hope the upcoming changes will be as minimalist as those of the past. reply 0cf8612b2e1e 16 hours agoparentConsidering how slow the new calculator app is, I am terrified of Microsoft updating the classics. These things used to open instantly and provide value. Now it is a visible pause and gigantic white space heavy interfaces. reply MenhirMike 16 hours agorootparentThe fact that the calculator has a $#!\"@ hamburger menu to switch between Programmer and \"can do floating point numbers\" mode is one of those asinine UX decisions that makes me fear any app update - \"Oh hey, they&#x27;re adding a neat feature, let&#x27;s see how they mess up UX on it\" is my default reaction nowadays.And yes, Notepad and Calculator take much longer to start now than they ever did - not sure if it&#x27;s SmartScreen, a Store Update check, or if SEND_DIAGNOSTICS is enabled on production builds, but I bet it&#x27;s some Program Manager making some idiotic decision because some useless KPI requires it. reply MarkSweep 15 hours agorootparentThe really damning UI feature of calculator, at least during the beginning of Windows 10, was the splash screen (aka loading screen). I understand that’s a standard feature of UWP apps, but the fact it was on screen long enough to be noticed was unacceptable. reply conductr 15 hours agorootparentprev> \"Oh hey, they&#x27;re adding a neat feature, let&#x27;s see how they mess up UX on it\"It still feels like they just break UX to support touch devices better than older UI would. I might just be showing my age, or out of touch, or perhaps because I only use Windows for work but I have no intention of ever using touch on a device running Windows. My laptop screen support touch and I turn it off as it&#x27;s and unexpected behavior to me (if I touch my screen I&#x27;m probably just pointing at something&#x2F;showing to another person). reply bluSCALE4 15 hours agorootparentprevAfter I found out that in GTA5, bad code loading a large JSON file made boot time jump from 2m to 6m, up to 15m. The JSON file was for in-game purchases, aka useless. reply zerocrates 15 hours agorootparentThough I believe that was a delay specifically for GTA Online only, and those purchases are absolutely the key and only point of the online mode for Rockstar.Of course that doesn&#x27;t mean it should have been loading so inefficiently regardless. reply frenchy 15 hours agorootparentThe next verison of GTA should be a game where you get a gig with a corporation called Rockstar, and all your jobs are focused around making them money. reply grishka 14 hours agorootparentprev> a $#!\"@ hamburger menuTouchscreen-first design does this to apps ¯\\_(ツ)_&#x2F;¯ reply MenhirMike 13 hours agorootparentThere would be plenty of space for humongous buttons to touch and only use the hamburger when the app is really small. But yeah, it&#x27;s not touchscreen, it&#x27;s that everyone designs exclusively for 6\" screens these days, which also explains why 60% of modern apps is just useless whitespace :&#x2F; reply grishka 12 hours agorootparent> everyone designs exclusively for 6\" screens these daysTechnically incorrect, I don&#x27;t ;) reply MenhirMike 10 hours agorootparentI wish there were more people that also don&#x27;t and still design for desktop screen sizes, but you seem to be a rarer and rarer breed :&#x2F; reply 3seashells 15 hours agorootparentprevCalculator is slow calculator now, for calculated promotion points. reply MikusR 4 hours agorootparentprev$#!\"@ hamburger and ordinary menu take the same amount of clicks to switch modes. The one from Windows 7 and the one from windows 10 take about the same time to start. The last fast starting one was in Windows XP. reply bee_rider 15 hours agorootparentprevWait, what is the difference between “programmer mode” and “can do floating point numbers” mode? The former implies the latter, right? reply godd2 15 hours agorootparentYou cannot enter fractional values in programmer mode.This is reasonable, but what is incredibly annoying is that when you type in a number while in Programmer mode, if you then change to Scientific mode, it blows away the number you typed in. reply MenhirMike 15 hours agorootparentprevProgrammer mode is useful if you need to convert Decimal&#x2F;Hex&#x2F;Binary, but if you try to enter e.g, 17.95 as a decimal number, you can&#x27;t. (In fact, it&#x27;ll display 17 NAND 95 - so the dot seems to be a shortcut for a bitwise operation) - it&#x27;s Integers Only.(I guess one could argue that programmer mode should include IEEE-754 floats, but I&#x27;m fine with that mode being limited to integers, it&#x27;s plenty useful already. But it does require switching to another mode if you need a calculator to calculate floating point numbers) reply gspencley 15 hours agorootparentprevBut how are you supposed to sync your calculations to One Drive and Teams if it&#x27;s just a simple, non-network-enabled calculator program? reply 0cf8612b2e1e 15 hours agorootparentI am now incapable of identifying satire. reply MikusR 4 hours agorootparentprevWindows calculator doesn&#x27;t do that. reply k12sosse 14 hours agorootparentprevUse PowerShell in Azure as your calculator - duh! ;-)shell.azure.com reply RajT88 15 hours agorootparentprevWindows 11 snipping tool is unnecessarily clunky and garbage as well.What I do on any Win11 machine is keep an archived copy of Windows 10&#x27;s System32 folder, and pin the old copies of apps to my start menu....From my cold dead hands! reply gibolt 15 hours agorootparentYeah, Mac solved this so elegantly.Windows really needs a simple default hotkey to do a partial screen capture quickly, and save it to a reasonable directory. reply adrenvi 15 hours agorootparentWin + Shift + S allows you to select a rectangular region and copy the image data to clipboard. reply secondcoming 14 hours agorootparentYes. This changed my life. I use it every day. reply zadjii 15 hours agorootparentprevI mean, win+shift+s (the default snipping tool hotkey) to grab a snip is pretty elegant IMO. Copies it straight to the clipboard, or lets you save (& edit) it from the notification. Probably one of my most used shortcuts. reply Macha 10 hours agorootparentIndeed, on Mac I normally end up swapping the cmd-shift-4 (screenshots to file, can open in preview instead) with cmd-shift-option-4 (screenshots to clipboard) as the latter is basically what I always want. reply hulitu 13 hours agorootparentprevPrintscreen to capture the whole screen. Alt + Printscreen to capture the current active window. reply koolant 15 hours agorootparentprevWin + Shift + S has been there for ages now. reply fortran77 14 hours agorootparentprevHave you ever used Windows? reply benbristow 14 hours agorootparentprevIs it? Just opened it on Windows 10 and it opens instantly and uses 23MB of RAM which is rather low for a modern application. Calculator is also fully open source now.https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;calculator reply Piisamirotta 15 hours agorootparentprevAh. I really hate Windows calculator nowadays. When looking for alternatives I found SpeedCrunch (https:&#x2F;&#x2F;heldercorreia.bitbucket.io&#x2F;speedcrunch&#x2F;). So far I have loved it, even did some regedit to open it from keyboard&#x27;s calculator button. reply yndoendo 14 hours agorootparentQalculate is my go-to for cross platform calculator that is useful and is not limited to the most basic +-*&#x2F; operations. https:&#x2F;&#x2F;qalculate.github.io&#x2F; reply FirmwareBurner 15 hours agorootparentprev>Considering how slow the new calculator app isWhat&#x27;s bad about it? For me it always does the job whenever I fire it up. Looks sleek, clean, scales nicely to whatever size you make it, and isn&#x27;t distracting. reply godd2 15 hours agorootparent> scales nicely to whatever size you make itIn Programmer mode if you type in a number larger than 32 bits and the window is scaled to its smallest size, the lower row of binary is cut off a little bit. reply FirmwareBurner 15 hours agorootparentHorrible. Unusable. reply sccxy 15 hours agorootparentprevI also like new calculator app in windows.TBH date calculator is my most used feature there.But calculator app in macOS... Real horror.Try to open units converter from top app menus. reply hulitu 13 hours agorootparentprevYou worked all life as a cashier with a hand calculator 15 x 15 cm ( 5 x 5 in).One day your boss comes: your calculator is obsolete, you need to use the one on the wall with every number being 25 x 25 cm (10 x 10 in). Good luck reply FirmwareBurner 12 hours agorootparentYour PC keyboard didn&#x27;t change though. The numbers and arithmetic symbols are still in the same places they&#x27;ve been for ~80 years.Also, the new calculator kept the same standard layout as before, it&#x27;s not like they now shuffled the numbers and buttons aground randomly to confuse you. If you&#x27;re that easily confused by a coat of paint, maybe you&#x27;re in the wrong job.And yes, workers did have to switch calculators from time to time. Source: my mom, an accountant.I get it, \"Microsoft bad, Windows bad\", but your argument is a very weak nothingburger. reply gibolt 15 hours agorootparentprevHow many adults are being distracted by the Windows calculator app? reply FirmwareBurner 15 hours agorootparentThat&#x27;s asking the right questions. reply rkagerer 15 hours agorootparentprevWhen I installed Windows 7, I copied the old mspaint executable over and kept using it, as I hate the stupid Office-style ribbon they implemented in the new one (excessive waste of screen real estate, takes more clicks, and the items collapse to uselessness when the window gets too small). reply ToDougie 14 hours agorootparentprevI&#x27;m blown away at how slow the calculator is. Used to be that every time I went to open the calculator app from Run, it would freeze my whole system. I have 64G of RAM and a $500 (at the time) Ryzen chip. I had to perform some arcane troubleshooting steps to fix it. reply dmitrygr 14 hours agorootparentprev> Considering how slow the new calculator app isAm I really the only person who has a copy of \"calc.exe\" from win 2000 that replaces c:\\windows\\calc.exe immediately after any windows [re]install? reply pjmlp 15 hours agorootparentprevA side effect from app sandboxing, and modern Windows API being all COM based. reply stainablesteel 14 hours agorootparentprevmy perspective: i left windows years ago and i never laugh at any comedian as much as i laugh at people in windows update threads reply blibble 15 hours agorootparentprevthey&#x27;ll probably add ads and a subscriptionlike solitaire reply Tempest1981 12 hours agorootparentPay for extended precision, or an LCD font reply tored 7 hours agorootparentprevAnd achievements. reply naikrovek 16 hours agorootparentprevthe new calculator app is open source, you can see what&#x27;s making it so slow if you want, and contribute fixes if you have the skills to do the fixing.MIT license.https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;calculator&#x2F; reply rappatic 16 hours agorootparentThis isn&#x27;t justification for an app being bad. A company shouldn&#x27;t release a broken and slow app on an OS I pay for and you can&#x27;t make it seem like that&#x27;s okay simply because Microsoft took ten minutes to put the code on Github. At that rate, why put effort into improving apps at all if users will do free work for you? reply naikrovek 12 hours agorootparentso make it less bad, or stop complaining.if you&#x27;re complaining without a plan of action to address the complaint, you are speaking for attention alone.fork it and fix it, then use that version. open your fork so you can help others.you other commenters can continue to complain and whine endlessly or you can put your money where your mouth is and actually do something.what&#x27;s it gonna be? reply tpush 15 hours agorootparentprevMicrosoft requires contributors to sign a CLA giving Microsoft effective ownership over your contributions.You should never be working for a for-profit organization without compensation. reply eddythompson80 15 hours agorootparentNot arguing for contributing to Microsoft’s repos, but most open source projects under most well respected foundations do the same. There is a big argument for or against CLAs but it’s not like it’s just them. reply tpush 15 hours agorootparentYes, lots of organizations want free labor. That has no bearing on whether its ethical.There&#x27;s a somewhat decent argument to be made for non-profit organizations, but there&#x27;s none for for-profit orgs like Microsoft. reply tbyehl 8 hours agorootparentI&#x27;ve always thought CLAs were a strong indicator that an organization doesn&#x27;t actually want one&#x27;s free labor. reply naikrovek 12 hours agorootparentprevsuccessfully reach out to all contributors to the Linux kernel and then tell me CLAs are bad for a corporation.you open source people, man. you give all developers a horrible reputation. put up or shut up, lol reply tpush 1 hour agorootparentThat copyright of the Linux kernel is being shared by all is contributors is essential in making it an actual public good.I have no idea what you want to express with your second paragraph, though. reply saagarjha 13 hours agorootparentprevI am being compensated by the things I use getting better. reply tpush 1 hour agorootparentThat is by definition not compensation.I have to say I&#x27;m disappointed seeing you defend exploitative labor practices. reply naikrovek 12 hours agorootparentprevso fork it and work on it yourself.do I have to tell everyone what the MIT license allows? reply iggldiggl 15 hours agorootparentprevUnfortunately some issues have been declared as \"by design\". E.g. due to a rigid interpretation of some accessibility guideline, the fact that clicking on the history or trigonometry menus moves the input focus there, so Numpad Enter suddenly starts re-opening that menu instead of pressing &#x27;=&#x27; is apparently perfectly intended and definitively not a bug. reply codetrotter 16 hours agorootparentprevWhy would anyone want to do free work for Microsoft? reply dagaci 15 hours agorootparentBecause it makes the things they use better mostly, they are => https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;calculator&#x2F;graphs&#x2F;contributors reply pjmlp 15 hours agorootparentprevThey shouldn&#x27;t, other than provides a good entry on the CV, that is all. reply dingnuts 15 hours agorootparentprevThis is the correct response. Telling community members that they can fix bugs in libre, community-supported software is one thing.Telling &#x2F;users&#x2F; who are NOT community members to make changes to some corporation&#x27;s application that happens to have source available, in their free time, and then donate that labor to some billion dollar corporation? fuck THAT.Fix it your own damn self, Microsoft. I have better shit to do. reply MikusR 4 hours agorootparentCalculator is MIT Licensed. Is that not libre enough? reply naikrovek 12 hours agorootparentprevopen source software should not be contributed to? I can not understand open source people, man.it only counts if it has \"libre\" in the name? MIT-licensed isn&#x27;t good enough? WTF replybenbristow 14 hours agoparentprevFor as basic a program it is, it&#x27;s rather useful.My work only currently only give machines with 16GB RAM which gets eaten up by Edge&#x2F;Teams&#x2F;Outlook, security applications incl. Defender and then any dev tools on top of that. Trying to campaign for 32GB machines but bureaucracy takes its time in the public sector. The IT team have also just removed Paint 3D for some reason via InTune.Just opened Paint right now on my Windows 10 desktop and it&#x27;s using 13MB of RAM for a blank canvas.For pasting a screenshot and putting some rectangles&#x2F;arrows on it, it works a charm.Something to be said about Paint! reply vel0city 14 hours agorootparentWhy do you even bother pasting screenshots? Snipping tool can save it without even needing to paste it anywhere. Win+Shift+s reply benbristow 1 hour agorootparentUsually to save them somewhere for later (in memory) reply fyloraspit 14 hours agorootparentprevSometimes you want mouse or dynamic menu in frame for highlighting. Most likely reason for print screen + paint IMO. reply grishka 14 hours agoparentprevIf you want the old simple Paint, you could just copy mspaint.exe from an older version of Windows. Same for things like calculator and notepad and games. reply michaelhoffman 16 hours agoprevI hope it keeps its blazing-fast startup. I regularly used Paint instead of Photoshop on a system where I had both because Paint starts instantly but Photoshop took a bit. reply jjcm 15 hours agoparentI use photopea quite a bit for this reason. It&#x27;s faster to open this in a web browser than it is to boot photoshop. The 1&#x2F;3rd sidebar ad it loads is... a lot, but I&#x27;m amazed that I prefer that to a 30s boot time. reply NayamAmarshe 6 hours agorootparent+1 for Photopea, best free tool ever! reply harles 16 hours agoparentprevI was just reflecting on why I fire up Paint more often than GIMP, and this is the reason. Especially when all I want to do is paste an image, add a small annotation, and copy it back out, there’s no tool as fast. reply badsectoracula 15 hours agorootparentIt is basically why i have KolourPaint installed on Linux and ready on a launch button despite having a bunch of other 2D image editing apps: it starts (almost[0]) instantly and is perfectly fine for cropping images, adding annotations, etc and then pasting it to imgur, discord, or whatever. The only thing missing is having a tool to draw shapes like arrows (not something the Win9x era MS Paint, which is what KolourPaint replicates, had, but it would be a useful feature IMO).[0] it takes somewhere between half to a second, there is a visible delay between double click and the window appearing but i can live with that. I&#x27;m not using KDE as my DE, it is possible it&#x27;d start instantly if i already had the KDE libs in memory. reply sebmaynard 13 hours agorootparentprevLightshot is a great app for exactly this. And lines, boxes etc reply Tempest1981 11 hours agorootparentIndeed. Also, I just read that mouse-wheel lets you adjust the line&#x2F;arrow size... reply dingnuts 15 hours agorootparentprevYou can do all this in the snipping tool without opening a separate app, which is in fact a little faster reply cfiggers 15 hours agorootparentYes, except (unless I&#x27;m mistaken) the snipping tool can&#x27;t add text and it can&#x27;t draw boxes, which are two of the core use cases of screenshot annotation. reply jve 14 hours agorootparenthttps:&#x2F;&#x2F;getgreenshot.org&#x2F; Fast, doesn&#x27;t crash, does what you say and more. reply IG_Semmelweiss 15 hours agorootparentprevi use monosnap for this reason (free version)The number of times it crashes vs snipping tool, its like night and day.yet i keep coming back for monosnap.... it made my snippets look somewhat professional (gone handwritten arrows lol) replyxp84 16 hours agoprevNice. Basically this gets it to parity with the feature-set I actually used in Photoshop when I pirated it as a teen. I assume most people with straightforward needs who aren&#x27;t pirates are going to be really happy to be able to just use this. Think about people editing a meme graphic or \"photoshopping\" an extra person into an image for non-serious purposes.(Sidenote: Shoutout for Pixelmator Pro, the Mac app that similarly has more than enough for me, and has allowed me to stop needing Photoshop. I&#x27;ve never bought an Adobe product outside of work, and now I don&#x27;t use any either.) reply gus_massa 16 hours agoparentI use Gimp most of the times. But when I have to edit a few isolated pixels of a geometric black image over a plain white background I prefer to use Paint. I can&#x27;t find how to make Gimp modify only one pixel without blurring all the neighbors. reply knome 16 hours agorootparentAre you using the paintbrush tool instead of the pencil tool? A pencil with the brush size turned down to one shouldn&#x27;t bother any of the other pixels. reply forgotpwd16 16 hours agorootparentprev>can&#x27;t find how to make Gimp modify only one pixel without blurring all the neighborsUse pencil tool set to size 1px. Pencil does no blurring; paintbrush does. reply tyingq 16 hours agorootparentprevPaint.Net is a nice middle ground between the two. reply circuit10 16 hours agorootparentI like it but it&#x27;s not available on Linux, so I use Pinta which is a fork of an old version of Paint.NET from when it was open source, it&#x27;s not quite as good though reply bgarbiak 16 hours agorootparentprevIIRC the pencil tool in Gimp draws hard-edged pixels without any extra configuration. reply owlstuffing 15 hours agoprevMicrosoft should just replace Paint with Paint.NET. In my experience, it is the best paint tool for Windows. Much more intuitive UI than photoshop or gimp. reply charles_f 15 hours agoprevAt some level I feel like they should just drop paint and install paint.net as a default.But then, that would probably get paint.net on a downward spiral, so maybe it&#x27;s for the best? reply haunter 15 hours agoparentPaint.NET was open source (MIT) until 2009, alas nothing happened from the forks after they changed the license.Code is still available, not sure worth building though in 2023 when there are other optionshttps:&#x2F;&#x2F;code.google.com&#x2F;archive&#x2F;p&#x2F;openpdn&#x2F;https:&#x2F;&#x2F;github.com&#x2F;rivy&#x2F;OpenPDN&#x2F; reply jmkni 15 hours agorootparentI think Paint.net is largely worked on and maintained by one guy, so I don’t really blame him from wanting to make some money out of his work tbhIt’s a great tool. reply charles_f 13 hours agorootparentI mean, give him a million, get the product, rename it paint, and suddenly paint on windows supports much more than just layers and transparency. reply sbjs 15 hours agorootparentprevWhat&#x27;s the best alternative? reply haunter 13 hours agorootparentThe current Paint.NET is really good and obviously has much more features compared to since the version from 2009. But it&#x27;s not FOSS which can be a deal breaker for some.If FOSS then Pinta [0] is the best option imo. GIMP is not bad either but feels very janky and all over the place sometimes (lacking some elementary features while overblown in other departments)Krita [1] is the other one I&#x27;d recommend though it&#x27;s a digital painting app, not a image editing one. But you can do edit images with it0, https:&#x2F;&#x2F;www.pinta-project.com&#x2F;1, https:&#x2F;&#x2F;krita.org&#x2F;en&#x2F; reply tombert 16 hours agoprevI always kind of felt that Microsoft should have made gradual updates to Paint. I&#x27;m not saying that they should be itching to take on Photoshop or anything, but I feel that between Windows 95 until Windows Vista, Paint was pretty much unchanged.I think it would have been kind of cool if they kept a small team dedicated to making Paint gradually better, getting it into parity with something like Paint.NET or something like that. reply toyg 15 hours agoparentOn one hand, yes; but on the other: people underestimate the value of consistency. Not changing is a feature for so many people who are already content with the existing program. Change for change&#x27;s sake is not always good. reply tombert 14 hours agorootparentI&#x27;m not suggesting that they just add a million new features every release, but I feel like adding transparent backgrounds, even in 2002 or so, would not have interrupted any existing workflows or anything.I feel like there was a lot of low hanging fruit that they could have added that left the interface largely untouched; they could also have had a \"basic\" and \"advanced\" layout, where the \"basic\" keeps things more or less how they were in Windows 95, and advanced is for integrating new features. reply ajdude 15 hours agoprevOne of my favorite \"MS Paint\" clones is KolourPaint[1]. I&#x27;ve been using it for over a decade (you have to search around to get it on non-linux platforms but I presently have it on MacOS). One of my favorite features is how it handles transparency, where it&#x27;s just treated like another \"color\".If anyone is heavy into pixel art, you may also be interested in Aseprite[2].[1] http:&#x2F;&#x2F;www.kolourpaint.org&#x2F;[2] https:&#x2F;&#x2F;www.aseprite.org&#x2F; reply JoeAltmaier 13 hours agoprevSeems like a lot to add to such a simple program. I bet they didn&#x27;t do that.Years ago I re-released an old and honored tool. A strange path.It was the simple code editor used on an early office computer. The programmers all used it daily.First, the source was gone. The team that made it, the last remaining member that remembered it, said \"It was just a checkpointed version of our general document editor. We&#x27;ve continued development and have a hundred more features now.\"Moving to the document editor wouldn&#x27;t do. Can&#x27;t have programmers confused by formatting and mail-merge features on a C source file.So I took the current Document Designer source, ripped out everything formatting-related, kept some useful multi-document features. Added some special programmer-specific features (paste-to-anchor point etc for fast prototyping). Got something not too much larger than the old Editor, and much much smaller than the latest document tool.And...hardly anybody used it. Even a little change from ol faithful was too much for most of the teams. No surprise; their job was writing code, not relearning tools to write code.Anyway I had my own personal useful tool after that. Even if I was the only one that knew how to use it. reply alexmolas 16 hours agoprevI didn&#x27;t know Pain was still updated. I guess they are preparing it to be another player in the generative AI race. reply FirmwareBurner 15 hours agoparent>I didn&#x27;t know Pain was still updatedSadly pain is always up to date :( reply distract8901 16 hours agoparentprevThey added Bing to goddamn Notepad reply FirmwareBurner 15 hours agorootparentWhere? I don&#x27;t see it and I&#x27;m up to date. reply MikusR 4 hours agorootparentRight click on text. reply soupfordummies 15 hours agorootparentprevYeah I really hope they&#x27;re joking! reply FirmwareBurner 15 hours agorootparentI think he&#x27;s just spreading FUD. reply distract8901 11 hours agorootparentI don&#x27;t think you understand what FUD means.Hint: it does not mean \"things I disagree with or don&#x27;t understand\" reply thefifthsetpin 14 hours agorootparentprevJust hit F1. reply FirmwareBurner 14 hours agorootparentHow is that \"Bing in Notepad\"? reply artursapek 16 hours agorootparentprevlol reply cout 16 hours agoprevI wonder what file format they will use to save layers. Does the PNG format support layers? reply gigglesupstairs 16 hours agoparentPNG does support layers. Yes. I remember using it in Macromedia Fireworks which had a native format PNG which functioned like Photoshop&#x27;s PSD and saved layers and its styles and stuff. reply Macha 10 hours agorootparentPNG doesn&#x27;t. What it does support are application defined ancillary chunks, which fireworks used for custom extensions such as layers reply Explore3003 16 hours agoprevJPEG XL support would be great! reply masfuerte 16 hours agoprevRead the headline and thought... they did this in Windows 2000. But they are not talking about UpdateLayeredWindow. reply andrewstuart 15 hours agoprevMicrosoft should just buy paint.net reply pipeline_peak 13 hours agoparentMicrosoft could whip up paint.net in 1 day.There&#x27;s a component in WPF called InkCanvas, it pretty much is Paint. reply andrewstuart 13 hours agorootparent>> could whip up paint.net in 1 day.You realise this is basically an HN meme? reply pipeline_peak 12 hours agorootparent> You realise this is basically an HN meme?No, not really. It’s a simple program and Microsoft would literally just need to glue together their own existing components.I did it for a job interview project in 4 days using C# and WPF. And I am by no means a talented developer.I could see when people think creating something like Uber in a day as an arrogant HN meme. But there’s no moving parts here. reply andrewstuart 10 hours agorootparentSure thing buddy - you go build paint.net in a day. lol. Post a reply to this tomorrow and I&#x27;ll check it out. reply pipeline_peak 10 hours agorootparentCan you not read, buddy? I said Microsoft, that means a lot more people with a lot more experience.What’s even funnier is thinking that MS buying a mediocre old paint editor is a good business move. Paint.Net is practically an undergrad’s project. reply RevEng 4 hours agorootparentI&#x27;m sure what you mean to imply is \"it would be trivial\", and I agree, but any professional developer can tell you it wouldn&#x27;t be literally a day.It would start off as being a week, maybe two or three as the developer runs into a few tricky corner cases. Then it would sit in a backlog for a few weeks until QA sees it. Then a few days later you would get it back with a bunch of issues, some of which are legitimate bugs, others that are differences in what each side thinks the requirements should be. This would in turn get passed to various stakeholders who would garden shed their vision for an all in one paint program, that would lead to a large meeting between developers, sales, product management, and many others. Eventually executive would step in with a specific vision with no obvious involvement from everyone else, and then the actual development would begin.As one of my co-workers says, \"It will be ready two weeks after the requirements stop changing.\"I&#x27;m not even being sarcastic. I&#x27;ve seen this same pattern on at least a dozen new products I&#x27;ve helped develop. This is when things go reasonably well. When this don&#x27;t go well, this horrible cycle repeats for years until either some eldritch horror is released, or the entire project gets shutdown.It takes more than 30 seconds to open a file, it takes more than 5 minutes to make a trivial one one change, it takes more than a day to test and push that one line change, and it takes a week before that change is verified and merged. An entire application - regardless of how specific the requirements and how well understood the implementation - will takes weeks to months to get to the point where it can be released to the public. reply andrewstuart 3 hours agorootparentpaint.net would not be trivial to build. replythot_experiment 12 hours agoprevand here I am, carting around my mspaint.exe from install to install since windows2k or something, because I&#x27;ve got Krita for when I want something to launch slowly and have a bunch of features that aren&#x27;t useful for drawing an arrow on a screenshot(modern paint seems fine, and it does launch quickly, but the vibes are off) reply nashashmi 15 hours agoprevTransparency has been the longest missing feature in mspaint. Quite frustrating it took so long. reply ilyt 16 hours agoprevJust preinstall Krita at that point... reply nullbyte 15 hours agoprevIt brings me so much joy to know that Paint is still under active development. reply issafram 13 hours agoprevPaint.net will still be the best free alternative to Photoshop reply jrflowers 15 hours agoprevTake that, Adobe reply crawsome 15 hours agoprevGiven the treatment they gave the photo viewer, asking people to sign-up for a subscription-based video editor, I have low hopes for these features to arrive without paying a hefty non-optional cost. reply k12sosse 14 hours agoprevYou can tell windows 3.1 users by how they open this app - back on 16-bit windows it was pbrush.exe - 32-bit it was renamed mspaint.exe - but both launch it. reply smegsicle 14 hours agoprevuh oh mtpaint starting to feel real competition reply imiric 15 hours agoprevI find it surprising that MS hasn&#x27;t acquired Paint.NET. It&#x27;s a stellar product, a decent free Photoshop alternative, and miles ahead of whatever MS Paint ever was. It even has .NET in the name, which probably confuses new users into thinking it is owned by MS, so kind of surprised MS hasn&#x27;t sued over that either.That said, I&#x27;m grateful that Paint.NET has managed to remain independent, as MS might&#x27;ve ruined it.I do wonder who still uses MS Paint that would find these new features compelling, and why MS keeps adding new features to it. Surely they&#x27;ve done some research that shows people want this? Or is it still developed to justify paying the team that maintains it? reply 0cf8612b2e1e 15 hours agoparentI like MS paint because I know exactly what it is. Starts instantly and can be used to quickly crop a photo or hastily circle a call out. No UI redesigns to make a developer feel useful.If we were talking acquisitions, they should also buy up the Notepad++ guy. reply itisit 15 hours agoparentprev> It even has .NET in the name, which probably confuses new users into thinking it is owned by MS, so kind of surprised MS hasn&#x27;t sued over that either.Not sure where the infringement would be. In any case, Paint.NET won second place last year in the \"Creativity – Graphics and 3D\" category of the Microsoft Store App Awards. [0][0] https:&#x2F;&#x2F;blogs.windows.com&#x2F;windowsdeveloper&#x2F;2022&#x2F;05&#x2F;27&#x2F;announ... reply enonimal 16 hours agoprev [–] Gearing up to integrate image generation, I&#x27;d guess reply codetrotter 16 hours agoparent [–] Probably yes. The video in the article shows background removal with a single click of the mouse after all – i.e subject isolation using AI.It would be a logical next step at some point to integrate AI image generation as well, using OpenAI Dall-E. reply speedgoose 16 hours agorootparent [–] It’s perhaps going to be the Bing version of OpenAI Dall-E, which is some kind of version 2.5 replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Microsoft is introducing an update to the Paint app, available to Windows Insiders, that includes support for layers and transparency.",
      "This update allows users to produce more intricate digital art by layering shapes, text, and image elements.",
      "The update also facilitates the opening and saving of transparent PNGs, with the Feedback Hub available for users' suggestions and comments."
    ],
    "commentSummary": [
      "Microsoft Paint, a feature on Windows, is being updated to incorporate layers and transparency support, which has taken users by surprise due to the app's previously antiquated impression.",
      "The refreshed calculator app has elicited assorted responses, with criticisms citing slow performance and feature deficiency. Additionally, there's debate over the value and performance of the app.",
      "While some users endorse gradual improvements to Paint's functionality, others suggest exploring alternative programs. There's anticipation surrounding the potential future integration of AI image generation."
    ],
    "points": 206,
    "commentCount": 142,
    "retryCount": 0,
    "time": 1695058477
  },
  {
    "id": 37561762,
    "title": "The anatomy of a Godot API call",
    "originLink": "https://sampruden.github.io/posts/godot-is-not-the-new-unity/",
    "originBody": "Sam Pruden Does Stuff A place to occasionally dump my programming thoughts. HOME CATEGORIES TAGS ARCHIVES ABOUT Home Godot is not the new Unity - The anatomy of a Godot API call Godot is not the new Unity - The anatomy of a Godot API call Posted Sep 17, 2023 Updated Sep 18, 2023 By Sam Pruden 32 min read Like many people, I’ve spent the last few days looking for the new Unity. Godot has some potential, especially if it can take advantage of an influx of dev talent to drive rapid improvement. Open source is cool like that. However, one major issue holds it back - the binding layer between engine code and gameplay code is structurally built to be slow in ways which are very hard to fix without tearing everything down and rebuilding the entire API from scratch. Godot has been used to create some successful games, so clearly this isn’t always a blocker. However Unity has spent the last five years working on speeding up their scripting with crazy projects such as building two custom compilers, SIMD maths libraries, custom collections and allocators, and of course the giant (and very much unfinished) ECS project. It’s been their CTO’s primary focus since 2018. Clearly Unity believed that scripting performance mattered to a significant part of their userbase. Switching to Godot isn’t only like going back five years in Unity - it’s so much worse. I started a controversial but productive discussion about this on the Godot subreddit a few days ago. This article is a more detailed continuation of my thoughts in that post now that I have a little more understanding of how Godot works. Let’s be clear here: I’m still a Godot newb, and this article will contain mistakes and misconceptions. Note: The following contains criticisms of the Godot engine’s design and engineering. Although I occasionally use some emotive language to describe my feelings about these things, the Godot developers have put in lots of hard work for the FOSS community and built something that’s loved by many people, and my intent is not to offend or come across as rude to any individuals. Deepdive into performing a raycast from C# We’re going to take a deep dive into how Godot achieves the equivalent of Unity’s Physics2D.Raycast, and what happens under the hood when we use it. To make this a little more concrete, let’s start by implementing a trivial function in Unity. Unity 1 2 3 4 5 6 7 // Simple raycast in Unity bool GetRaycastDistanceAndNormal(Vector2 origin, Vector2 direction, out float distance, out Vector2 normal) { RaycastHit2D hit = Physics2D.Raycast(origin, direction); distance = hit.distance; normal = hit.normal; return (bool)hit; } Let’s have a quick look at how this is implemented by following the calls. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 public static RaycastHit2D Raycast(Vector2 origin, Vector2 direction) => defaultPhysicsScene.Raycast(origin, direction, float.PositiveInfinity); public RaycastHit2D Raycast(Vector2 origin, Vector2 direction, float distance, [DefaultValue(\"Physics2D.DefaultRaycastLayers\")] int layerMask = -5) { ContactFilter2D contactFilter = ContactFilter2D.CreateLegacyFilter(layerMask, float.NegativeInfinity, float.PositiveInfinity); return Raycast_Internal(this, origin, direction, distance, contactFilter); } [NativeMethod(\"Raycast_Binding\")] [StaticAccessor(\"PhysicsQuery2D\", StaticAccessorType.DoubleColon)] private static RaycastHit2D Raycast_Internal(PhysicsScene2D physicsScene, Vector2 origin, Vector2 direction, float distance, ContactFilter2D contactFilter) { Raycast_Internal_Injected(ref physicsScene, ref origin, ref direction, distance, ref contactFilter, out var ret); return ret; } [MethodImpl(MethodImplOptions.InternalCall)] private static extern void Raycast_Internal_Injected( ref PhysicsScene2D physicsScene, ref Vector2 origin, ref Vector2 direction, float distance, ref ContactFilter2D contactFilter, out RaycastHit2D ret); Okay, so it does a tiny amount of work and efficiently shunts the call off to the unmanaged engine core via the extern mechanism. That makes sense, I’m sure Godot will do something similar. Foreshadowing. Godot Let’s do the same thing in Godot, exactly as the tutorial recommends. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // Equivalent raycast in Godot bool GetRaycastDistanceAndNormal(Vector2 origin, Vector2 direction, out float distance, out Vector2 normal) { World2D world = GetWorld2D(); PhysicsDirectSpaceState2D spaceState = world.DirectSpaceState; PhysicsRayQueryParameters2D queryParams = PhysicsRayQueryParameters2D.Create(origin, origin + direction); Godot.Collections.Dictionary hitDictionary = spaceState.IntersectRay(queryParams); if (hitDictionary.Count != 0) { Variant hitPositionVariant = hitDictionary[(Variant)\"position\"]; Vector2 hitPosition = (Vector2)hitPositionVariant; Variant hitNormalVariant = hitDictionary[(Variant)\"normal\"]; Vector2 hitNormal = (Vector2)hitNormalVariant; distance = (hitPosition - origin).Length(); normal = hitNormal; return true; } distance = default; normal = default; return false; } The first thing that we notice is that this is longer. That is not the focus of my criticism, and is partly due to the fact that I’ve formatted this code verbosely in order to make it easier for us to break it down line by line. So let’s do that, what’s actually happening here? We start by calling GetWorld2D(). In Godot, physics queries are all performed in the context of a world, and this function gets the world our code is running in. Although this World2D is a managed class type, this function doesn’t do anything crazy like allocate every time we run it. None of these functions are going to do anything crazy like that for a simple raycast, right? Foreshadowing. If we look inside these API calls we’ll see that even ostensibly simple ones like this are implemented through some rather convoluted machinery which will have at least a little performance overhead. Let’s dive into GetWorld2D as an example of that by unravelling some of its calls through C#. This is roughly what all the calls that return managed types look like. I’ve added some comments to explain what’s going on. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 // This is the function we're diving into. public World2D GetWorld2D() { // MethodBind64 is a pointer to the function we're calling in C++. // MethodBind64 is stored in a static variable, so we have to do a memory lookup to retrieve it. return (World2D)NativeCalls.godot_icall_0_51(MethodBind64, GodotObject.GetPtr(this)); } // We call into these functions which mediate API calls. internal unsafe static GodotObject godot_icall_0_51(IntPtr method, IntPtr ptr) { godot_ref godot_ref = default(godot_ref); // The try/finally machinery is not free. This introduces a state machine. // It can also block JIT optimisations. try { // Validation check, even though everything here is internal and should be trusted. if (ptr == IntPtr.Zero) throw new ArgumentNullException(\"ptr\"); // This calls into another function which performs the actual function pointer call // and puts the unmanaged result in godot_ref via a pointer. NativeFuncs.godotsharp_method_bind_ptrcall(method, ptr, null, &godot_ref); // This is some machinery for moving references to managed objects over the C#/C++ boundary. return InteropUtils.UnmanagedGetManaged(godot_ref.Reference); } finally { godot_ref.Dispose(); } } // The function which actually calls the function pointer. [global::System.Runtime.CompilerServices.MethodImpl(global::System.Runtime.CompilerServices.MethodImplOptions.AggressiveInlining)] public static partial void godotsharp_method_bind_ptrcall( global::System.IntPtr p_method_bind, global::System.IntPtr p_instance, void** p_args, void* p_ret) { // But wait! // _unmanagedCallbacks.godotsharp_method_bind_ptrcall is actually another // static variable access to retrieve another function pointer. _unmanagedCallbacks.godotsharp_method_bind_ptrcall(p_method_bind, p_instance, p_args, p_ret); } // To be honest, I haven't studied this well enough to know exactly what's happening here. // The basic idea is straightforward - this takes a pointer to an unmanaged GodotObject, // brings it into .Net, notifies the garbage collector of it so that it can be tracked, // and casts it to the GodotObject type. // Fortunately, this doesn't appear to do any allocations. Foreshadowing. public static GodotObject UnmanagedGetManaged(IntPtr unmanaged) { if (unmanaged == IntPtr.Zero) return null; IntPtr intPtr = NativeFuncs.godotsharp_internal_unmanaged_get_script_instance_managed(unmanaged, out var r_has_cs_script_instance); if (intPtr != IntPtr.Zero) return (GodotObject)GCHandle.FromIntPtr(intPtr).Target; if (r_has_cs_script_instance.ToBool()) return null; intPtr = NativeFuncs.godotsharp_internal_unmanaged_get_instance_binding_managed(unmanaged); object obj = ((intPtr != IntPtr.Zero) ? GCHandle.FromIntPtr(intPtr).Target : null); if (obj != null) return (GodotObject)obj; intPtr = NativeFuncs.godotsharp_internal_unmanaged_instance_binding_create_managed(unmanaged, intPtr); if (!(intPtr != IntPtr.Zero)) return null; return (GodotObject)GCHandle.FromIntPtr(intPtr).Target; } This is actually a substantial amount of overhead. We have a number of layers of pointer chasing indirection between our code and C++. Each of those is a memory lookup, and on top of that we do a bit of work with the validation, try finally, and interpreting the returned pointer. These may sound like tiny inconsequential things, but when every single call into the core and every property/field access on a Godot object does this whole journey, it starts to add up. If we look at the next line which accesses the world.DirectSpaceState property we’ll find it does pretty much the same thing. The PhysicsDirectSpaceState2D is once again retrieved from C++ land via this machinery. Don’t worry, I won’t bore you with the details! The line after that is the first thing I saw here that really boggled my bonnet. 1 PhysicsRayQueryParameters2D queryParams = PhysicsRayQueryParameters2D.Create(origin, origin + direction); What’s the big deal, that’s just a little struct packing some raycast parameters, right? Wrong. PhysicsRayQueryParameters2D is a managed class, and this is a full GC garbage generating allocation. That’s a pretty crazy thing to have in a performance sensitive hot path! I’m sure it’s just the one allocation though, right? Let’s have a look inside. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // Summary: // Returns a new, pre-configured Godot.PhysicsRayQueryParameters2D object. Use it // to quickly create query parameters using the most common options. // var query = PhysicsRayQueryParameters2D.create(global_position, global_position // + Vector2(0, 100)) // var collision = get_world_2d().direct_space_state.intersect_ray(query) public unsafe static PhysicsRayQueryParameters2D Create(Vector2 from, Vector2 to, uint collisionMask = uint.MaxValue, Array exclude = null) { // Yes, this goes through all of the same machinery discussed above. return (PhysicsRayQueryParameters2D)NativeCalls.godot_icall_4_731( MethodBind0, &from, &to, collisionMask, (godot_array)(exclude ?? new Array()).NativeValue ); } Uh oh. Have you spotted it yet? That Array is a Godot.Collections.Array. That’s another managed class type. Look what happens when we pass in a null value. 1 (godot_array)(exclude ?? new Array()).NativeValue That’s right, even if we don’t pass an exclude array, it goes ahead and allocates a whole array on the C# heap for us anyway, just so that it can immediately convert it back into a native value representing an empty array. In order to pass two simple Vector2 values (16 bytes) to a raycast function, we’ve now done two separate garbage creating heap allocations totalling 632 bytes! As you’ll see later, we can mitigate this by caching a PhysicsRayQueryParameters2D. However, as you can see from the doc comment I included above, the API clearly expects and recommends creating fresh instances for each raycast. Let’s move onto the next line. It can’t get any crazier, right? Foreshadowing. 1 Godot.Collections.Dictionary hitDictionary = spaceState.IntersectRay(queryParams); Whelp. That shadowing wasn’t very fore. That’s right, our raycast is returning an untyped dictionary. And yes, it creates garbage by allocating in on the managed heap, another 96 bytes. You have my permission to do a bemused and upset type of face now. “Oh, well maybe it at least returns null if it doesn’t hit anything?” you may be thinking. No. If it doesn’t hit anything, it allocates and returns an empty dictionary. Let’s jump straight into the C++ implementation here. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 Dictionary PhysicsDirectSpaceState2D::_intersect_ray(const Ref &p_ray_query) { ERR_FAIL_COND_V(!p_ray_query.is_valid(), Dictionary()); RayResult result; bool res = intersect_ray(p_ray_query->get_parameters(), result); if (!res) { return Dictionary(); } Dictionary d; d[\"position\"] = result.position; d[\"normal\"] = result.normal; d[\"collider_id\"] = result.collider_id; d[\"collider\"] = result.collider; d[\"shape\"] = result.shape; d[\"rid\"] = result.rid; return d; } // This is the params struct that the inernal intersect_ray takes in. // Nothing too crazy here (although exclude could probably be improved). struct RayParameters { Vector2 from; Vector2 to; HashSet exclude; uint32_t collision_mask = UINT32_MAX; bool collide_with_bodies = true; bool collide_with_areas = false; bool hit_from_inside = false; }; // And this is the output. A perfectly reasonable return value for a raycast. struct RayResult { Vector2 position; Vector2 normal; RID rid; ObjectID collider_id; Object *collider = nullptr; int shape = 0; }; As we can see, this is wrapping some perfectly reasonable raycast function in ungodly slow craziness. That internal intersect_ray is the function that should be in the API! This C++ code allocates an untyped dictionary on the unmanaged heap. If we dig down into this dictionary, we find a hashmap as expected. It performs six hashmap lookups to initialize this dictionary (some of them may even do additional allocations, but I haven’t dug that deep). But wait, this is an untyped dictionary. How does that work? Well the internal hashmap maps Variant to Variant. Sigh. What’s a Variant? Well the implementation is quite complicated, but in simple terms it’s a big tagged union type encompassing all possible types the dictionary can hold. We can think of it as being the dynamic untyped type. What we care about is its size, which is 20 bytes. Okay, so each of those “fields” we’ve written into the dictionary is now 20 bytes large. Oh, and so are the keys. Those 8 byte Vector2 values? 20 bytes each now. That int? 20 bytes. You get the picture. If we sum the sizes of the fields in RayResult, we’re looking at 44 bytes (assuming 8 byte pointers). If we sum the sizes of the Variant keys and values of the dictionary, that’s 2 * 6 * 20 = 240 bytes! But wait, it’s a hashmap. Hashmaps don’t store their data compactly, so the true size of that dictionary on the heap is at least 6x larger than the data we want to return, probably much more. Okay, let’s go back to C# and see what happens when we return this thing. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 // The function we're calling. public Dictionary IntersectRay(PhysicsRayQueryParameters2D parameters) { return NativeCalls.godot_icall_1_729(MethodBind1, GodotObject.GetPtr(this), GodotObject.GetPtr(parameters)); } internal unsafe static Dictionary godot_icall_1_729(IntPtr method, IntPtr ptr, IntPtr arg1) { godot_dictionary nativeValueToOwn = default(godot_dictionary); if (ptr == IntPtr.Zero) throw new ArgumentNullException(\"ptr\"); void** intPtr = stackalloc void*[1]; *intPtr = &arg1; void** p_args = intPtr; NativeFuncs.godotsharp_method_bind_ptrcall(method, ptr, p_args, &nativeValueToOwn); return Dictionary.CreateTakingOwnershipOfDisposableValue(nativeValueToOwn); } internal static Dictionary CreateTakingOwnershipOfDisposableValue(godot_dictionary nativeValueToOwn) { return new Dictionary(nativeValueToOwn); } private Dictionary(godot_dictionary nativeValueToOwn) { godot_dictionary value = (nativeValueToOwn.IsAllocated ? nativeValueToOwn : NativeFuncs.godotsharp_dictionary_new()); NativeValue = (godot_dictionary.movable)value; _weakReferenceToSelf = DisposablesTracker.RegisterDisposable(this); } The main things to notice here are that we’re allocating a new managed (garbage creating, yada yada) dictionary in C#, and that it holds a pointer into the one created on the heap in C++. Hey, at least we’re not copying the dictionary contents over! I’ll take wins where I can get them at this point. Okay, so what next? 1 2 3 4 5 6 7 8 9 10 11 12 if (hitDictionary.Count != 0) { // The cast from string to Variant can be implicit - I've made it explicit here for clarity Variant hitPositionVariant = hitDictionary[(Variant)\"position\"]; Vector2 hitPosition = (Vector2)hitPositionVariant; Variant hitNormalVariant = hitDictionary[(Variant)\"normal\"]; Vector2 hitNormal = (Vector2)hitNormalVariant; distance = (hitPosition - origin).Length(); normal = hitNormal; return true; } Hopefully we can all follow what’s happening here at this point. If our ray didn’t hit anything an empty dictionary is returned, so we check for hits by checking the count. If we hit something, for each field we want to read we: Cast string keys to C# Variant structs (This also does a call into C++) Chase some more function pointers to call into C++ in the way we’ve come to expect by now Perform a hashmap lookup to get the Variant holding our value (via function pointer chasing, of course) Copy those 20 bytes back into C# world (yes, even though we’re reading Vector2 values which are only 8 bytes) Extract the Vector2 value from the Variant (Yes, it also chases pointers all the way back into C++ to do this conversion) Well that’s a lot work for returning a 44 byte struct and reading a couple of fields. Can we do better? Caching query parameters If you can remember as far back as PhysicsRayQueryParameters2D, we had the opportunity to avoid some allocations by caching, so let’s do that quickly. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 readonly struct CachingRayCaster { private readonly PhysicsDirectSpaceState2D spaceState; private readonly PhysicsRayQueryParameters2D queryParams; public CachingRayCaster(PhysicsDirectSpaceState2D spaceState) { this.spaceState = spaceState; this.queryParams = PhysicsRayQueryParameters2D.Create(Vector2.Zero, Vector2.Zero); } public bool GetDistanceAndNormal(Vector2 origin, Vector2 direction, out float distance, out Vector2 normal) { Godot.Collections.Dictionary hitDictionary = this.spaceState.IntersectRay(this.queryParams); if (hitDictionary.Count != 0) { Variant hitPositionVariant = hitDictionary[(Variant)\"position\"]; Vector2 hitPosition = (Vector2)hitPositionVariant; Variant hitNormalVariant = hitDictionary[(Variant)\"normal\"]; Vector2 hitNormal = (Vector2)hitNormalVariant; distance = (hitPosition - origin).Length(); normal = hitNormal; return true; } distance = default; normal = default; return false; } } After the first ray, this removes 2/3rds of our per ray C#/GC allocations by count, and 632/738 of our C#/GC allocations by bytes. It’s still not a good situation, but it’s an improvement. What about GDExtension? As you may have heard, Godot also gives us a C++ (or Rust, or other native language) API to allow us to write high performance code. That will come to the rescue here, right? Right? Well… So it turns out GDExtension exposes the exact same API. Yeah. You can write fast C++ code, but you still only get an API that returns an untyped dictionary of bloated Variant values. It’s a little better because there’s no GC to worry about, but… Yeah. I recommend making another sad face right about now. A whole different approach - the RayCast2D node But wait! We can take a whole different approach. 1 2 3 4 5 6 7 8 9 10 bool GetRaycastDistanceAndNormalWithNode(RayCast2D raycastNode, Vector2 origin, Vector2 direction, out float distance, out Vector2 normal) { raycastNode.Position = origin; raycastNode.TargetPosition = origin + direction; raycastNode.ForceRaycastUpdate(); distance = (raycastNode.GetCollisionPoint() - origin).Length(); normal = raycastNode.GetCollisionNormal(); return raycastNode.IsColliding(); } Here we have a function which takes a reference to a RayCast2D node in the scene. As the name suggests, this is a scene node that performs raycasts. It’s implemented in C++, and it doesn’t go through the same API with all of the dictionary overhead. This is a pretty clunky way to do raycasts as we need a reference to a node in the scene which we’re happy to mutate, and we have to reposition the node in the scene in order to do a query, but let’s take a look inside. First we need to note that, as we’ve come to expect, each of these properties that we’re accessing does a full pointer chasing journey into C++ land. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public Vector2 Position { get => GetPosition() set => SetPosition(value); } internal unsafe void SetPosition(Vector2 position) { NativeCalls.godot_icall_1_31(MethodBind0, GodotObject.GetPtr(this), &position); } internal unsafe static void godot_icall_1_31(IntPtr method, IntPtr ptr, Vector2* arg1) { if (ptr == IntPtr.Zero) throw new ArgumentNullException(\"ptr\"); void** intPtr = stackalloc void*[1]; *intPtr = arg1; void** p_args = intPtr; NativeFuncs.godotsharp_method_bind_ptrcall(method, ptr, p_args, null); } Now let’s look at what ForceRaycastUpdate() actually does. I’m sure you can guess the C# by now, so let’s dive straight into the C++. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 void RayCast2D::force_raycast_update() { _update_raycast_state(); } void RayCast2D::_update_raycast_state() { Ref w2d = get_world_2d(); ERR_FAIL_COND(w2d.is_null()); PhysicsDirectSpaceState2D *dss = PhysicsServer2D::get_singleton()->space_get_direct_state(w2d->get_space()); ERR_FAIL_NULL(dss); Transform2D gt = get_global_transform(); Vector2 to = target_position; if (to == Vector2()) { to = Vector2(0, 0.01); } PhysicsDirectSpaceState2D::RayResult rr; bool prev_collision_state = collided; PhysicsDirectSpaceState2D::RayParameters ray_params; ray_params.from = gt.get_origin(); ray_params.to = gt.xform(to); ray_params.exclude = exclude; ray_params.collision_mask = collision_mask; ray_params.collide_with_bodies = collide_with_bodies; ray_params.collide_with_areas = collide_with_areas; ray_params.hit_from_inside = hit_from_inside; if (dss->intersect_ray(ray_params, rr)) { collided = true; against = rr.collider_id; against_rid = rr.rid; collision_point = rr.position; collision_normal = rr.normal; against_shape = rr.shape; } else { collided = false; against = ObjectID(); against_rid = RID(); against_shape = 0; } if (prev_collision_state != collided) { queue_redraw(); } } It looks like there’s a lot going on here, but it’s actually quite simple. If we look carefully we can see that the structure is pretty much the same as our first GetRaycastDistanceAndNormal C# function. It gets the world, gets the state, builds the parameters, calls intersect_ray to do the actual work, then writes the result out to the properties. But look! No heap allocations, no Dictionary, and no Variant. This is more like it! We can predict that this will be a lot faster. Timing it Okay, I’ve made a lot of allusions to all of this overhead being dramatically problematic and we can easily see that it should be, but let’s put some actual numbers to this with benchmarks. As we’ve seen above, RayCast2D.ForceRaycastUpdate() is pretty close to a minimalist call to the physics engine’s intersect_ray, so we can use this as a baseline. Remember that even this has some overhead from the pointer chasing function call. I’ve also benchmarked each of the versions of the code we’ve discussed. Each benchmark runs 10,000 iterations of the function under test, with warmup and outlier filtering. I disabled GC collection during the tests. I like to run my game benchmarks on weaker hardware so you may get better results if you repro, but it’s the relative numbers that we care about. Our setup is a simple scene containing a single circle collider that our ray always hits. We’re interested in measuring binding overhead, not the performance of the physics engine itself. We’re dealing with timings for individual rays measured in nanoseconds, so these numbers may look inconsequentially small. To better illustrate their significance, I also report “calls per frame” giving the number of times the functions could be called in in a single frame at 60fps and 120fps if the game did nothing but trivial raycasts. Method Time (μs) Baseline multiple Per frame (60fps) Per frame (120fps) GC alloc (bytes) ForceRaycastUpdate (raw engine speed, not useful) 0.49 1.00 34,000 17,000 0 GetRaycastDistanceAndNormalWithNode 0.97 1.98 17,200 8,600 0 CachingRayCaster.GetDistanceAndNormal 7.71 15.73 2,200 1,100 96 GetRaycastDistanceAndNormal 24.23 49.45 688 344 728 Those are some significant differences! We might expect that the fastest way to do a raycast in a reasonable engine/API is to use the function exposed for doing exactly that, which is taught as the canonical way in the documentation. As we can see, if we do that, the binding/API overhead makes this 50X slower than the raw physics engine speed. Ouch! Using that same API but being sensible (if awkward) about caching, we can get that down to only 16X overhead. This is better, but still awful. If our aim here is to get practical performance, we have to sidestep the proper/canonical/advertised API completely, and instead clunkily manipulate scene objects to exploit them to do our query for us. In a sensible world moving objects around in the scene and asking them to do raycasts for us would be slower than calling the raw physics API, but in fact it’s 8X faster. Even the node approach is 2X slower than the raw speed of the engine (which we’re actually underestimating). This means that half of the time in that function is being spent on setting two properties and reading three properties. The binding overhead is large enough that five property accesses takes as long as a raycast. Let that sink in. Let’s not even think about the fact that in the real world we may well want to set and read even more properties, such as setting the layer mask and reading the hit collider. At the lower end, those numbers are actually very limiting. My current project needs more than 344 raycasts per frame, and of course it does a lot more than just raycasting. This test is a trivial scene with a single collider, if we were making the raycast do actual work in a more complex scene these numbers would be even lower! The documentation’s standard way of doing raycasts would grind my whole game to a halt. We also can’t forget about the garbage creating allocations that happen in C#. I usually write games with a zero garbage per frame policy. Just for fun, I also benchmarked Unity. It does a full useful raycast, with parameter setting and result retrieval, in about 0.52μs. Before Godot’s binding overhead, the core physics engines have comparable speed. Have I cherrypicked? When I posted the reddit thread, a number of people said that the physics API is uniquely bad and that it isn’t representative of the whole engine. I certainly didn’t intentionally cherrypick it - it just so happens that raycasting was the very first thing I attempted when checking out Godot. However, perhaps I’m being a little unfair, so let’s examine that. If I had wanted to cherrypick a worse method, I wouldn’t have had to look far. Right next to IntersectRay are IntersectPoint and IntersectShape, both of which share all of the same problems as IntersectRay with the additional craziness that they can have multiple results, so they return a heap allocated managed Godot.Collections.Array! Oh by the way, that Array is actually a typed wrapper around Godot.Collections.Array, so every 8 byte reference to a dictionary is actually stored as a 20 byte Variant. Clearly I haven’t picked the very worst method in the API! If we scan the whole Godot API (via C# reflection) we luckily find that there aren’t that many things which return Dictionary. There’s an eclectic list including the AnimationNode._GetChildNodes method, the Bitmap.Data property, the Curve2D._Data property (and 3D), some things in GLTFSkin, some TextServer stuff, some NavigationAgent2D pieces, etc. None of those are great places to have slow heap allocated dictionaries, but none of them are as bad as the physics API. However, in my experience, very few engine APIs get as much use as physics. If I look at the engine API calls in my gameplay code, they’re probably 80% physics and transforms. Let’s also remember that Dictionary is only part of the problem. If we look a little wider for things returning Godot.Collections.Array (remember: heap allocated, contents as Variant) we find lots from physics, mesh & geometry manipulation, navigation, tilemaps, rendering, and more. Physics may be a particularly bad (but essential) area of the API, but the heap allocated type problems, as well as the general slowness of the pointer chasing, are deeply rooted throughout. So why are we waiting for Godot? Godot’s primary scripting language is GDScript, a dynamically typed interpreted language where almost all non primitives are heap allocated, i.e. it doesn’t have a struct analogue. That sentence should have set off a cacophony of performance alarms in your head. I’ll give you a moment for your ears to stop ringing. If we look at how Godot’s C++ core exposes its API we’ll see something interesting. 1 2 3 4 5 6 7 8 void PhysicsDirectSpaceState3D::_bind_methods() { ClassDB::bind_method(D_METHOD(\"intersect_point\", \"parameters\", \"max_results\"), &PhysicsDirectSpaceState3D::_intersect_point, DEFVAL(32)); ClassDB::bind_method(D_METHOD(\"intersect_ray\", \"parameters\"), &PhysicsDirectSpaceState3D::_intersect_ray); ClassDB::bind_method(D_METHOD(\"intersect_shape\", \"parameters\", \"max_results\"), &PhysicsDirectSpaceState3D::_intersect_shape, DEFVAL(32)); ClassDB::bind_method(D_METHOD(\"cast_motion\", \"parameters\"), &PhysicsDirectSpaceState3D::_cast_motion); ClassDB::bind_method(D_METHOD(\"collide_shape\", \"parameters\", \"max_results\"), &PhysicsDirectSpaceState3D::_collide_shape, DEFVAL(32)); ClassDB::bind_method(D_METHOD(\"get_rest_info\", \"parameters\"), &PhysicsDirectSpaceState3D::_get_rest_info); } This one shared mechanism is used to generate the bindings for all three scripting interfaces; GDSCript, C#, and GDExtensions. ClassDB collects function pointers and metadata about each of the API functions, which is then piped through various code generation systems to create the bindings for each language. This means that every API function is designed primarily to serve the limitations of GDScript. IntersectRay returns an untyped dynamic Dictionary because GDScript doesn’t have structs. Our C# and even our GDExtensions C++ code has to pay the catastrophic price for that. This way of handling binding via function pointers also leads to significant overhead, as we’ve seen from simple property accesses being slow. Remember, each call first does a memory lookup to find the function pointer it wants to call, then it does another lookup to find the function pointer of a secondary function which is actually responsible for calling the function, then it calls the secondary function passing it the pointer to the primary function. All along that journey there’s extra validation code, branching, and type conversions. C# (and obviously C++) has a fast mechanism for calling into native code via P/Invoke, but Godot simply doesn’t use it. Godot has made a philosophical decision to be slow. The only practical way to interact with the engine is via this binding layer, and its core design prevents it from ever being fast. No amount of optimising the implementation of Dictionary or speeding up the physics engine is going to get around the fact we’re passing large heap allocated values around when we should be dealing with tiny structs. While C# and GDScript APIs remain synchronised, this will always hold the engine back. Okay, let’s fix it then! What can we do without deviating from the existing binding layer? If we assume that we still need to keep all of our APIs GDScript compatible, there are a few areas where we can probably improve things, although it won’t be pretty. Let’s go back to our IntsersectRay example. GetWorld2D().DirectStateSpace could be compressed to one call instead of two by introducing GetWorld2DStateSpace(). The PhysicsRayQueryParameters2D issues could be removed by adding an overload which takes all of the fields as parameters. This would bring us roughly inline with the CachedRayCaster performance (16X baseline) without having to do caching. The Dictionary allocation could be removed by allowing us to pass in a cached/pooled dictionary to write into. This is ugly and clumsy compared to a struct, but it would remove the allocation. The dictionary lookup process is still ridiculously slow. We might be able to improve on that by instead returning a class with the expected properties. The allocation here could be eliminated with the cached/pooled approach the same way it could with Dictionary. These options aren’t pretty or ergonomic for the user, but if we’re in the business of doing ugly patches to get things running, they would probably work. This would fix the allocations but we’d still probably only be about 4X the baseline because of all of the pointer chasing across the boundary and managing of cached values. It may also be possible to improve the generated code for all of the pointer chasing shenanigans. I haven’t studied this in detail yet, but if there are wins to find there then they’d apply to the whole API across the board, which would be cool! We could probably at least get away with removing the validation and the try finally in release builds. What if we’re allowed to add additional APIs for C# and GDExtensions which aren’t GDScript compatible? Now we’re talking! If we open up this possibility* then in theory we could augment the ClassDB bindings with better ones that deal directly in structs and go through the proper P/Invoke mechanisms. This is the path to viable performance. Unfortunately, duplicating the entire API with better versions like this would create quite a mess. There might be ways through this by marking things [Deprecated] and trying to guide the user in the right direction, but issues such as naming clashes would get ugly. * Maybe this is already possible, but I haven’t found it yet. Let me know! What if we tear it all down and start again? This option obviously has a lot of short term pain. Godot 4.0 has only recently happened, and now I’m talking about a backcompat breaking complete API redux like a Godot 5.0. However, if I’m honest with myself, I see this as the only viable path to the engine being in a good place in three years time. Mixing fast and slow APIs as discussed above would leave us with headaches for decades - a trap I expect the engine will probably fall into. In my opinion, if Godot were to go down this route, GDScript should probably be dropped entirely. I don’t really see the point of it when C# exists, and supporting it causes so much hassle. I’m clearly completely at odds with the lead Godot devs and the project philosophy on this point, so I have no expectation that this will happen. Who knows though - Unity eventually dropped UnityScript for full C#, maybe Godot will one day take the same step. Foreshadowing? Edit: I’m taking the above out for now. I don’t personally care about GDScript, but other people do and I don’t want to take it away from them. I have no objection to C# and GDScript sitting beside each other with different APIs each optimised for the respective language’s needs. Was the title of this article melodramatic clickbait? Maybe a little. Not a lot. There will be people who were making games in Unity who can make those same games in Godot without these issues mattering too much. Godot may be able to capture the lower end of Unity’s market. However, Unity’s recent focus on performance is a good indicator that there’s demand for it. I know that I certainly care about it. Godot’s performance is not just worse than Unity’s, it’s dramatically and systematically worse. In some projects 95% of the CPU load is in an algorithm which never touches the engine APIs. In that case, none of this matters. (The GC always matters, but we can use GDExtensions to avoid that.) For many others, good programmatic interaction with physics/collisions and manually modifying the properties of large numbers of objects are essential to the project. For many others, it’s important to know that they can do these things if they need to. Maybe you get two years into your project thinking it will barely need raycasts at all, then you make a late game decision to add some custom CPU particles that need to be able to check collisions. It’s a small aesthetic change, but suddenly you need an engine API and you’re in trouble. There’s a lot of talk right now about the importance of being able to trust that your engine will have your back in the future. Unity has that problem with their scummy business practices, Godot has that problem with performance. If Godot wants to be able to capture the general Unity market (I don’t actually know that it does want that) it will need to make some rapid and fundamental changes. Many of the things discussed in this article will simply not be acceptable to Unity devs. Discussion I posted this article on the r/Godot subreddit and there’s quite an active discussion there. If you’ve arrived here from somewhere else and would like to give feedback or be pseudonymously rude to me on the internet, that’s the place to do it. Acknowledgements _Mario_Boss on reddit for being the first to bring my attention to the Raycast2D node trick. John Riccitiello, for finally giving me a reason to do more research on other engines. Mike Bithell, for letting me steal his foreshadowing joke. I didn’t actually ask permission, but he seems too nice to find me and hit me. Freya Holmér, because nothing has kept me more entertained while writing this than seeing her complaining about Unreal doing physics in centimetres, and waiting until the moment she shares my horror upon discovering Godot has units like kg pixels^2. Edit: One of my jokes finally landed. Clainkey on reddit for pointing out that I mistakenly had nanoseconds where I should have had microseconds. This post is licensed under CC BY 4.0 by the author. Share Recently Updated Godot is not the new Unity - The anatomy of a Godot API call Contents Deepdive into performing a raycast from C# Unity Godot Can we do better? Caching query parameters What about GDExtension? A whole different approach - the RayCast2D node Timing it Have I cherrypicked? So why are we waiting for Godot? Okay, let’s fix it then! What can we do without deviating from the existing binding layer? What if we’re allowed to add additional APIs for C# and GDExtensions which aren’t GDScript compatible? What if we tear it all down and start again? Was the title of this article melodramatic clickbait? Discussion Acknowledgements - - © 2023 Sam Pruden. Some rights reserved. Using the Jekyll theme Chirpy",
    "commentLink": "https://news.ycombinator.com/item?id=37561762",
    "commentBody": "The anatomy of a Godot API callHacker NewspastloginThe anatomy of a Godot API call (sampruden.github.io) 189 points by amitmathew 13 hours ago| hidepastfavorite157 comments neonsunset 13 hours agoSo the decision for slow interop is caused by choosing to have parity between GDScript and C# because the former forces the types to be heap-allocated...and community might dislike a performance-friendly change which will break the parity in favour of C#.To be honest, if Godot embraces it becoming the Noah&#x27;s Ark for Unity developers and prioritizes C# over GDScript to improve performance, it could be good both for Godot and C# ecosystems, and more emphasis on the ability to use very rich selection of libraries written for .NET would not hurt either. reply dandellion 12 hours agoparentI went into Godot thinking I&#x27;d hate GDScript and switch to C# pretty fast because I already knew it. Now I love GDScript, if I ever need performance I&#x27;ll probably just go straight for C++. reply pjmlp 29 minutes agorootparentExcept that is the whole point of languages like C#, being able to be productive in high level code, and not needing to write C++ when performance is called for by providing all the features for low level code, unless in very niche caches where those features still aren&#x27;t enough. reply syntheweave 11 hours agorootparentprevThis is the Godot way of doing it. It encourages just getting your hands dirty if you need the engine to do something very specific. The architecture is made with the understanding that you eventually want to transition from script implementations to native ones, but that you&#x27;ll ship with the game being somewhere in between. And the architecture has been set up with generous amounts of glue(which is the overhead encountered in OP) to make the transition uneventful.And while it&#x27;s desirable to optimize the glue and make scripting fast, that&#x27;s also a much larger engineering project; Unity did it by spending big. reply Rapzid 9 hours agorootparentYou don&#x27;t need to spend big though to address the issues author highlighted.Pretty straightforward API changes. After a design is decided on just some tedious typing work. Heck, it could be dog piled easy enough. reply lynzrand 7 hours agorootparentprevC# strikes the balance between speed and dynamic features IMHO. Especially for games that need mod support (which is not that uncommon at all), using C# allows for the modders to easily interface with existing code, while still getting near-native performance thanks to the type system and JIT. C++ code in such case would be too rigid to be modified and hard to interface wth. reply plopz 9 hours agorootparentprevThe author of the article seems to say that even the c++ api version is a heap allocated dictionary, so you would still take the perf hit reply johnnyanmac 4 hours agorootparentprev>if I ever need performance I&#x27;ll probably just go straight for C++.if that&#x27;s my go to solution, why am I using Godot instead of Unreal? Or even a more mature C++ renderer framework like bgfx, or even Ogre3D? In the context of a decent 3D game, I&#x27;ll need that performance the lion&#x27;s share of the time, only scripting to move GO&#x27;s&#x2F;Actors around the scene. reply generichuman 3 hours agorootparent> if that&#x27;s my go to solution, why am I using Godot instead of Unreal?No revenue split would probably the biggest reason. Also Unreal could still pull a Unity since it is not FOSS (unlikely, but still).Mind you, Godot people are working on optimizing engine performance (see my other comment) because they kind of rushed Godot 4 release.> even a more mature C++ renderer framework like bgfx, or even Ogre3D?IMO this is actually the way to go & it is probably the way I&#x27;d do it if I had the time to build editor-like tools by myself. But these days content is the hard & time-consuming part of building games. Which means you want non-coder gamedev to be able to do as much as they can, which implies a powerful editor. Godot & Unity have the advantage of pushing the game development to the editor instead of the code. reply johnnyanmac 1 hour agorootparent>Godot & Unity have the advantage of pushing the game development to the editor instead of the code.Indeed, but even Unity isn&#x27;t known for its high performance 3D titles. It COULD viable do them, but not without a lot of work.That was supposed to be something that DOTS would address, to take out \"a lot of work\" and make that performance almost be taken for granted. But... well, I don&#x27;t want to go on that rant again. You can excuse some performance and optimize later, but the sounds of things from this article (with general sentiments even from diehard Godot fans that Godot isn&#x27;t ready or serious 3D development) makes it sound like that optimizing will in fact exceed the content creation time.I&#x27;ve seen enough of the guts to know that Godot will likely never make its own DOTS. At least, not its own Burst compiler to get around the c# issues. Likely not IL2CPP either. Fortunately, you don&#x27;t need that and can get maybe 80% of the performance by simply following some data oriented principles. More than enough for all but the most lofty AAA games (think GTA6 levels of scale).---->Mind you, Godot people are working on optimizing engine performance (see my other comment) because they kind of rushed Godot 4 release.that&#x27;s good to hear, and I get it (Unity rushed out tons of features itself. Including DOTS). I hope I can one day contribute to that effort myself. Would make my future game dev life much easier and would be a good way to give back. reply talloaktrees 11 hours agorootparentprevI think this is correct... you can even go with Rust instead of C++ reply Rapzid 11 hours agoparentprevHere is a discussion from the proposal repo I found with the topic of improving C# support: https:&#x2F;&#x2F;github.com&#x2F;godotengine&#x2F;godot-proposals&#x2F;discussions&#x2F;4... .One of the first comments was quick to point out that in a poll from back then 81% of users used GDScript primarily. They followed up with saying:> As far as I know, the only reason why Godot got C# support is because of Microsoft&#x27;s grant (and of course neikeq enthusiasm ).It&#x27;s my impression poking around that a lot of the community, at least at the time, had a hard time imagining that for Godot to gain serious mainstream adoption as THE OSS game engine, the project would need to focus on the what the development makeup would be at that future time and how they can attract those crowds. So, the user makeup of now may not look like the makeup of then.Further down though somebody points out:> Most people using Godot will make toy projects. That is the same for all other engines and developer tools as a whole. C# is crucial in serious projects, at least in 3D which is where I spend most of my time. Having first-class support for it (both in terms of tutorials in the documentation as well as in the engine) will go a long way towards making better-performing games for developers making serious projects.This post has been timely as I&#x27;ve been checking out Godot going through a massive 12 hour tutorial but using C# instead of GDScript; because of course. The entire time my mind kept wandering back to this quote I&#x27;ve seen posted various places:> C# is faster, but requires some expensive marshalling when talking to Godot.I kept thinking \"but why?! C# has excellent interop support and should be able to map directly over Godot&#x27;s types. That seems like it&#x27;s going to be a serious limitation long term..\".EDIT: That said, I&#x27;m very impressed with Godot in general and a lot of the C# integration. They are making pretty good use of source generators making a lot of string magic replaceable with type-checked property accesses. Signal creation and exporting properties to Editor members works this way; cool! I think they will end up working through the worst of these interop perf issues. reply Nition 9 hours agorootparentThere was a time when Unity leaned heavily on UnityScript. The manual defaulted to it, the sample projects all used it, most tutorials used it and so did most of the asset store.As Unity continued to change more from a hobbiest game engine to one used by professionals, things changed and C# became the default. reply Rapzid 9 hours agorootparentEasily room for both though.Maintaining two API binding layers is perfectly feasible with the attention and resources Godot is attracting.Unlike with Unity interested parties can jump in and lend a hand. You&#x27;ll have two groups(perhaps overlapping) maintaining the compiled and script bindings.Maybe GDScript could just get the concept of a struct and the differences minimized over time too. reply johnnyanmac 4 hours agorootparentFrom what I&#x27;m reading here and elsewhere, we&#x27;d need to do some fundamental mucking into the c++ layer as well, given some data oriented hostile practices.But yes, I think it&#x27;d be best long term to treat GDScript ad C# the way Unity treated GameObjects and DOTS. DOTS&#x27;s goal wasn&#x27;t to kill gameobjects, and in fact a lot of work went into bridging between the two with hybrid packages. I guess we&#x27;ll see if that kind of initiative gets some momentum (and I&#x27;d love to help out if so!) reply trinovantes 11 hours agorootparentprevThat poll seems highly biasedHow many people have tried godot&#x2F;gdscript, hated it, and never came back? They&#x27;re not the people who are answering the survey reply Rapzid 9 hours agorootparentYeah it&#x27;s def biased towards existing users in many ways.Not a terrible thing if you want to concentrate on their needs, but not the best questions for filtering ideas for expanding adoption (perhaps). reply tacotacotaco 7 hours agorootparentprevHow many users that are happy with gdscript are taking time to read vote in a GitHub polling posted about c# issues? reply badsectoracula 12 hours agoparentprev> community might dislike a performance-friendly change which will break the parity in favour of C#.The community will dislike a change that will drop GDScript, check the Godot subreddit submission (linked by the article), a ton of comments were pro-GDScript and how easy and fast (development-wise) it is to use.If that change can happen without affecting GDScript i doubt anyone would have any negative thoughts. There have been a couple of suggestions in the subreddit post on how that could happen and the Godot developers seem to be thinking about it. reply __loam 11 hours agorootparentIt is pretty shitty how people seem to be coming into Godot and immediately shitting on GDScript without trying to understand why it was built that way, and I say that as someone who shit on GDSCRIPT initially. reply johnnyanmac 4 hours agorootparentI understand it. it was built a certain way to support ease of use above almost anything else.That ease of use is important, but also may be holding back Godot&#x27;s true 3d capabilities. From a cursory glance as someone wanting to work on a decent scale 3D game, I&#x27;d need to treat GDSCript as I do Unreal&#x27;s Blueprints as of now: miniize use only to high level scripting and maybe UI&#x2F;Shader code. That&#x27;s not me saying that GDScript nor blueprints is bad, just that it gets in the way of my specific use case.If I can help speed up the engine underneath as a whole in the process: wonderful. I don&#x27;t necessarily want to disrupt GDScript for my convenience either, but like any refactor some breakage is inevitable. reply sankhao 9 hours agorootparentprevHow long are they supposed to spend to start appreciating it enough to tolerate 20x performance penalty to core engine functions ? reply danbolt 7 hours agorootparentFor some, a few days of GDScript usage. For others, after they convert their bottlenecks into C++.UE’s blueprints are much(!) slower than native code, but they’re still an incredible tool in the toolbox. GDScript is kind of the same; it’s covering a critical role of non-performance-sensitive glue.That said, I’d really like to see Value Types added, as well as these sorts of heap allocations addressed in Godot 4 at some point. reply Kiuhrly1 3 hours agorootparentprevI would bet that it doesn&#x27;t matter for 99% of scripts. By the numbers, Godot&#x27;s primary use case so far has effectively just been game jams. reply tourmalinetaco 9 hours agorootparentprevIt’s not about spending time, it’s about being ignorant of GDScript’s use cases. It’s like if people went into Python communities and shat all over it because C is faster. It’s true, but it misses the point that iterating in Python is leagues faster and is fast enough for a large section of projects. reply jayd16 7 hours agorootparentI guess the argument is that GDScript has a different use case than Unity&#x27;s C#? Seems like Unity devs know what a C# scripting engine is like and feel it covers the use case fine. To say they don&#x27;t understand the use case would mean it must be different somehow. Can you go into that? reply cfgyuiikhg 8 hours agorootparentprevC++ > C# reply Rapzid 8 hours agorootparentprev\"People\" will always do shitty stuff. Also, \"people\" will get overly defensive about challenges to their status quo and world view..That said, how prevalent is the former vs the later really? reply TazeTSchnitzel 12 hours agoparentprevGDScript is in a much better place than most scripting languages because the compiler actually has quite a lot of static type information. If they invest in unboxing optimisations and a JIT, it can definitely become competitive with C#. reply jayd16 7 hours agorootparentI can&#x27;t help but think this is delusional. Microsoft has put massive effort and funding into C# and I just don&#x27;t see how Godot, being much more niche, can reach that level of investment. reply TazeTSchnitzel 2 hours agorootparentIf they targeted the .NET VM they&#x27;d be able to make use of a lot of that investment. But they don&#x27;t really need to. There&#x27;s a lot of low-hanging-fruit for performance, and narrowing the gap is more important than outright parity. reply xmonkee 13 hours agoparentprevI&#x27;ve only briefly played around with Godot, but how does one get type-assisted autocomplete on variables and methods? Is there a type-hinting for the IDE? Kind of broke it for me. Maybe I should&#x27;ve tried C#. reply rsstack 12 hours agorootparentYou have to make sure you opt into the type-safe GDScript. It&#x27;s not easy. There&#x27;s an IDE feature that highlights lines that are not type-safe but the default color is a hard-to-see grey. I changed it to be red (\"Line Number Color\" is the one for untyped lines), and then fixed all typing issues. Then, autocomplete works as does cmd+click to go to definition. reply TazeTSchnitzel 12 hours agorootparentprevThere is optional static typing. If you write type declarations, the editor will not only give better code completion, but it will use type inference to find errors statically where possible, and the runtime will check for the remaining type errors. In this way it&#x27;s more sophisticated than almost all other dynamically typed languages! reply janosdebugs 13 hours agorootparentprevIn GDScript you kind of get code completion. Kind of. 50% of the time it doesn&#x27;t give anything useful. reply jackmott42 12 hours agorootparentprevIf you use C# then you can use Visual Studio or Rider or VSCode and get that reply Dudester230602 12 hours agoparentprevThe history is cyclical here it seems: https:&#x2F;&#x2F;blog.unity.com&#x2F;community&#x2F;unityscripts-long-ride-off-... reply shmerl 12 hours agoparentprevWhat about other bindings, like with Rust? reply meepmorp 12 hours agorootparentApparently (per the article) the C++ bindings use the same API as c# and GDScript. Presumably rust would have the same limitations, unless you wrote a whole new API just for it. reply shmerl 11 hours agorootparentSounds like something they can improve then. reply fidotron 12 hours agoprevWhen I first encountered Unity it was because there were performance problems on a Japanese only version of Bejeweled built with it and they needed someone to fix it. It was a classic: creating huge numbers of new objects for particle effects hammering the gc, and the fill rate of the then cutting edge device GPUs not being a match for their screen resolutions.Unity took several years, and a huge amount of investment, along with improvements in the wider ecosystem, before performance became much less of a concern for normal developers. A lot of the work they did around C# usage to achieve this is really surprisingly intense.In the near future I seriously think 2D (and some subset of simple 3D) game devs would be better off looking at Defold, and if you want to make immersive 3D just bite the bullet and move to Unreal. These days cost of preparing assets drastically exceeds coding time anyway. reply johnnyanmac 1 hour agoparent>and if you want to make immersive 3D just bite the bullet and move to UnrealSince Godot is right there, I&#x27;d rather try and bring up a current up and coming engine to somewhat parity rather than give up to Epic.Also, iteration is godawful in Unreal. Not everyone is trying to make Gears 6, so I wouldn&#x27;t mind a more lean 3D engine to work with if I have low-poly or simply not-dense scenes to manage.>These days cost of preparing assets drastically exceeds coding time anyway.Only if performance isn&#x27;t a concern. There&#x27;s a reason engineers are still paid more than the equivalent artist at studios. That performance is still hard to work with or around, even in Unreal Engine reply fayten 5 hours agoparentprevI started evaluating Defold this past weekend and I have been very impressed.The developer experience is very similar to Godot, especially if you work on your scripts with external editors. Everything that has to do with building and running the app is faster than Godot though.Initial builds with Defold are very fast, it has on demand hotreload, and very small binary sizes. The small binary size makes it well suited for working on web games.The Godot Editor is still a tad bit nicer to use, but I do like the fact that the Defold editor is written in Clojure. I imagine it makes extending it fairly quick.The main reason I see myself sticking with Defold over Godot is because scripting is done in Lua.The Lua ecosystem has a lot history in the game dev space making it easy to search for answers. Lua is trivial to extend and has multiple statically typed options that compile to it. Notable statically typed languages are Typescript and Haxe. Also Lua has a lot practical use in many domains outside of the Defold editor unlike GDScripts lock-in to Godot.Defold has no where near the size of the community of Godot, but the developers seem very active on the forum and GitHub.I need to play around with it more before I’m 100% sold. Both Godot and Defold are great looking choices for 2D. reply Fraterkes 11 hours agoparentprevWhat advantage do you think defold has over godot for 2d (apart from currently seemingly being more polished)? Theres lots of people complaining in this thread that GDscript is not serious enough as an engine language, but Godot at least has a decent c++ api. Defold only seems to support Lua (although you can extend the engine with other languages afaik). reply fidotron 11 hours agorootparentBluntly, getting too hung up on languages is procrastination. Lua is more than fine. (GDScript isn&#x27;t actually that bad, just completely esoteric). Similarly I wouldn&#x27;t get hung up on Blueprints or C++ when worrying about Unreal, partly because their C++ is so particular it&#x27;s extreme.Defold is basically a pile of interesting simple subsystems, especially around game logic, that in combination become surprisingly powerful, and the experience of the devs shows. It is one of those tools that even if you never use it again some of the ideas stay with you. reply droptablemain 12 hours agoprevI know Godot team is very committed to GDScript, but I find it hard to see it as anything but a toy scripting language. Seems like it would be a real mess with a big, complicated project.Godot w&#x2F; C# however is very nice and gives you a lot of flexibility with your architecture and code. reply headcanon 11 hours agoparentI too found it a bit cumbersome past a certain code size. A lot of games don&#x27;t necessarily need a lot of raw or complex code to run though, so the 1st class benefits of GDScript start to shine, like the native debugger, autocomplete, Godot-specific accessor syntactic sugar.Its hard to compete with an almost 25 year old battle-tested language, but given the domain and the team size, its a good product and worth pursuing IMO. With that said, I wouldn&#x27;t want to give up the ability to have C# interop when I need it, and I imagine many games would need to be rewritten in C# after a certain point. reply dandellion 11 hours agorootparentIt&#x27;s probably not ideal for large projects, but it adds huge value for small ones, and for people who are learning, and I&#x27;d expect probably even in larger projects if combined with C++, C# or Rust. It&#x27;s miles better than the joke Unity had that was called Boo. I also like it a lot better than Blueprints in Unreal while still remaining extremely simple, although I can see how Blueprints can be better for more visual people. reply janosdebugs 5 hours agorootparentprevI concur. I believe GDScript should be split from Godot itself and support should be added for third party IDEs. It has a lot of hard-coded things specific to Godot in it and it suffers from it. The built-in code editor has a very confusing layout and its built-in code editing functions will never be as powerful as providing a language server to a dedicated IDE. reply gustavus 12 hours agoparentprev> but I find it hard to see it as anything but a toy scripting language. Seems like it would be a real mess with a big,We are talking about GDScript not JavaScript.... Oh wait, the parallels are remarkableSimple language designed to be easy to learn: CheckTurns large codebases into a tangled lovecraftian mess of spaghetti code: CheckLikely to be someone&#x27;s first introduction to programming languages: CheckHonestly reading the criticisms of GDScript so far is convincing me more and more that Godot is quickly going to become the next big thing, especially if the developers can figure out how to get people to use it for thier first programming project. After all most people I know who got into development started because they wanted to make their own video game. reply eddythompson80 11 hours agorootparentAs others have mentioned, JavaScript also has had over 20 years of serious investment from the best minds in the industry to improve it. We now have V8, TypeScript and plethora of tools that make writing serious applications possible or even preferable. I seriously would take TypeScript over most languages any day.How likely would you say the same effort is gonna go into GDScript? reply johnnyanmac 1 hour agorootparent>How likely would you say the same effort is gonna go into GDScript?Hard to predict. We&#x27;re still in the web 1.0 phase of Godot and dealing with questions like why we made atag. We could get serious talent addressing the binding layers, we could simply have a few key bottlenecks addressed, or we accept GDScript as the slow path and diverge. It could also simply fade away, but many don&#x27;t want that, and I&#x27;m also not too interested in deprecating something many have used to launch full games.Personally, my goal is less about making a scripting language perform as well as c++ and more about having options to make sure people can port from GDScript to some faster binding if&#x2F;when they run into performance bottlenecks. At worst, maybe a migration tool may be needed. Hard, but more doable than any of the above overhauls. reply gustavus 11 hours agorootparentprevWell that&#x27;s a point a lot of other people have raised and is the one I am trying to make, Godot and the associated ecosystem could become like the next generations Javascript. Companies didn&#x27;t start investing an enormous amount of time and effort into improving JS because they wanted to, they did it because they adopted it and then ran into cases where they needed the performance.My point being that a lot of the concerns are not foundational to Godot and GDScript and can be improved if there is an investment. But that investment will come with adoption, not the other way around. reply krapp 11 hours agorootparentJavascript only got the investment it did because it was the only language that would run in a web browser, when the web suddenly became a billion dollar business and javascript developers were a dime a dozen. Godot is able to support multiple languages. The goal should be improving and expanding language support, not repeating the necessary evil of making Javascript the One True Programming Language. reply dandellion 12 hours agoparentprevThey&#x27;re commited to GDScript because it&#x27;s a great scripting language. I started with Godot expecting to switch to C# pretty fast because I&#x27;m already familiar with C#, but I was pleasantly surprised with GDSCript. And I&#x27;m a programmer with more than 10 years of experience. For people who are still learning it must feel like the difference between trying to learn to ride a bycicle vs learning to fly a boeing 777. reply mardifoufs 11 hours agorootparentNot being rhetorical or snarky, but what do you like about gdscript? reply janosdebugs 4 hours agorootparentAlso not the parent, but: It&#x27;s a very simple language. It looks like Python and behaves like TypeScript. It makes it easy for beginners to get into game development without having the visual overhead of a C++ or C#. reply danbolt 7 hours agorootparentprevI can’t speak for the parent, but @onready combined with the $ operator is a delight when making a prefab.I also very much like the way signals are declared compared to C#’s code generation.I’m also a fan of TypeScript, so being able to do gradual typing while prototyping is a huge plus. It’s going to be a big advantage if they ever get JIT.Everything being reference-counted, rather than fretting about GC pauses is comforting too. reply krapp 11 hours agorootparentprevGDScript not a great scripting language. It&#x27;s an adequate scripting language whose only benefit is native integration with the client, and guaranteed support in perpetuity. But there is nothing about it that&#x27;s better than any other scripting language. No one would choose to use GDScript as a general purpose programming language.Other scripting languages are also easy to learn. Vanilla Javascript is much easier than GDScript. And so is Lua. Kids learn coding in Lua. Even Python isn&#x27;t that difficult compared to C#. reply dandellion 11 hours agorootparentI&#x27;d argue it&#x27;s better than Boo and better than Blueprints. Maybe even JavaScript, because JS has made itself very useful as time has passed but as a small scripting language embedded in browsers it sucked.It&#x27;s only benefits are native integration with the client, and guaranteed support in perpetuity, and it&#x27;s easy to write, and easy to learn, and teach, it almost never surprises you in ugly ways (which is awesome), it comes with a good enough debugger, its library pretty much covers what you need to make your average indie game... But what did the romans... I mean, GDScript, ever do for us? reply johnnyanmac 1 hour agorootparent>its library pretty much covers what you need to make your average indie gameHey, if GDScript could run my idea for a 3D Open hub Action RPG at 60fps with little concerns for performance, I&#x27;d have no issue.But alas, my desire for a proper binding or directly tapping into the c++ comes out of necessity, not some pride as a \"real game developer\". It&#x27;s not impossible for GDScript to one day become the WebASM of Godot, but we both know that WebASM was, and still is, decades in the making. I imagine it&#x27;s simply faster to forge a fast path API and let the slow path work for non-performance intensive games. reply krapp 11 hours agorootparentprevIt&#x27;s like you believe GDScript is the only language for which any of these features are possible. Other languages are also easy to write, learn and teach. Integration with the client is an implementation, not language, issue. Support is a business issue. Debugger quality is orthogonal, and you can make a game in assembly if you want (Railroad Tycoon.) reply dandellion 11 hours agorootparentThose are your words, not mine. The thread was talking about ditching GDScript in favour of C# and comparing it to other engine scripting languages. You want to go with the conversation on a trip to the mountains, you go alone. reply krapp 10 hours agorootparentYou&#x27;re being weirdly aggressive here, and you have yet to actually address any specific point I&#x27;ve made in any of my comments, much less defended your own, so I&#x27;m going to call it a night. replyLucasoato 12 hours agoparentprevThey should remember that even Unity had to deprecate the Boo language to go forward :) reply dandellion 11 hours agorootparentNot sure Unity is a good role model...Also, having used both Boo back in the day and GDScript in Godot more recently, I&#x27;d say that one big difference between the two is that Boo was really bad, and GDScript is actually pretty good. reply eddythompson80 11 hours agorootparentWhat&#x27;s wrong with Unity from a technical prospective not marketing&#x2F;business prospective? reply johnnyanmac 1 hour agorootparentUnity got a stigma for delivering half baked features and dropping support for existing features without a replacement. The epitome of this was where in Netcode, where I believe they had no officially supported solution despite the engine having 2 different packages which were both half-baked, all while the DOTS variant was far from release.I don&#x27;t think Godot should go that route. reply cfgyuiikhg 8 hours agorootparentprevIt was fat and glitchy from the get go. reply mattigames 12 hours agorootparentprevBoth Boo language and UnityScript were deprecated (the later was oftenly mislabeled as JavaScript, creating quite a confusion between newbies) reply rybosworld 12 hours agoprevOne thing that stands out as odd to me is the small selection of really \"good\" game engines.Unreal seems to have the most features and the steepest learning curve.Unity is comparatively easier to learn. Fewer features but C# support is a big plus. The major downside is that the company is run by incompetent MBA brains.Godot is open source (awesome!) but doesn&#x27;t seem ready for prime time. It doesn&#x27;t have as many features as the other two and the commitment to GDscript seems... odd.If I were picking a game engine today, it&#x27;d be between Unreal and Godot, but I&#x27;m not sure I&#x27;d feel great about either choice. reply johnnyanmac 44 minutes agoparent>One thing that stands out as odd to me is the small selection of really \"good\" game engines.not much odd about it. 3d game engines are a large endeavor, somewhere between making a Large desktop application and making a full blown OS. I&#x27;d say the largest ones are comparable to making a web browser engine or even a full blown IDE suite.There&#x27;s also some slight snide in that there is no truly \"good\" game engine. Just ones that teams put up with enough to get across the finish line. The sheer scale of the engines and the nature of them being developer suites means running into edge cases is inevitable.There are several mature 3D solutions out there, but the ones mentioned have the 3 largest communities and are strongly supported. But to throw out a few others- Open3dEngine (O3D3), and fork off Lumberyard, which is a fork off CryEngine, is probably the biggest off-the-shelf competitor to UE when it comes to delivering AAA level games.- Stride is an engine mentioned often in discussions, and it has a similar feel to Unity. But it simply isn&#x27;t as mature as Godot and lacks some platform support like Mac- UPBGE is the spiritual successor to the defunct Blender Game Engine, with a similar pitch: create your game without ever leaving your modeling suite.-Finally, while not a fully featured game engine, I do want to give some note to Ogre3D. It&#x27;s one of the oldest and most battle-tested graphics libraries out there, is MIT open sources, and is there for the kinds of developers who rolled their own engine&#x2F;framework and simply used Unity as a rendering backend. Ogre doesn&#x27;t include physics nor input (nor an editor), but it&#x27;s really good at throwing your hand rolled game logic and giving it something visual. I could make similar other recommendations for stuff like Raylib and BGFX, but Ogre still being supported after over 20 years is admirable.There aren&#x27;t tons of choices, but there are choices out there if you are willing to get your hands dirty. reply jayd16 7 hours agoparentprevIt really depends on what your game&#x2F;team needs. If you&#x27;re trying to make a match-3 then Unreal is maybe not the best choice. If you&#x27;re trying for competitive cutting edge graphics in an engine that is mostly done so you can actually focus on the graphics, then Unreal is top of list.I think its probably better to look at Unreal as state of the art but licensable. Other big game companies have cutting edge engines but they&#x27;re proprietary and not available to license. reply rudedogg 7 hours agorootparent> focus on the graphicsUnreal isn&#x27;t good just because of the graphics. It has loads of tools built in, and that&#x27;s where most of the value is in my opinion.This video will give you an idea: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=k2IP5DYQ0-0 reply zzo38computer 7 hours agoparentprevI think that which game engine will be \"good\" depend on manyu things including what kind of game, what computer system, FOSS, licensing, other features, etc. reply jay_kyburz 11 hours agoparentprevI&#x27;m thinking about going back to 2d. If Factorio and Slay The Spire can be smash hits in 2d, perhaps I can as well :)I&#x27;m a fan of Love but I have to admit the amount of Lua I wrote for just my prototypes did start to get a little scary.I might have a play with raylib. reply cmovq 9 hours agorootparentInterestingly both of the games you mentioned use custom engines. I think people overestimate the difficulty of rolling out your own 2D engine. reply mnk47 6 hours agorootparentprevIf you like or don&#x27;t mind using Go, check out Ebiten as well. I think it&#x27;s similar to Raylib in spirit, but it also supports Nintendo Switch, iOS and Xbox. IIRC it uses native graphics APIs (Metal for MacOS, DirectX for Windows) while Raylib just uses OpenGL. reply timmytokyo 12 hours agoprev>Unity has spent the last five years working on speeding up their scripting with crazy projects such as building two custom compilers, SIMD maths libraries, custom collections and allocators, and of course the giant (and very much unfinished) ECS project. It’s been their CTO’s primary focus since 2018.That would be Joachim Ante, who is no longer CTO. He&#x27;s \"on sabbatical\" and hasn&#x27;t contributed any posts to the forums for over a year. (He used to be quite active on Unity&#x27;s forums.) The entire leadership of the ECS&#x2F;DOTS team has resigned. reply johnnyanmac 38 minutes agoparentHe wasn&#x27;t just active on forums, he was actively developing on Unity itself, in the weeds with all the other engineers. He arguably got a bit too involved at times, but it was very admirable for a \"executive\" who could retire on the interest of his shares in Unity to still take so much care in the product. I can&#x27;t say that about many 10+ year old company founders, let alone ones that went public.And of course the moment he silently stepped out, Unity rolls all this BS out in less than 6 months. Joe wasn&#x27;t just champion of the tech, he was likely one of the last of the old guard up top keeping things focused on the actual product instead of how to extract infinite monies. reply pests 12 hours agoparentprev> the entire leadership of the ECS&#x2F;DOTS team has resignedRecently because of the drama or unrelated? That’s sad to hear as I was very excited on that effort. reply johnnyanmac 35 minutes agorootparentNot recently, but there was a steady drain of talent. The two heads of the DOTS initiative were gone by May of this year and the CTO, and the last original Founder of Unity, and arguably largest champion of DOTS, silently stepped down in April when introducing the new CTO. Johacim&#x27;s official status is apparently \"sabbatical\" (I&#x27;m sure he owns way too much stock and other sway to be hoisted out the traditional way). reply timmytokyo 12 hours agorootparentprevI don&#x27;t think it was due to the recent drama. (Although who knows, maybe they knew what was coming.) Mike Acton and Andreas Fredriksson left back in March 2023 I think. They posted to twitter.EDIT: It was in May. reply zamalek 12 hours agoprevThis dictionary stuff is truly bizarre. I have been playing around with GDExtension and Rust - specifically the meshing interface. This dictionary stuff shows up there. Not only couldn&#x27;t I figure out why (the GDScript angle makes sense in hindsight), but it also makes the API virtually impossible to discover: you have to resort to reading things up in the documentation.As much as I love Godot, the GDExtension interface really has no redeeming qualities and needs to go back to the drawing board. reply MrLeap 12 hours agoprevI didn&#x27;t read anything that indicated a hard block to creating new methods that bypass the binding layer? All the layers are open source.Unity has their fair share of bolted on methods with a \"NonAlloc\" suffix. Seems obvious to prioritize raycasts &#x2F; intersection &#x2F; collision code to receive this treatment.I&#x27;ve noticed a few lower lift things I&#x27;d like to submit PR&#x27;s for, just to see how&#x2F;if I can help. reply johnnyanmac 27 minutes agoparentI hear the recommended approach is to write your own module in the engine level code. But that means you need to compile the engine everytime you change your module. Possible, but you are losing a lot of iteration unless you are experienced in rolling in your own physics.Nothing stop you from modifying the engine code, but the problem lies in the binding layer, not the engine perormance (as you see in the blog, the raw engine performance for a raycast is about the same as Unity). That&#x27;s a bit trickier to fix without intimate knowledge of such bindings, and you&#x27;ll likely break a lot of GDScripting support (so by extension, c# scripting) in the process. reply gabereiser 12 hours agoprevNever underestimate the power of open source game developers. If we could do it with XNA, we’ll do it with Unity. If we have to tear the place down and rebuild it on a rust il-compliant vm instead of mono, we damn sure will [0][0] https:&#x2F;&#x2F;crates.io&#x2F;crates&#x2F;netcorehost*edit* everything in the article is on point. Function pointers as binding glue sucks. There’s tons of optimizations to be done in the engine for sure. However, having a robust C# api is on top of list since it was introduced. I do think Godot 5 should make a hard choice and just support C# instead of GDScript. reply Waterluvian 12 hours agoprevI tried using Godot for Web game toy projects and unfortunately the base file size is like 12MB and closer to 40MB if you use C#.I think Unity was like 3MB with C#Not a big deal for a lot of cases. Don’t let it deter you. But for my specific target it was a problem.To be honest I’m not sure how Unity can be so small. Does it compile the C# or something avoiding the need for a beefy runtime? reply amitmathew 12 hours agoparentThere&#x27;s a good post on Reddit of a dev creating small web exports using custom builds: https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;godot&#x2F;comments&#x2F;16lti15&#x2F;godot_is_not.... reply Waterluvian 12 hours agorootparentThanks. I first misread the C++ part but it sounds like they have some way of paring down the runtime when using GDScript. I’ll have to seek that out. reply Hamcha 12 hours agoparentprevDunno if it was your case but yes, Unity has had AOT compilation since forever with both ILCPP and (more recently) Burst. reply Scalene2 12 hours agoprevSo, what I&#x27;m gathering is if you&#x27;re bothered by the Unity (either a developer, publisher, gamer) developments, and wish Godot was a better alternative than it is, contributing to the project (by donating either time or money) is probably necessary to get there. reply zerr 13 hours agoprevHow usable is Godot&#x27;s native&#x2F;C++ API for actual game programming, akin to Unreal? (never mind the hot reloading). reply janosdebugs 13 hours agoparentIt&#x27;s a lot clunkier than Unreal. You&#x27;ll have to build a .so&#x2F;.dll using scons or cmake (good luck finding a CMakeLists.txt that actually works) and you have to register all types manually. You will also not be able to work around the issues with the APIs mentioned in the article around dictionaries AFAIK. There is no header tool and no autogenerated code for your classes. Oh, and reloading the DLL on Windows requires an editor restart. Plus I haven&#x27;t found a good way to debug load failures either apart from starting the editor from a console window.I ended up sticking to GDScript in the end, but the typing system is very basic and the code can get very messy quickly. Plus, as far as I could tell, there is a race condition between signals and the process function, but I need to debug that more thoroughly to be sure. reply lainga 12 hours agorootparentI used GDExtension extensively on a previous project, and I ended up just installing the .so, launching Godot headless, and immediately exiting, as part of my build script. To catch anything that might break extension loading. reply rsstack 12 hours agorootparentprevAre you using call_deferred from your signal handlers? reply janosdebugs 5 hours agorootparentNo. Should I? reply badsectoracula 11 hours agoparentprevTBH if i was making a game using Godot i&#x27;d just be modifying the engine itself to add any \"heavy\" functionality (the nodes system would most likely help here) and expose custom high level APIs to GDScript. IMO scripting languages should be used to script the behavior of a game: i.e. tell the engine what to do, not how to do it (which should be written in native code).From the subreddit submission about this blog post (linked in the post itself) it seems this is something some people already do.The GDExtension API seems something to use only if you want to write C++ extensions&#x2F;plugins to be usable by other people, but i don&#x27;t see much of a point in restricting yourself to it if you are making your own games. reply generichuman 12 hours agoprevResponse to the article from godot dev: https:&#x2F;&#x2F;reddit.com&#x2F;r&#x2F;godot&#x2F;comments&#x2F;16lti15&#x2F;godot_is_not_the...TL;DR is that they know this & working on it reply lainga 12 hours agoparentI might throw in a bit of gossip here - many who were following the development of GD 4 last year (myself included as I was using it :) ) thought it was kind of rushed out in time for GDC 2023 in March. And immediately after GDC, Godot 4.1 followed (and now rc&#x27;s for 4.2), with lots of fixes.So on the one hand, yeah, 4.0 lacks polish, and I certainly have felt some strong feelings working with GDExtension... but I also believe there was a reason for that, and I believe the team when they say they&#x27;re working on improvements now reply CreepGin 11 hours agoprevThe biggest problem for me in moving to Godot is the editor tooling. I find the @tool and EditorPlugin workflow to be very finicky. Editor restarts were needed to refresh&#x2F;correct changes from @tool scripts. And a lot of what I needed to do in the scene view (context-free) required hacky solutions. Not to mention, they changed a lot of related APIs in Godot 4, most of what I google are still from Godot 3 and don&#x27;t work in Godot 4 (no backward compatibility).That said, I&#x27;m still enjoying Godot, the only fun part in the past week. reply 0x_rs 8 hours agoparentIt&#x27;s made great progress in this regard since 3.x, and it&#x27;s an an extremely powerful system, but dealing with some @tool intricacies can very quickly make you waste days trying to figure out what&#x27;s going on. My greatest annoyance regrettably lies in one of Godot&#x27;s most convenient features, custom resources, that are so extremely tedious to use with @export from the editor in any meaningful, useful way that I&#x27;ve personally given up on it for the time being, their lifecycles even avoiding _init() cross well beyond the bug behavior territory and GDscript permits no alternative approaches code-wise that aren&#x27;t absolute monstrosities of jerryrigged variadic functions (not supported except for a few API calls resembling them) jumping one into another or endless walls of parameters you get lost into. reply tacotacotaco 6 hours agoprevI am so glad that Juan et al are leading Godot and not me because frankly I would be offended by all of this. It hasn’t been a week since the Unity fiasco and all I see is post after post from people complaining that Godot isn’t c# enough. This is like showing up for dinner and insulting cooking before you’ve even tasted the meal. There are definitely opportunities to improve Godot but there are more constructive ways to contribute than drawing a new floor plan for the house on the table cloth while the first course is being served.Have you even made a small game with gdscript? If Godot doesn’t meet your needs then there are dozens of other game engines to choose from, some are native c#. reply johnnyanmac 18 minutes agoparent>Have you even made a small game with gdscript? If Godot doesn’t meet your needs then there are dozens of other game engines to choose from, some are native c#.This dismissal of honest performance benchmarks is why I&#x27;m glad you&#x27;re not leading Godot. We&#x27;re not talking about some esoteric cloth simulation code being nitpicked. These are core architectures issues costing you 2x performance minimum, simply due to how the c++ and C#&#x2F;GDSctipt&#x2F;GDExtension layers talk.Take this as a warning, not a dismissal. Unity went down this exact path and we see how viable it became for large scale game development. I sympathize with you for those who are outright trashing the GDScript for being a scipting language, and I do wish those arguments would die down (it&#x27;s simply language wars). But there are definitely fixes here that would improve all diferent bindings, even if they never diverged (which IMO, they should).>This is like showing up for dinner and insulting cooking before you’ve even tasted the mealcooking is subjective (mostly), performance is not. This is more like asking why the cook is trying to carefully drain out the water with a loose lid instead of using a strainer. Sure, it may work for the cook and they&#x27;ve done it all this time, but I&#x27;d rather give a safer solution that won&#x27;t burn them long term, or spill out excess food. reply wkat4242 9 hours agoprevIt&#x27;s not Godot if you don&#x27;t have to wait for something. reply KMcPheeters 11 hours agoprevWhen the performance of Godot&#x27;s physics engine has been mentioned before I&#x27;ve seen https:&#x2F;&#x2F;github.com&#x2F;godot-jolt&#x2F;godot-jolt pointed to as a drop in more performant solution.Haven&#x27;t tried it in a project yet myself reply Dudester230602 12 hours agoprevIf Godot has a first-class C# support then why `Godot.Collections.Dictionary` exists when there is already `System.Collections.Generic.Dictionary`?https:&#x2F;&#x2F;learn.unity.com&#x2F;tutorial&#x2F;lists-and-dictionaries reply TazeTSchnitzel 12 hours agoparentBridging collection types between languages is never easy. Apple only manage it because they control the compiler and runtime on both sides, and that still comes with performance pitfalls. Allowing both to exist and telling people when to use one or the other is not only easier to implement, it has more predictable performance and gives the user more control. reply worble 12 hours agoparentprevhttps:&#x2F;&#x2F;docs.godotengine.org&#x2F;en&#x2F;stable&#x2F;tutorials&#x2F;scripting&#x2F;c...> The main difference between the .NET collections and the Godot collections is that the .NET collections are implemented in C# while the Godot collections are implemented in C++ and the Godot C# API is a wrapper over it, this is an important distinction since it means every operation on a Godot collection requires marshaling which can be expensive especially inside a loop.> Due to the performance implications, using Godot collections is only recommended when absolutely necessary (such as interacting with the Godot API). Godot only understands its own collection types, so it&#x27;s required to use them when talking to the engine. reply bakugo 12 hours agoprev> That’s right, our raycast is returning an untyped dictionary.This is probably the biggest red flag for me, why would you use an untyped dictionary for something as essential and commonly used as a raycast result? reply Narishma 12 hours agoparentPresumably because GDScript doesn&#x27;t support structs. reply TazeTSchnitzel 12 hours agorootparentIt does support classes with strongly typed fields, though, which are the next best thing. It&#x27;s just a poorly designed API, it&#x27;s not a reflection of language limitations. reply harpiaharpyja 11 hours agorootparentExactly. Why can&#x27;t it return a RaycastResult or whatever. reply nextaccountic 12 hours agorootparentprevIn this sense it&#x27;s like Lua right? And people don&#x27;t complain (much) about using Lua in gamedev reply krapp 11 hours agorootparentprevIf you want \"structs\" in GDScript, use Resources.I know it&#x27;s not the same thing at all but it&#x27;s as close as you&#x27;re going to get. reply jay_kyburz 12 hours agoparentprevThere are red flags everywhere.I think Godot has this really nice looking editor, and the nodes look nice, but the engineering foundations are just not there.I think a lot of people start Godot projects but not many finish them because they run into these WTF issues. reply syntheweave 11 hours agorootparentGodot has always made a point of prioritizing features and simplicity of modification over speed. In a lot of instances it does just ignore low-hanging opportunities for performance because the simple thing works and needs less code, which isn&#x27;t always useful, but means it&#x27;s easier to replace.Like, my reaction to the blogpost is, \"oh, OK, if raycasting nodes are fast, and I need a lot of raycasts, then worst case, I will introduce a custom node designed to do 300 raycasts in one call or whatever.\"The corresponding WTF in Unity tends to be \"sorry, no code access, SOL\" so the inclination of a Unity dev is that they need as direct an API as possible and it needs to be as fast as possible. There are tremendous numbers of shipped Unity games that don&#x27;t trust the engine to do what it&#x27;s supposed to and just use it as a HAL, and so there&#x27;s a culture clash in approach which makes Godot the wrong tool for a dev that wants that kind of system. reply TheMagicHorsey 12 hours agoprevCurious if anyone here has tried Bevy (Rust game engine).Godot has a beautiful editor and great tooling for a free engine ... but I worry about the choice of C# and GDScript as the expected way for devs to interact with the project (what if performance is a concern or you need to develop some low-level features).Another post in this thread said the C++&#x2F;native interface was not great and difficult to work with. And C++ even in the best of times is not my favorite thing to work in. reply Daegalus 12 hours agoparentI looked into this, but it seems most gamedev communities I asked, say Bevy isnt great, and everyone seems to be recommending Fyrox instead. No clue if it actually is, just parroting what I have been told.Either way, I don&#x27;t think either is ready for prime time as its still early development. reply MegaDeKay 12 hours agoparentprevBevy came up on &#x2F;r&#x2F;rust_gamedev a little while back. The general sentiment seemed fairly negative, with comments that you either do things Bevy&#x27;s way or the highway.https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;rust_gamedev&#x2F;comments&#x2F;13wteyb&#x2F;is_be... reply Narishma 11 hours agoparentprevBevy is even more immature than Godot. It doesn&#x27;t even have an editor and its reliance on webGPU leaves a lot of platforms in the dust. reply jms55 9 hours agorootparentAs a Bevy dev - what platforms? We support Vulkan, DirectX12, Metal, WebGPU (with a bit more limited features), and WebGL2 and GLES 3 (with a decent amount more limited features and performance). reply pjmlp 24 minutes agorootparentPlaystation for example. reply Narishma 7 hours agorootparentprevOlder devices. GLES 2 and DX9 to 11 mainly. reply jms55 4 hours agorootparentI double checked, and our rendering library _does_ partially supports DX11, as well as Angle, which lets you run the GLES3 backend on top of Angle on top of GLES2&#x2F;DX11. They tend to be fairly buggy and incomplete, though. Arguably only usable for simple 2D games.There&#x27;s been no real demand or contributors to improve it, but it&#x27;s open source so there&#x27;s no reason you or anyone else could stamp out the bugs https:&#x2F;&#x2F;github.com&#x2F;gfx-rs&#x2F;wgpu#supported-platforms. replygustavus 13 hours agoprevThe thesis of the article appears to be> However, one major issue holds it back - the binding layer between engine code and gameplay code is structurally built to be slow in ways which are very hard to fix without tearing everything down and rebuilding the entire API from scratch.If there is one thing I&#x27;ve learned from the prevalence of Java, or JavaScript, is that performance problems will get sorted out fairly quickly if people start flocking to a tool. reply mdbauman 12 hours agoparentIt&#x27;s for this reason that I appreciate this article, even though it has a (playful, well-intentioned) negative tone toward Godot which is a project I donate to.This is wonderful criticism! It&#x27;s thoughtful and well-researched. Hell, even I&#x27;m inspired to finally dive into Godot&#x27;s internals, which I&#x27;ve yet to do despite following the project for several years. I hope this inspires even more contribution and constructive criticism. reply lainga 12 hours agorootparentIt feels like a breath of fresh air to me. Yes, random people, come in and start using Godot, the more eyes are on this project the better. They&#x27;re articulating gripes I have had but couldn&#x27;t express. It would be great if, like JS, all the attention became an impetus to make Godot really fast. reply amitmathew 10 hours agorootparentprevTotally agree, that&#x27;s why I submitted this article. I&#x27;m personally invested in seeing Godot become successful, but this type of constructive criticism is great for the ecosystem (even though some people get very defensive). I think it&#x27;s exciting that there&#x27;s so much room for improvement and shining light into Godot internals is a great way to expose what needs to be done. reply AndriyKunitsyn 13 hours agoparentprevJavaScript is fast because it is used by every person every day, so lots of smart people from big companies put lots of thought how to make it fast, and I still get a feeling of overwhelming anger when browsing the new web Reddit on a 4-years old phone. Godot is a game engine supported by a community (not big companies).They are not really comparable. And sometimes, there are still performance ceilings imposed by a nature of the tool. reply mschuster91 12 hours agorootparent> so lots of smart people from big companies put lots of thought how to make it fast, and I still get a feeling of overwhelming anger when browsing the new web Reddit on a 4-years old phone.That&#x27;s more the problem of Reddit&#x27;s code being a pile of bull dung. Someone high-level in there decided to ditch the \"old\" API for the \"redesign\" in favor of GraphQL and as everyone who has ever worked with GraphQL is likely to have discovered, GraphQL is a hell in itself.And because whoever was moronic enough to call that decision doesn&#x27;t want to admit they have fucked up and Reddit can&#x27;t&#x2F;doesn&#x27;t want to afford maintaining two distinct APIs, they decided to rather shut down the old, working and performant API.Reddit is being killed by corporate bullshit. reply gustavus 12 hours agorootparentprev> JavaScript is fast because it is used by every person every day, so lots of smart people from big companies put lots of thought how to make it fast, and I still get a feeling of overwhelming anger when browsing the new web Reddit on a 4-years old phone. Godot is a game engine supported by a community (not big companies).I&#x27;m glad to see that tools can exist in only one of 2 states, massive world wide adoption and support by big corporate, or a couple of devs screwing around. I wonder if it is possible for a tool to transcend this limitation?If only there was a way for multiple people to use a tool, push for it, enhance it and for it&#x27;s adoption to grow and spread to large pieces of the community eventually gaining corporate support. But nah, things like that don&#x27;t happen. After all if there is one thing I&#x27;ve learned a scripting&#x2F;game&#x2F;animation engine that is easy enough to use and is flexible is something no one really needs anyone, a product like that would probably disappear in a FLASH, after all. reply Jensson 11 hours agoparentprevPython is still slow today. I&#x27;ve worked with performance sensitive python at Google, there is no way to make it fast no matter what tool you use except to rewrite it in a lower level language.Javascript is fast since it is such a simple language that is easy to optimize, more complex runtimes wont see nearly the same amount of gain from optimization efforts. reply guideamigo_com 13 hours agoparentprevJavascript still remains slow. Java is fast enough that it doesn&#x27;t matter. However, no one will use Java to build Linux kernel. reply pjmlp 20 minutes agorootparentMostly because Linus won&#x27;t accept anything other than C on his beloved kernel.Sun had experimental support for Java drivers on Solaris, Android has support for writing drivers in Java, Android Things only allowed for Java written drivers. reply viraptor 13 hours agorootparentprevJavaScript is very fast if you want&#x2F;need it to be. Emscripten&#x2F;asm.js approach is in the same category as native for example. Your simple code won&#x27;t be too fast, but not all code needs to be.Also, have you heard of JavaOS? https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;JavaOS reply wiseowise 13 hours agorootparentprevDefine slow. JS is plenty fast for a dynamic language. reply foderking 13 hours agorootparentprevJavaScript isn&#x27;t slow reply kevingadd 12 hours agoparentprevSolving some of those performance problems can also require a massive amount of engineering resources and investment, though. For example the way browsers (generally) overcame interop overhead was by self-hosting almost everything in JavaScript or making things into JIT intrinsics. That&#x27;s a tall order for something like Godot with two languages. reply karaterobot 13 hours agoprevThis matches up with what I&#x27;ve heard from people with experience in both Unity and Godot. As convenient as it would be if Godot was ready to swap in for Unity, it&#x27;s got some significant issues that would make that difficult to unfeasible for many people. reply CodeCompost 13 hours agoparentNobody suggested that Godot was on par with Unity. The hope is that resources get diverted away from Unity towards Godot so that it can develop into a top-notch game engine. It&#x27;s time to stop waiting and start working on Godot. reply fluxem 12 hours agorootparentYou clearly weren&#x27;t on reddit. Everyone there will urge you to switch to Godot even though your game is almost finished and Godot doesn&#x27;t have feature parity.It reminds me of the linux debate:- A: I hate Windows telemetry- B: Then switch to linux. It&#x27;s as good- A: But it doesn&#x27;t have a good video editor.- B: You should develop your own video editor and share it with the community- A: Ughh... I just want to edit my videos...I use linux and love it, but there are users who need Windows or Unity. reply sockaddr 12 hours agorootparent> You should develop your own video editor and share it with the communitySeems like a fun weekend project reply riwsky 12 hours agorootparentWe think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer reply hutzlibu 12 hours agorootparentprevIt is probably not on par with the professional video editors and it is not very intuitive to get started, but blender has a build in video editor, that should cover most basic needs.Otherwise sure, that misguided evangelicalism is hurting trust in open source software, when people claim it is ready, where it isn&#x27;t. reply karaterobot 12 hours agorootparentprevI&#x27;ve seen comments here on HN that make me believe some people think Godot is on par with Unity, or that it will be soon. The title of the linked article also acknowledges that some people in the world at least have a misapprehension to that effect. reply rhtgrg 12 hours agorootparentAll you have to do is look at games made with Unity and compare with games made with Godot.The proof, as they say, is in the pudding. reply jayd16 12 hours agoprevFunny that they have to use a custom dictionary setup when C# supports dynamic types.>Mixing fast and slow APIs as discussed above would leave us with headaches for decadesMeh. Unity has allocating and \"NonAlloc\" methods, even on the raycast API in question here. It&#x27;s a really minor issue.Sounds like Godot will need more time to bake before its perfect but we already knew that. reply robertlagrant 13 hours agoprevI&#x27;m not sure I understood everything here, but I think the tl;dr is that Godot has a powerful new rendering technique called \"foreshadowing\". reply panzagl 13 hours agoparentThat why it seems slow- if you want the gun to fire in act 2, you have to show it in act 1. reply CobrastanJorji 11 hours agoparentprevIt&#x27;s new in Godot 4.1.1. When enabled, if two opaque polygons are between the user and a light source, the shading can reflect their relationship. reply xg15 13 hours agoparentprevcue theatrical thunder and lightning reply anothernewdude 13 hours agoprev [–] Sounds a lot like Unity to me. reply hiccuphippo 13 hours agoparentYes, I was under the impression that Unity was also slow and people who use it don&#x27;t particularly care because it&#x27;s still fast enough for their purposes.But with so many eyes on Godot now, I can only see it getting better. reply debacle 11 hours agorootparentUnity just released first class support for ECS which will greatly speed up the engine for most users. reply spookie 12 hours agoparentprevI&#x27;m also concerned with the approach the dev is taking on their game. That&#x27;s a lot of raycasts. reply bowsamic 13 hours agoparentprev [–] Not really. Unity has great performance replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article compares Unity and Godot game engines, focusing predominantly on API call performance along with memory usage efficiencies.",
      "It scrutinizes the performance of raycasting in Godot, offering code examples and benchmarks, and suggesting improvements for memory management and API design.",
      "The author hints at potential solutions, such as a complete overhaul of the API or possibly using C# instead of GDScript, underscoring the need to enhance these aspects to compete effectively with engines like Unity."
    ],
    "commentSummary": [
      "Discussions focus on Godot game engine's performance, scripting languages (GDScript and C#), and compare it with other game engines like Unity, Unreal, and Lumberyard.",
      "There is an active critique and support, providing insights into Godot's limitations and potential for enhancements.",
      "Some participants proposed alternative game engines and areas where Godot could be improved."
    ],
    "points": 187,
    "commentCount": 153,
    "retryCount": 0,
    "time": 1695069810
  },
  {
    "id": 37564952,
    "title": "Your WiFi Can See You",
    "originLink": "https://mrereports.substack.com/p/your-wifi-can-see-you",
    "originBody": "Mr. E Reports Subscribe Sign in Discover more from Mr. E Reports Exposing the intelligence community and their crimes against humanity. Subscribe Continue reading Sign in Your WiFi Can See You And it's so much worse than you think MR. E SEP 18, 2023 12 2 Share When police suspected Danny Kyllo, an Oregon man, of growing cannabis in his home they drove to his house with a thermal imaging device to scan it. They found hot pockets in the house, which were used to obtain a search warrant and subsequently bust Kyllo. Fortunately, a 5-4 Supreme Court decision ruled the scan an unlawful search under the Fourth Amendment, requiring a warrant the police did not obtain. Score one for privacy, but the government is about to have a far more controversial and dangerous tool at its disposal to monitor what’s going on inside your home. Unlike a thermal imager, this device is already in your home – and you put it there. How It Works WiFi is electromagnetic waves in the 2.4 and 5 GHz ranges. It’s the same thing as the light you see, only it can penetrate walls due to its much longer wavelength. Just like light (and echolocation) these waves also reflect off various surfaces and, when reconstructed properly, can be used to create an image. Development of this technology goes back at least as far as July 2005, where researchers claimed at an IEEE Symposium that they had created an ultra-wideband high-resolution short pulse imaging radar system operating around 10 GHz. The applications for which were explicitly for military and police use, providing them with “enhanced situation awareness.” A few years later, in 2008, researchers at UC Santa Barbara created an initial approach for imaging with WiFi that they presented at IEEE ACC 2009. A year later they demonstrated the feasibility of this approach. The Race is On Sensing the potential of this new surveillance technology, other researchers began piling on. Progress was initially slow but, in 2017, two researchers in Germany demonstrated the ability to do WiFi imaging using techniques borrowed from the field of holography. According to Philipp Holl, an undergrad student and lead study author who worked with Friedemann Reinhard of the Technical University of Munich to develop the new method, “The past two years have seen an explosion of methods for passive Wi-Fi imaging.” At the time, the technology could only make out rough shapes of things. \"If there's a cup of coffee on a table, you may see something is there, but you couldn't see the shape,\" Holl says, \"but you could make out the shape of a person, or a dog on a couch. Really any object that's more than 4 centimeters in size.\" The Controversy Begins In 2018 the team at UC Santa Barbara published a paper titled “Et Tu Alexa?” examining the potential threats of this emerging technology. They examined the problem of adversarial WiFi sensing and the risk to privacy resulting from the widespread deployment of wireless devices, which could be used to track your precise physical location, movement, and other physiological properties. Fortunately, they also propose some countermeasures for defending against such attacks to reduce the quantity and quality of the WiFi signals captured by the attacker, such as Geo-fencing and rate-limiting. These methods are not as effective with IoT devices, though, due to the frequency with which they make transmissions. The Breakthrough Up until this point it was necessary to use frequencies higher than commercial WiFi (2.4 and 5 GHz) to achieve decent imaging resolutions. That all changed in February 2019 when a team from Michigan State University published a paper in IEEE Access outlining how they were able to use signals at 5.5 GHz, which matches the 802.11n/ac WiFi protocol, to create a 2-D image of two reflecting spheres and a reflecting X-shaped target, concluding “full 2-D imagery is possible by capturing the WiFi signals present in typical environments.” Adding AI and Going 3-D At MobiCom 2020, researchers from the University of Buffalo, presented their WiPose technology, touted as “the first 3-D human pose construction framework using commercial WiFi devices.” This system uses the 2-D imaging technology previously discussed to construct a 3-D avatar of the humans captured by it. The system uses a deep learning model that encodes the prior knowledge of human skeletons in the construction process of the 3-D model. In 2019, former DARPA contractor Ray Liu launched his first commercial product in the WiFi sensing domain. Pitched as a way of “Making the world safer, healthier, and smarter,” the original military and law enforcement usages mentioned when this technology was born in 2005 were cast aside. The company claims the technology is so accurate that it can sense your breathing using nothing but standard WiFi signals. In a 2021 company blog, Liu discusses the development of IEEE 802.11bf, a new WiFi protocol, which is aimed at standardizing WiFi imaging across all devices – thus making it easier for companies such as his to exploit compatible wireless networks. Liu was elected to serve as IEEE President for 2022, and the new standard continues to be developed to this day. Further refinements to the imaging technology itself have been made. In late 2021 another paper was submitted to IEEE outlining how the researchers were able to achieve high-resolution imaging results with commercial WiFi signals using beamforming on the 802.11n/ac protocol. Ready for Production The perfect WiFi imaging system may have just been introduced to the world in December 2022, when researchers from Carnegie Mellon University married the latest in WiFi sensing technology to a human form estimation engine known as DensePose. (Left Column) image-based DensePose (Right Column) WiFi-based DensePose DensePose is a technology developed by Meta/Facebook, beginning in 2018. It’s very similar to the WiPose system we previously discussed and aims at “mapping all human pixels of an RGB image to the 3D surface of a human body.” The researchers modified DensePose so that, rather than taking an RGB image, it would be compatible with the imagery being produced by state-of-the-art WiFi sensing technologies. The resulting system “can detect the pose of humans in a room based solely on the WiFi signals passing through the environment.” Big Brother’s New Eyes It’s telling how the pitch for this technology has pivoted from military and police use to keeping people safe in their own homes. The true purpose of this is obviously for law enforcement, the military, and intelligence agencies. We already live with mass digital surveillance and if you don’t believe that this won’t get incorporated into their plans to monitor everything you do, then you haven’t been paying attention. Apart from putting CCTV cameras in everyone’s living spaces, this technology offers a comprehensive and supremely surreptitious way of putting eyes in every room of your house and place of work. Indeed, this just may become the norm. With nearly a third of Gen Z favoring the installation of government surveillance cameras in your home, this less-intrusive method may just find even broader support from the brainwashed masses. It will be possible to know where you are in the house and exactly what you’re doing, from sitting on the toilet to making love. We’ve seen how easily intelligence agencies can get secret warrants to surveil anyone of particular interest. We’ve also seen just how easy it is for someone to become a target for surveillance. You very well might, one day, find your WiFi router and access points feeding imagery to an alphabet agency that didn’t like your social media posts, while armed thugs wait for the perfect moment to execute their next no-knock raid. Subscribe 12 Likes · 2 Restacks 12 2 Share Previous 2 Comments Jeremy 7 hrs ago At least based on the meme'ry online, the installation of government surveillance cameras at home question was seen as hilarious so the conclusion is likely as erroneous as common sense suggests. LIKE REPLY SHARE RANGER71 18 hrs ago In the name of progress https://apple.news/AVrbIQnLKSkyUQjD5tN_s_A LIKE REPLY SHARE Top New Community The Chris Sky Psy-Op The truth behind Chris Saccocia NOV 23, 2022 • MR. E 16 6 The Apocalypse of Yajnavalkya: Part 1 A sneak peek into the world's most controversial book AUG 14 • MR. E 12 1 The End of Hope The worst torture is the false belief that you can win a rigged game. APR 7 • MR. E 13 6 See all Ready for more? Subscribe © 2023 Mr. E Reports Privacy ∙ Terms ∙ Collection notice Start Writing Get the app Substack is the home for great writing",
    "commentLink": "https://news.ycombinator.com/item?id=37564952",
    "commentBody": "Your WiFi Can See YouHacker NewspastloginYour WiFi Can See You (mrereports.substack.com) 180 points by nunodio 8 hours ago| hidepastfavorite75 comments dang 3 hours agoRecent and related:WiFi can read through walls - https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37469920 - Sept 2023 (171 comments) reply xahrepap 7 hours agoprevI feel like “putting wifi in my home” is similar to “putting bodies that emit heat in my home”. Neither of which implicitly grant law enforcement to come and observe either through the walls of my home.I am not a lawyer, nor am I familiar with the SCOTUS case referenced in the article. But I really hope it is precedent enough to make any judge throw out any such violations without any hassle. reply matheusmoreira 7 hours agoparentStill enables parallel construction. They can illegally search and surveil you without your knowledge and there&#x27;s not much anyone can do about it. They can&#x27;t submit any findings as evidence but they can totally anonymously tip off other authorities or something.It must be physically impossible to violate our privacy or it doesn&#x27;t work. reply scrps 5 hours agorootparentThe one upside to this being based on open standards also means that there is no reason this tech can&#x27;t be used against say, police, politicians, executives.Though I&#x27;d like to actually look at how it is practically done first, for all I know this could require more than just access to an AP. This is not my area of expertise so maybe someone that is an expert can confirm my intuition that beamforming even on enterprise grade access points isn&#x27;t locative (if that is the right word?) enough on it&#x27;s own, I&#x27;d imagine this would require some type of kit to be deployed in order to accurately map a room?Also plaster walls or at least my plaster walls seem to be great at blocking anything over 2.4GHz :\\ reply cperciva 4 hours agorootparentIt&#x27;s probably not the plaster blocking the signal, but rather a wire mesh which was put up to support the plaster. reply scrps 4 hours agorootparentHere is where it gets interesting... No mesh, it is lath and plaster. I&#x27;ve had to cut into it in a few rooms and if there is a mesh I have yet to see it. I am wondering what was used in the plaster at this point. reply cperciva 3 hours agorootparentHuh. I know lime plaster can contain significant levels of iron impurities, but I think even in the worst case those would be low enough to have an insignificant effect. Now I&#x27;m really curious too. reply sambazi 4 hours agorootparentprev> It must be physically impossible to violate our privacy or it doesn&#x27;t work.it has to be expensive enough in order to not be worth it. reply XorNot 7 hours agorootparentprevParallel construction only works if you&#x27;ve actually committed a crime and left evidence somewhere it can be plausibly found - i.e. \"a random search of the public woods near your area found a shallow grave, and tire-tracks matching your car\" - but the search was conducted because what actually happened was you were telling a guy \"yes I definitely killed Tony and buried him in the woods\".People keep using the term as though it implies some legal means of fabricating evidence against someone: which is ridiculous because if you&#x27;re willing to fabricate evidence, you don&#x27;t need to actually find any.It&#x27;s also worth noting that in the case of incidental wifi surveillance, it&#x27;s likely the plain-sight doctrine would apply: if you&#x27;re being imaged and positioned by broadcasts you yourself are making and nearby devices can receive, then this would be ruled in plain sight and rightly so (I can&#x27;t stop my routers and devices from detecting nearby devices as part of interference avoidance). reply lcnPylGDnU4H9OF 6 hours agorootparent> committed a crime and left evidence somewhereA Harvard professor wrote a book titled Three Felonies A Day which argues that there are so many laws to break that an average working professional in America will commit an average of three felonies every day. I do not know the accuracy of the claim but this idea of over-criminalization is an interesting one to think about.Specifically, how likely is it for a person to have unknowingly committed a crime and left evidence? (Of course, it’s not a violent crime assuming a regular sane individual.) Are you certain you’ve never left evidence for any sort of “white collar” crime which you unwittingly committed? reply checkyoursudo 4 hours agorootparentI am skeptical of the claim but not the conclusion. I take the claim anyway to be a bit of hyperbole to sell more books.One of the biggest problems in law is what IT&#x2F;CS people might call technical debt. New laws get added all the time while old, often-outdated or useless laws rarely get removed. The problem is that those old, even outdated and useless laws can sometimes be leveraged against you in new or novel ways, sometimes that the drafters of the laws never would have expected. Legislative intent is not always clear and not always decisive. reply vjk800 3 hours agorootparentprevThere are (at least) two ways to think about the law. One (which technical people tend to subscribe to) is that the law is analogous to a computer code for the society and the ideal is that it is followed at all times. The second is that the law is there as a backup when someone is clearly causing trouble and other, more informal means to stop them have failed.In the second interpretation, it is desirable to have some sort of law always ready to apply when a problematic situation arises, and everyone committing technical felonies every now and then is just a (mostly) harmless side effect of the system working as intended.The second interpretation is, I believe, also often applied when new laws are written. Usually the logic is: there is some kind of a problem that the politicians need to address, and the problem itself is too complex to directly address so some kind of proxy law is made that gives an excuse to throw problematic people in jail. See e.g. loitering laws. reply matheusmoreira 6 hours agorootparentprevEven if you committed a crime, police should not be able to find out about it through illegal means. Police should not be allowed to run SIGINT operations on citizens. They should not even have that capability. Why is it normal for police to have these literal spy movie technologies? reply atoav 4 hours agorootparentBecause in the typical American mind a thee-not-me-mentality is very prevalent. They are the sole protagonists of their own lives and they believe they have invincible plot armor like the protagonists in the action movies.That means people believe the police having Gestapo-capabilities will only ever hurt others (who must then have deserved it!) and never them.That a free democracy is a finely balanced system between the power of the people and the powers of the three branches isn&#x27;t something they ever consider. That capabilities can breed their own perverse incentives and a culture of disservice is a few complexity levels further up the tree still. reply t0bia_s 3 hours agorootparentprevBecause people generally wants rulers above them. It is a state doctrine that tells you it is for your protection.But if someone stole your bike, usually police wont find it. reply 8organicbits 3 hours agorootparentprev> It&#x27;s also worth noting that in the case of incidental wifi surveillance, it&#x27;s likely the plain-sight doctrine would applyPlain-sight should mean observation with the natural human senses, and I think the courts agree. There is nothing \"plain\" about radio signals leaking through walls. The thermal imaging example in the article as well.https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Aerial_surveillance_doctrine> the drone observed “each window of Plaintiffs’ residence and outbuildings” and was “outside [law enforcement’s] visual line of sight,” violating both federal and Indiana law.Placing a device outside a home to collect WiFi signals doesn&#x27;t sound like \"visual line of sight\". I.E. clearly a 4th amendment search requiring a warrant.Collecting records stored by devices that had been in the area already seems plausible, but would presumably require a legal justification to search those (warrant, consent, etc). But to my understanding, devices don&#x27;t store such records. reply bhaney 6 hours agorootparentprevAh, so we don&#x27;t need to worry about egregiously illegal privacy violations as long as we have nothing to hide. Thanks. reply developer93 4 hours agorootparentI don&#x27;t think that&#x27;s what they were saying. I read it as the law is unlikely to protect you reply avidiax 3 hours agorootparentprevParallel construction is foremost a run-around of the \"fruit of the poisonous tree doctrine\".[1]It allows law enforcement to use methods that might amount to illegal searches, and then avoid judicial scrutiny of those methods.See, for examples of such methods, Stringrays, \"persistent surveillance\"[2] aircraft, and NSA information sharing agreements.[3]\"Plain sight\" doctrine isn&#x27;t going to apply to these methods. You aren&#x27;t allowed to use deconvolution to look through frosted glass and call it \"plain sight\", just as you can&#x27;t use methods not in typical use to abuse WiFi as an imaging service. You can probably use WiFi traffic metadata, however, just as you can retrospectively use cell tower data.[1] https:&#x2F;&#x2F;www.law.cornell.edu&#x2F;wex&#x2F;fruit_of_the_poisonous_tree[2] https:&#x2F;&#x2F;arstechnica.com&#x2F;tech-policy&#x2F;2014&#x2F;07&#x2F;a-tivo-for-crime...[3] https:&#x2F;&#x2F;www.engadget.com&#x2F;2017-01-12-obama-expands-the-nsas-a... reply brigandish 6 hours agorootparentprev> Parallel construction only works if you&#x27;ve actually committed a crime and left evidence somewhere it can be plausibly found...> if you&#x27;re willing to fabricate evidence, you don&#x27;t need to actually find any.I am reminded of \"If you give me six lines written by the hand of the most honest of men, I will find something in them which will hang him.\"Or, in the not famous enough \"Don&#x27;t talk to the police\" lecture[1], the policeman telling how he can just surveil anyone for long enough and they will break the law, because, as the lawyer points out in his part of the talk, there are so many laws that it&#x27;s basically impossible not to breach one at some point.[1] https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=d-7o9xYp7eE reply grecy 4 hours agorootparentprev> Parallel construction only works if you&#x27;ve actually committed a crimeWhat about if you&#x27;re doing something that isn&#x27;t illegal yet, but a new Politician decides he doesn&#x27;t like the cut of your hair and then targets you later because you were doing it.What about if you just have someone come over to visit your home who is known to disagree with someone that becomes a Political leader in the future and they punish you for it.etc. etc.There are a million reasons why privacy is very important. reply XorNot 3 hours agorootparent> What about if you&#x27;re doing something that isn&#x27;t illegal yet, but a new Politician decides he doesn&#x27;t like the cut of your hair and then targets you later because you were doing it.That would have nothing to do with parallel construction and everything to do with someone having sufficient political power to change the law to target you - either specifically or generally.Someone with that much power could also just wipe out illegal search and seizure restrictions.But again this is also based on a misunderstanding of the process: you can&#x27;t use parallel construction to make illegally obtained evidence valid to use. You can only use it to indirectly guide the discovery of other evidence which must already exist and be plausibly findable by regular means.i.e. if the police stop on the high way and notice disturbed ground and discover a dead body, that&#x27;s something which could happen by pure chance. Parallel construction is suggesting that a police car should stop at a particular point on a highway, and have a good look around. replymk_stjames 7 hours agoprevIf someone is worried that their government could be flashing the firmware of their WiFi router to use the beamforming antennae of the router to track them walking around their home....Then they should be way more worried that their government is using any access to their wifi router at all. Like, you got way bigger things to worry about at that point, right?There are a million surveillance side-channels that could be used to build profile-able information about what is happening inside a home from the outside. The concern isn&#x27;t that those things are possible (they always will be) the concern should be that there are authorities who could be (mis)using such avenues and not explicitly being disallowed in the first place. reply ARandomerDude 7 hours agoparentI disagree. The trajectory of technology generally is:possible -> prototype -> product -> common -> common and cheapIf Wi-Fi surveillance were to become common and cheap, other methods would be produced to make data harvesting common and cheap to the surveillance state&#x2F;bigco. The best way to avoid that dystopia is to safeguard it early in the process. reply mk_stjames 7 hours agorootparentWhat I&#x27;m trying to get at is, even if this type of Wifi surveillance becomes commong and cheap, it still requires access at a low level to the wifi router itself. Which should be hands-off in the first place. It&#x27;s like having cameras in your house on your local network... if the police could legally hack into your network, they could watch you on the cameras to see if you are committing crimes! But... the point is they shouldn&#x27;t be able to do that, regardless if you have the cameras or not... the line is drawn at the access.Same thing there. It shouldn&#x27;t matter if the tech is possible, produced, cheap, and installed in your home.... it&#x27;s all moot if the act of the surveillance isn&#x27;t followed through. We need to work towards ways to prevent the acts themselves, as we simply aren&#x27;t going to be able to prevent the possible sidechannel attacks from being possible. They are always possible. reply vermilingua 2 hours agorootparentIn Australia we have the Assistance and Access amendment to the Telecommunications Act 1997, which allows the government to demand that companies or individuals insert backdoors into their products for use by law enforcement. My WAP&#x2F;router already tries to update its own firmware, what’s to stop the govt forcing in new firmware with full cooperation from the vendors that enable this passive sensing and allow access to the output? reply transpute 5 hours agorootparentprev> it still requires access at a low level to the wifi router itselfNope, remote sensing only requires custom radio firmware on the passive surveillance receiver ( Which should be hands-off in the first place.Wi-Fi access points are often provided by ISPs. ISPs, especially in the US, are not to be trusted; https:&#x2F;&#x2F;security.stackexchange.com&#x2F;q&#x2F;71834 is just one example of that. reply momirlan 7 hours agorootparentprevi don&#x27;t think any safeguards will protect us from evils brought about by technology - it evolves too fast already for the society to catch up . reply ksey3 5 hours agorootparentWell someone has to pay for the tech. Its not free. And that puts an automatic upper limit on what happens.People have no idea how much debt Police depts (lets not even talk about the military) have racked up playing with high tech toys and paying off compensation every 2 days for people they accidentally harm or kill. So whats funny here as Big Bro gets access to more and more tools the more broke he gets.And guess what the financiers of this debt will do when it cant be paid off. Raid police pension funds. Thats how financialization of public service worksNo free lunch big bro. Have fun with all the \"cool toys\" while the good times last. reply akira2501 6 hours agorootparentprevYou are living in that dystopia already, then, and your efforts are equivalent to plugging a hole in the grand dam with your finger. reply enragedcacti 7 hours agoparentprevThe linked paper \"Et Tu Alexa?\" uses a single smartphone positioned outside of the property> We show that just by sniffing existing WiFi signals, an adversary can accurately detect and track movements of users inside a building. This is made possible by our new signal model that links together human motion near WiFi transmitters and variance of multipath signal propagation seen by the attacker sniffer outside of the property. reply devjab 4 hours agoparentprevI’m curious as to which government you think isn’t going to use surveillance tools on you, even if they aren’t exactly legal. I mean, after Echelon we still had the tinfoil disbelief to fall back on, but in a post-Snowden world it’s hard to imagine why you would think your government wouldn’t do this.You’re not exactly wrong in that you should also be worrying about: Boundless Information, Bullrun, Carnivore, DSCNet, Fairview, ICREACH, Magic Lantern, Main Core, Mainway, Media Monitoring Services, Muscular, Mystic, Nationwide Suspicious Activity Reporting Initiative, NSA Ant Catalog, PRISM, Room641A, Sentry Eagle, Special Collection Service, Stellar Wind, Tailored Access Operarions, Turbulence, and, X-Keyscore if you’re American. But that doesn’t mean you shouldn’t also worry about this, just as much. reply matheusmoreira 7 hours agoparentprev> Then they should be way more worried that their government is using any access to their wifi router at all.We should be worried about ISPs too. They give themselves access to their customer premises equipment. I had to hack their router to put it into bridge mode and then use another router with software I control to connect. reply ilaksh 6 hours agoparentprevYou don&#x27;t think that there is some agency like the NSA or such that can access your full Google search history and location in real time? There might even be some individuals with access that doesn&#x27;t even require search warrants or anything. reply edgyquant 6 hours agoparentprevNo, what could be a bigger worry than an omnipresent government intent on total control? reply kibwen 7 hours agoprevApropos of nothing, a conductive mesh with 1&#x2F;2\" holes suffices to block 2.4 Ghz signals, while 1&#x2F;4\" holes suffice to block 5 Ghz signals. https:&#x2F;&#x2F;www.homedepot.com&#x2F;s&#x2F;hardware%20cloth reply aidenn0 7 hours agoparentHah, I remember having a tiny condo and was unable to get wifi throughout it because it was constructed with a metal-mesh backing[1] instead of wooden lath for the plaster. Was like a 60db drop putting a device on the opposite side of a wall made from one of these.1: Kind of like this https:&#x2F;&#x2F;www.homedepot.com&#x2F;p&#x2F;Gibraltar-Building-Products-3-4-... reply tetsuhamu 7 hours agoparentprevYes!We build walls out of materials opaque to visible light. Our walls are basically windows for wifi, so make them opaque! It&#x27;s all EMR reply newhouseb 6 hours agoprevI&#x27;m a little skeptical of the paper in question: https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2301.00250.pdfIf you look at the training and test set info:> We report results on two protocols: (1) Same layout: We train on the training set in all 16 spatial layouts, and test on remaining frames. Following [31], we randomly select 80% of the samples to be our training set, and the rest to be our testing set. The training and testing samples are different in the person’s location and pose, but share the same person’s identities and background. This is a reasonable assumption since the WiFi device is usually installed in a fixed location. (2) Different layout: We train on 15 spatial layouts and test on 1 unseen spatial layout. The unseen layout is in the classroom scenarios.Depending on how they selected various frames -- let&#x27;s just say it was random -- the model could have learned something to effect of \"this RF pattern is most similar to these two other readings I&#x27;m familiar with\" (from the surrounding frames) and can therefore just interpolate between the resulting poses associated with those RF patterns (that the model has compressed&#x2F;memorized into trained weights).If you look at the meshes between the image ground truth and the paper&#x27;s results, you&#x27;ll see that they are strikingly similar. I find this also suspect because WiFi-band RF interacts a lot more with water than with clothes and so you would expect the outline&#x2F;mesh to get the \"meat-bag\" parts of you correct but not be able to guess the contours of baggy clothes. That is... unless it has memorized them from the training set. reply est 4 hours agoparentAlso> It should be noted that many WiFi routers, such as TP-Link AC1750, come with 3 antennas, so our method only requires 2 of these routers.> WiFi-based perception is based on the Channel-state-information (CSI) that represents the ratio between the transmitted signal wave and the received signal wave. The CSIs are complex decimal sequences that do not have spatial correspondence to spatial locationsAFAIK, access to raw CSI data is not available on consumer routers. You need specific FPGA&#x2F;SDR boards which happens to speak the WIFI 2.4G&#x2F;5G protocol.There are some open sources efforts such as https:&#x2F;&#x2F;github.com&#x2F;open-sdr&#x2F;openwifi reply transpute 4 hours agorootparent> FPGA&#x2F;SDR boardsCustom firmware is available for existing WiFi radios, e.g. sub-$20 ESP32 WROOM.Previous HN discussion: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34480760 reply est 4 hours agorootparentThus \"Your wifi\" can not see you unless you setup ESP32 as router and allows remote ROM updates. reply transpute 3 hours agorootparentA nearby adversary can \"see\" into your home&#x2F;building without any access to your router&#x27;s software or hardware.A nearby adversary needs only the reflections of Wi-Fi from your or neighboring routers. replytranspute 5 hours agoprevMatt Damon and Ben Affleck&#x27;s Incorporated (2017) TV series, https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=t7eKEHhSw00 & https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Incorporated_(TV_series), includes thought experiments on future workplace surveillance.> The series takes place in a dystopian Milwaukee in the year 2074, where many countries have gone bankrupt .. In the absence of effective government, powerful multinational corporations have become de facto governments, controlling areas called Green Zones. The remaining territories are called Red Zones, where governance is weak or non-existent.Consumer Wi-Fi 7 Sensing routers (estimated to arrive in 2024 with IEEE 802.11bf) can generate 3D images of human activity through the walls&#x2F;floor&#x2F;ceiling of homes and business, profiling human position, movements, breathing, keystrokes, emotion and more, https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34480760.After standardization by IEEE in consumer routers, which remote parties would be interested in WFH WiFi Sensing Analytics? How could regulators, employers, employees and home-builders respond? Remote, through-wall, X-Ray vision monitoring can be entirely passive when there is ambient WiFi traffic, e.g. reflections from routers inside targeted urban buildings of interest. This has been possible for more than a decade, using either expensive LE equipment or low-cost DIY&#x2F;RF&#x2F;hacker toolsets. Will IEEE standardization of Wi-Fi 7 Sensing give X-ray vision superpowers to every neighbor?On a positive note, there is some commercial interest in Li-Fi for wireless data transfer via optical light, so that data transfers will stop at a perimeter established by traditional walls. Until then, aluminum radiant barrier, conductive paint, RF shielding drywall (e.g. QuietRock) or metal mesh can reduce unwanted inbound&#x2F;outbound RF signals. For temporary shielding of a room, consider a Zipwall-like tension pole frame supporting a cube of aluminum radiant barrier (USD $0.25 per square foot), plus shielded air ventilation. reply Communitivity 3 hours agoprevI wonder if this will cause an interest in in Faraday Cage walls: outer wall sheetrock with a copper wire mesh support, with a ground to prevent re-radiating. You&#x27;d also need to have the same mesh on the house roof and attic walls. Copper would be best for conductivity. The mesh holes would need to no greater than 5mm to block 2.4Ghz and 5Ghz. This is rounded down to the nearest mm from the equation: hole size (m) = speed (m&#x2F;s) x frequency (Hz) (.0599 = 299,792,458 * 5,000,000,000)The better the conductivity of the metal the thinner the thickness can be. It&#x27;s differences between 10s of microns though, so in practical terms thin copper fire should work. Even thin steel wire should work.Your biggest leakage problem would be doors and windows. Window screens sized to stop mosquitos (1.2mm hole size) will help with this, for the windows. Metal screen doors should help.This won&#x27;t prevent all leakage, but should prevent most of it. reply danw1979 3 hours agoparentLeakage doesn’t seem to be the problem that is being explained here: it’s internal sensing within the home by existing wifi devices. reply Communitivity 3 hours agorootparentIt doesn&#x27;t need to be your wifi devices, it could be your neighbor&#x27;s, if you&#x27;re in a townhouse or your house is close enough.But I agree with another commenter: if they&#x27;ve hacked your wifi router then you have bigger problems, as they&#x27;ve likely hacked all your devices then. reply SEJeff 7 hours agoprevTomographic detection has been around for some time. There was a company that commercialized this and had a more or less real-time Harry Potter style “Marauder’s Map” of your home for a security system but they did a lousy job commercializing it.See a crappy demo of the Xandem solution: https:&#x2F;&#x2F;youtu.be&#x2F;0lnfOnZD6d8?si=1QqIO0mvHsP4mj5nTheir Xandem home product was fantastic, though failed commercially due to the price. reply gloosx 3 hours agoprevI don&#x27;t understand, why the author&#x27;s concern is focused around surveillance through wifi. The routers are private devices, so if secured properly, what kind of backdoor a third party could use to take imagery of your house insides?Law enforcement for decades has used commercial cellular networks to spy on people, and with the modern 5g technologies these surveillance suites are very well developed and able to see EVERYONE at scale with centimeter precision. reply SergeAx 38 minutes agoparentAmazon Alexa, Google Nest. BTW, every smatphone has a WiFi. reply CommieBobDole 5 hours agoprevLooking at the source articles, this seems to require specialized equipment and large antennas that are fairly close to whatever is being imaged. So unless there&#x27;s been some sort of breakthrough, this is not a case of \"someone could hack your router and see you in your home\". reply jaza 4 hours agoparentThe article is saying \"not yet, but soon, sooner than you think\". reply yoru-sulfur 7 hours agoprevA number of years ago I worked for a company that developed a home security product using this kind of tech: https:&#x2F;&#x2F;www.cognitivesystems.com&#x2F; reply system2 6 hours agoparentDoes it work as advertised? reply lusus_naturae 7 hours agoprevDina Kitabi was doing wifi sensing before 2017: https:&#x2F;&#x2F;people.csail.mit.edu&#x2F;dina&#x2F;publications.html?tag=syst... reply femto 6 hours agoparentNo doubt that work would have fed into the development of the WiFi 802.11bf standard. reply arsome 5 hours agoprevIf using an FLIR camera is considered an unreasonable search without a warrant, I fail to see how this wouldn&#x27;t be too. reply nomilk 6 hours agoprevThe wifi router sees you (a bit like a mini radar), but someone outside your premises would still have to hack the router? Or do ISPs already have that access?Also curious to know how far wifi routers &#x27;see&#x27;? If it&#x27;s more than a few meters there could be more potential surveillance by wifi router than by cctv! reply tetris11 5 hours agoparent> Or do ISPs already have that access?\"In order to comply with DOCSIS4.2 regulations, we require anonymous WiFi telemetry for 5GhZ or higher due to safety. You can disable this at any time and use 2.4GhZ.\"Or some such. Just give it time. reply srejk 6 hours agoparentprevLook up TR-069. Always use your ISP-supplied router in bridge&#x2F;modem mode only. reply nomilk 5 hours agorootparentIIUC:- &#x27;TR-069&#x27; is the name of a protocol by which (most) ISPs operate- It gives ISPs the rights&#x2F;ability to interact with your devices remotely (under the guise of maintenance and remote troubleshooting)- This isn&#x27;t something that&#x27;s going to happen one day far in the future; it already applies to most(&#x2F;all?) routers currently in use in countries whose ISPs abide by the protocolSo ISPs can already see inside customer homes?! (since they can already access the data necessary to use the WiFi device as a makeshift imaging system? - wowzers! reply tetris11 5 hours agorootparentprevhttps:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;TR-069Eye-opening indeed.. reply SergeAx 35 minutes agoprevI am a bit concerned about the source. It seems like a conspiracy theories head-to-toes, see their other publications. reply neets 6 hours agoprevI thought they made that up in that Person Of Interest[1] Episode, seems like they were just ahead of the curve on tech1. https:&#x2F;&#x2F;www.imdb.com&#x2F;title&#x2F;tt1839578&#x2F; reply cglan 7 hours agoprevAre there any home solutions to wifi sensing&#x2F;location sensing at home for diy iot purposes? reply toomuchtodo 7 hours agoparenthttps:&#x2F;&#x2F;wrlab.github.io&#x2F;Wi-ESP&#x2F;https:&#x2F;&#x2F;github.com&#x2F;StevenMHernandez&#x2F;ESP32-CSI-Tool reply sdrawdes 7 hours agoparentprevhttps:&#x2F;&#x2F;github.com&#x2F;FOULAB&#x2F;Project-COGSWORTH reply sdrawdes 7 hours agoprevReminds me of The Thought Emporium&#x27;s camera that can see WiFi. https:&#x2F;&#x2F;youtu.be&#x2F;g3LT_b6K0Mc?si=PRtkeXRqq8UmhpC9 reply TechBro8615 6 hours agoparentCNLohr made a similar project three years before that: https:&#x2F;&#x2F;youtube.com&#x2F;watch?v=aqqEYz38ensI know this stuff is dystopic, but I personally find it really cool. That said, I also disable WiFi and prefer ethernet wherever possible... reply gorgoiler 5 hours agoparentprevImaging like this is like scanning a scene with a one pixel camera. Cameras use a lens to focus an image onto a plane, and the plane has multiple sensors. What would that device look like for microwaves?(Welcome to HN!) reply metadat 6 hours agoprevRelated discussion 7 days ago:WiFi can read through wallshttps:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37469920 (171 comments) reply swader999 5 hours agoprevI just have a house full of manicans. Bonus, I never get lonely. reply Mistletoe 5 hours agoprevIt’s enough to make you want to sleep in a Faraday cage coffin every night. A privacy vampire. reply raavikant 5 hours agoprev [–] How? replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Researchers have advanced a technology that utilizes WiFi signals for imaging and monitoring human activity indoors, with recent strides enabling detailed 2D and 3D imaging.",
      "A significant apprehension surrounds this technology due to potential privacy invasions and potential misuse by authorities or intelligence agencies.",
      "As the technology progresses and becomes more widespread, it could enable extensive surveillance of individuals without their awareness or agreement."
    ],
    "commentSummary": [
      "The discussion emphasizes the use of WiFi signals for surveillance without permission or knowledge, raising significant concerns about privacy rights, legality, and possible misuse.",
      "Participants underline the importance of privacy and the need for protections against unauthorized government monitoring.",
      "Advancements including through-wall monitoring feature in the talk, along with methods to counter unwanted WiFi signals."
    ],
    "points": 179,
    "commentCount": 75,
    "retryCount": 0,
    "time": 1695090253
  },
  {
    "id": 37555139,
    "title": "Problems with homemade billing systems",
    "originLink": "https://www.getlago.com/blog/the-4-biggest-problems-with-homemade-billing-systems",
    "originBody": "Product Developers Pricing Solutions Resources Book a demo Blog The 4 biggest problems with homemade billing systems The 4 biggest problems with homemade billing systems This article was written by Vincent Pochet, Senior Backend Developer at Lago. —— In 2017, I joined Qonto, a B2B neobank in the making (now worth more than $5B), and got involved in the design of the billing system. As the company was scaling from cradle to unicorn, we faced many billing challenges: monthly and yearly plans, “pay-as-you-go” components (percentage on FX transfers, fixed fee on ATM withdrawals, user seats, printed cards, etc.), creation of invoices at scale, taxes, accounting… It was a never-ending nightmare. Now, when someone asks for advice about their billing system, my answer is clear: DO NOT build it yourself. It may look like an interesting project at the beginning, but you’ll soon regret it. And believe me, it only gets worse. As an engineer, if you’re told it’s a “two-month project”, run as far as you can before it’s too late and you become the “billing guy”! #1 Pricing changes all the time and billing needs to follow After creating the first billing engine of Qonto, we were pretty happy with what we had built. We could ingest millions of events every week, our calculations were correct, we could generate sequential invoices, we were able to apply marketing coupons and we managed to debit our internal ledger. Everything was going well! But then… The marketing team came up with a new yearly plan Billing was performed on a monthly basis. At the end of each month, we had to query the database to compute fees and display them as billing items on the invoices. Customers were able to switch from a “monthly_solo” plan to a “monthly_premium” plan (and vice versa), and the upgrade/downgrade logic was working well (although it had been difficult to implement). When the marketing team devised a new yearly plan to secure revenue for the next 12 months, I started to freak out: the entire billing logic was based on monthly boundaries. It took us two months to modify this logic. The hardest part was that the subscription fee had to be billed in advance, at the beginning of the annual period, but the overage related to usage-based features still had to be billed on a monthly basis. The finance team asked to switch from anniversary to calendar dates At the time, customers were billed based on the anniversary date of their contract. For instance, if a customer had signed up for a monthly plan on March 16th, they were billed on the 16th of each month. One day, the finance team requested a meeting with the engineering team and the CFO said: “It would be easier for everyone if we switched from anniversary dates to calendar dates for subscriptions. Right now it’s a mess from an accounting perspective, so let’s bill everyone at the beginning of the calendar period.” In other words, they wanted to switch from this… To this... 100,000 customers were affected by this migration. The accounting mess turned into an engineering nightmare, resulting in a three-month project. Implementing billing logics takes time and things get incredibly complicated over time. Other typical organic changes include (but are not limited to): • New features to be added to the pricing; • New country launch, with a different pricing; • New business line, when you start selling a white-label version of your product for instance; • Changes in tax legislation; and • Custom plans for “Enterprise” customers. #2 Your billing system needs to scale with your user base A home-made billing system is not scalable if you don’t maintain it. At Qonto, our billing database contained millions of rows. This database was linked to our internal ledger, which was used to deduct fees from our customers’ accounts. Processing high volumes of events is hard, and when something breaks, you need to check whether past events need to be computed again (but you can’t afford to ingest the same event twice). Generating invoices is also an important task. It’s not hard to create PDF files with line items on it. We used a library called Gotenberg to display backend aggregations on a beautiful HTML template. However, it’s hard to process fees and generate millions of invoices at the same time. Your backend queries and aggregates millions of rows asynchronously, and the calculation must be correct for each customer. The more complex the pricing, the more complicated the calculations. And the more customers you have, the more IT resources you need. #3 Grandfathering causes headaches A few weeks ago, we interviewed Nicolas Dessaigne, founder of Algolia and Group Partner at YC. He told us about their terrible experience with pricing and billing. After a series of “minor” changes to their price plans, they decided a few months ago to adopt a full usage-based pricing model and had to rebuild the entire billing logic. As most customers had signed long-term contracts based on existing plans, they had to exclude them from the migration. This is called “grandfathering”. In this situation, engineers must not only implement new billing rules, but they also need to maintain the old logic for grandfathered plans. In 2013, the pricing of Algolia was like this... Then in 2015 it was like this... And here is the 2022 update! #4 You must be prepared to staff an entire team Billing is never considered an expertise. It’s perceived as a background task and not as a prestigious project that engineers will fight for (they may even leave if they’re forced to work on it for a long time). However, sooner or later, billing will be a full-time job for at least one engineer. Because of pricing changes, scalability challenges and grandfathered plans, complexity increases and so does the workload. Billing is built around your company’s product, it’s a living organism, not a feature. At Qonto, the billing project was supposed to be completed by a single backend engineer in only two months. One year later, two backend engineers were still working on it full time. Then the team of two backend engineers grew into a team of 20 people, including product managers, backend engineers and frontend engineers as well. Hiring, onboarding and retaining people to take care of our billing system was a constant challenge. They would have preferred to work on our core product, and our management team also wanted to downsize the team. We considered implementing an off-the-shelf billing solution but there was nothing flexible enough and the switching costs were too high. Algolia also tried to migrate to Zuora before backing out and rebuilding their billing system for the fourth time. This is a decision you have to make in the early days, otherwise, at some point, your billing will be too complex and won’t fit any software product. I left Qonto in March 2021 and I still get phone calls from engineers currently working there and struggling with billing. They learned these four lessons the hard way. They now know what it costs to build a homemade billing system. Two hosting options, same benefits Whether you choose the cloud version or decide to host the solution yourself, you will benefit from our powerful API and user-friendly interface. Open source The optimal solution for small projects. Deploy Open Source Premium The optimal solution for teams who want control and flexibility on cloud or self-hosted version. Book a demo Subscribe to our updates Sign up for our newsletter, we promise not to spam you. Products Usage metering Price plans Coupons Add-ons Invoicing Instant charge Customer portal Spending minimums Email invoices Grace periods Timezones Credit notes & Refunds Developers GitHub Documentation API references Community Changelog Roadmap Solutions Engineers Product managers Revenue teams Business operations Finance leader Fintech CPaaS Artificial Intelligence Cloud Infra SaaS API B2C Hybrid plans Usage based Enterprise Multi products Advanced Resources Pricing Templates Compare Product updates Perks Status Stripe fees calculator Integrations Company Blog About us Hiring Love wall Beta 2023 Lago • Designed Worldwide Privacy Policy Security Terms of Service",
    "commentLink": "https://news.ycombinator.com/item?id=37555139",
    "commentBody": "Problems with homemade billing systemsHacker NewspastloginProblems with homemade billing systems (getlago.com) 173 points by thibo_skabgia 22 hours ago| hidepastfavorite99 comments 542458 21 hours agoGood article, and I generally agree that people are too quick to say “Ahh, this looks like a two week project” and ignore broader complexity. I’ve definitely seen a fair amount of that.My question is, how do I know that the OTS solution would actually be much better? For example, OP describes some tricky migrations from one type of billing to another, or complex grandfathering schemes - how would you have any guarantee that your third party solution would be able to support those? At least if it’s in-house you can implement the needed functionality eventually - if it’s third-party it might just be flat-out impossible. reply singron 17 hours agoparentSometimes the fact that you can&#x27;t change it is a great feature. Obviously if it&#x27;s missing a critical feature, then you are screwed, but often these requests for customization are made because it seems like they are quick and cheap to make and it will be automated and free after a one-time cost. However, having a whole team of engineers run your billing department is a lot more expensive than hiring someone in finance ops who can do a manual refund or spend an extra 5 minutes every quarter dealing with anniversary billing. Especially if you are B2B, you are going to run into that huge client that wants to be billed net 37 on the lunar calendar, and it&#x27;s better to get someone in finance to send invoices if necessary each week than to calculate moon phases in your product. reply specialp 21 hours agoparentprevWell one thing an OTS solution accomplishes is it makes costs and capabilities very clear to product and marketing. When you have an internal team doing something bespoke to your organization it is viewed as much more malleable.If the third party solution cannot support the billing system devised, or will support it for a large cost, companies are more likely to choose things that are more in the box. reply cjonas 20 hours agorootparentExcept with many of these enterprise solutions (SAP, Netsuite, Salesforce) most companies will still need to pay consultants 300 dollars an hour to configure and maintain the application. These costs often exceed the license cost itself.Bottom line, complexity is expense regardless if you build or buy. Companies need to factor the systems and operations impact of complicated pricing and billing models into the decision making process. reply kayodelycaon 19 hours agorootparentThe absolute cost isn&#x27;t important here. What&#x27;s important is that cost is now explicitly on the balance sheet were someone can see it. If you have an in-house system, that team is treated as an infinite supply of features for no apparent cost. (Ask me how I know.)And as a developer, I love being able to hand certain people over to vendors who are used to dealing with those certain people. Hell, if we picked the right system, those vendors will have useful domain knowledge I don&#x27;t have. reply PeterStuer 4 hours agorootparentThat is not the case in practice. Those AAA vendors will ask you, the customer, for all the domain knowledge in a very expensive &#x27;configuration project&#x27;, and shuffle many of your specifications into a bespoke &#x27;customization and integration project&#x27;. reply re-thc 20 hours agorootparentprev> Well one thing an OTS solution accomplishes is it makes costs and capabilities very clear to product and marketing.It gives that illusion. The reality is not so clear cut. There are lots of nuances to a capability that&#x27;s not just a box check. It could be half-done, i.e. certain bugs in certain scenarios. It could be slow, unreliable or something else altogether.There&#x27;s also: https:&#x2F;&#x2F;www.businessinsider.com&#x2F;former-vp-claims-salesforce-... reply ineedasername 21 hours agoparentprev>how would you have any guarantee that your third party solution would be able to support those...You could say the same thing about literally any 3rd party software or service you purchase. \"Why use cloud services to do anything?\" etc.Sure this thinking will hold for some things, but chances are that for something as standard as billing you&#x27;re not going to be implementing some revolutionary feature that gets you more customers. On the other hand, if you mess up billing because you rolled your own system that doesn&#x27;t have many standard options that a mature OTS system would offer it can lose you potential customers, because for months at a time you&#x27;re stuck building the most mundane and standard things like an annual plan that could be used to better appeal to buyers.Survey the main OTS options and see what they can do. It will probably give some ideas like \"Oh yeah annual plans, we&#x27;ll probably want them at some point\". At the very least you do this at the outset when you&#x27;re small. Worry about complex billing requirements that won&#x27;t be part of vanilla options in most OTS systems once you&#x27;re big enough & complex enough to require them. Otherwise you&#x27;re worrying about a Maserati problem. reply PeterStuer 4 hours agoparentprevMany of the OTS solutions in this area only cover the most obvious or common requirements. Anything advanced or specific comes with a very hefty bespoke project. reply ineedasername 21 hours agoparentprev>how would you have any guarantee that your third party solution would be able to support those...You could say the same thing about literally any 3rd party software or service you purchase. \"Why use cloud services to do anything?\" etc.Sure this thinking will hold for some things, but chances are that for something as standard as billing you&#x27;re not going to be implementing some revolutionary feature that gets you more customers. On the other hand, if you mess up billing because you rolled your own system that doesn&#x27;t have many standard options that a mature OTS system would offer it can lose you potential customers, because for months at a time you&#x27;re stuck building the most mundane and standard things like an annual plan that could be used to better appeal to buyers.For billing, a one of the most standard things ever for a business, you simply take a small amount of time to survey the OTS options and compare features. Any you know what? There&#x27;s an excellent chance that buy actually getting a comprehensive look at features offered by such options you will learn about the types of things you will absolutely want to do in the future. Things that, in retrospect, seem like obvious oversights, such as an annual plan. You&#x27;ll see features like that in your survey and say to yourself \"Oh, yeah, that&#x27;s something we may definitely want in the future, lets put that on our requirement list for a final choice.\" reply Spooky23 17 hours agoparentprevSometimes unlimited choice is a vicious thing.With the power to modify anything, people will make decisions and choices with serious consequences and benefits that may or may not be tangible, or worth the squeeze.I’ve spent most of my career working in and around government, and poorly scoped or framed legislation costs billions because it drives customization and development. The business and engineering impacts of those decisions are often not appreciated.If you work for a corporation, and you have a off the shelf billing system that fundamentally does what is needed… you have a chance of using ROI or some other metric to save the (expensive, hard dollar) customization for when there is real value.The other thing is that sometimes these custom systems create customer problems in the name of “helping”. I’ve had many times where some legacy SKU or billing model, left in place to make my life easier instead made things much much more difficult down the line. reply jasode 21 hours agoparentprev>, how do I know that the OTS solution would actually be much better? For example, [...] how would you have any guarantee that your third party solution would be able to support those?The business (employees in the accounting dept and&#x2F;or consultants) do a \"fit-gap analysis\" when evaluating potential software: https:&#x2F;&#x2F;www.google.com&#x2F;search?q=fit+gap+analysisFor the \"gaps\" of missing functionality, look at either :(1) customizations via extra programming or extra add-ons.(2) eliminate that software as a viable choice and move on to the next OTS software for evaluation> At least if it’s in-house you can implement the needed functionality eventually -The vast majority of Fortune 1000 businesses replaced their \"homegrown\" accounting and payroll systems they built in the 1960s&#x2F;1970s&#x2F;1980s -- with COTS ERP systems like Oracle Financials and SAP R&#x2F;3 because their internal IT teams never got around to the \"eventually\" option.There&#x27;s an anecdote (I think from the book \"I&#x27;m Feeling Lucky\") about a Google&#x27;s early startup years where an employee said they \"needed to buy SAP\" for accounting. The co-founder Sergei Brin was dismissive, \"why do we need to buy that when our programmers can build it?\" Well, he was 20-something at the time and naive about the complexities of financial accounting software for a global business. He must have eventually realized the true scope of the problem because instead of building in-house accounting software, Google bought Oracle Financials. About 20 years later, they migrated from Oracle to SAP. reply PeterStuer 3 hours agorootparentThat is not how any of this works. SAP is already sold to the C-suite on the basis of very fancy powerpoints and sales pitches before anyone in the enterprise can do a gap analysis. I have no experience with Oracle Finance, but I suspect the process will be the same. reply re-thc 20 hours agorootparentprev> Well, he was 20-something at the time and naive about the complexities of financial accounting software for a global business. He must have eventually realized the true scope of the problem because instead of building in-house accounting software, Google bought Oracle Financials.How do we jump to this conclusion? Did he realize the true scope of the problem or was it just left to someone else once Google grew in size? It doesn&#x27;t even explicitly say how Oracle Financials came to be at Google.> because their internal IT teams never got around to the \"eventually\" option.I doubt that&#x27;s really the case. Same with how cloud came to be. The sales people promised massive savings i.e. bonuses for management and here we are. reply p_l 19 hours agorootparentDon&#x27;t forget the money sinks that are any attempts to implement something differing with SAP or other big ERP systems.You delivered extra value for customers (making you more competetive) thanks to some particular ability of your previous (even manual) billing rules? Kiss them bye bye. reply scott_w 19 hours agoparentprevWe migrated to Zuora and, while there are limitations, they’re far less than your own limitations. Honestly, if you can’t model it in Zuora, what you can do is likely close enough that it’s not worth the 100x extra effort to build your own system. reply 6510 21 hours agoparentprevHAHAHA, my estimate was 2 weeks. This was 6 months ago. It is now a marvel of engineering with most of the 100 000 edge cases covered. You should read that as: not battle tested on (insane) users. (I&#x27;m so looking forwards to that) I estimate 2 years to completely finish it, give or take a decade or two. reply emilfihlman 21 hours agoparentprevIndeed. Very often organisations tend towards buying solutions without appropriately gauging the situation. One easy example from IT is buying servers vs renting them, but fortunately it seems that IT people have found the, for some uncomfortable, balance of \"it always depends\", and the analysis might turn multifaceted pretty quickly.I also feel that people often discount the cost of up top decisions that people below don&#x27;t like. reply vidarh 21 hours agoprevI&#x27;ve built several billing systems and managed a team that built a much larger one, and it&#x27;s not that hard, but it does require an attitude and attention to detail that a lot of people lack.E.g. one thing that worked well in the systems I&#x27;ve worked on was to break things into small state transitions, very firmly document the allowed states and their transitions, and log every transition to the database. Wherever possible (anywhere that didn&#x27;t interact with external API&#x27;s, and sometimes even when they did) we&#x27;d aim for state transitions to be idempotent. When they weren&#x27;t, we&#x27;d seek to reduce the non-idempotent call to an external dependency to just that call as a separate state transition.When something very occasionally went wrong, we could trace things in detail, and pinpoint it, and most of the time we could safely replay transitions around whatever went wrong until we could see what had happened.It wasn&#x27;t difficult to build these systems that way, but it was tedious, and something where it&#x27;s easy for people to be tempted into shortcuts.As for scaling, you spit out events for rollups, and can shard invoice generation. We handled millions of revenue on a machine far less powerful than my laptop today. Scaling really would not have been up there for me.To #3, maintaining old pricing isn&#x27;t generally that hard. Very few people drastically change how they account for usage, which tends to be the most complicated billing scenario. They tend to change amounts and periods and thresholds, which should be data, not code.With respect to a team, I agree. One of the places I worked on billing was Yahoo. At the time it wasn&#x27;t just one team, but several - my team (responsible for Europe) existed largely because the European finance and product teams didn&#x27;t trust the US payment services team to take their requirements seriously enough and so wouldn&#x27;t let them near the European premium services... Billing isn&#x27;t something you want to do yourself unless it&#x27;s either a core competency or you&#x27;re big enough to have to deal with that kind of bullshit.(As for size of teams, I&#x27;ve built a billing system with 2 people, but I&#x27;ve also worked on billing systems managed by 50... You really want to have a good idea whether you&#x27;re likely to end up towards the former or latter before you go ahead...) reply cvccvroomvroom 19 hours agoparentExactly. You really need experts in entity relationship data modeling (typically in Oracle DBMS), data hygiene, and codifying business rules. This isn&#x27;t something web developers should generally take on. Software involving money should consider approximating the formality of engineering approaches: waterfall development model, formal verification, and extensive unit and integration testing. There are hidden costs that edge up TCO for DIY. It&#x27;s possible COTS maybe very expensive or incapable of being customized sufficiently to meet the needs of the particular application, but DIY should generally be the last resort when no other viable solution exists. reply PeterStuer 3 hours agoparentprevExactly. With some experience the backend can be built by a single person in about 3 months if you have access to good testing data. I&#x27;ve done it. Customer ran verfication against their legacy mainframe, software was accepted right at delivery, 0 bugs found. Afaik it is still running in operations, with ofc some changes and adaptations over the years. reply 6510 20 hours agoparentprev> #3, maintaining old pricingI copy the entire product into the order. The thing one buys should be the thing on the screen when the decision is made. Poor pictures, description with typos, wrong categories, old&#x2F;wrong pricing etc reply vidarh 20 hours agorootparentYeah, that can be a good approach. Including all the data defining the old product in an unchanging way was one of the sticking points that led to the creation of my team at Yahoo. The US team didn&#x27;t think twice about changing invoice templates for examples, while the European finance teams broke out in hives at the thought of anything changing on already issued orders, much less invoices. I spent three years holding the fort to ensure the US team didn&#x27;t take over invoice processing on our behalf until they could guarantee at least the invoices were unchanging... reply swader999 20 hours agorootparentEverything has to be historically available. Think about returns and redoing the entire period and so on. Sometimes even the old code if you want to be perfect. reply wredue 20 hours agorootparentprevThis approach is generally okay. It at least keep your mandatory data for audit around.One issue with it is that it often ends up forcing you to build out a lot of backward compatibility into your audit system as things change. reply hrunt 21 hours agoprevHaving gone through &#x2F;exactly&#x2F; what this article is about, I know the pain. But I will point those that don&#x27;t read the article to the last paragraph:> We considered implementing an off-the-shelf billing solution but there was nothing flexible enough and the switching costs were too high. Algolia also tried to migrate to Zuora before backing out and rebuilding their billing system for the fourth time.Most (all? I have yet to find one) off-the-shelf systems are not geared towards everything businesses want to do with their billing. If you take the author&#x27;s advice and choose one of these systems early on, you will have the exact same headaches. Either the system simply won&#x27;t let you do what you want to do or the system &#x2F;will&#x2F; let you do what you want to do with custom or hack-ish solutions that reproduce the custom-solution problem, but in someone else&#x27;s system. Those that implement broad swathes of billing functionality are so complex, they make &#x2F;everything&#x2F;, even the most basic stuff, hard.BTW, when I say I know exactly how the author feels, all of what they described we encountered and implemented. We even looked at a migration to Zuora and came away with the same conclusion (also: really freaking expensive). We even had a new product that we setup on a third-party billing system, and we ran into the flip-side of the problems; we were not able to implement some billing functions we needed and we had to migrate off. reply akitzmiller 18 hours agoparentI came here to pretty much make the same comment. If you have a business that can get away with OTS software, more power to you. But complicated things are complicated and complex billing rules can be make-or-break for a company. Whether you start with 3rd party or start home grown, billing systems often need humans and there really isn&#x27;t a substitute. reply 6510 21 hours agoparentprev> there was nothing flexible enoughThey would have to solve the same problem in a generic way. (add 10 more years)> we had to migrate offAdds to the fun doesn&#x27;t it? My missing features hacked around previously exploded into an almost impossible puzzle. reply zer0x4d 20 hours agoprevBilling in general is very hard. Especially subscription billing. Just from my experience in building billing and also later on using a third party system for another projects, I can mention so many edge cases:1. Grandfathering2. Upgrading across different length memberships (monthly tier 1 to yearly tier 2)3. Crossgrading across different lengths (monthly tier 1 to quarterly tier 1)4. Offering free trial to tier 2 when user has tier 1.5. Promotional offers to non-paying users (half off first month)6. Downgrades7. Applying coupons to accounts (customer support wants to offer a specific user a free month of service when user has a quarterly subscription: billing schedule change)8. Differences in tax regulations across countries (VAT vs whole value tax)9. Differences in display prices (in some countries displayed prices should include tax)10. Handling currency exchange rate changes. reply Rafsark 16 hours agoparentI could add: - metering - relations with accounting&#x2F;finance softwares - timezones (if your customers are spread around the globe) - proration - payments and dunnings reply 1-6 17 hours agoparentprevSounds a lot like Autodesk. reply freefaler 21 hours agoprevLike with everything, wrap your billing in a layer and when you grow enough you put your billing under it if you need it or switch to a different provider without changing your codebase.It&#x27;s not wise to outsource parts that are essential for your business, because: 1. you depend on 3rd party service for a getting paid 2. your operations cost may rise or the service may change terms&#x2F;functionality reply bluGill 20 hours agoparentYou always need to outsource things. Unless you live on a desert island alone some things are out sourced. Most of us out source barter to a government run cash system, which we then further outsource to various banks.While out sourcing is a risk, it is also an opportunity to offload the hard work. You need to figure out what is worth doing yourself; what you outsource completely and ignore; and what you out source and audit.Accounting - including billing, is normally something you should out source and audit. Let someone else worry about all the hard and weird rules. However you need to audit it because if you do not someone corrupt will steal your money and you are still required to pay whoever you owe money after that happens - this is one way your company can go bankrupt. reply freefaler 17 hours agorootparentThat&#x27;s why you need an abstraction layer for each 3rd party provider. I&#x27;ve been doing online businesses 20+ years and every time we didn&#x27;t create an abstraction layer our switching cost rose.Many \"startups\" who are hungry enough to provide good value for money switch to \"enterprise\" pricing when investor money run out and your operations costs may rise a lot or you get \"discontinued\". Look what Atlassian did with self-hosted versions.Also for tracking&#x2F;logging layers it may be beneficial to mirror the data you&#x27;re sending to 3rd party providers to your own db&#x2F;logs, because you won&#x27;t get data locked when switching.So my point is that you need to outsource smartly and keep in mind that the 3rd party dependencies increase operations risk.BTW that&#x27;s why any user request shouldn&#x27;t depend on 3rd party resources in a blocking matter (e.g. synchronous JS from vendors&#x27; CDN). You might pay more for your hosting but if you factor the service disruption risks you may be better off for mission critical parts. reply brianmcc 21 hours agoprevI would instead say - commit properly to building your billing systems as a major and ongoing project, don&#x27;t dismiss it as a 2 week \"fun\" side thing.Not convinced any 3rd party product is going to have the flexibility to support ongoing innovation. If anything it&#x27;s more of a case for in-house expertise and resource. Find some folks who want to be the billing guys!Disclaimer: I built a large in-house billing system :-) reply agos 21 hours agoparentsee also: e-commerce systems. Hard to build, a lot of work, but way better than any 3rd party reply calvinmorrison 20 hours agoparentprevInterestingly, fastmail just flipped to having an external billing system after years of maintaining their own. AND when they bought POBOX they got that whole billing stack as well. I can imagine the nightmare but also the relief after exporting all of that work to another company.perl billing system: https:&#x2F;&#x2F;github.com&#x2F;fastmail&#x2F;Moonpig reply re-thc 20 hours agorootparent> Interestingly, fastmail just flipped to having an external billing system after years of maintaining their own.Would have nothing to do with your or someone else&#x27;s decision. And Fastmail may or may not have made the right decision. It&#x27;s hard to say. There are no simulators or time machines to gauge the impact anyway.The most likely answer is preference and available skillset in the area. I&#x27;ve seen managers that absolutely want to self build and those that never want to maintain anything as much as possible (by offloading). reply lowercased 20 hours agoprevWhy do I have the feeling that many people starting projects like this get in to situations where functionality is identified and they&#x27;re beat down with \"YAGNI!\"? I&#x27;ve been in situations where &#x27;build v buy&#x27; comes up - accounting, inventory, and similar domains. Identifying \"hey, we need to keep XYZ data to allow for future reporting...\" has been met with chants of \"YAGNI\" from people blind to the complexities.It&#x27;s just one of the things that might lead me to opt for &#x27;buy&#x27; in &#x27;build v buy&#x27;, but... it&#x27;s hard to trust the sales people involved in the process as well. And... taking the time to do real and full analysis... is time people often don&#x27;t want to pay for.And... once you&#x27;re \"in\" to a COTS system... you&#x27;re almost never leaving. The companies know that, the salesfolks know that, and have a lot of incentive to handwave away concerns, or just outright lie. Once you discover the lies... it&#x27;s likely too late to switch. reply hk1337 20 hours agoparentThat situation is why I don&#x27;t like about the software development process. YAGNI gets abused to go with the sloppy shortcut that only really saved maybe a day in creation but in reality wastes months worth of maintenance. reply wredue 20 hours agoparentprev>Once you’re “in”, you’re almost never leavingYeah. This problem is real. It’s so expensive to get out that we have had to make business process concessions because the COTS ended up not supporting, what I consider, basic functionality.We only found out years later on the basis of changing other business processes.And vendors will bleed you dry if they can. What some vendors consider to be “customizations” is insane. reply eternityforest 3 hours agoprevI&#x27;m amazed anyone would ever want to DIY something that touches money.People tend to get pissy about money. Almost as much as water plant scada systems and railway safety. Maybe moreso.And if you&#x27;re dealing with money, you&#x27;re already inherently doing cloud stuff, because you&#x27;re talking to the payment services, plus you already by definition have money to pay for a real billing system...It seems as crazy as people on r&#x2F;wallstreetbets advising their parents on how to put their retirement money into hand picked stocks or something. reply mlhpdx 20 hours agoprevThe point the article makes to me (as a leader) is “keep the billing model simple, honest and stick with it”. The cost of change is high, and the value isn’t to the people getting the work done. My peers would likely see this as “naive” and extoll the need to “sell to the buyer”. I see it as being value focused, and keeping the “business needs” rational, and in line. reply Rafsark 17 hours agoparentWhen you don&#x27;t understand the complexity of billing (revenue ops, top management or marketing), you often end up adding more and more complexity, and engineering teams have no choice. They have to build new billing features reply fmajid 17 hours agoprevI&#x27;ve worked on telco billing systems in the past and the generally accepted metric in the industry is 70% of billing systems projects fail. Not fail as in \"did not completely meet requirements or expectations\", fail as in \"did not deliver anything whatsoever\". reply PeterStuer 3 hours agoparentThat is the standard for any type of large enterprise software project though. reply oblib 16 hours agoprevI&#x27;ve been working on an simple invoicing app for small businesses for over 20 years. It has become a somewhat complex project and my app does not do much of what&#x27;s cited in this article.>> Now, when someone asks for advice about their billing system, my answer is clear: DO NOT build it yourself.Moving from using CGI.pm to CouchDB&#x2F;PouchDB about 10 years ago made it much easier to manage users and their data, and implement the UI. It also moved most of the workload to the user&#x27;s web browser and made the app much faster for users.CouchDB was practically designed for this. Early on their developers used \"invoicing\" as an example for using CouchDB.But... if you have a marketing team that keeps moving the goals it wouldn&#x27;t matter what you built the app with, it&#x27;s going to take time to build and debug it. Could be that CouchDB&#x2F;PouchDB could make that easier, but my experience is developers who&#x27;ve only been using SQL may get frustrated with it. reply m3nu 21 hours agoprevI built my billing system on top of Django and it&#x27;s used in 2 online services for a few thousand users and hundreds of invoices a month.You have to be mindful of some edge cases, but if the scope is kept narrow, it&#x27;s doable. The added benefit is the flexibility to fit it to your use case. And with Python, you get many high-level libraries, e.g. for decimal calculations and PDF generation. reply yugarinn 21 hours agoparentMy experience is that the scope is never kept narrow, which is exactly the same that the author is describing. reply acutesoftware 21 hours agorootparentWell, you can - you can have a policy that forces payments from a select group. Some users will complain when \"No, you cant pay with 2 chickens every 3rd full moon\", but that is too bad. reply dagw 21 hours agorootparentOf course you then have to be willing to turn down money&#x2F;customers. As someone who has worked for quite large traditional companies, we&#x27;ve ended up not buying from certain companies because they couldn&#x27;t invoice us like we wanted to be invoiced. reply 6510 20 hours agorootparentAnd miss out on all the fun with buyer-created invoices? reply icedchai 21 hours agoparentprevI built a billing system for a web hosting company in PHP, almost 20 years ago. This was for a few hundred users. There were a couple of edge cases, but nothing incredibly challenging. reply Rafsark 17 hours agorootparentWas it a basic subscription model or have you faced challenges with taxes, usage-based components, entitlements, grand fathering? reply icedchai 16 hours agorootparentBasic subscription and it was 20 years ago. There was some grandfathering but that was pretty simple to solve with a status column on a plan table, etc. reply Rafsark 17 hours agoparentprevThe use case is tiny at the beginning but the complexity increases over time to be honest. The scope is never as narrow as you think at the beginning of the project. Libraries are great, they can help you implement faster, but if the company grows, you will need an entire team to build and maintain it reply Pesky_Bees 1 hour agoprevThat&#x27;s why you hire Product and UX designers so the engineers don&#x27;t run wild into endless attempts to find an initial solution through the rabbit&#x27;s hole. reply robaato 18 hours agoprevWorked in mobile Telecoms billing back in the relatively early days. Remember discovering that Orange at the time made a good feature of competing (and acquired lots of new customers) by offering \"competitor match\" billing options. They made this as a business choice, but it cost them a team of over 100 contractors developing that system. Other telcos had packages but were constrained by what the packages offered as to what their marketing could offer. You pays your money and makes your choice! p.s. things learned the hard way in those early days - customers realised that a call held open for over 24 hours resulted in a reset of billing due to counter wrap around! reply PeterStuer 4 hours agoprevI was waiting for them to discover the rabbit hole that is &#x27;day counters&#x27;. (I wrote a lease payments calculator for the financial services industry. Many gotcha&#x27;s lurk there.) reply agentultra 18 hours agoprevIn my experience this happens frequently enough that there ought to be an article like this for every problem domain that software developers and executives under-estimate the complexity of.I work on card transaction processing and it&#x27;s absolutely more complicated than you can imagine looking at it from the outside. It&#x27;s a vast, distributed peer-to-peer system where trust is built into liabilities outside of the network itself. You want to build a system that is correct with regards to some specification in order to protect peoples&#x27; money... but the problem is that there is no specification that enumerates every possible transaction state because while there are \"typical\" sequences of messages to handle, your system also has to handle unexpected sequences that can&#x27;t be specified: there are no guarantees that systems on the other end are emitting properly formatted events let alone behaving as expected. Card payment handling systems have to have the ability to manually correct and adjust state by humans; reconcile with external systems (because yay, payment systems in the US aren&#x27;t based on instantaneous settlement until the roll-out of FedNow is complete), etc.I&#x27;ve watched many businesses walk straight into, it&#x27;s just X, how hard can it be? Only to watch their ARR shrink and stress levels rise when a project they thought would take a month turns into a year-long journey of discovery, reflection, and increased head-count. reply garganzol 20 hours agoprevI wrote a billing system myself, and my first impression was that it couldn&#x27;t get past 1000 lines of code. However, it had become a ~5000 lines monster and keeps on growing. Never underestimate details and edge cases. reply pantulis 20 hours agoprevBilling is one of those problems that seem deceivingly easy. Just talk to someone who has worked in telco billing and learn about the 4 horsemen of billpocalypse: metering, mediation, accounting, billing. reply trollied 19 hours agoparentRating can be great fun. US taxes (geocoding, urgh) are an absolute nightmare. Reversed charged calls, revenue sharing. Roaming is a pain (ASN.1 files). Even document numbering is a nightmare in some countries, as are legalities surrounding incorrect bills (corrective invoices) and crediting. Did 15 years in billing (now do finance software), and whenever I thought I knew it all there would be another curve ball thrown. reply Rafsark 17 hours agoparentprevYou can add taxes and negotiated contracts to the list tbh reply pmontra 17 hours agoprevI worked for a mobile phone operator many years ago. I was there when the company was bootstrapped. We built our own billing system because we would be billing for stuff that incumbents didn&#x27;t bill for, because they didn&#x27;t offer those services: we were the first 3G operator of the country and we also had to create and sell contents (example: YouTube was not born yet.)I was not in the billing team but we definitely followed rule #4 of the post: \"You must be prepared to staff an entire team\".And yes, #1 \"Pricing changes all the time and billing needs to follow\".About #2 \"Your billing system needs to scale with your user base\", we had to go from 0 to 3 million customers in 9 months to be viable. We were not typical, we made it.No idea about #3 \"Grandfathering causes headaches\" but there were probably many headaches, that one and others. reply mattboardman 21 hours agoprevIt looks like one core issue was designing the system around pre-determined bounds (monthly, yearly, etc). This happens in lots of systems and isn&#x27;t specific to billing. A job scheduling system will have the same problem (we built it to track seconds and now they want us to track calendar dates!). reply Rafsark 17 hours agoparentNavigating this challenge involves the ever-changing boundaries of billing. Initially, you build a monthly billing system, only to find that your team requires a yearly one, which seems relatively straightforward. However, complications arise when you introduce usage-based billing, incorporating weighted values akin to storage, layered on top of a quarterly plan. This complexity is further exacerbated when custom billing structures are needed for negotiated contracts, adding another layer of intricacy to the mix. reply jabart 18 hours agoprevWhen we launched in 2017, we went \"in-house\" with our billing. We found an open-source project and uses that for all billing as it had a recurring option and a billing portal. Fast forward a number of years, we now sync invoice data in the app and have a tiny billing portal there. We can still adjust invoices, keep old pricing, setup new pricing, auto-bill, check balances to see if a customer hasn&#x27;t paid yet. Total Lines of Code here are like under 2000 for this integration and it&#x27;s great. Also zero vendor lock in as well. Just another way to approach a billing system. reply jasonjayr 18 hours agoparentWhat open source project did you use? reply jabart 18 hours agorootparentInvoiceNinja. The install instructions had a few misses but has an API and webhooks so it&#x27;s great to integrate with. reply swader999 20 hours agoprevNow take your system and make it multi tenant. Imagine the horror of doing that. This is what you often get with \"off the shelf\". There&#x27;s a lot of broken dreams either way you go. reply ftxbro 21 hours agoprevThis is an ad by a company that sells billing systems. This is what ads look like now. reply danenania 18 hours agoparentA lot of useful content gets produced by companies as a form of marketing. Just like independent devs are often blogging with the goal of getting consulting gigs or advancing their careers. There&#x27;s a lot of garbage too, but if the content is good and reasonably objective, what&#x27;s the problem? reply tylergetsay 21 hours agoparentprevSame with \"Role your own auth\" reply dboreham 20 hours agorootparentI see what you did there. reply jwsteigerwalt 9 hours agoprevThe number backend engineers with competency in recurring revenue accounting is too small. Strange in our world that is more and more subscription driven. reply somsak2 19 hours agoprevShould have a (2022) label https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20220715000000*&#x2F;https:&#x2F;&#x2F;www.getl... reply siva7 18 hours agoprevI was that 20-something guy tasked with building a homemade billing system. Hint: Don&#x27;t. reply marcosdumay 17 hours agoparentIf you are not afraid of it, it means you don&#x27;t know nearly enough to be successful.Maybe there are 20-something people out there with enough world knowledge to make one. But it&#x27;s not a safe bet at all. reply Rafsark 17 hours agoparentprevEven if you find that guy, you&#x27;ll have to find 20 more in the next 3 years if the company grows reply cmilton 18 hours agoprevWhy are we against modifying our processes to align with a particular software platform? Maybe it is much easier to follow such guidelines during the early stages of a company, but much harder later on. Do standards exist in billing systems? reply kosolam 17 hours agoprevHow about accounting, would anyone recommend a modern open source accounting system that would be better than in-house? reply Rafsark 17 hours agoparentNot necessarily open source, but there are a lot of third party tools for accounting (netsuite, quickbooks, xero). And I would say they scale 100x better than an in house tool reply kosolam 48 minutes agorootparentNot open source is not interesting. You could just go with SAP and be done with it. reply hydroid7 21 hours agoprevI think it depends on the business you run. To be honest, I would not mind building a billing system if my company grows to an unicorn! reply Rafsark 17 hours agoparentHow do you know in advance it will grow into a unicorn? I tend to agree with you, but that&#x27;s a shame to have a team of 30 engineers working only on billing, even if you are a unicorn.. I would prefer to have them working on my product reply baq 18 hours agoprevI&#x27;m working on an in-house billing system right now. My only comment is &#x27;yes&#x27;. reply jerf 18 hours agoprevAnother problem with billing systems I&#x27;ve seen repeatedly is that the billing demands flows one way; marketing or product management basically scribbles on a napkin how they&#x27;d like to bill the customer, and that napkin gets passed to the billing team as a specification carved into stone. Basically, in practice it gets run as Waterfall, actual factual Waterfall, except the design phase is skipped. It goes straight from features to implementation.Now, on a business level I acknowledge that frequently the monetary costs of the billing system are negligible compared to the business advantages of the \"correct\" billing system. My complaint is more that the scribbled note often ignores what the rest of the company is doing for billing, e.g, \"yes usage based billing is great but do you really need to bill on furlongs per fortnight when the rest of the company is billing based on seats or some basic, easy-to-understand resource allocation?\", ignores that maybe there&#x27;s an easy thing that&#x27;s close to what you said but since it reuses all the existing stuff you can have it in two weeks instead of four months, etc. Do you really need to issue a discount coupon that takes off a percentage proportional to the length of the customer&#x27;s first name, unless the customer is in a jurisdiction where that&#x27;s illegal in which case we roll a die, unless the customer is in a jurisdiction where that&#x27;s illegal in which case we give them $10 off their first $100 and take off every prime-numbered dollar after that? Because I&#x27;ve got code that just takes 10% off their first month right here. Do you really need to bill on the first of the month unconditionally when the rest of the company bills based on subscription start date, or vice versa? Even if you&#x27;re not too worried about the costs of paying 6 months of developers, I bet you are worried about those 6 months of opportunity cost.I can&#x27;t even count the months of delays caused by treating napkin scribbles as carved stone I&#x27;ve personally witnessed, and I&#x27;m only tangentially involved in billing over all.And somehow managers who are deeply familiar with costs and benefits and tradeoffs and make sensible decisions all the time just become the most obstinate customers when it comes to billing. No matter how small the tweak proposed and how many months forward it may bring your release date to do something just slightly different (and consistent with the rest of the company&#x27;s existing policies), they will go to the wall for their billing deviation, even when it was frankly clearly nothing more than a whim or a transient thought at some point by somebody somewhere of no strategic or marketing consequence.It isn&#x27;t just engineers who underestimate the complexity of billing until it&#x27;s too late; it&#x27;s everyone, really. \"Just\" print an invoice turns out not to be so simple. reply Rafsark 17 hours agoparentMarketing teams can sometimes miss the mark on this one. There&#x27;s a misconception that improving billing is as simple as launching a new marketing campaign, but that couldn&#x27;t be further from the truth. Billing is a multifaceted aspect of engineering and business operations, and those who solely focus on it often prioritize following industry trends over gaining a deep understanding of the economic intricacies within the company.It&#x27;s crucial to bridge this gap in comprehension for more effective decision-making and strategy alignment between the marketing and billing departments. By doing so, we can ensure that both teams are on the same page and working together seamlessly to drive the company&#x27;s success. reply 1970-01-01 19 hours agoprevCreating and maintaining a billing system would be another good test for AI. reply Rafsark 17 hours agoparentWhat&#x27;s your vision? Can AI handle revenue prediction and billing engineering tasks? Both? How? reply 1970-01-01 14 hours agorootparentResolving the billing edge cases would be a test. There is no vision, it&#x27;s a question: Is your flavor-of-the-week AI &#x27;smart&#x27; enough to design and code a billing program for my startup? How about my medium-sized company with 4 offices in 3 different countries and 4 timezones? reply TomNguy 20 hours agoprevHow do you monitor for tax compliance changes, especially for international usage? reply Rafsark 17 hours agoparentI think billing, taxes and entitlements are 3 different products, with obviously strong relationships. I would rather give the advice to rely on a third party tax tool that is connected to the billing engine instead of the \"all in one\" if taxes are too complex. Otherwise, just use the basic tax codes of Lago&#x27;s billing engine reply madjam002 20 hours agoprevI’ve found Temporal and Xstate are helpful at building a reliable billing system reply Rafsark 17 hours agoparentCould you explain how? I know temporal has not built the billing engine themselves, so wondering how the tool can help with this.. reply crawsome 19 hours agoprevThis is an ad reply say_it_as_it_is 20 hours agoprev [–] Taxes alone are complicated enough to warrant the existence of TaxJar. Billing systems should not be underestimated. reply Rafsark 17 hours agoparent [–] This is completely true. Taxes, entitlements and billing can be 3 different products. I don&#x27;t understand how people think it&#x27;s a 3 week effort. Avoiding this separation can lead to a messy setup down the road. replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article spotlights four key issues with homemade B2B neobank billing systems: requirements for frequent pricing changes, scalability concerns, problems with grandfathering present plans, and the need for a dedicated billing team.",
      "The author recommends against creating an in-house billing system and emphasizes considering pre-made solutions early in the development.",
      "The case of Algolia's struggles with in-house billing is presented as an example, and the article ends with stressing the significance of an early choice on billing system implementation to evade complexity and compatibility issues."
    ],
    "commentSummary": [
      "The article covers the dilemma of picking between custom in-house billing systems or ready-made solutions, discussing potential difficulties with third-party options supporting complex requirements and migration processes.",
      "It underlines the significance of conducting fit-gap analyses for prospect software choices, and mentions a trend where companies are shifting from custom-made systems to commercial ERP (Enterprise Resource Planning) systems due to limits.",
      "There's the emphasis on understanding the intricacies of financial accounting software, the risks of vendor lock-in, and the complexity of constructing a resilient billing system - all highlighting the need for comprehensive knowledge of billing procedures for informed decision-making."
    ],
    "points": 173,
    "commentCount": 99,
    "retryCount": 0,
    "time": 1695039341
  },
  {
    "id": 37554406,
    "title": "Unix shells are generally not viable access control mechanisms any more",
    "originLink": "https://utcc.utoronto.ca/~cks/space/blog/sysadmin/UnixShellsNoMoreAccessControl",
    "originBody": "Chris Siebenmann :: CSpace » blog » sysadmin » UnixShellsNoMoreAccessControl Welcome, guest. Unix shells are generally not viable access control mechanisms any more September 17, 2023 Once upon a time, if you had a collection of Unix systems, you could reasonably do a certain amount of access control to your overall environment by forcing logins to have specific administrative shells. As a bonus, these administrative shells could print helpful messages about why the particular login wasn't being allowed to use your system. This is a quite attractive bundle of features, but unfortunately this no longer works in a (modern) Unix environment with logins (such as we have). There are two core problems. First, you almost certainly operate a variety of services that normally only use Unix logins as a source of (password) authentication and perhaps a UID to operate as, and ignore the login's shell. This is the common pattern of Samba, IMAP servers, Apache HTTP Basic Authentication, and so on. In some cases you may be able to teach these services to look at the login's shell and do special things, but some of them are sealed black boxes and even the ones that can be changed require you to go out of your way. If you forget one, it fails open (allowing access to people with an administrative shell that should lock them out). (One of these services is SSH itself, since you can generally initiate SSH sessions and ask for port forwarding or other features that don't cause SSH to run the login shell.) Second, you may operate general authentication services, such as LDAP or a Single Sign On system, and if you do these authentication services are generally blind to what they're being used for and thus to whether or not a login with a special shell should be allowed to pass this particular authentication. The only real solution is to have multiple versions of these authentication systems with different logins in them, and point systems at different ones based on exactly who should be allowed to use them. A similar issue happens with Apache HTTP Basic Authentication in common configurations, where you have a single authentication realm with a single Apache htpasswd file that covers an assortment of different services. If you need certain logins ('locked' logins or the like) to be excluded from some of these services but not others, either you need multiple htpasswd files (at least) or you need to teach each such service to do additional checks. (In general you're going to have to try to carefully review who should be able to use which of your services when, and the resulting matrix is often surprisingly complicated and tangled. Life gets more complicated if you're using administrative shells for reasons other than just locking people out with a message, for example to try to force an initial password change.) Today, the only two measures of login access control that really work in a general environment are either scrambling the login's password (and disable any SSH authorized keys) or excluding the login entirely from your various authentication data sources (your LDAP servers, your Apache htpasswd files, and so on). It's a pity that changing people's shells is no longer enough (it was both easy and convenient), but that's how the environment has evolved. (4 comments.) Written on 17 September 2023. « Apache's HTTP Basic Authentication could do with more logging Making a function that defines functions in GNU Emacs ELisp » These are my WanderingThoughts (About the blog) Full index of entries Recent comments This is part of CSpace, and is written by ChrisSiebenmann. Mastodon: @cks Twitter @thatcks * * * Categories: links, linux, programming, python, snark, solaris, spam, sysadmin, tech, unix, web Also: (Sub)topics This is a DWiki. GettingAround (Help) Search: Page tools: View Source, Add Comment. Search: Login: Password: Atom Syndication: Recent Comments. Last modified: Sun Sep 17 21:44:38 2023 This dinky wiki is brought to you by the Insane Hackers Guild, Python sub-branch.",
    "commentLink": "https://news.ycombinator.com/item?id=37554406",
    "commentBody": "Unix shells are generally not viable access control mechanisms any moreHacker NewspastloginUnix shells are generally not viable access control mechanisms any more (utoronto.ca) 172 points by ingve 1 day ago| hidepastfavorite134 comments chatmasta 14 hours agoWhen you SSH into GitLab, you get a \"shell,\" which at one point was a locked-down Unix shell where you could only run Git commands. That&#x27;s a workable solution, but they (rightly) migrated off it [0], so now it&#x27;s just a Go program [1] that runs an SSH server and interprets a small list of valid commands. This basically inverts the security model. The Unix shell approach is leaky, because you never know if you&#x27;ve plugged all the holes while only allowing the commands you want. Whereas the Go program is only capable of executing a narrow range of commands, which it interprets itself, in its own fake shell, while exposing an SSH server.Fun fact: one of the commands [2] the shell implements is `personal_access_token`, so you can programmatically mint a new PAT, since you&#x27;ve already authenticated via your SSH key: ssh git@gitlab.com personal_access_token someTokenName api,read_repository,read_api,read_user,read_registry 90[0] https:&#x2F;&#x2F;about.gitlab.com&#x2F;blog&#x2F;2022&#x2F;08&#x2F;17&#x2F;why-we-have-impleme...[1] https:&#x2F;&#x2F;gitlab.com&#x2F;gitlab-org&#x2F;gitlab-shell[2] https:&#x2F;&#x2F;gitlab.com&#x2F;gitlab-org&#x2F;gitlab-shell&#x2F;-&#x2F;blob&#x2F;main&#x2F;inter... reply kccqzy 13 hours agoparentHow is that different from writing your own shell that implements a tiny subset of the functionality with support for a small number of commands? You can even write it in Go if that&#x27;s your preferred language. reply chatmasta 13 hours agorootparentThat&#x27;s exactly what it is, but the point is that it&#x27;s not a Unix shell, and it&#x27;s not running as a subcommand of SSH (which you could also do with a custom shell). There&#x27;s no tty involved, and so the attack surface is reduced - if you exploit a logic error in the Go code (e.g. parameter pollution), you won&#x27;t be able to do more than run one of the commands that it already runs. But if you exploit a logic error in a Unix shell with parameter pollution, then the attack surface is much greater. That said, of course if an attacker finds a bug in the Go code that enables arbitrary code execution, then the attack surface is just as wide as any other binary on the system, including a Unix shell, so you still need to lock down the privileges of the process itself. reply jolmg 10 hours agorootparentI think the point is: how is bundling a custom ssh server into the same executable that interprets the commands, a reduced surface as opposed to using standard openssh&#x27;s sshd, and defining a simple shell like so: #!&#x2F;bin&#x2F;bash cmd_foo() { printf \"called foo with %s\\n\" \"$*\" } cmd_bar() { printf \"called bar with %s\\n\" \"$*\" } run_cmd_call() { local cmd=\"$1\"; shift local cmd_args=(\"$@\") case \"$cmd\" in foo) cmd_foo \"${cmd_args[@]}\" ;; bar) cmd_bar \"${cmd_args[@]}\" ;; *) printf \"%s: unknown command\\n\" \"$cmd\" >&2 esac } while (( $# )); do case \"$1\" in -c) read -ra cmd_call &2 exit 1 ;; esac shift done if [[ \"$cmd_call\" ]]; then run_cmd_call \"${cmd_call[@]}\" else while printf \"> \" read -ra cmd_call do run_cmd_call \"${cmd_call[@]}\" done fi(then, of course,) sudo useradd -ms &#x2F;path&#x2F;to&#x2F;custom-shell.sh userfoo sudo -u userfoo bash -c &#x27;mkdir ~&#x2F;.ssh; printf \"%s\\n\" \"$your_pubkey\" >> ~&#x2F;.ssh&#x2F;authorized_keys&#x27;How would you \"do more than run one of the commands that it already runs\" with this from a `ssh userfoo@localhost` call? reply garblegarble 10 hours agorootparentDon&#x27;t forget to disable sftp in your sshd_config. and ssh port forwarding. and hopefully there&#x27;s no sneaky way that an attacker can push an environment variable to break something like with LD_PRELOAD, and are you sure there are no directly&#x2F;indirectly exploitable bugs&#x2F;omissions in your script logic for invoking complex commands like git, and that your distributed filesystem works quickly across a few million home folders for your authorized_keys files, ... reply jolmg 9 hours agorootparentEDIT: Crap. You&#x27;re right. Port forwarding worked... So there&#x27;s that. `ssh -J userfoo@localhost other-place` also worked... I would imagine that&#x27;s all able to be disabled, but I see that the user is not fundamentally limited to what the shell allows. That&#x27;s disappointing. I thought it would wrap everything in `custom-shell.sh -c` calls like it does with the sftp and scp servers. I imagine&#x2F;hope that under a default configuration one is still not able to touch the contents of the server if the shell doesn&#x27;t permit it, but being able to use it as a proxy or listen on its ports is not what I expected.ORIG_REPLY:> Don&#x27;t forget to disable sftp in your sshd_configDoing `strace -fp $(pgrep sshd)` then `sftp userfoo@localhost` in another terminal, I see: $ sftp userfoo@localhost Connection closed. Connection closed $then in the `strace` output: [pid 408173] execve(\"&#x2F;tmp&#x2F;custom-shell.sh\", [\"custom-shell.sh\", \"-c\", \"&#x2F;usr&#x2F;lib&#x2F;ssh&#x2F;sftp-server\"], 0x55c5f3bae0d0 &#x2F;* 14 vars *&#x2F;) = 0 [pid 408173] write(1, \"&#x2F;usr&#x2F;lib&#x2F;ssh&#x2F;sftp-server: unknown command\\n\", 42) = 42As far as I understand, SSH&#x27;s security of what a user can do is completely based on what the shell permits the user to do.> are you sure there are no directly&#x2F;indirectly exploitable bugs&#x2F;omissions in your script logic for invoking complex commands like gitIncluding a ssh server in the same executable doesn&#x27;t save you from such bugs.> and that your distributed filesystem works quickly across a few million home folders for your authorized_keys filesOpenSSH&#x27;s sshd seems quite flexible in ways to authenticate users. I don&#x27;t think having a home folder per user is necessary if you don&#x27;t want it.> and hopefully there&#x27;s no sneaky way that an attacker can push an environment variable to break something like with LD_PRELOADSSH&#x27;s default is to not accept the client&#x27;s environment, and this shell itself provides no way to manipulate them. They can be nonexistent in the shell&#x27;s interface if you don&#x27;t want them.I put the script so someone can offer some concrete way (as in show me some code) to bypass it in a way that having a custom integrated SSH server would prevent it. I&#x27;d love to see a `ssh&#x2F;sftp userfoo@localhost` call that does something other than call cmd_foo, cmd_bar, or erring out. reply garblegarble 17 minutes agorootparent>> Don&#x27;t forget to disable sftp in your sshd_config >Doing `strace -fp $(pgrep sshd)` then `sftp userfoo@localhost` in another terminal, I see: [...] >execve(\"&#x2F;tmp&#x2F;custom-shell.sh\", [\"custom-shell.sh\", \"-c\", \"&#x2F;usr&#x2F;lib&#x2F;ssh&#x2F;sftp-server\"]If your sshd_config had \"Subsystem sftp internal-sftp\" as a shipped default, then I don&#x27;t think it would have gone through the user&#x27;s shell. Not a major issue, but there are just so many little points to be certain are locked down>I&#x27;d love to see a `ssh&#x2F;sftp userfoo@localhost` call that does something other than call cmd_foo, cmd_bar, or erring out.The reference to LD_PRELOAD was to hint at that (since if the attacker can get a binary on the system -- e.g. in a git repo or config file) they could change what functions bash is invoking... I&#x27;m not a wiley enough attacker, but I bet if you put something valuable behind it and put out a public challenge you&#x27;d be impressed with the opensshd vulnerabilities and other interesting features of ssh&#x2F;tty&#x2F;environment variables.As you probably guessed, my comment was just listing a few of the things (today) that you&#x27;re playing whack-a-mole with when it comes to making sure everything is secure.Writing their own &#x27;ssh server and shell&#x27; program is a different set of tradeoffs vs trying to fully lock down programs that are intended to be general-purpose... to be perfectly honest I don&#x27;t think I&#x27;d want to be involved in either route, the stakes are stressfully high... but there sure is a lot less code to audit and test in writing your own ssh+shell than there is trying to secure and keep up-to-date sshd, pam, bash, etc. (especially since they&#x27;ll be adding features that you explicitly do not want for your purposes) reply Jenda_ 9 hours agorootparentprevThere was another hole if the target command was executed using &#x2F;bin&#x2F;bash in a certain way: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Shellshock_(software_bug) It&#x27;s probably not the case when the command is the user&#x27;s shell, but it was exploitable when it was the command= in authorized_keys.Additionally, maybe they are using a custom server because they don&#x27;t want to fork a new instance of the handler on every login.Here is a stackoverflow topic on limiting the SSH user: https:&#x2F;&#x2F;serverfault.com&#x2F;questions&#x2F;152726&#x2F;how-can-ssh-allowed.... They suggest a few more options, probably not relevant for a gitlab-like use-case.> SSH&#x27;s default is to not accept the client&#x27;s environmentsshd accepts environment variables specified by AcceptEnv in &#x2F;etc&#x2F;ssh&#x2F;sshd_config. By default, they are mostly related to locales. $ LC_ALL=en_US.UTF8 ssh x date Tue Sep 19 01:14:41 AM UTC 2023 $ LC_ALL=cs_CZ.UTF8 ssh x date Út 19. září 2023, 01:14:46 UTC reply jolmg 8 hours agorootparent> There was another hole if the target command was executed using &#x2F;bin&#x2F;bash in a certain way: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Shellshock_(software_bug)I agree bash is not the best choice security-wise, but language choice is a bit beside the point with regards to whether or not to bundle a server.> Additionally, maybe they are using a custom server because they don&#x27;t want to fork a new instance of the handler on every login.That&#x27;s very valid, though the discussion was about having a reduced attack surface from adding a custom server.> Here is a stackoverflow topic on limiting the SSH user: https:&#x2F;&#x2F;serverfault.com&#x2F;questions&#x2F;152726&#x2F;how-can-ssh-allowed.... They suggest a few more options, probably not relevant for a gitlab-like use-case.Thanks for that.> sshd accepts environment variables specified by AcceptEnv in &#x2F;etc&#x2F;ssh&#x2F;sshd_config. By default, they are mostly related to locales.That seems to be a Debian-specific patch. Upstream doesn&#x27;t include AcceptEnv in the default config file and there seems to be no default value. Your example commands fail to set LC_ALL on my system. `ssh -o SetEnv=LC_ALL=...` also fails. reply Too 5 hours agorootparentprevBash actually has this feature built in. Restricted shell they call it. Start it as rbash or bash -r.It will lock down the shell to not allow cd, setting envvars, launching commands outside working directory and a lot more.If you look at the full list of things they disable you realize how many obscure holes are available you never would have thought about.Not that I would ever trust it enough to expose on a shared box. Likewise with a tailor made shellscript. I’d take a bespoke server in go any day. reply jart 4 hours agorootparentprevWhat difference does it being a subcommand make? Just use a seccomp filter. It&#x27;ll apply to all descendent processes. reply tedunangst 9 hours agorootparentprevAttackers cannot get access to features which don&#x27;t exist even if you forget to configure them off. reply fooblat 22 hours agoprevThis article could have been published 30 years ago. In professional unix admin circles this was already well known back them. Although I could be misreading it as the article is not very clear. I think this are the points it is trying to make:1. Once upon a time you could rely on the passwd file and shell behavior as an effective means of authentication and access control.2. It has been a very long time since that was an effective approach, for a variety of reasons, and you should not do this on modern production systems. reply tremon 20 hours agoparentEven 30 years ago, the core argument would have been nonsensical.1. They introduce their argument as if it is solely about shell access (the conclusion also only mentions \"login access control\"), but then the first example&#x2F;statement they make is about non-shell access (Samba, IMAP, Apache).2. The second argument conflates authentication and authorization, and concludes that to implement shell authorization properly, your only choice is to provide multiple authentication systems.Zero effort is spent on explaining why existing&#x2F;historic shell authorization systems (such as simple DAC groups or rbash) are inadequate, and it&#x27;s not clear to me what threat model they are using to arrive at their conclusion.edit: rethinking this, I think TFA is just lacking a clear problem statement. They seem to be talking specifically about non-shell services that (ab)use the user&#x27;s shell field in &#x2F;etc&#x2F;passwd as authorization information, and then complaining that many services did not follow suit. reply hinkley 19 hours agorootparentFew contractions foment confusion as much as “auth”. Don’t do it. reply AceJohnny2 14 hours agorootparentauthn vs authz: Authentication vs Authorizationauthn&#x2F;authentication: user proves who they are, with username&#x2F;password or otherwiseauthz&#x2F;authorization: based on who the user is, system determines what they are allowed to do, via group membership or otherwise reply AndrewDavis 14 hours agorootparentauthz may be confusing to non USA English speakers. I wouldn&#x27;t make the connection without it spelled out to me. Unfortunately I don&#x27;t have a better suggestion because auths as short for authorisation is probably worse. reply Affric 10 hours agorootparentIf you work with computers (rather than using them) and don&#x27;t default to USA English when discussing and using them you are likely in for a bad time. reply raxxorraxor 3 hours agorootparentprevI think it is less confusion than just calling it auth. I have read many articles about basic auth vs oauth. But the auth here isn&#x27;t the same. reply matttproud 5 hours agorootparentprevTBH, we&#x27;d be better if without any of the contracted forms. reply hinkley 14 hours agorootparentprevYou can&#x27;t pronounce authn and authz very well, but to be perfectly honest I&#x27;m not sure if that falls under the &#x27;pro&#x27; or &#x27;con&#x27; column. reply fragmede 12 hours agorootparentI think it&#x27;s a pro. in saying auth-enn and auth-zee (zed), it&#x27;s clear which of the two you&#x27;re talking about. reply PH95VuimJjqBqy 15 hours agorootparentprevthe only exception is if you mean both, but even that&#x27;s confusing if the context isn&#x27;t clear.spell them out or use authn&#x2F;authz. reply bigbuppo 11 hours agorootparentprevYou&#x27;re not thinking like a thought leader. reply 1vuio0pswjnm7 13 hours agoparentprevThis blog of generally gibberish hits the HN front page with an astounding frequency. IMHO, there are many interesting blogs on \"system administration\" topics that are submitted to HN every week that never reach the front page while there are a handful of familiar, low-quality ones that routinely appear on page one. reply itsanaccount 12 hours agorootparentMuch of tech is a theatre, a jobs program that keeps people employed in a middle class salary so long as they diligently pretend to be engineers. This theatre serves as a prop for a higher level theatre in our virtual economy for investors and their game of financialization.Its expected that as tech grows in number of workers clutching to that middle-class life-raft that the baseline of knowledge discussed in tech spheres (like this site) will sink lower. reply fragmede 12 hours agorootparentIs it September already? reply type0 11 hours agorootparentit has been for 30 years reply wang_li 19 hours agoparentprevI think the point being made is that the fact that a user has rksh as their shell means nothing to samba, ftp, some features of ssh, httpd, cron, and etc. Fundamentally unix has pretty simple permissions, you&#x27;re either root or you&#x27;re not. The existence of a user account on a system is often enough to enable SMB and SSH access even if the only purpose of the account is to own files and an application process and is never intended to have interactive logins or to transfer data to and from the server. reply nightpool 18 hours agorootparent> I think the point being made is that the fact that a user has rksh as their shell means nothing to samba, ftp, some features of ssh, httpd, cron, and etcWhich has been true for...... 30 years? If not longer? reply Attummm 22 hours agoparentprevWould be possible to share those reasons? reply crabbone 21 hours agorootparentHere are some:* Doesn&#x27;t scale. Having passwords in a plain text file is not a scalable solution for users directory. Can probably go up to a hundred users, but not much more.* In computer clusters you want user identity to \"stick\" to the user when they use multiple machines, containers etc. That&#x27;s why you have LDAP... but it doesn&#x27;t help all that much because user id is encoded into the file system (huge mistake...) which makes it very difficult to contain users to things they should control. If your only mechanism was the &#x2F;etc&#x2F;passwd, it would mean you&#x27;d have to constantly synchronize this file across all those machines and containers you have. reply Attummm 20 hours agorootparentIt may be old and not particularly appealing, but LDAP has been serving that role effectively at many companies.[0]Using a terminal remains standard practice for sysadmins and devops.[1]I believe there&#x27;s some confusion in both the article and the comment between authentication and authorization. LDAP is fully equipped to handle both tasks.[0]https:&#x2F;&#x2F;access.redhat.com&#x2F;documentation&#x2F;en-us&#x2F;red_hat_enterp...[1]https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;cli&#x2F;latest&#x2F;userguide&#x2F;cli-chap-co... reply hiAndrewQuinn 21 hours agorootparentprevTo anyone reading this and thinking \"yeah dummy, of course it doesn&#x27;t scale because you&#x27;re not supposed to store passwords in plain text in the first place\" I&#x27;ll direct you to Chapter 7ish of The Linux Programming Interface.If you look in your &#x2F;etc&#x2F;passwd right now, you&#x27;ll almost certainly see a single \"x\" where the (EDIT: no, it was still encrypted!) password originally was - nowadays that single \"x\" is an instruction to go look in &#x2F;etc&#x2F;shadow instead, for the salted hash of the password you&#x27;re trying to check.I think this minimizes the number of users who need read permissions to &#x2F;etc&#x2F;shadow, and the amount of time they need it for.This has been your seemingly useless bit of Linux trivia for today. :) reply vajrabum 21 hours agorootparent&#x2F;etc&#x2F;shadow was born not because &#x2F;etc&#x2F;passwd had a plain text password but because the hashes became crackable and &#x2F;etc&#x2F;passwd is a public read file. Linux has never had them. Here&#x27;s the man page indicating encrypted passwords for Unix v7 &#x2F;etc&#x2F;passwd release in 1979: https:&#x2F;&#x2F;man.cat-v.org&#x2F;unix_7th&#x2F;5&#x2F;passwd reply hiAndrewQuinn 20 hours agorootparentWhoops! My bad, this is an even better bit of trivia.My mistaken memory really sells the underlying point that everything old is new again. reply hinkley 19 hours agorootparentprevI have a vague recollection of my 20 floppy of Slackware already having &#x2F;etc&#x2F;shadow. That would have been fall of 92 or winter 93, based on where I was living at the time. reply fuzztester 11 hours agorootparentShadow file was definitely from quite a while ago.Can&#x27;t remember exact date, but might have been around time of SVR4 intro.I know because I remember going \"ugh\", but without investigating the reason why it (shadow) was introduced :) - which was of course wrong on my part. reply mmcgaha 18 hours agorootparentprevI have a vague recollection of being given the choice on a 90s vintage distribution with some warning about security and password length if I did not use shadow passwords. At some point in the early 2000s we started authenticating regular users against AD but the shadow file was still there for root. reply hinkley 16 hours agorootparentWe had a couple of labs of sparcstations that just went away a couple of times a year because something bad would happen with all of the NFS mounted partitions and they&#x27;d have to turn the cluster on one box at a time to prevent thundering herd issues with NFS.I think they may have been mounting parts of &#x2F;etc as well. People get the idea that managing accounts for a cluster of boxes should be centralized. It&#x27;s all fun and games until the network mount disappears. reply chasil 20 hours agorootparentprevThat was not a \"plaintext password,\" it was a DES hash (from 7th edition onwards).This is the same format used by the classic htpasswd utility.https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Crypt_(C)#Traditional_DES-base... reply dsr_ 21 hours agorootparentprevplaintext vs plain textunencrypted vs unstructuredOf course, unstructured is also incorrect; the passwd and shadow files have structured records, one per line. reply thwarted 17 hours agorootparent…and being structured, the passwd file content should be accessed with the getpwent family of functions. reply bhawks 9 hours agorootparentWhich unfortunately are not thread safe. reply neuromanser 16 hours agorootparentprev\"unencrypted\" is normally written as \"cleartext\". \"plaintext\" means \"(readable &#x2F; intended to be read) without a special viewer\". Your.ssh &#x2F;id_rsa is plaintext but not cleartext. reply hinkley 19 hours agorootparentprevMy school had 30k students, including grad and doctoral students. When they gave students shell accounts, they tried to put them all onto a single Sequent box. They got my incoming class, the following, and anyone previous who asked for one onto that box. I’m pretty sure it had an &#x2F;etc&#x2F;password file, and would have had about 8-10k people on it.After that they gave up. Even with aggressive ulimits it was too hard, and each new class was apportioned to a separate Sparc. Which was a shame because we learned an awful lot about Unix administration from users pranking each other, figuring out what they did and how they did it, protecting yourself and retaliating (some people would prank others without first protecting themselves).Made it a lot harder chatting with people from other classes as well. For electives and humanities you weren’t always in classes with people your exact age. They could be ahead of you or behind. reply naasking 20 hours agorootparentprev> Doesn&#x27;t scale. Having passwords in a plain text file is not a scalable solution for users directory. Can probably go up to a hundred users, but not much more.This simply cannot be true. A users directory that is frequently read will be in cache. The file is also fairly simple to parse. Even on old hardware you should be able to parse thousands of users. reply wang_li 20 hours agorootparentIt&#x27;s more of a problem of the number of nodes your users will be accessing than the number of users. It&#x27;s a PITA to make mechanisms for password changes, for adding and removing users, for unlocking accounts, etc. when you have a distinct file on each server that needs to be changed. As well as adding new nodes to the environment and bringing them up to date on the list of users and etc. reply naasking 18 hours agorootparentFair point, networking always makes things more complicated. I&#x27;m not sure it&#x27;s really that much more complicated though. Like any concurrency problem with shared state, a single-writer and multiple readers keeps things simple, eg. a single host is authoritative and any changes are made there, and any file changes trigger automated scripts that distribute the changed file over NFS, FTP, etc.As long as new nodes start with the same base configuration&#x2F;image this seems manageable. Simple to do with inotify, but even prior to that some simple C programs to broadcast that a change is available and listen for a change broadcast and pull the changed file is totally within the realm of a classic sysadmin&#x27;s skillset. reply crabbone 20 hours agorootparentprevYou would have to lock the file, or guarantee consistency in some other way. Right now, I don&#x27;t believe Linux does anything about consistency of reads &#x2F; writes to that file... which is bad, but we pretend not to notice.So... the system is kind of broken to begin with, and it&#x27;s kind of pointless to try to assess its performance.Also, it would obviously make a lot of difference if you had a hundred of users with only a handful being active users, or if you had a hundred of active users. I meant active users. Running programs all the time.NB. You might have heard about this language called Python. Upon starting the interpreter it reads &#x2F;etc&#x2F;passwd (because it needs to populate some \"static\" data in os module). Bet a bunch of similar tools do the same thing. If you have a bunch of users all running Python scripts while there are some changes to the user directory... things are going to get interesting. reply naasking 19 hours agorootparent> You would have to lock the file, or guarantee consistency in some other way.I think the standard approach to atomicity is to copy, change the copy, then move that copy overwriting the original (edit: file moves are sorta atomic). Not perfect but generally works.I agree that this approach is not good for a users directory, I&#x27;m just disagreeing that the reason it&#x27;s not good is performance-related. reply fragmede 12 hours agorootparentMoves are atomic. During the move, at no time is it possible to get the contents of file 1 and file 2 confused when reading from the file descriptors. (Confusion by the human operating things is eminently possible.) reply pjc50 19 hours agorootparentprevMost systems come with \"vipw\" which does the atomic-rename dance to avoid problems with &#x2F;etc&#x2F;password. In practice this works fine. Things get more complicated when you have alternate PAM arrangements.A whole bunch of standard functions like getpwents() are defined to read &#x2F;etc&#x2F;password, so that can&#x27;t be changed. reply codys 14 hours agorootparent`getpwents()` is not defined to only read `&#x2F;etc&#x2F;passwd`. There is only a requirement that there is some abstract \"user database\" or \"password database\" (depending on if you&#x27;re reading the linux man pages or the Single Unix Specification man pages).In practice, `getpwent` on linux uses the nsswitch mechanism to provide return values from `getpwent`. One can probably disable using `&#x2F;etc&#x2F;passwd` entirely when using glibc if: all users do use `getpwent`, and you remove `files` from the `passwd` entry in `&#x2F;etc&#x2F;nsswitch.conf`. reply pseudostem 20 hours agorootparentprevExactly, even large textfile based DNS servers have capability to \"compile\" the textfile to a db file for faster access. reply crabbone 19 hours agorootparentSo what if they do?This file is a public interface exposed by Linux to other programs. So what if Linux caches its contents when eg. Python interpreter on launch will read this file. And it&#x27;s not coming from some \"fake\" filesystem like procfs or sysfs. It&#x27;s an actual physical file most of the time. reply pjc50 19 hours agorootparentprev> user id is encoded into the file systemThis is kind of unavoidable, but you do have 32 bits to play with. Windows did it slightly better with the SID: https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;windows-server&#x2F;identity&#x2F;ad...> which makes it very difficult to contain users to things they should controlIt&#x27;s not the file system that&#x27;s the problem here, it&#x27;s that \"everything is a file\" is not true for a whole bunch of important stuff that you might want to apply access control to on a UNIX system. Such as the right to initiate TCP connections. This sort of thing is why containers are so popular.NIS and LDAP do let you have a large number of users. Heck, we managed a few thousand users in &#x2F;etc&#x2F;password back when I was running https:&#x2F;&#x2F;www.srcf.net&#x2F; .. in 2000. reply TFortunato 18 hours agorootparent> it&#x27;s not the file system that&#x27;s the problem here, it&#x27;s that \"everything is a file\" is not true for a whole bunch of important stuff that you might want to apply access control to on a UNIX systemI wonder if there has ever been an attempt to really lean into, and push the limits of sticking with the \"everything is a file\" philosophy in this realm.I.e. how far could you get with having special files for fine grained permissions like \"right to initiate a TCP connection\", and making access control management be, essentially, managing which groups a user belonged to? reply jampekka 17 hours agorootparentPlan 9 probably took this the furthest. Sad it didn&#x27;t take off. https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Plan_9_from_Bell_Labs reply generalizations 17 hours agorootparentprevI think that was Plan 9. reply tenebrisalietum 17 hours agorootparentprevI think Hurd and Plan 9 take the EIAF further. reply zare_st 13 hours agorootparentprevPlan9 tried to \"remedy this\".But in reality a file is not a good abstraction for an internet socket. The ACLs would in essence spell out firewall rules. Because the bigger question is where can it connect to than \"user\" that is connecting.That&#x27;s why this is done on the level of kernel networking, where kernel knows what process is trying to open a socket and can firewall it. reply dizhn 21 hours agorootparentprevThis sounds like a completely unrelated thing and you are not constrained by the plain text password&#x2F;shadow file for scale. NIS existed for many decades. You can even use Active Directory (or samba) for authentication and user management.But the article is not about this at all. reply crabbone 19 hours agorootparentThis shows you either didn&#x27;t read what you replied to or don&#x27;t understand the subject.This file is the public interface of the Linux system to everyone who wants to get information about users on the system. It doesn&#x27;t matter that alternative tools exist: they were already mentioned in the post you replied to. It&#x27;s not the point... reply dizhn 18 hours agorootparentIf I didn&#x27;t understand I&#x27;d be grateful if you could explain it.As far as I know that file does not get referenced for the users in an external directory server. That&#x27;s how the systems scale without needing to put the users in the file. Aren&#x27;t we talking about a high number of users (and their authorization levels) when talking about scalibility in this case? reply Phrodo_00 13 hours agorootparentprev> This file is the public interface of the Linux system to everyone who wants to get information about users on the systemNo it isn&#x27;t. PAM is. The password file is only one of the places where users might be defined. reply marginalia_nu 14 hours agorootparentprev> * Doesn&#x27;t scale. Having passwords in a plain text file is not a scalable solution for users directory. Can probably go up to a hundred users, but not much more.Why not? A file 100,000 line file will only take a moment to scan. reply generalizations 17 hours agorootparentprevI&#x27;ve wondered why we don&#x27;t have a passwd.d folder, the way we do with other things in the UNIX filesystem, with individual user accounts represented by individual files. Could even retain the same line-oriented format, just stored separately. reply sznio 16 minutes agorootparentI think a directory of 10000 files is worse than a file of 10000 lines. reply eternityforest 13 hours agorootparentprevThat&#x27;s the first understandable explanation I&#x27;ve ever heard of what exactly LDAP is and what it&#x27;s for! reply fanf2 22 hours agorootparentprevSeveral are outlined in TFA reply no_time 21 hours agorootparentTangent, I can&#x27;t find what \"TFA\" stands for. My gut tells me it&#x27;s \"The Fucking Article\". Am I correct? reply InvaderFizz 21 hours agorootparentYes.TFA = The Fucking Article.Much like:RTFM = Read The Fucking Manual. reply mannykannot 21 hours agorootparentprevAs I said to my mother-in-law, the &#x27;F&#x27; is silent. reply dotancohen 19 hours agorootparentWhen Gwen Shotwell was asked about the acronym BFR on international television, she replied \"Big Falcon Rocket\". Lovely response, quite dependent on context! reply pxc 18 hours agorootparentprevThis is like how I use &#x27;ofc&#x27; to abbreviate &#x27;of course&#x27;. Once upon a time the &#x27;f&#x27; may have stood for something, but I never use it that way. For me, &#x27;the F is silent&#x27;. reply qazxcvbnm 21 hours agorootparentprevI pretend its The Forementioned Article reply codetrotter 20 hours agorootparentI like to read it as The Featured Article reply NavinF 21 hours agorootparentprevYes, but in polite company you can pretend it means \"The Freaking Article\" reply BLKNSLVR 21 hours agorootparentIn polite company you say \"The article\" or \"read the manual\" and patiently await your reward, which is their asking \"what does the &#x27;F&#x27; stand for?\", to which you reply only with a condescending look and raised eyebrow(s).That look of realisation is precious.I&#x27;ve manufactured this experience once or twice, and it&#x27;s wonderful. reply klibertp 20 hours agorootparentprevI think \"Friendly\" was&#x2F;is popular explanation where devs tried to translate their subculture into something generally digestible. reply evilduck 20 hours agorootparentDigestible maybe, but swapping \"friendly\" in for \"fucking\" changes the tone and intent of someone&#x27;s statement. At least \"freaking\" expresses similar, if muted, exclamation.A recipient who is being not-so-subtly reproached with an F-bomb acronym might misunderstand what is being implied. reply hinkley 19 hours agorootparentBut when the person asking was management, and asking for the tenth time, there was a decided advantage to changing the tone.All of that is now wrapped up in the initialism. reply tremon 21 hours agorootparentprevFine instead of Freaking is much nicer. reply dotancohen 19 hours agorootparentprevOr the fine article. reply worble 21 hours agorootparentprevYes reply edgyquant 20 hours agorootparentprevI really hate this acronym and this comment is no different from saying “read the article” which is against the guidelines here reply codetrotter 20 hours agorootparent> saying “read the article” which is against the guidelines hereThe guidelines say that you should not accuse someone of not having read the article. However, as the guidelines say it is fine to point out that something is mentioned in the article.There is a subtle difference.From the guidelines:> Please don&#x27;t comment on whether someone read an article. \"Did you even read the article? It mentions that\" can be shortened to \"The article mentions that\".Parent comment was in line with the guidelines IMO.And as for “TFA” as an acronym I like to read it as meaning “The Featured Article”. Then it seems nice and friendly. reply taneq 22 hours agorootparentprevI&#x27;d assume any code in a non-memorysafe language that parses any freeform data entered by the user is a potentially exploitable security vulnerability, so an interactive shell is a huge surface area for attacks? reply dsr_ 21 hours agorootparentYes, but irrelevant here. Basically any shell access means you&#x27;ve changed from preventing remote code execution to preventing privilege escalation, which is much harder. reply hinkley 19 hours agorootparentprevWhich is why sudo has its own editor.If you fuck up the sudo file while saving it, you might no longer be able to log in to fix it. Before I knew about sudoedit I would open two shells as root, edit the file, then use a third window to make sure I could still sudo.With two windows I could accidentally close the subshell in one without locking myself out. Think if it like linemen, who use two tethers for climbing structures. They are never detached from the safety lines. reply neuromanser 16 hours agorootparentnext [–]sudo visudo reply photochemsyn 21 hours agorootparentprevAsk the chatbot:1. system: in the context of setting up secure remote access to a Unix-like system, discuss whether relying on the passwd file and shell behavior as an effective means of authentication and access control is a good approach. What are some reasons this is not (or is) an effective approach, which should not (or should) be used on modern production systems. user: system administrator on a Unix-based network. assistant: technically, there are several reasons...2. If you have a collection of Unix systems, can you reasonably do a certain amount of access control to your overall environment by forcing different logins to have specific administrative shells? reply eviks 22 hours agoparentprevBut when was it ever effective if 30 years ago it was already well known? reply tannhaeuser 21 hours agorootparentLast time it might&#x27;ve been effective was probably in old-school Unix time sharing with users connected via tty&#x27;s rather than TCP&#x2F;IP. Already early SQL databases, with the possible exception of Informix SE, had a client&#x2F;server process model where the server process had full access to all data files and would at best authenticate sessions but not individual accesses against &#x2F;etc&#x2F;passwd such as via Oracle&#x27;s pipe&#x2F;bequeather connector but more commonly would assume fixed global roles and handle auth on the app side. As soon as IP and \"services\" were introduced, &#x2F;etc&#x2F;passwd stopped being effective, as pointed out by bluetomcat [1]. Actually, even gaining shell access is considered game over from a security PoV, due to multiple privilege escalations.[1]: https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37462806 reply pmontra 22 hours agorootparentprevIt was effective for a ftp server accessing public directories in the home of users. I can&#x27;t remember the details but you would use the username and password of the user to exchange files with and get into that directory. All transmitted as cleartext, of course.30+ years ago we already had services (daemons!) with their own user id, to keep them isolated from root and the human users. This post is as news as the invention of hot water. reply j16sdiz 21 hours agorootparent> It was effective for a ftp server accessing public directories in the home of users. I can&#x27;t remember the details ...Most ftpd need a shell whitelisted in &#x2F;etc&#x2F;shells .In macOS, &#x2F;etc&#x2F;shells begin with this comment: # List of acceptable shells for chpass(1). # Ftpd will not allow users to connect who are not using # one of these shells. reply dizhn 23 hours agoprevI don&#x27;t get it. When was shell involved with using Apache basic passwords or LDAP authentication? It requires extensive extra steps (pam modules etc) to plug these things into a unix system for authentication purposes. Also that would be authentication but access control implies authorization. reply totetsu 23 hours agoparentYeah my reading comprehension process maxed out a core on this one.> First, you almost certainly operate a variety of services that normally only use Unix logins as a source of (password) authentication and perhaps a UID to operate as, and ignore the login&#x27;s shell. This is the common pattern of Samba, IMAP servers, Apache HTTP Basic Authentication, and so on.So you have a user on your server nginx:x:100:101:nginx:&#x2F;var&#x2F;lib&#x2F;nginx:&#x2F;sbin&#x2F;nologinAnd your also running samba network shares, you point your samba client at your server and use user nginx and inexplicably the password you also set for that user to login? This a service is using etc&#x2F;shadow basses authentication but not sending the message in &#x2F;etc&#x2F;nologin .. presumably samba won’t work really in this case..>In some cases you may be able to teach these services to look at the login&#x27;s shell and do special things, but some of them are sealed black boxes and even the ones that can be changed require you to go out of your way. If you forget one, it fails open (allowing access to people with an administrative shell that should lock them outIs this talking about setting up applications..like a web server, that would give a http access to a uses home for, And having these services authenticate with the servers etc&#x2F;shadow or configured Pam providers, and also then check out the shell in &#x2F;etc&#x2F;password to gracefully handle access management and error messages? reply tetha 18 hours agorootparentMh. I think the problem is: There are (at least) 2 fundamentally different use cases for linux&#x2F;unix systems.One is what I&#x27;d call a shell-server. On a shell-server, I have a bunch of accounts for users, and there are services supplied for these users. There will be &#x2F;home&#x2F;tetha, and &#x2F;home&#x2F;tetha&#x2F;share and &#x2F;home&#x2F;tetha&#x2F;http. And then you have some SMB sharing &#x2F;home&#x2F;*&#x2F;share and you login there with your password, Apache serves &#x2F;home&#x2F;*&#x2F;http for the intranet and so on. This is a very common setup at universities, for example.The other thing is what I&#x27;d call an application server or a service server (but that name sucks). Here, you have a system and the main purpose of the system is to serve a web page via nginx, or be a postgres node and such.These service-systems tend to be both more controlled, but also simpler. You need to grant a rather small, very known set of users access to these - 10 - 30 usually. And, honestly, these service-level systems tend to have very streamlined permissions, because realistically, shell-access is enough attack profile to be considered root access unless you are very diligent.Shell-servers however are very, very complex to handle. One big shell-server can be overall more complex than many infrastructures around in total. reply mypgovroom 21 hours agoparentprevOk. I read it twice trying to catch something but had the same thoughts as you. reply jmbwell 19 hours agoprevJudging by the thread here, the very premise seems difficult to imagine today. But there was a time, indeed, when end users regularly logged in to a Unix system as themselves, and their shell specified the program they could run, like a line of business app or something, and short of exploiting a vulnerability in that app, that was all they could access, sitting there at a physical terminal typing on a keyboard. There just weren’t that many other ways to spawn a process.Meanwhile, these days, it seems like the only user ever running anything is docker, and “access control” has an entirely different set of meanings. reply djur 10 hours agoparentAnd the author is a sysadmin at a university, and universities are pretty much the last vestige of that traditional Unix login model of accessing shared services. From some other articles he&#x27;s written, it sounds like his environment has been very conservatively updated over the years. reply salzig 23 hours agoprevIt’s always interesting to find people who don’t know about `-N` („Do not execute a remote command. This is useful for just forwarding ports.“[0]), which was quite neat to access the MySQL on the same host (with no root database Passwort, duh) at one of my previous employers.[0]: https:&#x2F;&#x2F;man7.org&#x2F;linux&#x2F;man-pages&#x2F;man1&#x2F;ssh.1.html reply madeofpalk 16 hours agoprevSide note - is this author noteworthy for something in particular? Their somewhat average posts consistently make it to the front page and tend to get a more mixed&#x2F;negative reaction than normal https:&#x2F;&#x2F;news.ycombinator.com&#x2F;from?site=utoronto.ca reply KaiserPro 17 hours agoprevI have had the pleasure(?) of running a large linux network, I don&#x27;t think it was ever really the case that shells were a viable access control mechanisms on their own.Firstly it only really worked if you were all sharing the same machine with remote terminals, which most people don&#x27;t do anymore. Second NFS happened when if you configured it badly you could just pretend you were any user you liked.I&#x27;m assuming this is part of the reason why kerberos was invented. Basically the only practical way to tie down a network of machines was to make sure that authentication was done with LDAP and Kerberos. LDAP did the name, UID&#x2F;GID and user metadata and kerberos the authentication. You could then use that ticket to gain access to other things (yes, even over HTTP, NFS or SSH)Nowadays you&#x27;d use active directory, which is LDAP+Kerberos, but with a nice expensive gui.&#x2F;etc&#x2F;passwd(or shadow) died _years_ ago, It was dodgy even in the 90s, let alone now. Its fine for single user machines, but not networked. reply blueflow 23 hours agoprevThe post does not back up its headline? How can a SHELL=&#x2F;bin&#x2F;false be bypassed via SSH?For the various kind of forwarding SSH supports, they all can be disabled via sshd_config. reply tyingq 23 hours agoparentI think it&#x27;s comparing to the \"old days\" when there was a more 1:1 relationship of service-on-a-port plus a specific userid. Because the article specifically calls out:\"One of these services is SSH itself, since you can generally initiate SSH sessions and ask for port forwarding or other features that don&#x27;t cause SSH to run the login shell\"But, even that sort of falls apart because we also had inetd in the \"old days\", which spawned lots of different things as different users without invoking a login shell.Generally, the article seems to be lamenting that not everything is gated by userids and groups anymore. That&#x27;s true, but it doesn&#x27;t seem recent to me. Aside from inetd, I could (and did) massage kermit (or uucp, etc) into being a multi-service gateway in the 1980&#x27;s.I suppose it&#x27;s somewhat correct in the idea that figuring how any particular service limits rights is now very complicated. You have the old familiar stuff (users, groups). But now you also have virtual machines, containers, namespaces, capabilities. And things like seccomp and apparmor. Then, various sandboxing schemes within utilities, languages and frameworks or OS facilities like ebpf. reply juped 20 hours agoparentprevIt seems to me (it&#x27;s kinda unclear) that the article is talking mostly about various other things pulling users from &#x2F;etc&#x2F;passwd and the ssh thing is an aside. For anon ssh you can have e.g. Match User anonymous PasswordAuthentication yes PermitEmptyPasswords yes DisableForwarding yes PermitTTY noand possibly ForceCommand and ChrootDirectory depending on how you&#x27;re sandboxing anonymous and to what; plus a restricted login shell (the above anonymous user of mine has gotsh, got&#x27;s version of git-shell). reply batch12 22 hours agoparentprevIve used something like &#x27;&#x27;&#x27; ssh -t user@host &#x2F;bin&#x2F;sh &#x27;&#x27;&#x27;To bypass shell restrictions in the past, but I&#x27;m not sure if that will work with your example. reply blueflow 22 hours agorootparentThis won&#x27;t work because the command send to the server is not an argument vector, but a command string interpreted by the users login shell.`ssh -t user@remote &#x2F;bin&#x2F;sh asdf` would execute `&#x2F;bin&#x2F;false -c \"&#x2F;bin&#x2F;sh asdf\"` on the remote. reply batch12 22 hours agorootparentMakes sense, thanks for the clarification. reply LinuxBender 20 hours agorootparentprevFeel free to try it out sftp share@ohblog.net (no pw) # grep share &#x2F;etc&#x2F;passwd &#x2F;etc&#x2F;shadow &#x2F;etc&#x2F;passwd:share:x:5002:5000::&#x2F;data&#x2F;sftp&#x2F;share:&#x2F;bin&#x2F;false &#x2F;etc&#x2F;shadow:share::19614::::::The partially redacted &#x2F;etc&#x2F;ssh&#x2F;sshd_config is copied to &#x2F;pub&#x2F; in the SFTP account.If you can bypass the restriction please do share how it was done. I don&#x27;t offer bug bounties but I think people would find it interesting. OS is Alpine Linux. All CPU mitigations are disabled in the VM. No MAC As in no SElinux or AppArmor. I won&#x27;t complain, just pretty please don&#x27;t DDoS the server or anything that would make that VPS do work.Feel free to also tinker with the web and voice chat server on that node. reply emmelaich 11 hours agoparentprevYou can pass various env vars with ssh, typically restricted to LANG and LC_*.But I vaguely remember that in the past it may have been anything, including SHELL. reply salzig 22 hours agoparentprevFor a port-forward with `-N` on client side.Besides that, I don’t know. reply dboreham 19 hours agoprevIn case anyone is tempted to read the article: it&#x27;s totally incoherent, even to someone knowledgeable in the field. reply flanked-evergl 17 hours agoprevI guess I&#x27;m just lucky that I have never ever in my life even once seen something almost similar to using a Unix shell as an access control mechanism.Maybe next we will hear that Hammers are not viable screwdrivers any more because, you know, that is also probably something everybody already knows. reply totetsu 8 hours agoparent>In the realm of unconventional tools, the potential application of biological structures has always been a topic of intrigue. This study delves into the material science behind the potential of hamster claws to function as micro-screwdrivers, given their inherent sharpness and structure.Using scanning electron microscopy (SEM), we characterized the microstructure of hamster claws, revealing a keratinous composition with intricate surface patterns. Nano-indentation techniques were employed to determine the hardness and resilience of the claws. Their sharpness was quantitatively assessed using a sharpness index, which considered both geometric and material properties.To test the potential application as micro-screwdrivers, we designed a set of experiments where hamster claws were used to engage with micro-screws commonly found in electronics and precision instruments. The results indicated that while hamster claws can fit into the grooves of certain micro-screws, the torque they can generate is limited due to their flexibility and the lack of a proper grip mechanism. reply post-it 17 hours agoparentprevYeah, this post is giving https:&#x2F;&#x2F;xkcd.com&#x2F;2071&#x2F; reply h2odragon 23 hours agoprevare they speaking about changing the \"shell\" field in `&#x2F;etc&#x2F;password` as a means of broader access control?That went away a long, long time ago; the question of \"which system am i authenticating myself to\" was often already too complicated for that way back when when \"shadow passwords\" were a new idea. reply poppafuze 18 hours agoprevAt best, this is an exposition of someone&#x27;s conflation of their perception of a controller problem as a view problem, with the implied model problem ignored. reply onetimeuse92304 22 hours agoprevI pretty much assume now that if somebody has ability to run an arbitrary process on the machine (ie shell), they have root access to that machine regardless of their starting access level. reply rlpb 13 hours agoprevI think this is being approached backwards - both by the article and by commenters here.A Unix account is a viable access control mechanism. If you create one, you give its human user access to do everything that the Unix user can do. This can be very useful.But others want to give humans users more restricted access than their corresponding Unix account. This is futile, since the Unix account is the basic unit of access control on a Unix or Unix-like system. The author is correct that if you want to do this then you must create a sandbox. General tools won&#x27;t do, because they&#x27;re not designed for it, and&#x2F;or are generally swimming against the tide. reply debarshri 22 hours agoprevOne of the learnings lately has been that when you think about access control, it is combination of the downstream access control mechanism + a network layer grant to access the resource. For eg. If you want to access a server or host, you have some authorization to SSH into it but also by you have network access only to that server or host from your source and then authorization is revoked also the network access to the server or host is revoked too. reply nickdothutton 21 hours agoprevI&#x27;d better re-read this article because I don&#x27;t think shells were ever (in my 20something-mumble years)a terribly strong access control mechanism(). Without putting in the effort. Meaning an appropriate (poss non-default) security model, chroot&#x2F;jails, possibly pledge etc. reply cardiffspaceman 11 hours agoprevI skimmed this article using a different POV than many of you, and it made sense. From the other POV it is clear why many of you don’t like or in some cases understand it.If your idea of “Unix” is Linux kernel with Busybox and dropbear, it makes sense. I think. reply hoistbypetard 16 hours agoprevThis is a response to the headline and not yet to the story: were UNIX shells ever an access control mechanism? I never viewed them that way. reply AtlasBarfed 21 hours agoprevBasically they want a somewhat poorly designed configuration map (environment variables) to be attached to these operations, but instead everything comes from adhoc&#x2F;bespoke configuration sources on a per app basis (like ~&#x2F;.ssh&#x2F;config) reply stonogo 17 hours agoprevUsing custom shells for IMAP or HTTP auth must have been the parallel reality where checkpassword(1) never existed. I certainly never ran into this behavior in the wild and would have strenuously objected had I done so. Setting custom shells is fine for providing tunneled services over ssh tunnels or the like (many git hosts use this functionality) but using them for access control just meant your only safety net was whether the program&#x27;s author was really good at preventing string overflows. This was never a good idea. reply dingosity 18 hours agoprevUhh... wat? reply 0xbadcafebee 18 hours agoprev [–] Can somebody translate this to english? A shell was never an access control mechanism. It was a prompt at which you could type commands to run. As a hack they added authentication to it some time in the 60&#x27;s at MIT.> Today, the only two measures of login access control that really work in a general environment are either scrambling the login&#x27;s password (and disable any SSH authorized keys) or excluding the login entirely from your various authentication data sources (your LDAP servers, your Apache htpasswd files, and so on). It&#x27;s a pity that changing people&#x27;s shells is no longer enough (it was both easy and convenient), but that&#x27;s how the environment has evolved.This literally makes no sense. Scrambling a login password is not an access control mechanism. Excluding \"a login\" (a what?) from an authentication data source... the hell does this mean? You mean disabling a user account?The last part, \"changing a user&#x27;s shell\", makes a little sense, as you used to be able to prevent a user from logging in by changing their shell to \"&#x2F;bin&#x2F;false\" or something, but that isn&#x27;t disabling authentication, that&#x27;s just breaking the shell. Authentication still works if the shell has changed, it just can&#x27;t execute the shell after you authenticate.The proper way of disabling the account is to set the \"disabled account\" bit in the shadow or password file. But that&#x27;s just one authentication mechanism (NSS). There is no universal authentication mechanism, so for any other given authentication mechanism, you need to disable it however that given method allows you. reply kmeisthax 16 hours agoparentTraditional Unix conflates authentication and authorization - if you can authenticate as a user you are authorized to use the computer, full stop. If you want to later revoke authorization, or only authorize certain services but not others, Unix provides no general means to do so.Scrambling the password works as a deauthorization mechanism solely because you strip the user of the ability to authenticate. This wouldn&#x27;t clear authorized_keys, though; services that needed to implement new authentication mechanisms generally did not bother extending &#x2F;etc&#x2F;shadow or NSS in an obvious way everyone agreed upon.Changing the login shell used to work because Unix used to be used solely through the medium of &#x2F;bin&#x2F;sh. This, however, conflates two different signals:- Who should be allowed to start an interactive session on this machine?- What kind of shell does this user prefer to use?We pretend that disabled users just really love using this one particular shell that immediately kicks them out of their login session.The only bulletproof way to deauthorize a user is to unperson them - delete the account. Except if you do this then their old files still sit there on the disk with their user ID. If you reuse the user ID then the new user gains authorization to the old user&#x27;s files; so you have to keep track of unpersons to avoid reusing their IDs or meticulously reassign their files to a dedicated cleanup user. reply 0xbadcafebee 13 hours agorootparent> Traditional Unix conflates authentication and authorization - if you can authenticate as a user you are authorized to use the computer, full stop.Incorrect. Nearly every service in a Unix computer has its own authorization mechanisms, there is no blanket authorization over an entire host.The login service has a \"nologin\" and \"securetty\" files. Sudo has its sudoers file. RSH has the .rhosts file. FTP daemons have their own authorization logic. NFS authorization options could override local filesystem permissions. Filesystem permissions defined the authorization for device files, but network services (such as CUPS) would add their own authorization mechanisms. Etc, etc, etc.The difference between authentication and authorization has been apparent since time-sharing systems were invented. The operators were authorized to perform different operations than the bog-standard users. There hasn&#x27;t been a time since the 60&#x27;s that everyone was authorized the same just because they were authenticated.> Scrambling the password works as a deauthorization mechanism solely because you strip the user of the ability to authenticate.Doesn&#x27;t work. If you&#x27;re using SSH key authentication you can skip password auth. With RSH, password auth isn&#x27;t required if you&#x27;re coming from a trusted host defined in a .rhosts file.> The only bulletproof way to deauthorize a user is to unperson them - delete the account.That doesn&#x27;t always work either. There may be more than one authentication system, and they may be configured to simply skip to the next authentication method if a user account is not found. So you would have to delete the account from every authentication system connected.You have to use the prescribed deactivation method for every authentication system connected, otherwise the first auth method may skip the deactivated user but the second system would pick up its own user with that name&#x2F;ID. reply bananapub 17 hours agoparentprev [–] I think you&#x27;ve got the wrong end of the stick - it&#x27;s about forcing the login shell to something (like sftp or rbash or or &#x2F;bin&#x2F;nologin whatever) which was a historically somewhat popular thing to do replyApplications are open for YC Winter 2024 GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Unix shells are losing their efficacy as access control mechanisms in modern Unix environments due to the focus of many services solely on Unix logins for authentication, disregarding the login's shell.",
      "Authentication services often fail to recognize the login's shell, creating challenges in restricting certain logins from accessing particular services.",
      "Viable solutions include scrambling the login's password or completely excluding the login from authentication data sources."
    ],
    "commentSummary": [
      "The discussions primarily focus on Unix shells, access control mechanics, password management, scalability, and deauthorization in Unix systems.",
      "Users question the efficacy of shells concerning authentication and access control, propose other methods for password management and tackling scalability, and discuss the intricacies of deauthorizing users in Unix systems.",
      "The text sheds light on limitations and vulnerabilities of various Unix components and puts forward probable solutions to these challenges, offering an overview of complexities and considerations in Unix/Linux system usage."
    ],
    "points": 171,
    "commentCount": 132,
    "retryCount": 0,
    "time": 1695033222
  }
]
