[
  {
    "id": 39251095,
    "title": "Browser Extensions: Unleashing the Power of Hackable Software",
    "originLink": "https://www.geoffreylitt.com/2019/07/29/browser-extensions",
    "originBody": "July 2019 Browser extensions are underrated: the promise of hackable software Photo by Rick Mason on Unsplash Recent conversations about web browser extensions have focused on controversy: malicious browser extensions capturing web history, and Google limiting the capabilities used by ad blockers. These are important discussions, but we shouldn’t lose sight of the big picture: browser extensions are a special ecosystem worth celebrating. Among major software platforms today, browser extensions are the rare exception that allow and encourage users to modify the apps that we use, in creative ways not intended by their original developers. On smartphone and desktop platforms, this sort of behavior ranges from unusual to impossible, but in the browser it’s an everyday activity. Browser extensions remind us what it’s like to have deep control over how we use our computers. Assembling our own software Once a software platform reaches a certain level of openness, it can fundamentally change the way that normal users relate to their software. By installing four different Gmail extensions that modify everything from the visual design to the core functionality, in some sense, I’ve put together my own email client. Instead of being a passive user of pre-built applications, I can start assembling my own personalized way of using my computer. The popularity of browser extensions proves that many people are interested in customizing their software, and it’s not just a hobby for power users. There are over 180,000 extensions on the Chrome store, and nearly half of all Chrome users have browser extensions installed.1 When people have an easy way to extend their software with useful functionality, they apparently take advantage. Hackable platforms, not custom APIs Browser extensions have remarkably broad use cases. I personally use Chrome extensions that fill in my passwords, help me read Japanese kanji, simplify the visual design of Gmail, let me highlight and annotate articles, save articles for later reading, play videos at 2x speed, and of course, block ads. The key to this breadth is that most extensions modify applications in ways that the original developers didn’t specifically plan for. When Japanese newspapers publish articles, they’re not thinking about compatibility with the kanji reading extension. Extension authors gain creative freedom because they don’t need to use application-specific APIs that reflect the original developers’ view of how people might want to extend their application. The web platform has a few qualities that enable this sort of unplanned extensibility. The foundational one is that the classic web deployment style is to ship all the client code to the browser in human-readable form. (Source maps are a key to preserving this advantage as we ship more code that’s minified or compiled from other languages.) The web’s layout model also promotes extensibility by encouraging standardized semantic markup—my password manager extension works because web pages reliably use form tags for password submissions instead of building their own version. Even with these advantages, it can still require clever tricks to modify a site in ways that it wasn’t built for. But it’s often a reasonable amount of work, not a years-long reverse engineering effort. The sheer variety of extensions available shows that extension authors are willing to jump through a few hoops to create useful software. Occasionally there are tensions between website developers and extension authors, but it seems far more common that developers are fine with their sites being extended in creative ways, as long as they don’t have to do any extra work. Extensions can even make life easier for application developers: if there’s a niche request that a small minority of users want, a motivated community member can just build an extension to support it. By building on a hackable platform, developers allow their users to get even more value out of their applications. Small tools, not big apps Many browser extensions are generic tools designed to enhance my use of all websites. I can use my annotation extension on every website everywhere, instead of needing a different highlighting tool for each article I read. Just like using a physical highlighter with paper articles, I can master the tool once, and get a lot of leverage by applying it in different contexts. In many software platforms, we think of the operating system as providing the cross-cutting tools, and third parties as providing standalone “apps” that are used in isolation. With browser extensions, third parties are also adding tools; a single piece of software has the leverage to change my experience across all the apps I use. When software is built in small units, it also changes the economics. Most extensions I use are free, and are perhaps too small in their feature set to support a full-blown business. And yet, people still choose to make them, and I benefit immensely from these little bits of software. Browsing the extension store feels more like going to a local flea market than going to a supermarket. Massive software built by huge companies isn’t the only way. The origins of openness It’s not an accident that this openness emerged on the web platform. Since the beginning of personal computing, there’s been a philosophical tradition that encourages using computers as an interactive medium where people contribute their own ideas and build their own tools—authorship over consumption. This idea is reflected in systems like Smalltalk, Hypercard, and more recently, Dynamicland. When Tim Berners-Lee created the World Wide Web, he imagined it fitting into this tradition. “My vision was a system in which sharing what you knew or thought should be as easy as learning what someone else knew.”2 There were some hiccups along the way3, but eventually that vision largely won out, and the Web became a place where anyone can publish their opinions or photos through social media platforms. Still, there’s a catch. When you’re using Facebook, you’re operating within a confined experience. You’re forced to publish in a certain format, and to use their app in a certain way (that includes, of course, seeing all the ads). There’s more room for authorship than just browsing a news website, but only within the strict lines the app has painted for you. Browser extensions offer a deeper type of control. Instead of merely typing into the provided text box, we can color outside the lines and deeply modify the way we use any application on the web. Browser extensions offer a kind of decentralization: large companies building major websites don’t get to dictate all the details of our experience. Improving on extensions We clearly need to work on protecting people from malicious extensions that invade their privacy. But beyond that, here are some bigger picture opportunities I see for improving on extensions: Accessibility: Today, it requires a big jump to go from using browser extensions to creating them: you need to learn a fair amount of web development to get started, and you can’t easily develop extensions in the browser itself. What if there were a quick way to get started developing and sharing extensions in the browser? You could imagine smoothly transitioning from editing a website in the developer tools to publishing a small extension. Compatibility: Because extensions hook into websites in unsupported ways, updates to websites often result in extensions temporarily breaking, and extension authors scrambling to fix them. Can we make it easier for website developers and extension authors to form stable connections between their software, without necessarily resorting to using explicit extension APIs? There are existing practices that fit into this category already—for example, using clean semantic markup, human-readable CSS, and source maps makes it easier to develop an extension. A simple change that would allow for more stable extensions would be to give users more control over when they upgrade to new versions of cloud software. If I have a 3 month window to continue using an old version after the new one is released, that would give extension authors more time to upgrade their software for the new version. Power: Web extensions are limited in their power by the typical architecture of web applications: they have broad rights to modify the browser client, but the server is off limits. For example, if my social media app’s server only provides an endpoint for querying my posts in chronological order, no browser extension can ever search through all my posts by keyword. How could we rethink the client-server boundary to enable extensions to make even deeper modifications? This raises tough questions around security and privacy. The modern browser extension API has done a good job balancing extensibility with security, and yet we’re still grappling with the consequences of browser extensions invading people’s privacy. Giving extensions more power would raise the stakes further. Still, we shouldn’t give up in the name of security—we should fight for extensibility as a value and find ways to balance these interests. The next platform I’m intrigued by a couple projects that are rethinking the web in ways that might make it more extensible: The Beaker Browser and the decentralized web community are exploring how the web works without centralized servers. It seems like their proposed architecture would give users fuller control over modifying the “server” side of web applications. Tim Berners-Lee is working on a new project called SOLID. I don’t yet understand precisely what they’re up to, but given Tim’s involvement I figure it’s worth paying attention. A key principle is giving users more ownership over their data, which would enable people to use extensions and other software to manipulate their data in flexible ways beyond what application server APIs allow. Computing is still young, and platforms are changing quickly. Modern browser extensions and smartphone platforms have only been around for about a decade. These platforms will evolve, and there will be new platforms after them, and we will get to collectively decide how open they will be. Browser extensions give us one example to strive for: a place where we routinely hack the software we use and make it our own. ▪ 2024 Updates I originally wrote this post in 2019. Coming back to this five years later, here are a few related projects which you might find interesting: I built a popular browser extension for Twitter, and wrote some reflections on using extensions as a way to fix problems in the software you use every day. I also touch on some of the downsides of the extensions platform in that post, if this one was too optimistic for your taste 😉 During my PhD at MIT with Daniel Jackson, I developed a tool called Wildcard that enables non-programmers to build a browser extension in a spreadsheet. My friend Glen has also been working on a neat platform called ExtensionPay for monetizing browser extensions, for anyone interested in making an extension into a sustainable project. More recently, I’m working towards malleable software powered by AI at the research lab Ink & Switch. If you’d like to follow that work you can sign up for my email newsletter. Discuss on Hacker News (2019) Discuss on Hacker News (2024) https://www.blog.google/technology/safety-security/update-project-strobe-new-policies-chrome-and-drive/ ↩ Weaving the Web, by Tim Berners-Lee (p33) ↩ Tim thought web browsers should also be website editors, and was disappointed when the Mosaic browser took off in popularity without including that feature. ↩ Subscribe I periodically write about programming tools, end-user programming, and other software topics. To get updates about new posts: Join my email newsletter Follow me on Twitter Subscribe via RSS",
    "commentLink": "https://news.ycombinator.com/item?id=39251095",
    "commentBody": "Browser extensions are underrated: the promise of hackable software (2019) (geoffreylitt.com)506 points by mufty 18 hours agohidepastfavorite250 comments gklitt 13 hours agoPost author here! I wrote this post five years ago. Since then, my conviction in the value of customizable software has only grown, but I've also updated my thinking in a few ways: 1) AI AI is rapidly getting better at coding. Current AI is often bad at high-level architecture but is capable of making small local tweaks. Seems like a good fit for the kind of code you need to write a browser extension! I'm exploring this direction; wrote more about it in \"Malleable software in the age of LLMs\" [1] 2) Security Having talked to people who worked on various extension platforms including the browser extensions API, I see more clearly than I did five years ago that security is often the key bottleneck to deploying extension platforms meant for mass adoption. Anytime you want everyday computer users to be installing invasive extensions to important software from untrusted third parties, it's gonna be challenging to protect them. That said, I still think that conversations around extensions tend to focus too much on security at the expense of all else. Customizability is important enough that it may be worth prioritizing it over security in some cases. I also think there are many reasonable paths forward here. One is to exchange extensions with trusted parties -- e.g, coworkers or friends -- rather than installing from random people on the internet. Another might be to only build your own extensions; perhaps that'll become more viable with AI-assisted programming, although that introduces its own new security issues. And finally, I've met a few people who have smart ideas for architecting software in a way that helps resolve the core tensions; see [2] for an example. 3) Backend access as a key limitation I've increasingly realized that the fact that browser extensions can only access client code in a fairly server-centric web means that many deep customizations are out of reach. Perhaps you can't read the data you want, or there's not a write API to do the thing you need. While I'm optimistic about what extensions can do within the boundary of the client, this is an inherent limitation of the platform. At Ink & Switch (the research lab I now work for), we're working towards local-first [3] software: collaborative software where the data and the code lives on your device. Among other benefits like privacy, we think this is the right foundation for more powerful extensions, since your data and the app code aren't locked away on a server. [1] https://www.geoffreylitt.com/2023/03/25/llm-end-user-program... [2] https://www.wildbuilt.world/p/inverting-three-key-relationsh... [3] https://www.inkandswitch.com/local-first/ reply jameshart 12 hours agoparentThe security problem of open platforms is the key. Anything that is open enough to let someone who knows what they're doing customize the system to their liking, will also be abused by bad actors persuading people who don't know what they are doing to customize the system in ways that harm them. The fact I can write my own custom keyboards on Android is great! But the fact someone can convince your grandparents to install a keyboard that includes an embedded key logger is not! Browser extensions have always been a malware-rich ecosystem. Joking about removing all the toolbars from your parents' Internet Explorer whenever you went home for thanksgiving dates back to about 1999. reply danielheath 9 hours agorootparentCustom keyboards are a great example of an app that - by default - shouldn't have write access to shared resources (that is, no network access, no writing to files which other apps can read). Adding either of those entitlements to a keyboard app should require extremely scary dialogs. Needs to be possible - perhaps you want your password manager with sync to be part of the keyboard app - but it's clearly a huge risk. reply jenadine 3 hours agorootparentUntil you want to be able to download language dictionaries or updated language model. Or if your keyboard is actually a remote keyboard or shared keyboard taking input from some other devices. reply simscitizen 9 hours agorootparentprevMobile OS vendors have already thought of that and came up with the exact same solution of requiring entitlements to access the network from a keyboard app: https://developer.apple.com/documentation/uikit/keyboards_an... The question is do you actually trust regular users to understand what’s going on when they’re asked for permission to grant an app the ability to do something sketchy? reply danielheath 9 hours agorootparentBear in mind that on iOS, you can't just prompt for permission; those \"regular users\" need to be able to navigate to the settings app, find the relevant (deeply nested) section, and enable it there. That narrows the gap significantly - to users who can't understand the issues, but can (even with the app providing an explanation) find reasonably well-hidden settings. reply LoganDark 4 hours agorootparentI've heard from a couple developers over the years that it's entirely impossible to implement a setting that will not be changed by people who don't know what it does. It doesn't matter if it's behind a footnote, an easter egg, a password input, a magic email code, a call with the main project developer, all of the above, etc. No matter how many steps you try to add, there are still an incredible number of idiots who will mindlessly tap through literally any number of dialogs, warnings, and disclaimers to get to what they want. Their brain will entirely filter out the path they took. They will probably not even remember a single one of those intermediate steps. The only thing they care about is that they're fixing some problem. This could be one of the reasons Apple and Google don't want you jailbreaking/rooting your devices. Someone will inevitably make a guide, and millions of idiots will follow it. It will legitimately make the device less secure for them because they won't have any idea what they are doing and likely won't even remember doing it. The only thing they care about is that they're fixing some problem. This is one reason why some people get so panicked and upset when anything on their computer changes unexpectedly, even if the change is actually harmless. They never actually understood anything. They had managed to accidentally get it how they want it through a combination of stuff that they don't remember. When anything changes, they have to go through that process again. Look, these people are great at following guides and learning routines. Repetitive, mindless tasks like data entry are perfect for them, because they have no other talent to worry about wasting. But because these people exist, you have to be really careful about what settings you add, no matter how well you think it is hidden, because they will be changed by people who don't know what they're doing. So far, the devs that have told me this have done so because I asked for some setting to turn off some safeguards, and they said that it's a near-universal request from power users, but they still can't do it, because the rest of their userbase is too clueless to be trusted with that setting. They'd receive bug reports from people who have no clue what went wrong, when the reality is that they disabled the safeguards in order to make something work, and then promptly forgot what happened once it worked the way they wanted. This has supposedly happened so many times in the past that they just don't take the risk anymore. Anyway, all this is to say that while hiding a setting, as opposed to automatically prompting for it, can definitely rule out a decent chunk of idiots, you will never be able to rule out the resourceful idiots that can mindlessly follow instructions. reply rrr_oh_man 3 hours agorootparentI think you underestimate how much we all are these resourceful idiots under the right circumstances. reply LoganDark 3 hours agorootparentI'm biased because I'm neurodivergent, which means I don't have as much experience with neurotypical thought processes. While I do use search engines and the resultant resources all the time, I don't follow steps completely cluelessly/mindlessly and later forget that I did it. I don't know what the equivalent would be for non-tech - I at least try to understand what a guide is doing so I can reproduce it independently later. I try to develop basic intuition for everything that I do. It is hard for me to imagine someone who lacks that ability. I don't mean to be offensive to anyone in particular, I just use \"idiots\" for the sake of argument to explain how any setting will eventually be found and changed. Is it normal to forget the steps you took to accomplish a task? To, say, specifically turn off a setting for crash protection, then completely pull a blank if the program gets into a crash loop later? reply jamwil 2 hours agorootparentI don’t remember what I had for lunch reply rrr_oh_man 3 hours agorootparentprev> Is it normal to forget the steps you took to accomplish a task? Yes, it’s very common. Immediately after doing it, in fact. reply LoganDark 1 hour agorootparent> Yes, it’s very common. Immediately after doing it, in fact. Do you not even make mental notes of permanent changes you've made to the system...? I mean, I don't think you'd, say, turn off some crash protection and then later complain about crashes. You'd remember that you previously turned it off, wouldn't you? I'm so confused, heh. reply ScoobleDoodle 22 minutes agorootparentNo, some set of people will forget. Even if they intended or desired to remember. Mental notes fade for some set of people. And at different time lengths. LoganDark 11 minutes agorootparenthmm, I suppose that's true. I have a lot of friends who also have dissociative disorders, and some of them just dissociate all the time and forget everything, regardless of whether they would've forgotten normally rrr_oh_man 45 minutes agorootparentprev> I'm biased because I'm neurodivergent, which means I don't have as much experience with neurotypical thought processes. > I'm so confused, heh I’m biased right now because you assume stuff about me that you maybe shouldn’t. Everyone’s experiences and thought processes might be starkly different from each other. (No matter which observational group you put people into.) reply LoganDark 23 minutes agorootparent> I’m biased right now because you assume stuff about me that you maybe shouldn’t. I only talked about \"typical thought processes\" because you said \"we all\" which I assume meant the general population. Didn't assume anything about you. Even though the base problem was given to me by another, everything I wrote about \"what makes a resourceful idiot / how they are a problem\" is based on my personal perception of the ones that I've seen. Which is most likely going to be a neurodivergent's impression of certain neurotypicals. AKA biased. And the \"I don't think\" was leading a question, not making an assumption about you. > Everyone’s experiences and thought processes might be starkly different from each other. ...which is I'm so hesitant to believe that everyone is a resourceful idiot. And why I made a disclaimer about the fact that my own thought processes might be starkly different from not just who I'm describing, but other brains in general. conradev 11 hours agorootparentprevA great XKCD on the topic: https://xkcd.com/2044/ I do think that with every turn of that cycle we end up with better compromises. They’ll still be compromises, though. reply samwillis 13 hours agoparentprevI'm so excited about the malleable software / local-first / local-AI crossover, I feel like we are at the dawn of a new era of software. If we play our cards right, we can bring back control of our data from the large corporations, have ownership, and more control of how we work. I'm particularly interested in how general purpose CRDT toolkits like Automerge and Yjs could become the backing filetype for local-first software with interoperable sync/collaboration backends. The user can then have direct access to the underlaying data via standard tooling. Files can be linked, embedded within each other, forked and merged. We could have a new hypermedia platform built on this, where all documents are possible to be shared, forked, edited in realtime... Basically, love what you are all doing at Ink and Switch, excited to see what you publish next. reply dustingetz 11 hours agorootparenttaking back control from evil corporations is a funding/finance problem, not a technology problem. Everyone dreams of democratized ownership until they have to pay the huge developer salaries. and the go to market costs are even higher than that, all channels are saturated and you have to be louder than the noise. reply pyinstallwoes 5 hours agorootparentIt’s absolutely a technology problem. The hacker mentality is still the one who innovates and a single person is more than enough to make a significant contribution towards a very different future. That person is probably already working on it. reply tomcam 9 hours agoparentprevI would normally agree with your assessment, but the problem is that the browser vendors often revoke APIs, and destroy good popular extensions. reply exe34 12 hours agoparentprevExecuting untrusted code would be a lot safer if browsers and mobile OSes would make it easy to provide fake resources to the app/extension. Yes, you may read my phone contents, and as far as you know, it's the contents, the whole contents and nothing but the contents - it just happens to be a folder to me. An empty folder. It's a new phone you see. Yes here's my contact list. Sorry it's mostly empty, there's just the costly premium number in there. I hope your mothership doesn't try to call it. Yes, here's my microphone. Oh thank you, yes, I do a good impression of Rick Astley. Pictures on my phone? Oh yes, right this way. It's all pictures of turnips. Do you like them? reply jwells89 7 hours agorootparentSimilarly every browser should have the capability to report to sites that the user has notifications enabled when they actually don’t to end those annoying in-site “pre-prompts” which bait you into saying no to the pre-prompts so they can try to ask you again later, rather than just deal with the fact that the user denied permission with the browser-level prompt and isn’t interested. reply klabb3 8 hours agorootparentprevI don’t think this is a bad idea per se (after all a fundamental principle of the open web is that the user should control the browser). However, although your suggestion is fun, it is mere civil disobedience for geeks. The million dollar question is: how do you deliver those capabilities (a) without having grandmas phone full of spyware and (b) without giving your favorite Silicon Valley thought leader a 40% cut and total control of the ecosystem? I don’t have the answer. Just trying to formulate the problem. reply yjftsjthsd-h 6 hours agorootparent> The million dollar question is: how do you deliver those capabilities (a) without having grandmas phone full of spyware and (b) without giving your favorite Silicon Valley thought leader a 40% cut and total control of the ecosystem? That seems orthogonal? Grandma's phone has the same spyware either way, but this makes it a toss up whether it can spy on anything real reply transpute 8 hours agorootparentpreviOS does offer options for \"read selected photos\" and \"add-only photos\". Contact list subset and pseudo-sensors (camera, microphone, accelerometer, barometer) are much needed. Preset location is also needed, but some apps enforce DRM or other policy by location. App-level network policy (whitelist, blacklist) is needed. For enterprise MDM, iOS allows per-app VPNs, which could enforce app-specific network filtering. With Apple Configurator policy files, Safari can have on-demand VPNs for specific websites. reply jwells89 7 hours agorootparent> iOS does offer options for \"read selected photos\" and \"add-only photos\". The annoying thing here is how apps insist on either requiring full album access so they can implement their own photo picker or don’t provide a button to re-trigger reselection of “selected photos”. I wish they’d just use the standard OS selector dialog and call it a day. I don’t care if the standard selector doesn’t meet some stupid product requirement, it’s good enough. reply transpute 6 hours agorootparent> don't provide a button to re-trigger reselection of \"selected photos\" iOS Settings should have an app setting menu to \"Edit Selected Photos\". reply nottorp 12 hours agorootparentprevThere is already a permission system? reply gleenn 11 hours agorootparentThe issue the parent is trying to solve is you don't really have fine grained enough control, or apps nag you and won't load until you give them everything they want. My mom has a cheap camera security app that allows me to see the live streams from remote. Every single time I open the app it asks me again if I want to allow it access to my local network. The answer is a resounding \"no\". If I could just say \"fake yes, here is my fake network\", then I wouldn't be continually coerced into giving permissions to something I really don't want to share. I can think of many similar examples, another really common one is giving apps access to my contacts. Absolutely not, stop asking me, here is \"Uncle Bob\" with phone number 1-222-222-2222. Leave me alone reply robocat 9 hours agorootparentI wish it were easier to deny internet access to Apps. It isn't a perfect solution but it prevents the simplest data theft. Unfortunately side channel attacks are still too easy: Either a cooperating app, or send once of high value data via a link click opening the browser. From what I can tell, internet access is the default just to allow apps to have advertising. Too cynical? Android originally could deny internet access to Apps which I found useful. Certainly I don't want an extension or plugin to have pull access to the internet. That may limit functionality. But often only push is needed (e.g. blocking list could be pushed). No third-party keyboard should have internet access. Edit: rewrote a little clearer. reply Zak 9 hours agorootparentprevXPrivacyLua for Android does just that. It requires LSPosed, which enables deep modifications of the OS and other apps. Needless to say, that has its own security implications. reply iansinnott 6 hours agoparentprev> Customizability is important enough that it may be worth prioritizing it over security in some cases. 100% this. It should at least be acknowledged that \"security\" often means less options for the user. reply pyinstallwoes 5 hours agoparentprevSolution: move everything to client side. reply nottorp 12 hours agoparentprevAre you sure browser extensions improve the web apps? Maybe they attempt to fix them because they're limited by the platform and mostly low quality software? reply akkartik 15 hours agoprevJust the framing of \"browser extensions\" is extremely problematic in the year 2024. Most browser extensions by weight are Google Chrome extensions. Google Chrome is unambiguously demonstrating that no API is safe in its quest to juice revenues. Anybody who builds extensions using Chrome's APIs should be very aware that they're quite possibly putting effort into something a juggernaut will stomp away without a second thought. I don't care to live in strategically lost situations like this, so I think the conversation should be about Firefox extensions. Which also don't have a great track record (the transition to Google Chrome compatibility a few short years ago still annoys me greatly), but are a qualitatively better counter-party to deal with. reply foobiekr 15 hours agoparentForget all that. 1. They increase the attack surface of the browser 2. They have routinely been transferred to (for money) or taken over by malicious entities 3. Often they subtly break things in ways that are fine for expert users but which result in support reach out by others The whole extension thing is a mess. reply syoc 14 hours agorootparentReplace browser with operating system or computer and expand extensions to user installable programs and it mostly still rings true. I believe users should be empowered to modify their installed applications as they see fit. reply Spivak 14 hours agorootparentIt doesn't ring true for installed software anymore — \"virus scanners\" have gotten to the point where they just work for most people, desktop software is more difficult develop (for your average hacker wannabe), more difficult to get users to install, and has far less valuable data to go after. I actually very much like Apple's approach to browser extensions forcing them to be truly installed software and in the purview of tools that protect the rest of the system. The Chrome browser extension ecosystem is perfectly fine in theory but suffers from reinventing installed software without taking any of the lessons we've learned about OS software. Nice cautionary tale but the web is different. reply dvdkon 14 hours agorootparentOn a typical PC, installed software has even more permissions than a browser extension, and all any malware author has to do is write their own keylogger or upload the browser cookie database. Sure, it's a little more effort, but I think the only real advantage that malicious browser extensions have over native programs is the discoverability and auto-update Google and Mozilla give them \"for free\". reply everdrive 14 hours agorootparentprevForget all that. 1. They increase the attack surface of the operating system 2. They have routinely been transferred to (for money) or taken over by malicious entities 3. Often they subtly break things in ways that are fine for expert users but which result in support reach out by others The whole web browser thing is a mess. reply moolcool 14 hours agorootparentprevSmall price to pay for adblock reply dev1ycan 14 hours agorootparentprevActually hilarious that we have people here defending removing extensions, as if they didn't live through the days of Internet explorer. Well, maybe they didn't I hope they enjoy the eventual return of popups. reply wiseowise 11 hours agorootparent> Actually hilarious that we have people here defending removing extensions, as if they didn't live through the days of Internet explorer. I wouldn’t be surprised if Gen Z didn’t live through it. reply Spivak 14 hours agorootparentprevThey never left they're just called modals now. reply AJ007 14 hours agorootparentEndless EU Cookie modals that you have to always click through because you clear cookies. reply sunshowers 14 hours agorootparentprevUltimately, as a society, we have to decide what is more important: the best of us or the worst of us. reply loktarogar 13 hours agorootparentFraming it like that makes it much more simplistic than reality. While there are some people you can clearly place into \"best\" or \"worst\", most people fit somewhere along a spectrum where their placement changes day to day. You ever had a bad day where you forgot to do something you would have done any other day? Do you want software that allows you to do anything on a good day but is potentially catastrophic on a bad day? The answer may still be yes, but regardless it's a more complicated a question than best vs worst. reply sunshowers 13 hours agorootparentThat's fair, I was being more flippant than necessary. :) reply userbinator 13 hours agorootparentprev\"Those who give up freedom for security deserve neither.\" reply CharlesW 12 hours agorootparentThe real quote is more nuanced: \"Those who would give up essential Liberty, to purchase a little temporary Safety, deserve neither Liberty nor Safety\". It's a balance, obviously. I'm happy to have guardrails if they improve non-technical users' safety. reply wiseowise 11 hours agorootparent> I'm happy to have guardrails if they improve non-technical users' safety. Not at the expense of expert freedom. reply CharlesW 9 hours agorootparentSafety is paramount for experts. Those who disregard the importance of safety are likely not experts in their field. If the \"console\" analogy doesn't resonate, think of Apple as NASCAR. NASCAR has created a private ecosystem. Participating in NASCAR as a team or a driver is a choice, contingent upon meeting their requirements and paying entry fees. NASCAR implements numerous safety measures — SAFER barriers, catch fencing, HANS devices, etc. — to protect everyone involved, whether spectators (users) or drivers and teams (developers and vendors). NASCAR prioritizes the ecosystem first, then spectators, then teams and drivers — in that order. It doesn’t compromise the ecosystem or spectator safety to accommodate individual teams or drivers. Driver safety is crucial, not just because NASCAR values them, but because incidents involving drivers can negatively impact the ecosystem and spectators. Those wishing for NASCAR to resemble the Baja 1000 are tilting at windmills. Similarly, people who want iOS to be like Android aren't just wasting their time, but also disregarding the preferences of users who prioritize platform safety. reply Pxtl 13 hours agorootparentprevHonestly as much as I love Firefox this is an underrated concern. Firefox allows their extensions to be far more powerful than Chrome's, but that power means they are also far more dangerous. If Firefox were to really take off (like it should, imho), are we really ready for a web full of people being attacked by the worst spyware ever? Chrome, for all its faults, has ruined their extension framework at least in part because they were trying to prevent this threat. How do we make this work? Endless notification spam from the plug-ins? Expensive certifications for each plug-in release? reply bee_rider 12 hours agorootparentI’d be really curious about in a system where browser extensions are limited to ~200 lines of code. No mechanism for distribution beyond typing text in. No concerns about permission. It would be interesting to see what people can do in an ecosystem where extensions can actually do anything but it is expected that people will actually read the code before running it. reply playingalong 12 hours agorootparentHow to encourage code golfing in real world usages? reply Pxtl 12 hours agorootparentprevMy reaction would be simpler: Anything that's identified as risky? Show the user. Extension is making an HTTP request? Show the body in a toast. Extension is reading the keyboard? Same thing. Extension is looking at the page? Little icon in the corner showing the name of the extension and that it looked. Can't be turned off. So extensions can still do all that crazy stuff, but they're noisy about it. reply jwells89 6 hours agorootparentIf nothing else, basic logs of everything an extension does should be kept so that technically knowledgable users can take a look at the logs periodically (and maybe have them watched automatically by tools) to make sure everything checks out. reply bee_rider 11 hours agorootparentprevI don’t really see this as simpler: 1) “identified as risky” seems like it could hide some significant complexity (and room for error). 2) An extension might need to read from the keyboard. I don’t want to OK it every time. If I check once and then mark it as OK, I’d be worried that it could do something evil with that permission somehow, in a far-flung bit of the code. reply johnfernow 8 hours agorootparentprevI agree. When an app uses GPS on my phone, I'm informed of that: a notification permanently displays in the top bar until it is no longer being used. Same with the camera and mic. If my clipboard is copied, I get a notification as well informing me of that and telling me which app did it. I'm not sure why a similar system doesn't exist for browser extensions. Furthermore, there are limits to what features you can and cannot disable for Chrome extensions, and as far as I'm aware there are no logs of what actions they took. I had an extension that randomly redirected me to scam URLs while doing completely innocuous things such as visiting the homepage for Gmail, YouTube, or performing a Google search (after pressing enter for the initial query, before clicking on any URL.) I had 15 extensions, and the redirects were infrequent enough that disabling extensions one by one wouldn't help much: it could potentially take months to track it down, and there's no way of disabling the permission to redirect to different URLs. I searched the minified source code for all of the extensions that I had, but none of them had the URLs I was redirected to. My guess is that they pulled data from a server and then redirected me to whatever malicious URL it pulled at that time. I also checked network traffic in the Chrome Task Manager to see if there was an extension sending data for unknown reasons, but again, nothing, so it likely periodically pulls a URL to redirect me to from some server, redirects me, and then sleeps for a few days. Short of un-minifying all 15 extensions and trying to understand the purpose of every redirect, many of which would be legitimate, I'm not sure what can be done. In the end, I removed every last extension aside from my password manager and uBlock Origin (which fixed the issue — over one month later I've never been redirected to a scam URL.) Many of the extensions I used were open source, but I don't think any hash system exists to verify the minified code matches the source files for Chrome extensions (maybe I could do that manually, but I don't want to do that every time there's an update for any of the 15 extensions I had.) It's unfortunate, as many of the extensions I used improved my productivity and helped me focus better and be distracted less. But as it is currently, the browser extension ecosystem simply isn't safe. From what I've heard, Firefox's review process is better in some ways than Chrome's, but their extensions can have even more control of your browser. I don't think it's impossible to design an extension system that is secure: extensions just need to have the ability to be granted extremely limited permissions, and any permission beyond what is reasonable should be denied in the review process for putting it on the Chrome or Firefox extension stores. Most of my extensions shouldn't have even needed Internet access (if they can execute JavaScript, they'd still be able to redirect me to a scam URL, but if it couldn't have pulled a URL from an external server, then the URL would need to be in the minified JS, so I'd have been able to catch it.) reply Dwedit 7 hours agorootparentprevTampermonkey? reply emodendroket 15 hours agoparentprev> Most browser extensions by weight are Google Chrome extensions. Google Chrome is unambiguously demonstrating that no API is safe in its quest to juice revenues. Anybody who builds extensions using Chrome's APIs should be very aware that they're quite possibly putting effort into something a juggernaut will stomp away without a second thought. How unlike developing for literally any other environment. reply akkartik 15 hours agorootparentI don't know if you're being sarcastic. There's a spectrum between developing for Lua (juggernaut is super friendly), Python (juggernaut is mostly friendly, even if 2->3 caused a lot of casualties), Go (in spite of the corporate backer, quite careful about not stomping) and Chrome. Yes, there's always a counter-party. My point is it saves a lot of later grief to consider up front the counter-party you're entering into a relationship with. Their incentives and track record. reply crazygringo 10 hours agorootparentWhich, for plenty of Chrome extensions, is fine. Google has removed capabilities for certain categories and it's pretty easy to figure out what's going to be risky. But I use a set of very useful extensions, none of which present any problem to Google, all of which are extremely useful, and all of which I expect to stick around. reply moffkalast 13 hours agorootparentprevQuite right. Google and other commercial platforms may cut features or make breaking changes out of greed, while open source projects do it because they chase shiny things and can't be arsed to do legacy support. The end result is the same. reply Animats 15 hours agoparentprevMost browser extensions seem to be used on Firefox, because Google is so hostile to ones on Chrome. With the decline of Firefox, the extension world has shrunk. I had something called \"Ad Limiter\" on both Firefox and Chrome for a decade. Identical code, even. Google sent me threatening messages last year, as they tightened the screws on ad blockers, and I dropped it for Chrome. reply akkartik 15 hours agorootparentThat's a good point. Perhaps Firefox will benefit from an embrace/extinguish maneuver for once. Become compatible with Chrome extensions, then take over the space as Google retreats. This path too passes through no longer referring to \"browser extensions\". reply Animats 15 hours agorootparentExtensions were compatible for years until Google changed the manifest format and parts of the API. reply w3news 3 hours agoparentprevThere is a standard for browser extensions. I build also browser extensions before the standard. So you can build now a browser extension that works in Chrome, Firefox, Edge and Safari. But indeed, you can also use some specific api's for only a single browser. That is really bad, like you build a site only for a single browser. But the base should be compatible. And because you always can see the extension source code, you can modify a version for your own that works well in your browser. (And you can share it again off course) reply swozey 15 hours agoparentprevHas Firefox fixed its syncing feature? You used to have to literally move a profile file around. I remember working in IT a long time ago and Firefox was an absolute nightmare to deal with corporately. But then, back then, we couldn't control Chrome extension installations.. reply akkartik 15 hours agorootparentI'm only on Firefox because there's nothing better, but its sync at least has been pretty rock solid for me for several years now. reply mozman 15 hours agorootparentprevSync was fixed as part of quantum. reply 1vuio0pswjnm7 12 hours agoparentprev\"I don't care to live in strategically lost situatios like this, so I think the conversation should be about Firefox extensions.\" Why would the conversation not be about editing the Firefox source code to add or remove \"features\" to meet one's personal needs. What is the point of \"open source\" if, to use the term from the submission title, the software is effectively un-\"hackable\". There is no small amount of \"attack surface\", and many unneeded \"features\", that could be removed from Firefox to someone's benefit, maybe it's only one user,^0 but but that will effectively never happen. Why. It is open source so anyone should be able to audit the code and change it to their liking. 0. To be clear, I am not commenting about \"most users\" or the majority of users or whatever. I am referring to the small class of users who are explicitly dissatisfied. In 1995, there were numerous non-commercial browsers. Netscape, the source of Mozilla, was one of the few attempting to commercialise. https://www.w3.org/Clients.html There is nothing wrong with having \"all-in-one\" programs. As long as other \"not-all-in-one\" programs also exist as alternatives. Arguably, the aim of the \"all-in-one\" program may be to obviate the existence of other programs, namely smaller, simpler ones. Those pushing gigantic web browsers might assume and argue, e.g., that it is inconvenient to have different programs for different tasks. This could be true. For some users. However it is also true that small programs can be made to work with each other. UNIX is the example. Over thirty years of continual growth. The companies behind the giant browsers probably could not survive without it. There is choice. Large \"all-in-one\" programs and small ones like UNIX utilities can co-exist. The two are not mutually exclusive. Personally, I prefer not to use a giant browser to make HTTP requests on the open internet. It is overkill and there is a profound lack of user control. (Hence \"solutions\" like \"sandboxing\", and an ever-incresing number of Band-Aids that serve only to add more needless complexity. The companies releasing these giant \"all-in-one\" programs are funded by advertising. Enough said.) For me the \"modern\" browser is more useful as an image viewer and media player. It is possible to \"browse\" the web without advertising, tracking or other annoyances, I do it every day,^1 but not with one of these giant advertising-supported \"all-in-one\" programs like the \"modern\" web browser. It is a losing battle to try. No amount of \"extensions\" can change the balance of power over those giant programs. Despite that these \"browsers\" are \"open source\", dissatisfied users who know how to program are not editing the source code to remove the bad bits. Instead they helplessly complain in forums like HN. 1. I am not a typical user. (Though I might be in 1995.) I prefer text over graphics. I like to read without distraction. Because text is easy for the user to manipulate, it seems to have a defense against advertising that is not available with graphics. For example, if text ads were inserted into response bodies, I can easily filter them out. reply yjftsjthsd-h 5 hours agorootparent> Why would the conversation not be about editing the Firefox source code to add or remove \"features\" to meet one's personal needs. Because extensions are way easier to write, less likely to break because they use mostly stable public interfaces, and don't require an amazingly long compile. reply tbtech_vn 8 hours agorootparentprevI'd very much love to be able to clearly remove features I don't want and use, including a lot of the things about profiles, then use a tool to remove all unused codepaths to make a fast, usable and hopefully easier to understand product. But who has the time to dig into the behemoths of firefox and chrome today? It's just too much code to easily grasp. reply akkartik 12 hours agorootparentprevOh I agree so much with you. https://akkartik.name/freewheeling reply throwaway63467 17 hours agoprevMany popular browser extensions were bought up by data brokers that use them to exfiltrate browser history, so not sure if they’re underrated, I think you have to be pretty careful as the extension security/privacy model is/was pretty awful. I e.g. know screenshotting extensions (Awesome Screenshot) that would vacuum up your browser history and send it to a data broker in Israel. So probably better to have that as a native browser feature. reply lapcat 15 hours agoparent> Many popular browser extensions were bought up by data brokers that use them to exfiltrate browser history, so not sure if they’re underrated I would say, as the developer of an upfront paid web browser extension, that upfront paid web browser extensions are underrated. ;-) It's a truism that if you're not the customer, you're the product. But what if you are the customer? I think a lot of the mistrust of browser extensions is due to the difficulty in monetizing extensions directly. If you're making nothing from an extension, and someone offers you a nice check to acquire the extension, it can be difficult to turn down that money, especially if the extension is a support burdern for the developer. Of course I have my price too, as almost everyone does, but at this point the price would have to be 7 figures (maybe 8??), which I don't think anyone would ever pay for my extension. My user base is relatively small, and thus doesn't provide a huge opportunity for data collection or other nefarious schemes, precisely because the extension is paid rather than free. reply mnau 15 hours agorootparentI will leave this as a gallery of emails with offers to buy extension hoverzoom: https://github.com/extesy/hoverzoom/discussions/670 Sidenote: The \"collaboration\" offers come from time to time even to non-extensions projects, if they are reasonably widely used. E.g. simple tools (rather widely used suite of android apps recently sold). reply vsnf 2 hours agorootparentSome of those offers are insultingly low. $3000 to purchase the whole project? Really? reply bbsz 12 hours agorootparentprevOut of curiosity, those Russian messages are in Russian because you are Russian or an eastern solicitor simply doesn't give a F? reply mnau 11 hours agorootparentWhat Russian messages? reply bbsz 11 hours agorootparent06/07/2016 and 10/30/2017, and 11/22/2018, I think there may be one or two more but I am too lazy. cool idea to publish those. i remember when the pirate bay was publishing takedown notices in a special, public, category reply mnau 10 hours agorootparentI am not the the developer of the extension. It's just interesting issue I have come across. reply Fnoord 14 hours agorootparentprevThis is fantastic. Too bad they redacted the names. These scumbags deserve to be known. And the saddest part of the story is you don't know if is true or a cover-up. On the other hand it appears to be MIT. Are Google Chrome extensions reproducible? reply lapcat 15 hours agorootparentprev\"Your real profit per day will be $ 9000.\" LOL reply mnau 15 hours agorootparentI believe the profit number, even the number of lines > 8 lines of code in the manifest of your extension. As long as they are lines [like ones used to collect card info](https://www.theregister.com/2018/09/11/british_airways_websi...) from British Airways (supply chain attack). For how many days will profit be collected is the question (plus the fun criminal investigation). reply Fnoord 14 hours agorootparentYup, and he won't care about the criminal investigation because from other side of iron curtain v2. But if you're from the side where the nation isn't the cover for criminal enterprise you could get in trouble. reply emodendroket 15 hours agorootparentprev> It's a truism that if you're not the customer, you're the product. Though, even if you are, paid products are often monetized in all the exact same ways. Why not. reply xmprt 14 hours agorootparentThe only difference between a paid and unpaid piece of software is the revenue stream. In a paid software, your incentive to not screw over existing users is because your app would get poorer ratings and you won't acquire new paying customers. I've seen many times where a paid app stops growing as much and turns into a subscription model or becomes unpaid, giving paid users some small benefit (or nothing at all) and starts screwing over all users indiscriminately. reply jwells89 15 hours agorootparentprevSomething that’d help here is if extension galleries displayed price tags and let you filter by paid (bonus points for being able to distinguish between one-time and subscription). reply jwells89 16 hours agoparentprevYes. Because of this and the lack of fine-grained permissions mentioned by a sibling comment, I tend to use desktop apps where I can instead of extensions, keeping my extensions list quite slim — basically all I install are FOSS extensions by “big” known-good authors (e.g. Raymond Hill) or projects that aren’t going to sell out. Of course risks exist with desktop apps too, but historically this kind of buy-and-exfiltrate scheme is comparatively rare with desktop apps, particularly on macOS where signed apps are sandboxed and can’t do a whole lot without user permissions. reply seanwilson 16 hours agorootparent> I tend to use desktop apps where I can instead of extensions How locked down are desktop apps now on Mac, Windows and Linux? I haven't kept up. Do they still a lot of access by default to do malicious things with? I recently saw someone install the Adobe Acrobat desktop app and it installed its own extension inside of Chrome without asking. Games can have scary DRM as well. Chrome extensions can't read/write to arbitrary places on your hard disk without asking for example and you can isolate them within separate profiles. Not saying they're perfect but there is robust sandboxing of what they're allowed to do. I'm curious how this compares to an Electron-based desktop app i.e. which is running Chrome on the inside but with the standard restrictions Chrome places on tabs and extensions unlocked. reply jwells89 15 hours agorootparent> How locked down are desktop apps now on Mac, Windows and Linux? It’s hit or miss. There have been advancements on macOS and Linux where there are mobile-style permissions and sandboxing in some cases, but one needs to be aware of how apps are packaged to be able to leverage these advancements. Adobe stuff and Chrome on macOS for example have basically free reign still as they have specifically opted out of OS sandboxing, while a lot of small indie apps are sandboxed. Chrome I think can be put in a sandbox on Linux by way of Flatpak. Windows has done practically nothing and is the same as it’s always been where desktop apps can do basically whatever they please, especially if given privileges with UAC (which seemingly every other Windows app needs for some reason). reply wongarsu 15 hours agorootparentWindows introduced better mobile-style permissions and sandboxing with the APPX format in Windows 8. However the only incentives to use it was the ability to build UWP apps and accessing the Windows Store. Everyone rejected the Windows Store, so developer adoption is close to zero (and now those incentives are gone too) reply lapcat 16 hours agorootparentprev> on macOS where signed apps are sandboxed and can’t do a whole lot without user permissions Mac App Store apps are (mostly) sandboxed. Developer ID signed Mac apps distributed outside the App Store are mostly not sandboxed. reply seagulls 13 hours agorootparentprevThe bar to write secure desktop software is significantly higher than for browser extensions. Especially with all the Electron crap these days, you're one XSS away from full-blown RCE. reply jwells89 13 hours agorootparentAbsolutely, but the short and long terms risk posed to most by installing random browser extensions willy-nilly is still almost certainly higher than that of instead opting for vetted desktop apps, especially if using PWAs in place of Electron apps where possible (which I do). reply asadotzler 12 hours agorootparentDesktop apps are no more vetted than Firefox extensions. reply jwells89 12 hours agorootparentI’m talking about community vetting. It’s usually easier to find discussions on the internet where people have discussed and scrutinized desktop apps (e.g. “this app phones home”) than it is to find the same for most browser extensions (which are often only heard about after having been turned into malware). The tooling is often better there too, e.g. one can keep a short leash on app network activity with Little Snitch and similar but I’m not aware of an equivalent for browser extensions. reply foobiekr 15 hours agorootparentprevIt's not the lack of a fine grained permissions model, it's the total lack of a real threat model and any consideration at all for what happens as extensions change over time. reply wintermutestwin 13 hours agoparentprev>probably better to have that as a native browser feature /Agree. It is crazy that I have to trust some unknown coder with all my browser data just to enable vertical tabs in Firefox. Of course many of these extensions are open source and thus auditable. As I lack the skill to detect nefarious code, I am wondering if this might be a good use case for AI. Anyone have thoughts on building a good malware finding prompts? reply towelpluswater 5 hours agorootparentThis is a really great idea and use case. It also makes a ton of sense as a pilot use case for this type of open source project given extensions are smaller in scope. I mean even having it document a best draft of what the extension code is doing would be awesome. Unless it’s made into an extension and then you have a recursive hell. reply seanwilson 16 hours agoprevI wish browser extensions had more fine-grained permissions but it's a tricky problem verifying if software is using permissions maliciously (see the Obfuscated C Code Contest and the Underhand C Contest) and how to communicate nuanced permissions to users (most users don't read and/or understand tech stuff, and can be easily mislead). A tip in Chrome that I never see mentioned if you want to be extra safe when trying extensions: - Go to Profiles > Add profile > Continue without account - Install any extensions you feel like in this profile and they're completely isolated from the tabs logins, history, cookies and so on in your regular profile. Similarly, you can run Chrome Beta or Chrome Canary for installing extensions into, alongside regular Chrome. E.g. you can install 10s of potentially risky web development extensions into this profile (they usually need a lot of access to do what they need to do), and keep them sandboxed away from the profile where you do your personal banking or login to work websites. It's not practical for every extension, but I do this for my web development stuff and only use a couple of extensions for personal stuff. I sell a browser extension where the permission I really want to ask for is \"can only observe the network traffic it sends/receives in its own tabs\" but I'm lumped with having to ask for the \"read and write all your data\" permission, but I make sure to share the above tip in the description (shameless plug: https://chromewebstore.google.com/detail/checkbot-seo-web-sp...). reply imhoguy 16 hours agoparentFirefox user here, I wish Multi-Account Containers had a way to disable extensions per container. I don't need any on my banking site. Sure I could use separate Profile but UX hurts here. reply thisislife2 16 hours agorootparentYeah, as you figured out, a separate profile is currently the only workaround. In case you aren't aware, there is an easy way to quickly launch it though in Firefox or Pale Moon - go to about:profiles and you can easily create / launch any profiles quickly in a new window. reply Terr_ 15 hours agorootparentprevIt may be a little paranoid, but I use a separate local user account for those kinds of things. Perhaps not convenient, but it certainly helps keep me on task when I'm in official-paperwork mode. :p reply SushiHippie 16 hours agorootparentprevYep firefox profile UX is sadly not good. But I just bind different firefox profiles to different keybinds in my WM reply fsflover 13 hours agorootparentprevI solved this problem by using Qubes OS. Different Firefox instances for different tasks run in dedicated VMs, with independent configs and extensions. It allowed to better organize my digital live and provided more security at the same time. reply sidwyn 15 hours agoparentprevThe \"read and change all your data\" permission is a huge hurdle for our shopping extension, especially since we only need to identify shopping pages. What I've tried to build trust is to open source our tracking analytics (e.g. https://github.com/Score-Extension/score-extension-analytics...). Hopefully transparency is one way to overcome this trust barrier. reply Springtime 15 hours agoparentprev> I sell a browser extension where the permission I really want to ask for is \"can only observe the network traffic it sends/receives in its own tabs\" but I'm lumped with having to ask for the \"read and write all your data\" permission Yeah it would be nice there were a way to limit the entire scope of an addon's permissions to a whitelist of domains. Chromium has a way of whitelisting domains an addon can run on[1] but I've assumed it doesn't affects the broader permissions you mention (general history, etc). [1] Click 'Details' of the addon and switch the 'Allow this extension to read and change all your data on websites you visit' option to 'On specific sites' then add the sites to the whitelist. reply seanwilson 15 hours agorootparent> Yeah it would be nice there were a way to limit the entire scope of an addon's permissions to a whitelist of domains. You can do this for the network read/write permissions, where the permission request dialog on install will tell you the URL patterns the extension wants access to. I can't do this for my specific extension though. My extension checks web pages for problems like broken links, so it needs to be able to fetch any web page URL you give it and then it has to fetch any URLs that are linked to on the page, so I have to ask for access to http://\\\\\\* and https://\\\\\\* (I could maybe get away with just the `activeTab` permission to check the domain of the current tab if the checks were more limited though). The extension is only doing operations like this within its own tab, when you have the extension open, and for it's own network requests, so it's frustrating there isn't a more granular permission I can ask for as I've isolated it as much as I could. It's a tricky problem though. Browser makers will have certain kinds of extensions in mind, and optimise to make the permission system and permission request messages friendly for those kinds of extensions. Less standard extensions usually have to settle for broader permissions with less friendly permission descriptions, until hopefully the permission system gets iterated on based on how it's being used in the wild (Manifest V3 in Chrome for example). reply justsomehnguy 15 hours agoparentprevOn Windows you can use apps packaged by portableapps.com. Needs AllowMultipleInstances=true in the .ini. reply PaulDavisThe1st 17 hours agoprev> Browser extensions remind us what it’s like to have deep control over how we use our computers. Uh. Linux users would like a word here. But more generally, there's a significant component of this that seems isomorphous to the question I was trying to discuss in a post I wrote several years ago called \"Is Open Source a diversion from what users really want?\" There seems to be much more excitement about ways to \"hack\" software that do not involve build systems than the complete, open-ended and (theoretically) unbounded access provided by FLOSS. It's not hard to see some obvious reasons why that would be true, but still a little disappointing. I tried to discuss that here, specifically in the contrast between Reaper's provision of scripting-but-closed-source versus Ardour's scripting-but-open-source. https://discourse.ardour.org/t/is-open-source-a-diversion-fr... reply Retr0id 16 hours agoparent> Uh. Linux users would like a word here. As a Linux user, I disagree. It's not quite the same. Yes, I could recompile my kernel if I wanted to. I can recompile most of userspace too. But it's a hassle, especially if you want to diverge from upstream, and maintain that divergence on a long-term basis. You can do some fun hacks with LD_PRELOAD et al, but it's nowhere near the degree of flexibility and ease of access of browser extensions. I am allowed to modify all the software as I see fit (and that's excellent), but the friction of actually doing so is (comparatively) high. reply nonrandomstring 16 hours agorootparentYou raise an important issue around persistence of state. The question isn't whether you need to recompile source, change config files, download application plugins or set-up a bunch of check-boxes in a nice GUI. It's whether you can trust those settings to stick. I've lost count of people telling me that phone settings I suggested simply \"reverted\" or somehow turned themselves back on/off. Even some Linux distros that use Snap alongside auto-updates etc are really quite sneaky. But to my mind web browsers (and I include all of them, Chrome, Firefox or whatever) are utterly treacherous. Any careful security stance requires constantly checking and re-checking that policies are still in effect. reply capitainenemo 16 hours agorootparentprevI feel gentoo reduces that hassle a fair amount since you can just toss the patches in and the distro pulls them in on updates. So long as you're not messing with APIs it's not too bad in terms of bitrot. ... I suppose you could do the same thing with debian too. You'd just need to maintain an overlay repo that rebuilds off the upstream deb sources for the packages you touched. At that point you're pretty much doing the same thing distro's volunteer maintainer is doing. Take an upstream package, add tweaks, rebuild them automatically with tweaks on the next upstream release. reply dvdkon 15 hours agorootparentIt's similar with NixOS, patching a package is just adding a few lines in a persistent (and generally short) config file. You \"only\" pay for that patch by having to update it for newer versions and by compile time. The developer experience isn't as good as browser extensions yet, though. Iterating on a patch means downloading that package to a local directory and building it there, which won't be enough for, say, patches to system libraries. You have to actually apply the system configuration for that, which means recompiling. reply Retr0id 16 hours agorootparentprevI should maybe give Gentoo a second try. I last tried it on a dual-core thinkpad and it was a pretty miserable experience due to the long compile times. These days I have fast computers, and I hear Gentoo even started shipping binaries recently. I have a huge amount of respect for the work distro maintainers do. It's not especially fun or glamorous work, and many are unaware that it even happens, but it's essential. reply blibble 13 hours agorootparentprevit's very easy with debian to maintain small patches on top of packages and dpkg-buildpackage will do all the hard work for you reply redder23 15 hours agorootparentprevWhat has compiling the kernel to do with it, its about the fact that Linux let you control ever single aspect of your OS and tweak it to your liking. Its a pretty good example of what shows you how it is to control your PC, more so then browser extensions. Just look at what a pain in the ass it is to remove Edge from windows, even now the EU has mandated it, its still a 10+ step guide that requires some tool from Github ... and b4 that you could not even to that. Your start menu in win11 is polluted with \"news\" and Bing AI crap ... with no simple way to just disable it. If you use Linux you are in control and there are no annoyances and almost no proprietary code from the very start. You have endless different Desktop Endorsements ... Linux offer way more control over the OS then any browser extensions do. Firefox killed the system where you could more modify the look of the Browser, I do not mind, but I am still making this point when we talk about feeling in control. You make no sense. reply asadotzler 12 hours agorootparentFirefox is every bit as open source as Linux. You can control every aspect of it and tweak it to your liking and you are not limited to extensions. reply lmm 11 hours agoparentprev> Uh. Linux users would like a word here. The shift of Linux to systemd was a very similar experience to the decline of browser extensions. Yes, you can change how your computer works. But unless you're willing to put a lot of effort into maintaining those changes, the APIs you use will be cut out from under you and it'll be harder and harder to make your computer do what you wanted rather than what someone else thought it should do. reply yoav 16 hours agoparentprevI think people see extensions as a way to bypass code signing, distribution, and brand building. So chrome (or whatever) becomes a platform for distributing and executing software. reply jlawrence6809 16 hours agoprevI built a chrome extension that is featured on the chrome web store[1] and the number of requests I get from shady data brokers looking to buy my extension and fill it with spyware is really concerning. A naive dev could build something cool and sell it off to someone thinking they'll maintain if for them but instead just cause a hazard for users. Google seems to do a decent job of reviewing the use of permissions but some extensions like mine really need access to everything on the page so I can only imagine what a data broker could do with it. Be careful what you install. [1] https://chromewebstore.google.com/detail/css-selector-helper... reply swozey 15 hours agoparentCool extension. I love when devs open source stuff that makes their lives easier. reply jlawrence6809 15 hours agorootparentThanks! Here is the repo if you have any issues/suggestions: https://github.com/jlawrence6809/CSS-Selector-Helper-for-Chr... reply swozey 15 hours agorootparentHow far did you have to deviate from the demo extension to make this? I've written themes for vscode and intellij but never done an actual extension because it's js/ts and I don't really enjoy writing those. I really wish they had a DSL for extensions to allow them to be more broadly written. Like, I feel like I have to basically learn js to learn to write a chrome extension and I'm a go/rust dev who will use it literally nowhere and I just want to make the AWS console not suck, for instance. But I keep trying to will someone like me into existence to make this extension and nobody is appearing lmao. reply jlawrence6809 12 hours agorootparentThis extension is pretty unlike most of the examples the chrome docs provide because it extends the devtools which most extensions don't do. There are a lot of hidden gotchas you have to look out for when extending devtools and the api they provide just isn't as well thought out. However I actually made the first version of this extension when I was just starting out learning html/css/js and I think it was good project for that. I wouldn't worry about making something presentable for the webstore at first. Just build whatever you need with really bare bones UI and iterate if you forsee it being useful for other people. Maybe even start with a greasemonkey script. reply swozey 16 hours agoprevI program (not js/ts), use a massive number extensions and consider myself an absolute power user of them and refuse to ever use a browser WITHOUT the chrome/firefox extension ecosystem, I've written themes for Chrome and VScode, but I'm still here- (like pink/cyan? get on in! https://marketplace.visualstudio.com/items?itemName=mikejk8s...). I have no idea via the Chrome prompts what extensions are able to do, read, see, access, etc. \"Allowed to access data on all websites\" - Is this literally all data? Like what I'm typing? Like does it know when I go URL to URL? it is just reading the assets? Is there a chrome API that limits their access that I can see? What do I actually need to worry about? I have a video zoomer that lets me zoom in on any video on any website, do I need to literally audit each extension myself and make sure it's not mirroring my data elsewhere or something? I have no idea. How would a non technical user know any of this? reply Rapzid 15 hours agoparentI'm pretty sure it's as bad as it sounds haha. Like another user mentioned because of this I only trust a few key extensions(and like that user uBlock, Bitwarden, etc) with this sorta access. I'd be very wary of those scrapy screen/session recording startups if for no other reason than they could be particularly vulnerable to supply chain attacks. reply Gigachad 7 hours agorootparentNot only is it theoretically as bad as it sounds, its as bad as it sounds in reality as well. Most of the top extensions get sold to ad companies and silently start sucking up all of your browsing data to sell on. Some of them start injecting their own adverts and tracker scripts on to pages, some of them are outright stealing your credentials. And you realistically have no way to sort the good from the bad. Especially when the good silently get sold to the bad and automatically updated. reply swozey 15 hours agorootparentprevYeah I always go to the source/project URL in the chrome store and IDEALLY it's a github repo with a bunch of contribs but I'm sure I've played loose with a few that had no other options. I just had one big extension I use get bought by someone last week when it updated. I gotta dig through that now.. I used to hide that extension update popup screen but now I'm glad I didn't. reply weaksauce 13 hours agoparentprevyes it’s that bad. i’ve written some webexts and if you ask for all data it really is all data... otherwise how would it work if you needed to change something on a page? i keep my list to my own bespoke one-off extensions or only the major big names or i audit the code manually. reply mmis1000 7 hours agorootparentYep, I always think the 'all data' means there is no official api to do it, so I screw it and make my own from ground up. Unfortunately browsers only make specific api for task that many people does. So there is always a portion of extensions need the 'all data' because there is no way otherwise. reply silvestrov 16 hours agoprevI think what we need the most is a \"view source\" for browser extensions installed from the store: make it easy to view the source and to extract the browser extension into a folder. Make it easy to find out which web pages they access and which they modified. Minimized/encrypted code in extensions should be forbidden. It should be very easy to read the code. E.g. this extensions says \"records user activity\", but what is that really: https://chromewebstore.google.com/detail/coffeelings/hcbddpp... reply a13o 16 hours agoparentIn chrome go to chrome://extensions, enable developer mode, and now you can view source for any extension in devtools. The content scripts are already available in the regular web page's devtools without enabling developer mode. The total list of websites is available in the installation popup for the extension. The chrome web store already bans code obfuscation. minification is allowed as there's no meaningful way to enforce the quality of variable names reply Fogest 16 hours agorootparentIt is very annoying to try and follow through minified code. I've tried to view the source and see what some extensions are doing but it can be a bit of a painful process. You can at least sometimes figure out what kind of GET/POST requests the extension may be making, but it's much more time consuming to try and ensure everything is safe. The other problem is that the extensions can update. You typically get zero notification an extension was updated. Most extensions start off safe, but later get sold and used to farm data. reply redder23 15 hours agorootparentThere is a button to format the code for minified files. reply Fogest 14 hours agorootparentFormatting isn't the issue. Just more time consuming to try and read the code when it's all got garbage variable and function names. Not that you can't do it, just slightly more effort. Also the bigger issue I mentioned in my comment relates to the problem of extensions updating without any notice. reply mardifoufs 10 hours agorootparentprevIs there any legitimate reason to minify code for extensions? The size gains are minimal since it's a one time thing. But I agree that it would be hard to enforce, though google \"manages\" to enforce even more ambiguous requirements on their play store haha. I guess they could make it a guideline or a requirement, and \"good faith\" devs would comply even if it would be hard to enforce. reply Sephr 15 hours agoparentprevYou can view the source of browser extensions hosted on the Chrome Web Store without installing them. I've occasionally used this tool for that purpose: https://robwu.nl/crxviewer/ This won't help against intentionally-obfuscated code but it should help with security & privacy research for most extensions. reply tech234a 8 hours agorootparentAlso available as an extension itself: https://chromewebstore.google.com/detail/chrome-extension-so... Edit: Firefox version: https://addons.mozilla.org/en-US/firefox/addon/crxviewer/ reply Sophira 16 hours agoprev> Today, it requires a big jump to go from using browser extensions to creating them: you need to learn a fair amount of web development to get started, and you can’t easily develop extensions in the browser itself. What if there were a quick way to get started developing and sharing extensions in the browser? You could imagine smoothly transitioning from editing a website in the developer tools to publishing a small extension. They're not full extensions, but userscripts and user styles go a long way, and extensions exist that allow people to create/use them in the browser (eg. Tampermonkey[0] and Stylus[1].) I consider them incredibly important, even though they can't do as much as extensions. [0] https://www.tampermonkey.net/ [1] https://chrome.google.com/webstore/detail/stylus/clngdbkpkpe... reply remram 15 hours agoparentUserscripts are underrated! I use them for all kinds of things, like fixing GitHub's useless landing page (taking me to my repositories instead), make the Mastodon \"follow\" button work (by hardcoding my instance's domain), block useless results from Google search results (stackshare and the like), redirect from the YouTube \"short\" view to the normal video video view, remove the stupid whitespace to the right of Gmail's scrollbar, etc. reply sanitycheck 14 hours agorootparentI've used Tampermonkey for a couple of moderately complex things and it does work well... I didn't come across a particularly nice way to use an external editor or integrate it with a normal dev workflow though, I wonder if anyone has tricks to share? I'm fairly satisfied with editing in VS Code, using a tsconfig.json with strict mode and checkJs turned on, then using JSDoc for typing. The ugly bit is the manual copy-paste into the Tampermonkey code area each time. reply remram 11 hours agorootparentI tend to copy/paste into the console anyway during development, so having to copy/paste into Tampermonkey too doesn't slow me down too much. I suppose it would be nice to have a more integrated workflow though. reply sanitycheck 30 minutes agorootparentYeah, I agree it's not slow as such - but I find it a bit distracting remembering to do it, and if I don't concentrate then I forget and then I risk confusing myself momentarily (not hard to do). reply dvdkon 13 hours agorootparentprevI don't use Tampermonkey (it's not FLOSS), but I'm pretty sure Violentmonkey autoreloads script files when that script was installed from a local file (maybe I had to enable it somewhere). reply sanitycheck 27 minutes agorootparentIt looks like you're right, I may try that instead in future. The userscripts I've made have been mostly for work and I immediately dismissed \"Violentmonkey\" as unsuitable because of the name, I'm not going to ask my clients or their (less technical) clients to install something that sounds quite nefarious. Unfortunate! (\"Tampermonkey\" is bad enough, but at least it's widely known.) reply mg 15 hours agoprevI prefer bookmarklets because they - Are easy to edit - Are inactive until clicked - Work in all browsers - Work on mobile - Integrate nicely into the UI. I can move them around, put them into any bookmark folder, assign shortcuts. I wrote this bookmarlet editor which makes it easy to convert between clean code and a bookmarklet: https://www.gibney.org/bookmarklet_editor reply dugite-code 7 hours agoparentWell that's a handy site you have there. Last time I fiddled with bookmarklets they didn't work on Firefox for Android, but now they do. This is going to be handy combining it with my Node-red instance. Got any good bookmarklets you want to share? reply mg 3 hours agorootparentYou can click on the question mark and then, when you click on one of the examples, it will fill the code area with the code for that bookmarklet. reply fabian2k 16 hours agoprevThey're much too big of a target now for spy- or malware. They have too much access to everything we do in a browser. And you can't just evaluate them once, they auto-update silently and you never know when they might be bought by a malicious actor. I use a very limited set of extensions I trust like uBlock origin and Bitwarden. Also some developer extensions, but usually not on my main browser. Everything else is just not worth the risk for me. reply empiricus 16 hours agoprevIs there a way to use browser extensions safely? Any extension that looks interesting needs access to everything I see on the screen (and even modify it), which to me seems a huge security risk. My understanding is that random extension is able to read and send somewhere almost all my data when I read my email, do online banking, etc. Do I understand correctly the situation? reply mozball 14 hours agoparent>My understanding is that random extension is able to read and send somewhere almost all my data when I read my email, do online banking, etc. Depends on the permissions requested by the extension but often yes. The permission \"Can read all data on any webpage\" means exactly that. > Is there a way to use browser extensions safely? Yes. Depending on your paranoia /security standards. Here's what you can do ( ordered by importance.) 1. Use more than one browser (but stay away from proprietary or less popular browsers) and/or use multiple profiles (both firefox and chrome has them) 2. Have separate profiles for banking, personal email, work and general browsing. (Also good for productivity) 3. Banking profile should have no extensions. 4. Use only mozilla-vetted 'recommended' and 'security reviewed' extensions in firefox for less important accounts. Check the permissions carefully and see if they're sane. I don't use extensions in chrome at all since google web store does no vetting at all beyond automated scanning. It's the wild west out there. 5. You can be less careful with general browsing profiles as long as you don't log into important accounts. Use firefox containers (this is more for privacy though than security) 6. If some addon is tempting but not reviewed - i try to review the code (if its small and readable enough). after vetting, i disable auto-updates. A greasemonkey script that does equivalent functionality is often preferable since the code is usually smaller and readable. Disable auto-update there too. Otherwise resist the temptation to install too many addons. reply fragmede 14 hours agorootparentChrome has controls to not allow an extension free reign on all sites despite it asking for them. Allow only on specified sites. it's not a default for some reason, but if the extension doesn't have access then it can't do anything, bad or good. Of course it doesn't help that it's a finance site that disables paste for which I need an extension to reenable, but at least I'm not letting the rest of my extensions get at my banking web session. reply empiricus 13 hours agorootparentprevSo the current options are 1. don't use extensions - this limits comfort and productivity, and the entire purpose of extensions 2. use extensions but lose security (are you feeling lucky today? what about tomorrow?) This seems so dumb. Is this the best solution from google/mozilla/etc? I am thinking that an option to disable all extensions on a particular site/tab could solve many issues, maybe even with default on for well known email and bank providers. This would encourage ppl to install more extensions because they don't care what happens when they just read reddit. reply ysavir 16 hours agoparentprevNot really, I don't think. I hear a lot of people saying that you can inspect the source if you follow steps X, Y, and Z, but that's not a one time thing. Each time the extension is updated you have to do a full audit. You can install it independently to avoid updates, but then you run the risk of things breaking or falling behind (such as adblocker lists). Happy to learn from more experienced people that I'm wrong on this, but that's my current expectation from decades of using browsers and extensions. For me, an extension can only require so much hands on effort before that effort outweighs the rewards of the extension. Years ago I had the Vimium plugin and loved it, but the provided functionality isn't worth the necessary audits. Not wanting to have to trust that it never sells out or gets hacked, I got rid of it. These days I just use a small handful of extensions (ublock origin, noscript, vuejs devtools) that I feel comfortable trusting and that make a significant impact on my browsing experience. I can manage without the rest. reply mozball 14 hours agorootparent- An addon like vimium shouldn't need too many updates so auditing and disabling auto-updates might be worth it. - Firefox has 'recommended' addons. In addition some of the more popular addons are security vetted (Their addon pages doesn't come with the scary \"not reviewed\" warning. These can be reasonably assumed to be safe. - Also read my other reply to gp. > These days I just use a small handful of extensions Same here. Resisting fomo and temptations for new shiny is the hardest part but still worthwhile imo reply senkora 16 hours agoparentprevIt's possible to extract the extensions source, save it locally, and then manually install it. That insulates you from the risk of a malicious update. (You could also audit the extension for complete safety, but TBH I'm usually too lazy to do that, and I assume that the risk of an extension currently being malicious is far lower than the risk of an extension later being updated to become malicious) reply seagulls 14 hours agorootparent> That insulates you from the risk of a malicious update. It also insulates you from critical security updates. Managing your own security is not without its risks. reply Hackbraten 16 hours agoparentprevYou're free to use only extensions which are open source. So you can build them yourself, and also spot check changes in the code whenever there's a new upstream release. reply gsuuon 14 hours agorootparentThat'd help, but a problem is they could still go closed-source and you wouldn't know - the store itself has no concept of open or closed source so it's not like you could check an \"uninstall if it goes closed source\" box. Maybe there's room for a browser extension that hosts other browser extensions but with a much better security model than what Google allows. reply dvdkon 13 hours agorootparentI think that'd be a great idea, an \"FDroid for extensions\": A store that serves exactly the code in the repo. Sadly I don't think Chrome/Firefox allow building this as an extension itself. reply Hackbraten 13 hours agorootparentprevYou don’t have to use the store to install and update the extension. You monitor the upstream GitHub release feed, and build and install the extension yourself on every update. reply pknerd 3 hours agoprevI have a few ideas that depend on Chrome extensions, the issue is, I do not know how to monetize them either via Ads or some sort of in-app purchases. reply w3news 3 hours agoprevI love to build extensions. Such a nice thing they made website source easy to read and manipulate for your own usage, and can even share your modifications to build an extension. It is just like your newspaper, you can write on it, cut precies out, etc. You can do with the site what you want for yourself. The newspaper designed also it how they like, but you can also grap your scissors and pen to change it for yourself. reply breadchris 13 hours agoprevWhat has always blown my mind is the lack of documentation/open source projects. With such powerful data we come across while browsing the web, it would only make sense to me there would be more tools to use an extend in this space. Browsing history is especially under valued. Even though the data technically exists, it is quite difficult to retrieve pages that have been visited, imo because of poor UX. Most people keep every Internet journey opened in hopes they will remember to return to it. I have been taking a stab at improving the UX with a history browser extension [1] which I have found myself legitimately finding value in using (a first for my personal projects lol). [1] https://github.com/lunabrain-ai/lunabrain/tree/main/js/exten... reply poisonborz 13 hours agoprevMore like overrated. An extension can't be better, can't offer more than what the host application allows. All these developers hang on by a thread. Compared to OS APIs, in-app APIs are more unstable. Goals, profit incentives affect a single application much harsher than how a wider ecosystem would react. It's good that they exist, but at most they are viewed as a necessary annoyance by their hosts. Chrome I won't even need to mention, but winds could turn anytime on something like VSCode as well. Sure, Webkit and VSCode are both open source and forkable along with their extension support, but any later development would rot compatibility until, and if, a popular fork emerges. reply monkellipse 17 hours agoprevI love the idea of browser extensions but they don’t appear to be worth the security/privacy risk for my use cases. I wonder how many others are like me and too paranoid to risk extensions at all? reply extesy 16 hours agoparentAt all? Not even ublock origin? That would actually go against your stated goal of security/privacy. reply monkellipse 16 hours agorootparentCorrect, none. I use Pihole for blocking. But the bigger point I think is that security conscious users are hesitant to employ extensions in general, even if some folks are ok with a couple select extensions they are still spooked by the general field. reply seagulls 14 hours agorootparentDNS blocking has not been effective for probably close to a decade, with domain-fronting, L7 adware/spyware, fingerprinting and other trickery. Parent comment correctly characterized the lack of UBO as a net security/privacy loss. reply Hackbraten 16 hours agoparentprevI use only very few extensions. If they're open source, then instead of installing them from the browser's store, I maintain them as AUR packages. [1] That way I force myself to build them from source. My habit is also to inspect the changes between upstream releases. It's mostly spot checks, but it's better than nothing. [1]: https://aur.archlinux.org/packages?O=0&SeB=nd&K=firefox-exte... reply swozey 15 hours agoparentprevI honestly can't imagine not using extensions. I'm 39 and have been on the web since Netscape etc in the early 90s and I honestly care more about the extensions than I do anything the browser actually does. Like, if there were no extensions I don't think I'd care at all if I used Firefox, Chrome, Opera, etc. But Chrome and Firefox have this massive, massive ecosystem of productitivy improving extensions. I'll give an example since I'm tooting so loudly about this, my job entails a lot of R&D and distributing knowledge to other engineers in a concise manner. I use an app called hypothesis- https://web.hypothes.is/ which is very popular in research groups. What it does is it lets me essentially annotate websites. So for instance I have an application with a front end UI, instead of writing readmes with no interaction to the front end UI I can actually annotate each page like a how-to, or a help doc. You go to that specific URL and get notified that there's a hypothesis doc on it to read. When I used to work at a k8s distro company I used it to help teach people how to deploy clusters, etc. Another one is Dark Reader that makes every single website dark mode.. Ublock I can't even remember a time of my life not using to block ads.. I do have null stuff via cloudflare dns as well but still use ublock everywhere since it's also a massive security improvement blocking chaotic javascript. It's amazing for training situations. https://web.hypothes.is/ reply FormulatedEdits 13 hours agorootparentHello. I used to use Dark Reader but then some it changed hands and a very questionable update appeared and freaked many people out, so I uninstalled. IIRC the changes were removed, or the additional code was not correctly activated, maybe both. Anyway, you may wish to check the status of that particular extension. I use some flag in config now to do approximately the same thing, it’s not as effective, but it’s close. reply imbnwa 10 hours agorootparentLink to Dark Reader changing hands and questionable update? reply mozball 14 hours agoparentprevYour paranoia is warranted. Like i replied in another thread up, there are a couple thing you can do. Use multiple browser/profiles. Keep a separate profile or two with no extensions for banking, shopping, email and other important stuff. You can be install a couple addons in your 'general browsing' profile. In general install only 'recommended' and security-reviewed addons with firefox. reply seagulls 14 hours agoparentprevThere's a handful of trustworthy extensions like uBlock Origin, otherwise any with full DOM access are basically a browser rootkit. reply GeekyBear 13 hours agoprevThe web has become unusable without extensions like uBlock Origin, but extensions can contain malware. I have moved over to only using extensions that have gone through Mozilla's manual code review necessary to become part of their \"recommended extensions\" program. > Before an extension receives Recommended status, it undergoes rigorous technical review by staff security experts https://support.mozilla.org/en-US/kb/recommended-extensions-... reply ww520 13 hours agoprevOne benefit I would add is that cross platform support is great for browser extensions. Browsers already run on different OS's and devices. Browser API and extension API are fairly uniform among the major browsers. It's close to the cross platform support of general websites. As an experiment I develop my latest browser extension on Firefox [1], Chrome, and Edge [2] at the same time to see how difficult it is to share the same code base. The difference is minuscule, like less than 0.01%. Chrome and Edge are essentially the same. Firefox is a bit behind in Manifest V3 support and needs a few lines Firefox specific API calls. The manifest files have a few differences. Overall, sharing the same code base is very feasible. [1] https://addons.mozilla.org/en-US/firefox/addon/one-page-favo... [2] https://microsoftedge.microsoft.com/addons/detail/one-page-f... Edit: You might ask where the Chrome version. Well, I had a heck of time to create a new Google account for deployment. Stay tune. reply kjkjadksj 12 hours agoprevI love working with hackable software. I kind of attack it at the source level vs writing for the browser however. For example, say there’s some tool on a git repo. I will shamelessly clone it and build off of it to my own liking. Maybe I add another 1% to the code base, or maybe that repo becomes 1% of a codebase I write on my own. These are tools I could never share however, because of the rampant plagiarism I am doing, and the fact I don’t much care about getting it to run on different systems beyond my own. That being said fast and loose coding like this is a very powerful way to iterate on personal projects that never need to be anything but. I wish more things were actually hackable especially mobile or appliance hardware. Companies never like giving the power users the reigns for some reason. reply feldrim 13 hours agoprevBrowser extensions, if we use the analogy as apps running within browser as an OS, are lacking simple capacities to manage the risks. Just like any app a user can install on their devices, extensions extend the attack surface. As we cannot avoid the risk by removing all of them, we can just allow users to have more control on them regardless of the browser they use. I suggested[0] using standard management APIs provided by browsers, therefore the ecosystem can use them as building blocks for FOSS and/or commercial tools. That's a very naïve idea but why not? 0. https://zaferbalkan.com/2023/10/03/browser-extension-api.htm... reply gymbeaux 15 hours agoprevI've had some ideas for browser extensions over the years, most recently a few months ago. I remember looking at Mozilla docs for making a Firefox browser extension and, as a SWE w/10 YoE (mostly fullstack web), I was left confused. The documentation felt incomplete and I left the article with more questions than I had before. reply dannysuarezpab 6 hours agoprevI really like your article I agree with your point that extensions are tools for extend current software functionalities and see beyond the creators... Currently Im working on a Gmail and Outlook extension for email called Mailverse that add superpowers to the current email clients. reply cc101 11 hours agoprevIt's possible that some here might confuse Web Extensions with Safari App Extensions. Safari App Extensions are not the same as Web Extensions. App extensions are written in native code (Objective C or Swift); they operate within Apple's sandbox; their data is saved within Apple's secure file system; and if they are sold via the Apple App Store, they are reviewed and approved by Apple. One never has absolute assurance that an app is proof against attack, but until I learn otherwise, I think Safari App Extensions are safe. reply mcoliver 16 hours agoprevI run a browser automation extension that only does actions on certain sites (clipping coupons for grocery store sites and credit card offers rewards). I created it this way specifically because I am terrified of extensions that want to read and write all sites. And you should be too. I wish the chrome store gave badges to extensions like mine to make people more aware, give a filter when searching for new extensions, and to encourage least permissive development. The chrome store extension rules are also unevenly enforced. Take a look at the source code for something like 1password. It is full of obfuscation and completely unintelligible which is against the store rules. I base64 encoded a single string that was my json dict in an otherwise completely readable js file and it went through on one publish but a few versions later was red flagged. reply dividendpayee 14 hours agoprevThere was a good article from John Loeber a few months back about browser extensions: https://loeber.substack.com/p/9-15-years-of-market-gaps-for-... He had the same point, where it feels like browser extensions are a big, somehow under-appreciated market. Browsers are huge platforms -- creating add-ons and making them more capable should be a popular, value-generating thing to do! But for a number of (developer) UX/UI issues, that just hasn't been the case. I hope this changes! reply atum47 11 hours agoprevBack when Facebook was fun i paid 5 dollars to write a cross text extension. Back then i was doing a lot of those jokes where you get a popular saying, strike one word and write another one to make it funny. What was funny to me is the fact the Facebook started to revert my posts when using this. I remember recording a video about it, don't know if i still have it though. reply quicon 14 hours agoprev\"Computing is still young, and platforms are changing quickly. Modern browser extensions and smartphone platforms have only been around for about a decade. These platforms will evolve, and there will be new platforms after them, and we will get to collectively decide how open they will be.\" I really like this final comment. As a non expert in computing, I also often think about how young is this field, and I fantasize about how it will evolve, hopefully towards a more accessible and open ecosistem. reply lxgr 13 hours agoparent> we will get to collectively decide how open they will be. The author is way more optimistic than me here. I'd love if that were the case, but with the way the wind is blowing, I doubt that it'll be a collective decision between users and the big tech companies running today's computing platforms. If anything, it'll come through regulation. It's highly unlikely that e.g. iOS or Android will suddenly and out of their own initiative open up their APIs in a way that would allow building anything like \"reading mode\"/distraction removers, ad blockers, data extraction allowing mashups between different apps etc. Google's main customers aren't Android users, but app developers who run in-app ads and sell in-app purchases; the same is to a large extent also true for Apple (although DMA-like changes might shake up things a bit, and their reasoning for not introducing such apps will likely be security and platform integrity, not ads). reply mosselman 15 hours agoprevI wanted to build an internal company extension, but for that (chrome) you still need to go through the review process with Google and it is even worse than Apple’s App Store reviews. reply fritzo 14 hours agoparentWould it be too much friction to host internally and require your users to \"load unpacked\"? reply julienreszka 14 hours agorootparentIt's really not hard I doubt it's a big friction reply smudge-ai 8 hours agoprevWhile I fully agree with the hacker ethos of this post, a major issue I have with extensions today is that they're hard to trust. Chrome updates them automatically in most cases, which means a malicious update can easily slip by undetected. There are hordes of data companies looking to buy popular extensions or pay their authors to sneak spyware or other trackers in. The risk surface is massive, which is sad because I believe extensions are also one of the best modalities for extending what people can do online. reply iansinnott 8 hours agoparentEntirely agree, although as a developer the auto updating is definitely a feature. Since it lets you assume users are all on the same version. It is definitely a risk for users though. You can also \"opt out\" of automatic updates, but the process is a bit involved. 1. Locate the extension on disk 2. Copy it to some other location 3. Add it as a developer extension via the \"Load unpacked\" button in the extensions screen. I would also advocate for extensions being open source, but of course most of them are not. reply deepsun 8 hours agoparentprevSame thing with NPM/PIP dependencies (they can launch arbitrary code and clean up after, unlike Java deps from maven that just copy immutable archives). reply quickthrower2 13 hours agoprevI love browser extensions both as a user and as a hacker. The elephant in the room is browser extensions are not a web standard and Google or Firefox can make a breaking change to you at any time “for security”. Also Chrome can boot you out of the store or ask for 100 point ID check in the future. Extensions are great but a web standard for them would be even better. reply lapcat 12 hours agoparentThey're working on that: https://www.w3.org/community/webextensions/ reply account-5 17 hours agoprevI quite like bookmarklets, easy to write. Tried a userscript but couldn't get into it. Never tried an extension, wouldn't know where to start. reply lstamour 16 hours agoparentStart with ChatGPT or a sample extension. The unfortunate part of web browser extensions is that, like the treadmill of web frameworks and app development, browsers can’t seem to stop changing and tweaking how extensions work and remove perfectly good functionality. So you end up sometimes having to rewrite an extension or its manifest with very little assistance from browser makers. But at least you don’t need to learn XUL any longer, so not all changes are bad ;-) reply notzane 16 hours agorootparentI made this extension fully using chatGPT to diagnose some layout issues. It’s super simple but chatGPT was definitely useful setting up the chrome boilerplate (and commenting what each option meant). Make sure you ask it to target the most recent version, they recently changed (to v3?) and it seems chatGPT prefers writing for the old version. https://github.com/notzane/red-box-outline reply olejorgenb 15 hours agoparentprevHow do you \"compile\" the bookmarklets? I know of https://bookmarkl.ink/ but then we're back trusting some third-party service again. I get that it's not rocket science, but this is definitively a small hurdle to overcome. reply account-5 11 hours agorootparentI don't compile them. I just write the JavaScript and wrap it in an anonymous function then save the code as a bookmark. reply ustad 16 hours agoparentprevCheck out Firefox examples on github, you’ll like it, I’ve had great experience learning from them to add nifty features to my browser: https://github.com/mdn/webextensions-examples reply dang 12 hours agoprevDiscussed at the time: Browser extensions are underrated: the promise of hackable software - https://news.ycombinator.com/item?id=20556382 - July 2019 (186 comments) reply sidwyn 15 hours agoprev> Compatibility: Because extensions hook into websites in unsupported ways, updates to websites often result in extensions temporarily breaking, and extension authors scrambling to fix them. Has anyone who's built a browser extension solved this? reply mcoliver 15 hours agoparentThe best you can do is get an early warning by running your extension via an automation framework and getting alerts on errors then publishing a fix and waiting for approval from Google. Too many unknown unknowns. You're searching for an element to modify or take an action on based on the text content/class/id/aria-label/type? Someone changed apple to train. Or completely changes the element hierarchy. How would you predict or recognize that to modify your logic and be certain it works before publishing to your hundreds/thousands/millions of users? reply narag 15 hours agoprevQui prodest is the question you must ask when you hear the usual points against, mostly security. It's not that every person that dislike extensions or repeat the same arguments is paid by \"them\", but it's a little shocking seeing so many negative opinions in a forum called Hacker News. This comment: https://news.ycombinator.com/item?id=39251996 by Retr0id hits the nail in the head. It's not that we cannot modify the software, but there are so many layers of inconvenience... what about modifying and recompiling the browsers themselves? They're so big now. The solution would be extensions. But no. Security. reply zubairq 16 hours agoprevI think that metamask is an example of a great add on that proves how great browser extensions are. Also, I think that the most popular browser extensions like metamask will eventually become built into every browser reply latchkey 15 hours agoparentMM terrifies me as an extension. I run it in its own separate browser profile with no other extensions installed. My fear is actually that another extension can hijack MM. reply zubairq 13 hours agorootparentYeah, I have wondered about that. Can browser extensions read or hijack data from other extensions? or are browser extensions sandboxed? reply latchkey 13 hours agorootparentIt doesn't matter. Everything has security holes. reply prakhar897 14 hours agoprevTangential: What tooling do you use to develop Extensions. I used React and couldn't find something any testing libraries which works on background and content scripts. reply sn0n 11 hours agoprevMeanwhile beaker has become archived and \"lives on in\" bluesky and solid is vaporware afaict... Ouch. reply ggm 10 hours agoprevNot chrome on android. Super annoying. Using the chrome-like alternates isn't the same. reply everybodyknows 17 hours agoprevNeeds [2019]. reply Retr0id 17 hours agoparentDoes it? Has the browser extension landscape changed significantly since then? reply sp0rk 16 hours agorootparentIt's just a Hacker News convention to include the year in parentheses if the article isn't freshly published. It doesn't have anything to do with the content of the article itself. reply Retr0id 16 hours agorootparentSure, but it's generally only done when that added context is important. I think this article could easily have been written yesterday. reply lapcat 16 hours agorootparent> Sure, but it's generally only done when that added context is important. No, it's almost",
    "originSummary": [
      "Browser extensions are an underrated ecosystem that allows users to customize their web browsing experience and enhance application functionality.",
      "They provide deep control over computer usage by modifying websites in ways not intended by developers.",
      "Despite challenges such as privacy concerns and the need for improved accessibility and compatibility, browser extensions offer a glimpse into open and customizable software platforms."
    ],
    "commentSummary": [
      "This article delves into the importance, possibilities, and security hurdles of browser extensions, emphasizing the necessity for robust security measures and user control.",
      "It evaluates the impact of extensions on user experience, limitations in accessing backend data, and the potential of customization.",
      "Privacy concerns, ad-supported browsing, and the comparison between open-source platforms are also addressed, highlighting the urgent need for enhanced security measures and user awareness regarding extension permissions."
    ],
    "points": 506,
    "commentCount": 250,
    "retryCount": 0,
    "time": 1707061411
  },
  {
    "id": 39250406,
    "title": "Frustration with Apple's Developer Support and Google's Bug Fixes: Why Writing Code for the Web is Crucial",
    "originLink": "https://mrmr.io/apple/",
    "originBody": "Write code for the web This is a yarn of three threads, and it got a bit long. The tldr is Apple doesn’t care for me as a developer I should write code for the web Nothing is set in stone, really The story is personal, but I hope readers find something they can relate to in their own life stream (I guess that's the point of blogs) Apple doesn't care for you, Mr. Developer Apple cares for me as a customer, but it doesn’t care for me as a developer. This dynamic had been subconsciously griefing me for years, until I was able to formulate it consciously recently. The dependency goes Developer -> Apple and Apple -> Consumer, there is no reverse arrow from Apple to the developers. If all developers stopped building for Apple's platforms tomorrow, Apple will still survive, almost intact, since it doesn’t hurt their core value prop (I'll expand on this below). Apple has no dependency on individual developers. They care for and need to collaborate with corporate dev “partners”, but that's different. Since companies, especially ginormous multinationals, behave in purely game theoretic cost/benefit terms, and since Apple has no reason to care about developers: indeed it doesn’t. Just to restate the obvious - such a stance isn't necessarily the case. There are other multinationals who've figured that developers form a core part of their strategy. This realisation has made me happier since I now know my place. I can like their products without wanting to develop for them. Google has a bug. I have dynamic light/dark mode, and if I search on Google at night, the first page shows up in light mode. With the rest of my machine in a subdued state, the glaring white of the background hurts my eyes. In the morning, my laptop automatically switches back to light mode. Now when I search on Google, the results first show up in an unreadable black background. One would think that of all the leetcode certified staff, there must be someone there who would know an O(n) algorithm for fixing this bug. But no, this bug has persisted for years (I've counted), and it is unlikely to be fixed in the future unless it gets accidentally fixed as part of some overhaul. Apologies for the snide, I know some great people who work there. My point is that Google isn't fixing this bug not because it doesn’t know how to, but because it doesn’t care. This bug has zero impact on its bottom line. The people like me who use alternate search engines like DDG have already moved on years ago. The rest of the (overwhelming) majority is stuck with Google. No matter how bad their search is - UX or results - they have a captive audience. I think DDG etc also have a marketing blind spot - they keep pitching themselves as a more privacy friendly alternative, they never go after the main course. I don’t use DDG because of its privacy benefits, I use it because it reminds me of early Google - simple low clutter UX, good quality verbatim search results, and unobtrusive ads. Okay Manav, but I thought you were ranting about Apple, why bring in Google? Google is an example where I never grieved much because I understood the dynamics. I know that I, the customer, am not their game theoretic target. So when I have to invariably use their products and face another user hostile interaction, I try to shrug it off and move on. I know that Google has entered a rent seeking phase, and while it is sad that the world is giving all its video content up to it for even more of a hostage situation in the future, but that’s for governments to deal with. With Apple I didn’t understand the dynamics. 2009-ish. I struggled to find a computer for my mom. Windows (at that time, I don't know about now) was just too insecure. Linux required constant tech support. Eventually I prepared for her an OpenBSD machine running Firefox and a basic game (Bubbles I think). Obviously, this was not ideal. It fulfilled some goals - it worked without requiring any tech support when I wasn't around, and I was ensuring her data safety and privacy - and she was surprisingly happy with too, but I was not happy about how this was such a shrivelled parody of what things could be, and how it limited the ways she could use computers to enrich her life. Around this time, I joined a new job as a developer for a company that was making iOS apps. After a week or so of using the mac at work it hit me - this is the computer I wanted for my mom! That is Apple’s value prop. As soon as I had saved enough I bought her a MacBook. And heartfeltly thanked Steve Jobs for engendering it. The earth has done many a revolutions since then, and Jobs has left earth, and the form factor my mother uses has changed from a laptop to an iPad. But it still satisfies that core value prop. Let's consider a different context. Even if there were no apps in my phone, I would still buy an iPhone. For myself likely, but most certainly for her. The people running Apple know all this. In the deep dungeons of Menlo Park when there are meetings of the core council, after all the sacrificial lambs have been slain and the blood and gore washed away, out comes the elder spreadsheet that encodes Apple’s business model, but nowhere in them is any cell, input or output, which involves developers. Sure, Apple doesn't mind if developers are also happy. But it knows it doesn’t need to care if they are. This lack of caring is never expressed out loud. It isn’t some conspiracy, it just is one of those things - you wouldn’t walk up to someone who you don’t care about and tell them you don’t care. Unfortunately all this results in sometimes schizophrenic behaviour on Apple’s part, as there are many individuals working at Apple who do care about developers and are trying to make things better. 2016-ish. As part of my annual Apple simping I was watching WWDC when they announced Apple Music APIs. I was ecstatic. My coworkers were puzzled at my ecstasy, “All this, can’t we already do with Spotify’s API?”. I didn’t know how to answer that, so I just repeated how this changes everything. Of course, and as is usual, I was wrong. It didn’t change everything. In fact, it didn’t change anything. Apple’s own music player was, and still is, unusably bad. Even talking about it makes me angry. The people making it can’t be so incompetent, and it seems to be working for the rest of the world, so I've never known what to make of this situation. With the APIs out, I'd thought maybe things will change. But nothing happened. Apple’s own player continues to make my blood pressure high anytime I have to use it. And whatever alternative players I have tried just all seem to go for the same generic “Spotify” approach to music. I think it is because the people who're making these apps were never around in the Justin Frankel era of Winamp, and haven't seen how a music player can provide a fast, seamless, endless yet still personal approach to music. That era is not coming back because it relied on piracy, but luckily we don't need to anymore - that's the great thing about Apple's music catalog! We can recreate that experience without needing to sail the high seas. So that's the backstory. Now recently I had a bunch of free time, and thought that I’ll write a music player. Mostly for myself, but I also wanted to write a tutorial. This is what I wrote in the README in the first commit: Here I'll be writing down my notes as I build Flowers. The world needs not one music player, or two, but many: each of us has our own way of connecting with music, and so maybe these notes will help others build the flower they want, nay need. Cute, dumb, and in vain. As I went about it, I realized that 8 years down the line, not only is the API still buggy, it is also still not public! Firstly you need to pay Apple. No, not for bulk usage etc, but just to try out the API. So there there goes my dream of writing a walkthrough – nobody’s going to pay Apple 100 bucks just to try things out. And it also partially explains why there has been no innovation. But that's not even it - Even if you pay them, you get a restricted API. The point where I disgustedly gave up was when I found out that while I was jumping all these kafkaesque hoops, instead you could just go to Apple's own web music player, type MusicKit.getInstance().developerToken in your browser console, and you’ll get an unrestricted root token for free! All these anecdotes Manav, what does all this mean? Write code for the web Write code that runs on the web. We’re lucky to have a shared platform that no single entity owns. Even benevolence can be ruined by incompetence. The web platform is in a precarious place – overreaching governments, browser duopolies, a complex developer ecosystem – so it is not a given it’ll remain thriving. But so far it has survived. And every year longer it survives, the more the chances that it’ll continue to thrive in the future. Ironically Google is the good guy here, they’re doing great work for the web. On the other hand, literally every single workaround I’ve had to write in the recent past in web related code has been due to Safari's princessness. Nothing is set in stone Which brings me to the third thread of this story, how all this made me reevalute my relationship with companies. Someone I know, someone who has a better grip on living than me, told me once that it's not useful to put people into the buckets of good and bad. People are a mix. Bad folks can do good actions sometimes, and vice versa. I don't know to what extent I've been able to internalize their message, but that's for another day. What I realized is that the same applies to companies. Companies are like people. I don't know if they're sentient, but otherwise they share many attributes with us: they're intelligent (they were the AI before AI), have personalities, they are born, thrive, live and die, they're even legal persons. Just like we can't live without other people, we can't live without companies. They have many inhumane characteristics, but like them or not, such fractal conceptions of human organizations will always be around. By not permanently bucketing companies into good or bad, I can have a more fluid interaction with them - I can reduce my dependence on them when they try to put me in a zero sum game, or reengage more with them if they're willing to be more symbiotic. Nothing is set in stone, really. I think most large companies and medium-size companies, and even small companies, are starting to look at the web as the ultimate direct-to-customer distribution chain, bypassing all middlemen, going directly from the supplier to the consumer. – Steve Jobs, Make Something Wonderful Manav Rathi Feb 2024 Tags: Rant Related posts React, SwiftUI and the IO monad All posts Home",
    "commentLink": "https://news.ycombinator.com/item?id=39250406",
    "commentBody": "Write code for the web (mrmr.io)424 points by mnvrth 19 hours agohidepastfavorite340 comments ahmedfromtunis 18 hours agoEarly on, I made the choice to not to invest in learning native mobile development. I focused all my limited time on the web, and I think that, for once, I made the right choice. Today, one can build amazing stuff for the browser. And, in my very humble opinion, the vast majority of apps should've been WebApps — except for maybe Uber, Google Drive, and games. I worked in the journalism business, and, in my country, the early 10s were filled with media outlets spending so much of the little money they had to build mobile apps. I was the sore thumb that rejected such trend. I knew very well that the quality of most of these apps would be questionable at best. And of course these companies won't be investing regularly in updating their mobile fronts. And that's exactly what happened: they're stuck with apps that are barely maintained, most of which looking like ancient artifacts from a bygone era. Because that's what they are. reply idle_zealot 17 hours agoparent> except for maybe Uber, Google Drive, and games I understand Drive working better as a native app; it needs to be able to provide a virtual filesystem to the OS and do background syncing and such. But why Uber? Uber has (or maybe had) a mobile site that works fine for requesting rides and basically everything the app can do. I don't see what value Uber being a native app adds for the user. The same goes for most mobile games; they tend to have simple graphics that the browser is perfectly capable of rendering with great performance, and they tend to reinvent all common UI components so losing access to native system buttons and such wouldn't make a difference. PWAs should also provide plenty of storage for local save files and other game data. The exception of course being games that really push the system to its limits. I don't expect, say, Death Stranding or the RE4 remake to work well without direct access to native graphics acceleration, and they're too large to load as a single webpage. I will reiterate though that this is not the case for most mobile apps, even extremely profitable F2P ones that would benefit greatly from an extra 30% revenue. Why then do they not target the web? My gut feel is that it's because mobile users are trained to look in the app store for apps and games, not the web, while desktop users expect apps to be available in the browser with the exception of some professional tools and high-end games. That is to say, it's largely a cultural problem. Of course, Apple's lackluster support or PWAs doesn't help. reply kmeisthax 14 hours agorootparentUber has the same problem that Death Stranding and Resident Evil have: you can't keep shittons of data cached on-device unless you have it installed as an app and not a webpage. Uber specifically needs to support someone traveling from Sao Paulo, Brazil to Mumbai, India without needing to have more than a whisper of connectivity at their destination. The way you do that is by keeping the UI for every payment provider in the world in the app. F2P mobile games need to be able to send you notification spam, because they're based around habituation and addiction. You couldn't do that in MobileSafari until very recently[0]. [0] Supposedly. I pinned Google Fi to my homescreen on my iPad, it asked for notification permissions, I see a notification count badge on it's app icon, but it still doesn't actually notify me when I get a text message or call. reply ncann 14 hours agorootparentI haven't run into no-network situation with Uber ever since every airport has wifi nowadays, but in that situation with no network, how does local on-device Uber data help to book a ride? reply p_l 13 hours agorootparentImagine spotty, dropping connection, low bandwidth. You do not want to load big chunk of JS because it was never cached, or as Uber used to do, download all the possible new promos every time you opened the app... You want to squeeze through the minimal data you need to arrange your core business function. reply kmeisthax 14 hours agorootparentprevIt's not \"no network\", it's \"minimal network\". i.e. bad wifi or garbage 3G. Or you're data roaming, in which case you'd get charged out the nose if the app decided to redownload stuff. reply rchaud 16 hours agorootparentprevUber had an excellent PWA, m.uber.com. I used it for years, but it didn't seem to work right on a new phone, with some UI elements not visible or off-screen, so I went back to the app. reply epistasis 15 hours agorootparentprevIt's interesting because Apple had to be pressured into even allowing native mobile apps. When the iPhone was launched they told everyone to develop web apps. reply raydev 14 hours agorootparentThere was a lot of internal pressure as well, smooth scrolling was such a key product feature that they eventually admitted it was impossible to do in a browser if there was any complexity to the app at all. reply p_l 14 hours agorootparentprevUber's native app and its focus on acting partially like webapp to dynamically download new content based on locality is one of the worst aspects using Uber, and is even worse with PWA. All it takes is for network connectivity to be a little bit spotty (admittedly, Uber made it work a little bit better these days). In the past, I've been more than once frustrated to rage from the fact that Uber couldn't just grab my A-GPS position, ping for address and send a short packet to order a car, but it had to download potentially megabytes of promo html before it let me actually do anything useful. Avoiding it with PWA is afaik even harder (currently looking exactly into making PWA because I don't want to deal with iOS). reply mplewis 17 hours agorootparentprevUber provides features like push notifications and Dynamic Island support to users. Web Push only became available recently, and it’s only available if you install the web app to the Home Screen. reply depressedpanda 15 hours agorootparentprev> Of course, Apple's lackluster support or PWAs doesn't help. Holding the web back is entirely intentional and makes strategic sense from Apple's point of view. They're well aware that a majority of apps would work perfectly fine as web apps -- but that also removes Apple from the equation. Only allowing Safari on iOS and intentionally gimping it solves the problem. I mean, I can't even blame them. Can't lose out on that sweet 30% cut, y'know? I'm only disappointed that legislation took this long to catch up to their shenanigans, and only in the EU. reply threeseed 14 hours agorootparentI wish PWA advocates were more intellectually honest. They don't care about the web and what is good for end users i.e. more APIs = less privacy. They simply want to build apps without Apple's fee and controls. reply depressedpanda 12 hours agorootparentWell, I wish Apple advocates would stop strawmanning and posting incorrect generalizing accusations. I care deeply about privacy. That means I trust the browser sandbox a lot more than native apps. There's obviously a reason sites like Twitter and Reddit try to force their native apps on you as soon as you visit their web page on a phone -- it'll allow them to spy more on you. reply threeseed 11 hours agorootparent> That means I trust the browser sandbox a lot more than native apps You shouldn't. The most privacy invading behaviour by far is the cross-site tracking that happens on the web. Where large data companies as well as Facebook/Google are building behavioural profiles with thousands of features and data is being packaged and sold to entities you have no knowledge of. And all of which is being facilitated by browser fingerprinting courtesy of Google recklessly adopting APIs with little care about privacy. reply depressedpanda 10 hours agorootparentYou're not talking about a sandbox then? A native app won't prevent Google or Facebook (or Apple) from tracking you. In my experience, native apps typically always require login, and then fingerprinting is unnecessary. Avoiding their services, and using an open source and privacy respecting browser with uBlock Origin installed will protect you however. The latter can be done on all OSes I've used -- except iOS. reply no_wizard 15 hours agorootparentprevHonestly if their new revenue share scheme stands up to the court I could see them taking 27% from PWAs as a possible future. It would be the only thing that spurs them to make it more robust. I don’t agree with it at all to be perfectly clear. I’m only postulating that realistically the only way Apple takes PWAs to first class status is if they get a cut. Its rather unfortunate reply depressedpanda 12 hours agorootparentYou're speaking like Apple will get away with their malicious compliance and petulant behavior. I'm not so sure they will, now that the cat is (finally!) out of the bag. reply ajross 16 hours agorootparentprev> I understand Drive working better as a native app; it needs to be able to provide a virtual filesystem to the OS and do background syncing and such. That's a circular dependency though. You want that stuff because you want to use Drive with native apps and not web apps. But Drive is a first class backend (actually even more featureful, frankly) to web apps too. Basically the native Drive client is for native/legacy interop, not a barrier to a PWA future. reply pmarreck 17 hours agorootparentprevUber could have been coded to a PWA just fine. Parent comment is simply unknowledgeable. It's funny because the very first thing that Steve Jobs pushed when the first iPhone came out was PWA's, essentially, and not an App Store, which didn't exist yet... and the same API's are still available in Webkit I believe reply rubicon33 16 hours agoparentprevI just wish Web wasn't such a frustrating development experience. While it has improved leaps and bounds in the last decade, I still find it to be suboptimal when compared to native offerings. Most of that frustration comes down to tooling and more specifically TypeScript. I'm not sure how to succinctly express my pain here but I have found generally that I wrestle with the type system in TypeScript far more than native offerings. reply CharlieDigital 16 hours agorootparent> I just wish Web wasn't such a frustrating development experience It's got to be better than building to target two totally different platforms (and even more once you factor in tablets, Windows desktop/Linux/macOS) and then jumping through hoops to get your app packaged, reviewed, and updated consistently onto the target devices. > Most of that frustration comes down to tooling and more specifically TypeScript I think that a lot of folks get frustrated with TS because they are coming at it from the wrong perspective. Instead of thinking about it like a static type system, think about it like \"JavaScript with shape definitions\". Give this a shot: https://chrlschn.dev/blog/2023/09/typescript-is-not-a-progra... reply threeseed 14 hours agorootparentThere are plenty of options e.g. React Native, Flutter if you want one codebase. And if you want to have a lowest common denominator user experience that's fine but you shouldn't expect users to reward you for it. That's why successful apps tailor the experience for each platform. reply CharlieDigital 14 hours agorootparentNeither solution takes away the fact that you have to jump through hoops for distribution. Web apps and PWA's built with HTML and JavaScript are likely to work as-is for a decade or more given we are likely to see minimal incompatibilities with existing HTML, JavaScript, and CSS in that time and likely to see more net new capabilities. > if you want to have a lowest common denominator user experience... One is a native app with a budget in the millions built by dozens of engineers. One is a web app built by...me: https://youtube.com/shorts/bxs9ZpIJ6RM Not sure that there's that much sacrifice when it comes to modern web apps. reply ahmedfromtunis 16 hours agorootparentprevI enjoy building for the web using \"native\" tooling, writing CSS and JavaScript \"by hand.\" Years ago I tried to up my game and learn to use grunt and babble and other build tools. However, I found the process extremely complex, only to end up with finicky systems that made me even more stressful, not less. I decided to ditch all that PoS and do things the easy (and logical) way. Never looked back. reply noduerme 15 hours agorootparentprevI came from the much cleaner, simpler type system of AS3, but I don't really see a problem with the complexity of TS. There are a lot of aspects of its type system I will probably never use... but I don't have to. I've written a lot of fairly complex business and game software for the web, and never found myself reaching for a conditional type. Meanwhile, keyof and union types are handy for autocomplete hints. And to me that's all TS is really about. It's just there to make large scale javascript less of a dynamic, chaotic, untyped mess. reply continuational 16 hours agorootparentprevThe TypeScript type system is obscenely complicated because it has to be able to type almost any weird pattern people used when there was no type system. Most dynamically typed languages are adding type systems right now, and they all face the choice between rewriting/wrapping the whole ecosystem or build a very complex and ultimately unergonomic type system. reply jwells89 16 hours agorootparentIt’s too bad that Swift → JS compiler projects are all stalled. Swift syntax is similar and I think it’d be a nice step up for people who don’t care for compatibility with legacy JS and want uncompromised static typing on the web. reply sodapopcan 15 hours agorootparentprevKeep your business logic on the server and use whatever language you want. I stick to vanilla JS and only use it for DOM manipulations. I try and leverage CSS for simple manipulations when I can. I don't go out of my way to, but it can do a lot these days. reply ciconia 12 hours agorootparentprev> I just wish Web wasn't such a frustrating development experience. I have to say, having recently gone back to doing some web dev work (doing some dashboards with lots of D3 visualisations), I was pleasantly surprised to see how better things have gotten. working with CSS is better (with stuff like grid, flex, nested selectors), and JS is also much much better (import maps, fetch API). I'd say for developing cross-platform apps it's a pretty good deal. reply jitl 13 hours agorootparentprevI stopped wanting to write SwiftUI because the compiler there is the worst I’ve ever used. It uses some kinds of complicated inference for ViewBuilders, and the inference can get confused if you have a type error and deadlock checking a View. It’ll put an error squiggle on an entire function and say “sorry we timed out trying to typecheck this, can you figure it out?” I’ve never used any typed language compiler that gives up on 100 line or less functions. reply mardifoufs 8 hours agorootparentprevDeveloping in native GUI frameworks is just as frustrating. Using QT for complex GUIs is more frustrating than HTML5/react or whatever reply FredPret 16 hours agorootparentprevThat's because you're doing something much harder with web dev. You're making something that works almost everywhere. It's actually insane this is even possible. I bet one day, computronium will run Javascript. reply yCombLinks 15 hours agorootparentNah, the shitty parts of web dev are in basic building blocks other ecosystems handle without a problem. The fact that javascript can run anywhere doesn't mean it needs to have crappy build systems. It doesn't mean it needs to have constant breaking changes and flaky dependencies. CSS is crap for layouts. I've build apps with at least 10 layout systems, and CSS is the worst of all of them. reply FredPret 15 hours agorootparentCorrect, but HTML + JS + CSS is the standard we seem to have settled on, and having a globally accepted standard for anything is a unique and historic achievement, let alone for free, fast, and robust global distribution of Turing-complete computation and content delivery. Now we can hope that those building blocks improve over time, but the current situation is so much better than it could have been. reply throwitaway1123 11 hours agorootparent> having a globally accepted standard for anything is a unique and historic achievement, let alone for free, fast, and robust global distribution of Turing-complete computation and content delivery. It really is amazing. > Now we can hope that those building blocks improve over time, but the current situation is so much better than it could have been. I think the web is under-appreciated in part because most people have never used internet connected GUI computers without it, but it's important to remember that there's no a priori reason for why the web had to exist. In an alternate timeline we could be living in a world where all interactive software has to be reviewed by a gatekeeper. I wasted an inordinate amount of time playing with this a few days ago: https://news.ycombinator.com/item?id=39205020 and I remember thinking, it's pretty cool that you don't need anyone's approval to release something like this, nor do you need to pay excessive fees to distribute it, nor does it take any meaningful time to download/install, and yet it works on all my devices, and it's relatively secure. reply mnvrth 14 hours agorootparentprevThis is beautifully put! I hope more people are able to understand the point you're making, especially how \"having a globally accepted standard for anything is a unique and historic achievement\". reply FredPret 14 hours agorootparentThank you! I guess I can make myself more clear - the good thing about CSS is not the way it does layouts, but that it can do layouts on almost any device. You can come up with a better system that runs on your own machine (1 unit of effort), but then you have to convince everyone and their dog to adopt your new standard (1 billion units of effort). reply host0 18 hours agoparentprevPWAs, especially those built with an offline-first approach, are so much closer to the native experience than those from half a decade ago. However what concerns me is Google's and Apple's incentives to help them get better to a point where they can rival native apps which could hit their bottom line. reply chilmers 17 hours agorootparentThe nice thing about the web is that while it’s no company’s favoured platform, it’s valuable to all of them as a way into each other’s platforms. That’s why whoever is “winning” the platform war a particular point tends to neglect the web. See Microsoft in the 2000s, and Apple now. But all their competitors are incentivised to push the web forward, because it’s a useful fallback as a way to get their apps into the walled garden. Then, eventually things shift and the company that was dominating falls behind a bit, or is facing antitrust problems, and suddenly they care about the web again. In that way, the web is a perpetual underdog, but never goes away. reply mnvrth 17 hours agorootparentWow, that's a great way of putting it! Thanks, I had never linked the dots that way before. reply treyd 15 hours agorootparentprev> The nice thing about the web is that while it’s no company’s favoured platform It is effectively Google's favored platform. They effectively get to be the gatekeepers for what standards become accepted. They control the most widely used engine, and fund the only opposition so they have an incentive to avoid going against them. Just look at how everyone dropped the ball in the recent JPEG-XL vs WebP news. reply behnamoh 15 hours agorootparentprev> See Microsoft in the 2000s, and Apple now. This. Apple's Safari (on iOS and macOS) is the Microsoft IE of 2020s. It's unbelievably behind the more sophisticated browsers (Firefox, Chrome) and yet, Apple couldn't care less. reply troupo 15 hours agorootparentIt's not IE by any imaginable standard. And it's only \"unbelievably behind\" if you count all the non-standard APIs that Chrome ships by default and calls them standard. As for why Safari isn't the new IE, I recommend Breaking the Web Forward https://www.quirksmode.org/blog/archives/2021/08/breaking_th... reply behnamoh 15 hours agorootparentA browser that doesn't support extensions unless they're installed as separate apps on the platform is not worthy of the name \"browser\"; it's a lock-in mechanism. reply ahmedfromtunis 17 hours agorootparentprev> what concerns me is Google's and Apple's incentives to help them get better I think that despite that, they did a decently good job. A few years ago, I created a voice-only social app: people can easily record and send voice memos. I built it using fairly new tech at the time: the web audio api and native web components. (I refuse to touch react-style frameworks with a 20-foot pole.) It was 99% compatible with all major web browsers from the get-go, even with its fairly advanced \"tech.\" The only thing that wasn't cross-platform was the audio files themselves. I had to make a deep-dive into the world of encodings and codecs so that I can offer users audio files in the formats that work on their machines. Obviously, there are areas where cross-platform-ity (??) is abysmal, but things will probably continue to evolve for the best ― hopefully. And yes, PWAs with their offline capabilities are just awesome! reply ederamen 17 hours agorootparentprevI'm hoping the latest EU regulation enforcing alternate browser engines will make a dent in this. ...but when I see how petulantly defiant Apple's responses to other regulations which threaten their lock-in strategies have been, I'm not optimistic. reply maxlamb 17 hours agorootparentprevI think you meant Google's and Apple's incentives to make them (PWAs) worse... reply px43 17 hours agorootparentMy understanding is that there is a lot of political power inside Google (from the Chrome side of things) working to ensure that PWAs are the primary way that users interact with their phones. It's definitely a point of contention, with pressure on both sides, but there is a balance there. Apple on the other hand is only incentivized to funnel everyone through their App Store, and for some ridiculous reason, app developers use this to justify going all in on native apps, which I think is what this article is addressing. reply dickersnoodle 17 hours agorootparentOne really good thing about native apps versus PWAs is that once the user installs a native app and learns the way it works they don't need to learn it again until they upgrade. With anything sourced from the web, the user is at the mercy of the wevdevs, who are at the mercy of the marketing and merchandising wizards who want to shake up the UI to try and goad users into buying more stuff. It's kind of like those round baskets that supermarkets like to clutter the aisles with; somehow they think it prompts you to buy things, but for me it adds one more product to my \"no, not buying this ever again\" list. reply nox101 16 hours agorootparentnot my experience. Apps update automatically for most people so they are no different than web pages. You can turn off the updating and then in 2-3 months you'll get a message the app no longer works with whatever service it's tied to and you're required to upgrade. On my desktop there might be a few apps that don't need to upgrade but on my phone, 90% of all apps are tied to a service. Uber/Lift, bank apps, apartment apps, hotel apps, bike rental apps, social media apps, dating apps, streaming music apps, video apps, etc. reply pmontra 17 hours agorootparentprevYes and no. Yes because for example I'm sticking to K9 mail 5.6 because of the UI change in the next versions. I picked K9 exactly because of the original UI so I'm not using the new one, which is like every other mail client. I saved the APK and I'm installing on any device of mine, also when my fingers slip on the update button. Uninstall and install 5.6 from the file manager. And no, because if the company behind the app wants to force people to upgrade they'll make the backend incompatible with the old app. Also no, because I don't think that many normal people invest time not upgrading an app because of the UI. reply aCoreyJ 17 hours agorootparentprevGoogle is the one pushing them. Apple has the incentive to keep people on the device though because they don't have an advertising business https://fugu-tracker.web.app/ reply nerdix 14 hours agorootparentRight. Google is incentivized to push PWAs and other forms of cross-platform mobile app development (hence Flutter). How often will the hot new app from some SV startup launch for iOS first with no Android app? Resource strapped companies that have to prioritize one platform will tend to pick iOS and that hurts Android (but is obviously good for Apple) reply rchaud 16 hours agorootparentprevApple has a giant advertising business, but yes, it's not on the web like Google's is. It's on their own app properties like Apple News and the App Store. reply threeseed 14 hours agorootparentApple does not have a giant advertising business. And it's so minor compared to what Google does it's ridiculous to even compare the two. reply jauntywundrkind 16 hours agorootparentprev> PWAs . . . are so much closer to the native experience than those from half a decade ago. On the one hand I love this. I'm glad PWAs are helping drive people who wants and expect a certain thing towards the web. I am also however super sad that PWAs removed so much greatness of the web. Most don't launch in the browser. None have visible url bars. They just added a new mode so PWAs can have some kind of tabs. They don't integrate with bookmarks, or extensions. They don't have forward/back. I like the web a lot, and I feel like PWAs are a major step backwards for me. What's really scary to me js that new permissions are being added which are PWA only. I no longer can have a web like experience; I am forced to have an app like experience, without any of the affordances I get with a user agent/browser at my back. I feel so undermined here. reply Sateeshm 5 hours agorootparentYou can use extensions with PWAs,at least on desktop reply echelon 17 hours agorootparentprevGoogle and Apple should be regulated to offer the same functionality and performance to the web. Their grip on the most important technology of the century is astounding and harmful. Honestly first-class web mobile installs without hoops and scare walls would be the thing to strive for. reply ben_w 17 hours agorootparentGoogle and Apple, both of whom make web browsers as well as OSes and could therefore cheat loads with anything like that. Of all the places to find someone conflating the companies with the operating systems Android and iOS. But even then… > same functionality > Honestly first-class web mobile installs without hoops and scare walls would be the thing to strive for. Coming to a future near you: npm install rm_minus_rf_slash What's the old quote? \"\"\" Tech Enthusiasts: Everything in my house is wired to the Internet of Things! I control it all from my smartphone! My smart-house is bluetooth enabled and I can give it voice commands via alexa! I love the future! Programmers / Engineers: The most recent piece of technology I own is a printer from 2004 and I keep a loaded gun ready to shoot it if it makes a strange noise. \"\"\" Except all the banks and railways require me to have a smartphone these days whether I want it or not. reply echelon 17 hours agorootparentOne word: sandboxing. reply ben_w 15 hours agorootparentIs one of the ways they make browsers and app store apps more secure, that that is dissimilar to old school installation. reply idle_zealot 17 hours agorootparentprevI'm not aware of any scare walls on iOS. On Android is a site is installable a little banner is supposed to pop up and ask if you want to install it. On iOS one has to open the site's share sheet and pick an install option nestled in there somewhere. I suppose that counts as a hoop, but how else would you surface the functionality without it being invasive and annoying for most users? I think a more pertinent problem is that most devs don't want to distribute their apps as PWAs because Apple deletes saved data at unpredictable intervals and PWAs can't push engagement notifications as effectively as native apps. When I switched from the Uber PWA to their native app I suddenly started getting bombarded with ads from them in my notifications. reply Aerbil313 15 hours agorootparent> devs don't want to distribute their apps as PWAs because Apple deletes saved data at unpredictable intervals Does anybody know what the current situation in iOS looks like? I researched this recently and the information is conflicting. The OPFS data (Origin Private File System, which is the new storage API for PWAs and is supported in Safari) is supposed to not be deleted without explicit user confirmation, but what this confirmation looks like is unknown (is it just the usual delete Safari history button?) and I didn't run any experiments. However I've seen someone claiming Apple separates a website and it's installed PWA's storages. This is reinforced by the fact that I can see the storage space each PWA takes in Settings (which was oddly not updated when a PWA using OPFS used some more storage). However I just deleted that PWA but there were no special prompts, just the usual native app deletion prompt, absent of the usual phrase \"Deleting this app will delete its data.\", only the phrase \"Do you want to delete this bookmark.\" was present. reply echelon 16 hours agorootparentprevThose are downloads though the app stores. The links merely funnel you though the stores. Native web downloads of apps are impossible on iPhone/iOS. You have to go through the app store. Native downloads of apks are possible on Android, but not until you navigate to the hidden settings and enable them. And even then, Google scares users from installing apps this way. Nobody in practice does this. It's effectively not permitted. reply troupo 15 hours agorootparentprev> Google and Apple should be regulated to offer the same functionality and performance to the web And who is going to determine which exact functionality they both must implement? reply mgaunard 15 hours agoparentprevWhy invest time fad-of-the-month technology at all? Invest your time into foundational pieces of the technology stack instead. reply throw156754228 17 hours agoparentprevSo tired of junk apps cluttering up my phone and tablet. Examples: There's a parking station near me that has an app, the library photocopier has an app. Could all be done on the web. reply aronhegedus 17 hours agorootparentI also don't understand why a parking meter needs an app, it should have a QR code>web>stripe payment (or a text to pay), rather than 2 buggy apps on iPhone and Android, which require data to download reply bonyt 17 hours agorootparentI’ve heard stories of people putting fake QR codes linking to fake payment portals for parking. I don’t think an app solves this problem though. https://www.schneier.com/blog/archives/2022/01/fake-qr-codes... reply threeseed 14 hours agorootparentprevThere are many reasons to have a parking app. You can extend your parking time, find closest parking meters, download tax invoices for businesses etc. All of these are much faster to do on a native app than a clunky mobile web site. reply mistrial9 17 hours agorootparentprevit is \"surveillance capitalism\" -- they want names and contact info for \"their customers\" .. this is a serious sickness in the modern times IMHO reply dtagames 17 hours agoprevThis author seems confused about something that he, himself explains. The reason that Apple doesn't care about developers is that they have created such a walled garden cult w/ users that developers are forced to produce products for their platform or they lose half their market -- or more. At my day job in mobile game development (for a small studio inside a large company), we have to fight Apple constantly -- not only on tech issues but on policies and approvals, too. But offering a mobile game without a way to run it on iOS would be unthinkable, so we comply. In many ways, Microsoft's original strategy for the PC was the exact opposite. It catered to developers and provided copious documentation, examples, and tooling for devs. The companies behind those devs were, themselves, motivated to make software for MS and, in turn, to promote and sell it. The tidal wave of individual developers making Windows software (think back to the shelves at computer stores) is what created the desktop OS that still rules the roost. reply mvdtnz 16 hours agoparentI have been writing software professionally for 15 years and I have never written a single line of code for an Apple device, and I never would. Contributing to the Apple ecosystem is a choice, no one is forced to do it. Any developer who contributes to the Apple ecosystem is taking an active part in the situation. reply Rohansi 16 hours agorootparentUnfortunately for mobile games iOS tends to be where the majority of the money is. reply lcnPylGDnU4H9OF 15 hours agorootparentWhere are all the good iOS games? I don’t know if I’d be able to name more than a handful[0]. How many people are making -- and similarly monetizing: upfront cost sans micro-transactions -- things like Mindustry? 0: Mindustry https://apps.apple.com/us/app/mindustry/id1385258906 and two more which are available with an Apple Arcade subscription, maybe also for individual purchase: Dandara https://apps.apple.com/us/app/dandara-trials-of-fear/id15761... and Fantasian https://apps.apple.com/us/app/fantasian/id1517339045. reply hylaride 14 hours agorootparentOver the past few years, the highest grossing games are freemium pay to win games, like Star Trek Fleet Comman. While they're available on both android and iOS, the fact that iOS users tend to me more affluent means that the revenue tends to come from in-app purchases on iOS more than android. reply criddell 16 hours agorootparentprevI don’t think it’s just games. iOS users in general are more willing to pay for software than Android users and Android users are more willing to pay than web users. reply raydev 14 hours agorootparentThis was my experience doing consulting/contracting for most of the 2010s. Clients would want both iOS and Android, and we’d release with full feature parity. In every instance the iOS apps brought in 2x-5x the revenue, even when our Android apps had more active users. reply criddell 12 hours agorootparentI’ve always wondered why that is. I’ve had both Android and iOS phones and I’ve spent far, far more on iOS apps. I don’t think I’ve ever paid for a web app. There’s something about the platforms themselves then encourage or discourage spending. reply lm411 13 hours agorootparentprevNot just games, iOS App Store revenue is far higher in pretty much all categories vs Android. reply Cheer2171 15 hours agorootparentprev> Contributing to the Apple ecosystem is a choice, no one is forced to do it. This is such a disingenuous argument that ignores the huge pile of incentives at play. Here is an equally disingenuous argument: anyone who sells or gives away any software to anyone in the US is taking an active part in the US military industrial complex. reply MSFT_Edging 15 hours agorootparent> anyone in the US is taking an active part in the US military industrial complex. Its annoying how this isn't that far off. Even big consumer facing companies will end up working for or supplying intelligence to the MIC. reply ahmedfromtunis 17 hours agoparentprevI think this is because Apple is a hardware company, that uses software to attract consumers. While Microsoft is a software company, that uses hardware to attract consumers. reply oneplane 17 hours agorootparentA better way to describe their differences is that Apple started as an end-user company (and became a services company while at it) where hardware was a means to an end. The products they sell are based around an experience rather than just a bag of components/parts, as those are just the means to an end. It's why their design envelope is the way it is, and why the kept vertically integrating more and more over the decades. In a way, that's where the IWM and SWIM came in so long ago, not because they had to, but because it delivered a better solution to that specific experience than anyone else. (granted, back then using a computer had a very different meaning than it does today but the reasoning still applies) Microsoft on the other hand is a typical SKU-mover, their business relies on selling 'parts' of someone else's workflow, but later on realised the only way to have a consistent experience is to also start making the hardware. Later on, they tacked on services, but because they kept the \"spray the market with SKUs\" business strategy it mostly just became 'more', rather than better integrated products for people. It's still \"move as much stuff\", even if it's not well-integrated. Heck, sometimes it is less-integrated because it enables moving more stuff. At the end of the day, most large tech multinationals end up manufacturing a lot of stuff over the various layers (hardware, software, firmware, entire devices, individual components etc), but that no longer describes what the company actually \"is\". (just like it doesn't really describe much to group them together as \"commercial money makers\", it isn't very specific or useful) reply jsjohnst 16 hours agorootparent> In a way, that's where the IWM and SWIM came in so long ago I’m going to assume based on minimal context you are meaning the 40 year old floppy controllers? Since these acronyms are fairly obtuse / not well known… IWM - integrated woz machine https://en.m.wikipedia.org/wiki/Integrated_Woz_Machine SWIM - replacement for IWM which supported the SuperDrive reply mlhpdx 16 hours agorootparentprev> Microsoft on the other hand is a typical SKU-mover As far as I can tell there is nothing typical about Microsoft. The sophisticated, deeply-root long game they’ve executed extremely well puts them in a very different place. As a contrast to Apple, the differences are striking - not because of the difference between “bing” and “bling”. reply oneplane 15 hours agorootparentTheir \"throw as many SKUs at the market and see what sticks\" model is very typical. That is not uniquely Microsoft and is very common in broad supply enterprises. The only thing that is unique about microsoft is being in the right place at the right time, making the right B2B licensing deals and entrenching for decades because of it. Operating systems, window managers, filesystems, graphics, SDKs, IDEs, it's all been done before (and after) at equal, better and worse quality levels. Their broad catalog is somewhat unique in having more SKUs that don't work well together (or at all) vs. ones that do, but the same applies to IBM and Google for example. And the things that do work well together tend to be based on technologies that have little to do with the company itself (like WebDAV and Kerberos), and that's not unique to Microsoft either. The sophistication is in their business and SKU broadness, not in making proper end-to-end vertically integrated user experiences that make people happy. (well, they have started to get there in the gaming market I suppose) If you have an example where that is not the case, and there was a product choice that merited their offering, I would love to hear about it. reply wslh 16 hours agorootparentprevThis. The Microsoft long term business execution is amazing despite their resources involved in zillions of failed projects and acquisitions, and some horrible apps (e.g. Teams). It doesn't matter they are a success anyway. Other companies would be buried much before that. Always recommend the book Hard Drive (just in a yesterday's thread) [1]. I need to not forget to add now \"Idea Man\" [2] by Paul Allen, Microsoft co-founder. It shows that the young Bill Gates was already a Titan. [1] https://news.ycombinator.com/item?id=39243044 [2] https://www.amazon.com/Idea-Man-Memoir-Cofounder-Microsoft/d... reply coffeebeqn 17 hours agorootparentprevApple is a vertically integrated company. They offer you (in theory) everything you need for computing reply steve1977 1 hour agorootparentIf you are a consumer or for example a creative pro. If you are a big enterprise, not so much. reply dtagames 8 hours agorootparentprevExcept cloud services for business, which are increasingly vital. And without AI, also a big deal now. reply clarkmoreno 17 hours agorootparentprevRe-read that. Microsoft uses hardware to attract customers? reply ysavir 17 hours agorootparentI don't know the GP's intention, but I'm guessing it's that MS's actual products are all software (Windows, Office, etc) and the allure is that they can be sold on/come packaged with relatively cheap and available computing. Compare to Apple, where they primarily sell hardware, and use software (whether their own or 3rd party) to make that hardware desirable, especially at high price points. reply rplst8 16 hours agorootparentprevYeah I don't agree with that assertion either. Microsoft will produce hardware to show the art of the possible with their software. It largely comes out of a situation where the PC/device manufacturers were doing a sh* job of it. reply dragonwriter 16 hours agorootparentHardware manufacturers that aren't Microsoft have little reason to take speculative risks that might prove that there's a market for a new configuration of commodity hardware to sell Microsoft’s latest non-commodity software; it's all potential downside with very little potential upside. reply yoav 16 hours agorootparentprevEveryone universally loved the zoon and the windows phone. reply generalpf 14 hours agorootparentYou dropped this: /s reply medstrom 17 hours agorootparentprevThat's an old saying. It's not so clear-cut now. reply asimpletune 16 hours agoparentprevApple doesn't make developers do anything. Developers write apps for their platform because it is the one where consumers want to spend their money. They do this by being mean to developers, or, rather, not allowing developers to be mean to customers. It's the same way that apple leans hard on their suppliers but at the end of the day it has led to creating a healthy and wealthy ecosystem that those developers and suppliers still seem to contribute apps to. reply mouzogu 15 hours agorootparent> not allowing developers to be mean to customers. apple has a history of denying apps for monopolistic reasons & collusion price fixing : https://apple.fandom.com/wiki/Criticism_of_Apple#Restriction... > t's the same way that apple leans hard on their suppliers but at the end of the day it has led to creating a healthy and wealthy ecosystem yes, like using child labour and working environments that require suicide nets: https://apple.fandom.com/wiki/Criticism_of_Apple#Student_and... reply viktorcode 14 hours agorootparentNot protecting Foxconn, but I suggest you to compare their work conditions with other Chinese companies, where the competing mobile devices are manufactured. Problem is China and its borderline slavery labor conditions. Thankfully, Apple started moving out to India and Vietnam. reply ysavir 17 hours agoparentprevThis feels like a different situation than the article author. My impression is that they were concentrating on services where mobile is just one point of access, but make sense in any scenario. Stuff like social networks, dating apps, reddit, stackoverflow, etc. Services that don't depend on the mobile experience whatsoever, unlike, say, Uber, that requires location tracking, or mobile games, which are often designed to be played on the go. If a business doesn't depend on the mobile experience, then they are still able to offer their services without offering a native app. People on mobile can still access the service through a mobile browser--perhaps not the ideal experience on mobile, but it is still an option. And I think that is the author's point: If there isn't a necessity or dependency on the mobile platform, don't build for it. reply dpkirchner 17 hours agoparentprevWhat sort of mobile gaming related things have you seen Apple push back on? reply ip26 17 hours agoparentprevYour description is basically aggregation theory in practice. reply LibrePenguin 16 hours agoparentprevnext [2 more] [flagged] skydhash 16 hours agorootparentI take more abuse from developers than Apple in the majority of cases. But I agree that some of their decisions is overreaching. But most developers have bad intentions especially with telemetry, privacy invading measures and dark patterns. Apple offers a safe place for those with simpler computing needs. Other ecosystems are worse for them. reply WarOnPrivacy 17 hours agoparentprev> This author seems confused about something that he, himself explains... they have created such a walled garden cult w/ users that developers are forced to produce ... In the presence of a cultish relationship, I think a persistent sense of puzzlement is reasonable. reply BuyMyBitcoins 17 hours agoprevYears ago I decided to delve into learning Swift and native iOS development. I simply could not get accustomed to using XCode. The UI/UX of XCode is atrocious in a way that I’m struggling to articulate. I found myself constantly opening and closing panels just to click on icons that weren’t intuitively grouped together. To open one panel would forcefully minimize the other. It’s like a tenth of my time was dedicated towards “panel jockeying”. I got the sense that Apple’s designers wanted to make an IDE that was visually pretty and minimalistic, not one that was low-friction for the developers. But IDE’s aren’t supposed to be minimalistic, they should be allowed to be as customizable and cluttered as each individual developer would like, according to the needs of what they trying to build. Imagine a physical garage workstation. Whereas Visual Studio would let me make my workspace as cluttered and customizable as I would want, Apple would insist that I put each tool back in the box before grabbing the next one. This is what I mean about panel jockeying. I’m curious if other developers feel the same way, of if my analogy makes sense. reply biddit 17 hours agoparentThis really speaks to me and is exactly what stopped me from my multiple attempts to get into native Mac/iOS development - the IDE. Your assertion of form over function has helped me articulate what I hated about xCode - so thank you for sharing. I've been in the JetBrains ecosystem for over a decade now and while there are faults, I've never felt that JetBrains wasn't designing their IDEs to work how I wanted them to work. reply abroadwin 16 hours agorootparentI'm sad that AppCode is being abandoned instead of updated for the new SwiftUI world. reply jwells89 16 hours agoparentprevReally depends on development style and what you’re used to, IMO. Xcode doesn’t bother me at all while Android Studio, which is built around the vaunted IntelliJ, is constantly getting on my nerves. Visual Studio (the IDE, not the editor) is similarly frustrating and has odd restrictions to boot (I can’t use italics in syntax highlighting? Why??). This goes for editors too. I find VS Code mildly irritating in ways that Sublime Text and TextMate aren’t. reply fauigerzigerk 14 hours agoparentprevI agree that Xcode usability is absolutely horrible. It's also unbelievably slow and it's crashing so much it would never survive an App Store review. But what really turned me away from making native apps for Apple platforms is the combination of bugs and lack of documentation. SwiftUI was just not fit for purpose when I last used it a year ago. But even more mature libraries are often barely documented. It's hard to know what's obsolete. The advantages of native apps for my work are small to begin with (from a user point for view). It's mostly about more reliable local storage. If productivity is so much lower than making web apps it's impossible to justify the cost and the extra risk putting myself at the mercy of some oligopolist overlord. reply yellow_lead 17 hours agoparentprevI would agree. Most times I look for something simple to find in other IDEs (Build Output, Project Settings, etc), I get lost easily and have to search Google on how to open that pane. reply airstrike 17 hours agoparentprevIMHO XCode is both great and terrible. I miss it when I'm in vscode, but I also hate it when I'm in XCode, if that makes sense? It's like a heavy IDE of yore, which means the more you comply with it, the more you'll enjoy it—but that also leaves a feeling that you're not really in control. It's definitely worse than it could be if Apple cared enough to make it half as snappy as vscode feels. The shitty support for vim keybindings alone makes me furious (e.g you can't redo most actions like \"c\" or \"r\") Finally, I'm not sure if you were using SwiftUI back then or if you were fighting with storyboards in UIKit. The latter is an ATROCIOUS experience I wouldn't wish upon my worst enemy. SwiftUI in comparison feels like the future, even if it's still in its early days and some stuff needs to be ironed out reply jwells89 15 hours agorootparentStoryboards are indeed terrible. XIBs are notably better due to not bogging the editor down with N screens but still not great. Both suck for version control being gigantic blobs of machine generated XML. This is why I abandoned storyboards and XIBs in iOS development 7-8 years ago. UIKit is actually a pretty decent code-only experience, particularly since the addition of anchor-based autolayout constraint building. It was actually a major point of frustration that it’s not practical to go code-only with Android Framework on Android and WinUI/Windows App SDK on Windows, both of which are heavily invested in XML layout and resource files. Jetpack Compose thankfully fixes this on Android at least. reply steve1977 15 hours agorootparent> Storyboards are indeed terrible. XIBs are notably better due to not bogging the editor down with N screens but still not great. Both suck for version control being gigantic blobs of machine generated XML. I guess these problems stem from their heritage in XIBs being NIBs, i.e. binary “deep frozen” object graphs. Using XML to make them version-control “friendly” was always a bit of a crutch. reply jwells89 15 hours agorootparentIt also doesn’t help that Xcode will make random little edits to storyboards/XIBs by simply viewing them. It conditions developers to commit minor changes to these files without looking closely which leads to unintentional changes finding their way into production. reply saintlunaire 16 hours agoparentprevI agree. Another thing I really dislike about XCode is the error/warning messages – you stop typing and wait with bated breath for them to appear, only for them to be positioned over the very line of code you’re trying to debug and can no longer see. reply jahewson 17 hours agoparentprevEveryone feels this way about Xcode, right? It’s a relic of the 90’s. reply Quothling 18 hours agoprevI once had to setup an Apple developer account to have one of our municipality apps shown as ours. I'm not sure why that was considering all our other apps never needed that, but it was what it was. It was a pretty terrible experience. First I needed an Apple account, and since I didn't really want to use my private one, I needed to create one for work. I couldn't create an \"organisation account\" so it was tied to me. Luckily we had an old iPhone waiting to be thrown out, so I could use that. Then I had to wait for days to have Apple confirm who I was, but this was basically just Apple calling the person I had listed as my boss and then having him say yes. I hope they did more research than that, but I'm not sure they did, and the people who called us were even worse at English than us so it was hilarious to say the least. Then we had to set up payments, because for some reason you need to pay money to have an Apple developer account. Whatever, in the budget of an entire city of 60.000 people, that's not even going to show up anywhere, right? Well... Since it's a foreign subscription and since Apple has no way of doing this as a B2B purchase that can be easily registered with our local tax agency, it had to go under yearly reviews. It was also only possible to pay with a credit card which again tied it to something the organisation would need an actual person to renew and since organisational credit cards are tied to people, and since people change jobs, and since you need to be in actual human contact with Apple to change owners... well you can imagine how much fun that was. This was some years ago, so maybe things have changed, but out of any of our 300+ enterprise IT solutions that I ever worked with, Apple was the only one that was this horrible. To be fair, I'm a developer, I'm not sure how I ended up with the task and maybe these things are simply more common in the operations side of IT than I know. reply coliveira 18 hours agoparent> This was some years ago This is probably worse today, they're all the time throwing more roadblocks to avoid people to publish apps in their \"ecosystem\", except if you're a large US company that can easily handle the paperwork. reply bmitc 18 hours agorootparentI have never understood how Apple gets away with it and even more than that gets a huge amount of love. Whereas, if Microsoft just looks at you wrong, pitchforks are out. I'm not cheerleading any company here, but it has been frustrating in my career where people absolutely refuse to touch or even consider anything Microsoft related but love their Apple computer. Apple is probably hundreds of times more agressive, more stubborn, and moat building than Microsoft. Microsoft's dev tools are some of the most open in the world for a big company, certainly more so than Apple and Google. reply javcasas 17 hours agorootparent> Whereas, if Microsoft just looks at you wrong, pitchforks are out. Not to nitpick, but Microsoft looking wrong at you is more like the Eye of Sauron looking wrong at you. Just pointing to the gigantic amount of dark patterns on Microsoft products, especially Windows. reply bmitc 17 hours agorootparentApple's iOS and macOS have very similar dark patterns, but they go unmentioned. Yes, Windows has been making some really bad decisions with the Windows and Edge products, and it is really annoying. But at least their dev tooling is top notch, and you can easily interact with actual Microsoft employees in their repos. It's a nice experience. reply javcasas 17 hours agorootparent> Apple's iOS and macOS have very similar dark patterns, but they go unmentioned So you agree pitchforks should be out both for Apple and Microsoft. Count me in. reply bmitc 15 hours agorootparentYes, I agree. But Microsoft should recieve some kudos for VS Code and .NET. What they've achieved is quite astounding. No one else has done what they have with the transition of .NET. Although, there is definitely an argument that it should have been cross-platform since the beginning. I don't understand why these companies make the OS level decisions they do though. They make billions. You think they could relax and try to capture users by making them happy instead of kidnapping them. reply hnfong 18 hours agorootparentprevAs an Apple user, the experience is generally fine, especially when compared to Microsoft. Most of the complaints about Apple seem to have come from devs. And in contrast, Microsoft [in]famously loves developers (Balmer on stage...) reply bmitc 17 hours agorootparentI was an Apple user and even fanboy for many years. I left because they ignored macOS and then started making it iOS like. I have far less issues on Windows than I do on macOS. Apple sucks dealing with anything that doesn't have their name on it. External monitors have terrible support because Apple refuses to implement a specific protocol making that nice, like everyone else does. You basically have to use Apple's monitors or their overpriced officially supported LG monitors they sell in the Apple Store if you want an integrated monitor experience. Several other issues with Apple exist with peripherals. I have to make special router settings in my home because my partner's work Macbook can't switch between the mesh network. My partner also had to get IT to give her admin access so that she could rename iTunes because it was the only reliable way to keep iTunes from opening every time her non-Apple Bluetooth headphones connected. Search about what it takes to rename an app like iTunes. It is literally insane. reply bigstrat2003 15 hours agorootparentprevIt's because Apple doesn't have users, they have fanboys. Apple customers aren't there because of the quality of the product, they're there because they've been convinced that Apple is cool. reply lm411 14 hours agorootparentI think you mistyped the URL - this is Hacker News, not Slashdot. Let's keep the discussion intelligent and not bother with the mud slinging or other insults. An interesting observation is that users of Apple products rarely go out of their way to insult others based on their choice of hardware / OS, while users of Android (and Windows & Linux to a lesser extent) seem to get quite needlessly enraged at the Apple users. No one is forcing Apple products on you guys, chill out. As someone who uses iOS, Android, Windows, Linux, Mac, and FreeBSD on a daily basis, I believe all of them have their pros and cons. On topic: Apple used to quite strongly prefer (and tell) developers to use web apps when appropriate instead of a native app. As someone that has helped many companies develop apps - the biggest complaint about not having a native app is that they won't be listed in the App Stores without one. Many of the apps I've helped develop could be web apps, but, companies really want to see their app in the App Stores. reply bitwize 17 hours agorootparentprevApple's entire stock in trade is \"computing as it should be\". Their stuff is the coolest platform ever to exist, which means that developing for their gear is like getting into Studio 54: only the hottest and coolest are allowed in, permitting anything less would tarnish the club's reputation. Which sucks for most devs but it's part of being the coolest. Microsoft just wants to dominate. They don't care if the UX is garbage, as long as the user doesn't have a choice. reply rchaud 16 hours agorootparentThis sounds it's coming from 2006 when we would laugh at the mess that was Windows Vista. Only the hottest and coolest allowed in? Let me share the good news with the thousands of developers making Bible apps and Clash of Clans clones. reply realusername 17 hours agorootparentprev> only the hottest and coolest are allowed in, permitting anything less would tarnish the club's reputation It's kind of a cult in other words, personally I'm not part of it and not afraid of blasphemy when I see stuff broken. reply bitwize 17 hours agorootparentIt would be kind of a cult, if Apple products weren't more pleasant and joyous to use by far than any of the alternatives. reply bigstrat2003 16 hours agorootparentThey aren't. I find them worse by far than any alternative. reply coliveira 15 hours agorootparentSorry, but I use Mac and Windows computers (by job requirement). Windows is and has always been clunky and difficult to use. Maybe you don't feel that, but it is a fact. I wound't spend any time with windows if I had a choice. reply nerdix 14 hours agorootparentThat's not a fact. That's your an opinion. In my opinion, macOS has an horrific UX especially for power users. It's so bad that there is a cottage industry of third party developers creating apps to refine the bad out of the box experience with macOS. Most macOS users buy several of these apps and it's just an accepted part of the culture around Macs. Windows isn't great and has other issues but I think it has a better UX overall. reply realusername 13 hours agorootparentI'm in the same camp but I do think MacOS has the best interface on a screenshot, it's when I'm actually using it that I find mountains of problems. I think the whole thing is organized on how good it would look to screenshot rather than usability. reply smoldesu 13 hours agorootparentprevThere's ample room to criticize either OS, and given the opportunity I'd avoid either both. Nobody I've ever met will unilaterally defend stock MacOS. You criticize Spotlight, they say \"buy Raycast!\" You mope about window management and they tell you to install Rectangle. You get jumpscared by the Apple Music popup when you put on headphones, and a concerned user will always chime in with the registry command to disable it. It's a jack-in-the-box of modal advertisements and sophisticated service clients. On Windows, I get the APIs I want like Vulkan without arbitrary software restrictions. It's also a ghoulish wasteland of service integration and user hostility, but at least they let you do what you want. That sort of immediacy is probably why a lot of users prefer Windows even if it's arguably poorly-designed. > Maybe you don't feel that, but it is a fact. Facts normally come with citations. Do you have anything better than your opinion to cite? reply realusername 17 hours agorootparentprevExperience is subjective I'd say, I certainly disagree with that after using the old iPhone of my wife for a year to not make more ewaste. I found the whole thing buggy and confusing, I finally got rid of it after an error loop when installing apps meant that I had to format it again to fix it. I'm sure most users do love it though, to each their own. There's certainly enough space for all kinds of products and all kinds of consumers. reply smoldesu 17 hours agorootparentprev> Which sucks for most devs but it's part of being the coolest. What if we determine that the \"coolness\" motivation is a scapegoat for anticompetitive behavior? AT&T could have argued the same thing but I'm not sure if the coolness of their infrastructure would have saved them from a breakup. reply pmontra 16 hours agorootparentprev> how Apple [...] gets a huge amount of love. The wind is changing. If somebody dared to write a comment on HN criticizing Apple in the 10s, downvotes rained down on that comment. HN's policy is to downvote to express disagreement, among the other things. That's not been the case anymore since a few years. This very thread would have been nearly impossible 10 years ago, maybe 5 years ago too. reply lm411 14 hours agorootparentprevWhat are these new roadblocks? From a North American perspective, the only thing I can think of that has changed is Google now requiring DUNS numbers for business accounts (while Apple has required them for a very long time, for business accounts). reply rstephenson2 15 hours agoparentprevIf you were doing it today it’d probably be much easier from the enterprise procurement side. They often give out single-use virtual card numbers per service now. reply realusername 18 hours agoparentprevIt's still the same, I had some error loop on the iPhone when setting up a developer account and their support just didn't understand what's going on until something like 6 months after, it somehow worked suddenly. It's as random as ever, it might work straight away or you are unlucky and hit some of the random bug in this process and it'll fail for a long time reply jwells89 16 hours agoprevThe web is great in theory, but the bare-bonesness of the browser environment makes it an exceedingly unappealing platform for apps specifically when you’re used to developing with a throughly batteries-included experience like you get on Apple platforms. Where on macOS a highly capable, polished app can be easily be developed with a list of dependencies and sub-dependencies that can be counted on a single hand (and with a little effort, none at all), the equivalent web app has many tens or hundreds because of all the feature gaps needing filling in. For example, I don’t see why browsers can’t furnish basic list and table views that are capable of efficiently recycling their cells without any (or extremely minimal) JavaScript. It’s not unusual to need to be able to scroll through hundreds or thousands of items without causing the device to chug or run out of memory, and that’s handled nicely out of the box in AppKit, UIKit, SwiftUI, Android Framework, Compose, and probably even Flutter (haven’t checked) but in the browser you’re either pulling in a library or writing custom code for this very basic capability. And that doesn’t even get into the package management and general tooling situation, where the same fundamental problems doggedly persist even as solutions come and go. reply auggierose 15 hours agoparentI find that developing with Electron is a much nicer experience than developing for Apple UIs. And I've written my share of iOS/iPad apps starting over 15 years ago. The nice thing about the Web is that there usually is a library/framework that fits your needs (Electron, for example). For Apple, often there is no good library. Also SwiftUI is so buggy. React just works, and is conceptually simpler (folks, two-way data binding is a bad idea). Apple has this whole idea of making everything simpler for you, but are often making everything more cumbersome and difficult for you. reply jwells89 14 hours agorootparent> For Apple, often there is no good library. Do you have examples? I’ve not often run into this in my projects. These days there’s Swift packages for most things, and in the worst case I’ll have to write a simple wrapper around some C or C++ library. SwiftUI is still green yes, which is why I’ve stuck to UIKit for anything moderately complex. reply auggierose 12 hours agorootparentI stopped coding in Swift about 3 years ago, and SwiftUI was green then, too. I don't expect it to become much better soon. Not much comes to mind right now in terms of libraries, I just remember constantly thinking that on the Web it is just an \"npm install\" away, while in Swift I would just have to code it up myself. UIKit is powerful, of course, but just requires too much ceremony most of the time. And SwiftUI is just not reliable. Writing wrappers for C libraries is not my thing, thinking about all implications of different memory models etc. is exhausting, and I'd rather not waste my time on low-level stuff like that if it can be avoided. Mostly I am thinking about how easy it is to do UI stuff in React, it's much simpler and quicker. The ceremony that XCode and UIKit require are exhausting, as well. Why exactly do I have to give 100 different icons via drag'n'drop for my app? Funny, the Electron app only requires one icon, and I just place it in the right path. What's a PITA in JavaScript is the commonjs / ecmascript modules mess, but I have a standard template now for coding up a module so that it works for both, and since then I much prefer npm over the Swift package manager, as well. Coding in the Apple ecosystem just feels sluggish to me now. It feels like the equivalent of a luxury prison. Being able to access things like general compute shaders via Metal is the only thing I miss, but I think via WebGPU this will be fixed soon enough, and in a cross-platform way at that. Just like Electron runs everywhere, not just on my Mac. So what if it is 200MB large compared to a few kilobytes? That doesn't really matter. That's a one time price you pay, and it is well worth it. reply jwells89 12 hours agorootparentMaybe we’ve just been writing very different types of software but as noted earlier, it’s been very unusual for there to not be built in or third party solutions to problems I’ve encountered in UIKit/AppKit. The ceremony involved seems pretty minimal to me considering the trouble saved. It’s a relatively small front loaded cost that precludes problems further down the line. As for icons there’s a bunch of shell scripts, AppleScripts, etc that will take care of minor one-time annoyances like icon scaling (though there’s actually good reason for 1x, 2x, and 3x versions of images to be distinct… scaling a single high rez icon down tends to look like a terrible smugdy mess on “normal” DPI displays). reply auggierose 10 hours agorootparentSure, you can streamline the process of icon creation. The point is that Apple often sets up things in a \"simplified\" way which is not really simpler, just more annoying. There are no \"normal\" DPI displays in Apple's ecosystem anymore, so one SVG should always be fine in many cases. Again, my main point is that coding Apple apps just takes too much time and ceremony compared to what you get out of it (an app that only runs in Apple's ecosystem). The only way to justify writing an app specifically for Apple products is when you depend on their special hardware and ecosystem features. Or of course if you don't care if your app runs anywhere else. Or if you have resources to blow. Apple treats software as an extension of their hardware: Generality and abstraction do not count for much, as long as this makes it possible to fit the software tightly to their current hardware. That's nice for Apple, but not so much for a developer who doesn't care for much more than generic hardware features. reply jwells89 9 hours agorootparentThere’s plenty of people in the Apple ecosystem that still use normal-DPI monitors. 2560x1440 27” monitors are very commonly used with Macs for example, and it’s not unusual for iOS devices to get hooked up to 1080p TVs. I’d say the investment is also worth it for those who don’t like developing lowest common denominator apps. reply auggierose 8 hours agorootparent> I’d say the investment is also worth it for those who don’t like developing lowest common denominator apps. If I look at Electron, that lowest common denominator is pretty high. I'd argue that if you put the same effort into an Electron app that you have to put into an Apple app, you get a much better Electron app compared to the Apple app. And you can run it on Linux and Windows. So, unless there is something essential for your app you cannot do in Electron, not liking the lowest common denominator still fits into the resource blowing category. Hell, blowing resources can sure be fun! reply skydhash 13 hours agoparentprev> For example, I don’t see why browsers can’t furnish basic list and table views that are capable of efficiently recycling their cells without any (or extremely minimal) JavaScript. The web was built with documents in mind, so that the html files already has all the data needed for display. The tables are already populated with all the data needed and in the correct order from the server. There is no need for recycling cells and rows. And all of that is snappy because we're talking about KBs of data for thousands of rows. reply jwells89 13 hours agorootparentEven static pages can get to be too large with a certain number of rows and would benefit from a variant of HTML’s table element that automatically recycles rows while displaying static data. The problem isn’t the size of the transmitted data but the strain the resulting page puts on the browser engine due to the huge number of DOM nodes. reply flawn 15 hours agoparentprevFlutter is pretty strong with it's out-of-the box APIs as they have a custom render engine. Together with WebAssembly (& alternatives like Flutter) the bare-bonedness shouldn't be a problem anymore. reply deadbabe 16 hours agoparentprevWhat’s wrong with Javascript reply bsimpson 15 hours agorootparentThe web was originally a document sharing program for academics. JavaScript was never a \"batteries included\" language because the web wasn't built as an app platform. It evolved into one, and the platform has improved for app development with things like modules, but it is still part of an ecosystem that expects the developer to bring the app framework. Custom Elements tried to solve this, but React came and solved the same problem in a more ergonomic way. So now, we're in the same place: apps are built with open source stacks like React where you have to bring your own toolkit because the browser doesn't make assumptions about which one you're using. The web is a lot like Linux: it evolved as thousands of developers try thousands of different ways of doing things. Some of those things are great and become de facto standards, but it's an ecosystem you can't manage top-down. If you've ever worked at Google, there's a famous internal deck about how decentralized ecosystems evolve like an amorphous slime mold. It's written in the context of Google product management, but it applies to things like Linux, and the web too. reply jwells89 16 hours agorootparentprevIt’s not JS itself that’s the problem, it’s the need to write it to fill gaps in browser facilities. Rather than there be 10k different implementations of a scrolling recycler list of wildly varying quality, there could just be one in each browser engine that web devs can rely on, allowing them to instead focus on writing application code. reply deadbabe 15 hours agorootparentEven if those facilities were available wouldn’t the complaints then shift to some other high level facility not being available natively? reply jwells89 15 hours agorootparentIt’d be rinse-and-repeat until browsers had a decent toolkit out of the box, yes. That doesn’t strike me as a bad thing. reply charcircuit 15 hours agorootparentprevYes, and as the next and next gets resolved over time browsers would be easier and faster to develop high quality products on enhancing the value of the web ecosystem. reply vorticalbox 16 hours agorootparentprevNothing really but lots of it is not a nice experience. reply behnamoh 15 hours agorootparentprev> What’s wrong with Javascript Javascript. reply lta 16 hours agorootparentprevWhat is not ? reply codegeek 18 hours agoprevSometimes, we forget how open the original web/www was and even today, how open it is overall compared to the \"app ecosystem\" monopolized by Apple and Google. Yes, there is \"cloud\" but nothing stops me from renting a server and hosting my own stuff. That didn't work out ? Np. Get it out of there and rent another one. Yes it may have \"lock in\" and not that easy but it's not impossible. With the whole App ecosystem, you only have 2 choices and you are at their mercy. Literally. I personally would never build an entire business around an \"app\". Never. I may have an app as a small supplement if at all and that too if my audience really demands it. I hate products that force an app on me when I am on a mobile device. No Thanks. Heck, bring back the whole m.website.com thing but I prefer to avoid the whole app ecosystem. reply mdaniel 16 hours agoparent> With the whole App ecosystem, you only have 2 choices and you are at their mercy. Literally. And if it's the two I'm thinking of, one of them is a customer-service black hole with a ban hammer, so overnight one could find oneself in the \"beg the front page of HN for help\" camp for any reason or no reason I say prayers that the new sideloading thing for the EU will inspire similar calls here in the US, but also I'm super cognizant that such a thing would require a sufficient mass of folks who know or care what \"walled garden\" or \"sideloading\" mean :( reply AshleysBrain 18 hours agoprevI don't think it's true that developers do nothing for Apple. If there were no third party apps for iPhone, Apple would sell many fewer phones. I suppose many could theoretically move to the web, but then the point still stands that third party apps are still making the iPhone much more worthwhile to own. With proper competition, OS makers work hard to attract developers - think of Ballmer's \"developers developers developers\" clip from back in the day - as they recognise that developers add value to their platforms and help sway consumers. The trouble these days is there is no meaningful competition. No matter what Apple's rules are, developers are obliged to provide something for iPhone, and Apple can rest assured there is negligible chance of a third mobile platform gaining traction. Apple know this and through their strict policies have imposed a cruel reversal of the situation: they claim they are doing developers a favour by deigning to allow them to access all of Apple's iPhone customers, and tax all their revenue for the service they ostensibly provide, even though developers do a lot to make iPhones worth having. It's an abuse of their market position and there's not much anyone can do about it - except as the blog says, publish to the web instead. It's not perfect but it's the only meaningful alternative to regulation, which Apple are obviously going to use every trick in the book to wriggle out of, because why willingly shut off billions of revenue from taxing app developers? Hopefully the web can gain traction as a way to avoid abusive app publishing rules. reply lagniappe 18 hours agoprevApple users are conditioned to pay for things. Apple applications have higher buy rates than other platforms because outside of HN the average Apple user is less capable and less likely to pirate or need \"Free\" to entice their attention. I don't think anybody writes software for Apple thinking that they \"care\" any more than Google or Microsoft. reply jahewson 17 hours agoparent> the average Apple user is less capable Do you actually have any evidence to back up this claim? I’m assuming the real reason is because the average Apple user has a higher income than the average non-Apple user. reply brookst 18 hours agoparentprevSame concept with more charitable framing: Apple has built an ecosystem where users don’t want to mess with tech and are willing to pay for value. But yeah, there is no morality here, it’s just business. reply aetch 18 hours agoparentprevMost users are incapable, that’s why they’re buying and using other peoples software. How much is your time worth? Apple does happen to capture the market of users that are able to pay more to save more time, though. reply mrweasel 18 hours agoparentprev> Apple users are conditioned to pay for things. Not that I entirely disagree, but that doesn't seem to be consistent with the complaint from app developers that they are unable or barely able to cover their development cost from app store sales. reply skydhash 16 hours agorootparentEveryone is conditioned to pay for things. Either in the form of taxes or directly. Everyone is used to recccurent payments (internet, electricity, water,…). Google, Facebook have been the biggest proponents of software in exchange of your data instead of money. reply 65 17 hours agoprevEvery day I think about how great the web is, and how much of a shame it is that Apple has tried to destroy as much of the web as possible by getting developers to make iOS apps instead of web apps. If there was no app store the web would be so much better. We'd have more variety in our content consumption, our social media sources, our algorithmic recommendations, our digital experiences. The web runs on everything, and there are so many great APIs that can be used to make immersive/next gen applications like WebXR. But someone making some WebXR app and charging for it on their own site wouldn't make money for Apple, so Apple never promotes these web apps. Long term... the web never dies. Companies come in and extract their profit and then die. But the web never dies. reply mark_l_watson 18 hours agoprevI like the author's healthy attitude towards large corporations - basic modern survival skill. If I had my way, I would not have to install any apps on my iPhone or iPads but platform limitations make this a requirement. I noticed that on Safari, the X/Twitter web app is not playing videos, even after I turned off Lockdown Mode for X. Is this Apple's fault for a platform inconsistency or X trying to force users to install their app? I would like to know. I specialize in deep learning and LLMs, but I have also always enjoyed web development. What holds me back is how complex the tooling is. That said, someone I know has been writing a lot about ClojureScript + Dart so maybe I will give that a try. I would like to find a simple stack for web apps that I could learn in a few days and that was well supported. Any suggestions? reply jahewson 17 hours agoparent> ClojureScript + Dart Ignore people who write about finding the perfect stack. The perfect stack does not exist and it is not requisite for shipping great software that real users value. Much of web development consists of knowing the browser well, and web.dev is a great resource for that. Beyond that just go and learn React + TypeScript, the new react.dev is great. While React is not perfect it’s such a good paradigm for creating UI that even Apple copied it when they created SwiftUI. Grab yourself Vite and get coding. The number one thing that I would recommend is to really go and study those resources. Start from page one of the docs and work your way through. There’s not much that you’re going to learn in a few days though, frontend work is hard for a good reason. reply mnvrth 17 hours agorootparentI would second that. Frameworks are secondary. React + TypeScript was the key (for me at least). After that it is just learning web standards - starting with CSS, HTML, JS - and then moving on to Canvas, WebAudio etc if one desires. reply mark_l_watson 14 hours agorootparentprevThanks for the advice jahewson and mnvrth. I have developed a few SwiftUI apps, so React may be similar. reply zshrc 18 hours agoprevApple provides thousands of APIs to make developing on the platform easier, has created their own programming language that integrates nicely with the platform and has a fully integrated IDE that works with both above. Sure, buddy… They don’t care about you unless you develop for their platform. And who can blame? The above is a terribly expensive and timely investment. reply bmitc 18 hours agoparentYou left out that they lock down anything that isn't those things. I tried developing an app of my own on macOS, not using Swift and their tools, and it's a huge pain. They have deprecated and version locked OpenGL for no other reason than to push their own tools. Trying to just get an external DLL to load was nearly impossible. Anything cross-platform needs to be converted down to their tooling, such as Vulkan to Metal. MacOS is even more of anomaly and special case than all the Linux distributions. It's crazy how much easier development on Windows is. And Microsoft's is almost all cross-platform. None of those things you mentioned allow you to work in a platform agnostic manner. Whereas Windows supports their own stuff like DirectX but still allows direct running of Vulkan, OpenGL, etc. reply ederamen 17 hours agorootparentThis. Apple has been slowly, relentlessly, sneakily squeezing out any open or cross platform dev options. If you want to develop for Apple's devices, you have to sacrifice your future career options with any other tech stack/vendor by investing your limited time into skills that are only relevant in the Apple ecosystem. Then you have to hope that it's still financially viable to do software dev in Apple's world for the next few decades - which given their demonstrated behavior of scraping back more and more of the pie for themselves and their shareholders, seems like a risky bet. reply hnfong 18 hours agorootparentprev\"I tried developing on , actively avoided using their tools and tech, and the experience sucked because nothing I tried was supported.\" reply bmitc 18 hours agorootparentIt wasn't that it was not supported. It is that Apple is openly antagonistic to and actively blocks anything that isn't theirs. And you also left out key information. I don't have any issues with the same development on Windows or Linux. Apple is the outlier and isn't doing some reasonable thing. Apple is by far the most troublesome platform to develop for. They literally make you do it their way, and you cannot recover any of that work for other platforms. reply kredd 17 hours agorootparentI generally agree with you, but on the other side of the coin, if you follow the processes they’re forcing on you… it’s pretty seamless. Sometimes it’s a bit buggy when they release new APIs or versions of tooling, but given the amount of people who do Native development and the money it generates, it gets resolved. reply bmitc 17 hours agorootparentI don't have infinite time and energy. It would be easier to just not support macOS rather than learn an entire stack just for them. Microsoft also has a Windows-only stack, but its perfectly fine to use others. reply kredd 6 hours agorootparentI’m not sure what you mean by “entire stack” when it comes to mobile development, especially with iOS. If you want to target >85% of all devices, you aim for iOS current version - 1, then Swift and xCode your way into AppStore. I am obviously over-simplifying it, but my point is it really depends on the market you’re aiming for. Like if you think you’ll make an app for users in North America, not supporting Android makes more sense. On the opposite side, if you want India, LATAM, you can drop iOS and never learn anything about Swift since your ROI will be very little. Again, heavily depends on what your goal is. reply pohl 16 hours agorootparentprevThis is an overestimation of how much effort it takes to learn another language and stack, especially in a world with Google, Stack Overflow, and LLMs. reply motoxpro 17 hours agorootparentprevI agree with the GP. I feel like you're trying to walk into a steakhouse and ask for pancakes KNOWING it was a steakhouse before you went in. Pancakes are great, but it says steakhouse right on the sign that says \"We sell steak and nothing else\" reply bmitc 17 hours agorootparentThe analogy doesn't work. It's not like Apple provides their platform and then if you don't want to use it then fine, you're on your own. They are actively hostile towards you doing something else. If I leap off your analogy, which again doesn't really work, it's as if they throw tomatoes from their steakhouse as you go towards the pancake house. reply motoxpro 13 hours agorootparentIf you go and develop for android (the pancake house) how can they possibly be hostile toward you? Sure if your want to develop for both then it's not ideal, and I agree it sucks to have to learn two languages, but that's not apples problem. reply skydhash 16 hours agorootparentprevAre they? They own the OS. And they reserve to themselves the rights to change how it behaves. There’s no standard API, only tooling. So yeah, it’s obvious that you’ll be in great pain if you try to sidestep the tooling. Everything apart from their SDK is and will always be an hack. reply bmitc 15 hours agorootparentAre they what? Actively hostile? Yes, they are. Of course they can do whatever they want. But it makes everyone slowly hate them. reply Eric_WVGG 18 hours agoparentprevI likewise have a fairly sunny perspective of Apple development, I even like XCode (since SwiftUI, at least… could never get the hang of Interface Builder). But the fact that he got in to work on a music app is illuminating. The Apple audio APIs are an absolute mess. Media Player, AVPlayer, Core Audio, AVFoundation, AVAudioEngine… it's like competing teams dating all the way back to NeXT kept writing their own libs and somehow they all persisted through the iPhone era. I spent about three months of the COVID lockdown trying to make a Shoutcast/Icecast player, it was excruciating. reply mnvrth 17 hours agorootparentOP here. Funny you mention Shoutcast, because a Shoutcast player is what I'd made earlier - https://github.com/mnvr/Soundtrack. So I sort of know what you mean :) And you know the reason why it's macOS only? because the Apple App store reviewer rejected it heh. reply ederamen 18 hours agoparentprevThey don't develop those APIs out of the goodness of their heart - it's a lock in strategy. They've made it practically impossible to write native cross platform code that runs on IOS, and they've everything they can (within political limits) to prevent web apps from being able to compete with native apps. reply dgfitz 17 hours agorootparentI’ve had success with Qt building an app that compiles on Android and iOS. I’m not in the app development business at $dayjob though. reply ederamen 17 hours agorootparentOh cool - I started looking into Qt a while ago but was confused by their license. Would you recommend it? reply rubymamis 17 hours agorootparentThe license may look confusing, but it's pretty permissive. I highly recommend Qt. I developed a block editor[1] (similar to Notion) using Qt C++ and QML, and my development experience is pretty great. [1] https://www.get-plume.com/ reply bowsamic 17 hours agorootparentprevMany apps have a cross platform C++ back end reply officialchicken 18 hours agoparentprevMy only concern is that standard would easily place Oracle as the winner of most corporate care for developers in a terribly expensive and timely investments!!! reply pjmlp 18 hours agorootparentActually I have my Oracle account since mid-1990's, and in some respects for DB development tooling, only MS SQL has feature parity. Specially for applications where the DB is a platform on its own. reply mouse_ 18 hours agorootparentprevYou put this better than I could have. Extended support can in fact be malicious. Appreciate your contribution. reply lutoma 16 hours agoparentprev> has created their own programming language For me, that's part of the problem. I'm sure Swift is a nice enough language based on what I've seen. But I'm not going to invest a substantial amount of time learning a language that is realistically only ever going to be useful to target one platform by a single company. reply TillE 18 hours agoparentprevSwift is great, but the vast majority of apps really don't need to be lovingly hand-crafted for each platform, and Swift is (currently) a bad choice for anything non-Apple. Personally if I were working on mobile stuff, I'd be looking at .NET MAUI. reply itsautomatisch 15 hours agorootparentI'm not sure why you think MAUI is a great choice considering its current status. There is a huge lack of learning resources, a very tiny community, and very few-to-none successful applications built entirely using MAUI at this point. There isn't any strong guidance or opinionated advice for how to architect or scale your app from Microsoft, either. Avalonia is a much better choice if you must use .NET, but outside of people who really, really like C# or have legacy apps they want to migrate, I don't think either will be very good choices for new greenfield apps. If anything, Microsoft has been signaling that Blazor Hybrid is what you should be using, and while it does use the same core technology under the hood, it is much more at home for C# developers who have been doing ASP.NET which has far more developers than WPF or Xamarin ever did. reply worksonmine 17 hours agoparentprevThey didn't have to develop their own language. It's part of the lock in. reply Retr0id 18 hours agoparentprevAre you writing this from the perspective of someone who has tried to consume an Apple API? reply righthand 18 hours agoparentprevIt’s eternally expensive as they extract more and more money from the developers in new ways too. Building those APIs and tooling is never paid off because there’s enough unpaid evangelists socially guilting you into paying for it. We should be so lucky that Apple extracts billions every year and says when and where to stop and go. reply blitz_skull 18 hours agoparentprevLol. Have you ever used their “IDE”? My guess is you haven’t based on the tone of your remark. Apple has been developer-hostile for years, and OP was just synthesizing their thoughts around this and TBH I found the conclusion to be fairly nuanced and apropos. reply whynotmaybe 18 hours agorootparentI've used every major ide since turbo Pascal in the 90`s and xcode is the less intuitive ever. I'm not an Apple user and when I have to do anything I don't know on Mac, the way to find it is always with the question \"what's the simplest way to do it\". Except xcode. I always have to google or gpt to find what I want. reply realusername 18 hours agorootparentprevxcode is indeed the worst coding software I ever used, it looks like some kind of 90s software which got left rot since. The whole thing takes 13GB to download where resuming fails, has an undocumented config format which doesn't work with git properly and is more sluggish than an Electron app. The app upload process itself is so broken that even Apple had to release a third party tool to bypass it. I refuse to believe they are using this thing internally, they must have some kind of special internal process to make this mess sort of work. I could say similar things about appstoreconnect where even reordering app images is broken with a race condition if you click too quickly and you have to reload the page. reply smoldesu 17 hours agorootparent> The whole thing takes 13GB to download You're one of the lucky ones, eh? reply aniforprez 17 hours agorootparentWhen the stars align and the mighty gods bless me, I am lucky to have Xcode download only 15GB to update. Once, the update got borked and consumed all free space in my Macbook when I kept retrying and almost bricked the laptop It is utterly insane to me that this is the only way to build and sign apps for Apple devices locally. At one point, I almost burst into tears cause I was excited that they finally added an update that highlighted the line the cursor was on reply romanovcode 18 hours agorootparentprevI still can't figure out how to format on save in Xcode. Or format in general? reply DelightOne 18 hours agorootparentFormat with this integration: https://github.com/ruiaureliano/X-SwiftFormat Then make CMD+s the shortcut. reply andrekandre 18 hours agorootparentthats great (i use swift format plugin myself) but really apple should provide these features not 3rd parties and ideally format while typing not just save reply Someone 18 hours agorootparentprev> Or format in general? I use the workaround of Command-A, command-X, command-V aka “Select All, Cut, Paste”. Unfortunately that loses cursor position in the file and gives you a free newline at the end of your content. reply noSyncCloud 17 hours agorootparentYou're supposed to end your document with a newline. That's a POSIX standard. reply mdaniel 16 hours agorootparentI don't mean this as snark, I'm genuinely interested: do you have a phrase I can search for or a link to that? I don't think I've ever heard that before but I would love to start citing it reply Someone 14 hours agorootparentBecause otherwise, files can contain characters that aren’t on any of its lines. https://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1...: “3.206 Line A sequence of zero or more non-characters plus a terminatingcharacter.” That’s consistent with: > echo testwc 1 1 5 > echo -n testwc 0 1 4 reply andoma 14 hours agorootparentprevMe too. Feels like I have an uphill battle with this at $dayjob. Would be nice with some ammo for the showdowns. reply Someone 17 hours agorootparentprevI know, but if your file already ends in one, it adds a second one, if it already ends in two, it adds a third one, etc. reply aetch 18 hours agorootparentprevLol. Have you ever used Android Studio IDE? The grass always looks greener on the other side but Xcode and iOS development is miles ahead of how slow and unhelpful Android Studio is the majority of the time. Don’t even get me started on opening up your app 6 months later to update the code and it’s a 99% chance some Android gradle build system or dependency has changed and broken the build… reply blashyrk 17 hours agorootparentI cannot disagree more on just about everything you wrote. In my experience, Xcode has been terrible, sometimes borderline unusable. The horrible project format, merging pains when working in a team, interface builder being trash in general but completely unusable when you use a lot of custom components with @IBDesignable properties across multiple modules - in that scenario it sometimes rebuilds everything in an endless loop, and brings even the most powerful Mac machines to their knees. In that state even changing a simple property on a view takes 15 seconds of 100% cpu time. Sure nowadays there's SwiftUI but IMO it can't be considered as anything more than an alpha until they fix the horrible navigation woes. And even then it still uses UIKit under the hood and you can see countless internally used constraints breaking all the time in the logs. At least that's how it was when I tried it a year and a half ago. And that's not even touching on the cruel joke that is the xcodeproj project format. Apple didn't even provide a package manager until relatively recently for God's sake. It was left to the community (cocoapods) to fend for themselves in this regard. reply pjmlp 18 hours agorootparentprevI love how it has become a meme in Android developer circles that a \"stable\" Android Studio release always requires a fix release shortly thereafter. Also Gradle breaking the DSL all the time has become such a pain, that eventually they had to create an upgrade wizard, as if debugging performance issues with the build wasn't fun enough. reply blashyrk 17 hours agorootparentSay what you want against Gradle, but that method of project/workspace configuration is miles, no, lightyears ahead of the dumpster fire that is the xcodeproj format, especially when working in a team. reply fsflover 18 hours agorootparentprevYou should try to write an app for a GNU/Linux phone, to see how being free from a duopoly helps in real life. For that, you simply use all desktop tools (and the app will also run on desktop). reply bogwog 16 hours agoparentprevHey man. I don't know you, but even if you had a highly successful app on the appstore, I (an internet stranger) would care more about you being hit by a bus than the Apple corporation would, or any of the MBAs over there that you worship. Any relationship you think you have with Apple is completely in your head. They take advantage of people like you all the time. Developing for Apple is an abusive relationship, and that's always a difficult thing to come to terms with. I know this comment is probably just going to annoy you and result in a long, ranty, defensive, passive aggressive response. I don't care. I still love you more than Tim Cook/Steve Jobs does/would, my fellow dev. reply mardifoufs 8 hours agorootparent>you worship Yeah okay, I guess you worship google if you support webapps too? reply carlosjobim 13 hours agorootparentprevSuch a nice and selfless friend! Will you also pay the same amount of money into the developer's bank account as Apple does for top selling apps? reply dickersnoodle 17 hours agoprevGo ahead and write \"code for the web\" but be prepared to have to choose among a bunch of different JS libraries and CSS design systems depending on who the CTO is for your organization and who has their ear; writing bare-metal HTML/CSS and minimal Javascript won't get you far with the C-suite and you'll be pulling your hair out trying to get things to play nicely with each other (or wind up digging through five or tenlayers to find a DOM element you need to augment). reply aniforprez 17 hours agoparentI would, a million times, pick writing web apps over interacting with Xcode in any capacity or deal with the proprietary project formats or deal with git in an iOS app project or debug the arcane errors that xcode spits out. Drilling down to figure out where a div is that I need to change is child's play in comparis",
    "originSummary": [
      "The author expresses frustration with Apple's lack of interest in developers and shares personal experiences with Google's indifference to bug fixes.",
      "They also express disappointment with Apple's music player and the limitations of its API.",
      "The author emphasizes the importance of writing code for the web and maintaining a flexible relationship with companies, rather than labeling them as entirely good or bad."
    ],
    "commentSummary": [
      "The debate focuses on the pros and cons of web apps versus native apps on Apple's iOS platform.",
      "Progressive Web Apps (PWAs) and Apple's control over app distribution are major points of discussion.",
      "Users express frustrations with Apple's development tools and suggest alternative platforms and languages to address these concerns."
    ],
    "points": 424,
    "commentCount": 340,
    "retryCount": 0,
    "time": 1707056678
  },
  {
    "id": 39256930,
    "title": "DEF CON 32 Saved: New Venue Found for Hacking Conference",
    "originLink": "https://forum.defcon.org/node/248360",
    "originBody": "Announcement Collapse No announcement yet. DEF CON 32 Was Canceled. We Un-Canceled it. Collapse This is a sticky topic. X X Collapse Posts Latest Activity Photos Search Page of 1 Filter Time All Time Today Last Week Last Month Show All Discussions only Photos only Videos only Links only Polls only Events only Filtered by: Clear All new posts Previous template Next The Dark Tangent The Dark Tangent Join Date: Sep 2001 Posts: 2712 Share Tweet #1 DEF CON 32 Was Canceled. We Un-Canceled it. 1 day ago DEF CON was canceled. We un-canceled it. After a great 25 year relationship Caesars abruptly terminated their contract with DEF CON, leaving us with no venue for DC 32, and just about seven months to Con! We don’t know why Caesars canceled us, they won’t say beyond it being a strategy change and it is not related to anything that DEF CON or our community has done. This kind of no-notice cancellation of a contract is unheard of in the conference business. The parting is confusing, but amicable. So now we have a challenge. Without a venue, will we be able to UN-Cancel DEF CON 32 before time runs out? Hackers are flexible. We find solutions. We need a space that can handle an event our size, and configurable enough to accommodate our content. We need a location close to our announced dates, and with super short notice... No small feat. We immediately scrambled a venue strike team to Las Vegas. Floors were walked. Meetings were held. Hands were shook and options weighed. When the smoke cleared, the field narrowed to one obvious choice and we began forging the requisite agreements. W00T! DEF CON Is UN-CANCELED! DEF CON 32 will still be August 8-11 2024, but now held at the Las Vegas Convention Center (LVCC) with workshops and training at the Sahara. DEF CON 32 will be an adventure where we can try things not possible in our old Casino Hotel spaces. What specifically you ask? Well we are still learning all the specifics but we will have more space, a proper food court, and the largest indoor venue LCD wall in the country. There are still many questions to be answered, and we have started a live FAQ section on the Forums for DEF CON 32 where we will be updating questions and answers. The initial FAQ is located here: https://forum.defcon.org/node/248358 I look forward to seeing everyone this summer, the start of a new DEF CON era! The Dark Tangent P.S. We made shirts and stickers: https://shop.defcon.org/products/def...d-mens-t-shirt https://shop.defcon.org/products/def...ed-sticker-set Last edited by The Dark Tangent; 7 hours ago. PGP Key: https://defcon.org/html/links/dtangent.html Tags: announcement, dc32 Stuck SaitoH Ham Radio Village Co-Lead Join Date: Apr 2021 Posts: 7 Share Tweet #2 6 hours ago Thank you very much for the transparency! Likes 1 Comment Post Cancel Previous template Next",
    "commentLink": "https://news.ycombinator.com/item?id=39256930",
    "commentBody": "DEF CON 32 Was Canceled. We Un-Canceled it (defcon.org)364 points by Spodera 6 hours agohidepastfavorite218 comments mrandish 6 hours agoI suspect Caesar's dropped DEF CON because the DEF CON attendees likely have a fairly low \"avg revenue per attendee\" yield because fewer of them gamble compared to the avg Vegas conference attendee. They also probably spend less on high-end restaurant dining and bar drinking inside the hotel. Since the pandemic Vegas has had a pretty strong resurgence in general and this may be a sign that Caesar's is doing well enough they've decided there are higher-revenue guests they can put in those rooms — even in the doldrums of August (a traditionally slow month for Vegas tourism). I happen to regularly attend an unrelated, non-tech conference that's always right around the same week as DEF CON. That conference also happens to attract attendees who don't gamble or spend much at the hotel other than room costs. The reason the conference organizer chooses August is they get better discounts on their costs from the hotel in exchange for filling up rooms that would otherwise be empty (except this hotel is lower-end and cheaper than Caesar's). This works out because unlike Caesar's this hotel is far off the strip and doesn't have nearly as much dining or gambling revenue potential anyway. reply nimos 6 hours agoparentI doubt they would cancel a contract already in place for that reason. Not renew the contract - sure. But canceling an already scheduled event because of low revenue per guest doesn't seem very likely to me? Or maybe it was some sort of ongoing agreement and canceling it was effectively \"not renewing\". reply johnnyanmac 3 hours agorootparent>But canceling an already scheduled event because of low revenue per guest doesn't seem very likely to me? Not to be TOO snarky, but given how quickly corporate cancels employee labor despite rising revenue, it would not surprise me for other corporate to also cancel \"low paying customers\" for \"high paying customers\". Loyalty is beyond dead so cancelling a contract is just a cost of business if they feel the alternative gives more money. reply TeMPOraL 59 minutes agorootparent> it would not surprise me for other corporate to also cancel \"low paying customers\" for \"high paying customers\" At least going by all the entrepreneurship articles I've read over the decade, \"firing your customers\" is a term of art, and a recommended approach for dealing with unprofitable and/or annoying customers - so I guess this shouldn't be surprising. > Loyalty is beyond dead so cancelling a contract is just a cost of business if they feel the alternative gives more money. Not to be TOO snarky, but that's kind of the point of contracts - contract cancellation terms aren't an \"or else...\" threat, but rather an agreed upon exit strategy. Termination fines aren't punishment, they're compensation for inconvenience. reply technick 1 hour agorootparentprevI will need to dig up the archives from DC 27 when the deal with Caesars forum was officially announced, but if memory serves me correctly DT said it was a 5 or 10 year contract. So unless there was some verbaige in the contract that allows Caesars to cancel for any reason, they're going to be cutting DEFCON a check. reply TeMPOraL 1 hour agorootparentA 5-year contract starting at DC 27 would hold thru DC 31, so DC 32 fits the \"not renewing\" hypothesis. reply flomo 3 hours agorootparentprevEveryone is missing \"but now held at the Las Vegas Convention Center (LVCC) with workshops and training at the Sahara\" part. So this more like they got passed to a different venue. Not \"vegas hates them\". reply vertis 2 hours agorootparentThe post says they had to do significant work to secure another venue. While it's possible the author could be lying there is no evidence of this so we must, at this point, take them at their word. reply dsr_ 42 minutes agorootparentArranging a convention site contract is always a lot of work, even if (hypothetically) the Caesar's rep suggested that they try LVCC. reply busterarm 4 hours agoparentprevAll of the more recent years that I did DEF CON I was with large groups of people going to high end restaurants and (ab)using the hotel bars. In fact the hotel bars were always packed. My suspicion is that Caesars is trying to do something like play with headcount. Late summer is not just a weak time for conferences but DEF CON needs a ton more space and a ton more human babysitting across that space than any other conference. You don't see EVO or BlackHat getting cancelled (same exactly time window) because they're pretty contained in one place. My guess is that Caesars needs to staff up a little for DEF CON or that they may even be considering reducing staffing in late summer. Con attendees are going to stay at their properties and use their bars/restaurants/tables anyway. ...although now that I think about it, EVO was moved up 2 weeks and has a new unannounced venue this year, so maybe this isn't isolated to DEF CON. ...and also the Venetian is having its convention space renovated until 2026... reply tptacek 3 hours agorootparentBlack Hat is a giant commercial conference run by a company that runs dozens and dozens of giant commercial conferences. No event venue is ever going to fuck with them. reply Klonoar 1 hour agorootparentprevIIRC EVO only moved because they outgrew the space/slot they'd been working in. The other reply to you outlines Black Hat. I very much doubt there's any conspiracy here. reply philshem 1 hour agoparentprevIt may have happened to physicists in 1986, although the APS conference was back in Las Vegas in 2023 https://qz.com/work/1249513/was-a-convention-of-physicists-r... (2018) reply mirekrusin 6 hours agoparentprevOr they figured they're somehow net negative when they do gamble :D reply c0pium 5 hours agorootparentThere are certainly a lot of DefCon attendees who think that this describes them. In my observation they are all very incorrect, usually humorously so fortunately. reply Eji1700 3 hours agorootparentVegas makes a fuckload of money off everyone who thinks they’re smart but doesn’t understand statistics reply sph 54 minutes agorootparentThere are only two types of people: those that believe they can outsmart the house, and those that never gamble. reply roygbiv2 14 minutes agorootparentThat's deffinately not true, I used to go to the casino under no illusion I'd come out poorer. I'd just do it because it was fun. reply yreg 47 minutes agorootparentprevIf that was true, the house wouldn't throw people out for suspected card counting. reply stavros 10 minutes agorootparentAka \"we don't want you here because you might win\". reply sgjohnson 32 minutes agorootparentprevAnd card counters. reply GuB-42 7 minutes agorootparentDo they still exist? They have closed most of the gaps previously exploited by card counters, and continuous shufflers are everywhere. I think the only ones who can make money are those playing poker and are really good at it. That's because they are playing against other players and not the bank. They still have to beat the rake. I'm not even sure comp players, that is those who play to get non-cash rewards like travels, restaurant and hotel stays while minimizing their losses can still have an advantage. I heard that casinos calculate comps by expected losses, making sure they stay on top (statistically). And they are cheaters, but it is like saying thieves can make money. shiandow 2 hours agorootparentprevThey probably make quite a lot off people who think they understand statistics as well. reply vsnf 1 hour agorootparentI'd be willing to be that the intersection of people who think this and then choose to engage in gambling anyway, is probably one of the highest grossing demographics that exist. reply TeMPOraL 48 minutes agorootparentIf true, we'll eventually see casinos sponsoring statistics MOOCs or other forms of relevant education. reply IshKebab 1 hour agorootparentprevNot just statistic. There are plenty of smart defcon people who understand statistics but don't understand that if you start winning they'll just kick you out. reply buzzert 55 minutes agorootparentNot sure that's true, actually. The usual strategy appears to be to comp the gambler with generous stays at the casino they're a patron of, with the expectation that they'll dump their winnings back in the next day. Taken with a grain of salt, as my only knowledge of this is via Hollywood movies. It does make sense from a game theory perspective though. reply smt88 5 hours agorootparentprevMy first thought was that GP was saying DefCon attendees would be counting cards, which is an effective and legal way to beat the house[1] (until you're caught and banned from the casino). 1. https://www.freep.com/story/entertainment/nightlife/2016/04/... reply daveguy 5 hours agorootparentCasinos in Vegas use too many decks and reshuffle frequently enough that there is no edge gained over the house when card counting. reply Kirby64 4 hours agorootparentThis is not true. Besides continuous shuffler machines, most casinos have 6 or 8 deck games that have plenty of 'penetration' (card counter term for depth into the deck that the cut card is placed) to offer an edge if you properly card count. There's also a big game to be played where rubes think they can card count and instead lose tons of money attempting to do so. The problem with card counting generally is that the casino has infinite money and never runs out, thereby they can sustain large expected value swings... whereas you need an enormous bankroll to handle those swings, assuming they don't throw you out before that happens. reply throwaway2037 3 hours agorootparentThe book \"Bringing Down the House\" by Ben Mezrich explains in layman's terms how card counting works for blackjack. reply clansimus 2 hours agorootparentprevThere's plenty of doubledeck blackjack with good penetration in Vegas, especially in high limits rooms. The problem nowadays is that the casinos are also counting, and the patterns are simple and easy to track with the tech we all have. Changing your bet even a couple times based on the count can have the pit boss getting a call to remove you. reply throwaway2037 3 hours agorootparentprev> effective ... way to beat the house Statistically, it is not effective. Your card counting needs to be (basically) perfect, and you need very deep pockets to handle extended drawdowns. reply serf 5 hours agorootparentprev'legal' has no meaning here when it's against every single casino policy in the world. reply ncallaway 4 hours agorootparentUh, yes, it does? There’s a huge difference between: “if you do X, you will be asked to leave” and “if you do X, the police will arrest you” Like, when I invite someone over to a dinner party, it is against my policy to insult my dog. If you do that I will kick you out (not actually, he’s a dumb klutz, you can insult him all you want), but that doesn’t make it illegal to insult my dog. reply photonthug 3 hours agorootparentTrue but not relevant. Police and legality do not need to be involved with certain kinds of casino justice. Security may just offer to beat your ass if you won't cease and desist, avoiding the paperwork. Could be bluff but they know where cameras are and have cop friends.. reply monkeywork 2 hours agorootparentYou need to check a calendar and see the current year - the days of Casinos' roughing up card counters is long long long gone. Might be great for your screenplay or fan fiction but doesn't match reality. reply photonthug 16 minutes agorootparentStrange that you can be so confident about this with private security when even actual police are sometimes involved in cases of excessive force, corruption, coverups. Besides, whatever your personal knowledge/experience is it can't be vast enough to prove a negative here, and only one counter example is needed. Regardless of the year I think you might want to reconsider your overly confident notions about fiction/reality or at least the condescending tone. I don't know what is institutionalized in what places, but have been threatened by casino security. Fuck around and find out I guess tptacek 3 hours agorootparentprevCaesars has a $9.3B market cap. They're not beating anybody up for \"casino justice\". reply throwaway2037 2 hours agorootparentprevAs I understand, in Las Vegas, as long as you do not use a device to aid with card counting (mind/mental only), it is legal. Is that still true? reply well_actulily 1 hour agorootparentSure, it's \"legal\"—but so is them banning you from playing blackjack or tresspassing you from their property. reply blitzar 1 hour agorootparentprevWinning is against casino policy too but that doesnt stop people trying. reply weinzierl 4 hours agoparentprev\"I suspect Caesar's dropped DEF CON because the DEF CON attendees likely have a fairly low \"avg revenue per attendee\" yield because fewer of them gamble compared to the avg Vegas conference attendee.\" There is the story that the American Physical Society was not allowed back after in 1986 Vegas supposedly suffered its worst week in history. First of all there is no real evidence that this story is true and secondly it doesn't make sense to me that they would cancel DEF CON after so many years for that reason. They would have done so much earlier, probably. https://skeptics.stackexchange.com/questions/39668/did-a-cas... reply elashri 3 hours agorootparentI heard this story many times. One of them was froma graduate student who attended this meeting. APS March meeting happened in Las Vegas again last year (2023). While there was no official ban for APS Conferences, there was a little interest in las vegas to host anything for APS for a ~35 years. reply teepo 3 hours agoparentprevCombine low ARPU with perceived risk (in the wake of the Vegas hacks last year) and a termination for convenience clause and this is a no brained for Caesars. There’s just not enough upside for Caesars to host in their marquee properties. reply RCitronsBroker 2 hours agorootparentim really sure you have found the answer, it’s most likely more of a perceived thing than any of us wants to admit. DEFCON attendees can be walking stereotypes at times anyways, but the combination of drunk, low yielding hacker(wo)men(tm) roaming your hotel probably just made the juice not worth the squeeze. reply throwaway2037 2 hours agorootparentprev> termination for convenience clause I never heard of this. Can you tell us more? reply spacebacon 2 hours agoparentprevThe simplest explanation is they don’t like hackers after their experience. So they push a bunch of hackers buttons with a last minute notice and prepare the honey pot to pen test their post ransom security posture and maybe in the process they find an amateur to pin it all on. reply lend000 3 hours agoparentprevI've heard stories about \"hackers\" at former DEF CON's pouring concrete down sinks and doing all sorts of other socially clueless vandalism, and resulting backlash for the organizers. While the infosec community is much bigger and more... \"normal\" than it was back then, I imagine the guests are still more of a liability than the average conference attendee and as you said, probably not big spenders. reply fortran77 5 hours agoparentprev> They also probably spend less on high-end restaurant dining and bar drinking inside the hotel. I'm not so sure. There's a _lot_ of drinking at DEF CON reply rdl 5 hours agorootparentIt's mostly with liquor bought from offsite and drunk in rooms/private parties, not via Caesar's venues or catering (there's a lot of that too, and this is summer dead period, so it still may be good). reply kortilla 4 hours agorootparentI explicitly remember them tapping out every keg at a bar there by 2pm about 10 years ago. reply kstrauser 4 hours agorootparentprevI can think of plenty of in-hotel bars packed with DEF CON attendees 24 hours a day during the conference. reply cratermoon 5 hours agoparentprevI heard a joke a tech conference people in Vegas many years ago. It goes something like \"people who go to tech conferences in Vegas bring one shirt and a $20 bill and never change either.\" So yea, programmers generally aren't gamblers because they know enough math to know the house always wins. reply rpmisms 4 hours agorootparentIn my experience, programmers like poker, but not games of chance. This also describes me. Poker is a data-heavy game of skill and memory, Craps is about the opposite. reply KptMarchewa 1 hour agorootparentMost people appreciate the skill poker requires, but like me never want to bother learning it. If I (very rarely) go to casino I'd just play games of chance for a defined loss budget and just stop playing when I either lose it or win enough to get dinner for the group. reply pjerem 26 minutes agorootparentprevIn my experience, potential gambling addiction has nothing to do with rationality or smartness. reply basil-rash 4 hours agorootparentprevCraps is not the opposite. Quite the opposite, actually. The magnitude of entropy casinos require you inject into the system each round is quite low in practice. Profiting off of that is all skill. reply mynameisnoone 3 hours agorootparentprevI went with a bunch of CS/bioinformatics/MD IITians to Reno, NV once. They were just there to gamble on games of chance. Personally, I think gambling is boring and stupid if the expectation isn't significantly positive. I'd gamble if skill was the dominating factor and the expectation wasn't so abysmal. reply guappa 1 hour agorootparentIf it was skill based, you'd be competing against a pro who does nothing else. At least with chance you have a chance :) reply karmasimida 3 hours agorootparentprevHouse sets the mean and variance, how could they ever lose? Only thing left to make it work is volume, transactions volume, so variance can be minimized. reply basil-rash 4 hours agorootparentprevEh, I’m a programmer and I go to vegas with other programmers fairly regularly. We know enough math to know the expected cost per entertainment•hour is comparable to many other pass-times. But even so we’re actually all net-positive on the city, thanks to a couple “lucky” craps runs. reply renewiltord 3 hours agorootparentCame to make the same comment. Vegas is a fun place. We spend some money and get some fun just like anything else. And same. A couple of roulette results has us “positive”. reply Orlan 5 hours agoparentprevSo… see you at Magic Live? reply p-e-w 6 hours agoparentprevThe simplest explanation is often the correct one. Casinos aren't exactly known for having moral qualms. They are, however, known for caring about their bottom line. They probably analyze every single event they host and then shuffle things around to maximize their expected revenue based on their past experiences with the same type of event. reply metabagel 4 hours agorootparentPut another way, they got a better offer reply Eji1700 3 hours agorootparentThat’s the weird part. I doubt they’re using the space so this strikes me as “think of the money we’ll save on hours” bean counting reply nodesocket 5 hours agoparentprevDoubtful, I'm sure it's related to the constant attacks against their infrastructure they must defend against (let's be honest, I'm sure Caesars is not defending successfully). The juice just ain't worth the squeeze. They have a business to run, and the risk of having a bunch of drunken and high hackers who happen to be the best in the world running amuck is not their idea of a good corporate event. reply lolinder 5 hours agorootparentCaesar's apparently explicitly said it wasn't related to anything the community did. It's possible that they're lying for some reason, but it's also possible that they're telling the truth. > We don’t know why Caesars canceled us, they won’t say beyond it being a strategy change and it is not related to anything that DEF CON or our community has done. https://www.reddit.com/r/Defcon/comments/1aj6ixn/def_con_was... reply andy800 5 hours agorootparent> for some reason To avoid any legal liability. Stating a specific reason would open them to possible \"breach of contract\" depending on whether the act(s) were significant enough or justifiable, based on the contract terms. Just say nothing, part amicably, everyone moves on without drama. With that said, they probably weren't lying. Most likely, months after ponying up $10 million to a sophisticated international hacking group, Caesars Entertainment probably doesn't want to invite some of the world's best hackers to stay and meet at its flagship resort. reply lolinder 5 hours agorootparent> To avoid any legal liability. Stating a specific reason would open them to possible \"breach of contract\" depending on whether the act(s) were significant enough or justifiable, based on the contract terms. This is how it works for at-will employment, but it would be a very weird contract that allows backing out only if you don't say why you're backing out. reply andy800 5 hours agorootparentLet's say Caesars states, \"we just got hacked and, as has been reported in every major newspaper, paid $10 million as ransom. We have reason to believe one or more attendees of DEF CON were part of that group.\" How does making this statement this benefit Caesars in any way? Now DEF CON can demand some proof of this claim, or sue for defamation, or state that without proof, Caesars isn't acting in good faith, whatever. reply jrockway 5 hours agorootparentI mean, attendees of DEFCON can hack Caesars even if someone else owns the projectors used for the Powerpoint presentations. reply andy800 4 hours agorootparentYes, most likely. That's why it would make zero sense for Caesars to state anything publicly that would antagonize members of the community. Saying nothing (or even praising DEF CON, and claiming it was a \"change in strategy\") is the smarter route. reply wkat4242 3 hours agorootparentprev> Most likely, months after ponying up $10 million to a sophisticated international hacking group, Caesars Entertainment probably doesn't want to invite some of the world's best hackers to stay and meet at its flagship resort. Most Def con visitors would be white hats so that would be a bit disingenious. I would expect most attendees to behave (reporting issues after finding one) Especially considering they just got hacked, a few pentests would be good for their business. reply RCitronsBroker 2 hours agorootparentyou say that like a person informed enough to know what a white hat is lol. Let’s be real here, even the ethical hacker bunch can look VERY wonky and rowdy to an outsider, especially if you are as far removed as the hospitality industry. The only time they had to deal with hackers in the recent past was decidedly painful for them reply michaelt 1 hour agorootparentprevDef Con has 30,000 attendees. And maybe 99% of them aren't assholes. But in such a large group, there's always going to be some people who'll decide to muck around with their hotel room's locks or something like that. reply andy800 2 hours agorootparentprevPrimarily, it's about public image. It would look idiotic to host this group, regardless of intention. And it's about insurance -- logical or not, their insurer probably insisted they quit inviting DEF CON and associating, in any capacity, with self-identified hackers. reply Lammy 5 hours agorootparentprevDunno if it has anything to do with it but they did get haxx0red last year at the same time as MGM, except Caesars paid up and MGM didn't. Hotel room cards, casino play cards, etc were down for ten days at a bunch of the MGM-owned properties (a.k.a. the half of the Strip not owned by Caesars) https://en.wikipedia.org/wiki/MGM_Resorts_International#Las_... https://www.bloomberg.com/news/articles/2023-09-13/caesars-e... https://www.vox.com/technology/2023/9/15/23875113/mgm-hack-c... reply ackbar03 5 hours agorootparenthttps://www.youtube.com/watch?v=7oM7-Jsa168&t=111s reply lrvick 3 hours agorootparentprevThere are actually very few people with pentesting skills at Defcon stronger than running burp suite, and fewer still of those that are blackhats. Those with skill can do very well for themselves legally, and know better than to risk their careers getting caught messing with casino systems. In practice the biggest abuse from Defcon to the venues is in the form of a subset of people constantly defacing casino property which no one reports because no one has sympathy for casinos. My favorite trolling of casinos at Defcon is the people dumping prop money everywhere. Casinos do not -like- that and spend a lot of resources running around picking them up which is funny to watch. reply nodesocket 3 hours agorootparentNot sure I agree with the idea there are very few world class hackers there. I've watched a few of the capture the flags and almost immediately they went over my head and I felt inadequate. lol. reply prmoustache 3 hours agorootparentI'd argue that the CTF competitors are a minority in attendance (but that doesn't mean they are none at DEFCON). reply ebiester 5 hours agorootparentprevWouldn’t you think that canceling and angering that community would be an even worse idea then? reply p-e-w 4 hours agorootparentprev> the constant attacks against their infrastructure they must defend against (let's be honest, I'm sure Caesars is not defending successfully) If there's any place in the private sector where I'd expect security (including digital security) to be literally top notch, a casino would be it. And casinos don't fuck around. If they catch some \"uber haxor\" laying a finger on their networks, you can bet they'd have him arrested in a heartbeat, regardless of whether he is a conference attendee or not. reply andy800 2 hours agorootparent> I'd expect security (including digital security) to be literally top notch I know why you'd expect that, regardless, you'd be very wrong reply Rebelgecko 4 hours agorootparentprevLast year was pretty bad for digital security in Vegas reply nodesocket 4 hours agorootparentprevUmmm, they did get hacked and held for ransom (paid millions) and lost untold millions more in revenue just recently. reply NanoYohaneTSU 6 hours agoparentprevNot everything is about money or the bottom line. Sometimes it's about politics. Vegas takes a loss on so many things. Nevada has grown more and more corporate over the years. This move doesn't surprise me at all. reply jrockway 5 hours agorootparentWhat are the politics? One of the richest and most profitable industries on Earth wants to have a conference where they show slide shows to each other. Really not much different than any other conference, and probably more ethical than most of them. reply jfoutz 5 hours agorootparentprev> Sometimes it's about politics. > Nevada has grown more and more corporate over the years. You make it sound like it's entirely about money and the bottom line. I have a hard time believing gaming doesn't provide _huge_ contributions to favorable politicians. I feel like you've got something to say, and maybe something really interesting. But what you've got if awfully vague. If you've got the time or inclination, I'd definitely read an elaboration of your meaning. reply MattGaiser 5 hours agorootparentprevIs DEF CON a highly political thing? reply kortilla 4 hours agoparentprevGambling isn’t the big margin for the casino and hackers aren’t immune to gambling. Most people who gamble know the odds aren’t in their favor. reply pxeboot 4 hours agorootparentYou can view their financial statements [1]. I am sure the 'casino' category includes things besides gambling, but it looks like the largest share of their revenue. [1] https://investor.caesars.com/news-releases/news-release-deta... reply somenameforme 4 hours agorootparentBe sure to subtract expenses. So for 2022 you have 2500 for casino, 500 for food, 1500 for hotel, 800 for \"other.\" And there's definitely some counterintuitive accounting going on there, because that 2500 would imply a profit margin of 41% on casino, but Vegas regulations require gaming machines to pay out at least 75%, leaving a profit margin max of 25%. The card games and other games of skill wouldn't have such restrictions, but it seems pretty difficult to imagine that they'd be high enough margin to result an overall of 41%. reply zmgsabst 3 hours agorootparentYou seem to misunderstand the 75% rate: The requirement is that the expected value for a play on a machine is >75%. And most are >90%. But that’s not a cap on profit margin, as 25% of the expense for a play may be more than the cost of that play. Eg, having a machine that costs $1 with $0.75 expected return (and $0.25 revenue for the casino) may only cost the casino $0.10 a play — which would be a 60% profit margin. reply somenameforme 2 hours agorootparentExpected return on a machine and profit margin on that machine are literally identical. Imagine there's a hypothetical $1 machine where we simply remove variance. So you insert $1 and you get $0.75 back. It should be clear that for each $1 of revenue, the casino profits $0.25. This is a 25% profit margin. Variance can add some noise, but does not change the long-term expectation, which is what the regulations are based on. reply dmurray 2 hours agorootparentThat sounds intuitive, but that's just not how revenue is defined for a business like a casino. The casino had $0.25 revenue, and its profit is whatever is left from the $0.25 after paying for heat, light, maintenance, cashiers, security, etc. Other businesses are treated like this too. If you are a high frequency trading firm and you buy 1000 shares stock for $99.99 each and sell for $100, you didn't have $100k of revenue - you had $10, and your profit is what's left after paying for staff and computers. Yes, if your business was a supermarket, it would indeed work the other way, and it's not obvious to the literal- minded where one treatment should stop and the other should start. reply somenameforme 19 minutes agorootparentYip, I agree. I'm aware of gross gaming revenue and was involved in the industry in a past life, though obviously never filing as a casino. The thing that misled me, at a glance, was their costs - $3.5 billion. I wasn't aware there'd been massive consolidation in the casino industry, and thought I was looking at a casino's costs/revenue (in which $3.5 billion would be insane without it including losses), not a sprawling corporate enterprise. reply zmgsabst 1 hour agorootparentprevThis is similar to not counting bank deposits as revenue and withdrawals as costs. Only when your money goes to pay fees is it booked as bank revenue. The same for money transmitters like Western Union. And perhaps is more obvious when you consider what happens when there’s only players, eg, poker. The pot is held in trust, until the game ends and the losers forfeit their money to the winner. At no point does it belong to the casino. That doesn’t change when the casino is also a player. reply boomboomsubban 2 hours agorootparentprevLook at it a different way. The casino never had that dollar, you inserted a quarter and they gave you light show that cost them a cent to put on. You enjoyed it so much, you did it four times. Now the casino has your dollar and it's \"costs\" were four cents in electricity/maintenance. A much higher profit tham 25%. reply andy800 2 hours agorootparentprevYou don't understand casino accounting. Gaming WIN is revenue. If you put $100 in and get $75 out, that's $25 in marginal revenue with zero corresponding costs. The $100 is a statistic that the casino records, but it does not factor into profit calculations (total, or margin). Gaming does have expenses -- labor (mostly dealers and slot attendants & mechanics), costs of purchasing and leasing the machines, and some other miscellaneous stuff... but profit margins on pure gaming are very high (and not limited in any way by the 25% maximum hold percentage that you reference) reply xmprt 4 hours agorootparentprevWhat's the big margin for the casino if not gambling? reply quickthrower2 4 hours agorootparentprevWhat is the big margin? Rooms? reply killjoywashere 4 hours agoprevYou know, why the fuck is DEFCON in August, in Vegas? Like, you know a nice place to visit in August? Kodiak, Alaska. Portsmouth, Maine. Sydney. List of places I would never want to visit in August? Vegas. Houston. Vegas. New Orleans. Vegas. Mumbai? Maybe. Baghdad? Definitely not. Also, Vegas. My friends in Christ, why, does anyone, think Vegas is a good idea in August? reply mikeflynn 1 hour agoparentIt started there initially because a bunch of hackers wanted to hang out together and the cheapest way to do that was to all fly in the Vegas in August. It’s tradition but also still somewhat true for the reasons you articulate. reply rhinoceraptor 2 hours agoparentprevThe heat is really not that bad. I absolutely hate the heat, living in the midwest the summers are unbearable to me. Yes, it's hot, but you can still walk outside without becoming a sweaty mess because it's so dry. And you're probably not going to be walking outside very far, it's a very unfriendly place to walk outside of the prescribed separated paths on the strip. reply johnnyanmac 3 hours agoparentprevIf we wanna be frank: lotta tech is in silicon valley and Vegas is probably the closest \"large\" hub to travel to (Maybe Los Angeles is closer, but not by much). It's the cheapest option without simply staying in SV. reply toast0 3 hours agoparentprevI don't care to go to Las Vegas, and I don't care to go to DEFCON, but you can easily fly from anywhere to Las Vegas, any time of year. (Subject to US visa issues, of course) Others have said August is off-peak for Vegas (perhaps because of the weather), which means its a good time for a conference as space should be less expensive. reply raldi 1 hour agoparentprevCheck out https://www.flightsfrom.com/explorer/LAS — particularly comparing its direct flights from all over the continental US to the same for other American cities. reply asmor 1 hour agorootparentThat settles it, DEF CON in Dubai, London or Amsterdam. I vote for Amsterdam. Frankfurt also has the most international destinations (just not volume). (Probably not Dubai, considering a few speakers would be thrown out at the border - or worse if they get though. It's also artificially inflated because it's almost all transit traffic). https://en.wikipedia.org/wiki/List_of_busiest_airports_by_in... reply wodenokoto 45 minutes agorootparent> It's also artificially inflated because it's almost all transit traffic Dubai is a center for large conferences and Expos. The row of High rise hotels along Sheik Zayed Road across from Dubai World Trade Center (the largest exhibition hall in Dubai) is astounding. Gitex, Gulfood and Arab Health are all conference that are largest in their class world wide. And while A lot of DXBs traffic is transfers, the city does see 15 million international visitors a year, putting it in the top 5 most visited cities. They can easily accommodate Def Con. There’s a lot to criticize Dubai for, but they literally built the city to be a center for international conferences. reply Onewildgamer 3 hours agoparentprevMumbai will be raining buckets in August, I'd avoid that city like a plague reply prmoustache 3 hours agoparentprevWhy Vegas in the first place really. This city should not even exist. reply euroderf 2 hours agorootparent\"Follow the money.\" reply TulliusCicero 1 hour agoparentprevVegas is probably cheap in August, both for the con to reserve space and also for the attendees to get hotels. reply masteruvpuppetz 2 hours agoparentprevDef Con in Dubai.. in August.. that'll be fun :D reply wkat4242 3 hours agoparentprevSydney is pretty cold in August. Definitely not the time of year to be there. reply rekoil 2 hours agorootparentPretty sure that was the entire point. reply ramraj07 3 hours agoparentprevI can stay in an acceptable room for two digit dollars a night in Vegas. That’s not true even in Mumbai. Cheap flights too. reply fragmede 1 hour agoparentprevHow about Denver? reply Eji1700 3 hours agoparentprevWe also believe in constant air conditioning unlike the East coast and defcon is probably not the group walking around outside the hotels much. The heat sucks but it’s not like it’s that hard to avoid on a conference trip. It’s when you live here and have to hop in your plasma generating car that makes you wonder what the fuck is wrong with you reply apapapa 1 hour agoparentprevI would rather avoid Vegas all year long. reply huytersd 2 hours agoparentprevThat’s the stupidest thing I’ve heard. It’s nice and hot in Vegas in August. Alaska? At best it’s fucking 50F, that’s deeply uncomfortable. Walking around in that feels like I’m dying inside. Also, it’s a goddamn convention not a business meeting. People want to drink, watch some shows, gamble a little bit, walk around on the strip. Have a good time in general. What the fuck are you gonna do in Alaska? reply busterarm 3 hours agoparentprevVegas is great in August. It might be super hot but it's also dry. Whenever I go out to DEF CON, I take a day to go out quadding around the desert and shoot some guns outdoors. The whole damn strip is air conditioned and misted so it's not really a problem. A few years back I participated in a scavenger hunt during DEF CON and it was taxing but I would do it again. New Orleans is hell on earth that time of year though -- never again. reply ChatGTP 1 hour agoparentprevPlease, for the sake of your own health, calm down. reply dash488 3 hours agoprevAfter the impact of the MGM hack this year Cesars probably revisited their insurance on getting compromised. After the auditors and lawyers looked at all the risks they came across DEF CON and said no because of the wording of how DEF CON is marketed. Their choice was probably to drop them or loose coverage. DEF CON is listed as a \"hacker convention held annually in Las Vegas, Nevada.\" where Blackhat is \"Black Hat is an internationally recognized cybersecurity event series providing the most technical and relevant information security...\" I imagine places like the convention center cant afford or care about insurance at this level. reply technick 1 hour agoparentCaesars was hacked by the same attackers that pwned Okta, and used the stolen keys and tokens to get into Caesars. It was nothing carried out by Defcon in any way. Anyone that takes this scene seriously knows Defcon is the place to be. Blackhat is a overpriced vendor circle jerk. The only way to make Blackhat relevant again is to kick out all of the vendors and if you can't do that, forbid them from collecting peoples information. This is going to be my 11th year at Defcon this year. I snuck into a couple of blackhats and didn't get any value from them. I've been around the block a few times. reply wkat4242 3 hours agoparentprevBlack hat is just one giant bunch of sales pitches. No I haven't been there but I've had to sift through recordings that my boss (who did attend) wanted me to look at because he was too drunk himself to do a proper evaluation. It doesn't provide information, it just provides sales suits a chance to blow their hot air :P If I'd ever go there it would just be an excuse to go to vegas to see DEF CON as well :P I work in security but I have no time for corporatism and sales bullshit. Edit: I know it's a bit of a hot take but I've been to so many conferences where sales goons spew all the pretty pictures and then later when we actually got our hands on the product it turned out that it couldn't do half the stuff that was promised. Or there were other weaknesses like excruciatingly bad support. I've become very cynical due to this. reply nopeYouAreWrong 1 hour agorootparentif we're going with hot takes, I've watched a lot of DefCon vids and many presenters come off as outlandish arrogant. not simply smug, more \"I am levitating above the normies.\" reply mmaunder 3 hours agoprevBomb threat last year caused evacuation: https://www.theregister.com/2023/08/14/def_con_bomb_scare/ In 2018 we had aggressive room searches post the Vegas shooting that caused a lot of friction: https://arstechnica.com/tech-policy/2018/08/security-theater... Point being that it’s been a rough ride over the last few years. Combine that with corporate events probably being far more lucrative for Caesars I.e suits drink and gamble harder than geeks - I’m not surprised by this. TBH my team and I skipped DEF CON last year and threw our own event in Banff instead because DEF CON has become quite boring with long lines and a Groundhog Day feel to it. If you’re looking for a proper con check out a local B-sides or a smaller legit con like Shmoocon. reply tkems 6 hours agoprevI find this strange but not surprising. I've heard of speed bumps in the past related to 'hackers in town' and I wouldn't be surprised if it comes out later that it had something to do with it, even if unfounded. I think overall, having that many 'hackers' in town makes people overly paranoid.I wonder if the ransomware incident last year played a role in this decision? [0] I'm guessing they wouldn't announce it for fear of boycott, but who knows.[0] https://www.cnbc.com/2023/09/14/caesars-paid-millions-in-ran... reply sircastor 5 hours agoparent>I wonder if the ransomware incident last year played a role in this decision? This makes more sense to me than the other explanations. Probably coupled with an underinformed general manager or company president. reply cududa 4 hours agorootparentThat’s probably not even tin foil hat-y. I’d wager a bet that the perpetrators of the hack had visited Cesar’s during defcon reply hipadev23 4 hours agoprevAfter last year, Caesars likely has a large insurance policy covering against ransomware attacks. That policy probably says something along the lines of \"valid as long as you don't knowingly invite tens of thousands of hackers to your property\" reply technick 1 hour agoparentWhich is stupid because Caesars breach was directly tied to Okta's breach. reply dolmen 1 hour agoparentprevBest hypothesis so far. reply helpfulclippy 5 hours agoprevI wonder if Caesars' cybersecurity insurer had an opinion about writing a policy for a casino resort that hosts something like DEFCON, especially after the MGM hack. reply orbit7 18 minutes agoprevTake it international new venue each year, maybe London? reply rekoil 16 minutes agoparentStockholm please! reply PedroBatista 6 hours agoprevNot a fan of what DEF CON has become in the last years, so I selfishly hope it somehow \"goes away\" and reborn in a more technical and actual hacker note. Too many \"security researchers\", \"staff engineers\" and people playing politics. But I suspect they will have no problem finding another venue, sponsor money has been flowing quite well, so I wish them well. reply verandaguy 4 hours agoparentI don't have a ton of love for politicking, but security researchers and staff engineers, a lot of the time, are people who either have a career in a really interesting area in infosec and can bring a lot to the table as teachers/presenters, or people who want to get into that area and who'd benefit massively from a place like DEF CON considering how accessible its talks, demos, and villages are to people of all skill levels. Socialising, learning hacking history, and getting to know the traditions is always a great side effect that the DC crowd's been good at passing on to new generations. Goons still give people shit for misbehaving, speakers still take shots, TOOOL still has some of the best workshops and tutorials on the conference floor and usually has some people who'll talk about breaking open Medecos or Fichets to anyone who'll listen. I'd venture to say it's against the spirit of the con to try and gatekeep it. Having said all that (and the irony not being lost on me) -- linecon's definitely getting worse, and I'm worried that DC's becoming a victim of its own success, with its accessible pricing and subject matter being counterbalanced by having to manage a 20-30k person crowd. I don't have a solution for this outside of decentralization, but I don't know if that's a good solution. reply tptacek 6 hours agoparentprevThere are dozens of other conferences that do anything else you want a security conference to do. The point of Defcon at this point is to be the giant annual social event. reply rockskon 5 hours agorootparentWell it's not going to do so well for that at the LVCC given the distance away from hotel parties. reply tptacek 4 hours agorootparentIt was famously at Alexis Park for years, which might as well be the moon if you're at Caesar's. reply busterarm 4 hours agorootparentAnd when it was at AP maybe 5000 people were attending? Today it's like 25k. And AP was tolerant of people treating their property like garbage. Caesars' certainly doesn't. reply busterarm 4 hours agorootparentprevAgreed, that's a pretty terrible location given what defcon is and at its current size. reply aestetix 1 hour agoprevEarly DefCons were on the strip, and for reasons never officially made public, DefCon got banned from the strip. AFAIU, that's why it was in the Alexis Park for many years. I was actually surprised when I learned it was allowed back onto the strip. Given the high rate of incidents and the surface area for attack, I completely understand why Caesar's dropped them. What I don't get is how they allowed DefCon back onto the strip in the first place. reply kqr2 6 hours agoprevAlso DEF CON was canceled, but we uncanceled it : https://www.reddit.com/r/Defcon/comments/1aj6ixn/def_con_was... reply sva_ 5 hours agoparentSo they're making a marketing campaign out of it with 'un-canceled' t-shirts https://shop.defcon.org/ reply V-eHGsd_ 5 hours agorootparenti'm pretty sure \"defcon is cancelled\" is an _ooooold_ meme reply scoutt 34 minutes agoprev> We need a space that can handle an event our size, and configurable enough to accommodate our content. I love this sounds like a pun about loading/executing a payload. reply Kye 33 minutes agoprevNow they have another thing in common with furry conventions! (tech people, furries, nonprofit, hotels randomly cancelling contracts) reply wkat4242 3 hours agoprevToo bad. Now I will definitely never go there. My work sometimes gets free tickets for black hat but that's a totally boring business conference. Not worth going to on its own. I hate mingling with sales suits, I'm a real techie. Def con was something I would be looking forward to and if my work paid for black hat I could have stayed the extra days to go there. reply tptacek 2 hours agoparentBlack Hat is the largest North American venue for vulnerability research. It's one of the closest things in the industry to a serious, peer reviewed conference, at least at this scale. It's certainly not the social scene Defcon is (indeed, it replaces that social scene with an enterprise sales hookup scene), but it's definitely not a \"totally boring business conference\". Black Hat talks are generally much better than Defcon talks. reply abnercoimbre 3 hours agoparentprevThis is me being opportunistic, but I'm an organizer for indie tech conferences [0]. We're not selling tickets with set venue dates yet (although a fundraiser is happening.) If nothing else, you might join the newsletter to see if it's your cup of tea later in the year. [0] https://handmadecities.com reply wkat4242 2 hours agorootparentThanks! I'm very unlikely to fly to the US for conferences though (in fact I've never been there in my life!). Especially to Seattle as it's so far away from me in Europe. I'm in Spain which is pretty much a low-wage country so things like intercontinental flights and foreign hotels are prohibitively expensive. Boston might be an option if it just happens to be around a time I might visit for work (but again I've never been there in my life so it's not all that likely even though we have a major office there). reply bentley 2 hours agoparentprevDEF CON hasn’t been canceled, it’s just changed venues. reply b33j0r 5 hours agoprevHaha, would you want ESP32 hackers at your venue? Breaking into wifi and rooms to make tech points? Hosting this never made sense. reply bboygravity 4 hours agoparentI smell potential for the hotel to get (free) help to become the most cybersecure hotel in the world. reply junon 3 hours agorootparentThat's not what's happening. reply Halan 52 minutes agoprevStop making conferences in Las Vegas is the most depressing place to visit reply subcosmos 5 hours agoprevHypothesis : Result of last years bomb-scare and evacuation? reply alberth 5 hours agoparentOr Caesar's being hacked last year and a belief (right/wrong/indifferent) that they don’t want to be hosting the kind of folks who could do that. https://www.securityweek.com/caesars-confirms-ransomware-hac... reply wkat4242 3 hours agorootparentIt's really much more likely the people that are in the business of defending against just that. reply belter 5 hours agoprevNext Year's DEFCON Keynote... \"How We Hacked Caesar's Booking System\" reply technick 1 hour agoparentAbsolutely, I'm already seeing some chatter from groups that want to get \"even\" with Caesars for displacing a 25 year tradition for strategy. I hope they have some good defenses installed now that you've pissed off no less than 10k infosec people. reply caymanjim 6 hours agoprevI find it hilarious that defcon.org can't handle the traffic from being on HN. reply bdcravens 4 hours agoparentThe IP of the site reports as being a Comcast IP address. Surely this isn't hosted on some guy's home server? Even their business class service wouldn't seem like a good fit, especially for an org like Defcon. reply tptacek 4 hours agoparentprevWhy would you expect them to be that kind of resilient? It's just a conference brochure site. reply paxys 4 hours agorootparent> It's just a conference brochure site. That’s exactly why it should be resilient. A fully static text-heavy site can serve basically unlimited traffic on a free host or a $5 VPS these days. reply fragmede 1 hour agorootparentWhy would you even host it? Throw it on a CDN and make someone else deal with serving it. reply taywrobel 5 hours agoparentprevWithout robust and easily scaled infrastructure in place ahead of time, an organic DDOS is one of the most difficult situations to mitigate. Not much can be done in terms of traffic shaping, rate limiting, or bot detection. reply paxys 5 hours agorootparentAn HN front page “DDoS” is like 20K hits. This isn't some complex scaling challenge. Any website on the internet should be able to handle it, especially a purely informational one. reply o11c 5 hours agorootparentAs a reference, 10K simultaneous hits was an achievable challenge back in ... 1999. reply quickthrower2 4 hours agorootparentNow you just front it with a CDN. Easy. reply px43 2 hours agorootparentprevThis also blew up on every social media and news site as well, not just here. reply MrBruh 5 hours agorootparentprevI had my blog be on the front page for ~6-8 hours racking up 100k+ unique loads. It also managed to survive just fine on a $5 VPS so I would hope that other sites could survive. reply caymanjim 5 hours agorootparentprevI agree. Protecting against DDoS attacks is incredibly difficult. I'm just enjoying the irony of Def Con, the premiere computer security and hacking convention, not being able to handle traffic. To be fair, I don't think they crashed; I saw a \"sorry too much traffic try later\" type message. Still amuses me. reply loriverkutya 5 hours agorootparentTo me this means they decided not to handle the traffic instead of can’t handle it. reply komali2 5 hours agorootparentprevI guess it's funny, but the attendees don't necessarily represent the organizers. The best hackers in the world may be in the building during Defcon but I don't think the Defcon organization itself necessarily employs them. reply shadowgovt 5 hours agorootparentprevOf course, a robust and easily-scaled infrastructure is pretty easy to rent these days... ... if you're willing to trust another company with your data. reply colecut 4 hours agorootparentI would trust just about any company with information that I want to be available to the public reply ranger_danger 5 hours agorootparentprevthe current way to most effectively get around DDoS seems to be using a proof-of-work based frontend run on as many revolving reverse proxies around the world as you can afford. this is what kiwifarms does. seems pretty effective and a lot cheaper than what the people bankrolling the attacks on them are spending. reply datadrivenangel 5 hours agoprevSo what are the consequences for Caesars doing this? It seems like this will cause a lot of extra work and maybe damages for DEF CON's organizers. reply Spodera 6 hours agoprevForum servers are being overloaded, from DEF CON's homepage: After a great 25 year relationship Caesars abruptly terminated their contract with DEF CON, leaving us with no venue for DC 32, and just about seven months to Con! We don’t know why Caesars canceled us, they won’t say beyond it being a strategy change and it is not related to anything that DEF CON or our community has done. This kind of no-notice cancellation of a contract is unheard of in the conference business. The parting is confusing, but amicable. TL;DR - DEF CON 32 will still be August 8-11 2024, but now held at the Las Vegas Convention Center (LVCC) with workshops and training at the Sahara. reply kqr2 6 hours agoparentWhat happens to all the previous Caesar hotel bookings? Have they negotiated with other hotels for a conference rate? reply Spodera 6 hours agorootparentIf you already have a reservation at a Caesars property, from what I saw you can keep it, you'll just have to find transportation to LVCC via the monorail or other means. Not sure on transfers. They have negotiated with Sahara on a rate and are looking to add more. reply kstrauser 4 hours agorootparentThat “just” is doing some heavy lifting. It was a minor hassle running back and forth from the Forum to Flamingo to pick stuff up and drop it off. Commuting from strip hotels to LVCC is going to be a pain in the ass. reply dylan604 2 hours agorootparentAnd yet every year the attendees at other large conventions like NAB have no issues with this. reply MarkSweep 5 hours agoprev> a proper food court Looking on the bright side, having a better option for a snack or meal on site would be nice. It was slim pickings in 2023. reply spacebacon 4 hours agoprevIt’s a honey pot. Antagonize “the world’s best hackers” to test their post MGM/Ceasars attack security posture. Someone probably convinced them their new fancy XDR is hacker proof and they are playing for skins now. reply nickchuck 3 hours agoprevI bet it was the googly eyes reply xena 5 hours agoprevReminds me that I need to make my DEF CON travel plans. Thanks for the reminder! reply throwawaaarrgh 6 hours agoprevChrist I'm old. I still remember when it was at the AP. Although I've left that part of my life behind, I'm glad DEF CON is still around. reply j0hnyl 5 hours agoparentIt went a bit downhill after the AP days imo. reply fragmede 1 hour agoparentprevRivera reply tptacek 6 hours agoparentprevThe last Defcon I went to was at the Aladdin. reply justinzollars 3 hours agoprevCaesars canceled DEF CON could it be because CaesarsPalace was hacked this year? I doubt they did it? reply mynameisnoone 3 hours agoprevI haven't been since DEF CON 19. Count me out. LVCC is even less cool than Caesars and it's a mile from the strip. It's only selling point is the Loop.[0] In the past, it was convenient to book at Caesars, or nearby at the Bellagio or Venetian-Palazzo. 0. https://en.wikipedia.org/wiki/Las_Vegas_Convention_Center_Lo... reply hartator 3 hours agoprevTo be honest, requiring an ID and a proof of vaccination to attend was against the anonymity principle of the convention in the first place. reply px43 2 hours agoparentIn 2021 I paid in cash, showed my physical ID and vax card, and was on my way with my badge. Nothing about the exchange was recorded. They take privacy incredibly seriously, especially the goons working registration. There was also basically no COVID that year, despite DEF CON happening right during the Delta outbreak. A big nursing conference the week before got hit hard with Delta, but DEF CON took that shit seriously, and it worked well. Pretty sure there was no vax check in 2022 and 2023, and I do know some people who got COVID in those years, but people who took decent precautions were generally able to dodge it. reply exogeny 5 hours agoprevShould have played more blackjack, nerdos! reply nodesocket 5 hours agoparentSir you have 20... Hit me! reply p-e-w 6 hours agoprev> This kind of no-notice cancellation of a contract is unheard of in the conference business. That's a big claim to make. Can someone with relevant experience confirm whether this is true? reply tptacek 5 hours agoparentI mean, they've been running this event, which has topped 20k attendees, for something like 30 years. So one entity with the relevant experience is... them? reply dclowd9901 6 hours agoparentprevI don’t think it’s that bold of a claim. Organizers ask attendees to spend a lot of money to attend, buy lodging and everything else, not to mention pay a lot to organize and it takes a lot of time and effort to line up all the required facilities. I’ve been part of organizing at least a few big events and if we had a late stage hard cancellation, there probably would’ve been lawsuits. reply caymanjim 6 hours agoparentprevIt's seven months away. It's not like it's seven weeks. reply QuinnyPig 6 hours agorootparentFor a conference at this scale, there's not a huge difference between seven weeks and seven months. reply lmm 5 hours agorootparentThere's a huge difference even at this scale. Seven months was apparently long enough for them to make arrangements to hold it at another venue nearby (I'm sure rearranging it is/will be a lot of hard work, but they're doing it); would seven weeks have been? reply camdenlock 4 hours agoprevnext [3 more] [flagged] DaSHacka 4 hours agoparentWhy are those mutually exclusive? reply DonHopkins 1 hour agorootparentHe's asking because he wants to sexually harass and assault women, and he wants to know if there's an official policy against it. reply quickthrower2 4 hours agoprevLemonade! reply brcmthrowaway 5 hours agoprev [–] I'm surprised any hacker of any standing would want to give the corrupt state of Nevada any tax money whatsoever. reply elicksaur 4 hours agoparent [–] This kind of comment on HN is always fascinating. Is it just simple trolling? Is it cosplay? Is it someone new (or old) to “hacking” sincerely trying to “no true scotsman” attending def con? reply wkat4242 3 hours agorootparent [–] I have to say as a European the choice of location has always puzzled me. Corporate interests basically run las vegas, I find it a really odd choice for such a free-thinking anti-establishment community. reply px43 2 hours agorootparent [–] DEF CON started as a bunch of teenagers running away from home to get ridiculously drunk and trash some place, and it hasn't changed much. Vegas was the perfect choice. It was a party. Getting to smash up an obscene capitalist hellhole was a key perk, and still is to this day. reply wkat4242 2 hours agorootparent [–] Ahh I see. This way it makes a lot more sense, thanks for the explanation :) I go to a lot of European hacker parties/camps and I can certainly recognise the mindset you mention (and I identify with that mindset as well even though I work in a corporate job). For this reason Las Vegas made no sense to me but in light of your comment it does now. And yeah getting ridiculously drunk is definitely part of the experience :D reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "DEF CON 32, a popular hacking conference, was initially canceled due to the termination of their venue contract but has now been relocated to the Las Vegas Convention Center.",
      "The conference will take place on August 8-11, 2024, and will include new features such as a larger space, a food court, and a large indoor venue LCD wall.",
      "Organizers have set up a live FAQ section on the DEF CON 32 forum to address any questions, and attendees can also purchase event-related shirts and stickers."
    ],
    "commentSummary": [
      "The DEF CON hacking conference, which was initially cancelled by Caesars Entertainment, has found a new location at the Las Vegas Convention Center.",
      "Speculations for the cancellation center around low revenue and a preference for guests that generate higher revenue at the hotel.",
      "Discussions at DEF CON include topics such as card counting in casinos, security concerns related to hackers, and recent hacking incidents. There are also debates about casino profitability, regulations, and the conference's location and atmosphere."
    ],
    "points": 365,
    "commentCount": 218,
    "retryCount": 0,
    "time": 1707103327
  },
  {
    "id": 39251909,
    "title": "Analyzing a Small Language Model's Token Prediction Beyond Self-Attention",
    "originLink": "https://shyam.blog/posts/beyond-self-attention/",
    "originBody": "Beyond Self-Attention: How a Small Language Model Predicts the Next Token Jan 29, 2024 · 17754 words · 84 minute read I trained a small (~10 million parameter) transformer following Andrej Karpathy’s excellent tutorial, Let’s build GPT: from scratch, in code, spelled out. After getting it working, I wanted to understand, as deeply as possible, what it was doing internally and how it produced its results. The original paper, as well every transformer tutorial I found, focuses primarily on multi-head self-attention, the mechanism by which transformers learn multiple relationships between tokens without relying on recurrences or convolution. But none of the papers or tutorials I encountered give a satisfying explanation of what happens after attention: how exactly do the results of the attention computation turn into accurate predictions for the next token? I thought I could run a few example prompts through the small but working transformer I’d trained, examine the internal states, and figure this out. What I thought would be a quick investigation turned out to be a 6-month deep dive, but yielded some results I think are worth sharing. Specifically, I have a working theory that explains how the transformer produces its predictions and some empirical evidence that suggests this explanation is at least plausible. For those readers familiar with transformers and eager for the punchline, here it is: Each transformer block (containing a multi-head self-attention layer and feed-forward network) learns weights that associate a given prompt with a class of strings found in the training corpus. The distribution of tokens that follow those strings in the training corpus is, approximately, what the block outputs as its predictions for the next token. Each block may associate the same prompt with a different class of training corpus strings, resulting in a different distribution of next tokens and thus different predictions. The final transformer output is a linear combination of each block’s predictions. I implemented imperative code that does what I’m proposing the transformer is doing. It produces outputs very similar to the transformer, which I’ll review in detail in a later section. In this post, I’m going to briefly introduce the model and training data, demo some evidence for my proposed explanation, give a detailed walkthrough of the imperative code implementation of it, and present the supporting evidence I have for my theory. I’ve tried to keep the main narrative succinct, with links to relevant technical details and justifications in the appendices or other notebooks in the repo. This project is my first foray into this type of open-ended ML research. I’m sure I have made errors or omissions that would be obvious to more experienced researchers. I welcome any feedback on this work at shyam.pather at gmail dot com. The Model and Setup 🔗 Disclaimer 🔗 I want to start by saying upfront: the code for the model I trained isn’t mine. It came from Andrej Karpathy’s video, Let’s build GPT: from scratch, in code, spelled out (highly recommend). I typed in the code by copying what I saw on the screen as I watched the video. For things that weren’t clear onscreen, I referenced the GitHub repo for the video and the nanoGPT repo. After getting it working, I made only minor changes to make it work with the rest of the code in/structure of my repository, resulting in this implementation. In summary: the core language model is Andrej Karpathy’s work, not mine. The analysis and all the supporting code behind it are my original contributions. I’ll acknowledge and cite influential papers, posts, tutorials, and other resources in the relevant places. Model Overview 🔗 The model is a 6-block, decoder-only transformer: Drawing code for architecture diagram It’s trained on the TinyShakespeare data set which contains 40,000 lines of Shakespeare’s plays. After about an hour of training on an RTX 4000 GPU, it is able to produce reasonable-looking faux Shakespeare. Given a prompt, the model predicts tokens that it thinks should follow. Let’s look at an example: starting with the prompt, ROMEO:, and sampling 500 tokens from the model’s predictions, we get: Code to spin up model and generate example output from prompt, 'ROMEO:' ROMEO: If thou wilt triumphant be virtue, and since from any bold virtue that is made a bawd of earth, then the duke desires of patience and perish: take up the other husband, dislike his tent back. First Citizen: Ourself goes, go back: you have no consul, but the disguised gods. Second Citizen: We choose him in the world, he did runk itself. First Citizen: Sir, I am I a man changed him and thriving, I have heard the king. CORIOLANUS: Consider him! AUFIDIUS: Most gracious irice, and you must danc It’s not Shakespeare but structurally, it’s plausible Shakespeare. It looks like the script for a play, the language sounds archaic, the character names/titles come from real Shakespeare plays. Most of the words are English words. Punctuation and capitalization are mostly sensible. Clearly, none of the text actually makes sense, but still, it’s not bad for an hour of training. The tokens in the model are characters, not words. Given a prompt, the model predicts a probability distribution for the next character. For example, given the prompt 'my most gr, the model predicts these probabilities for the next token: Code to display probabilities for next token after `my most gr` 'a' 0.819 'e' 0.081 'i' 0.059 'o' 0.036 'u' 0.004 'y' 0.001 'w' 0.000 'r' 0.000 'g' 0.000 's' 0.000 Appendix I provides a few more details about the model. Beyond that, if you want to know more, the code and Andrej’s video are the best resources. Transformer Block Structure 🔗 Each of the 6 blocks in the architecture diagram above contains two significant sub-components: a multi-head self-attention layer and a feed-forward network, wired together via a mix of direct and residual connections as follows: Drawing code for block internals architecture diagram The Block module implements this wiring in PyTorch: class Block(nn.Module): \"\"\"One transformer block\"\"\" def __init__(self, n_embed, n_head): super().__init__() head_size = n_embed // n_head self.sa = MultiHeadAttention(n_head, head_size) self.ffwd = FeedForward(n_embed) self.ln1 = nn.LayerNorm(n_embed) self.ln2 = nn.LayerNorm(n_embed) def forward(self, x): x = x + self.sa(self.ln1(x)) # The `x +` part is a skip connection x = x + self.ffwd(self.ln2(x)) # The `x +` part is a skip connection return x While many words have been written and spoken about multi-head attention, comparatively little has been said about the feed-forward network because, it seems, comparatively little is known: Screenshot from https://stats.stackexchange.com/q/485910 I started this investigation wondering what comes after attention. Literally, the feed-forward network does. In the transformer I studied, across all 6 blocks, the feed-forward networks comprise over 65% of the total trainable parameters, so they must play some important role. As I’ll show later, it turns out that the output of the feed-forward network is the primary factor that determines how a block transforms its input into its output. Demo: My Proposal In Action 🔗 In this section, I’m going to show an example that illustrates what I’m proposing the transformer is doing. In the next section, I’ll go into detail about how this is implemented. Imagine we did the following: Ran the prompt, 'And only l', through the model and extracted the output value of the feed-forward network in the first transformer block. Went back to the training corpus, found all substrings of the same length as our prompt (10-characters), ran all of them through the model, and filtered out just the ones whose feed-forward network outputs in the first block have a cosine similarity of 0.95 or greater when compared to that of the prompt, 'And only l'. We’d come up with this set of strings: Helper function to print results in a table Code to generate similar strings (will be explained later) 'hat only l' 's sickly l' ' asthey l' 'r kingly l' 're; they l' 'eby they l' 'ar, they l' 'im, only l' 'ling any l' 'life may l' 'nobility l' 'eBy any l' ' as they l' ', if any l' ' hastily l' 'tly they l' ' ghastly l' 'My only l' 'For many l' 'r in any l' ' till my l' 'all they l' 'hen they l' 'at Henry l' 'oolishly l' 'er:They l' 'may they l' 'or stony l' 'ur Henry l' 'l gladly l' 'yet they l' 'y;Delay l' 'e, on my l' 'or Henry l' 'I dearly l' ' if they l' ' she may l' 'tfairly l' 'ould say l' 'd all my l' 'her they l' ' Stanley l' ' and may l' 'uld they l' 'u all my l' 'friendly l' 'h gently l' 'e deadly l' 'f all my l' 'n all my l' 'Ere they l' 'steel my l' ' tell my l' 'e kingly l' 'learn my l' 'd he say l' 't basely l' 'Thursday l' 'iciously l' \" 'if any l\" ' as many l' 'hy glory l' 'not very l' 'a goodly l' 'e surely l' 'quiously l' ', fairly l' 'lord! my l' 'entle my l' ', he may l' 'our holy l' ' worldly l' ' my only l' ' all, my l' 'ul, they l' 'o lately l' 's in any l' ' no lady l' 'ter many l' 'Our holy l' 't vainly l' 'eA lady l' ' you may l' 'y greedy l' 'untimely l' 'directly l' 'er on my l' 'e wistly l' 'ng Henry l' 'And only l' 's kindly l' 'KE:They l' ' of many l' 'o, on my l' There’s a clear pattern across these: they all end in y l and several of them end in ly l. Similarity in the space of feed-forward network outputs seems to correspond to human-interpretable patterns. Next, imagine we went back to the training corpus, found each of these strings and built a distribution of all the characters that came after them. We’d find, for example: 'hat only l' is followed by i (“That only like a gulf it did remain”) 'l gladly l' is followed by e (“I’ll gladly learn.”) 'n all my l' is followed by both a and i (“In all my lands and leases whatsoever” and “never saw you before in all my life”) Doing this for the complete set of 94 strings, we’d end up with this distribution: Helper function to plot probability distribution for tokens Code to produce distribution from tokens that follow similar strings The various tokens in our model’s vocabulary appear on the x-axis and the normalized frequency of occurrence on the y-axis. This plot shows that i was the most frequent, then o, then a, and finally, e. Now let’s look at the final output of the transformer as a whole when given And only l as a prompt: Code to produce model predictions This is a probability distribution representing the model’s predictions for the next token. Notice that it’s strikingly similar to the normalized frequency distribution shown in the previous plot! We can quantify how similar they are. Hellinger distance is a measure of overlap between probability distributions. Given distributions 𝑃 P and 𝑄 Q, the Hellinger distance between them is: 𝐻 ( 𝑃 , 𝑄 ) = 1 2 ∑ 𝑖 = 1 𝑛 ( 𝑝 𝑖 − 𝑞 𝑖 ) 2 H(P,Q)=21i=1∑n(pi−qi)2 Or, in code: def hellinger_distance( p: torch.Tensor, q: torch.Tensor, ): return ((p.sqrt() - q.sqrt())**2).sum(dim=-1).sqrt() / math.sqrt(2) Hellinger distance of 0 means the two distributions are identical and 1 means they have no overlap. The Hellinger distance between the two distributions above - the distribution formed from the tokens that follow the strings with similar feed-forward network outputs and the distribution the model predicts - is 0.07: very nearly identical. For the sake of keeping the demo brief, I chose an example where the first block’s similar strings alone are enough to produce a distribution that closely matches the final output of the transformer. Typically, we’d need to need to do the same exercise - finding the strings in the training corpus that produce similar feed-forward network outputs to the prompt and building a distribution from the tokens that succeed them - for all 6 transformer blocks, and then calculate a weighted sum of the resulting distributions in order to get a good match. We’ll do that in the next section and see that across a sample of 20,000 prompts, the average Hellinger distance between distributions computed this way and the corresponding transformer output was just 0.17. This small average Hellinger distances suggests the results produced by this approach are a good approximation for the transformer’s outputs. In addition, as I’ll explain in the interpretation section, I think the approach itself is a reasonable approximation of what the transformer is actually doing. Implementation: Approximating the Transformer Output with Feed-forward Network Outputs 🔗 In this section, I’m going to walk through in some detail and with code, the exact procedure I used to approximate the transformer’s output using strings that produced similar feed-forward network outputs. If you’re not interested in the implementation, skip this section and proceed to the evaluation section. To recap, this is the procedure to compute the approximation: Run a prompt through the model and save the feed-forward network outputs for each block. For each block: Find the strings in the training corpus that produce the most similar feed-forward network outputs to the prompt for that block. For each string found, build a frequency distribution of the tokens that come after it in the training corpus. Sum the frequency distributions for all strings found for the current block. Compute a weighted sum of the frequency distributions for each block computed in the previous step. Normalize the weighted sum to get a probability distribution. Procedure Setup 🔗 The first step of the procedure - running a prompt through the model and saving the feed-forward network outputs for each block - is straightforward to accomplish with some basic PyTorch hooks. But the first part of step two - finding the strings in the training corpus that produce similar feed-forward network outputs - requires some additional machinery to do efficiently. I did all the analysis with length 10 strings for compute and storage efficiency (but I also observed that the results hold for both shorter and longer strings). The 1,115,394-character long training corpus contains 858,923 unique, length 10 substrings. Each feed-forward network output is a 384-dimensional vector of float32 values and the model produces 6 of them (one for each block). Comparing the 6 384-dimensional feed-forward outputs for any prompt to 6 * 858,923 = 5,153,538 feed-forward outputs from all the other strings takes a long time. To able to work with this data, I had to pre-compute things. I built the following pipeline: I chose 20,000 length 10 strings from the training corpus at random to use as prompts in this experiment. Overnight, I ran a process to compute the cosine similarity between the feed-forward network outputs the model produced for the 20,000 prompts and those it produced for the 858,923 unique length 10 substrings of the training corpus. I did this in batches and saved the results to disk. Even after pre-computing the cosine similarity results, searching through all of them to find the closest matches took a long time. Experiments showed matches of interest never had a cosine similarity below 0.7, so I ran another step to pre-filter the results of step 2 to just those entries with cosine similarity >= 0.7. This greatly reduced the number of entries to search through. The code for this pre-computation and pre-filtering is too much to include in this post, but the implementation is available in the cosine-sims experiment notebook. Procedure Walkthrough 🔗 In this section, we’ll build up the code step by step and run it on one prompt at a time and for just one block. Over the following sections, we’ll extend it to additional blocks, run it across a large number of prompts, and examine the results. First, we need to grab 20,000 length 10 strings from the training corpus to use as prompts: # Get all the unique substrings in the text strings10 = all_unique_substrings(text=ts.text, substring_length=10) n_prompts = 20000 torch.manual_seed(1337) indices = torch.randperm(len(strings10))[:n_prompts] prompts = [strings10[i.item()] for i in indices] As described in the Procedure Setup section, I previously ran all these strings through the model, grabbed the feed-forward network outputs for each block, and pre-computed the cosine similarities to all the unique length 10 substrings in the training corpus. And then I pre-filtered the results to just those with cosine similarity >= 0.7. The the cosine-sims experiment notebook that implements all this also exports a helper function, filter_on_prefiltered_results(), that we can use to find the most similar strings to a given prompt by searching over the pre-filtered results. If you’re curious about how this works, check out the notebook. It’s pretty straightforward and the unit test provides a simple example that illustrates the shape of the inputs and outputs. To use filter_on_prefiltered_results(), we just need to tell it how to find the prefiltered files: prefiltered_threshold=0.7 prefiltered_results_folder = environment.data_root / 'cosine_sim_results/large_files/slen10' / f'prefiltered_{prefiltered_threshold}' def prefiltered_filename(block_idx: int, q_idx: int) -> Path: return prefiltered_results_folder / f'cosine_sim_ffwd_out_{q_idx:05d}_{block_idx:02d}.pt' def load_prefiltered_data(block_idx: int, q_idx: int): return torch.load(prefiltered_filename(block_idx, q_idx)) Note on the use of q_idx here and in the rest of the code: q_idx refers to “query index”. The job that pre-computes all the cosine similarities takes a set of “queries” or values to compare to. These queries are the feed-forward network outputs the model produces for the prompts. There is a 1:1 correspondence between queries and prompts and so I’ve used the terms interchangeably in the code. To start, we’ll use the same prompt - 'And only l' - we used in the earlier demo. It happens to be the prompt at index 57: prompts[57] 'And only l' We’ll find the strings whose feed-forward network outputs in block 0 had a cosine similarity of 0.95 or greater when compared to the block 0 feed forward network output of the prompt. block_idx = 0 similarity_threshold=0.95 q_idx = 57 similar_indices = filter_on_prefiltered_results( load_prefiltered=lambda q_idx: load_prefiltered_data(block_idx, q_idx), q_idx_start=q_idx, q_idx_end=q_idx+1, filter_fn=lambda values: values > similarity_threshold ) similar_strings = [ [strings10[i] for i in indices] for indices in similar_indices ] len(similar_strings[0]) 94 This produced the 94 similar strings we saw in the demo. We can print them again to be sure: print(f\"Original string: {repr(prompts[q_idx])}\") print(\"Similar strings: \") data_columns=[ [repr(s) for s in similar_strings[0][i : i + 20]] for i in range(0, len(similar_strings[0]), 20) ] print(text_table( headers=[], data_columns=data_columns, col_widths=[18 for _ in data_columns] )) Original string: 'And only l' Similar strings: 'hat only l' 's sickly l' ' asthey l' 'r kingly l' 're; they l' 'eby they l' 'ar, they l' 'im, only l' 'ling any l' 'life may l' 'nobility l' 'eBy any l' ' as they l' ', if any l' ' hastily l' 'tly they l' ' ghastly l' 'My only l' 'For many l' 'r in any l' ' till my l' 'all they l' 'hen they l' 'at Henry l' 'oolishly l' 'er:They l' 'may they l' 'or stony l' 'ur Henry l' 'l gladly l' 'yet they l' 'y;Delay l' 'e, on my l' 'or Henry l' 'I dearly l' ' if they l' ' she may l' 'tfairly l' 'ould say l' 'd all my l' 'her they l' ' Stanley l' ' and may l' 'uld they l' 'u all my l' 'friendly l' 'h gently l' 'e deadly l' 'f all my l' 'n all my l' 'Ere they l' 'steel my l' ' tell my l' 'e kingly l' 'learn my l' 'd he say l' 't basely l' 'Thursday l' 'iciously l' \" 'if any l\" ' as many l' 'hy glory l' 'not very l' 'a goodly l' 'e surely l' 'quiously l' ', fairly l' 'lord! my l' 'entle my l' ', he may l' 'our holy l' ' worldly l' ' my only l' ' all, my l' 'ul, they l' 'o lately l' 's in any l' ' no lady l' 'ter many l' 'Our holy l' 't vainly l' 'eA lady l' ' you may l' 'y greedy l' 'untimely l' 'directly l' 'er on my l' 'e wistly l' 'ng Henry l' 'And only l' 's kindly l' 'KE:They l' ' of many l' 'o, on my l' Next, we’ll need to build a frequency distribution for the tokens that came after these strings in the text. To make this easy and efficient (we’ll eventually be doing many times), we can pre-compute the next token frequency distributions for all the unique length 10 substrings in the training corpus. The helper function build_next_token_map(), implemented in the text-analysis module, does this. next_token_map10 = build_next_token_map( text=ts.text, prefix_len=10, vocab_size=tokenizer.vocab_size, stoi=tokenizer.stoi ) The return value stored in next_token_map10 is a dictionary that maps each unique length 10 substring in the training corpus to a frequency distribution of the tokens that come after it. Conceptually, it looks something like this: { 'the common': { ' ': 12, \"'\": 1, ',': 1, '?': 1, 'a': 1, 's': 5, 'w': 3 }, ' the gods ': { 'b': 1, 'c': 1, 'd': 2, 'f': 1, 'g': 1, 'h': 2, 'k': 2, 's': 2, 't': 1, 'w': 2 }, ' authority': { '': 1, ' ': 5, ',': 5, ':': 2, ';': 1 }, ... } In reality, the values are actually tensors of shape (vocab_size,) where vocab_size is the number of unique tokens the vocabulary (65, in our case). The item at index i in the tensor is the count of occurrences of the ith token after the string in that entry’s key. So it looks more like: { 'the common': torch.tensor([ 0, 12, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 3, 0, 0, 0 ]), ' the gods ': torch.tensor([ 0, 12, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 3, 0, 0, 0 ]), ' authority': torch.tensor([ 0, 12, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 3, 0, 0, 0 ]), ... } Next, we need to sum the frequency distributions for all the strings we found to have similar feed-forward network outputs to our prompt. Because next_token_map10 stores the individual frequency distributions as tensors, this is easy to accomplish: total_freq_distribution = torch.stack([ next_token_map10[string] for string in similar_strings[0] ]).sum(dim=0) We stack up the distributions for each similar string into a single tensor and then sum across all of them. We can now turn this into a probability distribution by dividing each entry by the sum of all the entries: prob_distribution = total_freq_distribution / total_freq_distribution.sum() Finally, we can visualize this distribution: plot_prob_distribution_for_tokens(prob_distribution, title='Probability distribution using only block 0 similar strings') It’s the same distribution we saw in the demo. Now let’s code the comparison to the model output: tokens = encoding_helpers.tokenize_string(prompts[q_idx]) logits, _ = m(tokens) logits = LogitsWrapper(logits.detach(), tokenizer) logits.plot_probs(title='Probability distribution from model') Again, the two distributions look very similar, and in this example, the approximation uses only values from the first block. To better compare them, we can look at the distributions in text form: Helper function to print comparison of distributions as a table approx_top_tokens = top_nonzero_tokens(prob_distribution, tokenizer.itos) model_top_tokens = logits.topk_tokens(k=10)[0][-1] print_distribution_comparison(approx_top_tokens, model_top_tokens) Model Predictions Approximation Predictions ----------------- ------------------------- i: 0.437 i: 0.389 o: 0.204 o: 0.250 a: 0.195 a: 0.222 e: 0.160 e: 0.139 Finally, we can also compare the Hellinger distance between these distributions: hellinger_distance(prob_distribution, logits.probs()[0][-1]) tensor(0.0711) By combining the next token frequency distributions of the similar strings from just the first layer of the model, we are able to pretty closely approximate the output of the transformer. Of course, I chose an example that works particularly well. Here’s an example where the frequency distribution from just the first layer doesn’t work well: q_idx=40 prompts[q_idx] 'hing tremb' Using the same method, we can identify 57 strings from the training corpus that produce similar feed-forward network outputs to the prompt: block_idx = 0 similarity_threshold=0.95 similar_indices = filter_on_prefiltered_results( load_prefiltered=lambda q_idx: load_prefiltered_data(block_idx, q_idx), q_idx_start=q_idx, q_idx_end=q_idx+1, filter_fn=lambda values: values > similarity_threshold ) similar_strings = [ [strings10[i] for i in indices] for indices in similar_indices ] len(similar_strings[0]) 57 We can look up, sum, and normalize the frequency distributions of tokens that follow these strings in the training corpus, and compare the result to the model outputs, as we did before: total_freq_distribution = torch.stack([ next_token_map10[string] for string in similar_strings[0] ]).sum(dim=0) prob_distribution = total_freq_distribution / total_freq_distribution.sum() approx_top_tokens = top_nonzero_tokens(prob_distribution, tokenizer.itos) tokens = encoding_helpers.tokenize_string(prompts[q_idx]) logits, _ = m(tokens) logits = LogitsWrapper(logits.detach(), tokenizer) model_top_tokens = logits.topk_tokens(k=10)[0][-1] print_distribution_comparison(approx_top_tokens, model_top_tokens) Model Predictions Approximation Predictions ----------------- ------------------------- l: 0.999 e: 0.543 e: 0.000 l: 0.343 r: 0.000 r: 0.114 Unlike the previous example, these distributions are quite different. The top 3 tokens are the same in each, but they’re in the wrong order and their probabilities are far apart. These differences contribute to a large Hellinger distance: tokens = encoding_helpers.tokenize_string(prompts[q_idx]) logits, _ = m(tokens) logits = LogitsWrapper(logits.detach(), tokenizer) hellinger_distance(prob_distribution, logits.probs()[0][-1]) tensor(0.6305) For the prompt, 'hing tremb', just using the values from the first block results in a poor approximation of the transformer’s output. We’ll soon add the contributions from other blocks and when we do, we’ll get the Hellinger distance between the approximation and the real transformer output for this prompt down from 0.63 to just 0.02. Similarity Thresholds 🔗 In the preceding examples, I used a similarity threshold of 0.95: I searched for strings whose feed-forward network outputs in block 0 produced values with a cosine similarity of 0.95 or greater when compared to the feed-forward network output of the prompt. A different threshold would have yielded different results. For example, doing the same exercise for prompt id 57 ('And only l') with a threshold of 0.90 finds 612 similar strings, vs the 94 we had before: block_idx = 0 similarity_threshold=0.90 q_idx = 57 similar_indices = filter_on_prefiltered_results( load_prefiltered=lambda q_idx: load_prefiltered_data(block_idx, q_idx), q_idx_start=q_idx, q_idx_end=q_idx+1, filter_fn=lambda values: values > similarity_threshold ) similar_strings = [ [strings10[i] for i in indices] for indices in similar_indices ] len(similar_strings[0]) 612 If we do the rest of the approximation procedure, we see different (and worse) results: total_freq_distribution = torch.stack([ next_token_map10[string] for string in similar_strings[0] ]).sum(dim=0) prob_distribution = total_freq_distribution / total_freq_distribution.sum() approx_top_tokens = top_nonzero_tokens(prob_distribution, tokenizer.itos) tokens = encoding_helpers.tokenize_string(prompts[q_idx]) logits, _ = m(tokens) logits = LogitsWrapper(logits.detach(), tokenizer) model_top_tokens = logits.topk_tokens(k=10)[0][-1] print_distribution_comparison(approx_top_tokens, model_top_tokens) Model Predictions Approximation Predictions ----------------- ------------------------- i: 0.437 o: 0.584 o: 0.204 i: 0.251 a: 0.195 a: 0.095 e: 0.160 e: 0.066 u: 0.004 u: 0.002 l: 0.000 y: 0.001 The top 5 tokens are the same, but when ranked by probability, the approximation has a different ordering than the model. The Hellinger distance is also higher: hellinger_distance(prob_distribution, logits.probs()[0][-1]) tensor(0.2856) Loosening the similarity threshold introduced strings into the calculation that resulted in a worse approximation. Tightening beyond 0.95 also produces worse results than we got with 0.95, presumably because we’re excluding strings that were needed to produce a good approximation: block_idx = 0 similarity_threshold=0.97 q_idx = 57 similar_indices = filter_on_prefiltered_results( load_prefiltered=lambda q_idx: load_prefiltered_data(block_idx, q_idx), q_idx_start=q_idx, q_idx_end=q_idx+1, filter_fn=lambda values: values > similarity_threshold ) similar_strings = [ [strings10[i] for i in indices] for indices in similar_indices ] len(similar_strings[0]) 33 total_freq_distribution = torch.stack([ next_token_map10[string] for string in similar_strings[0] ]).sum(dim=0) prob_distribution = total_freq_distribution / total_freq_distribution.sum() approx_top_tokens = top_nonzero_tokens(prob_distribution, tokenizer.itos) tokens = encoding_helpers.tokenize_string(prompts[q_idx]) logits, _ = m(tokens) logits = LogitsWrapper(logits.detach(), tokenizer) model_top_tokens = logits.topk_tokens(k=10)[0][-1] print_distribution_comparison(approx_top_tokens, model_top_tokens) Model Predictions Approximation Predictions ----------------- ------------------------- i: 0.437 o: 0.278 o: 0.204 i: 0.250 a: 0.195 a: 0.250 e: 0.160 e: 0.222 hellinger_distance(prob_distribution, logits.probs()[0][-1]) tensor(0.1498) For the first block, 0.95 appears to be a sweet spot. I came up with this threshold through manual tuning: trying different values and binary searching towards one that produced the best results. The full history of this tuning exercise is in the similar space analysis notebook. In the end, I found the following thresholds produce the best results for each block: Block Similarity Threshold 0 0.95 1 0.94 2 0.85 3 0.76 4 0.81 5 0.89 When I first started exploring this space, I assumed the approximation would get better the more similarity I could find. I tried a number of techniques, including experimenting with Euclidean distance vs cosine similarity, searching across strings of different lengths, etc. Every time I succeeded in finding strings with more similar feed-forward network outputs to use in the approximation, the results got worse. I realized that, at least for some blocks, including less similar values in the mix produced better approximations, probably because those blocks had learned to map prompts to broader classes of strings in the training corpus. Going Beyond the First Block 🔗 Thus far, we’ve only considered feed-forward network outputs from the first block. Now we’ll incorporate the contributions from the other blocks. First, let’s find the strings that produce similar feed-forward network outputs in each block, using the similarity thresholds listed above. For now, we’ll do this for just one query (index 57, 'And only l'): similarity_thresholds=[0.95, 0.94, 0.85, 0.76, 0.81, 0.89] q_idx = 57 similar_strings_per_block = [] for block_idx in range(n_layer): similar_indices = filter_on_prefiltered_results( load_prefiltered=lambda q_idx: load_prefiltered_data(block_idx, q_idx), q_idx_start=q_idx, q_idx_end=q_idx+1, filter_fn=lambda values: values > similarity_thresholds[block_idx] ) similar_strings = [ [strings10[i] for i in indices] for indices in similar_indices ] similar_strings_per_block.append(similar_strings) Let’s summarize how many strings we found for each block based on these thresholds: print(text_table( headers=[\"Block Index\", \"Similarity Threshold\", \"# of Similar Strings\"], data_columns=[ [f\"{block_idx:>10}\" for block_idx in range(n_layer)], [f\"{threshold:>19}\" for threshold in similarity_thresholds], [f\"{len(similar_strings[0]):>19}\" for similar_strings in similar_strings_per_block], ], col_widths=[14, 23, 23] )) Block Index Similarity Threshold # of Similar Strings ----------- -------------------- -------------------- 0 0.95 94 1 0.94 47 2 0.85 70 3 0.76 108 4 0.81 175 5 0.89 2237 Now that we’ve identified the right strings for each block, we can do the next step of the approximation procedure: build the frequency distributions for the tokens that follow those strings, and sum them up. We’re going to be doing this several times over, so let’s define a function for it: def frequency_distribution_from_similar_strings( similar_strings_per_block: Sequence[Sequence[Sequence[str]]], next_token_map: Dict[str, torch.Tensor], ) -> torch.Tensor: # freqs_per_block_per_query is a list of lists of tensors. The outer list has # one item per block. The inner list has one item per query. Each # tensor is the next token frequency distribution for a particular # block and query. freqs_per_block_per_query: List[List[torch.Tensor]] = [[] for _ in range(n_layer)] for block_idx in range(n_layer): for similar_strings in similar_strings_per_block[block_idx]: freqs_per_block_per_query[block_idx].append( torch.stack([next_token_map[string] for string in similar_strings]).sum( dim=0 ) ) # Stack all frequency tensors into a single tensor of shape # (n_layer, n_queries, vocab_size) freqs = torch.stack( [ torch.stack(freqs_per_block_per_query[block_idx]) for block_idx in range(n_layer) ] ) return freqs This function, frequency_distribution_from_similar_strings(), does the equivalent of this code we looked at earlier: total_freq_distribution = torch.stack([ next_token_map10[string] for string in similar_strings[0] ]).sum(dim=0) But with two key differences: It does this calculation for all the blocks, using the similar strings we found for each block above. It allows for more than one query. In the code we’ve looked at so far, we only evaluated the approximation for a single prompt. In the next section, we’ll be running it for lots of prompts so I’ve written the code in a more general form to a allow for this. Specifically, the code allows for similar_strings_per_block to contain not just a single list of strings per block but multiple: one for each query. Let’s run this on the similar_strings_per_block we constructed earlier: freq_distribution = frequency_distribution_from_similar_strings( similar_strings_per_block, next_token_map10, ) freq_distribution.shape torch.Size([6, 1, 65]) It produces a tensor of shape (6, 1, 65): 6 blocks, 1 query, 65 tokens in the vocabulary. If we’d been working with more queries, the middle dimension would be larger. So now we have a frequency distribution for each block, based on the strings found for each block using the similarity thresholds. We now need to turn this into a probability distribution. Earlier, when we just had a single frequency distribution for a single block, we just normalized it. But now we have multiple frequency distributions - one for each block - and need to combine them. In my experiments, I found that a weighted sum of these distributions produced the best results. As with the similarity thresholds, I was able to find a set of good weights by trial and error. I also tried a deep-learning approach to find weights, but did not get better results than with the hand-tuned approach. The procedure for both hand-tuning and learning weights is implemented in the similar space notebook, the same one used for tuning thresholds. For now, let’s use the optimal weights I found: weights = torch.tensor([0.01, 0.01, 0.1, 1.5, 6, 0.01]).unsqueeze(dim=1).unsqueeze(dim=2) # (n_layer, 1, 1) total_freq_distribution = (freq_distribution * weights).sum(dim=0) prob_distribution = total_freq_distribution / total_freq_distribution.sum(dim=-1, keepdim=True) We multiply the frequency distributions by the weights, sum across all blocks, and then normalize into a probability distribution. We can now look at how the approximation’s distribution compares to the model’s. Note: in the code below, we have to index into the prob_distribution tensor with [0] because its first dimension is the number of queries. We’re only working with a single query, so we can just take the first element. approx_top_tokens = top_nonzero_tokens(prob_distribution[0], tokenizer.itos) tokens = encoding_helpers.tokenize_string(prompts[q_idx]) logits, _ = m(tokens) logits = LogitsWrapper(logits.detach(), tokenizer) model_top_tokens = logits.topk_tokens(k=10)[0][-1] print_distribution_comparison(approx_top_tokens, model_top_tokens) Model Predictions Approximation Predictions ----------------- ------------------------- i: 0.437 i: 0.363 o: 0.204 o: 0.265 a: 0.195 a: 0.213 e: 0.160 e: 0.147 u: 0.004 u: 0.011 l: 0.000 y: 0.000 hellinger_distance(prob_distribution[0], logits.probs()[0][-1]) tensor(0.0731) In this particular case, adding the other layers didn’t change the approximation much (if anything, it’s very slightly worse based on Hellinger distance). But let’s look at the example that didn’t work well when we considered just the first layer: prompt id 40 ('hing tremb'). similarity_thresholds=[0.95, 0.94, 0.85, 0.76, 0.81, 0.89] q_idx = 40 similar_strings_per_block = [] for block_idx in range(n_layer): similar_indices = filter_on_prefiltered_results( load_prefiltered=lambda q_idx: load_prefiltered_data(block_idx, q_idx), q_idx_start=q_idx, q_idx_end=q_idx+1, filter_fn=lambda values: values > similarity_thresholds[block_idx] ) similar_strings = [ [strings10[i] for i in indices] for indices in similar_indices ] similar_strings_per_block.append(similar_strings) freq_distribution = frequency_distribution_from_similar_strings( similar_strings_per_block, next_token_map10, ) weights = torch.tensor([0.01, 0.01, 0.1, 1.5, 6, 0.01]).unsqueeze(dim=1).unsqueeze(dim=2) # (n_layer, 1, 1) total_freq_distribution = (freq_distribution * weights).sum(dim=0) prob_distribution = total_freq_distribution / total_freq_distribution.sum(dim=-1, keepdim=True) tokens = encoding_helpers.tokenize_string(prompts[q_idx]) logits, _ = m(tokens) logits = LogitsWrapper(logits.detach(), tokenizer) approx_top_tokens = top_nonzero_tokens(prob_distribution[0], tokenizer.itos) model_top_tokens = logits.topk_tokens(k=10)[0][-1] print_distribution_comparison(approx_top_tokens, model_top_tokens) Model Predictions Approximation Predictions ----------------- ------------------------- l: 0.999 l: 0.997 e: 0.000 e: 0.002 r: 0.000 r: 0.000 hellinger_distance(prob_distribution, logits.probs()[0][-1]) tensor([0.0233]) Remember that for this example, when we used just the first layer’s similar strings, the approximation was quite different from the model’s prediction and had a Hellinger distance of >0.63. Now it’s nearly identical and has a Hellinger distance of 0.02. So using the rest of the layers really helped this example. In the next section, we’ll extend the code to evaluate the approximation over the whole set of 20,000 prompts. The section after that will look at how well the approximation does across all the prompts. Extending to All 20,000 Prompts 🔗 We now have all the pieces we need to run the approximation procedure for all 20,000 prompts. First, let’s find the strings with similar feed-forward network outputs for all the prompts, for all blocks: # Takes about 7 minutes to run similarity_thresholds=[0.95, 0.94, 0.85, 0.76, 0.81, 0.89] similar_strings_per_block = [] for block_idx in range(n_layer): similar_indices = filter_on_prefiltered_results( load_prefiltered=lambda q_idx: load_prefiltered_data(block_idx, q_idx), q_idx_start=0, q_idx_end=n_prompts, filter_fn=lambda values: values > similarity_thresholds[block_idx] ) similar_strings = [ [strings10[i] for i in indices] for indices in similar_indices ] similar_strings_per_block.append(similar_strings) Next, we compute the frequency distributions for each query based on the strings we found, perform the weighted sum, and normalize to produce a probability distribution. freq_distribution = frequency_distribution_from_similar_strings( similar_strings_per_block, next_token_map10, ) weights = torch.tensor([0.01, 0.01, 0.1, 1.5, 6, 0.01]).unsqueeze(dim=1).unsqueeze(dim=2) # (n_layer, 1, 1) total_freq_distribution = (freq_distribution * weights).sum(dim=0) prob_distribution = total_freq_distribution / total_freq_distribution.sum(dim=-1, keepdim=True) prob_distribution.shape torch.Size([20000, 65]) The output is a tensor of shape (20000, 65): one 65-entry distribution for each of 20,000 prompts. In order to compare, we need to run all the prompts through the model and get the output probability distributions the model predicts: tokens = encoding_helpers.tokenize_strings(prompts) logits, _ = m(tokens) logits = LogitsWrapper(logits.detach(), tokenizer) model_probs = logits.probs() model_probs = model_probs[:, -1, :] # We're only interested in the last token Now we have outputs from the approximation and from the model for all prompts. In the next section, we’ll measure the Hellinger distance between them and evaluate the results. Evaluating the Approximation 🔗 In earlier sections, we compared output from the approximation to output from the model for individual prompts. Now that we have both outputs for all prompts, we can compare them and look at aggregate results. First, we can compute the Hellinger distance between the approximation and the model’s prediction for each prompt: h = hellinger_distance(prob_distribution, model_probs) h.shape torch.Size([20000]) This produced 20,000 Hellinger distance scores, one for each prompt. We can start by looking at some basic stats: h.mean(), h.std(), h.min(), h.max() (tensor(0.1677), tensor(0.1215), tensor(0.0013), tensor(0.9994)) The average Hellinger distance is just below 0.17, with a standard deviation of around 0.12, suggesting a distribution that skews low (a good thing). We’ve also got at least one really excellent sample (a min of 0.0013) and at least one really terrible one (max of 0.9994). Let’s look at the distribution: Code for distribution plot Indeed, the distribution is skewed left, indicating most queries have Hellinger distance scores on the lower end. The numbers and the distribution graph look promising, but is the approximation really a good one? It’s hard to say without something to compare against and it’s not obvious what a good comparison might be. A thought experiment: let’s imagine that for some prompt, the model produced a distribution that looked like this: Code to generate first imagined distribution The tokens b and d have nearly the same predicted probability (0.49 vs 0.51). The model predicts an approximately equal chance of these tokens coming next. Now imagine our approximation, or another model, predicted this distribution: Code to generate second imagined distribution Nearly the same, but the probabilities are reversed: b has probability 0.51 and d has 0.49. Would we care about this difference? Clearly both distributions are saying that b and d are about equally likely. If used for inference, either distribution would probably produce acceptable results. For most use cases I could imagine, the difference would just be noise. The Hellinger distance between the two imagined distributions above is 0.0141. Not zero, but we’re saying it doesn’t matter for practical purposes. If 0.0141 is a Hellinger distance that doesn’t matter much, what about 0.02? Or 0.025? We can imagine there is some threshold Hellinger distance below which we wouldn’t care and above which we would consider distributions to be meaningfully different. What is that threshold value? If we knew it, then we could look at how close the average Hellinger distance between our approximation’s predictions and model’s come to this threshold. That would be a measure of the goodness of the approximation. I did an experiment to estimate what the threshold is. I trained the same transformer architecture three more times, starting with a different random seed each time and stopping at approximately the same training and validation loss as I did for the original model. This gave me three alternative transformers with roughly the same performance, but with different weights due to the different random initial starting points: Model Seed Est. Training Loss Est. Validation Loss Original Model 1337 0.9334 1.5063 Alternate 1 1442 0.9293 1.5038 Alternate 2 88 0.9294 1.4991 Alternate 3 99999 0.9339 1.4941 I used the same training/validation sets, hyperparameters, optimizer, etc. for the three alternate models as for the original model. The training code and output for the alternate models is in the alternate-models experiment notebook. Training code for the original model is at the end of the main transformer notebook. I then ran the same 20,000 prompts through the alternative models and calculated the Hellinger distance between their outputs and that of the original model. Appendix II shows the code used to do this. The table below shows the aggregate results. Comparison Mean Hellinger Distance Original vs Alternate 1 0.1064 ± 0.0823 Original vs Alternate 2 0.1057 ± 0.0817 Original vs Alternate 3 0.1053 ± 0.0828 The original model and the three alternate models are “equivalent” in the sense that they perform about equally well in terms of training and validation loss. I could have used any of them as the basis for this post. In other words, the differences between them likely aren’t meaningful - just noise. Across all three alternate models, the average Hellinger distance was ~0.11 ± 0.08. We only have 3 data points, so it’s not a perfect measure, but ~0.11 is probably a reasonable lower bound for the threshold Hellinger distance we are looking for. For comparison, the average Hellinger distance between the model and the approximation was ~0.17. A little higher than 0.11, but within a standard deviation. Plotting the distributions of the various Hellinger distances shows this nicely: Code to generate plot of Hellinger Distance Distributions There is clearly less deviation between the alternates and the original model than between the approximation and the original model, but it’s not wildly different. I think this result suggests the approximation is quite good. Most of the difference is within the “acceptable noise” threshold. Interpretation: Why Does the Approximation Work? 🔗 The analysis in the previous section shows that the outputs of the approximation are quite similar to the transformer’s outputs. But that doesn’t necessarily mean that the approximation procedure is similar to what the transformer is actually doing. The approximation and the transformer might just represent two different ways of computing the same result. My intuition is that this is not the case: I think the approximation is at least something like what the transformer is doing. In this section, I’ll break down how I think the transformer computes something similar to the approximation and then present some supporting evidence. The key ideas are: The transformer, as its name suggests*, performs a series of transformations on its embedded input. The transformer blocks transform embeddings within embedding space and the final linear layer at the end transforms from embedding space to logit space. Within each transformer block, the transformation from input to output embedding is done via vector addition: the block’s output embedding is its input embedding plus the output of the self-attention layer, plus the output of the feed-forward network. Of the two added components, the feed-forward network output value is dominant in determining the final output. Within embedding space, subspaces exist that correspond to specific tokens. An embedding within the subspace for a particular token produces an output distribution in which all the probability is concentrated on that token (that token has probability near 1 and all other tokens have probability near 0). Embeddings that lie between the subspaces for multiple tokens result in outputs that distribute all the probability across those tokens. The feed-forward network output at each block is an “adjustment vector” that orients the block output towards the subspaces for the tokens that the approximation procedure would predict: those that follow the strings in the training corpus that produce similar feed-forward network outputs at that block. In the subsections below, I’ll go into each of these ideas in more detail. *It’s unclear whether the name “transformer” alludes to transforming an input sequence to an output sequence (the use case in the original paper was machine translation) or the transformations within the layers of the model. The Model is a Series of Transformations 🔗 Once the input to the model has been embedded, we can view the model as a series of transformations: Drawing code for diagram showing transformations within the model The sequence of 6 transformer blocks takes a tensor in embedding space ( 𝑅 384 R384, since n_embed=384) as input and outputs another tensor in embedding space. In this sense, represents a transformation within embedding space. In fact, each transformer block is itself a transformation within embedding space and the stack of all 6 blocks composes these individual transformations. It isn’t literally implemented this way in code, but its equivalent to: output_embedding = block6(block5(block4(block3(block2(block1(input_embedding)))))) At the end of the sequence of blocks, the model sends the output embedding through a LayerNorm operation and then a linear layer that transforms from embedding space into logit space ( 𝑅 65 R65, since vocab_size=65). Finally, the softmax layer at the end turns the logits into probabilities for the next token. Transformation via Vector Addition 🔗 We looked at the internal logic within a transformer block in the earlier Transformer Block Structure section. To recap, the forward() method of the Block module looks like this: class Block(nn.Module): \"\"\"One transformer block\"\"\" ... def forward(self, x): x = x + self.sa(self.ln1(x)) # The `x +` part is a skip connection x = x + self.ffwd(self.ln2(x)) # The `x +` part is a skip connection return x This is equivalent to the following code, which, by using some intermediate local variables, clarifies what’s really going on: def forward(self, x): sa_out = self.sa(self.ln1(x)) ffwd_out = self.ffwd(self.ln2(x + sa_out)) return x + sa_out + ffwd_out The output of the block is equal to the input (x), plus the self-attention output (sa_out), plus the feed forward network output (ffwd_out). We can think of the block as taking the input embedding, and then making two adjustments to it. These values being added together are vectors in 𝑅 384 R384. If we imagine the embedding space reduced to just two dimensions, it might look something like this: Code to generate plot of of 1 block's vector additions in 2D The red vector represents the input embedding. The green vector represents the self-attention output (sa_out in code), and the blue vector represents the feed-forward network output (ffwd_out in code). The gray arrow represent the final sum, or the output of the first block: where you end up when you arrange the individual vectors tip to tail. The plot above shows the additions that happen within just one block. Subsequent blocks add their self-attention outputs and feed-forward network outputs, starting from the output of this block. If we add the vectors from those other blocks to the diagram, it looks like this: Code to generate plot of all blocks' vector additions in 2D Again, the red arrow represents the input vector, each green arrow represents one block’s self-attention output, each blue arrow represents one block’s feed-forward network output. Arranged tip to tail, their endpoint represents the final output from the stack of 6 blocks, depicted by the gray arrow. Though it’s only in two dimensions, the diagram above is based on real data and is drawn “to scale”, in a way: the length of each 2D vector is the same as the 𝑅 384 R384 vector it represents for a real query (index 57). In addition, the cosine similarity between each 2D blue / green arrow and the sum of the arrows that precede it is the same as the cosine similarity between the corresponding self-attention/feed-forward network output and the block input in the real data. Code to generate the 2D representation from real data is in the embedding adjustments analysis notebook. We can observe two interesting patterns: The feed-forward network outputs are generally longer than the self-attention outputs (the vectors have larger norms) Within a given block, the feed-forward network output and the self-attention output point in roughly the same direction. Look at what happens when we eliminate the self-attention outputs from the vector sum, leaving just the feed-forward network outputs: Code to generate plot of overlaying just the feed-forward vectors added The inner blue curve in the above plot represents the sum of the input vector and only the feed-forward network outputs from each block. The tip to tail arrangement of these vectors ends at a point far from where the previous arrangement (including the self-attention outputs) ended. But notice that the feed-forward-only endpoint (shorter gray arrow) is quite closely aligned in direction with the original endpoint (longer gray arrow). This plot shows values for only one query and we lose a lot of information dropping from 384 dimensions to 2. But the pattern does seem to hold in general and in the full, high-dimensional embedding space. The embedding adjustments analysis notebook provides a deep-dive into this phenomenon across all 20,000 queries. The takeaway is that simplifying the transformation performed by the blocks to just the contributions of the feed-forward networks results in an output vector that is shorter (has a smaller norm) than the original output but points in roughly the same direction. And the difference in norms would have no impact on the transformer’s final output, because of the LayerNorm operation after the stack of blocks. That LayerNorm step will adjust the norm of any input vector to similar value regardless of its initial magnitude; the final linear layer that follows it will always see inputs of approximately the same norm (see Appendix III for a walk-through of this). An important clarification: I’m not suggesting that we could remove the self attention computation from the transformer. The feed-forward networks take the self-attention output as part of their input (ffwd_out = self.ffwd(self.ln2(x + sa_out))); they would compute very different values were the self-attention outputs removed. What I am saying is that, after all block processing has been completed as normal including the self-attention computations, we get roughly the same result if we consider only the feed-forward network contributions, as our approximation does. This is probably because the feed-forward network outputs pass on some of the information they receive as input from the self-attention output. For some additional evidence that an approximation based only on feed-forward network outputs can produce similar outputs to the transformer, see Appendix IV. Token Subspaces 🔗 In the examples we’ve seen so far, the model outputs have been distributions that include significant non-zero probabilities for several tokens. For example: Code to generate plot of containing non-zero probabilities for several tokens Though we haven’t seen one yet, we might wonder whether specific inputs exist that compel the model to predict a single token with near certainty. In other words, do some inputs cause the model to output a probability distribution in which just one token has probability very near 1 and all other tokens very near zero? Such a distribution might look like this: Code to generate distribution in which one token has all the probability mass In fact, we can ask this question about any stage of the model. “Input” doesn’t have to refer to the initial input to the model, but could be the input to any layer within the model. For example, consider only the layers that transform the final block’s output embedding to logit space (the final LayerNorm and linear layers): Drawing code for diagram showing block 6 output to final layers Is there some embedding block 6 might emit that would yield an output probability distribution in which some token, say the letter a, has probability very near 1? Learning Token Subspaces 🔗 With the right math, it may be possible to find this embedding analytically. But it’s also possible to “learn” (in the sense of deep learning) such an embedding. Here’s the basic idea: Pick a point in the transformer where the input to subsequent layers is an embedding. This could be the input to any of the transformer blocks, or the point right after the final block (as shown in the diagram above). Pick a token to learn an embedding for. Create an embedding tensor and initialize it with random values. This tensor is the parameter the learning algorithm will optimize; the weights of the transformer are fixed. Execute a forward pass by evaluating the transformer from the selected point, using the embedding as input. This will produce some set of logits. Compute negative log likelihood loss relative to the token we’re learning an embedding for. Do a backward pass, updating the embedding tensor according to the gradients. My implementation of this is in the learned embeddings notebook. I used it to learn embeddings for all tokens at various stages of the model and saved them. We can load one - a learned embedding that produces a distribution giving token a probability almost 1 - and check that it does what we expect when given to the part of the model shown in the diagram above: Code to generate plot of probabilities from learned embedding for `a` As expected, all the probability mass is concentrated on a. Inference using this distribution would generate a with near certainty. The same procedure can learn embeddings for use at other parts of the model. If we wanted to find an embedding for a that could be input to block 6, we could run the same learning algorithm but use this part of the transformer in the forward pass: Drawing code for diagram showing block 6 onwards It’s more computationally expensive to learn embeddings at earlier stages of the model because the optimizer has to contend with a larger computation graph involving operations from all included blocks. Thankfully, as I’ll explain shortly, we need only the embeddings learned for the part of the transformer after all the blocks (embeddings that go straight into the final LayerNorm layer) to show how the transformer operates like the approximation. From Embeddings to Subspaces 🔗 For any token, the procedure described in the previous section can learn an embedding that makes the model predict that token with probability near 1. It turns out there isn’t just one such embedding for each token. We can learn many different embeddings that all produce probability distributions that assign a given token nearly all the probability mass. It was easy to learn thousands of unique embeddings for every token in the vocabulary. I think the model has learned a complex, non-linear embedding subspace corresponding to each token. Any embedding within that subspace results in an output distribution that assigns the token near certain probability. Each embedding I was able to learn is probably a point in the embedding subspace for the corresponding token. If we imagine the full embedding space ( 𝑅 384 R384) reduced to 𝑅 3 R3 (and the complex subspaces reduced to 2D planes), it might look something like this: Drawing code for diagram showing token subspaces I don’t know how to determine the exact subspaces for each token mathematically. But I do know how to get a workable approximation of them if we’re willing to pretend that they are linear. They are almost certainly not linear, even at the end of the model, because of the non-linear LayerNorm operation. But they are likely closer to linear near the end of the model because the LayerNorm is the only non-linearity. Earlier in the model, each feed-forward network introduces an additional non-linearity via its ReLU operation. This post on LessWrong illustrates of the non-linearity of LayerNorm clearly. Pretending the subspaces are linear actually works quite well for the part of the model after the transformer blocks. And that is the only part of the model we need to consider for this analysis (as I’ll explain soon). Linear Approximations for Subspaces 🔗 The idea is quite simple: for a given token, we can learn a whole lot of different embeddings, treating each one as a data point. Then we can determine the best fitting line, plane, or other low-dimensional linear subspace that fits the data. Again, if we imagine our embedding space reduced to just 3 dimensions, it might look something like the following diagram. The blue dots each represent a learned embedding and the red arrow is the line that minimizes projected distance from each point. Drawing code for diagram line fit to points in 3D space We can use Singular Value Decomposition (SVD) to find the best fitting linear subspace for the learned embeddings. To learn more about singular value decomposition in this context, I recommend reading Jeremy Kun’s excellent two-part post. Part 1 Part 2. Appendix V walks through the code that uses SVD to find a linear approximation for the subspace corresponding to one token. I did this for all tokens, using the embeddings I learned for the final stages of the transformer. In every case, I was able to find a single vector (1-D subspace) that approximates the token subspace quite well. For completeness, I also tried this at earlier stages of the transformer and found, as expected, that the linear approximations, even at higher dimensions, didn’t fit the data as well. The relevant experiments are in the approximation details notebook Mixing Subspace Approximations 🔗 By learning a large number of embeddings for each token and then using SVD on them, we can find one vector for each token that approximates its subspace. Given one of these vectors, any embedding that falls on its span will produce an output distribution that concentrates all the probability mass on the corresponding token. But many of the real transformer outputs we’ve seen distribute the probability mass across several tokens. How do we get from subspaces for individual tokens to embeddings that produce these more diverse distributions? We can create embeddings that produce probability distributions where several tokens have substantial probability via linear combinations of the subspace approximation vectors for those tokens. This is the distribution we get when we create an embedding by simply adding the approximation vectors for the subspaces for a and b: Code to generate plot of probabilities from `a` + `b` The sum of subspace approximations vectors for two tokens is an embedding somewhere in between the two subspaces, which results in a final distribution that is the combination of the two tokens. Sadly, adding the approximation vectors for a and b, without weighting either one, results in not quite a 50-50 distribution across the two tokens (as shown above). I think there are three reasons for this: The approximation vectors are just approximations and not perfect representations of their subspaces. The subspace approximation vectors are not perfectly orthogonal. To the extent that a’s vector has a small component that points in the direction of b, the sum results in an overweighting of b. The final linear layer of the model produces logits of different magnitudes for different tokens. For example, given the approximation for a, the logit for a is ~18.2. The logit for b from its approximation is ~19.5. Together, these errors accumulate and the softmax function at the very end exaggerates even small differences. For more analysis on the reasoning behind the differences and how they might be compensated for, see the combining token subspaces notebook. These imperfections aside, I think we can conclude that it’s possible to derive an embedding that produces a distribution for multiple tokens via a linear combination of the approximation vectors for those tokens’ subspaces. Putting it All Together 🔗 To summarize where we are, the preceding sections have shown: The transformer blocks perform a series of transformations in embedding space. Those transformations can be thought of as moving from one point in embedding space to another by adding the feed-forward network output vector to the input embedding. Embedding space contains subspaces corresponding to predicting particular tokens and embeddings between subspaces for multiple tokens result in predictions including all those tokens. This section adds the final piece, which is the correspondence between what the transformer is doing and what the approximation is doing: Within a block, adding the feed-forward network output vector to the input produces an output embedding that better aligns with the embedding subspaces of specific tokens. And those tokens are the same ones predicted in the approximation: they’re the tokens that follow the strings in the training corpus that yield similar feed-forward network outputs to the current prompt. Let’s look at an example that shows this. The following is the output distribution predicted by the approximation for the prompt, med me Aut (query index 33), using only the feed-forward network outputs from the final block: Helper function to perform the approximation for a single query in a single block. Code to plot output distribution for q_idx=33 in block 5 Based on the strings in the training corpus with similar feed-forward network outputs at the final block, the approximation predicts o is the most likely next token and h is next. Next, we need to look at the feed-forward network output for the prompt in this block and determine which token subspaces it’s most oriented towards. I’m going to show a little code here, because I think it’s the best way to explain what’s going on. Readers who aren’t interested in the implementation can focus only on the output. First we need to actually grab the feed-forward outputs (we haven’t needed them so far because we’ve been working with precomputed/prefiltered similarity data). We’ll use some helper functions that provide easy access to the transformer’s intermediate representations: # Tokenize the strings tokens = encoding_helpers.tokenize_strings(prompts) # Embed the tokens embeddings = accessors.embed_tokens(tokens) # Instantiate TransformerAccessors accessors = TransformerAccessors(m, device) # Run them through the model with hooks attached that let us look at # intermediate values _, io_accessors = accessors.run_model(embeddings) # Grab the outputs of the ffwd networks at each layer ffwd_outs = torch.stack([ io_accessors[block_idx].output('ffwd')[:, -1, :].clone() for block_idx in range(n_layer) ]) # Free up some memory del io_accessors _ = gc.collect() ffwd_outs.shape torch.Size([6, 20000, 384]) To determine which token subspaces the feed-forward network output aligns with, we’ll project it onto the subspace approximation for each token, then determine which projections are most similar to the original vector. To do this, we’ll need to get the projection matrix for the rank 1 approximation to each token subspace: The code below uses the projection_matrix_for_rank_k_approximation() helper function, defined in the SVD helpers notebook. In the case of a rank 1 approximation, the projection isn’t really necessary. We could just take the cosine similarity with the approximation vector, but I wanted to keep this code general because I tried out higher-dimensional approximations in other places. filename_for_token = FilenameForToken(tokenizer) subspace_dims = 1 projection_matrices = torch.stack([ projection_matrix_for_rank_k_approximation( original_matrix=torch.load( learned_embeddings_dir / 'no_blocks' / f\"{filename_for_token(token)}.pt\", map_location=device, )[:, 0, :], k=subspace_dims, ) for token in tokenizer.chars ]) Now we’ll perform the projections and find the top 5 most similar ones to the original feed-forward output vector: projections = projection_matrices @ ffwd_outs[block_idx, q_idx, :] values, indices = torch.topk( F.cosine_similarity(projections, ffwd_outs[block_idx][q_idx], dim=-1), k=5, dim=0, ) tokens = [tokenizer.chars[i.item()] for i in indices] list(zip(tokens, values.tolist())) [('o', 0.5074884295463562), ('h', 0.40787822008132935), ('i', 0.26926180720329285), ('u', 0.22823508083820343), ('y', 0.20325089991092682)] It turns out that o and h are the most similar, indicating that the feed-forward network output is most oriented towards the subspaces for these tokens. And these are the same tokens that the approximation predicted from the strings with similar feed-forward network outputs (see the distribution above). Another example, this time looking at query index 36 (if and thy), but staying in the final block: Code to plot output distribution for q_idx=36 in block 5 projections = projection_matrices @ ffwd_outs[block_idx, q_idx, :] values, indices = torch.topk( F.cosine_similarity(projections, ffwd_outs[block_idx][q_idx], dim=-1), k=5, dim=0, ) tokens = [tokenizer.chars[i.item()] for i in indices] list(zip(tokens, values.tolist())) [(' ', 0.5869003534317017), ('s', 0.47689366340637207), ('', 0.38412901759147644), ('$', 0.23048195242881775), ('a', 0.21783535182476044)] Here (space), s, and (newline) were the tokens predicted from what follows the strings with similar feed-forward outputs, and indeed these are the token subspaces most aligned with the prompt’s feed-forward output. Aggregate Performance 🔗 In the previous section, I purposely picked examples that exhibit strong correlation between the approximation’s predictions and the most aligned subspaces, to illustrate the point most clearly. Of course, there are other examples for which the correlation is less strong. Rather than looking at specific cases, let’s try to get a sense of how well the correlation holds up across all 20,000 prompts. This immediately leads to a question: what is the right measure of aggregate performance? Unfortunately, even if the hypothesis - that the prompt’s feed-forward output aligns with the subspaces for tokens predicted from the strings with similar feed forward outputs - is true, a few practical issues make it difficult to demonstrate objectively: We don’t have exact definitions of the token subspaces, just imperfect, linear approximations. Magnitudes don’t line up: the tokens with the most probability mass in the approximation’s predictions don’t always correspond to the subspaces with the greatest cosine similarity (because of the imperfect approximations, because the adjustment required may be bigger or smaller for some tokens vs others based on the input embedding’s current alignment, because, as explained in the Mixing Subspace Approximations section, the model is more “sensitive” to some tokens than others). Given these impediments, we can’t just do something simple like normalizing the cosine similarities and computing Hellinger distance with the predicted probability distribution. Instead, we need to devise a criterion on which to judge whether the data from a particular prompt supports the hypothesis or not. Then we can evaluate aggregate performance by how many of the 20,000 prompts satisfy the criterion. I experimented with several different approaches and in the end came up with this candidate criterion: High-level description: Do the subspaces for the tokens containing 90% of the probability mass in the approximation’s predictions appear in the top half of all token subspaces when ranked by cosine similarity with the prompt’s feed-forward output vector? Exact definition: Define top_n as the number of tokens required to cover at least 90% of the probability mass in the approximation’s predictions for this prompt. Define n_subspaces as tokenizer.vocab_size // 2 (32, based on our 65-token vocabulary). Determine: Are the subspaces for the first top_n tokens predicted by the approximation in the first n_subspaces subspaces ranked by cosine similarity with the prompt’s feed-forward output vector? Admittedly, this is an arbitrary definition and reasonable people could debate any of the specifics. But I do think it gives as an indication of whether the data from a particular example prompt supports the hypothesis, while allowing for some of the measurement challenges noted above. I evaluated this criteria at three places: the outputs of blocks 6, 5, and 4, using projection matrices derived from learned embeddings at each of these places. I didn’t evaluate at earlier blocks because the GPU time required to learn embeddings at those blocks became prohibitive. The further back in the model, the bigger the computation graph that the learning algorithm needs to optimize over. The table below shows the results: The code that produced these results appears at the end of the approximation details notebook Block # of Prompts Satisfying Criterion 6 16357 (81.78%) 5 10142 (50.71%) 4 7760 (38.80%) These numbers aren’t exactly a ringing endorsement. As expected, they get worse the further back we go, probably due to the increased non-linearity. What if we always used the subspace approximations from the very end of the transformer (which are likely to be the most linear), even when comparing against feed-forward network outputs from earlier blocks? The results get better: Block # of Prompts Satisfying Criterion 6 16357 (81.78%) 5 13652 (68.26%) 4 11630 (58.15%) 3 11469 (57.34%) 2 10404 (52.02%) 1 9942 (49.71%) Like many good findings, this one resulted from a bug. I accidentally ran the analysis using the projection matrices for the final part of the transformer with the feed-forward network outputs from earlier blocks and was surprised when the numbers turned out to be so good. It’s valid to use the subspace approximations (and corresponding projection matrices) from the end of the transformer at earlier stages. All blocks operate in the same embedding space and each one seems to make a small refinement on the output of its predecessors, rather than wild changes in direction. So if any block’s feed-forward network output adjusts an embedding towards the subspaces for a set of tokens as defined at the end of the transformer, it is likely also adjusting it towards whatever the subspaces would be for those same tokens at the block where it operates. The logit lens post, in the section “why? / is this surprising?” provides an explanation that supports this idea. In summary, the residual connections encourage the transformer to learn weights that operate within the same basis across blocks and the use of weight decay in training results in a computation that’s spread out over as many layers as possible, with each layer making only a small, incremental change. To put these numbers in perspective, I investigated how likely it would be for the criterion I’ve defined here to be satisfied by chance. In other words, if we assume the hypothesis is false and that the cosine similarities between the feed-forward network output and the token subspace approximation vectors are random, rather than expressing meaningful relationships, how likely would it be for the criterion to still be satisfied? I ran a simulation of this, taking care to ensure that the distribution of randomly generated cosine similarities matches the real data, among other details. The implementation is at the end of the approximation details notebook. The final results are: Block Likely % of Prompts Satisfying Criteria By Chance 6 20.76% ± 0.25% 5 20.55% ± 0.26% 4 18.37% ± 0.24% 3 18.20% ± 0.24% 2 17.04% ± 0.23% 1 16.31% ± 0.23% So the best performance numbers we have are clearly much better than chance. But in fairness, they’re still not a slam dunk. Even when we use approximations for the most linear subspaces we have, I think there is still a lot of noise in the measurement, for all the reasons outlined earlier in this section. Personally, I take the numbers to be overall supportive of the hypothesis, at least directionally, though I wish the evidence was more conclusive. Final Summary of Correspondence Between Transformer and Approximation 🔗 The analyses in this post point towards two ideas. First, that the approximation and the transformer produce similar outputs. Second, that there is a correspondence between the approximation procedure and what the transformer is doing. I think the evidence for the first idea is quite strong. The evidence for the second is less clear cut, but still suggests it’s probably at least partially right. To close, I want to provide a high-level summary of what I think that correspondence is, even if I can’t yet demonstrate it more definitively: Concept Transformer Approximation Prompts map to classes of strings in the training corpus. The transformer learns an embedding scheme, along with weights in its self-attention and feed-forward networks, that cause strings in the training corpus with similar characteristics to produce similar output values. Prompts that share those characteristics also produce similar output values. The approximation performs the same mapping as the transformer by examining the feed-forward network outputs from all substrings in the training corpus and identifying the ones similar to the outputs from a given prompt. Predictions for the tokens likely to follow a prompt derive from the frequency distribution of tokens that follow strings in the training corpus that produce feed-forward network output values similar to those of the prompt. A feed-forward network output is a compressed, latent representation, in the embedding space, of the frequency distribution of the tokens that follow strings in the training corpus that produce similar outputs. The weights in the final linear layer map the latent representation into logit space such that it become the correct probability distribution after applying the softmax operation. The approximation reconstructs the same frequency distribution manually, by looking up the strings identified as having similar outputs in the training corpus and counting the tokens that follow them. Normalizing the frequency distribution turns it into a probability distribution. Final output is a weighted sum of predictions from each block. As shown earlier, the transformer output is roughly the vector sum of all feed-forward network outputs and the input embedding. The learned weights in the layers within a block determine the magnitude and direction of the output and thus how much it influences the overall direction of the final sum. The approximation performs a weighted sum of the distributions determined for each block. The weights control the degree of influence of any given block and are manually selected to produce results as close to the transformer’s as possible. What About Attention? 🔗 I began this post by observing that most explanations of how transformers work focus on attention but don’t say how attention results turn into the final predictions. I may be guilty of the opposite: I’ve written at length about how the transformers produce their output probabilities and said very little about attention. To wrap up the analysis, I’d like to rectify this with a few words about attention. In the mechanism I’ve laid out, whether executed in the form of the approximation or the transformer, a key operation is mapping the prompt to a class of strings from the training corpus at each block. Predictions for the next token follow directly from the distribution of tokens that follow those strings in the training corpus. Making good predictions depends on mapping the prompt to the right class of training corpus strings. And that is the job of self-attention. The self-attention layers learn to identify patterns across the tokens that make up a prompt. Those patterns might be simple, such as a common sequence appearing at the beginning or end of the prompt (for example, as we saw earlier, strings that end in ‘y l’). They can also be more general: instead of matching specific tokens, they might match kinds of tokens, such as vowels or capitals, in specific places. The learned weights in the attention heads determine which patterns they respond to, and thus which strings in the training corpus produce similar values. The output of the self-attention heads, when passed through the feed-forward network, yield representations in embedding space that encode information about the distribution of tokens in the training corpus that follow those strings. Because the transformer has multiple blocks and each block has multiple attention heads (6 blocks and 6 heads per block in the one we looked at), it’s possible to evaluate each prompt against a large number of different potential patterns. The richness and diversity of the patterns that the attention heads can identify gives the transformer its predictive power. Closing Thoughts 🔗 I started this project because I wanted to understand the transformer architecture. It’s given me a satisfying explanation of what at least one transformer is doing, but has been even more fruitful as an exercise in learning how to learn. This was my first foray into an open-ended ML research project on my own. It taught me how to interrogate the internals of models, how to set up experiments to answer questions, and, perhaps most importantly, how to keep moving the project forward when I felt stuck. Language models have always seemed magical to me, from the first time I used ChatGPT. I wondered if finding a reductive explanation for what happens internally would rob them of their magic. In fact, I think the opposite has happened. I’ve come to appreciate the beauty in an elegantly simple mechanism that produces such rich complexity in its outputs. I don’t know whether the results I found here have any generality beyond the small transformer I trained or if any of it will be of use to anyone else. Regardless, it’s been a joy to do this work and I’m grateful to have had the things I needed along the way: time, resources, and endless support from my family and mentors. Appendices 🔗 I: Model Details 🔗 Some notable specs: Vocabulary size: 65 (the unique characters in the TinyShakespeare dataset) Embedding size (n_embed): 384 Number of transformer blocks (n_layer): 6 Number of attention heads (n_head): 6 Context window size (block_size): 256 The feed-forward networks comprise over 65% of the total trainable parameters: all_trainable_params = [p for p in m.parameters() if p.requires_grad] n_all_trainable_params = sum([np.prod(p.size()) for p in all_trainable_params]) ffwd_trainable_params = [ p for block_idx in range(n_layer) for p in m.blocks[block_idx].ffwd.parameters() if p.requires_grad ] n_ffwd_trainable_params = sum([np.prod(p.size()) for p in ffwd_trainable_params]) print( f\"{n_ffwd_trainable_params:,} ffwd params out of {n_all_trainable_params:,} total params ({n_ffwd_trainable_params / n_all_trainable_params:.2%})\" ) 7,089,408 ffwd params out of 10,788,929 total params (65.71%) II: Evaluation of Main Model vs 3 Alternate Models 🔗 As described in the Evaluation section, I trained the same transformer architecture used for the main model in this post three additional times, starting from a different random seed each time. This appendix shows the code used to measure the average Hellinger distance between the output of the main models and each of the three alternates, across the 20,000 sample prompts. The results provide a plausible lower bound for the threshold Hellinger distance that indicates a meaningful change in an output probability distribution. First, we instantiate the three alternate models from their saved weights: alt_models_dir = environment.data_root / 'alternate-models/model-training/20240112-training/outputs/' assert alt_models_dir.exists(), \"Alternate models directory does not exist. Run the training code in ../experiments/alternate-models.ipynb.\" # Instantiate the three alternative trained models m_alt1, _ = create_model_and_tokenizer( saved_model_filename=alt_models_dir / 'shakespeare-20240112-1.pt', dataset=ts, device=device, ) m_alt2, _ = create_model_and_tokenizer( saved_model_filename=alt_models_dir / 'shakespeare-20240112-2.pt', dataset=ts, device=device, ) m_alt3, _ = create_model_and_tokenizer( saved_model_filename=alt_models_dir / 'shakespeare-20240112-3.pt', dataset=ts, device=device, ) Next, we feed the input tokens into the original model and the three alternate models and get their output probability distributions: Helper function to get the model's output probability distribution for a batch of tokenized inputs. tokens = encoding_helpers.tokenize_strings(prompts) model_probs = get_model_probs(m, tokens) alt_model_probs1 = get_model_probs(m_alt1, tokens) alt_model_probs2 = get_model_probs(m_alt2, tokens) alt_model_probs3 = get_model_probs(m_alt3, tokens) Now we can compute the Hellinger distance between the outputs from the three alternative models and the outputs from the original model. Remember that each of the model probabilities tensors (model_probs and alt_model_probs*) is a 20,000x65 tensor i.e. 20,000 probability distributions of 65 elements each. We’re computing the Hellinger distance between those probability distributions. So for each alternative model, we end up with 20,000 Hellinger distance values. These values tell us, for each prompt, how the probability distribution for the next token predicted by one of the alternate models differed from the probability distribution predicted by the original model. h_alt1 = hellinger_distance(model_probs, alt_model_probs1) h_alt2 = hellinger_distance(model_probs, alt_model_probs2) h_alt3 = hellinger_distance(model_probs, alt_model_probs3) With the Hellinger distances computed, we can look at aggregate stats: h_alts = torch.stack([h_alt1, h_alt2, h_alt3], dim=1) h_alts.mean(dim=0), h_alts.std(dim=0), h_alts.min(dim=0).values, h_alts.max(dim=0).values (tensor([0.1064, 0.1057, 0.1053]), tensor([0.0823, 0.0817, 0.0828]), tensor([0.0005, 0.0008, 0.0008]), tensor([0.8351, 0.7881, 0.8743])) For all three alternate models, the average Hellinger distance was ~0.11 ± 0.08. All had very small minimums (<= 0.0008) and maximums around ~0.80. III: The Output Embedding’s Norm Doesn’t Matter Because of the Final LayerNorm 🔗 This appendix demonstrates the assertion from the Transformation via Vector Addition that the norm of the output embeddings from the final transformer block does not matter, because of the LayerNorm before the final linear layer. To begin, let’s grab the final block outputs for the first 1000 prompts: # Get the block outputs for the first 1000 prompts tokens_sample = encoding_helpers.tokenize_strings(prompts[:1000]) _, io_accessors_sample = accessors.run_model(accessors.embed_tokens(tokens_sample)) final_block_outputs = io_accessors_sample[-1].output('.')[:, -1, :].clone() del io_accessors_sample _ = gc.collect() final_block_outputs.shape torch.Size([1000, 384]) Next, let’s create a copy of those outputs scaled by a factor of 10: scaled_final_block_outputs = final_block_outputs * 10 scaled_final_block_outputs.shape torch.Size([1000, 384]) Comparing average norms, we see that those of the scaled outputs indeed are 10 times bigger: final_block_outputs.norm(dim=-1).mean(), scaled_final_block_outputs.norm(dim=-1).mean() (tensor(22.8909), tensor(228.9091)) Now, let’s put both the original and scaled outputs through the final LayerNorm of the model and calculate the average norm of the results: layer_normed_original = m.ln_f(final_block_outputs).detach() layer_normed_scaled = m.ln_f(scaled_final_block_outputs).detach() layer_normed_original.norm(dim=-1).mean(), layer_normed_scaled.norm(dim=-1).mean() (tensor(23.1262), tensor(23.1263)) They’re virtually identical. In the preceding example, the output norms were so close to identical because the two inputs differed only in scale: they had the same direction, or cosine similarity of 1. Vectors that have different norms and different directions will emerge from the LayerNorm with norms that are still quite similar but a little further apart. To see an example, we can add a little noise to one of the vectors and then scale it: original_vector = final_block_outputs[0] # Add some random noise to the original vector torch.manual_seed(42) # keep the noise consistent comparison_vector = original_vector + torch.randn_like(original_vector) * 0.1 # And scale it comparison_vector = comparison_vector / comparison_vector.norm() comparison_vector *= 10 * original_vector.norm() original_vector.norm(), comparison_vector.norm(), F.cosine_similarity(original_vector, comparison_vector, dim=-1) (tensor(23.7909), tensor(237.9092), tensor(0.9967)) The comparison_vector’s norm is exactly 10x that of original_vector, but they’re not perfectly aligned in direction, though still quite close. m.ln_f(original_vector).detach().norm(), m.ln_f(comparison_vector).detach().norm() (tensor(23.1496), tensor(23.1671)) Their norms after layer norm are close but further apart than in the previous example. If we add a lot more noise, we’ll end up with two vectors with quite different directions: original_vector = final_block_outputs[0] # Add some random noise to the original vector torch.manual_seed(4211) # keep the noise consistent comparison_vector = original_vector + torch.randn_like(original_vector) * 2 # And scale it comparison_vector = comparison_vector / comparison_vector.norm() comparison_vector *= 10 * original_vector.norm() original_vector.norm(), comparison_vector.norm(), F.cosine_similarity(original_vector, comparison_vector, dim=-1) (tensor(23.7909), tensor(237.9093), tensor(0.5178)) But their norms after layer norm are only a little more divergent: m.ln_f(original_vector).detach().norm(), m.ln_f(comparison_vector).detach().norm() (tensor(23.1496), tensor(23.0546)) So in summary: The LayerNorm will remove substantial differences in input norms. Norms of the outputs from the LayerNorm will vary a little depending on how closely aligned the input vectors were. IV: Summary of Experiment on Relative Impact of Self-Attention and Feed Forward Network Outputs 🔗 The embedding adjustments analysis notebook contains the implementation of an experiment to understand the relative impact of the self-attention outputs and the feed-forward network outputs on the final output of the transformer. This appendix summarizes the experiment and the results. Experiment procedure: I ran all 20,000 prompts through the model and captured the final output probability distributions as well as the intermediate self-attention outputs, feed-forward network outputs, and final block outputs for each block. For each block, I then ran two tests. First, instead of sending the block output as normally implemented (x + sa_out + ffwd_out, as shown earlier) to the next stage of the model, I sent a version that omits the self-attention output i.e. just x + ffwd_out, and saved the final output probability distribution that resulted. Then, I did the same thing but removed the feed-forward network output instead, sending on just x + sa_out. I then calculated the Hellinger distance between the probability distribution produced with the regular block output and that produced by each of the two modifications. The table below shows the results, averaged across all 20,000 prompts: For the implementation of this analysis, see embedding adjustments analysis notebook Block H(output(x+sa_out+ffwd_out), output(x+ffwd_out)) H(output(x+sa_out+ffwd_out), output(x+sa_out)) 1 0.11 ± 0.07 0.70 ± 0.17 2 0.07 ± 0.04 0.19 ± 0.11 3 0.09 ± 0.07 0.15 ± 0.10 4 0.06 ± 0.05 0.13 ± 0.10 5 0.04 ± 0.03 0.14 ± 0.10 6 0.03 ± 0.03 0.17 ± 0.10 Remember that Hellinger distance ranges between 0 and 1, with 0 meaning identical and 1 meaning no overlap. A larger Hellinger distance in this table means a larger divergence between the experiment output and the normal transformer output. The effect is most pronounced in the first block: omitting the feed-forward network output results in an almost completely different probability distribution (H = 0.70) but omitting the self-attention output results in a very similar distribution (H = 0.11). Across the rest of the layers, the difference is less dramatic, but the feed-forward network output always has the larger impact. Though I think these results support the notion that an approximation based only on feed-forward network outputs can produce similar results to the transformer, it would be interesting to see if the approximation would improve if we include the self-attention outputs, particularly for some of the intermediate layers. But I’m leaving that as an area for future investigation. V: Performing SVD to Get a Linear Approximation of a Token Subspace 🔗 This appendix walks through an example of how we can find a linear approximation for a token subspace using SVD. First, let’s load all the embeddings learned for the token a at the output of the last block of the transformer (input to the final layer norm and linear layer): learned_embeddings_dir = environment.data_root / 'learned_embeddings' multi_emb_a = torch.load(learned_embeddings_dir / 'no_blocks' / 'lower_a.pt', map_location=device) multi_emb_a.shape torch.Size([100, 1, 384]) We’ve got 100 different 384-dimensional embedding vectors. Each one, when given as input to the final blocks in the transformer, produces an output distribution that assigns nearly all the probability mass to the token, a. Each one can be thought of as a point in the subspace for token a. We can stack these embeddings form a 100x384 matrix: [ 𝑒 1 , 1 𝑒 1 , 2 … 𝑒 1 , 384 𝑒 2 , 1 𝑒 2 , 2 … 𝑒 2 , 384 ⋮ ⋮ ⋱ ⋮ 𝑒 100 , 1 𝑒 100 , 2 … 𝑒 100 , 384 ] ⎣⎡e1,1e2,1⋮e100,1e1,2e2,2⋮e100,2……⋱…e1,384e2,384⋮e100,384⎦⎤ Next, we can run SVD on this matrix: _, S, V = torch.linalg.svd(multi_emb_a[:, -1, :]) For this analysis, we’re only interested in the singular values (S) and the right singular vectors (V). We can plot the singular values: _ = plt.plot(S.numpy(), '-o') The first singular value is much larger (over 6x) than the next, which suggests the first right singular vector alone might be a good approximation for the subspace that predicts token a. We can test what gets predicted when we use this first right singular vector as an embedding: v0a = adjust_singular_vector_sign(V[0], multi_emb_a[:, -1, :]) logits = LogitsWrapper(accessors.logits_from_embedding(unsqueeze_emb(v0a)), tokenizer) logits.plot_probs(title='Next Token Probability Distribution from First Right Singular Vector of embeddings for Token \"a\"') In this distribution, a has probability near 1 and every other token has probability near 0. So the first right singular vector is effectively another embedding that produces an output predicting a with near certainty. But it’s different from the other 100 learned embeddings in an important way: it’s the vector that is best aligned with all of them. More formally, it’s the vector that minimizes the squared distance to all 100 other embedding vectors. In this way, the first right singular vector is like a good summary of the embedding vectors we started from. The first right singular vector is a unit vector (as are all the singular vectors): v0a.norm() tensor(1.0000) Any vector along its span will produce an output distribution predicting a, similar to the one above (see Appendix III for an explanation of why the transformer output is invariant to the scale of the final embedding). So the span of this vector is a good, linear approximation to the subspace for token a. The same results we saw here for token a hold for the other tokens too. For each, if we stack all the learned embeddings and perform SVD, we find that the first right singular vector forms a good linear approximation of the token subspace. transformer AI math",
    "commentLink": "https://news.ycombinator.com/item?id=39251909",
    "commentBody": "Beyond self-attention: How a small language model predicts the next token (shyam.blog)353 points by tplrbv 15 hours agohidepastfavorite68 comments tysam_and 8 hours agoSome of the topics in the parent post should not be a major surprise to anyone who has read https://people.math.harvard.edu/~ctm/home/text/others/shanno... ! If we do not have read the foundations of the field that we are in, we are doomed to be mystified by unexplained phenomena which arise pretty naturally as consequences of already-distilled work! That said, the experiments seem very thorough, on a first pass/initial cursory examination, I appreciate the amount of detail that seemed to go into them. The tradeoff between learning existing theory, and attempting to re-derive it from scratch, I think, is a hard tradeoff, as not having the traditional foundation allows for the discovery of new things, but having it allows for a deeper understanding of certain phenomena. There is a tradeoff either way. I've seen several people here in the comments seemingly shocked that a model that maximizes the log likelihood of a sequence given the data somehow does not magically deviate from that behavior when run in inference. It's a density estimation model, do you want it to magically recite Shakespeare from the void? Please! Let's stick to the basics, it will help experiments like this make much more sense as there already is a very clear mathematical foundation which clearly explains it (and said emergent phenomena). If you want more specifics, there are several layers, Shannon's treatment of ergodic systems is a good start (though there is some minor deviation from that here, but it likely is a 'close enough' match to what's happening here to be properly instructive to the reader about the general dynamics of what is going on, overall.) reply supriyo-biswas 1 hour agoparentIn another adjacent thread, people are talking about the implications of a neural network conforming to the training data with some error margin with regards to copyright. Many textbooks on information theory already call out the content-addressable nature of such networks[1], and they’re even used in applications like compression due to this purpose[2][3], and therefore it’s no surprise that the NYT prompting OpenAI models with a few paragraphs of their articles reproduced them nearly verbatim. [1] https://www.inference.org.uk/itprnn/book.pdf [2] https://bellard.org/nncp/ [3] https://pub.towardsai.net/stable-diffusion-based-image-compr... reply patcon 7 hours agoparentprevI appreciate what you're saying, but convergence (via alternative paths, of various depths) is its own signal. Repeated rediscovery perhaps isn't necessarily wastefulness, but affirmation and validation of deep truth for which there are multiple paths of arrival :) reply jackblemming 7 hours agoparentprev> the topics in the parent post should not be a major surprise to anyone who has read https://people.math.harvard.edu/~ctm/home/text/others/shanno... ! > which clearly explains it (and said emergent phenomena) Very smart information theory people have looked at neural networks through the lens of information theory and published famous papers about it years ago. It couldn't explain many things about neural networks, but it was interesting nonetheless. FWIW it's not uncommon for smart people to say \"this mathematical structure looks like this other idea with [+/- some structure]!!\" and that it totally explains everything... (kind of with so and so exceptions, well and also this and that and..). Truthfully, we just don't know. And I've never seen theorists in this field actually take the theory and produce something novel or make useful predictions with it. It's all try stuff and see what works, and then retroactively make up some crud on why it worked, if it did work (otherwise brush it under the rug). There was this one posted recently on transformers being kernel smoothers: https://arxiv.org/abs/1908.11775 reply Nevermark 1 minute agorootparentI think there is more here than a backward look. The article introduced a discrete algorithm method for approximating the gradient optimization model. It would be interesting to optimize the discrete algorithm for both design and inference times, and see if it had any space or time advantages over gradient learning. It also might have an advantage in other ways. For instance, given the most likely responses at each step, discard the most likely - and see if that avoided copyright issues. reply rrr_oh_man 24 minutes agorootparentprev> It's all try stuff and see what works, and then retroactively make up some crud on why it worked, if it did work (otherwise brush it under the rug). Reminds me of how my ex-client's data scientists would develop ML models. reply randomNumber7 7 hours agorootparentprev> It's all try stuff and see what works, and then retroactively make up some crud on why it worked People have done this in earlier days too. The theory around control systems was developed after PID controllers had been succesfully used in praxis. reply uptownfunk 5 hours agoparentprevOk but why didn’t Shannon get us gpt reply david_draco 3 hours agorootparentHe was busy getting us towards wifi first. reply 3abiton 7 hours agoparentprevKudos for pluggimg shannomçs masterpiece reply kmeisthax 13 hours agoprevI had the exact same idea after seeing Google point out that you can[0] get ChatGPT to regurgitate verbatim training data by asking it to repeat the same word over and over again[1]. I'm glad to see someone else actually bring it to fruition. This, of course, brings two additional questions: 1. Is this \"AI, hold the AI\" approach more energy-efficient than having gradient descent backpropagation compress a bunch of training data into a model that can then be run on specialized AI coprocessors? 2. Will this result wind up being evidence in the ongoing lawsuits against OpenAI and Stability AI? [0] Could. OpenAI now blocks generation if you fill the context window with a single word. [1] https://arxiv.org/abs/2311.17035 reply yorwba 12 hours agoparentThis approach cannot possibly be more efficient than running the original model because it relies on running the original model to get the activations to search the text corpus for strings with similar activations to compute the next-token statistics. You don't get to skip many steps, and you end up having to do a bunch of extra work. I'd be surprised if doing this with two completely separate corpora, one for training the model and the other to search for strings with similar activations, wouldn't lead to much the same results. Because the hard part is constructing similar activations for strings with similar next-token statistics in the first place. Note that in the per-layer weights [0.01, 0.01, 0.1, 1.5, 6, 0.01] the penultimate layer ist the most important, where the input has already been transformed a lot. So you can't expect to use this to replace a transformer with a simple grep over the training data. (My guess as to why the penultimate layer has a much higher weight than the final one is that this is due to induction heads https://transformer-circuits.pub/2021/framework/index.html which implement copying repeated strings from the input, with the penultimate layer determining what to look for and the final layer doing the copying.) reply bruce343434 3 hours agoparentprevIn my experience before they blocked it: it hallucinates something that looks like training data. A GitHub readme that under closer inspection doesn't actually exist and is incoherent. Some informational brochure about nothing. A random dialogue. reply noduerme 9 hours agoparentprevre: 2... if you copyright a work, then surely you also hold rights to a zip file of that work. So why not also the probability distribution of letters in that work? reply zarzavat 7 hours agorootparentTo be precise, you don’t hold rights to a zip file, copyright doesn’t know anything about files. You hold rights to a work, an abstract legal concept. Your rights to the work allow you to control the reproduction of that work, and distributing a zip file is an instance of reproducing the work. Probability distributions don’t contain enough information to reproduce a work (since they don’t preserve order). They are not copyrightable in and of themselves, and distributing a probability distribution of a work doesn’t amount to reproduction. reply kmeisthax 9 hours agorootparentprevIf the probability distribution is enough to reproduce a copyrighted work to the level of substantial similarity, then yes, a copy has legally been made. However, that's not the only question involved in a copyright lawsuit[0]. So far most of the evidence of copying has been circumstantial: a regurgitated Quake lighting function here, a Getty Images watermark there, but everything else has looked like wholly original output. We know from how these models are trained that copyrighted work is involved somewhere, but a court could just say it's Fair Use to scrape data and train a model on it. However, that defense is way harder to make if we can actually open up a model and show \"ok, this is where and how it's storing copied training set data\". At a minimum, it takes the \"how much was used\" Fair Use factor from \"a few watermarks\" to \"your honor, the entire fucking Internet\". [0] As usual we will assume jurisdiction in US court reply sureglymop 10 hours agoparentprevI found it interesting that in the arxiv paper you linked they are talking about an attack, ethics and responsible disclosure. But when it comes to scraping the entirety of the internet to train such models that's never referred to as an attack. reply kmeisthax 10 hours agorootparentScraping the whole web isn't considered an attack because, well, that's just how search engines work. That being said, there are all sorts of norms (e.g. robots.txt) qualifying what kinds of scraping are accepted. As far as I can tell, AI researchers assumed they could just piggyback on top of those norms to get access to large amounts of training data. The problem is that it's difficult to call copying an attack unless you go full MAFIAA[0]brain and argue that monopoly rents on creative works are the only functional backstop to the 1st Amendment. Hell, even if you do, the EU and Japan[1] both have a statutory copyright exception explicitly legalizing AI training on other people's text. It's not even accepted dogma among copyright holders that this is an attack. [0] Music And Film Industry Association of America, a fictional industry association purported to be the merger of the MPAA and RIAA announced on April 1st, 2006: http://mafiaa.org/ [1] Yes, the same country whose copyright laws infamously have no Fair Use equivalent. In Japan, it is illegal to review or parody a copyrighted work without a license, but it is legal to train an AI on it. reply refulgentis 12 hours agoparentprevI'm confused, you had the exact same idea that LLM output is based on probability of next token, which is based on the training data? If that's the case, no, its unlikely this result will end up becoming evidence, that is well known and fundamental. The author's contribution to discussion is showing this to a technical audience writing their own GPT, as they note, most \"how to implement this?\" focus on transformers reply kmeisthax 9 hours agorootparentMuch of the sales hype and other literature surrounding LLMs specifically obfuscates the role that training data plays in the model. Training data is \"learned from\", but that's implying the data goes away after the training process ends and you have a model that's solely composed of uncopyrightable knowledge about how to write or draw. If the models are actually retaining training data, and we have a way to extract that data, then the models didn't learn - legally speaking[0], they copied training set data. The idea I had wasn't \"LLMs are based on probabilities\", it was \"what if you benchmarked an LLM against a traditional search index over the training corpus\". The linked blog post doesn't completely rip out the LLMs entirely, just the feed-forward layer, but the result is what I thought would happen: an attention-augmented search index that is producing nearly identical probability distributions to the 66% of the model that was removed. [0] Programmers talking about copyright usually get tripped up on this, so I'll spell it out: copyright is a matter of data provenance, not bit-exactness. Just because the weights are harder to inspect does not mean no copyright infringement has occurred. Compression does not launder copyright. reply refulgentis 9 hours agorootparentShould make sure to establish this up front: People know this, it's not controversial. It's not only known to a few. It's how it works. Also note that this example purposefully minimizes training data down to an absurdity, so it is possible to correlate 1:1 that the next letter's probabilities to the input. The key of the rest of this comment,, and the discussions you reference, is the observation that's vastly harder once the training data is measured in terabytes, to the point the question becomes interesting. The argument of which you're speaking, the people you think are speaking literally are speaking figuratively: they know it reproduces _some_ training data, i.e. 2+2=4 was surely in the training data. Or c.f. NY Times v. OpenAI, where they were able to get it to complete an article given the first ~5 paragraphs of the article. The unsettled question, in US legal parlance, is if LLMs are sufficiently transformative of the training data that it becomes fair use. Eschewing US legal parlance: where, exactly, on the spectrum of \"completely original\" to \"photocopier with perfect recall\" LLMs fall, given we know it isn't at either of those extremes? What responsibility does that give someone operating an LLM commercially to the entities who originated the training data? reply kgeist 11 hours agoprev>I trained a small (~10 million parameter) transformer following Andrej Karpathy’s excellent tutorial, Let’s build GPT: from scratch, in code, spelled out As soon as I learned about Andrej Karpathy's NanoGPT, I trained it on War and Peace (in Russian), and what I found interesting is that it almost grokked Russian grammar despite being just a 3 MB model. Russian language has a complex synthetic-inflectional structure. For example, preposition \"na\" (\"upon\") requires the following noun to be in accusative case, which is manifested as ending -a for animate masculine nouns, but as null ending for inanimate nouns, or as -ia for nouns which end in a \"soft consonant\", -u for feminine nouns, etc. etc. Or the verb \"to use\" requires the following noun to be in instrumental case if it's used as a tool. Although it's not perfect and had mistakes, I found it interesting that NanoGPT was able to infer certain complex rules in just 3 minutes of training - and I searched in the texts for the exact examples it generated and found nothing verbatim. However, despite understanding grammar more-less, semantically, it was complete nonsense. reply lingeringdoubts 8 hours agoparentNot too surprising, since the inflections would be among the most common tokens in the training text. reply quag 9 hours agoprevIs the author claiming that LLMs are Markov Chain text generators? That is, the probability distribution of the next token generated is the same as the probability of those token sequences in the training data? If so, does it suggest we could “just” build a Markov Chain using the original training data and get similar performance to the LLM? reply golol 43 minutes agoparentLLMs are Marokov Chains in the following sense: States are vectors of context-length many tokens. Then the model describes a transitions matrix: For a given context-length sized vector of tokens it gives you probabilities for the next context-length sized vector of tokens. reply rrr_oh_man 18 minutes agorootparentCould you elaborate what context length means in this context? Maybe an example? reply two_in_one 3 hours agoparentprevFrom the post: > I implemented imperative code that does what I’m proposing the transformer is doing. It produces outputs very similar to the transformer. This means there is probably a way to bypass transformers and get the same results. Would be interesting if it's more efficient. Like given foundation model train something else and run it on much smaller device. reply yorwba 3 hours agorootparentI explained that it's not bypassing transformers and not more efficient in another comment: https://news.ycombinator.com/item?id=39254966 reply empiko 11 hours agoprevnice project, but the model that was being studied is really just a toy-model (both in size and training data). as such, this model can indeed be approximated by simpler models (I would suspect even n-gram LMs), but it might not be representative of how the larger LMs work. reply Closi 10 hours agoparentThis is probably true - i.e. you could make an even smaller model and then likely come up with an even-simpler explanation for how it worked. reply jimmySixDOF 11 hours agoprevThis was a good 3D visualization of the same systems and they probably should be read together for maximum effect .... LLM Visualization (https://bbycroft.net/llm) https://news.ycombinator.com/item?id=38505211 reply tysam_and 8 hours agoparentI appreciate the effort that went into this visualization, however, as someone who has worked with neural networks for 9 years, I found it far more confusing than helpful. I believe it was due to trying to present all items at once instead of deferring to abstract concepts, however, I am not entirely sure of this fact. > 1); x = *(float*)&i; x = x*(1.5f - xhalf*x*x); return x; } From https://betterexplained.com/articles/understanding-quakes-fa... In my case I don't have a huge amount of time to chase down every rabbit hole, but I'd love to accelerate intuition for LLM's. Multiple points of intuition or comparison really help. I'm also not a python expert - what you see and what I see from a line of code will be quite different. reply danielmarkbruce 9 hours agorootparentThe author is attempting to build an explicit mental model of what a bunch of weights are \"doing\". It's not really the same thing. They are minimizing the loss function. People try to (and often do) generate intuition for architectures that will work given the lay out of data. But, the reason models are so big now is that trying to understand what the model is \"doing\" in a way humans understand didn't work out so well. reply gjm11 11 hours agoparentprevThe post is long and complicated and I haven't read most of it, so whether it's actually any good I shan't try to decide. But the above seems like a very weird argument. Sure, the code is doing what it's doing. But trying to understand it at that level of abstraction seems ... not at all promising. Consider a question about psychology. Say: \"What are people doing when they decide what to buy in a shop?\". If someone writes an article about this, drawing on some (necessarily simplified) model of human thinking and decision-making, and some experimental evidence about how people's purchasing decisions change in response to changes in price, different lighting conditions, mood, etc., ... would you say \"You can just apply the laws of physics and see what the people are doing. They're not doing something more or less than that.\"? I mean, it would be true. People, so far as we know, do in fact obey the laws of physics. You could, in principle, predict what someone will buy in a given situation by modelling their body and surroundings at the level of atoms or thereabouts (quantum physics is a thing, of course, but it seems likely that a basically-classical model could be good enough for this purpose). When we make decisions, we are obeying the laws of physics and not doing some other thing. But this answer is completely useless for actually understanding what we do. If you're wondering \"what would happen if the price were ten cents higher?\" you've got no way to answer it other than running the whole simulation again. Maybe running thousands of versions of it since other factors could affect the results. If you're wondering \"does the lighting make a difference, and what level of lighting in the shop will lead to people spending least or most?\" then you've got no way to answer it other than running simulations with many different lighting conditions. Whereas if you have a higher-level, less precise model that says things like \"people mostly prefer to spend less\" and \"people try to predict quality on the basis of price, so sometimes they will spend more if it seems like they're getting something better that way\" and \"people like to feel that they're getting a bargain\" and so on, you may be able to make predictions without running an impossibly detailed person-simulation zillions of times. You may be able to give general advice to someone with a spending problem who'd like to spend more wisely, or to a shopkeeper who wants to encourage their customers to spend more. Similarly with language models and similar systems. Sure, you can find out what it does in some very specific situation by just running the code. But what if you have some broader question than that? Then simply knowing what the code does may not help you at all, because what the code does is gazillions of copies of \"multiply these numbers together and add them\". Again, I make no claim about whether the particular thing linked here offers much real insight. But it makes zero sense, so far as I can see, to dismiss it on the grounds that all you need to do is read the code. reply xanderlewis 11 hours agorootparentYou’re spot on; it’s like saying you can understand the game of chess by simply reading the rules. In a certain very superficial sense, yes. But the universe isn’t so simple. The same reason even a perfect understanding of what goes on at the level of subatomic particles isn’t thought to be enough to say we ‘understand the universe’. A hell of a lot can happen in between the setting out of some basic rules and the end — much higher level — result. reply danielmarkbruce 10 hours agorootparentAnd yet...alpha zero. reply xanderlewis 10 hours agorootparentMy entire point is that implementation isn’t sufficient for understanding. Alpha Zero is the perfect example of that; you can create an amazing chess playing machine and (potentially) learn nothing at all about how to play chess. …so what’s your point? I’m not getting it from those two words. reply danielmarkbruce 9 hours agorootparentUnderstanding how the machine plays or how you should play? They aren't the same thing. And that is the point - trying to analogize to some explicit, concrete function you can describe is backwards. These models are gigantic (even the 'small' ones), they are looking to minimize a loss function by looking in multi thousand dimensional space. It is the very opposite of something that fits in a human brain in any explicit fashion. reply gjm11 7 hours agorootparentSo is what happens in an actual literal human brain. And yet, we spend quite a lot of our time thinking about what human brains do, and sometimes it's pretty useful. For a lot of this, we treat the actual brain as a black box and don't particularly care about how it does what it does, but knowing something about the internal workings at various levels of abstraction is useful too. Similarly, if for whatever reason you are interested in, or spend some of your time interacting with, transformer-based language models, then you might want some intuition for what they do and how. You'll never fit the whole thing in your brain. That's why you want simplified abstracted versions of it. Which, AIUI, is one thing that the OP is trying to do. (As I said before, I don't know how well it does it; what I'm objecting to is the idea that trying to do this is a waste of time because the only thing there is to know is that the model does what the code says it does.) reply danielmarkbruce 6 hours agorootparentSure, good abstractions are good. But bad abstractions are worse than none. Think of all the nonsense abstractions about the weather before people understood and could simulate the underlying process. No one in modern weather forecasting suggests there is a way to understand that process at some high level of abstraction. Understand the low level, run the calcs. reply danielmarkbruce 10 hours agorootparentprevIt is very promising. In fact, in industry there are jokes about how getting rid of linguists has helped language modeling. Trying to understand it at some level of abstraction that humans can fit in their head has been a dead end. reply knightoffaith 8 hours agorootparentTrying to build systems top-down using principles humans can fit in their head has arguably been a dead end. But this doesn't mean that we cannot try to understand parts of current AI systems at a higher level of abstraction, right? They may not have been designed top-down with human-understandable principles, but that doesn't mean that trained, human-understandable principles couldn't have emerged organically from the training process. Evolution optimized the human brain to do things over an unbelievably long period of time. Human brains were not designed top-down with human-understandable principles. But neuroscientists, cognitive scientists, and psychologists have arguably had success with understanding the brain partially at a higher level of abstraction than just neurons, or just saying \"evolution optimized these clumps of matter for spreading genes; there's nothing more to say\". What do you think is the relevant difference between the human brain and current machine learning models that makes the latter just utterly incomprehensible at any higher level of abstraction, but the former worth pursuing by means of different scientific fields? reply danielmarkbruce 7 hours agorootparentI don't know neuroscience at all, so I don't know if that's a good analogy. I'll make a guess though - if you consider a standard RAG application. That's a system which uses at least a couple models. A person might reasonably say \"the embeddings in the db are where the system stores memories. The LLM acts as the part of the brain that reasons over whatever is in working memory plus it's sort of implicit knowledge.\" I'd argue that's reasonable. But systems and models are different things. People use many abstractions in AI/ML. Just look at all the functionality you get in PyTorch as an example. But they are abstractions of pieces of a model, or pieces of the training process etc. They aren't abstractions of the function the model is trying to learn. reply knightoffaith 7 hours agorootparentRight, I've used pytorch before. I'm just trying to understand why the question of \"how does a transformer work?\" is only meaningfully answered by describing the mechanisms of self-attention layers at the highest level of abstraction, with any higher level of abstraction being nonsense. More specifically, why we should have a ban on any higher level of abstraction in this scenario when we can answer the question of \"how does the human mind work?\" at not just the atom level, but also the neuroscientific level or psychological level. Presumably you could say the same thing about this question: The human mind is a bunch of atoms obeying the laws of physics. That's what it's doing. It's not something else. I understand you're emphasizing the point that the connectionist paradigm has had a lot more empirical success than the computationalist paradigm - letting AI systems learn organically, bottom-up is more effective than trying to impose human mind-like principles top-down when we design them. But I don't understand why this means understanding bottom-up systems at higher level of abstractions is necessarily impossible when we have a clear example of a bottom-up system that we've had some success in understanding at a high level of abstraction, viz. the human mind. reply danielmarkbruce 6 hours agorootparentIt would be great if they were good, but they seem to be bad, it seems that they must be bad given the dimensionality of the space, and humans latch onto simple explanations even when they are bad. Think about MoE models. Each expert learns to be good at completing certain types of inputs. It sounds like a great explanation for how it works. Except, it doesn't seem to actually work that way. The mixtral paper showed that the activated routes seemed to follow basically no pattern. Maybe if they trained it differently it would? Who knows. It certainly isn't a good name regardless. Many fields/things can be understood at higher and higher levels of abstraction. Computer science is full of good high level abstractions. Humans love it. It doesn't work everywhere. reply knightoffaith 6 hours agorootparentRight, of course we should validate explanations based on empirical data. We rejected the idea that there was a particular neuron that activated only when you saw your grandmother (the \"grandmother neuron\") after experimentation. But just because explanations have been bad, doesn't mean that all future explanations must also be bad. Shouldn't we evaluate explanations on a case-by-case basis instead of dismissing them as impossible? Aren't we better off having evaluated the intuitive explanation for mixtures of experts instead of dismissing them a priori? There's a whole field - mechanistic interpretability - where researchers are working on this kind of thing. Do you think that they simply haven't realized that the models they're working on interpreting are operating in a high-dimensional space? reply danielmarkbruce 5 hours agorootparentMechanistic interpretability studies a bunch of things though. Like, the mixtral paper where they show the routing activations is mechanistic interpretability. That sort of feature visualization stuff is good. I don't know what % of the field is spending their time on trying to interpret the models in a way that involves higher level, human can explain, approximating the following code type work though? I'm certainly not the only one who thinks it's a waste of time, I don't believe anything I've said in this thread is original in any way. I... don't know if the people involved in that specific stuff have really grokked they are working in high dimensional space? A lot of otherwise smart people work in macroeconomics, where for decades they haven't really made any progress because it's so complex. It seems stupid to suggest a whole field of smart people don't realize what they are up against, but sheesh it kinda seems that way doesn't it? Maybe I'll be eating my words in 10 years. reply knightoffaith 5 hours agorootparentThey certainly understand they're working in a high dimensional space. No question. What they deny is that this necessarily means the goal of interpretability is a futile one. But the main thrust of what I'm saying is that we shouldn't be dismissing explanations a priori - answers to \"how does a transformer work?\" that go beyond descriptions of self-attention aren't necessarily nonsensical. You can think it's a waste of time (...frankly, I kind of think it's a waste of time too...), but just like any other field, it's not really fair to close our eyes and ears and dismiss proposals out of hand. I suppose > Maybe I'll be eating my words in 10 years. indicates you understand this though. reply drdeca 11 hours agoparentprevUnderstanding how a given CPU (+ the other computer hardware) works, does not suffice to understand what is going on when a particular program is running. For that, you need to either read the program, or an execution trace, or both, or something along these lines, which is specific to the program being run. reply danielmarkbruce 10 hours agorootparentThis is the wrong analogy. The transformer block is a bunch of code and weights. It's a set of instructions laying out which numbers to run which operations on. The optimizer changes weights to minimize a loss function during training and then the code implementing a forward pass just runs during inference. That's what it is doing. It's not doing something else. If the argument is that a model is a function approximator, then it certainly isn't approximating some function that performs worse at the task at hand, and it certainly isn't approximating a function we can describe in a few hundred words. reply FeepingCreature 2 hours agorootparentWe have no reason at all to be certain of the latter. reply nl 11 hours agoparentprevA walk through with what the data at each point looks like is actually pretty useful. reply danielmarkbruce 10 hours agorootparentSure, it is. But trying to explain it as though the weights have some goal is weird. They aren't trying to do anything. You have a loss function. The optimizer keeps moving weights around in an attempt to minimize the loss function. It's not more or less than that. reply MarkusQ 8 hours agorootparentThis is just wrong. First of all, you're rejecting teleological anthropomorphizing (saying it's weird to act as if \"the weights have some goal\") but then in the very next line you talk about the optimizer making \"an attempt\" to accomplish a goal. All of which misses the point, since the question is about _explanations_ not goals and intentions. Then you reject out of hand any other level of explanation than the one you favor, saying \"it's not more or less than that\" when in fact it is both more and less than that; you can climb the ladder of abstraction either way to build more or less abstract explanations. We can dig down and talk about how the optimizer adjusts weights, or how tensor math works and how it's used in this case, or about how GPUs work, or gates, or transistors, etc. Or we could climb up and talk about (as this article does) and talk about what attention heads do, and why they work, when they work, when they don't, etc. reply danielmarkbruce 8 hours agorootparentThe optimizer has a goal. The weights in the model do not. The optimizer isn't the model. There is no contradiction if you know how it works. Climbing the layer of abstraction from model weights doesn't seem to work in this field. Just saying it's so doesn't make it so. reply MarkusQ 8 hours agorootparent> Just saying it's so doesn't make it so. Does that apply to you as well? reply danielmarkbruce 8 hours agorootparentOf course. You shouldn't take my word for it. You can learn the basics of AI/ML from a number of good texts. Simon Prince just released a very approachable text, although it doesn't cover much in the way of history to see the move to \"more data/more compute, less human lead abstraction\". I think Norvig's book covers that but I haven't read the latest version. reply MarkusQ 7 hours agorootparentYou sure like to make assumptions, don't you? :) reply awwaiid 12 hours agoprev [–] A thousand hands on a Ouija board. reply hnfong 4 hours agoparent [–] Is that an analogy? If so, it is an extremely interesting one, and I would like to know where does it come from :D reply two_in_one 3 hours agorootparent [–] as you'd expect: https://en.wikipedia.org/wiki/Ouija reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author trained a language model using a transformer and found that each transformer block has different associations between prompts and training data classes, resulting in diverse predictions.",
      "The model was trained on the TinyShakespeare dataset based on Andrej Karpathy's work on generating Shakespearean text.",
      "The author proposed a method for estimating the output of the transformer model and provided instructions and code for implementation.",
      "They analyzed the role of the feed-forward network, examined frequency distributions of tokens, and compared model predictions to training data.",
      "The author discussed the significance of LayerNorm in a transformer model and the impact of scaling and direction of input vectors on output norms.",
      "They explored the effectiveness of using only feed-forward network outputs for approximation and suggested further research on including self-attention outputs.",
      "SVD was used to achieve a linear approximation of token subspaces in the transformer model.",
      "Understanding the transformer architecture was deemed a valuable learning experience by the author."
    ],
    "commentSummary": [
      "The discussion touches on language models, neural networks, copyright, and understanding AI systems.",
      "It emphasizes the importance of a strong foundation in the field and the limitations of information theory in explaining neural networks.",
      "It raises legal questions regarding training AI models on copyrighted works and delves into the role of training data in language models.",
      "The conversation also explores the complexity of understanding how these models work and the challenges of interpreting and explaining transformer models.",
      "Overall, it highlights the need for different levels of explanation and abstraction when studying AI systems."
    ],
    "points": 353,
    "commentCount": 68,
    "retryCount": 0,
    "time": 1707065669
  },
  {
    "id": 39253099,
    "title": "\"Gödel, Escher, Bach: An Eternal Golden Braid\" - A Life-Changing Influence (2022)",
    "originLink": "https://philosophygeek.medium.com/why-g%C3%B6del-escher-bach-is-the-most-influential-book-in-my-life-49d785a4e428",
    "originBody": "Why Gödel, Escher, Bach is the most influential book in my life. Mark Johnson · Follow 7 min read · Feb 23, 2022 -- 73 Gödel, Escher, Bach: An Eternal Golden Braid (henceforth: GEB), the Pulitzer Prize winning book written in 1978 by Douglas Hofstadter, is described in its cryptic tagline as “a metaphorical fugue on minds and machines in the spirit of Lewis Carroll.” I recently reread GEB and got fired up by how brilliantly Hofstadter fuses computation, epistemology, and consciousness. After failed attempts at explaining the book to three of my smartest friends, I decided to write something up. The problem is that a simple reduction like “GEB is about how complex systems arise from simpler systems” is akin to describing Ulysses as “a day in the life of Leopold Bloom.” More detailed descriptions run the risk of diving into the depth that’s only understandable after having read the book. This post is a more modest attempt to explain to myself why GEB is important, and focuses on three mental models that have profoundly affected my life: epistemic limits, self-reference, and isomorphism. If it causes you to read or reread it, then all the better. Here we go! Kurt and Albert, hanging out at Princeton. The main character of the book is Kurt Gödel, the most important person in the 20th century you’ve never heard of. Gödel is the kind of guy that shows up to his buddy’s 70th birthday with an exact solution to the Einstein field equations as a present. Despite being the greatest mathematician of his generation, he wasn’t stuffy in the least: his favorite movie was Snow White and the Seven Dwarves. Gödel is most famous for his Incompleteness Theorems, which established limits on mathematics. For the first chunk of the 20th century, mathematicians were obsessed with formalizing mathematics and then proving meta-theorems about those formal systems. In particular, there was a strongly-held belief that for any well-formed formula (a “grammatically correct” statement in math, e.g., A=B is well-formed whereas AA==+B is not), you could use mathematics to decide whether it was true or false. If you think about it for a second, this makes perfect sense: it seems like you should be able to determine whether any statement is true or false. Nope! Gödel proved in 1931 that mathematics is not decidable, an earth-shattering result. He proved that there are statements in mathematics, which are true but not provable within the system. Worse yet, it turns out that you can’t build a more powerful mathematical system. Once a system becomes sufficiently complex, there will always be statements which are undecidable. You’re left with a choice: either have weak system of mathematics or accept that there will always be theorems out of reach. A rough analogy to incompleteness Heisenberg’s Uncertainty Principle, which shows that physics makes it impossible to determine both the position and velocity of a particle with exact precision. Wouldn’t it be nice if every question had an answer? That’s a lovely fantasy, but Gödel shows that there are fundamental epistemic limits to the universe, things that no genius will help us to know, no alien race could teach us, no machine could be built to solve, and no new kinds of mathematics will uncover. How frustrating. A key feature of powerful mathematical systems (or perhaps, any system that generates complexity…) is that they involve self-reference, that is, they contain ways of talking about themselves. “This sentence is true” is an example. Because self-referential systems can manipulate and talk about themselves, they systems are very powerful and immediately run into fun paradoxes. Is the statement “This sentence is false.” true or false? Either way, it doesn’t end well. A third major theme of the book is isomorphism, which is unique to Hofstadter’s vernacular. In formal mathematics, “isomorphism” takes on a version of “equivalence.” For example, it turns out that many different formalizations of mathematics are provably isomorphic, like Turing Machines, arithmetic, set theory, and formal logic. Hofstadter deliberately uses the term more loosely to describe two systems that are structurally similar. I find this quite useful because it forces one to define the structures of the system, why they are similar, and why other parts of the system are less important. We might describe the way that planets fly around stars as isomorphic to the way that electrons fly around nuclei. Escher’s famous Drawing Hands. The two minor characters, M.C.Escher and Johann Sebastian Bach, are reflections of Gödel in art and both liberally use self reference. Escher draws pictures of hands drawing hands (!) and water “falling” in an infinite loop. His images don’t just play tricks on the eye, they force paradoxical conclusions, regardless of your angle of interpretation. On the musical side, Papa Bach was most famous for his complex fugues, which are basically the same melody played on top of each other. Common versions of this you might have sung as a child are “Row, row, row your boat” and “Frère Jacques.” Both Escher and Bach are woven into the story (like a fugue?), providing tangible examples to the more abstruse mathematical concepts. A playlist full of Bach, fugues, and other tracks referenced in GEB. Perhaps the most astonishing part of the book is the quality of the writing itself. Each chapter begins with a clever dialog between Achilles and the Tortoise (inspired by Lewis Carroll), and a few of their anthropomorphic friends. They deal with a whole range of bizarre situations, like record player so powerful that it can play any record (including a record that can destroy the record player) and asking a Djinn for a meta-wish (“I wish for 5 more wishes”). Hofstadter’s greatest achievement is his palindromic Crab Canon in Chapter VII, which is a dialog that can be read backwards and forwards. Of course, these aren’t just cute dialogs: each is isomorphic to the themes in the following chapter. Oftentimes a dialog is a more understandable exposition of the chapter’s theme than the chapter itself. And, naturally in a book about self-reference, GEB itself is highly self-referential. Themes are often resolved hundreds of pages later and require going back to appreciate fully the depth of Hofstadter’s argument. Mercifully, he’s a gifted and lucid writer so, even though there are chapters that are dense, it’s always tractable to read. After 742 pages and even after having written the paragraphs above, I still struggle with a simple answer to the question: “What is this book about?” The best I can come up with is that GEB equips you with mental models to contemplate philosophy. So to end, a few personal examples about how GEB has influenced my own thinking. I recently joined Stand Together, who shares my strong belief in bottom up solutions. Perhaps the idea that bottom up solutions are better isn’t just an empirical statement of sociology, but fundamental to the nature of complex systems. Indeed, Hofstadter goes through many examples of how complexity emerges from simpler systems, often which look nothing like the higher-level systems. Consciousness itself doesn’t exist in neurons, and yet neurons as a system create consciousness in humans (this is critical to Hoftstadter’s argument that machines can think). There’s also a fantastical example in a dialog with the Anteater, who has conversations with Aunt Hilary, an ant colony. She is perfectly capable of having a robust conversation with the Anteater, powered by the ants in the colony. Of course, the ants themselves are individuals with their own cares and concerns and has no knowledge of the emergent intelligence, much like Aunt Hilary has no knowledge of her inner workings. How DNA expresses as proteins, how the brain functions at multiple levels, how we understand and use words, how programs don’t have access to the underlying transistors, how Aunt Hilary doesn’t know what the ants are doing… all of these are a set of isomorphisms that suggest that bottom up is better than top down. An additional tenet of Stand Together is “believe in people,” which means that the smallest units act intelligently. Like ants or neurons, we make local decisions every day that bubble up into the structure of society, without anyone telling us what to do. The idea that epistemic limits exist in something as universal as mathematics has humbled me about the limits of knowledge for complex human systems. Utopian thought experiments often generate useful frames of exploration, but ought not be confused with reality. Utopians often try to pull out the “bugs” from human systems, often which are endemic to the system, or “features,” as we might say in the trade. Bugs might not be desirable, but sometimes, the bugs can’t just be ripped out of the system without destroying the system itself. Our time would be better spent figuring out—within the system—optimize for minimizing the downsides of the “bugs” while maximizing the value of the features. Think about this with respect to capitalism, socialism, and communism… A final area where GEB has influenced me is in designing software products. Hugh Dubberly has been my collaborator for years, starting off with our deep dive into cybernetics, the study of feedback loops. We believe that iteration is key to quality; perfection is impossible out of the gate. Further, the system used to generate quality software is a series of feedback loops between customers and the company, product and engineering, and so on. Though specific product frameworks have changed over the years, that obsession with iteration and feedback permeates everything I’ve implemented. My modest goal in writing this post was to have something I could send to a friend, rather than to spend an hour fumbling a feeble explanation of Gödel, Escher, Bach. I had a secondary goal in the back of my head… if you have a copy of GEB on your shelf collecting dust and you’ve never read more than a chapter or two, dust it off and see how it goes this time.",
    "commentLink": "https://news.ycombinator.com/item?id=39253099",
    "commentBody": "Gödel, Escher, Bach is the most influential book in my life (2022) (philosophygeek.medium.com)327 points by drcwpl 15 hours agohidepastfavorite238 comments lisper 13 hours agoI love GEB. It is a masterpiece. But it is important to realize before diving into it that one of the things that makes it a masterpiece is that it is literary, that is, that it contains a wealth of detail that is, strictly speaking, unnecessary to the main point. Drawing a parallel between GEB and James Joyce's Ulysses is actually quite a good analogy. Indeed, Ulysses is almost nothing but \"unnecessary detail\", to the extent that it makes the book all but unreadable (which, I think, is no small part of its appeal). If you're waiting for either GEB or Ulysses to hurry up and get to the mother fucking point, you're going to be waiting a long time. In that regard, both books are good for techies to read because every now and then it's good to read something that drags you out of your comfort zone and futzes around in all kinds of obscure nooks and crannies before getting to the mother fucking point -- if indeed it ever does. Getting to the point can be important, but there is more to life (and literature). reply Gimpei 13 hours agoparentI love Ulysses and metafiction in general, but when people apply this kind of writing style to philosophy it drives me a bit up the wall. Philosophy at the end of the day is about arguing a point; it isn't about producing an aesthetic experience. So why would you abuse your reader by making it a pain in the ass to figure out what you mean? I know there are some fancy arguments around stretching limits of language, but none of them seemed all that sensible to me. The only advantage I see to obscurantist writing is that it makes it impossible to critique the philosophers work. Any critique will just be met by the response that you didn't fully understand the author's argument. The Searle/Derrida debates are a great example of this. The upshot is that people spend all their time debating what you actually mean. Which I guess is good for the philosopher's brand, but doesn't advance knowledge much. This isn't to say that you can't have beautiful writing in philosophy. I think Gaston Bachelard is a great example of both an elegant writer and a clear writer. That being said, people really really love GEB, so it probably is worth reading regardless of these misgivings. One of these days I'll get to it. reply holgerschurig 41 minutes agorootparent> So why would you abuse your reader by making it a pain in the ass to figure out what you mean? I think this statement is both wrong and irrelevant. Wrong: if you look into GEB, you notice that is has many chapters. And each chapter works on some topic --- and the chapters itself aren't a pain in the ass to figure out what they mean. Not at all. Each chapter transports its goal quite nicely and timely. Irrelevant: this book is not an academical paper. It's a book meant to amuse you, to enlighten you, to sharpen your thinking skills. An academic paper describing just how Gödel's work works can be a *LOT* thinner --- but, usually, is also a lot dryer. It's actually the case that this book gives you many good things that help you to understand (and accept or dismiss) such academic papers easier. But even here, mind the \"many\", if you expect \"all\" from the book, then you'll be dissatisfied. I also wouldn't say that GEB is a philosophical book, at least not in the way modern philosophy is understood. If you see it as a book dedicated to like (\"philo\") wisdom (\"sophie\"), then it is. But so are most after books in the IT field. reply heja2009 34 minutes agorootparentprev> Philosophy at the end of the day is about arguing a point; it isn't about producing an aesthetic experience. I'd argue that point. Nietzsche's \"Also Sprach Zarathustra\" (Thus Spoke Zarathustra) comes to my mind first. Art contains and conveys truth. Nietzsche's work is also a good example about why it's foolish to argue too much about what an author meant. He wasn't always clear in his thoughts - just like us all - and his thinking changed considerably anyway. reply verticalscaler 2 hours agorootparentprev> So why would you abuse your reader by making it a pain in the ass to figure out what you mean? A psychonaout or a shaman would argue some insights are only available on a drug infused spiritual trip. Musicians talk of flow. That state of discombobulation you dislike is not to meant to furnish you with answers but rather with new dots to notice and connect on your own. Not everything is a mathematical proof. I think the real reason this sort of style isn't attractive is because many try to emulate it who lack the talent and then it truly is noise. It is a lot to ask from a reader and the value is rarely there that is well observed. But sometimes the payoff is worth it. GBE resonates with a lot of people. reply wouldbecouldbe 13 hours agorootparentprevHahah the whole point of op is that there is value in not always having a point, but meandering, especially for techies. Are you set an proving him right? reply pierat 12 hours agorootparentI think it'd be more fair to say that GEB is just philosophical navel-gazing with the appearance of some deep philosophical truth... When the actual truth is they cheaped out on editorial staff :D I got it, and read a portion of it. It was a meandering and badly written mess with no point. It felt like Seinfeld in science-ish form. reply pohl 10 hours agorootparentI can imagine it being received differently today, but I read it twice in the 80s. It blew my mind. Now you can’t swing a stick without hitting some referent or concept in its pages, but that’s the internet age, efficiently routing the arcane to hypernerds worldwide. It’s made the whole world boring. reply jhanschoo 5 hours agorootparentThanks for the context. I'd recently tried reading GEB but couldn't get into it much since I already knew the formal version of the concepts it discusses about. I also have experience of an earlier time when expert knowledge was much less accessible, and where I treasured books like this that tried to compress every ambition within them, since they brought a lot of threads for further edification together in them. reply wouldbecouldbe 11 hours agorootparentprevYou seem deadset on proving original commenter right reply jurynulifcation 8 hours agorootparentDo you have substantive critique? Because the fellow you're responding to is giving substance to his criticism, the parent comment is giving substance, the article is giving substance, and whether GEB has substance is apparently up for debate. If the person you're replying to is deadset on proving the parent right, you are dead set on proving nothing. Please tell us what you disagree with, and the particulars of why, or maybe leave the discussion to the adults. reply pests 26 minutes agorootparentAll this enlightenment but apparently you can't be polite. reply generic92034 12 hours agorootparentprev> a meandering and badly written mess with no point Yeah, that is just typical for Pulitzer Prize winning books. /s reply inopinatus 1 hour agorootparentprevSuch sentiments are all very fine, but this is a book about classical music. reply DiscourseFan 9 hours agorootparentprevAh, nobody reads Kant anymore; and even if they do, they tend to read just the first critique, stick to the analytic and focus on the \"important\" sections. You have to read the whole thing! And then, if you're so privileged, and you have to time, nothing is more fruitful than going on towards the Groundwork for Metaphysics of Morals, and the Critique of the Power of Judgement. And once you've read that last book, maybe you will begin to understand Derrida. Or even better, you will begin to understand how to critique him. reply sparrowInHand 1 hour agorootparentprevThis. Philosophy is the grandparent of math. And if there is a point to math, then that there always has to be a proofable point. I think the problem is a cultural conflict between sub-concious problemsolvers and \"concious\" problem solvers. The later expect a algorithm , step by step instruction from the former, who when this is demanded, use there subconcious to produce a plausible story. When asked for there inspirations to deduce the process yourself, you get a crows nest of shiny things, bits and pieces, not making a reasonable whole - but they infuriatingly at the end of day do. Why not accept that there are things you can not directly communicate with and if you are hellbent on becoming such a thing, there is no step-by-step instruction. reply empath-nirvana 12 hours agorootparentprev> Philosophy at the end of the day is about arguing a point; it isn't about producing an aesthetic experience. So why would you abuse your reader by making it a pain in the ass to figure out what you mean? This is a funny thing to say about a discipline that was more or less founded by Plato, who was notably obscurantist if not outright esoteric. In any case, philosophy is not always about making a point and there is not always a point to be made. Sometimes it's just asking questions. reply ants_everywhere 9 hours agorootparentThis hasn't been my experience of Plato at all. I've always found his dialogs to be written with a surprising clarity. Is there a work in particular you're thinking of? reply serf 1 hour agorootparentParmenides, unless you're really into classical semi-formalized logic and that comes easily as explanation.. reply techno_tsar 8 hours agorootparentprevWhat part of Plato are you referring to? Republic is honestly written at a 10th grade English reading level. The most esoteric work of Plato is Timaeus, but most people do not see that as a foundational text in Western philosophy. reply swayvil 11 hours agorootparentprevWhen philosophy starts with a real observation (...then words, then discussion), obscurantism is appropriate and expected. Because words can only fit a real observation badly. When philosophy starts with words, a clear point is expected, for the obvious reasons. But that's two completely different worlds. reply ordu 4 hours agorootparentprev> I know there are some fancy arguments around stretching limits of language, but none of them seemed all that sensible to me. Did you read Robert Kegan \"The Evolving Self\"? It was pretty popular at HN some time ago. Robert Kegan stretching limits of language and he's got a reason: he introduces some notions like \"culture of embeddedness\" that you just can't define in a way like dictionaries do, one cannot simply get to a point. Kegan uses another approach, he gives his reader an experience that makes her to understand \"culture of embeddedness\". Philosophy by definition deals with uncategorised (or poorly categorised) aspects of our world, with aspects that have no good language to talk about them. Anything that was categorised to a point when there is an accepted language to talk about it is not a philosophy, it is a science or something. Philosophy makes first attempts to categorise and when philosophers found a way then a new branch of science emerges. When you try to talk about uncategorised things, you just can't talk about it. For example, lets assume that there are no widely accepted categorisation of colors, but you've developed one including names for colors like \"blue\", \"red\", \"magenta\" and so on. How you could communicate your ideas to others? The only way I know is to point to different examples of blue and say \"blue\", point to red and say \"red\". You need to lead your audience through experiences of blue/red/magenta while they train their neurons to classify them and only then you can refer to their experiences with words \"blue\", \"red\", \"magenta\" to convey the idea that they always appear in a rainbow in the same sequence. While you keep pointing at different colors saying their names, your audience will probably think, that it takes too long for you to get to a point. I didn't read GEB, so I cannot say how this reason to \"stretch limits of language\" relates to it, but you've sad that you know no good reason for such stretching, and I hope that now you know at least one reason for it. reply devwastaken 4 hours agorootparentprevPhilosophy is defined by the human experience, it is asking questions that do not have readily proveable answers, of answers that are heavily contextual to the individual. The probability of the answers weighed in the individuals head. These probabilities and likeliness to believe more of one philosophy over the other is often primarily from the details. Details are important because they are little parts that support the likelihood of the larger point being true. If the details don't work it's likely because the theory is innacurate. reply munificent 13 hours agoparentprevPersonally, I believe that the distinction between \"unnecessary detail\" and \"the point\" is mostly subjective and in the mind of the reader. The point of a book is to get the reader to reach a certain understanding. Every brain is different and while one literary path might get some readers to that destination, other paths may be required for others. But there is no singular point of understanding that exists solely at the end of that path. The path itself is what builds that understanding. Part of the art of writing is covering a large enough space of multiple paths to get most readers there without too many of them getting lost. reply felipefar 8 hours agorootparentThere is a very clear distinction between details and main point. If you assume that there is no main point in a text, you negate all that the writer is trying to communicate. reply TeMPOraL 4 hours agorootparentMaybe I've been reading books the wrong way my whole life, but I never assume there's a singular \"main point\" in the text. I mean, why would one waste time writing 100 or 1000 page text circling around some single point for the reader to guess, instead of spelling it clearly up front, and then arguing for it? reply jahnu 13 hours agoparentprev> Indeed, Ulysses is almost nothing but \"unnecessary detail\", to the extent that it makes the book all but unreadable I was always apprehensive of reading Ulysses… until I did. And then I read some basic analysis of it and then read it again and now I’m slightly baffled why people find it _that_ difficult. Yes it’s not at all typical but it’s really not that hard and it’s definitely no Finnegan’s Wake. Now that’s a challenge! reply ballooney 13 hours agorootparentI mean lots of the lore is just residual 16 year old’s getting into reading stuff. You read l’Estranger (translated), decide you’re an intellectual, and so have a go at Ulysses cos it’s the hardest one and so suitable for you, an intellectual. The internet has been a good transmission vector for the sentiment. If your ideas about cooking come from watching Masterchef (analogously), you might treat the idea of making a beurre blanc as a similar act of conquest, triumph in the face of splitting adversity whilst a thousand-voice choir crescendos in the background as you whisk. But a million french housewives happily made it for decades having never been told it’s impossible, and indeed knowing better that it isn’t. reply bdauvergne 12 hours agorootparentEverybody is cooking in France, not only housewives, it's a part of being French. reply jahnu 12 hours agorootparentprevHah! Nice cooking analogy :) reply 57FkMytWjyFu 10 hours agorootparentprevI like them apples. reply knightoffaith 12 hours agoparentprev>good for techies to read To be sure, you certainly didn't mean it this way, but for the sake of the poor guy who has seen GEB touted as one of these books that \"every programmer/computer scientist/CS major has to read\" and sees your post in the same light: You don't have to read GEB. You will be a fine techie without reading it. And you'll be a fine person in general without reading it. There is no spiritual revelation you can only get from GEB or some important technical knowledge that you'll get that you wouldn't get from a normal CS degree program. Try it out and see if you enjoy Hofstadter's writing - if you do, you might be in for a really enlightening and enjoyable experience. If you don't, no worries - your time is better spent elsewhere - there is no need to put yourself through the pain of slogging through a >700 page book that you don't enjoy. There are many other perfectly fine entry points for learning about topics in computer science, cognitive science, or philosophy of mind. reply nextaccountic 12 hours agorootparentI didn't read GEB. But I watched this https://www.youtube.com/watch?v=92WHN-pAFCs And it's an absolutely brilliant - and very direct - exposition of Alan Turing's Halting theorem. (unfortunately I can't find analogues of this for many other related subjects) Some people are saying that GEB is too convoluted, but the base material absolutely doesn't need to be. reply zqna 10 hours agorootparentNever understood this logical jump in reductio ad absurdum argument. \"If one finds a contradiction, therefore the initial premise is wrong\". The argument always assumes a binary state, either true or false. It excludes another valid state which is \"undefined\". The premise of the video above is effectively to prove that one can't build a machine that is capable of resolving a paradox. As if computers have limited capabilities and that they are uncapable to reason the way humans can. But that's not true. If you allow a machine to produce 3 answers, \"yes\", \"true\" and \"unresolvable\", then it is very much possible to build such a machine that produces such output, i.e. detects that a problem is indeed a paradox. By simply implementing this reductio ad absurdum algorithm. reply jlokier 2 hours agorootparentThe halting problem asks, \"Is it possible to write a program which correctly outputs YES or NO to the halting question for every possible input which is the code of a program?\" The binary requirement comes from the question, not a failure to think differently. The consequences of binary requirement with completeness (correct output for every input) is the point. If you modify the question as you suggest, to output UNRESOLVABLE in exactly the cases where the input is a paradox, that doesn't work either. It is not possible to reliably detect every self-referential paradox, for reasons analogous to (but more complicated than) it not being possible to reliably say for every program if it will eventually halt. Even so, it is possible to write a program which outputs UNRESOLVABLE in cases where that particular implementation of a halt-test program gets stumped and gives up despite there being another correct answer it hasn't found, or when it detects specific patterns. But that's more about hitting arbitrary limits of a particular implementation, so not as interesting theoretically. It is what you'd do in practice, if someone asked you to write a halting detector in the real world. The halting problem computation model has unlimited memory. Sometimes people say this means you can \"solve\" the halting problem in practice because real machines have bounded memory, therefore finite states, which must eventually repeat if a program does not halt. But this isn't really a solution, because the halt-test program needs exponentially more memory than the input program would be allowed if run. However you look at it, that is not \"in practice\", it is prevented by computational inaccessibility, and it also doesn't satisfy the principle of the halting problem, which is to ask if halt-test can be written in the same kind of universal programming system as the programs it analyses. (Besides, you can run programs with unlimited memory, by always being willing to pause, upgrade machine, and resume, whenever current memory is filled.) When analysed with denotational semantics, there actually is a third output called \"bottom\" (⊥) which means \"mathematical value representing doesn't terminate\". But even using mathematical approaches, there is no way to calculate when that's the output for all possible inputs to a correct halt-test function. reply zqna 9 hours agorootparentprevIn fact, there is an obvious way how to build a machine that tells whether the program will stop or not. One only needs to track all the states that the machine was in, and if a program enters a state that was already encountered before then it's obviously stuck. And there are only that many permutations of possible states that machine can be in. So a number (time) can be given of when the result will be guaranteed to be found (which of course could be beyond physical resources of the universe, but mathematics are fine not to take those into account) reply schoen 5 hours agorootparentThis is totally true for some models of computation (including some studied in academic computer science), but it's not true for the models of computation that the halting problem applies to, like the Turing machine, which explicitly has an infinite tape and therefore has the potential for an unbounded number of machine states. In fact, this unboundedness is a core part of what makes these models of computation so expressive. In the Gödel's incompleteness theorem analogy, you can say \"there is an integer such that ...\" or \"there is no integer such that ...\". In the Turing machine model, you can write programs that search for counterexamples to mathematical claims. Because these programs are written to try every integer, you can only tell if they eventually halt by yourself resolving the mathematical claim. For example, we can write a program that tests the Goldbach conjecture or the 3n+1 conjecture by brute force. Determining if these programs' search through all integers will halt or not is equivalent to resolving the status of these conjectures! reply FabHK 8 hours agorootparentprevwhile true: n += 1 reply knightoffaith 11 hours agorootparentprevRight. And to be sure, GEB might have Godel in the name, but it's not really a book about Godel's incompleteness theorems. It mentions them, but that is not really the main thrust of the book. It's not a book about Godel, Escher, and/or Bach, really (though of course it discusses the ideas of these three people a lot). Hofstadter, if I recall correctly, even says as much (or something along these lines) in the preface. So of course, there are more to-the-point resources for the technical concepts that Hofstadter employs. It's an understandable misconception to have when people so often say GEB is a must-read for techies and when it has a name that includes \"Godel\". So remember - GEB is about Hofstadter trying to argue for a point about a particular thesis (how the mind (\"I\") arises from the brain), and to that end, he employs many different concepts (many of which are from computer science) and explains them poetically (at least, a lot of people think so). But teaching you these concepts is just a secondary, even tertiary, goal of his. If you find his writing engaging, it might be a more fun way to learn about that stuff - but if not, don't worry, GEB wasn't primarily written to teach you anyway. I'm sure even the biggest GEB fans will agree. reply dmazzoni 11 hours agorootparentIt sounds like you're making a distinction between the plot and the theme. I'd argue the plot is entirely about Godel's incompleteness theorem. It's not just \"mentioned in passing\", the entire book is centered around a series of increasingly complex explanations that culminate in explaining the actual theorem itself. But just like a good novel is way, way more than just the plot, GEB is way more than just the Godel incompleteness theorem. Personally I didn't really find any other thesis or theme that compelling. However, I absolutely loved the clever ways in which he illustrated each concept. Others may have appreciated other aspects of it, and that's great too. Many people can enjoy the book for different reasons. reply trgn 10 hours agorootparent100% The Godel proof chapter feels like the apotheosis of the book. The rest almost feels tacked on, e.g. the whole dna-computing ... , even the \"strange loops\" part, which I would not think is the main crux of the book. reply knightoffaith 11 hours agorootparentprevI shouldn't have said \"mentioned\" to insinuate \"mentioned in passing\", you are right. But no, though it plays a central role in his argument, GEB is not about Godel's incompleteness theorem (I believe Hofstadter even says as much in the preface, and he laments that readers didn't get the overall point of the book - though I don't have my copy with me to confirm, so I might be misremembering exactly what he says). But anyway - you and anyone else of course are free to enjoy Hofstadter's explanations of Godel's incompleteness theorem - maybe you even think that it is the best/most interesting/most fun/most insightful explanation. I just mean to say that Hofstadter isn't writing this book primarily to explain technical concepts - his explanations are a means to an end. So someone who struggles to get through this book shouldn't feel bad - its main goal was never to be a \"must-read for programmers\" anyway. reply boznz 11 hours agorootparentprevIn 30 years I have read about 50 pages and looked at all the pictures. Does that count as reading it :-) reply dilippkumar 8 hours agoparentprev> If you're waiting for either GEB or Ulysses to hurry up and get to the mother fucking point, you're going to be waiting a long time. Ok, for us the lazy, what is mother fucking the point of GEB? A single HN karma point from me is on offer for the honest answers. reply sgdpk 8 hours agorootparentWhen systems (think \"automated\" systems like computer programs, mathematical axioms, formal systems, etc, where conclusions can be drawn/calculated \"mechanically\" from a few starting points) get large enough, they gain the ability to become self-referential. That is, they become expressive enough to encode statements about themselves. A hallmark of this are \"incompleteness theorems\" like those of Godel or the Turing halting problem. The book argues that these \"strange loops\" (of a system onto itself) are behind the emergence of intelligence and consciousness, because physical matter itself gives rise to human intelligence, albeit being a mechanical system. reply pgsandstrom 1 hour agorootparentI can appreciate a book being obscure or dragging things out when it is trying to give the reader an aesthetic experience. But this point seem to be one that would easiest be communicated clearly and succinctly. Or is the point \"there is so much mystery in these systems that perhaps there is room for an explanation for consciousness\"? Maybe then I would be more sympathetic. Or perhaps I should just read the book before condemning it :) reply serf 1 hour agorootparentthe point of the book is to give supporting evidence and guide the reader to the conclusion through testimony of thought and historical anecdote. in other words, GEB tries to coax the reader into a eureka moment, which is exactly why it has so many fans; it convinced each and every one of us that we were genius for just a split second. reply MatthiasPortzel 5 hours agorootparentprevGood summary. Examples of GEB’s “fluff” are missing from this comment section, so I wanted to jump in and add one of my favorite. Bach encoded his own name in music notes in a piece, and when discussing that, Hofstadter encodes a sentence in the first letter of each paragraph in that chapter. reply lefrancais 1 hour agorootparentGreat examples buddy.(I know it's bad). reply bazoom42 1 hour agorootparentprevGEB’s point is that self-reference - the ability for a system to “talk about itself” - is crucial for conciousness and for real artificial intelligence. reply therealdrag0 8 hours agorootparentprevIn the end, we are self-perceiving, self-inventing, locked-in mirages that are little miracles of self-reference. — Douglas Hofstadter, I Am a Strange Loop, p. 363 reply gradus_ad 8 hours agorootparentprevIt's turtles all the way down reply mcmoor 12 hours agoparentprevI've heard about this book so many times and was interested to read it. But I know that as a non native English reader, my track record against reading English proses is very poor, and looks like lots of value of the book is exactly in the prose? Maybe I'll have to skip this one, unless some brave souls have already translated it. reply martin_balsam 11 hours agorootparentI don’t know what language you speak, but GEB it’s been translated into multiple languages during the years. I read it in Italian as a teen, the Italian edition is beautiful, and incredibly well translated (the book includes many puns and language tricks.) reply mcmoor 3 hours agorootparentIt's Indonesian, which as I have guessed, doesn't have one at least as far as I google it. But it's astonishing that this kind of book have so many translations! Even into Chinese, wow! I guess it's truly one of most influential book. reply Pamar 10 hours agorootparentprevBut if memory serves the translators of the Italian version removed a whole chapter of it (I might have to check details because I am far from both my author-signed English edition and the Italian one I bought many years later). reply Phemist 11 hours agorootparentprevThe Dutch translation of GEB is excellent. I think others, e.g. the French, are also quite good/excellent, potentially because Hofstadter himself was quite involved in the translation process. Hell, much of GEB and especially his later books discuss translation extentensively, so it would be strange if there were authorized translations of GEB that were somehow lacking in quality. reply mtremsal 10 hours agorootparentCrucially the French translation was significantly rewritten, with contributions from Hofstadter. In doing so, the French version becomes a separate projection of the author’s original ideas, along a different vector than the original English version. The approach therefore illustrates the main point of the book, similarly to its cover. Very meta. :) reply kragen 8 minutes agorootparenti had no idea; now i have to read it, because i've only read the english original and part of one of the spanish translations reply the__alchemist 13 hours agoparentprevIs it also valid to draw a comparison to Gravity's Rainbow, or parts 2 and 3 of The Divine Comedy? I ask, because those were on a tier of their own for being impenetrable (to me); I'm worried this will go the same way, but am otherwise intrigued. Unrelated: Does anyone know of how to get an epub etc of this? Unavailable through Amazon etc. reply poncho_romero 12 hours agorootparentI would suggest against an epub. I tried finding one too, only to realize the formatting of the book is ill-suited for anything but its original printed format (it’s far too particular for the epub format). Maybe give your local library a try? reply lanstin 9 hours agorootparentprevHell is just inherently more interesting to the human mind than paradise or purgatory. I read Ulysses with a book group and a leader that had read it in grad school and a number of books that exist to explain Ulysses. I read GEB as a fourteen year old with no internet access and it changed my life, but the ending isn’t that important. The proof of how the incompleteness theorem works and the stuff on Koans is I think most of the meat. reply davedx 10 hours agorootparentprevGravity’s Rainbow. I never finished it. It’s more like One Hundred Years of Solitude though; GEB isn’t fiction reply greggsy 13 hours agorootparentprevThere are several copies on Internet Archive. reply daseiner1 12 hours agorootparentprevlibgen.rs reply Joeri 12 hours agoparentprevIt is funny that you bring Ulysses into it. I’ve tried reading both GEB and Ulysses multiple times and had to concede defeat every time somewhere between the 100 and 200 page mark. The same goes for the satanic verses. I suppose my mind just wants a book to get to the point more than it wants to get to the end. reply lisper 11 hours agorootparent> It is funny that you bring Ulysses into it. Actually, the original article did that. I just followed the author's lead here. reply nabla9 12 hours agoparentprevWell said. It won Pulizer for a reason. It's not for learning facts, its for thinking. GEB ponders and it's masterfully written. I can also recommend Metamagical Themas from Hofstadter. It's a collection of articles he wrote for Scientific American. reply hobs 13 hours agoparentprevAnd to be fair Hofstadter got the message and wrote \"I Am a Strange Loop\" for people who just want the to the point version. reply davedx 10 hours agorootparentHa I tried reading I Am a Strange Loop and found that meandering and verbose too. Didn’t finish it. He really labors the “greater than sum of parts” point for example. Ugh reply keithalewis 12 hours agorootparentprevI wonder if anyone on the Pulitzer Prize committee for GEB understood this was the idea he was attempting to communicate. There seems to be a large market for books that make dumb people feel smart. reply cout 12 hours agoparentprevI have not read Ulysses but I have read other works from Joyce and cannot imagine comparing him to Hofstadter. I put GEB in the same mental box as Sophie's World or the Mr. Tompkins books, where story is used as a means to the end of teaching something to an audience that would otherwise find the material unpalatable. reply lanstin 9 hours agorootparentThey both loved word play and enjoyed examples as much as they enjoyed generalities. reply mmaunder 12 hours agoparentprevThanks. Bailed on the motherfucker for this reason and will have to revisit. reply alkonaut 13 hours agoprevI found this book dull, uninteresting and pretentious. Whatever it tried to say in what 1000 pages could have been said in 200. People will probably recommend it highly in this thread, but here is a vote to just leave it alone. It's just not good if you like pop sci but don't like pretentious fluff around it. It's the least inspiring and mind-blowing book I ever read. This could be related to having read a lot on the topics in the past so the subject matter was familiar, and having zero tolerance for the type of writing in it. reply hnfong 7 hours agoparent> if you like pop sci That's the \"problem\" right there. It wasn't a pop sci book. TBH while Gödel's incompleteness theorem might be seen as \"science\" subject, it is actually squarely in the realm of meta-mathematics, a branch of philosophy. The \"writing style\" is what most would call \"literature\", which includes prose, poetry, stories, etc. It's not for everyone, for sure, but some people do enjoy it (I occasionally do, but I lose patience.) Calling it \"pretentious fluff\" sounds a bit extreme. reply qarl 12 hours agoparentprev> This could be related to having read a lot on the topics in the past so the subject matter was familiar I think you've hit the nail on the head. For many people, this book is their first encounter with much of this material (as it was for me, so long ago.) For you, it's like reading a tour guide of your home city. You're not the intended audience. reply tptacek 12 hours agoparentprevIf you're not a reading-for-pleasure person, or GEB's topics just aren't your thing, you're not going to like the book. It's not a technical volume; it's not something you read for skills acquisition. reply holgerschurig 24 minutes agoparentprevI had totally not your experience. I got this book when I was at the first semester of IT. Back in my university town, most students of IT or physics had this book, or lented it from a friend. And we discussed a lot about what was inside. So I wasn't a seasoned academic, but the new-kid-on-the-block. And my goal while reading was never to understand Gödel. Or to like Bach's music (I actually dislike most of his music). Or to get into arts -- but hey, Escher I like. My goal was to train my mind. To get into thinking models new to me, because they aren't taught in normal school. Also, for me this book was an extension. Even while still in normal school, I went to the university library to read \"Spektrum der Wissenschaft\" (the german version of \"Scientific American\", but without the nationalism in the title). Many articles were over my top ... but the \"Metamagicum\" articles I deeply enjoyed. So when this book come out I expected some extension of these articles ... and I was not disappointed. reply dtgriscom 1 hour agoparentprevI enjoyed the first part of it, but then ground to a halt about two-thirds of the way through. It became just so much recursive navel-gazing, and I lost interest. Wasn't worth the effort. reply nextaccountic 12 hours agoparentprevAnd to this I want to quote the first comment of this thread https://news.ycombinator.com/item?id=17461506 \"Why I Don't Love Gödel, Escher, Bach\" > stuntkite on July 5, 2018flag| favorite> I'd owned this book for many years and had a similar experience to OP. But one day someone gave me I am a Strange Loop, which I started reading and enjoyed way more. After getting in a little bit Hofstadter makes some apologies for GEB saying it was the sum of work of a very young person. I think he was 24? I think it's pretty incredible considering his age. The things that lead him to thinking critically about consciousness because of his disabled sister I found to be something that changed not only how I interacted with people that were differently abled, but also changed how I saw my own disabilities. Sure it's more than a bit full of itself, but I can't for the life of me think how I would edit it any differently. It's honest from the place he was standing. With a lot of thought, a desire to share, and a perspective that no one else could just stumble on. IMO, it really does kind of have to be what it is. > So, I decided to put down IAASL and try to really get through GEB first.. like for real this time... and found an Open Courseware[0] on the book to follow hoping that would help me really cut through it. It did! The trick that did it for me was the prof's suggestion that you just skip the first 300 or so pages and he picked up from there. > I'd thumbed through it and much like op had \"looked\" at every page, but once I skipped the first 300 and followed along with the course, it was like butter. There is something funny about all the intro dialogs that can fatigue by the time you get to the meat. > That said, I really enjoyed his reflection on GEB in IAASL and enjoyed the read of IAASL a lot more. Regardless of what you think about what Hofstadter proposes, I think his contribution to critical exploration of the consciousness is artful and invaluable. I think it's fun, compassionate, and beautiful, and it really changed how I see myself and my environment. > It sticks with me and I think about it a lot and it makes me happy to share it with people. > [0] https://ocw.mit.edu/high-school/humanities-and-social-scienc... reply wirrbel 12 hours agorootparentI think u never made it past the first 300 pages and now I wonder whether I should try again. I also wonder what happened to my copy of the book… reply tptacek 12 hours agorootparentprevSkipping the first several hundred pages is I think a really good trick for people already familiar with and jaded by the subject matter. reply jacobolus 12 hours agorootparentprevAccording to Wikipedia, Hofstadter was 34 (and 4 years out of physics grad school) when GEB was published. reply greggsy 12 hours agoparentprevI gave it a good hard go, but came to the same conclusion. It might (should?) have been razored to half the size by a more judicious publisher. reply dmazzoni 11 hours agorootparentFor those of us who enjoyed the book, that'd be removing the best part. For me this was one of those books that was more about the journey than the destination. 2001: A Space Odyssey could easily be trimmed to 25 minutes or less if all you care about is the plot. But should it? reply HKH2 7 hours agorootparent> 2001: A Space Odyssey could easily be trimmed to 25 minutes or less if all you care about is the plot. But should it? Yes. Tell me that you just sat there watching it without being distracted by your thoughts at all. Being able to handle torture doesn't make torture good. reply jamilton 2 hours agorootparentIs being distracted by your thoughts so terrible that the rest of the movie should be expunged? I'm more of the opinion that 5 minutes of graphics that were probably impressive at the time could be cut. reply HKH2 1 hour agorootparent> Is being distracted by your thoughts so terrible[?]... No, but it probably means you're forcing yourself to watch/like it. I never suggested expunging the movie. Regardless, excessively long movies/books expunge themselves unless they're famous enough to namedrop. reply greggsy 10 hours agorootparentprevWell, part of the plot is about humanity’s slow and inexorable evolution through the passage of space and time, so the meditative slowness sets the pace quite appropriately. GED (in my humble opinion etc), doesn’t really need some of the fluff. It’s a large book that’s easy to spot on the shelf, and it’s hard to avoid thinking that the publisher (and some readers) like it that way. reply joshxyz 11 hours agoparentprevsame sentiments. i was on 10th page and i was chuckling because i still got no idea what these geeks are talking about. definitely the day i realized im not that smart, too. haha. reply HKH2 7 hours agorootparentYou are smart for putting it down. reply pierat 12 hours agoparentprevWell you just don't understand it! /snark That's the usual refrain around hyper-preventious navel gazer books. The moment you criticize, your intellect is up for question, because \"you didn't understand it\". This form of logical fallacy is the worst in economics and philosophy. reply peterashford 9 hours agorootparentWell, yes. But that's also a convenient defense for people who didn't understand it. :o) reply mrbonner 10 hours agoparentprevYeah, I don't think it is for me either. I tried a few times but always found that the style of writing is so strange: if this is a science book, I expect succinct style. Instead, I found the dialogs of Achilles and the turtle are just abominable. What the heck are the intention of the author? Just write it like a science text book and I will probably get it. Also, as a big fan of Bach this book has less than 5% content about Bach. reply hnfong 8 hours agorootparentThat's just it. It isn't a science book, I don't think it ever was intended to be one. reply kleiba 13 hours agoparentprevDe gustibus non est disputandum. reply Almondsetat 13 hours agorootparentDe gustibus non disputandum est reply gordon_freeman 13 hours agorootparentprev\"In matters of taste, there can be no disputes.\" reply lanstin 9 hours agorootparentI am chris@disputingtaste.com At some point in life one realizes that there is important stuff in good taste. “I just couldn’t call a bad book good.” To quote Dorothy Sayers. reply pierat 12 hours agorootparentprevGo ahead, eat the book and let us know it tastes! reply booleandilemma 13 hours agoparentprevWhat's the most inspiring and mind-blowing book you've ever read? reply tootie 12 hours agorootparentNot OP, but as someone who found GEB pretentious, I think the most mind-blowing books I have read were probably Kurt Vonnegut. Mother Night and Timequake probably at the top. reply mynameisnoone 3 hours agoparentprevYep. It's like a conspiracy theory cult. I quit after 50 pages and found it to be a pseudoscientific, dilettante intellectual circle jerk ad nauseum desperate for hidden meaning. I'd sooner spend time reading articles from [Big City] Review of [Each Others'] Books. reply tootie 12 hours agoparentprevI feel exactly the same. It was like 1000 pages of patting himself on the back for being clever. I certainly didn't learn anything and there was very little art to his writing. reply empath-nirvana 13 hours agoprevI'm going to try and explain why it made a difference in my life as briefly as I can. I read it in the early 90s as a teenager at a community college in the suburbs, along with the Richard Dawkins' The Selfish Gene, at a point in my life when I was struggling with having a lot of doubt about Catholicism, and the _one_ thing that was keeping me from just giving up on religion entirely was that I just couldn't understand how the experience of _being_ could be anything but a spiritual soul, and those two books gave me the intellectual tools to basically completely rebuild my entire conception of what and who I was -- which is to say that I could finally see consciousness as the an emergent property of ordinary matter, and the relationship between consciousness and computation. It's an experience you can only have if you read the right books at the right time in your life. You can only be exposed to any particular idea for the first time once in your life, and if they ideas in those books are not new to you, I'm sure you'll find GEB pretentious and ponderous or whatever, but for me it was like fireworks going off on every page. I read it and reread it and took notes on it and used it as a launching point for more reading for years afterwards. edit with some extra thoughts: Keep in mind this book was written in 1979, when vanishingly few people had access to computers, let alone the internet. There was no wikipedia you could go to to scratch an itch about some topic. GEB is encyclopedic in scope and meandering because it _had to be_, he couldn't expect an audience who were familiar with _computers_ let alone artificial intelligence and set theory. It's really extraordinary that it's accessible as it is, given the breadth that it covered. Today, given the advances in all the things he was talking about, I would think it's mostly interesting to read for historical reasons. reply felipeerias 1 hour agoparentFunny, I did pretty much the same journey when I was young. As I grew older, I became disenchanted and eventually journeyed all the way back. There are questions about conscience, the origin of the universe, the ultimate nature of reality, etc. that we haven't answered yet and perhaps never will. We have theories and we have models. We don't know which theories are correct yet and, if we eventually find a model that seems to work, we might not even be sure if it reflects reality or simply predicts it (like Newton's equations). Furthermore, not all questions can be answered in this way. Every ethics system is grounded on metaphysics in one way or another: on concepts which are completely human-made, culturally-dependent, non-observable. Even the most Darwinian doctrines do this: genes don't actually \"want\" to be preserved and passed on, any more than rocks \"want\" to fall. Religious are simply more explicit about this than other belief systems. Finally, it certainly doesn't help Dawkins and his followers that, despite claiming to be on the side of reason and truth, systematically let their judgements be influenced by their prejudices and preconceived notions. A more informed and grounded view of history would understand that a notion of the transcendent and divine was the foundation for much of the progress of humanity. reply jijijijij 5 hours agoparentprevI can't recommend The Selfish Gene enough. I wish it was mandatory reading in school or something. Contrary to GEB it is very accessible and a phenomenal introduction to the theory of evolution. It gets a point across, school books don't. However, Richard Dawkins has become a very detestable person, cringeworthy culture warrior, rage beneficiary. So, make what you want with that info. Maybe consider a library or pirate the audio book. reply jabl 1 hour agorootparent> However, Richard Dawkins has become a very detestable person, cringeworthy culture warrior, rage beneficiary. Yeah.. My personal introduction to Dawkins was actually some TV documentary he made that covered about the same topics as the God Delusion, then I read the book itself. Only after that I read the Selfish Gene, which IMHO philosophically was a much more interesting book. I did sort-of follow the \"new atheism\" movement for a time, the \"four horsemen\" and all that [1]. But it seemed to pretty quickly spiral into the cringy culture war thing you mention, so I stopped. [1] I don't agree with many of the things Christopher Hitchens said, but damn he was a good orator and public debater. reply cscurmudgeon 12 hours agoparentprevFor me, it was the opposite. Reading Shadows of the Mind after GEB led me to finally see consciousness is NOT an emergent property of ordinary matter. reply tasty_freeze 12 hours agorootparentI find Penrose's argument uncompelling: he doesn't see how ordinary physics could result in the sensation of beingness and experience, and we don't really understand quantum mechanics, therefore quantum mechanics is responsible for consciousness. (obviously, his book works on it for 500 pages so my summary is a parody, but that was the gist as far as I can remember) reply bbor 11 hours agorootparentOr one could speculate that consciousness arises from as-yet-undiscovered noncomputable laws of quantum gravity operating within brain structures called microtubules, as Sir Roger Penrose did in his 1994 book *Shadows of the Mind* Heh yeah doesn’t seem great. I think it’s you nailed it. This is from one of the reviews posted above reply knightoffaith 12 hours agorootparentprevYou may want to read https://www.scottaaronson.com/writings/finite.html. Perhaps also https://iep.utm.edu/lp-argue/. The Lucas-Penrose argument is not generally accepted among philosophers. Of course, that doesn't necessarily mean it fails - truth isn't a popularity contest - but it does indicate there are some subtleties at play here; it's not so obvious Emperor of the Mind/Shadows of the Mind succeeds in its argument. reply hnfong 7 hours agorootparentSpeaking of Scott Aaronson he has more recently (2013) wrote a much longer exposition about consciousness and (quantum) computation, and has a chapter specifically for Penrose: https://www.scottaaronson.com/papers/giqtm3.pdf reply anon291 12 hours agoparentprevI haven't read the book but I believe many people confuse consciousness with qualia. I believe Hofstadter does as well and he's become critical of his earlier work. reply empath-nirvana 12 hours agorootparentEvery book about the nature of consciousness has problems. It's not a solved problem. For me it wasn't the details but just laying out the landscape and I could see how to get there from here, if that makes sense. reply adastra22 6 hours agorootparentI wish people wouldn't parrot the \"it's not a solved problem\" line. There are an abundance of solutions: take your pick. reply bbor 11 hours agorootparentprevBeautifully put! I recommend “Kants System of Perspectives” by Palmquist, it’s free online, short/skimmable, and explains in detail how Kant (200y ago!) was trying to do exactly this; acknowledge the unknowable parts of the problem, and instead focus on reasoning out the “landscape” or structure of it reply FabHK 7 hours agorootparentWhat an unexpected connection: Stephen Palmquist used to run the Hong Kong Philosophy Café, until he left the city recently, possibly in the wake of the political changes. https://en.wikipedia.org/wiki/Stephen_Palmquist Here's the free online book you alluded to: http://staffweb.hkbu.edu.hk/ppp/ksp1/toc.html reply strogonoff 5 hours agorootparentDoes anything like Philosophy Café exist in that city nowadays? reply jrflowers 13 hours agoprevGEB is crucial reading in that if you want to appear intellectually superior all you have to do is sprinkle “Ho ho! Much like the eternal golden braid I must say!” into conversation and no one will call you out on it or ask you to extrapolate (or if in the off chance that they do, you can say anything and still get away with it) The best response to someone bringing up GEB in casual conversation is to look them dead in the eye and simply say “I have also read that book.” This will instantly create palatable tension and a change of topic reply jacobolus 12 hours agoparent> palatable tension and a change of topic When someone enthusiastically mentions something they liked and wanted to talk about and you immediately take a shit on it, it's not really a surprise that this creates \"palatable tension\" and a change of subject (and likely a longer-term wariness to share when talking to you). If you really dislike discussing related topics, there are surely less condescending ways of expressing that. reply greggsy 12 hours agorootparentThey’re implying that many people use it as a way to take some moral high ground in a conversation, not knowing that others might also have acquired this ‘intellectual power’. reply jacobolus 12 hours agorootparentMy experience is that when faced with what seems at first like pseudo-intellectual nonsense, it's usually more productive to either explicitly say I don't feel like discussing the topic, or else try to get someone into a serious conversation about the details, instead of trying to insult or shame the other person. Sometimes people are just bad at smalltalk / earnestly oblivious to the impression they leave / trying hard to impress for whatever reason, and aren't really trying to be pretentious even if they initially come across that way. YMMV. reply karmakaze 13 hours agoparentprevI took the book with me on holiday and I couldn't put it down, almost literally reading right up until lights out each night. I was surprised and somewhat disappointed to be done in short time. The literary writing combined with the deep mathematical/philosophical meanings is entrancing. I don't often get to meet people IRL who have read the book and wish I had more opportunities to discuss it. One (of the many things) that stuck out to me was the idea of foreground and background. Prime numbers to me is background that remains when you construct all the composite numbers, so technically they're 'non-composite' lacking the property of being a product of distinct numbers. reply bqmjjx0kac 13 hours agoparentprevIs it possible these people are attempting to make conversation and link fun ideas together, rather than just trying to appear \"smart\"? reply akoboldfrying 13 hours agorootparentThat's what makes it tricky: Both are possible. reply adverbly 7 hours agoparentprev> GEB is crucial reading in that if you want to appear intellectually superior... This will instantly create palatable tension and a change of topic Ouch... Why would you assume that the other party's goal is to appear \"superior\", and not that they are legitimately passionate about something? Do you dislike when people are passionate about topics that don't interest you? Or do you just believe that it is fair to assume that everyone who outwardly likes this book is secretly doing so because they want to seem smart or something? Or something else? reply The_Colonel 13 hours agoparentprevThis is my impression as well. Kinda similar in its \"bragging rights\" to The Art of Computer Programming. GEB was a frustrating read. I mean, it's interesting in places, but it's just all over the place, jumping between many different topics. The central theme is meant to be the strange loops, but it's IMHO not very interesting concept and his application on the cognition is just author's personal conjecture. reply jacobolus 13 hours agorootparentIt's utterly unlike TAOCP. One is a comprehensive algorithms reference full of (hard) technical problems. The other is an extended personal essay. (Neither one is worth \"bragging\" about reading in my opinion.) \"Reading\" all of TAOCP would take literally years of intense effort even if you set aside all other activity. There are a lot of great problems inside, and plenty of dry humor, and I would recommend people try to at least skim sections of TAOCP which seem interesting or relevant to their work, but very few people are going to even nominally work through the whole thing, and the people who might are professional scholars of the topic. Reading GEB can be done leisurely over the course of a few days or maybe weeks, depending on how much time someone spends reading every day. It's not quite as easy a read as a pulp novel or comic book, but it also doesn't take any inordinate amount of work to make basic sense of, or require any special skills or background understanding to start on. It's a fun book to hand to a ~13–16 year old. reply jimhefferon 12 hours agorootparentYou are absolutely right. It was a great book to hand to a 21 year old me. I've often read the hate on this site for this book. At least for me, I find the discussions and analogies to help me in thinking about, and eventually understanding the material. I contrast it with a graduate intro to Recursion Theory which can leave a reader feeling that they followed all the precise arguments but still somehow missed a lot. reply The_Colonel 12 hours agorootparentprevI compared the two in the sense how it's fashionable to have them on your bookshelf, but IMHO few people actually enjoy them and understand them beyond the surface level. reply jacobolus 12 hours agorootparentThis discussion is evidence that some people really liked GEB and other people found it boring or too unfocused. It can't be that many people who bought it just to look cool on a shelf. The people who found it boring should perhaps try to appreciate that sometimes other people can genuinely like things they don't like (and vice versa I guess). Again, if you do any work with computer algorithms, it's worth checking out TAOCP at the library and skimming the sections relevant to your work. If you might need it as a reference, it's not a bad source to have at hand; I look things up in there maybe a few times a year for the past decade. Some parts are now a bit outdated in this fast-moving field, but it's still the best available survey source about some topics, and there are some nice explanations and a lot of great problems in there. Knuth is a pretty funny writer if you enjoy dry humor. reply kenjackson 13 hours agorootparentprevLet’s be clear. No one has just read the Art of Computer Programming. reply bqmjjx0kac 13 hours agorootparentMr Knuth almost certainly has reply comonoid 11 hours agorootparentNo, he wrote it only. reply BirAdam 12 hours agorootparentprevDebatable. How much did he dictate to a ghost writer? reply bqmjjx0kac 7 hours agorootparentI feel like it's not that kind of book, but I suppose you never know... reply DonHopkins 11 hours agorootparentprev>\"bragging rights\" to The Art of Computer Programming Or bragging rights to \"The Anatomy of Lisp\"! https://archive.computerhistory.org/resources/access/text/20... reply silviot 1 hour agoparentprevThis is very different from my experience. Whenever someone I was in a conversation with brought up GEB, it was always a great pleasure of mine. I'd get the chance to discuss the main ideas of the book, and the way I assimilated them. I tend to not even engage in conversations with people who do it mostly to show off the extent of their knowledge. I believe this second point is the important one. GEB is completely orthogonal to the problem you describe. reply Trasmatta 10 hours agoparentprevYour response to somebody appearing to be intellectually superior because they bring up GEB is to act even more intellectually superior? It sounds like you feel you're so far beyond them, you won't even engage in a discussion about it. reply matsemann 13 hours agoparentprevDo you also harass people for wanting to discuss a movie they just watched? reply tptacek 13 hours agoparentprevThis is funny but GEB is also good so you wouldn't want it to go much further than this. Congratulations for getting there, now it would be great if you could focus that same energy on shooting down people trying to build upon or me-too this snark. reply jrflowers 13 hours agorootparentIt is a good book, but it is a shame that so many pitch it as being a portal into a new and transcendent plane of understanding. Especially with it being a rather difficult read it leads to people trying to get more out of it than was in it to begin with. To quote one of my professors from back in the day: “Life is short and you don’t have to read it if you don’t want to” reply tptacek 12 hours agorootparentI'm with you. It's legit good snark; the problem is that it's asymptotically good. :) reply sam_goody 12 hours agorootparentprevThere is also the issue that it takes longer to read than you expect it to, even when taking into account that it will take longer to read than expected... ;) reply kevindamm 13 hours agoparentprevMy favorite response is to ask deadpan if they've finished reading the book. There is only one appropriate answer, IYKYK. reply tibanne 13 hours agorootparentI don't know. Tell me Kevin. Damm. reply kevindamm 12 hours agorootparentWell, I don't want to give too much away but... it has something to do with one of the interpretations of RICERCAR. reply Almondsetat 13 hours agoparentprevThe best response is to actually know things about Godel, Escher and Bach reply dilyevsky 12 hours agoparentprevWhat if they counter with “do you like apples?” reply jrflowers 11 hours agorootparentI offer them an apple slice from the bag that I carry around in my pocket reply huytersd 13 hours agoparentprevOr you can read it and not tell anyone. This comment is a pretty pathetic attempt at shaming anyone who displays even a modicum of discourse higher than the baser level. Congratulations. reply ctrw 13 hours agorootparentnext [4 more] [flagged] huytersd 13 hours agorootparentIt’s a grammatically correct sentence that uses all those words correctly. reply ctrw 13 hours agorootparentIt maybe grammatical but it is not semantically correct. reply huytersd 2 hours agorootparentSure is. reply cjfd 13 hours agoprevI loved this book when I was young. Like 16 years old or so. I am still very interested in things like formal systems and automated theorem proving and that started with this book. However, when I now look at the main idea of the book I find it quite cringeworthy, because, besides when he is speaking about real mathematics and science, much of it is very speculative and probably just false. At best it can be thought provoking, but I think it is just not very nice to immediately answer the some very real questions with highly speculative answers. It snared up the admiration of my 16-year-old self pretty effectively, though. reply zw123456 13 hours agoprevWhen I was working on m Masters degree in Electrical Engineering way back in 1978, when dinosaurs roamed the Earth :) I had a prof who was really amazing, he was a Comp Sci prof and he gave me a copy of GEB and it changed my life. OK, I'm probably overstating a bit, but that first edition copy, it's one of my prized possessions. Even to this day, every once in a while I pull it out and re-read a chapter. reply dmazzoni 13 hours agoprevI loved GEB, I read it twice and found it mind-blowing. That said, I don't think it was life-changing in the sense that it gave me any interesting perspective on life in any way. I didn't find any of the philosophies to be useful in that sense. However, what the book does do, is manage to explain an incredibly complex, deep mathematical theorem while using almost no mathematical notation. It does it all mostly through similes and wordplay and art, which is quite brilliant. One of the great things about the book is that even if you give up on the math, you can still appreciate each chapter as clever writing in its own right. reply 3abiton 7 hours agoparentAny books that changed your perspective on life? reply PaulDavisThe1st 6 hours agorootparent\"The Dice Man\" by Luke Reinhart. In my teens it didn't hurt that it had some pretty intense pornographic sex scenes, but the real takeaway - the idea that there is no \"singular you\", just a swirling multitudes who bob to the surface for attention depending on the context ... yeah, that changed my perspective on life. Too bad about the misogyny. Might even have been racist, but I don't remember that. reply adverbly 5 hours agoprevGEB is my personal bible. It looks like I'll need to write a post just like this when I have a chance as it impacted me for all of those reasons, but my most significant take-aways(abstraction, the significance of analogies, and the meaning-of-meaning) are not even on OP's list. reply codr7 13 hours agoprevSo people say; I usually have a pretty high tolerance when it comes to difficult books, but just couldn't keep my eyes open trying to read this one. Maybe I should give it another try... reply tetris11 13 hours agoparentSame. It has the exact same junior high-school giddiness of excitement as The Martian, another book that I just couldn't tonally work my way through. I don't mind books that explain concepts in fun ways, but I do find it jarring if I'm being treated like a child with an overbearing parent, telling me why I should be excited about something instead of just telling the story and letting me feel how I want to feel about it. reply perrygeo 13 hours agoparentprevI wanted to love this book. I understood the concepts and found them legitimately thought provoking - I just disliked the writing style. He uses elaborate metaphors, strained socratic dialogue, peppered with cultural references and visual cues... and relatively few paragraphs actually articulating the core ideas. I only understood Godel's incompleteness theorem when I looked it up elsewhere. It's like he focused entirely on the mystical \"look how deep all this stuff is...\" story and forgot to actually explain the subject at hand. reply OldGuyInTheClub 13 hours agoparentprevI share your experience and I've tried it three times. Couldn't get through any of his Metamagical Themas articles in Scientific American, either. I thought he was in love with writing, not communicating, and didn't want to remove a word once he'd put it down. reply lxe 13 hours agoparentprevI'm also struggling with it despite having it on my bookshelf for a while. Not an easy read. It's cited and referred to all over the place, so maybe it's worth getting through it. reply Vecr 1 hour agorootparentI've only read a few pages around a couple of cited parts, just to make sure the citing author is not just making things up. It's too long otherwise. reply munificent 13 hours agoparentprevThere's a point in the middle where he's building a foundation of formal systems that's a real slog. If you push through that, it gets progressively more interesting. reply hobs 13 hours agoparentprevI would say give yourself license to skip some stuff, the parable stuff (for instance) is either super fun or a chore, depending on the mood you are in. A lot of the Achilles stuff is more akin to poetry instead of an illuminating guide. reply syllablehq 13 hours agoprevThis book played an important role in pushing me into a career in software. During the recession in 2009, I lost my industrial design job. And GEB started me down a rabbit hole learning more and more about software and learning to code. That turned into a great software career. I've also read hacker news just about every day since. reply svat 13 hours agoprevThe most valuable thing about GEB for me was how self-indulgent it is. The book is entirely Hofstadter having fun: all those tricky dialogues, acrostics, puns where the setup and payoff are hundreds of pages apart, the marrying of form and content, just the overall tone of excited sharing…. Hofstadter has put a lot of himself into it—it's a deeply personal book—and it was revealing to me to see that one's emotions don't have to be set aside when writing, nor is it necessary for the kinds of feelings that mathematics evokes to be “translated” into more familiar ones. (Edit: As an aside, Hofstadter's translation of Pushkin's Eugene Onegin is similarly self-indulgent, and a joy to read. Of course one ought to read a couple of other translations first and keep them nearby, to become familiar with the content, but in terms of sheer wordplay and outrageous rhymes, it tops anything.) (Edit 2: Ha, a search reveals I posted a similar comment a little over a year ago: https://news.ycombinator.com/item?id=32830008 https://news.ycombinator.com/item?id=32878471) reply khazhoux 7 hours agoprevFun fact: 50% of printings of GEB have the pages all blank. This is rarely noticed. reply bobosha 13 hours agoprev\"if you have a copy of GEB on your shelf collecting dust and you’ve never read more than a chapter or two, dust it off and see how it goes this time.\" I will freely admit I am one of those who tried to read it multiple times, but couldn't grok it. reply digitcatphd 13 hours agoprevMIT Course on GEB: https://m.youtube.com/watch?v=lWZ2Bz0tS-s reply yreg 13 hours agoparentAre you supposed to watch the lectures after reading the chapters (which ones?) or how does it work? reply jiriro 11 hours agoparentprevOh, this is brilliant! Thanks for sharing. This is a very good introduction to the book. The goal of the book/lecture is to show how meaning emerges from not-meaning. So cool!:-) Also at the end of the lecture 1 he says that the book was written like Bach’s works - like a piece of music with theme, repetitions, inversions etc. Going to read it asap:)) reply tedheath123 13 hours agoprevMaybe the book went over my head but it didn't live up to my expectations. The book's thesis seemed to boil down to \"isn't recursion cool, maybe it has something to do with consciousness\". I did enjoy some of the digressions though. reply trgn 10 hours agoparentI thought it was going to be first something like \"isn't recursion cool\" (like, look at these fractals in sea shells!!!!), until GEB actually tried to explain, in detail, how Godel's proof works. tbh, that's kind of what makes the book cool, it just assumes anybody with a passing interest in compsci would also love to dive deep into fundamental theorems of mathematics. Those sort of bold assumptions, and the author just doubling down on them (e.g. you're going love these dialogues!). Crazy that it won a Pulitzer too, a real bestseller. reply jameshart 12 hours agoparentprevThe GEB concept of 'self reference' is not the same as 'recursion'. reply bazoom42 13 hours agoparentprevConsider it the Lisp equivalent to _whys poignant guide to Ruby. reply kibibu 13 hours agorootparentThat is about the strongest anti-recommendation you could have possibly made for me. reply atmosx 14 hours agoprevSomewhat related book: https://www.logicomix.com/en/index.html reply et1337 12 hours agoprevGEB is like Git. Git is fundamentally a DAG, that’s it. It’s beautiful. But it’s expressed via a bunch of tortured confusing words like refspecs, detached heads, tree-ish’s, and on and on. GEB is a love letter to some beautiful elegant ideas, but it’s expressed via a confusing assortment of record players, tortoises, and puns. One StackOverflow answer made the incompleteness theorem more clear to me than this whole book. reply dventimi 11 hours agoprevAfter 742 pages and even after having written the paragraphs above, I still struggle with a simple answer to the question: “What is this book about?” The best I can come up with is that GEB equips you with mental models to contemplate philosophy. No thanks. I'm not reading a book that even its fans can't adequately explain. reply mp05 5 hours agoprevI had an ex that went to Bard and she adored this along with the zen motorcycle book, the windup bird book, and pretty much anything by Gladwell and his contemporaries (definitely that pretentious Lehrer guy). The comments in this thread confirm that I was correct to avoid it. reply auggierose 12 hours agoprev> That’s a lovely fantasy, but Gödel shows that there are fundamental epistemic limits to the universe, things that no genius will help us to know, no alien race could teach us, no machine could be built to solve, and no new kinds of mathematics will uncover. That's not quite true. As an example, take differential calculus. It is hard to see how that would have been an idea to arrive at automatically by an algorithm, but a few geniuses (Newton/Leibniz) got there anyway. Similarly, even when a theorem doesn't follow from the usual axioms (and neither does its negation), maybe another genius comes along with an axiom that is quite obvious, and allows us to prove the theorem (just like peano axioms are axioms we would just accept as being obvious). So it is not that we cannot know everything, it's just that there is no guarantee and no obvious way of doing so. reply corysama 11 hours agoparentBut, that's not what Gödel proved. Not that things are hard, or non-obvious. But, that there are actual limits we won't be able to get past regardless of how much time and cleverness we apply to them. The limits might be very large. Might be unlimited for practical purposes. But, they are there waiting for us. I don't understand his theorems well enough to pretend to explain them to this audience. But, this is my understanding of them. reply auggierose 10 hours agorootparent> But, that there are actual limits we won't be able to get past regardless of how much time and cleverness we apply to them. Nope, all he proved is that there is no automatic and purely formal way of arriving at truth. Doesn't mean that for any particular truth we cannot stumble upon a way to understand it anyway. For example, we cannot write a program (= automatic and purely formal way) to determine in general if an input program halts. But for a particular input program, we might be able to understand whether it halts anyway. There is just no a-priori guarantee that we will. reply trgn 10 hours agorootparentI really don't think that this is the case. Godel proved that some truths cannot be proven. That seems pretty fundamental, and I think that's what made him so impactful, e.g. much more than e.g. the halting problem, which is about computation solely. What Godel also showed was that we can expand our a prioris, e.g. add another truth that \"feels\" intuitive to the axioms, and then we can understand some theorem or truth (ie. look under the hood, understand its mechanics). But that means expanding the system by some sort of oracle, some sort of \"genius intuition\". Fair enough, mathematicians will agree by consensus if it's the right think to do, but it's also infinitely regressive. So I'd agree with corysama here \"we won't be able to get past regardless of how much time and cleverness we apply to them\". reply auggierose 10 hours agorootparent> Godel proved that some truths cannot be proven. If by proof you mean using a fixed formal axiom system, then yes, you are right. But let's look at one particular such thing that cannot be proven formally within itself: The consistency of Peano arithmetic. But, I don't need a formal proof for that: It's obviously consistent, because the natural numbers form a model for Peano arithmetic. So I just gave you a proof, but it is not a formal one. Yet, I don't see how anyone of sound mind can reject this proof. So Peano arithmetic is obviously something we can assume when proving other, more complicated truths. And nobody can say, not even Gödel, that we will not keep finding similarly obvious truths for the more complicated truth we otherwise care about, but which might not be provable from the currently established axioms. Given that Gödel is a Platonist, he would probably agree with me on that. reply hnfong 7 hours agorootparentI'm not sure you're responding to the same point by the GP. GP claims some truths cannot be proven. You took one claim that could be proven ( https://en.wikipedia.org/wiki/Gentzen%27s_consistency_proof ) and tried to proclaim its truth without referring to the proof. Even the GP's claim is problematic. The Gödel's theorems don't prove that some truths cannot be proven. It only proves either ZF is inconsistent, or some truths in it cannot be proven. Ironically GP's implicit assumption that ZF is indeed consistent mirrors your response in which you claim Peanno is consistent... :D That said, if you somehow proved that \"some truths cannot be proven within a fixed formal axiom system (that is sufficiently powerful)\", and then you also take into account the Church-Turing thesis, i.e. that Turing machines can fully model our thoughts, then the proof of the claim about \"some truths cannot be proven\" is more or less complete. Of course you can always posit the existence of some \"unknown unknowns\" that are outside of our wildest imaginations and thus invalidating whatever we thought we have known. But that's outside of our current ability to reason. I could go on and talk about what I understand about divine inspiration and knowledge, but I suppose those topics are a bit too speculative for the crowd here... reply corysama 10 hours agorootparentprev> to determine in general if an input program halts. But for a particular input program, I think we are in violent agreement. This is an example of what I meant by \"unlimited for practical purposes, but still there.\" reply anthk 11 hours agoparentprevThe Greeks were very close to Calculus. reply thinkingemote 12 hours agoprevI've not read the book so am looking at the comments here for reasons why I should. The majority seem to be something like \"I read it when younger and it was like a powerful transformational psychedelic experience at a one day festival\". It is meaningful, reading it is something that is shared with others, but it is an event in time and is unrepeatable, and it's somehow unable to be fully communicated about to others who were not there. For some it seems foundational because it was encountered at a certain time in their life without really elucidating how their life actually changed (it's understandable because if it was done at their start of their life.) Is this accurate? reply tom_ 11 hours agoparentWhy not just read it? You can always give up halfway through if you get bored. I quite enjoyed it, but there's something of the Young Adult Reader about it that sat ill with me. One of those books where (once I'd read it) I found myself slightly wary of anybody that would bring it up. Same goes for Atlas Shrugged, and Zen and the Art of Motorcycle Maintenance. The dice man book is played too clearly for laughs IMO, but perhaps that one as well. reply adverbly 6 hours agorootparent> Same goes for Atlas Shrugged, and Zen and the Art of Motorcycle Maintenance. I've got all three of these on my bookshelf behind me. Seems like we have similar taste! Any other recommendations? One I'd add to the list would be Unsong(can find it free online). reply Vecr 1 hour agorootparentDo you also have a full printed out copy of Harry Potter and the Methods of Rationality on that bookshelf? I'm not sure you understood your parent comment's point. reply gjm11 9 hours agoparentprevThat may well be some people's experience. It wasn't mine. Not because I didn't like it: I enjoyed it a lot, I learned things from it, it was one of my favourite books, etc. I still think it's an impressive piece of work. But there was nothing particularly ineffable about it. So let me try to summarize some of the perfectly effable things that people who like GEB tend to like about it, which might (or might not!) make it worth reading for you. 1. It's a playful book. Hofstadter is having a lot of fun as he writes. (I think this is one of the things that people who don't like the book tend to really dislike: if you don't happen to enjoy the same things Hofstadter does, it can just feel self-indulgent.) Here's a fairly typical example of the sort of thing he does: the book alternates between ordinary chapters, where Hofstadter might explain some bit of mathematics or talk about an incident in the life of J S Bach or whatever, and dialogues between some imaginary characters. Each of those dialogues is named after a particular piece of music by J S Bach. For instance, one of them is called \"Crab Canon\", after one of the little pieces in Bach's \"Musical Offering\" which has the amusing property that it's the same forwards as backwards. So Hofstadter's dialogue is also the same forwards as backwards, and he's constructed it so that the conversation makes a reasonable amount of sense both ways around. That's a fairly superficial sort of play -- it doesn't have much to do with the deeper underlying ideas Hofstadter is trying to explore, it's just a bit of fun. But he does play around with the deeper ideas too. 2. It brings a bunch of apparently different ideas together and relates them to one another. The \"psychedelic\" aspects may come from this -- there's something of the \"wow, I never realised before, but everything is, like, one thing\" to it. And this is another thing that you might either really like -- he's made a bunch of unobvious connections between things you mightn't have seen links between, and connecting things better enriches your mind -- or really dislike, if you feel that the connections he's claiming to make are bogus. For instance, Hofstadter is very keen on what he calls \"strange loops\", in which category he includes (1) indirect self-reference, as in the machinery of Goedel's incompleteness theorem or \"quining\" (copy this down, first without the quotes and then again, after a colon, with them: \"copy this down, first without the quotes and then again, after a colon, with them\") and (2) what happens when a person thinks about their self and (3) Escher pictures like \"Print Gallery\" or \"Drawing Hands\" that somehow show something containing or creating a representation of itself and (4) the way in which DNA codes for proteins which make cells with machinery for converting DNA into proteins which etc. and (5) rather more tendentiously, one of Bach's canons which ends a tone higher than it starts so that if you kept on playing it the key would keep rising and rising. The common theme is something about traversing levels of a hierarchy and somehow coming back to where you started. If you agree with Hofstadter that this is an interesting and important general phenomenon (he thinks it's essential to how conscious minds work) and that all these diverse things are cases of it, then you'll find this enlightening, maybe even exciting. If you think he's just grouped together a bunch of things with little in common and convinced himself they're all the same thing, then not. 3. It talks about some really quite exciting mathematics (at least, for those who are able to be excited by mathematics): Goedel's incompleteness theorems. If you just want to learn about how Goedel's stuff works, you can get that more efficiently and probably more clearly in other places. But Hofstadter's explanation isn't so bad, and he intertwines it with all those other things he's interested in, and once again you might like it or hate it. In any case, for many people GEB was their first exposure to the idea that some statements that are just about the properties of the positive integers might be provably neither provable nor disprovable, and to the neat techniques Goedel cooked up by which, in some sense, statements about properties of positive integers can \"really\" be \"talking about\" mathematical statements and proofs and whatnot, and these are (again, for some subset of the population) exciting ideas. 4. Escher made some really cool pictures. Bach made some really cool music. If you happen not to be familiar with them before reading GEB, then being introduced to this cool stuff is a pretty valuable service GEB can do you. 5. Chunks of GEB are about artificial intelligence. The world of AI has changed a lot since GEB, of course, and today's AI systems don't have at all the sort of structure I think Hofstadter expected them to have. (It may be that they have some of that structure \"hidden inside\" -- artificial neural networks are mysterious and inscrutable in something like the same way as brains are -- but I think Hofstadter was expecting that structure to be in the code.) I haven't read GEB in a while and won't try to pronounce on how much value his thoughts-from-way-back-then have today. But I think one thing some people have found exciting about GEB, especially when reading it early in life, is that it was the first thing they read that took seriously the possibility that computers might be able to think in something like the same way as humans, and tried to think about how that might work. The ideas of AI are much more \"in the water supply\" these days; I doubt anyone will first hear about them from reading GEB any more. 6. (Same theme as 3, 4, 5.) There are just lots of interesting things in GEB. Zen koans. Fractals. Winograd's \"SHRDLU\" AI system. Bacteriophages. Non-euclidean geometry. Srinivasa Ramanujan. Etc. You won't learn much about any of these things from GEB, but encountering them at all is delightful if one happens not to have seen them before. So the experience of reading the book, if one happens not already to know everything, is one where at any moment you may suddenly encounter some fascinating new thing. I am not claiming that you should read GEB. It's pretty long. All the individual things you could learn from it, you could learn another way. If you're a generally-well-informed adult, you probably already know a lot of the things some people first encounter in GEB. You might not share Hofstadter's taste in wordplay and the like. But it definitely has merits that can be described. reply FabHK 7 hours agorootparentWow, this is a great explainer. I think you captured most of it. The other things others have highlighted is that when it came out in 1979, and many fans read it, there was no internet to speak of, definitely no WWW or YouTube, and these ideas were not as easily accessible as they're now. So for many this book might well have been the first encounter with many of the themes, thus mind-blowing. reply hnfong 7 hours agoparentprev> \"I read it when younger and it was like a powerful transformational psychedelic experience at a one day festival\" I never had a psychedelic experience but I wouldn't object to this way of framing my experience with the book. I would say there's something more \"profound\" than what you'd get from reductionistically learning the concepts introduced in the book one by one. It feels as if the author had a powerful transformational psychedelic experience and afterwards he wrote the book to share what he learned about consciousness and its relationships with self-referential computation. While it seems the crowd here is focused on the \"Gödel\" part, the book actually dives deep into a lot of auxiliary topics, for example (from memory having read it 20 years ago) - ideas about Zen, tensions between reductionism and holism, how DNA replicates itself, contrapuntal music etc. And while GEB isn't necessarily a great introduction to these topics, the concepts are indeed related in some way towards the philosophical question of consciousness, and IMHO the reader benefits from having been introduced these concepts in such a context. So if you're just trying to understand Gödel's theorem there are better books. And the mismatch in expectation is probably much of the frustration by those who didn't like the book that much. All that said, just try reading a couple chapters and see if you like it. The \"psychedelic experience\" isn't for everyone, so just try it and see for yourself if it vibes with you. reply bbor 11 hours agoparentprevI think it’s an incredibly important book to read if you’re interested in building software systems that mimic the human mind. reply knightoffaith 13 hours agoprevI don't have my copy with me right now, so perhaps I'm misremembering, but I recall Hofstadter explaining in the preface of my copy that the point of his book was how the mind - consciousness - could arise from the brain (or something like that). I myself failed to get past the ~100 page mark (he went in depth explaining topics that I was already familiar with from other sources, which bored me. And I didn't really find the connections to art and music that insightful or interesting - but maybe I'm just too uptight). My understanding from skimming the book and reading some of Hofstadter's other works (including his response to Searle's \"Minds, brains and programs\" article) is that the book is trying to establish how complexity can emerge in systems with many simple moving parts via recursion (or something like that) in different scenarios, suggesting that this is how consciousness emerges from a complex web of neurons (the brain). This seems a little wishy-washy to me. I don't see this as a good counterargument against Searle's argument that syntaxx alone is not enough to give rise to semantics. (I find Dennett's argument about intuition pumps a more convincing counterargument.) Maybe my understanding of Hofstadter's argument is too simplistic - I'm happy to be educated (I wasn't able to make it through even half of GEB after all). And of course, that's not to say that GEB isn't a valuable book - it seems like most readers really enjoy it and learn a lot, even if they don't much care for the ultimate cognitive science/philosophy of mind position Hofstadter is trying to defend. reply The_Colonel 13 hours agoparent> it seems like most readers really enjoy it and learn a lot, I think it's one of those books which most people actually don't enjoy that much, but don't want to admit it, because it has such an intellectual aura around it. reply leto_ii 13 hours agoprevGEB is one of the books that as an adult I have learned to not care about anymore. I've unsuccessfully tried reading it a few times (once made it a couple of hundred pages in) and came to the (personal) conclusion that if I want an intellectual challenge I'll just directly do maths instead of reading a semi-literary essay about maths. Just like (some of) Joyce's work, GEB seems to me a puzzle who's main prize is the satisfaction of having understood it - obfuscation and abstruseness for their own sake. For actually understanding Godel's work I would recommend Gödel's Proof (Nagel, Newman, somewhat ironically, prefaced by Hofstadter) or Philosophies of Mathematics (George, Velleman). reply tootallgavin 11 hours agoprevG.E.B is hegelian philosophy without one mention to Hegel and more mechanic than organic Anyone read the Phenomenology of Spirit and notice the same ideas? reply bbor 10 hours agoparentYES. So, so true. And Hegel in turn was just adapting Kant, despite calling him a “blockhead” in the preface (hilarious, can’t even imagine what 1800s German for “blockhead” is). It’s absolutely Hegelian. To make it more explicit for the HN audience: Hegel wrote a very famous book called the “The Experience of Mind” (or, in obnoxious philosophy language, “The phenomenology of Spirit”). In it, he details how he thinks the mind is made up of successive levels of distinct programs, each of which builds on what came for it in a very specific way he calls “dialectic synthesis”. In this way he goes through all the perceived capabilities of the mind (the big four in order roughly being sensation, understanding, awareness, and reason) and ties them to specific steps in this synthetic chain. I hope it’s obvious from that how this argument mimics GEB (which I understand as roughly “the human mind is a collection of looping programs with these characteristics”). reply PaulDavisThe1st 6 hours agorootparent> which I understand as roughly “the human mind is a collection of looping programs with these characteristics” uh ... the center of Hofstadter's worldview, which he re-enunciated later in \"I am a strange loop\" is almost precisely the opposite of Hegel's persepective. Hofstadter came to see the idea of \"heterarchical systems\" as central to creativity and, in his opinion, consciousness. These systems are precisely not what Hegel describes, with \"each level building on what came before it\", but instead the levels are functionally and physically entangled so that \"higher level\" ones can intimately and profoundly impact \"lower level\" ones and vice versa. reply heikkilevanto 13 hours agoprevGEB is in my top three as well. But looking back at it and the other important books in my life, it seems that it matters very much when I read them, and where I was at the time. For example, the Lord of the Rings was hugely influential for me, in part because I bought it (in one volume paperback) with the sales of my first program, and read it in school (also during our English lessons, the good teacher let me do that when she saw what I was reading). reply dave1010uk 13 hours agoprev> I had a secondary goal in the back of my head... if you have a copy of GEB on your shelf collecting dust and you've never read more than a chapter or two, dust it off and see how it goes this time. Dusted it off and after only a few pages, it's already a completely different read to when I read (a fraction of) it a decade or so ago. reply peterashford 8 hours agoparentI'm interested to know: How so? reply rongenre 8 hours agoprevPDF of GEB: https://archive.org/details/GEBen_201404 reply calf 13 hours agoprevGodel did not argue or show \"there are fundamental epistemic limits to the universe\". The universe is not a formal axiomatic system. This repeats an very old, popular pop-philosophy misconception about Godel's theorems and GEB seems to have done nothing to help with this category error. reply samatman 12 hours agoparentWhether or not the universe is (or, to split hairs, may be exactly expressed as) a formal axiomatic system, was very much an open question, one which Gödel helped answer conclusively in the negative. reply swozey 13 hours agoprevI bought this when I was like 19 and going through some \"i'm going to be a physicist\" mental break and obsessing over michio kaku and wild theories and it made absolutely no sense to me at all. Do I need to try again now that I'm a grown adult? I do not know math well. reply knightoffaith 13 hours agoparentDo you remember what you were confused about? The book does not assume the reader has any advanced math education (from what I remember) - it tries to teach you the relevant math itself. It is long and windy though, so maybe you read through too quickly or got bored. You'll probably have more success with it now that you're more mature. reply g-w1 13 hours agoparentprevYes, it's great. As long as you can follow logic and a bit of programming, you'll do fine. Just go slow (I had to re-read it a bunch). reply readthenotes1 13 hours agoparentprevI dont recall math equations but it's been a while reply hackandthink 12 hours agoprevAnother philosopher about Gödel: The Collapse of the Hilbert Program: A Variation on the Gödelian Theme* Saul A. Kripke https://arxiv.org/abs/2102.08346 reply j7ake 9 hours agoprevHow long does it take to really “read” GEB? There are puzzles and problems inside that could really extend how long you spend in that book. reply mullingitover 12 hours agoprevCommand+F \"chess\" Sweeping that one under the rug, are we? reply bbor 10 hours agoparentlol in this thread full of dozens of paragraph-long attacks on this book, yours is by far the most damning. And funny. For those that are unaware like me: Hofstadter's law is a self-referential adage, coined by Douglas Hofstadter in his book GEB…: Hofstadter's Law: It always takes longer than you expect, even when you take into account Hofstadter's Law. **History** In 1979, Hofstadter introduced the law in connection with a discussion of chess-playing computers, which at the time were continually being beaten by top-level human players, despite outpacing humans in depth of analysis. Hofstadter wrote: In the early days of computer chess, people used to estimate that it would be ten years until a computer (or program) was world champion. But after ten years had passed, it seemed that the day a computer would become world champion was still more than ten years away... This is just one more piece of evidence for the rather recursive Hofstadter's Law. reply johnmw 5 hours agorootparentDoug seems pretty shocked by the pace of AI development in this video [1]. Still, Hofstadter's law has held true for me more often than it has not! [1]: https://www.youtube.com/watch?v=Ac-b6dRMSwY reply rmu09 12 hours agoprevIt's a long time since I last read the book but I remember that in a sense the book ends at about 50%, that is a meta discussion in the book says it ends now and the rest that's following is a redundant. Will have to re-read it to find that location again... reply oh_my_goodness 12 hours agoprevI gave up at this point: \"We might describe the way that planets fly around stars as isomorphic to the way that electrons fly around nuclei.\" We might, but we'd be mistaken. Why bring incredibly complex topics into a general discussion unnecessarily? Why not even bother googling a correct explanation of those topics first? (I could guess why, but I don't want to ruin the article with spoilers.) reply thr0waway001 13 hours agoprevIt's the coolest looking book I own that I can't read cause I'm too dumb to understand even having taken some university math courses. lol reply labarilem 13 hours agoprevI love this book. It definitely changed the way I think and made me ask important questions. Bonus: it got me listening to Bach at the time. reply parski 1 hour agoprevIf ripping out the bugs causes systemic collapse then maybe that's a good thing. Regarding capitalism. reply tnias23 10 hours agoprevDoes this book rely on a lot of images, or would it be decent on audiobook? reply FabHK 6 hours agoparentIt relies on images, typography, and the arrangement of things. I'd say a physical book or PDF would work best. reply j_maffe 14 hours agoprevExcellent book. It really gave me such an interesting vision of how to see the mechanics of the world. reply cupcakecommons 13 hours agoprevGreat synopsis of the book and an excellent synthesis of its real-world implications. reply StopTheTechies 12 hours agoprevThe book ostensibly lacks Wittgenstein and/or Chomsky. Not much point in discussing the 20th century without them.... reply nullc 4 hours agoprevHere is one you won't hear many people say: I met my partner due to GEB. Some twenty years ago I was operating a muni network and was up at stupid hours due to a maintenance window. The people I normally talk to were all asleep, so I made a script that searched people on live journal that had similar tagged interests and added them to my aim buddy list-- then if they were online I messaged them. She was awake at an odd hour because someone had pulled a fire alarm in her dorm. I later went back to see why my program had added her, and there were a number of overlapping tags but it turns out there just weren't many people that listed \"douglas hofstadter\" as an interest on LiveJournal-- maybe 8 in total at the time-- so it had a lot of weight. reply enriquto 13 hours agoprevI loved the dialogues and the poems, with their various translations. The discussio around the \"musical offering\" is incredible, and helped me learn to appreciate music (not only Bach). This is one of my favourite books! The math was insufferable, however. And I say that as a mathematician and programmer... I wonder if musicians and poetry translators will find \"their\" parts correspondingly unbearable? reply peignoir 13 hours agoprevyeah emergence and strange loops are weird :) reply webdoodle 4 hours agoprevI picked up GEB after seeing a post about it here on HN. Although Hofstadter makes a few connections I was unaware of, such as Escher's artwork expressing recursive patterns, it was really underwhelming for the most part. As others have said, it takes 1000 pages to get to a point, that could have been made in significantly less. My only thought is that perhaps Hofstadters' rambling was an attempt at literary recursiveness, but his execution was lackluster. reply Barrin92 13 hours agoprev [–] Couldn't r",
    "originSummary": [
      "The article delves into the book \"Gödel, Escher, Bach: An Eternal Golden Braid\" by Douglas Hofstadter and its profound impact on the author's life.",
      "The book explores various thought-provoking concepts such as epistemic limits, self-reference, and isomorphism, using the characters of Kurt Gödel, M.C. Escher, and Johann Sebastian Bach as symbolic representations.",
      "The author emphasizes how the book has shaped their thinking, particularly in terms of approaching problems from a bottom-up perspective, understanding complex systems, and recognizing the limits of knowledge."
    ],
    "commentSummary": [
      "\"Gödel, Escher, Bach\" is a renowned book that delves into consciousness, art, music, and artificial intelligence.",
      "Opinions on the book vary, with some finding it thought-provoking and influential, while others find it dull or pretentious.",
      "The book's difficulty level, exploration of Gödel's theorems, and impact on readers are topics of discussion."
    ],
    "points": 327,
    "commentCount": 238,
    "retryCount": 0,
    "time": 1707072534
  },
  {
    "id": 39254172,
    "title": "Customize Your Search Experience with Stract: Open-Source, Non-Profit Search Engine",
    "originLink": "https://stract.com/",
    "originBody": "Search Explore Settings About Settings About search Customise your search with an optic: No Optic Copycats removal Discussions Indieweb & blogroll Academic Terms & PrivacyAPI",
    "commentLink": "https://news.ycombinator.com/item?id=39254172",
    "commentBody": "Stract: Open-souce, non-profit search engine (stract.com)289 points by FLpxpyJ 13 hours agohidepastfavorite70 comments denysvitali 9 hours agoEveryone here is complaining about the search results - but instead I think we should all take some time to appreciate that someone worked hard to create a search engine (including the scraper / crawler part) and making it open source (AGPL). The results will be improved over time I guess, and for the few search queries I've done - I'm fairly happy with the results. Kudos to the authors! reply vedranm 42 minutes agoparentI'm presently on leave at a university in Croatia and my research group's name is Group for Applications and Services on Exascale Research Infrastructure, so the acronym is GASERI. [1] The first result for gaseri on Stract is our presentation of group's research work, and the second result is our landing page. Can't complain. Viva open source, viva AGPL search engine! [1] https://group.miletic.net/ reply djbusby 9 hours agoparentprevI searched for a few things that were of the class \"you should have this and match DDG/Kagi/G/Y/B\" and the top 2/3 were matching. That's pretty good for a new-ish player. reply forgotmypw17 5 hours agoparentprevI've been doing some extensive searching for a particular topic for months using primarily Google, and I just found a bunch of sites that I had not previously found just running one query on this. I think that as with ChatGPT vs Bard, the result space is so huge, there are going to be many strength/weakness tradeoffs for any given query. reply Gratuity1901 4 hours agoparentprevIt is super hard to match the 'big' players. But straight programming a proper index listing is hard. reply ramon156 3 hours agoparentprev> type google > get anything but google ?? reply lock-the-spock 2 hours agoprevWonderful project, congratulations! I love the speed, clean design, many options, multilingual results, overall very impressive!! Some quibbles/points to consider: * I can't find anything on the people/organisation behind, and can onl guess from the Terms that the team is based in DK. * Search results are broad and interesting, maybe a bit more weighting for the joint occurrence of terms would be great. * Developing a site weight over time might be interesting, maybe even with user votes. Currently minor and major sites appear all together and e.g. a search for \"Donald\" gives me an interesting ranking order that gives neither the most famous Donald's nor the most reliable sites firet (not problematic per se - my fault for entering an unclear search term) * There are some interesting result patterns, with often official sites quite low. For instance search for \"EU\" with some term like subsidy (in any of the languages I speak) gives me random project websites but nothing from any of the official EU websites, or \"Microsoft 365\" (sorry...) gives me no MS website. * Very minor but hopefully a very easy fix: at least on Firefox mobile there is no direct way to add the search to my search engines, I had to add it manually. For other engines I can long press.on the search field and then get the option. Great work, keep it up! I will certainly start using this :-) reply eviks 4 hours agoprev> immensely customizable -- We aim to give you the ability to customize everything about the search. You can block sites, boost sites, prioritize links from specific sites and much, much more. Great! Can I use more than one optic? The drop-down list seems to allow only 1. > Oh, and if we ever become evil (maybe by changing our motto) please take our code and start a competitor. The most important part is the index data, what would be the deal with that? reply vladstudio 4 hours agoprevTo make Stract usable for me (slightly reduced vision), I had to apply the following custom CSS: ``` html, body, div, td, th, p, h1, h2, h3, h4, h5, b, i, strong, li, button { font-family: ui-sans-serif, sans-serif !important; webkit-font-smoothing: antialiased; font-weight: 400; text-rendering: geometricPrecision; } ``` reply skeptrune 10 hours agoprevThe option to return from only sites popular on HN, blogroll, and the other \"manage optics\" settings are incredibly cool and useful, I could see myself using this just for that feature alone. Exciting stuff. reply 3abiton 7 hours agoparentI'm more pessimistic about how would that drive bad actors to HN polluting the site. reply giancarlostoro 7 hours agorootparentI take it you aren't showing dead threads? If you look at newer submissions you'll see people voting bad stuff to death. HN is insanely decent at community driven self moderation. Not to knock the mods who put a lot of work into the site of course, but I assume the community's own self-moderation helps some. reply sangnoir 2 hours agorootparent\"Polluting HN\" is more than just the vitriol thats flagged - there are plenty of self-promoting, downright wrong, or comments clearly unrelated to the posted article but comment author gets to segue to their hobby-horse (off-topic discussions are annoyingly frequent, IMO). HN is better than most, but its not immune to being gamed. Once there is a financial incentive for it, it will become more common - see how Twitter turned out after offering monetary incentives for engagements. reply remram 5 hours agoprevSources: https://github.com/StractOrg/stract Backend in Rust (axum web framework, rocksdb), frontend with Svelte. reply gardnr 1 hour agoprevI just set it as my default search engine for a day. It's not quite there for my use cases. Can we help improve the search results? reply gregw134 12 hours agoprevWanted to say congrats on launching! I'm building a search engine myself, I can tell a lot of work went into this. I think the biggest thing you overlooked are page titles. When you issue a query it's a bit hard to quickly scan and judge what a site is about because the page titles are missing. reply pooper 5 hours agoparentHow do you crawl the web? Do you follow links around? How do you reach a page that isn't linked from anywhere you've crawled? reply mmkos 22 minutes agorootparentI mean that's what web crawling is, right? By extension, you just can't reach a page unless you stumble upon a link to it _somewhere_. Google gives you an option to submit a link and schedule a crawl that way, so that's another option if it's not being linked to from anywhere. reply gregw134 2 hours agorootparentprevI'm just using common crawl for now reply jqpabc123 11 hours agoprevclearly labelled, contextual ads based on your current search query and a subscription option without ads Perfect! This is the way the god of the internet intended search engines to work. But DuckDuckGo does the same and currently provides superior results based on a very brief test. So good luck with that. reply candiddevmike 10 hours agoparentWhat would happen to DDG if Microsoft stopped letting them use Bing? What does MS get out of this relationship? reply jqpabc123 9 hours agorootparentWhat does MS get out of this relationship? My guess --- a portion of the ad revenue. reply notoverthere 10 hours agoparentprevDuckDuckGo also allows you to switch off ads, for free, without any fuss or adblocker needed. Just go to the settings page. Although if you aren't going to support DDG with ad revenue, I'd suggest supporting with a donation if you can afford it and value their service. reply jqpabc123 5 hours agorootparentI really don't mind helping DDG take advertisers for all they are worth as long as it doesn't cost me my privacy or waste too much of my time. And if they take something away from Google in the process --- that's just an extra bonus. Turnabout is fair play don't you think? Google has worked very hard to take privacy away from users. reply godzillabrennus 11 hours agoparentprevDDG is amazing. To good to be viable I’m usually thinking. reply giancarlostoro 7 hours agoparentprevDDG rubbed me the wrong way when they decided to \"filter misinformation\" which is an arbitrary and biased thing. I don't side with Russia or anything, but I've seen this sort of thing get out of hand. reply 6510 5 hours agorootparentOne of my favorite topics is treated as misinformation and completely scrubbed from google and Bing. The idea seems to be the assumption that one must believe all that one reads. Imagine a discussion where everyone agrees? Stract produced only good results. Unusually good. reply ralusek 3 hours agorootparentI must know reply fulmicoton 1 hour agoprevThat looks quite promising! Thank you for crediting tantivy in the github README, that's well appreciated! Ping me if I can help with anything. reply pcblues 1 hour agoprevIf you are interested in setting up your own non-profit org marketplace or know someone who does, I made an example one using free tools (https://donate.pcblues.com/) that costs me only about $10 USD per month to host the example because it is just a hosted linux VM and not Saas or software subscription based. I configured the VM myself and then \"just\" installed the software and configured it. It hasn't been down for ages. I only just remembered to check it. It does everything from merch and service websites to escrowed payment transactions, user reputation, etc. reply icar 1 hour agoprevThe only current search engine that I can use in my native language, Catalan, is Google. I can't wait for a project like this one to get good at that. reply john-radio 6 hours agoprevVERY cool product. I have a quibble. I searched for \"cool pokemon to use\" and the top result was \"How to use Paypal on Amazon\" from \"online-tech-tips.com\". Understandable that the search results are not perfect - the second result was a perfect match for what I searched for - but anyway, clicking the \"dials\" icon gets me the following options: \"\"\"Do you like results from online-tech-tips.com? (thumbs down, thumbs up, or banned emoji options) Summarize result \"\"\" IMO this feedback widget and (maybe) its backing API could use work. It's not that I like or don't like results from online-tech-tips.com; it's that they're a bad result for the specific context of this search. reply highmastdon 2 hours agoprevGreat stuff! Just want to mention, when I search for “ExpressLRS use uart on older f4 fcs” it gives me about 15 results, but only the first two are unique. The other 13 are a literal copy of the first, both in content and in URL. Probably best to filter for uniqueness reply lpellis 9 hours agoprevSeems surprising ok for coding related queries ('celery rate limit'), I'm curious about their scraping setup, building that out must be quite a big task. reply latentdeepspace 1 hour agoprevCan someone provide a bit of background how the crawling part works? reply RDaneel0livaw 7 hours agoprevSo is this truly its own search engine / crawler / etc... and not using anyone else's searchs? I know ddg / kagi often use results from bing and other places, so just want to make sure. also, how can I add this to my firefox search inside the address bar / search field? reply fabrice_d 6 hours agoparent> also, how can I add this to my firefox search inside the address bar / search field? Navigate to https://stract.com/ then focus the url field: Firefox will display the new search engine at the bottom of the suggestions, on the \"This time, search with:\" line. reply dubbel 32 minutes agorootparentThat was hard to discover, thanks for your explanation. I was looking in \"Firefox Settings -> Search -> Search Shortcuts\" for a way to add it. I guess the functionality is not used very often, but it would be nice to have a hint on how to add new Search Engines there. reply mcdonje 6 hours agorootparentprevI didn't see that option on mobile, but I got it added. Click the search provider icon in the search bar, go to search settings, then manage search providers, then add new. Add it with this url: https://stract.com/search?q=%s reply spaduf 12 hours agoprevReally like the explore feature. It lets you put in a url and shows you similar sites. Very promising project. Love to see people actually thinking about what search would be rather than rehashing decades old ideas. reply a1o 10 hours agoprevSearching for \"adventure game studio\", neither the website that has the forums or the GitHub repository is in the first page. Most results on the first page of search are really old things. Neither Wikipedia or repology that has the package infos are anywhere in the results. reply sydbarrett74 2 hours agoprevVery impressive, and kudos to the developers and originators. I just hope Stract doesn't go 'corporate' the way DDG did. :( reply PixelForg 10 hours agoprevSearch still needs some improvement, I typed \"gundam watch order reddit\" and was expecting some reddit links, but none of the results are reddit links. Perhaps there's another way to limit search results to a particular site here? reply jzelinskie 9 hours agoparentIf you're looking for the answer to this question the \"relation graph\" from AniDB is probably the best thing around: https://anidb.net/anime/715/relation/graph reply mdhen 10 hours agoparentprevNormal way is \"site:reddit.com\" reply Gualdrapo 9 hours agorootparentThat is Google way, rather than \"normal\" reply mdhen 9 hours agorootparentI dont use google, that's how it works on ddg and kagi reply badsectoracula 5 hours agorootparentprevThe site: operator seems to work in most search engines these days. reply bbsz 6 hours agoprevI think a lot of people will now go and benchmark queries only to report back disappointed with results. Trying to build generalized search engine for the modern internet that will come close to Google/Bing would require a \"tech megaproject\" level of investment and commitment. Most likely only to end up with the same optimizations and architecture as existing big-search and the very similar level of experience. I think it's a better direction to build a search based on more limited amount of topic-based data and focus on great match engine within, then - just aggregate the relevant ones together. Far more maintainable also on the crawling part. I can use google/bing to find the Honda dealership or read keyboard reviews, or get 50 most useful unix commands. I also wonder if with the rise of LLMs, while it still may not be feasible in such large scale production environment, those can serve as guides/agents to also improve the query itself and not the results of the query, for example - a chat-like search where user answers shift the relevancy metrics for returned documents. This would fit perfectly for smaller but open source, customizable and thematic search. That being said. I think it's great that project as such pop up more often. (Phind.com was also on my radar this year) reply crotchfire 11 hours agoprevWhere does their crawl come from? reply com 12 hours agoprevFast, feels clean and uncluttered to use and the search results are fairly high quality. I like the “optic” idea. After reading the about page, I’m not sure what the developers are trying to achieve? Perhaps a sort of alternative-Universe Google search funded by search-context AdWords? reply mcny 8 hours agoprevIn swagger/open API, why is everything a post? I tried the first endpoint get suggestions and tried searching for Gemini or Gemin hoping it would at least auto complete a word but the result set is empty. https://stract.com/beta/api/docs/#/autosuggest/route reply vaicorinthians 1 hour agoprevI would give stract.org a shot tho reply safety1st 6 hours agoprevGenerates some pretty interesting results. No way to make it my default search engine? reply Pufferbo 11 hours agoprevTried searching for Dota (the video game), and the game’s website is buried by a bunch of SEO spam. It might not even have been crawled because it doesn’t appear on the first or second page. reply lastdong 11 hours agoprevOptics are a great idea, something we don’t see on other engines. Fully open source -`ღ´- Haven’t dig in to see what’s powering the search, I think DDG uses Bing reply tortoise_in 6 hours agoprevSo I have put two inquires of my local country but they didn't shown up reply kuratkull 11 hours agoprevIt's failing (completely wrong results) my goto query for testing search engines: \"best sub 10 usd Linux single board computer\" Try it out reply Levitating 11 hours agoparentDamn Pine64 has some fun stuff happening. Also I noticed DuckDuckGo performed much better than Google with this benchmark. reply vanous 12 hours agoprevCongrats! I tried to search for a particular domain data but neither search nor the explore would have the domain listed. What's the process to get unlisted domains indexed? reply daniel_iversen 12 hours agoparentawhh I can see DMOZ (https://en.wikipedia.org/wiki/DMOZ) is no longer! That used to be the seed for crawling the internet I believe, for search engines. reply cristoperb 7 hours agorootparentA static version (archive) of DMOZ is still available at http://www.odp.org/ reply stainablesteel 7 hours agoprevthis is a neat thing, i like it, i'll add it to my list of search engines i use reply abrowniejr 8 hours agoprevI searched for \"calories in 450 gm of steak\" and the top 3 results were: 1. Brexit as the start of the reversal of neoliberal globalization - softpanorama.org 2. Directory Search - Fulshear-Katy Area Chamber of Commerce - chamberorganizer.com 3. The 100 Best New Products of 2020 - gearpatrol.com And none of the Page 1 results were related to my search query... reply charcircuit 12 hours agoprev>how many bits are in a byte I checked 11 pages and none of the results were relevant. reply buo 11 hours agoparentI searched instead for \"size byte bits\", third result has the answer. It seems like the engine gives equal weight to all words in the search, so \"are\", \"in\" and \"a\" throw it off. reply kiwidrew 11 hours agorootparentexcellent! I'm tired of search engines that optimize for natural language queries because the inevitable trade-off is that they become useless at keyword/exact queries. reply AlienRobot 10 hours agoprevI searched for \"horror movies\" and the first result was a lemmy community that has literally \"616 subscribers\" \"30 Posts\" and \"76 Comments\" which is about as dead as you would expect from a lemmy instance. I also searched for \"league of legends\", and it couldn't find its homepage. I think its ranking algorithm may need improvement. Edit: also, I'd rather not say this, but do we really need another DuckDuckGo? I don't think Google fails at its job because of financial incentives. I think it might fail at this job simply put because the web of 2024 isn't the web of 1990. For example, the lemmy result, it's a link aggregation about horror movie articles. The search engine could literally do the job of the link aggregator, as it has a SERP that aggregates links, and yet it's aggregating links to link aggregators. Why are the search engines doing this? Because it's 2024. I wish someone tried a new approach at this problem rather than just copying Google's design and saying \"it's Google but not yucky\". reply redder23 10 hours agoparentIts so fucking easy to come in here and shit on things. I actually love that is suggests a Lemmy community over some fucking Subreddit. And 616 subs does not sound totally dead either. \"I'd rather not say this\" is bullshit saying. So why are you saying it then. What follows is also complete bullshit. Its something COMPLETELY different from DDG. DDG is just another commercial close source woke censored search engine that just does not track. Are the not actually PAYING M$ for Bing results? Or was the a another privacy search engine? B4 I use DDG I might as well just use Startpage or something and get Google results without tracking. There are plenty of \"private\" search engines now DDG, Brave Search, Quant, ... NONE of those are actually open source. There is SearX that is just a meta search engine proxy. In fact I recently thought \"why is there not actually a true open source search engine that can be self hosted that actually indexes things ... assuming this does this and not just proxies other search engines results. I am exited about this and hope it gets better and there will be like communities of users who use it, improve it, build indexing clusters and whatever, I am not coming in here demanding perfect results and totally failing to understand what this actually is. reply snvzz 8 hours agoprev [–] The search bar should really be full width. It can be very annoying to have your query not fit it while the window has plenty of room left. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The message encourages users to personalize their search options with different settings and features.",
      "It suggests removing copycat websites, exploring IndieWeb and blogrolls, and incorporating academic terms and privacy.",
      "The message highlights the availability of an API for additional customization choices."
    ],
    "commentSummary": [
      "Users are giving positive reviews to the Stract search engine, highlighting its open-source nature, fast performance, and multilingual search results.",
      "Some users have identified minor issues and made suggestions for improvements.",
      "Concerns have been raised about potential misuse and the influence of corporations on Stract.",
      "Comparisons have been made to other search engines like DuckDuckGo.",
      "There is interest in using free tools to create a non-profit marketplace.",
      "Overall, users are optimistic about Stract.org but acknowledge the need for further enhancements."
    ],
    "points": 289,
    "commentCount": 70,
    "retryCount": 0,
    "time": 1707078988
  },
  {
    "id": 39254807,
    "title": "Netflix Acknowledges the Rapid Growth and Challenges of Online Piracy",
    "originLink": "https://torrentfreak.com/netflix-piracy-is-difficult-to-compete-against-and-growing-rapidly-240204/",
    "originBody": "HOME > PIRACY > As a member of ACE and the MPA, Netflix is at the frontline of the global battle against online piracy. The company doesn't often address the subject directly but in a recent SEC filing, Netflix writes that it's difficult to compete against the free entertainment piracy offers. Not only that, it's growing rapidly too. From the launch of its online streaming service fifteen years ago, Netflix positioned itself as a piracy competitor. The idea was to take market share away from piracy sites, by offering a legal and more convenient streaming platform. Initially, this seemed to work. Netflix amassed hundreds of millions of subscribers, some of whom left their piracy habits behind. However, as the ‘streaming wars’ turned legal and convenient streaming platforms into isolated and pricey content silos, momentum started to shift. In recent years piracy started to grow again, including in well-served markets such as the United States. In theory, this may help Netflix in its battle with other legal platforms, but that’s a consolation prize if the war against piracy is lost. There are no concrete signs that Netflix is crumbling, but piracy is a concern. This isn’t breaking news; piracy has been repeatedly highlighted as tough competition in the company’s 10-K filings at the SEC. Piracy is a Tough Competitor Earlier this week, Netflix submitted its latest 10-K filing. The mandatory document provides information that helps investors to gather key information about publicly traded companies. In the “competition” section of the annual overview, piracy is again mentioned several times. Netflix explains that the online video landscape is a competitive business. New services and distribution models could impact the business of the leading video streaming platform. This includes legal competitors as well as piracy. “The various economic models underlying these channels include subscription, transactional, ad-supported and piracy-based models. All of these have the potential to capture meaningful segments of the entertainment video market,” Netflix writes. These are in part standard disclosures, as every company faces competition. However, Netflix believes that online piracy is particularly compelling because it’s free for consumers. That makes it very hard to compete against. “Piracy also threatens to damage our business, as its fundamental proposition to consumers is so compelling and difficult to compete against: virtually all content for free,” Netflix writes. Growing and Hard to Stop When Netflix launched, its on-demand streaming experience was more convenient than most pirate sites. At the time, torrent sites were dominant but still required users to have some technical knowledge and the patience to wait for content to download. Today, most pirate sites use on-demand streaming, taking away a major edge for Netflix. And because piracy is so compelling for consumers, it is growing rapidly worldwide, threatening legal services. “In light of the compelling consumer proposition, piracy services are subject to rapid global growth, and our efforts to prevent that growth may be insufficient,” Netflix notes. “If we are unable to successfully or profitably compete with current and new competitors, our business will be adversely affected, and we may not be able to increase or maintain market share, revenues or profitability.” (Un)authorized Copying? The concerns voiced by Netflix are real, but the company isn’t near its demise. These 10-K filings are supposed to detail risks and Netflix is not the only company mentioning piracy as a potential threat. A Netflix Competitor When we started looking for similar mentions by other businesses, we stumbled upon similar concerns and, strangely enough, some identical ones. Apparently, there’s quite a bit of copying going on, as SEC filings from several companies include identical passages. Netflix: “In light of the compelling consumer proposition, piracy services are subject to rapid global growth” Triller Corp: “In light of the compelling consumer proposition, piracy services are subject to rapid global growth” FuboTV: “In light of the compelling consumer proposition, piracy services are subject to rapid global growth” Redbox Entertainment: “In light of the compelling consumer proposition, piracy services are subject to rapid global growth” IMAQ: “In light of the compelling consumer proposition, piracy services are subject to rapid global growth” CuriosityStream: “In light of the compelling consumer proposition, piracy services are subject to rapid global growth” We don’t know where these references originate. Netflix has mentioned it for a while, that’s for sure, and apparently, the use of this language is widespread and subject to rapid global growth. It’s clear, however, that piracy is a concern for Netflix. While Reed Hastings wasn’t worried about piracy a decade ago, the company now spends millions of dollars tackling the problem. The streaming giant joined the MPA a few years ago and is also a member of anti-piracy coalition ACE. In addition, Netflix also has an in-house anti-piracy department that keeps an eye on piracy threats.",
    "commentLink": "https://news.ycombinator.com/item?id=39254807",
    "commentBody": "Netflix: Piracy is difficult to compete against and growing rapidly (torrentfreak.com)281 points by notamy 12 hours agohidepastfavorite543 comments PlutoIsAPlanet 11 hours agoStreaming became popular because it was easier than piracy and better than TV (watch anywhere, on demand, pickup where you left off etc). Streaming is no longer easier than piracy, why pay for 8 different services and have to waste your time figuring out whats on what when you can just have one service for free, even if its illegal, and have it all under one roof. The services have taken the piss and now they'll get the repercussions of it. reply izacus 1 hour agoparentYeah, also pirated content has: - 4K HDR video, not whatever the heck the buggy client delivers. - Atmos/TrueHD audio track that actually works, not whatever the broken app delivers (I'm looking at you Sky and rest of the ilk that still deliver HBO content with stereo). - Subtitles for ALL the languages, not just one or two. And those languages don't disappear when I go on a vacation, leaving me stuck with german audio and french subtitles. - Properly functioning offline playback for when I'm traveling, not randomly broken and disappearing offline mode (Netflix, Spotify and YouTube all blessed me with \"all your downloaded content is gone\" experience on long flights). - Works on all my devices not a random subset independent on which way greedy execs tried to extract \"ecosystem\" money from my playback device manufacturer. Looking at you ATV+. - Is actually available in my region and doesn't randomly disappear from my devices just because I decided to travel to visit my parents or have some time off. - Doesn't randomly disappear after 6 months when I started watching the series because some license expired. As you can see, I really tried to pay to get content from these people. And all I got was bunch of frustration. F'em, they brought this upon themselves for being user hostile arseholes. Again. reply rekoil 44 minutes agorootparentI personally find that Spotifys offline content has worked reliably over the years. You just gotta remember that when you get a new device you have to go to library and click download and then leave Spotify open for a few hours. Spotify is also a great example of how to beat piracy. I haven't downloaded an mp3 file since I got a Spotify Premium account in 2007. OK that's probably a slight overstatement, but it's near enough to the truth for this argument. They made it so simple and convenient to pay for access to music that I've had zero reasons to even consider piracy. They have everything (well, enough that I've never felt something was missing anyway), they have clients for everything, the pricing is reasonable, and they have great value-add in the form of playlist generation. As for the rest, full agreement, the low bitrate of streaming services is the big one for me as I'm a film enthusiast with great gear that definitely doesn't reach its potential using any streaming services. Now that it looks like physical media is going the way of the Dodo I'm wondering where I'm gonna get actually high bitrate content from in the future. reply CaptainMarvel 11 minutes agorootparentSpotify’s offline content was not been reliable for me. I am rarely offline, but recently I was taking a flight and my whole library was unavailable. I was so annoyed that I actually cancelled my subscription. reply mgoetzke 29 minutes agorootparentprevApple Music and Youtube Music both suffer from the issue of vanishing content. I have playlist where a growing percentage of songs are grayed out, not available, while they do exist in the store, just as part of a different album now. Very frustrating. Does Spotify do that ? reply take-five 21 minutes agorootparentSpotify does this too, at least in my region (Estonia). It's frustrating, but not that frustrating to go back to downloading MP3. reply rekoil 17 minutes agorootparentprevIt happens, but pretty rarely in my experience, though I'm probably not the most avid consumer of music so other people might have a better grasp of the extent of this issue on Spotify. reply izacus 27 minutes agorootparentprevI'm glad for you, but at least twice it happened to me that the songs on my phone ended up being unplayable after I enabled Airplane mode. (You just get the exclamation mark.) ¯\\_(ツ)_/¯ reply mgoetzke 31 minutes agorootparentprevThey cannot help themselves. Just like buying and watching a DVD/bluray was much worse than a pirated copy, as the DVD/bluray made you sit through an increasing amount of unskippable content. From \"do not steal this\" to \"not for kids\" or, worst of all, \"did you see this other content?\". Btw. Ads in front of streaming content I just selected, to tell me about other streaming content on the same service I did NOT select should also be on the NO list. reply nerdix 7 hours agoparentprevAlso, H265 encoded 1080p is hitting the sweet spot between quality and file size. About 500-ish MB per hour of content for decent quality. Its sort of like when VBR encoded MP3s became the standard in the privacy scene in the early 00s. The quality is good enough for most folks plus the reduced file size means that it downloads really fast on average connections and isn't prohibitively expensive to store so you can create a large library very quickly. Also I think the shift to SSDs from spinning rust delayed the adoption of widespread video piracy 10-ish years ago because the cost per GB was just too expensive and most people weren't going to buy a NAS. But now we are starting to move past the inflection point. SSDs are bigger, H265 reduces file sizes, hardware accelerated H265 encoding is easier (meaning more H265 content), and devices that can decode H265 are becoming cheap and ubiquitous. You can install Plex on your laptop, load up on content, and have a better UX than Netflix. reply radley 5 hours agorootparent> About 500-ish MB per hour of content for decent quality. That's still YIFY-quality video, which is a few steps below HBO/MAX, which happens to be among the lowest quality paid streams. ATV+ is killing it with 30Mbps 4K HDR+ video (it used to be as high as 48Mbps). Disney+ is close. The Netflix 4K plan serves up high 1080p bitrates, 2x better than their 1080p plan. Worth paying for. reply jszymborski 4 hours agorootparentNote that Netflix and co. won't stream 4K at all to Linux devices, and they often don't serve 4K if they deem your internet connection to be too slow. For many folks, pirated 1080p is par with what they'd get streaming, and pirated 2k or 4k is better. reply chmod775 58 minutes agorootparentDo they even stream 1080p to Linux devices by default now? I always had to use a browser extension to make it do that[1]. However the extension seems to be gone.[2] [1] https://github.com/vladikoff/netflix-1080p-firefox?tab=readm... [2] https://github.com/vladikoff/netflix-1080p-firefox/issues/28 reply dijit 45 minutes agorootparent720p for Linux. https://www.reddit.com/r/linux4noobs/comments/16yf3wi/why_do... reply y7 2 hours agorootparentprevAmazon Prime Video is even worse, on Linux they only serve 480p. Any pirated file is much better quality. reply Dylan16807 4 hours agorootparentprevWhat do you mean by 2k? Because people should not call 2560x1440 2k, and I've never seen a download that size either. reply ffsm8 3 hours agorootparentWhy do you single out 2k? The term \"4k\" is just as wrong and purely marketing driven as well. The resolution that's usually behind 2k, which is 1440p as you've correctly pointed out, is usually available as torrents too. reply Dylan16807 3 hours agorootparentRounding 1920 to 2k and 3840 to 4k is not too bad. And yes it's marketing to switch from height to width, but whatever. Rounding 2560 to 2k is massively confusing. Don't do it. 2.5k or don't use \"k\" at all. And when I go look at a couple torrent sites and scroll through movies and tv shows, I'm not seeing a single 1440p in the first couple pages. Some searches show barely anything at all. reply the_real_sparky 1 hour agorootparentI have a proposal for you: Take back the k from the marketers. Define: k = multiple of 1920x1080 pixel count 1920x1080 ~= 2M pixels = 1k 2560x1440 ~= 4M pixels = 2k 3840x2160 ~= 8M pixels = 4k So now 1440p = 2k, and k becomes meaningful. Problem solved! It also gives a solution for ultrawides like 7680 x 2160p, which are 8k. More interestingly, “8K” TVs now become 16k TVs, which marketers should like. We’ve come full circle! Now the k nomenclature also gives you an idea of how difficult that 16k display will be to drive with a video card relative to your existing monitor. reply Karellen 44 minutes agorootparent> I have a proposal for you: Take back the k from the marketers. And how do you propose getting your definition out in front of more people than the combined marketers of all the legit sources of movie, TV and other streaming content in the world? lol Pick the battles you have a whelk's chance in a supernova of winning. reply defrost 2 hours agorootparentprevThe first site I checked has 317 pages of 50x 2160p listings per page going back seven years. The most recent entry is: How To Train Your Dragon The Hidden World (2019) 2160p 4K BluRay 5 1-LAMA Format : HEVC Width : 3 840 pixels Height : 1 634 pixels Display aspect ratio : 2.35:1 Near the top is a recent TV episode: True Detective S04E04 Night Country Part 4 2160p MAX WEB-DL DDP5 1 DoVi x265-NTb Format : HEVC Width : 3 840 pixels Height : 1 920 pixels Display aspect ratio : 2.000 What makes things problematic is the overbearing love for letterbox like aspect ratios, even pirates have standards and they're having to bundle a slew of aspect ratios together .. this comes from the production companies. reply Dylan16807 2 hours agorootparentI think you agree with me? You overwhelmingly see 1280x720, 1920x1080, and 3840x2160, sometimes with a truncated height because of aspect ratio but usually advertised with the full height for consistency reasons. There's barely any 2560x1440. Anyone going above 1080p goes directly to 4k. Youtube is pretty much the only place I've ever seen 1440p encodes, and that's because they're super version-happy and make 20 different variants of a video. reply defrost 2 hours agorootparentPerhaps, I thought your complaint was not finding enough \"4K\" (not a term I like much). If it's about finding 1440p that'd mainly be because it's not a common broadcast format to the best of my knowledge - I just don't see it about much. Articles such as: https://en.wikipedia.org/wiki/4K_resolution don't mention it as a broadcast format, and articles that are specific to 1440p: https://en.wikipedia.org/wiki/1440p have it as : As a graphics display resolution between 1080p and 4K, Quad HD is regularly used in smartphone displays, and for computer and console gaming. reply ffsm8 1 hour agorootparentprev1440p isn't really available on official streaming platforms, so it is indeed a lot more rare. It's pretty much only available on original encodes, i.e. BluRay rips, this makes it a format that very rarely seen on currently airing shows, which are mostly webrips from official streaming platforms. You'll often see it alongside the usual resolutions for movies that have since been released on disk. reply thisislife2 53 minutes agorootparentprevI personally think streaming services are overestimating that the 2k or 4k streams that they offer is a huge advantage they have over \"pirate services\". I don't think they have properly researched the consumer psychology or the network effect that is making piracy popular among a large segment of the middle-class and lower strata. An hour long 30 Mbps 4k HDR+ video file will be roughly around 10-15+ GB with H.265 encoding. As others have pointed out, a well encoded 720p or 1080p video offers a decent enough viewing experience quickly at far, far smaller sizes than a 2k or 4k videos (file sizes will be 10 to 20 times smaller at these resolutions). Note also that some pirates encode videos with the CPU, than using hardware encoders, and thus these videos tend to have a higher quality with better compression (hardware encoders, while blazing fast, tend to do a poorer job than CPU video encoding). Thus, these smaller sized video files don't require high-speed internet, can be downloaded fast and also encourages people to save the videos longer. This allows some to create their own personal video library. So a side effect of this is that people store and share these videos longer, and their smaller sizes now allow streaming torrents of popular content. Some torrent sites today have even started offering this through the browser itself - so non-techies now don't even have to download any torrent software and learn how to use it. That's near-Netflix like convenience, with more content, for \"free\" - and that's what these services are up against. We also can't ignore that 4k videos are often only available at higher tier subscription plan. So even if Netflix, or other streaming services, think that some of these people can be enticed to subscribe to their services, with their high quality 2k or 4k videos, they will have to offer them at a lower price to beat the \"free\" model of piracy. (It's very hard to compete with \"free\" - just look at Google search engine's market share and its non-free competitors' market share to understand this). All this is of course irrespective of the fact that 2k and 4k resolution HDR+ videos are also increasingly available now a days on torrents too. reply plastic_bag 4 hours agorootparentprev> That's still YIFY-quality video, which is a few steps below HBO/MAX, which happens to be among the lowest quality paid streams. You don't necessarily have to pick YIFY. Qxr's uploads are a much better comparison. x265 encoded 1080p (with 5 - 6.5Mbps bitrate) from the highest quality sources and they look very good even on 55\" 4K panels. reply tasuki 2 hours agorootparentAre these, like, famous pirates or something? reply porbelm 1 hour agorootparentThey are well-known pirate groups, yes. reply mike_hock 2 hours agorootparentprevMaybe your eyesight is a lot better than mine but I can't discern pixels at sub-1080p resolution, especially in a movie where everything is kinda smudged out and individual pixel values don't matter as much. reply Jochim 1 hour agorootparentIt's usually blocky compression artifacts that show up at 720p and some 1080p encodes. They're most noticeable in dark or fast moving scenes. On Netflix, The Sandman looked abysmally bad at 720p. The complex backgrounds became a garbled mess and the dark scenes were filled with banding. reply exe34 2 hours agorootparentprevI grew up with analog TV where the colours used to bend and you had to kick the antenna with a stick after the wind moved it a bit. If I can tell the characters apart and the audio is synchronised, I don't need any better quality! reply anakaine 2 hours agorootparentYou and I both. I had the fortune of taking the kids out to look after a field camp years ago, where a storm.in the distance would cut internet and TV signals, and cloudy weather would knock stuff out for days. They learned a different way to use a compass and how to repoint an antenna to get signal using team work. Thoroughly rate it. reply freddie_mercury 3 hours agorootparentprev25% of all streaming globally is consumed on a mobile device. During the day (commuting to work, during lunch breaks) that number is much higher. Children's programming is also largely consumed on mobile devices (well, tablets). On small devices like that 4K HDR+ video is kind of meaningless. reply josephg 2 hours agorootparentIt’s also kind of meaningless at regular tv viewing distances. I have a 4k tv but from my couch there’s no way I can tell 4k apart from 1080. And honestly, 480 movies (dvd resolution) still look fine to me too. They have a certain aesthetic softness to them that I quite enjoy. 30 seconds into a 480 film I stop noticing the resolution at all. reply bayindirh 1 hour agorootparentThat's probably because your TV is seamlessly upscaling every signal it can receive to 4K. Newer TVs are very good at that. reply wkat4242 4 hours agorootparentprevMeh not everyone has a 4K 120Hz TV :) I still have an old 1080p 32\" LG from 10 years ago and it looks fine. In fact I find the motion upscaling weird, it gives this creepy uncanny effect. I still download H264 too because not all of my devices support H265 yet. I was thinking about something new but they're just too expensive. reply pmontra 2 hours agorootparentThe first two paragraphs: you are me. The last one: I don't think about upgrading because my TV is off all the time. I watch it on a tablet or on my phone. I use a Raspberry with a TV hat to stream free to air TV on my home network and apps for the IP based content. My TV is the backup device. About the subject of this thread, too many paid streaming services are maybe too inconvenient. If they are, the market will fix that. That is: some of them won't make enough money and close, or sell to a competitor, or just create content and license it to streaming services. My preference would be not to pay per month but per view. There are months when I don't watch any series and months when there is something I like. About movies, maybe no movie at all for a year or two, then a few of interesting ones, including old ones that I never watched and I feel like to watch. The last one was Gilda, 1948. reply realusername 2 hours agorootparentprev> The Netflix 4K plan serves up high 1080p bitrates Only if you are lucky and everything aligns well. You can't ask customers to debug their hardware to make it work. reply ebb_earl_co 2 hours agorootparentprevWhat is YIFY? reply Double_a_92 2 hours agorootparentA pirate group that was known for releasing movies in small files, but with a decent quality. reply 10729287 2 hours agorootparentprevSorry to focus on such a detail but VBR nevern been the standard in the Piracy scene. Back in the days it was 192kbps reply getcrunk 1 hour agorootparentprevAv1 doing 1080p at 3-4 mbps will be reach the point of diminishing returns imo for 1080p content. I can’t wait till that become mainstream. It’ll be the perfect archival quality/space effeciency. Too bad even devices that claim av1 support … even some from the makers of said codec .. ahem chromecast w/gtv .. still stutter or fail. reply Dylan16807 6 hours agorootparentprevIf we're looking at the storage cost, I'd say the 10x decrease in flash price is doing a lot more than the maybe-2x decrease in file size. reply xbmcuser 6 hours agorootparentFew people are building flash nas though the price pressure from flash storage on hdd prices did play a big part. Before flash storage prices started pushing pressure on hdd prices per tb larger drives had become more expensive than smaller drives. We are I think just 3-4 years away from 8tb SSD being cheaper than hdd for me that is the point where we will have mass SSD adoption for Nas drives. reply Dylan16807 4 hours agorootparentI'd guess that the kind of person that makes a NAS wouldn't have been stopped by h.264 sizes. The comment I replied to seemed pretty focused on ordinary system drives. reply Sakos 1 hour agorootparentprevAlso AV1. I got a 1080p version of a movie encoded in AV1 and it looks pretty damn good at only 1.5GB. The x264 version I have at over 2GB has a lot of obvious artifacting by comparison. reply DrBazza 14 minutes agoparentprevAnd they're going from binge-watching ad-free episode of an entire series, to releasing one episode a week... with adverts. Yeah, no thanks. I'll just VPN+torrent the ad-free version and watch that, thanks. On any device of my choosing as well. And why should I pay £8.99 or whatever to one streaming service, for one programme, that was previously \"free to air\" for the other series? Paramount taking back Star Trek from Netflix for example. I'm not sure how many multi-million dollar, unseen cancelled-after-one-season dramas with 50,000 viewers these studios need to create before they realise 'we need viewers to 'build a franchise' and make a profit'. Just imagine if these series or dramas were only seen by a few thousand people at most: Star Trek, Battlestar Galactica, Band of Brothers, Shogun, Dallas, Cheers and so on. Even How I Met Your Mother and Big Bang Theory were pretty much pre-streaming when they started, but syndicated to world-wide terrestrial channels. reply zug_zug 8 hours agoparentprevI used to try to go 100% online rental (primarily amazon rentals if I didn't want hassle). I figured it was hard to argue a 3.99 rental was any worse than blockbuster. So on vacation I tried to airplay mission impossible from amazon rental to the TV, and the screen just went black for copy-protection. So I refunded my rental and torrented it instead. I don't usually torrent, but sometimes the ecosystems put it squarely in my self interest. (Also, for whatever it's worth I have netflix active about 1 month a year because I find their recent content so unimpressive, but perhaps I'm not the target audience) reply bilalq 7 hours agorootparentTry disabling hardware acceleration in your browser. It'll probably fix the black screen issue. The situation is still ridiculous though. Streaming is no longer easier than piracy. reply ladzoppelin 6 hours agorootparentNo its a DRM issue, its why I run a hidden 25 foot HDMI from my computer to my TV as some broadcasts require the security inside the HDMI cable itself. (I could be wrong on this one but Widevine DRM handles this process I think) reply js2 5 hours agorootparentThere is no security inside the HDMI cable itself. It's an HDCP negotiation between the source and the sink which occurs over an i2c serial bus along the pair of DDC pins. You may be able to get away with just an HDMI dummy plug: https://www.amazon.com/4K-HDMI-Dummy-Plug-3840x2160/dp/B07FB... I'm not sure if these dummy plugs handle HDCP. If not you can use one in combination with this splitter: https://www.amazon.com/Splitter-l-b-y-Vision-Atmos-Scaler/dp... I own this splitter and know for a fact that it handles the HDCP negotiation. But I think it needs a display attached. reply jasomill 2 hours agorootparentThe GP was trying to play back Amazon DRM content via AirPlay, so HDMI/HDCP should only be relevant to the \"last-mile\" connection between the AirPlay receiver and an external display (so, if the receiver is a Mac laptop's internal display or a smart TV, not at all). AFAIK, DRM video playback over AirPlay only works from within iOS apps that explicitly support AirPlay[1], not via generic iOS/macOS screen mirroring. [1] https://support.apple.com/en-us/HT204289#iOS reply js2 2 hours agorootparentThat's true but my reply was solely to ladzoppelin where they seem to need to have an HDCP-compliant display connected. I was just offering an option besides a 25 ft HDMI cable. reply esperent 3 hours agorootparentprev> You may be able to get away with just an HDMI dummy plug Or, you can just pirate it and not have to put up with any of this BS. reply walteweiss 2 hours agorootparentprevWhat is an HDMI dummy plug? I read the description, but didn’t get it. What does it do? reply js2 2 hours agorootparentIt makes the source think there is a display connected by responding with an EDID block. It may also handle the HDCP negotiation but I'm not sure about that. Technically an unlicensed device isn't supposed to be able to implement HDCP so while the Chinese manufacturers are totally willing to skirt around that, they also don't go advertising it. reply 9991 7 hours agorootparentprevWeird. Jellyfin doesn’t make me do that. reply jackson1442 7 hours agorootparentIt’s a Widevine DRM thing, not a browser video thing. reply ncallaway 6 hours agorootparentI think that was the point of the previous comment. The UX of DRM-laden services breaks. You don’t have that problem with Jellyfin. reply martindbp 2 hours agoparentprev> The services have taken the piss and now they'll get the repercussions of it. Whose fault is it really that all content is not available on a single service? It's nobody's fault, it was inevitable given how things work. How could you \"fix\" it? You'd need legislation saying that all content must be made available for licensing at the same price for every service. But that's not enough, because why would HBO sell their content to Netflix and vice versa? No, you'd have to force them to make it available. But that's not enough either, as they could just set the price extremely high. And at this point what would be the point in having multiple streaming services at all? Perhaps we need legislation that enforces separation of content producers and distribution, like the Glass-Steagall Act but for TV, or Net Neutrality? reply josephg 2 hours agorootparentThere was a wonderful point made by Gabe Newell a few years ago on this. He said the only thing you need to do to “beat piracy” is provide a better service than piracy offers. And that’s pretty easy, for all manner of reasons. I thought he was way too optimistic at the time - but here I am years later with hundreds of games in my steam library. I haven’t pirated a video game in years. I think Gabe’s rule is like gravity. It’s inconvenient for space travel, but what are you gonna do? You don’t get to space by complaining about it. It’s the same in streaming. All the streaming platforms need to do is provide a service that’s better than piracy. And I really do feel for the streaming platforms here. That seems like a hard ask for them. Netflix rarely has content I want to watch any more. Shows I do want to watch are often on a different competing service. And they often remove shows and movies from the platform that I might have otherwise enjoyed. Im not subscribing to multiple streaming platforms at once - that’s ridiculous. I’m not surprised they’re complaining about piracy - they’re rapidly becoming a worse service than piracy and gabe’s law is kicking in for them. But at the end of the day, like space companies, complain if you want but it’s not my problem to fix. And there are plenty of fixes - make it easy to have a grab bag of streaming services which swaps streaming provider every month. Or give me access to every platform for my $10 and distribute the money based on what I actually watched. I dunno. Figure it out. Or don’t, and watch piracy eat your lunch. Your call, Netflix. reply martindbp 2 hours agorootparentI think TV and games are meaningfully different. Most people tend to watch way more number of shows in a year than buying/playing video games. Video games have more replayability, so it's something you need to \"own\" rather than stream. Therefore, subscriptions for games are not a big thing yet (I doubt it will ever be). As it stands now, if a game I want is not on Steam, that's OK, I can just install another game store. It's a bit inconvenient but not too bad. The problem with TV is cost. I'm OK switching between Netflix, Disney, HBO, Youtube, whatever, but I'm not OK with paying $70 a month, reply maccard 41 minutes agorootparent> Therefore, subscriptions for games are not a big thing yet (I doubt it will ever be) World of warcraft has had a subscription for 20 years and is wildly successful. PSN has a monthly subscription where you get 2-3 games per month added to your account that you can play as long as you're still subscribed. Gamepass has 25+ million subscribers [0] > I'm not OK with paying $70 a month Honest question, why not? That's what cable TV cost 15 years ago, and $70 now has the purchasing power of ~$50 then. [0] https://gameworldobserver.com/2023/09/15/xbox-game-pass-30-m... reply gtvwill 23 minutes agorootparent$840 a year just on watching TV is heaps. $4200 over 5 years. That's a fair wack cash. reply Mystrl 1 hour agorootparentprevI think media like TV/Movies might be fundamentally different than games because of how easy it is to serve pirated content. With games there's a lot of friction with getting cracks working and risks with malware while with TV/Movies you just watch them in your browser. Recently my friends showed me a pirate site that I'm in awe hasn't been taken down. Don't think I can post a link here but it has all the things I'd expect from a paid streaming service (slick and fast ui, large catalog with good discovery options, high quality streams, no lag, random ui niceties like a dimmer switch, skip op/ed and multi language subs). Being honest as long as something like this is available I don't really see what a site like netflix could offer that would convince me to pay. Actually the more I think about it the less I understand how it hasn't been taken down. reply maccard 46 minutes agorootparent> Being honest as long as something like this is available I don't really see what a site like netflix could offer that would convince me to pay Most people are honest, and will do the the legal thing when faced with two equal options. If the choice is between that UI but it's illegal, and a reskin of that UI but it's £30/mo, I'd pay. But it's not, as the other comments here said. It's a choice between that for free and a limited catalog of region restricted, ever changing, poor video quality streams spread of multiple services, and given the choice between paying 4 providers for a poor service and getting a good service from one location, people will pick the latter. reply somenameforme 2 hours agorootparentprevIt'll be naturally fixed by economics. We'll reach an equilibrium point where offering an exclusive streaming service becomes less profitable than simply licensing your content to another service or services. Obstinance will probably drag out the timeline, but it won't change the outcome. [1] - https://en.wikipedia.org/wiki/Tragedy_of_the_commons reply realusername 2 hours agorootparentThose media conglomerates are very stubborn though, they care more about keeping their rights than money itself. reply toast0 2 hours agorootparentprevMusic has compulsory licensing, and yet we have different radio stations, and multiple streaming services (only some of which is covered by compulsory licensing). If compulsory licensing for tv and movies also implied the service was picking the episodes, you'd have potential for competition if service A just played all of the episodes in order and service B only played the good episodes. Etc. reply izacus 1 hour agorootparentprevFRAND licensing is a thing. And forcefully splitting playback/delivery and content creation would create a much healthier market where content companies have interested in distributing their content to as many streaming services as possible to get revenue - e.g. see the situation with audio playback. With Spotify/YTM/Tidal being separate from music labels, there's much MUCH less of this exclusivity BS. reply ReptileMan 2 hours agorootparentprevIt's type of licensing called FRAND. Has been utilized with success. reply heavyset_go 8 hours agoparentprevI think the cable TV model was a mature business model for extracting the most amount of value possible from paying viewers before they choose to leave. Every streaming service will converge to what cable TV is like: paid channels, maximum amount of ads, additional premium content, attempts at lock-in, exclusive licensing of content, etc. Not adopting the cable business model is leaving billions of dollars on the table, so it will happen. reply MostlyStable 8 hours agorootparentI'm curious where the equilibrium is. There is some portion of people who will not torrent no matter what. Either because of lack of ability or moral views, or some other reason. There is a second set of the population that will never pay for media, or at least not more than very trivial amounts. Either because of lack of ability to pay, not thinking it's worth it, or not believing there is anything wrong with piracy. Then the is the third group that will happily pay for content when it is reasonably priced and, perhaps most importantly, easy and convenient. If it's too expensive or too difficult/annoying, they will pirate. The size of this third group dictates where the equilibrium is. If it's very small, then media will always converge in the cable style maximum extraction. But if that second group is large, then the cable model might lose more money than it gains by pushing consumers towards piracy. reply feoren 7 hours agorootparentYou're missing a 4th group: people who would enjoy watching content X, but simply won't bother if it's too annoying. The alternative isn't piracy, it's just doing something else with your time. There's not that many shows or movies that I care enough about to keep one subscription open for it. I had an HBO subscription for Game of Thrones, but I also enjoyed Succession, Last Week Tonight, and some other random stuff. After Game of Thrones shit the bed, I canceled HBO and simply don't watch those other shows. I never finished Succession; I'd like to one day, but it's not worth either the subscription or pirating. This just shows that the availability of piracy is NOT the problem. It shouldn't matter to HBO whether I've pirated it or not: I could be a paying customer if their prices matched their catalog's value, but it doesn't, so they get nothing, piracy or no. reply vineyardmike 5 hours agorootparentThe 4th group is important because they didn’t compete well throughout most of Cable’s tenure. Now, YouTube, TikTok, etc eat away at people’s video diet. Video games are now fully mainstream too. Normal social media (Twitter, Reddit, Meta) take even more. I’m in group 4. I’ll pay for streaming services to avoid ads, but never linear TV. TV is largely superior content - it’s professionally made and generally high quality. But if I get sick of jumping through hoops, I have plenty of other things to do. reply WWLink 6 hours agorootparentprevI support your move, it's better than piracy. The problem with piracy is if you become a fan of a series and gossip with your friends about it, now you're marketing that company's crappy service. It's a weird way to go about things. reply TeMPOraL 5 hours agorootparentIt's weird because TV is also culture. Game of Thrones isn't just some random piece of personal, single-player entertainment: it was, and is, an important cultural phenomenon for at least half of the planet. That's what makes the dynamics so weird: it's not just, or even primarily, about your personal entertainment - there are social aspects involved, from wanting to belong with your peer group (e.g. talk about latest GoT episode with your cow-orkers at the water cooler), to all kinds of rituals (e.g. watching romantic comedies with a date/spouse). Same applies to books and musics, to a lesser degree to magazines/news media. For some reason, it mostly doesn't apply to videogames; I used to think it was because they're a new thing, but these days, I think it's rather because the experience is much more individualized and requires much more effort (and time), making it a poor social object. reply prmoustache 1 hour agorootparent> It's weird because TV is also culture. Game of Thrones isn't just some random piece of personal, single-player entertainment: it was, and is, an important cultural phenomenon for at least half of the planet. > > That's what makes the dynamics so weird: it's not just, or even primarily, about your personal entertainment - there are social aspects involved, from wanting to belong with your peer group (e.g. talk about latest GoT episode with your cow-orkers at the water cooler) OTOH if you don't have that in common with other people, you just talk about other things. It is the same thing with soccer for example which is the #1 subject around coffee machines for men (and now increasingly women). reply TeMPOraL 52 minutes agorootparentYes. However, attention is finite, the intersection of topics any one in the group is willing to talk about is narrow, and my point is that TV shows occupy significant fraction of it - that's why they have this weird dynamic where seemingly extraneous entertainment product is treated by many as basic human right. Because in some sense, it is: it's a big component of the shared experience, which itself is the glue that holds society together. reply dasyatidprime 4 hours agorootparentprevI do observe it applying to video games in many circles. This is more true for social (cooperative or competitive) games and in game-adjacent social contexts like Twitch streaming, but it's also applicable to recognizing references and plot beats from widely-known classics or having similar experiences over particular moments or levels, even in single-player. Pokémon, Final Fantasy, Minecraft… reply masklinn 3 hours agorootparentprev> Same applies to books and musics, to a lesser degree to magazines/news media. For some reason, it mostly doesn't apply to videogames It absolutely does, but apparently not for your peer group. reply riwsky 4 hours agorootparentprevgames are culture—shared, social culture—you just need better friends and coworkers. reply whstl 7 hours agorootparentprevI started using pirated copies of some music software I actually paid for and own, because it became too problematic and inconvenient. Why do I need a kernel extension for this crap? Fuck iLok sincerely. Next step is not paying for anything altogether. A middle finger is the only thing those kinds of companies are getting from me in the future, and honestly I'd rather see their demise. If streaming ever becomes like this, I'm also going back to piracy. reply izacus 1 hour agorootparentprevI think a lot of that will be answered with even more aggressive lockouts, DRM and \"protections\". Platform like Apple's iOS really play into this because they're designed to not allow users to bypass the money extraction and vote with their wallets. reply Red_Leaves_Flyy 6 hours agorootparentprevGroup three depends on means, motive, and opportunity. reply tzs 5 hours agorootparentprevI liked cable, except for the price. Cut it by a factor of 3 and I'd probably subscribe again. For movies pretty much everything I wanted to watch would eventually show up. Typically a movie would first be in theaters, then become available for purchase on disc or digital, rental, and be included in one paid streaming service. Sometime later, maybe two years after it was in theaters, it should show up on some non-premium cable channel. The only time I couldn't wait for things to show up on non-premium cable was when \"Avengers: Endgame\" was coming soon. I knew that one would be hard to avoid major spoilers for once it was released in theaters so decided seeing it in theaters was a must. That meant I had to get get a month of Netflix, which had the Marvel movies then, to get caught up enough for \"Endgame\". I also liked cable for rewatching TV series. I'm not sure why, but when some channel is showing \"The Simpsons\" or \"South Park\" reruns on a regular schedule I like watching much more than I do when I watch an episode on an on-demand streaming service (or on DVD). I think it has something to do with the latter being more deliberate. I picked that episode, which somehow makes it so I feel like I need to take it more seriously. When it is just \"This channel shows a couple episodes every night\" then it seems a lot more casual, like listening to music on the radio. It is easier to just sit back and relax and watch. reply iamtedd 1 hour agorootparentWhy would you pay to be advertised at? We can argue about how pervasive advertising is and how subtle it can be, but cable TV literally has commercial ad breaks. reply FpUser 5 hours agorootparentprev>\"Every streaming service will converge to what cable TV is like: paid channels, maximum amount of ads, additional premium content, attempts at lock-in, exclusive licensing of content, etc.\" I am more than willing to pay for content subscription assuming they have everything under one roof and no ads. Pay for cable like service - fuck it. I'd rather watch the paint dry. reply jdhbb 4 hours agorootparentprevOther side of the equation is - Production of Content has become cheap. Distribution has become free with global reach. So we have Infinite amount of content floating about. Not even bringing generative AI into the picture. So infinite growing Supply. Finite demand. reply jpc0 2 hours agorootparent> Other side of the equation is - Production of Content has become cheap I spend a lot of time in both production and post production. The price for good content has certianly decreased but it most certianly hasn't become any definition of cheap. Consumers want very hogh quality content these days and that only comes with experienced staff, you can use cheaper gear. > Distribution has become free with global reach Every time someone says this I want to know what where they get this information. So now I am going to ask, prove it. Show me the VOD service or streaming service that is free with global reach. Even network egress from any provider costs a ton of money nevermind compute for transcoding and storage costs. Again prove it. reply jdhbb 1 hour agorootparentPorn is free. They are running the same services. They have proved global distribution is possible. Pretty much like Youtube and Tiktok. Media and Entertainment sector has all kinds of bizarre economics cause it's used by the Telcos(check what AT&T and Comcast own in ME sector), the Tech firms (google/youtube, amazon, netflix, apple) to play empire defense and market capture games. Demand is not decided by the consumer. Or marketing depts won't be getting the budgets they do for demand engineering from their tech and telco overloads. But all that free cash dries up or slows downs once the market capture phase is over which is what has happened with the end of the streaming wars. Disney, Viacom et al have realized they can't compete against tech platforms or telco subsidized shops on the distribution front. reply philipov 8 hours agorootparentprevThe money's not on the table if people can and will leave. That's the point of competition. Piracy is entirely in the spirit of free market capitalism: the equilibrium price of a good with infinite supply is 0. reply galahad_ 7 hours agorootparentThe problem ist that a production of f.e. a movie costs money. If the people that make movies don't earn money, they will not make any movies. So they have to earn that money somehow. If the price of their good is 0 why should they bother with filming? reply hedora 6 hours agorootparentThey could earn their money by not actively pissing off their customers. The streaming industry proved that consumers prefer paying for convenient streaming over messing around with privacy. For some reason, they feel they need to burn their own industry down in order to prove that people prefer piracy (or not watching) over inconvenient streaming setups. Is anyone really surprised by the current outcome? reply runevault 3 hours agorootparentEveryone else saw Netflix making a lot of money and decided just making a healthy amount of money renting the rights to them they had to try and make more money releasing their own service. Gotta love modern capitalistic greed. I wish streaming services was like movie theaters where production companies are not allowed to own theaters (aka the distribution network). reply galahad_ 2 hours agorootparentOwning theaters was forbidden for movie studios, but afaik making movies was always allowed for theaters. reply philipov 6 hours agorootparentprevYes, distribution is a service, and providing good service adds value that people are willing to pay for. reply vonjuice 2 hours agorootparentprev1. That IF is untested, it's hypothetical corporate propaganda. 2. That's a problem for them to solve, it's not a problem for me to solve by going like \"alright Sony, I'll do you this solid and pay and inconvenience myself instead of pirating just because I like you and I know you're struggling\" wink wink reply philipov 6 hours agorootparentprevBecause they love making movies, and can find other ways to get people to pay them for their labor rather than for copies of the movie. This is already happening today. reply mr_toad 4 hours agorootparentprevIt’s not an equilibrium price, because the average cost is higher than the marginal cost (which is indeed close to zero). Free markets don’t work unless the marginal cost (and the price) is higher than the average cost, for obvious reasons. reply galahad_ 2 hours agorootparentThe average cost ist always higher than the marginal cost for digital products, for obvious reasons. So you are literally saying free markets don't work for digital products. reply dgfitz 11 hours agoparentprevOf course this is 100% the correct reasoning. When I was a broke college kid I pirated all the things, and at the time wished, when I could afford it, I’d rather just pay. When I could, I did, for years. Now we’re back at the path-of-least-resistance is pirating. reply eastbound 11 hours agorootparentBut that means companies must sell for the marginal price of their website’s ergonomics, not for the marginal price of producing movies. (Which is not a big loss, because I can’t imagine producing the Netflix knockoffs be a hundredth the price of a 007). reply al_borland 10 hours agorootparentIt’s not about an individual website, it’s about the industry. It’s not that Netflix, or any other service, is hard to use on its own. It’s the fragmentation of the content that makes it hard to use. This was solved for with music where pretty much every service has pretty much every song. People don’t need multiple music services, they just pick the one that best fits their style and needs. This is what the movie and TV industry needs to do if they want to beat piracy. This is what they should have done 15 years ago. Their greed is hurting them more than it’s helping. reply data-ottawa 6 hours agorootparentI had Prime Video, and when searching for content I would get told to subscribe to some channel within Prime. Now Crave (in Canada) does the same thing. Netflix doesn't bother integrating with Apple TV's search or aggregation features to try and drive you into the app. The end result is searching for legitimate content is a terrible experience. If it's a result from Prime, I just pass. If it's from Crave, I pass. If it's on Netflix, I don't see anything at all. At least a decade ago streaming boxes had one search box and it would return results from all the sources, and if it appeared you knew you could watch it. What the streamers provide today is a significantly worse experience than piracy, and I don't know how they haven't realized this. reply inversetelecine 6 hours agorootparentAnd now prime wants another $2.99 to stream without ads. Something I had previously included. Nickle and diming also drives customers away. reply midasuni 3 hours agorootparentDrove me away. Increasing the price on my yearly renewal wouldn’t have been noticed. Sending me adverts means Amazon have lost out on far more than my cancelled prime subscription - I haven’t bought anything from them, and that’s over $100 a month revenue lost. reply vineyardmike 5 hours agorootparentprevLet’s not forget that “music solved this” ignores the fact that Spotify is not profitable, and may never be due to the record labels cut. Apple Music, Amazon Music and YouTube Music may or may not be profitable but are undoubtedly subsidized by their corporate parents. Not sure if Tidal is profitable but they aren’t popular and are much more expensive than people are generally willing to spend. It’s also potentially subsidized by a corporate parent. There’s also a trail of unprofitable or tiny businesses like SoundCloud and Bandcamp that may or may not survive quarter to quarter. It’s not a solved problem. reply shzhdbi09gv8ioi 5 hours agorootparent> Spotify is not profitable Their stock value doubled in the last year Their CEO Daniel Ek is worth 3.7 billion dollar. reply TheCapeGreek 4 hours agorootparentBoth of those numbers are not profit numbers. reply roenxi 2 hours agorootparentI wouldn't want to be involved in a company that isn't turning a profit. But, in fairness, this is an industry that is adjacent to Hollywood. It is conceivable that they have some sort of weird profit-laundering scheme going to make sure not too much of the money goes back to the artists or something. reply vineyardmike 2 hours agorootparentProfit and loss can be accounting fiction when needed (eg Amazon), but I think Spotify has the cartel curse. No matter how much money they make, it’s too easy for the labels to see the public finances of Spotify and decide they want to raise the price of licensing. Furthermore, the more people use Spotify, the less profitable the subscription becomes. reply jowea 8 hours agorootparentprevI wonder if mandating content licenses being non-exclusive and separating media distributors from producers would help. reply deaddodo 8 hours agorootparentThis seems to be the most obvious solution, and is what solved the movie industry's greed back in the the studio system days. But, the industry will fight it tooth and nail, because they lose control and potential margins on it at the expense of the consumer. reply dehrmann 6 hours agorootparentprevIt's mostly working for music. Spotify and Apple aren't competing on their catalogs. reply baby_souffle 7 hours agorootparentprev> It’s the fragmentation of the content that makes it hard to use. I wish somebody could get all the media execs in a room and just deliver a simple message: \"You can't compete on price; your competition is free. You must compete on experience if you're to compete at all.\" With just a few hours worth of time and effort, it's possible to get a pretty sophisticated and automatic system set up that essentially makes getting a copy of a movie a literal one-click operation. You won't be able to watch it _right then_ as the find, download, process, import ... etc pipe takes time to run but at the end you'll have a high quality copy that will work on any device anywhere in the world. Other than the immediacy, piracy is cheaper and less burdensome. reply TeMPOraL 5 hours agorootparent> I wish somebody could get all the media execs in a room and just deliver a simple message: \"You can't compete on price; your competition is free. You must compete on experience if you're to compete at all.\" Unfortunately, there is the House of the Mouse, aka. Disney Legal Military Industrial Complex, and they don't let individuals or governments tell them what they can or can't compete on. reply izacus 1 hour agorootparentprevThey did... and the execs ordered everyone from OS, hardware to software companies to add more DRM instead. reply internet101010 7 hours agorootparentprevYeah that's what I have and I will never go back. I open one app for VOD content and one for audio content. The quality is often superior to the streaming platforms for movies. Not having to care about which platform something is on is so nice. reply Geisterde 7 hours agorootparentprevWhich could be rapidly eclipsed, my synology nas is capable of following RSS feeds for new content, torrenting that content, and serving that content through my plex with audio options, subtitles, descriptions, cover art, ratings, age restrictions etc. (All behind a vpn of course) If I invested a weekend (4 days, maybe 2-3 weekends...) I could probably figure it out, and never ever ever again deal with the BS streaming services. It takes one person pissed off that they missed Ozark and that will be 1 click install, game on, open source tech only ever moves forward, its antifragile, unlike any individual business. reply prepend 7 hours agorootparentYou need to invest about 10 minutes. It’s not complicated at all and makes a much better experience. reply hedora 6 hours agorootparentSetting it up properly probably takes a day or so. Which HDDs should you put in the synology (or are there SSDs that make more sense; need to check $/GB…)? Which VPN service makes the most sense? Which RSS thingy is as good as Prime, Netflix, HBO, etc, etc, combined? reply acdha 9 hours agorootparentprevI’m not sure it has to be exclusive: some people are inveterate freeloaders but most of us understand that the artists have to get paid, so it’s not just a question of being so cheap that piracy isn’t appealing but also having users feel like they’re not being taken advantage of. The problem feels a lot like the situation MoviePass worked themselves into where they priced it unsustainably low but cheap money allowed that to run long enough to train a generation of customers that their content was only worth that much. I imagine Netflix would dearly love to say that your subscription includes a certain number of movies but you can add on more, or that there’s a premium tier, etc. but there isn’t an easy way to try something like that now that everyone is used to the current model. The other side of the problem is that studios really don’t want market pricing. Subscriptions are banking on the idea that you’ll still pay $12/month and find something else to watch even if the big series/movie they’re promoting isn’t very good, and they’re keeping the prices for rentals really high which makes means people are often looking at 1-2 months of streaming charges for a single movie rental. The combination of the two seems likely to make a lot of people pirating for years to come. Most people do not want to juggle multiple services and if they’ve learned how to pirate anything once it’s always going to be there the next time someone jacks up prices or plays games with availability. I think that’s going to last until someone has the courage to let services like YouTube or AppleTV offer rentals of everything at reasonable prices. reply doctorpangloss 5 hours agorootparentprev> But that means companies must sell for the marginal price of their website’s ergonomics, not for the marginal price of producing movies. But that means companies must also realize the tech industry is a really bad business for 99% of companies in it. reply HumblyTossed 7 hours agorootparentprevSo pay for several services and then pirate the content because it’s just more convenient. Best of both worlds. You’ve paid and you get the content in the form(s) you need. reply robertoandred 5 hours agorootparentprevI mean, stealing is always easier than paying for something. reply Dban1 5 hours agorootparentI'm sure you yourself can come up with counter-examples for that. reply roenxi 2 hours agorootparentAnd, furthermore, piracy isn't stealing. It is a major point here that the initial owner of the thing still possesses the thing. They're trying to assert some unnatural right of control over what other people do. It isn't even all that clear why this is a good idea. I can see how it makes sense for legislators because of the presumably significant kickbacks they get from lobbyists; but there is a still an open question of whether this is a good system. It looks like it has had a big negative impact on our ability to sustain a shared culture. If people have to buy in to a culture, a bunch of people won't. People still can't freely distribute Lord of the Rings for example. That isn't helping to get people reading it. J.R.R Tolkien has been dead for a while. Anything less famous than that has little chance to stick. reply calvinmorrison 5 hours agorootparentprevThats not true in even the most trivial sense. But the rise of piracy correlates strongly to not cost, but availability. Granted charging $1 per song is a little egregious as well reply kylebenzle 10 hours agorootparentprevThis is HACKER news, our central ethos is (should be) all information wants to be free. Sad to see the desire to pay for content is getting upvoted. reply lolinder 10 hours agorootparentThis is \"Hacker\" news for a bunch of independent definitions of \"Hacker\", and for most of us the definition isn't \"breaks into computer systems and leaks stuff on the internet\". For myself, when I think of the hacker ethos I think of Stallman and the GPL, which is explicitly designed around the idea that information can be ethically sold as long as you're actually selling it [0]. It's free as in freedom, not free as in beer, and under that philosophy being able to pay for services that respect your rights as a free human (which most current streaming services don't) is a desirable end state, not something to be condemned. > Many people believe that the spirit of the GNU Project is that you should not charge money for distributing copies of software, or that you should charge as little as possible—just enough to cover the cost. This is a misunderstanding. > Actually, we encourage people who redistribute free software to charge as much as they wish or can. If a license does not permit users to make copies and sell them, it is a nonfree license. If this seems surprising to you, please read on. [0] https://www.gnu.org/philosophy/selling.en.html reply defrost 10 hours agorootparentThis is the ballpit sandbox forum for YCombinator, a club for software startup investment fund managers who can quote from the Tibetian Book of the Dead. The lesson here is that attrition and shrinkage will always be with us, it's only the rate that varies in market accordance with the enshittification of product. The 'better' the product, the lower the shrinkage. As supported by a majority of thread comments \"I used to pirate when I was poor, then I didn't - now I do again as the commercial offering sucks\". [attribution] https://thebillionscompanion.net/all/tag/Tibetan+Book+of+the... reply topkai22 7 hours agorootparentprevIts revisiting the origin of that phrase, which “On the one hand you have—the point you’re making Woz—is that information sort of wants to be expensive because it is so valuable—the right information in the right place just changes your life. On the other hand, information almost wants to be free because the costs of getting it out is getting lower and lower all of the time. So you have these two things fighting against each other.” I find this to be a much more intelligent formulation of the concept, as it recognizes the reasons and the tension. We simply can’t get much of the media I like for free (gratis). That being said, it is maddening to have so much great content locked up. It feels insanely inefficient and that we produce far too much content unnecessarily. There are probably ways to balance this better- separation of content ownership, production, and distribution; more limits on copyright length; mandatory content licensing; simplification of older content license rules… That doesn’t mean it’s morally wrong to pay for content or put restrictions on distribution, just that optimum probably isn’t where we are. reply xboxnolifes 9 hours agorootparentprevFree as in libre, I agree. Free as in beer, I do not. The marginal cost of copying information may approach zero, but that's only relevant after the costs of creation are covered. The problem is: how does one make something \"free as in libre\" without affecting the \"not free as in beer\". reply usefulcat 6 hours agorootparentprevIf you're going to argue that \"hackers\" are wrong/dumb/whatever for desiring that content creators be fairly compensated for their work, by all means go ahead, but you're definitely going to need to do a lot better than that. reply Dylan16807 4 hours agorootparentprevWanting facts to be free is definitely part of that. Entertainment isn't really the same kind of information, and is more complex. reply dgfitz 9 hours agorootparentprevI’ve never equated “hacker == free” ever. In that save vein, no, all information should NOT be free. I sincerely reject your entire premise. reply orwin 9 hours agorootparentI think information should be free as in freedom, but have a cost that should be paid (in time, money...), is this a better for you? (I disagree with GP BTW). reply dgfitz 8 hours agorootparentMany hearts, I am the GP. reply nkurz 5 hours agorootparentOthers use it differently, but I think he was using \"GP\" to mean \"grandparent\", as in the comment you would get to if you clicked on \"parent\" twice starting at his comment. In that system, you would be the \"OP\", not the \"GP\", although this too is not fully standardized. reply scotty79 7 hours agorootparentprevInformation wants to be free. What people want is irrelevant. Trying to force information to be not free goes against the nature of information. You can try but it costs all of us. reply echelon 7 hours agorootparentprev> all information wants to be free Including your personal information, browsing habits, purchase history, genome, genetic disorders, medical history, sexual partners, kinks, embarrassing photos and videos, moments of social faux pas, biggest fears, darkest secrets, cheating, illegal behaviors, personal psychiatrist notes, privileged lawyer information, passwords, private keys, bitcoin keys, financial statements, tax records, debts, etc. ? reply paulddraper 7 hours agorootparentprev\"hacker\" = \"free information\" ?? I've never heard that definition. reply worthless-trash 5 hours agorootparentNot OP, but this spawned from Woz mentioning this on a video ( https://www.gettyimages.in/detail/video/at-the-first-hackers... ) in the hackers conference. You may be too young (not an insult) or in a different geography to have been seen this, however it was definitely a thing. reply elzbardico 8 hours agorootparentprevThis is not 1999's Slashdot my friend. reply hooverd 7 hours agorootparentprevYC is the man now dawg. reply hiddencost 4 hours agorootparentprevThis is the managed news site of YCombinator. It's a little garden designed to cultivate start-up culture for some of the wealthiest venture capitalists in the world to profit from. reply sharkjacobs 3 hours agoparentprev> the services have taken the piss? Which services do you mean? Netflix? Or do you mean the other 7 services, which didn’t exist back then. There was a brief window of time when streaming was able to compete with piracy (free) and it’s when there was just one streaming service. Netflix was able to have a robust (if not comprehensive) catalogue (because it had no competitors), and was dedicated to user experience (becaues it was willing to burn money to grow its customerbase), and had just secured multi-year licensing deals for content from all the major film and television distributors at bargain basement prices (because no one else had yet realized how valuable those streaming rights were). reply zelphirkalt 2 minutes agorootparentAren't they raking in millions or billions now? Not enough money to pay for the deals now? What is stopping them? Also their browser and software limitations are of their own making. Do away with those, and I might even consider a subscription. Obviously I will not, if they continue to discriminate against me. reply patrickaljord 10 hours agoparentprevI was paying for Netflix 10 years ago in France for a short while, wanted some English subtitles for my wife for some US shows but there were only subtitles in French so had to download the same show elsewhere if you know what I'm saying... with English subtitles readily available. I also remember watching some shows like the first two seasons but then only to find out the third season was licensed by another streaming service lol. Such a UX disaster for paying customers. reply interestica 5 hours agorootparentI was living in the only english-speaking country in South America. Netflix wouldn't give me French subtitles or secondary audio because apparently south america=spanish for their divisions. I don't see how subtitles would fall under the broader licensing agreements. The subs/langs were available for the same titles in Canada. (More absurd because France is a part of South America via French Guiana). reply Symbiote 2 hours agorootparentThis is a common issue across Europe. 40 million+ internal migrants (from one EU country to another), yet most shows on Netflix only have the local language for subtitles and often no option to avoid dubbing. It's worse than when we bought everything on DVD. Region 2 discs (Europe) would have many soundtracks and many subtitles languages on the disc. reply secretsatan 1 hour agorootparentprevThis has also bugged me, they must have access to every language, yet they just don't. I'm sure it's licensing issues. At least normally they have have the original audio versions, but just recently I saw that in Switzerland, they released Suicide squad but the german dubbed version only, why?? reply izacus 1 hour agorootparent> \"I'm sure it's licensing issues.\" I find this term so funny because it kinda makes the problem sound like a physics problem given down from some deity. When it's just the content provider choosing to screw you over by writing a contract that includes regional exclusivity. Especially when these same companies managed just fine when they put all languages on a DVD disc. reply sireat 1 hour agorootparentprevSame problem with Netflix absolutely no local subtitles/dubs available. Meanwhile one local streamer telecom has regional content + Disney for 7 Euros a month with dubs/subs. Another local telecom streamer has regional content + HBO with subs/dubs for 6 Euros a month. Not sure how they got HBO and Disney so cheap but these are multi-country regional streamers - half government / Scandinavian owned. So Netflix had to go. reply jiggawatts 1 hour agorootparentprevI go on this rant regularly, and apparently Netflix staff never visit Hacker News, or just don't care. Interestingly, it took quite a bit of dev effort to make their subtitles this bad. They had to figure out which subset of languages to show in each region, for example. I'm sure these decisions are made by the same team that refuses to show English subtitles, and instead always uses English for the hearing impaired. Because why would any human not be able to understand English unless they're deaf, right? Also... no French people are deaf. Everyone knows that. reply wkat4242 4 hours agorootparentprevSame here. I still have Amazon prime here in Spain but a lot of content only has the stupid Spanish dubbed audio. And my Spanish isn't that great. I mostly pirate everything and I have Amazon prime for the shipping so I don't really care but I would really be pissed if I'd paid top dollar. reply realusername 2 hours agorootparentprevI'm also pirating everything for my wife because of the subtitles. She's not a native French speaker and the French subtitles MUST align with the voice to help her which is never the case with Netflix. reply throwaway2474 5 hours agoparentprevExactly this. I recently wanted to gift my sick mother something to let her watch the shows she likes while she gets better. I check all the major streaming sites, genuinely wanting to buy a subscription because it would be way easier. I considered buying her subscriptions to all the major companies but decided the hassle of managing the logins would be too difficult for her. So in the end I reluctantly ended up torrenting everything onto a single usb stick for her. There is currently genuinely no other way to watch all the things you want in one place other than this. Netflix has accelerated the problem by making their catalogue nothing but shitty self made shows, every year it just gets worse. reply MichaelMug 49 minutes agoparentprevI read this same thing a lot on Reddit and HN. It surprising to me on HN because I would assume most people on here are making good money. For my family here is the monthly spend with taxes for my area included: HBO Max $ 13.53 Hulu $ 7.21 Disney+ $ 0.18 (w/ Hulu) Paramount+ w/ Showtime $ 10.82 Apple TV+ $ 10.81 AMC+ $ 7.56 Amazon Prime $12.53 $62.64/month. Now, about 20 years ago in 2004 my parents had basic cable and they paid $60/month. An my parents combined made no where near what I make today in my salary alone. And with inflation the cost of entertainment has gone down significantly. As for figuring out what is on what- Apple TV has made this easy. Is piracy easier? Legality aside you need a seed box, membership to specific sites, and VPN. Which can add up to more than $30/month just to download pirated content. Not to mention the time investment in figuring out how to get the movies and tv shows to your family members on all of their devices. A budget NAS with 20 TB is going to cost $500. A small PC to transcode also $500. Also all the technical know how required. Backing up 20 TB to the Backblaze is going to cost $13/mo. So lets not pretend piracy is easier. reply alpaca128 2 minutes agorootparentIt's not about the money, but about convenience, as is often repeated. I regularly buy games on Steam yet pirate movies. It can take 30+ minutes and multiple account registrations to figure out whether a movie is even accessible where I live in the preferred language at a given day. I don't watch them every day, so a subscription model doesn't make much sense for me. I'd also pay for interruptions in the stream, lower resolution/quality, and in some cases ads. Why would I ever do that? > Legality aside you need... I don't need a NAS. Transcoding content doesn't need special separate hardware. Technical know-how is also required to troubleshoot streaming issues. Backup is always necessary. Membership to specific sites? Never had one. If piracy was the dark art you make it seem then it would never be significant enough to make Netflix worry about it. reply AnthonyMouse 19 minutes agorootparentprev> It surprising to me on HN because I would assume most people on here are making good money. A lot of people are students, or just don't like paying money for a frustrating experience even if they can afford it. > $62.64/month. But then you haven't got access to the things which are currently on Netflix or Peacock or YoutubeTV or whatever it is this time, and feel anxious or resentful that you're paying a monthly fee for whichever one you're not currently watching, or else have to spend time canceling or re-signing up for them all the time. > Legality aside you need a seed box, membership to specific sites, and VPN. Which can add up to more than $30/month just to download pirated content. That's how the people who have the money do it, but they're not doing it for the money. > Not to mention the time investment in figuring out how to get the movies and tv shows to your family members on all of their devices. Why would that take a long time to figure out? > A budget NAS with 20 TB is going to cost $500. A small PC to transcode also $500. Used PCs are ~free and anyone can get 20TB of storage to put in it forBacking up 20 TB to the Backblaze is going to cost $13/mo. Why would someone pay money to back up to the internet what they downloaded from the internet to begin with? reply finaard 33 minutes agorootparentprev> Is piracy easier? Legality aside you need a seed box, membership to specific sites, and VPN. Which can add up to more than $30/month just to download pirated content. Not to mention the time investment in figuring out how to get the movies and tv shows to your family members on all of their devices. This part is trivial nowadays: sonarr/radarr and related handle discovery and library management, and jellyfin does the playback. Reasons I got back into piracy included trying to watch a series with my wife, and finding it gone. Seasons missing while streaming, UI becoming worse (for the last few years of Netflix I fully relied on an external paid service to discover new things on Netflix because of that). More and more content I cared about being removed, with lots of content I don't care about getting added - while price kept going up _and_ account sharing policies being put in that'd match my non-account sharing use case. I'm still paying for Disney+ - though based on the Netflix experience I'm adding everything I'm interested in to my local library. Amazon Prime got kicked out after they got too expensive - and even while I had prime I never actually used the video player as the UI is so horrible, but already back then pirated all the stuff I was paying for. reply fragmede 31 minutes agorootparentprevA seed box and a VPN is maybe $15 a month, and there are private torrent sites that don't charge monthly membership fees. If you've got a expansive family with lots of kids and want to fully replace Netflix with Jellyfin on a libel server it's a bunch more work, but torrenting shows onto your laptop to watch is fairly easy. At the end of the day, for someone that's motivated to not give companies money because they see them as greedy rather than a way to compensate the creators, it becomes a labor of spite. That $500 budget NAS, downloading content in native resolution so there's no transcode box needed, spread across all the families and cousins, is easily paid for in car repair favors or what else the family has to offer the uncle or aunt running the pirate video service. $500 vs $60/month? that's just 9 months. 3 if you figure there are three households where you're saving them $60/month. Personally, I rotate membership between one service and just limit my own tv time, but piracy really is too easy if you know what you're doing and are morally flexible. reply dijit 47 minutes agorootparentprevYou're missing Netflix, ironically. reply cvalka 43 minutes agorootparentprevPut.io reply 2Gkashmiri 41 minutes agorootparentprevoh no. you are trying to set up a full blown alternative. that's not how the majority of world does piracy. here is how it goes. you have a laptop/pc. you go to tbp, search for the top 100 for movies/tv shows and download the torrent, watch it and delete the file to save space. what you are describing is off-limits for majority of the world. i have rented seedboxes back in 2016 for 2 months to get those \"points\" to get to a private tracker. never before, never after. i have been using the above tbp+vlc method on a daily basis since 2005-06. only recently lookmovie and other streaming sites have made it a bit easier. oh, also, there is an outlier. popcorntime. it JUST WORKS. I used it on the 2nd day of its initial release, i saw the whole weeks drama with death threats and the subsequent forks that arose. i have been using that as well so i am confirming and NOT PRETENDING that piracy is easier that actually paying for stuff. reply mgh2 7 hours agoparentprevNetflix also became popular due to people hating ads. Ads or piracy, which funds criminal activity? Hint: Hear social media CEOs testify in congress. This seems like a PR piece to promote and justify Netflix's change in mind and business model. reply jerjerjer 11 hours agoparentprevStreaming also had no ads which is now gated behind even more pricey plans. reply cocoa19 11 hours agorootparentLow price and no ads were two of the main selling points to ditch cable (along with stream whenever you want). We reached the point were price is almost as high as cable was (with the standard 4-5 subscriptions), and we have ads. This is not a surprise to most though, the question has always been when would it happen. reply TeMPOraL 5 hours agorootparentNo ads used to be the main selling point of cable, decades ago, which goes to show that advertising is a cancer and will thoroughly corrupt and consume every communication medium over time. reply api 11 hours agorootparentprevWe are no longer under zero interest rates. Things have to actually be profitable. reply gchamonlive 10 hours agorootparentIsn't online streaming profitable? Does it absolutely need ads and granular channel subscriptions to be profitable? reply ta_1138 8 hours agorootparentIt's far less profitable than you'd think. Many of the competitors in the industry lose money. Licensing fees/production costs/royalties eat a bunch of the money, but the infrastructure to run a streaming service isn't cheap. I know at least one of the competitors that people would call the most successful is paying a 9 figure AWS bill, plus whatever the costs are for all the caching setup that makes most popular content live in ISPs. Add the typical army of developers building apps for the mobile devices, the billing team, people doing recommendation engines, fraud detection/security team, tagging all content, and translating every blurb in a bunch of languages, possibly pay for creating subtitles for all of those languages... it's not cheap. What is so frustrating about this is that so many of the costs would go down with consolidation. Every company has to handle the fact that age verification and privacy legislation in South Korea has to work in a just-so way, but every streaming service has to duplicate the logic. While the Netflix recommendation system probably can use some tweaks to take on all of D+'s content, I bet it'd be easier to tweak their system than to have every company have their own ML team handling recommendations. A lot of relatively low quality content wouldn't have to be made if we didn't need to make 'filler' to keep people spending sufficient time on a given streaming service while the next top quality release comes out. In a happy world, competition brings more better content at a cheaper price for consumers, but when I look at the state of streaming, what we have is many competitors that aren't breaking even, in exchange for a streaming experience that keeps getting worse. And there's little chance it's going to get better until at least half of the worst competitors give up and go back to licensing their content to whatever the big 3 end up being. And yes, we'd all be better off if we moved to the music model, where any subscription has 90% of the content, but do you really see, say, Apple, Netflix and Disney doing worldwide, full catalog cross-licensing deals? reply throwaway2037 7 hours agorootparent> I know at least one of the competitors that people would call the most successful is paying a 9 figure AWS bill (1) Netflix? (2) \"9 figure\". Just say 1B or 5B? There is a big difference. reply boring-alterego 10 hours agorootparentprevThey have to be continually profitable every quarter, and they have to show growth quarter over quarter or the value of the company will take a major hit, which will impact how much capital they can loan for future projects. reply relyks 8 hours agorootparentprevNetflix is the only streamer (out of the major ones) that is currently profitable reply blitzar 1 hour agorootparentprevWe are no longer under zero interest rates and I need fuel for my Jet(s) and my Yacht(s). reply vonjuice 2 hours agorootparentprevNo they don't? reply ihsw 8 hours agorootparentprevNo subscriber told streaming services that they needed to drop literal billions of dollars on content production. I do not want to subsidize their poor decision making. This is why streaming is so expensive. reply DoughnutHole 11 hours agorootparentprevEven an “ad-free” Netflix plan is infested with promotion of their in-house productions. reply worthless-trash 5 hours agorootparentYeah, this grinds my gears too. If i wanted to watch the other productions, i'd be watching the other production. I can see what is available on the 'trending now' list on the exact screen just before starting this video. reply masklinn 3 hours agoparentprev> One thing that we have learned is that piracy is not a pricing issue. It’s a service issue > 12 years old, and as perfect as the day it came out of GabeN’s mouth. I have not pirated a game in more than a decade, I bought a bunch on sale over Christmas though I likely won’t have the time (or hardware, HL:Alyx was cheap) to play them any time soon. Though it’s a bit of a minor chore I regularly purchase music I find via streaming (I like albums, offline use, and knowing it won’t go away). I’ve never felt the need to pirate a book. I don’t pirate tv/movies much, mostly because I don’t watch them much due to all the other entertainment available. I did pirate Taskmaster, until they started publishing the episodes on YT. reply norman784 1 hour agoparentprevDon't forget that when watching in your desktop/laptop is hard to get a good quality, like you have a 4k display, but every service want to serve you 1080p or lower quality when you already pay for the 4k content plan. reply tonylemesmer 26 minutes agoparentprevThe regional licensing and delisting of titles is frustrating too. All the separate walled gardens just making it less likely folks will pay. reply mike_hock 2 hours agoparentprevMore importantly, the pirated content isn't DRM-encumbered, you get to keep it and it can't be pulled whenever the streaming service feels like it. So the same situation as with DVDs all over again. reply PH95VuimJjqBqy 10 hours agoparentprevyep, as someone who stopped pirating when netflix started streaming, I'm back to pirating and I won't apologize for it. there are shows for which I cannot even purchase them, which I would be willing to do. You want to lock me into a subscription? jokes on you. reply chrischen 3 hours agoparentprevI can understand why Netflix et al want to block Apple from their data, but back in the beginning you could just say “Play the latest episode of House of Cards” on Apple TV and it just worked. At the end of day user experience is just degraded… and for what? reply Fnoord 10 hours agoparentprevSteam's Gabe Newell said it best: Piracy is an issue of service, not price. I went from full pirate (we didn't even pay for cable, it 'just worked' -- the beauty of analog) to nearly full streamer. The stuff I kept pirating wasn't available on stream. Then Disney started to take off their stuff from Netflix. HBO suddenly quit in The Netherlands, or I had to combine it with KPN or was it Ziggo (what if you can't have DSL or cable? And, what if you got better; fiber?). I'm still paying for like 6 services, but not forever. Because my children do not know or remember on which service they saw something. Heck, often they can't even remember the name. Or they know the name in our own language and the search doesn't work. And as cherry on top, Dutch public broadcasting organisation (NPO) updated their app which was a step back as it removed a lot of features, such as children profiles, and the navigation of the new UI is atrocious. Series paid for with public money suddenly got removed. I am done with this shit. I am. What I will do is download all my children's favorite series, and put them in Jellyfin. Then every month we pick a different service, and they can watch that, too. I will tell them it is beyond my control (a white lie). Also the quality of the streams is laughable. Only Apple bring something which deserves the mention 4k (and Apple are part of the problem). And a piracy setup is very easy. I will get fiber soon. I have a VPN with a container to run BitTorrent on. I have Usenet servers. I have a couple of lifetime ???znab accounts. All cheap. I wrote this post angrily in less than 10 min, I probably forgot to mention other reasons on top of this all. Yeah, the fuzzy feelings I got for not pirating. I admit, I will miss those. reply wkat4242 4 hours agoparentprevYup. It's a service problem, not a price problem. I pirate again too. The fragmentation is just crazy now. reply mhss 8 hours agoparentprev> Streaming became popular because it was easier than piracy and better than TV (watch anywhere, on demand, pickup where you left off etc). I think you overestimate how \"easy\" piracy is for the average user. Netflix revenue keeps growing (and subscribers), despite the \"crackdown\" on password sharing that many predicted would cause massive cancellations. reply mike_d 7 hours agorootparent> I think you overestimate how \"easy\" piracy is for the average user. You overestimate your view of piracy. The average person isn't curating libraries of lossless music collections and carefully re-encoding Anime dubs to match their sound system. People are Googling for the hundreds of sites that will stream a feed of a DirecTV box somewhere showing an NFL game, or show grandma how to connect to the Plex server their cousin runs. reply mhss 7 hours agorootparentMy comment was explicitly replying to an argument about how streaming services would \"suffer\" because of how piracy is much easier today. There's no signs of that. Just because Netflix mentions that risk in their SEC filings. The article admits as much, it's their responsibility and of course if there were no other alternatives it'd be better for Netflix, but is hardly something that has changed significantly to make a dent in their business. As I said, subscriptions and revenue keeps growing, there's no evidence of them \"suffering\" because of the alternative (viable to many) that piracy provides. reply selcuka 8 hours agorootparentprev> I think you overestimate how \"easy\" piracy is for the average user. Netflix revenue keeps growing [...] According to TFA piracy is also growing rapidly, so it's apparently easy enough. You may be thinking of usenet, torrenting, seedboxes etc. when it comes to piracy, but there are also (ad supported) public web sites where you can watch almost any content, or IPTV providers where you can pay a yearly fee and watch most things streaming providers offer once set up. reply inversetelecine 6 hours agorootparentThe number of 'normies' who talk about torrenting casually is a decent amount. Most at least know what it is. If not, they \"have a guy\" they get media from. reply mhss 7 hours agorootparentprevPeople that cannot afford Netflix will go to great lengths to get the content they want. They're also not mutually exclusive. Piracy growing doesn't mean Netflix isn't. People pay for Netflix AND download pirated movies all the time. reply Loughla 8 hours agorootparentprevMy parents found free sports online when their team was playing a team that wasn't shown locally. They're completely inept at technology. It's very easy these days. reply mhss 7 hours agorootparentIt's the lack of content availability that pushed them over the edge. Sports are a special case, notoriously hard that is super stupid and pushing people to piracy. I tried paying many times to watch a game my kid wanted and either the dumb apps or websites would not work on my TV. Make it easy to pay for the content and most people will take the easy route rather than search online for ad-ridded or dubious websites (unless they can't really afford it and then is not a real loss for the company anyway). reply blibble 8 hours agorootparentprevgoogling \"watch X online free\" is hard? reply shermantanktop 7 hours agorootparentIt's clicking on the links in the search results that requires a difficult leap of faith. reply xethos 7 hours agorootparentApple has spent years teaching people that if it runs without modification, it's safe. End of story. It came from the App Store, or it's allowed by Safari, so it's good to go. Couple that with malware getting more subtle [0], and Windows Defender getting better, and most people will assume they're virus free and have been for years (even if they're not). [0] think cryptominers in the background and data exfiltration vs so many browser toolbars you get 3\" of any given webpage at a time, and pop-ups on pop-ups reply NoPedantsThanks 8 hours agorootparentprevThey talk about piracy \"services,\" which is not your normal torrent user presumably. I guess it's Popcorn Time and the like, which makes it somewhat easier for the general populace. reply mhss 7 hours agorootparentYeah, still, it's easier, but not as easy nor convenient or widely available (e.g there's no Popcorn app in that TV you just bought. Defaults matter a lot. reply jug 2 hours agoparentprevAbsolutely this. I pirate Apple TV+ stuff right now. I subscribe to like three services. This is my maximum. I could maybe stretch to four but then the market needs to meet me there and do two things: 1. Make an effort in consolidating media rights and services to these four and 2. Work to lower the price to roughly $10 per service for at least high quality 1080p with no ads in high quality apps across the board. Bonus: Also work to standardize a media protocol so that you can see what you have access to in a common media library regardless what you use, or even in third party apps, as well as stream with whatever as long as you're a subscriber to the respective services. Do all these things and I actually think they have a good chance against piracy. It's all about what they're willing to do. They can absolutely make some good money on movies and TV shows in a piracy world. I'm convinced of this. Piracy is not the problem; piracy is just the other weight of the scale showing just how anti-consumerist they can become. reply aequitas 1 hour agoparentprev> when you can just have one service for free Most of the time piracy isn't even free. You got a server running for downloads which is an investment and uses power (24-7 sometimes), hard disks for storage aren't free. Indexers and newsgroups are paid (good ones at least). Comparing that to a monthly subscription of what used to be €10,- to Netflix and which had it all, makes it nonsense to pirate. reply underyx 7 hours agoparentprevWhile I directionally agree, I don't know how to reconcile this with antitrust. I wouldn't want Netflix to be the one company with all the content, with no competition. I guess what would be better is if all content was available on all the streaming providers, and then they'd compete on platform quality and price. reply suprjami 6 hours agorootparentI do. There needs to be one international company who is co-owned by all these media companies. They need to \"nationalise\" streaming amongst themselves. That way users only have to pay one streaming service, the content is available in every country, the rightsholders are paid fairly by the content consumed instead of these never-ending monthly fees for one show and ignoring the rest of the library. If they don't do this, piracy again becomes the best option and media companies will only ever get stupid disengaged customers and that's a bad business model because those people eventually leave. The time of predatory streaming service bullshit is over. Provide a service which puts customers first or they'll all go back to piracy. They already are. reply kmeisthax 2 hours agorootparentNationalization is too extreme. What you want is compulsory licensing where the government sets some price by which any streaming service can pay to license the content. How that price gets selected is... complicated and easy to fuck up[0], but I can imagine some metrics that could be used to target prices. e.g. the compulsory license cost must not exceed some multiple of the average negotiated rate. [0] Notably, radio has a compulsory licensing scheme, but the government turned up the license cost on Internet radio so high that nobody can make a profit unless they sell their soul to the RIAA ala Spotify. reply mdavidn 5 hours agorootparentprevThat is exactly what Hulu tried to be initially. reply bcardarella 7 hours agorootparentprevThere is considerable competition in this space and prices are not coming down but going up. Services and content is not improving but getting worse. reply kalleboo 5 hours agorootparentprevI mean it works for music right? reply giancarlostoro 7 hours agoparentprevHonestly if Apple integrated Netflix and all those other services into a unified UI for Apple TV that transparently lets you go between the apps without realizing it (they do own the OS...) it would be exactly what I want to see happen to these streaming services, that or they somehow share their content to the official Apple TV app itself, and let Apple TV query across all those other streaming apps, it would be great. Sometimes I'll look up something with my Apple TV and it will try to have me go to a paid version, for me to realized I watched it on a completely different app, or it shows me the app that only has 1 single season, when I have every season in a different service. I think there's room for improvement on the smart TV OS' to better integrate these services into one pluggable app. reply mdavidn 5 hours agorootparentThis is exactly what Apple attempted to do, but Netflix never provided API access or data, so Netflix shows never appeared in the unified interface. reply izacus 1 hour agorootparentprevNetflix actively fights against these integrations - both on Apple TV and on Google/Android TV they refuse to integrate with the APIs that would give you this exact experience. They also tend to refuse allowing their customers to watch on platforms they don't like - e.g. they actively blocked casting to Google smart displays and blocked Netflix app from working on Macs and Vision Pro so you can't download things. reply data-ottawa 6 hours agorootparentprevThat would be great, with the caveat that if a service cannot provide content for your account, it will not be shown in the search results. Prime is the worst offender of this, but having Crave and Bell TV app installed on an AppleTV means any search returns a result, but you can't watch any of them without buying another subscription to a \"channel\" they offer. reply thepasswordis 7 hours agorootparentprevThis is what amazon prime does. reply jaredhallen 4 hours agorootparentTrue. But it only solves half the problem. Or to put it another way, their particular implementation just leads to a different frustration. So Prime aggregates content, which is great. So I can browse a great meta-catalog of content from various services. The problem is I'm not subscribed to all these services. So I scroll through the catalog and find something interesting, but... you need to sign up for a monthly plan to watch whatever show on whatever service. It might be a little more palatable if you could pay (a reasonable amount) per show, or series or movie, or whatever. But as is, it's just an exercise in frustration. I'm not committing to a subscription to watch one show that happened to seem somewhat interesting. reply tzs 5 hours agoparentprev> why pay for 8 different services and have to waste your time figuring out whats on what Both my streaming boxes (Amazon Fire Stick and Xfinity Flex) handle that for me. I press the mic button on the remote, say what I'm looking for, and they tell me my options. Typically that means they tell me which paid services it is on (Netflix, Max, Peacock, etc) and offer to launch the app for that service, which free with ads services it is on (such as Pluto or Freevee), and show my options to buy or rent it. reply smsm42 5 hours agoparentprevWorse yet, most of those services have extremely crappy software (e.g. to watch a series, you have to go through several levels of menu for each epusode and some won't even remember which episodes you watched), put in insane amount of ads even for paid subscription, some have no subtitles, etc. While a good pirate site with proper software - or so I heard from some friends - provide much better service for free. reply DaleNeumann 5 hours agoparentprevMaybe it's me but consider the content on these streaming platforms which seem to me as a mix of syndicated shows and romantic action films. Where exactly people find value in tv tropes and expensive camera work is something only the producers in Hollywood can answer, pirating is a means to an end and by that I mean, most of it is so shit that it's not worth the price on the label to do the reasonable thing and pay for it. reply fxtentacle 4 hours agoparentprevIt's a content problem. I cancelled my subscriptions when I noticed that I'm not using them anymore. Years ago, I remember me and my friends were eagerly awaiting every new episode of Agents of Shield or The Expanse. But when I was browsing the IMDB top movies and top TV series recently, I just didn't find anything I wanted to watch (and that hadn't been in cinema half a year ago, thus old). The result is that recently, me and my friends all watched a lot less movies and instead played more video games. TLDR: Netflix's low-effort content is losing against video games. reply emporas 10 hours agoparentprevKick.com is banned in Greece as of June. It has some betting streams, not sure if someone bets on the platform, but it got banned anyway for illegal gambling. I don't care personally, just to put it out there. P2P download of files did fall a lot in the last decade, i think a second wave is coming. reply jasonwatkinspdx 7 hours agorootparentKick isn't really a streaming site, it's a front started by the Stake bitcoin casino after Twitch changed policy to ban streaming bitcoin slots. Stake had been handing out huge contracts to major streamers to show themselves having fun gambling on Stake, particularly xQc and TrainwreksTV. These streamers would act as if they were betting with their own money, including getting into six figure bets, without being clear they were on a contract that basically made it all upside to them even if they lost millions. The audience for these streamers slants young, and that didn't sit well with Twitch advertisers, hence the change in policy. Train et all didn't want their golden goose to end, so they worked with Stake to create Kick, with Train as one of the cofounders, and xQc and others pulling their audience over. One of Kick's selling points to streamers has been to be a wild west as far as copyright and content goes. They quickly took in all the streamers too toxic for other platforms, including people who'd done things like film themselves committing sexual assaults on Twitch. They've encouraged people to restream copyright content like marvel movies, and even porn. I don't know what the laws are like in Greece, but considering this free for all approach it's not surprising they'd run afowl of regulations somehow. Frankly it's amazing Kick hasn't gotten nailed by some sort of major anti piracy action, but I suppose being based out of the Seychelles complicates that for copyright holders. In any case, the point is Kick is an aberration. It's way too toxic for mainstream advertisers, so the only thing sustaining it financially is Stake paying people to stream gambling on Stake. reply internet101010 6 hours agorootparentThis is correct. Kick is just a front for bringing lifetime gamblers into Stake. That's only reason they were willing to pay xQc $100M for a two-year non-exclusive contract. reply emporas 7 hours agorootparentprevI didn't know any of that stuff, thanks for sharing. Casinos and betting literally disgust me, but given that in a handful of years, every person on the planet will have his own personal stock and bond market, i wonder how one can differentiate betting from investing. A saying i usually repeat to people, is that in the future, i.e. in a handful of years, everyone will be a billionaire for fifteen minutes. reply Dylan16807 6 hours agorootparent> every person on the planet will have his own personal stock and bond market I have no idea what you're talking about. > i wonder how one can differentiate betting from investing The biggest clue is how long you're going to hold ownership. If it's years, then it's almost certainly investing. > A saying i usually repeat to people, is that in the future, i.e. in a handful of years, everyone will be a billionaire for fifteen minutes. Wha? Wouldn't most people cash out? Then most people are billionaires permanently? How does this work? reply emporas 6 hours agorootparentWell, think about it that way: What's the purpose of copyright? It gives ownership of information, to the people who produced it, some years for a song or a movie. The presupposition is that said information, will be valuable in a year or 10 years. Along the same lines what's the purpose of investing? It captures ownership of a company, for some time, or indefinitely. The presupposition is that said company, will be valuable in a year or 10 years. Well as technology advances, everything becomes easier to",
    "originSummary": [
      "Netflix acknowledges the challenge of competing against the free and expanding content offered by piracy in a recent SEC filing.",
      "With the proliferation of isolated and costly streaming platforms, piracy has experienced a resurgence.",
      "Netflix recognizes piracy as a formidable competitor due to its attraction of free content for consumers and its potential to harm the company's business. They are actively combating piracy through their membership in anti-piracy organizations and their internal anti-piracy department."
    ],
    "commentSummary": [
      "Online forum users are discussing the increasing popularity of pirated content as a result of dissatisfaction with streaming services.",
      "Users argue that pirated content offers better quality and convenience, including features like 4K HDR video and offline playback.",
      "The conversation also covers topics like DRM issues, the future of streaming services, the ethics of piracy, and the profitability of streaming platforms."
    ],
    "points": 281,
    "commentCount": 543,
    "retryCount": 0,
    "time": 1707083062
  },
  {
    "id": 39250434,
    "title": "Plastic Bag Bans Successfully Reduce Consumption, Urgent Nationwide Action Needed",
    "originLink": "https://www.zmescience.com/science/news-science/plastic-bans-work-billions-of-plastic-bags-were-avoided-in-the-us-alone/",
    "originBody": "Science News Environment Health Space Future Features Videos Reviews About Us No Result View All Result Plastic bans work. Billions of plastic bags were avoided in the US alone No Result View All Result Home → Science → News Plastic bans work. Billions of plastic bags were avoided in the US alone Even though the US has no national ban on plastic bags, smaller-level bans have made a sizeable difference. byMihai Andrei January 31, 2024 in Environment, News, Pollution Reading Time: 4 mins read Edited and reviewed by Zoe Gordon “The bottom line is that plastic bag bans work,” said Faye Park, president of the U.S. PIRG Education Fund, in a statement. “People realize quickly it’s easy to live without plastic bags and get used to bringing a bag from home or skipping a bag when they can.” Image via Unsplash. Plastic ain’t all that fantastic Plastic bags are a victim of their own success. When they were first patented in Europe in 1965, society was shocked to see how cheap and durable they could be. Within a decade or two they became mainstream on the continent and in North America, and it wasn’t long before they started being widely used on the entire planet. But plastics were just a little too durable. They didn’t go away. They started accumulating in landfills and in the oceans. The environmental impact of plastic bags gained attention with the discovery of the Great Pacific Garbage Patch in 1997. Plastic bags (and plastic in general) had left its mark on the planet in an unprecedented form of pollution. Fast forward a couple more decades, and countries started fighting their urge to use cheap plastics and implement bans or other measures against plastic bags — and finally, there’s some good news. San Francisco pioneered the movement in the U.S. by passing the nation’s first plastic bag ban in 2007. Several other U.S. cities and states implemented plastic bag bans or restrictions. By 2023, ten states had statewide bans, with similar laws proposed in others. To get a state of how much this of a difference this made, five studied bans resulted in an average elimination of almost 300 plastic bags per person per year. Overall, in the US alone, billions of plastic bags were avoided with anti-plastic bag measures. The case against plastic The case against plastic bags is straightforward. Plastic pollution kills at least 100,000 marine mammals and 1 million seabirds every year and entanglement in plastic and other types of litter kills roughly 1,000 turtles per year. Plastic bags aren’t responsible for all of that, but they make up an important part of the problem. The results, which were published in a report, also highlight that imperfect measures leave loopholes or encourage buyers to opt for other single use bags. Well-designed single-use plastic bag bans across the country have successfully reduced single-use plastic bag consumption, cut down on plastic bag litter and driven consumers to make more sustainable bag choices. Policymakers should pursue these policies at the state and local levels,” the report says. The idea isn’t to shift from one type of single-use bag to another type of single-use bag. Paper bags are easier to recycle than plastic, but they take 3-4 times more energy to produce and usually generate more solid waste. Ultimately, the report concludes that regulation is the best current way to address plastic waste and plastic pollution. “Grocery stores, restaurants and retail shops should not be permitted to distribute plastic film bags of any thickness at checkout. Stores should be required to charge a fee of at least 10 cents for single-use paper bags. A 10-cent paper bag fee will limit the expected increase in paper bag use after a bag ban is imposed and may even reduce paper bag consumption altogether.” “Local and state governments should conduct regular enforcement to ensure compliance.” You can read the report in its entirety here. Facebook X LinkedIn Pinterest Email Tags: plastic bagplastic bansingle use plastic ADVERTISEMENT About Advertise Editorial Policy Privacy Policy and Terms of Use How we review products Contact © 2007-2023 ZME Science - Not exactly rocket science. All Rights Reserved. No Result View All Result Science News Environment Health Space Future Features Videos Reviews About Us © 2007-2023 ZME Science - Not exactly rocket science. All Rights Reserved.",
    "commentLink": "https://news.ycombinator.com/item?id=39250434",
    "commentBody": "Plastic bag bans work (zmescience.com)249 points by Brajeshwar 19 hours agohidepastfavorite533 comments tgsovlerkhgsel 16 hours agoIn addition to being counterproductive (as explained by others), there's another issue with these token environmental laws that add highly visible inconveniences with no actual benefit (e.g. bag or straw bans): they annoy people, leading to resentment and rejection of environmentalism as a whole, harming the chances of policies that would actually have a meaningful positive effect being adopted. [1] . My impression is that these laws are often popular among one group because they annoy another, perceived as being in the wrong - which then leads to the second group pushing laws that will annoy the first, even if they don't make sense. [1] backlash effect - https://en.wikipedia.org/wiki/Backlash_(sociology) reply LouisSayers 11 hours agoparentIn Australia, New Zealand in recent years we've moved to reduce plastic bags from supermarkets etc. I haven't actually heard anyone complain about it or get annoyed at some group of people - this whole concept of being annoyed at some group of e.g. environmentalists actually seems completely strange and alien. Yes, it takes a bit of getting used to, but now it's simply a habit to take a small folded up reusable bag most places when we head out (just incase we end up shopping), or to think ahead and grab some bags when we know we're going out for a shop. Really not a big deal. reply prawn 10 hours agorootparentThose thin plastic bags at checkout were phased out here in South Australia in 2009. I remember some resentment at first (\"Now I have to buy bin bags!\") but a few years later I don't think anyone cares. Anyone remotely prepared has reusable bags in their home/car, or collapsible bags in a handbag, and deliveries arrive in paper bags (which we then use as a bin under the sink). There are other single-use plastic phase outs in progress and the near-future also. Previously, plastic bags discarded roadside were a common sight, stuck in fences and trees. reply alatkins 9 hours agorootparentprevThe number of people that I see taking their bags into the supermarket would beit's simply a habit to take a small folded up reusable bag most places when we head out (just incase we end up shopping), or to think ahead and grab some bags when we know we're going out for a shop. For us ADHD people your simple habit is an insurmountable problem. Even if I have them in the car I'll be halfway done with shopping before I remember the bag. Fortunately they just let me pay for a new reusable bag each time. I resent it because it feels like token bullshit. Plastic bags were quite thin so there is quite literally 100x+ more plastic in the packaging of the products I'm buying than the bag it goes in. Environmentalists pushed the most inconvenient policy that has the least benefit instead of something that might actually move the needle enough to gain my support. reply mrexroad 4 hours agorootparentAmerican here, so I’m automobile-dependent, but I most often just put the individual items directly in my trunk, without bags. I do keep a collapsible bin in my trunk that I use when I get home to carry in a bunch of the smaller items. My wife usually tries to give me a reusable bag to use when i head out until I give her a half confused look. then she realizes there’s near-zero chance that I’ll remember to take the bag in the store. reply LouisSayers 8 hours agorootparentprev> For us ADHD people How often do you forget your wallet or phone? reply danielheath 8 hours agorootparentNot the same poster but... surprisingly regularly (glasses and keys, too). Before I got medicated, I found myself missing something essential multiple times a week. For me, at least, that stopped happening once I got the right dose. reply mrexroad 4 hours agorootparentStrict routines and places for EDC items can help too. For other things, like tape measures, buy a half dozen. Put them the first place you checked the last time you couldn't find it. reply subsubzero 13 hours agoparentprevAlso these environmental laws come from a place of non-rigorous fanciful thinking, ie. the paper straw push was pushed by some hollywood celebrity and never took into account that paper straws are loaded with forever chemicals, and are useless with shakes or drinks that take a long time to finish. The gas stove bans are beyond absurd, the claims are that they cause asthma and lung problems but I bought a few air quality meters and ran my gas stove at full blast with no discernible degradation registered on these meters. In addition going full electric leads to a non-redundancy in heating and cooling which pushes a greater strain on the electric grid and if said grid fails(see california) people are completly without heat or cooling. Lastly they push enormous costs onto individuals for miniscule gains. reply twothamendment 10 hours agorootparentIf you ran a gas stove and couldn't measure anything, I really question your meters or method - it maybe your house is build like a cheese grater. My meter was off the charts when turning on the gas range or oven. I didn't get a meter until after a trip to the in-laws when my wife noticed that cooking at their house didn't make her sick, but as soon as we were back home it was an issue. We swapped the gas for induction and love it - especially the part where she isn't getting sick from using it. Our house is built very tight. The air quality meter also got me to put in an air exchanger. It was great to have numbers before and after these changes. We had gas because I've always liked cooking on it and it works when the power is out. It was hard to give that up. Now we use a camp stove when the power is out. I think a ban is too strong. It seems like building codes requiring a range hood that actually works could be enough. reply lazide 9 hours agorootparentIf you have a vent that works or open a nearby window they aren’t an issue. reply pwnna 12 hours agorootparentprevComparing gas vs electric seems incorrect. Should be comparing with induction instead. It is way more energy efficient (altho not necessarily more cost efficient). It produces no combustion byproduct like gas (which your air quality meter may or may not be able to detect), and it is way faster and gives you better control. It also is safer without flames or leaks, and less likely to burn you. It is the way to cook in 2024 imo. reply Plasmoid 12 hours agorootparentprevThe idea behind the gas stove ban, is that it's supposed to be a gas appliance ban. Most of a house's gas consumption is heating and hot water, but people will keep gas because of the stove. Gas itself has a lot of problems. Ignoring the safety concerns (oof!), the micro-leaks from gas pipelines is a major source of greenhouse gases. But we can't clean up all the pipes if everyone is still using gas stoves. reply kwhitefoot 11 hours agorootparent> the safety concerns I know a lot of people worry about gas, my grandmother wouldn't have it in the house. But is there any solid evidence that it is as dangerous as people think it is? I have been cooking with gas for all my adult life and I'm 68 with no problems whatsoever. And until I left the UK in 1986 we used gas fired central heating and hot water, again with no problems. I don't know anyone personally who has ever had a safety problem with a gas installation. reply lmm 10 hours agorootparentIt only takes one Ronan Point to outweigh a great many people who never had a safety problem with a gas installation. The UK imposed a strict inspection and regulation regime (to the point that a common \"life pro tip\" is: if you're living in rented accommodation with gas and haven't received a gas safety certificate, never mention it to your landlord until you move out, that way you're always entitled to 100% of your deposit back and can avoid having to argue over wear and tear etc.) which has its own costs. reply seabass-labrax 10 hours agorootparent> ...a common \"life pro tip\" is: if you're living in rented accommodation with gas and haven't received a gas safety certificate, never mention it to your landlord until you move out, that way you're always entitled to 100% of your deposit back... It's only a 'life pro tip' if you get to keep it. Keep your life, that is. reply peteradio 11 hours agorootparentprevSo its subversive. Nice! reply cyberax 9 hours agorootparentprev> The gas stove bans are beyond absurd, the claims are that they cause asthma and lung problems but I bought a few air quality meters and ran my gas stove at full blast with no discernible degradation registered on these meters. Have you checked for formaldehyde? I did, and a stove results in a real increase in its content. reply mr_toad 6 hours agorootparentprev> useless with shakes or drinks that take a long time to finish I wonder how the ancients drank liquids, before the invention of the straw. reply uulu 9 hours agorootparentprev> ... going full electric leads to a non-redundancy in heating and cooling which pushes a greater strain on the electric grid ... What else than electricity if used for cooling? Genuine question. reply gruturo 9 hours agorootparentGas, or any heat source really. Yeah I'm serious - for cooling. A certain Albert Einstein invented it with one of his students, a certain Leo Szilard, and patented it: https://en.wikipedia.org/wiki/Einstein_refrigerator reply uulu 8 hours agorootparentYes, you are right, this apparatus is invented. ... and banning it will \"push a greater strain on the electric grid\" (to use GP words) with the current level of adoption. :-) Really? reply gruturo 2 hours agorootparentNo need to ban this kind of fridge - it's already uneconomical compared to electric ones - people won't suddenly go buy one unless they need it to run where electricity isn't available. It's the type installed in most RVs for example, makes sense there. reply thexumaker 12 hours agorootparentprevYou mean Texas lol. 30 years in SF and there’s been a couple blackouts during the summer for a n hour or 2. reply speakeron 13 hours agorootparentprevI think the problem with gas stoves is the nitrogen dioxide they produce. Do you have a meter for that? reply prepend 12 hours agorootparentYes, quite clearly it’s easy to measure nitrogen dioxide as well. A well ventilated stove has no problem with the nitrogen dioxide produced. So it’s an example of illogic to push for banning gas stoves rather than incentivizing proper ventilation. reply Nifty3929 15 hours agoparentprev\"they annoy people, leading to resentment and rejection of environmentalism as a whole\" I hadn't thought about it that way before, but I think you're exactly right. I feel this in myself. I am pro-environment, or at least I want to be. But when I think about the actual effects of all these laws and regulations it just makes me mad and want to ignore or fight ALL environmental laws. I have to use a paper straw now? It just seems to go on and on. reply latentcall 14 hours agorootparentDo people really need straws so much they get annoyed if they are made of a different material? You can just not use a straw. Or if it’s life or death you have one, purchase a collapsible metal straw and bring it with you. reply Geisterde 9 hours agorootparentYes, it is annoying, and its not about the straw. Its 100% about vapid environmental policies that do nothing for the environment. -The soil quality is going to hell, is it the fault of nitrogen fertilizers? It probably doesnt help, but far worse is the over farming of corn, which is the direct result of farming subsidies that shouldnt exist. -The chemical train crash in east Palestine Ohio? Operating per EPA regs; a company that had actual liability for harm would never transport chemicals without insurance, and the insurance company would base the rate on everything you would want them too, like installed safety features, rail upkeep, and training; instead, they are off scot free because they are protected by the people who are supposed to regulate them, as is only natural. -We are decades behind in energy infastructure because of nuclear alarmism. -Flint was an inside job, the government poisoned the people there and said it was safe. -I almost forgot paper straws, which are also terrible for the environment and even worse for humans. -We always ignore the biggest polluter, the pentagon. I could go on all day. If the EPA set out a few basic guidelines like \"hey, make sure people wear respirators!\", \"dont dump chemicals into the river!\", no one would complain. Its just, that will never be the federal government of the united states. reply ryandrake 9 hours agorootparentprevIt doesn't have to be a life-or-death issue in order to moderately annoy someone. In fact, straws are not a life-or-death issue, so it annoys people when they are treated as a life-or-death issue. And nobody has ever been annoyed into agreeing with a political view. Politicians should understand these kinds of regulations just create new opponents and entrench people already against you. reply thrownv7032g 12 hours agorootparentprevThe paper straws are awful products. They taste like paper and disintegrate quickly even though they are still covered in plastic. Wood utensils in comparison were an adjustment but they work fine. I have been avoiding lids and straws for two decades now, because they are wasteful, but when I have need of a straw and lid it's nice if they work. reply chasebank 13 hours agorootparentprevDo you really need an extra bedroom? Do you really need a car? Do you really need that extra pair of socks? Do you really need that coca-cola? Of course not. People obviously want it. Quite the slippery slope you're walking. reply latentcall 13 hours agorootparentAre you comparing a car with a straw? Really? Trust me you will be perfectly fine without a straw! Try it sometime. reply chasebank 12 hours agorootparentIf everything has an inherent carbon footprint, ie. harmful to the environment, then yes, we can compare a straw to a car. Have you ever gone on a vacation? I'd wager that flight you took is 10000x worse for the environment than whatever reduction in your lifetime straws. Trust me you will be perfectly fine without going on vacation! reply latentcall 12 hours agorootparentI really underestimated how passionate people are about straws. Do you have a blog per chance where you compare different straws and their manufacturers? Would you rate them on thickness, quality of plastic, how quickly liquid travels from beverage to mouth? Do you prefer straws wrapped in plastic or not? Can you describe the different subtle mouth feels? reply prepend 12 hours agorootparentI’m not passionate about straws. I’m just passionate about stupidity and critical thinking. It seems odd to me that someone would question whether straws are essential, but have no problem with vacations. I also am offended when people expressing any interest are deflected into “why don’t care about this so much?” It’s not important how passionate GP is about straws. Address the content of their message, not whether they are super interested in a topic or not. reply chasebank 12 hours agorootparentprevI'm not sure where I wrote I like straws. It's more of a \"I don't like the logic regarding straw bans\". reply the_doctah 11 hours agorootparentprev>Can you describe the different subtle mouth feels? Yes, paper feels way worse to drink out of, objectively. reply the_doctah 11 hours agorootparentprevDo celebrities really need private jets? No, the regular folk need to use shitty straws. reply triceratops 3 hours agorootparentWe should ban private jets too. reply PlunderBunny 10 hours agorootparentprevYou can buy a reusable metal straw if you want to. reply AngryData 7 hours agorootparentI liked the idea of metal straws but ive about busted out a tooth or two with them more than once since I got them and never use them now. reply Ekaros 12 hours agorootparentprevIsn't that private bedroom quite a big luxury, and many groups perfectly manage to survive living multiple persons per room... So why not force everyone to give up their private bedrooms? reply gregable 11 hours agorootparentprevIt slips both ways though. I suspect banning incandescent bulbs is probably a good change, even though there were people who obviously wanted them. reply grecy 13 hours agorootparentprevWe are at a point in society where we can't simply keep doing what those before us did. We are killing the planet, and every plant and animal on it, including ourselves. Yes, life is about to get less convenient and we have to give up some luxuries. It doesn't matter if you don't like it, it a fact. reply chasebank 13 hours agorootparent\"We are at a point in society where we can't simply keep doing what those before us did.\" Sure we can. And we will. The choices of most of the world's population surely disagrees with this. \"Yes, life is about to get less convenient and we have to give up some luxuries.\" I don't think that's the case, but we will see. For the record, ideologically, I'm on your side. Realistically, I just don't see a world where the majority of people change. The stop using the plastic straw argument applies to people who don't like straws. Tell the same anti-straw folks to stop traveling, stop having pets, no more children. You see where this goes - It just doesn't work. reply grecy 12 hours agorootparentThere are laws against plastic straws and plastic bags. There are laws against chemicals that are known to cause environmental harm. There are laws against vehicles that pollute too much. Soon there will be laws against cars vehicles that pollute at all. All we have to do is make laws, and more and more of them are coming. Again, it doesn't matter what an individual wants. reply JackSlateur 12 hours agorootparentYour trust in laws is fascinating For humans, laws and rules are meant to be overstepped : this is how our species got so far : by considering rules, laws and traditions, and say \"well, whatever, I will try it my way and see how it goes\" reply grecy 11 hours agorootparentAnd how is that working out? In recent months dozens of shops got multi-million dollar fines from the EPA for \"deleting \" diesel emission equipment. Now it's unheard of in the US. It works. reply the_doctah 11 hours agorootparentprevGeneralizations and platitudes, you can't even argue with this pie in the sky nonsense. reply Lanolderen 12 hours agorootparentprevWe can though. There's a lot of fearmongering but the reality is climate change won't kill us for longer than we'll live. There's also a lot of gradient in the effect of environmental laws. As an example my family still has tons of plastic bags that get reused over and over again until they become unusable or land as a trash bag. I don't know the math of it all but considering we buy plastic trash bags anyway we might as well get them from shopping instead of buying them separately. I have no idea what the global impact has been on plastic bag production but it could be interesting to see, assuming such a statistic exists. reply willsmith72 9 hours agorootparent> There's a lot of fearmongering but the reality is climate change won't kill us for longer than we'll live. I really don't like this take when I see it. Do you care about just the next 50 years, or the next hundreds+? Do you have children/grandchildren? Do you care about an increase in the quantity and impact of natural disasters? The resulting increase in death, suffering, insurance premiums? What about more unpleasantly hot days? Less snow on the ski slopes? All of that stuff is important to me, even though it almost certainly won't kill me personally. It's just such a no-brainer that the environment and climate is something to cherish and protect. Even if you're generally pro-environment, referring to environment-protection measures as fearmongering only makes it harder to enact them and increase their scale. reply grecy 12 hours agorootparentprev> the reality is climate change won't kill us for longer than we'll live. That depends heavily on where you live. 3.6 billion people live in areas that are at a very high risk from climate change. https://www.who.int/news-room/fact-sheets/detail/climate-cha... > There's a lot of fearmongering If you have better credentials than the WHO and leading climatologists around the world then I'd love to hear about them. reply prepend 12 hours agorootparentprevDo people really need the things they drink with straws? There’s two ways I generally think of this: 1) who knows what is actually essential. I can’t judge what others truly need and don’t know of an objective board who can. Is art necessary? Is coke necessary? Are luxury vehicles necessary? 2) people choose what’s needed by buying them and prioritizing their money toward it. So by existing and being purchased that’s proof enough of necessity. I’m sure as hell not carrying and keeping clean a collapsible metal straw on the off chance that I’ll want to drink a drink one day. It seems like the mental stress of that would outweigh the negative impact of a thousand plastic straws (not to mention the energy spent to create the collapsible metal straw in the first place). It’s important to keep in mind all the tradeoffs and not to create negative overall effects from the aim of improving only environmental effects. reply wkat4242 10 hours agorootparentprevIf you have sensitive teeth a straw is really needed with some drinks. And putting a collapsible one back in your pocket after it's sticky is a bit disingenious. I'd prefer focusing on better recycling than replacing by worse alternatives (eg PFAS coated paper) reply opan 9 hours agorootparentI would suggest either rinsing it in a bathroom somewhere, and/or carrying some sort of pouch or container you can stick it in so you don't dirty your pocket. Then you could clean it at home. In a pinch you could probably pour water from your water bottle on/through it outside if you carry a water bottle already. Could also wrap it in a handkerchief if you carry anything like that. reply wkat4242 7 hours agorootparentI don't normally carry any of that stuff to be honest. I don't usually even wear a coat. Just my phone, keys and sometimes a wallet. I live in Spain so 80% of the year I don't use a coat. And water I would just buy when I need it :) When I go hiking I do bring those things but then I don't come across places that sell iced drinks :) reply msie 13 hours agorootparentprevI am forever haunted by the death of a woman by a metal straw where it impaled her in the head when she accidentally fell in her home. reply PlunderBunny 10 hours agorootparentThe odds of something like that happening are so astronomically low that you’re better off spending your energy looking up for falling pianos. reply scoofy 13 hours agorootparentprevAs soon as I saw people were using metal straws as a replacement for plastic straws, I thought... that is a terrible, dumb idea. Yet people still buy them. It's pretty nuts. There are now perfectly functional composable straws, but people still need to compost them. reply opan 9 hours agorootparentprevThere are also silicone and glass straws. Plus a metal straw with a bend in it may be safer than a straight one. I was expecting someone else to have mentioned silicone straws already, I'd heard of them before in other straw discussions. Had to do a search and make sure they really existed. reply the_doctah 11 hours agorootparentprevImpaled through the eye, my god. reply billfor 9 hours agorootparentprevOr the guy that died of a rare bacterial infection that he got from using his dirty metal straw he never cleaned /s reply __salt 13 hours agorootparentprev> You can just not use a straw. This thought process is the problem. Solutions must benefit all parties. Telling one party to go without and deal with it creates resentment. reply p1mrx 13 hours agorootparentPerhaps it would be more useful to unban plastic straws, and then ban unsolicited straws. It's rather tedious to take a straw out of a drink, dry it off, and find somewhere to put it. reply Wowfunhappy 9 hours agorootparentThis is, in fact, how I think we should have handled plastic bags. The plastic bags themselves are fine. But the customer should have to actually request one. I can't tell you how many times I bought one small item I intended to carry out of the store with me, and the cashier just automatically put it in a plastic bag. \"Oh, I don't need a bag,\" I'd say. The cashier would then proceed to take the item out of the bag, and throw the bag away. (No animosity intended towards the cashiers, who have difficult jobs. But we should have a better process.) reply latentcall 12 hours agorootparentprevI can agree with this. Straws upon request makes more sense. reply johnnyanmac 13 hours agorootparentprev>Solutions must benefit all parties. but solutions never benefit all parties. Laws tend to tell some group what not to do, so most laws will inconvinience someone. You just need to figure out who the law and targeting (and REALLY targeting, not just what PR says) and then follow the money from there. reply __salt 11 hours agorootparentLess energy would be used by creating straws that are both convenient and good for the environment, like agave fiber straws. Solutions can benefit all parties in any context (and the best ones do), but environmental protection is unique because the solution must benefit all parties. This is because, while some will sacrifice for the sake of the world, some will not. Convenience compels the lazy to change. No amount of coercion, or threat of force will change that. reply latentcall 12 hours agorootparentprevSure, but not forever. My city banned plastic bags in grocery stores and I was annoyed and resentful because I used those when I cleaned out my cats litter box. However I got over it. reply RajT88 11 hours agorootparentprevPaper straws are an improvement. Just not perfect. I cannot speak to cost, but wax, paper/cardboard performs well enough for just about any food related purpose. Not suitable for shakes? Make them thicker. reply the_doctah 11 hours agorootparentI've never met a single person who likes using a paper straw over a plastic one. They feel weird in your mouth, they get soggy, and they fall apart. I fail to see how adding more paper material would fix the issue. reply RajT88 10 hours agorootparentPlastic straws perform better than your average paper straw, no argument there. I think the talking point is a lot of people do not care that much and paper straws are good enough. Wax would help way more for most use cases, but the added material would help for thicker mediums. Waxed paper straws are a thing and are almost as good as plastic straws. I am not sure why they are not more common. Pretty rare actually. reply MisterBastahrd 10 hours agorootparentThe reason we use plastic straws is because we've ALREADY used paper straws and consumers preferred the plastic ones. Especially now that all of our drink portion sizes are gigantic and straws tend to sit in drinks for hours. reply RajT88 9 hours agorootparentThat and they are cheaper to produce. Now we're learning more and more that using plastic for everything is not just bad for the environment, but also bad for our health. Consumer sentiment is shifting accordingly. For the record, I'm in camp, \"Don't care that much, whatever straws are fine\". I'd be happy if as a society we could just get people not to litter like other developed countries manage to. I have a lot of junk which ends up in my front yard (people tossing garbage from passing cars) and my backyard (a river where all kinds of styrofoam, plastic cups, plastic bags, etc. all wash up). If all the junk was biodegradable it wouldn't hang around my yard and require me to pick it up (or I could at least toss it on the burn pile). reply mcmoor 6 hours agorootparentI'm in camp \"I would rather gulp it right from the glass\", which seems like a better stance anyway. But I heard that some people really need straw, and if I'm in that position I'll 100% ask for the plastic one everytime. Then again if onlythe actual effects of all these laws and regulations it just makes me mad and want to ignore or fight ALL environmental laws Oh come on. How do you even make that jump? reply aiisjustanif 9 hours agorootparentprevAll of that channels your emotions in a negative way. Carrying reusable bags and straws really isn’t that difficult let’s be honest, we carry around other things. reply akira2501 13 hours agoparentprev> leading to resentment and rejection of environmentalism as a whole I think people like this because it explains why the \"other side\" doesn't just listen to their arguments and follow their prescriptions on how to live exactly. Meanwhile, these other people live _in_ the environment, and so they're obviously concerned about it at some level. My read is that \"environmentalism\" is used as a \"bully plank\" and people are rather tired of being manipulated for the ends of elites without any accountability for their policy failures and so generally tend to react quite negatively when it is naively brought into any conversation. reply scoofy 13 hours agoparentprevThey annoy people that already don't really care, and want an excuse to be vocal about it. It takes effectively zero effort to keep a reusable bag around, and you don't even have to use it every time to have a sizable impact. The reason why these bans are effective is because people like me don't want a bag anyway, and before these bans were in effect, I didn't have a choice. And these things do not end up in the trash, in part, because they are so light weight. The same thing happened when they banned smoking. Normalizing not using a bag unless you need one will feel normal for the next generation, but never feel normal for us. When your concerns about environmentalism extend only to \"unless it's inconvenient\" then you're not actually concerned with environment, you're concern with feeling socially shamed. Look, I have plenty of concern about faux environmentalism bullshit, like recycling plastic, but plastic bag bans are not one of them, because there are myriad alternatives that exist. reply prepend 12 hours agorootparent> It takes effectively zero effort to keep a reusable bag around I’ve found it takes lots of effort. It’s a chore to keep track of and bring it back out to the car. I probably now have 10 reusable bags that I’ve bought because I keep forgetting them at home, or in the car when I wasn’t expecting to go to the store. It’s not a huge effort, but it’s definitely non-zero. Obviously, I should be smarter. But I’m not, sadly. reply AStrangeMorrow 5 hours agorootparentI guess that depends on habits and preferences, but I much prefer my sturdy reusable bags over the basic plastic bags. Sure I do sometimes forget to bring my reusable bags, but even environmental reasons aside I hate plastic bags. It makes it harder to carry groceries, where you might have 10-20 bags instead of two big solid bags. They also frequently rip, so you have to pick stuff up and your groceries gets damaged. Then you have to toss all of them (or store them somewhere \"in case\" where you end up with 100s of them). reply scoofy 11 hours agorootparentprevIt's obviously not a habit for you, and that's fine. My point is that it's not physically difficult to do. It's just something you need to remember to do. This is what I mean by the next generation just being habituated to the process. There are plenty of these types of \"chores\" we accept because they are something we see as worthwhile. Wearing a seat belt, brushing our teeth, stepping outside to smoke a cigarette, putting on headphones to use our phone on the bus, not using a phone in a theater, etc., etc., etc. When this is normalize, nobody will notice, but the 20 years of transition will be slightly annoying, but my point is that, during that 20 years... we're not even materially changing our behavior. The stores have paper or slightly denser plastic bags for you. The thing that drives me crazy about this whole debate is how trivial it is. It's effectively the least possible change we can make to substantially change the culture, and very same people seem to claim that it's both (1) not enough change to matter, and (2) so much change that it's harming environmentalism as a cause. The entire point of doing it this way is behavioral economics. If you make it slightly annoying, most people will eventually change their behavior. The fact that so many people hate such a small change so much is exactly why I point out that the people who care so much about a slight behavioral change don't actually care about environmentalism, is that most folks haven't even begun to gauge the level of cultural change we need to actually fight climate change. If you think being asked to generally keep a grocery bag in your car is too much to ask, just wait until you're \"incentivized\" to take an ebike to the grocery store... If that's something you can't stomach even if it destroys the climate, then you don't care about the climate. reply prepend 8 hours agorootparentIt’s not physically difficult, it’s mentally difficult. And that’s what’s more important. Stopping smoking isn’t physically difficult. That’s not the part that matters. It’s mentally difficult. reply mrshadowgoose 8 hours agoparentprevAlthough this is also my impression based on personal observations over the years, I concur that it seems like a lot of activism in general is just a thin socially-acceptable wrapper around hating on other groups of people. Take carbon-neutrality for example. There's no shortage of people who will rage against flyers, car users, cruise takers, etc, etc as the cause of all carbon related ills in the world. And of course, the complainer's personal use of polluting energy is always objectively correct and morally just. And there are extreme examples like the \"Tyre Extinguishers\" group, which is clearly just a group of individuals who want to damage others' property under some thin veil of \"doing good for the environment\". I say all this as a person that actually does care about the environment, but have not fallen into the easy trap of thinking that hating on others is a solution. Meaningful central solutions are the answer. If one cares about carbon neutrality, I'd recommend advocating for across-the-board carbon taxation that is directly and honestly tied to the cost of fully and immediately offsetting a product/service's carbon footprint. And then it wouldn't matter one bit if someone wants to take several cruises a year, or drive an \"unnecessarily\" large car. But the social justification to hate on those people would evaporate, and that would make some other people sad. reply vegetablepotpie 8 hours agorootparentCan you say the same about cigarette smoking bans in restaurants? Just a socially acceptable way for people to hate smokers? Smoking or emitting carbon, or using plastic imposes negative externalities on other people. Second hand smoke causes lung cancer, carbon in the atmosphere leads to climate change, plastic in our environment can cause developmental and reproductive issues. There are reasons to oppose these things that do not include hating people. Some of the tactics may be ineffective or misguided, but this does not mean that everyone who cares about stopping harmful things hates a particular group of people. I like your idea of putting a price on carbon emissions. Many economists agree that it would be one of the most effective ways to eliminate emissions while allowing the economy to continue to grow [1]. Solutions are there, and they will be employed when the political will for it is built. [1] https://clcouncil.org/economists-statement/ reply IanCal 13 hours agoparentprevJust adding a cost to plastic bags here (UK) has cut down on the number of plastic bags I see caught in trees or bushes by an incredible amount. reply barrkel 13 hours agoparentprevThis is just not my experience. People don't complain and carrier bags as street rubbish, littering everything from urban trees to the countryside, just disappears. Charging is enough; bans aren't necessary, but sure, add them, no problem. reply Supermancho 11 hours agoparentprev> add highly visible inconveniences with no actual benefit Like quitting smoking, the benefit is not immediately obvious or universally effective (eg quitting at 70), but the benefit is still there. reply lo_zamoyski 9 hours agoparentprevWhat is your evidence it has caused backlash, or that it hasn't actually had a benefit? The whole article claims the contrary: that it works. I do agree that some behaviors can be counterproductive, like those pests who blockade busy roads or glue themselves to paintings \"for the environment\". Not only do these privileged and spoiled punks unjustly impede people from living their lives and destroy culture (they seem to always glue themselves to things that still pass as art, never the banana duct taped to the wall), but they only foment antipathy toward anything environmental, harming legitimate concern, action, and legislation in the process. The social deficits and ineptness of these people is astounding. reply yuliyp 9 hours agorootparentThe rest of this thread is some anecdotal evidence of a backlash, if nothing else. reply SoftTalker 10 hours agoparentprevAlso remember that plastic bags were introduced as being more environmentally friendly than paper (which comes from trees). reply PlunderBunny 10 hours agorootparentThere are good questions to ask about the pollution involved in making paper bags but: 1. That has to be compared against the pollution involved in making plastic bags, not against nothing at all. 2. Paper bags can use recycled paper, so it’s not really (directly) from trees. reply aiisjustanif 9 hours agorootparentprevYou do realize you are in Hacker News, where we believe science and evidence are ever changing things and that innovations do exist? reply hackerlight 9 hours agoparentprev> no actual benefit (e.g. bag or straw bans)' Can you provide a source for plastic bag bans having \"no actual benefit\"? The article is saying the opposite. reply PlunderBunny 13 hours agoprevMaybe I move in the wrong circles, but we banned both plastic shopping bags and plastic straws in New Zealand, and I don’t hear anyone complaining about it. Everyone I know uses paper bags (from the online supermarket deliveries) to line their bins. It’s not plastic bags and straws that make it to the landfill that are the problem - it’s the ones that don’t make it to the landfill. I haven’t seen a plastic bag stuck in a tree for years! reply mfer 12 hours agoparentWhen they banned single use plastic bags in New Jersey it increased the amount of plastic in use [1]. Starbucks straw less lids use more plastic than the old lid ands straw. With a lot of them ending up in the trash instead of recycling, it may not be a net benefit. The changes being made aren’t having quite the impact people had hoped. [1] https://www.usatoday.com/story/news/nation/2024/01/25/new-je... reply devonkim 11 hours agorootparentGiven that single use plastic bags are rather difficult to get recycled in so many metro areas (I remember reading a single digit percentage of it is even recyclable) it’s not clear if it going into the trash instead of recycling is not a huge impact. But increased use of single use plastics is certainly not desirable IMO similar to fossil fuels unless they’re compostable or similar types that at least can break down cleanly. reply mike_d 7 hours agorootparentThe best thing we can do for the environment is to ban recycling, kill the bike movement, and fight any solution that involves consumers. Some of it is maybe well intentioned, but it is an intentional distraction that we have to do thing to save the planet. No. Sanction countries that have rivers of plastic flowing into the ocean. Put the CEOs of chemical companies in prison when they have accidents. Eminent domain the entire farm when some crusty old fart refuses to allow a 15 foot wide section of land to be used for high speed rail. reply KennyBlanken 8 hours agorootparentprevPlastic bags cannot be recycled, period. Even worse, they clog up the recycling machinery. https://www.chicagotribune.com/2015/07/30/plastic-bags-a-hea... reply namrog84 8 hours agorootparentprevIn the context of plastic lids vs straws. While you seem to suggest that there is more plastic waste now by weight mass or quantity. There are other considerations. A lid is likely easier to see and pick up. Also a lid is potentially more likely to be less problematic for fauna whereas straws are known for being problematic (e.g. that one popular video of a straw stuck in turtle nose). I'm not saying we shouldn't try to do better though. But there are no doubt a lot of complex variables at play reply wavesounds 12 hours agorootparentprevThe main goal isn't to reduce the amount of overall plastic created its to reduce the amount of plastic trash that ends up on the streets, beaches, rivers, etc. reply gregable 11 hours agorootparentI sometimes am not clear on what the goal is. The article seems to argue that the goal is very narrowly to reduce the amount of plastic bags created/consumed and then claims a study shows that the bans do indeed achieve that goal. It's hard to imagine this goal not being achieved, but it's too narrow. I haven't seen any study showing that total plastic trash, incorrectly disposed, is reduced. It could be hard to study, I admit. I'd love to know the amount of the reduction as well. My guess would be there is a reduction, but it is fairly small. For example, in the San Jose survey: https://web.archive.org/web/20230512013405/https://www.sanjo... pre-ban creek and litter surveys only showed 9% single-use plastic bags and this dropped to 2%. I'd imagine 7% reduction is the upper bound on the impact, but it could be smaller than that if other litter increased. Maybe that's high enough to make the ban worth the inconvenience, I don't know what the right threshold should be. Broader goals could include reducing total plastic production, reducing fossil fuel mining, etc. I'm more suspicious that these goals are not being meaningfully affected by bag bans. reply giarc 11 hours agorootparentprevIf that's the case, is targeting rich developed countries with efficient waste management and pickup the best approach? I live in a very clean, North American city. I rarely see plastic bags blowing around. We have residential garbage pick up, and public spaces all have public bins. Our landfills are, what I would assume, are well run. Does the plastic bag ban in my city make sense? We never had an issue with plastic ending up in lakes/rivers etc. Now look to developing nations where rivers and streams are overrun with plastic. Do they have plastic bag bans? Doesn't seem like it and seems like that is where there should be one. reply stetrain 10 hours agorootparentIf the purpose it to keep plastic waste and microplastics out of your local environment and local drinking water sources, then local policies make sense. Should other places that also have that potential problem also do that? Sure, probably, if it's practical. But people in country X usually don't get to make local policy for people in country Y. reply abdullahkhalids 3 hours agorootparentprevPlenty of global south countries have plastic bag bans at various levels of governments, with varying degrees of implementation and success. Eventually, every country in the world should have it. And one shouldn't wait for others to do good. reply petesergeant 11 hours agorootparentprevIf I was going to steel-man the argument, I’d suggest that you’re adding some kind of extra economies of scale to production of less polluting alternatives? Also I note that mid-income countries like Thailand are also getting in on plastic bag reduction. The kind interpretation of that is that muang thai has finally discovered its eco-consciousness, but an alternative one is that they’re copying rich countries ‘cuz it’s fashionable, and that that effect might trickle down to the countries who are serious polluters reply petesergeant 11 hours agorootparentprevBuilding a wall around the Philippines would get you an overnight 10% reduction in ocean plastics https://ourworldindata.org/ocean-plastics reply lebean 10 hours agorootparentBuilding a wall of that scale overnight would be quite a feat, almost as impressive as successfully getting people to start using paper straws. reply petesergeant 9 hours agorootparentImplementation details left to engineering, I’m just an ideas guy reply avastmick 13 hours agoparentprevI have to second this. No one I know complained when it was introduced, and I saw no complaints aired in the media of any noticeable degree. We still have too much unnecessary plastic in packaging. My wife is Irish, and they started the removal of plastic bags over 20 years ago. It was carefully phased in over time. It led to a 90% reduction in plastic bag use [0]. They also weigh your trash (in Dublin, anyway) as a means of cost pressure to reduce waste and encourage recycling. It is stated to have reduced waste by 50% [1]. [0] https://www.irishenvironment.com/iepedia/plastic-bag-levy/ [1] https://www.irishtimes.com/business/economy/why-it-pays-to-c... reply romaniitedomum 11 hours agorootparent> They also weigh your trash (in Dublin, anyway) They weigh country-wide. In Ireland waste collection is done by private companies and they charge by weight. As I recall, they have something like subscription plans [1]. I'm Irish but I live in New Zealand now, and here the rubbish collection is paid for by your Council rates. I was still living in Ireland when the plastic ban was introduced. There was push-back from some companies that make plastic bags, unsurprisingly, but it worked really well. There was a bit of push-back here in NZ too, similar to what happened in Ireland. The usual grumbling, about interfering greenies, loss of freedoms, etc. [1] This is an example of one company's offerings: https://cportal.barnarecycling.com/signup/signup_page2.php?S... reply crtified 12 hours agoparentprevThe reduction in such litter is a boon in itself. However we can't allow it to distract from the bigger picture, for example with New Zealand : https://www.consumer.org.nz/articles/how-does-new-zealand-pa... \"We were the second-worst country for packaging recyclability. Here in clean, green Aotearoa, 57% of the packaging we assessed wasn’t recyclable in practice. That’s not too bad when compared to Brazil (92%), but we have a lot of room for improvement. Especially when our Aussie cousins beat us by a mile with just 14% of packaging not being recyclable.\" A big part of the problem is the commercial conflict of interest, which among other things means that robust data isn't available. For example, if we had year-on-year graphs showing domestic plastic production, consumption, sales, in different regions and industries, for different purposes, we could start to build a true big picture. Instead, the populace is reduced to arbitrarily celebrating visible wins, without really knowing whether we're winning or not. reply switch007 11 hours agorootparent> However we can't allow it to distract from the bigger pictur I sometimes wonder if that’s the real agenda Like recycling. Keeping us busy sorting things in to different bins. Without thinking about the absolute torrent of crap people order on a daily or weekly basis. But it’s ok because the cardboard and plastic packaging is going in a magical bin (which often isn’t that magical…) reply morepork 11 hours agorootparentThere is too much of a focus on recycling, when reducing the amount of stuff we consume would be far more beneficial. When I was at school they used to talk about the 4 Rs, reduce, renew, reuse, recycle (I'm sure different variations exist). With recycling being essentially a last resort as it's so difficult and inefficient. But now it seems there is barely any mention of the first 3. reply PlunderBunny 9 hours agorootparentA cynic would say that it’s harder for companies to make money selling products that reduce/renew/reuse. reply willsmith72 9 hours agorootparentprevit's not an agenda, it's 2 things. you tell someone, \"hey don't order so much, it's bad for the environment\", and they reply \"you can't stop me doing something i want\", like all the plastic-straw fans in this post given a person who will make the same order regardless, recycling is better than not. we need both things reply LouisSayers 11 hours agorootparentprevFor sure, having seen the way Germans recycle, NZ has a long way to go still. The plastic bag ban is a good start, but seeing how Germans reuse glass bottles is an eye opener, and I really don't understand why we have individual cucumbers wrapped in plastic?! reply asvitkine 9 hours agorootparentThe plastic wrap on the cucumbers makes them last longer. You can argue whether it's worth it, but there is a reason. reply OJFord 12 hours agoparentprevFwiw I haven't seen a plastic bag stuck in a tree for years either. I live in the UK where they're legal but there's a (mandated) 10p or something fee for them. (Personally I have a few of the slightly more expensive but much less disposable ones that I can reuse indefinitely.) Weirdly I did see cassette tape stuck in a tree recently though! reply coding123 12 hours agoparentprevSo this whole thing is really about trash not the environment reply PlunderBunny 9 hours agorootparentThat’s a good observation, except that the two are often linked of course. reply Apreche 17 hours agoprevOnce I went on vacation to a tropical island. I went to a grocery store on the island. I didn't bring a bag because I was a doofus who had not previously lived or traveled to a tropical island. There were no plastic bags. There were no paper bags. There was no option to pay money to get a reusable grocery bag like we see in the US. If you were a doofus like me who didn't bring some kind of bag, you only had two options. One was to miraculously carry your stuff home without bags. The other was to use the cardboard boxes that used to contain produce, if there were any left over. We carried our groceries back to the hotel in a cardboard box that previously contained fruit. It was a minor hassle in the moment, but I also realized that's how it should be everywhere. There are probably already enough bags in the world for all the carrying that humans need to do. Of course bags wear out, so we need to keep producing some amount of bags, but not many. Most stores should simply not have any kind of bag whatsoever. If you don't bring something of your own, you should be mostly SoL. reply _ph_ 10 hours agoparentI think selling fully reusable cloth bags would be the right thing to do, but otherwise I agree that stores shouldn't offer single-use plastic bags any more. reply Nition 9 hours agorootparentIn NZ after the single-use plastic bag ban some people argued against only having high quality reusable bags available by saying \"but I keep forgetting my reusable bags, and I so I keep having to buy more! Now I have 15 cloth bags at home - this is far more wasteful than when we had single-use plastic bags.\" But those complaints came in the first few weeks after the ban, and they dried up soon enough. Everyone learns eventually that they need to take their bags with them - at least I hope nobody is sitting at home now, five years after the ban, with 1000 reusable cloth bags. P.S. I've found jute bags the best value for money. They seem to really last forever whereas the thin cloth type start wearing out after a while. reply _ph_ 1 hour agorootparentYes, I have bought reusable bags just because I didn't bring one. :) But then I keep using it and over time this doesn't really create extra trash. reply interestica 5 hours agorootparentprevIn Canada this is an issue. The 'reusable' bags aren't cloth -- they're still a weave made from plastic. And it can be tied to culture and availability. It's much easier to 'remember' your bags if you're making a dedicated trip to the grocery store from home. But, if you want to spontaneously stop in after work, for example, you would have had to remembered a bag before you left home or had one on you at all times. We should be encouraging the type of city design that allows for (and encourages!) this type of spontaneous shopping. We should figure out the bagging system to match. Recyclable/compostable paper bags seem like a good thing. reply dharmab 17 hours agoparentprevThese days I keep a bin in my trunk, transfer my groceries from the cart into the bin, and then I carry the entire bin into my kitchen. reply gruez 17 hours agoparentprev>Most stores should simply not have any kind of bag whatsoever. If you don't bring something of your own, you should be mostly SoL. Yeah, because driving back to your house to get a bag and then driving back is so much better for the environment than a using a few 0.1mm thin bags. reply drekk 13 hours agorootparentPeople on tropical islands don't really use cars. I don't think most of the readers on HN understand the level of sacrifice required from everyone to avoid the worst of what's coming. It's going to require a lot more from everyone than using reusable bags while shopping or avoiding straws/using metal ones. I agree these environmental laws are simply green washing. But if people think these generate too much resentment they're wholly unprepared for what the moment requires. Like not eating meat with every single meal every single day of the week. Not having two cars per family regardless of whether they're ICE or not. Eliminating short-haul flights and restricting international travel. US consumers are used to overconsuming. Correcting for that will feel like a punishment to most. I don't see an alternative besides telling people to treat it like a world war. \"Victory gardens\" and all. reply johnnyanmac 13 hours agorootparent>I don't think most of the readers on HN understand the level of sacrifice required from everyone to avoid the worst of what's coming. It's going to require a lot more from everyone than using reusable bags while shopping or avoiding straws/using metal ones. To be frank, I don't think we will avoid the worst of what's coming. Because a lot of it isn't in control of consumers but from business emissions. e.g. wouldn't mind the ability to stop using my car tomorrow if I had reliable bus schedules that weren't separated by an hour per stop, but I have no faith that the transportation for my city will ever fix that in a timely matter. There's also negative inventive from stuff like ride-shares to want to fix that. WFH is another way to cut down on emissions to compute but instead companies are hunkering back down because they gotta justify their sunk cost on buildings. That's 2 of some dozen problems that could prevent the worst but whose cards aren't completely in the hands of the person. It just feels so hopeless. reply mr_toad 6 hours agorootparent> Because a lot of it isn't in control of consumers but from business emissions. Businesses ultimately only exist to provide to consumers. Even those that sell directly to businesses of governments wouldn’t exist without consumers. Consumers are either going to have to jump now or fall later. reply willsmith72 9 hours agorootparentprevthat's not business emissions though, that's a government problem change in government comes from people. capable government looks at real societal problems and shapes the future of the state to address them. of course public transport is a huge one, especially in the US reply cpursley 9 hours agorootparentprev> Eliminating short-haul flights and restricting international travel. This line in particular is an extreme totalitarian view, good grief. reply inkyoto 9 hours agorootparentprev> People on tropical islands don't really use cars. They do and do so extensively in some places. I have been to Wallis and Futuna islands, two very small islands in the Pacific being French overseas territories where the transportation around each island is exclusively by car and locals drive around all the time. They would only walk to the nearest fale and drive at all other times. Petrol was expensive (that was around EUR 2.5 a litre approximately a decade ago) but the French government nearly entirely subsidises it due to the two islands' economies being too small to cover extra costs. reply bmicraft 14 hours agorootparentprevIf you're there by car you can't convince me you actually *need* bags. reply latentcall 12 hours agorootparentprevGood point. This is why I keep reusable bags in my car for that exact scenario. It’s helped me a ton, you should try it. reply imp0cat 13 hours agorootparentprevKeep some large blue Ikea bags in your car instead. reply Libcat99 12 hours agorootparentI have half a dozen of these and have never needed anything else. Bonus: way easier to get everything inside at home. reply throwitaway222 17 hours agoparentprevThere are stores in the us that do the cardboard box method reply kibwen 12 hours agorootparentYes, Aldi's does this. reply Wowfunhappy 16 hours agoparentprevI would be fine with this outcome (even as someone who would find it personally annoying), but it's not the reality we live in today. You'd have to actually mandate this by law. The half measures we currently have are the worst of both worlds. They inconvenience people and increase fossil fuel emissions. reply kitten_mittens_ 16 hours agoparentprevWhen the pandemic hit in the US, and people were hoarding a bit, I had several times where I had to carry groceries home in a fruit box. reply beej71 15 hours agoparentprevOur Grocery Outlet does this, as does CostCo. reply Scoundreller 13 hours agorootparentCostco seems to have perfected it by rarely having enough boxes at the cash, so forward-thinking customers do their job for them by grabbing boxes from the product shelves. reply mechagodzilla 16 hours agoprevMy town did this, and I immediately went from picking up 1-2 plastic bags from my yard every week to basically zero in the last 4 years. I have no idea what the net impact on carbon emissions or other factors was, but the reduction in visible trash in my neighborhood was noticeable, immediate and seemingly permanent. reply AStrangeMorrow 5 hours agoparentYes. I feel like people often solely focus on CO2 emissions as the only metric. Especially some detractors of ecological pushes and regulations. Like \"it doesn't really decrease CO2 output, so what the point\", or \"it's such a tiny fraction compared to X industry of Y country, and such an inconvenience so what's the point\". I just don't understand how producing less stuff whose sole objective is to end up, in the best case, directly in landfill, is bad reply belorn 17 hours agoprevAs a regular diver in the Baltic ocean, my experience is aligned with the finding that the ban on plastic bags and utensils did have a real noticeable effect. Before the ban I saw trash every dive. Now it is much less common, closer to 1/10 of how it was before. Nowadays the most common trash I see are beer cans. reply ThinkBeat 17 hours agoprevThe problem at least where I live in Europe is that people stopped buying grocery plastic bags, which is good. But they were frequently re-used as garbage bags. Now people instead buy plastic garbage bag rolls. So even if the consumption of plastic shopping bags has decreased, the consumption of plastic garbage bags has greatly increased. Might well be easier to recycle the garbage bags. One could hope. reply nedt 2 minutes agoparentWell you can always take biodegradable garbage bags. Built-In recycling, they just become soil again. reply notzane 11 hours agoparentprevThere’s been some research showing the effect you’re seeing is real. > We estimate that CGB [carryout grocery bag] regulations lead to an average increase in purchased plastics of 127 pounds per store per month, ranging from 30 to 135 (37–224) pounds for 4-gallon (8-gallon) trash bags. https://link.springer.com/article/10.1007/s10640-022-00646-5 reply barrkel 13 hours agoparentprevLittering is the problem solved by charging for or banning free carrier bags, not usage of plastic bags. If your rubbish goes to landfill, don't expect a reduction in rubbish bags as a component. They're very light though. IMO it's better to incinerate them. That's what Switzerland does. I think it largely works, as long as you have enough routes to take dangerous chemicals (electronics and batteries mostly, heavy metals) out of the the waste pipeline. reply andrewstuart 13 hours agoparentprevPlastic bags are banned here in Australia. I too use plastic bags as rubbish bags. I can say that despite the ban on plastic shopping bags there is never any shortage of plastic bags. Admittedly I use small plastic bags but it's virtually impossible to buy food that doesn't use plastic bags - I use them. reply artiii 13 hours agoparentprevgrocery bags are made from newly produced plastic (sametimes clean recycled ak highest grade, because food requirements). Garbage bags, on the other hand, are made from the lowest possible grade recycl, which can't be recycled. reply gnicholas 14 hours agoparentprevI never bought trash bags before these bans went into effect. Now it’s one of our subscribe and saves. I wouldn’t be surprised if the net effect was that we are using more plastic now than before, even though we use reusable bags most of the time when we go shopping. reply mepiethree 17 hours agoparentprevAnecdotally, people in my life used to buy plastic garbage bags and use shopping bags as garbage bags (or dog poop bags) in smaller-sized garbage bins. Also anecdotally, despite the fact that I almost always grocery shop with reusable bags, I still somehow have plenty of plastic bags under my sink at any given time to use in my smaller trash bins. There are plenty of non-grocery places I get plastic bags: CVS, take out meals, Home Depot. These more than fill my need for small plastic garbage bags. reply gnicholas 14 hours agorootparentWhere we live they don’t give free bags at CVS or other stores. I think restaurants may have an exception for take out, but those bags often get sauce spilled all over the inside, making them unsuitable for saving or reuse (except immediately, as a trash bag). > Where I live (Silicon Valley), paper and plastic bags were both subject to the same treatment. In Menlo Park you can buy bags when you shop for $.25 each. The plastic bags at Safeway are much thicker (i.e., use more plastic, and are hypothetically reusable more times) than before. The paper bags are the same as before, but now you pay for them (the revenue goes to the store). reply lotsofpulp 17 hours agoparentprevNo, that type of thin plastic is not recyclable. reply dragontamer 13 hours agorootparentYes and no. They are recyclable. But because of their low weight, high amounts of contamination, and constant ability to get stuck in conveyor belts... Thin plastic bags are more trouble than they are worth. It's like Aluminum foil. Recycling plants are paid per ton of recycled material, and it takes lots and lots of aluminum foil before a ton of Aluminum is saved up. Except plastic is way harder to recycle than Aluminum (requires higher purities). reply PlunderBunny 13 hours agorootparentIt depends of course, on where you are and what type of recycling schemes are available. As a general rule, “don’t put plastic bags in with your regular recycling” is correct. But we have a very successful ‘soft plastic’ recycling scheme in New Zealand that results in useful products (I have 52 fence posts made with thousands of recycled milk bottles and plastic bags). reply dragontamer 11 hours agorootparentYeah, my location also has special containers for clean plastic bags. But general recycling no longer takes them. reply sokoloff 13 hours agorootparentprevThin grocery (and garbage) bags are sheet polyethylene, the plastic of which is just as recyclable as milk bottles. They are not accepted in many curbside recycling programs because the separation tech is not designed for them, not because the plastic itself is not recyclable. They are recyclable (and frequently recycled) via dedicated collection points. (Most of our grocery stores have them.) reply changoplatanero 18 hours agoprevSure, banning plastic bags means that there are less plastic bags. But that's a low bar to meet to call the result a success. For example, did the ban reduce the total amount of plastic produced? Plausibly, no it did not. From the report: > Because of the loophole in California’s bag ban allowing the use of thicker plastic bags, the amount of plastic bags discarded per person (by weight) actually increased in the years after the implementation of the ban. Did the ban on plastic make a meaningful reduction in co2 emissions? Did it make people happier? Did it make a meaningful improvement to the environment? reply stonith 10 hours agoparent> Did it make a meaningful improvement to the environment? https://www.abc.net.au/news/2023-06-07/most-plastic-bags-gon... It's not completely clear from this article if the measurement includes the thicker bags, but my guess is yes. reply rrr_oh_man 13 hours agoparentprev> Did the ban on plastic make a meaningful reduction in co2 emissions? Did it make people happier? Did it make a meaningful improvement to the environment? Those are the questions to ask. Instead, many people focus on behaviour control fantasies & gotchas. reply asynchronous 12 hours agoparentprevThis metric is the only one that matters and it’s completely sailed over in the article to instead support their point reply kylehotchkiss 13 hours agoprevNow my target bags use 5x the amount of plastic and cost me $.10 before I put them in the bin. If anything it seems like waste is increasing from them. Why can’t somebody make a paper bag with a handle that doesn’t rip in the parking lot? reply orf 13 hours agoparentWhy don’t you just reuse them instead of putting them in the bin? It seems like the waste is coming from you. reply mattlondon 13 hours agorootparentWhat are you supposed to do? Carry around bags with you all day in case you happen to go to the store? If it is a planned visit sure, but if you are just out and about you don't want to be carrying around spare bags in your jeans pocket just in case. reply mustacheemperor 13 hours agorootparentI actually essentially do that, I have a reusable bag that folds up into itself with a zipper into a package about the size of a wallet. Fits well in a purse or jacket pocket and is more pleasant to carry when full than a plastic bag anyhow. So really, there’s not even a small sacrifice involved for me, just a little bit of planning, to avoid making that waste. And isn’t this kind of ingenious gadget based solution much more in the hacker spirit than throwing up our hands and saying, give me back the old traditional way regardless of the flaws? reply jolmg 12 hours agorootparentSame. My bag's the size of half a wallet when folded, so it's not really a bother to carry a couple, let alone one. I tend to keep at least a couple in my car's glove box as well. I'm not sure my plastic usage has decreased though, since I used to recycle the thin plastic bags to line my trash bins. Now I buy oversized white plastic trash bags to line my bins. I just can't wrap my head around e.g. having a bathroom trash bin without a bag protecting it. It feels incredibly gross, though I do know one family that does that. I guess/hope they wash their bins very regularly. reply prepend 12 hours agorootparentprevThat’s cool that you do that. I don’t like carrying things that I rarely use and try to minimize stuff I carry. And that’s basically just a phone and my clothes. Even if you carry a bag, I frequently find that I need two or three or more. I think a better solution is to just have decomposable plastic bags and solve the issue that way. reply prawn 10 hours agorootparentWhere are you before you go into a shop? Surely some times it's at work or home, or the car. Might help to keep a couple of fold-up bags in each of those spots. Usually habits lag these phased changes by a couple of years, but you get better at being prepared or anticipating times you'll need them. reply prepend 8 hours agorootparentI’ll give you an example from today. I was out for coffee and wasn’t expecting to go shopping. I walked to a book store and bought some books. They have me a plastic bag. I’d normally turn it down and just carry the books, but it was raining a little bit. If it was important, I’d remember. I don’t think this is important so I don’t care enough to always carry a bag with me. reply prawn 6 hours agorootparentI'm not saying there are never situations where it's useful, just that they're uncommon and collateral damage in trying to change broader behaviour (whether by charging for bags or changing the bags on offer). In your example where I live, they'd offer a paper bag which would cover the little bit of rain. If there was enough rain to cause trouble, people would have an umbrella. reply DoctorOW 11 hours agorootparentprevI'm a little surprised you don't bring a wallet or money into a store. I'm not being pedantic, but folded it's around the same size. A good reusable bag carries about what two to three plastic bags do. Plus, you only have to carry a bag when you're actually going into the store, anywhere else you can keep it in your vehicle. reply prepend 8 hours agorootparentI buy things with my phone. Sometimes I might have cash but that’s almost never. reply orf 13 hours agorootparentprevDo you usually pop into target because you’re out and about, walking around with no access to a vehicle in which you might keep bags? And then you purchase enough stuff that you require a large plastic bag for? And this happens often enough for you to be throwing away large quantities of these bags, and rather than reflect on your habits and adapt, you say something else is causing the waste? reply mattlondon 12 hours agorootparentprevEdit: stores outside of the US are accessible without a car. It is very common for people to go to local stores during lunch breaks or on their way home from work on public transport etc. I personally don't have space in my jeans pockets after a wallet and the ridiculous size of modern phones to then also pack in 2 or 3 reusable bags too. Sure if you drive to a store, keep some bags there. I do this a lot but I am not going to carry bags around with me on the off-chance I might go to a shop that day. reply prmoustache 12 hours agorootparentThe answer is foldable shopping bags. Any intelligent person that regularly go to stores during lunch breaks or when commuting is either always carrying a backpack or a small foldable shopping bag. There are many folding bag designs that fit into your pants backpocket. I have no car and I very rarely leave home with either a foldable bag, a backpack, a drawstring bag and when I don't have one of those that is usually because I am using my bicycle which is equipped with a basket and panniers on the rear rack, or my motorbike with its top case. reply prepend 12 hours agorootparent> Any intelligent person that regularly go to stores during lunch breaks or when commuting is either always carrying a backpack or a small foldable shopping bag The problem is all the non-intelligent people, like me, who don’t carry this. I don’t carry a backpack around on my lunch break (or really anywhere other than when I’m hiking). reply rmccue 12 hours agorootparentI have a foldable shopping bag which is smaller than my wallet, you don’t need a full backpack. If I think I might want to pop into the shops while out, I’ll chuck it in my pocket. Sure, if you want to do a big family shop then it’s a bit different, but that’s more of an event anyway. reply prepend 8 hours agorootparentDoes it fit inside my phone? Because I need a bag about 1% of the time and I’m not carrying a bag for those occassions. reply prmoustache 12 hours agorootparentprevIt doesn't have to be a backpack. A foldable shopping bag fit in the pockets of your pants. If this is something you do regularly, you would learn at the second occurence to have any kind of bag ready at your place of work. reply prawn 10 hours agorootparentprevKeep a couple of fold up bags at work and take one on the lunch break if planning to shop or if shopping on the way home. I can't think of a time that I went out for a lunch break and then on a whim bought two full bags worth of groceries back to the office. reply 9935c101ab17a66 13 hours agorootparentprevJust leave them in your car? Most people drive to stores, especially in the US. I also almost always have a backpack with me when I’m out and about, I just leave some cloth backpacks bags in it. Realistically, how much stuff are you buying on trips to the store on a whim? reply Johnny555 13 hours agorootparentprevThe vast majority of Target stores are not urban stores, they are big suburban stores where few people visit without a car. But even in the city, I almost always have a small backpack with me, and in one of the pockets I have a very compact fold-up shopping bag that I use when I stop at the store on the way home. reply adrr 12 hours agorootparentprevMost americans drive cars. You can easily keep them in your cars. How do europeans do it with higher public transportation utilization? reply prmoustache 12 hours agorootparentEuro here. I rarely get out of the house without some kind of bag and when I do that's because I take a bicycle which is equipped with a basket and panniers. reply bmoxb 12 hours agorootparentprevMost people I know tend to go about their business with some sort of bag on them. reply razemio 13 hours agoparentprevReusable bags aren't an option for you? We have several of them and just put them in the trunk. If I forget to bring one (which happens 1-4 times a year) I buy a new one for 2€, replacing an almost broken one. reply kibwen 12 hours agorootparentThe amount of learned helplessness in this thread is astounding. I bought a pair of collapsible reusable bags years ago for $5. I use them every week and they're as good as new. The stronger construction means that each one is replacing at least three disposable plastic bags with every trip. You don't need disposable bags. And yes, I have a cat, and disposable bags are shite for litter, because they always, always have holes. reply prawn 9 hours agorootparentI imagine all the \"I can't fit bags in my pockets for my impromptu shopping trips\" people posting when mobile phones first became popular: \"Are they expecting us to put these in our pockets?! All the time!? Just in case someone calls???\" Seems like a broader resistance to change? reply PlunderBunny 7 hours agorootparentprevThere’s a certain point where “I keep forgetting” is really just “I don’t want to”. reply mattlondon 13 hours agoparentprevThey started as £0.05 here in the UK. Now a few years later it is £1 for a plastic bag from Waitrose. Outrageous. Even worse are the places that offer only paper bags, but charge you for them because they can now. 30p for a paper bag from Boots makes no sense when the whole point of the bag levy was to stop people using plastic ones. reply snet0 9 hours agorootparentI think the thought is that you should buy less bags. I'm seeing you in other comments, lamenting the fact that you routinely go shopping without any bags. I'll admit that there could be perfectly reasonable situations that are negatively impacted by the bag levy, however I think the entire purpose of this levy is to discourage you from buying bags. You should have a few, and re-use them when you go shopping. Eventually you'll replace them, but it'll be after a while and it'll be at negligible cost. If you're buying so many bags that a £1 cost is \"outrageous\" to you, you are the problem. reply willsmith72 9 hours agorootparentprevwhy shouldn't there be a cost? it's the best way to shape behaviour. reply morepork 11 hours agoparentprevMaybe they should have a deposit that gets refunded if you return it. Say it cost $1 per bag, but you could get it refunded if returned on your next trip you get most of the convenience with little hassle. reply okonomiyaki3000 11 hours agoprevIn Japan, they're not banned but there's an extra 2 or 3 yen charge if you want one. This charge amounts to just about nothing but it was enough to get almost everyone to switch to \"eco bags\". I suppose it's not the \"savings\" of a couple yen here and there that motivated people to use their own bags but simply the awareness that there was something of an expectation that they would bring their own. Prior to this, you might have looked a bit eccentric bringing your own bag to carry your groceries in but now it's just normal and Japanese love nothing more than being normal. reply Danieru 10 hours agoparentIndeed, it also normalized walking out of a store bagless while holding your purchase. I like it. I don't like when I ask for no bag, and the combini charges for one anyway. With that said, Japan never had a litter problem. Likewise our garbage system is highly developed with waste to energy using high temp incinerators. The ban-charge-requirement never could have had an environmental aspect. But if it can reduce oil imports, that's worth doing. reply tmm84 8 hours agorootparentI've personally got more attached to receipts here in Japan since I refuse to pay for any plastic bags (I carry an eco bag in my backpack for big stuff). I just put my purchase in my backpack with a receipt just in case. I don't know if it helps much because everything in Japan is wrapped in plastic it seems. reply nomilk 18 hours agoprevDepends what's meant by \"work\". In my country, single use plastic bags were replaced with thicker reusable plastic bags that many people discard after a single use. So the total volume of produced/discarded plastic probably increased despite the number of bags used probably going down. I don't have sources other than anecdotal evidence based on behaviours I observe. reply nativeit 18 hours agoparentI think it means “Billions of plastic bags were avoided in the US alone,” but that’s just me reading the headline. reply Wowfunhappy 18 hours agorootparentAnd if your goal is to eliminate a specific type of plastic bag, that's great! It's also a stupid goal. if your goal is to eliminate plastic, or fossil fuel emissions, I'm not convinced these bans have been effective. reply VintageCool 7 hours agorootparentThe goal is to eliminate trash in the environment. Thin plastic bags get discarded carelessly, then catch in the wind and get strewn around a wide area. Plastic bag bans have been sufficiently successful that people have forgotten this was a problem. reply dario_od 17 hours agorootparentprevGreat! Can you show me the numbers that make you think that these bans were not effective? reply Wowfunhappy 17 hours agorootparenthttps://www.freedoniagroup.com/press-releases/freedonia-repo... Edit: Apparently this is commissioned by the plastic industry. It does match my anecdata of how many people I see buying \"reusable\" bags at the supermarket check out, combined with https://ourworldindata.org/grapher/grocery-bag-environmental... reply emmo 7 hours agorootparentI've been using the same reusable bags for years, so they have more than hit those numbers. Why are the cotton numbers so wild? reply HPsquared 17 hours agorootparentprev\"Number of bags\" is only one way to measure this. It's probably the only metric that went in the right direction, so that's the one they report. reply risho 13 hours agoparentprevsame thing happened here. it became illegal for grocery stores to give out single use plastic bags here so now they just give out thicker bags that they call multi-use. reply LeafItAlone 17 hours agoparentprev> In my country, single use plastic bags were replaced with thicker reusable plastic bags that many people discard after a single use. What country? reply mNovak 16 hours agoprevA lot of people pointing out the various studies saying that plastic consumption increases after a bag ban; but isn't that expected in the short term? Everyone has to go buy 3-4 heavy reusable bags for the first time (or few times as they get used to the idea), that's obviously going cause a spike in plastic consumption, above a normal year of disposable bags. But the more meaningful question is if 5 years later, people are still buying excess heavy bags for a few uses or if the behavior actually adapts. reply ungruntled 13 hours agoparentI wonder what level of additional waste is now caused by these reusable bags that we will continue to see forever. FreshDirect will provide 2 heavy reusable bags each delivery I receive each week. They claimed to offer to pick these up but that has been suspended for years. They now suggest to “donate” the bags. Obviously these end up in the trash. The strange part of whatever law led them to this idea is that because these bags aren't rigid enough, products tend to be damaged and arrive organized like a trash pile where at least one thing spills all over everything else. Oh, and they still put frozen goods in thin plastic bags. I recall the best quality delivery for my use-case being products in standard takeout delivery paper bags wrapped in plastic to avoid leakage. I’m certain far less plastic was used in those cases, and the bags themselves could be easily used to store trash for the compactor avoiding the need for the thicker trash bags. reply ambyra 17 hours agoprevI always thought the plastic bags deal was not seeing the forest through the trees. Most food comes in heavy plastic. The food is consumed in a day or so, and the plastic lasts FOREVER. Forcing the companies to use paper or glass packaging, or having reusable returnable containers would have a bigger impact than banning those thin plastic bags. reply razemio 12 hours agoparentTrue, but who says laws stop there in the coming years? I sincerely hope these things will come aswell. Banning non reusable plastic bags however is a good and easy start. reply crtified 12 hours agoparentprevThe main issue there, imo, is that plastic is the main technology used to protect and seal food items, thereby making them last longer without spoiling. A good part of the world's food trade and economy now relies upon it. There would be mass starvation without it. Plastic has saved humanity insane amounts of energy over the short term, and has contributed to our population growth. The resulting environmental debt is mindbendingly massive, and I'm not convinced that the corporate world will willingly pay it. It will be paid though, one way or another, because Laws of Physics, entropy etc. The smartest species is also the stupidest. reply ambyra 12 hours agorootparentA bring your own bag/bottle/container store would be cool. Everything would be shipped in bulk to the grocery store, and people would take what they want in their own containers. reply crtified 11 hours agorootparentSome such outlets do exist. Its also reminiscent of older style open-air food markets. But globally speaking, the short term charms of plastics clearly won the economic race. The same cheap, low effort, high resistance elements that make plastics such an environmental problem are also the factors which dictated its use. reply treme 18 hours agoprevthe replacement bags are usually made out of another type of plastic that needs to be reused 100+ times minimum to make the trade off worthwhile, which is rarely the case. This is what superficial activism looks like. reply slimrec77 13 hours agoparentIt is literally a type of religious ritual to the Earth gods. The point is not that it helps or doesn't help, the point is \"we have to do something!\" Just like it is obvious from a breakdown of the data that is beyond stupid to send one giant diesel burning truck to pick up \"garbage\" then another giant diesel truck to pick up tin cans and cardboard at a net energy loss. Of course, it is impossible to stop this even if it would be rational thing to do. \"we have to do something!\" reply jasonkester 18 hours agoparentprevSuperficial is good in this case. One of the worst things about plastic bags is that they end up wrapped around every roadside bush in Africa, Southeast Asia and South America. I’ve never seen one of those beefy reusable carrier bags blowing around on the side of a road. reply postepowanieadm 17 hours agorootparentHow American plastic bags ban affects Africa, Southeast Asia and South America? reply dymk 16 hours agorootparentWe ship our plastic to those countries reply treme 17 hours agorootparentprevInstead they will wreck havoc on marine life because they decompose into loops of stringy rope that's difficult to break reply kibwen 12 hours agorootparentWhy does this comment feel like someone trying to vice-signal about their lack of care for the environment while attempting to manipulate the people who do by making up complete nonsense? No, reusable shopping bags are not worse for the environment than disposable ones. reply treme 7 hours agorootparenthttps://edition.cnn.com/2023/03/13/world/reusable-grocery-ba... \"a cotton bag should be used at least 7,100 times to make it a truly environmentally friendly alternative to a conventional plastic bag.\" reply alphaddx 4 hours agorootparentprevWell it wasn't \"vice-signalling\" as they posted a source and had a legitimate criticism yet you assumed bad faith and attacked them based on that. reply HPsquared 17 hours agoparentprevExactly. It's like the straws. Something highly visible and conspicuous, with very small impact. The definition of tokenism. reply kibwen 12 hours agoparentprevThis is a preposterous talking point, I wish people would stop parroting it. Reusable bags carry as much weight and volume as multiple disposable bags. And yes, you can use them hundreds of times. I have used mine every week for years and years. Disposable plastic grocery bags are wasteful and pointless, and those defending them so virulently come across as bafflingly pathetic. reply mjevans 11 hours agoparentprevI'd prefer a _ban_ on plastic bags rather than the 10 cent ding for a dime routine. Require the stores to offer a free paper bag, and allow them to offer an upsell to a handled paper bag. reply wslh 16 hours agoprevI use to travel around the world and the difference between European countries and USA is abysmal regarding trash. It turns obvious in a few hours. I am not an expert in garbage though but it is a trivial observation that anyone travelling to those two regions could easily observe. I just felt a \"denial of service attack\" on Denmark trying to separate the garbage in different classes that I couldn't perfectly distinguish without some training. reply userbinator 12 hours agoprevThe biggest problem with these bans is that \"single use\" bags almost never are. Everyone I know reuses them for various things, and not just as bags. The case against plastic bags is straightforward. Plastic pollution kills at least 100,000 marine mammals and 1 million seabirds every year and entanglement in plastic and other types of litter kills roughly 1,000 turtles per year. That's something for littering laws to deal with. reply ezzaf 12 hours agoparentThey may get reused, but the data shows that: * Sales of bin liners (for example) do not significantly increase when single use plastic bags are banned * Plastic bag litter is significantly reduced. If you want less litter, banning single use plastic bags is a great way to achieve that Sources https://www.abc.net.au/news/specials/curious-canberra/2017-0... https://www.parliament.act.gov.au/__data/assets/pdf_file/000... reply gnicholas 11 hours agorootparentThe evidence regarding sales of bin liners is mixed: > The study found California communities with bag policies saw sales of 4-gallon trash bags increase by 55% to 75%, and sales of 8-gallon trash bags increase 87% to 110%. These results echo earlier studies that also showed increases in sales of smaller plastic trash bags. But while sales of small garbage bags jumped after policies were implemented, sales of larger 13-gallon trash bags -- the size often found in kitchen trash cans -- remained relatively unchanged. [1] 1: https://www.sciencedaily.com/releases/2022/03/220329142327.h.... reply Nition 11 hours agorootparentprevPersonally I can say I switched to just not lining the bin after the plastic bag ban. I have a separate compost bin for food scraps so the main bin mostly doesn't get too dirty, and if it does get some liquid or whatever on it, a quick rinse with the hose fixes that. reply prawn 9 hours agorootparentWe use a paper bag (from grocery deliveries) as our general household waste bin under the sink. Also under the sink are a compost bin and then a recycling tub. The compost and recycling tubs fill up far faster than the general waste bag. reply wkat4242 10 hours agorootparentprevYuck that would not work here in Spain.. you'd have cockroaches all over. And I don't have a place to hose things down :( reply chasil 12 hours agoparentprevAnother major problem is that bag bans have been found in studies to result in bacteriological infection spikes. https://www.huffpost.com/entry/plastic-bag-ban_n_2641430 For myself, I (re)use these as trash bags, so I hope the ban never reaches my community. My grocery store also has a used bag deposit. reply littlestymaar 11 hours agorootparent> authors received monetary support from the American Chemistry Council, a trade group representing the interests of plastic bag manufacturers. End of the joke. reply chasil 9 hours agorootparentThe joke could be on you. https://www.health.ny.gov/publications/2827/ reply tptacek 12 hours agoparentprevYes. You can find a bunch of estimates for how many times you have to reuse a canvas bag for the environmental cost of its production to net out. You generally have a choice between plastics, paper, and fabric, and fabric seems to be the worst of all the options, and the one the plastic ban encourages. I think I like the approach I see in Chicagoland, which is just to charge for the plastic bags. (We keep all our plastic bags, but then, we have two dogs). reply userbinator 11 hours agorootparentIronically, a lot of \"fabric\" bags are just woven plastic, which is worse than blown film plastic in not being waterproof and also readily absorbing dirt. And for those who are scared of microplastics, they certainly shed fibers. reply tptacek 10 hours agorootparentWe should put a stiff price on the plastic bags, and then ban fabric bags. reply chasil 12 hours agorootparentprevTyvek bags get sold commonly in my area, and Tyvek does not survive a standard washing machine cycle. A canvas bag used for groceries should be washed between each use. reply loeg 11 hours agorootparentI can't imagine washing a grocery bag between every use. Why? reply chasil 11 hours agorootparentSalmonella are endemic to chicken. E. Coli have also been found in reusable bags. Fungus and yeasts are also a problem. You don't want these on your salad greens, fruits, or vegetables. Reusing the bags without washing risks exposure to pathogens. https://polymerinnovationblog.com/reusable-grocery-bags-may-... reply loeg 9 hours agorootparentI don't know what grocery stores are like where you live, but where I live, raw meat comes pre-packaged in plastic, and it is conventional to put it in a 2nd plastic bag before it is bagged with other groceries. Also, I don't know where you live, but it's generally a good idea to wash raw fruits and vegetables before you eat them, whether or not you put them in bags that also contain meat. For all of those reasons and more, I would not bother washing grocery bags. reply littlestymaar 11 hours agorootparentprevSomewhere else in this discussion you posted a study financed by a plastic industry lobby, and now from a plastic chemistry blog, don't you see a pattern? reply chasil 11 hours agorootparentAre these references good enough? https://www.health.ny.gov/publications/2827/ https://www.huffpost.com/entry/plastic-bag-ban_n_2641430 \"The study, released in August, found a spike in San Francisco hospital emergency room treatment due to E. coli infections and a 46 percent increase in deaths from foodborne illness in the three months after the bag ban went into effect in 2007.\" reply littlestymaar 10 hours agorootparentauthors received monetary support from the American Chemistry Council, a trade group representing the interests of plastic bag manufacturers. > Are these references good enough? Well, what do you think? reply chasil 10 hours agorootparentI think that if you don't want to wash your reusable bags, then you are very much free to refrain. For the rest of us, there are reasons to consider doing so, as advised by the the New York State Department of Health above. reply littlestymaar 1 hour agorootparentIn general, because they are so stringent, the advice from the department of health don't mean much and are only relevant if you either: - have a depressed immune system (including if you're old) - are a young child It's not something you need to follow blindly when you're healthy. Heck half of the food you can find in the finest restaurant in France is recommended against by US department of health, for instance: - raw milk cheese - raw eggs - “undercooked” meat Sure, washing your bags can't hurt, but that doesn't make a good argument in favor of plastic bags either, and made up “studies” from lobbyist won't make it so. (Department of health also recognizes plastic bags as a choking hazard for infants BTW!) reply gnicholas 11 hours agorootparentprevI wonder what the environmental impact of washing a bag 100 times is, compared to 100 single-use plastic bags (which may be reused as trash bags). reply manmal 12 hours agoparentprevEvery country has littering laws, and they don’t work. reply dotancohen 12 hours agorootparentYou mean that they are not effectively enforced. reply rabuse 12 hours agorootparentHow would you enforce them? reply mixmastamyk 8 hours agorootparentThe “perp” is given a ticket. Comes with a fine that helps pay for further enforcement. reply michaelt 10 hours agorootparentprevWe'll simply put a serial number on every single-use plastic bag, and link it to a government-issued photo ID card when you accept the bag. There will be a mobile app allowing you to register the transfer of a registered plastic bag to another citizen, possibly using the blockchain. The police will then arrest the owners of lost bags in their copious free time. /s reply littlestymaar 11 hours agoparentprev> That's something for littering laws to deal with. How are littering laws supposed to prevent plastic bags from flying away from landfills? reply prawn 9 hours agorootparentA fair bit of litter I see (on roadtrips at least) is from overflowing bins at rest areas where the council is using poorly designed bins, people are over-stuffing them, animals are pulling things out looking for scraps, etc. The small amount of local litter appears to come from rubbish trucks tipping bins into the trucks on windy days. reply 266 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Plastic bag bans implemented at the local and state levels in the US have successfully reduced plastic bag consumption and avoided the use of billions of bags.",
      "Plastic bags are a significant contributor to pollution and harm marine life, making bans crucial in addressing the plastic waste problem.",
      "The report recommends implementing regulations that ban plastic bags and charge a fee for paper bags, highlighting the effectiveness of plastic bag bans and advocating for their adoption at state and local levels."
    ],
    "commentSummary": [
      "The collection includes comments and discussions on a range of environmental topics, such as plastic bag bans, gas stove regulations, straw usage, and waste management.",
      "Opinions differ on the effectiveness and consequences of these initiatives, with some expressing frustration, skepticism, or support.",
      "The importance of personal habits, convenience, and considering multiple environmental factors is highlighted, and commenters provide specific studies or personal experiences to support their arguments."
    ],
    "points": 249,
    "commentCount": 533,
    "retryCount": 0,
    "time": 1707056918
  },
  {
    "id": 39250896,
    "title": "The RIM-116 Rolling Airframe Missile: Advanced Last-Ditch Defense Against Anti-Ship Missiles",
    "originLink": "https://www.navalgazing.net/RAM",
    "originBody": "View Edit History Print February 04, 2024 RAM I’ve previously discussed Standard, Sea Sparrow, ESSM and Phalanx, but there is one last air-defense weapon that deserves discussion. This is the RIM-116 Rolling Airframe Missile, better known as RAM. A RAM is launched from USS Green Bay Much like the other point-defense systems, RAM’s origins trace back to Eilat, and the panic that it provoked within the USN. It was conceived to work in pretty much the same niche as Phalanx, providing a last-ditch defense against incoming anti-ship missiles. Effective as it was, Phalanx had a serious limitation, even while it was still in development. The use of a gun limited effective range to no more than 1500 yards, which was a serious problem in the face of supersonic missiles. The available window to engage such a weapon was short, and even if the Phalanx did shoot it down, the debris was likely to strike the defended ship. The obvious solution was to use a missile, which could engage at significantly longer range. The initial program that led to RAM was based on the Redeye missile, the first American man-portable SAM system, although it would be fitted with a combined radar/IR seeker to allow it to engage closing targets (which were difficult to engage purely with IR seekers at the time) at reasonable range. The only real concern was the small size of the missile, 2.75″ in diameter and about 18 lb, and Congress directed the Navy to study something the size of Sidewinder, 5″ and about 160 lb, instead. The initial contract was signed with General Dynamics in 1976, with West Germany coming onboard as a development partner. Development didn’t go particularly smoothly, with delays from testing and cost adding up to around 5 years, and both the US and Germany came close to withdrawing from the program at various points. But things were eventually worked out, and RAM entered in the early 90s. A RAM in flight RAM is a rather unusual missile. It gets its name because it is fired from a rifled tube and rolls in flight thanks to the tube and four fins. The roll allows it to use only two control fins in flight, instead of the usual four. The basic airframe, motor, fuze and warhead initially came from the Sidewinder, while the IR seeker was derived from the Stinger. Because RAM was expected to be fired at incoming targets, where the hot engine would not be visible, the seeker would need to rely on glint (reflected IR radiation from the sun), which sharply limited range. As a result, initial guidance would be provided by a passive RF system, which could home in on the radar seeker of a typical cruise missile.1 The rolling missile could also get away with a 2-sensor radar inferometer, instead of requiring four sensors, like a more conventional missile. The accuracy of the RF seeker was limited, hence the inclusion of the IR seeker, but it was in theory possible for the missile to use it all the way to the target if it’s dark or cloudy and the IR seeker doesn’t work. The wide cone of the RF sensor also allows the missile to be fired “around the corner”, reducing the size of sectors blocked by the ship’s structure by 10-15°. Sailors load a RAM launcher aboard the Truman But there were serious concerns about the effectiveness of RAM against missiles that used IR or semi-active homing, so even before the Block 0 missile entered service, work began on Block 1, with an imaging IR seeker that is capable of searching for targets on its own. It still has the RF seeker, and is capable of using the original dual-sensor mode, homing entirely on IR or switching to IR search if it loses RF track. Block 1 entered service in 1999, and achieved a 95% success rate in intercepting incoming missiles across 180 trials. A further upgrade took place in 2002, to give better performance against helicopters, slow aircraft and surface targets. This upgrade, known as HAS, was implemented entirely as a software upgrade. In the mid-2000s, a second upgrade, Block 2, was started. It was a considerably bigger overhaul than Block 1, with a new 6.25″ motor2 (which increased range significantly, from 6 nm to 10 nm), a 4-fin steering system, and an improved RF seeker. Block 2 was cleared for service in 2015, and is currently in production. SeaRAM onboard Japanese helicopter destroyer Izumo The fire-and-forget nature of RAM meant that it imposed relatively minimal burdens on the firing ship’s combat system, particularly given the ability of the missile to search for targets after launch. The combat system still mattered in terms of firing at the correct time and in the right direction, but it opened up new possibilities for smaller ships that couldn’t support Sea Sparrow or the like. The most extreme version of this was SeaRAM, which was essentially a Phalanx system with the gun removed and replaced by an 11-round RAM launcher. Like Phalanx, it is capable of operating independently of the ship that carries it, automatically detecting and engaging incoming missiles. SeaRAM is primarily carried by the Independence class LCS, although a few Burkes are also fitted with the system to provide some defense against cruise missiles when the main radar is in ballistic missile defense mode. A RAM launcher on the German missile boat Ozelot But the more common launcher is the 21-round Mk 49, a trainable launcher integrated with the ship’s combat system and fitted to a number of ships, including all American aircraft carriers and amphibious ships, as well as the Freedom class LCS. Germany has equipped all of its warships with RAM, while Egypt, Greece, Japan, Mexico, Qatar, South Korea, Saudi Arabia, Turkey and the UAE have bought the system for their ships as well. If anything, RAM seems destined for wider service in the years to come. It is the most minimal system capable of providing protection against anti-ship missiles, a threat that has been graphically demonstrated in the Red Sea in recent months, and which is only likely to continue to proliferate. 1 There are claims that it can home in on the radar altimeter for IR-homing missiles, but I can’t find good support for that claim, and Norman Friedman says the opposite. ⇑ 2 I believe that in launchers for Block 2, the rifling is removed and initial spin is imparted by an ablative vane behind the rocket motor. ⇑ by bean 3 comment(s) Comments February 04, 2024redRover said... Because RAM was expected to be fired at incoming targets, where the hot engine would not be visible, the seeker would need to rely on glint (reflected IR radiation from the sun), which sharply limited range. I assume that if the sun is down even the blocked exhaust plume of a straight in missile would stand out enough in IR to be useful? February 04, 2024bean said... It isn’t clear in my sources. That may be the case, or it may be that the RF seeker would need to be used at night. I suspect that the Block I missile is somewhat better at this. February 04, 2024friendlynokill said... Can’t believe I only just came across this site. Just spent an entire morning reading through past articles. Love your content. Comments from SlateStarCodex: Leave a comment All comments are reviewed before being displayed. Name (required): E-mail (required, will not be published): Website: You can use Markdown in comments! Enter value: Search login Useful Posts About Topical Index Top Posts Virtual Meetups Discord SSC Naval Gazing Posts Bibliography Link Index Recent Comments Can’t believe I only just came across this site. Just spent an entire morning re… ′ or ‘1’=’1… It isn’t clear in my sources. That may be the case, or it may be that the RF se… > Because RAM was expected to be fired at incoming targets, where the hot engine… @Martin… He probably meant it does not accelerate very fast. If it’s moving at 36.5 knots… Bean: You stated ” the Battleship doesn’t move all that fast’.… The jury is still deliberating, but will announce the winner shortly.… I’d like to find out more? I’d care to find out some additional information.… RSS Feed Categories Airlines Amphibious Armament Aurora Aviation Battles Battleships BB 61 Coastal Defenses Design Engineering Falklands ICNW Intro Jutland Links Logistics Merchant Modern Naval Gazing News Norway Nuclear Open Pictures Protection Review Riverine RTW 2 Russia Space SYWTBABB SYWTBAMN Undersea WW 2 WWI Page last modified on February 04, 2024, at 01:16 PM Minimous theme originally by Andy Thompson, adapted by David Gilbert powered by BlogIt",
    "commentLink": "https://news.ycombinator.com/item?id=39250896",
    "commentBody": "Rolling Airframe Missile (navalgazing.net)174 points by cwillu 18 hours agohidepastfavorite103 comments metadat 17 hours agoSo many great articles on this site. This rainy Sunday is in serious jeopardy of falling off the cliff and being sucked into the gravity well wormhole that is navalgazing.net. https://www.navalgazing.net/Phalanx I've always wondered how the Battleship guns with the white dome cylinder chamber above the gattling gun worked, but the background story is way more interesting than I expected. Apparently it's called a Phalanx, and no, a person doesn't sit in the white cylinder chamber*. * Despite Battlefield 2 leading me to believe a person occupied the cylinder, anyone remember Wake Island with the USMC carrier, F-35B, and J-10? haha, good times. reply whartung 16 hours agoparentIt's a radar directed gun. The dome thing is the radar. Once, a sailor was on top of one and the radar turned on, and it cooked the sailor. That radar has some real power. The Phalanx motto is \"it flies, it dies\". It has an autonomous function. Turn it on, and it kills anything in its airspace. However, that's never turned on. WAY too dangerous, there's always a man in the loop. There's two primary challenges for the Phalanx, also known as the CIWS (though this often pronounced \"see whiz\"). The first is finding the target in the first place, notably a sea skimming missile, picking it out of the surface clutter. The second is that, once identified, determining that its actually destroyed the target in a timely fashion so as to not keep pumping bullets into a flying debris field. The system uses the radar to track both the target and the stream of bullets. So, to the radar system, all of the bullets are \"tracers\" to help the system direct the fire into the target. Another issue with the Phalanx is that it's not armored, and the possibility of it getting hit by the shrapnel of the missile it took out, is not zero. Unlikely, depending on trajectories, but not zero. The missile may not explode, but it's still a bunch of metal hurtling at several hundred miles per hour. While designed for the naval environment, the machines have also been deployed, and used in combat, mounted on trailers to be used in forward land positions as point defense against missiles, low flying aircraft, and even artillery shells. The CIWS was recently used to down a Houthi missile in the conflict currently happening in the Red Sea. reply chasil 12 hours agorootparentWould the Phalanx have stopped a barrage of Neptune missiles, which were reported to have sunk the Moskva? Was the defensive weaponry on the Moskva unable to defend against the Neptune, or did the crew make mistakes in operating it? reply jasonwatkinspdx 10 hours agorootparentSo this category of weapon is called a Close In Weapon System. Moskva had 6 of these, the main russian CIWS: https://en.wikipedia.org/wiki/AK-630 It also had 40 short range surface to air missiles. No one knows the exact details of the strike, but it's been suggested that Ukraine used a drone as a diversion. Another speculation is that in photos of the ship after the attack, the main search radar is in the downward, stowed position. This may suggest it wasn't on at the time of the strike, or it may have been moved as part of damage control. All that said, incompetence has been a consistent theme with the Russian military in this war, and in particular with their black sea fleet. Ukraine has had repeated success attacking with small unmanned surface craft, basically Ski-Doo style boats they've modified for remote piloting using a thermal camera and Viasat, and with a bunch of explosives. There's video footage from these attacks on twitter and such, and while it's very unclear, in some of them it appears there's no watch standing on the ship being attacked, or they're shooting at it using small arms rather than the CIWS. None of this is definitive, but it is all consistent with poor training, apathy, and incompetence on the part of the crews. reply Tuna-Fish 9 hours agorootparentThe last readiness inspection document of the Moskva was leaked. To put it short, none of the defensive weaponry was working properly, none of it had worked properly for a decade, and no-one was doing anything about it. reply MilStdJunkie 7 hours agorootparentprevIn short, Moskva no worky. From the 10 February 2022 readiness report – Four of six generators needed repair; two were emergency use only. – S-300F Fort [SA-N-6] director illuminator nonfunctional – Both Osa-MA [SA-N-4] directors nonfunctional – One AK-630 Gatling guns nonfunctional; others unable to load without manual process. – All (3) MR-123 directors for AK-630 limited function. Before we get all cocksure of ourselves over here on the USN side, we should probably stop to consider how incredibly down readiness rates have been on our own surface fleet. Between the dysfunctional surface fleet culture, the GWoT madness, government shutdowns, and a complete inability to find sailors who know how to work a mouse, the manpower problem is also, now, beyond critical. Wasn't too long ago we were apparently ramming everything with a navigation light and a destination somewhere in SEA. It's not unrealistic to see a Moskva in our own future, if we're not careful (and maybe stop drinking our own Rah-Rah-USA-Always-Wins koolaid). reply adrr 10 hours agorootparentprevRumor was CIWS wasn’t working on the Moskva. Also some of radars interfered with each other so they kept some of them off. That was inferred off of pictures from the sinking and the radar used to detect sea skimming threats was in the stow position. There is also more countermeasures than ciws. Warships also can deploy decoys and chaff to confuse radar homing missiles. reply chasil 10 hours agorootparentIf ciws was not operable, then why was the Moskva positioned within range of the Neptunes? reply Aeolun 9 hours agorootparentI think the answer to that question can probably be applied to most of the Russian war effort. reply dilyevsky 11 hours agorootparentprevIt should’ve had a ciws system. Probably older kashtan—m (there’s a newer panstir-m system). My guess it was either inoperable or switched off reply actionfromafar 11 hours agorootparentprevIIRC Moskva was partly broken. reply whartung 6 hours agorootparentprevConceptually, yes, the CIWS could have been effective against the missile the struck the Moskva. The US had a similar incident, back in '87, with the frigate USS Stark. It was struck by two Exocet missiles launched by an Iraqi fighter during the Iran-Iraq war. The Stark was in international waters, and the attack was unprovoked. Bluntly, the ship was not expecting the attack, and was unprepared for it. While it was equipped with the CIWS, it was in standby mode. Also, the Stark mounted a single unit in the center of the ship, facing aft. As I recall the missiles actually struck the forward quarter of the ship. Even if the CIWS was enabled, I believe they would have had to turn the ship for it to engage the missiles. As for the Moskva, let me qualify I have no support whatsoever for what Russia is doing. That said, I do have a base level of respect for fighting forces, and the people who staff them and stand the line. I was shocked to hear about the Moskva. There's no reason those missiles should have struck that ship. That ship is particularly designed, much like our cruisers, to defeat threats such as those missiles. For the US Navy, a prime role for our cruisers is to defend not just themselves, but a fleet role. In simple terms, the cruisers are there to protect the carriers, and are bristling with systems and sensors designed for that task from several threats. The Moskva was similar to that role. The CIWS is the \"last resort\". The CIWS is for when missiles penetrate the other layers of defenses in place, including things like the combat air patrol, air-to-air missiles, etc. The CIWS is a \"last mile\" defense. The Moskva had layers as well. It should have taken several missiles to penetrate what the Moskva could have utilized. Obviously, being in the West, I do not hear flattering things about the Russian forces, their state of readiness, training, morale, etc., etc. Based on that knowledge, and the fact that the Moskva was lost, it's clear to me that that ship should not have been where it was. Here was a high level ship of war, in a war zone, not as a spectator, but as a participant, and it was not ready for that attack. That ship and her crew were not qualified to be there, not prepared to be there, and those service people deserved better. The Russian navy failed that crew, and they should not have been in harms way like that. The Ukrainian attack should have failed utterly. It shouldn't have even been close. The Stark was very embarrassing, and there are many critics of the modern US Navy. The Stark, however, was completely surprised, they weren't a combatant, they weren't posturing, the Stark was in the area. The Moskva should not have been surprised. They clearly did not anticipate the attack (as I understand it, everyone was surprised the Ukrainians had this capability), but it's not really clear if the ship was prepared and capable even if they did. There's stories that even after it was struck, the crew did not perform well and contributed to the loss of the ship. reply xarope 7 hours agorootparentprevI do enjoy the visuals from the TV series The Expanse, when they use their CIWS equivalent to intercept missiles. Albeit I've always wonders how fast those \"bullets\" fly, versus the (again I'd assume) hypersonic (is sonic meaningless as a benchmark when talking about space?) missiles. reply georgeecollins 7 hours agorootparentYes, sonic is meaningless as a benchmark in space. The speed of sound varies with air pressure (altitude) but it is always an important barrier in a gas atmosphere. However, in space it means nothing. A bullet fired into a gravitational well could just keep accelerating until it struck something or went into an orbit. reply echoangle 1 hour agorootparentSpeed of sound is not affected by pressure, only by temperature. It’s still variably with altitude though because the temperature is dependent on altitude in the atmosphere. reply poooogles 16 hours agoparentprev>...only a handful of cases in which ships equipped with Phalanx have been the subject of missile fire, and none that have actually seen the system tested. Worth noting that this line is now out of date. Allegedly they've been tested in Yemen recently [1]. 1. https://www.businessinsider.com/houthi-missile-close-us-wars... reply jhartwig 16 hours agorootparentI had thought it had been used during the gulf war in a friendly fire event. reply Simon_ORourke 16 hours agorootparentThe Missouri got raked with it from its escort ship - not pleasant for anyone concerned I'd say. http://billgx.com/2019/10/autonomous-friendly-fire/ reply thebruce87m 14 hours agorootparenttldr: the Missouri fired a chaff and the other ships CIWS was all “not in my airspace” and started shooting at it even though the Missouri was behind the target. Interesting, I wonder if the whole battlegroup has synced systems now to avoid such things, e.g. 1) I’m going to launch something, everyone else disregard it as a threat and 2) if a threat has a friendly behind it, don’t shoot it. Maybe 2) is more of a judgement call since a few bullet holes is probably preferential to the alternative. reply ethbr1 14 hours agorootparentIFF in any capacity on Phalanx seems like a poor match. At ranges that close... you really want a \"deconstruct anything in the air\" device, not a \"consider what you're about to hit\" device. Which I'd imagine has been designer pushback on complicating it and going that route. Safety through deciding what mode to set it in; not through leaving it alone and forgetting about it. reply thebruce87m 10 hours agorootparentI don’t disagree, but I’m sure they had to come up with some mitigation after this. If each ship is shooting down the other ships primary/secondary/whatever countermeasure then in a worst case scenario you and your sister ships are only left with your CIWS. And they might still be busy shooting at your sister ships chaffs to deal with the real threat. I don’t know much about it, I just found it interesting. Seems a bit like Star Trek - “Their shields go down for a split second when they fire”. Maybe the system goes dark for a second to avoid shooting down outgoing items? Syncing the shields might make this better (or expose other weaknesses?) reply ethbr1 8 hours agorootparentThe issue is time. Say it's a YJ-83 clone, so around mach 1.4 terminal velocity (476 m/s). Phalanx tracks at 10km, engages at around 4km. So it has ~8.4 seconds of engagement time to try to destroy the incoming missile. ... And that's a relatively slow modern target. A P-800 gets up to mach 2.4. BrahMos is mach 3? I'd guess the Navy would generally rather keep CIWSs on a hair trigger, and just space their ships further apart in combat situations. And afaik, most of the CIWSs have been removed in favor of RAM, since the gun-limited engagement ranges don't make much sense anymore. reply jhartwig 15 hours agorootparentprevYep that's the one reply thsksbd 16 hours agorootparentprevAnd the system failed, requiring the last line of defense machine guns to engage. reply aoki 15 hours agorootparentPhalanx is the US CIWS based on the 20mm Vulcan gun. in other words, Phalanx is the “last line of defense machine guns” to which you are referring. You may be thinking of Aegis, the integrated combat control system. We don’t yet know why the Gravely was unable to intercept the missile further out. Could have been human error, could have been a sea skimmer getting too close before being detected. To date, Aegis has been extremely effective against ballistic missiles and old cruise missiles. But given enough time an enemy can test your potential weak spots. reply WalterBright 15 hours agoparentprevI've wondered many times why a radar-directed machine gun isn't used to protect against drone attacks. The drones the terrorists used are rather slow moving, so the gun doesn't have to be that sophisticated. And it should be relatively cheap. reply antoniojtorres 14 hours agorootparentThat space is currently littered with all kinds of new projects given the urgency. One that has stuck with me is the German Rheinmetall Skynex which works the way you are describing. It’s like a really high tech flak cannon. There are neat videos of it on youtube. reply envalid 15 hours agorootparentprevIf a target is moving slow enough it can be difficult to track. Similar to how you can defend against radar missiles by 'notching' aka flying perpendicular so the relative velocity of the target is near zero. Radar typically relies on the dopplar shift caused by the target moving to eliminate clutter. reply wolverine876 14 hours agorootparentIf they can't see the target, how do they shoot missiles at it? I've wondered the same as the GP: Why not just send a wall of metal at the drone from one of these things? If not that, why not traditional anti-aircraft fire, with projectiles that explode in shrapnel clouds? reply ethbr1 13 hours agorootparent> Why not just send a wall of metal at the drone from one of these things? If not that, why not traditional anti-aircraft fire, with projectiles that explode in shrapnel clouds? If this had been the 1950s or early 60s, every advanced military on the planet would have been able to do exactly that. However, both aircraft (MiG-19+) and missiles were speeding up. At some speed, it becomes impractical to solve a high-speed aircraft or missile problem with a gun. [0] Consequently, development from the 70s on turned to missiles capable of dealing with these threats. Which left the only remaining systems mostly consequences of failure to upgrade (e.g. the German Gepard). As the saying goes, history doesn't repeat itself, but it rhymes... [0] See: M19 (1945) > M42 (1953) > MIM-46 (1960-63, cancelled) > M-163 (1965) > MIM-72 (1967) https://en.m.wikipedia.org/wiki/M19_Multiple_Gun_Motor_Carri... https://en.m.wikipedia.org/wiki/M42_Duster https://en.m.wikipedia.org/wiki/MIM-46_Mauler https://en.m.wikipedia.org/wiki/M163_VADS https://en.m.wikipedia.org/wiki/MIM-72_Chaparral reply Gravityloss 4 hours agorootparentThere were still helicopters and slower ground attack aircraft to do gun defense against, and self propelled anti aircraft guns (SPAAG) like Tunguska, Marksman, Pantsir. New advanced anti-drone guns like Skynex seem to be expensive and have expensive shells. Yet the range is quite small. Could work for high value targets, not to defend a country against large amounts of inexpensive drones? reply WalterBright 13 hours agorootparentprevThe drones used by terrorists are homemade and slow moving. reply ethbr1 12 hours agorootparentBut those weren't the primary threat military defense of ~1965-2020 was designed against. (Higher then) Lower, faster, and/or stealthier were the worries. And aside from the economically-mobilized war that Ukraine is fighting (and Russia is gradually shifting to), it's unclear if capability or cost need to be optimized. For every conflict economically less than that, capability wins. I expect the FPV quadcopter grenade-on-a-drone solution will be looked back on like the Toyota Hilux tactical -- effective when introduced, but superceded and dominated by specialized systems produced by military industry. reply WalterBright 15 hours agorootparentprevThe solution then would be to have two radars a distance apart, so they can triangulate. reply jcrawfordor 14 hours agorootparentLocating the target isn't the problem, you don't need a Doppler shift to calculate range by time of flight. The problem is detecting the target at all. Radar in these sorts of defense environments will pick up an enormous number of returns off of the carrying vessel and sea surf, which is actually a rather difficult problem for radar because it reflects in myriad directions, it moves, etc. By far the easiest way to select an \"interesting\" radar return is to use Doppler shift to find something that is moving very quickly. That can't be just a wave, it has to be a missile. The problem is that rotary wing drones are very slow (compared to missiles) and so they don't present an obviously different shift from the background. A lot of R&D is going on right now into better ways to select slow-moving, low-cross-section objects like drones from the background. This sort of thing is much more difficult for missile defense than the more conventional radar application of airspace surveillance, because for several reasons (including the fact that this makes radar detection hard) missiles tend to come at you from close to or below the horizon. This means you're getting a huge amount of clutter (radar returns from the environment) around them. reply icegreentea2 14 hours agorootparentNot to minimize the challenges, but I think it's worth pointing out that a lot of the drones of topical interest are -not- quadcopters. The Houthis (for example) have been using UAVs (Samad and Qasef) are prop driven fixed wing drones with total mass in the 50kg+ class (and warheads in the ~20kg class). They have max speeds of 200+ kph, wingspans in the 2-5m range. The Samad has ranges in the 1000km+ range. Obviously, they could slow down in terminal stages of attack, and I have no idea what their stall speed would be like. These are more akin to the Shahed class drones that Russia has been deploying in Ukraine (though still much smaller), than the FPV drones we see attacking tanks/trenches. reply WalterBright 13 hours agorootparentThe British used radar guided artillery to knock down wave after wave of V1s, which flew at around 400mph. Surely we could do better today. reply robbiep 12 hours agorootparentReally? I had heard that sometimes pilots would nudge them off course or shoot them down but nothing about artillery, I thought radar came along a bit too late to use it for targeting with regard to the V1 reply WalterBright 4 hours agorootparent\"Impact\" by Benjamin King and Timothy Hutton \"The answer to Pile's problems of directing the guns was the American S.C.R. 584 radar, the one used on the American 90mm anti aircraft gun, which had power elevation and traverse and an automatic fuse setter. The S.C.R. 584 was a gun-laying radar and \"the most successful single application of the micro-wave ten-centimeter technique to ground fighting in World War IL It could automatically track an unseen target at night or in cloud or fog, supplying range, azimuth and elevation data to a gun director.\" The S.C.R. 684 had a range of 90,000 yards for early warning, and as a target got within 32,000 yards the set acted as a gun layer. It had no blind spots and could detect low-flying targets like the Fi 103. Unlike the British radar sets, it was also immune to Window. However, it was a complex piece of electronic equipment and required a number of scarce materials like tantalum, molybdenum and tungsten, as well as 140 vacuum tubes which were then in short supply in the United States. The fielded version weighed several tons and cost $100,000.\" Impact, pg 174-175 \"Aside from the redeployment, one of the reasons for the gunners' success was that new equipment had arrived. Anti-Aircraft Command received 135 of the long-awaited S.C.R. 584 radar sets and Pile was able to \"borrow\" an additional 165. Adapting these to the static British 3.7-inch gun required 200 modifications to the gun. Along with the radar sets came proximity fuses and 20 American batteries armed with the radar-controlled 90mm gun.\" Impact, pg 207 \"When controlled by the S.C.R. 584 radar set, the U.S. 90mm Ml Antiaircraft Gun was the finest antiaircraft gun of World War II. During the campaign they were operated 22 hours a day with two hours a day for maintenance.\" Impact, pg 271 reply robbiep 1 hour agorootparentthat's amazing. Thanks reply a4000 5 hours agorootparentprevPilots chasing down V-1s who shot at them from directly behind would risk immediately flying at high speed into the wall of shrapnel created by blowing up essentially a large powered bomb, often destroying or damaging their own aircraft in the process, which was very dangerous. To avoid this, some pilots developed the tactic of flying along side the V1 then using their wingtip against the V-1 wingtip they would then flip it over and off course disrupting the primitive autopilot system so it would then spiral out of the control and hit the ground and explode hopefully in relatively harmless field. reply robbiep 1 hour agorootparentyes, that's exactly what I had absorbed from somewhere reply fbdab103 3 hours agorootparentprevAh yes, the safer alternative of nudging your plane against the flying bomb. reply jabl 2 hours agorootparentprevThey used both (and barrage balloons as well). The space between the ~middle of the English channel and London was divided into \"belts\", with one type of defense operating in each belt to prevent interference (like flak shooting down your own fighters). At first the flak guns were located near London (as that was the same flak guns that were protecting London against aircraft). It didn't work so well because it was very hard to hit a fast moving target like the V1, as crews not used to it tended to use too little lead. And even if the V1 was hit, chances were that the wreck would fall down on London anyway. So eventually the flak batteries were moved to the coast, and they got gun-laying radars and proximity fuses which dramatically improved the effectiveness of them. reply actionfromafar 11 hours agorootparentprevI thought so too, but lately I have heard and read accounts akin to a lot of what happened in late WW2 with regards to radar technology was highly classified and much didn't enter the history books. Pilots shooting and nudging definitely happened but likely other, more advanced stuff happened too. The germans had fully automatic radar controlled flak guns. reply lupusreal 11 hours agorootparentprev> Automatic gunlaying (using, among others, the SCR-584 radar) and the proximity fuze played an important part in Operation Diver, (the British operation to counter the V1 flying bombs). Both of these had been requested by AA Command and arrived in numbers, starting in June 1944, just as the guns reached their free-firing positions on the south eastern coast of England. Seventeen per cent of all flying bombs entering the coastal 'gun belt' were destroyed by guns in the first week on the coast. This rose to 60 per cent by 23 August and 74 per cent in the last week of the month, when on one extraordinary day 82 per cent were shot down. The rate increased from one V-1 for every 2,500 shells fired to one for every hundred. https://en.wikipedia.org/wiki/SCR-584_radar#Operational_use > 90mm anti-aircraft guns were normally operated in groups of four, utilizing the SCR-584 microwave computer and being controlled by the M9 Director. The SCR-584 was accurate to about 0.06 degrees (1 mil) and also provided automatic tracking. Direction and range information was sent directly to the M3 Gun Data Computer, and M9 Director, which directed and laid the guns automatically. All the crews had to do was load the guns. https://en.wikipedia.org/wiki/M9_Gun_Director reply robbiep 1 hour agorootparentWow. I honestly find it amazing what they accomplished back then. I spent like 6 hours reading the history of AEGIS when someone posted it here last year reply icegreentea2 12 hours agorootparentprevTotally. I just guess the US didn't procure/deploy/use/turn-on the appropriate systems. Plenty of other posts in the comments identifying potential modern (and modern-ish) solutions that match your parameters. reply nradov 10 hours agorootparentprevThe US Navy used radar guided artillery to knock down wave after wave of Japanese kamikazes, which flew at similar speeds. They were still sometimes overwhelmed by saturation attacks. Modern US surface warships still have the same capability but most mount only a single large cannon (separate from the shorter ranged CIWS). There's no space for more. Research is underway to supplement those with lasers but those aren't operational yet and can only work with a clear line of sight. reply sudosysgen 7 hours agorootparentprevThe Shaheds fly much lower. You aren't likely to be able to shoot at them until they're close, unlike V1s where you could just send wave after wave of flak and destroy most of them. Also, the drones are getting faster and faster, with the latest ones having turbojet engines. reply WalterBright 13 hours agorootparentprevI'm talking about a land based system to defend military bases from drone attack, not sea borne vessels. reply jasonwatkinspdx 10 hours agorootparentprevThey are. In Ukraine German Gephard[1] systems have taken out a variety of drones. The older soviet systems can do some of this as well. There's also a ton of newer smaller projects in development, aimed at something you can put on a truck to counter low cost consumer drones. [1]: https://en.wikipedia.org/wiki/Flakpanzer_Gepard reply icegreentea2 15 hours agorootparentprevThey are to some extent? One issue of radar-directed machine guns in the context of the Red Sea is that machine gun range is limited, so they can only provide point defense. This means that a warship can only protect itself, and any commercial ships that are -very- nearby. Yes, you could probably figure out a way to rig up a lot of these onto smaller ships to escort individual ships/convoys through, but hey you go to war with the navy you have. Unironically, the LCS's might be useful in this role. More generally, you see stuff like the army's new M-SHORAD having missiles (Stinger+Hellfire) and 30mm + 7.62mm guns. reply WalterBright 15 hours agorootparentI was thinking of the recent drone attack on a military base, where three soldiers were killed. reply icegreentea2 15 hours agorootparentOh. Yeah, I mean there's nothing preventing such a system - land based Phalanx is a thing. We also don't know what air defenses were even at the base (maybe they already had land based Phalanx...). Sounds like the base was caught by surprise. reply swagasaurus-rex 13 hours agorootparentThere was a friendly drone returning to base, so their defensive systems were turned off. reply wolverine876 12 hours agorootparentprevIsn't it the same problem - you need complete sensor and shooter coverage of the entire perimeter. Bases are pretty large - much larger than ships. Also, as Hamas did, an attacker can try to overwhelm one point of defense, which means your sensors, targeting computers, and shooters need lots of extra availability (extra equipment and soldiers) everywhere. I don't think you'd want the perimeter of a base covered with auto-targeting/firing guns. And also, on what does all that metal (bullets) land? reply NavinF 10 hours agorootparent> on what does all that metal (bullets) land? The Phalanx CIWS uses explosive incendiary tracer rounds on land. They self destruct after missing the target to avoid collateral damage reply jasonwatkinspdx 10 hours agorootparentprevIn that specific attack there was a problem where it coincidentally happened at the same time a US drone was landing, so there was some confusion over the threat. reply giantrobot 15 hours agorootparentprevThere is such a system, it's called C-RAM and is basically a Phallanx mounted on a trailer. You can find videos of them working at bases in Iraq recently. There's also smaller systems in development like M-SHORAD and MADIS (Army and Marines respectively if I remember right). Whether stuff like that is deployed to a particular base is a question of them being available and the particular location being well supplied enough to use them. The M-SHORAD and MADIS systems are designed to mount on light vehicles so they can be deployed more readily and to smaller forward positions. reply ranger207 14 hours agorootparentprevRadar is essentially a searchlight shining up into the night sky. If you have a radar-directed machine gun pointing into the sky, it's relatively easy to triangulate where the emitter is and direct artillery to take it out. reply Aeolun 9 hours agorootparentGot it. Need one that can take out artillery rounds as well then. reply ianburrell 13 hours agorootparentprevThe US has Mk38 canon on lots of ships. The Mod3 added radar. Its primary focus is against small boats and could probably work against USV that Ukraine has been using against Russia. I hope the Navy is working in modifying to work against small and medium drones. The other thing that might help against drones are the automated weapons stations hat are on lots of vehicles. I think I read about project to add radar and automated targeting of drones. reply avar 13 hours agorootparentprevI'd think the primary defense against drones on a naval vessel would be that the ship's made of metal, and doesn't have squishy humans wandering around (unless it's a carrier). IOW a very small drone also means a very small payload. reply windexh8er 13 hours agorootparentThe size of the payload is still a significant problem if your enemy is close with many drones. This video [0] shows a Russian missile cruiser being sunk by Ukrainian drones recently in the Black Sea. The problem these boats have with drones is that they are slow all around compared to a rather nimble drone which can react to its operations. As can be seen in the video multiple drones are used to sink the large ship. Edit: A better article around the situation and effectiveness of the drones is at [1]. [0] Video Russian missile cruiser \"Ivanovets\" destroyed by sea drones at January 31/February 1 in Black Sea - https://v.redd.it/hevs3v05hyfc1 [1] https://www.theguardian.com/world/2024/feb/01/ukraine-sea-dr... reply lazide 13 hours agorootparentI think they were referring to aerial drones, which tend to have limited capacities for cheap ones. Those were seaborne (naval) drones carrying about 1000 kg of explosives each if I remember correctly. reply tyingq 15 hours agorootparentprevMaybe hard to sort out small drones from large birds? reply foofie 11 hours agorootparentprev> I've wondered many times why a radar-directed machine gun isn't used to protect against drone attacks. You mean something like Germany's Gepard? https://en.wikipedia.org/wiki/Flakpanzer_Gepard reply a12k 10 hours agoparentprevWow, you're right. Every link I follow on the posted article leads me into a fascinating wasteland of other articles from the site, each better than the last. This is some extremely high quality content. reply thunderbird120 17 hours agoparentprevThe article on VLS is also quite nice https://www.navalgazing.net/VLS reply duxup 5 hours agorootparentThat adaptive deck launcher seems like you could almost drop that (well and some radar) on any ship and turn it into a warship… reply paulmd 13 hours agoparentprevPhalanx: “is it for me???” https://www.reddit.com/r/NonCredibleDefense/comments/13kd6m0... reply saiya-jin 15 hours agoparentprevWent for Phalanx article immediately too when I saw the link, I guess there is just something about that 20mm beast throwing massive tungsten slugs at even crazier speeds that no missile can compete with (for attracting readers). These things require super quick reactions, operator is optional (and thats why no real usage in combat, USN ends up turning it off often in situations where they don't want to risk absolutely destructive friendly fire. Now if everything will be eventually 'ai'-infused, well, fuck. Navy apparently doesn't trust automation with their lives much, and fancier neural network with unpredictable decision paths ain't gonna improve things. reply hef19898 14 hours agorootparentYou always want a human in the loop. FDS sometimes drives into trucks or disengages randomly. Automated defence systems shoot at stuff they consider to be a threat, and that can be anything from non-combatants to friendlies. Or just the enemy, but the particular attack might be a diversion, hence a human might hold the defence back. Also just for stealth, radar is a giant beacon leading right to back to you. As is a rapid fire 20 mm cannon. Unless we have General AI, in which case, well, you'd still want a human in the loop, just at different level. reply maxglute 8 hours agorootparentprev>crazier speeds that no missile can compete with CIWS muzzle velocity is ~1000-1200 m/s, ~mach3, substantially slower than modern hypersonic threats at ~mach5+. Hence counters to hypersonic missiles are other advanced interceptors. There were papers in the 80s detailing physical limits from mechanical tracking to over horizon response times and TLDR IIRC was CIWS not suitable for fast, low flying and slightly maneuverable anti ship missiles. reply wolverine876 14 hours agoprevAlways important to look critically at the person speaking (I mean to imply nothing about them, just sharing what I found): It [the blog] grew out of my time as a tour guide on the battleship Iowa (BB-61) in Los Angeles, and although I now live halfway across the country, Iowa is still one of the mainstays of this blog. It also came from my frustration with existing sources, which tended to be either way too high-level or too technical, with very little to bring an interested general audience up to speed. ... I've been a military geek since I was in grade school, although my affections didn't settle on matters naval until after I graduated college with a degree in aerospace engineering. ... Unfortunately, I didn't like my actual job, and ended up getting another job in Oklahoma, doing work on military aircraft. https://www.navalgazing.net/About Certainly they should understand how flying machines work. I dunno about their history. :) reply lapsed_pacifist 14 hours agoparentbean used to post long and in depth posts into the SlateStarCodex Open Threads, then someone helped him set up his own site, and here we are. He organizes meetups at Naval History sites too. reply isoprophlex 17 hours agoprevI'll just comment that that is the most clever website name I came across in a long time! reply wolverine876 14 hours agoprev> The use of a gun limited effective range to no more than 1500 yards Aren't sniper rifles effective at that distance (if they hit)? A weapon not constrained to human limits in dynamic movement, heat, sound, pressure, size, etc, isn't effective at a much greater distance? reply jabl 13 hours agoparentThe shell itself flies considerably longer, although 20mm shells start to slow down and drop much more quickly than even slightly bigger shells like 30mm (see e.g. the Goalkeeper CIWS for a similar concept using the same 30mm gun as the A-10 aircraft). It's a question of hitting the target with a usefully high probability. At longer ranges dispersion is higher, so even if the aim is correct there's a chance it won't hit. And think of how it works; it shoots out a stream of shells, and tracks the target and the shells with radar. But what if the target is doing some evasive maneuvers, which AFAIU many anti-ship missiles do in their final phases? The shells fly out at about 900 m/s, and a subsonic missile flies at maybe 300 m/s. So by the time the shells whizz by the missile it has moved a quite considerable distance, and while the radar tracking can apply the correct amount of lead (with a Kalman filter or similar to take into account measurement noise), if the missile is doing some unpredictable maneuvers that might not help. With the missile closing at 300 m/s, you don't have very many cycles of shoot-see-where-the-shell-goes-adjust-aim before the missile slams into the ship. And to make matters worse, there are also supersonic anti-ship missiles. Now evasive maneuvers in the high-density air at the surface isn't really a concept that works out for Mach 3 missiles, but, well, the missile compensates by flying as fast as the cannon shells. Hence stuff like this RAM thingy is seen as a better system against high-end anti-ship missiles. reply ranger207 14 hours agoparentprevSniper rifles are firing a single shot, the bullet is smaller, and snipers can take significant time to prepare. I've heard of snipers firing a few hundred yards away from their target first to see exactly how their shot will be affected by drop and wind and only then aiming at their target. Sniper rifles have much less stress on them imposed by firing the cartridge than a Phalanx; a 12.7x99mm cartridge is nothing compared to 20x102mm. Finally, a sniper is firing a single carefully aimed shot; a Phalanx is firing 75 rounds a second from six rotating barrels. reply wolverine876 13 hours agorootparentHow does that reduce effectiveness of a Phalanx against a drone? With all that metal flying, the pinpoint accuracy problem seems solved. reply icegreentea2 12 hours agorootparentThe M61 (the gun the Phalanx is built around) apparently has a 80% dispersion of ~5 milliradians (see this amazing reddit post complaining about DCS - note that the evidence points to a range of values - I think 5 milliradians is a fair summary https://old.reddit.com/r/hoggit/comments/eyddn3/m61_and_gau8... ). At 1500m that works out to a 75cm radius (I'm pretty sure its radius...) disc. Assuming your firing at a missile flying right at you, and assuming a Cold War era threat (so say a Kh-22 fired from a Tu-22M), you're talking about a 92cm diameter target (46cm radius) that you're trying to hit. In reality, things are worse because in addition to dispersion, your actual point of aim of your stream of bullets will be effected by external factors (like wind). You can see how in the scenario layed out above, you do actually have what seems to be a reasonable chance of landing a few hits when you're spewing out hundreds of rounds within seconds. But you can also see how quickly the odds go to crap as you push the range further and further. I think that gets to the reality of what the \"effective range\" value is. It is a rule of thumb designed to help weapons officers quickly determine how best to react in a scenario. Remember that CIWS was envisioned to be deployed in a scenario where USN ships would be subject to saturation attacks by supersonic anti-ship missiles from multiple bearings and sources. Phalanx itself only has a magazine of ~1000 rounds, which is good for maybe only 10s of firing, so you really do need to conserve ammo. reply unwind 14 hours agoparentprevI am no expert but I imagine one important tradeoff is between cartridge power (necessary for long range with sufficient energy) and cartridge size/mass, these systems have high rates of fire (because trying to hit very fast-moving targets) and that can't be easy with a giant cartridge. What happened to \"Metal storm\"? reply laverya 14 hours agoparentprevHow long does it take for a bullet to reach 1500 yards? Around 2 seconds. How confident do you have to be in the movement of your target to hit them with a 2 second lead? Very. (Or you can follow the CIWS methodology and throw quite a few bullets at once) The effective range isn't limited by the ballistics of the weapon (The 20 mm shells it fires will easily go 5+km), it's limited by actually hitting the damn thing you're aiming at. reply droro 17 hours agoprevI had always wondered whether there was a missile that was spin stabilized with minimal control surfaces like this one. They mentioned it only had two control fins, but not whether they are independent. It seems possible to steer a spinning missile using a single actuator if the control pulses are precise enough. reply schiffern 4 hours agoparent>spin stabilized with minimal control surfaces The modern RAM Block 2 has four independently-actuated fins for agility[0], so nowadays the spin stabilization isn't being used to reduce the number of control surfaces. Spin stabilization is still quite valuable. If an actuator fails (or worse, jams hard over) the missile won't veer off course, it just has reduced control authority. [0] https://en.wikipedia.org/wiki/RIM-116_Rolling_Airframe_Missi... reply icegreentea2 16 hours agoparentprevFrom https://secwww.jhuapl.edu/techdigest/Content/techdigest/pdf/... it seems that the two control fins are driven off a single shaft/motor - so they are not independent. The RAM uses single plane tracking - basically imaging that it's only ever steering in pitch. But because it's constantly rolling, \"pitch\" will sweep over both axes and control to zero. reply 13of40 13 hours agorootparentYeah (and apologies if I get this wrong after 30 years) with a rolling airframe, you can have two fins on the same axis that \"wiggle\" back and forth at the same frequency as the rotation of the missile. Direction control comes from the phase of the movement, so the fins are pushed in the direction you want to go when it rotates to that point. reply amenhotep 16 hours agoparentprevVikhr also does this, and Starstreak and M1156 both use a clutch to transfer force from a freely spinning fin assembly at the nose to the rest of the projectile when appropriate, achieving guidance without any movable control surfaces! Missile designers are very clever. reply Fatalist_ma 14 hours agoparentprevBTW That's a pretty common control scheme for Soviet and Soviet-derived ATGMs. https://thesovietarmourblog.blogspot.com/2021/07/soviet-atgm... reply saiya-jin 15 hours agoparentprevnewer versions have 4 fins... better read the actual article, pretty good stuff there reply lazyeye 15 hours agoparentprevJust curious....what else have you always wondered about? reply LispSporks22 15 hours agoprevI don’t think these systems will work against cheap, tiny drones or swarms of drones. eg. Russian ships vs Ukrainian boat drones. reply ranger207 14 hours agoparentProbably not. They were designed to take out multi-ton supersonic cruise missiles so they need a larger warhead than would be required against drone swarms. Of course, since they're so large and capable they certainly could take down _a_ drone, and then it becomes a question of how large the incoming swarm is. Now, if the incoming swarm is large enough, then either each drone carries a tiny amount of explosives, or each drone is large and expensive. The US Navy has been building ships to protect against mass swarms of incoming cruise missiles since about the 60s or so, so if the incoming drones are large enough then you could also use larger missiles against them and rely on the RAM only for whatever leaks through. reply cgearhart 11 hours agoparentprevThat’s what the lasers are for. https://www.nationaldefensemagazine.org/articles/2022/10/19/... reply vasili111 4 hours agorootparentIn foggy weather lazers are not working well. reply kcb 6 hours agoparentprevIs there a fundamental difference between a drone and an anti-ship missile? reply jabl 2 hours agorootparentIn the sense that both are flying things loaded with explosives heading towards your ship, no there's no \"fundamental\" difference between them. In practice, there's a world of difference. Drones are cheap and slow, but can be a big threat in a saturation attack style scenario. Do you want to spend a $1M+ missile to shoot down a $5000 drone?. Do you even have enough missiles to shoot down a swarm of 100 drones? An anti-ship missile is much bigger and more expensive, but flies at around 300 m/s for a subsonic one, and up to 900 m/s for supersonic ones, giving much less time to shoot it down. And while the missile per se isn't armored, the warhead has a thick casing designed to penetrate deep into the ship before exploding, so hitting the missile with some shrapnel at short range isn't gonna help as the wreckage of the missile will still likely continue on a ballistic path and hit the ship. And the warhead is often big enough to cripple a warship with a single hit (well, maybe a carrier is big enough to keep on trucking). reply mynameisnoone 3 hours agoprevTo shrink MIC budgets, using lasers to counter tactical missiles in clear weather will be significantly cheaper to operate than expending expensive missiles. Repeated drone and improvised rocket attacks shouldn't lead to an expensive war of attrition that only makes defense contractors rich while taking away budget from better uses. reply CafeRacer 17 hours agoprev [–] I wish you could control with arrows or `wasd`. Or if at least it showed a message explaining how controls work. Took me a bit to figure out controls on a laptop. reply qwertox 17 hours agoparent [–] Did my adblocker block out a game on that page or did we read different articles? reply throwanem 17 hours agorootparent [–] I believe that was meant for https://news.ycombinator.com/item?id=39189285, \"Missile Game\". reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The RIM-116 Rolling Airframe Missile (RAM) is a last-resort defense weapon designed to counter incoming anti-ship missiles.",
      "RAM was developed as a more versatile and effective alternative to the Phalanx system, which had limitations in range and debris damage.",
      "RAM utilizes a unique rolling flight mechanism and combines radar and infrared seekers for precise guidance. It has been integrated into naval systems worldwide and is expected to be deployed more extensively in response to the growing threat of anti-ship missiles."
    ],
    "commentSummary": [
      "The discussion on Naval Gazing.net examines the effectiveness and limitations of defense systems like Phalanx and RAM against sea skimming missiles and drones.",
      "The poor readiness and performance of the Russian warship Moskva during a Ukrainian attack is criticized, underscoring the significance of well-equipped and prepared defenses.",
      "Historical examples of using radar-guided artillery during World War II to shoot down V1 flying bombs are explored, emphasizing the need for more advanced and accurate defense mechanisms against modern drones, potentially including land-based systems and lasers."
    ],
    "points": 174,
    "commentCount": 103,
    "retryCount": 0,
    "time": 1707060292
  },
  {
    "id": 39249005,
    "title": "Rye: A Convenient Solution for Python Packaging",
    "originLink": "https://lucumr.pocoo.org/2024/2/4/rye-a-vision/",
    "originBody": "Armin Ronacher's Thoughts and Writings blog archive tags projects talks about Rye: A Vision Continued written on Sunday, February 4, 2024 In April of last year I released Rye to the public. Rye, both then and now, represents my very personal vision of what an improved Python packaging and project management solution can look like. Essentially, it's a comprehensive user experience, designed so that the only tool a Python programmer would need to interface with is Rye itself and it gets you from zero to one in a minute. It is capable of bootstrapping Python by automatically downloading different Python versions, it creates virtualenvs, it manages dependencies, and lints and formats. Initially developed for my own use, I decided to release it to the public, and the feedback has been overwhelmingly positive. When I introduced it, I initiated a discussion thread titled “Should Rye Exist” referencing the well known XKCD #929 which humorously comments on the proliferation of competing standards. I did not feel well throwing yet another Python packaging tool into the ring. Yet it exists now and has user. This standard issue however I think is helped a bit by the fact that Rye doesn't actually do any of these things itself. It wraps established tools: Downloading Python: it provides an automated way to get access to the amazing Indygreg Python Builds as well as the PyPy binary distributions. Linting and Formatting: it bundles ruff and makes it available with rye lint and rye fmt. Managing Virtualenvs: it uses the well established virtualenv library under the hood. Building Wheels: it delegates that work largely to build. Publishing: its publish command uses twine to accomplish this task. Locking and Dependency Installation: is today implemented by using unearth and pip-tools. As you can see, Rye is not revolutionary and it's not intended to be. Rye itself doesn't do all that much as it delegates all the core functionality to other tools in the ecosystem. Rye packages these tools together in a user-friendly manner, significantly reducing the cognitive load for developers. This convenience eliminates the need to learn about various tools, read extensive documentation, and integrate these components independently. Rye lets you get from no Python on a computer to a fully functioning Python project in under a minute with linting, formatting and everything in place. It is sufficiently opinionated that many important decisions are made for you. For instance it starts you out with using pyproject.toml and picks a wheel build system for you. It also picks the linter and formatter, and the preferred Python distribution and decides on a build tool. Defaults Matter Rye is designed to select the best tools for the job — it picks winners. Why does it do that? This approach is inspired by my admiration for the developer experience in the Rust ecosystem, particularly the seamless integration of rustup and cargo. Their functionality made me long for a similar experience within the Python community. Crucially the way this works in the Rust world does not mean that cargo does everything. When you run cargo build it invokes rustc, when you run cargo doc it runs rustdoc. When you invoke cargo clippy it runs clippy for you and so worth. Cargo is a manager that delegates the important work to bespoke tools that are improved by sometimes entirely different teams. This also means that tools can be swapped out if they are found to be not the right choice any more. The experience in the Rust world also showed me that excellent Windows support is just a must have. That's why Rye is not just a great experience on macOS and Linux, it's also excellent on Windows. I am convinced that the Python community is deserving of an excellent developer experience, and Rye, as it stands today, offers a promising beginning. My belief is supported by evidence gathered from conducting in-person user interviews and demos, where Rye was well received. In fact, every individual who I was able to give a guided tour of Rye was impressed by how swiftly one could start working with Python. Because it was demonstrably designed to avoid interference with any pre-existing Python configurations, Rye allows for a smooth and gradual integration and the emotional barrier of picking it up even for people who use other tools was shown to be low. That said, Rye is a one person project and it does not address the fundamental challenges of some of the issues we have in the Python ecosystem. It does not solve multi version dependencies, it does not offer better performance for the installation of dependencies. It does not help with distributing executables for end user applications or anything like this. However I am getting multiple signals that the time is right for a tool like Rye to not just exist, but also to rally a larger number of the Python community embrace some of these standardization ideas. What's Next? Chris Warrick recently wrote a blog post where he looked back at the last year of Python packaging that made the rounds on Twitter. It laments a bit that we did not make much of a progress in packaging and it also talks a bit about Rye and correctly points out that Rye does not have enough contributors (basically just me). That's not a healthy setup. I still don't really know if Rye should exist. It has not yet become established and there are plenty of rough edges. I personally really enjoy using it but at the same time every time I use it, I get reminded that it would stop existing if I did not invest time into it which in some sense is what keeps me going on it. However I would love to see the community converge to a Rye like solution, no matter where it comes from. Learn More Did I spark your interest? I would really appreciate it if you give it a try and give feedback: a 16 minute introduction to Rye Project Website User Guide and Documentation GitHub Project Discussion Forums Discord This entry was tagged announcement and python © Copyright 2024 by Armin Ronacher. Content licensed under the Creative Commons attribution-noncommercial-sharealike License. Contact me via mail, twitter, github or bitbucket. You can sponsor me on github. More info: imprint. Subscribe to Atom feed",
    "commentLink": "https://news.ycombinator.com/item?id=39249005",
    "commentBody": "Rye: A Vision Continued (pocoo.org)160 points by ksbrooksjr 23 hours agohidepastfavorite39 comments seanhunter 18 hours agoI've been using rye for all of my python projects and it has generally been great. There's a slightly annoying piece at the beginning when you first set up a project because \"rye init\" doesn't actually produce a valid empty project - you have to add a few lines to pyproject.toml, but it certainly beats a lot of the other things I've used. reply p5v 15 hours agoparentHow does Rye compare to Poetry? reply Narushia 14 hours agorootparentHere's a handy Venn diagram which shows where Rye currently sits in the ecosystem: https://alpopkes.com/posts/python/figures/venn_diagram.png Poetry and Rye mostly do the same things, but Rye additionally does Python version management. I was personally recently reminded* that not only should one use dedicated environments for their projects, but also lock their specific Python versions. I've used Rye so far for Python/package/env management, and it does the job just fine. * (I upgraded from Fedora 38 to Fedora 39, which also bumped the system Python version from 3.11 to 3.12. And all of my virtual environments said boom.) :') reply forrestthewoods 11 hours agorootparentI adamantly believe that software projects should never rely on system dependencies. It’s just a totally broken concept and fundamentally bad idea, imho. reply seanhunter 12 hours agorootparentprevWay faster to do dependency resolution for one. It also uses ruff for linting which is crazy fast also. So all in all it’s a better and more productive dev experience as far as I’m concerned. Ig also encourages you to lock down specific versions of python whuch helps prevent catastrophic dependency problems on system upgrades. Otoh it’s definitely newer and there are still rough edges from time to time. reply nindalf 18 hours agoprevI think it’s interesting that rye uses ruff (https://github.com/astral-sh/ruff) for linting and formatting. That’s the right call, and it’s also correct to bundle that in for an integrated dev experience. I had to guess, that’s the path that the Astral team would take as well - expand ruff’s capabilities so it can do everything a Python developer needs. So the vision that Armin is describing here might be achieved by ruff eventually. They’d have an advantage that they’re not a single person maintenance team, but the disadvantage of needing to show a return to their investors. reply the_mitsuhiko 17 hours agoparentI really just want the end result :) reply linsomniac 6 hours agoparentprevI just learned that \"ruff fmt\" exists. Why would I use that over black? I've been using black for a couple years now, I'm loathe to change but ruff has a lot of good press. reply nindalf 4 hours agorootparentSome people use ruff because it’s much faster. Others do it because it’s one tool instead of multiple. reply oritsnile 12 hours agoprevI really like Rye and have used it a lot. Lately I've been using pixi more and more because of its cross-platform locking support, since I develop on Mac and deploy mostly to Linux. It also supports all cobda packages, which can be a big advantage. reply halostatue 8 hours agoprevI welcome anything that brings sanity to Python packaging. I have had several projects that I have been working with and have tried Poetry, Hatch, PDM and just plain old `requirements.txt` and I have been longing for something that Just Works like Rubygems and Bundler or Mix (Elixir) or Cargo, or even something that Sort of Works like npm or yarn. I haven't tried Rye yet, mostly because from my (outsider's) perspective, Python version management is better handled externally like rvm, rbenv, or chruby for Ruby or rustup for Rust; I have recently shifted to using mise (formerly rtx) because unlike asdf it does not use shims by default. Is there a good case for Rye over PDM if you leave out Python version management? reply mikkom 18 hours agoprevI'm usually using conda (mamba) nowadays as it also has binary support so I can use cuda etc. easily for deep learning purposes. Is this something Rye can also do? reply claytonjy 16 hours agoparentDoesn't this work pretty well everywhere else now, too? Torch at least is easy to pip install, as is poetry if you skip all the versions that were mispackaged. It's unclear to me what the benefit of conda is these days, other than making it much harder to reproduce your environment on other machines. reply imjonse 14 hours agorootparentIt allows installing non-python packages too for one, like the CUDA runtime. reply kamikaz1k 17 hours agoparentprevWhat is binary support? reply aldanor 14 hours agorootparentBinary packages compiled for your platform. More generally, any non-Python packages (e.g. libwhatever, or some binary utilities). reply mikkom 17 hours agorootparentprevDirect support for binary packages, for example gpu specific non open source drivers etc reply theusus 17 hours agoprevRegarding one man project. I always think of contributing back to the community. But things are not at all simple. I wish developers of projects would do weekly knowledge sharing videos. Or at least record themselves going through the code. reply the_mitsuhiko 17 hours agoparentI have been considering doing that but I wasn’t sure if there was appetite. reply theusus 16 hours agorootparentNow, there is at least one stomach ;) reply smitty1e 12 hours agorootparentprevThe sooner python packaging has a sane story, the sooner people will wonder why it took so jolly long. For those lacking time/skill to contribute, how can we support? reply the_mitsuhiko 12 hours agorootparent> For those lacking time/skill to contribute, how can we support? Improving documentation, spreading the word, giving feedback. Honestly anything helps. reply simonw 15 hours agoparentprevAre you looking for material that helps you understand the process open source developers use to create and maintain their projects? reply elbear 14 hours agorootparentI'm guessing he's talking about project-specific knowledge. reply theusus 17 hours agoparentprevAlso, I keep hitting myself in the wall when installing 3.12 Python reply VagabundoP 12 hours agoprevI'm about to do a full rewrite/refactor of my flask app and will be starting fresh with Rye. I was using a cobbled together pyenv+virtualenv+bunch of other tools to manage it all, but was spending too much time on environment setup/upkeep when I was not on my main laptop. Rye is a one stop shop for everything that I need. reply tootie 11 hours agoprevNot to dig on this project, but I find it constantly befuddling how awful the Python dev ecosystem is. And has barely improved after a decade of being so popular. The ergonomics of coding with Java or C# in 2010 are still so far beyond anything in Python in 2024, I constantly question why it's so popular. reply Daishiman 8 hours agoparentIf you have to question yourself that then you seriously lack experience with the language. reply edfletcher_t137 14 hours agoprevThis is an impressive effort, no doubt. And Python packaging is in a woeful state. But I'm sorry, the last thing the ecosystem needs it another alternative! https://chriswarrick.com/blog/2023/01/15/how-to-improve-pyth... https://xkcd.com/927/ reply forrestthewoods 13 hours agoparentAsk me how I know you didn't read the article. reply edfletcher_t137 12 hours agorootparentLOL I surely did, it even explicitly says: \"I did not feel well throwing yet another Python packaging tool into the ring. Yet it exists now and has user. (sic)\" TL;DR: I knew it was a bad idea, but I did it anyway. But I wouldn't have had to, anyway. Because I know how names work: \"Rye\" is not in the set of current named tools. Doesn't matter that it wraps them. It becomes one by doing so, and now we have 15. reply forrestthewoods 11 hours agorootparentYour first comment irked me because it adds zero value to the discussion. You lazily threw out XKCD 927 which the Rye author explicitly mentioned themselves. If you click into their link \"Should Rye Exist\" [1] you'll see that XKCD 927 is literally the first sentence and full width image. [1] https://github.com/mitsuhiko/rye/discussions/6 reply edfletcher_t137 10 hours agorootparentI included it because the author did, and irked me by openly admitting they recognize the argument but went against it anyway. There is good reason that particular XKCD hits so hard. Chris Warrick has done a ton of thinking - and a lot of writing [1] - on the subject and has very sage things to say about it. But unlike the author, he didn't decide to write another one. Because there are many good reasons not to. Thanks for letting me know you didn't bother reading that follow-up though: pretty clear who is providing zero value here :shrug: [1] In fact, here's more he has to say about it: https://chriswarrick.com/blog/2024/01/15/python-packaging-on... reply the_mitsuhiko 10 hours agorootparentI'm not entirely sure what you're suggesting other than \"accept the terrible status quo\". For what it's worth my personal use of rye predates the release by multiple years (not in this particular manifestation) and I held off releasing it for a very long time. reply edfletcher_t137 10 hours agorootparent> The PyPA should shut down or merge some duplicate projects, and work with the community (including maintainers of non-PyPA projects) to build One True Packaging Tool. To make things easier. To avoid writing code that does largely the same thing 5 times. To make sure thousands of projects don’t depend on tools with a bus factor of 1 or 2. -- https://chriswarrick.com/blog/2024/01/15/python-packaging-on... reply the_mitsuhiko 10 hours agorootparentI mean, I can also try to tell the PyPA to do a thing or two but that won't magically make it work. The PyPA is a bunch of separate individuals doing their thing and they are free to listen to people or ignore them. I do not have the power (nor do I believe I should) to force them to do a specific thing. reply odiroot 16 hours agoprev [–] Going for pip-tools, twine, virtualenv and build makes sense. But I wished he went for black instead. Ruff is not there yet. reply carso 15 hours agoparentWhen did you last evaluate it? I have been using ruff for advent of code (low stakes project) — last year it still felt a little beta-ish, this year it seemed more ready for prime time. reply _ZeD_ 15 hours agoparentprev [–] ruff seems better than black to me reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Rye is a Python packaging and project management solution that automates tasks like downloading Python, managing dependencies, and publishing.",
      "It aims to provide a convenient user experience by wrapping established tools and reducing cognitive load for developers.",
      "While it is not a revolutionary solution, Rye has received positive feedback and offers a promising start, inspired by the developer experience in the Rust ecosystem. However, it is currently a one-person project and does not address all the challenges in the Python ecosystem. The author hopes to gather more contributors and promote standardization ideas within the community."
    ],
    "commentSummary": [
      "Users on pocoo.org are highly satisfied with the Rye tool for Python projects, despite some minor issues with initial setup.",
      "Rye is compared to Poetry and is praised for its Python version management capabilities and similar features.",
      "There is a general agreement on the need for improved Python packaging tools and discussions around challenges related to system dependencies. Support for Rye is encouraged through feedback, documentation improvement, and spreading awareness."
    ],
    "points": 160,
    "commentCount": 39,
    "retryCount": 0,
    "time": 1707041712
  }
]
