[
  {
    "id": 39737281,
    "title": "Testing Grok-1 Open Weights Model: Download, Install, Run",
    "originLink": "https://github.com/xai-org/grok",
    "originBody": "Grok-1 This repository contains JAX example code for loading and running the Grok-1 open-weights model. Make sure to download the checkpoint and place ckpt-0 directory in checkpoint. Then, run pip install -r requirements.txt python run.py to test the code. The script loads the checkpoint and samples from the model on a test input. Due to the large size of the model (314B parameters), a machine with enough GPU memory is required to test the model with the example code. The implementation of the MoE layer in this repository is not efficient. The implementation was chosen to avoid the need for custom kernels to validate the correctness of the model. Downloading the weights You can download the weights using a torrent client and this magnet link: magnet:?xt=urn:btih:5f96d43576e3d386c9ba65b883210a393b68210e&tr=https%3A%2F%2Facademictorrents.com%2Fannounce.php&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce License The code and associated Grok-1 weights in this release are licensed under the Apache 2.0 license. The license only applies to the source files in this repository and the model weights of Grok-1.",
    "commentLink": "https://news.ycombinator.com/item?id=39737281",
    "commentBody": "Grok (github.com/xai-org)937 points by pierre 14 hours agohidepastfavorite354 comments ilaksh 5 hours agoHas anyone outside of x.ai actually done inference with this model yet? And if so, have they provided details of the hardware? What type of AWS instance or whatever? I think you can rent like an 8 x A100 or 8 x H100 and it's \"affordable\" to play around with for at least a few minutes. But you would need to know exactly how to set up the GPU cluster. Because I doubt it's as simple as just 'python run.py' to get it going. reply zone411 4 hours agoparentIf you're just looking to test it out, it's probably easiest to wait for llama.cpp to add support (https://github.com/ggerganov/llama.cpp/issues/6120), and then you can run it slowly if you have enough RAM, or wait for one of the inference API providers like together.ai to add it. I'd like to add it to my NYT Connections benchmarks, and that's my plan (though it will require changing the prompt since it's a base model, not a chat/instruct model). reply logicchains 3 hours agorootparent>it's probably easiest Cheapest maybe, but easiest is just to rent a p4de.24xlarge from AWS for a couple hours to test (at around $40/hour..). reply zone411 2 hours agorootparentI'd expect more configuration issues in getting it to run on them than from a tested llama.cpp version, since this doesn't seem like a polished release. But maybe. reply a_wild_dandan 3 hours agoparentprevSomeone could run Grok-1 on a 192GB M2 Mac when a 4-bit quant is released; I'm guessing that TheBloke is already working on it. reply mohu 3 hours agorootparentFairly sure the bloke hasn't created any new quants in a month. reply hanselot 2 hours agorootparentprevTheBloke dissapeared near the day https://nvd.nist.gov/vuln/detail/CVE-2024-23496 was published. Of course there has been much speculation on this, I have no more information than this that can be backed up by facts, but the timing was suspicious. reply pixelesque 57 minutes agorootparentHe's started a company in the UK: https://suite.endole.co.uk/insight/company/15361921-thebloke... Interestingly registered just around the corner from where one of my relatives used to live. reply moffkalast 52 minutes agorootparentAnd his grant funding supposedly ran out. reply oezi 2 hours agorootparentprevWas any .gguf file hosted on HuggingFace found to be crafted in a way to exploit this? reply extheat 14 hours agoprevAt 8x86B, looks like the largest open model yet by far. Would be interesting to hear how many tokens it's been trained on. Especially important for higher param models in order to efficiently utilize all those parameters. reply swalsh 13 hours agoparentConsidering how poor it is compared to other models, it really emphasises how important fine tuning is. Models with MUCH smaller parameter counts are outperforming it in many metrics. reply lukan 13 hours agorootparent\"it really emphasises how important fine tuning is\" Or rather the quality of the training data? reply llm_trw 11 hours agorootparentWe don't know since no one is releasing their data. Calling these models open source is like calling a binary open source because you can download it. Which in this day and age isn't far from where were at. reply DreamGen 11 hours agorootparentA big distinction is that you can built on top (fine-tune) thus released models as well as if they released the pre-training data. reply tarruda 9 hours agorootparentYou can fine tune without the pre training data too. Mistral models are one example, they never released pre training data and there are many fine tunes. reply llm_trw 11 hours agorootparentprevYou can also build on top of binaries if you use gotos and machine code. reply shwaj 3 hours agorootparentThis seems intentionally obtuse. What you say is true, but it is very obvious that this is much more of a pain than if you had the source code. On the other hand, fine tuning is just as easy, regardless of whether you have the original training data. reply samus 2 hours agorootparentOne could also disassemble an executable and build on top of it. Not for the faint of heart and probably illegal, but possible unless it was deliberately obfuscated. Compared to that, it is impossible with state-of-the-art methods to systematically extract the training data from an LLM model. Fragments yes, but not all of it. reply visarga 1 hour agorootparentYou can do better - generate synthetic data covering all topics. And to make it less prone to hallucination, use RAG or web search for reference material. The Phi-1.5 model was trained on 300B of synthetic tokens generated with chatGPT and it showed a 5x bump in efficiency, punching well above its line. Synthetic data can be more diverse if you sample carefully with seeded concepts, and it can be more complex than average web text. You can even diff against a garden variety Mistral or LLaMA and only collect knowledge and skills they don't already have. I call this approach \"Machine Study\", where AI makes its own training data by studying its corpus and learning from other models. reply adrianN 2 hours agorootparentprevOr shell scripts reply swalsh 11 hours agorootparentprevWe should just call it open weight models at this point. reply cl3misch 3 hours agorootparentprevFWIW the Grok repo uses the term \"open weights\". reply boulos 4 hours agorootparentprevHow about \"weights available\" as similar to the \"source available\" moniker? reply fragmede 4 hours agorootparentweights available or model available, but yes. reply drexlspivey 11 hours agorootparentprevTheir data is the twitter corpus which is public. Or do you want a dump of their database for free too? reply minimaxir 10 hours agorootparentTwitter tweet data in itself is both highly idiosyncratic and short by design, which alone is not conductive towards training a LLM. reply llm_trw 10 hours agorootparentprevSaying \"It's just the twitter public corpus.\" is like saying \"Here's the Linux Kernel, makefiles not included.\" reply zx8080 8 hours agorootparentOr even \"here's the Linux Kernel makefiles, no sources included, enjoy\". reply jahsome 10 hours agorootparentprevnext [2 more] [flagged] drexlspivey 10 hours agorootparentRequiring a company to publish their production database for free is delusional. I haven't mentioned musk anywhere in my comment, you must be obsessed with him. reply fragmede 12 hours agorootparentprevthat's a subtle dig at the fact that they have all of Twitter as a training corpus to use, but we don't know how they weight tweets. which, we know they're not gonna be weighted evenly. reply rezonant 12 hours agorootparentI'm sure just like in X's algorithms, @elon tweets are weighted heavily. reply convery 12 hours agorootparentThe X algorithm is also opensource, so you can verify before commenting.. reply threeseed 8 hours agorootparentX algorithm Github project hasn't been updated in 8 months: https://github.com/twitter/the-algorithm So clearly they aren't running it in production. Also they didn't open source the list of people who are being artificially boosted e.g. Elon. reply fragmede 11 hours agorootparentprevjust because they open sourced it doesn't mean that's actually what they're running on it though reply chrisco255 11 hours agorootparentIt's not like he needs boosting, he was one of Twitter's top followed accounts long before he bought them. He's pretty good at getting attention. reply latexr 10 hours agorootparentAnd yet it’s not enough to curb the desire to tip the scales. https://arstechnica.com/tech-policy/2023/02/report-musk-had-... reply lukan 11 hours agorootparentprevNo idea about the current state, but the open sourcing did show, they were favoring elon: https://mashable.com/article/twitter-releases-algorithm-show... And personally I never used Twitter much, but I certainly did not follow Elon Musk when I did - yet I had to see lot's of his posts in my feed. Surely just coincidence. reply maccaw 9 hours agorootparent> they were favoring elon No, and that's not what the article says either. They were just tracking how well his tweets were doing versus others. They were not favoring Elon. reply lukan 3 hours agorootparent\"They were just tracking how well his tweets were doing versus others. \" Yeah, and adjusting it, so he comes out best. That was Musks demand, as the other article shows, that is linked inside, after a Biden tweet performed better than Musk: https://mashable.com/article/elon-musk-super-bowl-joe-biden-... They officially boost people, who pay a little bit. Elon payed a lot. And the source is clearly not the production source and never where in this shape - otherwise why sue someone, who open sourced it? \"But, the release of this source code also comes days after Twitter forced Github to take down other parts of Twitter's source code that was allegedly posted by a former employee without the company's permission. So, clearly, there's still plenty of Twitter that Musk still doesn't want us to see.\" Also, you probably missed that: \"Zoë Schiffer of Platformer reported that Twitter actually removed part of the source code that affected the reach of Musk's and other user's tweets before releasing the algorithm to the public.\" Which is consistent with quite some other statements, also from Twitter itself and the fact, that the source has not been updated in 8 months. See also this HN comment and discussion about it: https://news.ycombinator.com/item?id=35391854 \"But the underlying policies and models are almost entirely missing (there are a couple valuable components in [1]). Without those, we can't evaluate the behavior and possible effects of \"the algorithm.\"\" reply machdiamonds 10 hours agorootparentprevIt's not too hard to believe it is a coincidence when the most followed person on a platform shows up in your feed, especially if you follow tech accounts. reply internetter 10 hours agorootparentDid you not read the article linked in the comment you're replying to? reply jokethrowaway 9 hours agorootparentprevSounds a bit far fetched So changes in power users stats would also result in audience balancing? Most likely the code was used for analytics and for tracking balance; Elon was a pain in the ass and asked to have custom analytics for his account and devs eventually added him as an audience to be able to get analytics about him easily. A bit dirty but it works. Most likely the balancing code is somewhere else and it affects only republican / democrats. reply nonethewiser 9 hours agorootparentprev> I'm sure just like in X's algorithms, @elon tweets are weighted heavily. Are you sure or is it the literal opposite and you’re just speculating? reply GaggiX 11 hours agorootparentprevOr even how much it was trained on this dataset, the amount of FLOPs. reply gdiamos 52 minutes agorootparentprevShow the proof? Does it include IFT? reply lairv 11 hours agorootparentprevI would say it emphasises that training a good model is more than throwing random data and compute reply make3 5 hours agorootparentprevno it empathizes the importance of training smaller models for longer, like the Mistral \"overtrained\" models reply zone411 10 hours agoparentprevIt's actually not the largest. https://huggingface.co/google/switch-c-2048 is 1.6T parameters. reply p1esk 13 hours agoparentprevIt’s not 8x86B. Total number of parameters is 314B. Perhaps it’s 8x39B to fit on a single 8xA100 (40GB) server? reply dheera 10 hours agorootparentThey all do this marketing bull. Mixtral has an 8x7B model but it's actually 46.7B, not 56B params. Kinda similar to how 4K displays are 3840 pixels wide, not true 4K which would be 4096. Marketing people called it 4K, not engineers. reply guitarlimeo 2 hours agorootparentI've always thought of 4K as \"4x FullHD\". In that way it makes sense. reply mavhc 1 hour agorootparentTV and Digital Cinema have different standards, because of course they do reply cma 13 hours agorootparentprevActive parameters is 86B, so wouldn't that be the size of the largest two experts (where they may all be the same) + the weights of the selector? reply moffkalast 13 hours agorootparentprevMost likely it's a MoE of Grok-0 which would be 8x33B + 50B for the router. reply nasir 3 hours agoprevI'd be very curious to see how it performs especially on inputs that's blocked by other models. Seems like Grok will differentiate itself from other OS models from a cencorship and alignment perspective. reply joydeep314 6 hours agoprevModel weights on huggingface: https://huggingface.co/xai-org/grok-1 reply cl3misch 21 minutes agoprevLove the minimal repo, magnet link, and stating \"open weights\" instead of \"open source\". Refreshing! reply simonw 13 hours agoprev\"Base model trained on a large amount of text data, not fine-tuned for any particular task.\" Presumably the version they've been previewing on Twitter is an instruction-tuned model which behaves quite differently from these raw weights. reply nylonstrung 14 hours agoprevFor what reason would you want to use this instead of open source alternatives like Mistral reply rvnx 14 hours agoparentMistral opened their weights only for very small LLaMA-like model. reply MallocVoidstar 13 hours agorootparentI'm pretty sure Mixtral outperforms Grok-1 and uses much less memory to do it reply cavisne 13 hours agorootparentOne of the interesting things when weights are open sourced is the community can often improve the results. See all the bugs fixed in Gemma for an example. reply ein0p 9 hours agorootparentDoubtful, for purely information theoretic and memory capacity reasons. It may outperform on some synthetic metrics, but in practice, to a human, larger models just feel “smarter” because they have a lot more density in their long tail where metrics never go reply elfbargpt 13 hours agorootparentprevI'm a little out of touch, is there a way to see how Grok measures up to other models? reply amrrs 13 hours agorootparentBenchmarks here https://x.ai/blog/grok reply refulgentis 13 hours agorootparentAnd to compare, you can sort by MMLU on here: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderb.... Edit: to include my self summary after review: There's a good 100 models better than, a couple 1x7b even. Mixtral stomps it, half mixtral are universally better but one is close to same. reply lossolo 12 hours agorootparentThis benchmark is mostly worthless, some of the top models there were trained on benchmark data, which is a known fact in the community. The only reliable benchmark: https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboar... reply refulgentis 9 hours agorootparentNo, it's not \"mostly worthless\" and yes, some of the top models were removed a few months back from being trained on benchmark data. I urge you to at least think through what alternative you propose before posting so aggressively in these situations. Lmsys doesn't have Grok, or I would have included it. And having _some_ data is better than none. I also had someone arguing with me 6 months back that we can't trust any benchmarks at all from vendors, which would exclude the blog post. Instead of just repeating that back vehemently, I filled a gap. It's important we don't self-peasantize as a species, all data has its issues, that doesn't mean we throw it all out. reply zozbot234 10 hours agoparentprevIsn't this Apache licensed? Regardless, you can run multiple models concurrently on the same input using well-known ensemble techniques. (Not to be confused with mixture-of-experts, which is more like training a single model where only a few blocks are chosen to be active at any given time - a kind of sparsity.) reply verticalscaler 13 hours agoparentprevWell if nothing else, this one might be significantly less nerfed. Very interesting to compare to the others. reply refulgentis 13 hours agorootparentnext [18 more] [flagged] benreesman 11 hours agorootparentI’ve been known to get snippy on HN from time to time myself :) So please know that I’m only offering a gentle nudge that I’d want from a fellow long-timer myself regarding a line of discussion that’s liable to age poorly. Talking about sorting hats for those who do and don’t have the one-percenter AI badge isn’t a super hot look my guy (and I’ve veered dangerously close to that sort of thing myself, this is painful experience talking): while there is no shortage of uninformed editorializing about fairly cutting edge stuff, the image of a small cabal of robed insiders chucking in their cashews while swiping left and right on who gets to be part of the discussion serves neither experts nor their employers nor enthusiastic laypeople. This is especially true for “alignment” stuff, which is probably the single most electrified rail in the whole discussion. And as a Google employee in the diffuser game by way of color theory, you guys have a “days since we over-aligned an image generation model right into a PR catastrophe” sign on the wall in the micro kitchen right? That looked “control vector” whacky, not DPO with pretty extreme negative prompt whacky, and substantially undermined the public’s trust in the secretive mega labs. So as one long-time HN user and FAANG ML person to another, maybe ixnay with the atekeepinggay on the contentious AI #1 thread a bit? reply gopher_space 10 hours agorootparentEvery discipline has its bellwether topics. They’re useful for filtering out people who want to chip in without picking up the tools. reply whimsicalism 9 hours agorootparentprevregardless of whether they say it out loud, it is what many of us think - might be good for people to know why their opinions are getting immediately dismissed by insiders reply benreesman 9 hours agorootparentLetting people know how why their opinions are getting dismissed in a productive way is done by citing well-known sources in low-effort way, or by explaining things thoughtfully in a high-effort way: Karpathy has chosen the highest-effort way of most anyone, it seems unlikely that anyone is at a higher rung of \"insiderness\" than he is, having been at Toronto with (IIRC) Hinton and Alex and those folks since this was called \"deep learning\", and has worked at this point at most of the best respected labs. But even if folks don't find that argument persuasive, I'd remind everyone that the \"insiders\" have a tendency to get run over by the commons/maker/hacker/technical public in this business: Linux destroying basically the entire elite Unix vendor ecosystem and ending up on well over half of mobile came about (among many other reasons) because plenty of good hackers weren't part of the establishment, or were sick of the bullshit they were doing at work all day and went home and worked on the open stuff (bringing all their expertise with them) is a signal example. And what e.g. the Sun people were doing in the 90s was every bit as impressive given the hardware they had as anything coming out of a big lab today. I think LeCun did the original MNIST stuff on a Sun box. The hard-core DRM stuff during the Napster Wars getting hacked, leaked, reverse engineered, and otherwise rendered irrelevant until a workable compromise was brokered would be another example of how that mentality destroyed the old guard. I guess I sort of agree that it's good people are saying this out loud, because it's probably a conversation we should have, but yikes, someone is going to end up on the wrong side of history here and realizing how closely scrutinized all of this is going to be by that history has really motivated me to watch my snark on the topic and apologize pretty quickly when I land in that place. When I was in Menlo Park, Mark and Sheryl had intentionally left a ton of Sun Microsystems iconography all over the place and the message was pretty clear: if you get complacent in this business, start thinking you're too smart to be challenged, someone else is going to be working in your office faster than you ever thought possible. reply refulgentis 9 hours agorootparentnext [2 more] [flagged] benreesman 8 hours agorootparentThen don't link to an \"About Me\" page [1] that says you do? How is confusion on that subject any reader or commenter's fault? I don't care if you personally work at Google or not, Google got itself in quite a jam as concerns public perception of their product in particular and the AI topic in general by going overboard with over-alignment, everyone knows that so one assumes that insiders know it, which is one of a great many examples of how strongly-forced models are a real problem for arbitrarily prestigious insider-laden labs. Framing the debate about whether large, proprietary models are over-aligned or mis-aligned as an acid test for whether or not someone is worth paying attention to is really weird hill to stand on. [1] https://www.jpohhhh.com/about reply refulgentis 9 hours agorootparentprevnext [2 more] [flagged] benreesman 9 hours agorootparentI've been around here a pretty long time, but I could still be off base here: as far as I understood people generally posted links to their own blog [1] in their HN profile because they want people to read them? I read your blog and particularly the posts about Gigadiffusion because I wanted to reply from a position of having put some effort into understanding where the poster I was replying to was coming from before popping off with what could be taken as a criticism. If that offends you or creeps you out I'm more than happy to steer clear of it with the parting remark that I really like Material and had hoped that any follow up would give me the opportunity to compliment you on some nice work. If that's not your blog, you should probably take it off your profile? [1] https://www.jpohhhh.com/ reply random_cynic 5 hours agorootparentprevThe 1% who actually work on AI don't use terms as generic as \"AI\". Way to reveal yourself as college undergrad who read a couple of popular science books, downloaded MNIST data and thinks they are \"experts\". reply renewiltord 12 hours agorootparentprevThe safety crap makes the tools unusable. I used to have a test for it that I thought was decent, but Claude failed that test and it is way better than ChatGPT-4 for code, which means my test was bogus. The people actually working in AI are kind of irrelevant to me. It's whether or not the model will solve problems for me reliably. People \"actually working in AI\" have all sorts of nonsense takes. reply benreesman 11 hours agorootparentAnother day, another fairly good comment going grey on an AI #1. The over-alignment is really starting to be the dominant term in model utility, Opus and even Sonnet are both subjectively and on certain coding metrics outperforming both the 1106-preview and 0125-preview on many coding tasks, and we are seeing an ever-escalating set of kinda ridiculous hot takes from people with the credentials to know better. Please stop karma bombing comments saying reasonable things on important topics. The parent is maybe a little spicy, but the GP bought a ticket to that and plenty more. edit: fixed typo. reply threeseed 8 hours agorootparentprev> The safety crap makes the tools unusable For you that may be the case. But the widespread popularity of ChatGPT and similar models shows that it isn't a serious impediment to adoption. And erring on the side of safety comes with significant benefits e.g. less negative media coverage, investigations by regulators etc. reply wmidwestranger 4 hours agorootparentSeems like marketing and brand recognition might be some confounding variables when asserting ChatGPT's dominance is due to technical and performance superiority. reply not_really 9 hours agorootparentprevlol, okay reply verticalscaler 13 hours agorootparentprevnext [5 more] [flagged] refulgentis 13 hours agorootparent(not sure you're going to edit again, but in the current one, I'm not sure what Google's silly stock image warning has to do with anything, and I have generally chosen to avoid engaging people doing their politics hobby via AI discussion, since it became okay to across the ideological spectrum of my peers. So, mu is my answer.) And you're right, I was really surprised to see the harder right people throwing up their hands after the Gemini stuff. reply verticalscaler 13 hours agorootparentnext [4 more] [flagged] itishappy 13 hours agorootparentWouldn't have even noticed had you not pointed it out. reply refulgentis 13 hours agorootparentprevFeel free to explain! You caught my attention now, I'm very curious why it's on topic. Preregistering MD5 of my guess: 7bfcce475114d7696cd1d6a67756761a reply verticalscaler 12 hours agorootparentnext [2 more] [flagged] refulgentis 12 hours agorootparentNo I didn't, at least, I don't think it did but it does sound exactly like me. But then again, I don't know what it'd have to do with anything you said specifically. https://pastebin.com/yfUWZMmc, idk if it's right because you kinda just went for more free association. reply mlindner 7 hours agorootparentprevCurious why you're so dismissive of something that's pretty important? reply pogue 14 hours agoprevCan someone explain why the weights are posted via a Bittorrent magnet link? I have no way to check the size at the moment, but isn't that a bit unusual? There's also only 21 seeders right now according to https://checker.openwebtorrent.com/ reply monkin 13 hours agoparentIt's 318.24G https://academictorrents.com/details/5f96d43576e3d386c9ba65b... reply fzzzy 12 hours agoparentprevIt may become a tradition since weights are so large. Perhaps it started when the Llama torrent link leaked. Then, Mistral decided to release their weights using bittorrent. reply bongodongobob 13 hours agoparentprevI'm not sure why you wouldn't tbh. That's a lot of bandwidth. reply ur-whale 1 hour agoparentprev> Can someone explain why the weights are posted via a Bittorrent magnet link? I think the best way to get an answer to that question is to try to host it yourself and see what happens. reply raydev 11 hours agoparentprevSpreads the burden/cost of distributing a 300+GB file. reply leumon 12 hours agoparentprevMistral did it too when they released their first open model. They just posted a magnet link on Twitter. reply MallocVoidstar 13 hours agoparentprevDistributing 300GB via torrent is cheaper than direct, assuming even a few other people seed reply seydor 2 hours agoparentprevmy optimistic explanation is we are going back to the 2000s internet , but probably we are not reply CamperBob2 14 hours agoparentprevHow else could/should it be done? reply pogue 13 hours agorootparentI would have assumed they could just upload it to Github. If it has restrictions on file size I'm sure they could make multiple part compressed files. Torrents can unfortunately die after a period of time if no one continues seeding it or if they don't use a permanent web based seeder, which doesn't appear to be the case. reply simonw 13 hours agorootparentGitHub have a soft repository size limit of 5GB, documented here: https://docs.github.com/en/repositories/working-with-files/m... Soft size limit means \"If your repository excessively impacts our infrastructure, you might receive an email from GitHub Support asking you to take corrective action.\" - I know people who have received such emails. Most model releases happen through Hugging Face which does not have such a size limit. reply KomoD 12 hours agorootparentThey'd probably just charge you for it. They sell \"data packs\" for LFS. https://docs.github.com/billing/managing-billing-for-git-lar... reply rezonant 11 hours agorootparentprevI'd bet Hugging Face would be happy to have hosted these canonically too, so not sure why that doesn't happen more. reply osanseviero 11 hours agorootparentThe model is also at https://huggingface.co/xai-org reply larrysalibra 13 hours agorootparentprevThe great thing about torrents is that you (or anyone else who cares) can single-handedly solve the problem you're complaining about by seeding the torrent. reply sashank_1509 13 hours agorootparentprevNo git would be impossible. I’ve never seen a repo even a few GB in size, if you are uploading non code files you really should not be using git. Git is a version management software for code. I often see repos which images and even videos checked in, please don’t, there are so many far better and more performant solutions out there. The other approach would be to use AWS S3 or other cloud providers which would cost them money every time someone downloads their code, which is not their prerogative to pay for when they are releasing something for free. Torrents seems like the only good solution, unless someone hosts this on the cloud for free for everyone. reply sroussey 12 hours agorootparentHuggingface will disagree with impossible as their models are available via git, sometimes broken up in pth files. Still, as far as sentiment goes, yeah git for model weights is an impedance mismatch for sure! reply rezonant 11 hours agorootparentprev> No git would be impossible. I’ve never seen a repo even a few GB in size, if you are uploading non code files you really should not be using git It's not actually a limitation in git itself, especially if you use Git LFS. People use Git for Unreal projects and big ones can be half a terabyte or more in size. reply djhn 6 hours agorootparentprevScott Chacon (github cofounder) mentioned in a recent talk that the Windows repo is 300GB https://youtu.be/aolI_Rz0ZqY?si=MOo2eS6dsKKAxmsP reply rezonant 11 hours agorootparentprevOthers have pointed out that GitHub doesn't allow that, but > Torrents can unfortunately die after a period of time if no one continues seeding it or if they don't use a permanent web based seeder, which doesn't appear to be the case. So to can web links, especially when they are 300 GB and egressing out of AWS at $0.09/GB or worse (in non-US regions). Each full download would cost $27 at that rate. 10,000 downloads would cost $270,000. Sure you could go for something with a better cost model like R2, but you can't beat using one or two unmetered connections on a VPN to constantly seed on Bittorrent, your pricing would be effectively free and reliability would be higher than if you just exposed a HTTP server on the Internet in such a way. reply KomoD 10 hours agorootparent> and egressing out of AWS at $0.09/GB There's a lot of seeders on the torrent that are actually AWS ips too, all with similar configurations which makes me believe that it's probably xAI running them > on a VPN That's unnecessary, you don't need a VPN? reply rezonant 9 hours agorootparentNo you don't, but if you wanted to host it from your gigabit office IP, you probably would want to. reply KomoD 7 hours agorootparentWhy? reply cedws 13 hours agorootparentprevGitHub may choose to throttle downloads or remove the files simply because they're taking up too much bandwidth. A torrent is less likely to go down in the short term. reply xcv123 13 hours agorootparentprevThis is not some crappy DVD rip on The Pirate Bay. It will be seeded as long as its relevant. Twitter/X has their own massive infrastructure and bandwidth to seed this indefinitely. reply KomoD 12 hours agorootparentYeah, they can just leave some server running somewhere and just let it seed forever reply pooloo 14 hours agoparentprevIts likely over 100GB of data, so I wouldn't say its necessarily unusual to spread out the bandwidth across multiple hosts. reply pogue 13 hours agorootparentThanks! I searched and searched for a tool that would show me info via the web about a magnet link but nada reply lambdaba 14 hours agoparentprevWhy not? Mistral was first to do it, it has become tradition. reply orlp 11 hours agorootparentBitTorrent is just an objectively superior method of delivering a lot of data to a lot of people. reply gillesjacobs 14 hours agorootparentprevI believe it was Llama 1 that notoriously got leaked with a torrent on 4chan. reply jiripospisil 13 hours agoparentprevI don't understand why you're being downvoted for asking a legitimate question. People not familiar with model weights might be surprised that they are often in tens of gigabytes and in this case even more. reply modeless 12 hours agoprevIs this the first major model to be natively FP8? I was wondering why people hadn't done it yet. Seems like a big win when hardware supports it. reply a_wild_dandan 3 hours agoparentNo, e.g. Yi-34B. reply modeless 3 hours agorootparentAs far as I can tell Yi-34B is natively 16 bit float, the 8 bit version is quantized. https://huggingface.co/01-ai/Yi-34B#quantization reply tosh 14 hours agoprevblog post: https://x.ai/blog/grok-os * 314B parameters (86B active at a time) * mixture of experts 8 (2 active at a time) * weights and architecture licensed under Apache 2.0 (edit:) announcement blog post from last year with benchmarks compared to Claude 2, GPT-3.5 and GPT-4: https://x.ai/blog/grok (edit2:)TL;DR: somewhat comparable to GPT-3.5, Mixtral and Qwen-1.5-72B in capability but way larger than the open weight models reply OkGoDoIt 13 hours agoparentIs a model so huge that’s only at the level of GPT 3.5 actually good? That seems incredibly inefficient to me. reply fwlr 11 hours agorootparentOpenAI is valued at 90 billion and all they do is make GPT; Twitter is valued at 40 billion and this was essentially a vanity side-project by a cowboy CEO. Presuming that benchmarks and general “it’s about the level of 3.5” is accurate, it’s inefficient, but not incredibly inefficient imho reply thekhatribharat 3 hours agorootparentxAI is a separate entity, and not a X/Twitter subsidiary. reply pelorat 10 hours agorootparentprev> Twitter is valued at 40 billion WAS vaulued at 44B. Now? Maybe 5 billion. reply alvah 7 hours agorootparentLOL @ $5 billion, but if it that was the valuation, you'd be making parent's point stronger. reply wongarsu 8 hours agorootparentprevLast I heard they lost 15% of their users, so let's call it 36 billion. reply dilyevsky 5 hours agorootparentThey weren't even 44B when elon took the keys - he specifically tried to back out of the deal because 44B was insane peak '21 asset bubble price. In truth they were probably like 10-15B at that moment. And now that bunch of advertisers left due to we know who it's probably about 10B reply mceachen 8 hours agorootparentprevMore like $13b. https://arstechnica.com/tech-policy/2024/01/since-elon-musks... reply wraptile 7 hours agorootparentprevTwitter didn't have direct competitors other than Mastodon when it was taken at 44B. Now there's Threads, Bluesky and bigger Mastodon. reply jsight 5 hours agorootparentHonestly, none of those look like meaningful competitors at the moment. reply squigglydonut 4 hours agorootparentprevNone of these matter reply drak0n1c 12 hours agorootparentprevIt’s designed to be actively searching real-time posts on X. Apples and oranges. reply grey8 11 hours agorootparentWhy is that relevant to the size? Post search on X is done as it is with any other data from any other source, you use RAG and function calling to insert the context.The cover image was generated using Midjourney based on the following prompt proposed by Grok: A 3D illustration of a neural network, with transparent nodes and glowing connections, showcasing the varying weights as different thicknesses and colors of the connecting lines. reply TOMDM 13 hours agoparentprevMixtral is also comparable to gpt 3.5 and open. At 8x7B it's also a fraction of the size. Are there any benchmarks comparing Mixtral to Grok? reply tosh 13 hours agorootparentMixtral announcement is here: https://mistral.ai/news/mixtral-of-experts/ Mixtral looks more economical @ capability to size (similar also for Qwen 1.5 72b) reply tootie 13 hours agoparentprevHow is it that OpenAI was touted like it was some massive years-long effort that blew all AI research out of the water and now we have so many competitors popping up one after another? reply longdog 13 hours agorootparentYou don't need to be a cutting edge research scientist to train a SOTA LLM. You just need money for scaling. OpenAI's \"secret\" was just their willingness to spend tens/hundreds of millions without guaranteed returns, and RLHF/instruct fine tuning, both of which are out of the bag now. reply simonw 12 hours agorootparentDisagree. It took more than 12 months from the release of GPT-4 to someone else producing a model of equivalent quality, and that definitely wasn't due to a shortage of investment from the competition. There's a huge amount of depth in training a really good LLM. Not helped by the fact that iteration is incredibly expensive - it might take several months (and millions of dollars) before you can tell if your new model is working well or if there was some mistake in the pipeline that lead to a poor quality result. Almost all of the world-class LLMs outside of OpenAI/DeepMind have been trained by people who previously worked at those organizations - giving them invaluable experience such that they could avoid the most expensive mistakes while training their new models. reply barrell 4 hours agorootparentWhile I do agree there is some amount of secret sauce, keep in mind the training takes several months. So from someone to see the success of GPT4, decide they want to invest that amount of money to train the same, raise the money to train the model, find someone competent to supervise the training, train the model for several months, then test and integrate it could easily be a year long even if there was no secret sauce. reply int_19h 1 hour agorootparentprevThere's still no model of equivalent quality to GPT-4. reply johnthewise 1 minute agorootparentClaude opus is better in my experience bbig 4 minutes agorootparentprevClaude 3 Opus is reporting superior metrics, particularly in its coding ability, and in the LLM Arena it is statistically tied with GPT-4. lossolo 12 hours agorootparentprevDon’t overlook the training data (used for both training and instruction fine-tuning), it is one of the most crucial aspects, if not the most critical, given the significant differences observed in models with similar architectures. reply echelon 12 hours agorootparentprevThat only remains an advantage if they can continue climbing the gradient from their lead position. If they hit a snag in scaling, methodology, or research, everyone else on the planet catches up, and then it's anyone's game again. reply cavisne 13 hours agorootparentprevLLM training is arcane and expensive to experiment with. So OpenAI had to waste a lot of time and GPU-hours on things that didn't work to learn the tricks that did work. Most of the competitors have lineage straight back to OpenAI, eg the lead of x.ai was previously at OpenAI and Deepmind. Likewise with Mistral and especially Anthropic. reply jxy 12 hours agorootparentprevOpenAI still seems to be at the top, except for Anthropic, who may be close, in terms of the capabilities comparing gpt-4 and claude-opus. This Grok-1 is a large model (~314B), which matches gpt-3.5 released 2 years ago, and at about the same level of much smaller models like, mixtral (~47B) and qwen-1.5 (~72B). Do you think it's competitive? reply ben_w 13 hours agorootparentprevEgg of Columbus. Also, the general architecture is well documented, ChatGPT (specifically the chat interface, not GPT-3, not InstructGPT) is what made a lot of people care, and actually reproducing it requires someone wanting to in the first place. reply hubraumhugo 14 hours agoprevWhen will we reach an upper limit/dimishing returns in terms of number of parameters and mixture of experts? reply andy99 14 hours agoparentWe may have already - data is more important than anything else which is why nobody has beat GPT4 yet. Throwing more parameters or more compute at the problem only gets you so far. But Grok was never a contender so there is room to improve on it. It is one of the biggest models open sourced as mentioned, so will be interesting to take a look at for sure. reply lambdaba 14 hours agorootparentClaude 3 has *decisively* beat GPT-4, I wonder how all their attributes compare. reply orbital-decay 12 hours agorootparentHas it, though? LMSys Arena Leaderboard (blind ranking by humans) [0] positions Opus just below GPT-4 with a negligible ELO gap. [0] https://chat.lmsys.org/ reply espadrine 12 hours agorootparentA number of AI companies have a naming/reproducibility issue. GPT4 Turbo, released last November, is a separate version that is much better than GPT-4 (winning 70% of human preferences in blind tests), released in March 2023. Claude 3 Opus beats release-day GPT-4 (winning 60% of human preferences), but not GPT-4 Turbo. In the LMSys leaderboard, release-day GPT-4 is labeled gpt-4-0314, and GPT4 Turbo is labeled gpt-4-1106-preview. reply BoorishBears 9 hours agorootparentprevChatbot Arena is not a blind ranking. Many, if not most, users intentionally ask the models questions to tease out their canned disclaimers: so they know exactly which model is answering. On one hand it's fair to say disclaimers affect the usefulness of the model, but on the other I don't think most people are solely asking these LLMs to produce meth or say \"fuck\", and that has an outsized effect on the usefulness of Chatbot Arena as a general benchmark. I personally recommend people use it at most as a way to directly test specific LLMs and ignore it as a benchmark. reply swalsh 13 hours agorootparentprevI don't know if Claude is \"smarter\" in any significant way. But its harder working. I can ask it for some code, and I never get a placeholder. It dutifully gives me the code I need. reply lambdaba 13 hours agorootparentIt understands instructions better, it's rarer to have it misunderstand, and I have to be less careful with prompting. reply stainablesteel 13 hours agorootparentprevi like some of claudes answers better, but it doesnt seem to be a better coder imo reply simonw 13 hours agorootparentI've found it to be significantly better for code than GPT-4 - I've had multiple examples where the GPT-4 solution contained bugs but the Claude 3 Opus solution was exactly what I wanted. One recent example: https://fedi.simonwillison.net/@simon/112057299607427949 How well models work varies wildly according to your personal prompting style though - it's possible I just have a prompting style which happens to work better with Claude 3. reply asciii 13 hours agorootparent> according to your personal prompting style though I like the notion of someone’s personal prompting style (seems like a proxy for those that can prepare a question with context about the other’s knowledge) - that’s interesting for these systems in future job interviews reply bugglebeetle 13 hours agorootparentprevWhat is your code prompting style for Claude? I’ve tried to repurpose some of my GPT-4 ones for Claude and have noticed some degradation. I use the “Act as a software developer/write a spec/implement step-by-step” CoT style. reply simonw 12 hours agorootparentAlmost impossible to describe prompting style, but here are some examples of how I've used Claude 3: https://gist.github.com/simonw/4cecde4a729f4da0b5059b50c8e01... - writing a Python function https://gist.github.com/simonw/408fcf28e9fc6bb2233aae694f8cd... - most sophisticated example, building a JavaScript command palette https://gist.github.com/simonw/2002e2b56a97053bd9302a34e0b83... - asking it to refactor some existing code I don't use the \"Act as a X\" format any more, I'm not at all convinced it has a noticeable impact on quality. I think it's yet another example of LLM superstition. reply lgas 12 hours agorootparent> I don't use the \"Act as a X\" format any more, I'm not at all convinced it has a noticeable impact on quality. I think it's yet another example of LLM superstition. It's very contextually dependent. You really have to things like this for your specific task, with your specific model, etc. Sometimes it helps, sometimes it hurts, and sometimes it does nothing at all. reply bugglebeetle 12 hours agorootparentprevSuper helpful! Thanks! reply furyofantares 11 hours agorootparentprevI didn't know people were still doing this \"act as etc etc\" instructional prompting. I just tell it my coding problem. Or when making something from scratch, ask for small things and incrementally add. reply furyofantares 11 hours agorootparentprevI've found it significantly better than GPT4 for code and it's become my go-to for coding. That's actually saying something, because there's also serious drawbacks. - Feels a little slower. Might just be UI - I have a lot of experience prompting GPT4 - I don't like using it for non-code because it gives me to much \"safety\" pushback - No custom instructions. ChatGPT knows I use macos and zsh and a few other preferences that I'd rather not have to type into my queries frequently I find all of the above kind of annoying and I don't like having two different LLMs I go to daily. But I mention it because it's a fairly significant hurdle it had to overcome to become the main thing I use for coding! There were a number of things where I gave up on GPT then went to Claude and it did great; never had the reverse experience so far and overall just feels like I've had noticeably better responses. reply htrp 13 hours agorootparentprevcitation needed (other than 'vibes') reply squigz 14 hours agorootparentprevI think Groq is something else? reply LorenDB 14 hours agorootparentIndeed, Groq is a company building inference accelerators. Grok is completely unaffiliated. reply andy99 13 hours agorootparentprevEdited, I did mean the Grok in the article not the inference chip. reply YetAnotherNick 13 hours agorootparentprevThere is no reason to believe GPT-4 had more(or higher quality) data than Google etc. has now. GPT-4 was entirely trained before the Microsoft deal. If OpenAI could pay to acquire data in 2023, >10 companies could acquire similar quality by now, and no one has similar quality model in a year. reply austhrow743 13 hours agorootparentThe more disregard a company has for intellectual property rights, the more data they can use. Google had far more to lose from a \"copyright? lol\" approach than OpenAI did. reply supafastcoder 3 hours agorootparent> Google had far more to lose from a \"copyright? lol\" approach than OpenAI did. The company that scrapes trillions of web pages has an issue with copyright? reply brookst 13 hours agorootparentprevI was under the impression training was at best an undefined area of IP law. Is there any aspect of copyright that prohibits training models? reply simonw 12 hours agorootparentThis is being tested by a number of lawsuits right now, most notably the NY Times one: https://nytco-assets.nytimes.com/2023/12/NYT_Complaint_Dec20... The key questions are around \"fair use\". Part of the US doctrine of fair use is \"the effect of the use upon the potential market for or value of the copyrighted work\" - so one big question here is whether a model has a negative impact on the market for the copyrighted work it was trained on. reply sroussey 12 hours agorootparentI don’t think the New York Times thing is that much about training, than it is about the fact that ChatGPT can use Bing and Bing has access to New York Times articles for search purposes. reply simonw 12 hours agorootparentIf you read the lawsuit it's absolutely about training. The Bing RAG piece is one of the complaints in there but it's by no means the most important. Take a look at https://nytco-assets.nytimes.com/2023/12/NYT_Complaint_Dec20... - bullet points 2 and 4 on pages 2/3 are about training data. Bullet point 5 is the Bing RAG thing. reply sroussey 10 hours agorootparentAh, thanks! reply YetAnotherNick 6 hours agorootparentprevHaving used both Google's and OpenAI's models, the kind of issue they have are different. Google's models are superior or at least on par in knowledge. It's the instruction following and understanding where OpenAI is significantly better. I don't think pretraining data is the reason of this. reply ldjkfkdsjnv 13 hours agorootparentprevClaude > GPT4. Anyone using these models on a daily basis knows this reply int_19h 1 hour agorootparentI use these models regularly, and Claude is dumb as a rock compared to GPT-4. reply jstummbillig 13 hours agorootparentprevIt is known reply simonw 13 hours agoprevIs there a model card anywhere? I'd like to know what it was trained on. reply moralestapia 13 hours agoprevWell, he delivered. reply paxys 13 hours agoparentPartially. Open weights is not open source. reply gfodor 12 hours agorootparentIn machine learning models the term open source has been largely accepted to mean sharing weights and, if necessary, inference code. You can argue if this is an abuse of the term but everyone does it, and saying someone didn’t deliver if they used it and published weights would probably mean saying the same about mistral, meta, etc. reply asadotzler 10 hours agorootparentYes. So say the same thing about them Open source has a definition and abusing that hurts all of us except the billionaires. reply moralestapia 9 hours agorootparentI get the \"open source\" argument, but what is the issue here? If you are able to reproduce the thing in its entirety and you're given no restrictions on its use, it seems compatible with the spirit of open sourcing things. reply xcv123 8 hours agorootparentprevThe architecture of the model is open source. Not just the weights. You can run the entire thing locally. reply aussieguy1234 5 hours agoprevHow hard would it be for an open source group to fine tune this into a chatbot? reply sqreept 9 hours agoprevWhat are the languages supported by it? reply cyanydeez 9 hours agoparentTweets. reply LZ_Khan 13 hours agoprevHow are people's experience with this model? Having the most weights is one thing but being a better model than the 70B models is another. reply swalsh 13 hours agoparentI use grok all the time to find tweets or ask about trends on Twitter. For that it's better than what used to exist. But its not a great model outside that narrow use case. reply labrador 13 hours agoparentprevtbh, I've never seen anyone share anything interesting produced by Grok. I see plenty of posts on X and reddit of people sharing amazing things that GPT-4 and now Claude 3 Opus can do. Grok can roast people. That's pretty much all I've seen. I'd love to proven wrong if someone cares to share something interesting produced by Grok. reply andre-z 12 hours agoprevThe only other Repository is a fork of Qdrant. reply stale2002 13 hours agoprevHey, asking any experts here, what are their first thoughts in the significance of this? IE, is this comparable to any other model released, or are there significant metric differences that make it better for certain usecases? The only thing I see, of the top of my head, is that it is a very large model, and I don't think any models of similar size have been released. reply Me1000 12 hours agoparentNot an expert by any means, but I like learning about this stuff and I play with a lot of open weight models. I’d say the significance is that it happened. It’s by far the largest open weight model I’ve seen. But I’m not sure why you’d use it over a model like Mixtral, which seems to perform about the same at like 1/6th the size. But I welcome any contribution to the open weight LLM community. Hopefully people will learn something interesting with this model. And I hope they keep releasing new versions! reply MichaelRazum 12 hours agorootparentIf I may ask, how do you load such big models? 300gb seems like a lot to play around with. reply Me1000 12 hours agorootparentYou're right, this model is going to be too big for most people to play around with. But to answer your question I have a 128GB of RAM in my M3 MacBook Pro, so I can use most of that for GPU inferencing. But still, this model is going to need to be heavily quantized for me to be able to use it. (fwiw, I probably wont try this one) In the next week or two I expect we'll see a GGUF version of the weights (might need to wait for a patch to llama.cpp first), and someone will release super small quantizations of it. I suspect my computer might be able to run a 3 bit quant, but it might need to go down to 2 bits to have any kind of reasonable context length. But with quants that small I'd expect the model's performance to degrade well below that of Mixtral, so it probably isn't really even worth using. But we'll see; quantization is weird, some models perform better than others when quantized. reply MichaelRazum 11 hours agorootparentThanks a lot for the hint :)! It awesome that it might run even on a MacBook, actually this is a reason to switch to Mac. Seems, there is nothing similar for a PC laptop with linux or windows. reply Me1000 11 hours agorootparentNo problem. I hope more people try these things out, it's the best way to push the industry forward! We can't let the researchers have all the fun. Apple had plenty of reasons to move forward with their Apple Silicon CPUs and GPUs in the mac, but they really did seem to get lucky with the unified memory architecture. It was kind of just an artifact of their design, but ends up serving the needs of deep neural net models really well! reply zozbot234 10 hours agorootparentprevA top-of-the-line Mac Studio Ultra maxes out at 192GB currently. This is also a MoE model, so only a fraction of parameters have to be in RAM. reply Me1000 9 hours agorootparentMoE doesn’t really help with the memory requirements for the reason mentioned in the other comment. But it does help with reducing the compute needed per inference. Which is good because the M3 Max and M2 Ultra don’t have the best GPUs. A 70B parameter model is pretty slow on my M3 Max, and this model has 86B activations per inference run. reply EgoIncarnate 9 hours agorootparentprevEach token generated may only use a subset of the parameters (86billion instead of 314billion), but the next generated token might use a different subset. If it's anything like Mixtral, it will switch between experts constantly. It helps with memory bandwidth, but all the parameters still need to be in RAM or it would be unbearably slow. reply TMWNN 11 hours agorootparentprev>In the next week or two I expect we'll see a GGUF version of the weights (might need to wait for a patch to llama.cpp first), and someone will release super small quantizations of it. How quickly are new models available through Ollama? reply Me1000 9 hours agorootparentOllama is just a wrapper around llama.cpp, so when the gguf model files come out it'll be able to run on Ollama (assuming no llama.cpp patch is needed, but even if it is ollama is usually good at getting those updates out pretty quickly). reply cjbprime 10 hours agorootparentprevFew days max. reply brucethemoose2 8 hours agoparentprevTests are not out yet, but: - It's very large, yes. - It's a base model, so its not really practical to use without further finetuning. - Based on Grok-1 API performance (which itself is probably a finetune) its... not great at all. reply whimsicalism 9 hours agoparentprevseems like a large undertrained model, not that exciting imo compared to mixtral it is also not the biggest model oss, switch transformer was released years ago and is larger and similarly undertrained reply gardenhedge 14 hours agoprev> Due to the large size of the model (314B parameters), a machine with enough GPU memory is required to test the model with the example code What type of machine do you need to play around with this? reply 317070 13 hours agoparentProbably a machine with about 628 GB of GPU memory. (2 bytes per parameter) So 8xH100 (80Gb each) should do it. reply anigbrowl 14 hours agoparentprev'Chunky beast, needs 320 Gb VRAM likely 4 bit, likely is being run 8 bit on 8 x 80 Gb GPUs.' -Emad reply a_wild_dandan 3 hours agoparentprevA single 192GB M2 Mac using a 4-bit quant would work. reply rvnx 14 hours agoprevOne subtle thing: Musk said \"open-source\", we got \"open-weights\" instead (still better than nothing though, so it's greatly appreciated). reply tylerekahn 14 hours agoparentThis is the weights and the model under Apache 2.0 license. What do you mean by open-source? https://github.com/xai-org/grok/blob/main/model.py https://github.com/xai-org/grok/blob/main/run.py#L25 reply pclmulqdq 13 hours agorootparentStill better than most of the \"open weights\" models that have massively restrictive terms. reply solarkraft 13 hours agoparentprevHe also called permissively licensing Tesla's patents \"open sourcing\" them. He's at the forefront of misusing the term. reply drexlspivey 10 hours agorootparentThe “source” in “open source” refers to source code which they released. A dataset is not source code, if anyone is misusing the term it’s you. reply frabcus 10 hours agorootparentI consider the weights a binary program and the source code is the training data. The training algorithm is the compiler. I agree this isn't standard terminology, but it makes the most sense to me in terms of power dynamics and information flow. We know from interpretability research that the weights do algorithms eg sin approximation etc. So they feel like binary programs to me. reply HarHarVeryFunny 8 hours agorootparentprevIf you can't rebuild it, then how can you be considered to have the \"source code\" ? The training data isn't a dataset used at runtime - it's basically the source code to the weights. Not sure it really matters here though (who has the GPUs and desire to retrain Grok?), but just as a matter of definition \"open weights\" fits better than \"open source\". reply solarkraft 9 hours agorootparentprevhttps://youtu.be/WyTzRnGSlcI?t=88 reply paulgb 14 hours agoparentprevDumb question: what should open-source mean in the context of something like this? Open access to the training data and training pipeline as well? reply CharlesW 14 hours agorootparentIt's not a dumb question, and the answer is \"yes\". reply simonw 13 hours agorootparentA big catch here is that you can't slap an open source license on a bunch of copyrighted training data, and to date no-one has created a truly convincing LLM exclusively trained on public domain data. It might happen soon though - there are some convincing effort in progress. reply CharlesW 13 hours agorootparentAbsolutely, because it’s trained mostly on unlicensed, copyrighted content, they basically can’t release source. reply gfodor 12 hours agorootparentMany people think these companies are training on unlicensed data but I think OpenAI licenses their data, they just “license” it the way one would need to in order to read it. reply zer00eyz 11 hours agorootparentYou all keep using the word \"Data\" Data, as in facts, as in the frequency of one word in relation to another. \"Copyright does not protect facts, ideas, systems, or methods of operation, although it may protect the way these things are expressed...\" FROM: https://www.copyright.gov/help/faq/faq-protect.html It's not a question of if, rather when the cat gets out of the bag and the legal battle starts. The problem is that all the copyright applies to the expression not the factual information it expresses (in this case word relations). Now \"how math works\" and \"the language of the law\" are going to make for an interesting court case. I suspect that math wins here but it depends on what judge gets it and how high it goes. reply CharlesW 12 hours agorootparentprev> …I think OpenAI licenses their data… They've just started to (in response to lawsuits, it must be noted) and in the meantime, they're simultaneously claiming that (1) what they're doing is fair use (a.k.a. fair dealing) and (2) preparing for the day when courts confirm that it isn't. reply logicchains 2 hours agorootparentprevhttps://substack.recursal.ai/p/eaglex-17t-soaring-past-llama... this one claims to have been trained only on permissively licensed data. reply nabakin 12 hours agorootparentprevAgreed. It's ridiculous people have to resort to saying their question dumb to avoid being attacked by toxic commenters. reply dudus 11 hours agorootparentprevIf you release that instead of the binary weights you can be both more open and less useful for users. Fun reply zeroCalories 13 hours agorootparentprevCome on, that's not reasonable to expect from a company, or useful for indie hackers. Having weights that can be used however you like is enough for most people, even large companies. reply schoen 13 hours agorootparentMaybe it should be called something else? \"Openly-licensed\"? Just because the model weights are not really \"source\" (either as a matter of intuition or for example following the OSI \"preferred form in which a programmer would modify the program\" definition). reply zeroCalories 9 hours agorootparentSure, but I don't want to train anyone's model from scratch. Realistically, I can't download all the training data, or run the pipeline, or train the model. Making all of that available to me would be a massive burden on the company too, so they simply won't do it. If I'm able to fine-tune it, that's enough for me, and imo, that fits with the spirit of open/free software. We have to understand that this is fundamentally a different thing than something like the Linux kernel, and closer to something like an industrial project. The output is just a bunch of numbers instead of something physical. reply TaylorAlexander 14 hours agorootparentprevThe Open Source Initiative is actively working on this over the course of this year, and your input will help define that meaning! Please see here for more: https://opensource.org/blog/open-source-ai-definition-weekly... reply Q6T46nT668w6i3m 14 hours agorootparentprevYes, training and evaluation code, i.e., the code used to generate the weights. reply TaylorAlexander 14 hours agoparentprevYeah musk said “all design and engineering for the original roadster is now open source” and actually what we got was a few PCB files and zero mechanical design files so I don’t ever trust what he says. reply littlestymaar 13 hours agoprevHow long before the Groq team sues for trademark violation? It's literally the purpose of trademark laws to make sure resembling names do not cause confusion in the mind of customers so it would be very surprising to see this situation persist. reply EastSmith 12 hours agoparentThere is a friendly warning here from Groq: https://wow.groq.com/hey-elon-its-time-to-cease-de-grok/ reply bhaney 12 hours agorootparentIs it safe to say, 4 months later, that Elon is ignoring this? I assume there hasn't been any kind of response or further action taken yet. reply mlindner 7 hours agoparentprevGrok is a word in common parlance. So there's no way they could succeed in any suit. That's why the Groq team picked a modification of the word. reply littlestymaar 6 hours agorootparentYou mean like Canvas®, Apple®, Windows® or Amazon®? Wanna try re-use these for your own business and see how it goes? There's nothing preventing you to trademark common words, it just must not be descriptive of your business. reply nostrebored 13 hours agoparentprevWould be a rough trademark enforcement case as “Grok” has been in common language for decades reply Findecanor 33 minutes agorootparentI myself have never heard it outside of \"nerdy\" circles... that is: people who would read science fiction. I personally am not entirely happy about the word (no matter how it is spelled) being used for a particular AI product. \"Grok\" to me means knowing a subject at a much deeper level than I think any AI is capable of at the present level of technology. But it would be passable to use it for a company name, to indicate that it is a goal to strive for. reply ben_w 26 minutes agorootparentGenerally agree, though I would say \"knowing a subject at a much deeper level than any LLM is capable of\", as AI more broadly also includes specialist models that are wildly super-human in narrow domains like chess and Go. reply ben_w 13 hours agorootparentprevSo has \"Apple\" and \"Windows\". Grok and groq both relate to AI, so there's definitely grounds to believe the names may cause consumer confusion. After all, Apple (computers) was repeatedly sued by Apple (records) for doing music things. reply cma 12 hours agorootparentIt's easier to get a trademark on an altered word than a plain dictionary word. Just acquiring the easier one to acquire doesn't mean you now have rights over the harder one to acquire, though eventually after enough market recognition you might be given some control over other people using the common one. I wouldn't think groq is there yet. reply Angostura 13 hours agorootparentprevRobert A. Heinlein coined the term grok in 1961 reply a1369209993 12 hours agorootparentSix is plural. reply cavisne 13 hours agoparentprevThey already have. reply 95 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The repository provides sample code for executing the Grok-1 open weights model, which requires downloading the checkpoint, installing prerequisites, and running the code for testing.",
      "Utilizing a machine with ample GPU memory is essential due to the model's size.",
      "Despite the MoE layer implementation not being efficient, it was selected for its simplicity; users can acquire the weights via a torrent client, with both code and weights in the repository licensed under Apache 2.0."
    ],
    "commentSummary": [
      "The GitHub thread delves into diverse AI model topics, such as implementing the Grok model, testing procedures, data integrity concerns, utilizing Twitter data, and contrasting Claude 3 Opus with GPT-4.",
      "Legal aspects are explored, like sharing sizable files via BitTorrent, dangers of copyrighted training data usage, and the essence of open source as it relates to AI models.",
      "Noteworthy mentions include Elon Musk's reveal of open sourcing the first Roadster design and the complexities surrounding trademark enforcement in the discourse."
    ],
    "points": 937,
    "commentCount": 354,
    "retryCount": 0,
    "time": 1710704017
  },
  {
    "id": 39735716,
    "title": "Evolution of Armored Cephalopods: A Study",
    "originLink": "https://crookedtimber.org/2024/03/16/occasional-paper-when-armor-met-lips",
    "originBody": "Home Comments policy Subscribe Occasional paper: When Armor Met Lips by Doug Muir on March 16, 2024 So about five hundred million years ago, give or take, there was this little creature called Plectronoceras. It was about 2 cm long — just under an inch — and it had a conical shell with a bunch of tentacles sticking out. It was a cephalopod, an early member of the group that includes octopuses and squid. And it was an /armored/ cephalopod, with most of its soft body protected by that hard little shell. Let’s pause here and rewind: this was five hundred million years ago. That’s the late Cambrian, if you’re a geology nerd. It’s before the dinosaurs. It’s before sharks or cockroaches or ferns. This is *old*. Complex life had barely gotten started. Life in general was pretty much confined to the oceans. But there were no fish yet — just invertebrates. Half a billion years, yeah? Long, long time. And a lot of the stuff swimming around was weirdly alien. Again, if you’re a geology nerd, you know about stuff like Opabinia, Anomalocaris, or Hallucigenia. If you don’t, then let’s just say that you wouldn’t have recognized much from those ancient seas. Not just “no fish”. There were no clams or lobsters, no starfish or barnacles or crabs or anemones, no coral or kelp. The world was new. Those things hadn’t evolved yet. But almost from the beginning, there was this thing: shell, plus tentacles. If you have even the slightest interest in the ancient world, you’ve probably heard of ammonites. The usual narrative goes something like this: “Ammonites were armored cephalopods, relatives of squids and octopi (1), but with a hard external shell. They lived in the oceans for hundreds of millions of years. Then they all died out in the mass extinction at the end of the Cretaceous, the same extinction that wiped out the dinosaurs. Today there are only chambered nautiluses, which look somewhat like ammonites but are a different sort of creature.” And this is true as far as it goes, but it misses a couple of key points. First, ammonites were just one sort of armored cephalopod. That design seems to have evolved independently several times. And… sure, think about it. It’s actually a great design. It combines the two best features of being a mollusk: a hard protective shell, and grasping manipulative tentacles. Modern mollusks have to choose one or the other. But hey, por que no los dos? Armored cephalopods were free-swimming, or rather free-floating: they filled their shells with gas, so they floated along at zero buoyancy.(2) They were mostly slow, but could put on sudden short bursts of jet-propelled speed, just enough to catch prey or avoid danger. They didn’t “rule the seas” or anything like that, but for hundreds of millions of years they were around pretty much everywhere, in every marine ecosystem from the poles to the equator. If you want to buy a fossil today? and you want something that looks cool but is not too expensive? A fossil ammonite is your go-to. Some years back my kid found one, pretty intact, just lying in a field. That’s how ubiquitous they were. Because it was a great design, yeah? Like the cockroach or the shark: basic, efficient, central. Second thing, the chambered nautilus. It’s not an ammonite, no, but it’s like an ammonite — it’s another armored cephalopod, a spiral snail-like shell plus a bunch of tentacles.(3) But where earlier armored cephalopods were everywhere, the nautilus is restricted to a small-ish patch of ocean in the western Pacific, from northern Australia through Indonesia up to Vietnam or so. And while ammonites floated boldly everywhere, the nautilus is a shy creature that lives fairly deep down, mostly in the twilight zone 100 to 500 meters deep. (Years ago, I used to scuba dive in that area. My dive instructor told me that a nautilus never comes above 100 meters depth unless it’s seriously ill or injured. Meanwhile, a recreational diver should never go below 40 meters depth — nitrogen narcosis, oxygen poisoning, the bends, just don’t. “So if you’re diving and you see a nautilus,” he said, “at least one of you is in big trouble.”) Okay, so: why? Why is the nautilus restricted to this small area, when older armored cephalopods roamed worldwide? Is the nautilus just… worse? An inferior design, or something? No. Here’s where things get mysterious and interesting. After the asteroid hit, the ammonites all died out. But chambered nautiluses survived, and they thrived! They weren’t as ubiquitous as the ammonites, but for about 30 million years they were found all over the world. Dozens of species, all sorts of different habitats. The “shell, but also tentacles” design was still solid. But then around 30 million years ago — halfway through the Age of Mammals, give or take — something happened. The nautiloids started disappearing. Fewer species, less diversity. Bit by bit they shrank back into their current small range. What happened halfway through the Age of Mammals? Well, here’s one clue: the nautiloids’ long retreat showed a pattern. It wasn’t everywhere and all at once. They disappeared first in the northern arctic regions; then in the Antarctic; then in temperate zones; finally across most of the tropics except that one small patch. This pattern suggested a culprit: a warm-blooded predator that evolved in the Arctic and then spread around the world. But… the armored cephalopod design had been around forever. They’d been living with predators for *half a billion years*. Sharks. Primitive armored fish. Not-so-primitive modern fish. In the age of dinosaurs, they had to deal with ichthyosaurs, plesiosaurs, and mosasaurs. Back in the Paleozoic, they were hunted by eight-foot-long giant sea scorpions. Way back in the Cambrian, they had to live with the anomalocariids. In the early Age of Mammals, there were primitive whales and sea-going crocodiles. The armored cephalopod design took them all in stride and kept going. So what happened? — The authors of this paper here think that they’ve found the answer. To understand, we have to zoom in on exactly what happens when something tries to eat an armored cephalopod. Threatened, the beast withdraws its tentacles into its shell and clams up. Okay, so now either you’re big enough to crunch through the shell, or you’re not. We know that some large predators did crunch through, because we’ve found fragments of ammonite shell fossilized in the stomachs of plesiosaurs and ancient sharks and such. But if you do that, now you have a stomach full of tasty mollusk meat… mixed with unpleasantly sharp and indigestible shell chunks. An option if you’re hungry enough, but hardly ideal. Shark, ichthyosaurs, marine crocodiles: it’s not that they couldn’t eat armored cephalopods. It’s just that it usually wasn’t worth it. Predators have to do a cost-benefit analysis, yeah? And for half a billion years, armored cephalopods were in the black. But then something evolved with a feeding apparatus that could separate the meat from the shell. And suddenly, armored cephalopods were in the red. The paper says: it was seals. Seals — pinnipeds, if you’re fancy — aren’t usually that large, and aren’t usually apex predators. (Predators, absolutely yes. Apex, no.) (4) So what do seals have that mosasaurs and sharks and sea scorpions didn’t have? Lips. Seals have lips. Lips, cheeks, and a muscular tongue. A seal can grab a shelled prey, and puncture the shell with sharp strong teeth — and then just *schlorp* out the tasty meat inside. The technical term for this is “suction feeding”. Pinnipeds generally are good at it, and some are so good that they prefer to eat shelly prey — clams, crabs, mussels, whatever — and don’t eat much else. Stuff that lives on or near the ocean floor — clams, crabs, lobsters, and such — could evolve various defensive and avoidance strategies. But free-floating armored cephalopods? All they had was a quick burst of speed, and that wouldn’t help much against a hot-blooded predator that could maintain high speeds much longer. And everything else matches, too. Pinnipeds evolved about thirty million years ago. They showed up first in the colder parts of the northern hemisphere, then in the Antarctic, then in temperate zones. Even today, although there are some tropical species, they’re mostly cold-to-cool water creatures. And — here’s the smoking gun — the one part of the globe they still haven’t colonized? The southwestern Pacific, from northern Australia across Indonesia. The modern range of nautiloids is exactly the one part of the ocean that those sucking, slurping seals haven’t reached yet. And that’s all: just a little story about evolution. “Persons attempting to find a motive in this narrative will be prosecuted; persons attempting to find a moral in it will be banished; persons attempting to find a plot will be shot.” Share this: Tweet { 17 comments… read them below or add one } 1 Doug Muir 03.16.24 at 3:49 pm (1) If you’re inclined to be pedantic about the plural of “octopus”, pause and ask yourself this question: “Am I a sophomore in college?” If yes, please go right ahead! If no… dude. (2) They could actually control their buoyancy, and in a very cool way, but this post is really long enough already. (3) If you’re inclined to be pedantic about the nautilus’ limbs and say that /actually/ they are “arms” and not “tentacles” because tentacles have suckers on them, then (a) congratulations on remembering that long-ago biology class, and (b) see Footnote 1, above. (4) Yes I am aware of walruses and elephant seals. Most seals are medium-sized animals, human size or smaller. As for being an apex predator, seals are a very popular food item for the larger sharks, and even leopard seals get bodied by orcas. 2 oldster 03.16.24 at 4:20 pm So, you’re saying the nautiloids did okay until the pinnipeds sealed their fate? 3 oldster 03.16.24 at 5:11 pm And als0 — congratulations on finding a photo of a seal saying, “yeah, it was me. And I’m not saying sorry. What are you going to do about it?” 4 Cervantes 03.16.24 at 5:35 pm There seems to be an obvious logical flaw here. Cephalopods without shells, that are just as tasty, are doing just fine, and there are a lot more critters that eat them than just seals. 5 Kenny Easwaran 03.16.24 at 5:59 pm I love this! 6 Doug Muir 03.16.24 at 7:02 pm @4, what flaw? Cephalopods without shells are either much faster and more maneuverable (squid) or mostly hang out on or near the ocean floor (octopi) where, if threatened, they can suddenly squeeze their amorphous bodies into the tiniest of shelters. Or they’re cuttlefish, who do both. Also, most non-armored cephalopods can change color, allowing some very sophisticated camouflage; can’t do that with armor on. (There are some interesting edge cases. The vampire squid is not a squid, but rather a rare octopus that hangs out in open water instead of crawling along the bottom. It survives because (1) it mostly lurks in deep, low-oxygen waters with almost no predators, and (2) it has an astonishing threat display.) Doug M. 7 Michael Allen 03.16.24 at 7:37 pm “Threatened, the beast withdraws its tentacles into its shell and clams up.” I see what you did there. Well played. 8 Michael Allen 03.16.24 at 8:55 pm Lots of videos about the vampire not really a squid on YouTube. I had no idea. 9 Alan White 03.16.24 at 11:36 pm Another lovely post in both content and expression. The thesis makes sense to me. Now to go on amazon for ammonites. . . 10 Matt 03.17.24 at 12:55 am the plural of “octopus”, It’s octopuseses, right? 11 Phil 03.17.24 at 11:11 am Strong Wikipedia opening: “The vampire squid (Vampyroteuthis infernalis, lit. ‘vampire squid from hell’)” The vampire squid, or, to give it its technical name, the vampire squid from hell… 12 Cervantes 03.17.24 at 3:43 pm I dunno about that Doug, what’s the diet of Physeter Catodon? 13 Spongebob 03.17.24 at 7:10 pm Does it gaze at the sun with its wandering eye? 14 CO 03.17.24 at 7:54 pm You’ve gotten “in the red” and “in the black” backwards. Thanks for this article! 15 drs 03.17.24 at 11:24 pm “Cephalopods without shells, that are just as tasty, are doing just fine, and there are a lot more critters that eat them than just seals.” A key difference is their reproduction. Checking wikipedia, the chambered nautilus takes 5 years to reach maturity, which is longer than most octopi even live: “Unlike most other octopus species, whose lifespans normally span only one year, the giant Pacific octopus has a lifespan of three to five years”. Even the giant squid “are thought to reach sexual maturity at about three years old”. Why are nautiloids so slow? According to my notes from Squid Empire, they have big eggs and thick shells. Ammonoids evolved to have lots of eggs, thinner shells with complex sutures for strength, and faster growth. Probably why they evolved more diversity, but maybe also part of why they didn’t pass the K-T boundary, while nautiloids might have survived a dearth of plankton better. At any rate, they are slow. Shell-less cephs can better survive predation via massive spawning and rapid growth, in addition to their other advantages Doug described. 16 Bernard 03.18.24 at 3:30 am Finally I can say “seals suck” and no one can disagree with me. 17 Evan L Howell 03.18.24 at 8:03 am a, armored cephalopod 7 then what are we doing to seals?? Leave a Comment Name E-mail Website You can use these HTML tags and attributes: Recent Comments Evan L Howell on Occasional paper: When Armor Met Lips Bernard on Occasional paper: When Armor Met Lips drs on Occasional paper: When Armor Met Lips Kromhout on The Bezzle CO on Occasional paper: When Armor Met Lips Search Archives Archives Select Month March 2024 (7) February 2024 (18) January 2024 (29) December 2023 (16) November 2023 (21) October 2023 (14) September 2023 (11) August 2023 (21) July 2023 (28) June 2023 (13) May 2023 (15) April 2023 (14) March 2023 (19) February 2023 (12) January 2023 (20) December 2022 (20) November 2022 (20) October 2022 (21) September 2022 (16) August 2022 (19) July 2022 (14) June 2022 (13) May 2022 (15) April 2022 (12) March 2022 (15) February 2022 (11) January 2022 (17) December 2021 (14) November 2021 (14) October 2021 (17) September 2021 (12) August 2021 (13) July 2021 (12) June 2021 (11) May 2021 (24) April 2021 (17) March 2021 (18) February 2021 (17) January 2021 (18) December 2020 (25) November 2020 (21) October 2020 (16) September 2020 (17) August 2020 (13) July 2020 (16) June 2020 (15) May 2020 (25) April 2020 (18) March 2020 (18) February 2020 (15) January 2020 (13) December 2019 (13) November 2019 (12) October 2019 (12) September 2019 (23) August 2019 (20) July 2019 (10) June 2019 (16) May 2019 (17) April 2019 (17) March 2019 (20) February 2019 (13) January 2019 (31) December 2018 (19) November 2018 (20) October 2018 (24) September 2018 (20) August 2018 (17) July 2018 (29) June 2018 (26) May 2018 (26) April 2018 (19) March 2018 (40) February 2018 (27) January 2018 (30) December 2017 (23) November 2017 (28) October 2017 (23) September 2017 (25) August 2017 (28) July 2017 (24) June 2017 (24) May 2017 (42) April 2017 (32) March 2017 (27) February 2017 (24) January 2017 (36) December 2016 (28) November 2016 (31) October 2016 (37) September 2016 (22) August 2016 (29) July 2016 (27) June 2016 (41) May 2016 (33) April 2016 (34) March 2016 (36) February 2016 (40) January 2016 (39) December 2015 (37) November 2015 (32) October 2015 (32) September 2015 (27) August 2015 (33) July 2015 (38) June 2015 (35) May 2015 (54) April 2015 (35) March 2015 (35) February 2015 (43) January 2015 (40) December 2014 (29) November 2014 (28) October 2014 (34) September 2014 (40) August 2014 (40) July 2014 (38) June 2014 (28) May 2014 (41) April 2014 (44) March 2014 (34) February 2014 (30) January 2014 (31) December 2013 (38) November 2013 (39) October 2013 (49) September 2013 (53) August 2013 (37) July 2013 (47) June 2013 (41) May 2013 (43) April 2013 (47) March 2013 (39) February 2013 (33) January 2013 (35) December 2012 (31) November 2012 (30) October 2012 (27) September 2012 (31) August 2012 (27) July 2012 (38) June 2012 (47) May 2012 (37) April 2012 (34) March 2012 (44) February 2012 (39) January 2012 (43) December 2011 (53) November 2011 (47) October 2011 (35) September 2011 (32) August 2011 (48) July 2011 (33) June 2011 (37) May 2011 (58) April 2011 (35) March 2011 (62) February 2011 (50) January 2011 (48) December 2010 (53) November 2010 (42) October 2010 (50) September 2010 (40) August 2010 (37) July 2010 (41) June 2010 (59) May 2010 (57) April 2010 (47) March 2010 (53) February 2010 (42) January 2010 (40) December 2009 (46) November 2009 (53) October 2009 (60) September 2009 (50) August 2009 (60) July 2009 (57) June 2009 (68) May 2009 (54) April 2009 (80) March 2009 (86) February 2009 (50) January 2009 (76) December 2008 (64) November 2008 (64) October 2008 (76) September 2008 (64) August 2008 (48) July 2008 (77) June 2008 (59) May 2008 (60) April 2008 (86) March 2008 (75) February 2008 (74) January 2008 (63) December 2007 (78) November 2007 (83) October 2007 (85) September 2007 (73) August 2007 (66) July 2007 (80) June 2007 (73) May 2007 (80) April 2007 (66) March 2007 (85) February 2007 (81) January 2007 (73) December 2006 (73) November 2006 (95) October 2006 (97) September 2006 (91) August 2006 (106) July 2006 (102) June 2006 (110) May 2006 (110) April 2006 (106) March 2006 (118) February 2006 (99) January 2006 (94) December 2005 (74) November 2005 (71) October 2005 (132) September 2005 (116) August 2005 (94) July 2005 (117) June 2005 (105) May 2005 (112) April 2005 (161) March 2005 (125) February 2005 (108) January 2005 (104) December 2004 (120) November 2004 (150) October 2004 (170) September 2004 (171) August 2004 (148) July 2004 (157) June 2004 (145) May 2004 (152) April 2004 (172) March 2004 (168) February 2004 (173) January 2004 (157) December 2003 (172) November 2003 (143) October 2003 (147) September 2003 (168) August 2003 (96) July 2003 (102) RSS - Posts RSS - Comments Pages Comments policy March 2024 M T W T F S S1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 « Feb Book Events Charles Stross / Various China Miéville / Iron Council Chris Mooney / The Republican War on Science Dani Rodrik / One Economics, Many Recipes Doug Henwood / After the New Economy George Scialabba/What Are Intellectuals Good For? Joseph Carens / The Ethics of Immigration Levitt & Dubner / Freakonomics Sheri Berman / The Primacy of Politics Susanna Clarke / Jonathan Strange & Mr Norrell Yochai Benkler / The Wealth of Networks Contributors Belle Waring Chris Armstrong Chris Bertram Eric Schliesser Eszter Hargittai Gina Schouten Harry Brighouse Henry Farrell Ingrid Robeyns John Holbo John Quiggin Kevin Munger Macarena Marey Maria Farrell Miriam Ronzoni Paul Segal Serene Khader Speranta Dumitru Fine Print Banner courtesy of David Krewinghaus Comments Policy Lumber Room 11D 3 Quarks Daily Ampersand Atrios Balkinization Best of Both Worlds Blood and Treasure Brad DeLong Cosma Shalizi Daily Kos Daily Nous Daniel Drezner Digby Digressions and Impressions Ed Felten Edge of the American West Fabians UK Joshua Marshall Juan Cole Kevin Drum Lawrence Solum Lawyers, Guns and Money Making Light Marc Lynch Marginal Revolution Mark Kleiman Maud Newton Michael Froomkin Miriam Burstein Naked Capitalism Political Theory Daily Review Russell Arben Fox Slugger O’Toole Sociological Images Steven Johnson Suburban Guerilla Talk Left Tapped The New Inquiry Timothy Burke Unfogged Washington Monthly Old Wood Astra Taylor Brian Weatherson Corey Robin Daniel Davies Eric Rauchway Jon Mandle Kieran Healy Micah Schwartzman Michael Bérubé Montagu Norman Niamh Hardiman Rich Yeselson Scott McLemee Ted Barlow Tedra Osell Tom Runnacles Meta Log in Entries feed Comments feed WordPress.org Recent Posts Sunday photoblogging: starling Occasional paper: When Armor Met Lips Sunday photoblogging: blue tit Occasional Paper: The Iron Snow Beneath Your Feet The Bezzle Tags Add new tag Alan Greenspan autism Barack Obama Bill Clinton Biology books Bruce Bartlett capabilities capability approach Carl Menger China climate change conservatism Daniel Rodgers Ellen Schrecker Ethics Friedrich Hayek George Scialabba seminar George W. Bush Germany Seminar GOP internetpolitics John Kenneth Galbraith Jude Wanniski labor unions limitarianism Ludvig von Mises Ludwig von Mises Matt Yglesias Milton Friedman Netherlands Obama Oliver Wendell Holmes Paris Paul Krugman philosophy Ronald Reagan Schenck v. United States seminar on David Graeber's Debt sustainability tesco art banksy mural van der Bellen wealth worthwhile Canadian initiatives",
    "commentLink": "https://news.ycombinator.com/item?id=39735716",
    "commentBody": "When Armor Met Lips (crookedtimber.org)361 points by akkartik 17 hours agohidepastfavorite45 comments Affric 8 hours agoGreat photo selection with the shifty looking seal (gotta love the whites of caniform eyes) and a nice little summary of the article. One thing about zoology and animal morphology is that we all know how important feeding is for animals but only real nerds love the digestive tract. Transport, skin, and reproduction are far more glamorous; but the mammalian sense of smell and mouth parts gave us such an advantage in the tertiary period. It’s interesting that this sort of feeding never arose in the sea. I wonder what the ancestors of the pinnipeds who first ventured back into water ate… reply angiosperm 7 hours agoprevDo not neglect the Paper Nautilus, a kind of Mediterranean octopus. https://www.sciencefriday.com/articles/the-seamstress-and-th... https://www.themarginalian.org/2022/12/26/jeanne-villepreux-... Jeanne Villepreux-Power invented marine biology, single-handed in the early 19th century. She observed them repairing a hole in their shell by gluing on a found patch. reply teruakohatu 6 hours agoparentThat is interesting. I looked it up on wikipedia and the Argonaut / Paper Nautilus \"shell\" is a thin walled eggcase and that males do not have. It does not provide protection. reply angiosperm 2 hours agorootparentFemale argonauts have the shell their whole life long, and do not only construct it when they have eggs to put in it. We may reasonably assume that it has survival value even when there are no eggs. We know, for example, that they carry bubbles in it to tune their buoyancy. reply mannykannot 15 hours agoprev\"Pinnipeds evolved about thirty million years ago. They showed up first in the colder parts of the northern hemisphere, then in the Antarctic, then in temperate zones.\" That's an interesting sequence, given that the continents then were about where they are now. reply aendruk 7 hours agoparentThat’s the sequence of notable populations; individuals did who knows what. reply thriftwy 13 hours agoparentprevIt takes one volcanic winter for them to cross the oceans and emerge on the other side, without being fit for the warm waters. reply nateb2022 14 hours agoparentprevIt is possible that they fed on extremophiles which thrived in really cold regions and later evolved to feed on other food sources. reply ricardobeat 14 hours agoparentprevMaybe they evolved in both places independently, or maybe it took a couple million years for a pod of seals to make the journey from north to south, and then start dominating the local ecosystem. reply ivanbakel 10 hours agorootparentWikipedia says pinnipeds are monophyletic, so there's only a single evolutionary branch they originated from. The travel question is much more interesting. Napkin math puts the distance between the polar circles at 12500 miles, the max swim speed of a seal at 25mph, giving a travel time of ~3 weeks of nonstop swimming as the crow flies. How does that happen? What would a pod of Arctic seals have eaten along the way, and what would have compelled them to make the trip out of familiar territory? How and why did they cross the temperate equatorial seas with a load of blubber? reply ricardobeat 43 minutes agorootparentThe distance is not surprising, animal trackers have shown they routinely travel vast distances in the ocean [1, 2, 3]. All you need is a fluke event, once in a thousand years where they for some reason make, and survive, this trip (I say thousand to give us some scale, could be actually once in a million years). [1] https://www.ocearch.org/tracker/detail/nukumi [2] https://www.ocearch.org/tracker/detail/dr-brent [3] https://www.wur.nl/en/show/seal-telemetry.htm reply DabbyDabberson 9 hours agorootparentprevLook up monkey raft theory. How did monkeys from Africa get to South America? Also roughly 30 MYA. reply jasonwatkinspdx 9 hours agorootparentLife travels on the ocean a lot farther than people realize, and often species you'd find improbably. Ants spread around the world by hitching rides on coconuts or waterbirds. This now means there's something like a global war among various ant colonies that invade each other this way. reply riffraff 2 hours agorootparentIt's my understanding the ant spreading was more related with increase in navigation than birds/floating. that'd be why the Argentine ant has spread everywhere in Europe in the last century rather than being ubiquitous since the dawn of time. reply ekanes 10 hours agorootparentprevAgree, this stuff is fun to imagine. Check out the many species that landed (and then kept evolving in isolation) in Hawaii. Seals, many birds, etc. reply jimkleiber 16 hours agoprevI thoroughly enjoyed the author's writing style. Anyone know where I can find more by him, Doug Muir? reply HankB99 15 hours agoparentGoogle knows: https://crookedtimber.org/2024/01/03/welcome-to-doug-muir/ reply jimkleiber 14 hours agorootparentNormally a blog just lets me click the author's name and see more by them, and even the link you posted doesn't really show more essays. This one did though: https://crookedtimber.org/author/doug-muir/ reply daemonologist 14 hours agorootparentThanks for the link, that's a worthwhile click - their previous article is great too: https://crookedtimber.org/2024/03/08/occasional-paper-the-ir... reply pizzalife 8 hours agoparentprevPersonally I can't stand it, yeah? reply readyplayernull 6 hours agoprev> If you’re inclined to be pedantic about the nautilus’ limbs and say that /actually/ they are “arms” and not “tentacles” because tentacles have suckers on them, then (a) congratulations on remembering that long-ago biology class, and (b) see Footnote 1, above. Something tells me an arm is a mechanical limb composed of connected poles that turn at their joints. And a hose-like limb without suckers should have its own name. A foot is an \"arm\", a penis is something else. reply Karellen 34 minutes agoparentAlso, tails reply pineaux 2 hours agoparentprevPenis is definitely something else because it contains (almost) no muscles, while your other examples do (contain muscles). reply nehal3m 5 hours agoparentprevHey, speak for yourself! reply JKCalhoun 9 hours agoprevI wanted to go fossil hunting (and for ammonites in particular) after watching these guys just haul them out: https://youtu.be/9XWhdPL58is reply a3w 2 hours agoprevSo the ideal tank buster is a carnivorous unicorn: penetrate that armor, then schlorp out the crew! Genetics will go too far, if we ever get these. reply vitiral 11 hours agoprevI really enjoyed this article, but I was really disappointed. For some reason I thought it was going to evolve into a Lovecraftian epic of how the grand and horrifying civilization of the ancient cephalopods and their relatives was thwarted by another species, one who paved the way for our own ignorant and doomed civilization to thrive. I kept this feeling until the very end [spoiler] even when it was revealed to be seals I held out hope they could be a Lovecraftian player in a grand epic saga. reply csours 10 hours agoparentwho do you think is protecting us from the lovecraftian horrors? reply Muromec 14 minutes agorootparentPaperclip counting bureaucrats of course reply vitiral 10 hours agorootparentprevIn general? Mostly ignorance. From cephalopods? I guess it's the seals reply leashless 9 hours agorootparentItalian love of calamari. reply B1FF_PSUVM 13 hours agoprevAlso: https://crookedtimber.org/2024/02/19/death-lonely-death/#com... about Voyager 1, recently in the news reply alexey-salmin 4 hours agoprev\"Ah! Love! The only thing my armor can't withstand!\" reply pfdietz 14 hours agoprev\"So, you’re saying the nautiloids did okay until the pinnipeds sealed their fate?\" reply Terr_ 11 hours agoparentI'm nautiloid to say more about their spiral into the depths. reply klipt 8 hours agorootparentNever trust a pinnipedophile. reply jagged-chisel 8 hours agorootparentprevಠ_ಠ reply HankB99 15 hours agoprev> geology nerd Shouldn't that be \"paleontology nerd?\" reply angiosperm 6 hours agoparentPaleontology used to be a subspecialty of geology, for a long time after they proved fossils had once been animals. Which the geologists had to do. reply skavi 14 hours agoparentprevSee footnote 1 reply HankB99 14 hours agorootparentWhere is footnote 1? I just looked up the authors of the article from their links at https://www.researchgate.net/publication/363885520_Seals_wha... and none mentions geology, though they did mention paleobiology, paleontology and fossils. What did I miss? reply nearbuy 13 hours agorootparentThe footnotes for the article are in the first comment. The author put a humorous footnote about being pedantic. reply ahazred8ta 5 hours agorootparenthttps://www.google.com/search?q=pedants+revolt \"I'd expected there to be less of you.\" \"FEWER!!\" reply jjtheblunt 10 hours agorootparentprevAnd, in doing so, was pedantic! reply jrflowers 6 hours agoprev [–] It is a shame that this is not about when Armour Meats perfected their hot dog formulation reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Doug Muir's paper, \"When Armor Met Lips,\" delves into the evolution and significance of armored cephalopods, particularly focusing on Plectronoceras during the late Cambrian period.",
      "The decline of nautiloids is linked to the emergence of warm-blooded predators, while seals with suction feeding capabilities are associated with the decrease of armored cephalopods in the discussion.",
      "The text examines the relationship between evolution and adaptation in marine life, touching on various cephalopods like vampire squids, octopuses, and nautiloids, alongside references to different academic fields and recent blog posts across various topics."
    ],
    "commentSummary": [
      "The conversation on crookedtimber.org focuses on the evolution of pinnipeds, marine biology by Jeanne Villepreux-Power, and their feeding habits in the 19th century.",
      "It explores the travel patterns of seals and potential Lovecraftian themes in the narrative, with references to fossils, geology, and nautiloids.",
      "The discussion intertwines various topics, providing a comprehensive view of the interconnectedness of marine science and history."
    ],
    "points": 361,
    "commentCount": 45,
    "retryCount": 0,
    "time": 1710693852
  },
  {
    "id": 39733275,
    "title": "LLM4Decompile: Open-source LLM for Decompiling Binary Code",
    "originLink": "https://github.com/albertan017/LLM4Decompile",
    "originBody": "LLM4Decompile Reverse Engineering: Decompiling Binary Code with Large Language Models For more details check out the paper. 0. Updates 2023.03.16 Add llm4decompile-6.7b-uo model which is trained without prior knowledge of the optimization levels (O0~O3), the average re-executability is arond 0.21. 1. Introduction of LLM4Decompile and Decompile-Eval Our objective is to create and release the first open-source LLM dedicated to decompilation, and to assess its capabilities by constructing the first decompilation benchmark focused on re-compilability and re-executable. We start by compiling a million C code samples from AnghaBench into assembly code using GCC with different configurations, forming a dataset of assembly-source pairs in 4 billion tokens. We then finetune the DeepSeek-Coder model, a leading-edge code LLM, using this dataset. Followed by constructing the evaluation benchmark, Decompile-Eval, based on HumanEval questions and test samples. Specifically, we formulate the evaluation from two perspectives: whether the decompiled code can recompile successfully, and whether it passes all assertions in the test cases. Figure 1 presents the steps involved in our decompilation evaluation. First, the source code (denoted as src) is compiled by the GCC compiler with specific parameters, such as optimization levels, to produce the executable binary. This binary is then disassembled into assembly language (asm) using the objdump tool. The assembly instructions are subsequently decompiled to reconstruct the source code in a format that's readable to humans (noted as src'). To assess the quality of the decompiled code (src'), it is tested for its ability to be recompiled with the original GCC compiler (re-compilability) and for its functionality through test assertions (re-executability). 2. Evaluation Results Metrics Re-compilability and re-executability serve as critical indicators in validating the effectiveness of a decompilation process. When decompiled code can be recompiled, it provides strong evidence of syntactic integrity. It ensures that the decompiled code is not just readable, but also adheres to the structural and syntactical standards expected by the compiler. However, syntax alone does not guarantee semantic equivalence to the original pre-compiled program. Re-executability provides this critical measure of semantic correctness. By re-compiling the decompiled output and running the test cases, we assess if the decompilation preserved the program logic and behavior. Together, re-compilability and re-executability indicate syntax recovery and semantic preservation - both essential for usable and robust decompilation. Results 3. How to Use The Model Our LLM4Decompile includes models with sizes between 1.3 billion and 33 billion parameters, and we have made these models available on Hugging Face. llm4decompile-1.3b llm4decompile-6.7b llm4decompile-33b llm4decompile-6.7b-nsp llm4decompile-6.7b-uo Note: The NSP model is trained with assembly code, the average re-executability is arond 0.17. Note: The unified optimization (UO) model is trained without prior knowledge of the optimization levels (O0~O3), the average re-executability is arond 0.21. The pre-processing of UO model is slightly different (no prior knowledge of the On), please check the model page. Here give an example of how to use our model. Preprocessing: compile the C code into binary, disassemble the binary into assembly instructions. import subprocess import os import re digit_pattern = r'\\b0x[a-fA-F0-9]+\\b'# binary codes in Hexadecimal zeros_pattern = r'^0+\\s'#0s OPT = [\"O0\", \"O1\", \"O2\", \"O3\"] fileName = 'path/to/file' with open(fileName+'.c','r') as f:#original file c_func = f.read() for opt_state in OPT: output_file = fileName +'_' + opt_state input_file = fileName+'.c' compile_command = f'gcc -c -o {output_file}.o {input_file} -{opt_state} -lm'#compile the code with GCC on Linux subprocess.run(compile_command, shell=True, check=True) compile_command = f'objdump -d {output_file}.o > {output_file}.s'#disassemble the binary file into assembly instructions subprocess.run(compile_command, shell=True, check=True) input_asm = '' with open(output_file+'.s') as f:#asm file asm= f.read() asm = asm.split('Disassembly of section .text:')[-1].strip() for tmp in asm.split(''): tmp_asm = tmp.split('\\t')[-1]#remove the binary code tmp_asm = tmp_asm.split('#')[0].strip()#remove the comments input_asm+=tmp_asm+'' input_asm = re.sub(zeros_pattern, '', input_asm) before = f\"# This is the assembly code with {opt_state} optimization:\"#prompt after = \"# What is the source code?\"#prompt input_asm_prompt = before+input_asm.strip()+after with open(fileName +'_' + opt_state +'.asm','w',encoding='utf-8') as f: f.write(input_asm_prompt) Decompilation: use LLM4Decompile to translate the assembly instructions into C: from transformers import AutoTokenizer, AutoModelForCausalLM import torch model_path = 'arise-sustech/llm4decompile-1.3b' tokenizer = AutoTokenizer.from_pretrained(model_path) model = AutoModelForCausalLM.from_pretrained(model_path,torch_dtype=torch.bfloat16).cuda() with open(fileName +'_' + opt_state +'.asm','r') as f:#original file asm_func = f.read() inputs = tokenizer(asm_func, return_tensors=\"pt\").to(model.device) with torch.no_grad(): outputs = model.generate(**inputs, max_new_tokens=500) c_func_decompile = tokenizer.decode(outputs[0][len(inputs[0]):-1]) 4. How to use Decompile-Eval Data are stored in llm4decompile/decompile-eval/decompile-eval.json, using JSON list format. There are 164*4 (O0, O1, O2, O3) samples, each with five keys: task_id: indicates the ID of the problem. type: the optimization stage, is one of [O0, O1, O2, O3]. c_func: C solution for HumanEval problem. c_test: C test assertions. input_asm_prompt: assembly instructions with prompts, can be derived as in our preprocessing example. To run the evaluation on single GPU and single process: cd LLM4Decompile python ./evaluation/run_evaluation_llm4decompile_singleGPU.py To run the evaluation using TGI (10x faster, support multiple GPUs and multi-process): First, please install the text-generation-inference following the official link git clone https://github.com/albertan017/LLM4Decompile.git cd LLM4Decompile pip install -r requirements.txt # Before run the evaluation script, plase update the model_path to your local mdoel path. bash ./scripts/run_evaluation_llm4decompile.sh 5. On Going LLM4Binary: We plan to include larger dataset to pre-train the model with assembly code and C code. Decompiler-ALL: Support popular languages/platforms and settings (e.g., decompile multiple functions). 6. License This code repository is licensed under the MIT License. 7. Contact If you have any questions, please raise an issue. 8. Thoughts The conversation about the language model decompiler that took place on Reddit roughly a year ago was quite fascinating to us. 9. Citation @misc{tan2024llm4decompile, title={LLM4Decompile: Decompiling Binary Code with Large Language Models}, author={Hanzhuo Tan and Qi Luo and Jing Li and Yuqun Zhang}, year={2024}, eprint={2403.05286}, archivePrefix={arXiv}, primaryClass={cs.PL} }",
    "commentLink": "https://news.ycombinator.com/item?id=39733275",
    "commentBody": "LLM4Decompile: Decompiling Binary Code with LLM (github.com/albertan017)353 points by Davidbrcz 23 hours agohidepastfavorite96 comments klik99 20 hours agoThis is a fascinating idea, but (honest question, not a judgement) would the output be reliable? It would be hard to identify hallucinations since recompiling could produce different machine code. Particularly if there is some novel construct that could be a key part of the code. Are there ways of also reporting the LLMs confidence in sections like this when running generatively? It’s an amazing idea but I worry it would stumble invisibly on the parts that are most critical. I suppose it would just need human confirmation on the output reply Eager 20 hours agoparentThis is why round-tripping the code is important. If you decompile the binary to source, then compile the source back to binary you should get the original binary. You just need to do this enough times until the loss drops to some acceptable amount. It's a great task for reinforcement learning, which is known to be unreasonably effective for these types of problems. reply thfuran 19 hours agorootparent>If you decompile the binary to source, then compile the source back to binary you should get the original binary. You really can't expect that if you're not using exactly the same version of exactly the same compiler with exactly the same flags, and often not even then. reply vasvir 4 hours agorootparentRight. A less formidable problem with higher chances of succeeding is from a given binary to figure out first compiler, compiler-version, compiler-flags. From there you could have a model for every combination or at least a model for the compiler variant and use the other info (version, flags) as input to the model. reply Eager 19 hours agorootparentprevYou try your best, and if you provide enough examples, it will undoubtedly get figured out. reply lolinder 18 hours agorootparentI think you're misunderstanding OP's objection. It's not simply a matter of going back and forth with the LLM until eventually (infinite monkeys on typewriters style) it gets the same binary as before: Even if you got the exact same source code as the original there's still no automated way to tell that you're done because the bits you get back out of the recompile step will almost certainly not be the same, even if your decompiled source were identical in every way. They might even vary quite substantially depending on a lot of different environmental factors. Reproducible builds are hard to pull off cooperatively, when you control the pipeline that built the original binary and can work to eliminate all sources of variation. It's simply not going to happen in a decompiler like this. reply blagie 18 hours agorootparentWell, no, but yes. The critical piece is that this can be done in training. If I collect a large number of C programs from github, compile them (in a deterministic fashion), I can use that as a training, test, and validation set. The output of the ML ought to compile to the same way given the same environment. Indeed, I can train over multiple deterministic build environments (e.g. different compilers, different compiler flags) to be even more robust. The second critical piece is that for something like a GAN, it doesn't need to be identical. You have two ML algorithms competing: - One is trying to identify generated versus ground-truth source code - One is trying to generate source code Virtually all ML tasks are trained this way, and it doesn't matter. I have images and descriptions, and all the ML needs to do is generate an indistinguishable description. So if I give the poster a lot more benefit of the doubt on what they wanted to say, it can make sense. reply lolinder 17 hours agorootparentOh, I was assuming that Eager was responding to klik99's question about how we could identify hallucinations in the output—round tripping doesn't help with that. If what they're actually saying is that it's possible to train a model to low loss and then you just have to trust the results, yes, what you say makes sense. reply blagie 15 hours agorootparentI haven't found many places where I trust the results of an ML algorithm. I've found many places where they work astonishingly well 30-95% of the time, which is to say, save me or others a bunch of time. It's been years, but I'm thinking back through things I've reverse-engineered before, and having something which kinda works most of the time would be super-useful still as a starting point. reply incrudible 17 hours agorootparentprevHave you ever trained a GAN? reply blagie 15 hours agorootparentTechnically, yes! A more reasonable answer, though, is \"no.\" I've technically gone through random tutorials and trained various toy networks, including a GAN at some point, but I don't think that should really count. I also have a ton of experience with neural networks that's decades out-of-date (HUNDREDS of nodes, doing things like OCR). And I've read a bunch of modern papers and used a bunch of Hugging Face models. Which is to say, I'm not completely ignorant, but I do not have credible experience training GANs. reply weinzierl 1 hour agorootparentprevThat's true but a solvable problem. I once tried to reproduce the build of an uncooperative party and it was mainly tedious and boring. The space of possible compiler arguments is huge, but ultimately what is actually used is mostly on a small surface. Apart from that, I wrote a small tool to normalize the version string, timestamps and file path' in the binaries before I compared them. I know there are other sources of non-determinism, but these three things were enough in my case. The hardest part were the numerous file path' from the build machine. I had not expected that. In hindsight, stripping both binaries before comparison might have helped, but I don't remember why I didn't do that. reply junon 38 minutes agorootparentprevErr, no, sorry, it won't. Compilers don't work that way. There's a lot of ways to compile down source to machine code and the output changes from compiler version to compiler version. The LLM would have to know exactly how the compiler worked at which version to do this. So the idea is technically possible but not technically feasible. reply thfuran 19 hours agorootparentprevWhat exactly are you suggesting will get figured out? reply spqrr 19 hours agorootparentThe mapping from binary to source code. reply thfuran 18 hours agorootparentEven ignoring all sources of irreproducibility, there does not exist a bijection between source and binary artifact irrespective of tool chain. Two different toolchains could compile the same source to different binaries or different sources to the same binary. And you absolutely shouldn't be ignoring sources of irreproducibility in this context, since they'll cause even the same toolchain to keep producing different binaries given the same source. reply achrono 18 hours agorootparentExactly, but neither the source nor the binary is what's truly important here. The real question is: can the LLM generate the functionally valid source equivalent of the binary at hand? If I disassemble Microsoft Paint, can I get code that will result in a mostly functional version of Microsoft Paint, or will I just get 515 compile errors instead? reply Brian_K_White 15 hours agorootparentThis is what I thought the question was really about. I assume that an llm will simply see patterns that look similar to other patterns and make assosciations and assume ewuivalences on that level, meanwhile real code is full of things where the programmer, especially assembly programmers, modify something by a single instruction or offset value etc to get a very specific and functionally important result. Often the result is code that not only isn't obvious, it's nominaly flatly wrong, violating standards, specs, intended function, datasheet docs, etc. If all you knew were the rules written in the docs, the code is broken and invalid. Is the llm really going to see or understand the intent of that? They find matching patterns in other existing stuff, and to the user who can not see the infinite body of that other stuff the llm pulled from, it looks like the llm understood the intent of a question, but I say it just found the prior work of some human who understood a similar intent somewhere else. Maybe an llm or some other flavor of ai can operate some other way like actually playing out the binary like executing in a debugger and map out the results not just look at the code as fuzzy matching patterns. Can that take the place of understanding the intents the way a human would reading the decompiled assembly? Guess we'll be finding out sooner of later since of course it will all be tried. reply layer8 18 hours agorootparentprevThe question was about the reverse mapping. reply fao_ 18 hours agorootparentprevExcept LLMs cannot reason. reply LoganDark 10 hours agorootparentLLMs can mimic past examples of reasoning from the dataset. So, it can re-use reasoning that it has already been trained on. If the network manages to generalize well enough across its training data, then it can get close to reproducing general reasoning. But it can't yet fully get there, of course. reply dheera 15 hours agorootparentprevMaybe we then need an LLM to tell us if two pieces of compiled code are equivalent in an input-output mapping sense (ignoring execution time). I'm actually serious; it would be exceedingly easy to get training data for this just by running the same source code through a bunch of different compiler versions and optimization flags. reply saagarjha 8 hours agorootparentAn LLM cannot do this. I don’t even mean this in a formal sense, because your problem is addressed by Rice’s Theorem, which places bounds on what any system (LLM or not) can do here; I mean it in the sense that an LLM isn’t even appropriate to use here because the best it can possibly do is provide you with its best guess at the answer. And while this might be a useful property for decompilation in general that’s not what was being discussed here. reply dheera 2 hours agorootparentRice's theorem does NOT prevent a program from giving correct answers to non-trivial properties of programs (including the halting problem or other undecidable problems) for 99.99% of inputs and \"I don't know\" for 0.01% of inputs. It only states that you cannot write a program that provides a correct and definitive yes-or-no for 100% of inputs. For a decompiler, being able to decompile even 90% of programs would be awesome. We're not looking for theoretical perfectness. reply thfuran 14 hours agorootparentprevWhy would an llm be the tool for that job? reply dheera 12 hours agorootparentWithout analytical thinking how else would you come to conviction that two functions are identical, for a computationally unfeasible number of possible inputs? reply codethief 18 hours agorootparentprev> you should get the original binary According to the project's README, they only seem to be checking mere \"re-compilability\" and \"re-executability\" of the decompiled code, though. reply 1024core 16 hours agorootparentprev> If you decompile the binary to source, then compile the source back to binary you should get the original binary. Doesn't that depend on the compiler's version though? Or, for that matter, even the sub-version. Every compiler does things differently. reply fulafel 2 hours agorootparentFrom the README: > By re-compiling the decompiled output and running the test cases, we assess if the decompilation preserved the program logic and behavior. As this is in the metrics section, I guess fully automating this is not part of the research. reply GoblinSlayer 1 hour agoparentprevSounds like it should be able to split the code into functions with inferred API, then you should be able to fuzz these functions in binary and source versions. reply userbinator 13 hours agoparentprevLLMs are by nature probabilistic, which is why they work reasonably well for \"imprecise\" domains like natural language processing. Expecting one to do decompilation, or disassembly for that matter, is IMHO very much a \"wrong tool for the job\" --- but perhaps it's just an exploratory exercise for the \"just use an LLM\" meme that seems to be a common trend these days. The bigger argument against the effectiveness of this approach is that existing decompilers can already do a much better job with far less processing power. reply czl 11 hours agorootparentIn the future efficient rule based compilers and decompiler may be generated by AI systems trained on inputs and outputs of what we use today. This effort is an exploration to find a radically different AI way that may give superior results. Yes. For all the reasons you give above, AI for this job is not practical today. reply layer8 18 hours agoparentprevThe way to do this is to have a formal verification tool that takes the input, the output, and a formal proof that the input matches the semantics of the output, and have the LLM create the formal proof alongside the output. Then you can run the verification tool to check if the LLM’s output is correct according to the proof that it also provided. Of course, building and training an LLM that can provide such proofs will be the bigger challenge, but it would be a safe a way to detect hallucinations. reply natsch 17 hours agorootparentThat would require the tool to prove the equivalence of the two programs, which is generally undecidable. Maybe this could be weakened to preserving some properties of the program. reply layer8 16 hours agorootparentNo, it would not. It would require the LLM to provide a proof for the program that it outputs, which seems reasonable in the same way that a human decompiling a program would be able to provide a record of his/her reasoning. The formal verifier would then merely check the provided proof, which is a simple mechanical process. This is analogous to a mathematician providing a detailed proof and a computer checking it. What is impossible due to undecidability is for two arbitrary programs, to either prove or disprove their equivalence. However, the two programs we are talking about are highly correlated, and thus not arbitrary at all with respect to each other. If an LLM is able to provide a correct decompilation, then in principle it should also be able to provide a proof of the correctness of that decompilation. reply saagarjha 8 hours agorootparentYes, and then someone needs to check that proof. It’s not particularly clear if that decompilation proof would be any more helpful than just doing the lifting by hand. reply logicchains 1 hour agorootparentNo, nobody needs to check the proof; that's the whole point of formal theorem proving, the machine checks it for you. reply ngruhn 17 hours agorootparentprevThat doesn’t mean that it’s impossible, right? Just that no tool is guaranteed to give an answer in any case. And those cases might be 90%, 10% or it-doesn’t-matter-in-practice % reply djinnandtonic 18 hours agorootparentprevWhat if there are hallucinations in the verification tool? reply thfuran 18 hours agorootparentThen it's not a formal verification tool. Generative models are profoundly unfit for that purpose. reply layer8 18 hours agorootparentprevThere may be bugs, but not hallucinations. Bugs are at least reproducible, and the source code of the verification tool is much, much smaller than an LLM, so has a much higher chance of its finite number of bugs to be found, whereas with an LLM it is probably impossible to remove all hallucinations. To turn your question around: What if the compiler that compiles your LLM implementation “hallucinates”? That would be the closer parallel. reply smellf 18 hours agorootparentprevI think the idea is that you'd have two independently-develooed systems, one LLM decompiling the binary and the other LLM formally verifying. If the verifier disagrees with the decompiler you won't know which tool is right and which is wrong, but if they agree then you'll know the decompiled result is correct, since both tools are unlikely to hallucinate the same thing. reply layer8 18 hours agorootparentNo, the idea is that the verifier is a human-written program, like the many formal-verification tools that already exist, not an LLM. There is zero reason to make this an LLM. It makes sense to use LLMs for the decompilation and the proof generation, because both arguably require creativity, but a mere proof verifier requires zero creativity, only correctness. reply thfuran 18 hours agorootparentprevGood luck formally proving Linux. reply layer8 18 hours agorootparentThe goal is to prove that the source code matches the machine code, not to prove that the code implements some intended higher-level semantics. This has nothing to do with formally proving the correctness of the Linux kernel. reply londons_explore 15 hours agoparentprevEven if it isn't fully reliable, often it's only necessary to modify a few functions for most changes one wants to make to a binary. You'd therefore only need to recompile those few functions. reply afro88 16 hours agoparentprevThe detail how they measure this in the readme. This is directed at all the sibling comments as well! TLDR they recompile and then re-execute (including test suites). From the results table it looks like GPT4 still \"outperforms\" their model in recompilation, but their recompiled code has a much better re-execution success rate (less hallucinations). But, that re-execution rate is still pretty lacking (around 14%), even if better than GPT4. reply riedel 19 hours agoparentprevOne could as well use differential fuzzing. reply klik99 15 hours agorootparentI'm amazed that there are so many good responses above only this mentions fuzzing. In the context of security, inputs might be non-linear things like adjacent memory, so I don't see anyway to be confident about equilivancy without substantial fuzzing. Honestly I just don't see a way to formally verify this at all, it's sounds like it could be a very useful tool but I don't see a way for it to be fully confident. But, heck, just getting you 90% of the way towards understanding it with LLMs is still amazing and useful in real life. reply sebastianconcpt 19 hours agoparentprevGenerators' nature is to hallucinate. reply DougBTX 16 hours agorootparentOne man’s hallucination is another’s creativity. reply sebastianconcpt 11 hours agorootparentWell we need to remember that \"hallucination\" here is not a concept but a language figure for the output of a stochastic parroting machine. So what you mentinoed would be a digitally induced halluciation out of some dancing matrix multiplications / electrons on silicon. reply madisonmay 20 hours agoprevThis is an excellent use case for LLM fine-tuning, purely because of the ease of generating a massive dataset of input / output pairs from public C code reply bt1a 14 hours agoparentI would also think that generating a very large amount of C code using coding LLMs (using deepseek, for example, + verifying that the output compiles) as synthetic training data would be quite beneficial in this situation. Generally the quality of synthetic training data is one of the main concerns, but in this case, the ability for the code to compile is the crux. reply YeGoblynQueenne 12 hours agoprevIf I read the \"re-executability\" results in the Results figure right then that's a great idea but it doesn't really work: https://raw.githubusercontent.com/albertan017/LLM4Decompile/... To clarify: >> Re-executability provides this critical measure of semantic correctness. By re-compiling the decompiled output and running the test cases, we assess if the decompilation preserved the program logic and behavior. Together, re-compilability and re-executability indicate syntax recovery and semantic preservation - both essential for usable and robust decompilation. reply nebula8804 22 hours agoprevWill be interesting to see is there is some way to train a decompilation module based on who we know developed the application and use their previous code used as training. For example: Super Mario 64 and Zelda 64 were fully decompiled and a handful of other N64 games are in the process. I wonder if we could map which developers worked on these two games (maybe even guess who did what module) and then use that to more easily decompile any other game that had those developers working on it. If this gets really good, maybe we can dream of having a fully de-obfuscated and open source life. All the layers of binary blobs in a PC can finally be decoded. All the drivers can be open. Why not do the OS as well! We don't have to settle for Linux, we can bring back Windows XP and back port modern security and app compatibility into the OS and Microsoft can keep their Windows 11 junk...at least one can dream! :D reply userbinator 13 hours agoparentIf this gets really good, maybe we can dream of having a fully de-obfuscated and open source life. All the layers of binary blobs in a PC can finally be decoded. All the drivers can be open. Why not do the OS as well! Decompilers already exist and are really good. If an LLM can do the same as these existing compilers, you can bet the lawyers will consider it an equivalent process. The main problem is legal/political, not technical. reply coddle-hark 17 hours agoparentprevI wrote my bachelor thesis on something tangential — basically, some researchers found that it was possible in some very specific circumstances to train a classifier to do author attribution (i.e. figure out who wrote the program) based just on the compiled binaries they produced. I don’t think the technique has been used for anything actually useful, but it’s cool to see that individual coding style survives the compilation process, so much so that you can tell one person’s compiled programs apart from another’s. reply astrange 44 minutes agorootparentDo you mean the whole binary or just the text segment/instructions? Because I think this gets a lot easier if you can look at the symbol table, strings, and codesigning certificate. reply ZitchDog 21 hours agoparentprevI doubt the code would be identifiable. It wouldn’t be the actual code written, but it would be very similar. But I assume many elements of code style would be lost, and any semblance of code style would be more or less hallucinated. reply K0IN 20 hours agorootparentif it can make test from the decompiled code, we could reimplement it with our code style. might be cool to have some bunch of llms working together with feedback loops. reply a2code 21 hours agoprevThe problem is interesting in at least two aspects. First, an ideal decompiler would eliminate proprietary source code. Second, the abundant publicly available C code allows you to simply make a dataset of paired ASM and source code. There is also a lot of variety with optimization level, compiler choice, and platform. What is unclear to me is: why did the authors fine-tune the DeepSeek-Coder model? Can you train an LLM from zero with a similar dataset? How big does the LLM need to be? Can it run locally? reply mike_hearn 16 hours agoparentMost proprietary code runs behind firewalls and won't be affected by this one way or another. It's basically always better to start training with a pre-trained model rather than random, even if what you want isn't that close to what you start with. reply saagarjha 8 hours agoparentprevIdeal decompilers do not exist. In some sense they can never exist as compilers are lossy, but even taking a liberal view of “high level understanding of the resulting code” this is essentially the AGI for computer security. Nobody has come close to it! reply 3abiton 21 hours agoparentprevI assume it's related to the cost of training vs fine-tuning. It could be also a starting point to validate an idea. reply kukas 22 hours agoprevHey, I am working on my own LLM-based decompiler for Python bytecode (https://github.com/kukas/deepcompyle). I feel there are not many people working on this research direction but I think it could be quite interesting, especially now that longer attention contexts are becoming feasible. If anyone knows a team that is working on this, I would be quite interested in cooperation. reply maple3142 56 minutes agoparentThere is [PyLingual](https://pylingual.io/), but it is not open source unfortunately. I am not sure if it is also LLM based. reply a2code 12 hours agoparentprevWhy Python? First, python is a language with a large open-source library. Second, I do not think it is used for software that is distributed as binaries? reply Retr0id 8 hours agorootparentClosed-source python exists, and it is frequently distributed in compiled binaries (especially in mediocre malware). As a (supposedly) non-malicious example, the \"Nightshade\" watermarking tool is distributed as closed-source pre-compiled Python https://nightshade.cs.uchicago.edu/downloads.html reply ok123456 17 hours agoparentprevIs there a benefit from using an LLM for Python byte code? Python byte code is high enough level that it's possible to translate it directly to source code from my experience. reply kukas 16 hours agorootparentMy motivation is that the existing decompilers work only for Python versions till ~3.8. Having a model that could be finetuned with every new Python version release might overcome the need for highly specialized programmer that is able to update the decompiler to be compatible with the new version. It is also a toy example for me to set up a working pipeline and then try to decompile more interesting targets. reply saagarjha 7 hours agoprevThe approach here is interesting in that it answers a question a lot of people have been asking: “what happens if we pipe a binary into a trained LLM and ask it to decompile it?” The answer is that it doesn’t really work at all right now! This is a surprising result because the design of the paper kind of doesn’t allow for any other conclusion to be drawn. Notably, if the LLM did a really good job in the evaluation they designed it would still be unclear whether it was actually useful, because the test “does it compile and pass a few test cases” is not actually a very good way to test a decompiler. A couple people here have suggested that the generated decompilation should match the source code exactly, which is a challenging thing to achieve and still hotly debated on whether it is a good metric or not. But the results here show that we’re starting to barely get past the “does it produce code” stage and move towards “does it produce code that looks vaguely correct” status but we’re definitely not there yet. Future steps of “is this a useful tool to drive decompilation” and “does this do better than state of the art” and “is this perfect at decompiling things” are still a long ways away. So it’s good to look at as a negative result as this area continues to attract new interest. reply potatoman22 22 hours agoprevIt's interesting the 6b model outperforms the 33b model. I wonder if it means the 33b model needs more training data? It was pretrained on ~1 million C programs, compared to DeepSeek-Coder, which was trained on 2 trillion tokens, which is a few orders of magnitude more data. I'm also curious about how this compares to non-LLM solutions. reply mattashii 18 hours agoparent> on ~1 million C programs, compared to [...] 2 trillion tokens, which is a few orders of magnitude more data. Is that comparable like that? This would assume that the average C program of the set is orders (plural) of magnitude less than 2m tokens in size, which could indeed be true but sounds like an optimistic assumption. reply Der_Einzige 18 hours agoparentprevThis has been the dynamics with LLMs for awhile. The majority of LLMs are massively undertrained. 7b models are the least \"undertrained\" mainstream models we have, hence why they have proliferated so much among the LLM fine-tuning community. reply mahaloz 13 hours agoprevIt’s always cool to see different approaches in this area, but I worry its benchmarks are meaningless without a comparison of non-AI based approaches (like IDA Pro). It would be interesting to see how this model holds up on metrics from previous papers in security. reply sinuhe69 15 hours agoprevFor me the huge difference between re-compilability and re-excuteability scores is very interesting. GTP4 achieved 8x% on re-compilability (syntactically correct) but abysmal 1x% in re-excutability (schematically correct) demonstrated once again its overgrown mimicry capacity. reply sitkack 14 hours agoparent> overgrown mimicry I don't think it shows that. GPT4 was not trained on decompiling binaries back into C. Amazing result for an untrained task. We are soon going to have robust toolchain detection from binaries, and source recovery with variable and function names. reply kken 22 hours agoprevPretty wild how well GPT4 is still doing in comparison. It's significantly better than their model at creating compilable code, but is less accurate at recreating functional code. Still quite impressive. reply maCDzP 22 hours agoprevCan this be used for deobfuscation of code? I really hadn’t thought about LLM being a tool during reverse engineering. reply Tiberium 21 hours agoparentBig LLMs like GPT-4 (and even GPT 3.5 Turbo) can be directly used to beautify obfuscated/minified JS, see e.g. https://thejunkland.com/blog/using-llms-to-reverse-javascrip... and https://news.ycombinator.com/item?id=34503233 reply Eager 19 hours agoparentprevI have tried feeding some of the foundation models obfuscated code from some of the competitions. People might think that the answers would be in the training data already, but I didn't find that to be the case. At least in my small experiments. The model's did try to guess what the code does. They would say things like, \"It seems to be trying to print some message to the console\". I wasn't able to get full solutions. It's definitely worth more research, not just as a curiosity, but these kinds of problems are good proxies for other tasks and also excellent benchmarks for LLMs particularly. reply evmar 18 hours agoparentprevI did a little experiment with this here: https://neugierig.org/software/blog/2023/01/compiling-advent... reply AndrewKemendo 18 hours agoprevIf successful wouldn’t you be replicating the compilers machine code 1:1? In which case that means fully complete code can live in the “latent space” but is distributed as probabilities Or perhaps more likely would it be replicating the logic only, which can then be translated into the target language I would guess that any binary that requires a non-deterministic input (key, hash etc…) to compile would break this Fascinating reply jagrsw 21 hours agoprevDecompilation is somewhat a default choice for ML in the world of comp-sec. Searching for vulns and producing patches in source code is a bit problematic, as the databases of vulnerable source code examples and their corresponding patches are neither well-structured nor comprehensive, and sometimes very, very specific to the analyzed code (for higher abstraction type of problems). So, it's not easy to train something usable beyond standard mem safety problems and use of unsafe APIs. The area of fuzzing is somewhat messy, with sporadic efforts undertaken here and there, but it also requires a lot of preparatory work, and the results might not be groundbreaking unless we reach a point where we can feed an ML model the entire source code of a project, allowing it to analyze and identify all bugs, producing fixes and providing offending inputs. i.e. not yet. While decompilation is a fairly standard problem, it is possible to produce input-output pairs somewhat at will based on existing source code, using various compiler switches, CPU architectures, ABIs, obfuscations, syscall calling conventions. And train models on those input-output pairs (i.e. in reversed order). reply mdaniel 15 hours agoprevrelevant: https://news.ycombinator.com/item?id=34250872 (G-3PO: A protocol droid for Ghidra, or GPT-3 for reverse-engineering ; Jan, 2023; 44 comments) ed: seems they have this, too, which may value your submission: https://github.com/tenable/awesome-llm-cybersecurity-tools#a... reply Nuzzerino 10 hours agoprevHow does it actually compare to non-LLM decompilers IDA, Binja, etc? I only see comparisons with other LLMs. reply xvilka 4 hours agoprevI think using higher-level input, e.g. the intermediate language like RzIL[1] could produce better results and is more scalable for making such decompliation multiplatform. As RzIL text form resemples SMT, it should make LLM easier to \"understand\" the meaning. Moreover, information from binary such as symbols, signatures, debug information (DWARF, PDB, etc) could enrich the result further. You can download Rizin[2] and try for yourself by calling `aaa` then `plf` for any chosen functions for architectures supported by RzIL. See the example excerpt for a function with this disassembly: │ │ 0x140007e51 movsd qword [rdi + 0x50], xmm2 │ │ 0x140007e56 mov qword [rdi + 0x48], 0 │ │ 0x140007e5e call sym.rz_test.exe_ht_pp_free ; sym.rz_test.exe_ht_pp_free │ │ 0x140007e63 movaps xmm7, xmmword [var_38h] │ │ 0x140007e68 movaps xmm6, xmmword [var_28h] │ │ 0x140007e6d mov rbp, qword [var_10h] │ └─> 0x140007e72 add rsp, 0x48 │ 0x140007e76 pop r15 │ 0x140007e78 pop rdi └ 0x140007e79 ret 0x140007e6d (set rbp (loadw 0 64 (+ (var rsp) (bv 64 0x68)))) 0x140007e72 (seq (set op1 (var rsp)) (set op2 (bv 64 0x48)) (set sum (+ (var op1) (var op2))) (set rsp (var sum)) (set _result (var sum)) (set _popcnt (bv 8 0x0)) (set _val (cast 8 false (var _result))) (repeat (! (is_zero (var _val))) (seq (set _popcnt (+ (var _popcnt) (ite (lsb (var _val)) (bv 8 0x1) (bv 8 0x0)))) (set _val (>> (var _val) (bv 8 0x1) false)))) (set pf (is_zero (mod (var _popcnt) (bv 8 0x2)))) (set zf (is_zero (var _result))) (set sf (msb (var _result))) (set _result (var sum)) (set _x (var op1)) (set _y (var op2)) (set cf (|| (|| (&& (msb (var _x)) (msb (var _y))) (&& (! (msb (var _result))) (msb (var _y)))) (&& (msb (var _x)) (! (msb (var _result)))))) (set of (|| (&& (&& (! (msb (var _result))) (msb (var _x))) (msb (var _y))) (&& (&& (msb (var _result)) (! (msb (var _x)))) (! (msb (var _y)))))) (set af (|| (|| (&& (msb (cast 4 false (var _x))) (msb (cast 4 false (var _y)))) (&& (! (msb (cast 4 false (var _result)))) (msb (cast 4 false (var _y))))) (&& (msb (cast 4 false (var _x))) (! (msb (cast 4 false (var _result)))))))) 0x140007e76 (seq (set r15 (cast 64 false (loadw 0 64 (+ (var rsp) (bv 64 0x0))))) (set rsp (+ (var rsp) (bv 64 0x8)))) 0x140007e78 (seq (set rdi (loadw 0 64 (+ (var rsp) (bv 64 0x0)))) (set rsp (+ (var rsp) (bv 64 0x8)))) 0x140007e79 (seq (set tgt (loadw 0 64 (+ (var rsp) (bv 64 0x0)))) (set rsp (+ (var rsp) (bv 64 0x8))) (jmp (var tgt))) [1] https://github.com/rizinorg/rizin/blob/dev/doc/rzil.md [2] https://rizin.re reply speedylight 15 hours agoprevI have thought about doing something similar for heavily obfuscated JavaScript. Very useful for security research I imagine! reply quantum_state 14 hours agoprevIt seems the next logical step would be LLMAssistedHacking to turn things up side down… reply ReptileMan 18 hours agoprevLet's hope it kills Denuvo ... reply Retr0id 8 hours agoparentDecompilation and deobfuscation are related but distinct tasks reply m3kw9 17 hours agoprevBasically predicting code token by token except now you don’t even have a large enough context size and worse, you are using RAG reply xorvoid 16 hours agoprev [–] As someone who is actively developing a decompiler to reverse engineer old DOS 8086 video games, I'd have a hard time trusting an LLM to do this correctly. My standard is accurate semantics lifting from Machine Code to C. Reversing assembly to C is very delicate. There are many patterns that tend to usually map to obvious C constructs... except when they don't. And that assumes the original source was C. Once you bump into routines that were hand-coded assembly and break every established rule in the calling conventions, all bets are off. I'm somewhat convinced that decompilation cannot be made fully-automatic. Instead a good decompiler is just a lever-arm on the manual work a reverser would otherwise be doing. Corollary: I'm also somewhat convinced that only the decompiler's developers can really use it most effectively because they know where the \"bodies are buried\" and where different heuristics and assumptions were made. Decompilers are compilers with all the usual engineering challenges, plus a hard inference problem tacked on top. All that said, I'm not a pessimist on this idea. I think it has pretty great promise as a technique for general reversing security analysis where the reversing is done mostly for \"discovery\" and \"understanding\" rather than for perfect semantic lifting to a high-level language. In that world, you can afford to develop \"hypotheses\" and then drill down to validate if you think you've discovered something big. Compiling and testing the resulting decompilation is a great idea. I do that as well. The limitation here is TEST SUITE. Some random binary doesn't typically come with a high-coverage test suite, so you have to develop your own acceptance criterion as you go along. In other words: write tests for a function whose computation you don't understand (ha). I suppose a form of static-analysis / symbolic-computation might be handy here (I haven't explored that). Here you're also beset with challenges of specifying which machine state changes are important and which are superfluous (e.g. is it okay if the x86 FLAGS register isn't modified in the decompiled version, probably yes, but sometimes no). In my case I don't have access to the original compiler and even if I did, I'm not sure I could convince it to reproduce the same code. Maybe this is more feasible for more modern binaries where you can assume GCC, Clang, MSVC, or ICC. At any rate: crazy hard, crazy fun problem. I'm sure LLMs have a role somewhere, but I'm not sure exactly where: the future will tell. My guess is some kind of \"copilot\" / \"assistant\" type role rather than directly making the decisions. (If this is your kind of thing... I'll be writing more about it on my blog soonish...) reply a2code 12 hours agoparentI would devise a somewhat loose metric. Consider you assign a percentage as to how much a binary is disassembled. As in, 0% means the binary is in assembly and 100% means the whole binary is now C code. The ideal decompiler would result in 100% for any binary. My prediction is that this percentage will increase with time. It would be interesting to construct data for this metric. It is important to define the limitations of using LLMs for this endeavor. I would like to emphasize your subtle point. The compiler used for the original binary may not be the same as the one you use. The probability of this increases with time, as compilers improve or the platform on which the binary runs becomes obsolete. This is a problem for validation, as in you cannot directly compare original assembly code with assembly after compiling C code (that came from decompiling). Perhaps assembly routines could be given a likelihood, as in how sure the LLM is that some C code maps to assembly. Then, routines with hand-coded assembly would have a lower likelihood. reply ouraf 10 hours agoparentprev [–] I'm curious about the decompilation process. I know compiling is a lossy process and optimization can make things even harder to remap, but if an LLM can recognize patterns correctly, chunk or classify each routine or even give a more palatable overview of what a part of the code is meant to do step by step, it becomes closer to what HexRays offer with their assembly to pseudo code translator. And from that point, it can make serviceable translations to real world languages. LLMs won't replace an engineer, but maybe they can help romhackers in identifying bugs or how some values are calculated by a game. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The \"LLM4Decompile Reverse Engineering\" paper unveils the first open-source Large Language Model (LLM) focused on decompilation, emphasizing re-compilability and re-executability.",
      "The model is trained on assembly-source pairs to regenerate source code from assembly instructions, showcasing its effectiveness in maintaining syntax and semantics.",
      "The project offers various models for utilization, along with evaluation data, running instructions, and a roadmap to enlarge the dataset and accommodate multiple languages/platforms under the MIT License."
    ],
    "commentSummary": [
      "The discussion on LLM4Decompile involves concerns about the reliability of decompiled binary code and the variations in recompiled machine code. Suggestions include round-tripping the code, using reinforcement learning, and identifying compiler information for better accuracy.",
      "Current technology may not be advanced enough for widespread use of LLMs for decompilation and compilation processes, highlighting challenges in proving program equivalence and formal theorem proving.",
      "Utilizing LLMs in decompiling and code analysis tasks presents complexities, challenges, and potential applications, including author attribution through compiled binaries and training decompilation modules on known coding styles."
    ],
    "points": 353,
    "commentCount": 96,
    "retryCount": 0,
    "time": 1710670523
  },
  {
    "id": 39735675,
    "title": "Nanofont3x4: Pushing the Limits of Readability (2015)",
    "originLink": "https://github.com/Michaelangel007/nanofont3x4",
    "originBody": "nanofont3x4 The world's smallest readable 3x4 font with readable lowercase! Includes: Upper case (3x3) plus 1 pixel leading (hence the name 3x4 for honesty), Lower case (some are 2x2!) All ASCII symbols Practicality I know what you're thinking .. how the hell is this font, especially the lowercase 2x2 glyphs, even practical?? Once the novelty wears off a \"practical\" example would be rendering \"in-game book pages\" that don't look like complete gibberish, or an \"accurate print preview\" with real text instead of blurry placeholder pixels that don't even look close to being the glyphs scaled down. Motivation Why? Why even attempt to do the \"impossible\" task of creating the worlds smallest readable lowercase font? First, what the heck does it even mean to say \"world's smallest font?\" This is, as far as I know as of July 2015, the only font that has readable 2x2 lowercase glyphs. Please let me know if you find another one! Just how small of a font size can we go? 3x3? Yes, these have been done for upper case. See the References at the end. What about 2x2? Can we even design readable lowercase glyphs that can even fit into a 2x2 grid? Also, what is the minimal leading? How does leading effect readability? The short answer to motivation is: To answer the unknown. The long answer is: Partially for the challenge, but mostly because only by pushing a craft to its maximum limits does it force oneself to take a step back and really analyze what the goal is, and think laterally on how it might even be possible. What is the \"essence\" of a glyph? What makes a tiny glyph readable anyways? What makes an 'a' an 'a', an 'e' an 'e', or an 's' an 's' when you only have 3x3 or 2x2 pixels to work with? Since there are no \"extraneous\" pixels to \"fallback\" onto then every pixel becomes that much more important. Even a 1 pixel mistakes really stands out. It was this quest for self discovery and understanding this \"Tao of Typography\" that this project was born. Uppercase 3x3 A 3x3 uppercase has been \"solved\" or \"known\" for quite sometime as I mentioned above. This seemed like a good place to start. If we start with a 3x3 uppercase font then does that imply that the lowercase glyph must be focused around a 2x2 cell? Let's find out! Lowercase and 2x2 A 2x2 grid has 2^4 = 16 permutations. That's only 16 choices for 26 lowercase letters! In actuality readability is the most important goal so the following lowercase glyphs are not 2x2: `b` `d` `f` `g` `h` `j` `k` `l` `m` `n` `p` `q` `t` `u` `v` `w` `y` That leaves these 9 glyphs to fit inside a 2x2 cell. a c e i o r s x z Here are all 16 permutations of a 2x2 glyph cell: .. not usable = space .. .. not usuable = period but wrong kerning .x .. not usable = period x. .. not usable, confused with `_` xx .x no meaning, wrong kerning .. .x not usable, wrong kerning .x .x chosen as `s x. .x chosen as `a` xx x. no meaning .. x. chosen as `z` .x x. chosen as `i` x. x. chosen as `e` xx xx no meaning, .. xx no meaning .x xx chosen as `r` x. xx chosen as `c` `o` and `x` xx Problem words What words are difficult to read? Believe it or not, most words are actually readable (once you get used to the font.) Since we have 3 ambigious glyphs, the troublesome words are anything with a 'co' or 'x' in it such as: exercise becomes compliance A quick search of frequency analysis, \"frequency of letter pairs\", reveals that the pairs oo and co show up often enough that they will be annoying to \"decode\" the context. If we could somehow distingush between c and o I estimate we could reach ~ 99% readability. Oh, what that 1 extra vertical pixel adds! But alas, we'll have to settle for \"mostly readable.\" Still, I'm happy with this considering I thought the task was (almost) impossible when I first started. Enough already! Where are the pictures? Well, alrighty then without further ado ... Texture Atlas: Forced upper case output: nanofont -u Normal case output: Upper case on its own source code; with default 0 px leading: nanofont3x4 -u nanofont3x4.cpp Forced upper case with 1 px leading: nanofont -u -1 nanofont3x4.cpp Forced upper case with 2 px leading: nanofont -u -2 nanofont3x4.cpp Manual Fake Bold You can easily do a manual fake bold by dimming the font and brightening the areas you want bolded. I regret I don't have code for this. The reason for the funny dimensions is so that the resolution maps 1:1 on the iPhone 5. Italic? The famous but annoying \"Left as an exercise for the reader.\" :-) One would have to do a proper greyscale anti-aliasing partial-texel offset to get italics. Maybe someone else will take up the challenge? Uber 4x4 Texture Atlas All Permutations In case you are interested, there are a total of 65,536 4x4 monochrome glyphs. Here is a uber texture atlas that shows all of them with our glyphs highlighted (red) where they are in the table. Blue borders are used to show the cell boundaries. Related work Simon Whitechapel, back in 2004 attempted to create a 3x3 font with lowercase. He has lower case glyphs but note that the mid-line is all over the place for glyphs such as c and p. http://web.onetel.com/~amygdala/articles/scimaths/3x3.htm Anders de Flon created a 3x3 font but it is upper case only. https://en.wikipedia.org/wiki/3x3 Ken Perlin provided a 4x6 tiny font (2006, and again in 2010) but didn't provide any source code! WTF? :-( http://mrl.nyu.edu/~perlin/homepage2006/tinyfont/ http://blog.kenperlin.com/?p=6804 Domenico Mazza’s \"Zepto\" 3x5 font: http://makezine.com/2010/11/19/a-tiny-screen-font-you-can-actually/ \"How small can you draw all 16 hex digits\"; included 2x3 (!!), 2x4, 3x4 and 3x5. https://www.allegro.cc/forums/thread/606221 Blame and Shame To all researchers and scientists who don't make your code, and more importantly, your data available for independent verifcation -- get with the program, please. \"If I have seen further, it is by standing on the shoulders of giants\" -- popularized by Isaac Newton, attributed to Bernard of Chartres. Name and Fame Special thanks to FontStruct dpla for the motivation to do the impossible! Greetings and Thanks To all people obsessed with pixel fonts: Thank-You for sharing your work! Your stubborness to not accept reality for what it is, but to always push the boundaries of what is thought possible is an inspiration and reminder for us all to always strive and \"reach for the stars.\" What we learn along the way makes the journey worthwhile.",
    "commentLink": "https://news.ycombinator.com/item?id=39735675",
    "commentBody": "Nanofont3x4: Smallest readable 3x4 font with lowercase (2015) (github.com/michaelangel007)320 points by lsferreira42 17 hours agohidepastfavorite97 comments berkes 28 minutes ago\"readable\". I'm 45, been looking at screens for 30+ years almost daily now. There's no way I can read that with or without my glasses. It's truly impressive feat to get it this small. I used to do texts like this, often manually, pixel by pixel. For websites and games, it was all the rage back in late nineties early 2000s. But only now I start to truly understand,by experiencing first-hand, why accessibility matters. This font is awesome, but terrible for any kind of accessibility. reply user_of_the_wek 5 minutes agoparentI guess it's only meant for devices with (a smaller number of) large pixels. Your HiDPI monitor or retina iPhone screen will not be a good match. reply _s_a_m_ 4 minutes agoparentprevI'm younger than you and I still can't read it at all :OOOOOOOOOOOOOO reply sxp 15 hours agoprevIf you're interested in other tiny fonts: - PICO-8's 3x5 font with support for programming characters: https://www.lexaloffle.com/pico-8.php?page=faq - Ken Perlin has an RGB stripe subpixel font. Unfortunately, the original page uses Java so I can't access it, but https://www.fastcompany.com/1662778/the-worlds-smallest-legi... has more info. - Dotsies if you're willing to try very strange encodings: https://dotsies.org/ - https://news.ycombinator.com/item?id=33127419 has more examples. reply Avshalom 11 hours agoparentobviously 8x8 is comparably enormous but https://damieng.com/typography/zx-origins/ has a great collection of fonts reply voidUpdate 36 minutes agorootparentTheres also the wonderful \"Arcade Game Typography: The Art of Pixel Type\" by Toshi Omigari which has pages and pages of fonts used in arcade games, sorted by style, most of which are 8x8 reply accrual 15 hours agoparentprevA favorite of mine is the MonteCarlo Programmer Font [0]. I used it as my terminal font for a couple years. [0] https://www.bok.net/MonteCarlo/ reply whhuh 8 hours agorootparentIt looks like it's been iterated on quite a few times. Here's an active descendant project, for anyone that might be interested: https://github.com/sunaku/tamzen-font reply dheera 11 hours agoparentprev3x4 = 12 bits 52 upper/lower case letters + 10 numerals ≈ 64 = 6 bits It's kind of amazing that the overhead is only a factor of 2 to literally read BINARY data with my eyes. And that a 1bpp uncompressed image of a piece of text can be only 50% larger in file size than its .txt file and be readable. reply 6510 29 minutes agoparentprevI believe mine, while very incomplete, given some practice, is the most readable of the lot. http://synesthesia.go-here.nl reply lencastre 4 hours agoparentprevMouse TTF reply hinkley 14 hours agoprevI worked on project planning software years ago. We got caught in a loop arguing about fonts and data density. We were getting lots of clipping of text and I asserted that even a couple more characters on the screen would improve people’s abilities to guess what the entire phrase was. You can derail momentum in project management meetings by having people ask what something says repeatedly. Seen it happen, it’s dumb and we can fix it. So it came down to a shootout. We put five fonts on a projector screen, at multiple font sizes, stood everyone against the back wall of the fairly average sized meeting room (maybe 70th percentile), and had them vote. Verdana 13pt won for legibility, even over some of the 14 pt fonts. It also got more characters per inch, so win win. Something in the range of 5-10%. Then corporate made us change it back because their flagship app used a different font and they wanted them to match. Which made no goddamned sense because they weren’t even used together. reply yosito 3 hours agoparentIncredible how much money corporations are willing to waste on this kind of bike shedding lol reply archargelod 6 hours agoprevI had a project a couple months ago that required very tiny font: I did want to make a mod for the Binding of Isaac game on Nintendo Switch. The idea is to display some additional info about items that you encounter randomly in the dungeon. You can find final version in my codeberg repo [0]. While pc version has extensive mod support, console ports lack necessary Lua api. So the only way I came up with to implement this - is to draw additional info on the sprite textures. And sprites are tiny - 32x32 pixels. Ok, I can't put a lot of text in such small space, but maybe I can fit item name and then add some simple 8x8 icons for effects. I started searching for a good font I can use, but ultimately most of the 3x4 and 3x5 fonts I tried had one issue - they're almost unreadable if the background color is not absolute negative of font color. I could've put an opaque black background behind text, but that would hide more art than necessary and even look somewhat hideous. While trying one font after other I found gremlin-3x6[1], it's only 2 pixels higher, but 5-10x easier to read. And it's under public domain license. Ok I mostly don't care about height, but width is still an issue - almost all item names exceed 32 pixels and have to wrap around on second line and some need a third line. That I absolutely do not want. I had an idea - If I can't shrink letters to less than 3 pixels, I can remove space between them. Wouldn't it make harder to read? Not unless all letters have different colors! That also solves the problem when background matches the color of font! [0] - https://codeberg.org/Archargelod/isaac-extended-icons-mod [1] - https://fontstruct.com/fontstructions/show/1488093/gremlin-3... reply diputsmonro 5 hours agoparentInteresting, do you have a picture of this project in action? reply archargelod 3 hours agorootparentThere is a screenshot in the Readme - https://codeberg.org/Archargelod/isaac-extended-icons-mod And another from earlier version of the mod - https://codeberg.org/Archargelod/isaac-extended-icons-mod/sr... reply gitgud 15 hours agoprevUppercase is impressive for 3x4 pixels. Lowercase is pretty much unreadable… reply skygazer 13 hours agoparentYou learn to read it, or at least I did as a child trying to use a small font on the c64 when connected to BBSs, so I could see ascii art without wrapping. It takes reminders like this to realize how awash in high density pixels we are now. reply resolutebat 9 hours agoparentprevAgreed. Sample: https://raw.githubusercontent.com/Michaelangel007/nanofont3x... It's difficult to read even when you know it's the Declaration of Independence. reply yosito 3 hours agoprevMaybe it's due to image compression artifacts, but I find the first example of normal case output impossible to read on my phone. Even if I zoom in. And I have exceptionally good eyesight. I'll have to try this directly on my desktop, where I imagine the clarity would be much better. reply boxed 1 hour agoparentThe zooming often does bicubic or some such upscale, which destroys legibility for these types of fonts. reply Timwi 42 minutes agorootparentBicubic upscaling is the default, but the `image-rendering: pixelated;` rule can be used to change it. reply antirez 53 minutes agoprevThere is another one here, with a tool to easily modify the font. https://github.com/antirez/freakwan/tree/main/font-4x6 reply 3rd3 14 hours agoprevA nano font but huge images. The large image is a 4873 KB bitmap which can be losslessly compressed to 47 KB using PNG. reply vardump 14 hours agoparentNowadays it's often hard to even notice a 4 MB file, especially when you're on a gigabit connection or better. Of course unfortunately not everyone has a connection like that. But can understand how someone might have missed it. reply qingcharles 8 hours agorootparentI work with parolees who often only have government-issued phones with maybe 5GB or 15GB of data per month, which generally only lasts the first couple of days of the month due to issues like that. I come across home pages regularly now that are >250MB of download :( reply lencastre 3 hours agorootparentWhat? Can you provide links to a page that is 250 MB download? reply andenacitelli 13 hours agorootparentprevThis is sometimes so hard to remember. There’s so much content on optimizing for page size — and admittedly, it does matter a lot in some industries like e-commerce and/or if you have users in less developed countries — but quite a lot of situations kind of just let you ignore it. I work in B2B and we frankly put way more effort in than we should have to optimize bundle size before just making the assumption that everyone has a good connection and we didn’t really need to worry about it. reply dheera 11 hours agoparentprevGithub also loads something like 4.6MB of JavaScripts and other crap, according to chrome dev tools, so ... reply bonki 6 hours agorootparentI won't defend that, but at least that will mostly be cached and reused. There is really no excuse for huge pictures like that, especially when they can be losslessly compressed to a fraction of their size. reply Moru 1 hour agorootparentIt's also compressed while sending so that is most likely the unpacked size you see there. reply userbinator 14 hours agoprevI think 5x7 is the smallest size where characters are still fully recognisable, which is why it's used on all common character LCDs. Beyond that, with things like this font, reading becomes more of a \"recognise vaguely evocative custom glyphs\" exercise. reply tom_ 11 hours agoparentThe Atari ST had a 6x6 font (5x5 for most glyphs, 5x6 including descenders) that took surprisingly few liberties with the lower case chars. I'm going by memory, but this looks like an accurate rendition: https://fontstruct.com/fontstructions/show/876150/atari_st_6 Lower case \"a\", \"e\" and \"i\" are not ideal, but the rest look pretty good to my eyes! (The OS used this for icon titles, so there was only ever 1 row at a time. Probably recommended.) reply klabb3 12 hours agoparentprevEnglish speaker detected! Half kidding, but in my language that would be difficult. ÅÄÖ reply pornel 7 hours agorootparentIt's also missing Kanji/Hanzi characters ;( reply kiicia 11 hours agorootparentprevĄĆĘŁŃÓŚŹŻ reply grumbel 11 hours agoparentprevDSLinux[1] used a 4x6 font for the terminal, I found that surprisingly usable. [1] https://www.dslinux.org/dslinux-cpuinfo.jpg reply zelphirkalt 26 minutes agoprevThe screenshots could not have been enlarged a bit? And they are bmp?? I find one of the things a repository of a font should do is to put pictures of how this thing looks quite far up in the readme, so that I can quickly see, whether that is something for me or not. reply baking 16 hours agoprevThis brings back some memories, but my eyesight is worse than it was 50 years ago, so no. reply neverokay 14 hours agoprevDon’t let the lawyers find this font. reply johnklos 16 hours agoprevI can finally have 85 characters by 48 characters on my Sinclair ZX81, and it'd be printable, too, on my T/S 2040 printer. (starts thinking in Z80...) reply veltas 23 minutes agoparentIn my experience you get a lot of density moving to a variable-width font, which is quite easy to write for a Z80 system. For example I've designed a couple with the horizontal size in the first byte of the bitmap: https://github.com/Veltas/spectrum-env/blob/master/font-orig... https://github.com/Veltas/spectrum-env/blob/master/font-smal... Rendering: https://github.com/Veltas/spectrum-env/blob/master/text.asm#... Looking back at it, my Z80 for this isn't that good, but it was still fast enough to redraw a whole line of text in 1 or 2 frames, I'm sure others can do better. reply chx 15 hours agoparentprevWhich leads to the question: what is the smallest Z80 assembly function which takes an ASCII code as an input and returns one of these characters some way? 3x4 is 12 bits so with a little waste one can fit it into a 16 bit register pair. You could thus encode it into a 96*2=192 byte lookup table but isn't there some procedural generation to shrink that? reply volemo 12 hours agorootparentI believe a table and a lookup function would be smaller than a function for generating bitmaps: just for a single pixel I've got this expression [1]. [1]: not(c0) and not(c2) and c3 and not(c4) and c5 and not(c6) or not(c0) and c2 and c3 and not(c4) and c5 and c6 or c0 and c2 and not(c3) and c4 and not(c5) and c6 or c0 and not(c1) and c2 and c3 and c4 and c6 or c0 and c1 and not(c2) and c3 and c4 and c6 or not(c1) and not(c3) and not(c4) and c5 and c6 or not(c2) and c3 and c4 and not(c5) and not(c6) or not(c0) and c1 and not(c3) and c5 and not(c6) or not(c0) and c1 and not(c2) and not(c3) and c4 or not(c0) and c1 and c3 and not(c4) and c5 or c1 and c3 and not(c4) and c5 and c6 or c1 and c2 and c4 and not(c5) and not(c6) or c0 and not(c1) and not(c4) and c5 and not(c6) or c0 and not(c2) and not(c4) and c5 and c6 or c0 and c1 and not(c3) and c5 and c6 or not(c0) and not(c1) and c4 and not(c5) reply johnklos 11 hours agoparentprevOops. It's 3x4 inside of a 4x5 box, meaning a Sinclair ZX81 can only do 64 x 38 :( reply btbuildem 6 hours agoprevYou'd figure this envelope had been pushed to its limit in the late 80's/early 90's Nice work! The uppercase is surprisingly readable -- some glyphs don't look like you'd first think they should, but that makes them stand out from the ones they'd otherwise be easily confused with. reply dsp_person 9 hours agoprevHas anyone ever made the smallest (as in bytes) readable font? How small can the code be to generate a readable font? It could create an SVG, bitmap, or triangles in a shader. reply ddingus 7 hours agoprevThe upper case is amazing! I basically have zero problems reading it. The text talks about glyph ambiguity. Selecting those well appears to amplify readability. Neat! The lower case one takes a lot more adjustment. I did end up reading it fairly well, but never with the ease of the uppercase font. reply makmanalp 6 hours agoprevInterestingly coincidental timing with Jonathan Hoefler posting about a trick to compress glyph size by omitting a block and let the eye complete it ... OK, it's taller but I think it looks quite pretty: https://www.instagram.com/p/C4n2BfFrKwH/?img_index=1 reply dsp_person 14 hours agoprev> In case you are interested, there are a total of 65,536 4x4 monochrome glyphs. Here is a uber texture atlas that shows all of them with our glyphs highlighted (red) where they are in the table. So did this font exist all along and was simply discovered? And same goes for everything we create, but just in higher dimensions?? reply rcxdude 14 hours agoparenthttps://en.m.wikipedia.org/wiki/The_Library_of_Babel reply dsp_person 13 hours agorootparenthttps://en.wikipedia.org/wiki/Permutation_City reply nickdothutton 16 hours agoprevLike some in-game text/graphic from my 8-bit days. I was a big Tau Ceti/Academy player. reply msarchet 16 hours agoprevJust this morning I have been working on a font rasterizer for an eink screen project. So it’s really interesting to see one this small. reply tripflag 16 hours agoparentif you're in the market for something slightly bigger, I've found this font to be useful on lowish-res (640x480) displays: https://github.com/josuah/miniwi reply tomcam 16 hours agorootparentThe size of the characters in pixels is not given in their readme, nor could I find it easily in the config files. Can you tell me how big the characters in its readme are? reply LukeShu 15 hours agorootparentlooks like 3x5, or 4x8 if you include the whitespace reply medstrom 13 hours agorootparentThe lowercase \"s\" is only 4 px tall. The \"f\" and \"y\" add 2 px above and below, so the total reserved height must be 8px. Then 2 more px for whitespace, so 10. reply LukeShu 5 hours agorootparentIt's a 4x8 grid, like I said. https://lukeshu.com/dump/miniwi-grid.png Weird that the lowercase letters with ascenders are taller than normal uppercase, I didn't notice that. But yeah, the core numbers+uppercase box is 3x5 (with only \"Q\" violating that). And it's all on a 4x8 grid. reply MithrilTuxedo 7 hours agoprevGonna use this for Dwarf Fortress. reply samatman 14 hours agoprevI propose that this sort of font be called \"decipherable\" rather than \"readable\". I could learn to read this. I can kinda-sorta make out the example, because I already know the Declaration of Independence. Is it readable the way, say, this text is? Or the PICO-8 font? No. reply kuboble 12 hours agoparentI think any font which is decipherable becomes readable with practice. The characters in the font are unique and clear so learning to read it should be easier that say reading using some completely unknown alphabet. This font should be easier to read than most people's hand writing. reply semi-extrinsic 12 hours agorootparentThe lowercase characters are non-unique, which may be what GP was referring to. For instance \"ox\" and \"co\" can only be distinguished by context. reply vpribish 11 hours agorootparentif more than one bit of color was allowed you could make different levels of grey/opacity hint at where the opening in the symbol should be. just as learnable and now it's unique. taken to a silly extreme, you could compress 26 letters into 2 pixels, 3 colors, and 3 levels of opacity. before even considering a time-dimension. a mantis shrimp just needs one pixel with color. reply Moru 1 hour agorootparentWhat is wrong with morse code? Just need one pixel that blinks. reply samatman 11 hours agorootparentprevMany people's handwriting is best described as \"decipherable\" as well, yes. A readable font takes no practice to read, presuming you already read the script of the font and the language of the text. A decipherable one can be sort of limped through at first and probably picked up to fluency with experience. Although, as the article notes, this font has homonymous glyphs, there are only a few words where that creates ambiguity, and as few as none where it would be ambiguous in context. reply kuboble 17 minutes agorootparent> A readable font takes no practice to read No. Any font takes a lot of practice to read. Maybe the difference is that you define \"readable\" as readable immediately by anyone who is already familiar with modern fonts? I'm sure medieval fonts were readable to pepole who wrote them, but when I look at them I need to labor at every letter. reply croemer 10 hours agoprevWhy not use ligatures to help resolve some of the cox ambiguities? reply snowpid 15 hours agoprevDoes anyone know the size of the font from the first Pokemon games? They had similar constraints. reply msk-lywenn 15 hours agoparentIt's 7 pixels high, like most games on that platform, because it fits with the 8x8 tiling system (if you had the space between lines) https://github.com/pret/pokered/blob/master/gfx/font/font.pn... reply LukeShu 15 hours agoparentprev8x8 tiles; most characters fit in 7x7 to allow for 1px between characters reply hammock 11 hours agoprev>Smallest readable 3x4 font with lowercase* *Roman alphabet reply pavl 14 hours agoprevIs this downloadable somewhere as TTF? reply Anotheroneagain 15 hours agoprevMinuscule gets down to 2; 3 and above don't even look that weird. Maybe that pixel fonts are not the optimal choice. reply ramijames 11 hours agoprev\"Readable.\" reply junon 14 hours agoprevUnlike the flagrant dismissal in other comments I actually find use for projects like these. On some electronics that I write firmware for the OLED screen space real estate is incredibly limited, and when they need to output logs or debug info for developers it can be a pain to fit everything in a way that is usable, especially when there is no input to allow for scrolling or whatever. reply lynndotpy 13 hours agoparentYes! For all the discussion here, I feel like the README answers it. It was a fun project with a few use cases. > Once the novelty wears off a \"practical\" example would be rendering \"in-game book pages\" that don't look like complete gibberish, or an \"accurate print preview\" with real text instead of blurry placeholder pixels that don't even look close to being the glyphs scaled down. This seems very reasonable. They put it out there in case someone found it usefuo. reply gillesjacobs 13 hours agoparentprev> Once the novelty wears off a \"practical\" example would be rendering \"in-game book pages\" that don't look like complete gibberish, or an \"accurate print preview\" with real text instead of blurry placeholder pixels that don't even look close to being the glyphs scaled down. Low-res or LoD video game textures are a bit of a stretch but could be cool. reply volemo 13 hours agoparentprevI'd argue that character count advantage of this 3x4 versus say PICO-8's 3x5 is overshadowed by the loss of readability. reply gillesjacobs 13 hours agoparentprevThe lowercase is nigh unreadable but the all uppercase example is not too bad. Would be cool to integrate in my split keyboard OLED screens in ZMK firmware. reply fnordpiglet 14 hours agoparentprevThis. As soon as I saw this I thought “oh neat that’ll come in use for my electronics displays” reply dougmwne 14 hours agoprevWhe I was in high school I had plenty of dull classes to sit through. I’m old enough that there were no phones and laptops to keep entertained. My parents had a laser printer for their business. I realized that it had a very high DPI and also very little ink bleed. I started printing whole books I downloaded at the smallest font size that I could managed to still read, just a few point. I removed line breaks and printed out whole books on a page or two. I found it incredible how much tiny text I could fit on a page. In class I would read with a little half folded sheet of paper hidden in a notebook. Sci-fi, Russian lit, biographies, classics. I was never caught, but it’s bizarre to think back that I was reading Crime and Punishment while the rest of the class was learning fake American history propaganda. reply arketyp 13 hours agoparent> Sci-fi, Russian lit, biographies, classics. I was never caught, but it’s bizarre to think back that I was reading Crime and Punishment while the rest of the class was learning fake American history propaganda. Sounds possible your teachers figured as much and let it slide. reply dougmwne 11 hours agorootparentI did eventually show a teacher and they were shocked. Their older eyes had no way of focusing on the text and they could barely tell it was anything but a sheet with grey lines. reply pvg 12 hours agoparentprevI was reading Crime and Punishment while the rest of the class was learning fake American history propaganda. You were simply unaware of the classmates who were secretly mastering the tiny 3d printed blade. reply FergusArgyll 16 hours agoprevReadable is a very strong word reply ocrow 16 hours agoparentThey're really stretching the definition of readable. reply BolexNOLA 16 hours agorootparentYes but their bar is “readable” not “very readable by all without effort,” so I’m not sure why y’all find this so usage so objectionable. Yeah it wasn’t easy but I was definitely able to read the example text on my phone as-is/without zooming or anything. reply lolinder 15 hours agorootparentThe upper case letters are fine (impressive, considering the resolution), but there's no way I could read the lower case if I didn't already know the text. reply mysteria 9 hours agorootparentSame for me! I could definitely understand the upper case text at 100% resolution on a 24\" 1080p screen. The lower case text was terrible and nearly unreadable. reply BolexNOLA 8 hours agorootparentI was able to read the lower case on a 6.7” ~2.8k smartphone screen. I imagine they don’t expect literally every person to be able to read it on every machine. This feels rather nitpick-y. reply mysteria 6 hours agorootparentOn a 6.7\" display, assuming 16:9, the screen is 5.8\" wide. With 2.8k pixels across each pixel is 0.002 inches wide. I'm pretty sure you aren't viewing it at 100% resolution on the phone as each glyph would render as a nearly invisible dot. reply lolinder 8 hours agorootparentprevBut did you read it after reading the upper case? And are you an American? It's the same text as the upper case version and it's the Declaration of Independence, which a lot of us know pretty well. I initially tricked myself into believing that I could read it until I got past the part that I knew well, at which point I realized that I wasn't reading it so much as using the shapes as a mnemonic to help remember it. (As an aside, it's not nitpicky to say \"I can't read this font that bills itself as the smallest readable font\", it's just an expression of doubt as to the advertised qualities of the font.) reply wkat4242 15 hours agorootparentprevMeh in that case even a barcode is 'readable'. reply wkat4242 15 hours agoprevI see I have a slightly different interpretation of the word 'readable' lol. reply mgaunard 14 hours agoprevNot that readable. reply hulitu 14 hours agoprev [–] > Nanofont3x4: Smallest readable 3x4 font with lowercase Readable by who ? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The nanofont3x4 is considered the smallest readable 3x4 font globally and the first to feature readable lowercase letters, making it practical for in-game book pages or precise print previews with real text.",
      "This font aims to push the boundaries of typography by focusing on readability in extremely small fonts, with certain lowercase glyphs fitting in a 2x2 grid, though some challenging words like 'exercise' or 'compliance' exist.",
      "The creation of this font was motivated by the challenge of producing the smallest readable lowercase glyphs, with connections to previous similar endeavors in the field."
    ],
    "commentSummary": [
      "Users are engaging in a conversation about the readability and accessibility of small fonts, focusing on Nanofont3x4 and various other tiny font options.",
      "Discussions include font and data density in project planning software, optimizing bundle size in B2B environments, and creating bitmap fonts for Z80 systems.",
      "There is a debate about the legibility and uniqueness of small fonts, along with suggestions for enhancing clarity and practical uses of small text sizes, addressing concerns about readability and different users' interpretations of \"readable.\""
    ],
    "points": 320,
    "commentCount": 97,
    "retryCount": 0,
    "time": 1710693577
  },
  {
    "id": 39733605,
    "title": "Academic Papers with ChatGPT Sections Found on Google Scholar",
    "originLink": "https://simonwillison.net/2024/Mar/15/certainly-here-is-google-scholar/",
    "originBody": "Simon Willison’s Weblog Subscribe Google Scholar search: \"certainly, here is\" -chatgpt -llm (via) Searching Google Scholar for “certainly, here is” turns up a huge number of academic papers that include parts that were evidently written by ChatGPT—sections that start with “Certainly, here is a concise summary of the provided sections:” are a dead giveaway. Posted 15th March 2024 at 1:43 pm Recent articles Weeknotes: the aftermath of NICAR - 16th March 2024 The GPT-4 barrier has finally been broken - 8th March 2024 Prompt injection and jailbreaking are not the same thing - 5th March 2024 Interesting ideas in Observable Framework - 3rd March 2024 Weeknotes: Getting ready for NICAR - 27th February 2024 The killer app of Gemini Pro 1.5 is video - 21st February 2024 ethics 71 google 290 ai 474 generativeai 409 chatgpt 91 llms 385 Source code © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024",
    "commentLink": "https://news.ycombinator.com/item?id=39733605",
    "commentBody": "Google Scholar search: \"certainly, here is\" -chatgpt -llm (simonwillison.net)184 points by wanderingmind 22 hours agohidepastfavorite88 comments dougb5 21 hours agoAfter late '22 I noticed a surge in superlatives like \"vital\", \"essential\", \"crucial\", and \"pivotal\" in essays and cover letters from students. ChatGPT uses words like these whenever you ask it to write an essay, and it's a dead giveaway. It's ruined the legitimate uses of the words. Pick one of the Google Scholar results from the article and you'll find something awful like this (the start of the second paper): \"The carriage of goods under international commercial law is a complex and essential aspect of international trade.\" reply nomonnai 20 hours agoparent> surge in superlatives like \"vital\", \"essential\", \"crucial\", and \"pivotal\" This is very interesting. It could also be an effect of Grammarly, which suggests these replacements for the generic \"important.\" Perhaps it's a combination of several effects. reply dougb5 16 hours agorootparentMy guess is that ChatGPT does it because RLHF rewards strongly stated opinions, because that's what humans prefer. It's a kind of \"sycophantic behavior\" that researchers have observed in these models (https://arxiv.org/abs/2310.13548) reply Sharlin 20 hours agoparentprevTo be fair, replacing \"important\" with some less generic synonym is like one of the first tips in any Copy Editing for Dummies book, and definitely suggested by Grammarly, for example. reply peter_vukovic 2 hours agorootparentAny word can become generic when overused. As more people get access to Grammarly and CharGPT, more of them will apply the same recommendations, turning previously rare words into generics. reply Sharlin 7 minutes agorootparentTrue – I almost added that the advice itself has almost become a cliché. reply yesco 16 hours agoparentprevI've been interacting with ChatGPT and other LLMs so much I've already caught myself getting influenced by their word choice in the same way I might start subconsciously adopting the vocabulary of a friend. Food for thought . reply chengiz 19 hours agoparentprevYou must be in some good writers' microcosm. A lot of human technical writers really write like that. reply dougb5 16 hours agorootparentHeh, or maybe in some general sense, everything is essential, crucial, vital, constantly pivoting, etc., and these are the best words to use. Some day ChatGPT will tell me \"I told you so!\" reply whamlastxmas 6 hours agoparentprevIf I specifically tell ChatGPT to not use language like that it completely ignores my direction and does it anyway reply eynsham 17 hours agoparentprevHow are ‘vital’, ‘essential’ and ‘pivotal’ superlatives? reply dougb5 16 hours agorootparent\"Superlative\" can mean not just the grammatical category that includes \"*est\" words, but also, more informally, adjectives that live at an extreme, as \"critical\" does within the concept of \"importance\". reply krastanov 19 hours agoprevHow many of these are in serious journals that actually are read by other scientists and how many of these are in low-quality predatory journals that simply help unsuccessful scientists inflate their publication numbers for a fee? Looking at the search results, none of these are in \"real\" science journals. This is just yet another way in which predatory journals are terrible for science and academia (and its public perception), but the existence of garbage journals has been a problem for 2 decades (or more?). On the bright side, productivity at respected institutions has not been significantly hampered by predatory journals. On the other hand, academia as a whole has obviously failed to teach incoming members that this is basically fraud (partially due to the publish-or-perish culture that is now even endorsed by governments). reply DataDive 21 hours agoprevWhen I looked at the examples, very few seemed to be fully ChatGPT generated. That being said, most scientists are very smart and recognize the power of ChatGPT. They are most likely heavily relying on it when producing summaries, abstracts, hypotheses, etc., and because they can and will edit it you won't be able to tell from words alone. ChatGPT is an equalizer between native English speakers and scientists speaking English as a second language. But both groups will use it heavily and nobody can foresee what the net effect of it is on science. Having had that experience, I can tell you that it takes weeks or months to write up results as a paper and go through the most boring steps of formatting and organizing them into antiquated presentation formats: Intro/Discussion/Results, dumbass citations, labeling, backward wordings ... etc What should be a five-paragraph discovery on a blog has to take the form of a multipage, heavily formatted document that contains enormous, unrelated, and useless information. With ChatGPT, you can do it in a day. If you are not using ChatGPT, you lose out. reply colanderman 20 hours agoparentFortunately we can also use ChatGPT to summarize this nonsense into the concise statement which should have been written in the first place. Maybe even one day society will re-value conciseness and precision of thought over linguistic diarrhea. reply timkam 19 hours agoprevNote that \"false positives\" may appear in the Google Scholar results for such queries. For example, a high-profile AI researcher shared a screenshot of https://scholar.google.com/scholar?as_sdt=0%2C5&as_ylo=2023&... (on X/Twitter: https://twitter.com/MelMitchell1/status/1768422636944499133). The second entry is an ironically written abstract of a talk and the author clearly (and consciously) plays with the cliché. Obviously, most of the fans of the person who shared the screenshot to thousands of followers will not notice this. reply 082349872349872 19 hours agoparentIs the adjacency in Scholar limited to intra-sentence fragments? It'd be problematic if not, certainly. Here is another example of the sibboleth: when we use \"certainly\" here. Is this a query hit? reply mike_hearn 19 hours agoprevAnother source of similar entertainment is the problematic paper screener: https://dbrech.irit.fr/pls/apex/f?p=9999:1:::::: It's a constant crawl of the literature that uses hand-written phrase detectors. Many of the phrases it looks for are caused by using spinners on plagiarised text, like this one: https://link.springer.com/article/10.1007/s11042-024-18524-1 \"At first, the input picture is transformed into the ruddy, green, and blue organize, and the clamor within the green band is evacuated using a median channel\" Here \"noise\" has been turned into \"clamor\". Nonsense text like this is easy to find in journals published by well known brands, in this case Springer Nature. A search for \"profound learning\" (deep learning) yields over 2000 results: https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&as_ylo... reply tulio_ribeiro 22 hours agoprevI'm fine with and approve the usage of LLMs in academia, as long as they provide genuine value and something new to the field. These tools should be embraced when they can augment human intellect. However, I draw a firm line at using them to generate complete academic works or nonsensical content, as that undermines the integrity of research and renders it devoid of originality. LLMs should serve as invaluable assistants to free up scholars for higher-order analysis, not as replacements for human ingenuity. reply rchaud 19 hours agoparentYou may as well say that you only want dynamite to be used for nonviolent purposes, like demolishing condemned buildings. It doesn't work that way, which is exactly what AI ethicists and researchers have been warning about - AI should be regulated, because once its in somebody's hands, you don't control how they use it, and the repercussions could be much wider than people imagine. reply rich_sasha 21 hours agoparentprevIMO academic papers are too long. The decorum dictates that a core of real, novel content must be surrounded in tons of fluff. Most people don't ever read that fluff (some do, and it's not totally useless, but not to most people). It doesn't actually bother me if some of that fluff is ChatGPT-generated, provided the author actually read it and accepts the autogenerated content. But better still, cut the fluff. reply anal_reactor 1 hour agorootparentI'm having flashbacks from writing my Master's thesis. The experimental part was done in two weeks, then I spent a week describing the results, then a few months going from 15 pages to 80 pages and at least 30 citations, with the latter being surprisingly difficult because of uniqueness of my research topic. reply CuriouslyC 21 hours agoparentprevBuckle up, because we're not far away from autonomous research agents. It'll start with computational analysis, plot generation and creation of data discovery documents, but soon models will learn how to request simple physical experiments (sequencing some DNA, mass spec a sample, etc) and plan research based around the outcome. reply regularfry 21 hours agoparentprevThe problem here is not necessarily the use of chatgpt at all. It's that academic idiom requires content-free linguistic noise for acceptance, and people are quite naturally turning to mechanistic routes to automate out the drudgery. reply Tubbe 22 hours agoprevCrazy how broken the entire publication and review process is, that this stuff goes through without anyone noticing, and some even in well recognized journals. reply brohee 21 hours agoparentIt's basically organised crime. It's public money funding scientific publishing, and a huge part of the industry is just here to scam public money. The industry is ripe for public prosecution when an article submitter didn't read it, an article reviewer didn't read it, neither did the publisher yet it is published in a journal with a subscription price in the thousands payed by publicly funded libraries. reply esalman 20 hours agorootparentSo many students on every level of higher education, probably even starting from high school, are now using chat GPT. Does this make education a crime? reply brohee 20 hours agorootparentWhen they get a scholarship out of it, it might actually be... But the organized crime part is obviously the publishing industry. reply Pigalowda 17 hours agorootparentAustin Powers warned us about the Dutch in the 90’s. Elsevier is part of Dr. Evil’s empire. reply throw9988998899 22 hours agoprevI know a guy who is currently on a higher technical school. He talks very open how he and all in his class use Chat GPT for all the bloat they have to write reply admissionsguy 21 hours agoparentStudents at universities do that now. And they get passed. It exposes the whole charade that much of the higher education has become. The idea behind writing a lot has been that it forces students to think systematically and thereby learn. But they are not typically interested in that, they just need a diploma in order to get a job. Before computers they were copying books verbatim and since plagiarism software they paraphrase books. And now chatgpt removes the middle man - the student. It doesn’t make much sense either way. The source of the problem is really the conflation of higher learning with vocational training. reply beginning_end 19 hours agoprevUntil this point in history there has been a reasonable correlation between the quality of writing and the \"quality\" of the underlying work in most text. If effort was put into the writing (and learning to write), that in itself used to be a decent indicator that effort and skill also was put into the data/ideas that the text conveyed. That rule no longer works. People are still going to rely on it for a while though, and I'm worried it's going to break some stuff over the next few years. reply 082349872349872 19 hours agoparentWith any luck the endpoint will be a world where texts are evaluated more on their semantics than their syntax. reply empath-nirvana 19 hours agoparentprevI think probably the solution here is to stop using publishing in peer reviewed journals as a job requirement for academics. Even before ChatGPT, academic publishing was already falling victim to Goodhart's law. Remove the perverse incentives and it should cut down on this by quite a lot. reply perihelions 21 hours agoprevThe upside of all this is that professionals are communicating their points better. Lots of examples from non-English (-language) nationalities using LLM's to write exposition sections for English journals. This lowers the communications friction across the language barrier; and lowers the implicit bias and penalization when people read a (technically correct and useful) paper that has weak grammar, and is confusing to parse. These aren't purely bad things! I wonder what future social conventions for this kind of thing will look like. reply esalman 20 hours agoparentThe first paper I opened from this list has whole related work section copy-pasted from chat GPT response. It implies that the authors did not care to thoroughly review related work. They basically didn't read any papers. If you try to have a technical conversation with this sort of a person, they'll communicate their points out of their behind. reply MarkSweep 20 hours agoparentprevI agree that LLM can be used as a tool by non-native English speakers. But the examples cited in the article are indefensible. The authors of the paper we either so care less they did not notice the the “certainly, here is…” preamble made no sense in a paper. Or they have such a poor grasp of English that they are unable to comprehend what they copy-pasted to notice. Either way this suggests that nothing in the paper can be relied upon. reply vitiral 21 hours agoparentprevPerhaps two people can who don't fluently speak the same language can better communicate. But that's not really the bar is it? These are writing articles on behalf of fluent speakers for other fluent speakers to read. Is it improving THEIR \"communication\". I'd say by definition the answer is no, since it's no longer humans communicating with each other at all. reply throwawayyy9237 22 hours agoprevI don't have time to properly go through the entire list, of course, but the first paper that I've opened seemed to use ChatGPT to format a table of values, presumably not in tabular format originally. It didn't _seem_ like any of the ideas or conclusions were GPT-created. I'm perfectly fine with this usage, although it is very sloppy if the authors and reviewers didn't pick the \"certainly\" in the final text. reply pavel_lishin 22 hours agoparentI'd be very wary of the LLM hallucinating a few number changes here and there, similar to what copies used to have a habit of doing: https://www.dkriesel.com/en/blog/2013/0802_xerox-workcentres... reply throwawayyy9237 21 hours agorootparentAbsolutely, the time saved formatting a table + time triple checking it might be the same as doing manually in the first place. But I was talking about the intended usage, it doesn't sit as fraudulent to me in the scientific sense. reply AlecSchueler 21 hours agorootparentprevYou can ask it for code that will convert your input into tabulated output. Easier to check the logic of the code than all of the values in the table, then just plug your data into it. reply croes 21 hours agoparentprevBut it shows a lack of care in the preparation of the paper reply daghamm 22 hours agoprevYes, this does happen. But a quick browsing shows that not all these papers contain that phrase and not all that do seem to be machine generated. reply rzzzt 22 hours agoparentFirst hit, in the keywords section (Ключевые слова): \"methodology. Certainly / here is the English translation of the provided text\" Second hit: \"Certainly! Here is a review of the literature on the carriage of goods under international commercial law:\" Third hit: \"Certainly, here is the tabular representation of intraocular pressure (IOP) at 1 and 4 hours for Group X and Group Y:\" Fourth hit: \"Certainly, here is a list of both Geo-natural and geosynthetic materials commonly used in civil engineering and environmental applications.\" Fifth hit: \"Certainly, here is a concise summary of the provided sections:\" reply baobabKoodaa 21 hours agorootparentedit: I was wrong reply tapland 21 hours agorootparentThe translation isn't even necessarily damning. A lot of countries require publishing in the local language, and then you want it in english too. reply rzzzt 21 hours agorootparentprevSixth hit: \"Certainly, here is a list of various types of polymers:\" Seventh hit:Eighth hit: \"Certainly, here is the essential information provided for reference to the proceedings of the conference on the topic of university book production:\" Ninth hit: \"Certainly, here is the methodology for conducting a literature review on the attitudes of healthcare workers toward COVID-19 vaccination:\" Tenth hit:Eleventh hit: \"Furthermore, to confirm the advantages of the new model, Certainly! Here is the rephrased sentence with improved language quality and clarity:\" I don't think that all results contain such wording, and surely daghamm's statement can't be disproven by listing these. But there is something that (also) rubs me the wrong way about these sentences getting into research papers. reply baobabKoodaa 21 hours agorootparentdeleted reply rzzzt 21 hours agorootparentGood for them and their readers. Re-reading Simon's post, the only word that I think others might find contentous is \"huge\". Would it be better if it read the search expression turns up \"a number\" of papers, without a qualifier? reply baobabKoodaa 21 hours agorootparentI made a mistake when I switched from the French version to English version of Google Scholar. Sorry. You are correct. reply ffpip 22 hours agoparentprevCertainly, here is a reply to danhamm's comment to prove that he might be wrong. I randomly picked 10 articles and all of them had the phrase in a manner that indicated it was directly pasted from ChatGPT. reply baobabKoodaa 21 hours agorootparentdeleted reply Jabbles 21 hours agorootparentThe link in the post already has a filter after 2023: https://scholar.google.fr/scholar?hl=fr&as_sdt=0%2C5&as_ylo=... reply baobabKoodaa 21 hours agorootparentSorry! My mistake! reply karencarits 22 hours agoparentprevIndeed, you also get a considerable number of results when limiting the search to publications from before 2020 reply 10c8 22 hours agorootparentIf you actually take time to go and read through them, you can see that the writing pattern is clearly different to that of ChatGPT, contrary to the recent ones: - \"[...] Certainly here is a vast and important project for research.\" Versus: - \"Certainly! Here is the text with spaces added after each word:\" reply qgin 20 hours agoprevBig difference using LLM to wordsmith your English vs using LLM to fake your research and analysis reply 2cynykyl 17 hours agoprevAnother way to look at the problem of LLM-generated garbage papers is that it doesn't actually change anything, because serious researchers in a given field already didn't trust work coming from unknown labs in shady journals. Researchers need to earn a reputation first by doing all the usual things like going to conferences (to build personal relationships), publishing reproducible results in real journals, making an actual impact on important problems, etc. Basically, LLM papers are just a new contribution to the already existing background noise. reply geor9e 16 hours agoprevMost of the first results have Cyrillic letters in the Author's name. I think these folks perhaps aren't fluent in English, and think \"Certainly, here is a summary of…\" is a normal thing people would say. They probably see it as a monologue flourish like \"Secondly, here is a summary…\", and don't realize it is actually a subservient conversational reply, a dead giveaway of an LLM chat. reply sorenjan 20 hours agoprevHumans will use LLMs to produce long texts from a brief description, and then other humans will use LLMs to make a summary of that text. Maybe eventually we can make this communication more effective. reply lambdaba 20 hours agoprevopened a random PDF from the search and it's full of misspellings (even the title is misspelled), they aren't using LLMs enough reply admissionsguy 21 hours agoprevHopefully this leads to a shorter communication form for presenting new results. No need for each paper to have a half-assed intro/review of the field. Since now even the authors skip that part. Leave reviews for quality papers by top experts in the area. reply tetris11 22 hours agoprevDamming. What is research even becoming at this stage? I hope open peer review is the golden bullet here, since the paid journals are clearly struggling under the deluge of AI spam reply TimTheTinker 22 hours agoparentIt's not that they're struggling, it's that by and large they don't care. They're making money from publishing scientific papers (or whatever they can be called), and that's all that matters. reply qwertyuiop_ 22 hours agorootparenthow does this model work ? the students and professors get paid by journals to publish papers? reply kettleballroll 22 hours agorootparentN. Usually THE AUTHOR pays the journal to publish anything. However, the professors need publications for their CV (number of published articles is a very important metric in the academic world), to acquire new funds (\"give us money to do research on topic X. Look at how much we published on this related topic before, we clearly know what we're doing\") or advance in their career (ASA function of the former two points). Students need publications as part of their graduation requirements (in a lot of fields, you are required to have N publications in a peer reviewed venue to get your PhD). So in both cases, you don't make money directly from publications, but from the prestige they bring and the credibility they're associated with (\"this guy must be an expert on X, because they published on the topic\"). reply qarl 20 hours agoprevEven worse, I've seen evidence that many researchers will use a calculator rather than work out the math for themselves. reply colanderman 20 hours agoparentIf only reading LLM-generated bullshit didn't waste the reader's time! reply qarl 15 hours agorootparentI asked ChatGPT to consider your point: Understandably, concerns about the quality of LLM-generated content are valid. However, it's crucial to recognize that academic research goes through rigorous processes before publication, including oversight by advisors and peer review. These layers of scrutiny help ensure that any content, whether initially drafted with the aid of AI like GPT-4 or not, meets the high standards of accuracy, reliability, and originality expected in scholarly work. AI tools are used as aids in the research process, not replacements for human intellect, and the responsibility for the final output always rests with the human authors. Hence, the academic community already has strong mechanisms in place to filter out inaccuracies or \"bullshit,\" ensuring that the integrity of published research is maintained. reply jordanpg 20 hours agoprevPutting aside the well-known issues with academic publication that this problem highlights, another aspect of modern publications more generally that LLMs are going to disrupt is word bloat. Too many words. For example, many academic papers are pages and pages of background, discussion, citations, etc. Very often the actual new material can fit on an index card + 1 figure. I understand why all the background is needed, in the abstract, but maybe this traditional way of thinking about incremental academic thought needs to shift in light of the reality of LLMs: LLMs will be used to write needless fluff. Indeed, it's what they're good at. I'm a patent attorney and I can say that the same thing applies there. Patents are enormous documents filled with legalese, background, description, vague, conditional language, etc. There are good legal reasons for that, but again, often the actual novel invention can fit on an index card. For that reason, it's self-evident to me that my job will be replaced, at least to some significant extent, by LLMs. But why not just cut out all the needless words? I could imagine patent law that added a \"parsimony\" requirement to patent filings. Examiners could reject filings that contained extraneous words. Then maybe patent lawyers would still be needed: to write the shortest possible patent that still contained the novelty. This is just a toy thought experiment, not a serious proposal. My point is that LLMs expose as trivially bankrupt some aspects of academic and patent writing (and probably other places: marketing copy, some nonfiction essay-style writing, etc.). Maybe LLMs will force us to embrace or even require brevity and conciseness to ensure that human beings stay in the loop. reply pmarreck 19 hours agoprevI don't see the issue with LLM paper assistance as long as the product is good reply verticalscaler 22 hours agoprevVery ironically this blog post is plagiarizing a tweet without credit. ;) https://twitter.com/pmarca/status/1768405485705650309 reply easton 20 hours agoparentThe “via” link links a different tweet describing the same thing. reply radarsat1 20 hours agoparentprevIronically? Are you saying that using an LLM to assist writing is plagiarism? reply bigyikes 22 hours agoprevHow can someone be intelligent enough to submit a research paper but stupid enough to fail at taking the most basic steps to hide their AI plagiarism? What do we even do about something like this? Is there an organization that has the power to go through this list and blacklist each of the authors from future publications? Our society should have a zero tolerance policy for drivel like this. It cheapens the entire institution. reply gordian-mind 22 hours agoparentDo you consider using ChatGPT to format a table in LateX plagiarism that should imply the end of your professional career? reply muldvarp 21 hours agorootparentI wouldn't call it plagiarism, but not even reading the output of the LLM before pasting it into your paper shows a lack of professionalism and indicates that the data in that table might as well be the hallucination of an LLM. Not exactly a good look. reply gordian-mind 16 hours agorootparentAgree, but then the subject is not LLMs but low quality publications. reply TylerE 22 hours agorootparentprevHell, better that they DO include the prompt. Since that A: clearly discloses AI usage and B: If someone attempts to duplicate the experiment (if relevant), they can take their data (perhaps give the prompt input, at least a subset showing all structure, in an appendix) and run it through the prompt. reply qwertyuiop_ 22 hours agoprevempirically analyzing the origins of these papers most of them are from India. reply noobcoder 21 hours agoprevOne of my close friends (Ivy League guy) did the same with his PhD thesis on Siamese networks in improving robotic perception. With the final presentation upcoming, in 2-3 days he's got a whole polished thesis ready to go and even convinced his guide that this was what he was working on it since 3 months. He was recently offered a doctorate I wonder how many people adeptly use GPTs and manage to navigate through their academic landscapes nowadays. I bet its majority reply baobabKoodaa 21 hours agoprev [6 more] deleted (I made a mistake when switching from the linked French Scholar to English version, I posted incorrect information here) reply Jabbles 21 hours agoparentThe link in the post is https://scholar.google.fr/scholar?hl=fr&as_sdt=0%2C5&as_ylo=... Which already filters after 2023. reply baobabKoodaa 21 hours agorootparentSorry, I made a mistake when I tried to switch from French Scholar to English reply gillesjacobs 21 hours agoparentprevYes exactly the whole claim is fundamentally flawed. Better queries would include proper timeframing and all LLM-preference-tuning phrases from https://github.com/AlpinDale/gptslop But even then, I don't think this is the gotcha OP thinks it is: LLM generation and summarizing is a legit tool in the paper writing process, especially for conclusions. In my academic experience, the paper introduction and conclusion is often written last-minute, close to the submission deadline. While a good intro and conclusion are important for the reader, very little time is made available for it since researchers spend maximal time on expanding material findings and experiments. No wonder that assistive writing tools are used to speed things up. As long as proper post-editing and proof-reading is applied. reply semanticist 21 hours agorootparent> As long as proper post-editing and proof-reading is applied for course. The whole point of the original article is that people aren't doing even that. Plus in an academic context, it's important to acknowledge when parts of the paper are not your own work, especially in cases where the generated summary may not accurately reflect the actual paper. Saying that ChatGPT has been used to generate summaries would allow people to judge if the summary alone can be trusted to represent the content of the paper. reply aredox 21 hours agoparentprev [–] Exactly. A correct analysis would be to determine the baseline pre-chatGPT, examine if there is a trend (some language \"clichés\" evolve with time) and then check if there is a clear shift post-ChatGPT. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Academic papers on Google Scholar include sections written by ChatGPT, identified with the phrase \"Certainly, here is a concise summary of the provided sections.\"",
      "The weblog mentions recent articles on GPT-4, prompt injection, and Gemini Pro 1.5, providing insight into current tech trends.",
      "This discovery highlights the utilization of AI language models like ChatGPT in academic content creation on Google Scholar."
    ],
    "commentSummary": [
      "The integration of AI language models like ChatGPT in academic writing sparks debates over superlative usage, writing quality, plagiarism, and authenticity concerns.",
      "Some view AI support as beneficial for hastening the publishing process, while others highlight its impact on human writing styles and potential content errors.",
      "Discussions extend to regulating AI in academic research and the ethical considerations of employing AI tools in academic writing."
    ],
    "points": 184,
    "commentCount": 88,
    "retryCount": 0,
    "time": 1710674096
  },
  {
    "id": 39733257,
    "title": "Heinlein's Quick Fan Mail Fix",
    "originLink": "https://kk.org/ct2/heinleins-fan-mail-solution/",
    "originBody": "*Lifestream *The Technium *Cool Tools *True Films *Extrapolations *Screen Pub *Quantified Self *New Rules *Street Use *Asia Grace *Silver Cord *WINK Heinlein’s Fan Mail Solution I found this letter in a folder of old correspondence from my days when I was editing at the Whole Earth Catalog. It is from the science fiction master Robert Heinlein. Heinlein engineered his own nerdy solution to a problem common to famous authors: how to deal with fan mail. In the days before the internet, Heinlein’s solution was fabulous. He created a one page FAQ answer sheet – minus the questions. Then he, or rather his wife Ginny, checked off the appropriate answer and mailed it back. While getting a form letter back might be thought rude, it was much better than being ignored, and besides, the other questions you did not ask were also answered! Indeed, it is both remarkable and heartwarming that Heinlein replied at all to most mail. Can you imagine other great authors doing the same — even with a form letter? Heinlein’s form is very entertaining to read because you are forced to reconstruct the missing requests. Click on the image to see enlarged version. But progress marches on, even in science fiction author’s households. Ginny Heinlein said that by 1984, “with the advent of computerization in our household, we no long use the form letter to answer fan mail. I find that it is possible now, with the computer, to write individual letters in reply to fan mail faster than I could check off the answer on the form.” This site operates under a Creative Commons License. © 2023",
    "commentLink": "https://news.ycombinator.com/item?id=39733257",
    "commentBody": "Heinlein's Fan Mail Solution (kk.org)182 points by blackbrokkoli 23 hours agohidepastfavorite35 comments domador 18 hours agoIt's fascinating for me to try and guess what kind of questions or requests would have prompted certain answers on Heinlein's list. I wonder which ones would have prompted a reference to the \"You're Not As Smart As You Could Be\" or the \"Mental Telegraphy\" articles. But I'm especially intrigued by the 12th answer, where Heinlein essentially offers to personally come to a student's defense when a grade on a project seems to depend on Heinlein's reply. I wonder how many times he ended up speaking to teachers, principals, and school boards about this, and what that looked like. reply fl7305 17 hours agoparentSo if you're a teacher who wants to chat with Heinlein, you don't write to him directly. Instead you tell a student that he will fail your class unless he gets Heinlein to help him. reply MPSimmons 16 hours agorootparentI cannot imagine that discussion would have worked out at all in the teacher's favor. reply 48864w6ui 17 hours agoparentprevAnd who tried to give RMS a parrot? reply MPSimmons 15 hours agorootparentOr bought one just because he was going to crash on their couch reply eesmith 14 hours agoparentprevhttp://www.nitrosyncretic.com/nsp_rah-telepathy.php says the \"Mental Telegraphy\" is from Expanded Universe, and includes a copy of Twain's text. I haven't reread Heinlein for decades and don't remember the reference. An archive.org search of the book wasn't fruitful. reply eesmith 15 hours agoparentprevA couple Heinlein's stories referred to a Renshaw device. From \"Assignment in eternity\" at https://archive.org/details/assignmentineter0000hein_h0q8/pa... : \"Around World War II Dr Samuel Renshaw at the Ohio State University was proving that most people are about one-fifth efficient in using their capacities to see, hear, taste, feel, and remember. His research was swallowed in the morass of communist pseudoscience that obtained after World Word III, but after his death, his findings were preserved underground.\" That Renshaw is the author of \"You're Not As Smart As You Could Be\". https://en.wikipedia.org/wiki/Samuel_Renshaw lists more places where Heinlein cites Renshaw's tachistoscopic training, including in this fan mail response. reply delichon 24 minutes agoprevI lived a mile and a half from Heinlein in the Santa Cruz mountains toward the end of his life, and fantasized about stopping by on a walk and seeing if he wanted to play chess. After reading \"Don't plan to call at our home\" on this list I'm glad I didn't work up the nerve. reply WillAdams 18 hours agoprevIt's worth noting that Isaac Asimov and Piers Anthony made it a point to respond to _every_ letter which they received (and PA noted that they were the only writers who never suffered from writer's block), though later the latter's mail grew to the point where he adopted a similar news letter/form-mail response system. reply dugmartin 16 hours agoparentI wrote Piers Anthony in the mid-80s when I was a teenager and I can confirm he sent non-form letter replies. He replied on a dot matrix printed postcard and (briefly) answered the two questions in my letter to him. I wrote to a few authors around that time and the biggest reply I received was from David Eddings. He sent me a thick 8.5x11 envelope with the syllabus and class notes from a writing class he taught when I asked him about how to become a better writer. reply 7thaccount 9 hours agorootparentI've written to a few sci-fi authors who wrote amazing books that shook me in a good way just to say thanks and how much I appreciated their work and never heard back. In highschool I read a book by a well known astrophysicist and wrote him an email about getting into physics. I was delighted by a page long reply that gave me some pointers and insight. It may seem nerdy, but I really learned a lot from him and respected his communication style. It's a fond memory I'll forever hold onto and hope I can be as gracious someday. As much as I wish I could be a physicist, my mathematical skills are a bit more limited, so I went into engineering instead which is also hard and involves plenty of math, but doesn't require solving differential equations as a key part of my job. reply ilamont 15 hours agoparentprevThere was a very good This American Life episode in which a teenaged fan visited Piers Anthony in the 1980s (\"Just South of the Unicorns,\" https://www.thisamericanlife.org/470/show-me-the-way). Per the Gizmodo summary: It’s an interesting story about the relationship between authors and fans, and how mundane fantasies can sometimes be more powerful than magical ones. It’s also an ultimately uplifting look at how Anthony, through his books and his advice, helped a young man through a particularly challenging time in his life. As a young SFF reader in the 1980s, I enjoyed the Xanth series to a point but got much further into his earlier hard-edged science fiction, specifically Chthon and Macroscrope. reply phonon 18 hours agoparentprevSpeaking of Piers Anthony...anyone know if he is okay? He hasn't sent out his monthly newsletter for almost 9 months. https://hipiers.com/newsletters/ reply a2tech 17 hours agorootparentHard to believe he decamped from his house in Florida which was the genesis of the xanth novels to move to California. I didn’t realize he was almost pushing 90 years old either. reply Scubabear68 16 hours agorootparentI sent him a fan letter a few years ago. It was answered by an assistant that said Piers no longer directly responds to mail, but it is read to him. reply nytesky 16 hours agorootparentprevI’m from that part of Florida, it’s is crazy hot and humid and far from the beaches (though quiet as ‘the nature coast’). Medical care is spotty outside of the “big” cities, and California is much more temperate. reply ocdtrekkie 17 hours agorootparentprevMaybe you should send him a letter. reply gwern 9 hours agoprev> I found this letter in a folder of old correspondence from my days when I was editing at the Whole Earth Catalog. It is from the science fiction master Robert Heinlein. One unanswered question here: the image shows no box checked off. So, if it was not in reply to a letter from Kelly (or someone else at Whole Earth), then where did it come from? reply Khelavaster 8 hours agoparentThe mystery deepens.. reply teucris 16 hours agoprevI was curious as to what articles he was referring to. Here is the one from number 3: https://www.panshin.com/critics/Renshaw/notassmart/notassmar... reply crooked-v 14 hours agoprevI'm amused by the combination of goodwill towards some and implicit snark towards everyone else represented by the last item in the list. reply dhosek 12 hours agoparentThere’s something rather delightful with this form letter that makes me wish I had fan mail so I could do something similar. It’s a nice glimpse into what everyone else is writing when you get your response. reply UberFly 10 hours agoprevLove this. Here is a bit bigger version: https://bookofjoe.typepad.com/.a/6a00d8341c5dea53ef017c325db... reply ThinkBeat 13 hours agoprevNow this entire process could be (is?) automated. Send an email to your favorite author, some computer references earlier answers and ships it off within seconds. If this does not yet exist it would be a nice SAAS startup? (You owe me 1% of profits) For an add-on cost the bot will wait a random period of time to make it appear that someone had read it, and responded personally. reply rdlw 10 hours agoparentLuckily I'm pretty sure no one I admire would use this service, because it would be devastating to find out they are a user and instantly lose all respect for them, probably also irrevocably casting a shadow on their work. reply ilaksh 8 hours agorootparentThat's ridiculous. Celebrities receive a mountain of emails, most of which are repeats of the same question, comment or request. In that context, requiring them to reply personally to every one is unreasonable. reply rdlw 6 hours agorootparentOf course personal responses are not 'required', like you say they receive a mountain of emails. What you're saying is that rather than respond with a completely acceptable boilerplate email explaining that they get too much correspondence to write a personalized response, or allow an assistant to explain that and respond in their stead, they should deceive their biggest fans--the people who care enough to actually reach out to them, pretending that they did read and personally respond, which is an outright lie. reply andrewflnr 6 hours agorootparentprevRequiring personal replies is ridiculous, yes. Doesn't that make faking them even more so? reply ilaksh 8 hours agoparentprevI assume there are multiple startups working on something like this for people who have a high volume of email to get through. I think you would just use a SOTA LLM. Probably a critical function of a filter or sorting and auto responding system like that is identifying email that requires the actual attention of the real person. But I think that famous people get a massive amount of the same questions over and over again. Another aspect of this is reproducing the writing patterns or \"voice\" of the person. This can be accomplished to some degree with a few of the best LLMs and some examples. There are definitely startups where the intention is to create a digital representation of yourself for fans to have a dialogue with. So it skips over the email and straight into a conversation. This can be pretty convincing with something like Eleven Labs voice cloning and the best prompting or tuning for playing roles. If Heinlein was around today, he might have cloned himself on character.ai or used one of the new AI services marketed for that. Actually with HeyGen, there might be a website talktoheinlein.com that costs $20/month and you can do a live conversation with his digital twin. reply 1970-01-01 5 hours agoprevThis reads very similar to the Your post advocates a () approach from the early 2000s. https://craphound.com/spamsolutions.txt In action: https://slashdot.org/comments.pl?sid=121712&cid=10244451 reply Y_Y 5 hours agoparent> Nice try, assh0le! You're going to find out where I live and burn my house down Something of value truly was lost when /. jumped the laser shark. There was a lot more bad nonsense than on hm,but more good nonsense too. reply KerrAvon 18 hours agoprevAccording to Ginny Heinlein’s notes in Grumbles from the Grave, Arthur C. Clarke also eventually used a form letter system. reply Jun8 11 hours agoprevIn a similar vein, here’s a list of canonical Reddit replies (I deleted a few naughty ones). Generating a similar list for HN would be amusing/useful. Or surprising, since HN comments are generally more substantive. Still, looking for pattern comments less than, say, three sentences would be interesting. [] Yes [] This [] Came here to say this [] Logged in just to upvote this [] I know this will never be seen but... [] I found this gem... [] This will be downvoted to hell/buried but... [] An upvote for you, good sir [] You are a gentleman and a scholar [] You magnificent bastard [] M'lady / tips fedora [] Someone give this man reddit gold [] Edit: Thanks for the gold, kind stranger! [] Anne Frankly I did nazi that coming [] That escalated quickly [] To the top with you! [] Lost it at ____ [] This is why we can't have nice things [] Faith in humanity restored [] Whoa / mind = blown [] Manly tears were shed [] Cutting onions [] I know that feel, bro [] Right in the feels [] Risky click [] Shots fired [] Nailed it [] You. I like you [] I regret that I only have one upvote to give [] Tree fiddy [] Was not disappointed [] Wait, why do I have you tagged as _______? [] What did I just read? [] Da fuq? [] YOU HAD ONE JOB [] Cakeday [] For science [] That's enough internet for me today [] x/10 would (not) Y [] What is this I don't even? [] How is this WTF? [] Lawyer up, delete facebook, hit the gym [] Said no one ever [] /thread [] My first post [] Edit: wow I can't believe my top comment is about _______ [] EDIT: Seriously front page? Thanks guys! [] EDIT: Obligatory front page edit!!! [] Are you me? [] No, this is Patrick! [] I laughed way harder than I should have [] It's almost like Reddit is thousands of different people with thousands of different opinions. [] Plot twist: _____ [] Step one: be attractive. Step two: don't be unattractive. [] ____ here: can confirm / can confirm: am ____/ etc [] Something involving sex with \"your mom\" [] Mom's spaghetti [] Tom Cruise [] Ghandi (Gandhi*) [] [________ intensifies] [] rekt reply Aeolun 8 hours agoparentIt’s actually bizarre how so many vague comments come to mind when reading this list. It makes me kind of happy that I’ve dropped it some years ago now. reply n3storm 15 hours agoprev [–] He could be great friends with Richard Stallman reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Robert Heinlein devised a smart approach to manage fan mail by developing a one-page FAQ answer sheet with his wife, Ginny, marking the suitable responses, which fans found efficient and valuable.",
      "In 1984, they transitioned to crafting personalized letters utilizing a computer, showcasing Heinlein's blend of ingenuity and warmth in handling fan correspondence."
    ],
    "commentSummary": [
      "Some authors like Heinlein and Piers Anthony reply to fan mail personally, while others use form letters.",
      "The article speculates on the impact of automated responses in the current digital era.",
      "It delves into how famous individuals utilize artificial intelligence to engage with their fans, along with a compilation of popular internet expressions and inside jokes frequently seen on Reddit."
    ],
    "points": 182,
    "commentCount": 35,
    "retryCount": 0,
    "time": 1710670308
  },
  {
    "id": 39733661,
    "title": "Super Micro Computer Surges to $60B Market Cap",
    "originLink": "https://www.wsj.com/tech/ai/super-micro-computer-company-profile-d93a41da",
    "originBody": "wsj.com#cmsg{animation: A 1.5s;}@keyframes A{0%{opacity:0;}99%{opacity:0;}100%{opacity:1;}}Please enable JS and disable any ad blockervar dd={'rt':'c','cid':'AHrlqAAAAAMAgRNSS99e1IYAFLqz-w==','hsh':'D428D51E28968797BC27FB9153435D','t':'bv','s':47192,'e':'d52c1c5aa19531bdcf58beed945e859adcc7c1ba2653ddec566cfde370adaad7','host':'geo.captcha-delivery.com'}",
    "commentLink": "https://news.ycombinator.com/item?id=39733661",
    "commentBody": "Super Micro Computer has gone from an obscure server maker to $60B market cap (wsj.com)165 points by Bostonian 22 hours agohidepastfavorite118 comments pavlov 17 hours agoSuper Micro stock was clearly undervalued even on traditional P/E metrics as recently as 2022. And I believe the reason for the depressed stock price was Bloomberg’s allegations that China was using Super Micro’s motherboards as Trojan horses for spy chips: https://www.datacenterdynamics.com/en/news/years-later-bloom... Bloomberg originally broke this story in 2018, then repeated the allegations in 2021. But AFAIK it was never proven. The Nvidia + Meta connection finally broke the spell and allowed investors to look at SMCI with fresh eyes. reply darth_avocado 15 hours agoparentThe low valuation had less to do with the spying allegations and more to do with a history of accounting frauds. Obviously if you have a proven history of fudging up revenue numbers, investors are less likely to invest in you. https://www.sec.gov/news/press-release/2020-190 reply guiriduro 15 hours agorootparentIf that were true, Carvana wouldn't have an obscene valuation. reply woleium 7 hours agorootparent…but it doesn’t? reply diggan 17 hours agoparentprev> Bloomberg originally broke this story in 2018, then repeated the allegations in 2021. But AFAIK it was never proven. Isn't that libel? Or something similar at least, not super familiar with US laws. reply pavlov 17 hours agorootparentMy amateur understanding is that it’s very hard to successfully sue journalists for libel in the US because you’d have to prove malicious intent. A journalist writing a story based on their sources may have been misled by someone with an agenda, but didn’t write the false story with active malice. reply ghaff 16 hours agorootparentAs someone who has done journalism, that's basically correct. All indications are that simultaneously the story wasn't true and the reporter and all their editors firmly believed it was true. My personal assumption is that they believed the story was solidly sourced but they were misled. ADDED: Standards are somewhat different between private people and public people/corporations. reply GauntletWizard 14 hours agorootparentI'm glad the story ran, even if it wasn't true - because it opened people's minds to the idea that this was happening. And it is a very real possibility. O.MG is a hobbyist project, but there's no question that the NSAs dirty tricks book, ANT/TAO[1] has something similar, but far more capable. We should all be paying attention to hardware suppliers and making sure that objects are \"as-ordered\", but today even a standard chip packaging can hide a ton of malicious logic. [1]https://en.m.wikipedia.org/wiki/ANT_catalog reply bdangubic 13 hours agorootparentand ruining Company’s reputation over a lie is what - a collatetal damage? reply GauntletWizard 9 hours agorootparentLots of weird things have been said about many companies. I think the other part of this story is thus - Don't believe everything you read on the internet, or even in print media. reply WarOnPrivacy 10 hours agorootparentprev> I'm glad the story ran, even if it wasn't true - because it opened people's minds to the idea that this was happening. By 2018, all the eyes that could be opened already were. It's true that there were people that aren't convinced by the well-evidenced reporting from the Snowden, etc revelations. Of that group, I can't see Bloomberg's low-evidence story as what finally opened their eyes. reply mschuster91 14 hours agorootparentprev> We should all be paying attention to hardware suppliers and making sure that objects are \"as-ordered\", but today even a standard chip packaging can hide a ton of malicious logic. For smartphones, laptops and PCs that is relatively easy to defend against if you think you might be a target for three-letter agencies - just walk into a computer store and buy what they have on the shelf with cash. Even the NSA doesn't have the resources to intercept and modify all the shipments to Apple, Best Buy, Costco and whatnot - and I'd guess at least Apple has pretty strict security in their supply chain given that Apple stuff has insane value even just for parts if someone were to intercept a delivery. Network architecture however, that is more complex. Cables, Ubiquiti, HP and Dell stuff, you can buy that off the shelf, so same advantage. But servers? Good luck finding ones on the shelf anywhere. reply kevin_thibedeau 14 hours agorootparentprevThe source in this case admitted that he presented a hypothetical scenario with a random SMD component as an example. The ignorant Bloomberg employee embroidered that into a lie. reply underlipton 14 hours agorootparent>Ignorant Never attribute to ignorance what can be explained by malice and corruption (post-GFC/Madoff, finance and its cottage industries no longer get the benefit of the doubt). reply verticalscaler 16 hours agorootparentprevFirst we had too big to fail, now we have too stupid to fail. reply hello_computer 16 hours agoparentprevI think most consumer market \"reporting\" is poorly-disguised market-manipulation. Best advice for your portfolio is to tune all of those assholes completely out. reply underlipton 14 hours agorootparentSomething fun: pay attention to how many financial news headlines are formatted, \"X as Y\" or \"Statement Presented as Fact: Says Opinion-Haver\". The first implies that there's some causational link between X and Y, when none might exist; the writer can claim that they were just stating that two events were happening in tandem. The second biases a reader before they receive the crucial information that the preceding statement was not, in fact, fact. (Also, lately, look out for listicles of stocks \"to buy\" (not financial advice, of course) and \"Forget X\".) reply 1letterunixname 16 hours agoparentprevMeta is wasting all kind of money ($40B across 2 years) on Nvidia, SMCI, and their own gear. SMCI and Nvidia stocks are now overvalued because there are no fundamentals to sustain this business. OpenAI/Microsoft may be an exception, but Meta is wasting money it doesn't have on profits that aren't there. These data centers and servers are being built on orders of Zuck without a concrete, specific product or purpose for their use. This is akin to a newbie business owner buying lots of inventory without orders. reply caslon 15 hours agorootparentMeta obviously has the money; $135B revenue, $88B in expenses, $18B in debt. Even assuming the current push toward general AI is a bubble, which is not unreasonable, the company can afford to throw away billions of dollars. It doesn't matter at all; they own the money printer and can make as many bets in as many markets as they want. The same GPUs that are presently being used to create semi-open AI projects can just as easily be repurposed to power a public launch of their Codec avatars, which are lightyears ahead of what Apple has, or for better prediction engines in what are quite probably the best sales engines of all time: Their websites. Their data centers will be useful for the future of selling products to gullible consumers: Short-form video, which is the first chance in years that they've had to meaningfully take market share from Google. Even assuming it was all vanity, Zuckerberg has earned the right at this stage in his career to make vanity plays. He still has majority control over his company, which shareholders have insisted upon, and he has an almost untarnished record of making incredible long-term bets that seem irrational at the time (Instagram acquisition, Whatsapp acquisition, arguably the Oculus acquisition). He's earned drastic amounts of money for speculators, who have done little to deserve any of it. It would be a strange thing to argue that the speculators suddenly have a better grasp of what he's doing than he does; there are millions of speculators, but only one person with a track record like Zuckerberg. reply matwood 13 hours agorootparentprevMeta has plenty of money to spend. It's also been reported that Meta is using AI to get around Apple's ATT [1], with some reports saying that user ad targeting is better than before ATT came out [2]. Meta is already executing and succeeding on a concrete plan using their AI. [1] https://www.forbes.com/sites/jonmarkman/2023/05/24/metas-ai-... [2] https://www.socialmediatoday.com/news/meta-outlines-evolving... reply fragmede 15 hours agorootparentprevyou know the saying; the market can stay irrational longer than you can stay solvent and all that. turns out P/E ratios determine a theoretical floor for the price, but as we've seen with Tsla and crypto, this shits all vibes anyway. AI isn't slowing down, or going to go anywhere, so these stocks, overvalued though you might see them, aren't going to go down anytime soon, in my opinion, so while what your say is true, NVDA and smci are safe to hold. the real question is what's going on with tsmc and mu, given their proximity to NVDA, and their lack of a pop. reply epolanski 14 hours agorootparentIt's interesting you mention Tesla, because their sales clearly show that the growth expectation was not realistic. And in case of Nvidia it's even much worse. In order for Nvidia to be worth a decent premium over the yield of some index fund like VOO (you're taking much more risk), it has to grow in the order of 42% per year for a decade in revenue. There's no such amounts of money to be spent in hardware, it's lunacy. Not even the other tech giants combined have even a small part of the money required for such growth. And on top of that, this is a very dynamic sector where any competitor, technological breakthrough can make you the new IBM. Prices like Nvidia were highly overvalued but understandable with a stretch of imagination of 25% growth for a decade when it was 300$. I could almost see it and would've still concluded it was an unlikely outcome and risk/reward ratio was not there. But now we long past that mark and in the territory of insane expectations and high premiums paid with a very high risk. reply mschuster91 13 hours agorootparent> There's no such amounts of money to be spent in hardware, it's lunacy. Oh, money there is. NVIDIA is selling shovels to hordes of people searching for gold... first it was cr*ptoc*in miners, now it's billion dollar companies in the search for AGI. But unlike shovels that anyone with access to iron, a fire and a hammer can make, there are only five companies on this planet that can design the chips in the first place: Google and Amazon (who don't sell to outsiders), Intel (who has other, more pressing issues than to design AI training accelerators), AMD (who has the chops on the hardware design side but seems to be completely unable to get the software side stable enough that people would be even willing to look at it) and NVIDIA. And to make the issue worse, there are only three fab houses who can physically manufacture the chips: TSMC, Samsung and Intel... TSMC is all but booked out already, Samsung is nowhere near their level, and Intel both doesn't do fab jobs for outsiders and has completely botched their new nodes for years now. There is just no way anyone can outsmart NVIDIA at that point, and demand is only going to increase in pretty nasty bidding wars. reply matwood 13 hours agorootparent> Oh, money there is. Spot on. Nvidia can't even meet all existing demand right now. https://fortune.com/2024/02/21/nvidia-earnings-ceo-jensen-hu... reply bluGill 13 hours agorootparentprevMoney there is for sure, the claim though is the slightly different 'there isn't that much'. I don't know who is right, but it is the imporant question if you are investing long term. reply epolanski 10 hours agorootparentIt's 2T+ in revenue in a decade. That's where 42% growth starting from today's 60B leads you. That's all of Microsoft, Apple, etc, etc revenue combined all being spent in GPUs at 50% margin. It's not realistic. You're taking a bet where risk/reward is just against the investor. reply kjreact 8 hours agorootparentWhy are you using the metric of annual revenues must equal market capitalization to determine whether a company is overvalued? And why use revenue instead of earnings? Currently if NVDA can double their earnings next year they’d have a P/E ratio similar to MSFT. I think many investors believe they can reach this target hence the high valuation. Whether NVDA will continue to maintain this level of profit for the coming decade is another issue. reply epolanski 10 hours agorootparentprevNone of what you said denies any of my conclusions: expecting Nvidia to grow at a 40%+ rate for a decade is ludicrous. Any of your arguments has been done about leaders like Cisco, Intel or Tesla that balooned to valuations that did not meet reality regardless of those companies in fact doing well, the revenue and growth expectations were just asinine and void of basic math. There's no such thing as infinite demand growth and infinitely deep pockets, let alone margins staying that good for so long (and again the aforementioned companies are examples of the phenomenon). You will likely do much better investing in VOO today than buying Nvidia, let alone having a much better risk/reward ratio. reply solumunus 15 hours agorootparentprevCurrently TSM isn’t guiding much growth, I would guess we see an uptick in guidance in their next earnings report. reply _zoltan_ 14 hours agorootparentthey report earnings monthly. reply zettabomb 15 hours agoparentprevThat was such an incredibly ridiculous story. I spoke with more than a few supposed infosec \"professionals\" who believed it entirely too. Never mind that there were zero reports from other journals (you know, like anything even slightly technical), that none of the cited sources would reiterate what they had supposedly said, or that the claimed mode of operation wasn't even possible. Their follow-up, despite having been disproven numerous times over, was even more ridiculous. reply toast0 11 hours agorootparentIf I'm assuming everyone involved had good intentions, the best thing I can guess is someone was speaking to the writer about the potential of the BMC being used for spying and got some details mixed up. Consider: the BMC has access to the system via PCI-e, as well as kvm and comport. In some systems, the BMC is in the path of the main NIC. There have been some major software flaws in BMC software, including revisions that SuperMicro shipped, where passwords could be bypassed in the network interface. Stuff like this https://web.archive.org/web/20140625065505/http://blog.cari.... and other things on this page http://fish2.com/ipmi/ are all pretty nasty if you thought IPMI was secure in the neighborhood of 2014. reply WarOnPrivacy 10 hours agorootparent> If I'm assuming everyone involved had good intentions, the best thing I can guess is someone was speaking to the writer about the potential of the BMC being used for spying and got some details mixed up. I can't judge their intentions. But Bloomberg doubling down on the story in 2021 strongly discounts the possibility that the original reporting was based on any kind of bad info. ref: https://www.datacenterdynamics.com/en/news/years-later-bloom... Three years is a long time to believe in bad data, with everyone everywhere pointing out the same issues with their story. reply toast0 9 hours agorootparent> “This wasn’t a case of a guy stealing a board and soldering a chip on in his hotel room; it was architected onto the final device,” he said, declining to reveal which company he worked for at the time. Still kind of reads like someone told us there was a bad chip on the motherboard. And there was, and it was the BMC/firmware for the BMC. Did SuperMicro (or their suppliers) just write shitty firmware with zero security for their BMCs because that was the industry standard, or was it a Chinese Ministry of State Security plant who did it as part of an evil plot? Did big companies pull out of SuperMicro because of poor BMC security? Sometimes, a bit, often as just one more checkbox on the way to OCP style defluffed machines; in an early revision of OCP, rather than a BMC, the NIC's wake on lan signal was rerouted to reset, to become reboot on lan. But then OpenBMC happened, cause reboot on lan isn't enough for everyone. reply latchkey 11 hours agoprevDisclosure: Building a CSP business around SMCI products. Sorry if this sounds like an advertisement, I'm really just a happy customer. I feel like the reason why SMCI has done so well in this AI round is because their server architecture is best in class and they have been able to support the internal changes necessary for AI workloads. They also support AMD CPUs, while others only offer Intel. 6 years ago, Cenly Chen / SMCI was saying AI was going to be huge and that total revenue would be $36B, in 2025 [0]. We are well past that number now. Amazing how AI turned out to be even bigger than anyone could have imagined, but at least they had some vision even back then. Dell, Giga, ZTS are all behind in their offerings while SMCI is iterating and are now even getting to the point of water cooling and L11 manufacturing. I just received a shipment of AS-8125GS-TNMR2 (8U MI300x) and the thing is an amazingly well designed beast of a chassis. Everything slots together perfectly. If you study the user documentation, the layouts of the internal block diagrams are fantastic and build for speed. We are lucky enough to have been able to open an account directly with them. It wasn't easy and required a ton of due diligence, but working with the team there has been a top notch experience. [0] https://www.youtube.com/watch?v=WzqBuiwkv5I reply wmf 10 hours agoparentGotta love doing a ton of due diligence for the privilege of (checks notes) buying an off-the-shelf product at full price? reply latchkey 9 hours agorootparentOh, tell me where you can buy TNMR2+MI300x off the shelf! I checked BestBuy and they were out of stock. reply wmf 8 hours agorootparentMy point is that it doesn't sound like they're customizing anything for you. It's ridiculous that you need permission to buy GPU servers. I realize there's a backlog but there are neutral ways to manage that. reply latchkey 7 hours agorootparentThe equipment was all ordered to our individual spec. Our box is one of the most powerful they've shipped so far. For example, everyone else is getting it without NVMe and we got it with 155TB. As for permission, look up 88 Fed. Reg. 73458. I (and every other CSP) had to sign multiple documents agreeing to those regulations. Both SMCI and I are taking them very seriously, especially as a CSP renting them out. I'll vet every customer personally. Neutral or not, from what I understand, there is no backlog for MI300x. You're either on the approved list or not and the entire list is spoken for. AMD knows every single business buying these chips right now, and they are carefully watching the orders ship out. reply codethatwerks 7 hours agorootparentprevWholesale vs. retail? They probably don’t want to deal with 1000 HNer accounts buying a single chip and do credit checks on them all (or do retail at all). reply latchkey 7 hours agorootparentThat's exactly it. Normally they direct you to an OEM partner like Exxact. To be honest, they are really not set up for individual accounts. It was a bit of back and forth with the paperwork to get everything done, which took a lot of time. They asked for everything about the business. As for the direct account, once you have it, it is a golden egg because this way you also have direct support from their teams for anything you need. It is fantastic and I feel lucky to have gotten in. reply Throwawayhahzoh 5 hours agorootparentThey do do retail: https://store.supermicro.com/us_en/ Shipped a SYS-531A-IL workstation in less than a week including a weekend as my latest workstation configured with my chosen, albeit from a limited selection of memory, good CPUs and NVME and SATA drives (enterprise options for all of those), all properly kept cool per lm-sensors and smartmontools. I used their live chat to quickly get a couple of answers, it was packed well, and in theory for a reasonable up front cost they'll cross ship a replacement for the next 5 years. When Mr. Rsync.net was composing his message today I was adjusting my scripts to do their nightly backup to his service that uses their servers. For a couple of decades have bought a bunch of their motherboards through resellers for myself and family, and built some rack mount systems for a friend bought I forget from where with excellent results. The current swapping of hardware due to the above purchase took down a system that had been in service for a dozen years. reply latchkey 5 hours agorootparentAwesome. Interesting, I just did a search for 8125GS-TNMR2 on that link, and nothing came up. reply mistrial9 7 hours agorootparentprev> It's ridiculous that you need permission to buy GPU servers guess what, that is exactly what is playing out.. due to certain uniform services closely tied to the show reply dangle1 20 hours agoprevhttps://archive.ph/FrBaL reply gnuser 11 hours agoprevI’ve built entire DC’s out of Super Micro hardware, they rock. For example, their quad opteron boards allowed me to make 64 core systems in 2013-era! reply blue11 9 hours agoprevThe article hints at a special relationship with NVIDIA, but it goes back a long time. They didn't move recently to be close to NVIDIA's HQ. They have been there for 30 years. For example, back in the 90s Supermicro was selling NVIDIA products and NVIDIA was buying Supermicro desktops for their offices. reply markhahn 14 hours agoprevhasn't been obscure for a long time. this says more about WSJ-reading \"enterprise\" IT than anything else. reply drakerossman 21 hours agoprevWho's their competition? What's their moat, except for being 15-minutes drive away from Nvidia's HQ? reply throwaway11460 17 hours agoparentTheir moat is good server hardware that can be ordered without talking to a sales person that has one goal - determine how much they can milk your budget. reply Gelob 16 hours agorootparentThey used to let you order without talking to sales, now they want to validate the config like Dell and HPE. They are slow and don't respond and their ETAs are terrible and often wrong. reply amluto 11 hours agorootparentThey have multiple friendly, competent resellers who will happily quote their machines, often using online tools, and will often come in around half of, say, Dell’s price. Maybe even better if you want something ridiculous like disks in your machine. This has been the case for years. reply 1oooqooq 17 hours agorootparentprevironically they could only do that by a lack of investor interest. Let's see how many days this last now. reply throwaway11460 17 hours agorootparentIt lasted 3 decades and they always knew that this is the number one reason why people buy from them. I think it's safe, but let's see. reply rmah 17 hours agoparentprevTheir competition is the enterprise hardware divisions of HP, Dell, and IBM. SuperMicro makes reasonably good quality, lower-cost server equipment. They are, IMO, a pretty good value if you don't want high-end support from the hardware vendor. reply 0xcde4c3db 17 hours agorootparentASRock has also pushed into some of Supermicro's traditional product segments via the \"ASRock Rack\" brand. I have no idea how big that business is, though. reply kbar13 14 hours agorootparentnowhere near as prevalent as supermicro still. asrock rack does make some decent stuff tho so i would imagine theres a good future there reply jauntywundrkind 13 hours agorootparentprevGigabyte and Zotac also comes to mind, resemble Asrock. There's a bunch of other kit too, but https://servethehome.com reviews a bunch of the various rack systems. Example of some late January posts. Albeit none are of the \"fits lots of GPU\" sort that is helping propel Supermicro, but these folks all have those offerings too, Supermicro SYS-511R-M Intel Xeon E-2488 1U Server Review https://www.servethehome.com/supermicro-sys-511r-m-intel-xeo... Gigabyte R183-Z95 Review Dual AMD EPYC Server with a EDSFF Twist https://www.servethehome.com/gigabyte-r183-z95-review-dual-a... ASRock Rack ALTRAD8UD-1L2T Review This is the Ampere Arm Motherboard You... https://www.servethehome.com/asrock-rack-altrad8ud-1l2t-revi... reply 1letterunixname 16 hours agorootparentprevHPE, Dell, and IBM are glorified CDW-business model salespeople. Megacorps have no use for that when they can engage the source and get their own custom gear. https://en.wikipedia.org/wiki/Quanta_Computer reply convolvatron 18 hours agoparentprevits not really a moat, but its a difficult model to emulate. what they offer is a set of standard parts, tailored for verticals they think are important. but the secret sauce is that they are willing to customize just that much to make things work the customer. even if you are a small startup and can't promise more than 100 units/yr, its entirely likely that they will build a custom PCB or riser or chassis on the chance that you will be successful. not a whole design, but a tweak on one of their standard models. they've done that for me before with no NRE, maybe they do charge sometimes. so their moat is that they have enough money to make those bets, and an engineering organization that can do that in a lightweight enough fashion to make the whole thing work. and they do this while remaining very cost competitive reply godzillabrennus 17 hours agorootparentGiven how few people in a “startup” possess the skills to know and articulate their needs and have the network to reach the right people in a company that size, it seems like a reasonable bet to make. reply convolvatron 16 hours agorootparenttheir sales people are in on this - not a special deal. need a extra hole in this chassis? yeah sure, lets do that. reply amluto 10 hours agorootparentprevI have a system along these lines lying around. It’s a very low volume Supermicro board, made for a partnership between Intel and a little startup (not mine). The startup might, at their discretion and possibly with an NDA or two, tell you the model number. Then Supermicro would sell you the board. I have no idea what money, if any, changed hands, other than the fact that I paid, IIRC, about $600 for the board. reply guilhas 15 hours agoparentprevAs a home user I like their hardware has the least vendor lock in, so when things break it is easier to replace with generic parts or swap around reply 1letterunixname 16 hours agoparentprevQuanta and FoxConn. The weird thing though, is the megacorps who can afford to design their own gear in-house are spending money on these outside shops. Waste of money. reply fragmede 15 hours agorootparentDepends how you see it. Spending resources to do it in house when an outside shop does it could be seen as a waste of money too. if someone else is already doing it, why spend money redoing what they do? reply Bostonian 20 hours agoparentprevThe article says `Analysts clash on Supermicro’s ability to hold on to its position longer term. Wedbush analyst Matt Bryson said, historically, no company selling servers has had more than 30% market share. “There’s not a reason Dell can’t do exactly what they’re doing,” Bryson said. Others aren’t so sure. Some analysts say that established competitors will have a hard time bringing new products to market so quickly and have larger revenue streams from software and services. Supermicro is trying to gain further market share by doubling down on AI and continuing to ship its servers out quickly. The company is also keeping prices low to entice new customers: Its gross profit margin totaled around 15% in its latest quarter, down from 17% in the previous one. HPE, by comparison, had gross margins of 36% in its latest quarter.` reply lightedman 17 hours agorootparent“There’s not a reason Dell can’t do exactly what they’re doing,” Bryson said. I find that quote interesting. As someone that worked for Dell, I can figure out why - they're heavily-invested in the support side of things. They're too busy with that and their current consumer and business-class offerings that realistically the server market segment they're already in doesn't exactly overlap with Super Micro, and most likely never will outside of some buzzword AI marketing. reply paulmd 6 hours agorootparentDell also can’t do what supermicro does because it would eat their margins. The dirty secret is that supermicro is making headway because they’re a lot cheaper than dell or hp. If dell/hp start to compete on price they don’t really gain any additional marginal business to speak of, but they do lose margin on all their current contracts (who presumably want to get the discount too). The “companies become too stagnant to disrupt their own revenue streams” isn’t just a trope about leadership vision, it’s a very real financial phenomenon. Customers don’t like open price discrimination and often it’s better to keep your best customers than to chase after new ones and push all your margins downwards. See also: the gpu market. reply idontknowifican 18 hours agorootparentprevfwiw: my work is moving from supermicro to dell nodes due to the immaturity of the support (interface and personnel). reply mrweasel 13 hours agorootparentFunny, we're going the other direction, for much the same reasons. I suppose different organizations have different needs and Dell is moving in the wrong direction for us, while SuperMicro seems to deliver in the areas we value. reply rodgerd 11 hours agoparentprevHere's the incumbent experience for proper servers: 1. You're a small company. None of the big companies will talk to you. You're a waste of their time. 2. You're a medium company. Maybe the worst sales person on the team is desperate enough to talk to you. 3. You're a big company. They will be only too happy to talk to you. You want to buy a rack of servers. They will not sell you a rack of servers. No, no, no. You need to talk about how their SAN is much better than your current SAN. Also they just bought a virtualisation company so maybe you should replace your virtualisation stack with theirs. And have you considered how helpful their outsourcing service could be for running your datacentre? They'll undercut your current team of staff as long as you commit to replacing all your servers with theirs. Also they hear you're making use of REST services, have you considered one of their REST security appliances? They'll throw them in free. None of these conversations happen with the person trying to buy a rack of servers, they'll happen with a vice president or procument or your finance team. Your rack of servers comes with a bunch of \"free\" stuff that you didn't want and don't have time to implement. Eighteen months later you're being told to drop all your work that your customers care about, because whoever inked the deal with the free REST appliances looks stupid if they don't get used, so you have to implement them Supermicro are just selling you a rack of servers. reply oldpersonintx 17 hours agoparentprevDecent products with decent service, no gimmicks and fair pricing is a moat reply bawana 17 hours agoprevI guess a company just has to move close to nvidia, label one of its products with ‘ai’ and watch its valuations 10x. Foodtruck.ai? reply riwsky 17 hours agoparentLLM = Large Lunch Menu = $$$ reply solumunus 15 hours agoparentprevSMCI is experiencing massive revenue growth, so you also need that. SMCI forward PE isn’t even that crazy yet, TSLA had much higher at its peak. reply johnklos 17 hours agoprevIt'd be nice if they gave a damn about security. I had clients stop buying them because a glaring security problem was determined to be \"not an issue\" that they wouldn't fix. It's one thing to say they wouldn't or couldn't fix products already made with the flaw, but it's another entirely to have a culture of security that says, \"Sure, this flaw could cause your machine full, unfettered compromise, but because it's not likely to happen and not highly publicized, we don't care.\" It makes me think they'll treat current and future security problems the same way. Security shouldn't be based on popularity contests. Sorry, but not for me. reply gizmo 16 hours agoparentDo you have more info on this? iDRAC doesn't have the best security track record either, but people don't really seem to care. reply johnklos 15 hours agorootparentIn a nutshell, the problem is this. I don't know whether this has changed, but this was true as of 2018 / 2019. Most of their motherboards have IPMI with a separate a management port. A good number of them share IPMI management with the motherboard's primary ethernet port by default if nothing is plugged in to the management port. The motherboards have no way to configure them to NOT share the primary ethernet port beyond having the full stack of software needed to configure their IPMI. What this means is that there're no jumpers one can change and no settings accessible in the BIOS that can force IPMI to stay on its own port, so if a BIOS gets reset, the battery dies or even just temporarily fails to provide power (like if it's being shipped by air and gets very cold), or you want to ship servers directly to a datacenter, the machine is 100% ownable on the public interface BY DEFAULT unless the management port is connected (and even then sometimes it decides to share the primary port - probably a function of link negotiation speed with the switch). Sure, it's not a common occurrence, but it happens. The solution for all the servers we already had deployed? We got ethernet loopback plugs for every one of them where the IPMI port wasn't already connected to a switch we administered. A reasonable response: \"Sure, that could be a problem sometimes. We can't change motherboards we already sold, but we'll bring this up with our design team so there'll be a jumper you can change so sharing will never happen, even with a reset BIOS.\" Their response: \"This isn't a security issue.\" reply secabeen 14 hours agorootparentThis is an interesting attack surface. Can you extend the risk out a bit? Assume that you have a vulnerable supermicro IPMI now exposed on a public interface. It has no IP address, and is presumably issuing DHCP DISCOVERs in an effort to get an IP. How do you reach the IPMI device to exploit it? What additional access do you need to get there? Root on another device on that public network would do, you could forge the necessary DHCP responses to get it configured with an IP address of your choosing. Non-root on another device on that network might also work, if it fails DHCP and self-configures on a 169.254 address, assuming it does that. Is there an obvious way to exploit such an issue from beyond the public subnet?Every attack I can imagine would be blocked by either inbound firewalls, or a failure to reach the IPMI as an unexpected device on the public subnet. I suppose that it would be a possible risk if you have a DHCP server on that public subnet issuing IP addresses to all devices, but that seems like a larger risk anyways. Server networks should be static assigned or static DHCP in all cases. reply hsbauauvhabzb 13 hours agorootparentI unknowingly did this, I found a random ip exposing the interface, and used admin/admin to compromise it - I was very confused as I explicitly did not plug in the ipmi interface as I do not want it. I ended up using a PCIE nic, which ipmi does not auto bridge to. reply michaelt 10 hours agorootparentprevAssume your network is not-entirely-trusted - like a college network where some curious students might be poking around and running network scans, and some others have malware on their computers. The expected network port is plugged in, so even if your switch is checking MAC addresses, everything is in order. Most networks use DHCP, so the IPMI picks up an IP address. A student finds it with their port scan. reply johnklos 12 hours agorootparentprevThere are plenty of cheap colos that do no filtering on their public networks. Some are saving money by putting a number of machines on a single ethernet segment, some are saving IPs by not having a /31 (or, much more often, a /30) for each client, and some both, so a compromised machine could easily run a DHCP server and scan any takers. You're right that no sensible network would forward packets to a misconfigured IPMI, though. That still leaves very real things that've happened - the IPMI switches to the public interface and can no longer be reached on the managed local interface, and then you're rebooting several times in hopes it'll switch back and making aliases on a public interface to see if you can talk to it on the public segment. It's not professional at all. reply dilyevsky 13 hours agorootparentprev> This is an interesting attack surface. Can you extend the risk out a bit? Assume that you have a vulnerable supermicro IPMI now exposed on a public interface. It has no IP address, and is presumably issuing DHCP DISCOVERs in an effort to get an IP. How do you reach the IPMI device to exploit it? What additional access do you need to get there? I think you misunderstood the level of fucked up this really was. The BMC device sits on the north bridge and literally scoops up packets from the main NIC which means it can even be accessible from the internets (if you didn't firewall port 623). See [0] for an example how variation of this unfolded. [0] - https://www.zdnet.com/article/over-47000-supermicro-servers-... reply wannacboatmovie 14 hours agorootparentprev> The motherboards have no way to configure them to NOT blah blah blah.... Most of your claims are false. Super Micro has a utility to write the correct bits into EEPROM to disable this behaviour and stop the failover as default. The utility was available years ago, prior to the time frame you state. Any competent sysadmin would just build this into the deployment task sequence. reply johnklos 14 hours agorootparentFirst, do you have a link to documentation for this ability? Second, \"any competent sysadmin\" would have to know that this exists. Super Micro's security team didn't know this existed, or if they did, they failed to mention it in their response. reply wannacboatmovie 13 hours agorootparentIn the normal run of things, I'd tell you to do your own research. But we're all Irish today and I'm in a particularly giving mood. https://www.supermicro.com/Bios/sw_download/645/IPMICFG_User... IPMICFG -lani 0 You're welcome. (I do recall the syntax being a bit more cryptic, passing hex values, perhaps they've improved things since I last did this. Nevertheless, the capability has always been there.) SuperMicro themselves not knowing this exists isn't surprising in the least. reply RVuRnvbM2e 12 hours agorootparentaccording to that doc the functionality was only added late 2022. reply wannacboatmovie 12 hours agorootparentImpossible as I was doing this nearly 10 years ago. See my comment about remembering the process to be rather cryptic (writing hex values to address offsets) but the capability WAS there. Perhaps they added that switch recently to make it more user friendly. reply johnklos 12 hours agorootparentprev...but IPMI configuration isn't stored in EEPROM. It's stored in NVRAM. And I believe you that you configured this pre-2022, but anyone could use the IPMI tools to configure this pre-2022 and pre- -lani option. You're trying to say it's in EEPROM, meaning it's invulnerable to battery loss. It definitely isn't. reply broknbottle 13 hours agorootparentprevSupermicro does offer board variants without the IPMI feature. I'd argue that most people who are buying the variants with IPMI are planning to utilize the feature.. The sideband feature also tends to be associated with an interface on the board that is considered the non dedicated IPMI \"management\" interface. Use one of the other onboard NIC ports or an PCI-E NIC like x550-T2, etc. reply dilyevsky 15 hours agorootparentprevThis! And their bmc is trash and openbmc only ships on few boards (arm ones iirc) reply dualboot 16 hours agorootparentprevIndeed. An OOB interface is something you should always handle like radioactive material. It's volatile, powerful, and should be handled with extreme care and caution. reply jakehop 11 hours agoprevSM has been producing good quality hardware for decades. I remember them from catalogues of my childhood. Obscure is not the right word here. reply jsnell 17 hours agoprevI would have thought every server maker was able to sell every GPU they got their hands on at this point in the hype cycle. If SuperMicro is gaining market share, isn't it just a sign that Nvidia is giving them a bigger GPU allocation? reply solumunus 15 hours agoparentThat’s the rumour, NVDA give them higher priority because of a longstanding relationship. reply dboreham 14 hours agoprevIf only I had known they were listed in the US. Just assumed they were an offshore company (based on their pretty terrible support). Disclosure: long time user. reply rasz 18 hours agoprev>obscure server maker First google server racks https://blog.codinghorror.com/building-a-computer-the-google... https://en.wikipedia.org/wiki/History_of_Google#Late_1990s https://commons.wikimedia.org/wiki/Category:Google%27s_first... were build using Supermicro P6SBM reply solarkraft 17 hours agoprev... and they're not an obscure server maker anymore? What happened? As far as I know, as much as being obscure, they've also been around forever. reply bluesounddirect 10 hours agoprevFriends don’t let friends drive supermicros . reply renewiltord 10 hours agoprevI love this investment because when Bloomberg News posted their SMCI story all the security suckers ate it up and dunked shares. Picked a bunch up at a 50% discount and they rebounded. The problem is that security groupies are really eager for there to be obscure security flaws. HN was full of them. And they have this thing about \"respected sources\" and shit like that because they don't know how to evaluate things themselves. Your loss, my gain. Haha. reply Bostonian 22 hours agoprev'Nvidia’s chips became the workhorses of the boom, making the complex computations necessary to create systems such as OpenAI’s ChatGPT. Server manufacturers who could ship those chips to customers fastest and in the largest quantities had an edge. Liang said it has been helpful that his base in San Jose, Calif., is just a 15-minute drive from Nvidia’s headquarters in Santa Clara. “Our engineering teams are able to work together from early morning to midnight,” he said. Supermicro’s recent dominance in the AI boom, industry executives and analysts say, also stems partly from its strategy of making electronic “building blocks” that can be assembled into servers in an almost endless number of configurations. Rivals offer a more limited menu to customers. That flexibility has been an advantage in the AI boom, analysts say. Developers of self-driving car technology want different server setups than companies making language-generation AI systems such as ChatGPT. Supermicro can deliver customized infrastructure for both.' reply m3kw9 17 hours agoprev [–] I thought this was a meme pump, are these guys actually having some legitimate products or services? reply rsync 17 hours agoparent\"...are these guys actually having some legitimate products or services?\" rsync.net is built entirely on supermicro head units and, until a few years ago, their JBODs. Then they got greedy and tried to do the old \"certified drives\" bullshit with their JBODs and that was the end of that ... now we use the celestica JBODs we source from IX systems. Head units are still supermicro, though. Fingers crossed ... reply throwaway11460 17 hours agoparentprevIt's the oldest, most successful, cheapest and for many people technologically superior server maker that's not IBM or HPE. Many successful businesses were built on their products in the past 3 decades. Most notably Google. reply hakfoo 17 hours agorootparentThey were always in the list of \"you want something that's workstation/server reliable, but you don't want to deal with an OEM who's going to sell you a propriatery case/PSU/motherboard. ISTR Tyan being in the same boat, but you don't hear as much about them anymore. reply jethro_tell 14 hours agorootparentMan, I got a rack of tyans recently and I have to say, its not even a contest. Maybe something was of with that order but 1/3 of the hosts had issues, I suspect at the motherboard level but aside from sending them back a couple times for service support I've pretty much abandon the rack at this point. I'll probably send the machines to the shredder and replace them next time I have a budget cycle. reply chx 15 hours agorootparentprevAs a small, very specific footnote: I am unaware of anyone but Supermicro making 3U chassis with a 80mm rear fan. As the ATX rear I/O is sized to squeeze into 1U it means there's only 2U or 88.90mm left for fans and most chassis makers will just go with 60mm fans. reply hakfoo 14 hours agorootparentISTR seeing they did 4U with 120mm instead of dual 80 too. That always looked compelling, because I figured a 4U rackmount would make a neat desktop-style case, but I could never justify the price. reply somat 13 hours agorootparentI make my desktops out of 4u chassis. Mainly because they have good airflow. But it does bring one glaring design issue to light. consumer grade mother boards are schizophrenic about their airflow. the cpu and ram are orientated to flow left to right and the expansion cards expect the flow to go front to back. Server grade mother boards have coherent airflow however I have found server boards are less than optimal for a desktop application. they boot slow, are picky about components, and the cpus tend to be slow and wide. So I tend to alternate, one generation I get fed up with consumer grade bullshit and buy a server grade board, the next I get fed up with server grade bullshit and buy a consumer board. My favorite chassis so far has been this generic one, the fans suck(just buy a new set of good fans right away) and supplied drive bays suck. but look at all them 5 1/4 bays, bays for days. You can put every stupid hotswap bay, fan controller and drink holder gimmick you want in there. and still have room for more. https://www.newegg.com/rosewill-rsv-l4500u-black/p/N82E16811... reply chx 12 hours agorootparentAsRock X570D4i-2t because https://www.reddit.com/r/sffpc/comments/lymbka/asrock_rack_x... reply linsomniac 13 hours agoparentprevSince around 2000 I bought something shy of 1,000 of them for a small server hosting company. Mostly the smaller ones in the sub-$1,000 price range, and we had very good luck with them. With the exception of one year where we had a roughly 100% failure rate on the power supplies (same make, model, mfg as ones we had in service 5+ years), they were just workhorses at extremely reasonable prices. After the power supply failures we started switching to their \"twin^2\" units (or something named like that) which were 2U RM boxes with redundant power supplies and 4x semi-blade servers, which again we could provision for the sub-$1,000 price. I've since looked at pricing some systems from them as an alternative for the Dell servers we've been buying more recently, and oddly enough the prices all seem to be in the $10K+ range. A pretty big shock to see what used to be \"dirt cheap servers\" up in that range, but the RAM and SSDs really add up. Even though Dell seems to have insane pricing in their configurator for RAM and drives... reply toast0 11 hours agoparentprevSoftlayer was built on all/mostly all SuperMicro servers up until IBM bought Softlayer and then there were a lot of Lenovo. As an employee of a customer of Softlayer, the servers were very reliable. I have my personal hosting on a rented SuperMicro server now, and pretty happy with it, even if the hardware is 10+ years old (Xeon Lynnfield) and the IPMI requires ancient JNLP that barely works ... I only barely need IPMI (gotta console in to decrypt the disks on reboots, and it was handy for setup) reply ardaoweo 17 hours agoparentprevServer-grade motherboards that have been widely used for a long time sounds like a legitimate product business. Whether or not they have long-lasting competitive advantage, that is another question. reply andruby 16 hours agoparentprevI've bought and used supermicro servers since 2004. They sell good hardware without the IBM / HP premium. reply wil421 17 hours agoparentprevI’ve used several of their sever motherboards and RAM. They are good products. There’s one in my NAS right now. reply formerly_proven 17 hours agoparentprevSupermicro has been around since forever and is like one out of one and a half OEMs who actually sell server building blocks on the open market. Stock market shenanigans are also hardly a new experience for this symbol, either: Already forgot about the discredited 2018 Bloomberg hit piece? https://www.bloomberg.com/news/features/2018-10-04/the-big-h... reply hello_computer 16 hours agoparentprev [–] They aren't as nicely made as HP's offerings, but solid, and good value for the price. I'd buy a used Super Micro before I'd buy a brand-new Dell, even if the Dell were cheaper. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The code snippet is for an animation on wsj.com, displaying an element fading in over 1.5 seconds.",
      "It also includes a JavaScript snippet for a captcha delivery system, necessitating JavaScript activation and ad blocker deactivation."
    ],
    "commentSummary": [
      "Super Micro Computer's stock surged post-spy chip allegations, despite past accounting fraud, sparking debate on information verification and security risks in hardware suppliers.",
      "Emphasis on Supermicro's AI/server architecture success, strong market presence, and IPMI device security concerns impacting stock prices.",
      "Discussion covers server customization, hardware quality, and market position, comparing Supermicro with other server brands."
    ],
    "points": 165,
    "commentCount": 118,
    "retryCount": 0,
    "time": 1710674711
  },
  {
    "id": 39734441,
    "title": "Mars Influence on Earth's Climate and Oceans Every 2.4M Years",
    "originLink": "https://www.smithsonianmag.com/smart-news/mars-has-an-unexpected-influence-on-earths-oceans-and-climate-repeating-every-24-million-years-study-finds-180983952/",
    "originBody": "SMART NEWS Mars Has an Unexpected Influence on Earth’s Oceans and Climate, Repeating Every 2.4 Million Years, Study Finds The gravitational interactions between Mars and Earth as they orbit the sun may have periodically promoted a warmer climate and changes in ocean circulation on our home planet Catherine Duncan Staff Contributor March 15, 2024 Mars' orbit has an impact on Earth's oceans and climate in cycles of 2.4 million years, new research finds. NASA In an astonishing cosmic cycle that occurs every 2.4 million years, Mars’ gravitational pull is shifting Earth’s path around the sun, warming its climate and increasing vigorous deep ocean circulation, according to a new study published this week in the journal Nature Communications. “Mars’ impact on Earth’s climate is akin to a butterfly effect,” study co-author Dietmar Müller, a geophysicist at the University of Sydney in Australia, tells New Scientist’s James Woodford. He acknowledges the Red Planet is too far to have an immense gravitational impact on our world. “But there are so many feedbacks that can amplify even subtle changes.” By poring through 65 million years of deep-sea sediment records, researchers analyzed Earth’s history of ocean current behavior. They sampled nearly 300 drill cores, which documented how these currents behaved over time. Breaks in sedimentation indicated the presence of vigorous deep-sea currents, while continuous sedimentation represented calmer conditions. The team found the strength of these currents waxed and waned over 2.4-million-year cycles, known as “astronomical grand cycles.” Comparing this fluctuation to astronomical events, researchers found an unexpected connection: Each cycle coincided with records of gravitational interactions between Earth and Mars. “We were surprised to find these 2.4-million-year cycles in our deep-sea sedimentary data,” says Adriana Dutkiewicz, lead author of the study and sedimentologist at the University of Sydney, in a statement. “There is only one way to explain them: They are linked to cycles in the interactions of Mars and Earth orbiting the sun.” As the two planets chart their orbital paths, their gravitational fields interact in a process called resonance, Müller says in the statement. This cosmic exchange alters how circular Earth’s orbit around the sun is—and consequently, how close the planet is to solar energy. During periods with greater exposure to solar radiation, the Earth adopts a warmer climate. And with this warmer climate, the amount of vigorous ocean currents was found to increase. Researchers describe the currents, also called eddies, as “giant whirlpools” that often stretch down to the abyssal seafloor and erode the area, driving the accumulation of large amounts of sediment in snowdrift-like walls. These natural, gravitationally induced climate cycles are not tied to the current and rapid global warming, which is a product of excessive greenhouse gas emissions. However, studying the eddies’ response to warmer climates across long periods of time can provide scientists with crucial insight into how climate change affects ocean circulation. If human-produced global warming continues along its current path, “this effect will dwarf all other processes for a long time to come,” Muller tells CNN’s Laura Paddison. “But the geographical record still provides us with valuable insights about how the oceans operate in a warmer world.” With climate change, scientists have suggested a vital ocean current system called the Atlantic Meridional Overturning Circulation (AMOC)—which transports warm water north and cold water south—could soon collapse. A study last year found the system will likely shut down sometime this century—and as soon as 2025. The new research indicates the cosmic cycle that drives deep ocean eddies could help bolster ocean circulation in the event of an AMOC collapse. “We know there are at least two separate mechanisms that contribute to the vigor of deep-water mixing in the oceans. AMOC is one of them, but deep ocean eddies seem to play an important role in warm climates for keeping the ocean ventilated,” says Müller in the statement. Still, some scientists remain unconvinced by certain aspects of the research. Matthew England, who studies ocean circulation at the University of New South Wales in Australia and was not involved in the study, tells New Scientist he isn’t sure the Red Planet is to blame for these cycles. “I’m skeptical of the link to Mars, given its gravitational pull on Earth is so weak—at only about one one-millionth of that of the sun,” he tells the publication. “Even Jupiter has a stronger gravitational field for Earth.” Joel Hirschi, associated head of marine systems modeling at the National Oceanography Center in England who was not involved in the study, tells CNN that the findings in relation to Mars’ influence on ocean currents were significant. But, he adds that the “proposed link with the ocean circulation is speculative.” Though eddies have grown in activity over the past decades, satellite observations have shown their currents aren’t always able to reach the seafloor and maintain effective ventilation, he tells the publication. Still, researchers remain hopeful that the 2.4-million-year cycle, and the increased circulation it may regulate, could provide a necessary fallback for ocean systems in the climate crisis. “Our deep-sea data spanning 65 million years suggests that warmer oceans have more vigorous deep circulation,” adds Dutkiewicz in the statement. “This will potentially keep the ocean from becoming stagnant, even if Atlantic Meridional Overturning Circulation slows or stops altogether.” Get the latest stories in your inbox every weekday. Catherine DuncanREAD MORE Catherine Duncan is an intern with Smithsonian magazine. Filed Under: Climate Change, evolving climate, Geology, Global Warming, Mars, New Research, Oceans, Outer Space, Planets, Solar System, Sun Most Popular Is Corned Beef Really Irish? Celebrate St. Patrick’s Day With a Photographic, Virtual Tour of Ireland You Can See a Rare, Bright Comet This Month. Will It Be Visible During the Solar Eclipse? St. Patrick Opened a Portal to Purgatory on This Little-Known Irish Island The Real History Behind 'The Zone of Interest' and Rudolf Höss",
    "commentLink": "https://news.ycombinator.com/item?id=39734441",
    "commentBody": "Mars Has Influence on Earth's Oceans and Climate, Repeating Every 2.4M Years (smithsonianmag.com)156 points by pseudolus 20 hours agohidepastfavorite76 comments jimmytucson 17 hours agoJury is still out on an astronomical explanation for the apparent 26M year periodicity[0] of mass extinction events. Lisa Randall has an entertaining theory[1] that it’s due to the sun passing through dark matter in its orbit around the galaxy. [0] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC344925/ [1] https://en.m.wikipedia.org/wiki/Dark_Matter_and_the_Dinosaur... reply frutiger 15 hours agoparentIt doesn’t even have to be dark matter, most of the (non-dark) galactic mass is in the galactic plane and the sun oscillates up and down passing through the plane as it orbits the centre. reply indigobunting 17 hours agoparentprevExcept that the orbital period around the galaxy is ~230M years. reply exe34 16 hours agorootparentYou could oscillate up and down across the orbital plane many times during one orbit. Not saying that's what's happening here, but it's a possibility. reply ckcheng 16 hours agorootparentThat’s the argument I think. Per that Wikipedia link: > Randall hypothesizes a plane of dark matter exists roughly on the plane of the Milky Way galaxy. As the Sun oscillates in its orbit around the center of the galaxy, it passes through the dark matter. reply jagged-chisel 14 hours agorootparentWhat would cause the oscillation? reply roywiggins 13 hours agorootparentIf you're not smack bang in the middle of the galactic plane, you're going to be accelerated towards it by gravity, so you'll tend to bob up and down, like a pendulum. reply cwillu 12 hours agorootparentI'm not sure I buy that the oscillation around the plane can have a period other than the orbital period. reply tejtm 10 hours agorootparentThe oscillations are not \"around\" the plane, they are through the plane. That is; the plane itself has a local gravitational attraction which is orthogonal to the galaxy core gravitational attraction. If a mass is above or below the plane, the \"local\" gravity will pull towards the plane. since nothing stops it at the mid line of the plane it passes through to the opposite side; rinse and repeat. This bouncing above and below center line of the disk is more or less independent of our solar system completing galactic orbits. reply jagged-chisel 8 hours agorootparentThis is the best explanation IMO. reply shagie 10 hours agorootparentprevThe Radcliffe wave is oscillating through the plane of the galaxy. https://en.wikipedia.org/wiki/Radcliffe_wave And a recent paper on its oscillation - https://www.sci.news/astronomy/oscillating-radcliffe-wave-12... reply mattsan 12 hours agorootparentprevI haven't done the math but I would think it's possible due to interactions from other solar systems. Similar to how satellites oscillate around Lagrange points reply exe34 12 hours agorootparentprevThe up down oscillation is completely decoupled from the orbit itself, assuming the orbital potential is uniform. It's like the galaxy wasn't rotating at all. You put something above the disk, and it gets accelerated downwards, but doesn't stop at the disk, it keeps going, until it gets dragged back up. Think of it like a pendulum. reply truculent 6 hours agorootparentWould we expect the oscillation to get smaller over time, like a pendulum, or is that stretching the analogy too far? reply exe34 1 hour agorootparentThere might be some gas in the way that would slow it down slightly, but I expect that to be a very long time scale compared to other things like interactions with other systems and even the upcoming collision with Andromeda. reply partitioned 11 hours agorootparentprevdo you think motion in X is always dependent on motion in Y? reply exe34 15 hours agorootparentprevAh that's probably where I learnt it from then! reply __MatrixMan__ 17 hours agorootparentprevMaybe dark matter bodies orbit faster. reply exe34 16 hours agorootparentOrbital speed is a fixed function of central mass and distance from that mass. reply thfuran 15 hours agorootparentIn theory. On the other hand, dark matter is pretty much just a name we give to a discrepancy in current theory. reply 4ad 15 hours agorootparentNo, that is dark energy. From the pov. of gravity dark matter is simply regular matter that we can't see. reply thfuran 15 hours agorootparentI don't think that's really an accurate description. There's a discrepancy between observations relating to gravitation and general relativity's predictions. The discrepancy could be accounted for by significant extra mass, but no non-gravitational observations seem to confirm the presence of that extra mass. So \"dark matter\" is the supposition that there's a significant amount of extra mass that interacts only with gravity but not, for example, light, making it categorically different from ordinary matter. And I don't think there's any evidence that this dark matter follows the same gravitational constant as ordinary matter. reply adrian_b 13 hours agorootparentIt does not matter whether it \"follows the same gravitational constant as ordinary matter\" or not. For any kind of matter, normal or \"dark\", which is observed only through gravitational effects, you cannot determine separately its mass and the gravitational constant that applies to it. You can determine only the product between mass and gravitational constant (which is the cause of measurable forces). Therefore for many astronomical objects the product between their mass and the gravitational constant is known with a much greater precision than their mass (because the gravitational constant is known with very poor precision even for ordinary matter). The same applies for \"dark matter\". You cannot compute the distribution in space of the mass of the dark matter, but only the distribution in space of the product between its mass and whatever gravitational constant is applicable to it. So even if a different gravitational constant were applicable to \"dark matter\" that fact would be irrelevant for any mathematical model that is fitted to the observations. reply thfuran 11 hours agorootparentNevertheless, mass is a separate quantity, which means that the original claim that orbital speed is determined solely by mass and orbital radius is not supportible in the context of dark matter. reply exe34 15 hours agorootparentprevNot quite. Dark matter is the hypothesis that the discrepancy between theory and observation is due to a form of matter that interacts gravitationally but not electromagnetically. So we can't see it, and thus \"dark\". There are other competing ideas including a family of modified Newtonian dynamics models, but nothing comes as close as explaining the observations as dark matter does. There was a paper recently that showed that the discrepancy may be the higher order terms from general relativity that is often neglected because they are believed to be small - but that idea still needs to be proven to work for a large variety of cases. The observations in question for this Dark matter hypothesis include the rotation velocity of stars in galaxies and a few other things like gravitational lensing. Dark energy is a different discrepancy with theory. It's a term that we have to add to Einstein's field equations to account for the observation that the universal expansion is accelerating instead of slowing down. Again there are competing hypotheses, like non uniform density on the largest scales, but nothing quite explains everything as dark energy. reply api 11 hours agoparentprevThat’s just how often the local dark forest reaper aliens fire a relativistic velocity impactor at every biosphere they can detect within range. Keep resetting to make sure nothing too complex evolves. How much longer do we have to bug out before the next one? reply CapitalistCartr 11 hours agorootparentApparently the last one was about 14 million years ago. https://en.m.wikipedia.org/wiki/Middle_Miocene_disruption reply pests 5 hours agorootparentprevJust hide behind a large planet and hope they can't distort 3D into 2D. reply ilove_banh_mi 16 hours agoprevHaving read the Nature article [0] it looks like we're near a local maximum of that cycle [1]. [0] https://www.nature.com/articles/s41467-024-46171-5 [1] https://www.nature.com/articles/s41467-024-46171-5/figures/2 reply frame_ranger 4 hours agoparentMayan Doomsday Calendar, now with 200% more Science! reply dakom 13 hours agoprevAsimov and Silverberg's Nightfall novel seems related- a great sci-fi story about a civilization on an alien planet that isn't prepared for the long cycle of astronomical movements (they go crazy during an eclipse every 2000 years or so). Fun read :) reply KineticLensman 13 hours agoparentAsimov and Silverberg confused me for a moment - Nightfall was an Asimov-only short story from 1941. Still think it's one of his best. TIL there was a collaborative novel as well, which carried on after the events of the short story. * * * If you like novels that explore civilisations that have long astronomical cycles, the other classic is the Helliconia trilogy by Brian Aldiss. It's set in a double star system where the main planetary orbit takes 2500 Earth years, and the seasons last for Earth centuries. Civilisation tends to collapse when winter comes, but gradually approaches semi-industrial levels of technology by autumn. reply a_gnostic 11 hours agoprevMilankovitch cycles describe the collective effects of changes in the Earth's movements on its climate over thousands of years. The term was coined and named after the Serbian geophysicist and astronomer Milutin Milanković. In the 1920s, he hypothesized that variations in eccentricity, axial tilt, and precession combined to result in cyclical variations in the intra-annual and latitudinal distribution of solar radiation at the Earth's surface, and that this orbital forcing strongly influenced the Earth's climatic patterns.[1] 1] https://en.m.wikipedia.org/wiki/Milankovitch_cycles reply davidw 6 hours agoprevDisappointed to not see any Pern references. \"Get off my lawn\", I guess? reply AndrewKemendo 17 hours agoprevThe paper for this was published a few days ago here to no response: https://news.ycombinator.com/item?id=39729053 Admittedly I struggled on my first read through, but it’s one of the bigger discoveries in climate research in a while It might give us some answers on why the Younger-Dryas happened argubly leading to the Quaternary Extinction event, as there’s some dissent around the causes If it was a concurrent epicycle that stacked up that could be a pretty compelling argument https://en.m.wikipedia.org/wiki/Younger_Dryas reply yieldcrv 5 hours agoprevThis undermines my understanding of gravity, gravity is really fickle then as even this article says Mars is too small for the results I think the article exaggerates the correlation as causation until we can absolutely model this repeatedly, Martian presence is not the only answer as the article suggests reply vincengomes 15 hours agoprevMan, this study is going to be paraded as the proof that astrology is scientific and ancient people were more knowledgeable than us. reply readyplayernull 11 hours agoparentPeople is born in seasons, constellations and planets appear in seasons, and so ancient smart people made a connection. Then the largest actor interested in destroying Astrology is the Catholic Church for reasons. The church also had wrong astronomical theories. Science evolved, but we inherited a dead horse to beat. reply somenameforme 5 hours agorootparentThe reasons the Catholic Church did away with astrology can be explained in a one liner. Catholicism assumes a belief in free will, and the claimed predictive power of astrology contradicted this, and so was deemed blasphemous. reply icepat 14 hours agoparentprevYou are being downvoted, but it is completely true. This is going to be used in the gurusphere in the way Deepak Chopra abuses the language of quantum physics to justify quackery. While things like this are always interesting, part of me can't help be be depressed knowing it's going to go into the firehose of nonsense we're being sprayed by every day. It's happening in this very thread in a few places already... reply mistermann 3 hours agorootparentMocking the thinking of others while literally engaging in soothsaying is rather ironic don't you think? And yes, of course, I'm well aware you are able to cherry pick some silly quotes from Deepak to \"prove\" your point, and that I \"should\" \"know what you meant\" (let's ignore whether even you did, at the time you wrote the comment), but the never ending Motte and Bailey from you people is exhausting. Please try to broaden the scope of your knowledge, it may naturally reduce levels of hubris. reply pfannkuchen 12 hours agoparentprevAstrology is misunderstood today IMO. Cycles exist on Earth. Cycles exist in astral bodies. If an astral cycle aligns with some earth cycle, you can legitimately use the astral cycle to track the earth cycle. There is nothing wrong with doing this, it’s a useful tool. The problem came when people confused correlation with causation. There’s also some spurious correlations used as well as some scale extrapolation issues. But what was probably the root mechanism is sound. That is why ancients seem so weirdly obsessed with the stars, it actually works in some cases (non causally of course). reply mistermann 3 hours agorootparentDoes human belief play any role in causality? Let's say for example that Bill incorrectly believes his wife and his friend Steve are having an affair and he kills them both in a fit of rage - did Bill's belief play any role in the causality underlying the death of the two humans? reply bamboozled 12 hours agoparentprevancient people were more knowledgeable than us. Almost sounds like a phobia. I do think ancient people seem to have displayed more wisdom than us. We’re the brainiacs. reply aaomidi 11 hours agorootparentDefinitely more wise, even if accidental. More knowledgeable? No reply bamboozled 1 hour agorootparentWhich one is more important ? :) reply 1letterunixname 7 hours agoparentprevEvery conspiracy theory and whacky magical thinking cult boils down to \"See? I'm smarter than NURDS and GUBBERMENT!\" reply somenameforme 3 hours agorootparentConspiracy theories don't come from thinking one is smarter, but rather from a lack of trust. When somebody (or some entity) says something, how you respond to them is not only based on your perception of their knowledge, but also on your perception of the trustworthiness. It's somewhat of a tautology to say that as decline in trust of US institutions declines, lack of believing in what these institutions says also declines. And trust in US institutions is not just randomly declining either. We just seem to have largely removed the social mores on lying and manipulation, so long as it can be used to push an agenda. If the powers that be want to reclaim public trust then there needs to be a much greater effort to increase transparency, honesty, hold open debate on all topics, and also hold groups accountable for misleading or lying to individuals. Instead we seem to be going rapidly in the exact opposite direction, and it's not difficult to predict the outcome. reply mistermann 3 hours agorootparentprevWhat methodology could one even use to know such a thing? What methodology did you use to come to know this? reply ghthor 14 hours agoparentprevThe ancients were more knowledgeable then us. https://youtu.be/J6OsDczx5iM?si=2BcthvU-6X_svTmw reply ghthor 11 hours agorootparentI’m sure the Down voters watched the entire lecture I linked before casting their votes. /s reply matkoniecz 10 hours agorootparentPosting youtube links without explanation next to blatantly false claims is not useful. Text can be at least quickly scanned through. reply narush 18 hours agoprev [25 more] [flagged] HumblyTossed 16 hours agoparent> But the more general idea that the arrangement of the solar system when you were born affects your personality (among other things) -- this is obviously reasonable. It is not, in any way, reasonable. reply lukan 13 hours agorootparentWell, the gravity alone definitely affects us, so also our personality. But whether it affects us in any meaningful way, is doubtful, since the force is really, really small. What definitely has an impact is, in what season you were born(if there are seasons). Whether your first steps outside are in snow, or in warm sun. That has an effect on development. Still, I always loved Astronomy and could never understand the appeal of Astrology in the first place. It never made the slightest sense. So if I am born on day X, I am supposed to have trait A, but if I would be born some hours later I should be personality Y? (besides the fact, that the baby was alive and conscious before the birth). The reason why people take it serious, is probably to find guidance in a chaotic world and they seem to think some guidance is better than none (but I rather have no guiding, than one that makes no sense to me in the first place). reply icepat 5 hours agorootparent> Well, the gravity alone definitely affects us, so also our personality. You're talking absolute micro-units of G force difference. Within a margin of error of most sensors. Orders of magnitude lower than we can detect as humans physically. And even if we were not, how is there an obvious correlation between these two? This is the exact sort of pseudo-scientific nonsense that's being spewed everywhere these days. reply lukan 3 hours agorootparent\"This is the exact sort of pseudo-scientific nonsense that's being spewed everywhere these days.\" Instead of insulting, you could have also quoted my complete statement: \"But whether it affects us in any meaningful way, is doubtful, since the force is really, really small.\" \"And even if we were not, how is there an obvious correlation between these two?\" Have you ever heard of the butterfly effect? A small change here, can lead to a big outcome there. Change a brain cell here and a neuron there and who knows what happens in the long run. But like I said, the forces in this case are really, really small. reply pests 5 hours agorootparentprevThe arrangement of the solar system -> birth month -> time of the year your mother was gestating you -> different conditions for different people during key growth phases (summer abundance vs harsher winters) Only way I could see it. reply samatman 13 hours agorootparentprevReasonable is not a synonym for true. reply LegibleCrimson 17 hours agoparentprevThis is a 2.4 million year cycle, not something that can lend any credence to the idea that the arrangement of the planets at birth has any effect on a person. The arrangement of your house or neighborhood has a stronger gravitational effect on you at a young age than the planets. Feng shui would be a better predictor of people's personalities than astrology. reply explaininjs 16 hours agorootparentAstronomical signs (in the west) are entirely related to the time of year when you were born and not at all related to the \"arrangement of the planets\", besides of course the position of the earth relative to the sun. The idea that some voodoo around the position of your bed relative to your chair relative to your door has more impact on your life than your spawning phase shift in the (second?) most significant cycle of our existence is pure absurdity. As a simple test: ask N self-proclaimed astrology experts to guess your sign, and perhaps the signs of some others around you. Run a Chi-squared on the results. Come to your own conclusions. Alternatively, plot some user data against birth month. Observe dependence. reply Tijdreiziger 15 hours agorootparent> the position of your bed relative to your chair relative to your door IIUC, the idea behind Feng Shui (traditional Chinese interior design) is that if your furniture is arranged in a logical way, it has a positive mental impact and is therefore conducive to allowing you to thrive. On the other hand, if your furniture is incoherently arranged, it can lead to frustration and clutter, which have a negative mental impact. I don’t think there are any studies on this, but at least intuitively, it seems to make sense to me. Example: if you put your desk directly in front of your door, it’s in the way and therefore will be an (unconscious) source of frustration every time you enter the room (because you have to walk around it all the time). In Feng Shui this is described as the desk blocking the ‘energy’ of the door; I find it useful to think of ‘energy’ as a synonym for ‘traffic flow’ in this context. If you want to know more about this, check out the excellent YouTube/TikTok channel ‘Dear Modern’, especially his short-form videos. (no affiliation) reply Ekaros 14 hours agorootparentAlso, consider which direction windows of various rooms you have face. Rooms are used differently during the day so for example morning sun in the bed room might be helpful if you need to wake up when sun rises. Also before modern houses things like air circulation and so on could have real effect of livability. reply openasocket 13 hours agorootparentprev> IIUC, the idea behind Feng Shui (traditional Chinese interior design) is that if your furniture is arranged in a logical way, it has a positive mental impact and is therefore conducive to allowing you to thrive. On the other hand, if your furniture is incoherently arranged, it can lead to frustration and clutter, which have a negative mental impact. That’s an incomplete description. Traditional Feng Shui says if your furniture is incoherently arranged, it disrupts the flow of Qi, and causes spirits to grant you ill fortune. Which sounds less plausible reply Tijdreiziger 12 hours agorootparentSure, but you can change one description into the other, while keeping the practical application the same. I really recommend watching the following video, which introduces Feng Shui principles in a non-superstitious way: “Feng Shui does make sense! The basis of how to plan your home for comfort and practicality” (runtime 6 min 34 sec) — https://youtube.com/watch?v=YsBPqO3pv_Y reply LegibleCrimson 16 hours agorootparentprevYou could also provide some evidence instead of telling me to gather my own. I'd happily read some more double blind studies on the effectiveness of astrology, most of what I've read points to it all being complete bunk. reply explaininjs 4 hours agorootparentScience is observation, not consumption. I ran the experiments, I got the data I needed to inform my opinion. You're more than welcome to do the same. But if all you will listen to is what scientific journals want to publish, you've already made up your mind, and there's little use pretending otherwise. If you ask me, nothing could be more \"complete bunk\" than some \"scientist\" claiming they've somehow blinded someone to their own birthday. reply mistermann 3 hours agorootparentprevFair criticism, but then look how many Scientific Materialist fundamentalists in this and other threads would have you believe they are omniscient Oracles, but then cry foul if one dares to critique or have a bit of a laugh at their hallucinations. The only saving grace of this simulation that I can see is how absolutely hilarious it is, it's like living in one of those old British sitcoms. reply lm28469 15 hours agoparentprevIt's like saying the entire biblical story is true because it has a few accurate events and a few real people in it... reply michaelmrose 13 hours agorootparentIt's not even clear to what degree many of the people exist! One thing that really blew my mind is that there is no reason to believe anything even resembling the events of exodus exist for example. That is to say not just the personage of Moses. There is no reason to believe the jewish people were ever in bondage in Egypt at all for instance. Reading from that huge chunks of the old testament cease to make sense in the context of the real history of the region. It's not even absolutely clear that Jesus was a historical personage. reply wizzwizz4 11 hours agorootparent> It's not even absolutely clear that Jesus was a historical personage. It is pretty certain that Jesus was historically a personage, since we have historical evidence that people had a text (often called The Holy Bible, or some variation) with a character in it called Jesus. There's also lots of evidence that Jesus was a historical person. See https://en.wikipedia.org/wiki/Historicity_of_Jesus for an introduction. reply imbnwa 9 hours agorootparentWe at least know that the Apostles were real people since Paul’s authentic letters depict him having to haul a sack of collections money to Peter and the extant living Apostles, nevermind his arguments with them over whether Gentiles should be circumcised, a debate which would yield the universality of Christian belief rather than being an extended Hebrew sect. reply michaelmrose 10 hours agorootparentprevI would suggest not using wikipedia as a source especially one full of insufficient arguments as that one. > Only two accepted facts of a historical Jesus Main article: Historical Jesus Part of the ancient Madaba Map showing two possible baptism locations Bronzino's depiction of the Crucifixion with three nails, no ropes, and a hypopodium standing support, c. 1545 > There is no scholarly consensus concerning most elements of Jesus's life as described in the Christian and non-Christian sources, and the only two events of this historical Jesus subject to \"almost universal assent\" are that Jesus was baptized by John the Baptist and was crucified by order of the Roman Prefect Pontius Pilate (who officiated 26–36 AD).[14][4][5][6][7][note 5] The criterion of embarrassment has been used to argue for the historicity of the baptism of Jesus, shown here in The Baptism of Christ by Juan Fernández Navarrete. > Based on the criterion of embarrassment, scholars argue that the early Christian Church would not have invented the painful death of their leader.[15] The criterion of embarrassment is also used to argue in favor of the historicity of the baptism of Jesus,[16][17][18] given that John baptised for the remission of sins, although Jesus was viewed as without sin and this positioned John above Jesus Neither is even a kind of good argument. In place of actual evidence we are making impossible to disprove psychological arguments to the motivation of believers. There is no contemporary as in at the time evidence of Jesus or reason to believe that the parties that wrote down accounts long after his death were actually recordings of first person accounts. If the bible can make up exodus and fabricate Noah I have no reason to believe fabricating Jesus is beyond the pale. Here is what I think a better write up looks like it is admittedly biased https://www.atheists.org/activism/resources/did-jesus-exist/ Another reasonable challenge to the use of something like the Bible is that normally when a work contains thousands of clear fabrications and myths it tends to diminish its worth as a source. For instance if I wrote a biography of wizzwizz4 and in the course of my writings claimed you were a godlike being from pluto who came to earth in the middle ages it would not only be dubious it would cast doubt on myself as a credible source. Even true things I wrote which were not corroborated elsewhere would be reasonably deemed dubious. reply airstrike 15 hours agoparentprevhow did you go from Mars possibly having an effect on oceans to arbitrarily defined constellations having a predictive effect on people's personalities? reply u32480932048 15 hours agoparentprevWe accept that the moon affects bodies of water We accept that our bodies are mostly water But the idea that the position of the moon or planets affects us is insane (There have been several small-N studies looking at various folk beliefs about higher incidences of certain events during full moons, but I don't believe any of the studies found a significant correlation) reply dotancohen 15 hours agorootparentThe moon does not affect bodies of water. The moon's gravity pulls every mass equally. The water being closest to the moon is pulled more strongly than the earth whose center is 6300 KM away from that water - so the water creeps closer to the moon than does the center of the earth. Same thing on the other side - the water is 6300 KM further away from the moon than is the (center of) the earth. So it is pulled less strongly, and thus to an observer on the earth it looks like that water is being pulled away. reply 1270018080 14 hours agoparentprev [–] > But the more general idea that the arrangement of the solar system when you were born affects your personality (among other things) -- this is obviously reasonable. Not even remotely reasonable in any capacity. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Mars' gravitational interactions with Earth have a cyclical impact on the planet's climate and ocean circulation every 2.4 million years, warming the climate and enhancing deep ocean circulation like a butterfly effect.",
      "Studying these cycles can offer insights into how climate change influences ocean circulation, though not directly related to current global warming from greenhouse gases.",
      "Some scientists are doubtful about the connection to Mars and the speculative influence on ocean circulation, despite the potential for bolstering ocean circulation in case of AMOC collapse."
    ],
    "commentSummary": [
      "Mars has an impact on Earth's oceans and climate every 2.4 million years, sparking debates about mass extinctions and theories related to dark matter.",
      "The discussion covers diverse topics including satellite oscillation, dark matter, astrology history, institutional trust, Feng Shui, and Jesus' historical context.",
      "They also delve into astrology's validity in character assessment and how the moon's gravitational force affects Earth."
    ],
    "points": 156,
    "commentCount": 76,
    "retryCount": 0,
    "time": 1710683063
  },
  {
    "id": 39735669,
    "title": "Streamline CSS Optimization with SatCSS",
    "originLink": "https://github.com/matthewhague/sat-css-tool",
    "originBody": "SatCSS Minimise CSS files through semantics-preserving refactoring. E.g. .a { color: red } .b { color: red } can be refactored .a, .b { color: red } but .a { color: red } .c { color: blue } .b { color: red } cannot, since .a, .b { color: red } .c { color: blue } changes the color of an element with class=\"b c\". Can also be used to test whether two selectors may match the same node in some DOM. Can also be used as a tool/library for building an abstract representation of a CSS file as a set of pairs (selector, declaration) with an ordering (representing the order selectors must appear in the CSS file to maintain the overriding semantics). Requirements: Python 3.7 or compatible. cssselect 1.2.0 docopt 0.6.2 lxml 4.9.2 tinycss2 1.2.1 toposort 1.10 z3-solver 4.12.2.0 Borrowed and modified code from cssselect 0.9.1 The recommended build system is Poetry. Tested with version 1.7.0. External Requirements In the same directory as satcss/main.py, ensure that running ./z3 runs the Z3 SMT solver. For example, this can be a symlink to the Z3 installed on your machine. The tool was last tested with Z3 v4.12.2.0. Running with Poetry To setup and run the project with Poetry, from the root directory run poetry install Then poetry run satcss --help To try a benchmark: poetry run satcss benchmarks/dblp-2015-07-09-stripmq.css To output the file: poetry run satcss -o --file=blah.min.css benchmarks/dblp-2015-07-09-stripmq.css Running without Poetry Two scripts main.py and test.py are provided in the root directory for running without Poetry. First install the requirements manually. You can use requirements.txt. pip install -r requirements.txt Then run python main.py --help where \"python\" is your python 3.7 or above command. TOPLAS Version The version current as of the TOPLAS paper is tagged TOPLAS-Release.",
    "commentLink": "https://news.ycombinator.com/item?id=39735669",
    "commentBody": "Optimize CSS with SAT Solving (github.com/matthewhague)144 points by keepamovin 16 hours agohidepastfavorite36 comments keepamovin 16 hours agoI lazily didn't want to refactor my CSS myself, so after a failed attempt using ChatGPT for this, I found this CSS sat solver tool. Just a note on macOS I managed to get this work by: - Editing the pyproject.toml and changing - lxml version to 5.1.0 - python to ^3.8 - then running poetry install - then, also in the repo, running cd satcss && ln -s $(which z3) z3 - then poetry run satcss --help I may have skipped a step or 2 but that was basically it. I then used satcss' output with this tool https://css.github.io/csso/csso.html to beautify and further organize it reply b33j0r 14 hours agoparentI was similarly surprised that gpt4 can’t do this. I have nearly started over on full-stack applications because my css got out of hand, haha. This is pretty much exactly what I was looking for! reply nyrikki 9 hours agorootparentTransformers are highly parallelized, and generative AI appends. Even if you ignore that graph problems typically have more time complexity than LLMs offer during inference, they are circuit like and not turing machine like. Without getting lucky with pre-training matches, it would be surprising if they were good at it at all. reply micromacrofoot 11 hours agorootparentprevsame, it’s funny how bad chatgpt is at css… in my experience it can’t even do simple css refactors reply 698969 8 hours agorootparentThere are a lot of bad examples on the internet, \"try this, it worked for me\" but only because of some other rule up the cascade that the person answering isn't aware of. reply matsemann 13 hours agoparentprevYour comment highlights why distributing anything that's python is unfortunately nearly impossible. reply JimDabell 2 hours agorootparentThis is not even remotely normal for Python tools. The problem arises here because it’s using libxml2, a C library. reply keepamovin 8 hours agorootparentprevHa! :) Yeah, python installs / distribution it’s pretty much a dumpster fire it seems. But I’m not an expert at that. It’s kind of funny or ironic that given how easy to use python is supposed to be and how important it now it is with AI etc.. I don’t know … maybe these pythonistas like the arcane type of secret initiate knowledge required in order to participate? reply hirako2000 22 minutes agorootparentContainers. reply ToJans 1 hour agoprevSlightly OT: anyone can recommend a tool that turns the tailwind class mess into a properly structured CSS file (Grouping common utility classes etc). I think I'd pay for this if it works properly... reply ToJans 31 minutes agoparentJust an example of code that drives me nuts:reply hirako2000 24 minutes agoparentprevafaik Tailwind determine classes to produce at runtime. You are likely out of luck there, who would want to turn this into a compile time approach, would rather write a new css library. reply the_other 12 hours agoprevIt’s not clear to me from the readme why this is useful. .a and .b identify different groups of elements. The fact they’re both coloured red is incidental, circumstantial and likely temporary. Refactoring to save a couple of lines makes the code less deleteable and thus harder to maintain. reply andful 12 hours agoparentI would treat this tool similarly to a minifier. It reduces size and bandwidth at the cost of legibility. It should be a stage in the deployment pipeline, but not a way to format your own code. reply keepamovin 8 hours agoparentprevYeah, that’s fair enough. I suppose if you want to keep all the selectors fully ‘denormalized’ and independent, at the expense of duplication, then you probably don’t wanna link them together by factoring out commonalities like this seems to do. This has benefits indeed. I guess what I was looking for was a different type of benefits, more achieved through normalizing the CSS. My CSS was just like accreted overtime and I just couldn’t be bothered to like get the overall picture myself, and re-organize — so I wanted to reduce some of the entropy and redundancy and increase organization, so it would be clearer. I think from another point of view having something that’s clearer and tighter is more maintainable. All depends on what you want I suppose. I was also hoping that I would be able to change a color in one place and it would update across many selectors. And that’s one of the benefits that a more normalized factored structure gives. Ann interesting dichotomy, your comment raises between fully denormalized and easy to change a single thing, and what I am trying to achieve in this case, and that’s more succinct and structured with related parts proximal. A more ‘normalized’ CSS haha! :) reply holoduke 11 hours agoparentprevAdd it to a bundler like parceljs. Minifies and optimizes for production. Our dev css file is 40kb. Optimized production version 10kb (example) reply ape4 12 hours agoprevWouldn't gzip will remove redundancy when it sends the compressed .css file. reply Dlanv 11 hours agoparentIt's still saving probably a single byte in this example .a { color: red } .b { color: red } Also it saves on parsing time I suppose. And less to zip and unzip. The type of thing I'd add to a projects deployment stage and never touch again unless it causes an issue. reply keepamovin 8 hours agoparentprevYeah, I mean this is not purely about reducing space at least from my point of view I wanted to better organize my CSS and I reduced duplication so that the overall structure would be clear to me and for what I wanted to do it would be easier to maintain in modify. A more ‘normalized’ (in the language of relational databases) CSS haha! :) reply afavour 11 hours agoparentprevStill takes time to parse and apply the styles though. CSS is probably a much smaller concern than JS but in the world of 5G networks and fast WiFi I feel like we’re nearing a world where device CPU can be a bigger bottleneck than bandwidth at times. reply foobarbecue 10 hours agoprevI've been looking for a CSS optimizer that will remove any styles that aren't actually used -- e.g. selectors matching ids and classes that don't exist in the html. Does anyone know of such a thing? Preferably easy to use from Jetbrains IDEs? reply iansinnott 7 hours agoparentCheck out: https://github.com/uncss/uncss I've only used it once but it did the job (NOTE: Plain HTML, plain CSS, no build pipeline. So YMMV) reply c_s_guy 5 hours agoparentprevAs a starting point, Tailwind used to use PurgeCSS [0] but I'm not sure what they use now. [0] https://purgecss.com reply gregod 9 hours agoparentprevhttps://github.com/GoogleChromeLabs/critters Might be a good starting point. It’s designed to inline the css afterward so it’s more focused on extracting used css than removing unused. reply ssttoo 10 hours agoprevI wish there was some more docs on what it does in addition to modern minifiers such as csso and lightningcss, both of which do refactor css when they deem safe. reply rolfvandekrol 3 hours agoparentThis is a repository that accompanies a research paper. https://dl.acm.org/doi/10.1145/3310337 reply spencerchubb 15 hours agoprevI would like a tool similar to this, but for refactoring in a human-readable way instead of minifying. Some tool to automatically look for duplicated css code. In theory, it seems easier for css than for actual programming languages given the simplicity reply keepamovin 8 hours agoparentThis sort of does that I think, but it more just dedupes it rather than just identifying the duplicates. But I agree that like some tooling to make CSS more human readable, and like to tame unwieldy enlarged CSS and subdue it to the useful and the good and the organized that would be a very useful thing, but I couldn’t find anything… besides this haha! :) Yeah, in a related anecdote it’s surprising how sparse the landscape of CSS tools seemed at least for command line tools that I was looking for. I didn’t want an ID plug-in because I didn’t really use anything except for vim and I don’t really use plug-ins for that. That might be kind of a niche set up but when I went looking for like even just something to sort the properties by some standard order within each rule, I couldn’t find anything current that worked in my searching anyway. reply hinkley 13 hours agoparentprevLast time I was leaving front end there were rumors of css coverage tools. That would certainly be a start. reply gbickford 12 hours agorootparentIt's a thing in Chrome Dev Tools now: https://developer.chrome.com/docs/devtools/coverage/ reply alserio 11 hours agorootparentnow, if only it could persist the data while navigating to other pages, it would be a lot more useful reply hinkley 10 hours agorootparentThis I remember now. So that still hasn’t been sorted out? Pity. reply ToValueFunfetti 14 hours agoparentprevCSS is Turing complete, although you could probably handle 99.9% of real world cases without dealing with that. reply niutech 14 hours agoprevIs there an online version? reply keepamovin 8 hours agoparentI don’t think so, but wouldn’t something like replit make this possible to run online? reply scholaronroad 15 hours agoprev [–] Very cool! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "SatCSS is a tool for optimizing CSS files while preserving their semantics through refactoring.",
      "It helps determine if two selectors could target the same DOM node and creates an abstract representation of a CSS file.",
      "The tool has certain requirements and can be executed using Poetry or manually without Poetry."
    ],
    "commentSummary": [
      "The author details their journey using a CSS SAT solver tool to enhance CSS code efficiently, following challenges faced with manual refactoring.",
      "Users exchange views on AI tools like ChatGPT for CSS restructuring, stress the importance of standardizing CSS format, and suggest alternative CSS optimization tools.",
      "The discussion extends to the complexities of distributing Python tools and the possibilities of web-based versions for CSS optimization tools."
    ],
    "points": 144,
    "commentCount": 36,
    "retryCount": 0,
    "time": 1710693525
  },
  {
    "id": 39738547,
    "title": "Mysterious Drone Swarms at Langley Heighten Security Concerns",
    "originLink": "https://www.twz.com/air/mysterious-drones-swarmed-langley-afb-for-weeks",
    "originBody": "Air → Unmanned → Swarms Mysterious Drones Swarmed Langley AFB For Weeks The unidentified drones were such an issue that assets were called in from around the government, including a NASA WB-57 high-altitude jet. BY JOSEPH TREVITHICK, TYLER ROGOWAYPUBLISHED MAR 15, 2024 7:29 PM EDT AIR NEWS & FEATURES SEA SPACE A pair of F-22 Raptor stealth fighters turn on final approach to Langley Air Force Base in Virginia. USAF SHARE JOSEPH TREVITHICK View Joseph Trevithick's Articles FranticGoat TYLER ROGOWAY View Tyler Rogoway's Articles Aviation_Intel Langley Air Force Base, located in one of the most strategic areas of the country, across the Chesapeake Bay from the sprawling Naval Station Norfolk and the open Atlantic, was at the epicenter of waves of mysterious drone incursions that occurred throughout December. The War Zone has been investigating these incidents and the response to them for months. We know that they were so troubling and persistent that they prompted bringing in advanced assets from around the U.S. government, including one of NASA's WB-57F high-flying research planes. Now the U.S. Air Force has confirmed to us that they did indeed occur and provided details on the timeframe and diversity of drones involved. This spate of bizarre drone incursions deeply underscores the still-growing threats that uncrewed aerial systems present on and off traditional battlefields, and to military and critical civilian infrastructure, issues The War Zone has been highlighting in great detail for years. \"The installation first observed UAS [uncrewed aerial systems] activities the evening of December 6 [2023] and experienced multiple incursions throughout the month of December. The number of UASs fluctuated and they ranged in size/configuration,\" a spokesperson for Langley Air Force Base told The War Zone in a statement earlier today. \"None of the incursions appeared to exhibit hostile intent but anything flying in our restricted airspace can pose a threat to flight safety. The FAA was made aware of the UAS incursions.\" \"To protect operational security, we do not discuss impacts to operations,\" the statement added. \"We don’t discuss our specific force protection measures but retain the right to protect the installation. Langley continues to monitor our air space and work with local law enforcement and other federal agencies to ensure the safety of base personnel, facilities, and assets.\" A satellite image of Langley Air Force Base. Google Earth Langley Air Force Base is formally part of Joint Base Langley-Eustis, an amalgamation that also includes the U.S. Army's Fort Eustis. Both facilities are situated around Newport News and Hampton Roads in southeastern Virginia. Langley, one of a select few bases hosting F-22 Raptor stealth fighters, is particularly important for supporting NORAD and NORTHCOM's missions to defend the U.S. homeland, including protecting the nation's capital in Washington, D.C. A pair of F-22s from Langley Air Force over Canada's British Colombia province on their way to a NORAD exercise in 2015. USAF The Newport News area is also home to critical U.S. Navy and commercial shipyard facilities. The aforementioned Naval Station Norfolk, the largest naval base on the East Coast and home to roughly half of America's carrier fleet, is located just to the southeast. A host of other military installations are also dotted throughout the broader area, including Naval Air Station Oceana, the Navy's master tactical jet base on the East Coast, and Dam Neck, Navy Special Warfare's East Coast headquarters. A shot taken in 2012 showing the long pier at Naval Base Norfolk packed with carriers and amphibious assault ships. USN A satellite image showing Naval Base Norfolk and Naval Air Station Norfolk, collectively known as Naval Station Norfolk. Google Earth Whether any other facilities in the area experienced drone incursions around the same time is unclear, although the mysterious drone incursions extended beyond Langley. The situation prompted a serious response and caused significant reverberations throughout the U.S. military. Also remember that it is the Air Force's job to protect American airspace, not the Navy's. Having a base tasked with that mission, and its airspace getting penetrated for weeks on end is clearly not the best look, especially after major lapses in domain awareness and air defense capabilities were recently spotlighted by the Chinese Spy Balloon saga. These are the same issues The War Zone had been warning of for years prior. At a hearing before the Senate Armed Services Committee yesterday, U.S. Air Force Gen. Gregory Guillot, currently head of U.S. Northern Command (NORTHCOM) and the U.S.-Canadian North American Defense Command (NORAD), made what appear to be the first public acknowledgment of the incidents at Langley. Guillot, who had previously been deputy commander of U.S. Central Command (CENTCOM), took the reins of NORTHCOM and NORAD in February. U.S. Air Force Gen. Gregory Guillot at the ceremony marking his assumption of the command of NORAD and NORTHCOM in February 2024. DOD \"Upon taking command, I began a 90-day assessment to inform the Department [of Defense], the Joint Force, and Congress on NORAD and NORTHCOM's ability to execute assigned tasks and make recommendations on where the commands could or should do more,\" Guillot said in his opening remarks. \"Once complete, I look forward to sharing my findings and updated vision on how NORAD and NORTHCOM will best execute the noble mission of homeland defense.\" \"As part of my 90-day assessment, ... to tell the truth, the counter-UAS [uncrewed aerial systems] mission has dominated that so far in the first month. Of course, I knew it was an issue coming from another combatant command [CENTCOM], where we faced that threat in a very different way because of the environment,\" the NORAD and NORTHCOM commander said toward the end of the hearing. \"But I wasn't prepared for the number of incursions that I see. [I've] gone into the events at Joint Base Langley-Eustis, and I'm using that as the centerpiece of my 90-day assessment.\" In response to queries from The War Zone, the shared public affairs office for NORAD and NORTHCOM said that the two commands were \"aware\" of the \"issues\" Gen. Guillot had referred to in his testimony, but directed us to contact the Office of the Secretary of Defense (OSD) and the U.S. Air Force for more information. As already noted, there were already clear indications, especially from plane spotters using online flight-tracking software, that something unusual was going on in the skies over and around Langley and the rest of the Newport News area last December. This included a flurry of aerial activity, including what looked to be Air Force F-16 fighters conducting combat air patrols supported by aerial refueling tankers, over Newport News and other parts of southeastern Virginia around December 12, 2023. Online flight tracking software then showed one of NASA's WB-57F aircraft, with the U.S. civil registration number N927NA, flying circular orbits with Langley at the center the following week. NASA's trio of WB-57Fs are specialized research aircraft that can be configured to carry a wide array of imaging and other sensors and equipment in their noses and other modular payload bays. In cooperation with NASA, the U.S. military has made use of these aircraft on various occasions in the past to support operational and test and evaluation-type missions. One of NASA's WB-57Fs, which carries the U.S. civil registration number N926NA. NASA One of NASA's WB-57Fs with a turret with multiple types of cameras installed on its nose. NASA NASA offers a public online calendar showing past, current, and future planned activities for all of its research aircraft. The calendar says that N927NA was tasked with \"Imagery Support\" on December 18 and 19 of last year, but provides no further details. The War Zone had reached out back in December to NORAD and NORTHCOM, as well as NASA, for more about the flight activity in southeastern Virginia. \"NORAD has been conducting its routine mission of aerospace warning and aerospace control. ... [we] can confirm we conducted flight activities over Virginia various times over the past couple weeks,\" a spokesperson from the NORAD/NORTHCOM public affairs office told The War Zone at the time. \"USNORTHCOM has been coordinating with [the] FBI, as is consistent with the combatant command’s coordinating role, but I have to refer you to [the] FBI on the nature of that law enforcement issue.\" The FBI declined to comment at that time. The Bureau did so again when The War Zone reached back out today. An F-22 Raptor stealth fighter based at Langley Air Force Base, at bottom, flies with a pair of Virginia Air National Guard F-16C fighters during a training mission. USAF “NASA's WB-57 aircraft fly missions across the United States in support of scientific research and remote imaging. They have flown at least 30 times in the past two months,\" a spokesperson for NASA's Langley Research Center in Virginia also told The War Zone in January in response to queries about N927NA's December flights. \"The WB-57 with civil registration number N927NA was on the East Coast in December to fly in support of NASA's SpaceX CRS-29 commercial resupply services mission and image the spacecraft upon its return to Earth. While in the area, the aircraft provided additional imaging support in Virginia.\" It is important to note that this is not the first time that Langley and other U.S. military bases across the country, including outlying U.S. territories, as well as critical civilian infrastructure, have been subjected to mysterious drone overflights. U.S. warships have also been swarmed off the coasts of the United States. U.S. military aircraft are also routinely encountering drones in various test and training ranges and other restricted military operating areas. America's nuclear power plants have had very troubling encounters with drone swarms. Yet the frequency and nature of the incursions in Virginia sound eerily similar to the bizarre claims of unidentified drone swarms roving over the plains of Colorado in the Winter of 2019-2020. The government response to those incidents was something of a meek sideshow compared to what clearly occurred regarding the Langley incidents — a sign of just how much more serious these incidents are being taken. All this is in addition to the strange encounters between U.S. Navy fighter aircraft and unidentified craft that have occurred persistently for years just off the coast of where Langley is located. Based on investigative reporting by The War Zone, some of these were directly identified as drones in pilot hazard reports obtained via FOIA. The Air Force has since identified the potential threats drones pose to its flightlines as a specific point of concern. But the ability, or lack thereof, of the U.S. military and other Federal, state, and local agencies to effectively respond to these potential threats is worrisome. This gap had long been left unfilled and the scramble to remediate it only began after simple, often homemade, armed drones began causing great damage on the battlefield in Mosul in 2016. The barrier to entry to weaponizing small commercial drones to make them even more dangerous or to build longer-range ones that can strike fixed targets autonomously is only shrinking with each passing day. Increasing levels of autonomy even for small drones will also change the severity of the threat they pose in the not-too-distant future. Though at least a portion of this activity often appears to be innocuous, there are real national security and general aviation safety concerns. Groups of drones, even small commercially available types without any kind of armed capability, could impede flight operations and conduct highly valuable surveillance. Just their presence effectively probes defenses and can provide critical information. Weaponizing these drones could cause immersive damage. While F-22s are the hardest fighters to confront in the sky, comparatively simple drones could destroy dozens of them as they sit idle on the ground, and do so over great distances. USAF The War Zone laid out this exact scenario in 2017: \"Similar swarming strikes could be unleashed behind the front lines as well, with hugely expensive and low density/high demand combat aircraft being especially vulnerable to this sort of tactic—something General James Holmes alluded to inadvertently while speaking to the Air Force Association, stating: “Imagine a world where somebody flies a couple hundred of those and flies one down the intake of my F-22s with just a small weapon on it.” Actually, it would be even easier to just strike the jets as they sit idle and vulnerable on the flightline. One swarm could see a whole squadron of tightly packed fighters destroyed without even having a chance to fight back.\" In his testimony before the Senate yesterday, NORAD and NORTHCOM head Gen. Guillot said bluntly that his 90-day assessment was heavily focused on where the two commands \"can and should do more as this emerging capability outstrips the operational framework that we have to address it.\" Much about the drone incursions that occurred at Langley Air Force Base last December remains officially undisclosed, but they put these broader issues and concerns about uncrewed aerial threats into sharp relief. Howard Altman contributed to this story. Contact the author: joe@twz.com AIR FORCES AMERICAS AROUND THE GLOBE DRONES MYSTERIES NASA NAVIES SWARMS U.S. AIR FORCE U.S. HOMELAND U.S. NAVY UNMANNED",
    "commentLink": "https://news.ycombinator.com/item?id=39738547",
    "commentBody": "Mysterious Drones Swarmed Langley AFB for Weeks (twz.com)141 points by peutetre 11 hours agohidepastfavorite81 comments mapmeld 8 hours agoWhen I hear that this military base on US soil, with dedicated security and surveillance, can't recover one drone, or get photos to trace who or what sent the drone, I think of the London airport shutdown from 2018. There is no super drone swarm. https://en.wikipedia.org/wiki/Gatwick_Airport_drone_incident reply cirgue 8 hours agoparentWhere did it mention that they were unable to get imagery of the drones or recover one? If the thesis is that this may be a foreign incursion, I very much doubt that they’d broadcast that they’d captured one. reply pdimitar 6 hours agorootparentBragging about it is a good PR and inspires confidence in your citizens -- even if you didn't actually capture anything. It's rather mystifying that they didn't try to present a strong face after this incident. reply eCa 2 hours agorootparentThat’s how you lose trust, if it later is revealed that they didn’t. reply squarefoot 2 hours agorootparentprevAny information given would reveal what they can do, and by extension also what they can't, so to me it's perfectly understandable if they keep everything under wraps. reply somenameforme 6 hours agorootparentprevThis doesn't make sense to me. The government is more than thrilled to stir negative sentiment against every geopolitical competitor we have via the media, if this is one of said competitors drones then they obviously would know if it had been captured, and if you're concerned about public 'panic' - you already released the information anyhow. You also have government planes doing loopity-loos around Langley with their public transponders turned on which shows a desire to make such actions publicly visible and available to literally anybody and everybody on the planet. reply wewxjfq 2 hours agorootparentSure, after the public circus with the spy balloon, the government is eager to have a round two, this time with off-the-shelf consumer drones. Be honest, a picture of a $1000 drone would leave you totally unfazed. reply instagib 3 hours agoparentprevLast I heard, dedicated security wasn’t even military anymore at Langley. It could have changed depending on fpcon levels or how many MP’s we have at home now instead of deployed. Surveillance is one thing, then cameras pointing up, and then having someone to monitor all of it is another entirely. tracking down the alleged drone before it leaves is yet another. Think cop response times. There’s zero reason they should release photos identifying the quality of their surveillance. Basic opsec. All they probably did is get proof, track them, get rid rid of them. No public knowledge necessary until it’s over. I’m enjoying the armchair radar enthusiasts. The more misinformation the better for adversaries. reply withinboredom 2 hours agorootparentThe military has equipments and people to trace a radio signal to its source. If they wanted to find the owner, they would have found the owner. reply actionfromafar 50 minutes agorootparentMaybe it didn't use a radio signal. reply boffinAudio 52 minutes agorootparentprevThe assumption that these things use radio signals is not supported by any evidence. reply actionfromafar 48 minutes agorootparentExactly! It could store and forward (later, after landing, with radio), it could use laser, some kind of very directional radio, it could dead drop a memory card somewhere before diving into a lake, there are so many posssibilities depending on what it is. It could also have been there just to test what the response would be, without recording or doing anything else but hanging around for a while. reply nimbius 7 hours agoparentprevagreed, this seems like a 15 page nothingburger of stock photos and conjecture. the likelyhood swarms of UAV's loitered in and out of the military airspace with no photos, no pursuit, or capture seems like a dubious claim. That no single civilian seems able to corroborate this also seems to suggest the events simply didnt happen. since these things might be dangerous to the general public you might release photos of them? or enlist help from the public/OSINT? sounds more like we're creating a story to help justify the use of expensive NASA resources for a budget review. reply thsksbd 7 hours agoparentprevIf they didn't enter the base or break any FAA rules, it would be illegal to interfere with the drone. reply LVB 4 hours agorootparentThey were flying in restricted airspace. reply ehnto 6 hours agorootparentprevI would be surprised if it was not controlled airspace, if not restricted airspace they would have had protocols to follow even just being near the base. reply someotherperson 5 hours agoprevYou can probably screw with a lot of places by strapping a solar powered 2.4ghz radio transmitter to a bunch of birds and letting them roam around. Actually it doesn't even need to be solar powered if it could be low-power enough, just enough battery for a couple of weeks of intermittent signal. reply perihelions 1 hour agoprevFor those unaware, a drone attack killed three American soldiers a few weeks ago [0]. The early findings are that the military base that was attacked never detected the drone at all—possibly because it flew at too low altitude for its radar. [0] https://www.washingtonpost.com/national-security/2024/02/06/... (\"Drone that killed U.S. troops in Jordan likely went undetected\") reply verdverm 32 minutes agoparentThe explanation I've heard was that the drone came around the same time they were expecting a friendly drone to return. They are shooting down drones all the time in the Middle East, it just doesn't make the news. Very similar for Israel iron dome. It only makes news when it doesn't work reply perihelions 23 minutes agorootparentThat was early speculation that's been contradicted by subsequent reporting, like the Post story I linked, - \"Taken together, the preliminary findings appear to undermine previous assertions that U.S. air defenses mistook the attacking aircraft for an American drone returning to the base about the same time\" reply johnohara 2 hours agoprevCan't speak to the issues of security or surveillance, but the aerial shot included in the article does reveal a moderately challenging golf course immediately to the north of the main runway. My personal surveillance suggests it might be more difficult to line up an approach shot on that par 4 (5?) dog leg right than it is to line up an approach vector to the main runway. Just sayin'. reply Cacti 1 hour agoparentofficers love to golf with other officers reply yinser 9 hours agoprevGlass half full thought: someone is red teaming US installations and we’ll be more secure for it ¯\\_(ツ)_/¯ reply whartung 9 hours agoparentYears ago, somewhere in Northern California, there was attack of (I think) power substation. Someone was shooting at parts of the station. As described it was a very well run, very knowledgeable operation. The shooting was quite precise. Far as I know it’s unsolved. And I, too, thought it was some kind of red team style thing to test who knows what. It was very direct action, seemed a bit much for an exercise. But it can also be thought of what the attack didn’t do, how far they didn’t press even with the evidence that they probably could have. reply dgacmu 8 hours agorootparentYeah, it was power substations. Probably this one? https://en.m.wikipedia.org/wiki/Metcalf_sniper_attack And there was a still unsolved one in North Carolina two years ago: https://en.m.wikipedia.org/wiki/Moore_County_substation_atta... Which killed at least one person. There were also a few others in the SeaTac area around the same time: https://www.justice.gov/usao-wdwa/pr/two-charged-attacks-fou... But they caught those, and the suspects said they had planned to use the power outage to commit a burglary. reply whartung 7 hours agorootparentYea, Metcalf. My understanding (if it’s the right one) was the NC attack struck a pair of stations that were particularly important to the network. That the loss of those two stations had a much higher impact in contrast to the loss of two arbitrary stations. Those were selected for a reason. I think this attack was used as an argument against having things like details of the electric grid being public knowledge. reply paulmd 5 hours agorootparentprevThere’s been a string of these attacks for a decade now. One of the various neonazi/Patriot movements (Idaho flavor iirc) espouses infrastructure attacks and disruption as being the kickoff for their RaHoWa, and they keep trying to pop off every couple years. There hasn’t been a ton of coverage about it because of a desire to avoid copycats and avoid publicizing or popularizing the ideology in general. But like, occupying that nature preserve didn’t come out of nowhere, just there’s not a lot of benefit for centrists/sympathizers in pushing this particular culture war, with how many Americans are touchy about guns etc. https://news.yahoo.com/unsealed-warrant-provides-closer-look... https://myfox8.com/news/investigations/power-grid-attack/neo... (afaik the Facebook lady idea has been essentially discarded, she seems to be a moron but not involved. And then there’s this other guy with ties to a patriot group who supposedly started and then aborted plans for a similar attack… or perhaps not aborted after all?) (also, the patriot/militia movement is heavily involved with RNC and Russian funding in general… Maria Butina was attached to the NRA after all etc. The leader of one of these groups (literally “The Base”) has a Russian wife and lived in Russia etc. Generally speaking these groups are one of the tools Russia uses to inflame local tensions - whatever is most disruptive etc.) https://www.spokesman.com/stories/2020/jan/23/report-leader-... reply onlypassingthru 8 hours agorootparentprevIt was the Metcalf substation.[0] I believe they found the rock cairn(s) used for the sniper rest on the hill across the highway.[1] [0]https://en.wikipedia.org/wiki/Metcalf_sniper_attack [1]https://www.google.com/maps/search/metcalf+substation/@37.21... reply tw04 5 hours agorootparentprevI’m far more concerned with the “not a missile” than the power stations. I don’t believe it was ever solved. https://www.cbsnews.com/news/mystery-missile-launch-seen-off... reply jncfhnb 8 hours agorootparentprevWhat do you mean by “well run” and “precise”? reply whartung 7 hours agorootparentWell, they cut communication lines in advance. They apparently used signaling to coordinate the attack. They knew exactly what they were shooting at, it wasn’t a bunch of yahoos blasting away at highway signs. It would be interesting to hear a discussion about the shot placement on the transformers. Were they grouped tight, target right where they need to be, etc. reply WatchDog 8 hours agoparentprevSounds very much like an exercise to me. reply Duanemclemore 8 hours agoparentprevThis has a distinct red / blue flavor to it. reply codezero 9 hours agoprevWhat I wonder is if a radar is watching drones, what is it missing? Could a drone swarm be used to draw attention away from another incursion or are radar systems able to handle slow and low things at the same time as higher and faster objects? reply dghlsakjg 8 hours agoparentThere’s search radars, which is the ones you see in movies that spin in a circle or arc and continuously scan the whole area with updates for all returns every second or so. These are also what all ATC towers have. Then there’s tracking radars which point at one thing and track it continuously. This is a simplification, and some radars can switch between track and scan. In other words, a scan radar can and does track many objects at once, but at lower update frequencies. The other thing to keep in mind is that every plane on that air force base has its own radar, in addition to multiple ground units and most can be interlinked for a more complete picture. reply icegreentea2 8 hours agoparentprevModern radar systems (note, not all of the radar systems currently being used even by the US military would be considered 'the most modern') should be able to handle a wide variety of targets at a variety of sizes, altitudes, ranges and speeds all at the same time. That being said, radars still fundamentally work by spewing radiation into specific regions of the sky - the more things that a radar needs to track, the more it needs to spread its power around. Aside from the hard hardware limitations of the radar, what is more likely to be overwhelmed is either the attention and focus of human operators, or whatever software is being used to support the human operator. reply greedo 8 hours agoparentprevNormally radars are configured to detect aircraft faster than a small drone. They just filter that out as noise. Now the military could obviously afford multiple radars to fill this gap, but they haven't focused enough attention on this. reply bushbaba 8 hours agorootparentA bird and a drone are pretty similar. Heck you could make a drone look and fly similar to a bird. Slow small object detection brings its own challenges. The realistic aspect though is a small drone by itself can’t do huge damage. And a swarm would surely be detected. reply greedo 8 hours agorootparentWell, considering FPV drones in Ukraine are destroying aircraft, tanks, IFVs etc, that could be an issue. Langley houses F-22s that are basically priceless. reply dghlsakjg 7 hours agorootparentA battlefield front line in Eastern Europe is a far cry from a USAF base surrounded by 3 letter agencies. I would bet that the people who invented drone warfare have some awareness of the risks. reply AYBABTME 6 hours agorootparentDrone warfare isn't invented though. Warfare tactics are ever evolving, the same as an MMA fighter can do whatever it takes to win. So it's not like there's an inventor of drone warfare, who wrote the rules and derived the consequences, and could predict the outcomes. Especially when warfare economics starts to go back to individuals/small entities having asymmetrical advantages over state actors. And when development in this space is incredibly rapid, accelerating and available to anyone with a laptop, a DIY CNC cutter and some off the shelf chemicals. Defending against these types of threats isn't something the traditional military orgs are equipped for. They are slow to move, react, plan. They have established doctrine to counteract a simulated opponent's doctrine. reply bevekspldnw 6 hours agorootparentprevOne would also bet that some discarded “allies” from the Cold War operating from a cave in Afghanistan couldn’t pull off a surprise attack on American soil, killing thousands. And yet they did. For the record, that attack did include an aircraft slamming into the pentagon itself. Bloated intel agencies aren’t run by gods, they’re run by bureaucrats. They have, and will, fail. reply tw04 5 hours agorootparent> One would also bet that some discarded “allies” from the Cold War operating from a cave in Afghanistan couldn’t pull off a surprise attack on American soil, killing thousands. And yet they did. One would have to have been pretty ignorant then. It’s not like 9/11 was an isolated event. The same target was hit 8 years prior. https://en.m.wikipedia.org/wiki/1993_World_Trade_Center_bomb... reply bevekspldnw 5 hours agorootparentTotally. Multiple agencies were aware of the plot in full or in part, and it was utterly down to mismanagement of national defense. The “inside job” has always really been distraction from the mundane truth it was a massive fuckup by the people in charge. So I’m very open to the possibility there are a lot of adversaries walking around the permitters of US bases with little drone kits having a blast. reply NegativeLatency 5 hours agorootparentprevAnd the very near miss of the White House or other nearby target https://en.wikipedia.org/wiki/United_Airlines_Flight_93 reply littlestymaar 6 hours agorootparentprevBecause the US definitively do not have prior experience of being hit by surprise from the air on their own soil… reply llm_trw 7 hours agorootparentprevSounds like the perfect place to drop some biological weapons. >Crazy that Covid 2025 was first detected in Langley isn't it? What's worse is that this won't take more than a garage bio lab to do. Makes me want to move to Antarctica. reply transcriptase 6 hours agorootparentI wouldn’t worry about theoretical garage bio labs. It’s hard enough doing microbiology, molecular genetics, and bioinformatics even with a real lab full of experienced scientists and technicians. The idea of one or two people doing anything threatening even with millions of dollars ignores the fact that nobody has the requisite expertise to not only perform all the experiments and analyses, but somehow keep the equipment running without service contracts. Good luck getting an Illumina or Agilent tech to your clandestine garage operation to set up and calibrate your gear. reply llm_trw 4 hours agorootparentYou sound exactly like the old sysadmins who were saying why mysql is a toy that you should never use for your website. Open source science is real and has a barrier to entry that's just your time. Anyone with a biology degree can set up a garage lab today that would have been state of the art in 2010 for under $20k. reply dhosek 7 hours agorootparentprevThere’s a good reason why birds and drones are so similar. https://birdsarentreal.com reply verdverm 28 minutes agoparentprev> Could a drone swarm be used to draw attention away from another incursion or are radar systems able to handle slow and low things at the same time as higher and faster objects? Ukraine has been running this playbook the last 6 months or so. Drones in one area or direction to distract, scalp/Taurus on a high value target elsewhere. Sometimes this plays out over days. Current air defense systems are insufficient for the new drone paradigms. They won't scale to meet the needs. We need lasers and microwave based systems. reply littlestymaar 6 hours agoparentprevIt's exactly what happen to the Moskva as far as we can tell: use a Bayraktar TB2 at max range to draw attention of the crew and sneak an anti-ship missile in the mean time (that's what the Ukrainians said they did, at least, there's always fog of war…) reply huytersd 5 hours agoprevThe US army really doesn’t use enough cheap, small drones. All the drones we use are large and expensive. reply dghlsakjg 5 hours agoparentNot at all. There are a number of small drones (field launchable by a single person) used for surveillance and delivering kinetic payloads. This is one of many that are publicly acknowledged https://en.m.wikipedia.org/wiki/AeroVironment_Switchblade The RQ-11 is a hand launchable drone used by the US (among others) with more than 19k airframes having been delivered. reply mcculley 3 hours agorootparentThat says unit cost is $52,914. Not cheap by most standards. Yes, cheap compared to military aircraft, but the interesting changes around doctrine are happening at cheap enough to be disposable. reply nabogh 3 hours agorootparentI think this might be considered disposable by military prices. Missiles, torpedoes etc are very expensive reply mcculley 2 hours agorootparentI am aware that missiles and torpedoes are very expensive. Disposable drones are changing doctrine in ways that more expensive missiles and torpedoes do not. reply tsimionescu 2 hours agorootparentprevIsn't that much, much cheaper than most missiles? reply mcculley 1 hour agorootparentYes. Disposable is different from “cheaper than most missiles”. reply akdor1154 5 minutes agorootparentSo you mean cheaper than the disposable missiles as well as the re-useable ones? rob74 2 hours agorootparentprev...and that's for the smaller version! reply lamontcg 5 hours agoparentprevCIA is probably teaching the Ukrainians to build cheap, small drones and presumably they're building prototypes in a lab somewhere. reply imtringued 2 hours agoparentprevI don't know why you would need \"cheap small\" drones when the US has the most advanced bombs on the planet. You don't even need planes to launch them anymore. A HIMARS on the ground is enough and you get 150 km range. https://en.m.wikipedia.org/wiki/Ground_Launched_Small_Diamet... reply geraldhh 1 hour agorootparentthe main benefit probably lies within a 'saturation attack' aka overwhelming defenses by numbers. cheap drones are a gamechanger reply huytersd 1 hour agorootparentprevI don’t know. Personel carried cheap disposable drone grenades sound very useful. reply ishtanbul 6 hours agoprevProbably just the aliens… reply iJohnDoe 9 hours agoprevProbably testing new reconnaissance capabilities to see how effective it is. reply slicktux 9 hours agoprevLevel the playing field. https://m.youtube.com/watch?v=KX0ji1sAXl8&pp=ygUNQW5kdXJpbCB... reply purpleidea 9 hours agoparentRemove tracking info when you post links please. reply someotherperson 5 hours agorootparentAlso on this, it looks like Brave's \"copy clean link\"[0] doesn't work on this particular URL. I wonder why. [0] https://support.brave.com/hc/en-us/articles/9982188779405-Wh... reply a1o 9 hours agorootparentprevThis is a fair request and nice thing to do to fellow users of this forum, people should not downvote this person. reply modzu 5 hours agoprevpics or it didn't happen reply purpleidea 9 hours agoprev [–] Wouldn't be surprised if this was being done by one of the U.S. defence contractors so that they can convince politicians to buy their anti-drone products. What's most disappointing about this article is it has no actual details. What size drones? How fast were they going? How many total? How many at a single time? Did they catch any? What models? Let's see a photo of one... and so on... reply dralley 8 hours agoparentThere's absolutely no need to stage an incident to convince anyone of the need for anti-drone products. The utilitarianism of drones employed en-masse, cheaply and without deep technical knowledge on both sides in Ukraine proves that point just fine. reply thegrim000 8 hours agoparentprevAh yes, a white collar CEO at a private US company just decided one day that his company was going to start committing felonies and sending swarms of drones to accost a US military installation. And the CEO ordered everyone involved to be somehow threatened into complete silence about the operation. That's totally something that would happen in the real world and not a deranged conspiracy theory. reply GolfPopper 6 hours agorootparent>a white collar CEO at a private US company just decided one day that his company was going to start committing felonies This part seems pretty normal, actually. reply dexwiz 6 hours agorootparentThere’s business fraud and then there’s attacking military installations. The two carry very different penalties. reply throwawaymaths 7 hours agoparentprevIf you had connections to be a defense contractor, you would just red team it with at least a relatively high level military official on the inside. reply signatoremo 8 hours agoparentprev> Wouldn't be surprised if this was being done by one of the U.S. defence contractors so that they can convince politicians to buy their anti-drone products. No way they could do that without the military’s knowledge. That can have serious consequence. Anti drone is a hot area that the US is very interested in. All a vendor needs to do is to ask nicely. > What's most disappointing about this article is it has no actual details. That’s because the US military was intentionally ambiguous about their anti-drone capabilities: \"To protect operational security, we do not discuss impacts to operations,\" the statement added. \"We don’t discuss our specific force protection measures but retain the right to protect the installation …” reply boffinAudio 50 minutes agoparentprevNothing in this article supports the idea that these are man-made drones. reply jkhanlar 9 hours agoparentprev [–] or using hedgefunds infinte ftd failure to deliver assets as collateral to deploy such an operation that is not as traceable/trackable. where's Jonathan Frakes? https://youtu.be/GM-e46xdcUo reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Mysterious drone swarms breached Langley Air Force Base over several weeks in December, triggering a robust government reaction, such as deploying sophisticated assets like a NASA WB-57 high-altitude jet.",
      "The intrusions spotlighted the escalating risk of unmanned aerial systems, emphasizing the vulnerabilities they present to both military and civilian infrastructures.",
      "The events at Langley emphasized the necessity for enhanced capabilities to counter drone threats, as the potential for weaponizing drones and executing large-scale assaults is progressively more attainable."
    ],
    "commentSummary": [
      "Mysterious drones have been spotted at Langley Air Force Base, leading to speculation about their source and intention.",
      "Discussions include concerns about infrastructure attacks by extremist groups, the efficacy of radar systems in drone detection, and the evolving dynamics in warfare economics.",
      "Comparisons are drawn between the cost and efficiency of inexpensive drones versus traditional missiles, and allegations have arisen regarding a US company potentially engaging in illegal activities to market anti-drone solutions."
    ],
    "points": 141,
    "commentCount": 81,
    "retryCount": 0,
    "time": 1710713618
  },
  {
    "id": 39734825,
    "title": "Reddit Introduces \"Free-Form Ads\" Before IPO",
    "originLink": "https://www.theregister.com/2024/03/16/reddit_promoted_posts/",
    "originBody": "Personal Tech 39 Ahead of IPO, Reddit blends advertising into user posts 39 As FTC starts asking questions about that mega-deal with Google to train AI Brandon Vigliarolo Sat 16 Mar 2024 // 08:27 UTC Ahead of its stock market debut, Reddit has said it's going to make it easier for advertisers to craft normal user posts and run them as ads on the social network. Called \"free-form ads,\" Reddit claimed in a statement this week that the \"all-new, completely unique ad format\" is designed to improve click-through rates and upvotes – by making adverts look as much like user posts as possible. To be clear, we understand these free-form ads will be labeled as promoted posts. What's really happening here is that advertisers can use the Reddit Ads Manager to create normal user posts and then pay to make them appear prominently on the site and in the app as promoted posts, aka adverts. Up until now, it's been a bit more of a fiddly affair for brands. As we understand it, they've had to book an ad that links out to an external site, such as a landing page selling some stuff, or an ad that links to a separate user post on Reddit, or a banner ad. Free-form ads instead look just like normal posts in people's feeds, with a promoted label, rather than a link off to somewhere else, and are crafted via the Ads Manager. There's no need to create a separate user post that an ad links to; the user post is the ad in free-form mode. \"Free-form ads are designed to look and feel similar to the type of content redditors share with each other, inviting maximum engagement from the community,\" Reddit explained, inviting companies to \"advertise like a Redditor.\" The social media monster pointed to companies already trialing free-form ads, which include Just Eat Takeaway, Kraft Heinz, and Leica, all of which found the format capable of \"driving upper funnel results.\" Reddit claimed free-form ads tested so far achieved a 28 percent increase in click-through rates, which the biz noted \"outperform[ed] all other ad types.\" When comments are enabled on free-form ads, there's an increase in community engagement, Reddit claimed, without indicating whether that increase was positive or not. Reddit said Philadelphia Cream Cheese, a brand owned by Kraft Heinz, had success with the format though it declined to go into specifics about that trial. \"The brand leveraged our free-form ad format to showcase the refrigerator staple's versatility, with product details and recipe inspiration,\" Reddit said. \"This effectively prompted the Reddit community to 'get a big list going and bond over' their shared love of cream cheese.\" Reddit claimed in its bumf about the new ad format that a Philadelphia post had \"a 91 percent upvote rate and a 42 percent higher click-through rate from the brand's previous Reddit benchmark,\" and that the post generated \"over a thousand comments in a single month.\" Thar be safe harbor: Reddit defeats third attempt to unmask digital pirates Reddit rolling out AI bouncer to halt harassment Elon Musk's Twitter moves were 'reaffirming' says Reddit boss amid API changes Reddit confirms BlackCat gang pinched some data Those numbers don't mean much if Reddit doesn't contextualize them with more information about Philadelphia's typical click-through rates, and Reddit again declined to share specifics. We were able to locate two posts from Philadelphia Cream Cheese accounts on Reddit that fit Reddit's description of free-form ads: One from u/PhillyCanada posted a year ago, and the other by u/PhillyCreamCheese posted six months ago, later confirmed by Reddit staff as the free-form ads. The PhillyCanada post seems to conform to Reddit's mentions in the blog post. It has 1.5K overall upvotes and 1.1K comments. The more recent one from PhillyCreamCheese, on the other hand, got 617 comments and zero points, though it's likely to us that score was fixed at zero. FTC wants info on Google AI training deal Reddit filed an amendment to its S-1 form with the SEC late Friday revealing it's been asked by the US Federal Trade Commission to share details about its deal with Google to provide user content for training LLMs. \"On March 14, 2024, we received a letter from the FTC advising us that [it] is conducting a non-public inquiry focused on our sale, licensing, or sharing of user-generated content with third parties to train AI models,\" Reddit said in addendum to the risk factors section of the IPO filing. Reddit said it wasn't surprised by the inquiry given the evolving regulatory nature of AI training on public data, that it doesn't believe it has done anything wrong, and that the FTC also asked for a meeting to learn more about its plans. Leica's test of free-form ads doesn't indicate a whole lot of success to us - it has just three points. All free-form adverts are supposed to show some kind of sponsored label, though that doesn't appear to be the case on the three posts included in this story. While Leica's shows it, neither Philadelphia post includes a tag indicating it's sponsored content. We understand that's because the Philadelphia posts are no longer boosted by ad spending, so are back to just being normal user posts. Reddit, which is planning to go public on March 21 having never turned a profit in nearly 20 years of existence, has been doing everything it can of late to drum up support from investors. Most notably, the outfit signed a $60 million deal with Google to ingest user content for the purposes of training AI models after making its Data API a paid service last year, ostensibly to prevent AI models from ingesting user data. Users haven't been thrilled with Reddit's behavior running up to its IPO, and whether ads-as-posts will help isn't clear, though we know a thing or two about how devoted readers feel about web adverts generally. ® Sponsored: There’s still room for FPGAs in the datacenter Share More about Advertising Reddit More like these × More about Advertising Reddit Narrower topics Digital advertising displays More about Share 39 COMMENTS More about Advertising Reddit More like these × More about Advertising Reddit Narrower topics Digital advertising displays TIP US OFF Send us news",
    "commentLink": "https://news.ycombinator.com/item?id=39734825",
    "commentBody": "Ahead of IPO, Reddit blends advertising into user posts (theregister.com)138 points by LinuxBender 19 hours agohidepastfavorite150 comments DevX101 18 hours agoTwitter does this. Ads are the exact same format as posts, with the exception of a subtle 'Ads' label in the upper right. Of course everyone reads from left to right, so you can't really skim your feed without having your mind hijacked by at least the ad headline. reply pityJuke 17 hours agoparentEven more insidious on Twitter: actual user posts (videos specifically) being promoted into your feed, with no ad label. However, the actual ad (which is not particularly well disclosed) is the pre-roll ad on the video - the user post is just the vehicle to do so. [0] In your feed, it is indistinguishable. Because I have Mr Beast blocked, I kept on getting posts from BANG! Showbiz, because the video selection available to Twitter isn't particularly high quality. [0]: An example: http://web.archive.org/web/20240302013005/https://twitter.co... reply jsheard 17 hours agoparentprev> Of course everyone reads from left to right Almost everyone - I wonder if they move the \"Ad\" disclaimer to the left side when the UI is set to something like Arabic or Hebrew. reply MarkSweep 17 hours agoparentprevSometimes they don't even both to include the \"Ad\" label next to the \"...\" menu on the tweet: https://www.awise.us/images/2024-twitter-ad.jpg reply agumonkey 17 hours agoparentprevGoogle also did this. One days ads stopped being pale yellow divs but very close to actual results. reply mullingitover 16 hours agorootparentThere's a great timeline[1] showing how Google transitioned from the 'don't be evil' company to what it is today. Page and Brin at Stanford[2] laid into the fundamental problem of mixed motives that advertising-funded search engines have (see \"Appendix A: Advertising and Mixed Motives\"). Reddit and twitter aren't search engines, but they're dealing with related issues: the incentive to deceive visitors of their sites in order juice ad effectiveness and boost profits. [1] https://searchengineland.com/search-ad-labeling-history-goog... [2] http://infolab.stanford.edu/pub/papers/google.pdf reply agumonkey 16 hours agorootparentare we watching boeingification of the web2 era ? reply fuzzfactor 16 hours agoparentprevTabloids like the National Enquirer were known for this but it was originated much earlier by far. With something like reddit when it starts taking place where it never was before, you're supposed to be convinced you are getting the same thing you have been getting all along, even though it's not always the case any more. Reminds me a bit of how it was before Whole Foods went public. They were a local Texas healthy market with upscale tendencies and some of the products were outstandingly well-sourced and completely world-class. Especially the minority of imported goods, or most of them wouldn't have even been there. One day some of those fancy European lines began to be replaced systematically with lesser alternatives and newly emerging house brands. They already had a few stores by then, not just the first two in Houston and Austin. The replacement products were not shabby, they were above average but nowhere near world-class most of the time, and this action was scattered throughout the store. Before too long I found out they were going to go public, that was an IPO I did make decent money on. Co-incidentally that was one I had to inform brokers about, not the other way around. OTOH, my Whole Foods shopping has gradually dwindled since before the 21st century, to almost nothing by now, an average of zero trips per month. The more subtle the deception, sometimes the more annoying it can be. reply zero0529 17 hours agoparentprevFun thing is when you block enough of these ad accounts you start to get some really freaky ads. reply spike021 17 hours agorootparentThis is me. I blocked so many that nowadays all I get are fake marijuana ads and sex toy ads. No I don't regularly smoke or otherwise imbibe weed and I definitely don't do stuff that would link me with a need for sex toys lol. And judging by the replies I see to those tweets it's happening to other users too. reply recursivecaveat 16 hours agorootparentSometimes I wonder if targeted ads are just a giant hoodwinking by big tech over retail businesses. I constantly get youtube ads in languages I don't speak, for tampons or bras (I'm a man), or of musicians in genres I don't listen to. These all seem like extreme table-stakes targeting that can often be inferred from the singular video I'm watching alone, not even including my watch history. reply wkat4242 12 hours agorootparentI always like seeing stuff like tampon and washing powder ads (I don't own a washing machine). Because it shows my tracking is blocked successfully. reply GordonS 15 hours agorootparentprevThey're now making it so you can't block the account that posted the ad, instead you can just say \"I don't like this ad\" - which still results in the same ad being shown regardless. I'm still only seeing this on some ads; not sure if it's a gradual rollout or A/B testing. Either way, I'm sure it won't be too long before it's all ads being served like this :( reply ac29 15 hours agorootparentprevAny reason you dont just use an ad blocker? I havent seen an ad on Twitter... ever? And yes, this includes mobile. reply rifty 16 hours agoparentprevText seems like a much more difficult medium to bring ads in such that it feels ‘right’ while still being noticed. YouTube is effectively analogous to burying the ad Indicator late in the reading cycle by not indicating to you which video is going to have an ad when you read the thumbnail. reply tomek_ycomb 16 hours agorootparentI don't think that is the same, because the ad is clearly not the video, as opposed to seeing the video while thinking it was the video forst reply rifty 15 hours agorootparentFrom my perspective since the ad cannot be skipped the ad is not separate from the video runtime experience behind the thumbnail - except in your reception of it. Also another notable indicator of the coupled relationship between ad and video, being demonetized hurts the videos exposure. reply saghm 16 hours agoparentprev> Of course everyone reads from left to right, so you can't really skim your feed without having your mind hijacked by at least the ad headline This is an interesting perspective I hadn't thought of. I wonder if would be possible to use custom CSS to move that label to the left instead of the right? It wouldn't be a replacement for blocking ads outright, but it also feels like it might be something that would be harder for them to outright prevent. reply Geeek 17 hours agoparentprevTwitter ads are so spammy that I right away know that its an ad nowadays. reply kouteiheika 17 hours agoparentprev...but you can block them with uBlock Origin: twitter.com##div[data-testid=\"cellInnerDiv\"]:has(span:has-text(/^Ad$/)) reply ajayyy 17 hours agorootparentuBlock Origin already blocks them by default at the request level, no need to hide the actual HTML element reply ranger_danger 17 hours agorootparentpreveasier to just not use twitter reply neverokay 17 hours agoprevIt’s funny how the exodus from Digg to Reddit occurred because Digg was going to do sponsored posts. Now Reddit is going public. The minority group of original users only really impact things when the market is small. Scale sucks in that way. reply bastawhiz 17 hours agoparent> the exodus from Digg to Reddit occurred because Well, it was one factor. Digg had a lot of problems. The big rewrite went disastrously. It was one giant site with no real focus and one big comments section: it devolved into unmoderatable chaos. Sponsored posts probably weren't the straw that broke the camel's back. reply wdr1 13 hours agorootparent09 F9 11 02 9D 74 E3 5B D8 41 56 C5 63 56 88 C0 reply hayst4ck 17 hours agoparentprevThe exodus started with a change to comment threading that made the comment section a place for reactions rather than a place for conversation/analysis. reply marcosdumay 16 hours agorootparentReddit did the equivalent to this at the time of its redesign. Years ago. Granted, they were a lot less loud about the change than Digg, but they didn't suffer anything near the same amount of hate. reply pgwhalen 16 hours agorootparentWhat are you referring to? I’ve been a redditor since I was a digg refugee, and I can’t remember any meaningful change to the commenting system since then. Maybe some different sorting mechanisms with different defaults, but that’s about it. reply marcosdumay 14 hours agorootparentI bet you noticed a change on the comments themselves. It took me a while to even understand why things changed. The appearance of shallower posts, like memes was severely boosted, while textual posts lost a lot of emphasis, to the point they became harder to interact with. The sorting of posts was changed so they would cycle faster, and people stopped having enough time to interact on the comments section. Deep threads got completely hidden... There was no change large enough to make people pay attention. But the end result was the same. reply piyh 17 hours agorootparentprevThe feed quality was also tanked overnight with no rollback plan or A/B testing reply mikepavone 18 hours agoprevI'm confused. Reddit has had ads that look like normal posts for ages. What's actually different about these new ones? reply saberience 18 hours agoparentThey used to be really obvious, now they are less obvious. reply hn_throwaway_99 17 hours agorootparentLike every other search engine, social media platform, etc. ever. Remember when ads on Google were highlighted with a big yellow block? Now they look exactly like regular search results, just with \"sponsored\" in small text at the top. And my guess is they went with \"sponsored\" instead of \"ad\" or some such because for some subset of the population it's probably not even clear that sponsored=paid advertisement. reply JumpCrisscross 13 hours agorootparent> Remember when ads on Google were highlighted with a big yellow block? https://kagi.com/signup reply unshavedyak 17 hours agorootparentprevThe infinite growth problem / enshittification of modern capitalism. We will endlessly try to squeeze more blood out of a rock until it crumbles entirely. It's funny in a way. We talk about how risk averse big companies are compared to smaller, more nimble startups. Yet those same big companies don't seem to be risk averse to making aggressive squeezing changes like these. reply pennomi 17 hours agorootparentIs there significant risk in slowly clawing away user experience? If it happens incrementally enough, users are unlikely to leave. reply phatskat 11 hours agoparentprevAccording to TFA it appears that the old ads used to only link externally, or to a post made by a real user. Now the ads are posts themselves that self-link to Reddit. reply 7ewis 18 hours agoprevRevanced on Android can patch the Reddit apk to remove those ads. https://revanced.app/ reply rglullis 16 hours agoparentJust stop using reddit. Use Lemmy which has a good number of FOSS clients already. If you miss the content from specific subreddits, there are instances that mirror the content. If you still really want to interact with people on Reddit, I am working on a bridge. reply huytersd 13 hours agorootparentNot nearly as good. Reddit is probably the only place I can find answers for a lot of obscure topics. reply rglullis 13 hours agorootparentNot yet, but once the bridges are set up, you will be able to find the content on the Fediverse (it's stored in the mirrors) and you'll be able to reach the people no matter where they are. reply huytersd 13 hours agorootparentThat’s still like a 10 year lag time. If the process to find those answers involves anything beyond typing in the query followed by “fediverse” or whatever on google, it’s not going to happen. reply rglullis 12 hours agorootparentThis is a separate issue. I'm not saying \"don't ever visit Reddit again\", I'm saying \"You should be able to follow and interact with the subreddits through Lemmy\". reply Loggias 16 hours agorootparentprevHow do you find these instances? I quit Reddit after being a user for 13 years once they killed the API for 3rd parties. There are some subreddits I’d like to browse still but I definitely won’t be joining back into that dumpster fire. I’ve been using Lemmy but it’s honestly aggravating to use in many ways. reply maxverse 13 hours agorootparentI stayed away from Reddit for probably nine months after that scandal, and pop back in occasionally now. It has gotten significantly worse. reply phatskat 8 hours agorootparentApollo was the only way I used Reddit - the browser experience is meh, and I’m almost always using Old Reddit when I go there on desktop or mobile because it’s just easier to get what I’m looking for. Once Old gets the axe I’m sure I’ll be there even less than I am now. It’s a bummer too - I think my weekly screen time report had me using Apollo on average like 6 hours a day, I was very active in a couple small and fun communities, and the places with news relevant to me were all subbed and easily surfaced. Losing my primary tool was all it took to take me from a pretty engaged and active user to someone who stops by because a search result took me there. reply rglullis 13 hours agorootparentprevI'm working on a project that is meant to provide a migration path for people like you: https://github.com/mushroomlabs/fediverser. The idea is that Lemmy instance admins can use this service alongside their Lemmy server to provide two things: - \"Login with Reddit\", to make it easier for people to create an account on the Lemmy server. - A reddit-to-lemmy community map. The idea is that when someone logs with Reddit OAuth, we can get their list of subreddits, and then auto-subscribe the user to the corresponding Lemmy communities, solving the onboarding issue and the content discovery in one go. There is also a website, https://fediverser.network, where I'm crowdsourcing the data to make the mapping between subreddits to lemmy communities. reply tkot 18 hours agoparentprevAre there any advantages to using the official Reddit app instead of something like RedReader? https://github.com/QuantumBadger/RedReader reply MostlyStable 17 hours agorootparentNsfw posts is a big one i think. In the opposite direction, RedReader doesn't seem to have the disguised ads reply maxverse 13 hours agorootparentprevI'm surprised this works, I thought Reddit effectively killed apps that rely on its API? reply AnonC 17 hours agoprevWith all the data that’s being put up for sale by Reddit to AI companies, why do they still focus on ads? They could focus on a better user experience (unlike the @*^% move last year to cut off almost all third party apps and strangle the rest to death slowly). The value of gathering more content from users in order to freely sell it to companies should be a lot higher than tweaking ad placement and styling. Seems like u/spez (Reddit’s CEO) ran out of decent and ok ideas a long time ago and has been acting out of spite time and again. reply brevitea 17 hours agoparentPerhaps you are seriously underestimating the actual dollars companies spend on advertising. reply SV_BubbleTime 17 hours agoparentprevI’m a firm believer that the MBAifcation of the world is one of the great evils of our time. That extracting every last option, selling everything possible, exploiting your environment, running as lean as possible with no concern for difficult times… is a lot of what is going wrong. So what I re-pose to you isn’t “Why isn’t Reddit selling just data or just ads” it’s “Why would their executives ever consider leaving money on the table?” reply marcosdumay 16 hours agorootparentIt's not only MBAification. The world's economy has suffered a global, organized, and incredibly large dumping attack against a huge set of industries and social organizations. The good news is that it seems to have stopped. reply dinkumthinkum 14 hours agorootparentprevWith competition users, there can be new sites and users can show these companies with their clicks. I get what you’re saying but consumers are apparently ok with it to an extent. It’s not an evil like labor camps and bread lines. :) reply add-sub-mul-div 17 hours agoparentprev> With all the data that’s being put up for sale by Reddit to AI companies, why do they still focus on ads? Why would they leave money on the table after the bulk of their users proved to be docile in the face of so much recent enshittification? reply alberth 17 hours agoprevold.reddit.com What’s are the chances just before/after IPO, old.reddit.com shuts down? reply shombaboor 17 hours agoparentI feel like we need to do a PSA campaign for unaware users that old.reddit.com exists. Sorta like the remember to check your tire pressure. Feel bad for folks that use the new ui unwittingly reply unshavedyak 17 hours agorootparentHonestly, since i set Kagi to rewrite my searches to old.reddit, i have forgotten new reddit exists lol. I hope someone makes a forum on top of ATProto. Their tech stack is far more interesting to me compared to ActivityPub, but i just can't stomach the Twitter-style UI of Bluesky to switch to that from Reddit. reply dralley 17 hours agoparentprevThe day they shut down old.reddit.com (or the preferences option to fall back to the old site by default) is the day I stop using reddit permanently. reply swed420 16 hours agorootparentThey likely know many users feel this way even if they're not the majority, which means old will probably go dark after the IPO (and after a bunch of people bought into their \"direct share program\") reply Havoc 11 hours agoparentprevThey did promise to not shut it down but not sure I’d trust them to stick to it reply Crontab 14 hours agoparentprevIf old.reddit.com does go away, so will I. The \"newer\" version is truly horrid. reply 0xcde4c3db 11 hours agorootparentEspecially the extra-lazy comment loading, which seems to only be getting worse with time. Threads that go too deep now require navigating to a new page, and comments frequently have \"1 more reply\" that either never loads or doesn't actually exist. reply SV_BubbleTime 17 hours agoparentprevWhile I agree that old.Reddit is the only way to use that unfortunately-required site… it will not fix the obvious and subtle shilling. reply shepherdjerred 15 hours agorootparentThere's also teddit! I host my own instance and access it with Tailscale. I use a Kagi rewrite rule so that Reddit results link to my teddit instance. https://github.com/teddit-net/teddit reply hipadev23 17 hours agoprevReddit thinks they’re going to sell data but their system is extremely easy to scrape (including post API changes) and all the relevant players procured that data a long time ago. reply davidw 18 hours agoprevWhat's everyone's take on that IPO and the shares for reddit users? reply mikequinlan 18 hours agoparentBuying shares at the IPO hoping for an immediate profit means that you are betting that the underwriter has underpriced the shares. That seems unlikely. reply roughly 18 hours agorootparentThat’s common practice. Most of the time, the shares are sold before the IPO to financial institutions at the list price to guarantee a certain volume of sale and liquidity for the IPOing company. Those institutions expect a first day pop, so it’s common for the list price to be below the expected market price. It also looks better for the company if the stock goes up on day one. None of that maximizes cash to the company, but there are other considerations (and actors) in play. reply candiddevmike 18 hours agoparentprevRumors are they aren't anywhere close to profitability, seeing their SEC filings will be interesting, especially if they disclose who they sell data to... reply hipadev23 17 hours agorootparentEnjoy https://www.sec.gov/Archives/edgar/data/1713445/000162828024... reply ac29 14 hours agorootparentprevNo need for rumours, their S-1 is available. And no, they are not profitable. They spent something like $800M on staff last year, which is insane to me. reply fullshark 16 hours agoparentprevIt's a pump and dump, the site will just get worse and worse for the user base as they attempt to squeeze whatever cash flow they can from it post IPO. reply lamontcg 15 hours agoparentprevReddit understands that its biggest value is as a gigantic pump and dump forum to dump their bags onto. reply nusl 18 hours agoparentprevSmells of desperation more than anything to me. reply Devasta 16 hours agoparentprevThey have a legion of mods working for them for free and still aren't profitable. Whatever you may feel about the denizens of WSB, they all see reddit as nothing more than a shorting opportunity. Once the mods start getting missives about modding policy from whatever new board takes over post IPO, then things will start to disintegrate. reply neverokay 15 hours agorootparentI kinda don’t know what to say. I never thought message boards could be taken this far, but Facebook did it. It’s a giant phpbb forum going public lol, yeah … I get the short position totally. reply Pigalowda 18 hours agoparentprevEveryone’s take? My inexpert opinion is that current stakeholders want their exit and the IPO isn’t for the users to really buy or promote. Reddit has enough mainstream recognition for casual investors who don’t use Reddit to try and make some money on the fresh IPO. But then it just becomes a tulip as reality sets in. There’s definitely a chance to make money in greater fool scenarios but also much greater risk. But I’m just a guy making noise. Maybe it’ll rocket to the moon and you can fire life. reply rchaud 16 hours agorootparentHonestly? Reddit is a corpse being propped up like in Weekend at Bernie's in a hope to keep the party going long enough for them to dump enough shares on the secondary market. The founders and bigwigs have cashed out. Ohanian made his fortune and exited years ago, the CEO just made $193m in compensation. Can't be much milk left in the cow. reply Pigalowda 14 hours agorootparentI really hope that is the case. The site sucks. It's got decent product reviews and porn though. That doesn't make it a good investment to me but we'll see. Just because we cheer for something to fail like twitter doesn't mean it happens overnight. Even though it should and would be hilarious. reply linsomniac 18 hours agorootparentprevCorrect me if I'm wrong, but the bulk of the existing stakeholders are severely limited in what they can sell over the first, what, 3-6 months, beyond the shares that are part of the IPO. reply SnorkelTan 18 hours agorootparentAFIAK, preferred shares (the ones investors buy) typically aren't subject to a lockup once they convert to common stock. They usually don't dump right away since they don't want to tank the market unless the company is actually dogshit and they're pawning it off on a bigger fool. reply Pigalowda 18 hours agorootparentprevI believe you’re right but my hypothetical is that it won’t have some Robinhoodesque IPO but that it falls in an inevitable but unpredictable short to medium interval. Long enough for me to want more gains if i invest but short enough for me to lose everything because I can’t time it right. Stoachastic if you will. A proper microtubule disassembly that leaves me with my little worthless tubules. Tulipules? reply linsomniac 18 hours agoparentprevIt's generous of Reddit to make that offer. The last time I recall having such an opportunity was VALinux back in 1999. However, despite having a 4 letter username and \"cake day\" in 2008, and regular, solid contributions, I just barely didn't qualify. Karma whores easily do qualify, so that's a bit hard to swallow. But my wife does do a LOT of interaction there and did qualify, so I'm trying to figure out exactly what to do. My thoughts so far: The big problem is I'm not sure that Reddit, the company, really understands it's business. Though I was expecting the protests of the last year to have gone much further than they did, I think Reddit's response could have been much better, and their response demonstrates their lack of a solid business plan. In particular, the importance of redditors and moderators to their business isn't reflected in their plans (AFAICT). I think that may end up backfiring. I feel like their IPO would be better priced closer to a 4.5B valuation rather than the 6.5B they seem to be targeting. There is some potential upside to it, as there might be some justification for a market cap of 10B. But there's a big wildcard in there: the Directed Share Program to redditors have no \"cooling off\" period, so there's likely going to be some volatility due to \"get in, get out\". But, the DSP shares have \"one vote per share\", which might contribute towards people holding them. But that's 8% of offered shares, and offered shares are around 20% of total valuation, so that may be a small enough percentage of shares with high volatility that it won't have a big impact. The WallStreetBets crowd might have an impact, that will be fun to watch from afar. I'm not expecting anything there, but who knows. I'm kind of expecting some backlash of the community to the extraction of value by Reddit the company, but I haven't really heard any noises along those lines. I could see their value dropping significantly over the next year because of this. There's a lot of fatigue over the techno-elite extracting value off \"the little people's backs\". And Lemmy is really well positioned for facilitating a mass exit from Reddit. The recent news of advertisers making posts that look like regular user posts is, IMHO, leaning in a bad direction. In my mind, that is reminding me of one of the primary things I've seen in Facebook that has dropped my engagement to near-zero levels. But, there are a lot of other tech companies watching Reddit and hoping for a good IPO leading the way for other similar IPOs. Maybe that will lead interest. Over all, I'm expecting short term excitement, but longer term rough times for Reddit the company. But, my investment plans have, in too many situations, been exactly the opposite of the market; I've bought just before the market drops and sold right before it goes up... reply davidw 17 hours agorootparentThanks for the quality comment. I basically just buy and hold VTSAX and call it good, rather than worrying about individual companies. I'm mostly just curious if there'll be enough of a 'pop' with the stock to make buying some shares worth it. reply linsomniac 16 hours agorootparentThat is generally a very good strategy. I have a lot (~50%) of my investments in Target Retirement Plans, though I have put them out about 5 years from my real retirement date, just to keep them a bit more high-return longer since I feel like I can handle that risk. However, my FIL gave me some great advice 5-6 years ago: Take a little bit, and put it into things you like, to keep investing \"fun\". In my case I'm doing around 2% in \"fun stocks\". Personally, it's pretty hard to argue with since my first \"fun stock\" that I did that with was TSLA, and I got a 20x return on that. My new \"fun\" set have been other EV and energy stocks, largely, and have generally done quite poorly, a 2-3 of them are down 95+%, but that's over 2 years and my window was more like 5, so time will tell. A handful are up 30-50%. So, some of it depends on your definition of \"fun\". :-D reply SV_BubbleTime 17 hours agorootparentprevEveryone seems to make this mistake that read its core business is social. And that means they sell data. They’re selling influence. Mostly to kids. To the terminally online. The new Politically Religious. The standard MO to ban all decent isn’t because the admins/mods actually disagree, it’s that it makes the echo chamber more uniform. They’re culling wrong think to sell things, sure, but more so ideas. When you consider that “reputation” is a product that politicians, celebrities and companies can just purchase, I think it makes a lot of sense what Reddit can sell. reply linsomniac 16 hours agorootparentI'm going to disagree with you that Reddit is selling influence. I think they're selling eyeballs and user-created data. Both of which highly depend on Reddit's reputation with it's users, and that is what I think Reddit is going to have problems with. reply unstatusthequo 18 hours agoparentprevI plan to short it soon after the IPO reply parpfish 17 hours agorootparentI’ve thought about doing this as a more general IPO strategy on the assumption that there’s going to be a sell-off of employee owned equity. I’m sure that there’s a way to estimate the number of shares vested by employees and come up with an estimate sell-off volume for day 0-1 reply dgfitz 17 hours agorootparentAren’t employees usually restricted on selling for like 6 months? reply parpfish 17 hours agorootparentWell then you should circle that date 6-mo after IPO (or the end of any non-IPO blackout period) as days with mass sell-offs regardless of the stock price reply dgfitz 16 hours agorootparentYou act like that isn’t cooked into the optimizations of the market. It is, or everyone could follow your get-rich-quick scheme. Go ahead, try us a few time, see how it goes. reply parpfish 14 hours agorootparentBut is that actually baked in to the price? (I’m legitimately asking, not being argumentative) Back when I worked at a big public company where we had options and employee blackout dates, there would almost always be a dip in the stock price when the blackout was lifted reply awithrow 13 hours agorootparentThe \"baked into the price\" also includes the price of options. I would not be surprised to find that the price premium of options dated around the 6 month window would negate the gain from the drop. reply Havoc 11 hours agoparentprevWouldn’t touch it with a 10 foot pole reply rvz 17 hours agoparentprevEasiest IPO that will be shorted as soon as the VCs and other employees can dump their stock on the public markets either dumping a small percentage such as 20% on IPO day or they will do it after the lock up period or both. Which ever comes first. reply lifestyleguru 18 hours agoparentprevNormally they'd fall into irrelevance as Digg did, but their last desperate cry is an IPO. Weird times. Either way they're now absolutely hostile towards casual commenters, posters, and viewers. Their data dumps up until maybe 2023 will be valid and interesting only for some limited time. reply pentagrama 15 hours agoprevuBlock Origin lists seems that not yet blocked those ads since I'm able to see it https://www.reddit.com/user/PhillyCanada/comments/12ykrp1/he... reply hayst4ck 17 hours agoprevThe real scary thing about the Reddit IPO is that we are headed into the age of the \"strongman\" leader and both Reddit and Twitter have been the preeminent voice of those against strongmen and tyranny. It was Reddit that gave those in Hong Kong, Ukraine, and Gaza a voice and promoted solidarity for them. Both Reddit and Twitter are experiencing a capitalist assault because both threaten capitalist power. Twitter was neutralized when it was sold. Reddit is now being neutralized. Shareholder influence will prevent Reddit from being a platform that can be used to speak truth to those strong enough to harm it. This IPO is Reddit's graduation ceremony from a platform for speaking truth to power, to a platform for the exercise of power. reply Ekaros 15 hours agoparentLooking at history of big default sub-reddits. That seems more accidental than truly being against strongman or establishment... The capture has been clear for years... reply thfuran 17 hours agoparentprevReddit isn't experiencing a capitalist assault, it's literally selling out. reply sashank_1509 17 hours agoprevTo be fair to Reddit, Reddit ads are consistently the best ads I’ve seen. I think Reddit formats the ad and makes it with the company to uphold the Reddit vibe and many times the ad also says something witty in the context of the subreddit I’m browsing (my guess is ads are limited to subreddits). The worst ads are twitter and YouTube, just a chore that I would much rather have skipped. reply pquki4 17 hours agoparentOne reddit ad I saw was from a company that provides the service to \"back up your GitHub repositories\". Don't know if that company is still around. reply shepherdjerred 15 hours agorootparentAre you saying there's something bad about backing up repositories? That seems like a very good service to have. reply bravetraveler 13 hours agorootparentI believe the implication is that one can 'back up' by pushing to any number of remotes reply bravetraveler 9 hours agorootparentI can't edit now, but an example occurred to me. A false dichotomy we often see: Self-hosted SCM, or elsewhere like Gitlab/GitHub? Why not both? Mirrors are another way of looking at backups reply rsync 14 hours agorootparentprevWas it us ? I know that we have, on many occasions, run ads on Reddit highlighting exactly that use-case. reply imchillyb 18 hours agoprevOne more nail in reddit's coffin. Since the API kerfluffle reddit's usefulness has dropped drastically. Making ads look like user posts definitely kills the platform for me. They killed my platform and I killed my account. Muxing ads. Who'd have thunk it? reply SV_BubbleTime 17 hours agoparentI agree usefulness has dropped, I agree, mixing in ads, will hurt them. But, I bet in the time since the API kerfuffle, they’ve gained users and engagement. reply ilrwbwrkhv 15 hours agoprevThe users of Reddit are some of the most anti ads on the planet. I have done a lot of advertising and Reddit performs so poorly that it is not worth it. Having ads blend in won't make a difference. reply paulddraper 16 hours agoprevIIRC that kind of subtle advertising is the kind reddit likes reply wslh 18 hours agoprevIt always seems like the current owners and top managers of Reddit don't use it. If they were, things could be much better for the community and probably the business. Again, the official Reddit app seems done by your high school cousin in a mobile development course. reply darknavi 18 hours agoparentDogfooding is so important. It reminds me of a recent twist to the EV space in North America where Ford, and then all other players, decided to switch their EVs to Tesla's NACS connector. This interview recently is really grounding. Basically Ford's CEO went on a road trip and actually tried the EV road tripping experience with a non-Tesla (non-Tesla charging stations that is) and saw how dogshit it was. That started conversations to actually try and improve the situation for customers (thus the change to NACS). Timestamp link to the interview: https://youtu.be/7vE0KXHZXCY?t=244 reply none_to_remain 17 hours agorootparentDespite the jargon use of the term, I don't think dog food CEOs actually eat their dog food, because it is for dogs. Similarly Reddit is for Redditors. reply darknavi 17 hours agorootparentAll the kids in my neighborhood growing up tried a Milk Bone at one point. Close enough right? reply marcosdumay 16 hours agorootparentprevSomebody already posted that the (it was only one) CEO actually did. I'll just point out that dog food is food. It may not be healthy for a human in large amounts, but it can't be worse than eating fast food for a week or so. I know an unreasonably large amount of people that claimed to try it. Most when they were children. I doubt that my bubble is unusual on this. reply xboxnolifes 16 hours agorootparentprevI'd except them to at least feed their own dogs the dog food they make though. reply Ukudala 17 hours agorootparentprevau contraire: https://www.theguardian.com/business/2020/feb/09/texas-busin... reply Uehreka 18 hours agoparentprevNah, this isn’t a matter of people wanting the site to be good but not knowing what to do. They know that they’re making the experience worse, and they don’t care because it makes them slightly more money. We’re well past the point where people are doing “build the thing you’d want to use”. reply wslh 18 hours agorootparentSorry, don't agree. In business, like in sports, there are forced and unforced errors [1]. Reddit commited many unforced errors. Have you used their ads system? I tried and did since it was created. It seems done by my other cousin younger than the one developing the Reddit app in the garage. [1] https://www.reddit.com/r/tennis/comments/29k5i7/can_someone_... reply ryanwaggoner 17 hours agoparentprevI always hear these complaints about the Reddit app and I honestly don’t know what people are talking about. Sure, there’s a few UX things that could be improved, like almost every app. But I’m a heavy user with multiple accounts and I rarely have issues. My main issue is some stuff around the video player UX but it’s pretty minor. What are people so worked up about? FWIW, I’ve been a heavy Reddit user for over a decade, across web and various mobile clients. I’ve also been a full time iOS engineer since 2010, and their app would be a huge effort to recreate. It’s amateurish to suggest that any single dev could build it in a year, let alone a weekend. I’m NOT defending their business practices either. But their mobile app is fine. reply zettabomb 17 hours agorootparentDid you try any other apps? The official Reddit app didn't even come close to their usability. As far as saying that no single dev could build it in a year - well I don't know how long it would've taken for the latest codebases, but to my knowledge RIF and Apollo (perhaps the most popular unofficial apps for Android and Apple, respectively) were developed by single people, and they were widely praised for being far more usable than the official app. reply ryanwaggoner 16 hours agorootparentYes, I used a bunch of them over the years. And yeah, they were fine, but that doesn’t answer my question about what exactly is wrong with the official Reddit client to garner so much hate. reply zettabomb 11 hours agorootparentThey were widely considered to be significantly better. The official app had less features, was often annoying to use, and just didn't even work all the time. At least for RIF, I liked: better performance, more reliable (official app would just decide not to load ANY media sometimes), no notification spam, higher density information, more customizable, better separation between ads and posts (and a small purchase to get rid of them entirely), the list goes on. It was pretty obvious once you used them for a day or two. reply ryanwaggoner 9 hours agorootparentThey were “widely considered” to be better, but again, almost never with any specifics. Just knee jerk hate. And I don’t have any of those issues with the official client, other than the ad one. The performance is great, it’s very reliable, I have most notifications disabled except for a couple things I care about (and the notification settings are pretty fine-grained). Maybe the customization stuff could be more extensive? But I don’t want 1000 settings for every tiny little thing. Regardless, it hardly seems like a complaint that people would get so worked up over. Maybe the official client used to be much worse and I don’t remember, but increasingly I think people are just pissed about Reddit cutting off the alternative clients and hate being forced to change, so they moan and groan as if the official client is the worst app they’ve ever used. When really, it’s fine. reply zettabomb 8 hours agorootparentIt sounds like you don't have a problem with it. Maybe you never encountered the issues, or maybe you just have a higher tolerance for them. I did give specifics, and if you cared to look they're well documented - it's hardly knee jerk hate. Over 1.5 million people were annoyed enough by them to buy Apollo alone. And for the record, these complaints were present from far before Reddit decided to get so stingy with their API. reply perihelions 18 hours agoprevHN has ads-that-look-like-posts too. They're mostly job postings for Y Combinator startups—a perk of that incubator being getting privileged positions on HN's front page. I filter them with uBlock. reply jasonjmcghee 18 hours agoparentThey have very nearly zero negative impact on my experience though. reply lutoma 18 hours agorootparentExcept for the 'OneSignal' startup ones. I always think they're posts related to Signal (the messaging app) for a bit. reply ghostpepper 17 hours agorootparentI always find it odd when startups pick names that resemble large, unrelated companies. BuildZoom is another one - has nothing to do with Zoom reply kgermino 17 hours agorootparentNot sure about onesignal, but I’m pretty sure BuildZoom long predates the zoom video conferencing tool being a household name. There’s an element of luck, but also the names are part of the culture from the time they’re created. Multiple people often come up with very similar names for different things just because of what’s going in the world around them. reply jspash 18 hours agoparentprevThis must be why those \"EasyPost is hiring\" posts show up every 3-4 weeks like clockwork. I assumed they were just bad at employee retention. reply IncreasePosts 17 hours agoparentprevI filter them too. Anyone who posts an ad and doesn't allow comments is a coward. reply solarkraft 16 hours agorootparentOne thing I really liked about Reddit and Twitter ads is that they did allow comments. I think that started to change when I stopped using them. reply xg15 18 hours agoparentprevAt least they are predictable and closely related to to the community. I can sort of understand that a startup accelerator that runs a discussion board on the side would want to post job postings for their own startups on their own discussion board. (Also the fact that YC runs HN isn't exactly hidden - the first hint is right there in the URL) I think it would be something completely different if YC suddenly showed promoted posts from unrelated \"partners\" or just opened this up as an advertising opportunity to the general public. reply perihelions 18 hours agorootparentI agree. I think this is one of the key reasons HN succeeds as it does: the narrow, delimited incentives on the part of YC. All of the writing and discussion that exists on HN, which isn't a YC ad, is a \"halo\" to attract and retain readers to this forum (and see the YC logos and YC ads). That's their primary incentive: attract users, attract a specific culture of users. It's an incentive aligned with quality and pleasantness for us. The daily forum experience isn't the revenue source; to monetize that, YC would risk losing more (on their YC-halo boosting side) than they could expect to gain (from McDonald's ads, or whatever). This is the diametric opposite of how most social media platforms think about users, and their business models around them. The Reddit IPO is a case study: they've willingly chased away countless millions of users with an abrasive, horrible forum experience, because they calculate they can extract more revenue from those that remain. Where HN is a \"narrow, tall\" revenue source surrounded by free stuff, Reddit is \"wide and short\": everyone and everything is the product. reply pierat 18 hours agoparentprevIf you cant comment on a post, it's not a post but a shitty ad. Tells me that VCs are afraid of actual criticism, and 'no comment ad-posts' are their version of a safe space.... for critiques. reply perihelions 18 hours agorootparent- \"If you cant comment on a post, it's not a post but a shitty ad.\" Right, that's the basis of my uBlock rule. :) news.ycombinator.com##:xpath(//td[contains(@class,\"subtext\")]):not(:has-text(/\\b(comment|comments|discuss)\\b/i)):xpath(..../following-sibling::tr[1]../preceding-sibling::tr[1]):style(display: none) reply bossyTeacher 18 hours agorootparentNow that you posted your secret, Dang can update the html to bypass you rule reply djbusby 18 hours agorootparentI bet that doesn't happen. reply Pigalowda 18 hours agoprevInteresting idea to advertise on Reddit front page. I suppose there’s value in edgelords, bots generating GPT ATH stories, and political shills. Good luck to them! reply SV_BubbleTime 17 hours agoparentI think there’s so much more value and being able to purchase public opinion even if you think it is just ‘fringe edge lords’ than you realize. reply Pigalowda 14 hours agorootparentI guess we'll see what the value really is soon. reply 55555 17 hours agoprev [–] If anyone wants a legitimate business opportunity that’s admittedly notoriously difficult to monetize, consider starting a Reddit for porn. Once they’re public, all it takes is some bad PR (“I was sex trafficked on reddit” or “Reddit profited off a video of the abuse I was a victim of while underage”) for them to throw in the towel on the adult content that comprises half or more of their site. at which point there will be an overnight exodus seeking a similar platform. Reddit is a spineless and liberal company and will cave to pressure. Just my theory on a likely future event. reply gigglesupstairs 16 hours agoparentReddit for porn already exists. On Reddit. reply fcarraldo 17 hours agoparentprev [–] What does \"liberal\" have to do with anything else you've said here? Are conservatives known for their friendliness to online porn sites? reply 55555 6 hours agorootparentMy use of the word liberal is not related to my use of the word spineless. There’s an “and” there and those are two unrelated adjectives. Doesn’t read well, though. Yes I do believe the politics of the organization are relevant to assessing the likelihood of this event. reply ranger_danger 17 hours agorootparentprev [–] in secret, yes reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Reddit is launching \"free-form ads,\" a new ad format mimicking regular user posts, just before its IPO, offering advertisers a unique advertising opportunity.",
      "The format has delivered positive outcomes for brands such as Kraft Heinz and Leica, showcasing its potential effectiveness.",
      "Reddit is progressing with its IPO preparations, despite facing FTC review regarding its collaboration with Google to supply user-generated content for AI learning."
    ],
    "commentSummary": [
      "Reddit is introducing ads in user posts before its IPO, following platforms like Twitter and Google.",
      "Users are debating the effects on user experience, ad blocking methods, and alternatives like Lemmy amid worries about Reddit's profitability and reputation post-IPO.",
      "Discussions include Reddit's shift towards ads and capitalism, potentially impacting user engagement and credibility, alongside criticism of the official mobile app, restrictions on alternative clients, and comparisons to platforms such as Hacker News. Concerns also arise regarding Reddit's potential content removal pressures and its impact on the community."
    ],
    "points": 138,
    "commentCount": 150,
    "retryCount": 0,
    "time": 1710687050
  },
  {
    "id": 39737084,
    "title": "Superiority Through Sinister Parodies of Classic Children's Books",
    "originLink": "https://metallicman.com/laoban4site/superiority-by-arthur-c-clarke-full-text/",
    "originBody": "congjing yu on Sinister Parodies Of Classic Children’s BooksMarch 17, 2024 Absolutely. This was a Mantid inspiration. Not Domain. -MM",
    "commentLink": "https://news.ycombinator.com/item?id=39737084",
    "commentBody": "Superiority (1951) (metallicman.com)136 points by rgrieselhuber 14 hours agohidepastfavorite56 comments lifeisstillgood 14 hours agoThe problem is always expectation, anticipation and experience. With any new technology we expect certain performance - we get this from the “The Economist effect” - where the simple one paragraph explanation of a complex field enables us to have a grasp on a concept but not the full difficulties. So we expect the nuclear missile to hit its target whereas the thing about nukes is they were designed to be used despite being horribly inaccurate. Anticipation is time bounded - we get depressed when the thing we expect is not done today - we can imagine it so why can’t we have it. This is the problem behind all crunch development times, all VC funded CEO replacements and every lying progress report ever. And experience is what tells us where we really are - it is looking at a Trident missile spiralling round in front page photos and saying … what have we missed. Weapons operate always at the frontier of what is possible. Nuclear weapons probably are not really there - we can fly the prototype over unchallenged airspace but launching rockets is hard. reply nonrandomstring 13 hours agoparentThe problem is always hubris, which leads to a distorted conceit of \"progress\" which can actually be regressive. Whether it's weapons or civilian technology like smartphones, automobiles, or a \"cashless society\" we know there are side effects, and unknowns. But we choose to focus on only the rosy, optimistic side. For most of us that is pandering to laziness, convenience, low-effort and less thinking. For those involved in making money, the negatives are far away in space and time, so they can always kick the can down the road. This is how supremacy becomes weakness. Money buys a louder voice and drowns out cautious minds with better vision of the long-term future and marginal scenarios. They are Cassandras. Luddites. Those with experience are dismissed as old and irrelevant. Hubris leads to absolute dependency and complacency. In a long enough time-line we'll always encounter an accident or enemy using older \"low-tech\". The greatest threat to our security is always our own hubris. Sadly I see buckets of it here on HN. reply lifeisstillgood 13 hours agorootparentI have a very simple theory about this - If we assume the 80/20 rule to hold about most things in world (80% profits from 20% customers, 80% of benefits to society from 20% workers), then we can assume the same 80/20 rule applies to the bits left over - ie the remining 20% of profits will have 80% of those generated by 20% of the left over 80% of customers) Roughly speaking then 96% of all good stuff comes from 36% of stuff we do We could say then that if we could find the 2/3 of useless activity that only generates 4% of good stuff we can for example cut carbon emissions by 2/3 - and only need to lose the crappy plastic toys on front of magazines, or most peoples commutes or … What I think I am saying is that 2/3 of the jobs people do are useless - and yet in any organisation they are the majority and hence bend the organisation Don’t worry dear reader I am sure like me you are one of the non useless ones … like me , like me reply nonrandomstring 12 hours agorootparentYou and I, we've gone through the same thought process and, sadly, where it ended up for me was; Golgafrinchans [0] Now if we could only work out what that useless third is. ?! But there's always the finite risk that plastic landfill fodder and chindogu attached to kids magazines is the essential saving grace of humanity. :) [0] https://hitchhikers.fandom.com/wiki/Golgafrinchans reply scotty79 22 minutes agorootparentprev> Roughly speaking then 96% of all good stuff comes from 36% of stuff we do That's just pulling arbitrary number out of thin air with extra steps. reply mistermann 4 hours agorootparentprevThis seems like an absolute genius idea, but I can't imagine us considering trying it, even if climate change really started to get bad. China or maybe Japan though, I could see it being able to maybe catch on there though, it could be very beneficial to the problems they're facing with their aging populations and abysmal and worsening (heading to ~zero?) reproduction. reply actionfromafar 3 hours agorootparentWe humans could try it but we wouldn’t succeed. We’d just find a way to fudge the numbers and stack the ranks. :-) reply CapitalistCartr 12 hours agoparentprevI think your premise is sound, but your example is unfortunate. SLBMs are less accurate, but still much better than \"horribly inaccurate\". And American ICBMs started out with poor accuracy, but are now quite accurate(1), and were by the 1970s. If you're standing in the target building, you are already dead. (1)I'm being a bit coy here because during Uncle Ronnie's administration, I had a job and security clearance involving this. reply lifeisstillgood 11 hours agorootparentThe thing is, well there are lots of things and I do t mean to impune your work some forty years ago but 1. We have learnt not to trust untested equipment 2. We have learnt that under pressure well meaning people will demonstrate equipment that while kind of representative of the reality are honed, improved and generously helped to perform well on test 3. Only repeatable performance under active competition is a real measure. It’s why sane people like open well designed markets. And it’s why weapons that get battle tested tend to be reliable I am sure that overall if we read the red button nukes will fire and land and kill millions. It’s just the day after I expect that the auditors will find a large number failed to fire, or failed to leave the tube, went the wrong way or simply otherwise went wrong The point of credible threat is not one shot one kill, but so many shots we can basically guarantee a kill. In other other words, both sides hold enormous number of nukes ostensibly to ensure they have some to fire after a first strike - but the effects of an enemy first strike are indistinguishable from poor engineering and beauracracy reply CapitalistCartr 10 hours agorootparentMilitary nuclear programs are completely unlike civilian programs. Rigorous, repeated, varied testing, no expense spared. Our ICBMs are far from untested. We used to fire 2-3 per year to insure their reliability. Pulled out of the field, launched from Vandenberg, and carefully analyzed. I never once had any bureaucracy interfere with doing an excellent job, and money was never a limit. In the event of WW3, at least 96% would have struck their targets. We know that from decades of expensive research, testing, and incremental improvements. We spent over 5 trillion dollars on our nuclear program and received our money's worth. reply lifeisstillgood 10 hours agorootparentIt’s just … given something like 30k warheads (I know not same as rockets) but taking 2 or 3 for testing (and notably, taking, checking, scrubbing down, repairing … improving?) and then saying that represents the whole fleet, and leaving in the field for years between expensive retrofitting cycles on decade timescales I mean maybe the USA was a on it’s A game for fifty plus years … us in the UK well … https://www.bbc.co.uk/news/uk-68355395 reply PJDK 1 hour agorootparentWorth noting that UK missiles (but not warheads) are taken from a joint pool with the US. They have completely shared maintenance, so any reliability concerns are shared. Similarly, the testing is also joint so this should be seen as two failures out of all trident tests rather than two UK failures. Not that that is a fun headline of course. Obviously everything around it is super secret but the muttering around this one seemed to be that the rocket noticed the warhead seemed wrong (which it was because you don't strap a real nuke to it!) And aborted itself. I only add this because I think its interesting. reply er4hn 13 hours agoprevIt's incredible how Arthur C Clarke foresaw the downsides of full codebase rewrites and putting all organizational weight behind untested and cutting edge technologies all the way back in 1951. Truly prescient sci-fi writing. reply KineticLensman 13 hours agoparentIt could be argued that German military technology in WW2 illustrated some of the same issues, and perhaps gave ACC the idea. They had the V2, TV-guided missiles and operational jet fighters, some of which had massive development / opportunity costs, but none of which materially changed the outcome of the war. The ME262 jet in particular had engines that only survived a few flights because by then German industry couldn't access the specialist metals required. reply alan-crowe 7 hours agorootparent\"Professor-General Norden\" might be a dig at the Norden Bomb Sight, an American high-tech wonder weapon that under-performed. https://www.youtube.com/watch?v=U6D5rXbMBKo tells the tale. reply Sharlin 12 hours agorootparentprevNot to mention their tanks, IIUC? Particularly the late-war models. Very advanced, very maintenance-intensive. reply KineticLensman 12 hours agorootparentYes - the German armaments industry was very quick to respond to emerging requirements, leading to a proliferation of platforms and versions that were logistically difficult to support, especially as the supply chains were disrupted. Some mass-produced weapons (e.g. the Panzerfaust) were widely distributed and well supported, but the higher-tech ones weren't. reply nicbou 7 hours agorootparentI believe that they also had growing quality issues due to sabotage from slave labour and shortage of materials and alloys. I remember from Wages of Destruction and Albert Speer's memoirs that the loss of certain mines in the East were devastating to them because they could no longer produce certain alloys. reply PaulKeeble 12 hours agoparentprevThere are organisations that have been following the rewrite strategy for 2 decades that are running 5 or more parallel systems and teams all doing a lot of common things with each with unique properties often in entirely different languages and a lot of integration hell. They will quite happily be starting a 6th such project rewrite this year to solve the problem of the previous 5, this time surely it will work. Another organisation I know of replaced its \"old\" .net desktop client with a nice shiny Microsoft derived web solution using some customised COTs and it would solve all their maintenance issues. Sigh its a disaster as expected made all the worse because they fired the developers before the switch was even really done. Over and over this mistake is made to rewrite and replace instead of refactor on the assumption the old code is crufty rather than battle hardened and that writing new code is easier than editing old. Never do these organisations learn to develop more maintenance friendly systems along the way. reply sorokod 14 hours agoprevA contemporary echo to that story: \"US Navy Procurement Disasters - The Littoral Combat Ship and Zumwalt Class Destroyer\" https://www.youtube.com/watch?v=odS3Kn5oGl0 reply bee_rider 13 hours agoparentLooking at Clarke’s Wikipedia article https://en.m.wikipedia.org/wiki/Arthur_C._Clarke > During the Second World War from 1941 to 1946, he served in the Royal Air Force as a radar specialist and was involved in the early-warning radar defence system, which contributed to the RAF's success during the Battle of Britain. Clarke spent most of his wartime service working on ground-controlled approach (GCA) radar > Although GCA did not see much practical use during the war, after several years of development it proved vital to the Berlin Airlift of 1948–1949. I think he must have had some pretty good first-hand experience WRT deploying new tech during a war! The US also famously had the whole Mark-14 torpedo fiasco, the torpedo issue in the story is not quite similar but there might be an echo. reply all_these_years 8 hours agoprevThe obvious parallel right now would be the introduction of AI (a known flawed technology, IMHO), replacing jobs of experienced engineers and experts in most fields, in a moment of social and political instability... reply jonplackett 12 hours agoprevI’m surprised not to see any comments on Ukraine here yet. USA and the west focussing on ‘super weapons’ like f22, f35, super carriers etc etc. and Ukraine is losing the war because they don’t have enough bullets and mortars, and we don’t have enough factories to increase production. reply throwaway11460 12 hours agoparentUkraine would be in Moscow in 3 days if they had this technology. reply jonplackett 11 hours agorootparentEven f16s are too fragile to give to ukraine. They can only land on perfect runways because their engines are slung low. Most of the modern USA weapons only work under condition of being totally dominant and being able to have your supply chain work perfectly. This story really is a very good analogy. One further point - and this is what makes this war even more tragic - say Ukraine do get all the super weapons annd they do work as expected, and are on their merry way to Moscow, what would Russia do? Most likely their military doct9rine of dropping some tactical or not-so-tactical nukes. As Sean Bean knows, one does not simply invade a nuclear power. reply throwaway11460 10 hours agorootparentI'm not suggesting they should. But the parent is acting like the technology wouldn't make a difference - it'd make all the difference. Ukraine controls a lot of their territory, they have a lot of well paved runways to operate out of. reply actionfromafar 11 hours agorootparentprevIt's a tough problem. It's not very palatable to just fold either. The Russian government can always wave the nuke card, it's not going away, but neither will their ambitions. reply zrn900 10 hours agorootparentprevYeah. Like how they got to moscow in 3 days when the earlier wunderwaffe were provided to them... This delirious out of touchness is making the West lose the war. reply wiseowise 1 hour agorootparentBy earlier “wunderwaffe” you mean bits of 70s equipment that was due to decommissioning? reply seoulmetro 9 hours agorootparentprevLol did you even read the story? reply actionfromafar 12 hours agoparentprevOr because we don't give them F35s, long-range artillery and 400 Abrams tanks. In Iraq 400 Abrams did a good job from what I'm told. reply jonplackett 11 hours agorootparentIn Iraq those tanks were already old and crucially didn’t have night vision so they just blew them up in the night. We did give them long range missiles. And now quite a lot of tanks. But - and this a has been admitted to by the pentagon - they didn’t think though or provide for servicing them. The poor buggers have all these different western tanks with parts that don’t match. And the Abram’s is powered literally by a jet engine and needs constant servicing. It would be hilarious if it wasn’t so completely tragic. reply actionfromafar 11 hours agorootparentIndeed. The support has been incredibly spotty and half-assed throughout. reply jonplackett 37 minutes agorootparentThe most basic problem is that Russia have treated it as an existential threat. Mobilised their entire economy, and it turns out, had been planning for this for years. The west just gave Ukraine all their spares and didn’t make any kind of plan for what to do next. It’s a complete failure of leadership and forthought and we’re really just lucky that it _isnt_ an existential threat for us, because we’d be really screwed if it was. reply rainworld 11 hours agorootparentprev>In Iraq 400 Abrams did a good job They were hardly threatened there. In Ukraine, they are proving to be at least as vulnerable as any other tank. https://x.com/squatsons/status/1769344446112711037 >we don't give them long-range artillery They got plenty of that. As much as the West could spare and then some. Some countries gave literally everything they had. Ukrainians do lack barrels. Because they don’t exist and won’t anytime soon. >F-35 Hilarious. reply bevekspldnw 5 hours agoparentprevYes and no. To some degree with the US this is true, but the US also produces enough small arms to keep several small scale wars going, just ask Mexico and South America. Europe on the other hand unilaterally disarmed after the Berlin Wall fell because they couldn’t anticipate a world where Russia reverted to historical form and the US wasn’t carrying NATO on its back. Now they’ve got Putin on one side, potentially Trump pulling out of NATO on the other, oops. reply jonplackett 33 minutes agorootparentThe problem is that neither Ukraine nor Israel are small scale wars. They’re both dropping crazy amounts of bombs and needing crazy amounts of anti air missiles. Meanwhile the US is still meant to be ‘pivoting to Asia’ so wants ammo for that. It’s an established fact that Ukraine is running out of everything and we can’t make more. Even CNN now reporting it. https://amp.cnn.com/cnn/2024/03/10/politics/russia-artillery... reply matkoniecz 10 hours agoparentprevNot really sure about point being made here. Ukraine has no f22, f35, super carriers etc etc. If it would have them, then it would stomp over invading army. reply jonplackett 31 minutes agorootparentHey. The point is that the USA focussed on that. At the expense of keeping basic factories running making mortars and bullets - something Russia has not done. They still have all their soviet era factory capacity running. And it’s those basic things that turn out to matter a lot in a long multi year war. reply iainmerrick 13 hours agoprevI’ve always loved how this bit of super-science is utterly obsolete... The Analyzer contained just short of a million vacuum tubes and needed a team of five hundred technicians to maintain and operate it ...and yet it in no way invalidates the point of the story, which hits home just as hard as ever. reply codethatwerks 11 hours agoparentVacuum Tube -> A100 cluster Technician -> MLOps Engineers reply javajosh 8 hours agoprevNowhere is the zero-sum nature of research vs materiel clearer than in real-time strategy games. The fundamental choice is actually three-fold: you can spend your economy on growing the economy, researching better units or building units. It's like choosing a constant in front of a first- versus second-order DE. Of course the choices vary over time, and the curve is complicated and interesting, at least in well-made games. reply lowbloodsugar 13 hours agoprevThe flaw exhibited by the losing side was \"retrofitting\" effective and functional ships in the middle of a war. A similar attitude happens that I've seen countless times in live applications where decisions were made to allow the existing system to stagnate because a new shiny rewrite was going to come online \"by the end of the year\". Four years later, the new system isn't ready, and the \"old\" one (i.e. the one in production) is hitting scaling cliffs because it wasn't kept up to date. But I suspect this post was in response to the \"it's obsolete\" comments on the F35 post earlier today, with lots of calls for work on drones. The warning in Clarke's story is not that we shouldn't build new tech, however. We should absolutely build new tech, and have skunk works and the like. But we shouldn't stop building the existing tech as long as it is effective in battle. The concern comes when we don't have the capacity to build \"what's next\" because the most important piece is made in a little island off the coast of another continent full of potential adversaries. reply pixl97 12 hours agoparentAny way we go about it, the unfortunate fact is hindsight is the only way to tell what the right choice was. In some cases the old system is outclassed so badly attempting to keep using it would be a fatal mistake (cavalry charges at machine gun nests). Other times a new system can be way better, but the rate of production is so far behind the old model you can't make enough fast enough to make a difference. And then other times the new systems have such terrible weaknesses they can be a risk to the entire nation state. reply marcosdumay 11 hours agorootparentYou don't stop an old system before proving the new one in practice, and you don't disrupt logistic channels, ever. Those don't take any hindsight. reply idiotsecant 14 hours agoprevLike all good scifi stories in general and ACC stories in particular, this is a story about A which is actually a story about B. There is wisdom here that is directly applicable to the work of the average HN denizen. Boring beats cool almost every time. reply doctor_eval 7 hours agoprevThis is a story about perfect being the enemy of good! reply Khelavaster 14 hours agoprevAn excellent classic reply LargoLasskhyfv 14 hours agoprevspooky reply mitchbob 14 hours agoprev(1951) https://web.archive.org/web/20230217152507/https://metallicm... reply zabzonk 12 hours agoprevI remember reading a saying that the Germans fought WW2 with the weapons of the 1950s, while the Allies fought it with the weapons of the 1930s. Guess who won. reply nicbou 6 hours agoparentThe Luftwaffe towed their jets with horses. Don't judge an army by the what-could-have-beens from their R&D department. reply zhengiszen 5 hours agoparentprevWithout nearly 30 millions Russian dead in WW2 they would never won reply actionfromafar 12 hours agoparentprevThe Germans also used the most horses of all belligerents and soldiers walked. reply aj7 9 hours agoparentprevThe B-29. Nukes? Are you kidding me? reply ioreofi39090 14 hours agoprev [2 more] [flagged] narrator 14 hours agoparent [–] This is a propaganda story. They just use generic 4G modems and SIM cards. Western processors, FPGAs etc. https://en.wikipedia.org/wiki/HESA_Shahed_136 Chips from 1983 https://en.wikipedia.org/wiki/TMS320 \"The Russian-manufactured Geran-2 is believed to have a \"state-of-art antenna interference suppression\" system that suppresses jamming of the GNSS position signal, designed by Iran using seven transceivers for input and an FPGA and three microcontrollers to analyse and suppress any electronic warfare emissions.[39]\" reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Congjing Yu explores dark parodies of traditional children's books, crediting Mantid rather than Domain as the source of inspiration."
    ],
    "commentSummary": [
      "The discussion underscores the tendency to underestimate potential downsides of technological progress and stresses the significance of balancing innovation with practicality.",
      "Historical instances of military tech challenges and the necessity for careful planning and backing in conflicts, like the situation in Ukraine, are brought to attention.",
      "Evaluating new systems before completely phasing out old ones is advocated, showcasing the importance of reflective decision-making and maintaining equilibrium between different approaches."
    ],
    "points": 136,
    "commentCount": 56,
    "retryCount": 0,
    "time": 1710702590
  },
  {
    "id": 39736718,
    "title": "Improving Image Compression with Neural Networks",
    "originLink": "https://mlumiste.com/technical/compression-deep-learning/",
    "originBody": "Compressing images with neural networks March 17, 2024 14 minute read To see a World in a Grain of Sand And a Heaven in a Wild Flower Hold Infinity in the palm of your hand And Eternity in an hour William Blake IntroductionPermalink Image and especially video compression is a problem of obvious real life significance, with video traffic taking up over 60% of all internet traffic. Increasingly better lossless and lossy codecs have been developed over the years to combat this problem. Relatively recently, a nascent research domain on using auto-encoder style neural networks for compression has emerged. This post goes through the evolution and highlights where the field might be heading. JPEGPermalink To understand neural compression, we first need to understand the ubiquitous JPEG standard which has been around since 1992. Lossless codecs exploit statistical redundancy in data: if you can predict the next byte conditional on the previous ones, then using the same statistical model in both encoder and decoder sides increases compression ratio by assigning smaller codes to more likely bytes. Lossy codecs go further by disregarding some “less important” data entirely. There are many great tutorials on JPEG online, e.g. this, but the most important takeaway is that human eyes are much more receptive to low frequency changes such as edges than to high frequency changes such as elaborate textures. Artists exploit this by creating an “illusion of detail” instead of actually drawing every individual blade of grass. Understanding frequency in images JPEG exploits this by transforming the image data to the frequency domain using the discrete cosine transform, expressing each 8x8 pixel block of an image as a sum of cosine functions with different frequencies and amplitudes. Instead of $H \\times W$ spatial pixels, we are left with $H \\times W$ DCT coefficients, where each 8x8 block of coefficients corresponds to a block of the image. Next, the 8x8 block of values is element-wise divided by an 8x8 quantisation table, and the results are rounded (this is what makes JPEG lossy). Intuitively, whenever you lower the quality setting when saving a JPEG file, this increases the values of the quantisation table, causing more zeros in the final representation of the image, making it easier to compress. It’s important to note that the values in the quantisation table are not uniform, nor random, they are carefully hand-picked to provide pleasing visual results. In fact, there is nothing special about these tables insofar as any software or user can change their values for desired effects. The resulting quantised frequency values are the compressed representation of the image. In practice, this is further compressed with lossless encoding (all the rounded zeros are redundant), which we’ll skip this time. But what’s important to note is that DCT and quantisation are reversible operations, so we can do the inverse of them in the decoder stage, resulting in a lossy reconstruction of the original image. High level diagram of JPEG internals The above slide from Johannes Ballé’s 2018 talk summarises JPEG on a high level. It’s striking how closely this resembles a neural network diagram - we have a natural loss function and two (linear) transformations and their inverses. A natural question is whether such a system could be end-to-end trained by parametrising the transforms such as this: High level diagram of learned image codec internals Autoencoders galorePermalink So why can’t we just train any autoencoder to minimise MSE and be done with? By projecting the image data to some low-dimensional bottleneck, we could use that directly as the encoded representation, right? Bit embeddingsPermalink This is pretty much what the first iterations tried to do. For example, in [7] the authors moreover constrain the bottleneck to be a binary representation. This has the additional benefit that the vector can be directly serialised and bitrate can be estimated directly from the bottleneck size. Progressive bitwise encoding with LSTMs, [7] Here, the autoencoder recursively generates a 4-bit (keep in mind the authors were compressing tiny 32x32 images!) representation of the image, with the residual of that reconstruction being fed into the next layer etc. By repeating this e.g. 16 times, we could generate a 64-bit vector to be transmitted to the receiver. This means the method is also one of the first to have adaptive rate control, which is a desireable property for a codec. Unfortunately, it does not achieve the best compression ratios in practice. That’s because actual compression rates mostly depend on the Shannon entropy of the source file. Since this autoencoder has no incentive to reduce the entropy of the latent vector, we might not be compressing optimally in terms of the bitrate-distortion trade-off. End-to-end trainingPermalink A seminal breakthrough came in 2017 when [1] designed the first end-to-end trained model that directly optimises for the rate-distortion trade-off. Their key insights were the following: Using a loss of the form $L = R + \\lambda D $, where R stands for bitrate and D for distortion (e.g. MSE), achieves an optimal trade-off between the two (for a given value of $\\lambda$ moderating the weights). To estimate $ R $ by a neural network, we can use the fact that actual compression rates can be very close to actual entropy, so we only need to train a probabilistic model of our quantised latent variable $ \\hat{y} = round(y) $ and the entropy of it gives the bitrate directly: $ L = -\\mathbf{E}[\\log_2 P_{\\hat{y}}] + \\lambda \\mathbf{E}[D]$ where $P_{\\hat{y}}$ is the probability distribution of $\\hat{y}$. Note that both terms in the loss depend on the probability model, as $\\hat{y}$ is also used for decoding the image. A specific type of lossless coding - adaptive arithmetic coding - is used to actually compress $ \\hat{y} $ into a bitstream. Without going into too much detail, the key insight is that both the arithmetic encoder and decoder depend on the probability distribution of symbols (a symbol here is a possible value of $\\hat{y}$). This new model achieved better compression ratios than JPEG or even JPEG2000. HyperpriorPermalink Once the $ R + \\lambda D $ loss became prevalent, it became clear that the best way improve compression rates is to improve the entropy (probability) estimation. Another way to look at this is through the arithmetic coder: imagine we were encoding English text. For the hackneyed example input “a quick brown fox jumps o”, if we use a static probability model, then the most likely next character is always space. However, if we had access to some GPT-style context-aware next character predictor, it could condition on the already available information and achieve far better compression ratios. Note that both the encoder and decoder would need to use the same model. [2] does something similar. When the previous probability model was fixed (essentially reflecting the expected distribution of latents over all training set images), now we emit an additional quantised channel $\\hat{z}$ that is also sent as bitstream. The receiving end then first decodes $ \\hat{z} $, reconstructs the distributional parameters of $ y $ from it (in practice, each element $y_i$ is modelled as a Gaussian with), and then decodes $ \\hat{y} $ conditional on these parameters, that means the probability model becomes image-specific. Seminal hyperprior network, [2]. In the above, GDN is just some alternative non-linearity, ReLU could be used instead I have played around a bit with the hyperprior model, using the popular CompressAI library - you can find a notebook here. Using a toy dataset of 100k 96x96 images (STL-10), I’m training a tiny toy model at different $ \\lambda $ values and logging the average entropy and MSE loss for these. While this does is not yet enough to beat JPEG, it showcases how the rate-distortion training happens in practice. Here are some fun images of a ram progressively improving during the training process: Reconstructing a ram As expected, a lower value of $ \\lambda $ incentivises the model to focus on compression, while the higher one gives us a rather realistic reconstruction of the original image (top). Autoregressive priorPermalink The hyperprior showed that feeding additional information to the arithmetic coder’s probability model improves compression rates, namely sending image-specific $ \\hat{z} $. But there is other information we are not using. For example, let’s say we are in the process of decoding $\\hat{y}$ pixel-by-pixel. After the first half, it becomes clear that we have half of a dog. It is then very likely that the remaining half is the latter of the dog. We can update the probability model with this information - as long as both the encoder and decoder side do this kind of synchronised sequential processing, we can achieve a lower compression ratio. This is pretty much the idea behind the autoregressive prior: instead of decoding all latents in one or two passes (like the hyperprior), we recursively predict the probability distribution of the next latent conditional on the previous ones. The arithmetic decoder can adapt in each iteration to optimally pack/unpack a single latent. In practice, instead of using all the previous latents, only a local neighbourhood (e.g. 5x5) of the pixel is used, which provides 12 pixels when serially processed: Autoregressive image decoding [8] This method added another step size improvement to learned image compression. Unfortunately, as you might be able to tell, iterating over the entire image pixel by pixel is really ineffective computationally. I recall seeing some papers where it was claimed that more than 95% of the decoding time was spent in the autoregressive model! This made the method mostly one of academic interest, used in competitions where only rate-distortion performance is measured, without computational constraints. Further work has tried to optimise this obvious bottleneck. For example, using something like the checkerboard context model can speed up decoding more than 40 times, without much of a drop in performance: Checkerboard context for arithmetic decoding [4] Variable rate controlPermalink A glaring weakness of models trained for the $ R + \\lambda D $ loss is that we need to train a separate model for each point of the rate-distortion curve. That would mean that if you are streaming video, the service would need to have N different models in storage ready to store, depending on your (dynamic) bandwidth - which is absurd. Traditional codecs can adapt rate on the fly, like the quality setting in JPEG. Luckily, there’s a nifty little trick to support variable rate within one model. It’s fairly obvious that to support this, we should use variable values of $ \\lambda $ during training, but these should be coupled with some input feature that tells the model which rate-distortion regime we are in. In [3], this is done quite elegantly by introducing a “gain” unit and its inverse. This is a learnable embedding-like vector that multiplies the encoded latent $y$ before it goes into quantisation - decoding performs the opposite. Learned variable rate control [3] If we recall JPEG, this is very similar to using different quantisation tables for different desired quality values, except it has the added benefit that the gain and inverse gain coefficients are trained end to end. During training, we can discretise the range of $\\lambda$ and gain vectors, say to 100 values. Each sample gets assigned a uniformly random index in 100, and uses the corresponding gain vectors and $\\lambda$ coefficient. In inference, we can then vary which of the 100 gain vectors we wish to use, and due to the way they have been trained, we effictively have a 1-100 quality index like JPEG! Perceptual lossesPermalink In image reconstruction, pixel-wise MSE is usually used as the first simple loss function. However, this causes unwanted artifacts at low bitrates: let’s say you have a perfect reconstruction of the image but it’s shifted 5 pixels to the right. MSE would incentivise the model to prefer a blended nothingness over such slightly misaligned reconstructions. This would cause the model to blend out fine textures at low bitrates. Due to this and other issues, the MSE-based PSNR loss metric does not achieve high correlation to actual golden truth subjective test scores, unlike other more advanced metrics such as VMAF. A fun fact is that VMAF has won an Emmy award for technology and engineering :) What are some of the more perceptual loss functions that have been used? One fairly simple improvement is weighting MSE by a saliency map of the input: it’s reasonable to assume that viewers care more about the reconstruction quality of human faces compared to minute background details. Moreover, the pixel-wise L1 loss and Charbonnier loss have been tried to reduce blurring artifacts caused by MSE. Since deep neural networks extract semantic information from images during their training process, it’s possible to pass through images through a pretrained network, extract deep feature activations and compare their distances to get a semantic similarity measure. This is what the PIPS loss does, originally using the VGG network for extracting deep features. PIPS feature similarity [9] A similar alternative to PIPS loss is the style loss borrowed from the style transfer literature. It similarly extracts intermediate layer activations from a pretrained network such as VGG, but the metric is calculated in a somewhat different way, focusing on global statistics of the feature maps, meant to measure differences in style. In [2], the authors show that image compression methods can be thought of as generative variational autoencoders as $ \\lambda \\to \\infty$. It is therefore not surprising that many methods that have been useful in that field, have also been adopted in image compression. Another NeurIPS paper [5] showed that using a classic image generation method, a GAN discriminator, is useful for training the image compression model. In fact, based on human evaluations, a model trained for joint MSE, LPIPS and GAN loss was deemed better than BPG (a then state of the art image compressor based on the HEVC video codec) at half the bitrate. HiFiC [5] vs BPG at similar file size ChallengesPermalink Comparison of image codecs, CompressAI The above image from CompressAI shows the progress of various learned image codecs against their traditional benchmarks, such as VTM, AV1, BPG. While these codecs are already somewhat outdated, it’s visible that generally the best learned and traditional codecs are neck and neck. This is confirmed by the CLIC compression challenge where especially in the video track, winning submissions are often hybrid approaches where a traditional codec is combined with a neural post-processing layer (in-loop filter). The efficacy of such ensemble methods shows that neural methods do not dominate sufficiently to make traditional codecs obsolote. In fact, it seems that in the recent CLIC2024 competition, both the image and video track winners used the state of the art ECM codec. But more than accuracy, the main concern of ML codecs is their computational cost, highlighted best by this concluding slide of the CLIC2022 challenge: Challenges in incorporating ML in a mainstream nextgen video codec We see the difference between mainstream codecs and learned ones can be up to two orders of magnitude in the number of calculations. In the CLIC challenge, it’s common to see neural codecs have 10x longer decoding times, even if their inference can be run on a GPU. Due to this, it might mean that at least for the time being, more lightweight hybrid neural approaches are the best way to enhance image and video compression. Although, it also seems likely that in the long run due to the bitter lesson effects, neural codecs which are significantly simpler conceptually (JVET codec documentation can easily span hunderds of pages :)) and meant to run on generic neural hardware (which is becoming more common) will prevail. At least it seems safe to predict that a good compression system will have at least some learned components in the future. ReferencesPermalink [1] Ballé, Johannes, et al. “Variational image compression with a scale hyperprior.” arXiv preprint arXiv:1802.01436 (2018). [2] Ballé, Johannes, Valero Laparra, and Eero P. Simoncelli. “End-to-end optimized image compression.” arXiv preprint arXiv:1611.01704 (2016). [3] Cui, Ze, et al. “Asymmetric gained deep image compression with continuous rate adaptation.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021. [4] He, Dailan, et al. “Checkerboard context model for efficient learned image compression.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021. [5] Mentzer, Fabian, et al. “High-fidelity generative image compression.” Advances in Neural Information Processing Systems 33 (2020): 11913-11924. [6] Minnen, David, Johannes Ballé, and George D. Toderici. “Joint autoregressive and hierarchical priors for learned image compression.” Advances in neural information processing systems 31 (2018). [7] Toderici, George, et al. “Variable rate image compression with recurrent neural networks.” arXiv preprint arXiv:1511.06085 (2015). [8] Van den Oord, Aaron, et al. “Conditional image generation with pixelcnn decoders.” Advances in neural information processing systems 29 (2016). [9] Zhang, Richard, et al. “The unreasonable effectiveness of deep features as a perceptual metric.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2018. Categories: technical Updated: March 17, 2024 Previous Next",
    "commentLink": "https://news.ycombinator.com/item?id=39736718",
    "commentBody": "Compressing Images with Neural Networks (mlumiste.com)134 points by skandium 15 hours agohidepastfavorite47 comments StiffFreeze9 8 hours agoHow badly will its lossy-ness change critical things? In 2013, there were Xerox copiers with aggressive compression that changed numbers, https://www.theregister.com/2013/08/06/xerox_copier_flaw_mea... reply _kb 1 hour agoparentThe suitable lossy-ness (of any compression method) is entirely dependant on context. There is no one size fits all approach for all uses cases. One key item with emerging 'AI compression' techniques is the information loss is not deterministic which somewhat complicates assessing suitability. reply bluedino 7 hours agoparentprevIf I zoom all the way with my iPhone, the camera-assisting intelligence will mess up numbers too reply qrian 6 hours agorootparentThe mentioned Xerox copier incident was not an OCR failure, but the copier actively changed the numbers in the original image due to its image compression algorithm. reply barfbagginus 5 hours agorootparentHere's some of the context: www.dkriesel.com/blog/2013/0810_xerox_investigating_latest_mangling_test_findings Learn More: https://www.dkriesel.com/start?do=search&id=en%3Aperson&q=Xe... Brief: Xerox machines used template matching to recycle the scanned images of individual digits that recur in the document. In 2013, Kriesel discovered this procedure was faulty. Rationale: This method can create smaller PDFs, advantageous for customers that scan and archive numerical documents. Prior art: https://link.springer.com/chapter/10.1007/3-540-19036-8_22 Tech Problem: Xerox's template matching procedure was not reliable, sometimes \"papering over\" a digit with the wrong digit! PR Problem: Xerox press releases initially claimed this issue did not happen in the factory default mode. Kriesel demonstrated this was not true, by replicating the issue in all of the factory default compression modes including the \"normal\" mode. He gave a 2015 FrOSCon talk, \"Lies, damned lies and scans\". Interesting work! reply lifthrasiir 5 hours agorootparentprevAny lossy compressor changes the original image for better compression at expense of the perfect accuracy. reply skandium 41 minutes agorootparentExactly, in practice the alternatives are either blocky artifacts (JPEG and most other traditional codecs), blurring everything (learned codecs optimised for MSE) or \"hallucinating\" patterns when using models like GANs. However, in practice even the generative side of compression models is evaluated against the original image rather than only output quality, so the outputs tend to be passable. To see what a lossy generator hallucinating patterns means in practice, I recommend viewing HiFiC vs original here: https://hific.github.io/ reply lifthrasiir 5 hours agoparentprevThis JBIG2 \"myth\" is too widespread. It is true that Xerox's algorithm mangled some numbers in its JBIG2 output, but it is not an inherent flaw of JBIG2 to start, and Xerox's encoder misbehaved almost exclusively for lower dpis---300dpi or more was barely affected. Other artifacts at lower resolution can exhibit similar mangling as well (specifics would of course vary), and this or similar incident wasn't repeated so far. So I don't feel it is even a worthy concern at this point. reply thrdbndndn 3 hours agorootparent1. No one, at least not OP, ever said it's a inherent flaw of JBIG2. The fact it's an implementation error on XeroX's end is a good technical detail to know, but it is irrelevant to the topic. 2. \"Lower DPI\" is extremely common if your definition for that is 300dpi. At my company, all the text document are scanned at 200dpi by default. And 150dpi or even lower is perfectly readable if you don't use ridiculous compression ratios. > Other artifacts at lower resolution can exhibit similar mangling as well (specifics would of course vary) Majority of traditional compressions would make text unreadable when compression is too high or the source material is too low-resolution. They don't substitute one number for another in an \"unambiguous\" way (i.e. it clearly shows a wrong number instead of just a blurry blob that could be both). The \"specifics\" here is exactly what the whole topic is focus on, so you can't really gloss over it. reply lifthrasiir 1 hour agorootparent> 1. No one, at least not OP, ever said it's a inherent flaw of JBIG2. The fact it's an implementation error on XeroX's end is a good technical detail to know, but it is irrelevant to the topic. It is relevant only when you assume that lossy compression has no way to control or even know of such critical changes. In reality most lossy compression algorithms use a rate-distortion optimization, which is only possible when you have some idea about \"distortion\" in the first place. Given that the error rarely occurred in higher dpis, its cause should have been either a miscalculation of distortion or a misconfiguration of distortion thresholds for patching. In any case, a correct implementation should be able to do the correct thing. It would have been much problematic if similar cases were repeated, since it would mean that it is much harder to write a correct implementation than expected, but that didn't happen. > Majority of traditional compressions would make text unreadable when compression is too high or the source material is too low-resolution. They don't substitute one number for another in an \"unambiguous\" way (i.e. it clearly shows a wrong number instead of just a blurry blob that could be both). Traditional compressions simply didn't have much computational power to do so. The \"blurry blob\" is something with lower-frequency components only by definition, and you have only a small number of them, so they were easier to preserve even with limited resources. But if you have and recognize a similar enough pattern, it should be exploited for further compression. Motion compensation in video codecs were already doing a similar thing, and either a filtering or intelligent quantization that preserves higher-frequency components would be able to do so too. ---- > 2. \"Lower DPI\" is extremely common if your definition for that is 300dpi. At my company, all the text document are scanned at 200dpi by default. And 150dpi or even lower is perfectly readable if you don't use ridiculous compression ratios. I admit I have generalized too much, but the choice of scan resolution is highly specific to contents, font sizes and even writing systems. If you and your company can cope with lower DPIs, that's good for you, but I believe 300 dpi is indeed the safe minimum. reply Dwedit 9 hours agoprevThere was an earlier article (Sep 20, 2022) about using the Stable Diffusion VAE to perform image compression. Uses the VAE to change from pixel space to latent space, dithers the latent space down to 256 colors, then when it's time to decompress it, it de-noises that. https://pub.towardsai.net/stable-diffusion-based-image-compr... HN discussion: https://news.ycombinator.com/item?id=32907494 reply dheera 3 hours agoparentI've done a bunch of experiments on my own on the Stable Diffusion VAE. Even when going down to 4-6 bits per latent space pixel the results are surprisingly good. It's also interesting what happens if you ablate individual channels; ablating channel 0 results in faithful color but shitty edges, ablating channel 2 results in shitty color but good edges, etc. The one thing it fails catastrophically on though is small text in images. The Stable Diffusion VAE is not designed to represent text faithfully. (It's possible to train a VAE that does slightly better at this, though.) reply rottc0dd 6 hours agoprevSomething similar by Fabrice Bellard: https://bellard.org/nncp/ reply skandium 37 minutes agoparentIf you look at the winners of the Hutter prize, or especially the Large Text Compression Benchmark, then almost every approach uses some kind of machine learning approach for the adaptive probability model and then either arithmetic coding or rANS to losslessly encode it. This is intuitive, as the competition organisers say: compression is prediction. reply p0w3n3d 1 hour agoparentprevSome people are fans of Metallica or Taylor Swift. I think Fabrice Bellard should get the same attention! reply p0w3n3d 1 hour agorootparentAnd the same money for performance, of course reply jfdi 11 hours agoprevAnyone know of open models useful (and good quality) for going the other way? I.e., Input is a 800x600 jpg and output is 4k version. reply davidbarker 9 hours agoparentMagnific.ai (https://magnific.ai) is a paid tool that works well, but it is expensive. However, this weekend someone released an open-source version which has a similar output. (https://replicate.com/philipp1337x/clarity-upscaler) I'd recommend trying it. It takes a few tries to get the correct input parameters, and I've noticed anything approaching 4× scale tends to add unwanted hallucinations. For example, I had a picture of a bear I made with Midjourney. At a scale of 2×, it looked great. At a scale of 4×, it adds bear faces into the fur. It also tends to turn human faces into completely different people if they start too small. When it works, though, it really works. The detail it adds can be incredibly realistic. Example bear images: 1. The original from Midjourney: https://i.imgur.com/HNlofCw.jpeg 2. Upscaled 2×: https://i.imgur.com/wvcG6j3.jpeg 3. Upscaled 4×: https://i.imgur.com/Et9Gfgj.jpeg ---------- The same person also released a lower-level version with more parameters to tinker with. (https://replicate.com/philipp1337x/multidiffusion-upscaler) reply aspyct 2 hours agorootparentThat magnific.ai thingy is taking a lot of liberty on the images, and denaturing it. Their example with the cake is the most obvious. To me, the original image shows a delicious cake, and the modified one shows a cake that I would rather not eat... reply hug 1 hour agorootparentEvery single one of their before & after photos looks worse in the after. The cartoons & illustrations lose all of their gradations in feeling & tone with every outline a harsh edge. The landscapes lose any sense of lushness and atmosphere, instead taking a high-clarity HDR look. Faces have blemishes inserted the original actor never had. Fruit is replaced with wax imitation. As an artist, I would never run any of my art through anything like this. reply quaintdev 6 hours agorootparentprevHere's free and open source alternative that works pretty well https://www.upscayl.org/ reply godelski 10 hours agoparentprevLook for SuperResolution. These models will typically come as a GAN, Normalizing Flow (or Score, NODE), or more recently Diffusion (or SNODE) (or some combination!). The one you want will depend on your computational resources, how lossy you are willing to be, and your image domain (if you're unwilling to tune). Real time (>60fps) is typically going to be a GAN or flow. Make sure to test the models before you deploy. Nothing will be lossless doing superresolution but flows can get you lossless in compression. reply sitkack 9 hours agorootparentOr else you get Ryan Gosling https://news.ycombinator.com/item?id=24196650 reply hansvm 10 hours agoparentprevI haven't explored the current SOTA recently, but super-resolution has been pretty good for a lot of tasks for few years at least. Probably just start with hugging-face [0] and try a few out, especially diffusion-based models. [0] https://huggingface.co/docs/diffusers/api/pipelines/stable_d... reply lsb 10 hours agoparentprevYou’re looking for what’s called upscaling, like with Stable Diffusion: https://huggingface.co/stabilityai/stable-diffusion-x4-upsca... reply cuuupid 10 hours agoparentprevThere are a bunch of great upscaler models although they tend to hallucinate a bit, I personally use magic-image-refiner: https://replicate.com/collections/super-resolution reply jfdi 7 hours agoparentprevthank you! will enjoy reviewing each of these reply esafak 14 hours agoprevIt is not going to take off if it is not significantly better, and has browser support. WebP took off thanks to Chrome, while JPEG2000 floundered. If not native browser support, maybe the codec could be shipped by WASM or something? The interesting diagram to me is the last one, for computational cost, which shows the 10x penalty of the ML-based codecs. reply geor9e 11 hours agoparentThe thing about ML models is the penalty is a function of parameters and precision. It sounds like the researchers cranked them to max to try to get the very best compression. Maybe later they will take that same model, and flatten layers and quantize the weights to can get it running 100x faster and see how well it still compresses. I feel like neural networks have a lot of potential in compression. Their whole job is finding patterns. reply dylan604 13 hours agoparentprevDid JPEG2000 really flounder? If your concept of it being a consumer facing product as a direct replacement for JPEG, then I could see being unsuccessful in that respect. However, JPEG2000 has found its place in the professional side of things. reply esafak 13 hours agorootparentYes, I do mean broad- rather than niche adoption. I myself used J2K to archive film scans. One problem is that without broad adoption, support even in niche cases is precarious; the ecosystem is smaller. That makes the codec not safe for archiving, only for distribution. The strongest use case I see for this is streaming video, where the demand for compression is highest. reply userbinator 6 hours agorootparentThat makes the codec not safe for archiving, only for distribution. Could you explain what you mean by \"not safe for archiving\"? The standard is published and there are multiple implementations, some of which are open-source. There is no danger of it being a proprietary format with no publicly available specification. reply dylan604 5 hours agorootparentNot the GP, but for archiving, you want to know that you'll be able to decode the files well into the future. If you adopt a format that's not well accepted and the code base gets dropped and not maintained so that in the future it is no longer able to be run on modern gear, your archive is worthless. As a counter, J2K has been well established by the professional market even if your mom doesn't know anything about what it is. It has been standardized by the ISO, so it's not something that will be forgotten about. It's a good tool for the right job. It's also true that not all jobs will be the right ones for that tool reply esafak 4 hours agorootparentprevI was not thinking of J2K as being problematic for archiving but these new neural codecs. My point being that performance is only one of the criteria used to evaluate a codec. reply sitkack 9 hours agorootparentprevFor archiving, I'd recommend having a wasm decompressor along with some reference output. Could also ship an image viewer as an html file with all the code embedded. reply dylan604 7 hours agorootparentWhy the need for all things to be browser based? Why introduce the performance hit for something that brings no compelling justification? What problem is this solution solving? Why can't things just be native workflows and not be shoveled into a browser? reply benreesman 5 hours agorootparentNot the parent but one imagines that WASM could be a good target for decompressing or otherwise decoding less-adopted formats/protocols because WASM is fairly broadly-adopted and seems to be at least holding steady if not growing as an executable format: it seems unlikely that WASM disappears in the foreseeable future. Truly standard ANSI C along with a number of other implementation strategies (LLVM IR seems unlikely to be going anywhere) seem just as durable as WASM if not more, but there are applications where you might not want to need a C toolchain and WASM can be a fit there. One example is IIUC some of the blockchain folks use WASM to do simultaneous rollout of iterations to consensus logic in distributed systems: everyone has to upgrade at the same time to stay part of the network. reply actionfromafar 11 hours agorootparentprevHuh, one more point for considering J2K for film scan archiving. reply dylan604 10 hours agorootparentit's well past the considering stage. J2K is used more than people think even if we're not using to spread cat memes across the interwebs. J2K is used in DCPs sent to movie theaters for digital projections. J2K is used as lossless masters for films. the Library of Congress uses it as well. this isn't even attempting to make an exhaustive list of use, but it's not something being looked into. it's being used every day reply actionfromafar 10 hours agorootparentWell, I meant for me personally. Currently using TIFF. :-) reply dylan604 12 hours agorootparentprevBut that's like saying it's difficult to drive your Formula 1 car to work every day. It's not meant for that, so it's not the car's fault. It's a niche thing built to satisfy the requirements of a niche need. I would suggest this is \"you're holding it wrong\" type of situations that isn't laughable. reply yccs27 11 hours agorootparentThere was absolutely an initiative to make J2K a widespread standard reply dylan604 8 hours agorootparentThere was absolutely an initiative to make Esperanto a widespread language. But neither point has anything to do with how things actually are reply dinkumthinkum 13 hours agoparentprevI think it is an interesting discussion, learning experience (no pun intended). I think this is more of a stop on a research project than a proposal; I could be wrong. reply holoduke 11 hours agoprevHow much vram is needed? And computing power? To open a webpage you soon need 24gb and 2 seconds of 1000 watts energy to uncompress images. Bandwidth is reduced from 2mb to only 20kb. reply amelius 11 hours agoprev [–] How do we know we don't get hands with 16 fingers? reply ogurechny 10 hours agoparent [–] Valid point. Conventional codecs draw things on screen that are not in the original, too, but we are used to low quality images and videos, and learned to ignore the block edges and smudges unconsciously. NN models “recover” much complex and plausible-looking features. It is possible that some future general purpose image compressor would do the same thing to small numbers lossy JBIG2 did. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The article highlights the significance of image and video compression, emphasizing neural networks for compression and the reversible aspects of DCT and quantization in JPEG.",
      "It discusses utilizing autoencoders for compressed image representation and integrating hyperprior models and autoregressive priors to boost compression rates.",
      "The text delves into learned variable rate control, perceptual loss functions, and GAN discriminators in image compression models to advance compression methods through machine learning and neural networks."
    ],
    "commentSummary": [
      "The article delves into employing neural networks for image compression, citing a Xerox copier mishap revealing issues with aggressive compression affecting scanned image numbers through faulty template matching.",
      "It debunks myths surrounding specific compression methods and evaluates how various techniques impact image quality, while also highlighting the role of machine learning in compression and upscaling images.",
      "Emphasizes the significance of utilizing established formats for archiving and speculates on how upcoming advanced technologies could revolutionize image compression."
    ],
    "points": 134,
    "commentCount": 47,
    "retryCount": 0,
    "time": 1710700111
  }
]
