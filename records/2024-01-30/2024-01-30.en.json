[
  {
    "id": 39175873,
    "title": "Creating a C Program using libcurl with --libcurl Option",
    "originLink": "https://everything.curl.dev/libcurl/libcurl",
    "originBody": "--libcurl We actively encourage users to first try out the transfer they want to do with the curl command-line tool, and once it works roughly the way you want it to, you append the --libcurl [filename] option to the command line and run it again. The --libcurl command-line option creates a C program in the provided file name. That C program is an application that uses libcurl to run the transfer you just had the curl command-line tool do. There are some exceptions and it is not always a 100% match, but you might find that it can serve as an excellent inspiration source for what libcurl options you want or can use and what additional arguments to provide to them. If you specify the filename as a single dash, as in --libcurl - you get the program written to stdout instead of a file. As an example, we run a command to get http://example.com: curl http://example.com --libcurl example.c This creates example.c in the current directory, looking similar to this: /********* Sample code generated by the curl command-line tool ********** * All curl_easy_setopt() options are documented at: * https://curl.se/libcurl/c/curl_easy_setopt.html ************************************************************************/ #includeint main(int argc, char *argv[]) { CURLcode ret; CURL *hnd; hnd = curl_easy_init(); curl_easy_setopt(hnd, CURLOPT_URL, \"http://example.com\"); curl_easy_setopt(hnd, CURLOPT_NOPROGRESS, 1L); curl_easy_setopt(hnd, CURLOPT_USERAGENT, \"curl/7.45.0\"); curl_easy_setopt(hnd, CURLOPT_MAXREDIRS, 50L); curl_easy_setopt(hnd, CURLOPT_SSH_KNOWNHOSTS, \"/home/daniel/.ssh/known_hosts\"); curl_easy_setopt(hnd, CURLOPT_TCP_KEEPALIVE, 1L); /* Here is a list of options the curl code used that cannot get generated as source easily. You may select to either not use them or implement them yourself. CURLOPT_WRITEDATA set to a objectpointer CURLOPT_WRITEFUNCTION set to a functionpointer CURLOPT_READDATA set to a objectpointer CURLOPT_READFUNCTION set to a functionpointer CURLOPT_SEEKDATA set to a objectpointer CURLOPT_SEEKFUNCTION set to a functionpointer CURLOPT_ERRORBUFFER set to a objectpointer CURLOPT_STDERR set to a objectpointer CURLOPT_HEADERFUNCTION set to a functionpointer CURLOPT_HEADERDATA set to a objectpointer */ ret = curl_easy_perform(hnd); curl_easy_cleanup(hnd); hnd = NULL; return (int)ret; } /**** End of sample code ****/ Previous API compatibility Next multi-threading Last modified 16d ago",
    "commentLink": "https://news.ycombinator.com/item?id=39175873",
    "commentBody": "--libcurl (curl.dev)653 points by Tomte 20 hours agohidepastfavorite126 comments nindalf 19 hours agoOne pattern I really like is opening the networks tab of the browser, finding the request I'm interested in and \"copying as curl\" - the browser generates the equivalent command in curl. Then I'd use something like https://curlconverter.com/ to convert it into request code of the language I'm using. In a way curl is like the \"intermediate representation\" that we can use to translate into anything. reply gullywhumper 18 hours agoparentFor R users, the httr2 package (a recent rewrite of httr) has a function that translates the copy as curl command to a request: https://httr2.r-lib.org/reference/curl_translate.html reply ekianjo 18 hours agorootparentoh WOW! this is super useful. Thanks for the pointer! reply elaus 19 hours agoparentprevcurlconverter.com looks amazing, instant bookmark – thanks! I also use the browser's 'copy as curl' function quite frequently as it's so convenient, having all auth and encoding headers set to _definitely_ work (instead of messing around with handmade, multi-line curl command lines) reply vdfs 19 hours agorootparentBe aware that online service like this one might log your request which could have sensitive data, I'm not saying it does, but those websites give me the creep reply spdustin 14 hours agorootparentI watched the network logs, and it doesn't seem to transmit anything. Additionally, their privacy info clearly states: > We do not transmit or record the curl commands you enter or what they're converted to. This is a static website (hosted on GitHub Pages) and the conversion happens entirely in your browser using JavaScript. reply lp0_on_fire 11 hours agorootparentPrivacy policies on most websites mean jack when they can be changed at any time for any reason, imo. Not to mention the pattern nowadays is: offer a free service, pay a little lip service to privacy concerns, then the enshittification train comes rolling down the tracks a few years/ months later. Not saying this site is going to go down that path but IMO giving the benefit of the doubt with regard to privacy on the internet is bad practice for 2024. reply judge2020 18 hours agorootparentprevI agree that it's possible, and that the majority of utility websites do use a backend to provide their utility, but it seems curlconverter.com doesn't make any requests to a website to convert and instead does so in javascript. It would be nice if more sites offered themselves as PWAs that worked when you set \"throttling\" to \"offline\" in the dev menu, so that you could ensure that no data is leaving the browser when working with sensitive info. reply adolph 18 hours agorootparentMaybe that would be a nice browser plugin. Something that blocks any further requests. I guess it would work similarly to ad blockers, only once enabled blocks everything. reply dotancohen 12 hours agorootparentJust unplug the cord, or disable the WiFi, for a few seconds. As we are presumably discussing sensitive data, nothing is as certain as the end of the cord in your palm. reply judge2020 12 hours agorootparentAlthough an attack vector is it saving all that info in a buffer, continuously trying to beam it up until it gets a success response from the server. reply diegoperini 17 hours agorootparentprevTrue. Luckily it's open source. You can do `npx curlconverter --language go example.com` behind a firewall after downloading the npm module. reply cloudwalk9 16 hours agorootparentprevI kinda wish the address bar in any browser had an \"advanced\" popout menu that's basically a curl frontend with all of its bells and whistles. Basically move that functionality from the dev tools. reply elaus 13 hours agorootparentSadly most major browsers go the opposite direction, removing more and more \"advanced\" functionality and information from the address bar (e.g. stripping the protocol, 'https://', from the URL) reply LtdJorge 12 hours agorootparentYeah, I hate that. Firefox also removed http when editing in the address bar reply Crespyl 8 hours agorootparentFirefox on Android has recently started to hide the URL entirely when using the search-from-addressbar feature. Instead of the URL of the search page, it shows the search terms, which is redundant since the terms are already shown by the page itself. reply tracker1 19 hours agoparentprevYeah, that has made my life so much easier when troubleshooting an API endpoint. I can tweak the request params to run against a local instance as well as pipe through jq for formatting etc. reply appplication 18 hours agoparentprevThis is such an interesting and true observation. Anytime something isn’t working with an endpoint, first step is “can you get it to work with curl”. reply Mavvie 19 hours agoparentprevThis is pretty interesting. It's not like HTTP needs an intermediate representation, but since cURL is so ubiquitous, it ends up functioning as one. cURL is popular so people write tools that can export requests as cURL, and it's popular so people write tools that can import it. reply everforward 19 hours agorootparentThe benefit of curl over raw HTTP is the ability to strip away things that dont matter. Eg an HTTP request should have a client header, but they're typically not relevant to what you're trying to do. curl is like an HTTP request that specifies only the parts you care about. It's brief, and having access to bash makes it easy to express something like \"set this field to a timestamp\". reply 1vuio0pswjnm7 12 hours agorootparentActually \"Copy as cURL\" adds much that is not required. In some cases this can be useful. However if all one cares about is what is actually needed to succesfully request a resource using HTTP, then \"Copy as cURL\" always includes too much. It includes \"things that dont matter\". HTTP is more flexible than \"Copy as cURL\". There are things that can be done with HTTP that cannot be done with cURL. reply incorrecthorse 18 hours agorootparentprevHTTP requests don't need to have any specific headers, and, if anything, curl will only add ones for you. reply everforward 14 hours agorootparent> if anything, curl will only add ones for you. That's kind of what I mean. E.g. I believe curl will add a Content-Length header, which is good to have, but I don't need every example HTTP call to show me that. To me a curl call is kind of shorthand for \"these are the parts unique to this request, do the appropriate thing for general-use headers\". If I see a raw HTTP request missing a Content-Length header (assuming it could use one), I don't know whether to assume that I do the normal thing, or whether the server ignores Content-Length, or perhaps if the server specifically errors out when it's set. Vice-versa, if a raw HTTP request does have a Content-Length header, I'm not sure if that means it's required or just supported. If I see a curl call specifying Content-Length, it sets off the \"something weird is going on\" bells in my head. Nobody specifies that in curl, so it's presence is odd and worth looking at. reply ricardo81 17 hours agorootparentprevBut specific HTTP requests might. Like cookies, an accept header, or anything. reply tracker1 19 hours agorootparentprevI've used a similar tool as part of API logging, filtering out the signature on the bearer token... It's useful with request id for dealing with error reporting. reply spdustin 15 hours agoparentprevhttps://curlconverter.com/ is a great example of intelligent UX. Whatever browser you're using is shown in the instructions for \"Copy as cURL\". Very clever. See: https://github.com/curlconverter/curlconverter.github.io/blo... reply rpozarickij 16 hours agoparentprevIn curlconverter.com clicking on \"C\" redirects you to the --libcurl option documentation page instead of generating a C snippet. Wouldn't a more user-friendly way be to still generate a C snippet, but to mention that it can be done with the --libcurl option too? reply npteljes 19 hours agoparentprevPostman can also import in \"curl format\", so yeah, the representation works. reply ricardo81 17 hours agoparentprevI've used 'copy as curl' a bunch. Often find I have to append --compressed so the command line will provide the uncompressed output. reply xnx 15 hours agoparentprevGreat Chrome feature! For those who haven't seen it, it also includes copy as PowerShell, fetch, and Node.js commands. reply xwowsersx 20 hours agoprevHaving a flag in the command line interface that spits out the source code of the program doing the same stuff as your command is pretty cool. It's like lifting the hood and showing you what's going on. This not only helps you get a better grip on how things work but also lets you make changes to fit your needs. You can tweak or add stuff based on what you want, making the whole experience user-friendly. It's all about giving users the power to customize things their way. reply OskarS 19 hours agoparentIt's also just great documentation for a programming library. Like, if you're using libcurl and realize you need to do a range request (or whatever), or \"copy as curl\" from browser network tab, you can just do it on the command line and add `--libcurl` and find out exactly how to do that with the C library. It's the bee's knees. reply xwowsersx 18 hours agorootparent100% reply xuhu 19 hours agoparentprevI wish this also worked for Gnome settings, network settings, firewall config GUIs, and anything that can do things using CLI commands. reply INTPenis 14 hours agorootparentWith dbus we're moving in that direction. reply smy1es 20 hours agoprevThis kind of thing was one of the reasons Visual Basic macros for Microsoft Office was so successful. You can perform actions in Word, Excel, watch the macros they produce then customise themselves to your needs afterwards in code. It is a simple and powerful concept, so good to see it in curl. reply jasomill 18 hours agoparentWhile it doesn't appear to have been updated in many years, Microsoft built a similarly useful tool[1] that lets you browse the structure of a given Office document and see C# code that generates various components of it. [1] https://github.com/dotnet/Open-XML-SDK/releases/tag/v2.5 reply actionfromafar 20 hours agoparentprevI was going to say, there has to be a generic pattern for things like this. Also made me think of AREXX on the Amiga. reply sph 19 hours agorootparentThere is, but we need to throw away our outdated current programming model. Think Lisp or Smalltalk. There should not be a separation between program written in some language, operating system and shell [1]. You'd simply run: CURL url: \"https://example.com\" method: 'post in an interactive system, that can either represent your shell, or your application code. We need --libcurl because UNIX is not an interactive environment, so there is an enormous abyss between runtime and compile time. (Syntax in this example from a Smalltalk-like environment I have been designing, should be understandable enough) --- 1: \"An operating system is a collection of things that don't fit in a language. There shouldn't be one.\" — Dan Ingalls, 1981 reply xnorswap 18 hours agorootparentWhat you're looking for exists, it's powershell. Invoke-WebRequest -Method 'Get' -Uri 'https://example.com' And the response is a typed object: That you can pipe properly by piping objects not strings, e.g. : Invoke-WebRequest -Method 'Get' -Uri 'https://example.com'Select-Object -ExpandProperty Headers Outputs: Key Value --- ----- Age 438810 Vary Accept-Encoding X-Cache HIT Content-Length 1256 Cache-Control max-age=604800 Content-Type text/html; charset=UTF-8 Date Mon, 29 Jan 2024 15:08:42 GMT Expires Mon, 05 Feb 2024 15:08:42 GMT ETag \"3147526947+ident\" Last-Modified Thu, 17 Oct 2019 07:18:26 GMT Server ECS (nyb/1D15) reply freedomben 18 hours agorootparentYes yes, power shell is powerful and really good. I wanted to hate it because it seems too verbose and I don't like the mix of capitals and hashes in names. but the APIs they make available from .Net are pretty phenomenal. Extreme verbosity aside, done day I'm have to seriously learn it reply shirogane86x 14 hours agorootparentFor simple interactive usage aliases go a long way, and with tab completion it gets really fast to write. For example, Invoke-WebRequest is iwr, Select-Object I think is just select. Some others are ForEach-Object which is %, and Where-Object can be ? Or where. Also, since PowerShell is case insensitive, you don't really need to use all those capitals - the worst you'll get is a warning from your editor, if it has PowerShell integration (around 99% sure about this) reply xnorswap 1 hour agorootparentIt also has tab completion of parameters, so you can tab complete not just commands but things like -ExpandProperty too reply actionfromafar 18 hours agorootparentprevCan't hate it, but I can't love it either. I wish some other more \"normal\" languages (Ruby? Python?) had a better \"shell\" story and dotnet integration. reply freedomben 2 hours agorootparentFor so long I have dreamed of a ruby-like shell language. At one point I even built a prototype that was a ruby DSL, but I found myself continually having to implement functions and it never seemed to get to a point where I could use it all day and not run into missing functionality that I had to add. Some day I'll return to it, but it will have to be a day when I don't have a real job :-D reply int_19h 6 hours agorootparentprevFor Python, https://sh.readthedocs.io/ is very nice, but unfortunately no Windows support. reply sph 17 hours agorootparentprevCan you copy and paste that snippet as is into a C# program? This is what I meant. Powershell has a high level object model but it doesn't make Windows itself anymore programmable and interactive, as a whole than zsh does for Linux. It is no Lisp Machine. reply supriyo-biswas 19 hours agorootparentprevGiven that we already have the concepts of exported entry points and extern in C, this should already be possible to a certain extent. The only other thing which needs to happen is that ELFs should have the concept of exported data structures, so while \"URL\" might not be a defined structure in the OS, something like curl can provide it. Too bad we're stuck with UNIX/POSIX model - you could even take this idea of exported data structures and have the terminal represent data in the preferred users format instead of having tools like jq. reply int_19h 6 hours agorootparentWith CLR assemblies, you get rich embedded metadata including UDTs like structs and unions, and architecture-portable JIT-compiled bytecode to boot - both sufficient to map the entirety of C data and execution model. reply sph 19 hours agorootparentprevWe have dlopen and we can list exported symbols, but we have no information about a function's arguments, ABI and calling convention, so it's pretty much impossible to turn UNIX into a fully late-bound and interactive REPL. Same issue with syscalls. The only way is starting from scratch, with a novel approach and programming model. reply supriyo-biswas 18 hours agorootparentWhy can’t you add additional data about the data types of parameters in a separate ELF section? It would be only used by programs that look for it like a specially designed shell. The calling convention can be assumed to be the same as used by the OS/arch combination. reply sph 18 hours agorootparentYou could, but you then have to recompile the world with this new information stored as a ELF header or something, and good luck if the library is not written in C (so it has its own conventions, ABI, memory model and binary format) I'm talking about the status quo today, not how you can improve in a perfect world where everybody adopts a better way of doing things. Implementing an half-baked Smalltalk layer on top of UNIX will not turn UNIX into a Smalltalk environment. reply overboard2 19 hours agorootparentprevOr you could add some more sections to object files. Just make clang and/or gcc export function signatures, structs, enums, and typedefs reply actionfromafar 18 hours agorootparentI wonder if one of the existing interpreted languages (python/javascript/ruby/whatever) could maintain a patch for llmv/gcc which did exactly that and in the process make the most incredible seamless integration ever between itself and C (and also C++ now with its ABI stabilizing!) reply Tyr42 19 hours agorootparentprevPowerShell? reply philzook 17 hours agorootparentprevYou might find this work and talks on liballocs by Stephen Kell interesting. The pitch is how to enable smalltalk like reflection for unix as it exists today. https://www.humprog.org/~stephen/#works-in-progress reply sph 1 hour agorootparentI saw his talk on the topic, but his home page has a lot of good reading material. Thanks for sharing! reply cm2187 20 hours agoparentprevOr the excel side by side xaml/UI of the wpf editor in visual studio. reply undecisive 15 hours agoprevTo compile it you'll need to tell it to link to libcurl, e.g. with -lcurl on gcc: curl https://ifconfig.me --libcurl ip_fetcher.c # Output: your ip address, and a file ip_fetcher.c gcc -o ip_fetcher ip_fetcher.c -lcurl # Output: no errors, just a file ip_fetcher ./ip_fetcher # Output: your ip address (I'm sure most people are saying \"no duh\" right now, but I'm probably not the only one on here who doesn't write C code every day!) reply chungy 14 hours agoparentIt might be even easier this way: $ curl https://ifconfig.me --libcurl ip_fetcher.c $ make LDFLAGS=-lcurl ip_fetcher $ ./ip_fetcher (I just find make a heck of a lot easier than running direct cc commands) reply jicea 16 hours agoprevShameless promotion: Hurl [1] is an Open Source cli using libcurl to run to test HTTP requests with plain text! We use libcurl for the super reliability and top features (HTTP/3 for instance) and we've added little features like: - requests chaining, - capturing and passing data from a response to another request, - response tests (JSONPath, XPath, etc...) There is nice syntax sugar for requesting REST/SOAP/GraphQL APIs but, at the core, it's just libcurl! Using verbose option, you can grep the curl commands for instance. (I'm one of the maintainers) [1]: https://hurl.dev reply cassepipe 12 hours agoparentUsed it and really liked it Thanks for your work reply oleg_antonyan 20 hours agoprevI wish something like this was possible with ffmpeg reply r0ckarong 20 hours agoparentThe compiler has been thinking about the answer for a few decades ... reply donatj 20 hours agorootparentBefore I found out that statically compiled ffmpeg was a thing, I was trying to compile ffmpeg on a 2010 Mac. It was a multi-day process and every time like two days in I'd hit some snag. reply a3w 19 hours agorootparentprevNot our problem, we just need to write the feature request ;) But yes, perhaps as a follow-up, we would need to ask for 1. a speed-up of the process or 2. find a way to look up and redistribute existing conversion flows reply d3m0t3p 20 hours agoparentprevThat would be awesome reply bufo 20 hours agoparentprevSeriously!! reply jraph 20 hours agoprevNice. Daniel Stenberg is really careful to details and dev / user experience. This level of polishing is astonishing. reply pbaam 20 hours agoprevI remember this option was mentioned in a 3 hour video[1] where Daniel Stenberg himself went through most of the curl command line options. [1] https://www.youtube.com/watch?v=V5vZWHP-RqU reply pimlottc 17 hours agoprevThe submission title should be two hyphens (--), not an em-dash and a hyphen (—-) reply altairprime 15 hours agoparentEmailing the mods about this would enable them to see your comment and fix it; use the Contact link in the footer. reply pimlottc 11 hours agorootparentReported, thanks! reply Joker_vD 20 hours agoprevIn an enthusiastic tone of an AI enthusiast: Thankfully, now that we have ChatGPT, this feature is obsolete and the curl executable doesn't have to contain half-baked quines in it anymore! reply bayindirh 20 hours agoparentI'd never change --libcurl and gengetopt[0] with some output from some artificial thingy which babbles semi-truths which doesn't understand what it's doing. They are deterministic tools which does what you want in a battle tested way, and will let me sleep well at night, which is an underappreciated feature of mature programs. [0]: https://www.gnu.org/software/gengetopt/gengetopt.html reply avgcorrection 16 hours agorootparentHurray for deterministic tools! Yes, there is no reason to risk using an LLM when you have a code generator that you can trust. reply bayindirh 2 hours agorootparentA code generator needs neither hundreds of GPUs nor an internet access to work. Not having to train a code generator in ethically questionable ways is also a huge plus, too. reply jcul 18 hours agorootparentprevFirst I've heard of gengetopt, thanks! reply bayindirh 2 hours agorootparentYou're welcome! It's a great tool IMHO. reply ijustlovemath 20 hours agoparentprevIt's not a quine though, as it's not producing curl itself reply throwaway_08932 19 hours agorootparentIn Italian, it's common to append 'ino' or 'ina' to something when you want to imply it's a smaller or cuter version of itself. So 'quinino' might work here. reply Biganon 11 hours agorootparentReading this comment gave me a fever... I wish a plant could help me get better... reply bayindirh 19 hours agorootparentprevIf you want something with more rhyme, poco-quine can also work, I guess, pq for short. reply neuromanser 16 hours agorootparentlibpq is taken by PostgreSQL though ;) reply tomtomtom777 20 hours agorootparentprevOne could argue it's a half-baked quine. reply kevindamm 19 hours agorootparentmm parbaked quine, just pop it in the compiler add a main driver and serve with some gengetopt for a delicious webcrawler! reply ukuina 20 hours agoprevWait, this option would give you a compilable C program that would replace the need for scripting around the original curl call!? reply notRobot 20 hours agoparentYes, it has always been possible with libcurl but you had to write the boilerplate code yourself. reply EasyMark 14 hours agoparentprevyou'd probably have to modify it with a few parameters to make it useful for generic web pages and such, I would think of it as more of a base to build on so you don't have to dig through documents for hours reply LoganDark 20 hours agoparentprevThe idea is that you can copy-paste the C code into an existing program or at least use it as a reference to know exactly which libcurl API calls are needed to replicate the curl call. reply pimlottc 17 hours agoprevSomeone should buy libcurl.com and make it return the source code to generate a request to itself. Bonus points for setting the same headers and options as the triggering request. reply LatticeAnimal 13 hours agoparentSadly, it looks like godaddy is charging $5000 for the domain. Otherwise that would be a fun project. reply zokier 20 hours agoprevOne convenient thing in browser web developer tools is the ability to copy requests from network tab as either curl commands or even as javascript code. I love to see more this sort of things! reply dewey 20 hours agoparentAnd was curl is such a \"standard\" to represent a request there's also many tools converting that curl output into native code (Like Go) already, which makes it very fast to reproduce something without manually having to set all the flags. I'm always happy this feature exists without even needing third party extensions in the browser. reply p4bl0 19 hours agoprevWhat an awesome idea! That's kind of like a library documentation but alive. Very cool. reply oaiey 19 hours agoprevNot sure if I like patching more and more parameters into the executable for gimmicks like this. Would be also work when you exchange the executable name instead of addding a parameter. Like ... curl-as-librucl https://www.example.com reply eichin 16 hours agoparent> Added in 7.16.1. > Jan 29 2007 so this isn't a new trend... reply fweimer 16 hours agoprevI think I saw this first with the ASM bytecode toolkit: https://asm.ow2.io/javadoc/org/objectweb/asm/util/ASMifier.h... Possibly via the Eclipse extension: https://marketplace.eclipse.org/content/bytecode-outline reply Daviey 19 hours agoprevThis is really nice, but a feature that some applications implement that I wish was available in the library is some way of outputting curl command line reference which is equivalent of the requests being made. In-fact, I often find myself, MITM'ing myself using burp-suite and \"copy as cURL command\" feature for exactly this. reply theanonymousone 15 hours agoprevIs there any libcurl wrapping for Java? reply junon 15 hours agoparentOf course there is. reply ahmedfromtunis 19 hours agoprevCan someone give an example use case for this utility? I'm really curious to learn how would you use this? reply fullspectrumdev 18 hours agoprevTried this out, it doesn’t seem to work at all with a good few flags - notably —upload-file reply hhthrowaway1230 20 hours agoprevI love this! Very nice transition for people to move from commandline to the library when they need to! reply EasyMark 14 hours agoprevdang.... i wish I knew that when I was working on my own little hacker news filter program just to practice some old school c >:( reply Too 17 hours agoprevAs if the world needed more unsafe C code connected to the internet. Rust-evangelism aside, I guess one can run the program under ltrace to achieve almost the same result. reply avgcorrection 16 hours agoparent> As if the world needed more unsafe C code connected to the internet. Assuming (as usual) that the code generation is solid because of curl’s reputation: why not trust it? It would be pretty bad if the generator could emit memory-unsafe code. (I don’t know.) reply Too 4 hours agorootparentOh don’t get me wrong. I trust curl and I have no reason not to trust this tool. It’s the consumer of this tool and all the code around it that will get written I don’t trust. reply eichin 16 hours agorootparentprevFor a trivial example, the code just calls curl_easy_init, a bunch of curl_easy_setopt, curl_easy_perform to do the work, and curl_easy_cleanup. (It leaves comments like \"CURLOPT_WRITEDATA set to a objectpointer\" in a comment block on params for which \"You may select to either not use them or implement them yourself\" - that's presumably where you are going to write your own unsafe code :-) reply avgcorrection 14 hours agorootparentAh I see, thanks. reply ricardo81 17 hours agoparentprevlibcurl is used in billions of situations. Fair point about memory allocations in C, but often alt languages rely on other people's code which you'd implicitly trust to do the same thing. So then it becomes an argument of testing and trust. All the same, you trust strangers code or you write your own. reply EasyMark 14 hours agorootparentI suspect OP has already made up their mind on using C at all :) reply ricardo81 13 hours agorootparent:) Stabilisers for some, but definitely not others. reply ranger_danger 15 hours agorootparentprevand often \"your own\" turns out much buggier. reply ricardo81 14 hours agorootparentturtles all the way down, just pick your first turtle. reply rafaelmn 19 hours agoprevcode generator that isn't LLM based - how quaint :) reply ape4 20 hours agoprevUseful idea reply Waterluvian 20 hours agoprevI use the browser equivalent of this all the time to generate javascript code for requests. It’s very cool to see this for C and hopefully other languages, too. reply jari_mustonen 20 hours agoprevPlease also add support for different so we can write —-libcurl example.py reply 0l 20 hours agoparentLibcurl is a c/c++ library... Just use Requests reply npteljes 19 hours agoparentprevTry https://curlconverter.com/ - \"Convert curl commands to Python, JavaScript and more\" reply justinclift 20 hours agoparentprevYou can probably get pretty far with the generated C code, then look up the same curl options in the Python bindings: https://github.com/pycurl/pycurl reply ok123456 19 hours agoparentprevTools already convert a curl command line to a Python requests script. [1] https://github.com/spulec/uncurl reply boffinAudio 20 hours agoparentprevIts quite effective to use ChatGPT to convert C code to Python .. reply a3w 19 hours agorootparent... or \"curl calls to python webscraping\". Although having to look for the right version of libraries with different number names and therefore different feature sets was tedious to do manually, AI might just guess fast and sometimes even right as to which import to use. reply chfritz 17 hours agoprev [–] It says everything about C when you need a code generator for something like this. This is why development in python or node.js is so much faster. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The --libcurl command-line option in curl allows users to create a C program that utilizes libcurl for performing transfers.",
      "The generated program serves as a reference for utilizing libcurl options and adding additional arguments.",
      "While the program code includes various curl_easy_setopt options, some options may require manual implementation."
    ],
    "commentSummary": [
      "The discussion focuses on the advantages of using the curl command compared to raw HTTP and alternatives to online services.",
      "Participants talk about the \"Copy as cURL\" feature and tools for converting cURL commands, as well as viewing and modifying code.",
      "The conversation also covers the use of Invoke-WebRequest in PowerShell, limitations of the UNIX/POSIX model, libcurl capabilities, and the preference for deterministic tools over AI."
    ],
    "points": 653,
    "commentCount": 126,
    "retryCount": 0,
    "time": 1706534360
  },
  {
    "id": 39178886,
    "title": "Meta AI Unveils Code Llama 70B: A More Efficient Code Generator",
    "originLink": "https://twitter.com/AIatMeta/status/1752013879532782075",
    "originBody": "Today we’re releasing Code Llama 70B: a new, more performant version of our LLM for code generation — available under the same license as previous Code Llama models.Download the models ➡ https://t.co/fa7Su5XWDC• CodeLlama-70B• CodeLlama-70B-Python• CodeLlama-70B-Instruct pic.twitter.com/iZc8fapYEZ— AI at Meta (@AIatMeta) January 29, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=39178886",
    "commentBody": "Meta AI releases Code Llama 70B (twitter.com/aiatmeta)526 points by albert_e 16 hours agohidepastfavorite254 comments chrishare 13 hours agoCredit where credit is due, Meta has had a fantastic commitment towards open source ML. You love to see it. reply satvikpendem 0 minutes agoparentMeta has source available licenses, not open source ones. reply joshspankit 11 hours agoparentprevYes but: if the commitment is driven by internal researchers and coders standing firm about making their work open source (a rumour I’ve heard a couple times), most of the credit goes to them. reply mvkel 7 hours agoparentprevWasn't LLaMa originally a leak that they were then forced to spin into an open source contribution? Not to diminish the value of the contribution, but \"commitment\" is an interesting word choice. reply hnfong 3 hours agorootparentIt wasn't a leak in the typical sense. They sent the weights to pretty much everyone who asked nicely. When you send something interesting to thousands of people without vetting their credentials, you'd expect the stuff to get \"leaked\" out eventually (and sooner rather than later). I'd say it's more appropriate to say the weights were \"pirated\" than \"leaked\". That said, you're probably correct that the community that quickly formed around the \"pirated\" weights might have influenced Zuckerberg to decide to make llama2's more freely accessible. reply raxxorraxor 1 hour agorootparentIf the image synthesis is an example, I believe this to be a winning strategy in the long run to get the most competent model. The LLM space just moves a bit slower since the entry requirements for compute power are that much higher and data preparation is maybe a bit more \"dry\" than tagging images. reply simonw 15 hours agoprevHere's the model on Hugging Face: https://huggingface.co/codellama/CodeLlama-70b-hf reply israrkhan 12 hours agoparentI hope someone will soon post a quantized version that I can run on my macbook pro. reply annjose 2 hours agorootparentOllama has released the quantized version. https://ollama.ai/library/codellama:70b https://x.com/ollama/status/1752034686615048367?s=20 Just need to run `ollama run codellama:70b` - pretty fast on macbook. reply LVB 14 hours agoprevI'm not very plugged into how to use these models, but I do love and pay for both ChatGPT and GitHub Copilot. How does one take a model like this (or a smaller version) and leverage it in VS Code? There's a dizzying array of GPT wrapper extensions for VS Code, many of which either seem like kind of junk (10 d/ls, no updates in a year), or just lead to another paid plan, at which point I might as well just keep my GH Copilot. Curious what others are doing here for Copilot-esque code completion without Copilot. reply ado__dev 10 hours agoparentYou can try it with Sourcegraph Cody. https://sourcegraph.com/cody And instructions on how to change the provider to use Ollama w/ whatever model you want: Install and run Ollama - Put ollama in your $PATH. E.g. ln -s ./ollama /usr/local/bin/ollama. - Download Code Llama 70b: ollama pull codellama:70b - Update Cody's VS Code settings to use the unstable-ollama autocomplete provider. - Confirm Cody uses Ollama by looking at the Cody output channel or the autocomplete trace view (in the command palette). - Update the cody settings to use \"codellama:70b\" as the ollama model https://github.com/sourcegraph/cody/pull/2635 reply petercooper 14 hours agoparentprevhttps://continue.dev/ is a good place to start. reply speedgoose 13 hours agorootparentContinue doesn’t support tab completion like Copilot yet. A pull/merge request is being worked on: https://github.com/continuedev/continue/pull/758 reply sestinj 11 hours agorootparentRelease coming later this week! reply jondwillis 13 hours agorootparentprevBonus points for being able to use local models! reply israrkhan 12 hours agorootparentprevThis looks really good.. reply dan_can_code 11 hours agorootparentIt's great. It's super easy to install ollama locally, `ollama run `, change the continue config to point to it, and it just works. It even has an offline option by disabling telemetry. reply madeofpalk 9 hours agorootparent> Windows coming soon ugh, not so easy. reply dan_can_code 3 hours agorootparentYes, that is certainly a downside I forgot to mention. Sorry to get your hopes up. reply sestinj 14 hours agorootparentprevbeat me to the punch : ) reply sestinj 14 hours agoparentprevI’ve been working on continue.dev, which is completely free to use with your own Ollama instance / TogetherAI key, or for a while with ours. Was testing with Codellama-70b this morning and it’s clearly a step up from other OS models reply dan_can_code 11 hours agorootparentHow do you test a 70B model locally? I've tried to query, but the response is super slow. reply sestinj 11 hours agorootparentPersonally I was testing with TogetherAI because I don't have the specs for a local 70b. Using quantized versions helps (Ollama's downloads 4-bit by default, you can get down to 2), but it would still require a higher-end Mac. Highly recommend Together, it runs quite quickly and is $0.9/million tokens reply raxxorraxor 1 hour agoparentprevI use the plugin Twinny in conjunction with ollama to host the models. Easy setup and quite powerful assistance. You need a decent rig though, since you don't want any latency for features like autocomplete. But even if you don't have a faster rig, you can still leverage it for slower tasks to generate docs or tests. Twinny should really be more popular, didn't find a more powerful no-bullshit plugin for VSCode. reply SparkyMcUnicorn 13 hours agoparentprevThere are some projects that let you run a self-hosted Copilot server, then you set a proxy for the official Copilot extension. https://github.com/fauxpilot/fauxpilot https://github.com/danielgross/localpilot reply water-data-dude 12 hours agorootparentWhen I was setting up a local LLM to play with I stood up my own Open AI API compatible server using llama-cpp-python. I installed the Copilot extension and set OverrideProxyUrl in the advanced settings to point to my local server, but CoPilot obstinately refused to let me do anything until I’d signed in to GitHub to prove that I had a subscription. I don’t _believe_ that either of these lets you bypass that restriction (although I’d love to be proven wrong), so if you don’t want to sign up for a subscription you’ll need to use something like Continue. reply siilats 11 hours agorootparentThis plugin just installs in Jetbrains IDE and sets up a local llama.cpp server https://plugins.jetbrains.com/plugin/21056-codegpt reply israrkhan 12 hours agorootparentprevI tried fauxpilot to make it work on my own llama.cpp instance, but didn't work out of the box. Filed a github issue, but did not get any traction. Eventually gave up on it. This was around 5 months ago. Things might have improved by now. reply apapapa 11 hours agoparentprevFree Bard is better than free ChatGPT... Not sure about paid versions reply ignoramous 10 hours agorootparentBard censorship is annoying. One thing I've found (free) Bard to better than the rest at is summarizing book chapters, manuals, and docs. It is also surprisingly good at translation (X to English), as it often adds context to what its translating. With careful prompt engineering, you can get a lot out of free Bard except when its censored. reply apapapa 10 hours agorootparentNot sure why you talk about Bard's censorship and not Chatgpt's but cause in my experiences it is much worst. reply dxxvi 5 hours agorootparentprevI'm learning Rust. It seems to me that Bard is better than OpenAI 4 I use for free at work. reply marinhero 12 hours agoparentprevYou can download it and run it with [this](https://github.com/oobabooga/text-generation-webui). There's an API mode that you could leverage from your VS Code extension. reply cmgriffing 11 hours agoparentprevI've been using Cody by Sourcegraph and liking it so far. https://sourcegraph.com/cody reply turnsout 16 hours agoprevGiven how good some of the smaller code models are (such as Deepseek Coder at 6.7B), I'll be curious to see what this 70B model is capable of! reply ignoramous 16 hours agoparentAlphaCodium is the newest kid on the block that's SoTA pass@5 on coding tasks (authors claim at least 2x better than GPT4): https://github.com/Codium-ai/AlphaCodium As for small models, Microsoft has been making noise with the unreleased WaveCoder-Ultra-6.7b (https://arxiv.org/abs/2312.14187). reply passion__desire 15 hours agorootparentAlphaCodium author says he should have used DSPy https://twitter.com/talrid23/status/1751663363216580857 reply eurekin 15 hours agorootparentprevAre weights available? reply moyix 15 hours agorootparentAlphaCodium is more of a prompt engineering / flow engineering strategy, so it can be used with existing models. reply hackerlight 14 hours agorootparentprevIs this better than GPT4's Grimoire? reply jasonjmcghee 16 hours agoparentprevMy personal experience is that Deepseek far exceeds code llama of the same size, but it was released quite a while ago. reply turnsout 16 hours agorootparentAgreed—I hope Meta studied Deepseek's approach. The idea of a Deepseek Coder at 70B would be exciting. reply whimsicalism 16 hours agorootparentThe “approach” is likely just training on more tokens. reply imjonse 15 hours agorootparentIt's the quality of data and the method of training, not just the amount of tokens (per their paper release a few days ago) reply hackerlight 14 hours agorootparentprevThere's a deepseek coder around 30-35b and it has almost identical performance to the 7b on benchmarks. reply CrypticShift 15 hours agoparentprevPhind [1] uses the larger 34B Model. Still, I'm also curious what they are gonna do with this one. [1] https://news.ycombinator.com/item?id=38088538 reply hackerlight 13 hours agoparentprevSeems worse according to this benchmark: https://huggingface.co/spaces/bigcode/bigcode-models-leaderb... reply pandominium 11 hours agoprevEveryone is mentioning using 4090 and a smaller model, but I rarely see an analysis where the energy consumption is used. I think Copilot is already highly subsidized by Microsoft. Let's say you use Copilot around 30% of your daily work hours. How much kWh does an opensource 7B or 13B model use then in a month on one 4090? EDIT: I think for a 13B at 30% use per day it comes around 30$/no on energy bill. So probably with a even more smaller but capable model can beat the Copilot monthly subscription. reply fennecfoxy 3 minutes agoparentDon't even need to do hard math: compare using co-pilot style LLM (bursts of 100% GPU every wee while) vs gaming on your 4090 (running at 100% for x hours). reply Retric 10 hours agoparentprevSubscription models are generally subsidized by people barely using them. So I wouldn’t be surprised if the average is closer to 10%. reply Lapha 8 hours agoparentprevRunning models locally using GPU inference shouldn't be too bad as the biggest impact in terms of performance is ram/vram bandwidth rather than compute. Some rough power figures for a dual AMD GPU setup (24gb vram total) on a 5950x (base power usage of around 100w) using llama.cpp (i.e., a ChatGPT style interface, not Copilot): 46b Mixtral q4 (26.5 gb required) with around 75% in vram: 15 tokens/s - 300w at the wall, nvtop reporting GPU power usage of 70w/30w, 0.37kWh 46b Mixtral q2 (16.1 gb required) with 100% in vram: 30 tokens/s - 350w, nvtop 150w/50w, 0.21kWh. Same test with 0% in vram: 7 tokens/s - 250w, 0.65kWh 7b Mistral q8 (7.2gb required) with 100% in vram: 45 tokens/s - 300w, nvtop 170w, 0.12kWh The kWh figures are an estimate for generating 64k tokens (around 35 minutes at 30 tokens/s), it's not an ideal estimate as it only assumes generation and ignores the overhead of prompt processing or having longer contexts in general. The power usage essentially mirrors token generation speed, which shouldn't be too surprising. The more of the model you can load into fast vram the faster tokens will generate and the less power you'll use for the same amount of tokens generated. Also note that I'm using mid and low tier AMD cards, with the mid tier card being used for the 7b test. If you have an Nvidia card with fast memory bandwidth (i.e., a 3090/4090), or an Apple ARM Ultra, you're going to see in the region of 60 tokens/s for the 7b model. With a mid range Nvidia card (any of the 4070s), or an Apple ARM Max, you can probably expect similar performance on 7b models (45 t/s or so). Apple ARM probably wins purely on total power usage, but you're also going to be paying an arm and a leg for a 64gb model which is the minimum you'd want to run medium/large sized models with reasonable quants (46b Mixtral at q6/8, or 70b at q6), but with the rate models are advancing you may be able to get away with 32gb (Mixtral at q4/6, 34b at q6, 70b at q3). I'm not sure how many tokens a Copilot style interface is going to churn though but it's probably in the same ballpark. A reasonable figure for either interface at the high end is probably a kWh a day, and even in expensive regions like Europe it's probably no more than $15/mo. The actual cost comparison then becomes a little complicated, spending $1500 on 2 3090s for 48gb of fast vram isn't going to make sense for most people, similarly making do with whatever cards you can get your hands on so long as they have a reasonable amount of vram probably isn't going to pay off in the long run. It also depends on the size of the model you want to use and what amount of quantisation you're willing to put up with, current 34b models or Mixtral at reasonable quants (q4 at least) should be comparable to ChatGPT 3.5, future local models may end up getting better performance (either in terms of generation speed or how smart they are) but ChatGPT 5 may blow everything we have now out of the water. It seems far too early to make purchasing decisions based on what may happen, but most people should be able to run 7b/13b and maybe up to 34/46b models with what they have and not break the bank when it comes time to pay the power bill. reply theLiminator 14 hours agoprevCurious what's the current SOTA local copilot model? Are there any extensions in vscode that give you a similar experience? I'd love something more powerful than copilot for local use (I have a 4090, so I should be able to run a decent number of models). reply sfsylvester 14 hours agoparentThis is a completely fair, but open question. Not to be a typical HN user, but when you say SOTA local, the question is really what benchmarks do you really care about in order to evaluate. Size, operability, complexity, explainability etc. Working out what copilot models perform best has been a deep exercise for myself and has really made me evaluate my own coding style on what I find important and things I look out for when investigating models and evaluating interview candidates. I think three benchmarks & leaderboards most go to are: https://huggingface.co/spaces/bigcode/bigcode-models-leaderb... - which is the most understood, broad language capability leaderboad that relies on well understood evaluations and benchmarks. https://huggingface.co/spaces/mike-ravkine/can-ai-code-resul... - Also comprehensive, but primarily assesses Python and JavaScript. https://evalplus.github.io/leaderboard.html - which I think is a better take on comparing models you intend to run locally as you can evaluate performance, operability and size in one visualisation. Best of luck and I would love to know which models & benchmarks you choose and why. reply theLiminator 14 hours agorootparentI'm honestly more interested in anecdotes and I'm just seeking anything that can be a drop-in copilot replacement (that's subjectively better). Perhaps one major thing I'd look for is improved understanding of the code in my own workspace. I honestly don't know what benchmarks to look at or even what questions to be asking. reply hackerlight 13 hours agorootparentprev> https://huggingface.co/spaces/mike-ravkine/can-ai-code-resul... - Also comprehensive, but primarily assesses Python and JavaScript. I wonder why they didn't use DeepSeek under the \"senior\" interview test. I am curious to see how it stacks up there. reply Eisenstein 14 hours agoparentprevWhen this 70b model gets quantized you should be able to run it fine on your 4090. Check out 'TheBloke' on huggingface and the llamacpp to run the gguf files. reply coder543 14 hours agorootparentI think your take is a bit optimistic. I like quantization as much as the next person, but even the 2-bit model won’t fit entirely on a 4090: https://huggingface.co/TheBloke/Llama-2-70B-GGUF I would be uncomfortable recommending less than 4-bit quantization on a non-MoE model, which is ~40GB on a 70B model. reply nox101 13 hours agorootparentfortunately it will run on my UMA mac. it's made me curious what the trade offs are. Would I be better off with a 4090 or a Mac with 128+gig of uma memory reply zten 13 hours agorootparentWell, the workstation-class equivalent of a 4090 -- RTX 6000 Ada -- has enough RAM to work with a quantized model, but it'll blow away anyone's budget at anywhere between $7,000 and $10,000. reply coder543 13 hours agorootparentprevEven the M3 Max seems to be slower than my 3090 for LLMs that fit onto the 3090, but it’s hard to find comprehensive numbers. The primary advantage is that you can spec out more memory with the M3 Max to fit larger models, but with the exception of CodeLlama-70B today, it really seems like the trend is for models to be getting smaller and better, not bigger. Mixtral runs circles around Llama2-70B and arguably ChatGPT-3.5. Mistral-7B often seems fairly close to Llama2-70B. Microsoft accidentally leaked that ChatGPT-3.5-Turbo is apparently only 20B parameters. 24GB of VRAM is enough to run ~33B parameter models, and enough to run Mixtral (which is a MoE, which makes direct comparisons to “traditional” LLMs a little more confusing.) I don’t think there’s a clear answer of what hardware someone should get. It depends. Should you give up performance on the models most people run locally in hopes of running very large models, or give up the ability to run very large models in favor of prioritizing performance on the models that are popular and proven today? reply int_19h 11 hours agorootparentM3 Max is actually less than ideal because it peaks at 400 Gb/s for memory. What you really want is M1 or M2 Ultra, which offers up to 800 Gb/s (for comparison, RTX 3090 runs at 936 GB/s). A Mac Studio suitable for running 70B models with speeds fast enough for realtime chat can be had for ~$3K The downside of Apple's hardware at the moment is that the training ecosystem is very much focused on CUDA; llama.cpp has an open issue about Metal-accelerated training: https://github.com/ggerganov/llama.cpp/issues/3799 - but no work on it so far. This is likely because training at any significant sizes requires enough juice that it's pretty much always better to do it in the cloud currently, where, again, CUDA is the well-established ecosystem, and it's cheaper and easier for datacenter operators to scale. But, in principle, much faster training on Apple hardware should be possible, and eventually someone will get it done. reply coder543 6 hours agorootparentYep, I seriously considered a Mac Studio a few months ago when I was putting together an “AI server” for home usage, but I had my old 3090 just sitting around, and I was ready to upgrade the CPU on my gaming desktop… so then I had that desktop’s previous CPU. I just had too many parts already, and it deeply annoys me that Apple won’t put standard, user-upgradable NVMe SSDs on their desktops. Otherwise, the Mac Studio is a very appealing option for sure. reply Eisenstein 13 hours agorootparentprevThe great thing about gguf is that it will cross to system RAM if there isn't enough VRAM. It will be slower, but waiting a couple minutes for a prompt response isn't the worst thing if you are the type that would get use out of a local 70b parameter model. Then again, one could have grabbed 2x 3090s for the price of a 4090 and ended up with 48gb of VRAM in exchange for a very tolerable performance hit. reply coder543 13 hours agorootparent> The great thing about gguf is that it will cross to system RAM if there isn't enough VRAM. No… that’s not such a great thing. Helpful in a pinch, but if you’re not running at least 70% of your layers on the GPU, then you barely get any benefit from the GPU in my experience. The vast gulf in performance between the CPU and GPU means that the GPU is just spinning its wheels waiting on the CPU. Running half of a model on the GPU is not useful. > Then again, one could have grabbed 2x 3090s for the price of a 4090 and ended up with 48gb of VRAM in exchange for a very tolerable performance hit. I agree with this, if someone has a desktop that can fit two GPUs. reply zten 10 hours agorootparentMulti-GPU in desktop chassis gets crazy pretty quickly. If you don't care about aesthetics and can figure out both the power delivery and PCI-E lane situation, https://timdettmers.com/2023/01/30/which-gpu-for-deep-learni... has an example that will make a Bitcoin mining rig look clean. Water cooling can get you down to 2x slot height, with all of the trouble involved in water cooling. NVIDIA really segmented the market quite well. Gamers hate blower cards, but they are the right physical dimensions to make multi-GPU work well, and they are exclusively on the workstation cards. reply sp332 11 hours agorootparentprevThe main benefit of a GPU in that case is much faster prompt reading. Could be useful for Code Llama cases where you want the model to read a lot of code and then write a line or part of a line. reply dimask 12 hours agorootparentprev> The great thing about gguf is that it will cross to system RAM if there isn't enough VRAM. Then you can just run it entirely on CPU. There is no point to buy an expensive GPU to run LLMs to be bottlenecked by your CPU in the first place. Which is why I do not get so excited with these huge models, as they gain less traction as not as many people can run them locally, and finetuning is probably more costly too. reply int_19h 11 hours agorootparentprevGGUF is just a file format. The ability to offload some layers to CPU is not specific to it nor to llama.cpp in general - indeed, it was available before llama.cpp was even a thing. reply Eisenstein 10 hours agorootparentI'm pretty sure I didn't assert that it was more that a file format or that llama.cpp was a pioneer in that regard? reply siilats 11 hours agoprevWe made a Jetbrains plugin called CodeGPT to run this locally https://plugins.jetbrains.com/plugin/21056-codegpt reply bredren 10 hours agoparentAre seamless conversations still handled, using the truncation method described in #68? I was curious if some kind of summary or compression of old exchanges flagged as such might allow the app to remember stuff that had been discussed but fallen outside the token limit. But possibly request key details lost during summary to bring them back into the new context. I had thought chatgpt was doing something like this but haven’t read about it. reply martingoodson 14 hours agoprevBaptiste Roziere gave a great talk about Code Llama at our meetup recently: https://m.youtube.com/watch?v=_mhMi-7ONWQ I highly recommend watching it. reply fullspectrumdev 11 hours agoprevThis looks potentially interesting if it can be ran locally on say, an M2 Max or similar - and if there’s an IDE plugin to do the Copilot thing. Anything that saves me time writing “boilerplate” or figuring out the boring problems on projects is welcome - so I can expend the organic compute cycles on solving the more difficult software engineering tasks :) reply Havoc 16 hours agoprevNot sure who this is aimed at? The avg programmer probably doesn’t have the gear on hand to run this at the required pace Cool nonetheless reply svara 15 hours agoparentIt's aimed at OpenAI's moat. Making sure they don't accumulate too much of one. No one actually has to use this, it just needs to be clear that LLM as a service won't be super high margin because competition can simply start building on Meta's open source releases. reply FrustratedMonky 14 hours agorootparentSo. Strange as it seems, is Meta being more 'Open', than OpenAI that was created to be the 'open' option to fight off Meta and Google? reply chasd00 14 hours agorootparentIf Meta can turn the money making sauce in GenAI from model+data to just data then it's in a very good position. Meta has tons of data. reply holoduke 14 hours agorootparentprevMeta is becoming the good guy. Its actually a smart move. Some extra reputation points wont hurt Meta. reply adventured 14 hours agorootparentprevThe moat is all but guaranteed to be the scale of the GPUs required to operate these for a lot of users as they get ever larger, specifically the extreme cost that is going along with that. Anybody have $10 billion sitting around to deploy that gigantic open source set-up for millions of users? There's your moat and only a relatively few companies will be able to do it. One of Google's moats is, has been, and will always be the scale required to just get into the search game and the tens of billions of dollars you need to compete in search effectively (and that's before you get to competing with their brand). Microsoft has spent over a hundred billion dollars trying to compete with Google, and there's little evidence anybody else has done better anywhere (Western Europe hasn't done anything in search, there's Baidu out of China, and Yandex out of Russia). VRAM isn't moving nearly as fast as the models are progressing in size. And it's never going to. The cost will get ever greater to operate these at scale. Unless someone sees a huge paradigm change for cheaper, consumer accessible GPUs in the near future (Intel? AMD? China?). As it is, Nvidia owns the market and they're part of the moat cost problem. reply dragonwriter 14 hours agorootparent> VRAM isn't moving nearly as fast as the models are progressing in size. Models of any given quality are declining in size (both number of parameters, and also VRAM required for inference per parameter because quantization methods are improving.) reply alfalfasprout 13 hours agorootparentprevand this is why the LLM arms race for ultra high parameter count models will stagnate. It's all well and good that we're developing interesting new models. But once you factor cost into the equation it does severely limit what applications justify the cost. Raw FLOPs may increase each generation but VRAM becomes a limiting factor. And fast VRAM is expensive. I do expect to see incremental innovation in reducing the size of foundational models. reply KaiserPro 13 hours agorootparentprev> The moat is all but guaranteed to be the scale of the GPUs required to operate these for a lot of users for end users, yes. For small companies that want to finetune, evaluate and create derivatives, it reduces the cost by millions. reply staticman2 14 hours agorootparentprev>The moat is all but guaranteed to be the scale of the GPUs required to operate these You don't have to run them locally. reply brucethemoose2 14 hours agorootparentprev> VRAM isn't moving nearly as fast as the models are progressing in size. And it's never going to. The cost will get ever greater to operate these at scale. It is in at least 2025. AMD (and Intel, maybe) will have M-Pro-Esque APUs that can run a 70B model at very reasonable speeds. I am pretty sure Intel is going to rock the VRAM boat on desktops as well. They literally have no market to lose, unlike AMD which infuriatingly still artificially segments their high VRAM cards. reply moyix 16 hours agoparentprevYou can run it on a Macbook M1/M2 with 64GB of RAM. reply reddit_clone 15 hours agorootparentI am not too familiar with LLMs and GPUs (Not a gamer either). But want to learn. Could you please expand on what else would be capable of running such models locally? How about a linux laptop/desktop with specific hardware configuration? reply MeImCounting 14 hours agorootparentIt pretty much comes down to 2 factors which is memory bandwidth and compute. You need a high enough memory bandwidth to be able to \"feed\" the compute and you need beefy enough compute to be able to keep up with the data that is being fed in by the memory. In theory a single Nvidia 4090 would be able to run a 70b model with quantization at \"useable\" speeds. The reason mac hardware is so capable in AI is because of the unified architecture meaning the memory is shared across the GPU and CPU. There are other factors but it essentially comes down to tokens per second advantages. You could run one of these models on an old GPU with low memory bandwidth just fine but your tokens per second would be far too slow for what most people consider \"useable\" and the quantization necessary might star noticeably effecting the quality. reply int_19h 11 hours agorootparentA single RTX 4090 can run at most 34b models with 4-bit quantization. You'd need 2-bit for 70b, and at that point quality plummets. Compute is actually not that big of a deal once generation is ongoing, compared to memory bandwidth. But the initial prompt processing can easily be an order of magnitude slower on CPU, so for large prompts (which would be the case for code completion), acceleration is necessary. reply MeImCounting 10 hours agorootparentThats a good point. For example both the RTX 4090 and the RTX 6000 Ada Generation use the AD102 chip. The RTX 6000 Ada though, would be able to run 70b models due to the larger memory pool despite having the same memory interface width. reply tinco 9 hours agorootparentprevThere are no other laptops on the market with as much VRAM as the 64GB MBP's have as far as I know. You could make a Linux desktop computer with two 3090's, linked together giving 48GB of VRAM. Wich apparently can run a 4-bit quantized 6k context 70B llama model. People are recommending Macbooks because they're a relatively cheap and easy way to get a very large amount of RAM hooked up to your accelerator. Note that these are quantized versions of the model, so they're not as good as the original 70B model, though people claim their performance is really close to original performance. To run without quantization you'd need about 140GB of VRAM. Which would only be possible with an NVidia H100 (don't know the price) or two A100's (at $18,000 each). reply 2OEH8eoCRo0 15 hours agorootparentprevHow? It's larger than 64GB. reply coder543 15 hours agorootparentQuantization is highly effective at reducing memory and storage requirements, and it barely has any impact on quality (unless you take it to the extreme). Approximately no one should ever be running the full fat fp16 models during inference of any of these LLMs. That would be incredibly inefficient. I run 33B parameter models on my RTX 3090 (24GB VRAM) no problem. 70B should easily fit into 64GB of RAM. reply sbrother 15 hours agorootparentCan I ask how many tok/s you're getting on that setup? I'm trying to decide whether to invest in a high-end NVIDIA setup or a Mac Studio with llama.cpp for the purposes of running LLMs like this one locally. reply coder543 14 hours agorootparentOn a 33B model at q4_0 quantization, I’m seeing about 36 tokens/s on the RTX 3090 with all layers offloaded to the GPU. Mixtral runs at about 43 tokens/s at q3_K_S with all layers offloaded. I normally avoid going below 4-bit quantization, but Mixtral doesn’t seem phased. I’m not sure if the MoE just makes it more resilient to quantization, or what the deal is. If I run it at q4_0, then it runs at about 24 tokens/s, with 26 out of 33 layers offloaded, which is still perfectly usable, but I don’t usually see the need with Mixtral. Ollama dynamically adjusts the layers offloaded based on the model and context size, so if I need to run with a larger context window, that reduces the number of layers that will fit on the GPU and that impacts performance, but things generally work well. reply sorenjan 13 hours agorootparentWhat's the power consumption and fan noise like when doing that? I assume you're running the model doing inference in the background for the whole coding session, i.e. hours at a time? reply coder543 13 hours agorootparentI don’t use local LLMs for CoPilot-like functionality, but I have toyed with the concept. There are a few things to keep in mind: no programmer that I know is sitting there typing code for hours at a time without stopping. There’s a lot more to being a developer than just typing, whether it is debugging, thinking, JIRA, Slack, or whatever else. These CoPilot-like tools will only activate after you type something, then stop for a defined timeout period. While you’re typing, they do nothing. After they generate, they do nothing. I would honestly be surprised if the GPU active time was more than 10% averaged over an hour. When actively working on a large LLM, the RTX 3090 is drawing close to 400W in my desktop. At a 10% duty cycle (active time), that would be 40W on average, which would be 320Wh over the course of a full 8-hour day of crazy productivity. My electric rate is about 15¢/kWh, so that would be about 5¢ per day. It is absolutely not running at a 100% duty cycle, and it’s absurd to even do the math for that, but we can multiply by 10 and say that if you’re somehow a mythical “10x developer” then it would be 50¢/day in electricity here. I think 5¢/day to 10¢/day is closer to reality. Either way, the cost is marginal at the scale of a software developer’s salary. reply sorenjan 13 hours agorootparentThat sounds perfectly reasonable. I'm more worried about noise and heat than the cost though, but I guess that's not too bad either then. What's the latency like? When I've used generative image models the programs unload the model after they're done, so it takes a while to generate the next image. Is the model sitting in VRAM when it's idle? reply coder543 13 hours agorootparentFan noise isn’t very much, and you can always limit the max clockspeeds on a GPU (and/or undervolt it) to be quieter and more efficient at a cost of a small amount of performance. The RTX 3090 still seems to be faster than the M3 Max for LLMs that fit on the 3090, so giving up a little performance for near-silent operation wouldn’t be a big loss. Ollama caches the last used model in memory for a few minutes, then unloads it if it hasn’t been used in that time to free up VRAM. I think they’re working on making this period configurable. Latency is very good in my experience, but I haven’t used the local code completion stuff much, just a few quick experiments on personal projects, so my experience with that aspect is limited. If I ever have a job that encourages me to use my own LLM server, I would certainly consider using it more for that. reply sbrother 14 hours agorootparentprevThanks! That is really fast for personal use. reply int_19h 11 hours agorootparentprevI run LLaMA 70B and 120B (frankenmerges) locally on a 2022 Mac Studio with M1 Ultra and 12Gb RAM. It gives ~7 tok/s for 120B and ~9.5 tok/s for 70B. Note that M1/M2 Ultra is quite a bit faster than M3 Max, mostly due to 800 Gb/s vs 400 Gb/s memory reply nullstyle 14 hours agorootparentprevHere's an example of megadolphin running on my m2 ultra setup: https://gist.github.com/nullstyle/a9b68991128fd4be84ffe8435f... reply 2OEH8eoCRo0 15 hours agorootparentprevI'm aware but is it still LLaMA 70B at that point? reply andy99 15 hours agorootparentIt's a legit question, the model will be worse in some way... I've seen it discussed that all things being equal more parameters is better (meaning it's better to take a big model and quantized it to fit in memory than use a smaller unquantized model that fits), but a quantized model wouldn't be expected to run identically to or as well as the full model. reply coder543 15 hours agorootparentYou don’t stop being andy99 just because you’re a little tired, do you? Being tired makes everyone a little less capable at most things. Sometimes, a lot less capable. In traditional software, the same program compiled for 32-bit and 64-bit architectures won’t be able to handle all of the same inputs, because the 32-bit version is limited by the available address space. It’s still the same program. If we’re not willing to declare that you are a completely separate person when you’re tired, or that 32-bit and 64-bit versions are completely different programs, then I don’t think it’s worth getting overly philosophical about quantization. A quantized model is still the same model. The quality loss from using 4+ bit quantization is minimal, in my experience. Yes, it has a small impact on accuracy, but with massive efficiency gains. I don’t really think anyone should be running the full models outside of research in the first place. If anything, the quantized models should be considered the “real” models, and the full fp16/fp32 model should just be considered a research artifact distinct from the model. But this philosophical rabbit hole doesn’t seem to lead anywhere interesting to me. Various papers have shown that 4-bit quantization is a great balance. One example: https://arxiv.org/pdf/2212.09720.pdf reply cjbprime 14 hours agorootparentI don't like the metaphor: when I'm tired, I will be alert again later. Quantization is lossy compression: the human equivalent would be more like a traumatic brain injury affecting recall, especially of fine details. The question of whether I am still me after a traumatic brain injury is philosophically unclear, and likely depends on specifics about the extent of the deficits. reply coder543 14 hours agorootparentThe impact on accuracy is somewhere in the single-digit percentages at 4-bit quantization, from what I’ve been able to gather. Very small impact. To draw the analogy out further, if the model was able to get an A on a test before quantization, it would likely still get a B at worst afterwards, given a drop in the score of less than 10%. Depending on the task, the measured impact could even be negligible. It’s far more similar to the model being perpetually tired than it is to a TBI. You may nitpick the analogy, but analogies are never exact. You also ignore the other piece that I pointed out, which is how we treat other software that comes in multiple slightly different forms. reply berniedurfee 13 hours agorootparentReminds me of the never ending MP3 vs FLAC argument. The difference can be measured empirically, but is it noticeable in real world usage. reply jameshart 12 hours agorootparentprevBut we’re talking about a coding LLM here. A single digit percentage reduction in accuracy means, what, one or two times in a hundred, it writes == instead of !=? reply coder543 12 hours agorootparentI think that’s too simplified. The best LLMs will still frequently make mistakes. Meta is advertising a HumanEval score of 67.8%. In a third of cases, the code generated still doesn’t satisfactorily solve the problem in that automated benchmark. The additional errors that quantization would introduce would only be a very small percentage of the overall errors, making the quantized and unquantized models practically indistinguishable to a human observer. Beyond that, lower accuracy can manifest in many ways, and “do the opposite” seems unlikely to be the most common way. There might be a dozen correct ways to solve a problem. The quantized model might choose a different path that still turns out to work, it’s just not exactly the same path. As someone else pointed out, FLAC is objectively more accurate than mp3, but how many people can really tell? Is it worth 3x the data to store/stream music in FLAC? The quantized model would run at probably 4x the speed of the unquantized model, assuming you had enough memory to choose between them. Is speed worth nothing? If I have to wait all day for the LLM to respond, I can probably do the work faster myself without its help. Is being able to fit the model onto the hardware you have worth nothing? In essence, quantization here is a 95% “accurate” implementation of a 67% accurate model, which yields a 300% increase in speed while using just 25% of the RAM. All numbers are approximate, even the HumanEval benchmark should be taken with a large grain of salt. If you have a very opulent computational experience, you can enjoy the luxury of the full 67.8% accurate model, but that just feels both wasteful and like a bad user experience. reply coder543 15 hours agorootparentprevYes. Quantization does not reduce the number of parameters. It does not re-train the model. reply manmal 15 hours agorootparentprevSure, quantization reduces information stored for each parameter, not the parameter count. reply rgbrgb 15 hours agorootparentprevQuantization can take it under 30GB (with quality degradation). For example, take a look at the GGUF file sizes here: https://huggingface.co/TheBloke/Llama-2-70B-GGUF reply connorgutman 16 hours agoparentprevThis is targeted towards GPU rental services like RunPod as well as API providers such as together AI. Together.ai is charging $0.90/1M tokens at 70B parameters. https://www.together.ai/pricing reply ttul 16 hours agoparentprevYeah, but if your company wants to rent an H100, you can deploy this for your developers for much less than the cost of a developer… reply dimask 11 hours agoparentprevThere are companies like phind that offer copilot-like services using finetuned versions of CodeLlama-34B, which imo are actually good. But I do not know if such a larger model is gonna be used in such a context. reply oceanplexian 8 hours agoparentprevI have a multi-GPU rig designed exactly for this purpose :) Check out r/localllama. There are literally dozens of us! reply blackoil 16 hours agoparentprevHow feasible would it be too fine tune using internal code and have an enterprise copilot. reply lozenge 16 hours agorootparentConsidering a number of Saas offer this service, I'd say it's feasible. reply keriati1 16 hours agorootparentprevWe actually run already in-house ollama server prototype for coding assistance with deepseek coder and it is pretty good. Now if we would get a model for this, that is on chatgpt 4 level, I would be super happy. reply eurekin 15 hours agorootparentDid you finetune a model? reply keriati1 15 hours agorootparentNo, we went with RAG pipeline approach as we assume things change too fast. reply eurekin 15 hours agorootparentThanks! Any details how you chunk and find the relevant code? Or how you deal with context length? I.e. do you send anything other than the current file? How is the prompt constructed? reply jejeyyy77 15 hours agorootparentprevalready been done reply kungfupawnda 14 hours agoparentprevI got it to build and run the example app on my M3 max with 36 gb ram. Memory pressure was around 32 gb reply dimask 12 hours agorootparentDid you quantise it? At what level and what was your impression compared to other recent smaller models at that quantisation, if so? reply kungfupawnda 11 hours agorootparentNo I just ran it out of the box but I had to modify the source code to run for Mac. Instructions here: https://github.com/facebookresearch/llama/pull/947/ reply Spivak 16 hours agoparentprevPeople who want to host the models presumably, AWS bedrock will def include it. reply bk146 15 hours agoprevCan someone explain Meta's strategy with the open source models here? Genuine question, I don't fully undestand. (Please don't say \"commoditize your complement\" without explaining what exactly they're commoditizing...) reply pchristensen 15 hours agoparentMeta doesn't have an AI \"product\" competing with OpenAI, Google's Bard, etc. But they use AI extensively internally. This is roughly a byproduct of their internal AI work that they're already doing, and fostering open source AI development puts incredible pressure on the AI products and their owners. If Meta can help prevent there from being an AI monopoly company, but rather an ecosystem of comparable products, then they avoid having another threatening tech giant competitor, as well as preventing their own AI work and products from being devalued. Think of it like Google releasing a web browser. reply IshKebab 15 hours agorootparentGoogle releasing a (very popular) web browser gives them direct control of web standards. What does this give Facebook? reply eganist 15 hours agorootparentOP already mentioned that it adds additional hurdles for possible future tech giants to have to cross on their quest. It's akin to a Great Filter, if such an analogy helps. If Meta's open models make a company's closed models uneconomical for others to consume, then the business case for those models is compromised and the odds of them growing to a size where they can compete with Meta in other ways is mitigated a bit. reply patapong 15 hours agorootparentprevI think we should not underestimate the strategic talent acquisition value as well. Many top-tier AI engineers may appreciate the openness and choose to join meta, which could be very valuable in the long run. reply jwkane 14 hours agorootparentExcellent point -- goodwill in a hyper-high demand dev community is invaluable. reply fngjdflmdflg 12 hours agorootparentprevWeb standards are probably the last thing Google cares about with Chrome. Much more important is being the default search engine and making sure data collection isn't interrupted by a potential privacy minded browser. reply gen220 15 hours agoparentprevThey're commoditizing the ability to generate viral content, which is the carrot that keeps peoples' eyeballs on the hedonic treadmill. More eyeball-time = more ad placements = more money. On the advertiser side, they're commoditizing the ability for companies to write more persuasively-targeted ads. Higher click-through rates = more money. [edit]: For models that generate code instead of content (TFA), it's obviously a different story. I don't have a good grip on that story, beyond \"they're using their otherwise-idle GPU farms to buy goodwill and innovate on training methods\". reply esafak 14 hours agorootparentThat stuff ultimately drives people away. Who thinks \"I need my daily fix of genAI memes, let me head to Facebook!\"? reply eurekin 15 hours agoparentprevTotal speculation: Yann LeCun is there and he is really passionate about the technology and openness reply observationist 15 hours agorootparentThe faux-open models mean the models can't be used in competing products. The open code base means enthusiasts and amateurs and other people hack on Meta projects and contribute improvements. They get free R&D and suppress competition, while looking like they have principles. Yann is clueless about open source principles, or the models would have been Apache or some other comparably open license. It's all ruthless corporate strategy, regardless of the mouth noises coming out of various meta employees. reply sangnoir 13 hours agorootparent> The faux-open models mean the models can't be used in competing products. Just because certain entities can't profitably use a product or obtain a license doesn't make it not-open. AGPL is open, for an extreme example. This argument is also subjective, and not new - \"Which is more open BSD-style licenses or GPL?\" has ben a guaranteed flameware starter for decades. reply observationist 10 hours agorootparentI'm not arguing about BSD or GPL. I'm saying that the \"open source code, proprietary binary blob\" pattern Meta is running with is about quashing potential competition, market positioning, and corporate priorities over any tangential beneficial contributions to open source AI. It's shitty when other companies do it. It's shitty when Broadcom does it. It's shitty when Meta does it. It's never a not shitty thing to do. reply importantbrian 14 hours agorootparentprevMeta's choice of license doesn't indicate that Yann is clueless about open-source principles. I don't know about Meta specifically, but in most companies choosing the license for open source projects involves working with a lot of different stakeholders. He very easily could have pushed for Apache or MIT and some other interest group within Meta vetoed it. reply skottenborg 15 hours agorootparentprevI doubt personal passions would merit the company funding required for such big models. reply eurekin 15 hours agorootparentGiven how megacorps spend millions on a whim (Disney with all recent flops) or, when just a single person wants it (Ms Flight Simulator?) - I wouldn't be surprised to be honest... But sure, sounds more reasonable reply og_kalu 14 hours agorootparentDisney didn't spend millions on a whim. It's just the reality of box office that even millions in investment are no guarantee for returns. reply eurekin 3 hours agorootparentFinancially, they have underperformed significantly over longer period of time (10 years): For shareholders, this subpar performance has destroyed value. Disney stock has underperformed the stocks of Disney’s self-selected proxy peers and the broader market over every relevant period during the last decade and during the tenure of each non-management director. Furthermore, it has underperformed since Bob Iger was first appointed CEO in 2005 – a period during which he has served as CEO or Executive Chairman (directing the Company’s creative endeavors in this role) for all but 11 months. Disney shareholders were once over $200 billion wealthier than they are now Which is radically different from previous 90 years https://trianpartners.com/wp-content/uploads/2023/12/Trian-N... reply chasd00 14 hours agoparentprevMy opinion is Meta is taking the model out of the secret sauce formula. That leaves hardware and data for training as the barrier to entry. If you don't need to develop your own model then all you need is data and hardware which lowers the barrier to entry. The lower the barrier the more GenAI startups and the more potential data customers for Meta since they certainly have large, curated, datasets for sale. reply Lerc 15 hours agoparentprevIf they hadn't opened the models the llama series would just be a few sub-GPT4 models. Opening the models has created a wealth of development that has built upon those models. Alone, it was unlikely they would become a major player in a field that might be massively important. With a large community building upon their base they have a chance to influence the direction of development and possibly prevent a proprietary monopoly in the hands of another company. reply simonw 15 hours agoparentprevAI seems like the Next Big Thing. Meta have put themselves at the center of the most exciting growth area in technology by releasing models they have trained. They've gained an incredible amount of influence and mindshare. reply andy99 15 hours agoparentprevI think a big part of it is just because they have a big AI lab. I don't know the genesis of that, but it has for years been a big contributor, see pytorch, models like SEER, as well as being one of the dominant publishers at big conferences. Maybe now their leadership wants to push for practicality so they don't end up like Google (also a research powerhouse but failing to convert to popular advances) so they are publicly pushing strong LLMs. reply crowcroft 15 hours agoparentprevMeta's end goal is to have better AI than everyone else, in the medium term that means they want to have the best foundational models. How does this help. 1. They become an attractive place for AI researchers to work, and can bring in better staff. 2. They make it less appealing for startups to enter the space and build large foundation models (Meta would prefer 1,000 startups pop up and play around with other people's models, than 1000 startups popping up and trying to build better foundational models). 3. They put cost pressure on AI as a service providers. When LLAMA exists it's harder for companies to make a profit just selling access to models. Along with 2 this further limits the possibility of startups entering the foundational model space, because the path to monetization/breakeven is more difficult. Essentially this puts Meta, Google, and OpenAI/Microsoft (Anthropic/Amazon as a number four maybe) as the only real players in the cutting edge foundational model space. Worst case scenario they maintain their place in the current tech hegemony as newcomers are blocked from competing. reply siquick 12 hours agorootparent> Essentially this puts Meta, Google, and OpenAI/Microsoft (Anthropic/Amazon as a number four maybe) as the only real players in the cutting edge foundational model space. Mistral is right up there. reply yodsanklai 12 hours agorootparentMistral has ~20 employees. I'm sure they have good researchers, but don't they lack the computing and engineering resources the big actors have? reply crowcroft 12 hours agorootparentprevI'm curious to see how they go, I might have a limited understanding. From what I can tell they do a good job in terms of value and efficiency with 'lighter' models, but I don't put them in the same category as the others in the sense that they aren't producing the massive absolute best in class LLMs. Hopefully they can prove me wrong though! reply Philpax 15 hours agoparentprevAside from the \"positive\" explanations offered in the sibling comments, there's also a \"negative\" one: other AI companies that try to enter the fray will not be able to compete with Meta's open offerings. After all, why would you pay a company to undertake R&D on building their own models when you can just finetune a Llama? reply two_in_one 9 hours agorootparentWhatever Meta's motivation is they help diversify models suppliers. Which is a good thing not to be locked in. As usual reality is more complicated with many moving part. Free models may undercut small startups. But at the same time they stimulate secondary market of providers and tuners. reply bryan_w 15 hours agoparentprevPart of it is that they already had this developed for years (see alt text on uploaded images for example), and they want to ensure that new regulations don't hamper any of their future plans. It costs them nothing to open it up, so why not. Kinda like all the rest of their GitHub repos. reply emporas 14 hours agoparentprevYan Le Cunn has talked about Meta's strategy with open source. The general idea, is that the smartest people in the world do not work for you. No company can replicate innovation from open source internally. reply yodsanklai 12 hours agorootparent> The general idea, is that the smartest people in the world do not work for you Most likely, they work for your competitors. They may not be working to improve your system for free. > No company can replicate innovation from open source internally. Lot of innovation does come from companies. reply emporas 10 hours agorootparent>Lot of innovation does come from companies. Of course, i am not arguing that. But when it comes to software as general as code generation, or text generation, the possible applications are so broad, that a team of A.I. researchers in a company, however talented and productive they are, cannot possibly optimize it for every possible use case. That's what Yan Le Cunn is referring to, and i agree with him. There are a lot of companies which push deep learning forward, and do not release their code or weights freely. reply Too 14 hours agoparentprevMeta still sit on all the juicy user data that they want to use AI on but they don’t know how. They are crowdsourcing development of applications and tooling. Meta releases model. Joe builds a cool app with it, earns some internet points and if lucky a few hundred bucks. Meta copies app, multiply Joes success story with 1 billion users and earn a few million bucks. Joe is happy, Meta is happy. Everybody is happy. reply Calvin02 14 hours agoparentprevControversial take: Meta sees this as the way to improve their AI offerings faster than others and, eventually, better than others. Instead of a small group of engineers working on this inside Meta, the Open Source community helps improve it. They have a history of this with React, PyTorch, hhvm, etc. All these have gotten better as OS projects faster than Meta alone would have been able to do. reply flir 14 hours agoparentprevReally enjoying how many different answers you got. (My theory: if there's an AI pot of gold, what megacorp can risk one of the others getting to it first?) reply datadrivenangel 15 hours agoparentprevAI puts pressure on search, cutting into google's ad revenue. Meta's properties are less immune to pressure from AI. reply pyinstallwoes 15 hours agoparentprevTo be crowned the harbinger of AGI. reply apples_oranges 15 hours agoparentprevOT: You „don‘t“ or you „don’t fully“ understand? ;) (I try to train myself to say it right ..) reply theGnuMe 15 hours agoparentprevBill Gurly has a good perspective on it. Essentially, you mitigate IP claims and reduce vendor dependency. https://eightify.app/summary/technology-and-software/the-imp... reply anonymousDan 12 hours agoprevCan anyone tell me what kind of hardware setup would be needed to fine-tune something like this? Would you need a cluster of GPUs? What kind of size + GPU spec would you think is reasonable (e.g. wrt VRAM per GPU etc). reply a_wild_dandan 3 hours agoparentI just use my M2 MacBook Pro. Works great on big models. reply ahmednazir 14 hours agoprevCan you explain why big tech company make a race to release an open source model? If model is free and open source then how will they earn and how will they compete with others? reply stainablesteel 14 hours agoparentthey want to incentivize dependency reply jampekka 14 hours agoparentprevCommoditize your complement? reply dhess 10 hours agoprevCan anyone recommend an Emacs mode for this model? reply doctoboggan 14 hours agoprevIs there a quantized version available for ollama or is it too early for that? reply coder543 13 hours agoparentAlready there, it looks like: https://ollama.ai/library/codellama (Look at “tags” to see the different quantizations) reply ramshanker 16 hours agoprevAre these trained on internal Code bases or just the public repositories? reply albertzeyer 16 hours agoparentWithout knowing any details, I'm almost sure that they did not train it on internal code, as it might be possible to reveal that code otherwise (given the right prompt, or just looking at the weights). reply eigenvalue 16 hours agoparentprevWould be a really bad idea to train on internal code I would think. Besides, there is no shortage of open source code (even open source created by Meta) out there. reply changoplatanero 16 hours agorootparentCorrect that it’s a bad idea to train on internal code. However surprisingly there is a shortage of open source code. These models are trained on substantially all the available open source code that these companies can get their hands on. reply andy99 16 hours agoparentprevThe github [0] hasn't been fully updated, but it links to a paper [1] that describes how the smaller code llama models were trained. It would be a good guess that this model is similar. [0] https://github.com/facebookresearch/codellama [1] https://ai.meta.com/research/publications/code-llama-open-fo... reply make3 16 hours agoparentprevpeople are able to extract training data from llms with different methods, so there's no way this was trained on internal code reply edweis 15 hours agoprevHow come a company as big as Meta still uses bit.ly ? reply geor9e 15 hours agoparentIronically it doesn't help to use link shorteners on twitter anyway - all URLs posted to twitter count as 23 characters. The hypertext is the truncated original URL string, and the URL is actually a t.co link. reply esafak 9 hours agorootparentShorteners these days are for analytics, not shortening per se. reply esafak 14 hours agoparentprevBecause this is a marketing channel. They handle tracking of FB/IG messages by other means, intended for engineers. reply smcleod 10 hours agoparentprevYeah those links don't even work for me anymore as it's a tracking site. reply nemothekid 15 hours agoparentprevWhat else would they use? reply 3pt14159 15 hours agorootparentSomething like meta.com/our_own_tech_handles_this reply nemothekid 15 hours agorootparentNot sure it's preferable to hire people at fb salaries to maintain a link shortener than just to use a reputable free one? reply junon 15 hours agorootparentEvery big company has one of these anyway, and usually more involved (internal DNS, VPN, etc). A link shortener is like an interview question. reply huac 15 hours agorootparentprevtheir own shortener, e.g. fb.me, presumably reply transcriptase 15 hours agorootparentprevfb.com seems like a reasonable choice. reply Cthulhu_ 15 hours agorootparentprevTheir own? reply kmeisthax 12 hours agoparentprevNot only that, the announcement is on Twitter, a company that at least used to be their biggest competitor. Old habits die hard, huh? reply robin-whg 14 hours agoprevThis is a prime example of the positive aspects of capitalism. Meta has its own interests, of course, but as a side effect, this greatly benefits consumers. reply Ninjinka 16 hours agoprevBenchmarks? reply colesantiago 16 hours agoprevLlama is getting better and better, I heard this and Llama 3 will start to be good as GPT-4. Who would have thought that Meta, that has been chucking billions on the metaverse is on the forefront of Open Source AI. Not to mention their stock is up and they are worth $1TN, again. Not sure how I feel about this given the fact of all the scandals that have plagued them and the massive 1BN fine from the EU, Cambridge Analytica, and last of all caused a genocide in Myanmar. Goes to show that nobody cares about all of these scandals and just moves on onto the future, allowing Facebook to still collect all this data for their models. If any other startup or mid sized company had at least two of these large scandals, they would be dead in the water. reply WhackyIdeas 15 hours agoparentIt does seem like the nicest thing Facebook have ever done by giving so much to the open source LLM scene, I know that it might have been started by a leaker, but they have given so much voluntarily. I mean don’t get me wrong, I don’t like the company but I do really like some of the choices they have made recently. But I do wonder in the back of my mind why. And I should be suspicious of their angle and I will keep thinking about it. Is it paranoid to think that maybe their angle is putting almost some kind of metadata by style of code being unique to different machines that they can trace generated code to different people? Is that their angle or am I biased in remembering who they have been for the past decade? reply fragmede 16 hours agoparentprevModel available, not open source. These models aren't open source because we don't have access to the data sets, nor the full code to train them, so we can't recreate the models even if we had the GPU time available to recreate them. reply colesantiago 16 hours agorootparentEveryone using AI in production is using Pytorch by Meta. Which is open source. I do not know anybody important in the AI space apart from Google using TensorFlow. reply Philpax 15 hours agorootparentThat may be true, but it's largely irrelevant. The ML framework in use has no bearing on whether or not you have the data required to reproduce the model being trained with that framework. reply colesantiago 14 hours agorootparentDo you and the GP have 350K GPUs and quality data to reproduce 1:1 whatever Facebook releases in their repos? Even if you want to reproduce the model and they give you the data, you would need to do this at Facebook scale, so you and the GP are just making moot points all around. https://about.fb.com/news/2023/05/metas-infrastructure-for-a... https://www.theregister.com/2024/01/20/metas_ai_plans/ The fact that these models are coming from Meta in the open rather than Google which releases only papers with no model tell's me that Meta's models is open enough for everyone to use. Besides, everyone using the Pytorch framework benefits Meta in the same way they were originally founded as a company: Network effects It's relevant. reply Philpax 14 hours agorootparentThere are organisations that are capable of reproduction (e.g. EleutherAI), but yes, you're right, not having the data is largely irrelevant for most users. The thing that bothers me more is that it's not actually an open-source licence; there are restrictions on what you can do with it, and whatever you do with the model is subject to those restrictions. It's still very useful and I'm not opposed to them releasing it under that licence (they need to recoup the costs somehow), but \"open-source\" (or even \"open\") it is not. reply austinpena 16 hours agoparentprevI'm really curious what their goal is reply ppsreejith 16 hours agorootparentIf you go by what Zuck says, he calls this out in previous earnings reports and interviews[1]. It mainly boils down to 2 things: 1. Similar to other initiatives (mainly opencompute but also PyTorch, React etc), community improvements help them improve their own infra and helps attract talent. 2. Helping people create better content ultimately improves quality of content on their platforms (Both FoA & RL) Sources: [1]Interview with verge: https://www.theverge.com/23889057/mark-zuckerberg-meta-ai-el... . Search for \"regulatory capture right now with AI\" > Zuck: ... And we believe that it’s generally positive to open-source a lot of our infrastructure for a few reasons. One is that we don’t have a cloud business, right? So it’s not like we’re selling access to the infrastructure, so giving it away is fine. And then, when we do give it away, we generally benefit from innovation from the ecosystem, and when other people adopt the stuff, it increases volume and drives down prices. > Interviewer: Like PyTorch, for example? > Zuck: When I was talking about driving down prices, I was thinking about stuff like Open Compute, where we open-sourced our server designs, and now the factories that are making those kinds of servers can generate way more of them because other companies like Amazon and others are ordering the same designs, that drives down the price for everyone, which is good. reply Arainach 16 hours agorootparentprevDisclaimer: I do not work at Meta, but I work at a large tech company which competes with them. I don't work in AI, although if my VP asks don't tell them I said that or they might lay me off. Multiple of their major competitors/other large tech companies are trying to monetize LLMs. OpenAI maneuvering an early lead into a dominant position would be another potential major competitor. If releasing these models slows or hurts them that is in and itself a benefit. reply michaelt 16 hours agorootparentWhy? What benefit is there to grabbing market share from your competitors... in a business you don't even want to be in? By that logic you could justify any bizarre business decision. Should Google launch a social network, to hurt their competitor Facebook? Should Facebook, Amazon and Microsoft each launch a phone? reply Arainach 15 hours agorootparent>Should Facebook, Amazon and Microsoft each launch a phone? * https://www.lifewire.com/whatever-happened-to-the-facebook-p... * https://en.wikipedia.org/wiki/Fire_Phone * https://en.wikipedia.org/wiki/Windows_Phone reply Filligree 16 hours agorootparentprev> Should Google launch a social network, to hurt their competitor Facebook? I mean, Google did launch a social network, to hurt their competitor Facebook. It was a whole thing. It was even a really nice system, eventually. reply zaat 13 hours agorootparentI enjoyed using Google plus more than any other social network, and managed to create new connections and/or have standard, authentic, real conversations with people I didn't know, most of them ordinary people with shared interests that I would probably wouldn't meet otherwise, some of them are people I can't believe I could connect with directly in any other way - newspapers and news sites editors, major SDK developers. And even with Kevin Kelly. reply wanderingstan 15 hours agorootparentprevAnd it turned out that Facebook had quite a moat with network effects. OpenAI doesn’t have such a moat, which may be what Meta is wanting to expose. reply esafak 12 hours agorootparentGoogle botched the launch, and they never nurture products after launch anyway. Google+ could have been more successful. reply Arainach 14 hours agorootparentprevWho says they don't want to be in the market? Facebook has one product. Their income is entirely determined by ads on social media. That's a perilous position subject to being disrupted. Meta desperately wants to diversify its product offerings - that's why they've been throwing so much at VR. reply PheonixPharts 16 hours agorootparentprevI imagine their goal is to simultaneously show that Meta is still SotA when it comes to AI and at the same time feed a community of people who will work for free to essentially undermine OpenAI's competitive advantage and make life worse for Google since at the very least LLMs tend to be a better search engine for most topics. There's far more risk if Meta were to try to directly compete with OpenAI and Microsoft on this. They'd have to manage the infra, work to acquire customers, etc, etc on top of building these massive models. If it's not a space they really want to be in, it's a space they can easily disrupt. Meta's late game realization was that Google owned the web via search and Apple took over a lot of the mobile space with their walled garden. I suspect Meta's view now is that it's much easier to just prevent something like this from happening with AI early on. reply dsabanin 16 hours agorootparentprevI think Meta's goal is to subvert Google, MS and OpenAI, after realizing it's not positioned well to compete with them commercially. reply api 16 hours agorootparentCould also be that these smaller models are a loss leader or advertisement for a future product or service... like a big brother to Llama3 that's commercial. reply idkyall 12 hours agorootparentI believe there were rumors they are developing a commercial model: e.g. https://www.ft.com/content/01fd640e-0c6b-4542-b82b-20afb203f... reply blackoil 16 hours agorootparentprevDevil's advocate: they have to build it anyway for Meta verse and in general. Management has no interest in going into cloud business. They had Parse long time back but that is done. So why not to release it. They are getting goodwill/mindshare, may set up industry standard and get community benefit. It isn't very different from React, Torch etc. reply jedberg 16 hours agorootparentprevCommoditizing your complement. If all your competitors need a key technology to get ahead, you make a cheap/free version of it so that they can't use it as a competitive advantage. reply nindalf 16 hours agorootparentThe complement being the metaverse. You can’t handcraft the metaverse because it would be infeasible. If LLMs are a commodity that everyone has access to, then it can be done on the cheap. Put another way - if OpenAI were the only game in town how much would they be charging for their product? They’re competing on price because competitors exist. Now imagine the price if a hypothetical high quality open source model existed that can customers can use for “free”. That’s the future Meta wants. They weren’t getting rich selling shovels like cloud providers are, they want everyone digging. And everyone digs when the shovels are free. reply Ruq 16 hours agorootparentprevZuck just wants another robot to talk to. reply throwup238 16 hours agorootparentPoor guy just wants a friend that won't sell him out to the FTC or some movie producers. reply ttul 16 hours agorootparentprevIf you want to employ the top ML researchers, you have to give them what they want, which is often the ability to share their discoveries with the world. Making Llama-N open may not be Zuckerberg‘s preference. It’s possible the researchers demanded it. reply elorant 15 hours agorootparentprevPrevent OpenAI from dominating the market, and at the same time have the research community enhance your models and identify key use cases. reply pennomi 15 hours agorootparentMeta basically got a ton of free R&D that directly applies to their model architecture. Their next generation AIs will always benefit from the techniques/processes developed by the clever researchers and hobbyists out there. reply regimeld 16 hours agorootparentprevCommoditize your compliments. reply megaman821 15 hours agorootparentprevThey were going to make most this anyway for Instagram filters, chat stickers, internal coding tools, VR world generation, content moderation, etc. Might as well do a little bit extra work to open source it since it doesn't really compete with anything Meta is selling. reply akozak 16 hours agorootparentprevI would guess mindshare in a crowded field, ie discussion threads just like this one that help with recruiting and tech reputation after a bummer ~8 years. (It's best not to overestimate the complexity/# of layers in a bigco's strategic thinking.) reply tsunamifury 16 hours agorootparentprevTheir goal is to counter the competition. You rarely should pick the exact same strategy as your competitor and count on out gunning them, rather you should counter them. OpenAI is ironically closed, well meta will be open then. If you can't beat them, you should try to degrade down the competitors value case. Its a smart move IMO reply wrsh07 16 hours agorootparentprevRule 5: commodify your complement Content generation is complementary to most of meta's apps and projects reply refulgentis 16 hours agorootparentprevSame as MS, in the game, in the conversation, and ensuring next-gen search margins approximate 0. reply hendersoon 16 hours agorootparentprevTheir goals are clear, dominance and stockholder value. What I'm curious about is how they plan to monetize it. reply refulgentis 16 hours agorootparentUse it in products, ex. the chatbots reply aantix 16 hours agorootparentprevTo undermine the momentum of OpenAI. If Meta were at the forefront, these models would not be openly available. They are scrambling. reply cm2012 15 hours agoparentprevCambridge Analytica is not a real scandal (did not affect any elections), and FB did not cause a genocide in Myanmar (they were a popular message board during a genocide, which is not the same thing) reply make3 16 hours agoparentprevwith pytorch (& so many open publications), Meta has had a unimaginably strong contribution to ai for a while reply b33j0r 16 hours agoprev [–] There is a bait and switch going on, and sam altman or mark zuckerberg are the first to tell you. “No one can compete with us, but it’s cute to try! Make applications though” —almost direct quote from Sam Altman. I have 64gb and an RTX 3090 and a macbook M3, and I already can’t run a lot of the newest models even in their quantized form. The business model requires this to be a subscription service. At least as of today… reply ttul 16 hours agoparentA 70B model is quite accessible; just rent a data center GPU hourly. There are easy deployment services that are getting better all the time. Smaller models can be derived from the big ones to run on a MacBook running Apple Silicon. While the compute won’t be a match for Nvidia hardware, a MacBook can pack 128GB of RAM and run enormous models - albeit slowly. reply b33j0r 15 hours agorootparentOk, well now that we’ve downvoted me below the visibility threshold, I was being sincere. And Altman did say that. I am not a hater. So. Maybe we could help other people figure out why VRAM is maxing out. I think it has to do with various new platforms leaking memory. In my case, I suspect ollama and diffusers are not actually evicting VRAM. nvidia-smi shows it in one case, but I haven’t figured it out yet. Hey, my point remains. The models are going to get too expensive for me, personally, to run locally. I suspect we’ll default into subscriptions to APIs because the upgrade slope is too steep. reply moyix 16 hours agorootparentprevMy Macbook has a mere 64GB and that's plenty to run 70B models at 4-bit :) LM Studio is very nice for this. reply whimsicalism 16 hours agoparentprevthe connection to the classic bait-and-switch seems tenuous at best reply kuczmama 16 hours agoparentprevRealistically, what hardware would be required to run this? I assumed a RTX 3090 would be enough? reply summarity 16 hours agorootparentA Studio Mac with an M1 Ultra (about 2800 USD used) is actually a really cost effective way to run in. Its total system power consumption is really low, even spitting out tokens at full tilt ( At least as of today… This is the exact opposite of bait and switch. The current model couldn't be un-opensourced and over time it will just become easier to run it. Also unless there is reason to believe that prompt engineering of different model families is very different(which honestly I don't believe), there is no effect of baiting. I believe it will always be the case that best 2-3 models would be closed weights. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The company has launched Code Llama 70B, an improved version of their code generator.",
      "The new models are offered under the same licensing terms as previous versions.",
      "The release aims to enhance the efficiency of the code generation process."
    ],
    "commentSummary": [
      "The discussion covers various topics related to AI models, hardware requirements, open source licensing, and Meta's strategies.",
      "It explores the use of Ollama as an autocomplete provider, the availability of local models, hardware choices for running large models, and the benefits of quantization in improving efficiency.",
      "Discussions also touch on Meta's decision to open source models, its impact on competition, and the potential motives behind it, as well as the broader implications of AI models, market value of companies, and concerns about data privacy."
    ],
    "points": 526,
    "commentCount": 254,
    "retryCount": 0,
    "time": 1706548294
  },
  {
    "id": 39180953,
    "title": "Introducing Boardzilla: A Framework for Web-Based Board Games",
    "originLink": "https://www.boardzilla.io/",
    "originBody": "Boardzilla, a framework for making web-based board gamesTldr: We’ve made a framework for web-based board games. You can try out some games over at https:&#x2F;&#x2F;boardzilla.io, or you can take a look at https:&#x2F;&#x2F;docs.boardzilla.io to learn more about how to develop your own game. Source is available at https:&#x2F;&#x2F;github.com&#x2F;boardzillaHey y’all. My brother and I have made a framework for board games. During the pandemic we started to look at BGA but got discouraged by how old-fashioned the tools were and how cumbersome the development process was. We set out to make our own framework where you could use the same code for both the client and server. Our hope is anyone familiar with Typescript and CSS could code up a game without worrying about state management, persistence or networking.It’s still very much a wip, and we&#x27;re rapidly adding features and games. But we’ve got our first draft of developer docs done, and we&#x27;ve put up a few games we&#x27;ve developed to showcase and test out the platform. Source for the games and framework is available on Github, and we’re excited to code more games and hopefully encourage other people to try it out. Happy for any feedback.",
    "commentLink": "https://news.ycombinator.com/item?id=39180953",
    "commentBody": "Boardzilla, a framework for making web-based board games (boardzilla.io)445 points by joshbuddy 14 hours agohidepastfavorite80 comments Boardzilla, a framework for making web-based board games Tldr: We’ve made a framework for web-based board games. You can try out some games over at https://boardzilla.io, or you can take a look at https://docs.boardzilla.io to learn more about how to develop your own game. Source is available at https://github.com/boardzilla Hey y’all. My brother and I have made a framework for board games. During the pandemic we started to look at BGA but got discouraged by how old-fashioned the tools were and how cumbersome the development process was. We set out to make our own framework where you could use the same code for both the client and server. Our hope is anyone familiar with Typescript and CSS could code up a game without worrying about state management, persistence or networking. It’s still very much a wip, and we're rapidly adding features and games. But we’ve got our first draft of developer docs done, and we've put up a few games we've developed to showcase and test out the platform. Source for the games and framework is available on Github, and we’re excited to code more games and hopefully encourage other people to try it out. Happy for any feedback. rosmax_1337 1 hour agoI don't think the foremost message on the landing page should be a \"join our discord\" call. It should feature examples and links to the source/docs. In fact, the prominence of \"join our discord\" on projects like this is off-putting for me personally. I don't doubt I am alone in this regard. reply catapart 13 hours agoprevLooks good! There's a lot of moving parts for such a wide array of functionality, so this looks like a lot of really good work! If nothing else, it's nice to have a game engine that focuses on tabletop games, that doesn't necessarily focus on how to render said games. All of that said, as someone who has developed a card and dice game, using plain html and javascript to create a programmatic version that could be played online, I am still having a hard time figuring out how to recreate that kind of thing, using your system. Based on the docs, I can see it supports cards. And based on my experience developing the same basic gameplay, I can see that it's general-purpose enough to be suited for a wide variety of games (and my game is simplistic, so it should be possible). Feels like it should be a pretty straightforward build, with a robust enough library, and yours seems to be fairly robust, so my thinking here is that a step-by-step tutorial would go a long way. I know enough to see why you've chosen the abstractions you have (Actions, Flows, selections, conditions, prompts, etc), but without seeing them constructed, it's hard to know where to make the specific changes I would want to make. Anyway, just one opinion! It may feel more straightforward to other people. But I don't think I would personally make much headway with it, without a tutorial. reply AlecSchueler 10 hours agoparentIt looks like it's based on two main classes, which make sense in context of the physicality of board games. You have things you move around (`Piece`s), in your hand, from player (`Player`) to player and onto different spaces on the table itself (`Space`). So for game you'd need your dice and your cards to be defined subclassing `Piece`, and give them to your players (which subclass `Player`) and maybe also to a Teller (also `Player` who manages the deck, before thinking about which instances of `Space` you'll need to set up on your board for your rules. Your rules all in an instance of the `Game` class and it itself has this concept of actions, at their core instances of the `Action` class. Those take arguments that allow you to write the prompts for the user, define when the action is legal in play, and what effect it should have. Beyond that the API then just kicks in and seems to have a lot of handy functions for finding pieces, making comparisons, all the stuff you expect. Beyond that just depends on your game logic. There's also some handy functions to filter for different Spaces and Pieces so that you can build up the UI logic quite quickly, judging by the examples. reply andrewghull 13 hours agoparentprevThanks for the comment, and yes we'd love to add more docs. (it's time consuming!) I understand that even a simple-sounding game can feel daunting to translate into a formalized API. I did record a video of me building part of a game FWIW. https://docs.boardzilla.io/game/creating-a-game reply smcin 10 hours agoprevYou're using Typescript+CSS. Was hoping you'd support Python on the server, for game logic. Your timing is good because developers and publishers currently using BGA are unsettled and spooked by Asmodee's business model. BGA Studio's stack is JS/CSS + PHP (client and server) + MySQL [0] Yucata.de is JS + HTML + .NET 4.5 on the server [1] (TTS was using Lua, which I looked into but seemed eccentric and limited, it's not even OO, why on earth choose a non-OO language for a boardgame). Also here's a useful review of sites/frameworks from 2021: \"VassalEngine: Survey of other boardgame software\" [2] Can you please please please integrate with Python on the server? [0]: https://boardgamearena.com/doc/Studio [1]: https://www.yucata.de/en/FAQ#t17 [2]: https://forum.vassalengine.org/t/survey-of-other-boardgame-s... reply gam6121147 9 hours agoparent> why on earth choose a non-OO language for a boardgame Lua has usually been a very popular choice for game developers. One reason is that it gives you an easy way to embed a scripting language in your game. You can do OOP without classes [0]. > Can you please please please integrate with Python on the server? You expect the developer to port and maintain their entire project to Python because you can't be bothered to learn a new programming language? [0]: https://en.wikipedia.org/wiki/Lua_(programming_language)#Obj... reply Ensorceled 9 hours agorootparent>> > Can you please please please integrate with Python on the server? > You expect the developer to port and maintain their entire project to Python because you can't be bothered to learn a new programming language? They are literally begging, 3 \"pleases\", rather than \"expecting\". reply askvictor 8 hours agorootparentprevI remember a book on the shelf at home in the 90's entitled \"Object Oriented Programming in Macro Assembler\". And indeed, C has function pointers, so you can do OO. Just not some of the more fun stuff. reply tgtweak 9 hours agorootparentprevSeems pretty trivial to support Python serverside. Python is very easy to run in a lightweight isolated container (and you can compile it to static executable). The game data that the engine makes available to the server side code looks pretty standard and doesn't look like a huge shim to maintain. You could even cross compile the Python to typescript for a simplified server side environment (typy for example). reply monkeywork 9 hours agorootparent>Seems pretty trivial to ..... Then go do it. If it's pretty trivial.... reply joshbuddy 2 hours agoparentprevThe reason the server isn't in Python is because in boardzilla, you don't express your game twice, once for the client and once for the server. Instead, you express the game once in typescript, and that code is actually used on both the server and client. The client is operating a lot like the server, just without the hidden information. reply leetrout 9 hours agoparentprevWhat is Asmodees business model and what does it have tondo with BGA? reply oezi 2 hours agorootparentAsmodee is hedgefund-owned French board game company. Drive by the injected money they have been 'consolidating' the board game publishing industry. For instance they bought the rights for Settler of Catan. A lot of people fear that they will raise prices or reduce the offering to return money to the investors at some point. reply rtpg 8 hours agorootparentprevAsmodee owns BGA, also owns a huge amount of game publishers. I have a decent collection and then looked at my games and noticed that Asmodee published like 70% of my games. Just feels like we're reaching a point where if you want to get a game published at a larger scale and distributed, you'll likely really be incentivised to go through Asmodee. reply factormeta 9 hours agoparentprevthere is nothing wrong with Typescript, but it is the react part that worries me. Wouldn't you want something more light weight on the front end than react for games? Like svelt, marko or solid? reply joshbuddy 2 hours agorootparentThe React aspects are really minimal, all the animation logic is by outside of React. We're really just using it for the jsx syntax to make it easier to return markup-looking components. reply jvehent 12 hours agoprevDon't called your project *Zilla. The copyright owners of Godzilla are known to go after everyone who tries to use the \"Zilla\" suffix. Mozilla learned its lesson long ago and had to negotiate a special agreement. reply joshbuddy 12 hours agoparentUm, wow, I had no idea. I've got some other pretty good domain names at the ready, so, maybe I need to pull the trigger on that. reply gkanai 11 hours agorootparentAs an 10 year Mozilla veteran, I agree you should switch domains/project names. Mozilla was able to negotiate a deal for a number of reasons- Mitchell herself is a lawyer, the project is a NPO, and probably other reasons too, but it's not worth the effort to defend a slightly infringing name against the copyright holder in this case. reply soneca 11 hours agorootparentprevUsing this thread as bug report. There is a typo on the “Credits” card on game pages: ”arist” -> ”artist” reply joshbuddy 11 hours agorootparentThanks Soneca. Fixed and deployed. reply doublerabbit 12 hours agorootparentprevHi Josh, I'm receiving an error upon accessing the page. Object.hasOwn is not a function. (In 'Object.hasOwn(t,n)', 'Object.hasOwn' is undefined) reply jitl 7 hours agorootparentYour browser is too old to work with the library. Object.hasOwn is available for 93% of users according to https://caniuse.com/mdn-javascript_builtins_object_hasown reply joshbuddy 1 hour agorootparentPolyfill added btw, happy to take any more bug reports either here or in discord and thanks! reply joshbuddy 2 hours agorootparentprevI'm going to add a polyfill later for this. Thanks for the bug reports reply Aeolun 10 hours agoparentprevHow would this even remotely relate to Godzilla or it’s copyright? It’s not even close to a big scary nuclear fueled monster. I’ll note that the only TM case they ever lost was against a company selling trash bags named ‘trashzilla’, partially because it did not constitute a danger to Toho’s business interests. Edit: Uh, I missed the logo. Definitely a problem. reply cyberninja15 11 hours agoparentprevWoah, this is wild. Never knew how aggressive the copyrights were for *Zilla. reply gkanai 11 hours agorootparentWhen you consider the most recent Godzilla film, Minus One, is the 37th in that franchise, and was not only nominated for an Oscar, and may be it's most lucrative, you can see why Toho would aggressively police that copyright. Minus One is legit a great film. reply thfuran 7 hours agorootparentNo, I can't. That makes it even less reasonable. Edit: on the other hand, they apparently elected to use Godzilla as their logo, which kind of ruins the otherwise considerable unrelatedness. reply paradox460 10 hours agorootparentprevWonder how the people of Zillah Washington feel about it reply marssaxman 8 hours agorootparentI'm fairly sure that the antiquity of the Book of Genesis trumps any conceivable trademark claim! reply xky 10 hours agoparentprevMeta also pursues trademarks applicants for trademarks ending in 'book' as well. reply 20after4 8 hours agorootparentThat seems a lot more dubious than the *zilla trademark. reply dharmab 9 hours agoparentprevRevzilla and Partzilla also had a long legal conflict over the *zilla name. reply pbhjpbhj 2 hours agorootparentIt's pretty clear that \"zilla\" is a genericised word suffix, cf \"bridezilla\", in English. Whilst it seems to originate with Godzilla -- which honestly also doesn't seem to associate with a particular company but instead I would say it's a now traditional monster name in stories, like Dracula -- noone is confusing this with any Godzilla franchise. These sort of attempts to own a word sten, across trademark categories, are an over-step that legislators need to rein in IMO. Does the recent Sky trademark battle speak to this? This comment is entirely my opinion and does not relate to my employer. reply dharmab 2 hours agorootparent> Whilst it seems to originate with Godzilla -- which honestly also doesn't seem to associate with a particular company but instead I would say it's a now traditional monster name in stories, like Dracula This is a really weird take. Dracula is in the public domain, while every piece of Godzilla media is still copyrighted and trademarked to Toho, one of the big four movie studios of Japan. reply darekkay 2 hours agoprevIf you're looking for something more low-level, I can recommend boardgame.io [1]. [1] https://boardgame.io/ reply joshbuddy 1 hour agoparentYeah, I can totally recommend boardgame.io as well. Nicolo is super helpful and friendly. Also check out his other project https://boardgamelab.app/. reply cableshaft 10 hours agoprevNice. I have a ton of board game prototypes I'd like to make web games of at some point, so I spent a little time writing up some functions for a general board game library myself for making board games, but didn't get too far (just some grid generation, custom die rolling, card drawing, tournament functions, some tests to demonstrate various scenarios, and a few other things). But I'll never have time to do it properly, especially juggling other projects. Maybe I'll just see how easy it is to work with your library. I concur it's a bit painful to learn the BGA platform, as it's 1) stuck on their platform, you won't be able to put the game anywhere else, and 2) all in PHP, which I prefer not to write in anymore, and 3) with documentation scattered around in various places, including a couple of Powerpoint presentations. reply eps 2 hours agoprevAll sample games show an error: Object.hasOwn is not a function. (In 'Object.hasOwn(e,A.from)', 'Object.hasOwn' is undefined) The stack trace is: su@https://www.boardzilla.io/build/_shared/chunk-ZEYFFM3P.js:16:114536 Fs@https://www.boardzilla.io/build/_shared/chunk-QNN53S6I.js:9:128279 Ch@https://www.boardzilla.io/build/_shared/chunk-QNN53S6I.js:11:180657 ... Perhaps put some polyfill in place? reply joshbuddy 1 hour agoparentPolyfill is in place, please let me know if its fixed and thanks reply joshbuddy 2 hours agoparentprevThank you, polyfill incoming! reply Longwelwind 1 hour agoprevYou might be interested by an open-source project I started a few years ago but never really finished: https://github.com/ravens-engine/core I had the idea after developing Swords and Ravens, an online adaption of A Game of Thrones: the Board Game (2nd edition), and realizing that there was a way to make a library to allow people to create board games without having to bother with the network part. I also wrote a blog post about it: https://longwelwind.net/blog/networking-turn-based-game/ Good luck in your project! reply joshbuddy 1 hour agoparentOh wow, thanks, I'll check this out. We were doing something more like merely recording the game moves and applying them to arrive at the game state (having deterministic per-session rng). It's a good way to do it, but for the same of simplicity, this time around we just record the whole json blob per move. It simplifies undos and it also lets us easily move through history if you want to view a previous state. But I'd love to try to apply some of your lessons to a later version. The challenge for us now is making a good api for expressing board games. That's been really tricky tbh. We've been working on this for ~7 years and have gone through many iterations of this api trying to get it right. I sure hope we're closer this time. reply Longwelwind 1 hour agorootparentYou might be interested in the Phase abstraction of my engine. Basically you represent your board game as a tree of phases through which your game progresses in. Each node of your phase tree can contain state. At any point, your game is at leaf of your tree, and the state of the game is composed of all the states of all the nodes from the current leaf-phase to the root of the tree. Each leaf node also have a set of possible inputs describing what possible action players can take to progress into the game. This makes it really easy to handle games that have special \"rare\" phase that can break the flow of the game (for 7 Wonders: Duel, for example, phases where players must resolve a wonder's effect that asks them to destroy an opponent's card). reply JoeOfTexas 5 hours agoprevThis is a very difficult space. I think the best way to be successful is to have an instant hit game, or spend tremendous community effort getting players every day of the year. I tried my luck with https://acos.games early on when there was very few alternatives to BGA. I still develop for it behind the scenes when I'm bored. My biggest failure is UI/UX. People don't care too much about the awesome technology hiding behind the scenes. =( reply jmpavlec 4 hours agoparentLooks really well done. The documentation was very well written. I don't have a use case at the moment but it was interesting reading through the documentation to see what is possible. The state management and replaying games seemed neat. I'm writing my own set of games and just thinking about making the system flexible enough for others to integrate with feels like a big undertaking. Cudos for the work you put in. reply jamager 13 hours agoprevLooks very cool, congrats on the initiative! I think it can be great to develop prototypes, but to build a BGA alternative you will need permission from the publishers to implement / distribute their games, and given how big is BGA and that it belongs to Asmodee, that is going to be tough... Hope I am wrong, good luck! reply vindex10 13 hours agoparentIANAL, it seems, game mechanics can't be copyrighted, unless you use the same images for cards / board, or literally copy the rule book. > In short: you can trademark the name, logos. Have copyright on art (even the board itself as art). You can't copyright the mechanics, processes or rules. That explains the Monopoly/Scrabble clones. https://www.reddit.com/r/gamedev/comments/10f2nov/comment/j4... which refers to this youtube video: https://www.youtube.com/watch?v=iZQJQYqhAgY reply jamager 1 hour agorootparentMechanics can't be copyrighted, but the board gaming community can smell rip offs from afar and don't particularly appreciates them. Besides, it's going to be madness to advertise that you have the game X on your platform but can't use the name or any artwork. If I were the OP, i'd try to gear this towards publishers and make it as easy as possible to create functional prototypes. They need to iterate countless times on prototypes, one little change at a time, and this could be a huge time saver for them, facilitate playtasting, etc... reply joshbuddy 13 hours agoparentprevHey jamager, thanks for the kind comments. Totally agree, Asmodee has basically prevented a real competitor from entering the market. We have an idea in mind that doesn't involve going up against BGA, but instead looks at more smaller games from indie publishers. Not sure how viable that is, but not sure what else someone is supposed to do. reply iamevn 12 hours agoprevReally cool looking! Excited to mess around with this. Happy to see the everyPlayer[1] flow command to allow for simultaneous actions. This is something that's missing from the other boardgame frameworks I've tried. [1]: https://docs.boardzilla.io/game/flow#everyplayer reply lencastre 12 hours agoprevA couple questions: did you get permission from Friese for Funkenschlag? Why Typescript? Any plans to control for players who rage quit, anti-play (in all forms), cheat (two connections from same IP)? I tried but couldn’t test right away, is this more like BGA or TTS? reply andrewghull 12 hours agoparentYes it's like BGA, not TTS. No, we haven't done much to secure the online playing experience for strangers other than simply enforcing the rules of the game. We've been so far more focused on the API side of building game than the service side of having customers, but these are all on our radar as table stakes to make this an nice experience for random players. reply sjrd 10 hours agoprevThis looks very interesting. One question: would I be able to develop a game using my favorite compile-to-JS language? Or would the game infrastructure force me to use TypeScript? reply mNovak 4 hours agoprevLooks great! I'll poke around--my dream is make a WH40k-like turn based strategy.. curious to see if this can handle sufficiently convoluted rules. reply joeld42 13 hours agoprevHey this looks great. I'm working on some AI bots for board games and this might be really useful as a frontend for local testing of the bot players. I'll check it out. reply andrewghull 13 hours agoparentWe've been planning to add a way to do pluggable AI functions (both for solitaire play and for testing). If you want to try, take a look at the TestRunner class for an easy way to automate playing the rules for multiple players. reply joemi 13 hours agoprevI'm looking forward to trying this out! You might want some sort of mention in the website copy about it being self-hostable (unless I'm misinterpreting the docs). The website copy talks about your hosting and the related benefits (as it rightly should), but I didn't see any mention of self-hosting so I almost didn't click through to the docs to see that local development is possible. reply joshbuddy 13 hours agoparentHey Joemi. I wanted to make a self-hosted single game runner, but I haven't gotten to it yet. The interface used by the game/server is detailed here https://github.com/boardzilla/boardzilla-devtools/blob/main/... so it should be pretty easy to make this runner. I hope to get to it soon. reply joemi 10 hours agorootparentAh, I guess I need to look at the docs more closely. reply jslakro 9 hours agoprevGood initiative but I consider screentop.gg is a solid and pretty consolidated option on that field. It's quite easy to create games on that system reply Illniyar 12 hours agoprevWow, this seems unreasonably well made! Kudos. I'll definitely try it out. reply fayazara 1 hour agoprevOne thing about the JS ecosystem, there's a shit ton of libraries and frameworks. reply doublerabbit 12 hours agoprevThe page immediately errors for me. Error Object.hasOwn is not a function. (In 'Object.hasOwn(t,n)', 'Object.hasOwn' is undefined) The stack trace is: o@https://www.boardzilla.io/build/_shared/chunk-IP5CEA34.js:3:... rt@https://www.boardzilla.io/build/root-NYPPT2G6.js:3:2506 Fs@https://www.boardzilla.io/build/_shared/chunk-QNN53S6I.js:9:... Ch@https://www.boardzilla.io/build/_shared/chunk-QNN53S6I.js:11... _h@https://www.boardzilla.io/build/_shared/chunk-QNN53S6I.js:11... Jg@https://www.boardzilla.io/build/_shared/chunk-QNN53S6I.js:11... Ei@https://www.boardzilla.io/build/_shared/chunk-QNN53S6I.js:11... rs@https://www.boardzilla.io/build/_shared/chunk-QNN53S6I.js:11... Rh@https://www.boardzilla.io/build/_shared/chunk-QNN53S6I.js:11... Rh@[native code] Ba@https://www.boardzilla.io/build/_shared/chunk-QNN53S6I.js:4:... Ia@https://www.boardzilla.io/build/_shared/chunk-QNN53S6I.js:4:... iPhone - ios 14.8 reply joshbuddy 11 hours agoparentHey Doublerabbit. Thank you for the bug report and stack trace, super helpful. I'm taking a look at it now. I've got to go to bed soon, but I'll try to fix this as soon as I can. reply joshbuddy 11 hours agoparentprevI think I have a fix deployed now. Please try it again if you have a moment. reply doublerabbit 11 hours agorootparentWonderful, Working perfectly. Thank you. reply doublerabbit 10 hours agorootparentHi Josh, It appeared I'm still receiving errors when attempting a few of the boards. Error Object.hasOwn is not a function. (In 'Object.hasOwn(e,A.from)', 'Object.hasOwn' is undefined) The stack trace is: su@https://www.boardzilla.io/build/_shared/chunk-ZEYFFM3P.js:16... Fs@https://www.boardzilla.io/build/_shared/chunk-QNN53S6I.js:9:... Ch@https://www.boardzilla.io/build/_shared/chunk-QNN53S6I.js:11... _h@https://www.boardzilla.io/build/_shared/chunk-QNN53S6I.js:11... Jg@https://www.boardzilla.io/build/_shared/chunk-QNN53S6I.js:11... Ei@https://www.boardzilla.io/build/_shared/chunk-QNN53S6I.js:11... rs@https://www.boardzilla.io/build/_shared/chunk-QNN53S6I.js:11... Rh@https://www.boardzilla.io/build/_shared/chunk-QNN53S6I.js:11... Rh@[native code] Ba@https://www.boardzilla.io/build/_shared/chunk-QNN53S6I.js:4:... Ia@https://www.boardzilla.io/build/_shared/chunk-QNN53S6I.js:4:... reply josephcsible 13 hours agoprev [–] https://github.com/boardzilla/boardzilla-core/blob/main/LICE... Commons Clause :( If you're worried about some corporation leaching off of you, why not go AGPLv3 instead? reply andrewghull 13 hours agoparent [–] Co-creator here. I thought AGPL was ok as well, but Commons Clause specifically tried to address perceived deficiencies in the AGPL although I admit that it didn't really seem to catch on. I'm not super well versed in the technical details. Why would you recommend AGPL? reply josephcsible 13 hours agorootparent [–] https://fedoraproject.org/wiki/Licensing/CommonsClause has several links explaining what's wrong with the Commons Clause. The reason the AGPLv3 is my preferred replacement is that the FAANGs are irrationally afraid of it and refuse to let it be used anywhere in the company (e.g., https://opensource.google/documentation/reference/using/agpl...), but it's still 100% Free Software and Open Source and so allowed to be packaged in Linux distributions that disallow proprietary software. reply andrewghull 13 hours agorootparentAppreciate that! We'll take another look at this. reply pessimizer 13 hours agorootparentRemember (I'm sure you do) that you can relicense the code for the use of anyone who wants to pay you for it. AGPLv3 protects you, but doesn't hold you back. reply whatyesaid 6 hours agorootparentprevDamn, poor little Google seems really scared. I'm not sure if that info is true. You have to provide the whole source of the entire product and not just a segment of code (eg library) you forked which they do often? reply Tyr42 11 hours agorootparentprevI'd also suggesting reading some of /dev/lawyer's blog posts, though I'm cooking and can't pull up anything in particular ATM. reply dingnuts 13 hours agorootparentprev [–] How is AGPLv3 \"Free\" software when it is a proprietary source-available license? The AGPL requires that anyone who forks the software make their changes available to the owner regardless as to whether they intend to publish those changes. That sounds like proprietary copyright wherein third parties are allowed to view, but not modify, the source code, to me. RMS has given software people a really weird definition of \"free\" reply boucher 12 hours agorootparentI've seen a lot of takes on the GPL but this is a new one to me. As another comment points out, you only have to make changes available to others if you make the software itself available to others; either by distributing a binary or by using it to provide a web service. But this is, fundamentally, the point of \"Free\" software: someone who uses a piece of software should be entitled to change that software as they desire. This obviously implies that if you make your own modified software available to others, they too must be able to make their own changes. reply josephcsible 12 hours agorootparentprevThe AGPLv3 only requires you to make your modified source code available to people that you choose to give a copy of the binary to or allow to use on your computer over the network. You never have to make anything available to the original owner if you don't let them use your modified version. reply SamBam 8 hours agorootparentprev [–] > That sounds like proprietary copyright wherein third parties are allowed to view, but not modify, the source code, to me. ... Except that they're literally allowed to modify the source code? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Boardzilla is a new framework designed for developing web-based board games, created out of dissatisfaction with existing tools.",
      "The framework simplifies the development process by allowing developers to use the same code for both the client and server, removing concerns about state management, persistence, and networking.",
      "Although still a work in progress, Boardzilla has released developer documentation and sample games on Github, actively adding features and games, and welcomes feedback from users."
    ],
    "commentSummary": [
      "Boardzilla is a web-based framework that streamlines the creation of board games, and user feedback is being used to make improvements.",
      "User suggestions include prioritizing examples and documentation on the landing page and providing a step-by-step tutorial.",
      "Discussions also cover topics such as incorporating Python on the server side, utilizing Lua in game development, and concerns regarding using React for game development. Additionally, trademark conflicts, bug reports, and alternative game development platforms are being discussed.",
      "Developers are considering implementing security measures against cheating and rage quitting, adding support for AI bots, and exploring self-hosting options.",
      "There is also a conversation about selecting the appropriate software license, particularly the AGPLv3 and Commons Clause licenses."
    ],
    "points": 445,
    "commentCount": 80,
    "retryCount": 0,
    "time": 1706555573
  },
  {
    "id": 39175883,
    "title": "Florida House approves bill banning social media for kids <16",
    "originLink": "https://abcnews.go.com/GMA/Family/florida-house-representatives-approves-bill-ban-social-media/story?id=106672586",
    "originBody": "ABC News Video Live Shows Election 2024 538 Interest Successfully Added We'll notify you here with news about Turn on desktop notifications for breaking stories about interest? OffOn LOG IN Stream on Florida House of Representatives approves bill to ban social media for kids under 16 The bill now heads to the GOP-controlled state Senate for consideration. ByKatie Kindelan January 25, 2024, 6:26 PM 2:02 Stock photo Lakshmiprasad S/Getty Images/iStockphoto Legislators in the Florida House of Representatives on Wednesday approved what could be the strictest regulation on social media and kids in the country. Florida House Bill 1 would prohibit children under the age of 16 from using most social media platforms, regardless of parental approval. The social media platforms the bill would target include any site that tracks user activity, allows children to upload content or uses addictive features designed to cause compulsive use. The House passed the bill by a vote of 106 to 13, with many Democrats joining the chamber's Republican majority in support of the bill. It now heads to the Republican-controlled Senate for consideration. State Rep. Fiona McFarland, a Republican, described social media as \"digital fentanyl\" for kids when promoting the bill on the House floor. \"It's like a digital fentanyl, and even the most plugged-in parent or attuned teen has a hard time shutting the door against these addictive features,\" McFarland said. Another Republican legislator, state Rep. Tyler Sirois, also argued in support of the bill, saying that social media platforms are \"taking advantage of kids growing up.\" Most social media platforms currently have a minimum user age of 13. Editor’s Picks US surgeon general issues major advisory on social media use and teens' mental health Mom's viral post about another's pool day shows 'fake' aspect of social media Mom influencer opens up about why she erased her kids’ faces from social media The bill would allow the termination of social media accounts belonging to kids under 16, including the deletion of information for pre-existing accounts. It would also require that social media sites use \"reasonable age verification methods\" to verify users' ages. The Florida State capitol. STOCK PHOTO/Getty Images The bill is opposed by those who argue that it infringes on First Amendment and parental rights. MORE: Penélope Cruz explains why her kids don't have social media Meta, the parent company of Facebook and Instagram, argued that social media regulation should be overseen on on a federal basis, and that parental approval should be sufficient for minors' use of social media, according to The Associated Press. \"Many teens today leverage the internet and apps to responsibly gather information and learn about new opportunities, including part-time jobs, higher education, civic or church gatherings, and military service,\" Meta representative Caulder Harvill-Childs wrote to the House Judiciary Committee, according to the AP. \"By banning teens under 16, Florida risks putting its young people at a disadvantage versus teens elsewhere.\" The legislative action in Florida comes at a time when social media companies, as well as parents, legislators and medical providers, are trying to figure out how to approach social media and kids. Meta on Thursday announced a series of new safety measures aimed at teens, including restricting private messages from strangers and instituting new parental controls. Stock photo Lakshmiprasad S/Getty Images/iStockphoto The new safety measures came just one day after New York City Mayor Eric Adams classified social media as a \"public health hazard\" and an \"environmental toxin,\" saying young people must be protected from \"harm\" online. In his State of the City address Wednesday, Adams claimed TikTok, YouTube and Facebook are \"fueling a mental health crisis by designing their platforms with addictive and dangerous features.\" \"We are the first major American city to take this step and call out the danger of social media like this,\" the mayor said. \"Just as the surgeon general did with tobacco and guns, we are treating social media like other public health hazards and ensuring that tech companies take responsibility for their products.\" MORE: American Psychological Association issues advisory for teens and social media Last year, the American Psychological Association issued first-of-its-kind recommendations intended to help teenagers use social media safely, including setting time limits, encouraging family discussions about social media and parental monitoring. The U.S. Surgeon General last year also issued an advisory warning of an urgent public health issue regarding social media usage and youth mental health. In the advisory, released in May, Dr. Vivek H. Murthy called for more research to determine the extent of mental health impacts on young people, including the type of content generating the most harm, societal factors that could protect youth and ways in which social media can be beneficial. \"To date, the burden of protecting youth has fallen predominantly on children, adolescents and their families,\" Murthy wrote. \"The entire burden of mitigating the risk of harm of social media cannot be placed on the shoulders of children and parents.\" Murthy called on social media companies to prioritize safety and privacy in their product designs and ensure minimum age requirements are enforced. He said he believes 13 is \"too early\" for kids to be on social media, describing the age as a \"time when kids are developing their identity, their sense of self.\" The advisory also outlined how policymakers can enact change in three ways: creating policies limiting access to potentially harmful content, developing curricula about digital and media literacy in schools, and increasing funding for related research. Related Topics Technology Twitter Facebook Top Stories E. Jean Carroll says she plans to use $83 million on 'something Donald Trump hates' Jan 29, 12:51 PM Trump lawyer blasts independent monitor's report ahead of fraud trial ruling Jan 29, 11:04 PM Activists splatter 'Mona Lisa' with soup in Louvre Jan 28, 11:13 AM Dying thief who stole ‘Wizard of Oz’ ruby slippers from the Judy Garland Museum gets no prison time Jan 29, 5:07 AM 5 arrested in deaths of 6 found murdered in desert: Sheriff Jan 30, 2:54 AM ABC News Live 24/7 coverage of breaking news and live events ABC News Network About Nielsen Measurement Children's Online Privacy Policy Contact Us Do Not Sell or Share My Personal Information Interest-Based Ads Privacy Policy Terms of Use Your US State Privacy Rights © 2024 ABC News",
    "commentLink": "https://news.ycombinator.com/item?id=39175883",
    "commentBody": "Florida House of Representatives approves bill to ban social media for kidsBut the only way to do this is to require ID checks, effectively regulating and destroying the anonymous nature of the internet That seems intuitive, but it's not actually true. I suggest looking up zero-knowledge proofs. Using modern cryptography, it is easy to send a machine-generated proof to your social media provider that your government-provided ID says your age is ≥ 16, without revealing anything else about you to the service provider (not even your age), and without having to communicate with the government either. The government doesn't learn which web sites you visit, and the web sites don't learn anything about you other than you are certified to be age ≥ 16. The proofs are unique to each site, so web sites can't use them to collude with each other. That kind of \"smart ID\" doesn't have to be with the government, although that's often a natural starting point for ID information. There are methods which do the same based on a consensus of people and entities that know you, for example. That might be better from a human rights perspective, given how many people do not have citizenship rights. > (and probably unconstitutional under the First Amendment, to boot.) If it would be unconstitutional to require identity-revealing or age-revealing ID checks for social media, that's all the more reason to investigate modern technical solutions we have to those problems. reply px43 6 hours agorootparentA super interesting example of this is the proof-of-passport project. https://github.com/zk-passport/proof-of-passport Today you can scan your passport with your phone, and get enough digitally signed material chained up to nation level passport authorities to prove anything derived from the information on your passport. You could prove to an arbitrary verifier that you have a US passport, that your first name starts with the letter F, and that you were born in July before 1970, and literally share zero other information. reply xyzzy123 1 hour agorootparentThe selective disclosure is super cool, I wonder how it works since smthing like a hash of DG1 is what is actually signed, how can you selectively disclose verified data from \"inside\" the hashed area? It does not sound very feasible to me but I am not an expert in zk-snarks etc. There are some wrinkles that prevent passport data being used more broadly - technically it is a TOS violation to verify passports / use the ICAO pkd without explicit permission from ICAO or by direct agreement with the passport holder's CSCA (country signing certificate authority). Some CSCAs allow open use but many do not. Also, without being too pedantic about it, what you are able to prove is more like possession of a document. An rfid passport (or rfid dump & mrz) - or in fact any kind of identity document - does not prove that you are the subject - you need some kind of biometric bind for that. reply protocolture 7 hours agorootparentprevI dont want government crapware on my device to access the internet. I also dont want third party crapware on my device to access the internet. \"wowee it can be done without revealing my identity to anyone but the government or the corp running the chain\" No thanks reply mirekrusin 5 hours agorootparentIdentity is not revealed. reply sangnoir 4 hours agorootparentDo you think the NSA would balk at that challenge? reply mirekrusin 2 hours agorootparentIt doesn't have to be based on crypto designed by NSA, does it? reply sangnoir 2 hours agorootparentDesign is far from the only threat vector. Any implementation that is less than perfect is prone to all kinds of attacks. A few years ago, there was a report that the NSA could decrypt a double-digit percentage of encrypted web traffic thanks to a larger-than-expected bag of factored primes they keep handy. reply satellite2 12 hours agorootparentprevI'm not a cryptographer so I might miss something but I have the impression that - either a stolen card can be reused thousands of time meaning that it's so easy to get a fake that it's not worth the implementation cost - either there is away to uniquely identify a card and then it becomes another identifier like tracking ids. reply ndriscoll 11 hours agorootparentAssuming you can make active queries to the verifier, you could do something like - Have your backend generate a temporary AES key, and create a request to the verifier saying \"please encrypt a response using AES key A indicating that the user coming from ip X.Y.Z.W is over 16\". Encrypt it with a known public key for the verifier. Save the temporary AES key to the user's session store. - Hand that request to the user, who hands it to the verifier. The verifier authenticates the user and gives them the encrypted okay response. - User gives the response back to your backend. Potentially the user could still get someone to auth for them, but it'd at least have to be coming from the same IP address that the user tried to use to log into the service. The verifier could become suspicious if it sees lots of requests for the same user coming from different IP addresses, and the service would become suspicious if it saw lots of users verifying from the same IP address, so reselling wouldn't work. You could still find an over-16 friend and have them authenticate you without raising suspicions though, much like you can find an over-21 friend to buy you beer and cigarettes. Since you use a different key with each user request, the verifier can't identify the requesting service. Both the service and the verifier know the user's IP, so that's not sensitive. If you used this scheme for over-16 vs. over-18 vs. over-21 services, the verifier does learn what level of service you are trying to access (i.e. are you buying alcohol, looking at porn, or signing up for social media). Harmonizing all age-restricted vices to a single age of majority can mitigate that. Or, you could choose to reveal the age bucket to the service instead of the verifier by having the verifier always send back the maximum bucket you qualify for instead of the service asking whether the user is in a specific bucket. reply woodruffw 10 hours agorootparentIf you can make active queries to the verifier, so can any adversarial party. These kinds of ZK-with-oracle schemes need to be very carefully gamed to ensure they're truly ZK, and not just \"you learn nothing if you only query once.\" > and the service would become suspicious if it saw lots of users verifying from the same IP address This implodes under CGNAT, cafe internet, hotel internet, etc. reply ndriscoll 10 hours agorootparentYou can make active queries, with the user's involvement. The verifier can potentially have a prompt with e.g. \"The site you were just on would like to know that you are over 21. Would you like to share that with them?\" We do need to get people onto ipv6 so CGNAT can die. Restricted services could potentially disallow signups or require more knowledge (e.g. full ID) if coming from shared IPs as a risk mitigation strategy, depending on how liable we want to hold them to properly validate age. If you've already signed up for facebook at home, obviously you don't need to validate your age again at the cafe. Fake IDs exist in the real world. The system doesn't have to be perfect, and we can say that there's some standard of reasonable verification that they should do for these sorts of cases. Personally I'm more in favor of an approach where sites label their content in a way where parents can configure filters (ideally using labels that are descriptive enough that we don't get into fights over what's \"adult\", and instead leave that decision to individual families), but if we're going to go an ID-based route, there are at least more private ways we could do it, and I think technologists should be discussing that, and perhaps someone at one of these big companies can propose it. reply hanspragt 11 hours agorootparentprevThere is no way my 94-year-old neighbor can successfully do any of that. reply ndriscoll 10 hours agorootparentThat's the protocol for the computer, similar to oauth. From the user perspective, your 94-year-old neighbor would have an account with id.gov that they've somehow previously established (potentially the DMV or the post office does this for them), and the user flow works much like \"Sign in with Google\" buttons do today. reply jlokier 6 hours agorootparentprevThere's a unique identifier, but it's your secret and can't be used for tracking. Sites needing verification don't learn anything except that you \"have\" a token matching the condition they are checking. This includes not learning your unique identifier, so they can't use it for tracking. The issuer also doesn't learn anything about your verification queries. You have an incentive to keep the secret token to yourself, and would probably use existing mechanisms for that: You might manage like your phone number, private email and other personal accounts today. Not perfect, but effective most of the time for most people. You might decide to share it with someone you trust, like your sibling. That's up to you, but you wouldn't share it widely or with people you don't trust, even under pressure, because: To prevent mass reuse of stolen tokens, it's possible to use more cryptography to detect when the same token is reused in too many places, either on the same site or across many sites, without revealing tokens that don't meet the mass-reuse condition, so they still can't be used for tracking. If mass-reused tokens auto-revoke, they can't be reused thousands of times by anyone, and that also provides an incentive to avoid sharing with people you don't trust. I won't pretend this is trivial. It's fairly advanced stuff. But the components exist these days. The last paragraph above requires combining zero-knowledge proofs (ZKP) with other branches of modern cryptography, called multi-party computation (MPC) and perhaps fully homomorphic encryption (FHE). reply neom 12 hours agorootparentprevIt would be neat if some authority like the passport office or social security office also provided a virtual ID that includes the features OP described and allowed specific individual attributes to be shared or not shared, revoked any time, much like when you authenticate a 3rd party app to gmail or etc. reply smeej 8 hours agorootparentPutting on my conspiracy hat for a minute: They don't want to make it easy for you to authenticate anonymously. They obtain their surveillance data from the companies that tie you, individually, to your data. They'd be shooting themselves in the feet. reply plorg 7 hours agorootparentprevYeah, hell no. reply shuntress 10 hours agorootparentprevThese are the things that the post office should be handling. reply WalterBright 8 hours agorootparentprevYou can also use fake ID to buy booze. Making it illegal is, by itself, enough to discourage a lot. reply mirekrusin 5 hours agorootparentYou can't run for loop on buying booze with fake id. reply j16sdiz 7 hours agorootparentprevThose need some kind of face to face interaction. The perceived risk of being caught is much higher. reply vlovich123 9 hours agorootparentprevmDL wide scale rollout works be using the trusted computing element that is part of your phone and enrollment would be the same as obtaining a driver’s license in the first place. There is no physical card - there is an attestation that only an enrolled device can hand out with revocation support in case of security flaws. Is it going to be absolutely secure? No. The cost just needs to be high enough that it becomes inaccessible to the vast majority of adolescents. Theft of your parents phone becomes a lot easier attack vector but phone biometrics/password requirements will thwart that for most parents. This doesn’t need to be 100% fool proof. reply lukev 14 hours agorootparentprevIt'd be cool if any of the proposed bills actually suggested something like this. They do not. They specify an ID check. reply Aurornis 11 hours agorootparentprev> The government doesn't learn which web sites you visit, and the web sites don't learn anything about you other than you are certified to be age ≥ 16. If the zero-knowledge proof doesn't communicate anything other than the result of an age check, then the trivial exploit is for 1 person to upload an ID to the internet and every kid everywhere to use it. It's not sufficient to check if someone has access to an ID where the age is over a threshold. Implementing a 1:1 linkage of real world ID to social media account closes the loophole where people borrow, steal, or duplicate IDs to bypass the check. reply vlovich123 9 hours agorootparentAs I mentioned elsewhere, you’re falling for letting perfect be the enemy of good. The ZKP + phone biometrics only needs to raise the cost of bypass above what adolescents have access to. And no, you can’t just share the same ID because there’s revocation support in the mDL and it’s difficult to extract the raw data once it’s stored on the trusted element. This is very similar to how credit cards on phones work which are generally very difficult to steal. reply Aurornis 8 hours agorootparentSorry, you’re not thinking like a group of 15 year olds trying to get online. The revocation list means nothing when they can get ahold of someone’s older sibling’s ID and sign up for social media. Did everyone just forget what it’s like to an ambitious kid who wants to get online? Do people really think a platform that needs people to jump through these hoops and use this imaginary international ID architecture is feasible? Does anyone really think that kids won’t just set their location to Estonia and/or use a VPN to circumvent all of this? reply protocolture 7 hours agorootparentThis. The parties that this will detriment are all older people. Kids will simply bypass it. reply vlovich123 3 hours agorootparentprevYou’re thinking like a group of technically proficient 15 year olds and their friends. That’s a small minority. The vast majority of teens are likely to be stymied. Revocations are not for the individual ID but if an exploit is found compromising the IDs stored on a trusted element. Your older siblings ID can’t be used to sign for millions of accounts - just those who the older sibling lets borrow their phone that has their ID (and assuming there isn’t some kind of uniqueness cookie that can be used to prevent multiple accounts under a single ID). That’s a much different and more manageable problem (fake ids via older siblings have been a thing for forever). reply Cheezemansam 7 hours agorootparentprev>As I mentioned elsewhere, you’re falling for letting perfect be the enemy of good No, this line of reasoning deserves nothing but absolute contempt when it comes to laws. We are not talking about getting the finnicky API to work at your job. Too often laws have had unintended consequences as a result of loopholes or small peculiarities. If the damn law doesn't even work on a fundamental level then it should be opposed on principle. reply vlovich123 3 hours agorootparentYou’ve just described literally every single law. Congrats. You’re now appreciating what it’s like to live in a law-based society. reply j16sdiz 7 hours agorootparentprevIt don't have to be perfect, but it need to have some way to do spot checks. If there is no risk involved, everyone will jump on doing it. reply jlokier 5 hours agorootparentprevThere are technical methods to detect and revoke large-scale reuse of an uploaded id. I wrote more detail in another comment. That only covers large-scale reuse. It doesn't cover lending your id to your younger sibling if you want to, or if they find a way. Maybe that should be acceptable anyway. Same as you can lend your phone or computer to someone to use \"as you\", or you can buy them cigarettes and alcohol. Your responsibility. reply jcranmer 8 hours agorootparentprev> Using modern cryptography, it is easy to send a machine-generated proof to your social media provider that your government-provided ID says your age is ≥ 16, without revealing anything else about you to the service provider (not even your age), and without having to communicate with the government either. There's just one problem. How does the machine proving your age know that you are who you say you are? Modern cryptography doesn't have any tools whatsoever that can prove anything about the real body currently operating the machine--it can never have such a tool. And the closest thing that people can think of to a solution is \"biometrics,\" which immediately raises lots of privacy concerns. reply johnhenry 12 hours agorootparentprev> I suggest looking up zero-knowledge proofs. Sure, but is the Florida legislature actually looking into stuff like this? reply dotancohen 10 hours agorootparentWhy would they, when it is not in the governments interest? reply dotancohen 10 hours agorootparentprevThe government need not know what sites you visit. It is damaging enough that the government know that you are visiting sites that require an age verification. You can then be flagged for parallel construction if you should, I don't know, start a rival political party. reply dullcrisp 10 hours agorootparentNot if this were widespread. I wouldn’t be too bothered if the government knew that I either watched an R-rated movie or rented a car or purchased alcohol or created a Facebook account. reply brewdad 9 hours agorootparentNow do need an abortion or sought out gender affirming care. Today’s “no big deal” can become tomorrow’s privacy nightmare. reply ndriscoll 6 hours agorootparentYou missed the either in GP's comment. i.e. they know you did one of those things because you requested an over-18 token, but not which one. The more covered activities there are, the more uncertainty they have about why you might have asked for a token. reply harimau777 5 hours agorootparentThis isn't really my area of expertise, is there a way to know for sure that those are all the same token? Or could the government just lie and say they are all the same when in reality they can really differentiate. reply emporas 13 hours agorootparentprevDefinitely, we can use a government issued id, or we can create our own. Social graphs i call em. Zero knowledge proofs have so many ground breaking applications. I have made a comment in the past, relevant to how could a social graph be build, without the need of any government [1]. We can create effectively one million new governments to compete with existing ones. [1] https://news.ycombinator.com/item?id=36421679 reply paulryanrogers 11 hours agorootparentGovernments monopolize violence. At least at the foundational level. When too many of them compete at once it can get very messy very quickly. reply emporas 11 hours agorootparentLet's suppose that 1 million new governments are founded, and violence still can be enforced only by the existing ones. The new governments will be in charge of ids, signatures, property ownership and reputation. Governments of Rust programmers, or Python programmers, or football players, or pool players, or truck drivers will be created. When a citizen of Rust programmers social graph uploads code, he can prove his citizenship via his id. We may not even know his name, but he can prove his citizenship. He can sign his code via his signature, even pull request in other projects. He can prove his ownership of an IT company, as it's CEO, the stock shares and what not. And he will be tied to a reputation system, so when a supply attack happens, his reputation will be tainted. Other citizens of Rust's social graph, will be able to single out the id of the developer, and future code from him will be rejected, as well as code from non-citizens. Speaking of supply chains, how about the king of supply chains of products and physical goods? By transferring products around, in a more trustworthy way, by random people tied to reputation, Amazon may get a little bit of competition ain't it? see also an older comment of mine https://news.ycombinator.com/item?id=38800744 reply dotancohen 10 hours agorootparentGovernment is not the correct word to use for this idea. reply emporas 10 hours agorootparentAllright, social graphs then. I use social graphs and e-gov interchangeably, but social graphs might be better. reply thomastjeffery 11 hours agorootparentprevI've been thinking a lot lately about decentralized moderation. All we need to do is replace the word \"moderate\" with \"curate\". Everything else is an attestation. We don't really need a blockchain, either. Attestations can be asserted by a web of trust. Simply choose a curator (or collection of curators) to trust, and you're done. reply emporas 10 hours agorootparentYeah, blockchain is not needed at all. A computer savvy sheriff might do it, an official person of some kind. Or even private companies, see also \"Fido alliance\". Additionally the map of governments which accept Esthonian passport might be of some relevancy here[1]. [1] https://passports.io/programs/EE1 reply janalsncm 6 hours agorootparentprevSo I hash some combination of ID, name and birthday and send it to Facebook to create an account. Facebook relays that hashed info to a government server which responds with a binary yes/no. Of course you need to trust that the hash is not reversable. That doesn’t stop kids from using Facebook, but it stops kids’ ID from being used to create an account. reply endgame 6 hours agorootparentprevAnd as with electronic voting, the contract will go to the lowest bidder with the worst security, not the company that's got the CS chops to do it right. reply fennecbutt 9 hours agorootparentprevWe did a hackathon at work and one of the guys from one of my project teams covered this stuff as his project. I trust that it _would_ work 100%,but what I don't trust is that a government would implement it properly and securely, because no government works like that lmao (even NZ's great one). I mean living in the UK now I got like a dozen different fucking gov numbers for all manner of things, dvla, NHS, nin, other tax numbers, visa, etc...why isn't there just one number or identity. Gov.uk sites are mostly pretty stellar besides. reply thomastjeffery 11 hours agorootparentprevYou can send a proof that someone's government-provided ID says that their age is ≥ 16. That's not enough proof to levy a requirement. reply delichon 14 hours agorootparentprevnext [12 more] [flagged] reaperman 13 hours agorootparentMinors dont get full constitutional protections. I think they should have more rights than they do, but the first amendment is already more limited for minors than for adults. Tinker v. Des Moines has repeatedly been chipped away (e.g. Bethel School District v. Frasier). Minors cant produce pornography. Minors have their freedom of association and expression limited by employment laws. Minors speech is not free at home due to parental control or at school. Minors can’t consent to medical treatment which limits their ability to discuss sensitive issues. Minors cant vote or run for public office, which limits their direct participation in political expression and civil engagement. reply delichon 13 hours agorootparentnext [2 more] [flagged] PH95VuimJjqBqy 12 hours agorootparentdo you _want_ 3 year olds voting? All that would do is give people with children multiple votes. reply riversflow 14 hours agorootparentprev> There is no age limit to that restriction Wrong I’m afraid. Minors don’t have full rights, that’s why their bag/locker/car can be searched randomly, why they can have speech abridged, why they can’t see NC-17 movies(edit: scratch that one). The supreme court has weighed against minors many times. reply Longlius 13 hours agorootparent>why they can’t see NC-17 movies This is not a law, this is a voluntary regulation undertaken by the motion picture industry. reply riversflow 13 hours agorootparentGood call, edited. reply delichon 13 hours agorootparentprev> why they can’t see NC-17 movies The first amendment is a restriction on Congress, and does not apply to the private Motion Picture Association that maintains the film rating system. > why they can have speech abridged I don't think that's true of federal law. There are cases, such as in school, where more restrictions are permitted to the school. But those restrictions are not based on age. reply reaperman 13 hours agorootparent> There are cases, such as in school, where more restrictions are permitted to the school. But those restrictions are not based on age. After 18 you can choose whether to be in school or not. Those restrictions are voluntary for adults but compulsory for minors (who do not get to choose whether they go to public school, private school, homeschool, or just pass the GED) reply datavirtue 13 hours agorootparentprev”that’s why their bag/locker/car can be searched randomly\" This has always disgusted me about public school. No better way to erode rights and democracy than ingraining absolute tyranny in children. I was never search but I was convicted and punished many times on zero evidence, just some authority assuming I did something. reply freetanga 14 hours agorootparentprevYou are free to express yourself in a park. You are not free to enter the White House to express yourself. Enforcing a property owners right to refuse entry (digital or physical) does not prevent you from expressing yourself, but rather from doing it that specific property. Social Networks are not a public space reply IanCal 14 hours agorootparentprevFreedom of speech isn't, as a US legal term, about being able to post on privately owned websites. reply NegativeK 13 hours agorootparentIt is about the government's (state and federal) ability to regulate if you can post on privately owned websites. reply JeremyNT 15 hours agorootparentprevYes, this isn't the right solution. The power needs to be given to the users. A better solution is more robust device management, with control given to the device owner (read: the parent). The missing legislative piece is mandating that social media companies need to respond differently when the user agent tells them what to send. I should be able to take my daughter's phone (which I own), set an option somewhere that indicates \"this user is a minor,\" and with every HTTP request it makes it sets e.g. an OMIT_ADULT_CONTENT header. Site owners simply respond differently when they see this. reply basil-rash 11 hours agorootparentAlready exists, simply include the header Rating: RTA-5042-1996-1400-1577-RTA in HTTP responses that include adult content, and every parental controls software in existence will block it by default, including the ones built into iPhones/etc and embedded webviews. As far as I know all mainstream adult sites include this (or the equivalent meta tag) already. In general, I don’t think communicating to every site you visit that you are a minor and asking them to do what they will with that information is a good idea. Better to filter on the user’s end. reply pdpi 9 hours agorootparentThat's a much better approach in general. It's much easier to regulate and enforce that websites must expose these headers so that UAs can do their own filtering. Adult Content = headers in response, no ifs, ands, or buts Response headers are encrypted in the context of HTTPS, so there's no real sacrifice in privacy. Implementation effort about as close to trivial as can be. No real free speech implications (unless you really want to argue that those headers constitute compelled speech). All in all, it's a pretty decent solution. reply joshyeetbox 9 hours agorootparentprevThis is not a response header. It's a meta tag that's added to a websites head element to indicate it's not kid friendly. The individual payloads returned from an adult site don't include this as a header. reply JeremyNT 11 hours agorootparentprevI honestly wasn't aware of this, and it sounds like a great solution for \"adult content.\" Certainly, the site specifying this is better than the user agent having to reveal any additional details about its configuration. reply cmiles74 13 hours agorootparentprevEmancipation of children is also a thing, where a minor may petition the court to be treated as an adult. This also falls afoul of a blanket age restriction. https://jeannecolemanlaw.com/the-legal-emancipation-of-minor... reply FireBeyond 9 hours agorootparentI think there's an ambiguity here. Even on this page, talking about \"for all purposes\". This seems to mostly refer to parental decision making and rights. i.e. even as an emancipated minor, being treated as a \"legal adult\" does not mean you can buy alcohol or tobacco. reply selectodude 8 hours agorootparentBeing an actual legal adult does not mean that you can buy alcohol or tobacco. reply Terr_ 14 hours agorootparentprevExactly, any design for this stuff requires parental-involvement because every approach without it is either (A) uselessly-weak or (B) creepy-Orwellian. If we assume parents are involved enough to \"buy the thingy that advertises a parental lock\", then a whole bunch of less-dumb options become available... And more of the costs of the system will be borne by the people (or at least groups) that are utilizing it. reply notsound 15 hours agorootparentprevAnother option that allows for better privacy and versatility is the website setting a MIN_USER_AGE header based on IP geolocation. reply Terr_ 14 hours agorootparentGeolocation to that degree not that reliable and not necessarily 1:1 with jurisdiction or parental intent. If we're already trusting a parental-locked device to report minor-status, then it's trivial to also have it identify what jurisdiction/ruleset exists, or some finer-grained model of what shouldn't work. In either case, we have the problem of how to model things like \"in the Flub province of the nation of Elbonia children below 150.5 months may not see media containing exposed ankles\". OK, maybe not quite that bad, but the line needs to be drawn somewhere. reply cesarb 12 hours agorootparentprevSounds like you want PICS (though it works on the opposite direction, with the web site sending the flag, and the browser deciding whether to show the content based on it). reply PH95VuimJjqBqy 12 hours agorootparentprevthen propose the RFC. I haven't read the legislation myself but I don't see why this couldn't still be done, I doubt the legislation specified _how_ to do it. reply Izkata 12 hours agorootparent> \"Reasonable age verification method\" means any commercially reasonable method regularly used by government agencies or businesses for the purpose of age and identity verification. So no, that wouldn't work right now. reply lelanthran 2 hours agorootparentprev> It's the same problem with requiring age verification for porn. It's not that anyone wants kids to have easy access to this stuff, Depends on the argument being made, on the ideology of the audience, on the current norms, etc. I had an exchange here on HN some time back (topic was about schools removing certain books from their libraries), and very many people in support of those books, which dealt with gender-identity and sexual orientation, also supported outright porn (the example I used was pornhub) for kids of all ages as long as those books with pictures (not photos) of male-male sexual intercourse could stay in the library. Right now, if you made the argument \"There are some things kids below $AGE shouldn't be exposed to\", you'll still get some (vocal) minority disagreeing because: 1. They feel that what $AGE kids get exposed to should be out of the parent's hands (\"Should we allow parents to hide evolution from their children?\", \"Should we allow parents to hide transgenderism from their children?\") 2. They know that, especially with young children, they will lose their chance to imprint a norm on the child if they are prevented from distributing specific material to young children. In the case of sex and sexual education, there is currently a huge push for specific thoughts to be normalised, and unfortunately if it means that graphic sexual depictions are made to children, so be it. The majority is rarely so vocal about things they consider \"common sense\", like no access to pornhub for 10 year olds. reply paulddraper 14 hours agorootparentprev> destroying the anonymous nature of the internet (and probably unconstitutional under the First Amendment, to boot.) The First Amendment guarantees free expression, not anonymous expression. For example, there are federal requirements for identification for political messages. [1] These requirements do not violate the First Amendment. [1] https://www.fec.gov/help-candidates-and-committees/advertisi... reply o11c 14 hours agorootparentIn particular, \"anonymous speech is required if you want to have free speech\" is actually a very niche position, not a mainstream one. It just happens to be widely spammed in certain online cultures. reply paulddraper 13 hours agorootparentCorrect. I am staunch believer in the moral and societal good of free speech. Anonymous speech is far more dubious. Like, protests seem valuable. Protests while wearing robes and masks however... reply TillE 12 hours agorootparent> Protests while wearing robes and masks however This is absolutely permitted in America. It is illegal in countries like Germany which have no strong free speech protections. reply jonathankoren 11 hours agorootparentNo. It is not \"absolutely permitted in America\". They're Klan Acts, because a bunch of guys in masks and robes marching through is obviously a threat of violence. https://en.wikipedia.org/wiki/Anti-mask_law reply jcranmer 8 hours agorootparentThose laws are frequently overturned as unconstitutional, but may still remain on the books because we don't do a good job of clearing out laws that were ruled unconstitutional. As a general rule of thumb, almost every time someone brings an edge case about whether or not speech is First Amendment-privileged before SCOTUS, SCOTUS rules in favor of the speech. (The main exception is speech of students in school.) SCOTUS hasn't specifically ruled on anti-mask laws to my knowledge, but I strongly doubt it would uphold those laws. reply jonathankoren 7 hours agorootparentThe last court case was 2004, and the New York anti-mask was upheld. You don’t have to pontificate. The link was right there. reply whiddershins 6 hours agorootparentRandom aside, I haven’t seen NY enforce the anti mask legislation during the Halloween parade, various protests, or Covid. So I bet a new constitutional challenge could be erected. reply jonathankoren 5 hours agorootparentYou're trolling right? reply chasd00 10 hours agorootparentprevThere’s no issue protesting in masks. Antifa did it all the time with no repercussions. reply harimau777 5 hours agorootparentprevThere are places where it wouldn't be safe to protest without masks. In that case people effectively would be losing their freedom of speech if it isn't anonymous. reply protocolture 7 hours agorootparentprevProtests while wearing robes and masks protect the protester from followup action, and it also protects their families. If you assume a just government that doesnt demand revenge for every petty slight, you are living in a fantasy land. reply TillE 12 hours agorootparentprevAmerica has a very long tradition of anonymity being part of free speech, going back to the Federalist Papers. This is not some new online issue. reply unethical_ban 7 hours agorootparentWealthy, politically connected men with the ability to read and write about political philosophy and get it distributed is a bit different situation than a thousand Russian AI trollbots posting bad-faith \"opinions\" on American current events. reply dathinab 10 hours agorootparentprev> effectively regulating and destroying the anonymous nature of the internet Technically you can make that work without issues (You only need to prove your age not your identity, something which can reasonably be archived without leaking your identity). There are just two practical issues: - companies, government and state (at least US police & spy agencies) will try to undermine any afford to create a reasonable anonymous - it only technically works if a \"reasonable degree\" of proof is \"good enough\", i.e. it's must be fine that a hacker can create a (illegal?) tool with which a child could pretend to be 16+, e.g. by proxing the age check to a hacked device of an adult. Heck it should be fine if a check can be tricked by using the parents passport or phone. I mean it's an 16+ check, there really isn't much of a reason why it isn't okay to have a system which is only \"good enough\". But lawmakers will try nonsense. Interestingly this is more a problem for the US then some other states AFIK due to how 1) you can't expect everyone 18+ to have an id and everyone 16+ to be able to easily get one (a bunch of countries have owing (not carrying) id requirements without it being a privacy issue. 2) Terrible consumer protection making it practically nearly impossible to create a privacy preserving system even if government and state agencies do not meddle. Similar if there wouldn't be the issue with passports in US it probably wouldn't touch the First Amendment as it in the end protects less then a lot of people believe it does. reply Beldin 3 hours agorootparentprev> But the only way to do this is to require ID checks, No, it isn't. Check out Yivi [1]. Its fundamental premise is to not reveal your attributes. It's based on academic work into (a.o.) attribute-based encryption. The professor then took this a step further and spun off a (no profit) foundation to expand and govern this idea. [1] https://privacybydesign.foundation/irma-explanation/ reply Terr_ 14 hours agorootparentprev> But the only way to do this is to require ID checks Not necessarily, consider the counterexample of devices with parental-controls which--when locked--will always send a \"this person is a minor\" header. (Or \"this person hits the following jurisdictional age-categories\", or some blend of enough detail to be internationally useful and little-enough to be reasonably private and not-insane to obey.) That would mostly puts control into the hands of parents, at the expense of sites needing some kind of code-library that can spit out a \"block or not\" result. reply qwertox 14 hours agorootparentprevLately I'm repeatedly reminded of how in Ecuador citizens, when interviewed during a protest, see it as a normal thing to tell their name as well as their personal ID number into the camera when also speaking about their position in regards of the protest. They stand to what they are saying without hiding. Since about half a year I've noticed the German Twitter section getting sunk in hate posts, people disrespecting each other, ranting about politicians or ways of thinking, but being really hateful. It's horrible. I've adblocked the \"Trending\" section away, because its the door to this horrible place where people don't have anything good to share anymore but disrespect and hate. This made me think about what we're really in need for, at least here in Germany, is a Twitter alternative, where people register by using their eID and can only post by using their real name. Have something mean to say? Say it, but attach your name to it. This anonymity in social media is really harming German society, at least as soon as politics are involved. I don't know exactly how it is in the US but apparently it isn't as bad as here, at least judging from the trending topics in the US and skimming through the posts. reply Kye 14 hours agorootparentPeople have zero qualms about being absolute ghouls under their wallet names. The people with the most power in society don't need anonymity. The people with the least often can't safely express themselves without it. Also: https://theconversation.com/online-anonymity-study-found-sta... >> \"What matters, it seems, is not so much whether you are commenting anonymously, but whether you are invested in your persona and accountable for its behaviour in that particular forum. There seems to be value in enabling people to speak on forums without their comments being connected, via their real names, to other contexts. The online comment management company Disqus, in a similar vein, found that comments made under conditions of durable pseudonymity were rated by other users as having the highest quality. \" reply qwertox 13 hours agorootparentThere are two points which matter: - No more bots or fake propaganda accounts. - Illegal content, such as insults or the like, will not get published. And if it does, it will have direct consequences. I'm also not tending towards a requirement to have all social networks ID'd, but I think that a Twitter alternative which enables a more serious discussion should exist. A place where politicians and journalists or just citizens can post their content and get commented on it, without all that extreme toxicity from Twitter. reply belval 12 hours agorootparentThe thing is, the political climate is very toxic and the absence of anonymity can have a real impact for things that are basically wrong think. Say for example I held the opinion that immigration threshold should be lower. No matter how many non-xenophobic justifications I can put on that opinion, my possibly on H1B colleagues can and would look up my opinion on your version of Twitter and it would have a real impact on my work life. There is a reason why we hold voting in private, it's because when boiled down to its roots, there are principles that guide your opinions they are usually non -reconciliable with someone else's opinion and we preserve harmony by keeping everyone ignorant of their colleagues political opinions. It's not a bad system, but it's one that requires anonymity reply DylanDmitri 12 hours agorootparentprevOr, to post on a political forum you must have an ID. You can have and post from multiple accounts, but your id and all associated accounts can be penalized for bad behavior. reply pohuing 14 hours agorootparentprevPlenty of hate under plain names on Facebook, been that way for a decade and I doubt it will change with ID verification. reply Zak 12 hours agorootparentprevThe algorithm powering the trending section, which rewards angry replies and accusatory quote-tweets is at least a good a candidate as a source of harm to political discourse than anonymity. reply adaptbrian 11 hours agorootparentTake it a step forward, ban Engagement based algorithmic feeds. I've said this and I'll continue to say this type of behavioral science was designed at FB by a small group of people and needs to be outlawed. It never should have been allowed to take over the new age monetization economy. There's so much human potential with the internet and its absolutely trainwrecked atm b.c of Facebook. reply 15457345234 1 hour agorootparentI agree. It's been deliberately designed to cause strife. They are genuinely anti-human companies that seem to want conflict and tension to occur. https://www.theguardian.com/commentisfree/2014/jun/30/facebo... This article was very on-the-nose, that really should have been the writing on the wall. reply claytongulick 11 hours agorootparentprevI wonder what percentage of the hate stuff is bots. reply PeterisP 6 hours agorootparentMy experience with propaganda bots is that the really nasty hate stuff will usually be posted by actual, real people (perhaps as a result of being prodded by bot-provided outrage), and bots will rather have all kinds of more subtle hinting and agenda-pushing - because bots are managed by semi-professionals who care about bots not being blocked and (often) don't really care about the agenda they're pushing, while there also is a substantial minority of semi-crazy people who just don't care and will escalate from zero to Hitler in a few minutes. reply simmerup 14 hours agorootparentprevAttach their pictures too, so you can see the ghoul spouting hate is a basement dweller reply giantg2 14 hours agorootparentprev\"probably unconstitutional under the First Amendment, to boot\" Probably not. Minors have all sorts of restrictions on rights, including first amendment restrictions such as in schools. \"(And if it is, let's start talking about gun ownership first...)\" Are you advocating for removing ID checks for this? If not, it seems that this point actually works against your argument. Not saying that I agree with a ban, but your arguments against it don't really stand. reply 1vuio0pswjnm7 13 hours agorootparentprevHow many social media users who create accounts and \"sign in\" are \"anonymous\". How would targeted advertising work if the website did not \"know\" their ages and other demographic information about them. Are the social media companies lying to advertisers by telling them they can target persons in a certain age bracket. reply bimguy 4 hours agorootparentprev\"Unconstitutional\" arguments only go so far. I am not America (I'm a proud Australian) so I can easily see the incredibly obvious and ridiculous destruction \"freedom\" in your country entails. Anonymity services can still exist without fostering an environment to addict children and young adults to social media or a device... and without your precious \"rights\" being taken away. reply giancarlostoro 14 hours agorootparentprev> But the only way to do this is to require ID checks COPPA has entered the building. If you're under 13 and a platform finds out, they'll usually ban you until you prove that you're not under 13 (via ID) or can provide signed forms from your parent / legal guardian. I've seen dozens of people if not more over the years banned from various platforms over this. We're talking Reddit, Facebook, Discord and so on. I get what you're saying, but it kind of is a thing already, all one has to do is raise the age limit from 13 to say... 16 and voila. reply woodruffw 14 hours agorootparent\"Finds out\" is the operative part. COPPA is not a proactive requirement; it's a reactive one. Proactive legislation is a newer harm that can't easily be predicted based on past experiences with reactive laws. reply giancarlostoro 13 hours agorootparentIndeed, nothing is stopping said companies from scanning and assessing age of a user uploading selfies though. This is allegedly something that TikTok does. My point being, the framework is there, and then people actually report minors, the companies have to take it seriously, or face serious legal consequences. reply woodruffw 11 hours agorootparentHow do you know the selfie is from the \"primary\" user? And how do you know they're underage, versus being a chubby-faced 18 year old (like yours truly was?) reply stevage 8 hours agorootparentprev>It's not that anyone wants kids to have easy access to this stuff, but that any of these laws will either be (a) unenforceable and useless, or (b) draconian and privacy-destroying. Surely not. Imagine: government sells proof of age cards. They contain your name, and a unique identifier. Each time you sign up to a service, you enter your name and ID. The service can verify with the government that your age is what it needs to be for the service. There are laws that state that you can't store that ID or use it for any other purpose. Doesn't seem impossible. reply throwaway421967 8 hours agorootparent- Would only work if the government, does not have access to the reverse mapping. (otherwise law enforcement will eventually expand to get its hands on it) - It will likely be phished very quickly (and you'd have no way of knowing since no one is storing it). (making it short lived, means u'd have to announce tot he goverment eveytime u want to watch porn) - Eventually there will be dumps of ids, like there are dumps of premium porn accounts now Still doesn't do anything about foreign websites. reply ttt3ts 10 hours agorootparentprev> effectively regulating and destroying the anonymous nature of the internet The bulk of the internet has not been anonymous for a while. Facebook requires an id already, Google tracks you using Google and the OS, reddit is tightening to control bots, amazon requires a phone number. Think about it. What portion of your activities day to day on the Internet are anonymous? Now try to do them anonymously. It isn't practical/possible anymore and the internet of yesteryear is gone. reply terpimost 10 hours agorootparentprevIt is pretty hard to give access to youtube to your kid with an account where age is stated. Yes kids can open browser in private mode… but they rarely do because it is a friction. If every social media would be moved to adult category the current rules in operating systems would do a good job. I am not sure about 16 years ( i would support it as a father )… but up to 13-14 feels appropriate, there is PG-13 reply spogbiper 12 hours agorootparentprevI propose the Leisure Suit Larry method. Just make users answer some outdated trivia questions that only olds will know when they sign up for an account. reply badpun 12 hours agorootparentIn the Internet era, the answers will just be googleable. People wil quickly compile a page with all possible questions and with answers to them. reply jonathankoren 11 hours agorootparentBut with ChatGPT, all the answers will be wrong. reply onion2k 11 hours agorootparentpreveffectively regulating and destroying the anonymous nature of the internet Social media is on the internet. It is not the internet. reply forgetfreeman 9 hours agorootparentprevThe internet has not been anonymous in fact or theory for decades now, and if you think the government can't get your complete browsing history on a whim I'm guessing you haven't paid any attention to the news about NSA buying user data bundles from online brokers. That said, \"muh freedoms\" is hardly a quality argument in the face of the widely documented pervasive harms caused to children by exposure to social media. The logical extreme of your position would be to declare smoking in public a form of self-expression and then demand age limits be removed for the sale of tobacco products because First Amendment. :P reply hobs 8 hours agorootparentAs the OP said, if freedoms are not a quality argument then we can rid ourselves of millions of guns, but this is a non-starter for the freedom crowd. reply pokstad 6 hours agorootparentprevAlcohol and tobacco websites have been doing fine without checking IDs. reply RandallBrown 14 hours agorootparentprevThe definition of \"social media\" in this bill actually seems to exempt anonymous social networks since it requires the site \"Allows an account holder to interact with or track other account holders\". reply paulddraper 14 hours agorootparentprev> privacy-destroying We ARE talking about social media. Like, the least private software on the planet. reply ZunarJ5 9 hours agorootparentprevWe say this is the only way, but what about regulating these companies properly??? reply zamadatix 9 hours agorootparent\"Regulating them properly\" could mean a lot of things to a lot of people. Do you mean e.g. just not allowing porn on the internet or do you mean e.g. just not requiring identification verification? If neither, what way of regulation that allows distinction without banning content or identifying users? reply huytersd 13 hours agorootparentprevAh so be it. I don’t care much for the things that come from anonymous culture. I want gatekeepers. This tyranny of the stupid online is pretty tiresome. reply einpoklum 14 hours agorootparentprev> destroying the anonymous nature of the internet Aren't the really problematic social networks the ones where you've lost your privacy and anonymity long ago and are being tracked and mined like crazy? reply lukev 14 hours agorootparentThat's like saying \"80% of the internet has gone to shit, might as well destroy the remaining good 20%\". reply jf22 11 hours agorootparentI don't think it's like saying that at all. reply slily 13 hours agorootparentprevPeople can post all kinds of illegal things online and no one is suggesting that content should be approved before it can be visible on the Internet. It doesn't have to be strictly enforced to act as a deterrent. How effective of a deterrent it would be has yet to be seen. reply NoMoreNicksLeft 14 hours agorootparentprev>But the only way to do this is to require ID checks, effectively regulating and destroying the anonymous nature of the internet Ban portable electronics for children. Demand that law enforcement intervene any time it's spotted in the wild. If you still insist that children be allowed phones, dumb flip phones for them. It could be done if there was the will to do it, it just won't be done. reply jakderrida 5 hours agorootparentprev> It's the same problem with requiring age verification for porn. It's not that anyone wants kids to have easy access to this stuff I do. Anything to avoid \"the talk\", tbh. I grew up Catholic. I never had \"the talk\". I don't even know where to start. Blow jobs? reply dustedcodes 11 hours agorootparentprevnext [3 more] [flagged] Buttons840 11 hours agorootparentGovernment knows best when it comes to minors, right? And all of us need to have our government papers ready if we want to participate in the most important forum of communication of our time. Talk about power imbalance. > who can’t even articulate the difference between man and woman Can you articulate that difference? Personally, I'd expect a more complete definition from a kid today than I would from a conservative adult. For example, my dad might refer to DNA when explaining the difference between a man and a woman, and he would also confidently say he's a man (I mean, he is my dad), but since he's never had a DNA test, how can he be sure? It's obviously about more than DNA. My mom might say a woman is someone who can have children, but what about women who cannot have children, are they not women? Etc. reply I_am_uncreative 10 hours agorootparentprev> who can’t even articulate the difference between man and woman Ahh, you're one of those people. I now can just discount everything you said. reply thinkingtoilet 15 hours agorootparentprev> effectively regulating and destroying the anonymous nature of the internet. Not at all. Just the social media sites, which are objectively bad for kids. As an adult, you do what you want on the internet. reply lukev 15 hours agorootparentAnd what makes a site a social media site? Anywhere you can post interactive content? You do realize that laws like this would apply to sites like HN, Reddit, the comment section of every blog, and every phpBB forum you ever used? It's not just Instagram and Tiktok. reply thinkingtoilet 14 hours agorootparentI think a perfectly clear line could be drawn that would separate out phpBB from TikTok very easily. I genuinely don't understand this comment, we shouldn't do it because it's hard or the results might be imperfect? reply lukev 14 hours agorootparentKids want to communicate. Whether it's TikTok, Discord, phpBB, chatting in Roblox or Minecraft, they will if they can. If we want to \"ban social media\" we'll need a consistent set of guidelines about what counts as social media and what doesn't, and what exactly the harms are so they can be avoided. I don't believe that's as easy as you think. reply borski 7 hours agorootparentprev> I genuinely don't understand this comment, we shouldn't do it because it's hard or the results might be imperfect? Yes. Imperfect solutions, when driven by the government, don’t change for decades, causing terrible consequences. “SSH is a munition” comes to mind. reply woodruffw 14 hours agorootparentprevI think your comment would be much stronger if you laid out precisely what you think that line would be. Laws do not have to be perfect to be good, but they do have to be workable. It's not clear that there's a working definition of \"social media\" that includes both TikTok and Reddit but doesn't include random forums. reply thinkingtoilet 13 hours agorootparentSo if a random person on the internet doesn't have a perfect solution then it shouldn't be considered? reply lazyasciiart 11 hours agorootparentTheir opinion that it would be straightforward shouldn’t be considered. reply woodruffw 11 hours agorootparentprevThis is not a charitable reading. Nobody is asking for a perfect solution; it is reasonable to demonstrate some prior consideration for the ways in which most solutions are dangerously imperfect. reply chasd00 10 hours agorootparentprevTo me the issue is it’s a waste of government and waste of time. Parental controls already exist on all devices. The answer to every problem can’t be “more government, more laws”. reply CalRobert 14 hours agorootparentprevAny clear line would be gamed pretty quickly I imagine. reply anonym29 15 hours agorootparentprevTrying to force independently owned and operated forums to enforce laws that might not even be applicable in the country that the owners / admins live and work in is going to be about as effective as trying to force foreign VPS/server/hosting providers to delete copyrighted content from their server using laws that don't apply in their jurisdiction. reply blitz_skull 14 hours agorootparentprevYou know, I'm not really sure that requiring IDs for access to porn / social media is a terrible idea. Sure it's been anonymous and free since the advent of the internet, but perhaps it's time to change that. After all, we don't allow a kid into a brothel or allow them to engage in prostitution (for good reasons), and porn is equally destructive. But with the topic at hand being social media, I think a lot of the same issues and solutions apply. It's harmful to allow kids to interact with anyone and everyone at any given time. Boundaries are healthy. Aaaaand, finally there's much less destruction of human livelihood by guns than both of the aforementioned topics if we measure \"destruction\" by \"living a significantly impoverished life from the standard of emotional and mental wellbeing\". I doubt we could even get hard numbers on the number of marriages destroyed by pornography, which yield broken households, which yield countless emotional and mental problems. So, no, guns aren't something we should discuss first. Also, guns have utility including but not limited to defending yourself and your family. Porn has absolutely zero utility, and social media is pretty damn close, but not zero utility. reply tempestn 14 hours agorootparentYou think watching porn is equally destructive to engaging in prostitution? I'd hate to see what kind of porn you're watching. reply blitz_skull 6 hours agorootparentI think you’re vastly underestimating the destructive nature of porn. I’m no longer watching porn, by the grace of God. reply diputsmonro 12 hours agorootparentprevThe biggest problem with this is how we would define \"porn\". Some states are currently redefining the existence of a transgender person in public as an inherently lewd act equivalent to indecent exposure. I have no doubt that if your proposal were to pass that there would be significant efforts from extremist conservatives to censor LGBT+ communities online by labeling sex education or mere discussion of our lives as pornographic. How are LGBT+ people supposed to live if our very existence is considered impolite? Nevermind the fact that the existence of a government database of all the (potentially weird) porn you look at is a gold mine for anyone who wants to blackmail or pressure you into silence. The horrors and dangers of porn are squarely a domestic and family issue. The government does not need to come into my bedroom and look over my shoulder. reply lelanthran 34 minutes agorootparent> The biggest problem with this is how we would define \"porn\". Some states are currently redefining the existence of a transgender person in public as an inherently lewd act equivalent to indecent exposure. This is the first I've heard about this. Link? reply blitz_skull 6 hours agorootparentprev> The biggest problem with this is how we would define “porn”. I wholeheartedly agree. And that’s a problem we should lean into and solve. Its difficulty doesn’t make it less worth of solving. > The horrors and dangers of porn are squarely a domestic and family issue. Therein lies the problem however. Every systemic issue in our world begins in a family or domestic situation of some form. While I am well aware and also concerned about the implications of government overreach here, I don’t think we can throw up our hands and say, “Meh”. At a minimum it can begin with education. We can teach people about the destructive nature of porn (and social media). The fact that this impacts every family, domestic situation, and therefore indirectly or directly touches every single life in our society actually kinda makes it a great candidate for government oversight. reply andy99 19 hours agoparentprevI'm in favor of kids not using social media, but not of the government forcing this on people nor spinning up whatever absurd regulatory regime is required. And the chance of actually enforcing it is zero anyway. It's no more realistic to expect this to work than to expect all parents to do it as you say. It's just wasted money plus personal intrusion that won't achieve anything. reply dylan604 14 hours agorootparentThere is a societal problem that is beyond just parenting. The peer pressure for kids to feel left out and ostracized because they are the only ones not on the socials is something a teen is going to definitely rebel against their parents on. It's part of being a teen. I'm guessing the other parents would even put pressure on the parents denying the social access. To me, the only way out of this is by changing one nightmare for another giving the gov't the decision of allowing/denying access. Human nature is not a simple thing to regulate since the desire for that regulating is part of human nature reply diputsmonro 12 hours agorootparentIs talking to other people online really so bad that we need the government to step in and tell us who we can and can't talk to? How quickly will that power expand to what we can and can't talk about? I agree that neither solution is perfect, but exchanging an imperfect but undoubtedly free system of communication for one that is explicitly state-controlled censorship is an obvious step backwards. \"Thinking about the children\" should also involve thinking about what kind of a society you want to build for them. A cage is not the answer, especially not with fascism creeping back into our politics. reply dylan604 12 hours agorootparentI think you are willingly playing this down as \"talking to people online\" to make some point. However, it is beyond what one kid online says to another online. It is what predators say to those kids online. I don't just mean Chester and his panel van. I'm talking about anyone that is attempting to manipulate that kid regardless of the motive, they are all predators. Social media has long since past just being a means of communicating with each other, and you come across as very disingenuous for putting this out there. reply diputsmonro 11 hours agorootparentI think people are being incredibly disingenuous when they imagine that the government won't abuse this power to censor and harm marginalized communities. Many states are trying to remove LGBT books from school libraries for being \"pornographic\" right now, for example. All it takes is some fancy interpretations of \"safety\" and \"social media\" for it to become a full internet blackout, for fear of \"the liberals\" trying to \"trans\" their kids. I don't deny that kids can get into trouble and find shocking or dangerous things online. But kids can also get in trouble walking down the street. We should not close streets or require ID checks for walking down them. Parents should teach their kids how to be safe online, set up network blocks for particularly bad sites, and have some kind of oversight for what their kids are doing. Maybe these bills should mandate that sites have the ability to create \"kid\" accounts whose history can be checked and access to certain features can be managed by an associated \"parent\" account. Give parents better tools for keeping their kids safe, don't just give the government control over all internet traffic. reply dylan604 10 hours agorootparent> when they imagine that the government won't abuse this power I've suggested no such thing. In fact, I described putting regulations in place as a \"nightmare\". Parenting alone will not work. Self regulation from the socials will not work. The entire situation is a nightmare of our own making. There is no simple solution. These tendencies of human nature were present long before social media. It was just fuel for the fire for the worst qualities. reply arcticbull 15 hours agorootparentprevIt seems like you are in favor of something that requires coordination, but don't believe in coordination. Is there a different way you think this could be achieved? reply kelseyfrog 15 hours agorootparentI don't believe in the disbelief of coordination so it seems like we're at a bit on an impasse. Please expound. reply m_mueller 7 hours agorootparentI think GP means the coordination between parents, and I agree on that: if you can’t get a strong majority of parents to agree on keeping their children off of social media & smart phones, you as parents have the choice between two outcomes: enforcing isolation vs. letting social media slip through and just trying to delay as much as possible. Almost all parents either don’t care or opt for the second option (which I also think is the better one), but the dynamics today are that between 10-12yo the pressure to get your kid a smartphone will mount and you have to give in at some point. Being able to wait till 16 would be much better IMO. reply ajhurliman 15 hours agorootparentprevWe have a ban on gambling for minors, so if you see social media as more harmful than gambling (personally, I do) it probably makes sense. reply monkeynotes 15 hours agorootparentGambling is outright banned for a majority of regions in the US, not just kids. I don't think they are equally bad, just different bad. Gambling is addictive, and it destroys people. Social media is addictive and socially toxic, on the whole it erodes the very fabric of a society. reply ribosometronome 15 hours agorootparent>Social media is addictive and socially toxic, on the whole it erodes the very fabric of a society. It's interesting how every generation seems to decry new forms of media as eroding the fabric of society. They said it about video games, television, music, movies, etc. I'm sure we're right this time, though. reply lumb63 13 hours agorootparent… we’ve been right. Television, music, video games, movies, etc., have decimated social capital and communication in the western world. It is not uncommon to “hang out” with people who are your “friends” without ever interacting with them in any meaningful way because everyone is focused on, e.g., a television. That’s assuming everyone isn’t too busy playing video games to leave their houses. Whether you agree or not that they’re “eroding the very fabric of a society” (I would argue they are), it should be acknowledged that almost all the downsides predicted have come to pass, and life has gone on not because these things didn’t happen, but in spite of them having happened. reply viraptor 13 hours agorootparentPeople are interacting through online games quite often though. You can't throw them all in one basket. You can make friends / longer term connections that way (I did) or keep in touch with people you don't live near to. reply badpun 12 hours agorootparentThat's not community, though. You're not inhabiting the same space, share the same problems and try to work them out them together. You're not helping each other out if someone falls into trouble. reply sapphicsnail 1 hour agorootparentYou can do all of those things sans inhabit the same space with people you meet online. I met my 2 girlfriends online and we support each other through everything even though we aren't in the same place most of the time. I'm part of a niche group on reddit that supports each other emotionally in a way I've struggled to find in physical spaces. I still meet up with people in physical space, but I absolutely get meaningful social connection from online spaces. reply viraptor 4 hours agorootparentprevIt's up to you what you build that way. You can build a real community, you can build a shallow friend group, the medium doesn't have to limit it. My boss met his wife in a MUD. reply badpun 1 hour agorootparentYou can meet your wife on a Tinder, via ads posted in newspaper or on a cruise. It does not mean that those are communities. reply bee_rider 14 hours agorootparentprevTelevision clearly did something pretty significant to society. Cable news makes the country more polarized, that we didn’t do anything about it doesn’t mean it wasn’t a problem. We just missed and now the problem is baked into the status quo. Video games are typically fiction, so the ability to pass propaganda through them is usually a little more limited. It isn’t impossible, just different. Social media is a pretty bad news source for contentious issues. We should be pretty alarmed that people are getting their news there. reply mynameisash 13 hours agorootparentprevIt seems that all data on the subject of social media point emphatically to, Yes! It's terrible for adolescents[0] (and probably society writ large?). [0] https://jonathanhaidt.com/social-media/ reply angra_mainyu 14 hours agorootparentprevApples-to-oranges comparison. Plenty has been written already on the ravaging effects of social media on society and it's pretty plain to see. https://www.mcleanhospital.org/essential/it-or-not-social-me... > Rates of sexual activity have been in decline for years, but the drop is most pronounced for adults under age 25 (Gen Z). > For Gen Z, a rise in sexlessness has coincided with a decline in mental health. > Sexual activity can boost mood and relieve stress and may serve as a protective factor against anxiety and depressive disorders. https://www.psychologytoday.com/intl/blog/the-myths-sex/2022... missing-out-the-benefits-sex reply bcrosby95 11 hours agorootparentprevSocial media isn't new. It's over 20 years old by now. If you count things like forums you're looking at 30+ years old. reply monkeynotes 15 hours agorootparentprevYeah, none of those are comparable. TV never lead to children bullying each other anonymously which then leads to kids committing suicide. reply rchaud 15 hours agorootparentprevNot anymore. A 2018 Supreme Court decision opened the floodgates to legalized sports gambling, much of it online. The only states with existing bans are Hawaii and Utah, which combined have only 5 million residents. https://en.m.wikipedia.org/wiki/Murphy_v._National_Collegiat... reply alexb_ 15 hours agorootparentThis is just blatantly not true, anyone who has tried to gamble on sports can tell you that companies go to quite incredible lengths to make sure that nobody outside of the few jurisdictions where they're legal can gamble online. I live in Nebraska, and I have to go travel across to Iowa before I can do anything. reply ajhurliman 12 hours agorootparentTrue, it's state by state, but a lot of states allow it. And even if you're not in the right state you can always go to sites from other countries (e.g. betonline.ag) reply monkeynotes 15 hours agorootparentprevOof. reply WindyLakeReturn 12 hours agorootparentprevThe ban we have on gambling seems weak. From trading card games to loot boxes to those arcade games that look to be skill based but are entirely up to chance, children are allowed to do all. The rules feel very inconsistent to the point that they appear arbitrary in nature. reply Ferret7446 8 hours agorootparentThere's a pretty clear difference between gambling and \"gambling\": whether you're winning money. reply ajhurliman 12 hours agorootparentprevAgreed, the gaming industry has done it's damndest to undermine the restrictions of gambling. reply soco 19 hours agorootparentprevIs there an alternative? Self-control - as we have now - brought us here. If the government shouldn't step in then the only other option left (only I can see) is magic. And we have a bad record with magic. reply matthewdgreen 19 hours agorootparentprevSeveral governments have already effectively banned sites like Pornhub by creating regimes where people have to mail their ID to a central clearinghouse (which creates a huge chilling effect.) The article talks about “reasonable age verification measures” and so saying it’s unenforceable seems a little bit premature. Also, you can bet those measures won’t be in any way reasonable once the Florida legislature gets through with them. reply rschneid 19 hours agorootparent>effectively banned In my opinion, these governments haven't implemented 'effective' bans (though maybe chilling, as you say) but primarily created awkward new grey markets for the personal data that these policies rely on for theatrics. Remember when China 'banned' youth IDs from playing online games past 10PM? I think a bunch of grandparents became gamers around the same time... https://www.nature.com/articles/s41562-023-01669-8 reply rft 18 hours agorootparentAnother example is some Korean games requiring essentially a Korean ID to play. A few years ago there was a game my guild was hyped about and we played the Korean version a bit. You more or less bought the identity of some Korean guy via a service that was explicitly setup for this. Worked surprisingly well and was pretty streamlined. reply seanw444 18 hours agorootparentprevWhich is exactly what happens for markets that are desirable enough. We compare bans of things not enough people care about, to bans of things that people are willing to do crazy things for. They don't yield the same results. reply monkeynotes 15 hours agorootparentNo policy is 100% effective. Kids still get into alcohol, but the policy is sound. reply angra_mainyu 15 hours agorootparentprevAt least from personal experience, when there was a period where my ISP in the UK started requiring ID verification for porn, I literally ceased to watch it. Making something difficult to do actually works to _curb_ behavior. reply 55555 18 hours agorootparentprevYou're proving the argument that the parent set forth. Anyone who wants to visit Pornhub can just visit one of the many sites that isn't abiding by the new law. However, that's not due to a lack of legislation, but rather a lack of enforcement, or, perhaps, enforceability. If laws always worked I'd be for more of them. My argument is not that we should never make laws because it's futile but rather that some laws are more futile than others and having laws go unenforced weakens government and enforcing them inequitably is injust. reply monkeynotes 15 hours agorootparentAlso social policy enforcement is a generational thing. The UK is only just getting toward outright banning cigarettes by making it illegal for anyone born after X date from ever buying them. Eventually you have a whole generation that isn't exposed to smoking and on the whole thinks the habit is disgusting, which it is. reply verall 15 hours agorootparentExcept that some people born after that date will still acquire them, get addicted, and then what? Prosecute them like drug possession? It's infantilizing and dumb. Grown adults should be allowed to smoke tobacco if they so wish and smoking rates are already way down due to marketing and alternatives. Noone needs to be prosecuted. reply zhivota 13 hours agorootparentYou don't need to prosecute any buyers at all though. All you need to do is make it illegal to sell in shops, and illegal to import. There will be a black market, sure, but how many people are going to go through the trouble and expense to source black market tobacco? Not that many. And everyone benefits because universal healthcare means everyone shares the cost of the health effects that are avoided. reply monkeynotes 13 hours agorootparentShould mention the govt. has to find budget to fill the gap of tobacco budget, but they've been slowly doing this as demand has slumped since 2008. reply badpun 11 hours agorootparentprev> how many people are going to go through the trouble and expense to source black market tobacco? Not that many. Just see how many people already go through that trouble to source illegal drugs... reply monkeynotes 13 hours agorootparentprevI think it's hyperbolic to look at tobacco like other drugs. Tobacco is a lifestyle thing, it doesn't get you high, it's a cultural habit. There are only upsides to getting rid of the social demand for it. If you think taking tobacco away from consumers is infantilizing, why yes, yes it is. We are dealing with children's futures. Adults get to continue smoking, children less likely to even want to smoke as the social acceptance goes down and with that there is less and less desire to smoke. Nicotine doesn't do much other than get you addicted, no one is chasing after a pronounced high with it, people start smoking because it's perceived as cool. I can't imagine an adult wanting to start smoking, most adults get addicted in their teens. I think you have have an import ban, and a black market, and still see significant gains in eroding the demand. I do not think people should be prosecuted for possession, but the UK will probably make some bad decisions there, but that doesn't mean the overall policy is bad. reply __turbobrew__ 13 hours agorootparentprevMy main issue is that the only effective way to ban access to a website is to also ban VPNs and any sort of network tunneling. A great firewall would have to be constructed which I am very much against. Even China’s firewall is surpassable and it is questionable how much it is worth operating given the massive costs which would be incurred. I think the government should invest in giving parents the tools to control their child’s online access. Tools such as DNS blocklists, open source traffic filtering software which parents could set up in their home, etc. reply Taylor_OD 17 hours agorootparentprevDoes this actually work or does it just push those same people to sketchier websites? reply angra_mainyu 15 hours agorootparentWorked in my case when my ISP required ID for it in the UK. I just noped out of it entirely. reply JoshTko 19 hours agorootparentprevI used to want no govt intrusion for this. Then I understood how there are teams of PHDs tweaking feed to maximize addition at each social network. I think there could be even limits, or some sort of tax on gen pop. reply protocolture 7 hours agorootparentIt would be less imposing on the populace if you just demanded a government hit squad kill those PHD's instead of demanding everyone submit to some onerous government crapware to access the internet. reply OJFord 18 hours agorootparentprevWe don't even have to speculate, isn't it already the case forSmoking sections haven't been a thing in the US for a long time... Yes, the regulations that made this happen are good. That's my point. > Waste from single use vapes is also a huge problem. Nothing like the cigarette butts that used to be everywhere. > Similarly there are health effects specific to vaping, time will tell if cancer is among them. We've plenty of data to safely conclude vaping is safer than smoking tobacco. That doesn't make it safe, but it's absolutely safer. reply lbhdc 15 hours agorootparentnext [2 more] [flagged] ceejayoz 15 hours agorootparent> It seems like vaping is close enough to smoking to say that it is functionally close enough to be equated. Bullshit. Both are nicotine delivery methods. One is far better for both individual and societal reasons. Water and whiskey are both wet, but that doesn't make them the same. > But you have raised my curiosity about your relationship with vaping. Do you work in the industry? No, nor do I vape/smoke. I'm just old enough to remember how shitty it was to have smokers everywhere, in a way that isn't the case for vapers... and I've seen the multi-decade decline in lung cancer incidence stats. reply huytersd 12 hours agorootparentprevNicotine by itself is harmless besides the addictiveness. A nicotine addiction is not going to drastically affect your mental state or cause socially disruptive behavior like domestic violence or armed robbery so it’s really nothing to be concerned about. reply sneak 15 hours agorootparentprevNo, they are not remotely the same. Nicotine isn’t really that harmful, but combustion byproducts very much are. Also the effects on bystanders are orders of magnitude better. Most of the harm from smoking comes from the smoke, not the nicotine or associated addiction. reply huytersd 12 hours agorootparentprevVaping (non flavored) vapes has basically a negligible impact on your health. reply akerl_ 17 hours agorootparentprevThis must be why there’s not 50 vape shops in my town with big neon signs. reply ceejayoz 17 hours agorootparentDon't get me wrong, I'd love to see vaping turn into a prescription-only smoking cessation aid, but it's not smoking. I'm 100% happy with even a one-for-one replacement of smoking for vaping, even in kids, given the dramatically lower risk of resulting health problems. reply brodouevencode 15 hours agorootparentprevIt's more influence than actual regulation enforcement. Smokers these days are seen as social pariahs in some circles. reply CJefferson 14 hours agorootparentprevI'm fairly sure more 13 year olds are on social media, than are drinking, smoking, or on drugs. reply scythe 18 hours agorootparentprevDrinking, smoking and drugs don't depend on a central point of control. Social media companies of any significance can be counted on two hands, and are accountable to corporate boards. reply huytersd 12 hours agorootparentprevAbsolutely. Almost no kids smoke cigarettes (vaping non flavored varieties have almost no risk associated with it) and drunk driving is a shadow of what it used to be. Getting alcohol for someone under 21 is not child’s play either. reply WarOnPrivacy 19 hours agorootparentprevAre you asserting that parents should not have the rights to determine this for their own children? reply giarc 18 hours agorootparentAre you a parent? It's not as easy as saying \"no social media\" to your kids. In this day and age, it's basically equivalent to saying \"you can't have friends\". Online is where kids meet, hang out, converse etc. I'd LOVE to go back to the days before phones and social media, where kids played with neighbours and ride their bikes to their friends house, but that's slowly slipping away. reply foobarian 14 hours agorootparentWe try pretty hard to get our kids to play with their friends in person (we invite them or give rides to playdates) but what do they do when they meet up? Sit on the couch with their tablets and play virtually in Roblox :-) reply amalcon 11 hours agorootparentWhen my friends would meet up in the 80's-90's, quite a bit of Nintendo happened. Is it really that different? The proportion of video games should eventually drop (not to zero), in favor of (if you're lucky) talking and whatever music bothers you the most. reply eitally 11 hours agorootparentprevYou organize playdates for middle & high schoolers? reply somenameforme 18 hours agorootparentprevI'm almost invariably anti-regulation, but in this case - absolutely! There's extensive evidence that social media is exceptionally harmful to people, especially children. And in this case there's also a major network effect. People want to be on social media because their friends are. It's like peer pressure, but bumped up by orders of magnitude, and in a completely socially acceptable way. When it's illegal that pressure will still be there because of course plenty of kids will still use it, but it'll be greatly mitigated and more like normal vices such as alcohol/smoking/drugs/etc. It'll also shove usage out of sight which will again help to reduce the peer pressure effects. This will also motivate the creation/spread/usage of means of organizing/chatting/etc outside of social media. This just seems like a massive win-win scenario where basically nothing of value is being lost. reply ryandvm 16 hours agorootparentprevDo you also think that children should be allowed to buy cigarettes? I'll be honest, I am not certain that social media is any less deleterious than tobacco. I'm a pretty pro-market guy, but there are times when the interests of the market are orthogonal to the interests of mankind. reply tstrimple 14 hours agorootparentI can pretty confidently say that the half a million deaths a year attributable to smoking is a little more deleterious than getting bullied online and the suicides which follow. Many orders of magnitude more. reply nkohari 13 hours agorootparentJust because one thing is worse than the other doesn't mean the less-bad thing is suddenly good. reply tstrimple 13 hours agorootparentThe original statement was: > \"Do you also think that children should be allowed to buy cigarettes? I'll be honest, I am not certain that social media is any less deleterious than tobacco.\" To which I pointed out that cigarettes kill far more people than social media. And your response was somehow that I'm implying that a less-bad thing is good? Are you sure you're following the conversation? It's really not clear that you're addressing anything I said, and it's unclear what your point is. reply amerkhalid 18 hours agorootparentprevYes, parents should not have unlimited rights to determine what is good/bad for their children. Social media is a powerful, addictive and dangerous. Pretty much anywhere on this earth, parents will end up in jail or lose custody of their kids, if they give them harmful substances like drugs. Social media should be regulated like how drugs, alcohol, and cigarettes are regulated. reply 20after4 15 hours agorootparentBecause drug regulations are so effective with no collateral damage at all. /s reply lupire 17 hours agorootparentprevParents can make accounts to use on collaboration with their children. The law prevents the corporation from directly engaging with the child without parental oversight. reply brightball 19 hours agorootparentprevParents don’t have the right to get their kids a tattoo, vote or buy alcohol before a regulated age. Is this different? reply 6yyyyyy 19 hours agorootparentActually, in many states there is no minimum age to get a tattoo with parental consent. https://en.wikipedia.org/wiki/Legal_status_of_tattooing_in_t... reply lupire 17 hours agorootparentLikewise for cosmetic surgical modification of the penis by a doctor, or puncturing the earlobe by a non-medical-professional. reply brightball 17 hours agorootparentprevThat is…shocking reply 20after4 15 hours agorootparentprevMissouri law allows minors to consume alcohol if purchased by a parent or legal guardian and consumed on their private property. edit: Apparently Missouri is not the only state. I had trouble finding a definitive list though. There are also other exceptions such as wine during religious service. reply WindyLakeReturn 12 hours agorootparentWhile there are exceptions, and in general exceptions seem pretty common, they still require businesses to officially get approval and it gives power to parents to enforce rules that would otherwise be hard to do so. Even with the exceptions for children to legally be allowed to drink, I would be surprised if that led to more kids drinking than alcohol obtained illegally, which means the question should be back on how well does the law work (obviously not perfectly, but there is a large gap between perfect and so poorly that it is useless purely from an efficiency perspective). reply WarOnPrivacy 19 hours agorootparentprevSo your answer is yes? Your position is that decisions about youth access to social media should be fully taken from the parents and made by govs instead. Penalties can be assumed from your examples. This is the reality that you want imposed on parents and children - yes? reply somenameforme 17 hours agorootparentThere's actually a really simple and elegant penalty - forfeiture of the device used to access the social media. With all seized devices to then be wiped and donated to low income school districts/families. This gets more complex when using something like a school computer, but I think it's a pretty nice solution outside of that. That's going to be a tremendous deterrent, yet also not particularly draconian. reply runsWphotons 15 hours agorootparentYep thats what low income families need: more cell phones. reply foobarian 14 hours agorootparentNow they too can get on social media! reply brightball 19 hours agorootparentprevIs your position that no age limits should exist for anything? reply samus 18 hours agorootparentprevThis is already the reality for alcohol and plenty of other things. Maybe not everywhere. Reality check: parents giving unrestricted access to these things are usually perceived as irresponsible. reply blitz_skull 14 hours agoparentprevYou're not the only one! I also believe that this is a Big Deal™ that we need to take seriously as a nation. I have yet to see any HN commentator offer a robust pro-social media argument that carries any weight in my opinion. The most common \"they'll be isolated from their peers\" argument seems pretty superficial and can easily be worked around with even a tiny amount of efforts on the parents' part. As an added bonus, this latest legislation removes the issue of \"everyone is doing it\". I mean, sure, a lot still will be—but then it's illegal and you get to have an entirely separate conversation with your kid. :) reply lelanthran 23 minutes agorootparent> The most common \"they'll be isolated from their peers\" argument seems pretty superficial and can easily be worked around with even a tiny amount of efforts on the parents' part. This is so incorrect it makes the flat earth theory look good. reply adamredwoods 7 hours agoparentprevI am against government regulation of websites and classification of websites. If we allow government to do this, it will be politicized at some point. We need to find a better way, for example (just a quick idea), social websites run and protected by a school-by-school basis. This way, it can be regulated and controlled. In other words, government should regulate what they already have control over, not impose new control measures over things they don't. reply dathinab 10 hours agoparentprevIMHO the general idea isn't terrible the implementation is, suppar. But hy that's why it's good that's it's not yet US wide as it means there is time to make improvements. - I'm not the biggest fan of hard cutoff - addictive dark patterns which cause compulsive use should be general banned or age restricted no matter where they are used, honestly just ban most dark patterns they are intentional always malicious consumer deception not that far away from outright committing fraud. (And age restrict some less dark but still problematic patterns.) - I think this likely will make all MMORPGs (and Roblox, lol) and similar 16+, I'm quite split about that. I have seen people between 14-18 get addicted to them and mess up their education path. But I have also seen cases of people which might not be alive anymore today if they hadn't fund a refuge and companions in some MMORPG. - I guess if it can make platforms like YT, Facebook, Instagram, Snapshat etc. implement a \"teen\" mode with less dark patterns and tracking it would be good. - The balance between proving your age and making things available and keeping privacy is VERY tricky (especially in the US) and companies, the government and spy agencies will try to abuse the new requirements for age verification to spy more reliable on everyone 16+. - It's interesting how that affects messengers. Many have less dark patterns, some do not track users, or can easily decide not to track children. The aren't social networks per-se. But most have some social network like features. Even such which do not try to create compulsive use might still end up with it as long as their as is \"live\" chatting. reply trogdor 9 hours agorootparent>addictive dark patterns which cause compulsive use should be general banned or age restricted no matter where they are used, honestly just ban most dark patterns they are intentional always malicious consumer deception not that far away from outright committing fraud. How would you write a law that accomplishes your goal? reply dathinab 1 hour agorootparentThe usage with of user interface design patterns which take advantage of [..] to cause compulsive use is not allowed. In context of this law user interface refers to the mechanism of a user interacting with a software, this includes any kind of user interface no matter weather which medium it uses for presenting information and allowing interactions and no matter which form it is presented at in the software. Where [..] is a rough definition of dark patterns which there exist multiple of in CS and which you can base on factors like 1) deceptiveness, 2) intend to manipulate the users behavior, 3) how it interact with various human feedback systems etc. The thing about law people on HN often tend to forget or complain about is that they intentionally do not need to be perfect precise scientific definitions, predicate logic, or anything like that. This means you can reasonable describe them by likely effect and weather it seems reasonable that the effect could have been intended (without bothering to actually determine intend) without needing to go into _any_ technical details outside of clearing that indeed any form of user interface is covered. reply vimax 5 hours agorootparentprevI'm not a lawyer, but I imagine something like deceptive advertising regulations could serve as a starting point. reply jijijijij 12 hours agoparentprevI am an adult and I wish someone would take social media away from me. Honestly, I think social media has done more harm than good and I wish it would just cease to exist. However, especially in Florida, social media may be the only way for some teens to escape political and religious lunacy and I fear for them. I think it's not wise to applaud them taking away means of communication to the \"outside\", in the context of legislation trends and events there. reply trogdor 9 hours agorootparent>I am an adult and I wish someone would take social media away from me. Why don’t you take it away from yourself? Just delete your account. reply jijijijij 52 minutes agorootparentI tried but then got betrayed by myself. Addiction mechanics are a real thing and sooner or later social media will pop into your life again. For me it's Reddit, not Instagram or Twitter. Luckily, HN makes me always ragequit at some point, cause y'all are a bit special. reply Andrex 6 hours agoparentprevRandom ideas! - Make this an ISP level thing? Somehow? They already know the makeup of a household. If they know a house has kids, something something ToS \"You as the parents are liable...\" Then maybe repeat those scary RIAA letters but \"for good\" when someone in that household hits a known adult IP? - Maybe browsers send an \"I'm an adult\" flag similar to \"Do Not Track,\" and to turn it on, the user has to enter a not-to-be-shared-with-kids PIN? If the browser and OS can coordinate, OSes would be able to tell the browser if the user is an adult, and skip the PIN entering. - Force kids to use a list of Congress-approved devices that gate access to the wider Internet? YouTube Kids but for everything. Yes, hacker kids will be able to get by, but this being Hacker News, they'd deserve the fruits of that particular labor. Just spitballing. Anything obvious I'm missing? PS- I am neither for nor against the Florida-type legislation as of this comment. reply jimt1234 9 hours agoparentprevOne thing I've found interesting with social media and children is that almost every parent I know recognizes the impact social media has on their children, but they willfully ignore it or feel powerless to avoid it. I hear stuff like, \"It's impossible for kids to not be on social media. It's required these days.\", and \"The social consequences will be worse for them if they're not a social media.\" reply robotnikman 13 hours agoparentprevSame here. With the way the internet is nowadays, its probably best to keep kids off the internet until they are older. One just has to look at whats on places like Youtube 'Kids' to see all the stuff that is not kid friendly and probably detrimental to their mental health. reply ben_w 14 hours agoparentprev> I might be the only one here in favor of this, and wanting to see a federal rollout. I'm not American, I think it's perfectly reasonable to ban kids from the internet just by applying the logic used for film classification. Even just the thumbnails for YouTube (some) content can go beyond what I'd expect for a \"suitable for all audiences\" advert. This isn't an agreement with the specific logic used for film classification: I also find it really weird how 20th century media classification treated murder perfectly acceptable subject in kids shows while the mere existence of functioning nipples was treated as a sign of the end times (non-functional nipples, i.e. those on men, are apparently fine). Also, I find it hilariously ironic that Florida also passed the \"Stop Social Media Censorship Act\". No self-awareness at all. reply protocolture 7 hours agorootparentFilm classification is also dumb. In Australia Margaret Pomeranz used to run banned film viewing sessions that ended up in cops wrestling them for dvds. https://www.theguardian.com/film/2023/jul/31/margaret-pomera... reply ben_w 41 minutes agorootparentHence my second paragraph. The first is saying that no matter what your rules are, the internet may at any time show you a clip from the highest rated category and should be treated as such. The internet may also at any time show you outright banned content, but that's often treated as a separate battle no matter if that's Tiananmen Square (or \"content promoting terrorism\", although now I'm wondering if the Chinese Communist Party think the former is a subset of the latter?) or sexual acts you're not allowed to show in film (varies by jurisdiction). reply babyshake 7 hours agoparentprevI'm in favor of this if the only enforcement actions are against social media companies for being predatory, and not against families for breaking the law and allowing their kids on social media. And it's useful for indicating that social media on the balance is not good for kids. reply Spooky23 7 hours agoparentprevCan they use email? Text? The telephone? It’s dumb policy, because Florida GOP. The smart move is to target advertising for kids. If you attack the ability to advertise to the underage audience using mom’s iPad, social media will self police. reply reactordev 5 hours agoparentprevThe answer to our problems is not less freedom. It should never be less freedom. I’m opposed of any law that restricts freedom of information. No matter the age. I think we need to do better on educating kids on online behaviors and we should hold social media companies accountable for addictive features but what we absolutely shouldn’t do is blame little Jenny and take away her access to groups and social interaction online. reply rrrrrrrrrrrryan 5 hours agorootparentYes. Little Jenny should also be free to purchase and drink a bottle of whiskey, but only if her parents are sufficiently negligent. I would've backed your argument up until a few years ago, but the science is coming down pretty hard now showing that social media use is absolutely detrimental to still-developing minds. reply reactordev 5 hours agorootparentI’",
    "originSummary": [
      "The Florida House of Representatives has passed a bill that would prohibit children under 16 from using most social media platforms, irrespective of parental consent.",
      "Advocates argue that social media is detrimental to children, while opponents claim it violates both First Amendment rights and parental autonomy.",
      "This development arises amidst ongoing discussions among social media companies, parents, lawmakers, and healthcare providers on how to address the effects of social media on children's mental well-being."
    ],
    "commentSummary": [
      "The conversation delves into the regulation of social media, age verification systems, privacy concerns, government control, and the impact of internet usage on children.",
      "Alternative solutions like zero-knowledge proofs and reputation systems are discussed, alongside debates on the pros and cons of government intervention.",
      "The conversation highlights the difficulties of striking a balance between freedom of speech, protecting minors, and safeguarding privacy in the digital era. It also touches on the detrimental effects of social media on mental health and the possible necessity of better education on responsible online conduct."
    ],
    "points": 416,
    "commentCount": 613,
    "retryCount": 0,
    "time": 1706534416
  },
  {
    "id": 39183063,
    "title": "New cases of Alzheimer's linked to outdated medical procedure",
    "originLink": "https://www.statnews.com/2024/01/29/first-transmitted-alzheimers-disease-cases-growth-hormone-cadavers/",
    "originBody": "In the Lab Scientists document first-ever transmitted Alzheimer’s cases, tied to no-longer-used medical procedure By Andrew Joseph Jan. 29, 2024 Reprints A toxic protein called beta-amyloid is believed to destroy brain neurons in Alzheimer's patients. Adobe LONDON — There was something odd about these Alzheimer’s cases. Part of it was the patients’ presentations: Some didn’t have the classic symptoms of the condition. But it was also that the patients were in their 40s and 50s, even their 30s, far younger than people who normally develop the disease. They didn’t even have the known genetic mutations that can set people on the course for such early-onset Alzheimer’s. advertisement But this small handful of patients did share a particular history. As children, they had received growth hormone taken from the brains of human cadavers, which used to be a treatment for a number of conditions that caused short stature. Now, decades later, they were showing signs of Alzheimer’s. In the interim, scientists had discovered that that type of hormone treatment they got could unwittingly transfer bits of protein into recipients’ brains. In some cases, it had induced a fatal brain disease called Creutzfeldt-Jakob disease, or CJD — a finding that led to the banning of the procedure 40 years ago. It seemed that it wasn’t just the proteins behind CJD that could get transferred. As the scientific team treating the patients reported Monday in the journal Nature Medicine, the hormone transplant seeded the beta-amyloid protein that’s a hallmark of Alzheimer’s in some recipients’ brains, which, decades later, propagated into disease-causing plaques. They are the first known cases of transmitted Alzheimer’s disease, likely a scientific anomaly yet a finding that adds another wrinkle to ongoing arguments about what truly causes Alzheimer’s. “It looks real that some of these people developed early-onset Alzheimer’s because of that [hormone treatment],” said Ben Wolozin, an expert on neurodegenerative diseases at Boston University’s medical school, who was not involved in the study. advertisement Newsletters Sign up for Daily Recap All the health and medical news you need today, in one email Please enter a valid email address. Privacy Policy Other outside scientists agreed that they found the findings legitimate, in particular because only people who had received cadaveric growth hormone prepared in a particular way — a method that doesn’t eliminate protein bits — went on to develop dementia. Such incidents of illness are known as “iatrogenic,” meaning the result of a medical procedure. In conditions like iatrogenic CJD, the transmissible agents are known as prions — basically, misfolded pieces of protein that go on to cause disease, like a sort of infectious bug. Researchers debate the definition of a prion and whether it could include amyloid. But regardless, the paper’s authors “provide tantalizing evidence that, under extraordinary circumstances, Alzheimer’s disease is transmissible by a prion-like mechanism,” Mathias Jucker of Germany’s University of Tübingen and Lary Walker of Emory University wrote in a commentary also published Monday. Both the study’s authors and outside researchers stressed Alzheimer’s is not some contagious disease that you could catch by caring for a relative, for example. Cases tied to cadaveric growth hormone treatment are also no longer possible; a synthetic hormone has been used instead for decades. The paper’s authors, who run a special prion disease research and treatment center in London, describe just five Alzheimer’s patients out of the more than 1,800 people who were known to have received cadaveric growth hormone in the U.K. from 1959 to 1985. Still, the researchers said the findings were a reminder of the continuing importance of practices like sterilizing neurosurgical instruments, which, in theory, could transfer prions if not properly cleaned between patients. But in addition to being a scientific curiosity — and another example of the fallout of the use of cadaveric growth hormone — these cases could also stir up the decades-long, oft-contentious fight over the roots of Alzheimer’s. “We think from a public health point of view, this is probably going to be a relatively small number of patients,” said John Collinge of the MRC Prion Unit at University College London, the senior author of the paper. “However, the implications of this paper we think are broader with respect to disease mechanisms — that it looks like what’s going on in Alzheimer’s disease is very similar in many respects to what happens in the human prion diseases like CJD, with the propagation of these abnormal aggregates of misfolded proteins and misshapen proteins.” Many scientists believe that beta-amyloid plays a role in the development of Alzheimer’s, and therapies that can clear the protein from people’s brains have finally, after decades of failed attempts, started showing some benefits for patients. But most experts also think amyloid isn’t solely responsible. So in these cases, was there something else going on in addition to the amyloid transfer? “It raises questions about whether amyloid alone is able to cause problems,” said Marc Dhenain, an Alzheimer’s expert at the French research center CEA, who was not involved in the new study. Outside researchers said they were also left scratching their heads about some of the findings. They said there was limited information, with findings reported from just a few patients, and data like genetic sequencing and autopsy results available from just a sample of them. They noted, however, that there didn’t seem to be much inflammation in the patients’ brains, another hallmark of Alzheimer’s that can be induced by amyloid. They also wondered why there was so little presence of another protein called tau in the people’s brains despite their cognitive losses, even though tau levels often correlate to cognitive decline. Some researchers speculated that the patients may have had some other genetic mutation that could increase their risk of Alzheimer’s. Two of the patients had intellectual disabilities, another risk factor. “Can the pathology be transmitted? Yes, it can, and that’s important conceptually,” Wolozin said. “The question is, what’s driving disease? There are many weird things about these rare cases. What’s unclear from the images is, why would they develop such severe dementia that quickly?” Related: Cognito raises $35 million for Alzheimer’s treatment device, touts benefits compared to drugs Globally, more than 200 cases of iatrogenic CJD have been documented in recipients of cadaveric human growth hormone, with the bulk of patients in France, the U.K., and the U.S. And for years, researchers have been gathering clues that there may be an iatrogenic form of Alzheimer’s as well. (The vast majority of Alzheimer’s cases are what’s known as sporadic, developing among older people. There are also some early-onset cases that are tied to inherited mutations.) In 2015, the UCL prion researchers reported finding lots of beta-amyloid in the brains of patients who died of iatrogenic CJD, including a buildup in the brain’s blood vessels, which is known as cerebral amyloid angiopathy and which is seen in most people with Alzheimer’s. It led them to hypothesize that the hormones the patient received had been contaminated not just with the CJD prions, but seeds of amyloid as well. It also made them wonder whether the patients, had CJD not killed them, would have gone on to develop Alzheimer’s. The paper describes eight patients who were treated at the U.K.’s National Prion Clinic from 2017 to 2022, none of whom had CJD and only some of whom were found to have Alzheimer’s. They had received the hormone treatment as children for different causes of short stature: some had brain tumors, others had developmental conditions, others just lacked their own natural hormone. Five of them had been diagnosed with Alzheimer’s or likely had Alzheimer’s disease, with their symptoms starting between the ages of 38 and 55. Among the other three patients, two had some cognitive impairments, while the third was asymptomatic. The researchers noted that the presentation of iatrogenic CJD often differs from that of the more common sporadic cases, so it’s possible the transmitted Alzheimer’s cases would look somewhat distinct from typical cases. Gargi Banerjee, the lead author of the paper, said the UCL researchers considered other possible causes of these patients’ Alzheimer’s. Given the age of the patients, these weren’t sporadic cases, which almost always develop only later in life. The patients didn’t have known genetic mutations that could cause early-onset dementia. The researchers also considered the underlying reasons why the patients had received the hormone in the first place — was it a result of their cancer treatment or congenital conditions? — but that wasn’t a likely explanation either. “The thing about this group is thinking about them as a whole,” she said. “In the people who did have symptoms, they had various different illnesses, but the only combined fact was that they all had this particular type of growth hormone, this particular preparation in childhood. … There was no other unifying cause for this.” About the Author Reprints Andrew Joseph Europe Correspondent Andrew Joseph covers health, medicine, and the biopharma industry in Europe. andrew.joseph@statnews.com @DrewQJoseph Tags Alzheimer’s dementia research Exciting news! STAT has moved its comment section to our subscriber-only app, STAT+ Connect. Subscribe to STAT+ today to join the conversation or join us on Twitter, Facebook, LinkedIn, and Threads. Let's stay connected! To submit a correction request, please visit our Contact Us page.",
    "commentLink": "https://news.ycombinator.com/item?id=39183063",
    "commentBody": "Alzheimer’s cases tied to no-longer-used medical procedure (statnews.com)404 points by leeny 12 hours agohidepastfavorite152 comments bjourne 11 hours agoAfaict, this is not an entirely new finding. For example, this page (https://www.nature.com/articles/d41586-018-07771-6) references a 2018 article about the issue of Alzheimer's being transmitted (or rather \"seeded\") through growth hormone extracted from cadavers. Perhaps the new article provides additional evidence or perhaps the other article only suspected a casual link and this one proves that there is one. I haven't read it thoroughly. What I find interesting and scary is the \"seeding\" part. The small amount of growth hormone injected to the children cannot itself have caused Alzheimer's. But it must have caused some rewiring of the brain to make it accumulate more plaque which over a period of many decades slowly decreased their brain performance. If this effect is synthesizeable then one can easily imagine many countries using it to develop terrifying biological weapons. reply diob 11 hours agoparentIt mentions brain proteins contaminating it, makes me think of prions. reply EdwardDiego 10 hours agorootparentYeah, these paragraphs from the paper sounds like the amyloid proteins can sometimes look/act like a prion if you squint: > The far wider relevance of prion mechanisms was first exemplified with the discovery of yeast prions but has also widened considerably with the recognition that the more common human neurodegenerative diseases, including Alzheimer’s and Parkinson’s diseases, involve accumulation and spread of assemblies of misfolded host proteins in what is often described as a ‘prion-like’ fashion with experimental transmission of relevant pathology in primates or mouse models. However, the importance for human disease was unclear until the recognition of human transmission of amyloid-beta (Aβ) pathology via iatrogenic routes after prolonged incubation periods, causing iatrogenic cerebral amyloid angiopathy (CAA) and raising the possibility that iatrogenic Alzheimer’s disease may occur at even longer latency. reply jychang 41 minutes agorootparentThe thing is (despite popular misconception that prions are common and many proteins that can turn into prions), there is only a specific protein that can be a prion- the PrP protein. https://en.wikipedia.org/wiki/Major_prion_protein Demonstrating \"some other other protein behaving like a prion\" is close to nobel prize territory. reply DoingIsLearning 3 minutes agorootparentPerhaps a first pass research question is: Do people who undergo spinal, brain, retinal surgery have a higher incidence of Alzheimer when compared with the rest of the population? Otherwise there is no point in chasing this \"some other other protein behaving like a prion\" avenue. reply goku12 8 hours agorootparentprevThe article mentions that the procedure was banned due to incidence of Creutzfeldt-Jakob disease. That is a prion disease. reply klipt 5 hours agorootparentThey still use pieces of cadaver for other surgeries. E.g. gum and bone transplants done by periodontists. Seems risky to me. reply pvaldes 1 hour agorootparentPeriodontists have been also linked with a higher number of Alzheimer cases. Is a group of risk in this disease. reply woleium 5 hours agorootparentprevas long as it’s not spine or brain matter it’s probably okay. During the mad cow outbreak (BSE, the bovine equivalent of CJD) in the UK, they banned “beef on the bone” but not all beef. reply djmips 5 hours agorootparentprevMight depend on the age of the patient. reply eigenvalue 10 hours agoprevI wonder what other stuff is being unwittingly transferred during standard blood transfusions. I've seen some interesting research about the \"signalling\" power of blood from both older and younger donors, with the \"young blood\" causing a slow down of senescence in cells, and the \"old blood\" causing a speed-up in senescence. Any time you take biological material out of other human beings and put them in a different body, it seems like you are introducing a lot more uncertainty and risk than when you inject a person with a comparatively \"simple\" small-molecule drug. reply palijer 6 hours agoparent>Any time you take biological material out of other human beings and put them in a different body, it seems like you are introducing a lot more uncertainty and risk But this always had to be weighed against the risks of not getting the transfusion. Typically the consequences of not getting a transfusion when it is medically indicated is pretty severe. We don't really need to worry about unlikely not-yet-understood edge cases that happen years later from procedures we've done millions of times. We gotta gotta about blood loss. reply axus 10 hours agoparentprevMy mom developed an allergy to eggs after a blood transfusion. She long regretted never being able to eat another chocolate eclair. reply eigenvalue 10 hours agorootparentThat’s nuts. Seems like researchers should look really closely at weird side effects of blood transfusions since these are effectively “natural experiments” that would be impossible or unethical to run normally. reply kccqzy 9 hours agorootparentGenerally, I think if a patient needs blood transfusions there's a bigger and more immediate life-and-death problem right there. I think allergies would be preferable to death. Is my understanding correct? I hope blood transfusion hasn't become a routine procedure these days. reply fastball 8 hours agorootparentI don't think GC was suggesting that we stop blood transfusions, but rather that we do better tracking to improve our understanding in a way that normally would be unethical (if not for the fact that, as you say, blood transfusions are a life-saving intervention). reply eigenvalue 8 hours agorootparentYes exactly. reply bobsmooth 8 hours agorootparentprevYou get a blood transfusion for any major surgery. reply Arrath 8 hours agorootparentMakes me wonder if there is any possibility to give blood in the lead-up to your surgery, such that you're transfused with your own blood? reply blendergeek 8 hours agorootparentIt's called an autotransfusion and it's a thing. https://en.wikipedia.org/wiki/Autotransfusion reply mhaberl 3 hours agorootparentprevYou get it IF there is a need for it, not always. I had an open heart surgery and didn't get a blood transfusion. reply EdwardDiego 10 hours agorootparentprevThey do. These incidences are rare, but documented, and researched. https://www.scientificamerican.com/article/when-peanut-aller... https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4562830/ It looks like the allergy isn't permanent though. reply d1sxeyes 1 hour agorootparentprevNo, eggs, not nuts. I'll see myself out. reply RajT88 8 hours agorootparentprevWell, the boffins figured out somehow that if you get a blood transfusion from an Opossum you become immune to all snake venom for a while. (Seems wikipedia removed this factoid now... and other sources state \"most\" snake venom) I assume they tested on animals, not humans. reply bglazer 8 hours agorootparentCan you provide a source? That sounds unlikely reply RajT88 7 hours agorootparentHere is something scholarly: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5315628/ I cannot find anything now talking about a blood transfusion (I probably read about it 10 years back). It more or less jives with the scholarly article in that \"immunogenic effects are reported\". A slightly less dry read: https://www.grunge.com/1105496/how-an-opossum-could-be-the-r... reply withinboredom 9 hours agorootparentprevYou are supposed to declare your issues before (or at least after) you give blood. Kinda messed up that whoever donated that blood didn't report it. Then again, I'm glad the blood was available... reply elzbardico 7 hours agorootparentWe can't conclude that the donor had an egg allergy. The immune system is complicated as hell, it could be the case that he had some weird protein circulating on his blood that was recognized by her immune system, and by chance this protein was structurally close enough to some egg protein that now eggs trigger an immune response on her. reply scythe 7 hours agorootparentWorth noting that the most prevalent protein in blood is albumin, and the major protein in eggs is, yep, albumin. reply hirvi74 5 hours agorootparentAre you implying there is a connection? Even if it's just for nonsense, I'd love to read it. Because other prevalent commonalities, you could make the same argument about water, for example. reply jraph 3 hours agorootparentprevI don't think I ever had to declare allergies when donating my blood (France). They focus on infections and cancers I think. Why would they ask for allergies if we don't yet know that they can be transmitted like this? reply withinboredom 3 hours agorootparentI thought it was well known that certain allergies (eggs/peanuts) can transfer via transfusions? At least for awhile. It looks like there’s some literature on it at least, but I didn’t dig into it or anything. I just thought this was “common” knowledge since that’s what I was told years ago by a nurse I was dating in college, a long time ago. reply jraph 2 hours agorootparentMaybe it's known but I don't know about it :-) reply fbdab103 6 hours agorootparentprevSome 30% of people have allergies. It can already be challenging to source enough blood with the current eligible population. reply BurningFrog 5 hours agorootparentprevKilljoy note that the allergy may well be unrelated to the transfusion, despite occurring after it. reply dehrmann 6 hours agorootparentprevEggs or chicken eggs? Could always use duck eggs. reply teaearlgraycold 9 hours agorootparentprevIf your mom’s still around let her know about vegan egg alternatives! reply elzbardico 7 hours agorootparentI am not a vegan, but I eat a lot of vegetables, most vegetables, if prepared correctly are delicious, there are a lot of traditional dishes in every cuisine in the world that take absolutely no animal products on their preparation and have a wonderful taste and aroma. I see no reason to eat those franken-foods just because you're a vegan, a lot of them have strange additives to make them taste and look like animal stuff, it is simply not worth the risk to eat them IMHO. Of course, this is a personal view point. reply Cpoll 7 hours agorootparent> I see no reason to eat those franken-foods just because you're a vegan, a lot of them have strange additives to make them taste and look like animal stuff, it is simply not worth the risk to eat them IMHO. \"Just Egg\" is mung bean and canola oil. You can DIY it for cheaper, it's like buying pancake mix. There's always a few strange additives for these sorts of products (improve shelf life, anti-caking, emulsion, whatever), but that's not a vegan-specific thing. reply pnw 3 hours agorootparentprevAquafaba is a completely natural vegan egg substitute used in baking. https://en.m.wikipedia.org/wiki/Aquafaba reply axus 4 hours agorootparentprevShe honestly asks me to bring over vegan mayonnaise sometimes. She also has a good egg-free cake recipe. reply syndicatedjelly 8 hours agorootparentprevVegan egg tastes like butts reply lostlogin 10 hours agoparentprev> I've seen some interesting research about the \"signalling\" power of blood from both older and younger donors, with the \"young blood\" causing a slow down of senescence in cells Maybe the Silicon Valley ‘blood boy’ can be brought to market. reply aeternum 8 hours agorootparentI believe Ambrosia was the (real) startup that was the inspiration for that story arc. Looks like they're still around but have pivoted to boring wearables and of course AI. A supposed win for the FDA, but perhaps a loss to humanity since there does seem to be evidence that the idea actually worked. reply q1w2 10 hours agoparentprevNot only blood transfusions, but ligament/tendon transplants from cadavers are extremely common for people who tear their ACL. It would be a disaster if this type of surgery also transmitted some prior/protein misfolding disease decades later. Millions would be impacted. The practice stared in the 1980s, but really only became popular in the early 2000s with the boom in arthroscopic surgery standardization. Hopefully the blood-brain barrier prevents this. reply eigenvalue 10 hours agorootparentOh man, I bet you’re right and enough time hasn’t gone by to see the fallout from it! I bet rich people will start bidding up tendons and ligaments from younger cadavers (probably mostly motorcycle accident victims). Although given that so many of those have toxoplasmosis, maybe that’s also not great… reply fbdab103 5 hours agorootparentOne estimate says that 30-50% of all human beings have toxoplasmosis, so I would put that as pretty low on the list of risks. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3963851/ reply ikrenji 8 hours agorootparentprevthose tissues are unlikely to carry prions. prions are concentrated in the brain. reply coffeebeqn 8 hours agorootparentprevCould they test the donor first for these proteins or whatever is causing it? reply PeterisP 7 hours agorootparentWe probably could, but only after we'd figure out what exactly is causing it. reply selcuka 10 hours agoparentprev> I've seen some interesting research about [...] the \"young blood\" causing a slow down of senescence in cells, This is excellent material for the conspiracy theorists. reply caycep 10 hours agoprevThere is a line of research into the \"transmissible' phenomena of a lot of these neurodegen dz. Epidemiologically, it's probably not infectious from one person to another bc we don't see spouses getting PD or AD often. But there's interesting phenomena - i.e. one of the Parkinson's stem cell implantation trials got a lot of press after the trial (which failed, didn't show benefit) bc after subjects passed several years later and got autopsied, they found clumps of parkinsonian proteins (lewy bodies) on the histology slides of the implanted stem cells. Similarly, there's some papers w/ mice w/ knockout Parkinsonian genes getting parkinsonian features and lewy bodies when injected w/ abnormal misfolded synuclein from another mouse. What exactly to do w/ this, no one is entirely sure yet. reply dimask 9 hours agoparent> we don't see spouses getting PD or AD often Actually it may seem so [1], though still there is not any conclusive evidence to support a transmission hypothesis really as all this could be due to increased stress and such factors. Also, brain surgeons' increased risk of AD and more reports of associated risks with regard to contamination from brain operations [2] (similar to the article's ones) provide more indications that such a hypothesis is not completely implausible. Though also far from strongly supporting it or anything, as there is no proper experiment design with control groups etc to make better conclusions. [1] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2945313/ [2] https://www.medicaldesignandoutsourcing.com/report-suggests-... reply foota 9 hours agoparentprevI guess if your brain proteins are mixing with someone elses you normally have a bigger issue. reply knodi123 8 hours agorootparentI don't know where your neuro-jack is installed, but if it's not in the skull then I don't know how you even connect to the all-mind. reply msla 7 hours agoparentprev> w/ mice Is this a special kind of mouse? reply denysvitali 2 hours agorootparentw/ = with w/o = without This comment is full of abbreviations (PD = Parkinson's Disease, AD = Alzheimer's Disease) that makes it a bit difficult to read (if you don't know / realize what those abbreviations stand for) reply mbo 6 hours agorootparentprev\"w/\" is an abbreviation for \"with\". \"w/ mice\" should be read as \"with mice\". reply pvaldes 1 hour agorootparentThe savings of just two characters does not worth the effort and mental pause to decode it reply elzbardico 7 hours agorootparentprevMice are a good animal model for a lot of the human physiology. reply devmor 8 hours agoparentprevCan you explain more or give a source on that parkinsons stem cell trial? I’m not sure I understand what happened and I’d like to learn about it. reply secondcoming 10 hours agoparentprevnext [3 more] [flagged] jurynulifcation 10 hours agorootparentnext [3 more] [flagged] ra 10 hours agorootparentmakes it hard to read reply s1artibartfast 10 hours agorootparentread it somewhere else? reply hackernewds 7 hours agoprevI thought the beta ameloid theory was largely debunked since the publisher had manipulated data. Now it's attached again? I'm so confused reply rajup 11 hours agoprevMy understanding was the beta-amyloid hypothesis itself is under some amount of scrutiny and may not truly explain Alzheimer's. Wonder if this finding adds more evidence for the amyloid hypothesis. reply scythe 7 hours agoparentThis finding definitely supports a brain protein hypothesis, but it could just as easily point to tau protein as amyloid. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3399531/ reply Mistletoe 11 hours agoparentprevI thought CJD was caused by a prion. Was it in the growth hormone prep from the cadavers? reply kibibu 11 hours agorootparentFrom the article: > In the interim, scientists had discovered that that type of hormone treatment they got could unwittingly transfer bits of protein into recipients’ brains. In some cases, it had induced a fatal brain disease called Creutzfeldt-Jakob disease, or CJD — a finding that led to the banning of the procedure 40 years ago. reply treyd 11 hours agorootparentprevYeah that's correct. The article mentions a few cases of people who died from transmitted CJD that may have also received some beta amyloid or tau proteins that could have catalyzed development into Alzheimer's if only they didn't die from CJD, which seems to develop more quickly through that route. If Alzheimer's is truly is some kind of prion-related disease, which the research is suggesting. reply caycep 10 hours agorootparentprevthere's transmissible CJD, but most cases are a genetic version where you get the prions from your own cells and then it cascades, if I recall reply m3kw9 11 hours agoparentprevNot sure why scientists has to pin point a single cause where as you have different vectors that can cause the same disease is also plausible reply pedalpete 11 hours agorootparentOr that we've been lumping multiple different types of cognitive decline as a single disease. reply hmottestad 10 hours agorootparentCan’t you make a definitive diagnosis from an autopsy. Could be that multiple diseases lead to the same findings in the autopsy. While someone is alive a diagnosis of Alzheimers is more of a diagnosis of elimination. There are several drugs that can be used to slow the progression and I assume that those have a role to play in solidifying the diagnosis. reply tsimionescu 2 hours agorootparentYou can check for amyloid plaques in an autopsy, but apparently plenty of old people that show no significant cognitive decline also have amyloid plaques. So, while we would definitively call it Alzheimer's if you have significant cognitive decline + amyloid plaques, it's not 100% clear that this is a single diagnostic. reply hackernewds 6 hours agorootparentprevThat ventures wildly into whether the realm of physicalism explains malaises that happen in the \"consciousness\" realm. spooky reply 0cf8612b2e1e 10 hours agorootparentprevScientists are a bit more savvy than you are giving them credit. Anything neurological is unlikely to have a silver bullet treatment. So researchers are going after what looks most promising with respect to understanding the disease as well as a path towards treatment. Which is currently AB (or Tau), but there is plenty of skepticism that we are chasing the only measurement we have. Yet many other correlations are even hazier than AB. reply resoluteteeth 7 hours agorootparentprevI don't think scientists are necessarily assuming a single cause so much as that it's a moot point since we still haven't identified any causes for sure. If some cases are caused by amyloid plaques and some aren't, for example, and we develop a treatment that cures the amyloid plaque cases but not the others, it will probably become extremely obvious that there are different causes at that point. reply arcticfox 11 hours agorootparentprevThey don't, but my surface understanding is that the medicines that effectively nuke beta amyloid have no effect overall. Which would mean it's not even one of many causes, it seems to not be a cause at all. reply elcook4000 4 hours agoprevThis is really fascinating, horrifying and hopeful at the same time. It makes sense that some neurodegenerative diseases with unknown etiologies are caused by prions or prion-like proteins. It could be fruitful to study the rate of Alzheimer's among nursing assistants who work in elder care. I found a few resourcas stating a higher rate for caregiver's in general along with nurses. I have thought decreased immune function from aging leads to the increased permeability of the blood brain barrier: which leads to the infiltration of pathogens and now possibly prions. I would assume this could be, or possibly has been, studied in animal models. reply caesil 5 hours agoprevThis is one of the reasons that, despite no direct evidence of harm, I will only ever take marine collagen instead of bovine collagen. Do not fuck with the possibility of ingesting cow nervous system tissue, let alone human. reply dmead 4 hours agoparentTell that to my heart valve. All bovine baby. reply yungporko 2 hours agoparentprevwhy is cow or hunan nervous system tissue particularly dangerous? what animal is marine collagen from and why is it less dangerous? reply ultra_nick 1 hour agorootparentPrions. Non-mammal prions have a different enough shape to avoid breaking mammalian biomechanisms. reply Terr_ 12 hours agoprev4 hours ago: https://news.ycombinator.com/item?id=39179368 Recycled comment follows. ___________ To highlight: > The authors and other scientists stress that the research is based on a small number of people and is related to medical practices that are no longer used. Also that there is zero-reason to believe in any person-to-person spread: > The study does not suggest that forms of dementia such as Alzheimer’s disease can be contagious. Lastly, a fun vocabulary word [not in that article]: \"Iatrogenic\" - An illness caused by medical examination or treatment. reply dekhn 11 hours agoparentDon't forget nosocomial- infections acquired in a healthcare setting. reply admissionsguy 11 hours agoparentprevAnother one is \"nosocomial\" - (disease) originating in a hospital reply sebazzz 4 hours agoprevPrions are most scary bit of biology there is. Almost impossible to eleminate, causes victim proteins to unfold. reply lswank 8 hours agoprev\"Is it circumcision?\" I love how this is what people say when reading over my shoulder. reply inasio 9 hours agoprevAlzheimer's as a (transmittable) prion disease is the stuff of nightmares (prion disease == incurable) reply fastball 8 hours agoprevI'm somewhat surprised by this publication's restraint when referencing Creutzfeldt-Jakob disease, as usually news orgs like mentioning the fact that Creutzfeldt-Jakob is the human form of mad cow disease. reply yungporko 2 hours agoparentit does say that that BSE is the bovine equivalent of CJD in the article reply jokoon 10 hours agoprevweird that the bulk of patients are in france and UK, does that indicate diet? reply EdwardDiego 9 hours agoparentAll the researchers are all UK based, and the UK became verrrry interested in prion diseases due to a very bad habit UK agriculture had gotten into of incorporating culled animals (other cattle, which furthered the spread, but the ultimate cause was likely grinding up sheep infected with scrapies, an ovine spongiform encephalopathy) supplementary cattle feed for animals later eaten by humans which led to the CJD outbreak in the UK. European agriculture also had a similar bad habit, but maybe less sheep or scrapies or something? France probably imported British beef and so were affected also. They certainly banned it the longest afterwards. And I was about to do the usual Kiwi thing of \"Just let the cows eat grass, duh\", but every Western country tends to supplementary feed dairy cows, including mine, except we prefer to use palm kernel, thus promoting deforestation in Borneo, yay! But it turns out a lot of British beef comes from their dairy herds, so \"just feed the beef cattle grass\" wouldn't have helped. reply coffeebeqn 8 hours agorootparentCows are also fed “sewage solids” from water treatment plants. Now that I think about it maybe I should stop eating / drinking cow products reply the_optimist 9 hours agorootparentprevA compelling book on this topic: https://www.simonandschuster.com/books/Deadly-Feasts/Richard... reply sidewndr46 7 hours agoparentprevMy understanding is France has a history of ignoring risks, including intentional spread of HIV to patients: https://en.wikipedia.org/wiki/Contaminated_blood_scandal_in_... reply radium3d 8 hours agoprevHmm, I wonder if the same could occur from a bone graft from a cadaver, like for a tooth implant? reply LASR 8 hours agoprevI wonder what other practices from decades ago are lurking to be discovered as catastrophic to currently living people. I remember reading about cattle rearing practices a while ago that might be responsible for some prion related diseases. Can’t remember the exact source. These things get you from entirely unrelated sources. reply Gare 5 hours agoparentThere was a whole epidemic: https://en.wikipedia.org/wiki/United_Kingdom_BSE_outbreak reply jollyllama 11 hours agoprevWhy isn't the procedure used anymore, since they stopped using it before this was discovered? reply munificent 11 hours agoparentSecond paragraph of the article: >In the interim, scientists had discovered that that type of hormone treatment they got could unwittingly transfer bits of protein into recipients’ brains. In some cases, it had induced a fatal brain disease called Creutzfeldt-Jakob disease, or CJD — a finding that led to the banning of the procedure 40 years ago. In other words, it gave people mad cow disease. reply bemusedthrow75 9 hours agorootparentNot “mad cow disease”, but CJD. “Mad Cow Disease” specifically refers to the variant form that is said to have been caused by prions from infected animals turning up in beef products. Though in fact all we know for sure about the link there is that it’s the same prion in the cattle and human cases; the suggestion that there’s a direct food chain connection is still only considered very likely, not proven. reply MathMonkeyMan 9 hours agorootparentprevThat's not to mention the zombies. There was a huge cover up. reply skissane 11 hours agoparentprevWe worked out to produce these hormones (or equivalent compounds) synthetically, so we no longer need to extract them from the brains of deceased humans, a procedure which risks transmitting disease. Not just human growth hormone, also other hormones used to be derived this way, e.g. those used to induce ovulation in fertility treatment. I know someone who received cadaver-derived fertility hormones in the 1980s. She has a small risk of developing CJD and dying from it. It hasn't happened yet, and probably never will, but no one can say for sure if she is infected. If you don't develop symptoms (some people are infected but never progress, others suddenly develop symptoms one day after decades of being asymptomatic), the only way to know for sure if you had it is at autopsy, through destructive testing of brain tissues. reply bemusedthrow75 11 hours agoparentprevBecause it was shown to also spread a prion disease (CJD, though not the variant kind as far as I have understood) reply Reubend 11 hours agoparentprevAs the article says, there is now a synthetic hormone which is given to the patients. We don't need to extract the hormone from dead people anymore. reply bemusedthrow75 9 hours agorootparentYes, but that just would cause the derived product not to be needed, when it was actually actively banned because it was proven to have caused some cases of CJD. reply jjtheblunt 10 hours agoparentprevDid you even try reading? It's made super clear at the start. reply jollyllama 10 hours agorootparentYes, I read the whole thing but I missed the second half of the last sentence of the second paragraph. The fact that one poster replied that it's due to the advent of synthetic hormones and others replied about the spread of CJD indicates it's not super clear, but sibling replies got me the insight I was looking for. reply jovial_cavalier 11 hours agoprevI've often wondered if Alzheimer's is actually novel instances of prion disease. For instance, Kuru was developed within I think a handful of generations in a population of ~20,000 with limited opportunities for transmission (only transmitted when someone dies and their family eats their brain). If the base incidence rate for a novel prion is that high, can you explain Alzheimer's as just that? It would be sad news for pharmaceutical companies - it would render Alzheimer's as a disease in the same class as cancer. Total systems breakdowns that are low-probability but inevitable on a long enough timescale. reply karmajunkie 10 hours agoparentit’s almost certainly not, or you would see epidemiological evidence for chains of transmission. prion diseases require contact transmission which is all but absent from the alzheimer’s story. reply resoluteteeth 7 hours agorootparent> it’s almost certainly not, or you would see epidemiological evidence for chains of transmission. prion diseases require contact transmission which is all but absent from the alzheimer’s story. I think what the parent comment is saying is, what if it isn't transmitted, what if a prion is just occurring in the brains of the people who develop alzheimers? That seems unlikely to me, but on the other hand, the article seems to be suggesting that transferring brain proteins from people with alzheimers to people who are younger will cause them to develop alzheimers which would be roughly consistent with that. I don't know whether that would be possible, but, for example, what if there was somehow a specific protein in the brain that could easily be misfolded to become a prion, and the eventually if people live long enough they tend to produce that prion at least once, so it occurs essentially as a result of old age, but it can theoretically also be transmitted in the manner described in the article? reply throwaway8877 10 hours agorootparentprevIs it still possible that the chains are there but are missed because of the disease very slow progress? reply jovial_cavalier 9 hours agorootparentprevWe don't eat the brains of people with Alzheimer's, so we don't see it transmit? I know CWD transmits from pretty much any shedding of the animal, but is that necessarily universally true? Can we map that onto human beings who practice hygeine and don't eat leaves that another human urinated on? reply karmajunkie 8 hours agorootparentconsumption of neural tissue isn’t the only transmission vector though. (See TFA for an example, in fact.) If it were a prion disease, just the law of averages dictates that we’d see some transmission from things like organ transplants from pre-symptomatic carriers. There’s no evidence of that at all that i’m aware of. (disclaimer: I work at a startup in cognitive testing, so while i’m certainly not a researcher in the field, i do see quite a bit of research on dementia-adjacent diseases) reply jovial_cavalier 8 hours agorootparentBut don't you also have a windowing effect there? How many >50 y/o people are donating their organs? You could also explain the age skewness by allowing for the fact that it takes time for the prions to replicate to the point that you notice symptoms. Other I think this would predict: lifetime exposure to mutagens is a predictor for Alzheimer's. Alzheimer's is more heritable from the mother than the father. reply genman 11 hours agoprevSo is this small bit of information any use for figuring out the general case? reply Reubend 11 hours agoparentYes, definitely. It hints to the root cause being \"very similar in many respects to what happens in the human prion diseases like CJD, with the propagation of these abnormal aggregates of misfolded proteins and misshapen proteins.\" So while this might be unrelated to the general cases, it's still a promising area of investigation. reply anthk 7 hours agoprevThanks to CIC Biogune too for the early research: https://www.cicbiogune.es/news/us-alzheimer-association-fund... reply gustavus 9 hours agoprevI'll admit that I am somewhat ignorant here. But I thoughts the \"Alzheimer's is caused by plaque buildup in the brain\" theory was on its last leg and pretty much disregarded by most new scientists. reply devmor 8 hours agoparentI am also a layman but I believe I’ve read that its no longer believed to be caused by the plaque, but the plaque is a clear signal/comorbidity of the underlying cause - whatever it is? reply boringuser2 11 hours agoprevI actually don't believe this conclusion based off the fact that it would be slam-dunk evidence for the pathogenesis of Alzheimer's, which we don't have. reply echelon 11 hours agoparentMultiple pathways could lead to the disease. It's entirely possible that a metabolic dysfunction or viral origin causes immune dysfunction and the downstream dysregulation, misfolding, amyloid/tau signals, etc. There might be multiple entry points to causing this disease. reply boringuser2 11 hours agorootparentThat might be true, but I don't think this makes the conclusion any more compelling because it now fails Occam's razor. Chances are greater the conclusion is simply incorrect. reply cbsmith 11 hours agorootparentThat's a misapplication of the razor here. The simplest explanation for the statistically unusual prevalence of the disease amongst these patients is indeed that there is a causal link. reply magicalhippo 8 hours agorootparentprevIsn't the point that when comparing multiple explanations of a phenomenon one applies Occams razor to pick the \"best\" one? In this case we don't have any explanations to compare yet, just suggestive lines of research. So it's premature to bring out the razor. reply echelon 11 hours agorootparentprev> it now fails Occam's razor. I mean, mechanistically speaking, so does cancer. I don't think that's quite the lens to apply to biological systems. Biology is wildly complex and subverts expectations all the time. reply boringuser2 11 hours agorootparentIt is the lens to apply to any system. You don't defend a tenuous conclusion by doubling down with a tenuous defense. \"Science\" is constantly inaccurate. I can pull two papers right now with opposing conclusions. Assuming a scientific conclusion is simply incorrect is a pretty good bet, even if you have absolutely no context or idea what is happening at all. reply echelon 11 hours agorootparent> It is the lens to apply to any system. I can point to countless instances where it fails to adequately guide investigation in biology. Occam's razor leads to premature simplification. When the space is vast, dynamical, and unknown, it's absolutely not a tool. Do you think, for instance, that V(D)J recombination satisfied Occam's razor when we asked ourselves how adaptive immunity worked, or how some forms of diseases such as SCID manifested? There is a metric ton of pure serendipity in the study of biology. We're drawing new connections between systems all the time. This is just a new data point. reply boringuser2 10 hours agorootparentI think you're misunderstanding slightly the utility function of the logical device here. It's not so much that we cannot analyze a complex system using this criterion, it's moreso a tool that allows us to identify a logically tenable pathway for investigating a specific element of reality. For example, a poster above noted that I was misapplying the razor because he went one level below where I was analyzing. He's not necessarily incorrect, and neither am I. You interrogate reality at multiple levels of magnification, which is why you don't need to know how genes specifically work to know that they work and make predictions based on their prevalence, for example. reply dukeofdoom 5 hours agoprevMight be somewhat related. Government in Canada has fully legalize what they call MAID (Medical assistance in dying). Do they know something big might be coming soon. http://hmi-us.com/publications/sars-cov-2-prion-like-domains... reply m3kw9 12 hours agoprevI thought it was still early in 2024 and we have this news but good to know you need to actually transplant brain matter to get it reply aurizon 11 hours agoprevNew prion diseases are being found in other animals and might evolve to enter humans. There is a deer version being watched - not yet found in people, AFAWK? https://www.cdc.gov/prions/cwd/index.html There have been many cases of assorted Human Papilloma viruses (HPV) transmitted by kissing as well as assorted variances of oral sex. Recent vaccines are very effective.https://www.cdc.gov/vaccines/vpd/hpv/public/index.html reply CommanderData 11 hours agoparentDo you mean HSV? I have seen articles implicating that virus but it's the first time I've heard of a HPV association. reply caconym_ 10 hours agoparentprevWhat do these two things have to do with each other? reply m-i-l 11 hours agoprevSlightly more clickbaity title than the BBC's \"Medicine stopped in 1980s linked to rare Alzheimer's cases\"[0] which also says \"The findings do not mean Alzheimer's is infectious - you cannot catch it from contact with people who have it... The researchers say all of the people in their study had been treated as a child with cadaver-derived human growth hormone, or c-hGH, that was contaminated with brain proteins that are seen in Alzheimer's disease... used to treat at least 1,848 people in the UK between 1959 and 1985\". [0] https://www.bbc.co.uk/news/health-68126907 reply caconym_ 10 hours agoparentI didn't find it clickbaity at all. The contents of the article were exactly what I expected based on the headline, and I didn't know this was possible. The really interesting thing here is that Alzheimer's seems to be transmissible from person to person via biological material. The BBC headline you quoted, \"Medicine stopped in 1980s linked to rare Alzheimer's cases\", totally buries the lede and is borderline misleading. reply jessriedel 9 hours agorootparentMy reading is that m-i-l is using \"clickbaity\" to mean \"doesn't prevent all possible wrong conclusions one could draw from the headline\". (In this case, the wrong conclusion would be thinking that because there exists cases of alzheimer being transmitted through some mechanism that Alzheimer was contagious.) While it's certainly understandable that we are all tired of clickbait that purposefully misleads the reader, imo we should not overcorrect by demanding that headlines cannot be misinterpreted by arbitrarily ignorant readers. reply m-i-l 1 hour agorootparentThat's right. Living in the UK and taking an interest in popular science during the \"mad cow disease crisis\"[0], I had been aware that prion diseases can be transmitted in humans if (for example) you eat (infected) brains of your dead ancestors[1], but the headline as originally submitted (\"Scientists document first-ever transmitted Alzheimer's cases\") did suggest to me that we might be at the start of something altogether new and much more alarming. [0] https://en.wikipedia.org/wiki/Bovine_spongiform_encephalopat... [1] https://en.wikipedia.org/wiki/Kuru_(disease) reply eek2121 9 hours agorootparentprevI didn't either, and I didn't even read the article. The headline was actually dead on factual for me. reply oli-g 9 hours agorootparentFor me as well, and I didn't even read the headline. reply josu 9 hours agoparentprev>The findings do not mean Alzheimer's is infectious - you cannot catch it from contact with people who have it. Alzheimer's being an infectious disease is still being researched. The article does nothing to disprove it. reply mike_d 9 hours agoparentprev> \"The findings do not mean Alzheimer's is infectious\" It may not be infectious like a cold or herpes, but it was amazing to me that it is transmissible. I spent some time Googling and it looks like the appropriate term is \"Donor-Derived Infections.\" reply tsimionescu 2 hours agorootparentI think that, in principle, any disease that a donor has has some small chance of affecting the recipient. Infections, cancers, prion diseases, toxins are well known to do so, some immune issues also have a clear pathway, but in principle even genetic issues could affect the recipient depending on a whole host of complicated biology that we don't fully understand. reply BizarroLand 9 hours agoparentprevI wonder if we will ever find methods of flushing the brain out or doing anything within the human brain that will allow us to defend against things like this. I know our brains are very protected in our bodies and for good reason but I still wonder how far we will be able to go if we can ever safely and humanely bypass that. reply wannacboatmovie 10 hours agoprev [8 more] [flagged] EdwardDiego 10 hours agoparentI'm not sure it's anywhere as widespread as you think it is, or why it's relevant to this apart from \"it too involves medicine\" and, well, people are researching medical interventions longitudinally full-time, so I suspect it'll be analysed accordingly. Interesting that you've prejudged it to be clearly bad, I'd wait for scientific evidence of that. reply wannacboatmovie 10 hours agorootparentScientific evidence is already emerging: https://archive.is/lUUQi Big time issues with bone density. People in their 20s breaking hips and developing osteoporosis. That's not okay. reply EdwardDiego 7 hours agorootparentThat makes sense. reply plorkyeran 10 hours agoparentprevPuberty blockers have bene in use for about 40 years now. If it takes another 20 or 30 years to determine that it was a bad idea then the consequences must have turned out to be pretty mild. reply caconym_ 10 hours agoparentprevAre puberty blockers extracted from the brain matter of cadavers? reply StimDeck 10 hours agoparentprev [–] That depends on if you are familiar with the concept of trade-offs and recognize that the alternatives could have been worse. reply wannacboatmovie 9 hours agorootparent [–] Not sure I want to dignify your snarky response but are you implying that there are only two possibilities; suicide or taking a handful of pills with untold consequences? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Scientists have discovered the first cases of transmitted Alzheimer's disease, linked to a past medical procedure involving cadaver-derived growth hormone.",
      "Patients who received the hormone as children developed signs of Alzheimer's disease decades later.",
      "The hormone transplant introduced the beta-amyloid protein into the patients' brains, causing disease-causing plaques and suggesting a prion-like transmission mechanism."
    ],
    "commentSummary": [
      "The discussions delve into various topics such as disease transmission during medical procedures, concerns about blood transfusions, and the use of biological materials from other individuals.",
      "There is an exploration of the connection between prion diseases and neurodegenerative disorders like Alzheimer's and Parkinson's.",
      "The discussions also analyze the causes and transmission of Alzheimer's disease, emphasizing the need for additional research to comprehensively address the risks associated with certain medical practices."
    ],
    "points": 404,
    "commentCount": 152,
    "retryCount": 0,
    "time": 1706564398
  },
  {
    "id": 39178521,
    "title": "Building and Installing Oxide Helios: A High-Performing Distribution of Illumos",
    "originLink": "https://github.com/oxidecomputer/helios",
    "originBody": "Oxide Helios Helios is a distribution of illumos powering the Oxide Rack. The full distribution is built from several consolidations of software, driven from tools and documentation in this top-level repository. Consolidation Public? Description boot-image-tools ✅ Yes Tool for assembling boot images for Oxide hardware garbage-compactor ✅ Yes Build scripts for packages beyond the core OS helios-omicron-brand ✅ Yes Zone brand for Omicron components helios-omnios-build ✅ Yes Build scripts for packages beyond the core OS helios-omnios-extra ✅ Yes Build scripts for packages beyond the core OS illumos-gate (stlouis branch) ✅ Yes Core operating system (kernel, libc, etc) phbl ✅ Yes Pico Host Boot Loader pinprick ✅ Yes ROM image compression utility illumos/image-builder ✅ Yes Tool for building bootable illumos disk images amd-firmware ❌ No AMD CPU firmware binary blobs (will be available in future) amd-host-image-builder ❌ No ROM image construction tools for AMD CPUs (will be available in future) chelsio-t6-roms ❌ No Chelsio T6 network interface card firmware blobs (will be available in future) pilot ❌ No A utility for low-level control of Oxide systems (will be available in future) NOTE: Not all consolidations are presently available to the public. We're working on this, but for now you can set OXIDE_STAFF=no in your environment when you run gmake setup to skip cloning and building software that is not yet available. Getting started NOTE: These instructions are for building your own operating system packages and installing them. If you're just trying to use Helios, you probably do not need to do this. See helios-engvm for information about pre-built Helios software. The best way to get started is to be using a physical or virtual build machine running an up-to-date installation of Helios. There are some details on getting a virtual machine installed in the helios-engvm repository. There are also some details there about install media that you can use on a physical x86 system. Prerequisites If you used the instructions from helios-engvm to create a virtual machine, you should already have all of the packages needed. If you used one of the ISO installers to set up a physical machine, or some other way of getting a Helios environment, you may need to install the pkg:/developer/illumos-tools package. You can check if you have this installed already with: $ pkg list developer/illumos-tools NAME (PUBLISHER) VERSION IFO developer/illumos-tools 11-2.0 im-i If missing from your system, it can be installed with pkg install. It's also a good idea to be running the latest Helios packages if you can. You can update your system with: # pkg update Pay careful attention to the instructions printed at the end of every update. You may be told that a boot environment was created and that you need to reboot to activate it. You should do that with the reboot command before moving on. Install Rust and Cargo using Rustup Official Rust and Cargo binaries are available from the Rust project via the same rustup tool that works on other systems. Use the official instructions, but substitute bash anywhere you see sh; e.g., at the time of writing, the (modified) official install instructions are: $ curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rsbash Clone the repository and build the tools On your Helios machine, clone this repository and run the setup step: $ git clone https://github.com/oxidecomputer/helios.git Cloning into 'helios'... $ cd helios $ gmake setup cd tools/helios-build && cargo build --quiet ... Cloning into '/home/user/helios/projects/illumos'... ... Setup complete! ./helios-build is now available. NOTE: If you do not have access to private repositories in the oxidecomputer GitHub organisation, you can request that the setup step only use the public repositories; e.g., $ OXIDE_STAFF=no gmake setup The Rust-based helios-build tool will be built and several repositories will then be cloned under projects/. Note that, at least for now, the tool takes a little while to build the first time. While the tool will initially clone the expected project repositories, subsequent manipulation (e.g., pulling updates, switching branches) is only performed for some repositories. You can see which repositories the setup step will update by looking at auto_update in the config/projects.toml file. You should otherwise expect to manage local clones as you would any other git repository; switching branches, pulling updates, etc. Building illumos The operating system components at the core of Helios come from the stlouis branch of illumos-gate. The packages that ship on Helios systems are mostly stock illumos with some additions for Oxide hardware and a few minor packaging transformations. To make it easier to build illumos, helios-build provides several wrappers that manage build configuration and invoke the illumos build tools. The upstream illumos documentation has a guide, Building illumos, which covers most of what the Helios tools are doing on your behalf if you are curious. Building during development While making changes to illumos, you can perform a \"quick\" build. This disables the shadow compilers and some of the checks that we would otherwise require for a final integration. $ ./helios-build build-illumos -q Dec 04 22:04:49.214 INFO file /home/user/helios/projects/illumos/illumos-quick.sh does not exist Dec 04 22:04:49.215 INFO writing /home/user/helios/projects/illumos/illumos-quick.sh ... Dec 04 22:04:49.215 INFO ok! Dec 04 22:04:49.216 INFO exec: [\"/sbin/sh\", \"-c\", \"cd /home/user/helios/projects/illumos && ./usr/src/tools/scripts/nightly /home/user/helios/projects/illumos/illumos-quick.sh\"] ... Depending on how many CPUs you have on your build machine, and the performance of your local storage, this can take some time. The full build log is quite large, and can be seen via, e.g., $ tail -F projects/illumos/log/nightly.log Once your build has completed successfully, there will be a package repository at projects/illumos/packages/i386. These packages can then be transformed and installed in various ways. Installing: locally on your build machine To install your newly built packages on the build machine: $ ./helios-build onu -t my-be-name Jan 29 09:33:44.603 INFO creating temporary repository... ... Jan 29 09:35:53.050 INFO O| beadm activate my-be-name Jan 29 09:35:53.911 INFO O| Activated successfully Jan 29 09:35:53.921 INFO onu complete! you must now reboot This will transform and install the illumos packages you just built and create a new Boot Environment with the name you pass with -t (e.g., my-be-name above). The new boot environment can be seen with beadm list, and has been activated by onu so that you can reboot into it. See beadm(8) for more information about boot environments. When rebooting, it is a good idea to be on the console so you can see any boot messages and interact with the boot loader. helios console login: root Password: Last login: Mon Jan 29 09:34:20 on console The illumos Project stlouis-0-g27e9202a98 January 2024 root@genesis:~# reboot updating /platform/i86pc/amd64/boot_archive (CPIO) syncing file systems... done rebooting... You can see that your updated packages are now running: $ pkg list -Hv system/kernel pkg://on-nightly/system/kernel@0.5.11-2.0.999999:20240129T090642Z i-- Critically, the system/kernel package shown here comes from the on-nightly publisher (your local files) and has a quick build version (2.0.999999). Installing: on another machine, using a package repository server If you have a build machine and a separate set of test machine(s), you may wish to use the package repository server (pkg.depotd) on your build machine. You can reconfigure the test system to prefer to install packages from your build machine over the network without needing to copy files around. First, transform the packages from your most recent build and start the package server: $ ./helios-build onu -D Jan 29 09:39:46.885 INFO creating temporary repository... Jan 29 09:39:46.886 INFO repository /home/user/helios/tmp/onu/repo.redist exists, removing first ... Jan 29 09:41:00.428 INFO starting pkg.depotd on packages at: \"/home/user/helios/tmp/onu/repo.redist\" Jan 29 09:41:00.428 INFO access log file is \"/home/user/helios/tmp/depot/log/access\" Jan 29 09:41:00.428 INFO listening on port 7891 Jan 29 09:41:00.428 INFO ^C to quit [29/Jan/2024:09:41:01] INDEX Search Available [29/Jan/2024:09:41:01] ENGINE Listening for SIGTERM. [29/Jan/2024:09:41:01] ENGINE Listening for SIGHUP. [29/Jan/2024:09:41:01] ENGINE Listening for SIGUSR1. [29/Jan/2024:09:41:01] ENGINE Bus STARTING [29/Jan/2024:09:41:01] ENGINE Serving on http://0.0.0.0:7891 [29/Jan/2024:09:41:01] ENGINE Bus STARTED The server is now running, and will remain running until you press Control-C or terminate it in some other way. You will need to know a DNS name or IP address (e.g., via ipadm show-addr) on which your build machine can be contacted. Now, on the target machine, confirm that you can contact the build machine: $ pkgrepo info -s http://genesis:7891 PUBLISHER PACKAGES STATUS UPDATED on-nightly 549 online 2024-01-29T09:40:50.716102Z Examine your existing package publisher configuration. On a stock Helios system, it should look like this: # pkg publisher PUBLISHER TYPE STATUS P LOCATION helios-dev origin online F https://pkg.oxide.computer/helios/2/dev/ Just one publisher is configured, using the central repository. We want to add a second publisher and make it the preferred source for packages. We also want to relax the \"sticky\" rule; i.e., that packages should only be updated from the publisher from which they were first installed. # pkg set-publisher -r -O http://genesis:7891 --search-first on-nightly # pkg set-publisher -r --non-sticky helios-dev # pkg publisher PUBLISHER TYPE STATUS P LOCATION on-nightly origin online F http://genesis:7891/ helios-dev (non-sticky) origin online F https://pkg.oxide.computer/helios/2/dev/ For now, depending on what you're doing on the test system, it may be necessary to uninstall the entire meta-package before proceeding. This is especially true if you have zones based on the lipkg brand. You can do this via pkg uninstall entire. The stock onu tool from illumos does this automatically. Perform a dry-run update to confirm that we are going to get updated packages from the quick build on your build machine: # pkg update -nv Packages to update: 325 Estimated space available: 20.67 GB Estimated space to be consumed: 564.95 MB Create boot environment: Yes Activate boot environment: Yes Create backup boot environment: No Rebuild boot archive: Yes Changed packages: helios-dev -> on-nightly SUNWcs 0.5.11-2.0.22430 -> 0.5.11-2.0.999999 SUNWcsd 0.5.11-2.0.22430 -> 0.5.11-2.0.999999 ... Note that the version is changing from a stock Helios version (which is the commit number on the master branch of illumos) to 2.0.999999, the quick build version. A new boot environment will be created, and a reboot will be required. Run the operation again without the -n flag to update: # pkg update -v ... DOWNLOAD PKGS FILES XFER (MB) SPEED Completed 325/325 5311/5311 107.1/107.1 4.9M/s PHASEITEMS Removing old actions 1213/1213 Installing new actions 892/892 Updating modified actions 5921/5921 Updating package state database Done Updating package cache 325/325 Updating image state Done Creating fast lookup database Done Reading search index Done Building new search index 582/582 Updating package cache 2/2 A clone of helios exists and has been updated and activated. On the next boot the Boot Environment helios-1 will be mounted on '/'. Reboot when ready to switch to this updated BE. *** Reboot required *** New BE: helios-1 Updating package cache 2/2 Assuming the update was successful, you should be able to reboot into your update software! # reboot updating /platform/i86pc/amd64/boot_archive (CPIO) After reboot, note that the publisher configuration is persistent: Loading unix... Loading /platform/i86pc/amd64/boot_archive... Loading /platform/i86pc/amd64/boot_archive.hash... Booting... Oxide Helios Version stlouis-0-g27e9202a98 64-bit (onu) Hostname: helios helios console login: root Password: The illumos Project stlouis-0-g27e9202a98 January 2024 # uname -v stlouis-0-g27e9202a98 # pkg publisher PUBLISHER TYPE STATUS P LOCATION on-nightly origin online F http://genesis:7891/ helios-dev (non-sticky) origin online F https://pkg.oxide.computer/helios/2/dev/ In future, you should be able to do a new build, restart the package server, and then pkg update -v again on the test machine. Installing: producing packages without installing them If you just want to transform the packages from a quick build without installing them, you can do so with the -P flag: $ ./helios-build onu -P Jan 29 09:45:36.040 INFO creating temporary repository... Jan 29 09:45:36.040 INFO repository /home/user/helios/tmp/onu/repo.redist exists, removing first ... Jan 29 09:46:14.901 INFO O| Republish: pkg:/text/locale@0.5.11,5.11-2.0.999999:20240129T090648Z ... Done Jan 29 09:46:15.602 INFO exec: [\"/usr/bin/pkgrepo\", \"refresh\", \"-s\", \"/home/user/helios/tmp/onu/repo.redist\"], pwd: None Jan 29 09:46:15.907 INFO O| Initiating repository refresh. Jan 29 09:46:24.978 INFO transformed packages available for onu at: \"/home/user/helios/tmp/onu/repo.redist\" This may be useful if you just want to inspect the contents of the built repository; e.g., $ pkgrepo info -s tmp/onu/repo.redist PUBLISHER PACKAGES STATUS UPDATED on-nightly 549 online 2024-01-29T09:46:15.448096Z $ pkgrepo list -s tmp/onu/repo.redist PUBLISHER NAME O VERSION on-nightly SUNWcs 0.5.11-2.0.999999:20240129T090617Z on-nightly SUNWcsd 0.5.11-2.0.999999:20240129T090618Z on-nightly audio/audio-utilities 0.5.11-2.0.999999:20240129T090618Z on-nightly benchmark/filebench o 0.5.11-2.0.999999:20240129T090618Z ... $ pkg contents -t file -s tmp/onu/repo.redist '*microcode*' PATH platform/i86pc/ucode/AuthenticAMD/1020-00 platform/i86pc/ucode/AuthenticAMD/1022-00 platform/i86pc/ucode/AuthenticAMD/1041-00 platform/i86pc/ucode/AuthenticAMD/1043-00 platform/i86pc/ucode/AuthenticAMD/1062-00 platform/i86pc/ucode/AuthenticAMD/1080-00 platform/i86pc/ucode/AuthenticAMD/1081-00 platform/i86pc/ucode/AuthenticAMD/10A0-00 platform/i86pc/ucode/AuthenticAMD/2031-00 ... You can also preserve the package files for later analysis such as the comparison of the output of multiple builds, or transport them to remote systems for installation. Making changes When making changes to the system it is generally best to start with a pristine built workspace, as you would have left from the quick build in the previous section. Once your build has completed, you may wish to make a change to a particular source file and rebuild a component. There are many components in the illumos repository, but we can choose a simple one as an example here. To build a particular component, we must first use bldenv to enter the build environment: $ ./helios-build bldenv -q Jan 29 09:50:22.895 INFO file /home/user/helios/projects/illumos/illumos-quick.sh exists, with correct contents Jan 29 09:50:22.895 INFO ok! Build type is non-DEBUG RELEASE is VERSION is stlouis-0-g27e9202a98 RELEASE_DATE is January 2024 The top-level 'setup' target is available to build headers and tools. Using /bin/bash as shell. $ pwd /home/user/helios/projects/illumos/usr/src A new interactive shell has been started, with PATH and other variables set correctly, and you can now change to a component directory and build it: $ cd cmd/id $ dmake -S -m serial install ... This will build and install the updated id command into the proto area: $ ls -l $ROOT/usr/bin/id -r-xr-xr-x 1 user staff 17428 Jan 29 09:51 /home/user/helios/projects/illumos/proto/root_i386-nd/usr/bin/id This kind of targetted incremental edit-and-recompile is a good way to make changes with a short cycle time and have some expectation that they will compile. Once you have changes you want to test, there are various things you can do next. Option 1: Most correct and slowest You can always do a new built of the entire OS. This is the only process that is (as much as anything can be) guaranteed to produce correct results. If, while doing something more incremental, you are experiencing an issue you cannot explain, a full build is always a good thing to try first. $ ./helios-build build-illumos -q This will rebuild all of illumos and produce packages you can then install in the usual way, as described in previous sections. Option 2: No guarantees but faster If you have updated some of the binaries in the proto area (e.g., by running dmake install in a kernel module or a command directory) you may just be able to regenerate the packages and install them without doing a full build. Within bldenv, regenerate the packages: $ cd $SRC/pkg $ dmake install ... Publishing system-zones-internal to redist repository Publishing system-test-zfstest to redist repository Initiating repository refresh. Once you have updated packages you can use them to start a package repository server or install locally, as described in the previous sections. Option 3: It's your computer At the end of the day, the operating system is just files in a file system. The packaging tools and other abstractions often create a kind of mystique which separates the engineer from this concrete reality -- but you are an adult and it is your computer! Other things you can do include: Just running the modified binary on the build system, or using scp or rsync to copy it to the test system and run it there. Sometimes this works! If the binary requires changes to libraries or the kernel, it might not work. Creating a new boot environment and adjusting the files in it. Boot environments are separate ZFS file systems that can be modified, snapshotted, cloned, and booted. They can be created with beadm create and mounted for modification with beadm mount. The boot loader allows you to select a different boot environment, and you can activate a specific boot environment permanently or just for one boot using beadm activate. See the beadm(1M) manual page for more information. Creating a wholly new disk image or ramdisk and booting that in a virtual machine or via PXE. There are some Helios-specific tools for creating images that can be made to include packages from a quick build, or even just arbitrary additional files, by modifying image templates. These tools are in turn based on the upstream illumos/image-builder. If you want advice on how to do something not completely explained here, or just to streamline your workflow, please don't hesitate to reach out! OS Image Archives As part of building OS images for Gimlets, an image archive is produced that includes the boot ROM and the root file system ramdisk image. It also includes some metadata in a JSON file, using the same format as the omicron1 brand (see IMAGE ARCHIVES in omicron1(7)). The contents of the file represents a committed interface between Helios and the parts of Omicron which need to download and install OS images on physical systems in the Oxide rack. The relevant contents for Omicron usage will always include at least: Filename Description oxide.json Metadata header file, with at least a v=1 key and a t=os key to identify it as an OS image. image/rom The host boot ROM image. (32MiB) image/zfs.img The host root file system ramdisk image. (arbitrary size) In addition to the committed files listed above, some additional files may be present for engineering or diagnostic purposes; e.g., a unix.z compressed kernel, and a cpio.z compressed boot archive, for use with nanobl-rs; or an array of extra ROM files with suffixes that represent different diagnostic capabilities. Additional files are not committed and may change at any time in the future. Software that interprets image archives should ignore any unrecognised files. Licence Copyright 2024 Oxide Computer Company Unless otherwise noted, all components are licenced under the Mozilla Public License Version 2.0.",
    "commentLink": "https://news.ycombinator.com/item?id=39178521",
    "commentBody": "Helios: A distribution of Illumos powering the Oxide Rack (github.com/oxidecomputer)388 points by eduction 17 hours agohidepastfavorite221 comments milon 14 hours agoI'm glad this is out, i'm going to deploy this locally and learn as much about it as possible. Oxide is pretty much the company I dream to work at, both for the tech stack, plus the people working there. Thank you Oxide team! reply EvanAnderson 14 hours agoparentI'm excited to see how this compares to SmartOS. I'm pretty heavily invested in SmartOS in my personal infrastructure but its future, post-Joyent acquisition, has been worrying me. I really wish I did work for an org big enough to use Oxide's gear. Not having to futz around with bogus IBM PC AT-type compatibility edifice, janky BMCs and iDRACs, hardware RAID controllers, etc, would be so unbelievably nice. reply nwilkens 13 hours agorootparentSmartOS is being actively developed since the aquisition from Joyent[1] in April 2022. We've released a new version every two weeks post acquisition, and are continuing to develop and invest. We also hold office hours events roughly every two weeks on Discord[2], and would love for you to stop by and ask any questions, or just listen along! [1]: https://www.tritondatacenter.com/blog/a-new-chapter-begins-f... [2]: https://discord.gg/v4NwA3Hqay reply rjzzleep 10 hours agorootparentIllumOS needs to attract new developers. To do that, the platform build needs to become a lot more straightforward. It's a pretty huge endeavour in my opinion. I'd be happy to help out on that regard, but in the past Joyent has not been very open to outside support. reply jclulow 36 minutes agorootparentI've noticed you make this comment repeatedly when illumos is mentioned on HN. I think you're underestimating the irreducible complexity of the build process for what is essentially a whole UNIX operating system, save for a few external dependencies. It's not just a kernel, but an extensive set of user mode libraries and executables. The build is complex in part because it's a complex body of software. I also think you're overestimating the extent to which make(1S) is the reason we're not more popular than Linux. There are any number of more relevant factors that make someone choose one operating system or another. Also, certainly for me personally my goal is not world domination, merely the sustainable maintenance of a body of software that helps me solve the problems that I work on, and which I enjoy using and developing as a result. I agree we need (as do all projects!) new developers, both now, and over the long term. We work as we can to make improvements to the build process, and the documentation. We are a relatively niche project, but we do attract new developers from time to time, and we're making changes at least as rapidly as we ever have in the past. There are a number of actively maintained illumos distributions (OmniOS, SmartOS, Tribblix, OpenIndiana, and now Helios) and there are a variety of commercial interests that ship more proprietary appliances on top of an illumos base. For our part at Oxide we continue to encourage our staff to get involved with illumos development as it makes sense for them, and we try to offer resources and assistance to the broader community as well. If you would like to contribute, we have a guide to getting started: https://illumos.org/docs/contributing/ Please, though, it's \"illumos\", not \"IllumOS\"! reply rjzzleep 7 minutes agorootparentI do, yes, but your comment makes it clear that this is a problem that you either don't really think of as a problem, or that you don't know how to address. Building open source communities is hard work. Telling everyone how amazing your product is(even if it is), is only a small part of it. The lesson to take away from your time at Joyent should be that, that way of community building didn't work, and there needs to be some change. Even in the early 2000s linux had a make menuconfig or make xconfig setting to build linux. And yes this is different, it's a posix distribution. Yocto was a relatively niche project as well and it also addresses the issue of building a collecting of posix applications into a big project, so does gentoo's stage. I'm sure that at the time of it's creation OpenSolaris was ahead of its curve, but that's how many years ago? You know as well as I do that sprinkling LD_LIBRARY_PATHs here and there and then removing undocumented dot files here and there isn't really a sane way to handle such a build process for a curious third party. Most will probably drop it before it gets to that point. There have been many many projects that have reworked their entire build architecture, some of which took years to flesh out fully. What needs to happen for illumos to get a boost of development in the long term is: 1. first for you to acknowledge on a political level that there is an issue that needs to be addressed here, and 2. to then work with the community, and it doesn't have to be across the board, but you need to be willing to invest in some experts and some people interested in solving this, so they can grind out something that is more sane in this current world. \"Read our getting started guide\" isn't really all that useful, when most of the complex issues happen after that and are often met with \"this isn't how we do things\". reply _rs 13 hours agorootparentprevI had been using SmartOS for a long time but finally had to bite the bullet and give up. I ended up deciding on Proxmox on a ZFS root and am quite happy with it. reply icybox 12 hours agorootparentI've been running smartos at least since 2015 where I co-located my server. There have been times where I felt like giving up, but people like danmcd, jperkin and others always stepped in and fixed what needed to be fixed for LX to be usable and working. (Keeping java updated and running is hard, uphill battle. Thanks!) I always ran a mixture of OS and LX zones and bcantrill's t-shirt with \"Save the whales, kill your VM\" made sense. I've used zones in Solaris 10 even before and they just click with me. FreeBSD's jails are nice, but far from it. And linux's cgroups are a joke. And using KVM/VMs for security containerization is just insane. At dayjob, I've implemented multiple proxmox clusters, because we're linux shop and there's no way to \"sell\" smartos or tritonDC to die-hard debian colleagues, but I've managed to sell them ZFS. With personal stuff, I like my systems to take care of themselves without constant babysitting and SmartOS or OpenBSD provide just that. I don't dislike windows, I love UNIX. You could really feel those extra 20y UNIX had compared to linux. I migrated all my stuff to proxmox for like 2 months. And then went back to SmartOS, because there was something missing ... probably elegance, sanity, simplicity or even something you'd call \"hack value\". reply unethical_ban 4 hours agorootparentAnd here I am, having compared the SmartOS documentation and ease of installation to Proxmox... and with very few complaints am using Proxmox to host a file server on bare metal/Samba container and a OPNSense on VM. I remember buying the OpenSolaris Bible in 2008, getting really excited to dig into my second Unix (after FreeBSD). And then, the Sun went down on me... and I stuck with Ubuntu 10 years. reply geek_at 13 hours agorootparentprevthe nice thing about the Proxmox + ZFS setup is that it works and is even recommended without using hardware raid controllers. Less headaches either way. I recently wrote a guide [1] how to use proxmox with ZFS over iSCSI so you can use the snapshot features from a SAN [1] https://blog.haschek.at/2023/zfs-over-iscsi-in-proxmox.html reply rjzzleep 12 hours agorootparentprevI feel the same. I used a SmartOS distro called Danube Cloud for a long time and am looking to move and looked at Harvester[1] and OpenNebula, but with everything I know about Kubernetes(and LongHorn) I'm reluctant to use something so heavily based on Kubernetes. At its peak I reached out multiple times to Joyent to fix their EFI support for virtualization. The Danube team had similar experiences with them, working on live migrations for VMs, and a few months back I did a rebase of the platform image to a more recent illumos stack. Two of the fundamental issues with Illumos is that they don't seem to understand that they need to fix the horrendous platform build to get community support to keep up with the pace of development of other OS's. The platform build is a huge nasty mess of custom shell scripts, file based status snapshots, which includes the entire userspace in the kernel build. Basically if your openssl version is out of wack the entire thing will fail. Not because it has to, but because it was never adapted to modern needs of someone just wanting to hack on a kernel. It's fixable, but I don't see any desire to fix it, and even if that desire eventually shows up it might just be too little, too late. [1] https://harvesterhci.io/ reply DominoTree 6 hours agoparentprev>> Oxide is pretty much the company I dream to work at, both for the tech stack, plus the people working there. Thought I was the only one :P reply yjftsjthsd-h 6 hours agorootparentI mean, I phrase it as \"my dream job is systems integration at Sun\" (but Oxide is the living equivalent) reply refulgentis 14 hours agoparentprevCan you get me excited? I spent 20 seconds browsing the homepage and walked away with \"so the idea is vertical integration for on-premise server purchases? On custom OS? Why? Why would people pay a premium?\" But immediately got myself to \"what does a server OS do anyway, doesn't it just launch VMs? You don't need Linux, just the ability to launch Linux VMs\" Tell me more? :) reply mustache_kimono 14 hours agorootparent> so the idea is vertical integration for on-premise server purchases? On custom OS? Why? Why would people pay a premium? As I understand it, re: vertical integration, the term is actually \"hyperconverged\". Here, that means it's designed at the level of the rack. Like -- there aren't per compute unit redundant power supplies. There is one DC bus bar conversion for the rack. There is an integrated switch designed by Oxide. There is one company to blame when anything inside the box isn't working. In addition, the pitch is they're using open source Rust-based firmware for many of the core components (the base board management controller/service processor, and root of trust), and the box presents a cloud like API to provision. If the problem is: I'm running lots of VMs in the cloud. I'm used to the cloud. I like the way the cloud works, but I need an on-prem cloud, this makes that much easier than other DIY ways to achieve (OMG we need a team of people to build us a cloud...). reply steveklabnik 14 hours agorootparentThe terminology in this space is confusing, but \"hyperconverged\" isn't really what we're doing. I wrote about the differences here: https://news.ycombinator.com/item?id=30688865 (That said I think other than saying \"hyperconverged\" your broad points are correct.) reply SteveNuts 14 hours agorootparentprevIt seems like the folks on HN tend to think the world runs on AWS (I'm not trying to say they don't have a huge market share), but many huge enterprises still run their own datacenters and buy ungodly amounts of hardware. The products that are on the market for an AWS-like experience on-prem are still fairly horrible. A lot of times the solutions are collaborations between vendors, which makes support a huge pain (finger pointing between companies). Or, a particular vendor might only have compute and storage, but no offering for SDN and vice-versa. This sucks because then you have two bespoke things to manage and hope they work together correct. These companies want a full AWS experience in their datacenter, and so far this looks to be the most promising without dedicating huge amounts of resources to something like Openstack. reply lijok 14 hours agorootparentWouldn't a \"full AWS experience in their datacenter\" be AWS Outpost? reply mustache_kimono 14 hours agorootparent> \"full AWS experience in their datacenter\" ... Including the bill! reply mardifoufs 14 hours agorootparentprevIs AWS outpost truly a full AWS stack/experience? I thought it wasn't actually meant to be a \"data center in a box\" experience, but more so a way to run some workloads locally when you are already using AWS for everything else. reply PeterCorless 10 hours agorootparentSome data products will run successfully in AWS Outposts. Others will not. For example, AWS itself can't run DynamoDB in an AWS Outpost. It recommends users to run ScyllaDB in DynamoDB compatible mode. e.g., https://www.scylladb.com/2020/09/15/scylla-cloud-on-aws-outp... Disclosure: I worked at ScyllaDB. reply adfm 14 hours agorootparentprevWith DHH and others promoting a post-SaaS approach (once.com, etc.) we might see hardware refresh as cost-cutting. Astronomical compute bills and lack of granularity bring all things cloudy into sharp focus. reply threeseed 9 hours agorootparentWhat they are doing is SaaS by stealth. You buy their product once, but it only has bug and security fixes for 3 years. Which means every business is going to need to upgrade on a cycle anyway. reply mlindner 8 hours agorootparentPeople don't usually throw out their server hardware after 3 years. After 3 years is up they'll probably sell service plans. And with the code being all open source some owners may go the self-supported route, though probably most will buy service plans. reply threeseed 8 hours agorootparentWas actually referring to DHH and the 37signals products. Whilst I think we will see a trend back towards more on-premise hardware I don't think SaaS is going away anytime soon. And in fact it's arguably better for everyone because the software is being continually maintained. reply refulgentis 12 hours agorootparentprevThe \"(finger pointing between companies)\" took me from confusion to 100% understanding, was at Google until recently. It was astonishing to me that it was universally acceptable to fingerpoint if it was outside your immediate group of ~80 people.* Took me from \"why would people go with this over Dell?\" to \"holy shit, I'm expecting Dell to do software and make nvidia/red hat/etc/etc etc/etc etc etc help out. lol!\" * also, how destructive it is. never, ever, ever let ppl talk shit about other ppl. There's a difference between \"ugh, honestly, it seems like they're focused on release 11.0 this year\" and \"ughh they're usless idk what they're thinking??? stupid product anyway\" and for whatever reason, B made you normal, A made you a tryhard pedant reply _zoltan_ 14 hours agorootparentprevOpenStack is pretty smooth sailing these days and I bet you it would be much cheaper to just get 3 FTEs for your OpenStack install than an Oxide rack reply linksnapzz 13 hours agorootparentWhere, exactly, are you getting these 3FTEs qualified to touch production OpenStack infra, for more than a year, where their aggregate cost is less than a rack of equipment? reply _zoltan_ 11 hours agorootparentif you need OpenStack you're not running one rack, but a couple dozen. reply KAMSPioneer 2 hours agorootparentThat...sounds like a market segment you've just discovered for Oxide. reply gtirloni 13 hours agorootparentprevThe rack doesn't require FTEs? reply linksnapzz 12 hours agorootparentNot three of them; it ought to be about as difficult to administer as a single rack of hw, +Vsphere, if that. reply milon 14 hours agorootparentprevHaving a solid on-prem rack product to me is a great thing. I like IaaS services a lot, don't get me wrong, and I think they're the right pick for a bunch of cases, but on-prem servers also have their \"place in the sun\", so to speak :) I could present any number of justifications that I don't think I'm qualified enough to defend, but the gist is that at the bare minimum, I'm glad the option exists. As to why I'm personally excited: I enjoy the amount of control having such an on-prem rack would afford me, and there surely could be a great amount of cost-savings and energy-savings in many scenarios. Sometimes, you just need a rack to deploy services for your local business. I like the prospect of decentralizing infrastructure, applying all the things we've learned with IaaSes. reply bionsystem 3 hours agorootparentIn the last 10 years and 6 different clients/employers I worked there is pretty much no way to run production on the cloud. Only 1 of them had some stuff running in the (GCP) cloud at all. Of all of the 6 infrastructures I've seen, only 1 of them is half decent, with 6 dedicated teams around the datacenter working closely together (by dedicated I mean, nothing is required of them concerning the core software product that the company develops). Network, Unix/Virtu, Windows, Storage, PC, and datacenter. That's 30+ people just to run a couple big datacenters and a few more server rooms. The service was actually quite good with VMs/zones delivered under an hour and most tech issues solved in half a day. The other infrastructures were either bigger or smaller, with more or less people, and were all terrible, sometimes needing weeks of email exchanges with excel attached to get a single VM. AWS was the dream everywhere I went for everybody. Oxide may be coming out with a product that will solve a LOT of issues. SmartOS/IllumOS has all the tech to be self-sufficient (virtualization, storage, SDN...), add support for networking and storage and you get a complete product that a handful of people can run (well, you still need a windows team in most cases but fine). reply lijok 14 hours agorootparentprev> Why would people pay a premium? I would pay a premium just to not have to deal with HPE, DELL, etc reply _zoltan_ 14 hours agorootparentDell's been nothing but fantastic for us (compute, not storage.) reply sarlalian 11 hours agorootparentDell is a mixed bag depending on how well the individual region you are dealing with is doing overall. Things were great for us, but something changed and now getting good support for hardware failures has been a nightmare of jumping through hoops, time zone handoffs to other teams, and forced on-site techs to replace a stick of ram. reply throwup238 14 hours agorootparentprevThe best elevator pitch I've heard is \"AWS APIs for on-prem datacenters\". They make turn-key managed racks that behave just like a commercial cloud would with all the APIs for VM, storage, and network provisioning and integration you'd expect from AWS, except made to deploy in your company's datacenter under your control. reply magnawave 13 hours agorootparentI guess the wildcard is price. AWS's pricing model works kinda at their OMG eyewatering scale - aka all the custom hardware they design is highly cost optimized, but just doing custom hardware has a notable cost. This is easily covered by their scale, to make for their famous margins. [during their low scale times, they did use a good bit of HP/Dell, etc] Oxide seems to be no different (super custom hardware) only major difference being the \"in your datacenter\" part. Since you own the cost of your datacenter, Oxide has to come in a lot cheaper to even compete with AWS, but how do you do that with low volume [and from the look of it not-cost optimized, but instead fairly tank-like] bespoke hardware? Feels like the pricing / customer fundamentals are going to be pretty rough here outside perhaps a few verticals. reply kaliszad 10 hours agorootparentOxide seems to be a lot more efficient than a rack full of 1U servers with each having 2 PSUs + 2 ToR switches + 1 management switch somewhere for all the OOBMs. All those little fans and power conversions eat a lot of power, the fans and the PSUs all cost something too. Also, have fun managing all of that in a secure manner or debugging anything at all. Once you add the VMware licensing you might end up with more or less the same cost up front and quite likely higher overall cost. And I am not even beginning to talk about racking/ stacking of the whole rack. I haven't seen much support even when Dell/EMC owned VMware and together produced the VXRail lineup and the company I used to work for was presented as the reference project in Saxony, Germany at that time. All of the boxes would add up to about 2 standard racks but it was representative of the other bigish customers in that area and time. I imagine, some of the customers will order 1-2 racks half full and over a few years possibly add a few sleds, these will probably demand great GUI/ manual experience and possibly competitive Oracle/ SAP/ MSSQL benchmarks and I can imagine Veeam integreation. Other customers such as the DoE or some big enterprise customers will order whole rows of racks and demand perfect automation options. That is just a guess. reply sarlalian 11 hours agorootparentprevDatacenter costs are weird. The first big cost is having a datacenter. However once you have the space, power, cooling and that part makes sense, then the actual hardware going into it can have a pretty decent premium and still be highly competitive with AWS. It will also depend heavily on what you are doing and producing, if the answer to that is a large amount of data, and it needs to transit out of AWS, suddenly the cost of a pretty large datacenter is really cheap in comparison. AWS egress fees have a markup that will make your accountants panic. From a hardware standpoint, once you need GPU compute or large amounts of RAM, the prices get pretty dumb as well. reply kortilla 14 hours agorootparentprevThat’s the elevator pitch for open stack reply steveklabnik 12 hours agorootparentYou are not wrong that OpenStack is sort of similar in a sense, but the difference is that Oxide is a hardware + software product, and OpenStack is purely software. reply capitol_ 14 hours agorootparentprevThat just sounds like a bunch of api's on top of linux. reply throwup238 14 hours agorootparentJust like Dropbox is a bunch of APIs on top of FTP. reply adamnemecek 14 hours agorootparentprevOne company making both HW and SW generally leads to really good, integrated experiences. See e.g. Apple. reply 0cf8612b2e1e 13 hours agorootparentI am really hoping the broader industry takes note. By owning the platform, the Oxide team was able to dump legacy stuff that no longer makes sense. reply throwawaaarrgh 14 hours agorootparentprevIt's a mainframe. If you can't get excited for mainframes it'll be hard to be excited about this. IllumOS is the OS/360 to Oxide's System/360. (It won't get that popular but it's a fair enough comparison for illustrative purposes) reply linksnapzz 13 hours agorootparentIt's a mainframe, for people who do not, actually, know what a mainframe is or does. reply kaliszad 10 hours agorootparentI bet it costs a fraction of what a similarly powerful mainframe would cost. However I don't think the customers for each overlap that much. If you need a mainframe, you need one and there is no discussion about possible alternatives because there are none. reply panick21_ 13 hours agorootparentprevExcept that it use the same standard CPU as commodity machine. Doesn't have much of the extra reliability stuff. It can go from vertical to horizontal scaling. The OS is open source Unix. And yeah its not like a mainframe at all really. reply codethief 13 hours agoprevCan anyone ELI5 what Oxide's offer is? I've looked at their website and still got no clue. Is it hardware + software I can purchase and use on-premise? Is it a PaaS / yet another cloud provider? reply steveklabnik 13 hours agoparentI believe you're being downvoted because there is already a big thread about this here, though I think that's a bit unfair to you. I haven't posted in that thread yet because I wanted to let others say what is meaningful about the product to them, but this seems like a good place to put my reply. Regardless of all that: it is hardware + software you can purchase and use on-premise, that's correct. The differentiator from virtually all existing on-prem cloud products is that we are a single vendor who has designed the hardware and software (which is as open source as we can possibly make it, by the way, hence announcements like this) to work well together. Most products combine various other products from various vendors, and are effectively selling you integration. We believe that that leads to all kinds of problems that our product solves. Another factor here is that we only have two SKUs: a half rack and a full rack. You don't buy Oxide 1U at a time, you buy it a rack at a time. By designing the entire rack as a cohesive unit, we can do a lot of things that you simply cannot do in the 1U form factor. There is a running joke that we talk about our fans all the time, and it's true. Because our sleds have a larger form factor than a traditional 1U, we can use larger fans. This means we can run them at a lower RPM, which means power savings. That's the deliberate design choice. But we also have gained accidental benefits from doing things like this: lower RPM also means that our servers are way quieter than others. That's pretty neat. Some early prospective customers literally asked if the thing is on when it was demo'd to them, because it's so quiet. Is that a reason to buy a server? Not necessarily, but it's just a fun example of some of the things that end up happening when you re-think a product as a whole, rather than as an integration exercise. reply codethief 6 hours agorootparentThanks so much for elaborating! reply chrishare 13 hours agoparentprevOn-prem, fully-integrated compute and storage solution with cloud-like APIs to provision resources, all with a commitment to open source. reply haolez 9 hours agorootparentDo you know if they support GPUs or whatever is needed to host LLM models? reply steveklabnik 9 hours agorootparentThe current product does not have any GPUs in it. https://news.ycombinator.com/item?id=39183072 reply mkoubaa 12 hours agorootparentprevMainframe 2.0 reply danpalmer 9 hours agorootparentThis is really not accurate in any way that matters I don't think. It's a mainframe in as much as you buy a rack and spec it out. It's not a mainframe in that the performance is typical server performance rather than the mainframe profile which is very different and requires different considerations, the compute model is typical server compute rather than the mainframe compute model which (aside from compatibility layers) is a radically different environment to build software for. reply yjftsjthsd-h 8 hours agoprevSweet:) And a big thanks for writing what appears to be clear and straightforward documentation; IMO that's an area that the illumos community has historically struggled with. And seeing a new source release talking about consolidations gives me the warm fuzzies, even if this does seem to depart from the traditional gate paradigm unless I'm seriously misreading the repo organization here. Some (mostly tooling) questions: - Why gmake? Especially since dmake is needed later anyways? - Instructions say run rustup with bash explicitly; is that a defect in upstream, or is the local sh not completely posix compatible? - How is this developed internally? Do Oxide folks run illlmos workstations or is this all developed in Virtual machines or SSHed to servers? - Why MPL? GPL compatibility? reply steveklabnik 7 hours agoparentI can't answer all your questions, because I don't actually work on helios, but I do have an answer to some of them: > Do Oxide folks run illlmos workstations or is this all developed in Virtual machines or SSHed to servers? I wrote about this topic here: https://news.ycombinator.com/item?id=39181727 That said, some folks certainly run illumos on a workstation. > Why MPL? GPL compatibility? On MPL: https://news.ycombinator.com/item?id=39181844 That said in that comment I didn't really speak to the \"why.\" We feel like it's a good compromise in the possibility space: more copyleft than BSD, but also less restrictive than the GPL. reply criddell 13 hours agoprevI’m unfamiliar with illumos so I went to their webpage and the very first thing it says is: > illumos is a Unix operating system Is illumos an actual Unix (like macOS) or a Unix-like OS (like GNU/Linux)? reply steveklabnik 12 hours agoparentActual Unix. Wikipedia is pretty good: https://en.wikipedia.org/wiki/Illumos > It is based on OpenSolaris, which was based on System V Release 4 (SVR4) and the Berkeley Software Distribution (BSD). Illumos comprises a kernel, device drivers, system libraries, and utility software for system administration. This core is now the base for many different open-sourced Illumos distributions, in a similar way in which the Linux kernel is used in different Linux distributions. reply Thoreandan 12 hours agoparentprevNobody's paid to have it pass Open Group Unix Branding certification tests https://www.opengroup.org/openbrand/register/ so it can't use the UNIX™ trade mark. But it's got the AT&T Unix kernel & userland sources contained in it. PDP-11 Unix System III: https://www.tuhs.org/cgi-bin/utree.pl?file=SysIII/usr/src/ut... IllumOS: https://github.com/illumos/illumos-gate/blob/b8169dedfa435c0... reply BirAdam 8 hours agoparentprevIt was an open source branch of Solaris that Ian Murdock worked on while he was at Sun under the name Project Indiana. It descends from UNIX SVR4. reply msla 6 hours agoparentprevLegally, NetBSD isn't actually Unix. The brand doesn't mean what people seem to think it means. reply yjftsjthsd-h 6 hours agorootparentRight, \"unix\" roughly means - Derived from Bell Labs unix source - Legally allowed to use the UNIX trademark (AKA certified Unix) - A unix-shaped OS (similar but not 100% the same as POSIX complacence) and those things are basically independent. Most GNU/Linux are unix-likes but not derived from original unix code or certified, but there's been 1-2 that did get certified. The BSDs are (now quite distantly ) derived from unix source but not certified (although ex. UnixWare is IIRC). Solaris was all 3 but OpenSolaris and now illumos are obviously unix-like and still based on the original code but not certified UNIX™. (Take all this with a grain of salt; I'm typing this all from memory and IANAL) reply GrilledChips 4 hours agorootparentprevThis isn't NetBSD. NetBSD broke off loooooooong after the release of BSD that Sun used to build this OS. reply chucky_z 13 hours agoparentprevActual Unix. I believe it is in the Solaris family. reply busterarm 14 hours agoprevNot that I'm not rooting for Oxide, but their product is still so niche and early stage that I can't imagine any actual businesses buying their stuff for a long time. They only just shipped their first rack to their first customer at the end of last summer and it's Idaho National Laboratory. State research institutions are basically the only entities positioned to gamble on this right now. reply steveklabnik 14 hours agoparentJust a small note, but from when we announced this back in October, two customers were mentioned: https://oxide.computer/blog/oxide-unveils-the-worlds-first-c... > Oxide customers include the Idaho National Laboratory as well as a global financial services organization. Additional installments at Fortune 1000 enterprises will be completed in the coming months. reply lijok 14 hours agoparentprevThis describes every single product in its early days in existence. If you're planning to launch any other way, you've doomed the company before you even launched. Lucky few survive, in spite of, and that's what contributes to the 9/10 startups statistic. Lazer focus on the first set of customers that will help you cross the chasm. Only then mass market. reply newsclues 12 hours agoparentprevI hope they sooner or later release a smaller, cheaper, homelab product for people to learn or for startups that will lead to future rack sales or workers. reply steveklabnik 12 hours agorootparentThis is a common request and we absolutely understand the desire, but I suspect such a thing, if ever, will be a long time off. Given that the product is designed as an entire rack, doing something like this would effectively be a different product for a different vertical, and we have to focus on our current business. Honestly it's kind of frustrating not being able to reciprocate the enthusiasm back in more than just words, but it is what it is. reply amluto 10 hours agorootparentFor what it’s worth, there’s a somewhat common view at least in the Linux community that it’s important for hardware vendors to make their tech stack targetable from the office or home. This isn’t to be polite or to make money — it’s to foster adoption among developers, which drives sales. Some examples: x86 owned the desktop, workstation and laptop world for a long time. So everyone targeted x86, which made x86 the default in the datacenter. It was hard for ARM to break in and it mostly happened when AWS did it by fiat. If ARM had made some loss-leader actually useful laptops and workstations available, it might have happened sooner. But x86 largely didn’t deploy AVX-512 in client machines, so people who wrote libraries only used it for fun or benchmarking, so it wasn’t widely used, and most users flubbed it anyway. (And might have gotten it right if they had the hardware on their desk.) People target Nvidia datacenter GPUs. But people have targeted them for a long time, because they have them in their gaming machines too. Xilinx used to push free academic gear quite hard, because that was a big lead into people learning how to use their gear. So, if I were giving Oxide straightforward sales advice, absolutely don’t get distracted with small systems. But maybe, if Oxide thought of it as lead generation, Oxide should do it anyway. If I could buy something small enough to be affordable but big enough to be useful [0], I might get one. And I’d target it with my own stuff, and fix bugs, and evangelize it at little cost to Oxide. [0] For me, maybe 100-150TB of spinning rust (or cheap NVMe or the ability to attach a JBOD), plus anywhere from 4-64 cores, in a format that works on 120V and fits in, say, 16U or less, at a credible price point, would be quite likely to net Oxide a sale. (Just one sale but still!) It could be sold as a developer thing, and there would be absolutely no expectation that it would perform like the real thing. If I found it awesome, I might buy a couple more. But I would also use it and make things work on it and talk about it, and if a whole bunch of people did this, Oxide might get a bunch of real sales. (Also, I get the idea behind two SKUs, but can buyers at least configure storage and compute separately? Different workloads need radically different ratios.) reply mbreese 8 hours agorootparent> This isn’t to be polite or to make money — it’s to foster adoption among developers, which drives sales. I get that in theory… and it makes sense most of the time. I’m not sure it does this time though. You don’t exactly “target” Oxide as an OS or platform. Rather, you use it to run VMs on. Those VMs are whatever you want. Other than that, I’m not sure what else having a home-lab version of Oxide would look like. A different competitor to Proxmox? reply amluto 6 hours agorootparentPeople target AWS and GCP and Azure, and they write actual code that interacts with them, do test deployments there, and do real deployments there. reply steveklabnik 10 hours agorootparentprevI certainly understand that strategy generally, see also Adobe giving Photoshop licenses to students back in the day so that they'd be familiar professionally. It's just that doing so amounts to building an entirely new product, and as a relatively young startup, focus is more important. We're going deep, not wide. Someday :) > (Also, I get the idea behind two SKUs, but can buyers at least configure storage and compute separately? Different workloads need radically different ratios.) Right now, this early: no. Sleds have compute and storage located together, so the unit of customization is currently \"number of sleds in the rack\" which according to https://oxide.computer/product/specifications apparently is currently three, not two, at the moment: 16, 24 or 32 sleds. You are right that these need to be different for certain customers and workloads, we just aren't ready to support those just yet. We'll get there. Same issue, different aspect. reply mattclarkdotnet 5 hours agorootparentSee also Cloud Foundry, where the late arrival of something devs could use on a laptop was probably key to its failure to capture the market for PaaS. reply amluto 6 hours agorootparentprevOn reading the specs, you’re using 2.91TiB U.2 drives. On the list of “oh my gosh too much engineering and too many stock keeping units,” allowing them to be swapped for the much larger U.2 devices one can buy now seems fairly easy. In case there’s a potential thermal issue, most NVMe drives I’ve checked have active power states that are a bit slower but have reduced power consumption. But Oxide is small and shouldn’t listen to me unless a customer asks for this. reply newsclues 9 hours agorootparentprevUntil then maybe sell a few to schools to provision for student access? A future of developers and CTOs who grew up with Oxide! reply steveklabnik 9 hours agorootparentMy friends and I had a small server set up in college, and those were some of the best times of those years. :D reply newsclues 12 hours agorootparentprevI appreciate the response, I totally understand and don’t expect it to materialize soon, but am still hopeful that someday it will be a possibility. reply whalesalad 8 hours agorootparentprevcan't wait to find liquidated oxide gear on ebay in 2035. all my current homelab gear is \"ancient\" enterprise gear like R720's etc reply faitswulff 5 hours agorootparentprevWe'll have to wait for it to hit Groupon. reply chologrande 13 hours agoparentprevI work at a recently IPO'd tech company. Oxide was a strong consideration for us when evaluating on prem. The pitch lands among folks who still think \"on prem.... ew\". Looks like a cloud like experience on your own hardware. If only it were as cheap as dell... reply busterarm 2 hours agorootparentAs did some elements of my own company, but business risks like those are not for fledgling public companies. To be honest, right now anyone in a _public_ company advocating for it at this stage of development should have all of their decision making power removed if not outright be shown the door. That goes double if it's your CTO...which is exactly what ended up happening with us. I'm not saying \"no, never\", but clearly \"no, not right now\". reply RandomChance 13 hours agoparentprevMy company looked at them, and we were very impressed with the product. The only issue was that they are built for general compute and we really needed the option for faster processors. reply elzbardico 13 hours agoparentprevLarge financial institutions surprisingly are good customers for new, still-untested computing technology. I would not get surprised if Oxide next customers were a few giant banks and funds. reply __d 13 hours agorootparentIn my experience, some financial institutions have a very good understanding of risk. They are able to identify, and most importantly, quantify risk in a way that many businesses cannot. Consequently, they're able to take risks with new hardware/software that other companies shy away from. reply technofiend 11 hours agoparentprevIt is somewhat niche, but Broadcom's purchase of VMWare now puts 0xide closer to Nutanix in that you can go buy a fully supported virtualization platform from a vendor who welcomes your business. I don't know the actual number, but it seems Broadcom is only interested in enterprise customers with huge annual spends. reply rmccue 10 hours agorootparentWith Broadcom’s plan for VMWare, Oxide certainly seems to have had excellent timing here. reply __float 14 hours agoparentprevWe have historically had private institutions with impactful research labs. Are there any of those still kicking? reply ahmedfromtunis 11 hours agoprevIt is great that the software is open-source, but would be you useful to be deployed on other hardware? And what would happen if, for whatever reason, a company can no longer purchase Oxide racks, will it need to start over its infra, or can it just build around Oxide hardware? reply steveklabnik 10 hours agoparentIt is not likely that it would be immediately useful outside of our hardware, but the main thing they're doing is deploying virtual machines. If they decided to no longer use the Oxide rack they have purchased, they would move their VMs to whatever infrastructure they choose to succeed it. reply mihaic 11 hours agoprevI'm really curious: what kind of workload would companies want to run on a custom Unix that isn't Linux/Mac/BSD? I'm rooting for more mature OS diversity, I just have no idea who the end users would be and what their needs would look like. reply rhinoceraptor 10 hours agoparentThe compute you'd provision on the Oxide rack are virtual machines, they've ported bhyve from FreeBSD and added live migration. I'm pretty sure you could even boot Windows Server on it if you were being held hostage. As for why they used Illumos, many of the people came from Sun, Joyent, etc. so there's an obvious bias. However they do have a compelling reason that this is not an IBM compatible x86 personal computer, there's no BIOS, no UEFI, no traditional BMC, as far as I can tell they've removed as much proprietary firmware and binary blobs as they could possibly remove, while still using modern x86. Each sled has a service processor and a hardware root of trust that directly boots the CPU, loads the AMD training blob, and boots the OS. It would be difficult to upstream the changes required to do that into a Linux or BSD for a computer only you currently have. So you'd have to maintain your own downstream fork, there is no one else responsible for the robustness of the OS, so it might as well be OS that you have had to support and develop for years. reply steveklabnik 10 hours agoparentprevThis is not a user-facing detail of the product. Customers run VMs on the rack, they do not build their applications for illumos. They're gonna run whatever operating system in those VMs that they need to accomplish their goals. reply tw04 7 hours agorootparentThat being said - there's something to be said for enterprise support. Are there plans to support importing/converting/running third party OVAs? Many vendors will support something running in KVM, I can't recall the last time I saw b-hyve as a supported hypervisor. I'd imagine as broadcom slowly destroys vmware's market share vendors will look to alternatives, but I doubt b-hyve is even a blip on their radar at this point. reply steveklabnik 7 hours agorootparentI don't know the status of supporting OVA as a file format, but we absolutely support creating and uploading your own images. Here are the current docs on how to do so: https://docs.oxide.computer/guides/creating-and-sharing-imag... reply epistasis 11 hours agoparentprevZFS is native on illumos, and the containerization equivalent, etc, is pretty great. There's a good argument that your servers in the cloud don't need to be on the same OS, as long as you can hire enough talent to work on them. reply GrilledChips 4 hours agoparentprevYou'd have no idea that it isn't Linux. You don't run code on this OS, you run code on VMs that it provides. reply lifeisstillgood 14 hours agoprevI would be interested in how did you first hear of Oxide. I somehow landed on their podcast because it covered . The podcast is for me amazeballs marketing - it does everything but sell their product (might be a good idea to add a pitch in for each out-tro!) I mean they talk about it, like “we had such a tough time getting the compiler to do something something and then veer off to discuss back in the day stories. Ah never mind. Keep talking guys hope it works out reply zengid 13 hours agoparentIf you listen to their original podcast 'On The Metal' it was infamous for it's overly repeated use of 2 or 3 pre-recorded self promotions, so much so that a fan recorded their own commercial for them to air. 'Oxide and Friends' however isn't really what I would consider a podcast, but a recording of live \"spaces\" or group calls, beginning on Twitter and now happening in Discord. IMO it's not really best consumed as a podcast, but rather to participate in live. If you tune in live you'll pick up on the vibe of the recordings a lot better. https://oxide.computer/podcasts/oxide-and-friends reply Hackbraten 13 hours agoparentprevWas following @jessfraz on Twitter back then, so I got word of Oxide when they first announced it there. reply __d 13 hours agorootparentBetween Jess, Bryan, and Adam, it was hard to miss :-) reply ahmedfromtunis 12 hours agoparentprevFor me, it was when Pentagram showcased their branding when Oxide was first announced. reply nubinetwork 14 hours agoprevI was hoping for this since they announced the server rack... nobody wants a paperweight if (God forbid) oxide were to go out of business. reply steveklabnik 13 hours agoparentTo be clear about it, the \"paperweight problem\" is very important to us as well. It's worth remembering that the MPL doesn't care if a copy is posted openly on GitHub or not, and (I am not a lawyer!) we have obligations to our customers under it regardless if non-customers can browse the code. reply snvzz 4 hours agoprevThe name is, by unfortunate coincidence, also used by another operating system's microkernel[0]. 0. https://sr.ht/~sircmpwn/helios/ reply sneak 15 hours agoprevI know they’re ex-Sun, but is there any real technical benefit for choosing not-Linux (for their business value prop)? I know of the technical benefits of illumos over linux, but does that actually matter to the customers who are buying these? Aren’t they opening a whole can of worms for ideology/tradition that won’t sell any more computers? As someone who runs Linux container workloads, the fact that this is fundamentally not-Linux (yes I know it runs Linux binaries unmodified) would be a reason against buying it, not for. reply steveklabnik 15 hours agoparent> does that actually matter to the customers who are buying these? It's not like we specifically say \"oh btw there's illumos inside and that's why you should buy the rack.\" It's not a customer-facing detail of the product. I'm sure most will never even know that this is the case. What customers do care about is that the rack is efficient, reliable, suits their needs, etc. Choosing illumos instead of Linux here is a choice made to help effectively deliver on that value. This does not mean that you couldn't build a similar product on top of Linux inherently, by the way, just that we decided illumos was more fit for purpose. This decision was made with the team, in the form of an RFD[1]. It's #26, though it is not currently public. The two choices that were seriously considered were KVM on Linux, and bhyve on illumos. It is pretty long. In the end, a path must be chosen, and we chose our path. I do not work on this part of the product, but I haven't seen any reason to believe it has been a hindrance, and probably is actually the right call. > the fact that this is fundamentally not-Linux (yes I know it runs Linux binaries unmodified) would be a reason against buying it, not for. I am curious why, if you feel like elaborating. EDIT: oh just saw your comment down here: https://news.ycombinator.com/item?id=39180814 1: https://rfd.shared.oxide.computer/ reply wmf 14 hours agorootparentThe Linux vs. Illumos decision seems to be downstream of a more fundamental decision to make VMs the narrow waist of the Oxide system. That's what I'm curious about. reply amluto 14 hours agorootparentEspecially since Oxide has a big fancy firmware stack. I would expect this stack to be able to do an excellent job of securely allocating bare-metal (i.e. VMX root on x86 or EL2 if Oxide ever goes ARM) resources. This would allow workloads on Oxide to run their own VMs, to safely use PCIe devices without dealing with interrupt redirection, etc. reply wmf 14 hours agorootparentI'm not affiliated with Oxide but I don't think you can put Crucible and VPC/OPTE in firmware. Without a DPU those components have to run in the hypervisor. reply amluto 14 hours agorootparentPossibly not. But I do wonder why cloud and cloud-like systems aren’t more aggressive about splitting the infrastructure and tenant portions of each server into different pieces of hardware, e.g. DPU. A DPU could look DPU could look like a PCIe target exposing NVMe and a NIC, for example. Obviously this would be an even more custom design than Oxide currently has, but Oxide doesn’t seem particularly shy about such things. reply binjip 3 hours agorootparentprevIt would be great if that RFD will become public someday, if it of course possible, especially if it's a long read. reply throwawaaarrgh 14 hours agorootparentprevA team should always pick the tools they are most familiar with. They will always have better results with that, than trying to use something they understand less. With this in mind, using their own stack is a perfectly adequate choice. Factors outside their team will determine if that works out in the long term. reply wmf 14 hours agorootparentA handful of the team are more familiar with Illumos and the next hundred people they hire after that will be more familiar with Linux. reply steveklabnik 14 hours agorootparentTo be clear, we at the time had already hired people with deep familiarity with Linux at the time this decision was made. In particular, Laura Abbott, as one example. It is true that the number of developers that know Linux is larger than the ones that know illumos. But this is also true of the number of developers who know C as the ones who know Rust. Just like some folks need to be onboarded to Rust, some will need to be onboarded to illumos. That is of course part of the tradeoff. reply Jtsummers 13 hours agorootparentprevIf your hiring decisions are always based on what people are currently familiar with, you'll always be stuck in the past. You may not even be able to use present day tooling and systems because they could be too new to hire people for. You're much better off hiring people who are capable of learning, and then giving them the opportunities to learn and advance their knowledge and skills. reply throwawaaarrgh 10 hours agorootparentEveryone is capable of learning. I can hire someone who is capable of learning Japanese. They can then try to teach the rest of the team Japanese. Does that mean it's a good idea to switch all our internal docs to Japanese? Maybe if I was building a startup in Japan. Similarly, writing internal docs in English for a startup in Japan would be of equal difficulty and value. Hooray, we're learning! And struggling more than needed to build a product. You're better off hiring experienced people who are highly productive. If they're highly productive with one stack, it makes no sense to change their stack so they're no longer productive, or hiring people who aren't familiar with it and waiting for them to become productive. There's nothing wrong with using old, well established things. They're quite often better than new things. As long as they're still supported, just use whatever builds a working product. It's the end product that matters. reply Jtsummers 9 hours agorootparent> Everyone is capable of learning. I can hire someone who is capable of learning Japanese. They can then try to teach the rest of the team Japanese. Does that mean it's a good idea to switch all our internal docs to Japanese? The difference between Japanese and English is much, much bigger than the difference between one Unix OS and one Unix-like OS. This is a remarkably disingenuous argument. If you really don't understand the difference in scope, there's no point in discussing anything with you because you've managed to disprove your opening sentence with yourself as the counterexample. reply pjmlp 12 hours agorootparentprevAs someone that knows UNIX since 1993, starting with Xenix, many that are familiar with Linux, are actually familiar with a specific Linux distribution, as the Linux wars took over UNIX wars. That being the case, knowing yet another UNIX cousin isn't that big deal. reply throwawaaarrgh 14 hours agorootparentprevA lot of people out there claim to know Linux, yet few can prove it. OTOH, if they gain a cult following with lots of people using their stack, those people might become more familiar with their stack than most Linux people are with theirs. They could grow a captive base of prospective hires. That's not the big concern though. The big concern is whether vendor integration and certification becomes a stumbling block. You can hire any monkey to write good-enough code, but that doesn't give you millions in return. Partnerships with vendors and compliance certifications can give you hundreds of millions. The harder that is, the farther the money is. A totally custom, foreign stack can make it harder, or not; it depends how they allocate their human capital and business strategy, whether they can convince vendors to partner, and clients to buy in. Anything very different is a risk that's hard to ignore. reply steveklabnik 14 hours agorootparentprevI do not personally agree with this. I do think that familiarity is a factor to consider, but would not give it this degree of importance. It also was not discussed as a factor in the RFD. reply bcantrill 15 hours agoparentprevKeep in mind that Helios is really just an implementation detail of the rack; like Hubris[0], it's not something visible to the user or to applications. (The user of the rack provisions VMs.) As for why an illumos derivative and not something else, we expanded on this a bit in our Q&A when we shipped our first rack[1] -- and we will expand on it again in the (recorded) discussion that we will have later today.[2] [0] https://hubris.oxide.computer/ [1] https://www.youtube.com/watch?v=5P5Mk_IggE0&t=2556s [2] https://mastodon.social/@bcantrill/111840269356297809 reply kaliszad 14 hours agorootparentPerhaps you could talk a bit about the distributed storage based on Crucible with ZFS as the backing storage tonight. I would really love to hear some of the details and challenges there. reply bcantrill 13 hours agorootparentYes! Crucible[0] is on our list of upcoming episodes. We can touch on it tonight, but it's really deserving of its own deep dive! [0] https://github.com/oxidecomputer/crucible reply panick21_ 13 hours agorootparentThe timing of your podcast is the least convenient thing ever for us poor Europeans. And then the brutal wait the next day until its uploaded. The only thing I miss about Twitter Spaces is that you could listen the morning after. reply kaliszad 11 hours agorootparentYes (hello from Czechia), however there will always be somebody who this is inconvenient for. Also, I have to confess I was at times immersed in other work that I made a few Oxide and Friends live. I might stay up tonight. I am looking forward to the crucible episode. It sounds like it could be a startup on its own, it wouldn't be the first distributed file/ storage system company. reply StillBored 14 hours agoparentprevLinux is a nightmare in the embedded/appliance space because one ends up just having platform engineers who spend their day fixing problems with the latest kernels, drivers, core libraries, etc, that the actual application depends on. Or one goes the route of 99% of the IoT/etc vendors, and never update the base OS and pray that there aren't any active exploits targeting it. This is why a lot of medium-sized companies cried about Centos, which allowed them to largely stick to a fairly stable platform that was getting security updates without having to actually pay/run a full blown RHEL/etc install. Every ten years or so they had to revisit all the dependencies, but that is a far easier problem than dealing with a year or two update cycle, which is too short when the qualification timeframe for some of these systems is 6+ months long. So, this is almost exclusively a Linux problem; any of the *BSD/etc. alternatives give you almost all of what Linux provides without this constant breakage. reply bcantrill 14 hours agorootparentThis is a really, really good point -- and is a result of the model of Linux being only a kernel (and not system libraries, commands, etc.). It means that any real use of Linux is not merely signing up for kernel maintenance (which itself can be arduous) but also must make decisions around every other aspect of the system (each with its own communities, release management, etc.). This act is the act of creating a distribution -- and it's a huge burden to take on. Both illumos and the BSD derivatives make this significantly easier by simply including much more of the system within their scope: they are not merely kernels, but also system libraries and commands. This weighed heavily in our own calculus, so I'm glad you brought it up! reply trhway 14 hours agorootparent>including much more of the system within their scope: they are not merely kernels, but also system libraries and commands. giving limited resources of the dev team it may lead to limited support of the system outside of the narrow set of officially supported/certified hardware with that support falling behind on modern hardware, as it happened with Sun, and vendor lock-in as a result into overpriced and low performing hardware. There is a reason that back then at Solaris dev there was a joke about embedding Linux kernel as a universal driver for Solaris kernel in order to get reasonable support for the hardware around. reply cross 14 hours agorootparentThis is less of an issue for us at Oxide, since we control the hardware (and it is all modern hardware; just a relatively small subset of what exists out there). Part of Sun's issue was that it was tied not just to a software ecosystem, but also to an all-but-proprietary hardware architecture and surrounding platform. Sun eventually tried to move beyond SPARC and SBus/MBus, but they really only succeeded in the latter, not the former. reply mardifoufs 14 hours agorootparentprevWell they aren't burdened by having to make their own processors, like Sun had to do, or their own full custom chips in general. They just have to support the selection of hardware they pick, and they have complete oversight of what hardware runs on their racks. So I'm not sure if the sun comparison is relevant here, since they can still pick top of the line hardware. Just not any hardware reply trhway 11 hours agorootparentAny issues with funding or whatever, and their customers would get locked in on the yesterday's \"top of the line hardware\" (reminds how Oracle used lawyers to force HP to continue support Itanic). Sun was 50K persons company, and they struggled to support even reasonably wide set of hardware. Vendor lock in is like a Newton law in this industry. reply linksnapzz 13 hours agorootparentprev>that support falling behind on modern hardware, as it happened with Sun, and vendor lock-in as a result into overpriced and low performing hardware. The Oxide hw is using available AMD SKUs for CPU. reply pjmlp 12 hours agorootparentprevInteresting that you bring up embedded/appliance space, as I have noticed there are plenty of FOSS alternatives coming up, key features not being Linux based, and not using GPL derived licenses. FreeRTOS, Nuttx, Zephyr, mbed, Azure RTOS,... reply GrumpySloth 14 hours agorootparentprevCentOS wasn’t used in embedded systems. reply dralley 13 hours agorootparentSure it was. So is RHEL. Embedded isn't limited to devices equal or less powerful / expensive than the Raspberry Pi. reply sarlalian 10 hours agorootparentprevArista EOS is definitely CentOS Linux release 7.9.2009 (AltArch) based. reply mlindner 9 hours agorootparentprevEven Windows was and is used substantially in embedded systems. reply GrumpySloth 7 hours agorootparentI know about that. This is a special edition for embedded though. But CentOS is news to me. CentOS was targeted for servers. reply skullone 15 hours agoparentprevIt seems healthy to have options, almost like the universe is healing a bit after oracle bought Sun. I can't imagine better hands bringing the oxide system together than that team. As an engineer who works entirely with Linux anymore, I pine for the days of another strong Unix in the mix to run high value workloads on. Comparing openvswitch on Linux, to say, the crossbow SDN facility on Solaris, I'd take crossbow any day. Nothing \"wrong\" with Linux, but it is sorely lacking in \"master plan\" levels of cohesion with all the tooling taking their own path, often bringing complexity that requires even for abstraction with yet more complicated tooling on top. reply pjmlp 15 hours agoparentprevTheir customers run virtualised OS on top of this. This is no different from Azure Host OS, Bottlerocket, Flatcar or whatever. This maters to them, as knowing the whole stack, some of the kernel code is still theirs from Sun days, and making it available matters to the customers that want source code access for security assement reasons. reply spamizbad 14 hours agoparentprevSeems strange to me too but it sounds like the end-users basically never interact with this - it's just firmware humming along in the background. As long as its open-source and reasonably well documented its already lightyears ahead of what else is out there. reply NexRebular 14 hours agoparentprevNot everything needs to be linux. Besides, if monocultures are supposed to be harmful, why is linux being thrown to everything nowadays? Very dangerous to have a single point of failure in (critical) applications. reply moondev 14 hours agoparentprevThe main drawbacks to me are 1. No support for nested virtualization, so running a vm inside your vm is not available. This prevents use of projects such as kubevirt or firecracker on a Linux guest, and WSL2 on a Windows guest. 2. No GPU support If the base hypervisor was Linux, it would be way more capable for users it seems. I also wonder if internally Linux is used for development of the platform itself so they can create \"virtual\" racks to dogfood the product without full blown physical racks. With all that said, I do not know the roadmap and admittedly there are already quite a few existing platforms built on kvm, so as their hypervisor improves and becomes more capable it could potentially become strategic advantage. reply steveklabnik 13 hours agorootparent> I also wonder if internally Linux is used for development of the platform itself Developers at Oxide work on whatever platform they'd like, as long as they can do their work. I will say I am in the minority as a Windows user though, most are on some form of Unix. > so they can create \"virtual\" racks to dogfood the product without full blown physical racks. So one of the reasons why Rust is such an advantage for us is its strong cross-platform support: you can run a simulated version of the control plane on Mac, Linux, and Illumos, without a physical rack. The non-simulated version must run on Helios. [1] That said we do have a rack in the office (literally named dogfood) that employees can use for various things if they wish. 1: https://github.com/oxidecomputer/omicron?tab=readme-ov-file#... reply moondev 13 hours agorootparentInteresting thanks for the insight. > I will say I am in the minority as a Windows user though, most are on some form of Unix. Now i'm imagining Helios inside WSI - Windows Subsystem for illumos reply steveklabnik 13 hours agorootparentYou're welcome. I will give you one more fun anecdote here: when I came to Oxide, nobody in my corner of the company was using Windows. And hubris and humility almost Just Worked: we had one build system issue that was using strings instead of the path APIs, but as soon as I fixed those, it all worked. bcantrill remarked that if you had gone back in time and told him long ago that some of his code would Just Work on Windows, he would have called you a liar, and it's one of the things that validates our decisions to go with Rust over C as the default language for development inside Oxide. > Now i'm imagining Helios inside WSI - Windows Subsystem for illumos That would be pretty funny, ha! IIRC something about simulated omicron doesn't work inside WSL, but since I don't work on it actively, I haven't bothered to try and patch that up. I think I tried one time, I don't remember specifically what the issue was, as I don't generally use WSL for development, so it's a bit foreign to me as well. reply panick21_ 13 hours agorootparent> that was using strings instead of the path API Man you can't let Brain live that one down can you? :) reply steveklabnik 12 hours agorootparentI didn't bother to git blame the code, I myself do this from time to time :) reply yjftsjthsd-h 7 hours agorootparentprev> Now i'm imagining Helios inside WSI - Windows Subsystem for illumos I mean... WSL2 is just hyperv with some integration glue, and illumos isn't Linux but unix is unix; that might well be doable. reply fragmede 12 hours agorootparentprevHow is Oxide for GPU-heavy workloads? reply steveklabnik 12 hours agorootparentThere are no GPUs in the rack, so pretty bad, haha. We certainly understand that there's space in the market for a GPU-focused product, but that's a different one than the one we're starting the company off with. There's additional challenge with how we as a company desire openness, and GPUs are incredibly proprietary. We'll see what the future brings. Luckily for us many people still desire good old classic CPU compute. reply NexRebular 10 hours agorootparentWould pass-through to VM work? At $work I'm running SmartOS servers with GPU passing to a ubuntu bhyve for the occasional CUDA compute and it works wonderfully. Wonder if similar could be possible with Helios? reply steveklabnik 9 hours agorootparentThe software interface isn't the problem: the problem is that there are no physical GPUs in the product. There's nothing to pass through. reply mardifoufs 14 hours agoparentprevI think it's a good idea to have more choice, especially in OSS. A Linux mono culture isn't any better than a chromium mono culture. They might be able to do stuff that just isn't practical if they stuck with Linux. They are also probably more familiar with illumos, or at least familiar enough to know that they can use it to do more than with linux reply greggyb 14 hours agoparentprevIf you're running in one of the big 3 cloud providers, the bottom-level hypervisors are not-linux. This is equivalent. Are you anti-AWS or anti-Azure for the same reason? This is the substrate upon which you will run any virtualized infrastructure. reply qmarchi 14 hours agorootparentSmall note, that's not true for Google Cloud, which runs on top of Linux, though modified. Disclaimer: Former Googler, Cloud Support reply bewaretheirs 11 hours agorootparentAs I understand it, there's linux running on the Google Cloud hardware but the virtualized networking and storage stacks in Google Cloud are google proprietary and largely bypass linux -- in the case of networking see the \"Snap: a Microkernel Approach to Host Networking\" paper. In contrast, it appears that Oxide is committing to open-source the equivalent pieces of their virtualization platform. reply refulgentis 14 hours agorootparentprevAnother Xoogler here: any idea what they mean by it's not Linux at the bottom for other providers? Like, surely it's _some_ common OS? Either my binaries wouldn't run or AWS is reimplementing Linux so they can, which seems odd. Or are they just saying that the VM my binary runs on might be some predictable Linux version, but the underlying thing launching the VM could be anything? reply p_l 14 hours agorootparentOld AWS used to be Xen, Nitro afaik uses customised VMM and I don't recall if it's not a custom OS or hosted on top of something. Azure is Hyper-V underneath IIRC, a custom variant at least (remember Windows Server Nano? IIRC it was the closest you could get to running it), with sometimes weird things like network cards running Linux and integrating with Windows' built-in SDN facility. Rest of the bigger ones is mainly Linux with occasional Xen and such, but sometimes you can encounter non-trivial VMware deployments. reply zokier 11 hours agorootparentNitro is supposed to be this super customized version of KVM. reply bewaretheirs 11 hours agorootparentprevWhen your programs are running on a VM, the linux that loads and runs your binaries is not at the bottom; that linux image runs inside a virtual machine which is constructed and supervised by a hypervisor which sits underneath it all. That hypervisor may run on the bare machine (or what passes for a bare machine what with all the sub-ring-zero crud out there), or may run on top of another OS which could be linux or something else. And even if there is linux in the middle and linux at the bottom they could be completely different versions of linux from releases made years apart. reply bpye 14 hours agorootparentprevAzure runs a version of Windows, see: https://techcommunity.microsoft.com/t5/windows-os-platform-b... reply antod 11 hours agorootparentprev> Or are they just saying that the VM my binary runs on might be some predictable Linux version, but the underlying thing launching the VM could be anything? Yup. eg with Xen the hypervisor wasn't Linux, even if the privileged management VM (dom0) was Linux (or optionally NetBSD in the early days). The very small Xen hypervisor running on the bare metal was not a general purpose OS, and didn't expose any interface itself - it was well hidden and relied on dom0 for administration. reply qmarchi 14 hours agorootparentprevCorrect, that the Hypervisor isn't running Linux. I think the only provider where that would make sense would be Microsoft, where they have their own OS. reply tptacek 14 hours agorootparentprevI don't about EC2 but Lambda and Fargate are presumably Firecracker, which is Linux KVM. reply zokier 11 hours agorootparentAWS \"Nitro\" hypervisor which powers EC2 is their (very customized) KVM. https://docs.aws.amazon.com/whitepapers/latest/security-desi... reply wmf 14 hours agorootparentprevI suspect a lot of people would (irrationally) freak out if they saw how the public cloud works because it's so different from \"best practices\". Oxide would probably trigger people less if they never mentioned Illumos but that's not really an option when it's open source. reply thinkingkong 15 hours agoparentprevThis has been / will be the market education challenge; Its the same one Joyent had with SmartOS. Theyre correctly pointing out that the end user or operator will basically never interact with this layer, but it does cause some knee-jerk reactions. All that said, there are some pretty great technical benefits to using illumos derived systems the least of which is the teams familiarity and ability to do real diagnosis on production issues. I wont put words in anyones mouth but I suspect thats going to be critical for them as they support customer deployments w/o direct physical access. reply sarlalian 10 hours agoparentprevAs a customer, I expect most of the technical advantages will be basically being a down stream consumer of ZFS. From a developer / maintainer of an OS, Dtrace and ZFS are large technical wins. Part of the overall value proposition of Oxide, is \"correctness\". You get an OS/Hardware stack that are designed to work together. You get 20 years of cruft thrown out. You get a lot of tooling, API's, etc written in a performant memory safe language (rust). Also you get a really fantastic podcast about the whole process. And as a customer you get a company that understands their stack from driver to VM and has a ton of internal expertise debugging production problems. reply shusaaafuejdn 15 hours agoparentprevAs far as performance and feature set, probably not anymore (I would have answered differently 10 years ago, and if I am wrong today would love to be educated about it). However, if we are considering code quality, which I consider important if you are actually going to be maintaining it yourself as oxide will have to do since they need customizations, then most of the proprietary Unix sources are just superior imo. That is, they have better organization, more consistency in standards, etc. The BSDs are slightly better in this regard as well, it really isn't a proprietary vs open source issue, it's more about the insane size of the Linux kernel project making strict standards enforcement difficult if not impossible the further you get from the very core system components. Irregardless of them being ex-Sun (and I am not ex Sun), if I needed a custom OS for a product I was working on, Linux would be close to the last Unix based OS source tree I would try to do it with, only after all other options failed for whatever reason. And that's not even taking into account the licensing, which is a whole other can of worms. reply kardianos 15 hours agoparentprevIn one podcast, the reason given was staff familiarity and owning the full stack, not just the kernel I believe. reply quux 15 hours agoparentprevAren't they also ex-Joyent? Joyent ran customer VMs in prod on Illumos for many years so there's a lot of experience there. reply steveklabnik 15 hours agorootparentMany people, including part of the founding team, are ex-Joyent, yes. Some also worked at Sun, on the operating systems that illumos is ultimately derived from. reply littlestymaar 15 hours agorootparentprevbcantrill used to work at Sun then became CTO at Joyent, so the reason why Joyent ran Illumos is probably the same reason as why Oxide is, because Cantrill likes it and judges that it's a good fit for what they are doing. reply steveklabnik 15 hours agorootparentAs I elaborated above, bcantrill did not decree that we must use illumos. Technical decisions are not handed down from above at Oxide. reply littlestymaar 14 hours agorootparentI saw your comment[1] after I wrote mine, but I'm not saying that he's forcing you guys to use it (that would not a good way of being a CTO at a start-up…), but that doesn't prevent him from advocating for solutions he believes in. Would you say that Oxide would have chosen Illumos if he wasn't part of the company? [1]: https://news.ycombinator.com/item?id=39180706 reply steveklabnik 14 hours agorootparent> Would you say that Oxide would have chosen Illumos if he wasn't part of the company? I don't know how to respond to this question, because to me it reads like \"if things were completely different, what would they be like?\" I have no idea if you could even argue that a company could be the same company with different founders. What I can say is that this line of questioning still makes me feel like you're implying that this choice was made simply based on preference. It was not. I am employee #17 at Oxide, and the decision still wasn't made by the time I joined. But again, the choice was made based on a number of technical factors. The RFD wasn't even authored by Bryan, but instead by four other folks at Oxide. We all (well, everyone who wanted to, I say \"we\" because I in fact did) wrote out the pros and cons of both, and we weighed it like we would weigh any technical decision: that is, not as a battle of sports teams, but as a \"hey we need to drive some screws: should we use a screwdriver, a hammer, or something else?\" sort of nuts-and-bolts engineering decision. reply littlestymaar 14 hours agorootparent> we weighed it like we would weigh any technical decision: that is, not as a battle of sports teams, but as a \"hey we need to drive some screws: should we use a screwdriver, a hammer, or something else?\" sort of nuts-and-bolts engineering decision. I'm not saying otherwise. In fact, when I wrote my original comment, I actually rewrote it multiple time to be sure it wouldn't suggest I was thinking it was some sort of irrational decision (that's why I added the “it's a good fit for what they are doing”), but given your reaction it looks like I failed. Written language is hard, especially in a foreign language, sorry about that. reply steveklabnik 13 hours agorootparentIt's all good! I re-wrote what I wrote multiple times as well. Communication is hard. I appreciate you taking the effort, sorry to have misunderstood. Heck, there's a great little mistake of communication in the title: this isn't just \"intended\" to power the rack, it does power the rack! But they said that because we said that in the README, because that line in the README was written before it ended up happening. Oops! reply sunshowers 14 hours agorootparentprev(I work at Oxide.) Bryan is just one out of several illumos experts here. If none of those were around, sure, maybe we wouldn't have picked illumos -- but then we'd be unrecognizably different. I came into Oxide with a Linux background and zero knowledge of illumos. Learning about DTrace especially has been great. reply bbkane 3 hours agorootparentI'd like to learn DTrace (especially after the recent 20yr podcast episode), but I worry it'll never make into mainstream Linux debugging, and hence only useful for more niche jobs. reply vvern 15 hours agoparentprev> yes I know it runs Linux binaries unmodified Is it that it runs Linux binaries unmodified or that it runs vms and manages VMs which run Linux, and as an end-user, that's what you run your software in? reply bcantrill 15 hours agorootparentIt runs VMs -- so it doesn't just run Linux binaries unmodified, it runs Linux kernels unmodified (and, for that matter, Windows, FreeBSD, OpenBSD, etc.). reply tonyarkles 15 hours agorootparentprevAs far as I recall it's not a VM. They run in \"LX Branded Zones\" which does require a Linux userland so that the binaries can find their libraries etc but Zones are more like \"better cgroups than cgroups, a decade earlier\" than VMs. reply bcantrill 15 hours agorootparentNo, it's a VM, running a bhyve-based hypervisor, Propolis.[0] LX branded zones were/are great -- but for absolute fidelity one really needs VMs. [0] https://github.com/oxidecomputer/propolis reply bpye 14 hours agorootparentDo you have a solution for running containers (Kubernetes, etc)? Are you spinning up a Linux VM to run the containers in there, doing VM per container, or something else? reply panick21_ 12 hours agorootparentCostumers can decide I would assume. Most likely you install you install some Kubernetes and then just have multible VMs distributed across the rack. And then run multible Pods in each node. VM per container seems like a waist unless you need that extra isolation. reply bpye 12 hours agorootparentI wondered if there was any support for running containers built in - something like EKS/AKS/GKE/Cloud Run/etc - but looking at the docs it appears not. I agree that VM per container can be wasteful - though something like Firecracker at least helps with start time. reply panick21_ 4 hours agorootparentFrom the podcast it seems that they want to deliver a minimal viable product. Their primary costumers already have a lot of their own higher level stack. They might get into adding more higher level software eventually depending on what costumers want. reply Extigy 15 hours agoparentprevPerhaps Illumos is particularly well suited for a Hypervisor/Cloud platform due to work upstreamed by Joyent originally for SmartOS? reply jeffbee 15 hours agoparentprevDo you have the same gut reaction to ESXi? reply stonogo 14 hours agorootparentI sure do. We've finally got to a place where we don't need weird hardware tricks to containerize workloads -- this is why a lot of shops pursue docker-like ops for production. When I buy hardware, long-term maintenance is a factor, and when my whole operations fleet relies on ESX, or in this case a Solaris fork, I'm now beholden to one company for support at that layer. Buying a rack of Supermicro gear and running RHEL or SLES with containerized orchestration on top means I can, in a pinch, hire experts anywhere to work on my systems. I have no reason to believe Oxide would be anything but responsive and effective in supporting their systems, but introducing bespoke software this deep in the stack severely curtails my options if things get bad. reply apendleton 14 hours agorootparentI think the value proposition they're offering is a carefully integrated system where everything has been thoroughly engineered/tested to work with everything else, down to writing custom firmware to guarantee that it's all ship-shape, so that customers don't have to touch any of the innards, and will probably just treat them as a black box. It seems like it's chock-full of stuff that they custom-built and that nobody else would be familiar with, by design. If that's not what you want, this probably isn't the product for you. reply jeffbee 13 hours agorootparentprevI can somewhat see your point, but in my experience you can't rely on RHEL or whatever vendor Linux to correctly bring up random OEM hardware. You will slowly discover all of the quirks, like it didn't initialize the platform EDAC the way you expected, or it didn't resolve some weird IRQ issue, etc. Nothing about my experience leads me to believe Linux will JFW on a given box, so I don't feel like Linux has an advantage in this regard, or that niche operating systems have a disadvantage. Certainly I feel like a first-party OS from the hardware vendor is going to have a lot of advantages. reply internetter 12 hours agoprevI really want one of these racks in my bedroom. Unfortunately, somehow I think I couldn't afford one ;) reply xer0x 4 hours agoprevOh the feels! reply mad_vill 10 hours agoprevoh wow ... can't wait to waste a bunch of time trying to get this running in hyper-v. reply rhinoceraptor 10 hours agoparentI would recommend trying SmartOS or OmniOS instead, since the Oxide rack isn't filled with IBM compatible personal computers on sleds, and they have no BIOS or UEFI. reply alberth 13 hours agoprevLicense MPL 2.0 is an interesting license choice, for an operating system. EDIT: why the downvotes? reply steveklabnik 13 hours agoparentQuoting from an RFD co-authored by bcantrill and myself describing Oxide's policies around open source: > For any new Oxide-created software, the MPL 2.0 should generally be the license of choice. The exception to this should be any software that is a part of a larger ecosystem that has a prevailing license, in which case that prevailing license may be used. EDIT: I also am confused about why you are downvoted. Are there any major operating systems distributions that are MPL licensed? I can't think of any off the top of my head. Beyond that it's a simple question. reply cosmic_quanta 13 hours agorootparentI had to look up RFD, and I like the idea! https://oxide.computer/blog/rfd-1-requests-for-discussion reply steveklabnik 13 hours agorootparentAh thanks! Yeah I should have mentioned this in my comment, thank you for adding the context. By the way, you can browse public RFDs here: https://rfd.shared.oxide.computer/ I didn't include any links to any RFDs in my comments today because I have only been referencing non-public ones. reply alberth 13 hours agorootparentprev> [if Oxide-created software] is a part of a larger ecosystem that has a prevailing license, in which case that prevailing license may be used How does that work if the prevailing is BSD/MIT/ISC? You're saying that Oxide can then be licensed under BSD/MIT/ISC? reply steveklabnik 13 hours agorootparentSo I decided to cut off my quote but the next line has the answer: > For example, Rust crates are generally dual-licensed as MIT/Apache 2. We often produce components that we share with the broader open source world. For example, dropshot[1] is our in-house web framework, but we publish it as a standalone package. It is licensed under Apache-2.0 instead of MPL 2.0 because the norm in the Rust ecosystem is Apache and not MPL. > You're saying that Oxide can then be licensed under BSD/MIT/ISC? I am saying that we do not have one single license across the company. Some components are probably BSD/MIT/ISC licensed somewhere, and I guarantee that some third party dependencies we use are licensed under those licenses. That's different from \"you could choose to take it under BSD,\" which I didn't mean to imply, sorry about that! 1: https://crates.io/crates/dropshot reply lloydde 4 hours agoparentprevMPL 2.0 has been the preferred license for CTO Bryan Cantrill and crew for more than a decade: “And because any conversation about open source has to address licensing at some point or another, let’s get that out of the way: we opted for the Mozilla Public License 2.0. While relatively new, there is a lot to like about this license: its file-based copyleft allows it to be proprietary-friendly while also forcing certain kinds of derived work to be contributed back; its explicit patent license discourages litigation, offering some measure of troll protection; its explicit warranting of original work obviates the need for a contributor license agreement (we’re not so into CLAs); and (best of all, in my opinion), it has been explicitly designed to co-exist with other open source licenses in larger derived works. Mozilla did terrific work on MPL 2.0, and we hope to see it adopted by other companies that share our thinking around open source!” https://bcantrill.dtrace.org/2014/11/03/smartdatacenter-and-... Also discussed around 38 minute of https://youtu.be/Zpnncakrelk?si=DkSW6CM_MS-q1Gyd Although not explicitly stated there are like deeper roots here “The one important exception to these generalizations is Sun Microsystems' CDDL, which was a true improvement on MPL 1.1, and which continues to cover a substantial amount of important open source software. … I encourage Oracle, the current CDDL steward, to consider relicensing its CDDL code under MPL 2.0, which is as worthy a successor to CDDL 1.0 as it is to MPL 1.1.” from Richard Fontana’s article at the time of the MPL 2.0 release, https://opensource.com/law/12/1/the-new-mpl With its compatibility with strong, older copyright licenses I’m surprised the license has not had more widespread adoption. It is a not too hot, not too cold porridge of a file level copyleft and CYA OSS license with the strong backing of Mozilla. reply temptemptemp111 14 hours agoprev [–] What even is Oxide Computer? It makes no sense - it was publicized with all sorts of anti-blob, freedom, and posts about management engines and a sort of alternative to RaptorCS/IBM (which now has blobs again)... Yet most of that stuff is now buried/removed and Oxide Computer is just a hardware platform with unnecessary lock-in. For the bunker of the rich to be able to run their own mini-cloud? Sure. For anything else it seems like a bad design. reply steveklabnik 13 hours agoparent> Yet most of that stuff is now buried/removed Nothing has changed with regards to our anti-blob and pro-open source stances. I am not sure what you're referring to here. > with unnecessary lock-in. What lock-in are you referring to here? The way that things run on the rack is via virtual machines, you can run virtual machines on many providers. We even have a terraform provider so that you can use familiar tools instead of the API directly, if you believe that is lock-in (and that stuff is all also fully open source). reply temptemptemp111 13 hours agorootparentI don't expect anyone to see my comments unless they're really looking since I've been shadow banned for many years now - so I appreciate your reply. To be clearer regarding my questions: - What happened to Project X (supposedly coreboot++ for latest AMD CPUs)? It seems dead, despite being more reported on than Oxide's attempts in working with AMD (to achieve the same outcomes, presumably - what's the difference?). Loads of well meaning people have approached this with virtue, innocence and skills; perhaps another approach is needed that fully respects the dynamic between the user, the chip manufacturers and the governments and banks they're in debt to. - Does Oxide attempt to sandbox, completely remove or 'verify as benign' aspects like the PSP? For example, if someone could verify that the PSP cannot possibly be affected over the network, then peace of mind could be more affordable regarding things like supply chain attacks and bad actors with AMD/Intel/Apple management engine secrets. Not referring to software lock-in, just hardware. And it isn't very nefarious like other hardware lock-in (serialization, see Rossmann Group). Just hardware on the rack-level: replacing oxide gear & upgrading oxide gear (not sure about repair, that could be easy). And if the offering were of a less blobby architecture, then many of us would be happy to pay a bit more for the hardware as a system. However, if the hardware platform is FOSS, then it won't be unnecessarily difficult to mix and match and integrate the Oxide gear with other DC-class gear. reply steveklabnik 12 hours agorootparentSo, your comment was not dead when I saw it. This reply was, but apparently now has been vouched for. > What happened to Project X (supposedly coreboot++ for latest AMD CPUs)? I don't recall what you're referring to specifically, maybe this was a thing before I started at Oxide. I do know that we deliberately decided to not go with coreboot. I believe the equivalent component would be phbl[1]. It boots illumos directly. Bryan gave a talk about how we boot[1][2] with more reasoning and context. > Does Oxide attempt to sandbox, completely remove or 'verify as benign' aspects like the PSP? The general attitude is still \"remove or work around every binary blob possible,\" but the PSP is unfortunately not able to be worked around. > However, if the hardware platform is FOSS We fully intend to do this, by the way. Just haven't yet. It'll come. 1: https://github.com/oxidecomputer/phbl 2: https://www.osfc.io/2022/talks/i-have-come-to-bury-the-bios-... 2: https://news.ycombinator.com/item?id=33145411 reply temptemptemp111 11 hours agorootparentTo see more on \"Project X\", see the Phoronix article on it. At the very least, it would be resourceful if the Oxide devs had a chat with the Project X devs who have since given up - learnings can be had and time can be saved. And yes, coreboot itself is now untennable, but is also kind of a slang for the a category of deblobbed software. reply SirGiggles 4 hours agorootparentIs it possible you're thinking of the more recent AMD OpenSIL initiative? reply mlindner 9 hours agorootparentprevIf you're referring to this: https://www.phoronix.com/news/Project-X-AMD-Zen-Coreboot This seems to make no mention of Oxide at all. Perhaps you're connecting two different unrelated organizations together as Oxide appears to have never had any relation to it. I think you're just perhaps confused about what the situation is. reply sanderjd 7 hours agorootparentprevFor what it's worth, you don't seem to be shadowbanned as far as I can tell. Your original post seems to be dead due to downvotes, but this one seems to be in a totally normal non-dead non-shadowbanned state. reply wmf 5 hours agorootparentHe is banned; his comments here are only alive because I rescued them. reply throwawaaarrgh 14 hours agoparentprev [–] It's a mainframe. You use it like you use mainframes, but probably easier, as they're adopting more modern functionality. You won't be aware you're using it, just like you aren't aware when you use a zSystem. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The text provides instructions for building and installing the Oxide Helios operating system, as well as modifying and updating its packages.",
      "It mentions the use of diagnostic ROM files.",
      "All components of Oxide Helios are licensed under the Mozilla Public License Version 2.0."
    ],
    "commentSummary": [
      "The discussion on Hacker News revolves around Oxide, an Illumos distribution and hardware company, and its release of Helios, which is compared to SmartOS.",
      "Users express enthusiasm for Oxide's customized hardware and infrastructure solutions for on-premise servers, but raise concerns about pricing and the need for a comprehensive on-premises cloud experience.",
      "The conversation expands to topics such as the challenges in attracting developers to Illumos, sustaining maintenance, the advantages of using SmartOS, vendor integration and certification, virtual machines and containers, compatibility with different hardware platforms, and the importance of open-source firmware. Collaboration with Project X developers and the desire for an open-source hardware platform are also mentioned."
    ],
    "points": 388,
    "commentCount": 221,
    "retryCount": 0,
    "time": 1706546825
  },
  {
    "id": 39185198,
    "title": "New Algorithm for Faster Integer Linear Programming",
    "originLink": "https://www.quantamagazine.org/researchers-approach-new-speed-limit-for-seminal-problem-20240129/",
    "originBody": "Researchers Approach New Speed Limit for Seminal Problem Read Later Share Copied! Comments Read Later Read Later algorithms Researchers Approach New Speed Limit for Seminal Problem By Lakshmi Chandrasekaran January 29, 2024 Integer linear programming can help find the answer to a variety of real-world problems. Now researchers have found a much faster way to do it. Read Later Kristina Armitage/Quanta Magazine By Lakshmi Chandrasekaran Contributing Writer January 29, 2024 View PDF/Print Mode Abstractions blogalgorithmscomputational complexitycomputer scienceerror correctionlinear algebratraveling salesperson problemAll topics Introduction The traveling salesperson problem is one of the oldest known computational questions. It asks for the ideal route through a certain list of cities, minimizing mileage. Despite seeming simple, the problem is notoriously difficult. While you can use brute force to check all the possible routes until you find the shortest path, such a strategy becomes untenable after just a handful of cities. Instead, you can apply a rigorous mathematical model called linear programming, which roughly approximates the problem as a set of equations and methodically checks the possible combinations to find the best solution. But sometimes, you need to optimize for problems involving whole-number amounts. What good is a factory optimization plan that manufactures 500.7 couches? For this, researchers often turn to a variant of linear programming called integer linear programming (ILP). It’s popular in applications that involve discrete decisions, including production planning, airline crew scheduling and vehicle routing. “Basically, ILP is the bread and butter of operations research both in theory and practice,” said Santosh Vempala, a computer scientist at the Georgia Institute of Technology. Since they first formulated ILP over 60 years ago, researchers have discovered various algorithms that solve ILP problems, but they’ve all been relatively slow in terms of the number of steps required. The best version they could come up with — a kind of speed limit — comes from the trivial case where the problem’s variables (such as whether a salesman visits a city or not) can only assume binary values (zero or 1). In this case, ILP has a runtime that scales exponentially with the number of variables, also called the dimension. (If there’s only one variable, it takes just two steps to test every possible combination and solve the problem; two variables mean four steps, three mean eight steps, and so on.) Abstractions navigates promising ideas in science and mathematics. Journey with us and join the conversation. See all Abstractions blog Unfortunately, once the variables take a value beyond just zero and 1, the algorithm’s runtime grows much longer. Researchers have long wondered if they could get closer to the trivial ideal. Progress has been slow, with the record set in the 1980s and only incremental improvements made since. But recent work by Victor Reis, currently at the Institute for Advanced Study, and Thomas Rothvoss, at the University of Washington, has made the biggest runtime leap in decades. By combining geometric tools to limit the possible solutions, they created a new, faster algorithm for solving ILP in almost the same time as the trivial binary case. The result received a best-paper award at the 2023 Foundations of Computer Science conference. “This new algorithm is extremely exciting,” said Noah Stephens-Davidowitz, a mathematician and computer scientist at Cornell University. “It represents the first [major] improvement to ILP solvers in nearly 40 years.” ILP works by transforming a given problem into a set of linear equations that must satisfy some inequalities. The specific equations are based on the details of the original problem. But while these details may differ, the basic makeup of ILP problems remains the same, giving researchers a single way to attack a multitude of problems. Share this article Copied! Newsletter Get Quanta Magazine delivered to your inbox Subscribe now Recent newsletters Victor Reis helped create a new algorithm that solves integer linear programming problems in record time. Maryna Viazovska Introduction That’s not to say it’s easy work. It wasn’t until 1983 that the mathematician Hendrik Lenstra proved that the general problem was even solvable, providing the first algorithm that could do it. Lenstra thought about ILP geometrically. First, he turned the inequalities at the heart of ILP into a convex shape, such as any regular polygon. This shape represents the constraints of the individual problem you’re solving, whether it’s couch production or airline scheduling, so the shape’s interior corresponds to all possible values that could solve the inequalities, and thus the problem. Lenstra called this shape the convex body. The problem’s dimension influences the dimension of this shape: With two variables it takes the form of a flat polygon; in three dimensions it is a Platonic solid, and so on. Lenstra next imagined all the integers as an infinite set of grid points, known in mathematics as a lattice. A two-dimensional version looks like a sea of dots, and in three dimensions it looks like the points where steel beams in a building connect. The dimension of the lattice also depends on the dimension of a given problem. To solve a given ILP problem, Lenstra showed that you just look for where the possible solutions meet the set of integers: at the intersection of the convex body and the lattice. And he came up with an algorithm that could search this space exhaustively — but to be effective, it sometimes had to break the problem up into pieces of smaller dimensions, adding many steps to the runtime. Thomas Rothvoss and Reis combined previous work with a geometric approach to achieve an ILP algorithm that runs almost as quickly as the theoretical ideal. Dennis Wise/University of Washington In the following years, several researchers explored how to make this algorithm run faster. In 1988, Ravi Kannan and László Lovász introduced a concept called the covering radius, borrowed from the study of error-correcting codes, to help the convex body and lattice intersect more efficiently. Roughly, the covering radius makes sure that the convex body always contains at least one integer point, no matter where you place it on the lattice. As a result, the size of the covering radius also determines how efficiently you can solve the ILP problem. So it all came down to determining the size of the ideal covering radius. Unfortunately, this proved to be a hard problem on its own, and the best Kannan and Lovász could do was narrow down a possible value by searching for upper and lower bounds. They showed that the upper bound — the maximum size of the covering radius — scaled up linearly with the dimension. This was pretty fast, but not enough to significantly speed up the ILP runtime. Over the next 30 years, other researchers could only do slightly better. What ultimately helped Reis and Rothvoss break through was an unrelated mathematical result that focused purely on lattices. In 2016, Oded Regev and Stephens-Davidowitz showed, in effect, how many lattice points could fit within a specific shape. Reis and Rothvoss applied this to other shapes, which allowed them to better estimate the number of lattice points contained in an ILP covering radius, lowering the upper bound. “The latest breakthrough came with the realization that you can actually do other kinds of shapes,” Regev said. This new tightened upper bound was a vast improvement, allowing Reis and Rothvoss to achieve a dramatic speedup of the overall ILP algorithm. Their work brings the runtime to (log n)O(n), where n is the number of variables and O(n)means it scales linearly with n. (This expression is considered “almost” the same as the run time of the binary problem.) Related: Computer Scientists Break Traveling Salesperson Record Researchers Refute a Widespread Belief About Online Algorithms Computer Scientists Inch Closer to Major Algorithmic Goal Matrix Multiplication Inches Closer to Mythic Goal “It’s a triumph at the intersection of math, computer science and geometry,” said Daniel Dadush of the national research institute CWI in the Netherlands, who helped pioneer the algorithm Reis and Rothvoss used to measure the ILP runtime. For now, the new algorithm hasn’t actually been used to solve any logistical problems, since it would take too much work updating today’s programs to make use of it. But for Rothvoss, that’s beside the point. “It’s about the theoretical understanding of a problem that has fundamental applications,” he said. As to whether the computational efficiency of ILP could be further improved, researchers are still hopeful that they’ll keep approaching the ideal runtime — but not anytime soon. “That would require a fundamentally new idea,” Vempala said. By Lakshmi Chandrasekaran Contributing Writer January 29, 2024 View PDF/Print Mode Abstractions blogalgorithmscomputational complexitycomputer scienceerror correctionlinear algebratraveling salesperson problemAll topics Share this article Copied! Newsletter Get Quanta Magazine delivered to your inbox Subscribe now Recent newsletters The Quanta Newsletter Get highlights of the most important news delivered to your email inbox Email Subscribe Recent newsletters Comment on this article Quanta Magazine moderates comments to facilitate an informed, substantive, civil conversation. Abusive, profane, self-promotional, misleading, incoherent or off-topic comments will be rejected. Moderators are staffed during regular business hours (New York time) and can only accept comments written in English. Show comments Next article The Quest to Decode the Mandelbrot Set, Math’s Most Famous Fractal",
    "commentLink": "https://news.ycombinator.com/item?id=39185198",
    "commentBody": "Researchers have found a faster way to do integer linear programming (quantamagazine.org)292 points by pseudolus 8 hours agohidepastfavorite107 comments mzl 1 hour agoLowering the algorithmic upper bound for a core NP-complete problem is always extremely interesting. However, this is not necessarily related to improving runtime for practical implementations solving the problem in question. Solvers for mixed integer programming (MIP) use a lot of algorithms in conjunction with loads of heuristics. Building up the library of heuristics and strategies is a crucial part of why the improvement in MIP solvers have outpaced Moores law. From https://www.math.uwaterloo.ca/~hwolkowi/henry/teaching/f16/6..., the improvements in hardware from 1990 to 2014 was 6500x. But the improvements to the software are responsible for 870000x performance improvement. The referenced article may become another part of the puzzle in continuing performance improvements for MIP solvers, but it is not in any way a given. reply WhitneyLand 46 minutes agoparent1. It’s “run time”, not “runtime”. Sorry to be pedantic but they are different. The former is for algorithms the latter is for Java. 2. The article seems to say that they have improved the performance concretely, not simply that bounds have been tightened. It’s still a NP class algorithm so as n grows large the exponent will still grow large but the base value is lower. reply eigenket 44 minutes agorootparentRun time, runtime and run-time are all commonly used for the thing you want to call run time. None are incorrect. https://www.collinsdictionary.com/dictionary/english/runtime reply WhitneyLand 32 minutes agorootparentInformally they are used interchangeably. However, their definitions are different. The subject here, is academic research involving algorithmic complexity. What papers do you know of that refer to algorithmic complexity and “runtime”? Context matters. It is incorrect in this context. reply eigenket 26 minutes agorootparentGoing on arXiv computer science and searching for the string \"runtime\" returns 6135 results, many of which seem use it in the sense we're talking about here. https://arxiv.org/search/cs?query=runtime&searchtype=all&abs... In any case Quanta Magazine is not a formal academic journal, and neither is hacker news. We're having an informal discussion about a popular science article. reply WhitneyLand 12 minutes agorootparentYour 6135 results are ignoring that there are two different usages. I said “run time” relates to algorithmic complexity. That’s one definition. The other definition is also valid in research, as it relates to environments. Like Java. The first result in your query is an example of the second definition. It is not an example showing these are interchangeable. reply eigenket 8 minutes agorootparentFine, I edited \"most\" in my previous comment to \"many\". There are still plenty of examples (even on the first page) where the \"runtime\" is used to mean algorithmic complexity https://arxiv.org/abs/2401.14645 https://arxiv.org/abs/2401.13770 https://arxiv.org/abs/2401.12253 https://arxiv.org/abs/2401.12205 https://arxiv.org/abs/2401.10856 GaryNumanVevo 11 minutes agorootparentprevKeeping the HN tradition of \"pedantry at all costs\" alive, I applaud you reply mzl 11 minutes agorootparentprevThe original paper (https://arxiv.org/pdf/2303.14605.pdf) that the linked article reports on is a theoretical algorithms paper, and does not contain any references to actual solvers or implementations. For practical cases it is not that important what the worst-case complexity is, but rather what the expected complexity is of solving problems that occur in practice. reply whyever 29 minutes agorootparentprevIsn't this a stylistic choice? For example, some style guides would suggest to use \"run time\", but also \"run-time error\". reply ur-whale 6 minutes agorootparentprev> Sorry to be pedantic As always, a guaranteed way to make friends. reply junon 43 minutes agorootparentprevWas #1 really worth mentioning...? reply nkh 7 hours agoprevFor now, the new algorithm hasn’t actually been used to solve any logistical problems, since it would take too much work updating today’s programs to make use of it. But for Rothvoss, that’s beside the point. “It’s about the theoretical understanding of a problem that has fundamental applications,” he said. I don't see how \"it would take to much work updating today's programs\". Most domain specific models call out to Gurobi, CPLEX, or FICO solvers for large problems, and open source ones like SCIP for the small ones. There is a standard MPS format where you can run exchange models between all of these solvers, and the formulation of the problem shouldn't change, just the solving approach inside the solver. Can someone enlighten me? I could see if they are arguing, this will require a new implementation, and if so, there is a ton of benefit the world would see from doing so. reply diegoveralli 2 minutes agoparentMaybe what they mean is that, despite an asymptotic advantage, the new algorithm performs worse for many use cases than the older ones. This might be due to the many heuristics that solvers apply to make problems tractable as others have mentioned, as well as good old software engineering optimization. So the work that's required is for someone to take this algorithm and implement it in a way that levels the playing field with the older ones. reply math_dandy 7 hours agoparentprevThe new algorithm of R&R would need to replace the algorithms at the core of Gurobi, CPlex, etc. These tools are marvels of engineering, extremely complex, results of decades of incremental improvements. If would likely take significant research effort to even figure out a way to incorporate the new discoveries into these engines. reply nkh 6 hours agorootparentWhy would it need to replace them? From the article, they claim they have found a way to reduce the upperbound faster when searching large Integer problems. I don't see how that effects the current searching process. All of these solvers you can enter in an upperbound yourself if you have knowledge of the problem and know a previous solution. So it seems if this is just a programmatic way of reducing the upper bound, it should fit right in with current approaches. What am I missing? reply whatyesaid 6 hours agorootparentIt's a research paper. You can write a theoretical paper and let others apply it practically, which others can figure out the practical aspect and report results of benchmarks, or others can also build on the theory. This paper only has 2 authors. The other solvers are probably applying technique specific tricks and speedups, and you're working with approximate optimization, it's not that easy to move everything over. reply hdesh 5 hours agorootparent> This paper only has 2 authors. So? I don't get the relevance of the author count. reply black_puppydog 5 hours agorootparentIt's quite easy to go tell other people what they should do with their time. These researchers are in the business of improving algorithms. Implementing them in large industrial (or open source) code bases in a maintainable way -- and then actually maintaining that code -- is a different skillset, a different set of interestes, and as was pointed out, besides the point. Either you believe their results, then be grateful. Someone (yoU!) can implement this. Or you don't. In which case, feel free to move on. Your tone comes off as entitled. reply bnegreve 3 hours agorootparent> Implementing them in large industrial (or open source) code bases in a maintainable way -- and then actually maintaining that code -- is a different skillset, a different set of interestes, You're making a very general point on how algorithm research and software development are two different things, which is of course true. However OP's question is genuine: a lot of research in OR is very practical, and researchers often hack solvers to demonstrate that whatever idea offers a benefit over existing solving techniques. There are no reason to believe that a good new idea like this one couldn't be demonstrated and incorporated into new solvers quickly (especially given the competition). So the quoted sentence is indeed a bit mysterious. I think it just meant to avoid comment such as \"if it's so good why isn't it used in cplex?\". reply imtringued 2 hours agorootparentprev>business of improving algorithms You do realize that the solver companies are in exactly the same boat, right? reply black_puppydog 1 minute agorootparentno they're not. they're in the business of making their customers' problems solve fast and well. That's of course strongly related, but it is _not_ the same. An algorithm may well be (and this is what OP might be hinting at) be more elegant and efficient, but execute worse on actually existing hardware. benterix 16 minutes agorootparentprevAnd given how much the licenses cost, I'd love a new player to show up and bring them down to a reasonable level. reply unnah 2 hours agorootparentprevI don't think they're talking about a bound for the optimum objective value, but a theoretical upper bound for a covering radius related to a convex body and a lattice. The bound would be useful in a lattice-based algorithm for integer linear programming. I don't think there exists an implementation of a lattice algorithm that is practical for non-toy integer linear programming problems, let alone one that is competitive with commercial ILP solvers. reply math_dandy 6 hours agorootparentprevEvery time an integer feasible point is found during the iterative process these algorithms use (branch and bound), you get a new upper bound on the global minimum. It’s not clear to me how these dynamically generated upper bounds highly specific to the particular problem relate to the upper bounds of a more general nature that R&R produce. reply nkh 5 hours agorootparent> upper bounds of a more general nature that R&R produce If it's an upper bound, it should be pretty easy to plug into the existing stuff under the hood in these solvers. Can you provide my insight into how the R&R \"Upper bound\" is different and \"more general in nature\"? reply raverbashing 1 hour agorootparentprevHonestly? The search for the 'exactly optimal solution' is way overrated I think you can get a moderately efficient solution using heuristics at 1/10 of the time or less Not to mention developer time and trying to figure out which constraints make your problem infeasible. Especially as they get more complicated because you want to make everything linear reply FwarkALark 5 hours agorootparentprev> If would likely take significant research effort to even figure out a way to incorporate the new discoveries into these engines. What? Have you ever used a solver before? The actual APIs exposed to the user are very simple interfaces that should allow swapping out the backend regardless of the complexity. The idea a new algorithm—short of something like \"updating the solution to adjust to a change in data\"—would not require any sort of research to slot in as an implementation for the existing interface. reply adgjlsfhk1 5 hours agorootparentthe interface is simple, but modern solvers apply a ton of heuristics that often dramatically reduce problem size, so a naive implementation of a better algorithm that isn't hooked deeply into the core of an existing ilp solver is likely to be very slow reply FwarkALark 3 hours agorootparentWhy is this exposed to the user? If it isn't exposed to the user, what on earth are you talking about? reply 7thaccount 1 hour agorootparentWhy would the API expose the heuristics to the user? Because an intelligent user can make minor adjustments and turn certain features on/off to sometimes dramatically increase performance depending on the problem. reply rocqua 2 hours agorootparentprevFrom what I gather the parent post is saying that it is easy to make a naive implementation of this improvement, but due to naivety of the implementation it will be slower in practice. Hence it is a lot of work (and thus difficult) to actually put this improvement into practice. reply imtringued 2 hours agorootparentprevThese solvers get faster every year, how exactly are they supposed to stay the world's fastest if people invent better algorithms all the time that never get implemented by the commercial offerings? reply __alexs 1 hour agoparentprevThe open source solvers are a mess of 30 years of PhD students random contributions. It's amazing they work at all. If you can possibly avoid actually implementing anything using them you will. reply rocqua 2 hours agoparentprev> I don't see how \"it would take to much work updating today's programs\". I think some peeps are not reading this sentence the way you meant it to be read. It seems to me you meant \"I don't know what part of this research makes it especially hard to integrate into current solvers (and I would like to understand) \". But people seem to be interpreting \"why didn't they just integrate this into existing solvers? Should be easy (what lazy authors)\". Just trying to clear up some misunderstanding. reply npalli 6 hours agoparentprevYou seem to be confusing problem formulation with the problem solution. It is true there is a standard way to exchange the problem formulation through something like MPS (though it seems AML's like AMPL etc. have taken over). All this format gives you is a standard mathematical formulation of the problem. However, the solution is something very specific to the individual solver and they have their own data structures, algorithms and heuristic techniques to solve the problem. None of these are interchangeable or public (by design) and you cannot just insert some outside numbers in the middle of the solver process without being part of the solver code and having knowledge of the entire process. reply nkh 5 hours agorootparentAll these solvers use branch and bound to explore the solution space and \"fathom\" (i.e. eliminate candidate search trees if the lowest possible value for the tree is above an already found solution). The upper bound that the solver calculates via pre-solve heuristics and other techniques does vary from solver to solver. However, they all have a place for \"Upper bound\", and there are mechanisms in all of these solvers for updating that value in a current solve. If this paper were a complementally orthogonal implementation from everything that exists in these solvers today, if it can produce a new upper bound, faster than other techniques, it should be fairly plug and play. I have an undergrad OR degree, and I have been a practitioner for 18 years in LP/MIP problems. So I understand the current capacities of these solvers, and have familiarity with these problems. However, I and am out of my depth trying to understand the specifics of this paper, and would love to be corrected where I am missing something. reply Aaronmacaron 1 hour agorootparentWhat is OR? reply aix1 1 hour agorootparentOperations Research: https://en.m.wikipedia.org/wiki/Operations_research reply mulmboy 3 hours agorootparentprevIn many cases you can actually insert outside numbers in the middle of the solver process via callbacks. For example see IncumbentUpdater at https://python-mip.readthedocs.io/en/latest/classes.html And various C APIs for solvers have other callbacks It's generally quite limited of course, for the reasons you mentioned. reply 7thaccount 1 hour agorootparentprevThe math programming languages of AMPL, AIMMS, GAMS...etc are dying in my industry and being replaced by general industry languages like Python/Java + Solver API. reply soperj 6 hours agorootparentprevWouldn't the \"open source ones like SCIP for the small ones.\" be public by design? reply laserbeam 3 hours agoparentprevI honestly think that's just journalism for \"no one implemented it in production yet\". Which is not surprising, for an algorithm less than a year old. I don't think it's worth expanding and explaining \"too much work\". That being said, sometimes if an algorithm isn't the fastest but it's fast and cheap enough, it is hard to argue to spend money on replacing it. Which just means that will happen later. Furthermore, you might not even see improvements until you implement an optimized verision of a new algorithm. Even if big O notation says it scales better... The old version may be optimized to use memory efficiently, to make good use of SIMD or other low level techniques. Sometimes getting an optimized implementation of a new algorithm takes time. reply xkcd386 1 hour agoparentprevThe randomized algorithm that Reis & Rothvoss [1] present at the end of their paper will not be implemented in Gurobi/CPLEX/XPRESS. It remains a fantastic result regardless (see below). But first let me explain. In terms of theoretical computational complexity, the best algorithms for \"integer linear programming\" [2] (whether the variables are binary or general integers, as in the case tackled by the paper) are based on lattices. They have the best worst-case big-O complexity. Unfortunately, all current implementations need (1) arbitrary-size rational arithmetic (like provided by gmplib [3]), which is memory hungry and a bit slow in practice, and (2) some LLL-type lattice reduction step [4], which does not take advantage of matrix sparsity. As a result, those algorithms cannot even start tackling problems with matrices larger than 1000x1000, because they typically don't fit in memory... and even if they did, they are prohibitively slow. In practice instead, integer programming solver are based on branch-and-bound, a type of backtracking algorithm (like used in SAT solving), and at every iteration, they solve a \"linear programming\" problem (same as the original problem, but all variables are continuous). Each \"linear programming\" problem could be solved in polynomial time (with algorithms called interior-point methods), but instead they use the simplex method, which is exponential in the worst case!! The reason is that all those linear programming problems to solve are very similar to each other, and the simplex method can take advantage of that in practice. Moreover, all the algorithms involved greatly take advantage of sparsity in any vector or matrix involved. As a result, some people routinely solve integer programming problems with millions of variables within days or even hours. As you can see, the solver implementers are not chasing the absolute best theoretical complexity. One could say that the theory and practice of discrete optimization has somewhat diverged. That said, the Reis & Rothvoss paper [1] is deep mathematical work. It is extremely impressive on its own to anyone with an interest in discrete maths. It settles a 10-year-old conjecture by Dadush (the length of time a conjecture remains open is a rough heuristic many mathematicians use to evaluate how hard it is to (dis)prove). Last november, it was presented at FOCS, one of the two top conferences in computer science theory (together with STOC). Direct practical applicability is besides the point; the authors will readily confess as much if asked in an informal setting (they will of course insist otherwise in grant applications -- that's part of the game). It does not mean it is useless: In addition to the work having tremendous value in itself because it advances our mathematical knowledge, one can imagine that practical algorithms based on its ideas could push the state-of-the-art of solvers, a few generations of researchers down the line. At the end of the day, all those algorithms are exponential in the worst case anyways. In theory, one would try to slightly shrink the polynomial in the exponent of the worst-case complexity. Instead, practitioners typically want to solve one big optimization problems, not family of problems of increasing size n. They don't care about the growth rate of the solving time trend line. They care about solving their one big instance, which typically has structure that does not make it a \"worst-case\" instance for its size. This leads to distinct engineering decisions. [1] https://arxiv.org/abs/2303.14605 [2] min { c^T x : A x >= b, x in R^n, some components of x in Z } [3] https://gmplib.org/ [4] https://www.math.leidenuniv.nl/~hwl/PUBLICATIONS/1982f/art.p... reply Bimos 25 minutes agorootparentThanks for your information. I think it really bridge the gap between the people who are interested in this algorithm and MILP \"users\". I have two more questions. 1. Usually we deal with models with both integer and continuous variables (MILP). Conceptually B&B tackles ILP and MILP in similar ways. Is there any difficulty for lattice based method to be extended to solve MILP? 2. How likely do you think this lattice type algorithm will overcome the difficulties you mentioned and eventually replace B&B, totally or partly (like barrier vs simplex methods)? reply ubj 7 hours agoprevMinor nitpick, but the title of this submission should specify \"Integer Linear Programming\", since the integer part is a much bigger deal. Polynomial time algorithms have been known for linear programming for decades; _integer_ linear programming is NP-hard. reply eru 4 hours agoparentYou are right that integer linear programming is NP-hard; but faster algorithms for continuous linear programming are also super interesting and impactful. Continuous linear programming is also _hard_. Not in the sense of NP-hard, but in the sense of there being lots of algorithmic and engineering aspects that go into an efficient, modern LP solver. Even just the numerics are complicated enough. (And many integer linear programming solvers are based on continuous linear programming solvers.) reply ubj 3 hours agorootparentTrue, these are all fair points! I didn't intend to diminish the impact or complexity of linear programming solvers. Well-written solvers are some of the most useful and powerful computational tools that exist today. reply dang 4 hours agoparentprevI think we fixed that, albeit by accident when I edited the title earlier. If it needs further fixing let me know! reply ford 7 hours agoprevSoftware engineers interested in ML/algorithms should learn about linear programming. It's surprising how many problems can be formulated as linear optimization. For example, in college I was talking to my Industrial Engineer friend about the average minimum number of swaps required to place billiards balls in an acceptable starting position in the rack (triangle). We both happened to write programs that used monte-carlo sampling to solve it - but my solution did BFS on the state space of a graph, and his used linear programming (which was _probably_ more efficient) reply mp05 5 hours agoparentI foresee a future where industrial engineering and CS are combined into some super-degree. There is currently a surprising amount of overlap in the OR side of things, but I'm shocked by how few IE grads can program their way out of a box. It's a shame, really. reply ur-whale 0 minutes agorootparent> but I'm shocked by how few IE grads can program their way out of a box. It's a shame, really. These days, you could replace the \"IE\" in your sentence by any of many, many disciplines and still be correct. As much as mathematicians will hate to hear this, CS is a new and more tangible/practical way to do maths and should therefore hold a spot in a general education as central as maths has in the last few centuries. reply 7thaccount 1 hour agorootparentprevOperations Research is basically Industrial Engineering + Mathematical Optimization + programming familiarity. It's super useful. reply maxFlow 4 hours agorootparentprevCS already is the super-degree. reply fuzztester 4 hours agorootparentHow so? reply axus 4 hours agorootparentIt qualifies you for an opinion on any subject. reply shermantanktop 2 hours agorootparentA CS degree also qualifies you for on-the-job training in writing code, that odious task that your professors find trivial but somehow are also terrible at it. reply Al-Khwarizmi 1 hour agorootparentWe just don't have time. Incentives are elsewhere. Any time devoted to writing good code for a paper is time we cannot use to work on the next paper, (shudder) grant application, or a plethora of other things that we are either forced or incentivized to do. I miss coding from when I was in a more junior stage of my career and could afford time for it, and I think my fellow professors mostly feel the same, I don't think many would dismiss it as trivial or odious. reply tylerhou 5 hours agoparentprevILP is NP-complete. reply BlindEyeHalo 1 hour agorootparentJust because it is NP-hard in the worst-case doesn't mean it is not practical. As can be seen in the many theorems under which conditions the regular polynomial-time LP algorithm provides an integer solution. reply eru 4 hours agorootparentprevYes? We do manage to solve ILP problems in practice quite nicely. In fact, most NP problems that you come across in practice are relatively tractable for most practical instances. Eg for the knapsack problem you have to actually work very hard to get a hard instance in the first place. reply imtringued 2 hours agorootparentThat's not correct. First of all, you can't solve a neoclassical economy using LP, because equilibrium constraints can only be represented as complementarity constraints. You would have to give up on some aspects, like dynamic prices. The linear complementarity problem in itself is NP hard. So you're screwed from the get go, because your problems are now LPCC problems. Good luck finding an LPCC solver. I can confirm that an open source QPCC solver exists though, which should be even slower. Next is the fact that if you wanted to build a neoclassical economy model, only global optimization will do. This means that you need to simulate every time step in one large LPCC model, instead of using a finite horizon. Due to the perfect information assumption, you must know about the state of every person on the planet. You're going to need millions of variables due to simple combinatorial explosion. It's kind of startling how these assumptions, which are supposed to make analytical solutions tractable by the way, also make non-analytical solutions literal hell. And before you say that prices can be determined iteratively, as I mentioned, you would run into the problem that future prices are unknown to you, so how are you going to plug them into the second time step? The very thing you want to calculate depends on it's future value. Economics is a weird science, where experienced reality works much better than the theory. reply 7thaccount 1 hour agorootparentComputational economics is a relatively new field where intelligent agents are used with lots of runs instead of general optimization solvers I believe. Pretty nifty. One of my colleagues publishes a good bit on it. reply adgjlsfhk1 4 hours agorootparentprevit's not. it's np-hard. the easiest proof is that the best known algorithm is greater than O(2^N) reply PartiallyTyped 42 minutes agoparentprevOne of my favourite courses in grad school was approximation algorithms and it involved reductions to LP. Lots of fun, can recommend. reply Duanemclemore 7 hours agoprevGreat short article. I haven't looked deeply into the math behind this yet, but this looks to be a preprint [0]. It doesn't appear they're looking directly at the Space Groups as a way to reduce out any symmetries or repetitions that may occur (thus generalizing simplifications of the problem \"space\"), but it would be interesting to see whether those structures apply or not. I say this as someone who writes software to apply the Space Groups and describe the Voronoi cells around points (or groups of points) distributed through them, so I'm familiar with the \"uncanny\" ways effects propagate. [1] I'm also not a mathematician (just a lowly architect), so I'm way out of my depth here. But it's fascinating and as someone looking at paths across these generated honeycombs, this result bears more investigation for me as well. [0] https://arxiv.org/pdf/2303.14605.pdf [1] If you know a mathematician who might be interested in collaborating on this kind work, ping me. This is ongoing work, and as I said I'm out of my depth mathematically. But have run into some interesting properties that don't seem that deeply investigated which may bear deeper study by an actual expert. reply fuidani 7 hours agoprevAbout the travelling salesperson problem, below is a quote from the latest Sapolsky's book Determined: A Science of Life without Free Will. I am not sure how relevant this is for software developers, but still fascinating: \"An ant forages for food, checking eight different places. Little ant legs get tired, and ideally the ant visits each site only once, and in the shortest possible path of the 5,040 possible ones (i.e., seven factorial). This is a version of the famed “traveling salesman problem,” which has kept mathematicians busy for centuries, fruitlessly searching for a general solution. One strategy for solving the problem is with brute force— examine every possible route, compare them all, and pick the best one. This takes a ton of work and computational power— by the time you’re up to ten places to visit, there are more than 360,000 possible ways to do it, more than 80 billion with fifteen places to visit. Impossible. But take the roughly ten thousand ants in a typical colony, set them loose on the eight- feeding- site version, and they’ll come up with something close to the optimal solution out of the 5,040 possibilities in a fraction of the time it would take you to brute- force it, with no ant knowing anything more than the path that it took plus two rules (which we’ll get to). This works so well that computer scientists can solve problems like this with “virtual ants,” making use of what is now known as swarm intelligence.\" reply jcranmer 6 hours agoparentThere's been more than a few of these \"nature solves NP-hard problems quickly!\" kinds of stories, but usually, when one digs deeper, the answer is \"nature finds local optima for NP-hard problems quickly!\" and the standard response is \"so does pretty trivial computer algorithms.\" In the case of TSP, when you're trying to minimize a TSP with a Euclidean metric (i.e., each node has fixed coordinates, and the cost of the path is the Euclidean distance between these two points), then we can actually give you a polynomial-time algorithm to find a path within a factor ε of the optimal solution (albeit exponential in ε). reply pas 6 hours agorootparenthttps://scottaaronson.blog/?p=266 \"\"\" I went to the hardware store, bought some glass plates, liquid soap, etc., and found that, while Nature does often find a minimum Steiner tree with 4 or 5 pegs, it tends to get stuck at local optima with larger numbers of pegs. \"\"\" reply defrost 5 hours agorootparent\"Did he try jiggling it a bit, and then less and less and less?\" ( Annealing /s ) reply gregod 4 hours agoparentprevThe Evolutionary Computation Bestiary [1] list a wide variety of animal behavior inspired heuristics. The foreword includes this great disclaimer: \"While we personally believe that the literature could do with more mathematics and less marsupials, and that we, as a community, should grow past this metaphor-rich phase in our field’s history (a bit like chemistry outgrew alchemy), please note that this list makes no claims about the scientific quality of the papers listed.\" [1]: https://fcampelo.github.io/EC-Bestiary/ reply FredPret 6 hours agoparentprevIf you try to make your path close to a circle, it’s obviously not guaranteed to be optimal, but it’ll probably be close enough for most small practical applications reply whatever1 5 hours agoprevIt’s great result but probably not useful. Similarly to how interior point methods have better theoretical complexity than simplex for LPs, but fine tuned simplex in reality almost always wins. reply klysm 7 hours agoprevSo many discrete optimization problems can be translated into linear programs. It's a really powerful set of tools to know, kind of like SAT solvers. reply idatum 7 hours agoparentI only recently learned about linear programming. I started with PuLP and Python to get a grasp. It was one of those \"How did I miss this??\" moments as a developer. reply ByteMe95 6 hours agorootparentDo you have any recommendations on where to start? reply mp05 5 hours agorootparentWinston's \"Operations Research: Applications and Algorithms\" is the authority so far as I can tell. Trivial to find old editions online. reply idatum 6 hours agorootparentprevI wish I can remember how I even learned LP tools existed. I started with this: https://coin-or.github.io/pulp/ reply eru 4 hours agorootparentThe Google OR-tools library is also a good starting point. I learned about linear programming in uni, but alas I don't think a mathematician's course on linear programming would be a good starting point for practical programmers. reply graycat 3 hours agorootparentprevFor positive integers m and n, have a m x n matrix A of real numbers. Then also have n x 1 x, 1 x n c, and m x 1 b. Seek x to solve LP1: maximize z = cx subject to Ax = b x >= 0 Instead just as easily can do minimize. Instead of =, might be given >= and/or = 0 is feasible. If there is such an x, then LP1 is feasible; else LP1 is infeasible. If LP1 is feasible and for any feasible x we have z bounded above, then LP1 is bounded and has an optimal x (z as large as possible) solution. Else feasible LP1 is unbounded above. So, LP1 is feasible or not. If feasible, then it is bounded or not. If bounded, then there is at least one optimal solution. Regard n x 1 x as a point in R^n for the real numbers R. Cute: If all the numbers in LP1 are rational, then have no need for the reals. The set of all feasible x is the feasible region and is closed (in the usual topology of R^n) and convex. If LP1 is bounded, then there is at least one optimal x that is an extreme point of the feasible region. So, it is sufficient to look only at the extreme points. To determine if LP1 is feasible or not, and if feasible bounded or not, and if bounded to find an optimal x, can use the simplex algorithm which is just some carefully selected linear algebra elementary row operations on z = cx Ax = b The iterations of the simplex algorithm have x move from one extreme point to an adjacent one and as good or better on z. A LOT is well known about LP1 and the simplex algorithm. There is a simpler version for a least cost network flow problem where move from one spanning tree to another. If insist that the components of x be integers, then are into integer linear programming and the question of P = NP. In practice there is a lot known about ILP, e.g., via G. Nemhauser. I used to teach LP at Ohio State -- there are lots of polished books from very elementary to quite advanced. I attacked some practical ILP problems successfully. I got into ILP (set covering, a darned clever idea since get to handle lots of goofy, highly non-linear constraints, costs, etc. efficiently) for scheduling the fleet at FedEx. The head guy at FedEx wrote me a memo making that problem my work -- officially I reported to the Senior VP Planning, but for that ILP work in every real sense reported to the head guy. The promised stock was very late, so I went for a Ph.D. and got good at lots of math, including optimization, LP, and ILP, etc. Conclusion: A career in LP or ILP is a good way to need charity or sleep on the street -- literally, no exaggeration. For some of what AI is doing or trying to do now, LP/ILP stands to be tough competition, tough to beat. And same for lots more in the now old applied math of optimization. Bring a strong vacuum cleaner to get the thick dust off the best books. reply andrewp123 3 hours agoprevIt seems their result has been out for almost a year now... https://arxiv.org/abs/2303.14605 I'm curious how this affects Traveling Salesman. I was under the impression that all NP-Complete problems take O(n!). Does this method improve it at all? reply Tarean 26 minutes agoparentOften, the concrete problems we are interested in have some internal structure that make them easier to solve in practice. Solving Boolean formulas is NP-complete but we routinely solve problems with millions of variables. ILP (and sat) solvers are interesting because they are great at ruling out large areas that cannot contain solutions. It's also easy to translate many problems into ILP or SAT problems. reply blackbear_ 3 hours agoparentprevDepending on the problem it can also O(2^n), but that is always the worst case scenario. Modern ILP solvers employ a variety of heuristics that in many cases significantly reduce the time needed to find a solution. Anecdotally, some years back I was developing MILPs with millions of variables and constraints, and most of them could be solved within minutes to hours. But some of them could not be cracked after weeks, all depending the inputs. reply adrianN 3 hours agoparentprevWe actually don't know how long NP-complete problems take to solve. We conjecture that it's superpolynomial, but that can be exponentially faster than O(n!). reply yau8edq12i 3 hours agoparentprevSo what? It takes time for the community to digest the result, grasp its significance, and then write popularization articles about it. If you want to know what's being discovered right this second, read arXiv preprints. If you want to know what was discovered semi-recently and you want an explanation in layman terms that puts the results in perspective, read popularization pieces a while later. reply rurban 3 hours agoprevSo this is for the special case of non-negative and non-zero weights only, right? But those cases are the only sane ones, avoiding recursive loops winning a time-travel-alike race. reply fuzztester 4 hours agoprevI remember the news articles when Karmarkar's algorithm for linear programming was announced. https://en.wikipedia.org/wiki/Narendra_Karmarkar https://en.wikipedia.org/wiki/Karmarkar%27s_algorithm reply coliveira 6 hours agoprevWhile this is an interesting theoretical result, we need to remember that they found an algorithm that is (log n)^O(n). In other words, this is not practical to solve problems with moderate to large size n. reply adgjlsfhk1 5 hours agoparentcompared to the previous bound of n^n, log(n)^n looks pretty good. reply Bimos 7 hours agoprevI have a dumb question: how long will it take before this result becoming a pratical MIP solver beating SCIP or gurobi? reply ubj 3 hours agoparentDon't forget about the HiGHS solver [1]. MIT licensed and getting to the point where it's outperforming SCIP on the Mittelmann benchmarks [2]. [1]: https://github.com/ERGO-Code/HiGHS [2]: https://mattmilten.github.io/mittelmann-plots/ reply FreakLegion 1 hour agorootparentHiGHS is more of an alternative to Bonmin and Minotaur than Couenne and SCIP. In my experience though the presolvers in SCIP are extremely dangerous, and it's easy to end up with a local optimum even when that isn't your goal. reply _dark_matter_ 7 hours agoparentprevCouldn't either of those implement this algorithm? reply genman 6 hours agoprev[2023] The paper was uploaded first in March https://arxiv.org/abs/2303.14605 reply ken47 7 hours agoprevThis discovery may change the world in unpredictable and, perhaps very big, ways. Discoveries like this put all the self-important feature / model developers that we work with in our big tech day jobs into context. reply CyberDildonics 7 hours agoparentCan you explain specifically what about it you think will change the world and why? reply aaron695 7 hours agoprevLinear programming is very cool, I loved Vasek Chvatal's book as a kid having accidently bought it thinking it was for computers. But it's tricky to understand and implement and it struggles with real life constraints. i.e. This whole specialty just for integers. Monto Carlo is trivial to understand and implement, adapts to changes and constraints trivially and should be just as good. I'm sure for something high end like chip design you will do both. I'd be surprised to hear of real life stories where linear programming beats Monty Carlo. reply eru 4 hours agoparent> But it's tricky to understand and implement and it struggles with real life constraints. i.e. This whole specialty just for integers. Integers are actually harder to deal with than rational numbers in linear programming. Many solvers can also deal with a mixed problem that has both rational and integer variables. Monte Carlo simulations are an entirely different beast. (Though you probably mean simulated annealing? But that's close enough, I guess. Linear programming is an optimization technique. Monte Carlo by itself doesn't have anything to do with optimization.) One problem with these other approaches is that you get some answer, but you don't know how good it is. Linear programming solvers either give you the exact answer, or otherwise they can give you a provable upper bound estimate of how far away from the optimal answer you are. reply anon291 4 hours agoparentprevLinear programming on reals is \"easy\"... You can just check all the points. I believe you can follow the shell of the legal polytope and just use a greedy algorithm to choose the next point that will minimize your goal. If you can get away with a continuous linear program I don't see why you'd use monte carlo. The simplex method will get you an exact answer. reply CyberDildonics 7 hours agoprev [–] People really need to come up with better names. \"Linear Programming\" or \"Integer Linear Programming\" mean absolutely nothing. Also anything dealing with finding the minimum distance distances can be short circuited by keeping the shortest distance and not taking paths that exceed that. This is how approximate nearest neighbor works and can still speed up the full solution. Figuring out full paths that have short average distances first can also get to shorter distances sooner. You can also cluster points knowing you probably don't want to jump from one cluster to another multiple times. reply eru 4 hours agoparentLinear programming solvers use lots of heuristics (not entirely unlike the ones you sketched) internally. The important thing to keep in mind is that their heuristic only speed up the amount of time spent finding the optimal solution. But you still get a prove at the end, that they actually found the optimal solution. (Or if you stop earlier, you get a provable upper bound estimate of how far you are away at worst from the optimal solution.) Heuristics like the ones you sketched don't (easily) give you those estimates. reply ford 7 hours agoparentprevThis made me wonder why it's called programming (since clearly it's not the sense of the word programming most HN'ers are used to). https://en.wikipedia.org/wiki/Mathematical_optimization#Hist... reply dang 7 hours agorootparentFrom that link: Programming in this context does not refer to computer programming, but comes from the use of program by the United States military to refer to proposed training and logistics schedules, which were the problems Dantzig studied at that time Is that also true of 'dynamic programming'? reply mturmon 5 hours agorootparentIf you don’t know, you are in for a treat. Here is Bellman’s own description of how he came up with the term “dynamic programming “ — I spent the Fall quarter (of 1950) at RAND. My first task was to find a name for multistage decision processes. An interesting question is, ‘Where did the name, dynamic programming, come from?’ The 1950s were not good years for mathematical research. We had a very interesting gentleman in Washington named Wilson. He was Secretary of Defense, and he actually had a pathological fear and hatred of the word, research. I’m not using the term lightly; I’m using it precisely. His face would suffuse, he would turn red, and he would get violent if people used the term, research, in his presence. You can imagine how he felt, then, about the term, mathematical. The RAND Corporation was employed by the Air Force, and the Air Force had Wilson as its boss, essentially. Hence, I felt I had to do something to shield Wilson and the Air Force from the fact that I was really doing mathematics inside the RAND Corporation. What title, what name, could I choose? In the first place I was interested in planning, in decision making, in thinking. But planning, is not a good word for various reasons. I decided therefore to use the word, ‘programming.’ I wanted to get across the idea that this was dynamic, this was multistage, this was time-varying—I thought, let’s kill two birds with one stone. Let’s take a word that has an absolutely precise meaning, namely dynamic, in the classical physical sense. It also has a very interesting property as an adjective, and that is it’s impossible to use the word, dynamic, in a pejorative sense. Try thinking of some combination that will possibly give it a pejorative meaning. It’s impossible. Thus, I thought dynamic programming was a good name. It was something not even a Congressman could object to. So I used it as an umbrella for my activities (Autobiography, p. 159). See: https://pubsonline.informs.org/doi/pdf/10.1287/opre.50.1.48.... People up-thread have been getting cranky about the use of “programming “ in this sense. Now of course, “programming“ for optimization has been well-entrenched since the 1970s at least. But perhaps Bellman’s story does give some cover to those who feel the word “programming“ has been wrongly appropriated? reply dang 2 hours agorootparentOh gosh—I was vastly out of the loop: https://hn.algolia.com/?dateRange=all&page=0&prefix=true&que... Thanks! That's a classic for sure. reply kevindamm 7 hours agoparentprevabsolutely nothing? you mean other than the relationship with linear systems and linear algebra? reply cmrx64 7 hours agoparentprev [–] take \"programming\" to mean \"scheduling\" and the ancient crusty term acquires some meaning. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Researchers have created a groundbreaking algorithm that can solve integer linear programming problems much faster than previous methods.",
      "The algorithm is a significant improvement and brings the runtime close to the theoretical ideal, but it is not yet practical to use in existing programs due to their complexity.",
      "This development is a major advancement in the field and has the potential for future improvements."
    ],
    "commentSummary": [
      "Researchers have developed a faster approach for performing integer linear programming, but its real-world application and impact are uncertain due to other factors like heuristics and strategies that also influence solver performance.",
      "The discussion revolves around the challenges and potential advantages of integrating new techniques into existing algorithms, including the usage of lattice-based algorithms, branch-and-bound methods, and linear programming to solve complex problems.",
      "The primary focus is on optimization and linear programming techniques, their limitations, and their potential for practical applications."
    ],
    "points": 294,
    "commentCount": 108,
    "retryCount": 0,
    "time": 1706577581
  },
  {
    "id": 39176297,
    "title": "Amazon and iRobot's $1.7 billion acquisition deal canceled due to regulatory issues",
    "originLink": "https://www.cnbc.com/2024/01/29/amazon-terminates-irobot-deal-vacuum-maker-to-lay-off-31percent-of-staff.html",
    "originBody": "SKIP NAVIGATION MARKETS BUSINESS INVESTING TECH POLITICS CNBC TV INVESTING CLUB PRO MAKE IT SELECT USA INTL WATCH LIVE Search quotes, news & videos WATCHLIST SIGN IN TECH Amazon terminates iRobot deal, Roomba maker to lay off 31% of staff PUBLISHED MON, JAN 29 20248:37 AM ESTUPDATED MOMENTS AGO Rohan Goswami @IN/ROHANGOSWAMICNBC/ @ROGOSWAMI KEY POINTS Amazon and iRobot mutually agreed to call off their planned acquisition, writing that there was \"no path\" to regulatory approval and sending iRobot shares down sharply. iRobot, which makes the Roomba, said it will lay off around 350 employees and that its founder and CEO Colin Angle would step down. The Amazon-iRobot deal was originally valued at $1.7 billion, but a number of regulatory examinations drove the purchase price down before ultimately killing the deal. In this article IRBT AMZN Follow your favorite stocks CREATE FREE ACCOUNT The iRobot headquarters in Bedford, Massachusetts, US, on Friday, June 16, 2023. Amazon.com Inc.'s proposed $1.7 billion deal to buy robot vacuum firm iRobot Corp. was given the all-clear by the UKs antitrust agency. Photographer: Sophie Park/Bloomberg via Getty Images BloombergBloombergGetty Images Amazon said on Monday it would not move forward with a planned acquisition of vacuum-maker iRobot , with the two companies saying in a release there was \"no path to regulatory approval for the deal.\" The Roomba maker also announced it would lay off 31% of its employees, around 350 people, and that its chair and CEO, Colin Angle, would step down effective immediately. Shares of iRobot fell 10% in morning trading on the news. The fate of the deal was plunged into uncertainty after The Wall Street Journal reported that the European Union would not offer regulatory approval. The European Commission, the executive body of the EU, launched a probe in July, saying that the proposed deal could result in Amazon hindering iRobot rivals from competing on Amazon's online marketplace. The commission argued that Amazon could delist or reduce rival products' prominence in search results or elsewhere. \"Our in-depth investigation preliminarily showed that the acquisition of iRobot would have enabled Amazon to foreclose iRobot's rivals by restricting or degrading access to the Amazon Stores,\" Margrethe Vestager, the European Commission's executive vice president, said in a statement. She added that Amazon's control over the marketplace \"could have restricted competition in the market for robot vacuum cleaners, leading to higher prices, lower quality, and less innovation for consumers.\" \"We're disappointed that Amazon's acquisition of iRobot could not proceed,\" David Zapolsky, senior vice president and general counsel at Amazon, said in a release. iRobot said it would focus on margin improvements, reduce spending on research and development, and pause all work on \"non-floorcare\" products, including its air purifiers and robotic lawn mowers. \"The termination of the agreement with Amazon is disappointing, but iRobot now turns toward the future with a focus and commitment to continue building thoughtful robots and intelligent home innovations that make life better,\" iRobot's Angle said in a release. Amazon will pay iRobot a previously agreed upon $94 million breakup fee. The terminated deal, first announced in 2022, would have originally valued iRobot at roughly $1.7 billion. The robotic vacuum maker has a market capitalization of under $400 million, following Monday's news and prior reports that the EU would move to block the deal. In July, iRobot entered into a $200 million financing facility from the Carlyle Group, in order to fund the company's operations as a stopgap until the Amazon deal closed. Amazon declined to provide a comment beyond the release. Regulators around the world have moved to scrutinize large technology companies, citing potential anti-competitive effects. Amazon is also one of the subjects of a Federal Trade Commission inquiry into the investments and partnerships between Big Tech and artificial intelligence developers such as Anthropic and OpenAI. In Europe, both Britain's Competition and Markets Authority and the EU's European Commission have delayed or halted several deals. Those include Meta 's acquisition of Giphy, Adobe 's terminated acquisition of Figma and Microsoft's investment in OpenAI, as well as Microsoft's purchase of Activision Blizzard. — CNBC's Annie Palmer contributed reporting. WATCH: Amazon-iRobot deal a 'no-brainer' WATCH NOW VIDEO04:10 Amazon's acquisition of iRobot a no-brainer for company's robotics plans, says WSJ's Stern Don't miss these stories from CNBC PRO: Forget the 'Magnificent 7,' these Nasdaq stocks are next in line to lead the rally, according to the charts Nvidia is now 'deeply overbought' and due for 'consolidation,' says chart analyst Eli Lilly's Zepbound is off to a strong start, but here's what needs to happen to push shares higher Investors are shifting into this type of bond fund at the fastest pace in three years Subscribe to CNBC PRO Subscribe to Investing Club Licensing & Reprints CNBC Councils Select Personal Finance CNBC on Peacock Join the CNBC Panel Supply Chain Values Select Shopping Closed Captioning Digital Products News Releases Internships Corrections About CNBC Ad Choices Site Map Podcasts Careers Help Contact News Tips Got a confidential news tip? We want to hear from you. GET IN TOUCH CNBC Newsletters Sign up for free newsletters and get more CNBC delivered to your inbox SIGN UP NOW Get this delivered to your inbox, and more info about our products and services. Advertise With Us PLEASE CONTACT US Privacy Policy CA Notice Terms of Service © 2024 CNBC LLC. All Rights Reserved. A Division of NBCUniversal Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis. Market Data Terms of Use and Disclaimers Data also provided by",
    "commentLink": "https://news.ycombinator.com/item?id=39176297",
    "commentBody": "Amazon and iRobot call off their planned acquisition (cnbc.com)280 points by bitsage 20 hours agohidepastfavorite418 comments nerdjon 14 hours agoI have some conflicting feelings on this, the CEO stepping down and the layoffs as soon as this was announced makes it seem like this was/is a company on the verge of shutdown or at least having serious problems. At what point should an acquisition be allowed on the sake of something being able to continue to exist and possibly save jobs? Sure there would have almost guaranteed be job cuts with the acquisition due to redundancy, but would it have been the same amount? However on the flip side, it feels like iRobot has been stagnating for years and entering some weird categories. I still fail to see why they entered the air purifier market and them selling a stick vacuum next to their iRobot is sure one way to say \"our expensive robot doesn't do everything we claim it does\". I finally ditched my iRobot for a Roborock a couple months ago and it's been amazing. It is shocking how much better it is, when it was in the middle of a clean and I could tell it to go start a different clean and it just did it? It didn't complain or anything, I shouldn't be surprised by this but after the experience with iRobot feeling like it stands in my way every time this felt like magic. It genuinely makes me sad to see iRobot not be what they used to be, it feels like they got complacent with Roomba. reply stackskipton 13 hours agoparent>At what point should an acquisition be allowed on the sake of something being able to continue to exist and possibly save jobs? Sure there would have almost guaranteed be job cuts with the acquisition due to redundancy, but would it have been the same amount? IMO, probably never. I think this would open too many shenanigans of running a possible acquisition into the ground or attempt to lobby whatever government agency would regulate this. At some point, to make rules easy to enforce, you have to absorb some collateral damage. reply thatsjustwrong 13 hours agorootparentUS antitrust law has a failing company doctrine. If you can show that absent the merger, the company would almost certainly fail, and no other likely purchaser exists, then you have the right to buy it regardless of any competitive concerns. reply notatoad 10 hours agorootparent>and no other likely purchaser exists this seems like the important part of that doctrine, and a perfect justification for why an amazon acquisition shouldn't be allowed. letting a company fail really just means letting it sink far enough that some other acquirer will pick it up. reply dangerlibrary 10 hours agorootparentOr it'll just fade into oblivion, like Convoy. reply throwaway2037 6 hours agorootparentThis Convoy? https://en.wikipedia.org/wiki/Convoy_(company) If yes, it sounds rough! > On 19th October 2023, Convoy ceased operations and laid off remaining staff. Remaining staff were given no severance and were told their stock options were worthless.[9] In a memo sent that day to employees, Lewis points to \"a massive freight recession and a contraction in the capital markets\" as major factors resulting in the company's failure.[10] reply techie128 5 hours agorootparentprevNothing fades into oblivion. The company is obligated to liquidate its assets which includes IP. This gives a big opportunity to build new products that may be more economically viable. This would not be possible if the company would be acquired by the incumbent who will just acquire the company’s IP and sit on it. reply ben_w 54 minutes agorootparent> The company is obligated to liquidate its assets which includes IP \"We have no idea who owns this IP in order to ask for permission, because the company went bankrupt\" comes up fairly often in discussions about copyright duration and video games. reply gms 11 hours agorootparentprevDo you know why this didn't apply regarding JetBlue trying to acquire Spirit? reply bobthepanda 10 hours agorootparentSpirit being about to \"certainly fail\" is debatable, and JetBlue is not the only willing purchaser of Spirit. One outcome is Spirit goes into bankruptcy reorganization and still operates, which is hardly unprecedented for an American airline. Nearly every major airline has filed for Chapter 11 since 2002, the lone exception being Southwest. reply throwaway2037 6 hours agorootparentSmall note: JetBlue was started in 1999/2000, so it would be included on the \"exception\" list. But overall, your point stands. Chapter 11 re-org is bizarrely common in US airlines. Warren Buffett has many funny quotes about the terrible return on investment for US airlines -- both debt and equity. reply basch 4 hours agorootparent>Delta entered Chapter 11 on Sept. 14, 2005, amid high fuel prices and the burdens of high labor and pension expenses. Delta significantly reduced its labor and pension costs while under court protection. It’s always interesting that Chapter 11 is a way out of pension promises. That someone can take employment at a certain wage, and then the company can renege on the back half of the compensation once the person retires. reply lotsofpulp 1 hour agorootparentHence one should be wary of accepting the promise to be paid decades in the future by anyone other than the US federal government, or much more regulated entities like insurance companies. If the payer is not US federal government, stick to broad market index funds in 401k/IRA. reply philwelch 4 hours agorootparentprev> Chapter 11 re-org is bizarrely common in US airlines. Philip Greenspun has a good writeup about why that is: https://philip.greenspun.com/flying/unions-and-airlines reply edgyquant 11 hours agorootparentprevThat sounds terrible and should change ASAP reply cortesoft 9 hours agorootparentWhy? How is market competition or the public served by forcing a company to go out of business instead of being acquired? The end result is the same (no competitor), but overall productivity is hurt. Why is that better for the public? reply ironmagma 7 hours agorootparentGoing bankrupt doesn't necessarily mean going out of business. Also, as another commenter said, \"letting a company fail really just means letting it sink far enough that some other acquirer will pick it up.\" reply throwaway2037 6 hours agorootparentGreat point: In US bankruptcy law, \"Chapter 7\" is liquidation, and \"Chapter 11\" is re-org. Often, a well-managed Chapter 11 bankruptcy can allow a company to reduce debt burden and emerge as a stronger company, saving many jobs in the process. Chapter 11 bankruptcy is common in the US airline industry. reply bombcar 5 hours agorootparentChapter 11 is basically finding a lower bidder - the lenders are willing to take ownership of the failing company. reply troupo 4 hours agorootparentprev> How is market competition or the public served by forcing a company to go out of business instead of being acquired How is market competition or the public served by companies whose only strategy is to fail and be bought by the ever shrinking number of ultra rich mega corporations? reply bobthepanda 10 hours agorootparentprevIs it? T-Mobile was cleared to buy Sprint because Sprint was not able to be a going concern. The end result is the same, with 3 national cell carriers. reply philwelch 4 hours agorootparentprevWhat if it’s only likely to fail as a consequence of your own anticompetitive actions? reply troupo 4 hours agorootparentprevSo that's why most \"innovative\" startups in the US (apart from those bleeding billions dollars a year) never seek profit and hope to be purchased reply i_am_jl 13 hours agorootparentprevI was really hoping this was a novelty account that just went around HN threads, calling out misinformation. reply Nullabillity 12 hours agorootparentDid we read the same post? \"The US implements X\" doesn't counter \"X is a terrible idea\" at all. reply hn_throwaway_99 8 hours agoparentprev> them selling a stick vacuum next to their iRobot is sure one way to say \"our expensive robot doesn't do everything we claim it does\". > I finally ditched my iRobot for a Roborock a couple months ago and it's been amazing. I'm not saying those two sentences are totally incompatible, but Roborock's home page at https://us.roborock.com/ is basically half divided between \"Robot Vacuums\" and \"Cordless Vacuum Cleaners\" reply TehCorwiz 6 hours agoparentprevThe phrase you're looking for is \"Too big to fail\". We tried that, payed the credit cards of a lot of bankers while a lot of people lost their homes. Make no mistake, people losing their jobs is bad, but saving a bad company doesn't make it better. Everything dies, we should let companies die too. reply pjc50 19 minutes agorootparentBanks have counterparty risk in the way that robot vacuum companies don't. Letting the banks go bankrupt would have destroyed a lot of perfectly viable businesses among their customers. The cloud companies will count as TBTF too for the same reason. Although it's harder to see how Amazon might get into a \"we'll have to switch the datacenters off tomorrow unless we get bailed out\" financial position\", since it's easier for them to trade as a going concern, if there was a risk of that happening they would definitely get a bailout. Also, remember that the bailouts were in almost all cases loans. reply adastra22 4 hours agorootparentprevThis isn't a too big to fail situation. Amazon wanted to buy iRobot. iRobot wanted the acquisition to go through. The government stepped in and blocked the acquisition on the grounds that they'd rather Amazon and iRobot remained competitors. But what actually happens now is that iRobot lays off its core staff in a desperate attempt to avoid insolvency. The regulators could wish all they want that iRoomba stay a competitive player in this space, but businesses don't run on wishes and rainbows. reply ZainRiz 3 hours agorootparentI think the claim is that some other company in the robot vacuum business can buy iRobot instead of Amazon and get that sweet sweet IP reply jen20 5 hours agorootparentprevI wonder if Roombas are wide spread enough to cause outrage if they stopped working because the company went out of business? reply mark_story 5 hours agorootparentIsn't this an implicit risk anyone who purchases a 'smart' device takes? Every company will eventually discontinue their cloud service for a product line when it suits their needs. We have seen this countless times in technology and that anyone assumes that cycle will repeat with all these devices is fooling themselves. reply fooey 12 hours agoparentprevThe problem with allowing an acquisition to proceed just to save the company being acquired from failing is you end up with deals being structured to intentionally to cause harm if the deal collapses Look at what Kroger is doing with Albertson's for an example. If that merger fails, it's very likely Albertson's will go bankrupt because the shareholders looted the company of all their assets to ensure it can only survive if it's acquired https://www.washingtonpost.com/business/2022/11/03/albertson... reply lotsofpulp 12 hours agorootparentHow do shareholders loot a company? They own the company, which means they get to decide what to do with the company's assets. reply o11c 12 hours agorootparentBelieve it or not, some people think \"the purpose of a business is to produce useful things\". Crazy, right? reply Phiwise_ 10 hours agorootparentI agree. It's crazy that someone who didn't put up the skin to acquire the business thinks they should get to run it anyway. reply cortesoft 9 hours agorootparentprevYeah, and some people believe the earth is flat. If we wanted a society where the purpose of a business is to produce useful things, then we should not have the rules set up like they are. You can't create an entire system of rules and regulations that set it up so the incentives are all aligned for companies to exist to make money, and then be surprised when those incentives work. reply tylersmith 8 hours agorootparentprevIt's not crazy that people are wrong about facts, but then they should keep their opinions to themselves. reply johngladtj 11 hours agorootparentprevProviding shareholder value is the useful thing they are supposed to produce. Anything else is a side effect reply pi-e-sigma 11 hours agorootparentprevIt's not exactly true. A company is a separate legal entity and its owner can't do as he pleases with the company's assets. reply schnitzelstoat 46 minutes agoparentprevI got a Roborock because it was cheaper, but it's been amazing. It seems Roomba was just selling on it's brand name. reply nyjah 12 hours agoparentprevI have the same experience. I bought the most expensive iRobot and that thing sucks. You can’t irobotitfy your house enough for it to just run. I bought a Roborock s6 and it’s amazing. Immediately got one for my mom and she’s been raving about it for years already. reply prepend 11 hours agorootparentI did the same thing and it broke 1 month after the 1 year warranty expired. A cryptic error that means buying a new brain for it. Ridiculous that a $800 robot breaks. Bought two sharks for $300 each. Not quite as good, but so much cheaper. And doesn’t require bags. reply blagie 8 hours agorootparentprevFYI: Cheap Roombas seem better than expensive ones. I have a random one which bangs around the house for a long time, and the house ends up clean. Push a button, come back an hour later, and you're done. I like the simplicity. reply nyjah 8 hours agorootparentI agree with this that they are better, but actually before the expensive roomba I had a cheap roomba and it worked terribly. Insisted on shoving itself right under the perfect height furniture. My house is pretty basic. But my brother had similar roomba and I know he had better luck than me, but now has a shark. reply faeriechangling 7 hours agorootparentprevI had one and it got trapped and strangled itself regularly and ran over things and was totally useless. I pretty much wouldn't recommend those basic robot vacuums. reply Semaphor 6 hours agorootparentprevI have a cheap (well, also old, there wasn’t too much else back then, it’s 10 years old) iRobot Roomba 620 and am very happy with it. Extremely repairable, which is not something I read about modern ones of any manufacturer. reply SoftTalker 6 hours agorootparentprevI think they are utterly ridiculous. Get a decent canister vacuum that plugs into the wall and you'll have clean floors in 15 minutes (or less if your house is small) with no frustration, no ugly charging station to trip over, no batteries to catch on fire or require hazmat disposal when they don't work anymore, no apps, no accounts, no spying. reply mardifoufs 5 hours agorootparentI think it's pretty fair to assume that anyone buying a Roomba knows about normal vacuums, and still prefer the room as for reasons that are obvious. ( Also lol at the \"batteries\" that catch fire, as if that's actually a thing that people should worry about. Might as well be scared of your canister vacuum's motor burning out and destroying your house, or never buy a laptop!!) reply AuryGlenz 6 hours agorootparentprevGreat. Ours vacuums and mops, and don't tell me you can do both in 15 minutes. There's nothing worth buying more than free time. reply Dylan16807 4 hours agorootparentprev15 minutes how often? Once you spend 15 minutes 20 times that's a pretty significant amount. reply throwaway2037 6 hours agorootparentprevWhy is this downvoted? Sure, a bit emotional, but the content still stands. There are so many places where Roomba and friends cannot reach. I still was a washer / dryer than can fold my clothes and change the sheets on my bed. That seems like an almost impossible task for a robot. reply isbvhodnvemrwvn 38 minutes agorootparentIt's not 'a bit emotional', it's a tantrum with as much logic as saying why have a washer/dryer when you still have to hand wash some stuff. reply ankitml 13 hours agoparentprevMy roomba was cleaning much better 4 years ago than last 2 years, their app was slick 4 years ago and clunky in last 2 years. Random Product innovations degraded the core product. They did get complacent. reply imp0cat 13 hours agorootparentOr maybe the wheels are worn and slipping. Before you junk it, put new rubber on it. Mine was completely transformed. reply seanmcdirmid 11 hours agoparentprevI think Chinese competitors just undercut them too much and provide quality that is almost as good, and they are able to innovate faster to boot. reply nytesky 9 hours agorootparentHave Chinese based electronics improved in the last decade. I remember the concerns about devices catching fire (hoverboards) and lead in vinyl and other materials (I remember an Amazon kids pencil bag that was recalled two years ago) For a household product that runs autonomously throughout my house and interacts with many surfaces, and with small kids in the house, I want a reputable supplier, not just whiz bang features and a slick app. Maybe Roborock is that, but there is so much opacity when dealing with Chinese companies even in cases where the govt is not involved. reply dghlsakjg 7 hours agorootparentIgnoring political concerns (which is a big problem); Chinese manufacturers make great products. DJI, huawei and insta360 are three off the top of my head that are perfectly capable of going up against global brands. Additionally, almost every high quality brand has manufacturing in China; Apple, Microsoft, Nikon, Canon, Dyson, the list goes on. This is more a problem of “you get what you pay for”. No name hoverboards were a real problem because designing a compact, high power draw, lithium powered device is hard to do. Segway, ninebot, etc all make great ‘hoverboard’ like products. reply insane_dreamer 4 hours agorootparent> Chinese manufacturers make great products. this should read \"A few Chinese brands make great products\" Most Chinese brands make low quality products; corner cutting is rampant. > Additionally, almost every high quality brand has manufacturing in China; Apple, Microsoft, Nikon, Canon, Dyson, etc. That is because the foreign brands set the specs and define the standard of quality, which the factory (i.e., Foxconn) follows to the letter. So Chinese _manufacturers_ can make great products (when required and paid) but Chinese _brands_ generally do not. This is one reason why Chinese brand products are cheaper than their foreign brand counterparts even when the foreign product was also manufactured in China. DJI and Anger are two Chinese brands that have worked hard to develop a good reputation internationally, and deliver top products. I wouldn't trust Huawei though. reply dghlsakjg 4 hours agorootparentSo Chinese factories are capable of high quality manufacturing. And Chinese brands are capable of high quality design and manufacturing. That’s what I was trying to get across. As I said earlier you get what you pay for. I can buy great products made in China by Chinese companies and terrible products made in Switzerland by Swiss companies. reply hn_go_brrrrr 3 hours agorootparentYou're saying \"it's possible\" and your parent post is saying \"it's unlikely\". You're not disagreeing, exactly, but... reply seanmcdirmid 7 hours agorootparentprevChinese can make great products. You can also lose your shirt with some sort of shanzhai abomination. And you don't always pay bottom dollar for that, you are just getting 不便宜，也没好货. This is why it is very important for China to develop some reliable high quality brands. reply Brybry 7 hours agorootparentprevThe Roborock vacuum experience is pretty good. We have a fearless cat and I've never felt it was unsafe around the vacuum. Generally it just works. The only \"issues\" I've had to deal with are: a) it'll eat cords so the cleaning area has to be picked up before vacuuming (or bad areas zoned off) b) it can (rarely) get stuck so the cleaning area has to be picked up (ie. a step stool I left out) c) it can (rarely) get lost if there are major changes to stuff in the cleaning area (ie. if a bunch of cardboard boxes are left out) d) very rarely (less than once a year) I have to \"restore\" the map (I think if c happens it might start hallucinating new map area) I don't know what the replacement part or customer service experiences are like though. We've had our Roborock for 3+ years now, which isn't a huge amount of time but I wouldn't consider it poorly made. Maintaining it to clear out pet hair and such is simple. reply seanmcdirmid 8 hours agorootparentprevObviously, but there are some serious contenders here who are actually working on a reliable brand. But if you just buy a random generic bot...ya...it probably isn't going to work out well. reply karmasimida 6 hours agorootparentprev> Have Chinese based electronics improved in the last decade You would surprised. The answer is yes and yes. Roomba like robots are dominated by Chinese companies and they are the most advanced manufacturers technology wise as well. reply barnabyjones 3 hours agorootparentprevThey can go back to their base station and self-empty/refill a water tank for mopping now, which is an amazing upgrade over just a vacuum cleaner. Or at least in theory it is, my DreamE somehow got progressively dumber with subsequent updates and now not only gets stuck and confused all the time, but usually can't even find its way back to the station. It seems the software on these things still has a ways to go. reply Mistletoe 9 hours agorootparentprevThere’s no moat on robot vacuums, they just had their initial brand recognition. Lots of companies should pay attention… I love my Roomba, it’s been truly a life-changing invention for me and my go-to mental construct for how robots can improve our lives and how good design can be very simple yet effective. But I probably bought it used on eBay and I continue to buy batteries every 5-10 years for it on eBay when it needs a new one. reply bane 13 hours agoparentprevIf you are an acquirer, and you suspect that the company you are planning to acquire is likely to go under, it can often be cheaper to wait for that event then buy up IP, and hire as many staff as you can for continuity. For example, see Bigelow Aerospace's inflatable space station tech and then Sierra Space's tech. I don't have any inside knowledge, but I'd imagine the path Sierra is on is likely one similar to the above strategy. reply mft_ 12 hours agoparentprevHonestly, I’m not sure about “not what they used to be”: I don’t think Roombas have ever been very good. The early ones were noteworthy for being novel, but (IMO as an early adopter) weren’t very good at cleaning. About ten years later I experienced someone else’s (then) modern Roomba, and it was still essentially the same crappy product, randomly banging into walls and struggling with carpet edges. When the Roborock appeared with LIDAR, it was like the future in comparison, and frankly what Roomba should have already delivered years before. When an established company with a big market share and opportunity for a technology lead is so out-innovated, it suggests that something is fundamentally rotten. reply mynameisvlad 12 hours agorootparent> Roborock appeared with LIDAR LIDAR is indeed a game changer, but Roborock was far from the first manufacturer to use it in botvacs. The first Roborock with LIDAR was released in 2019 and I remember getting a Neato botvac with LIDAR in the early 2010s. reply JoshTriplett 11 hours agorootparentHas anyone innovated in the direction of privacy? \"This is not a connected device, it does not talk to the cloud, there are no privacy considerations because we don't receive any of your data and never will\". reply glenneroo 9 hours agorootparentI picked up a Roborock E4 a few years back, both because it was relatively cheap and a couple thorough reviews indicated it was one of the better devices out there. I initially spent a couple hours trying (and failing) to connect it to the Android app, only to discover (out of frustration) that pressing the start button on the unit would clean just fine! I was absolutely relieved that it wouldn't be uploading anything anywhere, the only \"downside\" was that I couldn't indicate which areas/rooms to ignore, although I'm not sure that would have mattered much since I constantly move things around. I have no idea if any other models run without smartphone/tablet setup but if so, that would potentially eliminate any privacy issues ;) Maybe it's time to create a public spreadsheet with a list of models that run without a smartphone, and potentially any downsides to doing so... reply hnhnhnhnhn 11 hours agorootparentprevhttps://valetudo.cloud is a project which allows you to use many vacuums without connecting to the cloud. reply wizardwes 11 hours agorootparentprevNot directly. Valetudo exists though, it's a custom firmware you can flash to a lot of different models that is privacy focused. reply ska 10 hours agorootparentprevIt doesn't even have to be a privacy angle, but complexity. There is nothing I would actually want a robot vacuum to do that requires a network connection, and few things that would even benefit from it (e.g. makes firmware updates easier, which you mostly shouldn't have to/want to do anyway). reply troupo 1 hour agorootparentI like starting it up automatically when I leave the house. For that you need automation and network connectivity reply KerrAvon 11 hours agorootparentprevYes, assuming this isn't vaporware. They're supposed to start shipping in March. https://maticrobots.com Edited to add this quote from the website: Matic's intelligence is localized on the device, and it never sends any of your data to the cloud for processing. That means no user information is ever sold, shared, or even collected in the first place. reply throwaway2037 6 hours agorootparentprevI bet 99% people of who buy privacy invading products do not care. Yes, I know that HN loves to go on and on about privacy invasion. Yes, many educated people will tell you that they don't like companies monetising their personal info and habits, but if you watch their actions, they do not care. reply schnitzelstoat 25 minutes agorootparentI don't care. I have a Roborock. I genuinely don't care if the Red Army knows how dirty my apartment is. reply cqqxo4zV46cp 11 hours agorootparentprevProbably not, because that is a niche interest. We both know that. I’m not salty about it. But come on, get off the soapbox. reply _dark_matter_ 11 hours agorootparentThat's not true at all. Consumer reports often has privacy as a quality category in device judging. Clearly many people care about privacy _in addition_ to other concerns. reply mft_ 12 hours agorootparentprevSure; apologies for not providing a comprehensive history of the space - I was just speaking from my (limited) experience. reply MBCook 11 hours agorootparentprevI think they did a great job. But VSLAM was a mistake. The fact that the robot is not reliable at navigating unless you have all the lights on in your house is just stupid. LIDAR is clearly superior. There’s a reason every reasonable competitor is using it, even on very cheap models. I’d be happy to go back to them but I’m not touching them again until they have LIDAR. reply cqqxo4zV46cp 11 hours agorootparent“Yeah, but humans have two eyes and they do just fine!” /s reply tmchu 10 hours agorootparent\"Yeah, but have you tried cleaning the house blind folded!!?\" /s reply madeofpalk 8 hours agoparentprevWhat if allowing Amazon to purchase iRobot harms the market and shuts down three other competing companies, sending many more people out of a job. Oh, and iRobot still lays of people because they can share a bunch of functions with Amazon now. reply cowsandmilk 8 hours agorootparentWhat Amazon acquisitions have put other companies out of business? Amazon bought Eero, yet I haven’t seen other WiFi router companies go out of business. Amazon bought Ring, then a competitor was so successful that Amazon bought Blink. This notion that Amazon’s acquisition would put competitors out of business seems to be a weird dream. Especially when it is known an easier path to “unfairly compete” would be to just create an Amazon-labeled robotic vacuum and put that on the market. reply FwarkALark 5 hours agoparentprev> At what point should an acquisition be allowed on the sake of something being able to continue to exist and possibly save jobs? Never? The entire point of private companies is that they take risk on themselves. If we have to bail them out we might as well just nationalize them. See: the entire american airline industry. reply k8sToGo 14 hours agoparentprevThe company can still be acquired to save jobs. Just not by Amazon. reply nerdjon 14 hours agorootparentThat's valid, but then requires asking who. If the company is on the verge of closing, clearly there is something wrong with the business model. So for it to actually save jobs they have to first have the 1.7 billion that Amazon was going to pay and then have enough capital and a business reason that makes the acquisition make sense. There are not many companies like that out there, particularly for a company like iRobot. There was synergy between iRobot and Amazon. I am not advocating for a monopoly, but clearly another company wasn't stepping up to take amazon's place. reply jsight 14 hours agorootparent> That's valid, but then requires asking who. Neato! Oh wait, they shut down. Google! Maybe, but would the EU see that any differently anyway? Yeah, there's not really a long list here. Maybe one of the non-US home automation companies might make sense, but they'd likely just put it on a glidepath to shutdown. reply mlazos 13 hours agorootparentI feel like google acquired a ton of robotics companies and then completely failed at productizing any of them. They’ve sold them off now I think so I don’t think this fits with anything they’re doing. reply jsight 12 hours agorootparentExactly, the closest thing with Google is probably Nest. It'd make some sense, but I'm not sure if making sense is considered a plus at Google. :) reply nerdjon 14 hours agorootparentprevGoogle likely wouldn't be allowed either, I mean they may not operate a store like Amazon but I highly doubt the idea of searching \"robot vacuum\" and iRobot always being the top or in a special box would fly. I mean there is Microsoft who would have the money but it just doesn't really fit into anything they are doing. Dyson? Sony? But yeah a non US company also opens up other questions that may block it. reply CobrastanJorji 14 hours agorootparentprevIs this a thing that needs fixing, though? Isn't part of the goal of capitalism to let the different business models fight with the idea that the best ones win and everybody else fails and shuts down? Closing when something is wrong with your business model is the goal, not something to be avoided. reply nerdjon 14 hours agorootparentThere is still the technology, the people. While you're not wrong, just because the model doesn't work doesn't mean there isn't still value there. If you are presented with the choice to shutdown a company or have it live on in another company saving some jobs. I would think the second is the better option. It is possible that the only reason the business model doesn't make sense is not having a diverse enough of a catalog so research in a specific thing can be justified by multiple products. reply skydhash 13 hours agorootparentThe technology can be sold. As for the people, they entered in a contract with a business. If the business does not pan out, I don't see why they should be \"saved\". If the failure was due to bad leadership, there will be another company in the same sphere. And the value of these skills is rarely tied to the product itself. reply NegativeLatency 13 hours agorootparentThat’s a very lopsided agreement. And I’d prefer to live in a society where workers are treated with some level of dignity and respect. reply skydhash 12 hours agorootparentWorkers rights are orthogonal to socialism for businesses. And there's more opportunity for abuse when companies are \"too big to fail\". Business survival should be tied to market fit and the ability to attract talent and retain them, not handouts and acquisitions. reply lotsofpulp 12 hours agorootparentprevWorkers (people in general) should be taken care of (provided a floor for quality of life) by the government. reply thfuran 13 hours agorootparentprevLet them eat cake, amirite? reply k8sToGo 13 hours agorootparentprevIBM of course ;) reply paxys 13 hours agoparentprevThe fall of Roomba shows that capitalism is working exactly as intended. The company got complacent and stopped innovating. Their products stagnated, quality decreased and prices kept going up. Meanwhile competitors brought better products to the market. Roomba's market share started reflecting that reality. What happens if they get taken over by Amazon? Investors get a big exit, and employees hold on to their jobs for a bit longer (before the inevitable layoff). Amazon then uses its massive retail power to push an inferior product and edge out the smaller companies doing better work and competing on merit, leaving the space worse for everyone. reply sirspacey 10 hours agorootparentRoomba strikes me as having a very similar challenge to many hardware companies: 1. Prove you can do something amazing 2. Raise a bunch of capital to scale it, betting on future revenue 3. Deliver! Great product, stellar growth, soaring revenue, re-invest in product 4. Flood of copycat competitors, drives up acquisition costs, eliminates higher price point, revenue tanks 5. Pivot. Try to sell another story, build the start of another product leap, loose traction in a new way, pivot. 6. Seek an acquisition before the music ends 7. FTC In other words, product development (esp. app + hardware) is expensive, difficult, and constantly changing. We don’t have a capitalist system competing on product merit but acquisition arbitrage. In almost all cases the inferior product wins. reply Andrex 12 hours agorootparentprev> The fall of Roomba shows that capitalism is working exactly as intended. Only if your definition of capitalism includes a robust antitrust component (which mine does, but I know I differ from the mean here). reply ulfw 8 hours agoparentprevWho says the layoffs wouldn't have happened with the Amazon acquisition? Who says the CEO still had a role to play as Amazon VP and wouldn't have left soon after closing anyway? These acquisitions tend to come with huge layoffs anyway. reply paxys 16 hours agoprevI can't help but feel that a lot of these large companies are choosing not to fight regulatory agencies over acquisitions because after the recent tech correction the targets are now worth a lot less than what they bid for them (similar to Elon trying to get out of buying Twitter). Easier to pay the break-up fee and move on. reply ActionHank 15 hours agoparentSomething else that has happened since the initial talks of this merger is that Roomba machines have fallen far behind the competition who are both more driven and taking bigger risks offering better products for the same price or less. reply izacus 13 hours agorootparentI think this market is pretty much the poster child of how bad the consolidation and lack of anti-trust is hurting US economy right now. You have a whole bunch of Chinese companies fighting it out on the market (as the founding fathers would have wanted for US!), offering better and better products, leaving Roombas in the dust. While iRobot sleeps on their laurels, barely innovates and at best is only capable of solving their flaccid inability to compete by... being sold to a monopolist which can push it via their store. That's what they wanted. If US would still have this kind of healthy market of multiple (smaller) companies competing on product, not IP lawsuits and patents, the government wouldn't have to enact foreign sanctions to defend the economy. reply chucke1992 12 hours agorootparentThe thing is the american workforce is too expensive at this point. reply nojvek 10 hours agorootparentDesigned in US, Assembled in China. I feel there is a lot that can be done in US but the electronics industry is slowly eroding. reply throwaway2037 6 hours agorootparent> the electronics industry is slowly eroding Or: The \"electronics industry\" (essentially contract manufacturing) is much too low margin to make economic sense in a highly developed country like the US. Have you seen the wages in the hyper competitive electronics industry in China? They are awful. Design and marketing is where most of the value can be captured. Look at Philips N.V. They are essentially strictly design in advanced countries, and manuf. in developing countries. They are doing well. reply TylerE 13 hours agorootparentprevRoomba was the Nomad. Who will be the iPod? reply slowmovintarget 13 hours agorootparentTesla seems to be trying... but who knows. reply dylan604 13 hours agorootparentwhat cleaning robot does Tesla offer? what conversation are you in? reply Izkata 13 hours agorootparentTesla smart home: https://www.teslasmart.com/ Includes robot vacuums: https://www.teslasmart.com/smart-vacuum-cleaners reply adolph 13 hours agorootparentThats TeslaSmart, not Tesla, the US automaker who also has a general purpose humanoid robot in development: https://en.wikipedia.org/wiki/Optimus_(robot) reply Izkata 12 hours agorootparentTheir brand is just \"TESLA\", not only can you see it on their homepage a few times but it's also stamped into their products like that: https://www.teslasmart.com/data/images-xl/3413-Tesla-Smart-R... Why assume that original comment was talking about the car manufacturer when there are robot vacuums from a much older company branded like that? reply TylerE 10 hours agorootparentProbably because 99.99% of people have never heard of the other one? This is literally the exact kind on confusion trademarks are intended to prevent. reply tooltower 15 hours agorootparentprevI haven't kept up in recent years. Who are the better competitors these days? reply vosper 15 hours agorootparentI don’t know if they’re better, but from watching reviews I believe Roborock and Dreame are considered pretty good competitors. I’m certainly happy with my Roborock (which has no cameras, and their latest model also comes in a no-camera variant) I wouldn’t touch an iRobot (or any camera vacuum) after their camera scandal: https://www.technologyreview.com/2022/12/19/1065306/roomba-i... reply KerrAvon 11 hours agorootparentFWIW, production Roombas don't send pictures back to the mothership. The scandal was that pictures from development Roombas -- explicitly expected to be reviewed by humans -- leaked. reply darknavi 14 hours agorootparentprevRoborock (Xiaomi) and Dreame are the big players from what I see. Both conveniently have models that can be \"de-clouded\" with Valetudo: https://valetudo.cloud/ reply JoshGlazebrook 14 hours agorootparentprevRoborock is miles ahead of Roomba. reply BossingAround 14 hours agorootparentIn what ways? The Roomba J7 I'm using is pretty amazing at what it does; the only thing I'd appreciate would be a remote control for some manual cleaning. The automatic stuff is pretty great. reply izacus 13 hours agorootparentFor starters, the best Roborock supports improved mopping, including a dock with self-emptying container, self-cleaning mop and proper refills. Together with noticably better obstacle avoidance algorithm. The new Xiaomi versions now even sport extendible arms to reach corners fully. reply Izkata 13 hours agorootparentOn mops: The newest Dreame model can actually disconnect the mops and leave them in the dock, so because of the relationship through Xiaomi people seem to expect that to come soon to Roborock as well. reply ponector 11 hours agorootparentprevRoborock can do more with lower price tag. Isn't it \"miles ahead\"? reply driscoll42 11 hours agorootparentprevI usually look at rtings robot vacuum reviews for recs: https://www.rtings.com/vacuum/reviews/best/robot reply datavirtue 13 hours agorootparentprevDoes it matter? We are talking about fun consumerist novelties facing a shrinking population with shrinking disposable income. No one needs these \"things.\" reply bombcar 5 hours agorootparentThat’s the real issue. As far as I can tell everyone who wants a robot vacuum already has one, and if they’re crappy enough that you’d upgrade, they’re also crappy enough that you’ll likely just give up. If they work, you’d not feel the pressure to upgrade. reply coffeebeqn 8 hours agorootparentprevAmazon should have known this better than anyone - I’d imagine they’re the biggest seller of robot vacuums in the world reply dopeboy 14 hours agoparentprevIt's a worrying trend, especially for those of us in startup land. With the Figma deal following through and less companies going public, the assembly line funding model is showing some serious cracks. Hope it's just temporary. reply paxys 13 hours agorootparentThe entire model of startups taking on lots of VC funding, burning it over a few years to acquire customers and then having a big exit, all without bothering with a business plan or making a single dollar in profit, is basically over. The reason VC funding is drying up, big unicorns aren't going public and acquisitions are halted is that investors are actually starting to drill into the numbers now, and finding nothing but hot air. reply dylan604 13 hours agorootparentI'm okay with this. Very little good has come from this. How many of the social platforms would be where they are now if they had to fund themselves differently? reply throwaway2037 5 hours agorootparentprevReal question: When will Stripe go public? That has to be the biggest unicorn in years. And barrier to entry is quite high, so they have built a nice moat. I wonder if the VCs push them to go public. reply no_wizard 14 hours agorootparentprevits only worrying if you can't make a sustainable standalone business at the end of the day. Which should be the goal. A major acquisition should have always been seen as a last resort, only preferable to going out of business. Startups hoping to cash out by selling to a competitor is its own kind of silliness in the first place and was largely fueled by lax regulation environments and 0% interest cash. What ever happened to building durable businesses as an explicit goal reply dopeboy 13 hours agorootparentA lot of big businesses don't start off as sustainable ones. So they buy time, through venture capital, to become sustainable ones. This kind of news hurts the chances of that category of companies from being created. reply no_wizard 10 hours agorootparentI didn't posit that a business should start off as a sustainable one. I asked what happened to building a durable business as a goal? I understand the VC model enough to know that sometimes for years you run red because you need economies of scale or some other engine to finally turn over and then at scale the business will start to generate bigger cash flow once it reaches that tipping point. Even if a bit simplified as an explanation, this isn't what I'm talking about. What I'm asking directly is why exiting to a big company became a goal in and of itself. Lots of VCs poured money into companies with the distinct hope that those businesses would at the very least be acquired. I think this is the silly part. Every investment should be from the perspective of can this be durable if standing on its own two feet? and a big acquisition is not something that should be taken into consideration as part of the investment and business building strategy of either the VC firms or the founders. That all got lost. Building a durable business should be the ultimate goal, and selling the business to a big company should be seen as lower status than it currently is. Its not that I'm saying acquisitions don't make sense sometimes, I'm not. However as far as goal setting and running a business is concerned, it shouldn't really be thought about as a viable fallback or exit strategy until its readily apparent and available, but that hasn't been true for some time now. VCs and founders often explicitly look at acquisition as one of their \"success targets\" and hopes for a business. Lots of people on HN have admitted that they started businesses with some hopes that they could possibly be acquired. That is the drift away from sanity I'm talking about. reply dopeboy 8 hours agorootparentUnderstood and apologies for misunderstanding. Every single founder I know (and I know a lot of them) doesn't build a business for the specific reason to be acquired. Is it a thought that crosses their mind? Definitely. Is it something investors think about? Absolutely. But the day-to-day, 12 hour+ grind for them is all about product-market fit and drawing revenue - the things that make a durable business. reply izacus 13 hours agorootparentprevWhich big businesses are that? And how many categories of sustainable mid-sized companies creating a healthy competing market were hurt with VC cash flood, where VC fed companies dumped prices, killed healthy competition and then died themselves (together with whole product categories) when the VCs lost interest because they weren't a unicorn? reply dopeboy 12 hours agorootparentAirBnB, Uber, Cloudflare to name some. Those mid sized companies you refer to did fine for three reasons: * If they were competing against venture backed companies, they were likely playing in big markets. My bet is they are still alive today. * They got acquired by the bigger companies and ended up capturing even more short term economic value than they would have otherwise. * These venture backed companies expanded the market, actually helping smaller players. I would bet money that VRBO's business increased as AirBnB got bigger. I agree that there's something sleazy about injecting a ton of capital in a niche space; at the very least, it's distorts all the dynamics in it. But you can't deny that this short term chaos creates long term economic value for everyone else. Uber demonstrated you could add a tech layer to the taxi business and make it more efficient for riders + introduce a whole new set of people to the driver business. Did this harm existing taxi drivers? Unquestionably. I know it's hard to look at that business model as innovation but it is because now there's extreme price pressure on these companies (esp Lyft) to remove the costliest part of the equation - the driver. So as a result, you have a ton of very motivated energy towards solving that, via autonomous driving. It took mediocre business innovation[0] (uber) to drive meaningful tech innovation (autonomous driving). [0] - Purposely differentiating the tech innovation (which Uber deserves a lot of credit for) vs the biz innovation (which last I read is getting better, though still shaky). reply itdoesnotmatter 12 hours agorootparentprevI hope not. I'd like to see people start building sustainable businesses intended to last and stop the whole cycle of accumulating users and hoping to get acquired before the funding runs out. I think the ecosystem is healthier for Figma and Adobe remaining separate. reply baq 14 hours agorootparentprevIf you work at figma this… shouldn’t be a problem? If you work at a temporarily-not-an-unicorn, no antimonopoly institutions care about you. reply eric-hu 13 hours agorootparentFor the employees at Figma, their shot at cashing out evaporated when the Adobe deal was called off. I know people who were ready to leave Github in the past, but their manager told them to wait a bit, there’s a big acquisition coming and their equity would be worth sizeably more. So here’s one of those scenarios: you’re working at Figma. Perhaps you’re burned out or just want to try out something different. This acquisition deal with Adobe has been signed and you’re grinding through the days waiting for your chance to cash in on the years of work with Figma that have nearly paid off. Then this happens. This affects startups similarly. The non IPO exit path got that much less attractive. reply datavirtue 13 hours agorootparentIf your grants at Figma aren't paying yet, they didn't give you anything. My ETFs are paying me every six months. reply throwaway2037 5 hours agorootparent> My ETFs are paying me every six months. Equity ETFs or bond ETFs? If equity, I guess the max you can get is 4% yield, which is still worse that money market, plus you take equity risk. If bond ETFs, they will tank when rates fall. I never understand the appeal of bond ETFs; money market funds are enough for my fixed income needs. They are basic, easy to understand, liquid, etc. reply baq 2 hours agorootparent> they will tank when rates fall you mean yields? the etfs themselves will rocket up. reply dopeboy 13 hours agorootparentprevTemporarily-not-an-unicorn companies will need to raise more money to become a unicorn. If those investors don't see light at the end of the tunnel, these companies get squeezed. reply x0x0 12 hours agorootparentprev> With the Figma deal [falling] through There's just no world in which the market leader ought to be able to buy their #1 competitor, particularly in a deal where the economics ($10b on $300m rev) only make sense from an anticompetitive standpoint. I have no idea what these folks were thinking. reply adventured 12 hours agorootparentprevThe longer interest rates remain high, the more the pool gets drained of speculation. The next up cycle in speculative behavior won't follow until after interest rates hit the floor again (inevitable). reply ado__dev 16 hours agoparentprevTotally agree there. Why spend $1.4b when the company is worth maybe a quarter of that now? reply dheera 14 hours agorootparentWhy don't they structure deals as a multiplier of VOO? \"We will acquire you for 2 million shares of VOO\" or something like that. If the only reason is that it's an unorthodox way of doing a deal, I say do it anyway and forge a way into the future of M&A. reply runako 13 hours agorootparentVOO is up since the iRobot acquisition was announced, which would have resulted in the effective price increasing over time, the opposite of what they want. Nasdaq is ~ flat since then, which also would not capture the economic dynamic desired (some tech valuations have fallen). The Nasdaq Tech 100 does an even worse job at capturing the dynamic: it's up nearly 50% since the deal was announced. You'd have to use something a lot more specific, like a basket of specific stocks or some very small sub-index. But then a smaller basket could have a lot more volatility that may be endogenous to a small set of companies, and someone has to get paid for that volatility. reply paxys 14 hours agorootparentprevThis isn't totally uncommon. Companies just use their own shares instead of VOO. reply tomatocracy 13 hours agorootparentprevThis type of structure creates risks around ability to pay which a well advised seller would try to avoid. You could design something which would mitigate that but it would likely end up costing more for the buyer to set up and run anyway. In private deals (one where the company being sold is not directly listed), the traditional way to solve this problem is via earn-outs. In both public and private deals, using the purchasers' stock as the acquisition currency is also relatively common. reply 1980phipsi 14 hours agorootparentprevIt’s baffling they don’t do it. reply chucke1992 12 hours agoparentprevIt actually puts ABK acquisition an absolutely perfect steal - due to their stock crash they were literally worse exactly as they supposed to be post correction. So the price was the ideal. reply akira2501 15 hours agoparentprevIs this sour grapes on behalf of large companies? They're _deciding_ \"not to fight\" because of the \"tech correction\" and not, \"the regulatory agencies are finally waking up and applying the law.\" Easier to pretend they're doing it by choice rather than by force of law, which is finally able to reach them, due to the \"social correction\" of no longer worshiping tech companies. reply s1artibartfast 15 hours agorootparentThis misses the point. The regulatory agencies have already applied the law. However, there are options to appeal these decisions, and companies choosing not to do so says something. Furthermore, it is just common sense. If you signed a purchase contract for $1.5 Billion, and it is now worth $0.5 billion, no rational actor would want that deal to go through. reply raverbashing 15 hours agorootparentprevIt's kind of the opposite of sour grapes, it's like signing the purchase of a car but then you find out somebody else is selling a similar car for 20% off and you feel like you don't want to buy the car anymore but you signed but then the other guy can't sell you for some reason and you're all too happy to let that slide. reply hmottestad 10 hours agoprevI hope iRobot manages to survive. I have one of their J9 robots with a camera and object detection. It’s amazing how it can vacuum our toy-riddled floor. I’m very weary of buying a robot vacuum with a camera from a Chinese company. I don’t fully trust iRobot, but I trust them much more than I trust Chinese brands. I also couldn’t see myself going back to a robot vacuum without a camera. After I got kids our old Neato vacuum robot got very little use since it always required that everything was picked up off the floor. That being said I’m quite frustrated by my new J9. It’s supposed to have a feature called SmartScrub, but it’s not in the app. The robot is also supposed to drive back to the base to refill its water tank, but it hasn’t done that a single time since we bought it a month ago. The combination of vacuuming and mopping is also a bit unfortunate since it sucks up a lot of humid air when passing over areas that it’s already mopped, causing a buildup of wet-ish dirt in the air duct. reply outworlder 9 hours agoparentiRobot is getting left behind, technologically. I'll omit the competitor that I use now. I'll just say that it has a base that empties the robot dust bin, refills the clean tank, empties the dirty water tank, washes and dries the mop. And the mop actually looks like something that resembles a mop (and rotates), it's far better than the 'swiffer' kind of thing that the Braava has, or that the J9 has. I don't see the same wet dirt issue because the navigation takes care of that. It does have a camera and the parent company is indeed Chinese. Neato is even worse and their robots aren't much better than they were 5 years ago. They started out really strong but any improvements were marginal. reply mmmooo 6 hours agorootparentin case you didn't know neato is suspending operations and closing up shop reply beeboobaa 8 hours agorootparentprevAs someone looking for a robot vacuum, care to just share to name? reply idopmstuff 6 hours agorootparentIt's probably a Roborock. I had a Roomba that I found disappointing for years and after reading positive reviews of Roborock here and on Reddit, I finally pulled the plug on an S7 MaxV Ultra. Worth every penny. To be clear, the CCP definitely has a detailed map of my home. But man, I've got two dogs that shed a lot, and it's great. Sometimes you've gotta accept the tradeoffs, y'know? reply gassius 2 hours agorootparentprevI don't know the parent's brand but I have a CECOTECT CONGA 11090 that fills exactly the description. CECOTEC is a spanish brand tho, and may not be available everywhere, and almost sure they manufacter the hardware in China so very possible the same specs are offered by other brands elsewhere, but those are my two cents reply feb 34 minutes agorootparentFrom valetudo.cloud: Conga is a brand that uses existing robot designs with a slightly customized cloud.They’re not a robot manufacturer. reply MetallicCloud 5 hours agorootparentprevSounds like the Dreame L20 Ultra. I have it, it's miles better than any irobot I've owned. reply waskip 2 hours agorootparentprevMaybe Ecovacs deebot OMNI reply hypercube33 8 hours agorootparentprevIf it's compatible with that open firmware/ home assistant I'm in for one at least. reply hedora 9 hours agoparentprevOurs is collecting dust because it was so flaky. You can solve the base problem by putting some grippy tape in the wheel wells. (They made the charging base out of some plastic that's impassible for the robot.) I realized I was spending 30-60 minutes debugging and manually charging our roombas for each (usually aborted) run. Once the Amazon acquisition was announced, I realized it meant that they'd inevitably send upskirt angle shots of the family to Bezos for ad targeting and forwarding to local law enforcement agencies. That was the final straw. Supposedly, the Chinese ones are slightly better for privacy because you can MITM their backend cloud connection and point it at a local version of whatever service the robot thinks it needs in order to work. I bought a broom and mop. They're much less work. So were the older, dumber roombas (especially the square mops from the Mint acquisition). YMMV. reply outworlder 9 hours agorootparent> They made the charging base out of some plastic that's impassible for the robot. The _exact_ same problem Braava has. Whenever the wheels collect a small amount of dust, they slip too much and the robot can't climb. I actually added sandpaper to the base to help with the grip. Seems like such an easy fix; it's mind boggling that the same issue is present across models, for years! > I bought a broom and mop. They're much less work. Yeah, although, if you have pets, even a not so reliable robot will massively cut the amount of cleaning you have to manually do. reply glenneroo 9 hours agorootparentprevRoborock E4 (older model) runs without any app connection at all, not sure about other models but that definitely eliminates any privacy issues and is significantly easier than MITM :) reply xyst 10 hours agoparentprevPersonally my experience with iRobot has not been smooth. I have a dog that sheds a shit ton of fur. The robot would get clogged frequently. A few parts would just stop working. Went through 2 replacements before calling it quits. Switched to Roborock and haven’t looked back. Going on 1 yr now. reply kccqzy 8 hours agoparentprevI think the best products have switched from cameras to LiDAR. reply apapapa 9 hours agoparentprev> I hope iRobot manages to survive. Seems like an easy company to replicate (and it has been too) reply treprinum 10 hours agoparentprevHow is a company 10,000 miles away from you going to hurt you compared to a local one that has access to all government services (and vice versa)? I'd rather prefer Chinese or Russian companies to spy on me because what are they going to do with it that would matter? Will I get more spam in Mandarin/Cyrillic I can't decipher anyway? reply elefanten 9 hours agorootparentThis is completely ignorant of how influence works and how espionage assets are developed. A foreign entity would have no obstructions to using collected information as ruthlessly as possible, whereas even a corrupt/politically-motivated/unrestrained domestic entity would have to weigh the comparatively massive risk of being implicated or exposed. “what are they going to do with it?” If you have valuable information or responsibilities, that could be motivation enough for foreign agents to seek leverage. If you don’t, then it’s equally fantastic to think the CIA gives a shit about you for some reason. reply outworlder 9 hours agorootparentprev> How is a company 10,000 miles away from you going to hurt you compared to a local one that has access to all government services (and vice versa)? I'd rather prefer Chinese or Russian companies to spy on me because what are they going to do with it that would matter? Will I get more spam in Mandarin/Cyrillic I can't decipher anyway? Don't forget – if someone unlocks your door and leaves it ajar, then anyone can get in. Chinese companies are not well known for their cyber security practices. Even if you don't care how they could use this information (either individually, or in the aggregate across millions of people), you should also care about who else would be able to hack into the feed. You don't want a \"kia boyz\" situation with a camera in your home. Imagine some kid finds an easily exploitable vulnerability and releases some software allowing anyone to tap into your camera? This has happened with security cameras already. It's even worse these days with cloud connected equipment, all you need is a vulnerability in the cloud command and control. And, although most US agencies are not allowed to collect data from citizens in US soil, they can (and have) collected data procured from elsewhere. The bottom line is, the same information can end up much, much closer to you. reply pwnna 18 hours agoprevThe progressive loss in consumer robotics company in the West to their Chinese counter parts has been disappointing. Much like drones, I suspect this is short sighted as the underlying technology eventually have national security concerns. Now maybe these companies are likely just mismanaged and the cost of North American engineering is too high? That said, it still seems like there is a structural problem here that very few hybrid software-hardware companies succeed. reply kredd 16 hours agoparentThe problem is, Chinese consumer tech is full of extremely competitive and cut-throat companies. Some countries don’t like how their government is giving a tons of subsidies for them to progress like crazy (see BYD in 2012 vs now), but they’re delivering results. Combined with their low cost of engineering, the prices in the products are also pretty low, so it’s a no-brained for an average person to buy something for double the price for half of the functionality, just because it was designed in US. reply izacus 13 hours agorootparentIt's competition. Competition made US what it is and now the US economy has moved from competing on product quality to competing on who buys a better lawyer and politician to block competing companies from existing. Or just buying them. I wonder how much money is burned in the economy just paying people to write EULAs, laws and service agreements to more effectively avoid liability and screw over customers vs. how many is actualy going into improving products and services? reply pwnna 14 hours agorootparentprevYou're exactly right. I'm not asking for people to choose an inferior, pricier product. My thoughts is that China has the environment to have extreme competition which is leading to better product. This is distinctly not the case here. This is the structural problem that will eventually lead to a loss of competitive edge. Your call out to BYD is a good one, because it is conceivable that even western-made cars will be made non competitive in 10 years and it seems that we are sleeping through the news (or even encouraging it). I hope I'm wrong, but the road ahead is filled with challenge because the direction is fundamentally wrong, and it will take a lot of effort to reverse course, if that is even possible. reply chucke1992 12 hours agorootparentA lot of western companies do not compete anymore - established european countries are basically oligopolies and their lunch is slowly eaten by the more aggressive chinese companies and Tesla. American companies - aside Tesla - is in the same situation. Rich on government contracts and control over their home market. Basically a lot of established manufacturers are IBMs of this era. reply hylaride 11 hours agorootparentUh, Tesla got tons of money direct via carbon credits and indirect via consumer EV credits. They're actually in trouble for possibly lying about their range and getting more credits than they should have. reply kredd 13 hours agorootparentprevYeah it sucks. We’re trying to play catch up game for manufacturing industry, but it’s abysmally hard to get it going. I don’t think we can easily pour down money and the talent and processes would just reappear in a couple of years either. So, my assumption is high-scale tech protectionism wars are going to start. reply pwnna 11 hours agorootparentI think people focus on protectionism because that is the traditional tool to fight things like foreign government's unfair subsidy practices. However, you cant just have protectionism without fostering competition and innovation in order to succeed in creating a more competitive product/market. Example would be USA protectionism against Canada's bombardier. It only protected Boeing but didn't actually make Boeing make better planes, as we can see from all the recent issues. So I think protectioism is fine as long as we properly setup an environment that allows for and encourages competition and innovation. However, that doesn't seem to be a path we are used to taking . reply kredd 9 hours agorootparentAbsolutely agreed. I do think it will go the Argentina way if/when we start mass banning imports of Chinese consumer tech. Well, unless, as you mentioned we start heavily investing and encouraging local competition. I guess, time will show, but I hope we don’t cut ourselves out of good products just because “they’re foreign”. reply throwaway2037 5 hours agorootparentprev> Example would be USA protectionism against Canada's bombardier. Can you give some specific examples? I couldn't find anything. I'm pretty sure that US and Canada have nearly free trade, due to NAFTA. reply slavik81 3 hours agorootparenthttps://en.wikipedia.org/wiki/CSeries_dumping_petition_by_Bo... reply bombcar 5 hours agorootparentprevIirc Boeing complained about Bombardier not being US and the end result is Airbus owns them and they make a plane in the US somewhere. reply Apocryphon 15 hours agorootparentprevWe used to fund basic research in this country. Now we don't even give corporate handouts in strategic industries. (Okay we do, see CHIPS Act, but too little too late?) reply esics6A 12 hours agoparentprevThe problem is Chinese companies are subsidized by their government to manufacture things of little or no intrinsic or critical value. Automated vacuum cleaners and consumer drones are niche electronic novelties. Electric cars using solid state batteries are also novelty that will be obsolete once electric engines that use liquid fuels become mainstream (fuel-cells). The purpose of subsiding what are zombie companies is to maximize employment to ensure internal stability. The wins these companies show are propaganda wins only and don’t make the country more competitive. Foreign manufacturing is also migrating out of China at an alarming rate as shown by falling exports and GDP growth. None of the development in the Chinese technology sector is sustainable. These companies would never survive on their own without subsidies and are dependent on them. It’s a cascading failure waiting to happen in the Chinese economy and will likely be a global shock. At least the Americans may appear to take longer to develop winning companies but once they do they tend to be sustainable and long lasting as organic enterprises. Edit: The American free market is working as intended because it rightly values robotic vacuums as useless devices. reply russli1993 31 minutes agorootparentyou have so much incorrect views of Chinese companies, the technologies these companies have, what is actually happening on the ground in China. You also vastly underestimate the real complexity of making today's products, even as mundane as a hair dryer or a toy. Chinese manufacturing makes making them look easy, people think all you need is bunch of cheap labor and you are set. No it's not. Also, for white label products, examples like hair dryer, washing machines, air conditions, its the Chinese companies who design, build and test, the entire lifecycle of the product, importers buy them and slap on their own brand. Think what goes into a hair dryer? Exterior design, looks good and functional. How you make the plastic cover, do the plastic injection molding? How you design all the internal parts, fan, motor housing, heating wire, power circuits, micro-controllers etc, and make sure everything fits. Some companies even do individual components themselves, like the brushless motor, or there is a Chinese supplier that makes them, which provides much faster time to response. Then do the testing for each component, electric, heat, water, moisture testing. Then design a mass manufacturing system with automation and human labor that achieves really high yield and low wasted materials. This is the hardest part, its easy to make a hair dryer by hand taking 100 human hours and make sure it works. It's much harder to make 1M hair dryers per month, that is going to be used in all sorts of environments and with all kind of abuse, make sure they work well for a number of years so customers don't return them, or you go bankrupt from recalls and warranty, and make sure you only have to throw out the absolute smallest number of manufacturing defects, and really control your cost structure so you still make a profit when importers are squeezing your price. Then the supply chain and logistics, shipping from suppliers and shipping to customers. Then create a number of products for different markets. China can manufacture for cheap, but people don't realize manufacture for cheap and at massive quantities is a technology itself. It's also management, business process, even company and worker culture. China doesn't have the cheapest labor cost. It's the combination of everything that produces a physical product with the level of quality, fit and finish at the price point. reply brcmthrowaway 11 hours agorootparentprev> Electric cars using solid state batteries are also novelty that will be obsolete once electric engines that use liquid fuels become mainstream (fuel-cells). This seems like a big statement, can other experts comment? reply hmottestad 10 hours agorootparentCurrent battery tech has a slow and steady progression of improvement. There is bound to be a market disruptor at some point in the future, but it’s far from a guarantee that it’s going to be hydrogen. I believe the biggest hurdle to any changes to current battery tech is that it costs so much to develop an entirely new process and build factories. Most innovation is in the form of small adjustments. For hydrogen to overcome this hurdle it would have to either be extremely cheap or have some unique property. For cars I don’t see hydrogen having that much of an advantage, but maybe electric planes would be feasibly powered by hydrogen due to the much improved weight to energy ratio. reply throwaway2037 5 hours agorootparentprevYeah, I agree. The OP's statement is insane. > electric engines that use liquid fuels become mainstream (fuel-cells) That is surely hydrogen. Do they understand the conditions that are required to stored hydrogen as liquid, let alone natural gas? reply nateglims 16 hours agoparentprevIt's hard to call it mismanaged when they did the playbook that is expected by prevailing finance and economic views since the mid 70s: paring down to one thing and increasing what you give to shareholders over time. Or perhaps that is the structural problem. reply bagavi 10 hours agoparentprevhttps://maticrobots.com/ This is an American company I believe considering they are taking privacy seriously reply sneak 15 hours agoparentprevIt’s only disappointing if you care about the relative power of national economies. As someone who doesn’t care at all about stack ranking or any nation’s “national security”, as a consumer, more competition, and more and cheaper products is a simple and uncomplicated win. Almost all of my favorite companies are in Shenzhen presently. I would move there if I could do so easily. reply skydhash 14 hours agorootparent> Almost all of my favorite companies are in Shenzhen presently. All my favorite devices were designed/engineered either in Japan or in the USA. I'd take good engineering over cheap manufacturing every time. And we could do with lower number of devices. While they are probably made in the same factory, I'd love a focus on quality instead of price. reply mardifoufs 5 hours agorootparentFollowing your own logic, you'd never have had any Japanese engineering if everyone thought the same thing about japan in the 60s and 70s. It used to be considered the place where cheap stuff and knock offs were made, but it evolved from there. Same seems to be happening to China right now reply mcmoor 11 hours agorootparentprevAs someone who actually care about those but not a US citizen, I welcome all of these! It's just funny seeing the free trade principles that's been repeated over the ages getting reversed like this. Now this is the end of colonization. reply baq 12 hours agorootparentprevunless you are a descendant of Chinese or at least Asian people, if you move, you may find what \"national security\" is about. countries compete, albeit on different rules - having a monopoly on violence and a centrally controlled money printer tends to do that - so your dream of 'just pure free market competition' can only ever be that - a dream. reply nine_k 15 hours agorootparentprevWith China's policy being what it currently is, we're going to feel the economic consequences, in the US and in Shenzhen alike :( Good thing if it's going to be only limited to economic consequences. reply kilroy123 14 hours agorootparentprevI'm curious, what are the companies? reply kube-system 15 hours agorootparentprevGeopolitics giveth cheap consumer electronics, and it can also taketh them away. reply fhub 13 hours agoprevRoborock seems to be eating iRobot's lunch. Which I find fascinating as the now ex-CEO of iRobot is was so cocky about avoiding using lasers (LIDAR) for navigation and sticking to cameras only (which had a real privacy issue). See this interview with Lex Fridman https://youtu.be/1d9Dj9dT_pw?si=j8CbM7GgRMm6Mu0m&t=1362. Lex pushes him on LIDAR and he doubles down on camera only. \"Vision is the future, I can say that without reservation\" reply dylan604 13 hours agoparentWhat kind of safety concerns are there around the use of lasers around pets/children? I don't think members of either of those groups understand the \"Don't look into laser with remaining eye\" stickers. reply fhub 13 hours agorootparentThe LIDAR units used by these robot vacuums aren't powerful enough to damage cat/children's eyes. I trust that they wouldn't be on the market if they were. reply tuyguntn 13 hours agorootparentI don't trust any corporation in any matter: https://www.marketwatch.com/story/3m-says-earplug-litigation... reply fallat 8 hours agorootparentI hope you realize you are born, fed, sheltered, and survive because of corporations. Corporations's goals can easily align with the well being of yourself. Trust is one part of many stats a company has to maintain like an RPG character, and it doesn't come for free. reply dylan604 5 hours agorootparentLike in any RPG, you can take hits to that trust and lower its score. It takes several rounds to slowly let that score build up. So, what's your point exactly? reply fhub 12 hours agorootparentprevDo you eat any food made by corporations? Or grown by any corporations? Both lasers and food are regulated in the USA by the FDA AFAIK. reply dylan604 12 hours agorootparentYour blind faith in \"just because a product is available means it is safe\" makes me wonder if you're familiar with the concept of a recall. Across all industries, companies have done something to the point that it is in the public's interest to remove that item from the market. There are so so many e.coli cases, listeria cases haunting the ice cream maker of my childhood to the point i never buy their brand now, and so many other food products that have been recalled for various reasons. I kind of wished I lived in your world that exists in your mind. reply fhub 11 hours agorootparentI'm replying to the comment \"I don't trust any corporation in any matter\". But that is hard to do in practice. It would just take too much time for an individual to test every food they ever eat. Likewise I'm not going to take apart every product that is CE or CL certified to make sure it meets standards that I'm not even educated on. I don't have blind faith, I have some faith that trusting the certification processes in most matters is more cost effective for me than to verify everything myself. Especially for lasers that have strictly defined classes based on power output. That sort of trust does take on some risk. But I'm content with that level of risk. Things seem much safer now than they did when I was a kid. reply dylan604 10 hours agorootparentYes, because no decision made by any gov't body has ever \"missed\" something. In fact, recalls happen only because of the existence of these bodies. The FAA allowed Boeing to make the MAX series of planes. The CE and CL have certified products that have later been recalled. These agencies aren't perfect. The FDA has allowed things like Olestra. There are plenty of other examples. What's the difference between trusting any corp and trusting any agency? I'd say anyone trusting any corp/agency without any doubt would be somewhat delusional. reply fhub 9 hours agorootparentI think I understand what you are trying to say. You are saying that me saying \"The LIDAR units used by these robot vacuums aren't powerful enough to damage cat/children's eyes. I trust that they wouldn't be on the market if they were.\" Equates to me trusting every corporation on every safety issue AND that I trust the certifications to be perfect AND I trust nothing ever gets missed. But all three of these is clearly (to me the author) not what I'm saying there. \"The LIDAR units used by these robot vacuums aren't powerful enough to damage cat/children's eyes. I trust that they wouldn't be on the market if they were.\" My elaboration on this is that I trust that those lasers are of low enough power to not damage cat/children's eyes because consumer grade lasers are one of those things that I believe to be well regulated and certified before going on the market. I also happen to think that competitors would be tearing these things down at launch and if they found they were out of compliance they'd be very vocal about it. Then I go onto say \"Do you eat any food made by corporations? Or grown by any corporations? Both lasers and food are regulated in the USA by the FDA AFAIK.\" specifically to someone saying \"I don't trust any corporation on any matter\". I just find that to be a hard position to take in any practical sense. Society only functions with some level of trust between supplier or consumer. reply dylan604 9 hours agorootparentyour trust that things would not be on the market just seems very rose tinted glasses to me, but maybe my hesitancy in agreeing is slightly misplaced. rather than the blind trust, maybe it would be the \"shocked to find out\" something slipped past the regulations. we see this happen all the time. for me, the trust isn't there. i'm honestly shocked at this point that i don't get sick from mass produced food, or hurt by any other product. i just let the other animals in the herd go first more than trusting the regulators EDIT: here's a headline from just within the past hour recalling 50,000 cars with a \"DO NOT DRIVE\" warning[0]. that's from a very heavily regulated industry. these types of things are precisely why \"trust\" is not really warranted on my part. it's more of just blind acceptance, not trust. [0] https://www.reuters.com/business/autos-transportation/toyota... reply ex3ndr 12 hours agoparentprevCurrent roborocks are mostly vision plus some lidar for some precision reply fhub 12 hours agorootparentThey have a mix of models. Some with cameras and some without. The naming scheme is a bit inconsistent but generally the models with V have camera(s) reply Hadriel 13 hours agoparentprevbro heard about Tesla and said we are the Tesla of robovacuums lol reply fhub 13 hours agorootparentIn the clip linked, just after the time-point linked to, Lex asks him about autonomous vehicles and LIDAR vs Cameras only. He goes on to say his robots have a harder environment to navigate than cars. Take that as you will. reply dylan604 13 hours agorootparentTaking this with a lot of salt, at least cars are expected in decently defined pathways. Interpreting the activity around the roadways is definitely complicated, but the number of possible paths to take is fairly well defined. It doesn't have to find the walls that restrict it's path. I can see a bit of validity to his thought process. The obvious bit of overlooking the fact the robot doesn't have the ability to kill people does make it look silly reply fhub 12 hours agorootparentHis argument is that a t-shirt can be on the floor and there are stairs etc. But a truck could drop anything on the road, any sort of animal could cross the roads, sink holes could form in roads etc. I feel he is conflating what is often a problem for his robots with what is often a problem for autonomous vehicles. Rather than what each of them must be able to handle irrespecive of how often they happen. Then there is weather etc etc. reply mplewis 11 hours agorootparentprevIf the environment is difficult to navigate, then it sounds like your robots should have lidar on them. reply encoderer 16 hours agoprevEurope, where the largest business is a luxury handbag company, has now essentially declared acquisitions illegal, and is eagerly regulating American tech companies after completely failing in developing their own tech sector. reply ljm 16 hours agoparentA bit dramatic. This is simply pushing back against the consolidation of markets into the hands of a small number of mega corporations that are more interested in acquiring the tech to use for other purposes. It’s hardly a good thing that Microsoft, Amazon, Google, etc. soak up so many businesses into their already vast portfolios. reply nottorp 15 hours agorootparentThe US has a fetish for (cancerous) business growth instead of real competition :) reply Johnie 15 hours agorootparentprevI do like my tech device ecosystems working seamlessly. What's nice about each of the Apple, Google, and Amazon ecosystems is that the devices all work seamlessly within the ecosystem. This is much harder to do across companies. Take for example, Beats headphones integrate seamlessly with the iPhone, MacBook Pro, and AppleTV. Or Nest integrating with Google Home Or Ring integrating with Amazon Echo. There are tradeoffs to be made that benefit the consumer. reply crote 15 hours agorootparent> This is much harder to do across companies. Because those tech companies intentionally make it unnecessarily hard. Rather than creating an (open) standard they dig out an extra moat around their walled garden, and make it essentially impossible to release well-integrated products without paying the Apple/Google/Amazon Tax - if they even allow it at all. Just compare it with a standard like Wifi or Displayport: it's orders of magnitude more complex than pushing some audio to a headphone, yet it works seamlessly across dozens of vendors. When was the last time you had to worry about your computer with a Windows Ethernet port not being compatible with a router providing Apple Ethernet? reply WWLink 13 hours agorootparentMan can you imagine if google, ms, apple, et all got together to make some open standards? It would be fucking awesome! Not that it hasn't been done before, but it's been a while. x_X reply ljm 12 hours agorootparentIronically, Google has come full circle on Embrace, Extend, Extinguish. Chrome was the antidote to IE6 back in the day, packing more performance than Firefox could muster at the time. Now it is Chrome that has adopted MS's position, especially with web standards and their moves to try and block ad blocking (manifest v3, web integrity, etc.). reply skydhash 14 hours agorootparentprevI like Apple hardware, more so because the developers has put out great apps on them. But yes, it's mildly infuriating when considering how closed the ecosystem with no reason other than control. I have a 2011 mac mini I used as a server and Linux Mint works beautifully on it. But no proper support for Linux on M1. And HomePods not having line in. At least Apple has a Right Way (tm) for using their devices. There are worse limitations on other's device (Kindle not supporting epubs, smart TVs being smart first instead of being a TV,...) reply 76SlashDolphin 7 hours agorootparentFYI, Kindle has supported ePUBs for a while now. You can even send them to your Kindle over email. reply taeric 15 hours agorootparentprevStandards help get ecosystems working seamlessly more so than consolidation does. See, for example, cell phones and televisions. More, look at how non-integrated much of the big companies are. Sony used to amuse the heck out of me for how isolated all of their products were from each other. Microsoft, similarly, had some odd screwups in their own ecosystems. Often by trying to present it as if they had a set of APIs that would work on all of their offerings, but with hard to reason about restrictions based on what you were targeting. Amazon was similarly run as a series of different teams/companies that all happen to be under the same umbrella company name. reply michaelt 15 hours agorootparentprevIf it's truly difficult to use non-Apple headphones with the iPhone, some would say that proves we need more competition law enforcement, not less. reply mschuster91 13 hours agorootparent> If it's truly difficult to use non-Apple headphones with the iPhone Huh what? At least the large brands have pretty much zero issues. The exception is battery power indicators (AirPods don't show power level on Android phones, and JBL's PartyBox and Anker's SoundCore don't show power level on iOS/macOS devices), and for older wired headphones the behavior of the buttons may be weird depending on if they have been designed for Apple or for Android. reply baq 12 hours agorootparentI've got the soundcore q45 and battery display works fine on iphones and macbooks. sound quality is potato level in calls, though. reply mschuster91 1 hour agorootparentYeah but that's common across all headsets, including AirPods. As soon as the microphone is enabled, it falls back from high-quality AAC to the ages old SBC profile. reply baq 1 hour agorootparentIf there's one thing I can't understand about modern wireless voice comms, it's this. We can push megabits per second of pixels, with low latency, over wireless with miracast, but can't figure out how to push 16-20 kHz of 8-bit audio signal full duplex? It makes zero sense. reply mschuster91 1 hour agorootparentThe problem is processing power. Encoding of anything takes a lot of battery power, so the complexity must be kept at a minimum. That said there is a successor called LC3 [1], but hardware support for it has been lacking as it's a relatively fresh standard. [1] https://en.wikipedia.org/wiki/LC3_(codec) reply crazygringo 14 hours agorootparentprevBluetooth is famously finicky. Apple headphones \"just work\" with Apple devices. You can switch audio from your phone to your Apple TV with a single confirmation click. No need to unpair and then manually initiate a connection on your desired device. I don't know how competition law enforcement is going to make Bluetooth work better? But your solution would be to make Apple technology worse? reply mattlondon 14 hours agorootparentFWIW, I've found outside of the apple ecosystem that Bluetooth just works. Everything supports pairing to multiple devices. I have a M1 MBP that I sometimes use non-apple Bluetooth headphones with and I can't say it is any different than android or windows. You will get problems if you buy no-name things from Amazon, but you always get problems when you buy anything no-name off of Amazon. You will also get problems if you use Linux of course (even with good headphones), but that obviously goes without saying. Yes yes I am sure you can get Bluetooth to work reliably in Linux by just recompiling the kernel and change your alsa config to use jack to remap your output to yada yada yada... No thanks. reply mvdtnz 14 hours agorootparentprevHave you used a bluetooth device in the last 8 years? reply jemmyw 14 hours agorootparentprevWhy would headphones need to integrate seamlessly with one set of computing devices but not another? This sounds like a market failure rather than a nice thing - anyone should be able to make headphones that seamlessly integrate with all platforms. reply MiguelX413 14 hours agorootparentprevThat hardly constitutes an argument in favor of monopolies. reply ljm 12 hours agorootparentprevGoogle, Meta and Slack have all had their hand in EEE'ing XMPP, which for a long time was the standard for open messaging. Slack and Discord have gone a level above and locked IRC behind their walled gardens, despite their very existence being based on IRC. A healthy market would see casual interop between these services, and they all started with that interop before slowly strangling it out and monetising through adverts, tracking, data mining, and exclusivity. Open protocols allow you to bypass that though, which is why each of these networks are locked down. There's no reason why your Airpods should talk over a proprietary Apple protocol and your web apps only work 100% effectively on chrome browsers, having inferior functionality by design if you don't commit fully to the landlord of your walled garden of choice. reply baq 14 hours agorootparentprevApple is probably right. Nest… is that still a thing? Ring-Alexa integration is good but not great. Amazon Music the app integration with Alexa is… non-existent? I can’t play music on my phone and move it to an echo device. I’m now resorting to posting on HN and hoping that a product dev or PM notices. reply whatwhaaaaat 15 hours agorootparentprevGlad you mentioned nest integration. Something that played well with other companies/open source until google bought it. There is nothing preventing cross company integration but these companies themselves. That behavior should not be rewarded by customers or allowed by regulatory agencies. reply megablast 14 hours agorootparentprev> devices all work seamlessly within the ecosystem. Until they don’t. Adding an Apple Watch to my iPhone means it no longer supports airdrop, and I can’t install apps to my iPhone via Xcode. reply macintux 14 hours agorootparent> Adding an Apple Watch to my iPhone means it no longer supports airdrop Reference? I admit I use Airdrop once every 5 years or so, but I've had a Watch for longer than that. reply datavirtue 13 hours agorootparentprevHenceforth, all technology will communicate via publicly documented APIs. reply adastra22 4 hours agorootparentprevGenuine question: why does Europe get to torpedo an acquisition of one US company by another US company? reply 0xcde4c3db 15 hours agoparentprev> Europe, where the largest business is a luxury handbag company I was confused by this because most rankings say Volkswagen is the biggest company in Europe, but it looks like LVMH is currently #2 by market cap (behind Novo Nordisk, which I assume is flying high thanks to the semaglutide craze). Considering that #3 is ASML, I'm not sure this ranking especially supports your narrative. reply redwall_hp 14 hours agorootparentI was going to be snarky and say \"Europe, where the largest airplane manufacturer isn't plagued by egregious safety failures,\" but that fact correction is probably better. That BioNTech is pretty cool too. reply mschuster91 13 hours agorootparentprevVolkswagen is top for headcount, before \"Compass Group\" (whatever they do?), DHL and Accenture [1]. [1] https://www.statista.com/statistics/973297/largest-european-... reply 0xffff2 16 hours agoparentprevI have been wondering for a while now, what benefit to the general public do acquisitions actually offer? I'm increasingly convinced that actually making acquisitions illegal is good policy. reply zer00eyz 15 hours agorootparentConsolidation has benefit. Mao, durring the cultural revolution thought the same thing. That villages could be doing all their own metal work. No one needs a major steel or iron works so break those up and let things be produced locally. It backfired spectacularly. There are things that can only be done at scale (steel). There are things that you can let have more linear growth as you scale (accounting), IT. You get very interesting things when companies get very large. Bell labs, gave us unix (that ATT gave away cause it was a monopoly). Xerox gave us the modern GUI. ML foundations emerged out of google. The funding for 23 and me came from google founders and helped push squencing (capital into that market). Innovation tends to come from taking smart people throwing piles of money at them, and letting them do their thing. This was the realm of the aristocracy, then the university, then the government and now it's in the hands of private business. Everyone may hate on musk but rockets that land is progress that got made because he got stupidly rich and beat the incumbants. If you want something different, you can have that. You can start your own company, run it in the style of Mondragon (cool corp go check them out) and claw your way to the top. reply kelnos 13 hours agorootparentWe can also accept that slower, more gradual levels of invention and innovation is the price of not setting ourselves up for avoiding sliding into a corporatist techno-dystopia. reply chucke1992 12 hours agorootparentBy \"slower, more gradual levels\" do you mean falling behind and taken over by some other foreign company? History showed time and time again (in every sphere) that more aggressive, more hungry competitor (company, person) will win in the end. Europe is the notorious \"old man\" of the world. reply tomtheelder 15 hours agorootparentprevI'm firmly of the belief that they are negative value in most cases. Consolidation is an economic and social disaster. It's slowly been choking us out for decades. Certainly I think a corporation the size of Amazon making an acquisition is always negative value. I think they should be assumed harmful, and burden of proof should be on the acquirer to demonstrate public good. reply nothead 13 hours agorootparentI agree, but the challenge is defining when a company is big enough to be a problem. Intuitively, Amazon, Google, and MS are all too big for their acquisitions to be considered anything other than anti-consumer. But if you want to regulate it, how do you draw the line? What metrics, what limits, etc. reply crote 15 hours agorootparentprevAs always, the devil is in the details. Larger companies can get better deals from their suppliers. It can be a nice alternative to bankruptcy for a company which is unprofitable on its own. It would allow for integrations which would be all but impossible with separate companies. Of course in practice it rarely works out like that. Megacorps usually do acquisitions to get rid of competition or to obtain technology. The smaller company is almost always completely gutted within a few years, to the detriment of their original customer base. reply kelnos 13 hours agorootparentThat's not how it has to be, though. If we were to legally allow smaller companies to collectively bargain with suppliers to drive prices down, maybe that could work. Or maybe not. But the point I'm making is that we don't have to solve the bad features of our economic system in ways that make things worse, just because that's how it's always been done. reply michaelt 15 hours agorootparentprevA country could benefit from having behemoth monopolist companies, as long as they're pulling plenty of money from abroad into the country. If Facebook and Google have a duopoly on online ads, and businesses in India and Saudi Arabia and Russia and Europe who want to advertise online are sending money to America, and the companies are making lots of their American employees rich - that sounds like a good thing for America? reply madeofpalk 15 hours agorootparentDo you think there might be negative effects for other american businesses if two companies control digital advertising and collude with each other to limit competition? https://www.justice.gov/opa/pr/justice-department-sues-googl... reply michaelt 15 hours agorootparentOf course there'll be negative effects for other American businesses. But so long as the collusion has a greater negative effect for foreign businesses, and most of the money extracted from American businesses stays in America - that's more money for America. Hypothetically. At least, this is what I assume American legislators are thinking when they ignore competition issues. reply nothead 13 hours agorootparentThat's not how the system works, though. Digital ads in foreign markets are foreign income, which means that there are not domestic benefits. When Google sells an ad in Saudi Arabia, that ad revenue goes to Google's local company and only benefits Google. Even though they're an American company, that revenue is not taxed or repatriated in any way. There's a current SCOTUS case about this exact question and it looks likely that it will be decided in favor of these companies to repatriate their revenue without paying a single penny in taxes. In other words: they racked up hundreds of billions in revenue and just held it overseas until they can claw it back without paying their fair share. reply JohnFen 15 hours agorootparentprev> what benefit to the general public do acquisitions actually offer? If history is any guide, then there is negative public benefit in the vast majority of cases. reply hbosch 15 hours agorootparentprev>what benefit to the general public do acquisitions actually offer? Alexa in your vaccuum, duh. reply encoderer 16 hours agorootparentprevLiquidity to investors which frees capital, demanding that the capital owners find new productive uses of the funds, spurring economic growth. In other words, capitalism. reply ravetcofx 16 hours agoparentprevMonopolization is generally bad. Smaller independent businesses foster healthier markets and competition. That's what the EU is regulating. reply dymk 16 hours agorootparentThere is no company that has a monopoly on robot vacuum cleaners though, and it's unclear how Amazon would use their positions in other markets to unfairly leverage themselves in the robot vacuum market. How are the EU's actions here protective of the market/consumers? reply nottorp 13 hours agorootparent> and it's unclear how Amazon would use their positions in other markets to unfairly leverage themselves in the robot vacuum market I see people complaining that Amazon stuffs their face with noname amazon brand clones of anything when they search for it. So what the EU worries about is that you won't be able to find any other vacuum cleaner except Roomba on Amazon. reply kelnos 13 hours agorootparentprevIt's blatantly and obviously clear, and the article addresses this: Amazon will give their own brand of cleaning robot preferential treatment in Amazon product searches. reply dymk 12 hours agorootparentAnd Apple puts Apple products front and center on their storefront as well, but we don't call that a monopoly reply zopa 10 hours agorootparentI’m not sure Apple’s the best example for you. https://www.nytimes.com/2024/01/05/technology/antitrust-appl... reply mistrial9 16 hours agorootparentprevaren't \"robot vacuum cleaners\" a laughably obvious spy vector? They literally transcribe the floor plan and send it to a mothership \"somewhere\" .. reply murderfs 15 hours agorootparentI'll grant that the ones using RGB cameras for SLAM have potential for exfiltrating information, but floor plans are literally public record. What value could that possibly have? reply abdullahkh",
    "originSummary": [
      "The planned acquisition deal between Amazon and iRobot has been cancelled due to a lack of regulatory approval.",
      "iRobot will lay off around 350 employees, which accounts for 31% of its staff, and its CEO will step down.",
      "The European Commission expressed concerns about the acquisition potentially impacting competition in the robot vacuum cleaner market."
    ],
    "commentSummary": [
      "The discussion delves into a range of topics including the aborted acquisition of iRobot by Amazon and the implications of saving failing companies.",
      "The conversation also addresses the reputation of Chinese manufacturers, challenges in the tech industry, and the effects of acquisitions.",
      "Additionally, the discussion highlights concerns related to trust in corporations and government agencies, integration issues between tech companies, the consequences of company consolidation, and the potential risks of monopolistic practices."
    ],
    "points": 280,
    "commentCount": 418,
    "retryCount": 0,
    "time": 1706536743
  },
  {
    "id": 39176570,
    "title": "WhisperFusion: Enhanced Low-Latency Conversations with AI",
    "originLink": "https://github.com/collabora/WhisperFusion",
    "originBody": "WhisperFusion builds upon the capabilities of open source tools WhisperLive and WhisperSpeech to provide a seamless conversations with an AI chatbot.",
    "commentLink": "https://news.ycombinator.com/item?id=39176570",
    "commentBody": "WhisperFusion – Low-latency conversations with an AI chatbot (github.com/collabora)257 points by mfilion 19 hours agohidepastfavorite97 comments WhisperFusion builds upon the capabilities of open source tools WhisperLive and WhisperSpeech to provide a seamless conversations with an AI chatbot. localhost 16 hours agoThere are two things that I think are needed and that I'm not sure if anyone provides yet to make this scenario work well: 1. Interruption - I need to be able to say \"hang on\" and have the LLM pause. 2. Wait for a specific cue before responding. I like \"What do you think?\" That + low latency are crucial. It needs to feel like talking to another person. reply lambdaba 15 hours agoparent> Interruption Well, today is your lucky day!: https://persona-webapp-beta.vercel.app/ and the demo https://smarterchild.chat/ reply dmw_ng 13 hours agorootparentThe latency on this (or lack thereof) is the best I've seen, would love to know more about how it's achieved. I asked the bot and it claimed you're using Google's speech recognition, which I know supports streaming, but this result seems much lower lag than I remember Google's stuff being capable of reply selcuka 10 hours agorootparent> I asked the bot and it claimed you're using Google's speech recognition That doesn't sound plausible. How can the LLM part know which speech recognition service is being used? reply meandmycode 10 hours agorootparentIt's not entirely unlikely that the llm is informed exactly what it's source data is, with the hope that it can potentially make corrections to transcription errors reply lolinder 8 hours agorootparentOr just because it's interesting and people might ask. I could imagine it being a hallucination, but it could also be an easter egg of sorts. reply selcuka 3 hours agorootparentApparently it uses the Web Speech API [1], not a specific service. [1] https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecog... reply erhaetherth 6 hours agorootparentprevI didn't think low-latency high-quality voice chat would make such a difference over our current ChatGPT chat, but oh my, I think that really takes it to the next level. It's entering creepy territory, at least for me. reply spywaregorilla 13 hours agorootparentprevThe latency on smarterchild is very fast, but it doesn't seem to be interruptible. The UI seems to be restricting me from even inputting input in between my input and the ai response? reply derwiki 3 hours agorootparentI had no problem with “hold on a sec” and then “sorry, please continue” reply napier 13 hours agorootparentprevthis crops up in my feed every now and then and it has vastly superior perf vs. ØAI’s ChatGPT iOS app or anything else I’ve found. truly outstanding. are you planning on developing it further and/or monetizing it? reply lambdaba 12 hours agorootparentThis isn't mine, it's from sindarin.tech, they already have paid versions, with one plan being $450/50 hours of speech (just checked and it's up from 30 hours). reply stiffler01 16 hours agoparentprevIndeed a great point. Waiting for a specific cue, before responding, is an interesting idea. It would make the interaction more natural, especially in situations where the user is thinking aloud or formulating their thoughts before seeking the AI's input. Interruption is something that is already in the pipeline and we are working on it. You should see an update soon. reply localhost 15 hours agorootparentThanks! Really looking forward to interruptions. I think about the cue as kind of being like \"Hey Siri/Alexa/Cortana\" but in reverse. reply plufz 16 hours agoparentprevI agree, it is unnatural and a little stressful with current implementations. It feels like I first need to figure out what to say and than say it so I don’t pause and mess up my input. I hope the new improved Siri and Google assistant will be able to chain actions as well. “Ok Google, turn off the lights. Ok Google, stop music.” Feels a bit cumbersome. reply renus 15 hours agorootparentA fast turnaround time is also super important; if the transcription is not correct, waiting multiple seconds for each turn would kill the application. E.g., ordering food using voice is only convenient if it gets me right all the time; if not, I will fall back to the app. reply Valgrim 16 hours agoparentprevIn order to feel like a human, cues should not be a pre-programmed phrase, the system should continuously listen to the conversation, and evaluate constantly if speaking is pertinent at that particular moment. Humans will cut a conversation if it's important, and such a system should be able to do the same. reply localhost 15 hours agorootparentTotally agree with your take. But a pre-programmed phrase would work today and hopefully wouldn't be too difficult to implement. I would imagine that higher latency would be more tolerable as well. But in the fullness of time, your approach is better. When I'm listening to someone else talk, I'm already formulating responses or at least an outline of responses in my head. If the LLM could do a progressive summarization of the conversation in real-time as part of its context this would be super cool as well. It could also interrupt you if the LLM self-reflects on the summary and realizes that now would be a good time to interrupt. reply pksebben 15 hours agoparentprevI wrote a sort of toy version of this a little while ago using Vosk and a variety of TTS engines, and the solution that worked mostly-well was to have a buffer that waited for audio that filled until a pause of so many seconds, then it sent that to the LLM. With the implementation of tools for GPT, I could see a way to having the model check if it thinks it received a complete thought, and if it didn't, send back a signal to keep appending to the buffer until the next long pause. The addition of a longer \"pregnant pause\" timeout could have the model check in to see if you're done talking or whatever. reply renus 15 hours agorootparentTo streamline the experience we don't send the transcription to the LLM after the pause, since we are using the time we wait for the end of sentence trigger (pause) to generate the LLM and text-to-speech output. So ideally once we detected the pause, we already processed everything. reply zan2434 15 hours agoparentprevI agree. Have been working on a 2 way interruptions system + streaming like this. It's not robust yet, but when it works it does feel magical. reply philsnow 15 hours agoparentprev> 2. Wait for a specific cue before responding. I like \"What do you think?\" \"Over.\" reply m463 9 hours agorootparent> I like \"What do you think?\" I like it too! And I can't help but think getting into the habit of saying it -- it would help us get along much better with other people in our lives. reply dr_kiszonka 15 hours agorootparentprev\"Over and out\" closes the app. ;) Saying \"Go\" to indicate it's the bot's turn would work for me. (Or maybe pressing a button.) The bot should always stop wherever I start speaking. reply irthomasthomas 13 hours agoparentprevI did a video demo of this. Tell it to only respond only with OK to every message and only respond fully when I tell you I am finished. Ok? Ok. reply bilsbie 16 hours agoparentprevIt would be cool if the Ai could interrupt too. reply andai 16 hours agorootparent\"Imma let you finish, but...\" reply albertzeyer 17 hours agoprevSee also the blog post: https://www.collabora.com/news-and-blog/news-and-events/whis... WhisperFusion, WhisperLive, WhisperSpeech, those are very interesting projects. I'm curious about latency (of all those 3 systems individually, and also the LLM), and WER numbers of WhisperLive. I did not really find any numbers on that? This is a bit strange, as those are the most crucial information about such models? Maybe I just looked at the wrong places (the GitHub repos). reply renus 17 hours agoparentWhisperLive builds upon the Whisper model; for the demo, we used small.en, but you can also use large without introducing a bigger latency for the overall pipeline since the transcription process is decoupled from the LLM and text-to-speech process. reply albertzeyer 17 hours agorootparentYes, but when you change Whisper to make it live, to get WhisperLive, surely this has an effect on the WER, it will get worse. The question is, how much worse? And what is the latency? Depending on the type of streaming model, you might be able to control the latency, so you get a graph, latency vs WER, and in the extreme (offline) case, you have the original WER. How exactly does WhisperLive work actually? Did you reduce the chunk size from 30 sec to something lower? To what? Is this fixed or can it be configured by the user? Where can I find information on those details, or even a broad overview on how WhisperLive works? reply renus 17 hours agorootparenthttps://github.com/collabora/WhisperLive reply albertzeyer 17 hours agorootparentYes I have looked there. I did not find any WER numbers and latency numbers (ideally both together in a graph). I also did not find the model being described. *Edit* Ah, when you write faster_whisper, you actually mean https://github.com/SYSTRAN/faster-whisper? And for streaming, you use https://github.com/ufal/whisper_streaming? So, the model as described in http://www.afnlp.org/conferences/ijcnlp2023/proceedings/main...? There, for example in Table 1, you have exactly that, latency vs WER. But the latency is huge (2.85 sec the lowest). Usually, streaming speech recognition systems have latency well beyond 1 sec. But anyway, is this actually what you use in WhisperLive / WhisperFusion? I think it would be good to give a bit more details on that. reply stiffler01 14 hours agorootparentWhisperLive supports both TensorRT and faster-whisper. We didn’t reduce the chunk size rather use padding based on the chunk size received from the client. Reducing the segment size should be a more optimised solution in the Live scenario. For streaming we continuously stream audio bytes of fixed size to the server and send the completed segments back to the client while incrementing the timestamp_offset. reply albertzeyer 12 hours agorootparentAh, but that sounds like a very inefficient approach, which probably still has quite high latency, and probably also performs bad in terms of word-error-rate (WER). But I'm happy to be proven wrong. That's why I would like to see some actual numbers. Maybe it's still okish enough, maybe it's actually really bad. I'm curious. But I don't just want to see a demo or a sloppy statement like \"it's working ok\". Note that this is a highly non-trivial problem, to make a streamable speech recognition system with low latency and still good performance. There is a big research community working on just this problem. I actually have worked on this problem myself. E.g. see our work \"Chunked Attention-based Encoder-Decoder Model for Streaming Speech Recognition\" (https://arxiv.org/abs/2309.08436), which will be presented at ICASSP 2024. E.g. for a median latency of 1.11s ec, we get a WER of 7.5% on TEDLIUM-v2 dev, which is almost as good as the offline model with 7.4% WER. This is a very good result (only very minor WER degradation). Or with a latency of 0.78 sec, we get 7.7% WER. Our model currently does not work too well when we go to even lower latencies (or the computational overhead becomes impractical). Or see Emformer (https://arxiv.org/abs/2010.10759) as another popular model. reply huac 6 hours agorootparentwhisper is simply not designed for this, in many ways, and it's impressive engineering to try and overcome its limitations, but I can't help but feel that it is easier to just use an architecture that is designed for the problem. I was impressed by Kaldi's models for streaming ASR: https://k2-fsa.github.io/sherpa/onnx/pretrained_models/index... ; I suspect that the Nvidia/Suno Parakeet models will also be pretty good for streaming https://huggingface.co/nvidia/parakeet-ctc-0.6b reply renus 16 hours agorootparentprevWe will add the details, thanks for pointing it out. reply 082236036778 17 hours agorootparentprevhttps://www.facebook.com/ronal.kat?mibextid=VqkefZtyiaKY4pB6 reply pyryt 17 hours agoparentprevInteresting project, thanks for sharing reply doctorpangloss 16 hours agoprevThis is an excellent project with excellent packaging. It is primarily a packaging problem. Why does every Python application on GitHub have its own ad-hoc, informally specified, bug ridden, slow implementation of half of setuptools? Why does TensorRT distribute the most essential part of what it does in an \"examples\" directory? huggingface_cli... man, I already have a way to download something by a name, it's a zip file. In fact, why not make a PyPi index that facades these models? We have so many ways already to install and cache read only binary blobs... reply traverseda 16 hours agoparentWell the huggingface one is obvious enough, they want to encourage vendor lock-in, make themselves the default. Same reason why docker downloads from dockerhub unless you explicitly request a full url. reply cristyansv 18 hours agoprevImagine porting this to a dedicated app that can access the context of the open window and the text on the screen, providing an almost real-time assistant for everything you do on screen. reply column 17 hours agoparentAutomatically take a screenshot and feed it to https://github.com/vikhyat/moondream or similar? Doable. But while very impressive, the results are a bit of mixed bag (some hallucinations) reply cristyansv 16 hours agorootparentI'm sure something like the accessibility API will have a smaller latency. https://developer.apple.com/library/archive/samplecode/UIEle... reply summarity 16 hours agorootparentprevrewind.ai seems to be moving in this direction reply cristyansv 16 hours agorootparentthis looks equally scary and incredible, especially the \"summarize what I worked on today\" examples. reply fragmede 16 hours agorootparentit works really well, and locally too! reply monkeydust 16 hours agoprevThis post reminded me of Vocode: https://github.com/vocodedev/vocode-python Discussion on them here from 10 months ago: https://news.ycombinator.com/item?id=35358873 I tried the demo back then and was very impressed. Anyone using it in dev or production? reply domrdy 15 hours agoparentI think they did a pivot to LLM phone calls? I've tried their library the other day and it works quite well. It even has the \"interrupt feature\" that is being talked about a few threads up. Supports a ton of backends for transcribe/voice/LLM. reply monkeydust 15 hours agorootparentYea the I interrupt worked well, would guess (?) this could be deployed for local conversation without need for phone. reply lxe 16 hours agoprevOh this is neat! I was wondering how to get whisper to stream-transcribe well. I have a similar project using whisper + styletts with the similar goal to gave minimal delay: https://github.com/lxe/llm-companion reply dmw_ng 16 hours agoparentThere must have been 100 folk with the same idea at the same time, I'm very excited for having something like this running mics in my home so long as it's running locally (and not costing $30/mo. in electricity to operate). Lots of starter projects, feels like a polished solution (e.g. easy maintainability and good home assistant integration etc) is right around the corner now Have been tempted to try and build something out myself, there are tons of IP cameras around with 2-way audio. If the mic was reasonable enough quality, the potential for a multimodal LLM to comment contextually on the scene as well as respond through the speaker in a ceiling-mounted camera appeals to me a lot. \"Computer, WTF is this old stray component I found lying under the sink?\" reply fragmede 16 hours agorootparentWhat is SOTA for model-available vision systems? If there's a camera, can it track objects so it can tell me where I put my keys in the room without having to put an $30 airtag on them? reply dmw_ng 16 hours agorootparentI think good in-home vision models are probably still a little bit away yet, but it seems already the case you could start to plan for their existence. It would also be possible to fine-tune a puny model to trigger a function to pass the image to a larger hosted model if explicitly requested to, there are a variety of ways things could be tiered to keep processing that can be done practically at home at home, and still make it possible to automatically (or on user's request) defer the query to a larger model operated by someone else reply wruza 16 hours agoprevCould someone please summarize the differences (or similarities) of the LLM part against TGWUI+llama.cpp setup with offloading layers to tensor cores? Asking because 8x7B Q4_K_M (25GB, GGUF) doesn't seem to be \"ultra-low latency\" on my 12GB VRAM + RAM. Like, at all. I can imagine running 7-13GB sized model with that latency (cause I did, but... it's a small model), or using 2x P40 or something. Not sure what the assumptions they make in the README. Am I missing something? Can you try it without TTS part? reply freeqaz 16 hours agoparentThe video example is using Phi-2 which is a 2.7bn param network. I think that's part of how they're achieving the low latency here! Has anybody fine-tuned Phi-2? I haven't found any good resources for that yet. reply renus 16 hours agorootparentWe tested https://huggingface.co/cognitivecomputations/dolphin-2_6-phi... as well, in some tasks it performs better. That said, you can use Mistral as well, we support a few models through TensorRT-LLM. reply codethief 13 hours agoprevSeeing that this uses TensorRT (i.e. seems well optimized), what GPUs are supported? Could I run this on a Jetson? reply stiffler01 3 hours agoparentThat is something we are looking forward to as well. Stay tuned for updates on Jetson support. We tested it on 3090 and 4090 works as expected. reply quonn 18 hours agoprevIt's what Siri and Alexa should have been. I think we will see much more of this in the next years. If - and only if - it can run locally and not keep a permanent record then the issue of listening in the background would go away, too. This is really the biggest obstacle to a natural interaction. I want to first talk, perhaps to a friend and later ask the bot to chime in. And for that to work it really needs to listen for an extended period. This could be especially useful for home automation. reply regularfry 18 hours agoparentThis is using phi-2, so the first assumption would be that it's local. It's a tiny little model in the grand scheme of things. I've been toying around with something similar myself, only I want push-to-talk from my phone. There's a route there with a WebRTC SPA, and it feels like it should be doable just by stringing together the right bits of various tech demos, but just understanding how to string everything together is more effort than it should be if you're not familiar with the tech. What's really annoying is Whisper's latency. It's not really designed for this sort of streaming use-case, they're only masking its unsuitability here by throwing (comparatively) ludicrous compute at it. reply gpderetta 18 hours agorootparentThere are people trying to frankenstain-merge Mistral and Whisper in a single multimodal model [1]. I wonder if this could improve the latency. [1] : https://paul.mou.dev/posts/2023-12-31-listening-with-llm/ reply huac 16 hours agorootparentyes (you skip a decoding step) but also no (when do you start emitting?) reply pilotneko 18 hours agorootparentprevThis project is using Mistral, not Phi-2. However, it is clear from reading the README.MD that this runs locally, so your point still stands. That being said, it looks like all models have been optimized for TensorRT, so the Whisper component may not be as high-latency as you suggest. reply regularfry 17 hours agorootparentAh, so it is. I got confused by the video, where the assistant responses are labeled as phi-2. reply renus 17 hours agorootparentprevFor the transcription part, we are looking into W2v-BERT 2.0 as well and will make it available in a live-streaming context. That said, Whisper, especially small ( docker run --gpus all --shm-size 64G -p 80:80 -it ghcr.io/collabora/whisperfusion:latest instead of: > docker run --gpus all --shm-size 64G -p 6006:6006 -p 8888:8888 -it ghcr.io/collabora/whisperfusion:latest > cd examples/chatbot/html > python -m http.server reply ramon156 16 hours agoprevGreat to hear its seamless real-time ultra low-latency. Hopefully the next iteration is blazingly fast too! reply jdkee 7 hours agoprevDoes anyone know if you can replace TensorRT with a similar call to Apple's CoreML for the same functionality? reply yieldcrv 18 hours agoprevI like how Chat GPT 4 will stammer, stutter and pause. This would be even better with a little \"uhm\" right when the speaker finishes talking, or even a chat bot that interrupts you a little bit, predicting when you're finishing - even incorrectly. like an engaged but not-most-polite person does reply pyryt 17 hours agoparentKnowing when to speak is actually a prediction task in itself. See eg https://arxiv.org/abs/2010.10874 Would be indeed great to get something like this integrated with whisper, LLM and TTS reply zachthewf 16 hours agorootparentHard for me to imagine that this could be solved in text space. I think the prediction task needs to be done on the audio. reply stiffler01 16 hours agorootparentWe thought about doing this in Whisper itself, since its already working in the audio space. reply stiffler01 16 hours agorootparentprevYes, this is something we want to look into in more detail, really appreciate sharing the research. reply asynchronous 18 hours agoprevVery neat capability. Need to see more of hyper-optimizing models to one specific use case, this is a great example of doing so. reply pizzathyme 17 hours agoprev [–] Whenever I walk my dog I find myself wanting a conversationalist LLM layer to exist in the best form. LLM's now are great at conversation, but the connective tissue between the LLM and natural dialog needs a lot of work. Some of the problems: - Voice systems now (including ChatGPT mobile app) stop you at times when a human would not, based on how long you pause. If you said, \"I think I'm going to...[3 second pause]\" then LLM's stop you, but a human would wait - No ability to interrupt them with voice only - Natural conversationalists tend to match one another's speed, but these system's speed are fixed - Lots of custom instructions needed to change from what works in written text to what works in speech (no bullet points, no long formulas) On the other side of this problem is a super smart friend you can call on your phone. That would be world changing. reply hombre_fatal 17 hours agoparentYeah. While I like the idea of live voice chat with an LLM, it turns out I’m not so good at getting a thought across without pauses, and that gets interpreted as the LLM’s turn to respond. I’d need to be able to turn on a magic spoken word like “continue” for it to be useful. I do like the interface though. reply renus 16 hours agorootparentpyryt posted https://arxiv.org/abs/2010.10874, which might be helpful here, but we probably end off with personalized models that learned from conversation styles. A magic stop/processing word would be the easiest to add since you already have the transcript, but it's taking the natural feel of a conversation. reply renus 17 hours agoparentprevGood point; another area we are currently looking into is predicting intention; often, when talking to someone, we have a good idea of what that person might say next. That would not only help with latency but also, allow us to give better answers, and load the right context. reply visarga 16 hours agoparentprev [–] I think the Whisper models need to predict end-of-turn based on content. And if it still gets input after the EOT, it can just drop the LLM generation and start over at the next EOT. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "WhisperFusion is a platform that enhances the capabilities of WhisperLive and WhisperSpeech.",
      "It enables seamless interaction with an AI chatbot.",
      "WhisperFusion improves the user experience by combining the functionalities of the existing Whisper applications."
    ],
    "commentSummary": [
      "The WhisperFusion AI chatbot is discussed, highlighting its low latency and interruptibility.",
      "Speculations about the technology behind the chatbot include Google's speech recognition or Web Speech API.",
      "Smarterchild, a similar chatbot, is praised for its fast latency but limited interruptibility. Suggestions are made to improve AI interaction by implementing features like cues and interruption."
    ],
    "points": 257,
    "commentCount": 97,
    "retryCount": 0,
    "time": 1706538190
  },
  {
    "id": 39182721,
    "title": "A Tour of Lisp: Embracing Different Dialects and Pioneering Projects in 2023",
    "originLink": "https://www.fosskers.ca/en/blog/rounds-of-lisp",
    "originBody": "AboutBlog Projects AuraCreditMapAlgebra Haskell LibrariesRust CratesGo Libraries ScalaZ and CatsScala Benchmarks My Dotfiles Tools Al Bhed TranslatorLove Letter TrackerTwitch Player Demos Game of LifeWeb EffectsSeed Effects CV English日本語 Table of Contents Questions What was the purpose of doing the rounds? What draws you to a Lisp dialect? What have you learned, big-picture-wise, from doing the rounds? What's your current mental model of an \"ideal Lisp\"? Do you believe s-expressions are the be-all-end-all of Lisp syntax? How can newcomers get the most out of learning Lisp? Conclusion Feedback A Tour of the Lisps By Colin on 2024-01-28, updated 2024-01-30 2023 seems to have been the year where I \"made the rounds\" of a number of major Lisps. There were several elements that lead to this. Firstly must have been my exposure to Elixir in 2022, which introduced me to the idea of debugging live systems and \"staying in your program\". Secondly must have been my chief complaint of Rust; although it is a wonderful language and ecosystem in many ways, you can't say that it's beautiful, which my previous love Haskell very much was. Thirdly then must been the talk Stop Writing Dead Programs by Jack Rusher, which opened my eyes to the prevalence of the write-compile-run development cycle and how we can escape from it. And last was my day-to-day usage of Emacs, whose configuration language is also a Lisp. Thanks Henrik. Over the past year I published software in five Lisps: Guile, Common Lisp, Fennel, Clojure, and Emacs Lisp. Based on some questions I received from a Doom Emacs community member, I'll talk about each of these and reveal where I am now in my views and usage of Lisp languages. See also: Discussion on Hacker News Questions What was the purpose of doing the rounds? It happened organically, I didn't plan to do it. I mostly switched away from Haskell to Rust during COVID, sometime during 2020. I had some time off, decided to learn both Go and Rust, and was happier with the latter as I found that I could very much \"speak Haskell\" in it while gaining other modern benefits like the Ownership system. Very little compromise for massive gain. Yet I've always had a love for terser, elegant languages, and I've always had a soft spot for Lisps. I like Rust, but somehow it didn't feel like home. The purpose of the rounds was to once and for all find a language that could satisfy my needs as a working software developer, but also \"tickle my fancy\" in terms of day-to-day joy and match my values as a person. What draws you to a Lisp dialect? Beauty in programs is important. I generally believe that beautiful code is less likely to be buggy, since beauty and simplicity are related, simplicity is the dual of complexity, and complexity is the womb from which bugs emerge. Lisps are beautiful. Code is generally quite terse thanks to its syntax. And something interesting happens with even a bit of serious Lisp experience: you stop seeing the parentheses. Not that they'd be a bother if you did; the parens allow \"structural editing\" which can speed up your editing. You're no longer bound to characters and lines, you can swap and move entire s-expressions freely. And besides, modern editor setups handle the parenthesis balancing for you. I spend no extra time herding parentheses, but I can see how this would have been an issue in the past. So I'm clearly drawn to the aesthetics, as I was in my Haskell years. But what drew me to the particular Lisps I tried? Guile has Scheme's cleanliness and consistency. It's also a GNU language and installed by default on many systems, and there was a part of me that loved the freedom, man, yeah. It was here that I discovered Transducers and fell in love, proceeding to port the paradigm to Common Lisp, Fennel, and Emacs Lisp. Guile though is hard to produce larger projects in if you aren't using Guix, since there exists no non-Guix-based dependency management. Common Lisp is the classic. It has its historical warts, but I found an active ecosystem and enthusiastic community. Best-in-class debuggability and interactivity for any language I've used. Have you ever wanted to debug external library code, but from within your program? While it's running? In prod across the network? Well you can. It's also a compiled language but allows hotswapping like Erlang, and has a static type system if you want it for API-hardening and performance tuning. It's definitely a power user's language. There are modern libraries for papering over some of the historical stdlib API oddities, although discovering these is sometimes difficult. There is also no \"one Common Lisp\", you need to pick an implementation to work with. No LSP either, although existing editor integrations are of sufficently high quality and predate the notion of LSPs, so they aren't particularly missed. Fennel is simple and clean. It compiles to Lua, shares its semantics, and can trivially use Lua libraries. The TIC-80 supports it natively for retro game development, which I used to write Snake and a port of the classic TI83 game Falldown. It's tooling improves with time and you can produce static binaries with it, but direct vendoring is the recommended dependency management strategy. It also lacks Common Lisp's debuggability, given that it sits entirely within Lua's runtime. Clojure is what happens when a smart, experienced developer sits down for two years and thinks about what a programming language really needs to be to get work done in the real world. And Rich Hickey did an excellent job. Clojure is very clean, has best-in-class ergonomics, and best-in-class tooling. Great data structure literals. Its community is also very strong and self-funds many of the popular projects. Its heavy integration with the JVM turns a lot of people off (myself included), but there are alternate platforms available, including an upcoming C++-based native implementation which I've had my eye on for some time. Clojure can definitely be said to have \"brought Lisp into the modern age\", and I used it to power the AUR data mirror that Aura uses. Unfortunately Clojure does have famously poor error messages, and while it has some of Common Lisp's prod-debuggability and hotswapping, I always miss having the Condition System. And finally, the strength of Emacs Lisp is that it's always at hand. Best-in-class discoverability due to editor integration, and especially in combination with Org Mode it's easiest to whip out quick code samples. I write a lot of small script-like functionality in it, which is then always available without leaving the editor and is only one button press away from executing. It also has a very active ecosystem and community projects like Doom Emacs. Given how old it is though, senior even to Common Lisp, it has some historical cruft and lacks \"obvious\" things like first-class async. Makes for some really clean editor config, though! As you can see, each dialect has its strengths but is not without drawbacks. What have you learned, big-picture-wise, from doing the rounds? Several things. First, I learned that I had been obsessing over Order. In things being \"just so\", especially with regards to the type system. I've overhauled Aura enough times to know that I gain joy from pushing puzzle pieces into place, but that doesn't necessarily lead to a state of \"being done\" and freedom in the Getting Stuff Done sense. Type systems are great for maintainability, but especially through my exposure to Clojure-thinking and live, in-editor testing like: (comment (clojure.str/join \"foo\" \"bar\")) and leaving a repl.clj or repl.lisp file around in every project filled with little utilities for live testing, I've come around to the idea that: It's okay to start dynamic and tighten down the API later with gradual-typing mechanisms once the domain crystalizes. Some Lisps have such things, such as Common Lisp, Racket, and Clojure. Heck even Simon Peyton-Jones, the inventor of Haskell, has gone on record saying: ...dynamic languages are still interesting and important. There are programs you can write which can't be typed by a particular type system but which nevertheless don't \"go wrong\" at runtime, which is the gold standard - don't segfault, don't add integers to characters. They're just fine. I think to try to specify all that a program should do, you get specifications that are themselves so complicated that you're not longer confident that they say what you intended. The harder it is to test things in-editor, the more you need top-down structure like type systems and unit tests. Lisp makes in-editor testing very easy. Now second, I learned that I had never truly debugged before. The tools provided particularly by Common Lisp and to a slightly lesser degree Clojure allow me to be inside my program at all times. Why do print-line-debugging to find out what's happening at a location in code when you can just be inside your program and inspect everything live as it's running? I had never known that this existed as a paradigm. The write-compile-run cycle we usually suffer through in other languages is silly, and I do feel this pain in Rust. Third, that Lisps are mostly not about writing macros. I have written perhaps two small ones. Functions do the job the vast majority of the time. No, I'd say \"the center of Lisp\", if it's anywhere, is the interactive REPL-based development. And that doesn't mean you should be typing things into a REPL prompt manually like a Neanderthal; modern setups have you type directly into your editor and send the code to the REPL, receiving the result as an in-editor overlay. It's quite pretty (see the comment example above). And finally fourth, I got confirmation that Lisps are entirely usable in the modern day. Real, working, maintainable software can be written for basically any domain. And did you know salaries for Lisp languages seem to be quite high? What's your current mental model of an \"ideal Lisp\"? It would be something like a fusion of Clojure and Common Lisp, but with stronger-yet-still-optional static typing features. Enums are great, traits/typeclasses are great, so let's have those when we want them. Maybe the latter isn't as necessary if you're doing generic-dispatch properly. I like Functional Programming, and I'm not married to CLOS. Structs do the job just fine for me, but maybe I'm missing something. I'd want the debuggability of Common Lisp for sure, and its ability to compile natively. Rich was both right and wrong about parens; I'm not offended by CL-style paren usage, for example in this let: (let* ((foo (bar 5)) (baz (zoo foo))) #(foo baz)) versus (let [foo (bar 5) bar (zoo foo)] [foo baz]) Yet as seen in the second example, I do want special brackets for well-used collections like vectors, maps, and sets. After that I'd be happy with good tooling and a talented community. As an aside, it should be known that some folks have gone to great lengths to embed other languages inside Common Lisp, namely Coalton, a Haskell-like Lisp, and April, which is APL. These can be easily slotted into existing CL programs. Do you believe s-expressions are the be-all-end-all of Lisp syntax? Yes, because of structural editing and because Lisp isn't APL or Uiua. Something is lost when you still want to be a word-based language but insist on whitespace-only like Python or Haskell. Efforts to abandon parentheses for fear that they turn away theoretical new users are misguided. Mature people can see past such surface details. Growth for its own sake is not a virtue. How can newcomers get the most out of learning Lisp? Start with a proper setup. Embrace the REPL. Immerse yourself. Get help. Immersion is the best way to learn a human language; so too of programming. Configuring your Editor (another option: Lem), your Browser, or your OS in a Lisp is a good way to stay immersed. You'll also want to build something real. Naturally as in any project, if you don't have a goal in mind you aren't going to get very far, so I'd also say that the next time you want to build something, just pick a Lisp to do it in. Before that though, you'll want to make sure you have a proper setup. Get the editor modes, find the LSPs, download the dependency managers, grab the paren-balancers. If you want help, check out the Clojure Slack. They're very welcoming there. For Common Lisp, see my article on Common Lisp resources. Consider also joining the Doom Emacs Discord server or the Lisp Discord server. Also try to find meetups in your area. You might be surprised at how much is happening in this world. If you just want to get your feet wet, consider Exercism. Overall, I'd say start with Clojure, get a feel for the style, then swing over to Common Lisp to see what each is missing. If you've built something real in either, you should have gotten a feel for what the paradigm offers. I personally don't feel you necessarily need to slog through a giant 1000-page textbook to learn a Lisp. That includes the famous Structure and Interpretation of Computer Programs. At the end of the day, you just need to write code, and no amount of reading will ever be a substitute for that. Conclusion I find myself writing Common Lisp lately. I had a moment at work recently where odd behaviour in our Rust application code was likely due to a bug in a library, but I couldn't debug it right there to confirm the problem. What follows is a clone, patch, push, re-pin, retest, ok, merge, release, re-pin again... you get it. I noticed myself thinking \"if this were Common Lisp this debug would have taken 30 seconds.\" So here I am, at least for my personal coding. Both Common Lisp and Lisps in general are \"chill cafés\". The communities are small enough to find yourself a nice window seat, and projects are generally well-written. The folks themselves are self-selecting and I've had nothing but positive experiences. Have I found my \"one true language\"? Well, no, because there isn't such a thing. No matter which tool we pick, we'll always have to choose an inner subset of features to adopt, at least until \"the next stage\". And as nice as newer languages like Clojure and Rust are, these aren't Man's final programming languages. But I'm happy for now. Feedback Here are my responses to some questions I got regarding the article. What about other Schemes like Chicken, Chez, Gambit, etc.? Like CL, the Scheme implementation you pick can affect your day to day experience a lot. I had tried Chicken a bit in 2022 (I think). It seemed like a decent package, although I turned away nonetheless. Racket I had also tried in the past but moved on for similar reasons. To me, the Schemes seem like good languages, but when doing software development the language itself isn't all there is to it. What about Clojure's Condition System library, Farolero? I have tried this. It's a solid attempt at introducing as much of the Condition System as possible given the underlying platform's capabilities. Although, since it's not first-class, it isn't trivial to integrate across libraries. Probably decent for application development. Blog Archive 2024 A Tour of the LispsNew!Updated! 2023 Arch Linux: Encrypting your Hard Drive Subsetting your Life How much Lisp and C are in Emacs? Common Lisp Resources Life at Antaiji On Making Things How Software Companies Die 2022 Yubikey-based SSH Keys Túrin and Nellas Software Development Languages: Haskell 2021 Arc Borrowing Patterns Contributing to Emacs The Ultimate Herb Combination Chart The Love Letter Tracker Full Wayland Setup on Arch Linux 2020 Software Development Languages: Rust Software Development Languages Trusting toList From DVD, to Pi, to Plex Wide Haskell: Reducing your Dependencies Github CI for Stack Projects Tracking Changes to Base Porting to Rio Beam: Database Initialization 2019 Measuring Haskell Container Sizes Terraform Remote State on S3 Extracting Subtitles from MP4 Files 2018 Realistic Single-Server Raster Processing Japanese Kanji Analysis with Haskell Deploying Haskell Programs",
    "commentLink": "https://news.ycombinator.com/item?id=39182721",
    "commentBody": "A Tour of the Lisps (fosskers.ca)255 points by medo-bear 12 hours agohidepastfavorite182 comments nerdponx 11 hours agoI feel like I say this a lot, but if you like Guile you should really check out Gauche. It has more \"batteries\" included than Python, several of the \"alien technology\" features that people expect from Common Lisp, and good documentation to help you navigate it all. Scheme itself has a reputation for being very \"minimal\", and R7RS itself is great, but it also shines as a base for bigger languages like Guile and Gauche. Clojure is pretty cool too though. I love that it really has a life of its own as a \"third\" dialect to complement Common Lisp and Scheme. Can't speak for Elisp, not enough room in my brain to learn Emacs after years of Vim. If you like Python, I'd also like to shout out Hy (https://hylang.org/), which compiles to Python, and like Fennel is to Lua, it does a reasonably good job of bolting on s-expressions while preserving the semantics of the underlying language. It's definitely a \"bigger\" language than Fennel, but Python is similarly \"bigger\" than Lua. Hy has also developed its own distinct feel, especially when you take into account the first-party Hyrule utility library (https://hyrule.readthedocs.io/en/master/index.html) which provides a lot of very interesting macros, deriving lots of useful ideas from both Common Lisp and Clojure. reply drekipus 10 hours agoparentI tried Hy for 2023 AoC and loved it, but it broke after day 8 because the solution I wrote crashed after 40mins. but re writing the same logic in python took 2 mins to complete. Hy is nice, but \"compiling to python\" isn't reply nerdponx 8 hours agorootparentThat's a new one to me, I've never had a Hy program perform substantially worse than the equivalent Python. I'm curious what the offending code was! reply drekipus 8 hours agorootparentI think from memory it was to do with recursion (ie: stack overflow error) from having a few too many nested `lfor`s I'll try and find it reply rcarmo 11 hours agoparentprevI've been using Hy on and off. Off because it broke \"let\" at one point and I had a few critical things (like my entire site engine) written in it, and on because I like it a lot. But it's not a great LISP for performance (Fennel can run rings around it when using luajit), and I wish they shipped a 1.0 (I cringe every time I read the changelog for a release and check the \"breaking changes\" part, which is always... beefy). reply Zambyte 6 hours agorootparent> Off because it broke \"let\" at one point It wouldn't be very difficult to provide an implementation of `let` that behaves how you would like though, so there is that at least. reply p4bl0 3 hours agoparentprevYour description of Gauche makes me think of Racket, which I really like. How would you compare the two? reply lioeters 3 hours agoparentprevhttps://github.com/shirok/Gauche http://www.practical-scheme.net/gauche/index.html reply dannyobrien 7 hours agoprevAs an amateur coder, this really matches my journey, though with a slightly different ordering: after the usual BASIC/Perl/Python homebase, a few years really expanding my mind in Haskell-land, followed by a \"well, I've bounced off Lisp in the past, let's see what I make of it now\", then having real joy in writing Clojure, exploring Guile and Scheme, and then coming finally to appreciate Common Lisp, warts and all. I still \"see the parentheses\", and I wish there was a way (outside Coalton) to integrate the security that a strong typechecker can provide, but I really do enjoy working in the Lisp ecosystems now, and feel like I understand Lispers excitement at what they have. reply Y_Y 56 minutes agoparent> to integrate the security that a strong typechecker can provide Have you checked out typed-racket? That's a reasonably nice way to get strong typing by tackling type correctness contracts on top of otherwise standard racket. There's also Alexis King's marvellous Hackett, which seems to be finished for now, but was positive proof that you can have - and eat - all the curry-flavoured cake you want while staying in scheme. reply Jeaye 5 hours agoprevI appreciate the shout out to jank, the native Clojure dialect on LLVM with C++ interop. :) Lisp has such a colorful history and Clojure, in my opinion, made it even more accessible and practical. People complain about Clojure's error messages, but, coming from C++, anything that doesn't corrupt memory and segfault is golden. reply whartung 12 hours agoprevRegarding Guile, and, mind, I'm on macOS, but I found it not easy to get started with. I'm looking for a compiled Scheme or Lisp. By that I mean, I want \"prog.ext\" to create the executable \"prog\". I want this because I want to make some command line utilities, and I would like the actual code compiled (vs some p-code bundled with an interpreter). I had tried to get started with Guile, as all of its packages looked attractive, but was stymied. It's been awhile, so I can't express details. It had to do with a combination of things like packages, modules, and I know I ran into a version issue (I think I wasn't running the latest version from my package manager, so it didn't have something in the documentation). Anyway, for such an extensive system, I found it surprisingly frustrating. I've been trying Gambit, which seems like it would be a really nice fit, but it's fighting with my Mac, because macOS doesn't put stuff in /usr/include (wchar.h in this case), and all of my efforts to fix that have failed (save I have not tried upgrading to whatever the latest macOS is). Maybe I can try ECL, maybe that will work. But, anyway, Guile, while attractive, just surprised me at how I wasn't really able to make it work for me enough to make me look elsewhere. reply armchairhacker 10 hours agoparentTry Racket. It’s easy to setup (from experience on macOS), easy to learn, batteries included, and a compiled Scheme (create binaries via `raco`, https://docs.racket-lang.org/raco/exe.html). It’s real highlights are very powerful macros and the ability to override the reader, so you can effectively create arbitrary languages (examples include reimplementations of Java, Lua, and Datalog, and a documentation generator with embedded Racket called Scribble). IMO it’s a research language first and foremost. But it has unusually good production support and online resources (I mean it when I say it’s easy to setup and learn), so I think it fits everything you asked for. reply nerdponx 11 hours agoparentprevI suggest Gauche as an easier-to-use Guile alternative. I had similar annoyances with actually getting Guile to just do what it says it's supposed to be able to do. I've advertised Gauche frequently here on HN (including in another comment in this thread). I have no affiliation with the project, but I like it a lot and I think it's under-appreciated. Its author hosts its documentation under the domain name \"practical-scheme.net\" (https://practical-scheme.net/gauche/man/gauche-refe/index.ht...) and I think the name is very well deserved. reply Zambyte 5 hours agorootparentGuile is really designed with embedding into a program in mind. Gauche is definitely more geared towards their use case, but I think their issues with Gambit can probably be resolved with a few environment variables or command line options reply Paul-Craft 4 hours agorootparentprevEasier to use in what sense? reply wglb 11 hours agoparentprev> I'm looking for a compiled Scheme or Lisp. By that I mean, I want \"prog.ext\" to create the executable \"prog\". sbcl compiles and generates an executable quite nicely. reply djha-skin 11 hours agoparentprevJanet is the perfect lisp for cli tools. https://janet.guide. It is my favorite. reply sph 11 hours agorootparent, should be (unquote) and ,@ should be (unquote-splicing) while ; is for comments, yet Janet completely disregards this convention. It's stupid, but I dislike Janet because of this. reply nerdponx 11 hours agorootparentprevI'm still not sure of what the Janet value proposition is compared to an R7RS Scheme. Not that it shouldn't exist! I just don't quite get the use case either. reply ianthehenry 8 hours agorootparentJanet’s value proposition is pretty similar to Lua — easy embedding, simple C API, minimalist runtime. But Janet improves on some weird Lua warts (block-scoped variables by default, 0-indexed collections, separate types for sequential and associative arrays). Plus it bundles a pretty nice standard library. I’ve seen this comparison before, so clearly Janet isn’t doing a good job of explaining itself, but I think the only thing Janet and Scheme have in common are a few parentheses. Different core data structures, different feelings about mutability, completely different macro system… Guile and Janet share PEGs (sorta) and embeddability but I didn’t think those were standardized at all. (I don’t really know any schemes.) reply zem 1 hour agoparentprevchicken is a very pleasant compiled scheme, with a friendly community and a decent set of packages available. reply uticus 7 hours agoparentprevGuile always surprised me not because of anything LISPy, but because of its relationship with TCL https://vanderburg.org/old_pages/Tcl/war/ reply Pinus 2 hours agorootparentIn RMS’ message that started that discussion, he stated that the GNU project intended to provide two languages — one Lisp-like (which I assume eventually became Guile), and one with a more algebra-like syntax. Did anything ever come out of the latter? reply Y_Y 46 minutes agorootparentThe current advice from GNU is that Guile is the one true language for the project[0]. There are terms of languages that are currently part of GNU[1] but none that seem to meet the description of algebraic and blessed. [0] https://www.gnu.org/prep/standards/html_node/Source-Language... [1] https://www.gnu.org/manual/blurbs.html reply tmtvl 6 hours agoparentprevThe creator of Programming Language Benchmarks 2 is on Apple Silicon, and apparently SBCL compilation works out of the box: . reply davexunit 11 hours agoparentprevI'm not a mac user but I got someone who had never used guile before setup using homebrew to get guile and emacs, and then the guile homebrew tap (https://github.comad/aconchillo/homebrew-guile) for guile libraries. reply coliveira 11 hours agoparentprevMy understanding is that guile is an extension language. Create a prog.c and link to guile, that's how I think you can create an executable. reply davexunit 11 hours agorootparentCreating a C program that links to libguile is kind of a legacy use-case at this point. It was the original purpose of Guile back when it was but a simple interpreter. The trajectory for the past decade or so has been to build up Guile as a platform for writing your entire application. Rather than embedding an interpreter in a C program, the recommended approach is to write a Scheme program that uses the C FFI if and when necessary. The interpreter was once written in C but is now written in Scheme (a minimal C interpreter is kept around for bootstrapping purposes.) There's a sophisticated optimizing compiler that emits bytecode for the Guile VM as well as a JIT compiler. A new garbage collector and a Wasm compiler backend are currently being developed. The big missing piece is ahead-of-time native compilation, but the work on the Wasm backend will help that along as it needs to solve a lot of the same problems. reply 7thaccount 12 hours agoparentprevWhat about chicken scheme or chez scheme? reply nerdponx 11 hours agorootparentChicken kind of felt like a dying ecosystem when I tried it, and it's also not very fast. The standard library itself is kind of limited, so if I couldn't find an egg for something I wanted to do, I felt very stuck. Chez is good because it's supported by Akku, but I'm not sure if it will ever support R7RS. It does have a really nice FFI though, and the docs are very good. reply hajile 10 hours agorootparentR7RS isn't worth supporting right now. R5RS is too small for real work unless you add tons of SRFIs, so they created R6RS for people who want to get stuff done. But the the R5RS people got ticked off because \"its too big\". R7RS was supposed to be a compromise with a tiny R7RS-small, but later adding most of the R6RS features with R7RS-large. It's now been a decade and R7RS-large seems to be completely dead. reply zilti 9 hours agorootparentR7RS is very much worth supporting; there'll shortly be a Chicken Scheme 6 with full support for R7RS-Small. R7RS-Large development is slow, but not at all dead. reply hajile 7 hours agorootparentIt's been 10 years since R7RS launched and I believe around 15 years since R7RS-large began work. That goes a bit beyond \"slow\". reply rcarmo 11 hours agorootparentprevI've got similar interests to the OP and the parent comment. I had some fun with both Chicken and Chez, but building standalone/static binaries was a bit of a pain. The interesting thing for me (a while back) was realizing that even doing something as simple as an HTTPS request would be a bit of a challenge: https://taoofmac.com/space/blog/2019/06/20/2310 reply timbit42 10 hours agorootparentprevAlso, Larceny (R7RS, UTF-8, compiles to x86, x64, ARM, C). reply zilti 9 hours agoparentprevI can highly recommend Chicken Scheme, it does exactly what you want :) It has also a nice friendly community, and a couple hundred extensions. reply codr7 46 minutes agoprevI spent a lot of time unsuccessfully bending Common Lisp into something I could love. It's still one of my favorite problem solving languages, but far from trivial to fix from the inside. Tried writing my own readers that compile to Common Lisp. Then I ended up on a side track to write my own interpreters; mostly Forths at first, then gradually more Lisp-like and practical creations. My latest attempt may be found here: https://github.com/codr7/jalang reply galaxyLogic 10 hours agoprevIncludes this great comment by Simon Peyton-Jones in support of incremental typing: \"I think to try to specify all that a program should do, you get specifications that are themselves so complicated that you're not longer confident that they say what you intended.\" reply netcraft 12 hours agoprevafter decades of programming, lisp has always been that thing that intrigues me endlessly, but I haven't had a chance to actually wield it myself. but so many people whose opinions I respect love lisps, and not just for a little while. Clojure especially. Other ideas like datomic and xtdb are also high on my list of things I need to experience. I think im going to have to make an intentional effort to find a lisp job next time. reply epgui 12 hours agoparentDoing Clojure at CircleCI, a few years ago, redefined who I was as an engineer… FWIW. It’s an incredible language. Today I work with python (sigh) and every day I long for Clojure. reply behnamoh 11 hours agorootparentI keep hearing sentiments like this but then I wonder, if Clojure oris so much better than Python , then why are we still writing in those languages? If it's because of libraries, then the question is: Why do people write libraries for these languages and not the Lisp ones? reply crq-yml 6 hours agorootparentLisps are \"wizard\" languages: the runtime semantic is kept close to the syntax, which also means that you can rapidly extend the syntax to solve a problem. This quality is true of Forth as well, and shares some energy with APL and its \"one symbol for one behavior\" flavor. With respect to their metaprogramming, the syntactical approach hands you a great foot-gun in that you can design syntax that is very confusing and specific to your project, which nobody else will be able to ramp up on. But Algols, including Python, are \"bureaucrat\" languages: rather than condensing the syntax to be the exact specification of the program, they define a right way to form the expression, and then the little man inside the compiler rubber stamps it when you press the run button. In other words, they favor defining a semantics and then adding a syntax around that, which means that they need more syntax, it's harder to explain the behavior of a syntactical construction, and it's harder to extend to have new syntax. But by being consistent in a certain form-filling way, they enable a team to collaborate and hand off relatively more code. IMHO, a perfectly reasonable approach I'm exploring now for personal work is to have a Lisp(or something that comes close enough in size, dynamic behavior, and convenience, like Lua) targeting a Forth. The Forth is there to be a stack machine with a REPL. You can extend the Forth upwards a little bit because it can metaprogram, or downwards to access the machine. It is better for development than a generic bytecode VM because it ships in a bootstrappable form, with everything you need for debugging and extension - it is there to be the layer that talks to the machine, as directly as possible, so nothing is hidden behind a specialized protocol. And you can use the Lisp to be the compiler, to add the rubber-stamping semantics, work through resource tracking issues, do garbage collection and complicated string parsing, and generate \"dumb\" Forth code where it's called for. That creates a nice mixture of legibility and configurability, where you can address the software in a nuts-and-bolts way or with \"I want an algorithm generating this functionality\". reply Karrot_Kream 1 hour agorootparent> IMHO, a perfectly reasonable approach I'm exploring now for personal work is to have a Lisp(or something that comes close enough in size, dynamic behavior, and convenience, like Lua) targeting a Forth. I've had this idea for a while now but never got around to actually executing it. I'd love to follow your progress if you're doing it publicly. reply coldtea 1 hour agorootparentprev>If it's because of libraries, then the question is: Why do people write libraries for these languages and not the Lisp ones? Because libraries already exist for those languages, as well as support, vendors, and a big ecosystem, familiar syntax, and jobs. So they write libraries for languages that are already popular. If you meant, \"but why wasn't some Lisp the one that gain popularity back in the day, when C, C++, Python, and Java didn't exist or where still fresh?\" I think because: 1) it was too advanced for the procedural mindset at the time, 2) it was not sufficiently efficient in those primitive 16 bit machines 3) fragmentation and most importantly, no killer app and major vendor backing or OS first-class support (like C had for UNIX, C++ for Windows, and Java got from SUN). reply crote 8 hours agorootparentprevBecause it's weird enough to be off-putting, and it doesn't solve a real-world problem. If you look at the arguments in favor of Lisp, they'll often boil down to it being \"beautiful\", \"elegant\", \"flexible\", or even \"magical\". It's a very minimal language which allows you to do absolutely anything - a lot of which would be an absolute nightmare in most other languages. You could implement just about any programming paradigm in Lisp if you want to. I believe this makes it very appealing to computer scientists, or other people with a more mathematical background. However, this flexibility is also a massive footgun: if you're not careful your junior developer might end up reinventing the wheel a dozen times, and writing completely unmaintainable code in the process. On the other hand, most other programming languages look kind-of the same. They are all quite opinionated about how stuff is supposed to work, with a lot of hardware details leaking into the language. However, they are very easy to learn: most of it is just \"do a bunch of operations in succession\" taken to the extreme. Anyone who knows C# will be able to pick up a basic understanding of C, Python, or JavaScript well within a day, and a lot of people new to programming will be able to write not-completely-terrible code within a month or two when given the right guidance. They don't need to know about all the abstractions and technical details to be a functioning member of your team. When you're running a business, you don't care about any of that beauty or flexibility. You want code which is quick and easy to write, trivial to read, and understandable by even the worst programmer in your company. In practice that means in your comparison you'll be choosing Python over Lisp. Heck, Go was developed entirely around this principle, cutting out as many language features as possible. And because all the other companies are making the same choice there will also be way more libraries for Python, making the gap even larger. So yeah, in a stroke of irony Lisp isn't more popular because it is better. reply evdubs 7 hours agorootparent> Because it's weird enough to be off-putting, and it doesn't solve a real-world problem. Nonsense. Quoth Wikipedia, \"Lisp pioneered many ideas in computer science, including tree data structures, automatic storage management, dynamic typing, conditionals, higher-order functions, recursion, the self-hosting compiler, and the read–eval–print loop.\" These are all solutions to real world problems. So many languages borrow features that were originally developed in Lisp. > However, this flexibility is also a massive footgun: if you're not careful your junior developer might end up reinventing the wheel a dozen times, and writing completely unmaintainable code in the process. Plenty of code written in Lisp looks just like your Python, Ruby, JavaScript, Java, et al programs where you're defining structures or classes, writing and calling functions, importing useful libraries, etc. Plenty of this Lisp code is just as maintainable as the non-S-expression code. > Anyone who knows C# will be able to pick up a basic understanding of C, Python, or JavaScript well within a day Same with Lisp. It's just: (function arg1 arg2) Instead of function(arg1, arg2) > You want code which is quick and easy to write, trivial to read, and understandable by even the worst programmer in your company. There is plenty of \"lowest common denominator\" code like this written in Lisp. Much Lisp code is not buried under inscrutable macros, just like not all Java code is buried under layers of inscrutable classes. reply kazinator 7 hours agorootparentprev> Why do people write libraries for these languages and not the Lisp ones? 1. They want their names to be widely recognized, so they find a popular bandwagon to hop onto. 2. Raw numbers? More people using Python means more people trying to make libraries for Python, means more libraries remaining in the race after you eliminate the crap from people who don't know how to write libraries. Note that Python is, by now, an old language. It wasn't instantly popular, and you wouldn't have predicted it. In, say, 1999, you had to be some GNU/Linux person to even know what Python is. It was far from obvious that, of all things, it would get so popular. That Eric Raymond article in the Linux Journal around that time probably gave it a bit of a boost. Python definitely rode on the coattails of increasing GNU/Linux popularity, too. More people using Linux started asking questions how to script this and that, and going \"gack!\" at shell or perl programming. It seems Python might appeal to survivors of VisualBasic shifting gears into GNU/Linux stuff. reply epgui 10 hours agorootparentprevFor the same reason people can't change their well-established habits and opinions, particularly those that have network effects. Language popularity has little to do with how well languages are designed or how simple they actually are. Most popular languages are popular for historical reasons. reply galaxyLogic 10 hours agorootparentprevBecause of habit. There is always a cost in trying to learn something new, to the level you are on with some other language and it's tooling and libraries. Programmers who program in some language for a few years realize they are still learning more about it, all the time. Therefore they know if they switched to another language it would also take them a few years to attain the same level of mastery. And then there is similar inertia with an organization that has lots of code in some specific language already. reply colingw 10 hours agorootparentYes, it's more than just about the language itself, as I describe here: https://www.fosskers.ca/en/blog/software-dev-langs For some people, community signals are very important. Massive conferences or raw number of libraries, etc., indicate some inner quality of that language's ecosystem that they value. reply __MatrixMan__ 7 hours agorootparentprevMost Python users aren't software engineers. They're students, scientists, business analysts... We're lucky that we were able to drag them away from Excel. Asking them to learn yet another programming language might be a bit much. And if they did, it wouldn't probably be a lisp (it would probably be among: C, Javascript, Julia, Go, Nim). I want to move on from Python, but there are so many more people that I can help if I stay. reply tugberkk 1 hour agorootparentI have the same problem. It is a good language to teach to non-cs majors. If you want to build something out of the box, use Python. However, GPT is coming and maybe we won't even use Python anymore for simpler tasks. reply roenxi 6 hours agorootparentprevI don't think anyone is saying Clojure is better than Python in the abstract. Clojure has a different style from Python and that style can be more fun and is better for some specific tasks like application programming and anything that wants to use multiple CPU cores. Tends to have better long term prospects too, old Python code doesn't work in my experience but things on the JVM have cockroach powers. As for why things are and aren't popular, who knows? It is quite possible that people just don't like the look of the parenthesis, or there are a couple of key libraries that aren't good but nobody vocal has put there finger on which ones. The error messaging isn't quite up to a standard that people want to deal with. Or maybe popularity is really just about pure random chance, at most 4 languages get to have >20% market share, by definition. reply Zambyte 10 hours agorootparentprevSocial forces and economics. Lisps were winning in academia for a while. Unix started winning in the engineering world before Lisp systems really got their feet under them. Engineers made a lot of money, and academia shifted focus to the systems that were making money. Now everyone programs for Unix instead of Lisp. reply tikhonj 10 hours agorootparentprevAt the end of the day, things get popular through social processes, so it's far more a matter of social factors—some of which are practically random—than any intrinsic qualities of the thing itself. reply ParetoOptimal 8 hours agorootparentprevI can say at least that Haskell is this way for me. reply karmakaze 11 hours agoparentprevMy take on Lisp after going (partway) through SICP, is that it's a syntax and not so much a language. The language is what you build up for the particular kinds of things you need to do. This is both the strength and weakness of Lisp, with a tight-knit competent team, everything is elegantly achievable. However on a small/understaffed team or one with high turnover, each member has to onboard onto that team's language built using Lisp. Imagine the best and worst DSLs that you've had to use. Joining a Lisp team would be somewhere on that spectrum though I hope their homegrown/app language is far better than the average/bad DSL. Clojure is much better in that it has many 'batteries included' and opinions on things to make different codebases less different than with other Lisps. reply nerdponx 11 hours agorootparentIf it's any help, SICP is the wrong place to start for actually learning Scheme as a practical language. It's for learning about \"the structure and interpretation of computer programs\", which is not the same thing as \"writing useful computer programs\". reply bcrosby95 11 hours agorootparentprevMy take on Lisps is that people overblow the DSL aspect of it. I just write functions that call other functions, as opposed to methods that call other methods. reply pfdietz 11 hours agorootparentThat's a fine thing to do in Common Lisp. If you ever change your mind, it's very easy to change a function into a generic function and split off the body into one or more methods. reply horeszko 8 hours agoparentprev> I think im going to have to make an intentional effort to find a lisp job next time. Does anyone have any tips on where to find a lisp job? reply reikonomusha 7 hours agorootparentLisp jobs are sometimes advertised in Who's Hiring threads, Reddit's r/lisp, etc. There's a Lisp job advertised on Reddit [1,2] right now even. Another good source is to check out the companies in [3] and see if they have any openings on their website (or cold-emailing). [1] https://www.reddit.com/r/ProgrammingLanguages/s/AZmouaoARl [2] https://jobs.lever.co/dodmg/af802f7f-4e44-4457-9e49-14bc47bd... [3] https://github.com/azzamsa/awesome-lisp-companies reply john-shaffer 5 hours agorootparentprevhttps://jobs.braveclojure.com/ reply JonChesterfield 8 hours agoprevI really like Kernel. John Shutt's thesis language. It throws away a bunch of incidental complexity from scheme, most notably the minor disaster of hygienic macros. It follows a minor tangent on cyclic data which I'm not convinced matters very much. Implementations are a bit DIY. An interesting play would be to implement rsr7 scheme in kernel, semi-metacircular fashion. reply spindle 17 minutes agoparentDoes picolisp scratch that itch for you? reply charlotte-fyi 11 hours agoprevDon't really get the criticism of Clojure for being hosted on the JVM, particularly relative to its status as a \"productive\" Lisp. Like oh, you get access to one of the biggest and most mature library ecosystems out there as well as best in class operational tooling? Obviously there are use cases where the JVM doesn't fit and all things being equal I prefer shipping statically linked binaries too, but the JVM still feels like an obvious \"pro\" here. reply outworlder 8 hours agoparent> Don't really get the criticism of Clojure for being hosted on the JVM, particularly relative to its status as a \"productive\" Lisp. If you are doing long running server side apps, it is a better fit. Even better if you are already a Java shop. Otherwise, its either detrimental or, at the very least, a source of very 'alien' behavior, not the least of which being the stack traces. That gets pretty obvious when you compare with the likes of Common Lisp with its incredibly elegant system that's essentially Lisp _almost_ all the way down. The JVM has its own advantages of course. Billions of dollars of optimization work being one of them. Being able to use java libraries to fill the gaps (at the expense of the less elegant stack traces and some It can be a complete show stopper in many applications. Say, you want to interface with C libraries. Or embed some form of Lisp in your app. Browser-based apps (emscripten doesn't help you), which is why Clojurescript exists. Or you are building something like an iOS application. I have successfully embedded (although not shipped to the App store) Chicken Scheme in a couple of different ways. The first, as a library with all the cross compilation nonsense. And the second, by simply telling the compiler to stop at C code generation, adding the C blob in the app, and compiling everything together with the rest of the app. That gave me a remote REPL which was amazing for debugging. reply roenxi 7 hours agorootparentFair points, I say. But interfacing with C isn't a show stopper is it? Clojure can bridge to C by exposing the C library though the JNI/JNA framework. It isn't much fun and there are certainly situations where I'd say Clojure was a poor choice for calling C/C++ libraries, but if you need to do it then it can be done. reply MathMonkeyMan 9 hours agoparentprevI hardly wrote any Clojure, but the only thing that bugged me was the startup time of the repl. It's been talked about enough. Yes, that problem goes away if I use a proper setup with a language server or whatever, and yes it doesn't matter for \"situated\" production applications, but it still peeved me. What do I care if it's in the JVM? Sure, a JVM instance uses a lot of memory to help the garbage collector, but that doesn't bother me. JVM is just an old, mature ecosystem. Every runtime we work with (browser, nodejs, CPython, your decades of hand-written C++, the Go standard library) shares design tradeoffs with the JVM. Nothing inherently off-putting about it. reply coffeemug 9 hours agorootparentI haven’t touched JVM in ages, but there are two things off putting about it. First it’s viscerally slow. They have a state of the art GC, amazing benchmarks, tons of work going into performance, but it still feels slow and laggy when you develop on it. None of the other ecosystems you mentioned have that problem (including Python). Second, they have a bad sense of design. The class library comes from a culture of needing three classes to open a file, and that culture permeated through the entire ecosystem. Almost all the software in it feels bloated and over engineered. The modal JVM experience is spending 95% of your time dealing with “enterprise-y” boilerplate that turns out to have nothing to do with the enterprise and everything to do with bad design decisions and the culture downstream from those. C++ has its own flavor of this problem, but certainly not Python or Go. reply Capricorn2481 1 hour agorootparent> First it’s viscerally slow. They have a state of the art GC, amazing benchmarks, tons of work going into performance, but it still feels slow and laggy when you develop on it. None of the other ecosystems you mentioned have that problem (including Python). I really can't relate to this. What part of the process is noticeably slower than Python? I have a lot of Python projects and couldn't say any of them are slower than JVM apps. reply 7thaccount 7 hours agorootparentprevI couldn't agree more. I'm not very knowledgeable on Java, but was blown away every time I looked to see the crazy amount of boilerplate to do anything. There are all these design patterns that seem to only exist because the language is so terrible. Thousands of people who aren't professional developers write millions of lines of Python each year (just a guess, but sounds right) and the vast majority just write code and don't need 50 classes in their application to do something. reply roenxi 7 hours agorootparentYou're talking about something different - Java the language is a bit ugly, but this is about JVM performance (ie, the runtime virtual machine that is installed to execute Java programs) with Clojure, where there is not much boilerplate to speak of. Although the JVM is one of the sleekest environments around though and I'm confused by the fellow saying it is \"viscerally slow\". Clojure loads slowly, but after that everything happens at speed. reply john-shaffer 5 hours agorootparentClojure itself actually loads pretty quickly, but almost every project has enough libraries to make loading the project take a few seconds. reply truculent 8 hours agoparentprevIt also gives you access to Babashka if you want Clojure for other use-cases where start-up time is an issue https://babashka.org/ reply whateveracct 11 hours agoparentprevThe JVM precludes general tail-call elimination though. reply jordibc 10 hours agorootparentIt does preclude it, but clojure found an arguably elegant solution to it, using recur[1] instead. As a plus, in addition to achieving the same result as tail-call elimination, it does check that the call is indeed in tail position, and also works together with loop[2]. For me, it made me not miss tail-call elimination at all. [1] https://clojuredocs.org/clojure.core/recur [2] https://clojuredocs.org/clojure.core/loop reply lispm 2 hours agorootparentIn Scheme: (define (foo) (bar)) the call to bar is a tail call. How does recur optimize this? Well, it doesn't, since \"general TCO\" / \"full TCO\" means that any tail call gets optimized, such that the stack does not grow. Clojure recur/loop is just a loop notation. Looping construct in most languages provides a simple conversion of self recursion (a function calls itself recursively) to a loop: update the loop variables and then do the next loop iteration. But the general case of tail calls is not covered by a simple local loop, like what is provided by Clojure. reply whateveracct 9 hours agorootparentprevThe issue arises when you program really heavily with closures and function composition. You sadly cannot do functional programming as in \"programming with functions\" without care on the JVM. reply packetlost 10 hours agorootparentprevIt is, IMO, a missed opportunity to use a hard-coded identifier for `recur`ing instead of the `(let sym ((...)) ...)` form that would let you nest loops. Aside from that, I agree. Tail-call optimization's benefits are wildly overblown. reply whateveracct 9 hours agorootparentThe benefits aren't overblown if you are someone who learned Lisp with a functional approach. As in, using higher-order functions etc. You have to be careful whenever you approach a problem that way on the JVM. reply xdavidliu 6 hours agorootparentwhat does tail-call optimization have to do with higher-order functions? I thought the former pertains to iterative procedures written with recursive syntax, where the recursive call is at the very end of the function and called by itself, so stack size is O(1). Higher-order functions means passing functions to things like map, filter, etc. reply throwaway17_17 6 hours agorootparentIn the context of higher order functions, tail call elimination allows for the avoidance of building up intermediate stack frames and the associated calling costs of functions when doing things like composing functions, particularly when calling large chains of nested function calls. The benefits of TCO for something like mapping a function can also be pretty large because the recursive map can be turned into a while loop as you describe at the beginning of your comment. The optimization of stack frame elision is pretty large for function calls on the JVM and the stack limits are not very amenable to ‘typical’ higher order function ‘functional programming’ style. reply packetlost 9 hours agorootparentprevCan you provide an example? reply thmorriss 10 hours agorootparentprevthe clojure loop construct is often cleaner than code written to be tail recursive reply uyrifo 9 hours agorootparentAnd often faster: https://medium.com/hackernoon/faster-clojure-reduce-57a10444... Yet it’s always noted as code smell associated with “inexperienced candidates” in interviews. For that matter, first and last too: https://medium.com/hackernoon/faster-clojure-reduce-57a10444... The amount of paired programmers suggesting changing nths to firsts and lasts is demoralizing. reply ska 11 hours agoparentprevI think hosted is almost tautologically a mixed bag - you get access to the host (both +ves and -ves) but you also introduce a layer and some invariable friction. reply askonomm 11 hours agoparentprevAnd quite a lot of people actually do ship statically linked binaries with Clojure, using GraalVM. Clojure LSP server for example is distributed as a static binary. reply colingw 10 hours agorootparentYes, although that solution can't yet be said to be \"push button\". My impression is that there are a decent amount of people who want non-JVM, native Clojure. Hence efforts like Janet and Jank. reply LispSporks22 10 hours agoparentprevI know of one company that dumped its clojure code base because lambda costs were too high. They even experimented with graal as a fix. reply whalesalad 9 hours agorootparentI would have dumped lambda as a runtime before dumping the codebase. Obviously we don't know the full story but that sounds a lil silly to me. Plus lambda's will stay alive for quite a while if they are in use. So startup time is felt once, but then would be identical to any other language or runtime. reply hiAndrewQuinn 2 hours agoprevScheme has a special place in my heart after working through SICP as a late high schooler. I probably haven't actively written anything in Scheme in a decade, but it later inspired me to learn Haskell to a decent level of depth, just to see what the \"other\" functional languages feel like. So much the same, yet so much different. reply eggdaft 2 hours agoparentScheme was taught in my first year of CS. One thing I noticed is that those who hadn’t coded through their teenage years found scheme much easier to learn than other languages we were being taught in the first year. I think that was because scheme felt more like mathematics and everyone had done A Level maths (in the UK), many to an advanced level. Immutability and functions make a lot more sense than weird variables that keep changing and - what the heck are classes? We just unlearn this mathematical thinking when we start hacking Python etc. reply im_down_w_otp 11 hours agoprev> Why do print-line-debugging to find out what's happening at a location in code when you can just be inside your program and inspect everything live as it's running? Yes, that's because print-effing one's way to understanding is also among the most crude methods of debugging. An excellent alternative to doing that is to use an actual debugger. That will also allow you to be \"inside your program\". reply ctrw 8 hours agoparentThe difference with lisps is that the debugger is always part of the runtime and you have full access to all the languages capabilities while inside. I've yet to see a C debugger which does more than a fancy version of printf. Imagine being able to compile functions on the go while still running your main program with all the state saved. reply foobarian 11 hours agoparentprevHow do you \"get inside your program\" on a locked down production box? Watched by SOX auditors like hawks? Any answer offered needs to be comparably easy to checking in an extra printf and letting ci/cd deploy it. reply im_down_w_otp 10 hours agorootparentI used to be an Erlang engineer, and you would have needed very explicit security controls around being able to attach to an Erlang cluster to get the \"insider your program\" experience. This was in FinTech, so I'm familiar with the constraint. I would imagine you'd need the same thing for accessing the runtime of any production deployed application(s) in such an unfettered manner. Erlang, Lisp, C, or other? reply thetwentyone 7 hours agoprevAlso an interesting Lisp for it’s size is femtolisp from one of the co-creators of Julia: https://github.com/JeffBezanson/femtolisp reply rcarmo 1 hour agoparentI have a femtolisp quote at the top of my LISP resources page: https://taoofmac.com/space/dev/lisp reply matheusmoreira 13 minutes agoprev> How can newcomers get the most out of learning Lisp? Make your own lisp. I chose that route even though I had never written lisp code before. Finally understood its elegance. reply dartharva 4 hours agoprevArticle topic notwithstanding, I am curious in the design choice of moving the \"Table of Contents\" to the right side of the text and links for other blog entries to the left. Aren't the sides conventionally swapped as with default Blogspot/Wordpress blog templates? Also, what was the objective behind shaping the text as a Q&A-style interview when it's only a personal blog? reply colingw 2 hours agoparentThe questions were originally posed by a Doom Emacs community member, so I answered them as-is. The site is designs as it is because I'm not a frontend guy xD reply huqedato 12 hours agoprevAfter reading the article (BTW a good one), one thing intrigues me: why the author left Haskell for Lisp(s) ? reply crote 8 hours agoparentI can't speak for the author, but I do know the reason I personally left it: Haskell-the-language is amazing, Haskell-the-ecosystem is awful. Like the other commenter I completely agree that knowing Haskell makes you a better programmer, and when I code in other languages I really miss certain parts of it. On the other hand, the entire Haskell ecosystem feels like a half-finished PhD thesis, and often-promoted libraries suffer from fundamental flaws which are \"open research questions\". There's a constant drive in many Haskellers to make their code as abstract, type-safe, and generic as possible, but unfortunately practicality and ease of use are often forgotten along the way. It's a great language, but if you're trying to write production code which builds upon the wider ecosystem you're in for a world of pain. In 2024 Rust has probably rifled enough through Haskell's pockets to make it mostly irrelevant as anything but a testbed for programming language researchers. reply kccqzy 11 hours agoparentprevI did the same and I treat Haskell as a rite of passage. At this point it's cliche to say that Haskell makes you a better programmer in other languages, but it's true. You really immerse yourself in Haskell for a few years, and you become a better programmer even when you aren't writing Haskell. I'm referring to universal things like \"making invalid states unrepresentable\" or \"preferring to parse than to validate\" or \"functional core imperative shell\" that can work well and lead to cleaner code in any language. (The actual reason I left Haskell was because I switched my employer. Choosing an employer is IMO a bit more important than choosing the language to code in.) reply ParetoOptimal 8 hours agorootparentNon-mainstream languages can be a very positive signal for employers I've found. reply colingw 10 hours agoparentprevHi. I actually left Haskell for Rust. And when I say \"left\", I mean I don't start new projects in it. I still maintain my libraries. I wrote Haskell for 10 years or so, both FOSS and professionally. I've \"been around the block\" so to speak and consider myself to have a decent view of the landscape. Overall, Rust lets me code in the style I want while being very resource efficient. I write Rust professionally. reply Hizonner 7 hours agorootparentThe part that confused me was your leaving Haskell for Rust and then leaving Rust for Lisp. Were both of those transitions aimed at some common goal? I mean, if I left Haskell, I think the main reason would be to to shed the load of thinking simultaneously about laziness and how it interacts with optimization. OK, almost any non-Haskell language gets you out of that. But choosing to leave Haskell specifically for Rust would be about efficiency first and foremost, especially improving on the size and complexity of the RTS (and its limited platform support). I could also see wanting the concurrent programming benefits of Rust's ownership system. And it's nice to be able to write embedded or kernel code. And there's a bandwagon to jump on. Lisp, on the other hand, doesn't really seem like an improvement over Haskell in any of those ways. It solves different problems. Lisp feels like it's on the \"opposite side\" of Haskell from Rust. So why did you \"reverse\" and try Lisp to begin with? I agree that Rust is ugly, by the way. Honestly I think it started with keeping the C syntax and went from there. reply colingw 1 hour agorootparentI should make it clear that I haven't left Rust; I write it every day professionally. I transitioned from Haskell to Rust to capture efficiency and small binaries. Since I'm in the game of shipping CLI tools, this was important for me. You're right though that Lisp is on the other end of that; we're back to bigger runtimes with no tree-shaking, since that would hinder debugging. For now I'm experimenting with the Interactive Programming paradigm because the debugging story is just too good. For long-lived programs, this may be the way to go. Rust code can be made nice to look at it, but it isn't the default nor the trend. reply behnamoh 11 hours agoparentprevAt some point you realize Haskell and/or its ecosystem keeps breaking things too much and you move on. reply ParetoOptimal 8 hours agorootparentI love Haskell and know there are efforts to fix this, but I find it hard to disagree. reply tome 1 hour agorootparentNot just efforts, but both GHC HQ and the Core Libraries Committee making stability a top priority. Expect casual breaking changes in the Haskell world to become a thing of the past, in very short order (in fact the most recent release of GHC (9.8) had very few breaking changes). reply cmrdporcupine 12 hours agoprevWhile I agree with the author that there's something about Rust (my day-job lang & goto personal lang these days) that just lacks... elegance... I've also never found the Lisp (and many other FPs) emphasis on recursion all that compelling. Aesthetically or computationally. I find the programs written this way hard to reason about and read. Personally. For me, the underutilized gold mine feels like logic languages (Prolog et al.) Though I know recursion is used there a lot, too. reply pfdietz 11 hours agoparentI don't think Common Lisp has an emphasis on recursion. As evidence, consider that it doesn't in the standard require any kind of tail recursion optimization and has multiple ways of doing iteration. Scheme is the language with the fetish about doing things with recursion. reply ctrw 8 hours agorootparentCommon lisp let's you do recursion when you need it and not when you don't. Using recursion for iterating through a list is stupid because you bury the lead in the middle of a function and you do the same thing until you stop. Using recursion for a tree or more connected graphs isn't because they are fundamentally recursive. reply xdavidliu 6 hours agorootparentyou can argue that lists are fundamentally recursive too: they are a car and and cdr, where the cdr itself is another list. reply cmrdporcupine 9 hours agorootparentprevThat's fair. Any time I've decided to spend time in the sexpr-world, I've gone straight to the hard stuff, into Scheme. Apart from Elisp, anyways. I've never worked in the source tree of a large Common Lisp program. Though I've bought the books and read the tutorials etc. reply mrkeen 11 hours agoparentprev> I've also never found the Lisp (and many other FPs) emphasis on recursion all that compelling. Recursion appeals to me because of immutability: I do not want to do a summation by writing the wrong answer to memory, and then updating it in-place until it's the right answer. It makes me have to think not only about what the right answer is, but when it is. But can't I just use stack variables? Well, I don't think summation of primitive ints using stack variables is too taxing, but what about computing a word-count instead of a summation? Then your 'stack variables' are pointers into Strings and mutable HashMaps. When you return the result, do you make a defensive copy? How deeply do you copy? Your clone of a HashMap can still reference mutable data in your original HashMap. Do you name your variable 'result' when it does not yet contain the result? You might have to think about the primitive/object split - when is it pass-by-value and when is it pass-by-pointer? Do you have in-values and out-values? With immutability, you only have values, and you only pass-by-value. If you're sharing objects across threads, and you decide to lock - is that sufficient? What if the caller takes the lock, does a get(), releases the lock, and then starts mutating the gotten value, bypassing the protection of the lock. Now you're thinking about defensive copies and maybe deadlocks too. Just share an immutable value without a lock. Sometimes recursion is aesthetically better, but usually it's just what you're left with after you take a big hammer to mutability and its surrounding issues. And most of the time (in Haskell but I'm guessing in Lisps too) you don't write out direct recursions, but instead use maps and folds which have recursion under the hood. reply galaxyLogic 10 hours agorootparentI think mutable variables (inside functions) are just fine, as long as the function itself always returns the same result for the same arguments. That is the essence of \"pure\" I think. A function should be a \"black box\". I don't care what happens inside it as long its externally observable behavior is pure. reply thuuuomas 11 hours agoparentprevHave you checked out any of the minikanren logic programming environments people have implemented in scheme? The Reasoned Schemer is an accessible introduction to that space if you’re not put off by the socratic-themed dialogue. & the Barliman demo is still pretty exciting even after LLM codegen. https://youtu.be/er_lLvkklsk reply nerdponx 11 hours agorootparentI'd like to see a practical use case for Minikanren other than quines. It's fun and interesting, but I'm not smart enough to go from \"a tiny set of primitives\" to \"solving practical problems\". I had trouble even implementing the basic \"moses is a man\" examples you see in Prolog tutorials. reply tmtvl 6 hours agorootparentMiniKanren has been used in the medical world: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9562701/ reply cmrdporcupine 9 hours agorootparentprevYeah me too I was just looking at an Elisp port (!) of minikanren the other day, and, yeah, all the examples were algorithmic puzzles, rather than solutions to problems. reply erichocean 7 hours agorootparentprevIt's also available in Clojure: https://github.com/clojure/core.logic If you want to write one yourself, it's pretty easy: https://www.youtube.com/watch?v=y1bVJOAfhKY reply colingw 10 hours agoparentprevManual recursion often isn't needed. You can get basically all of what you want from Transducers: https://github.com/fosskers/cl-transducers reply epgui 12 hours agoparentprevIt’s like anything else, you get used to it just as easily. reply whartung 11 hours agorootparent...or don't use it. Scheme likes to tout it because of tail call optimization. Common Lisp can do it, but doesn't really shove it as a first principle of \"Common Lisping\" (and CL does not dictate tail call optimization). I don't use it (save when it's necessary). I just iterate using the supplied iterators (do, dolist, loop, whatever). I can't speak to other functional languages. reply ska 11 hours agorootparentCL's iteration primitives are more capable that those provided by most languages, which often makes for a very clean approach. Recursion is sometimes the easiest way to reason about a problem though, so it's nice to have decent support and TCO. reply epgui 10 hours agorootparent> Recursion is sometimes the easiest way to reason about a problem though Yes, it's one of the things that allow FP algorithms to be more declarative. It's difficult to seriously argue that recursion is fundamentally more complex-- the contrary argument is more obvious to me. Everything that's left is the fact that it requires people to unlearn things, which is uncomfortable and requires effort, but unfamiliarity is not an indicator of complexity. reply cmrdporcupine 12 hours agorootparentprevI take it as: mathematicians like recursion, so they put some recursion in your recursion... reply epgui 10 hours agorootparentThere's a strong argument to be made that mathematicians tend to build the simplest abstractions conceptually, especially in comparison to computer scientists. Personally, when I stumble upon some maths, I pay close attention. It has paid off. reply dogprez 9 hours agoprev> [Fennel] also lacks Common Lisp's debuggability, given that it sits entirely within Lua's runtime. I'm not sure exactly what feature the OP was referring to. It sounds like they don't think you can get a REPL for an executing Fennel process? You can. If you are only using it for AOT Fennel->Lua you can't, you have to include its runtime. reply schemescape 8 hours agoparentLast I checked, Lua was “bring your own debugger”. Assuming that hasn’t changed, a REPL is nice, but you can’t pause and inspect anything by default. reply colingw 7 hours agorootparentTo be the most fair, Fennel 1.4 recently released with an `assert-repl` form that opens a repl when some assertion fails, in which you can inspect local variables, etc. That's getting closer to CL. https://git.sr.ht/~technomancy/fennel/tree/1.4.0/item/change... reply schemescape 7 hours agorootparentThanks for the correction! I had only used Lua, and thought Fennel had no runtime, so I assumed this was not possible. reply dogprez 7 hours agorootparentprevFennel has a form `assert-repl` which will drop into the REPL wherever, if the condition fails. For writing games you can launch the REPL in the game loop if a keyboard button is pressed. But what you can't do, that I know of, is interrupt arbitrary execution and get a Fennel REPL. You'd probably need a lua debugger of some sort for that. I'm not that familiar with that though. reply emidln 8 hours agorootparentprevWhy can't you just put the equivalent of `(repl)` wherever you want to debug and drop into your REPL? reply schemescape 7 hours agorootparentShort answer: I don’t know. That sounds like a good idea, but how would that access local variables in the caller (to inspect state)? I remember the Lua C API exposes a lot of information, but I didn’t think it was accessible from scripts. Of course, it was a long time ago and I could have easily missed something at the time. Happy to be corrected! Edit: you might also run into difficulties trying to redefine non-global functions to add the call to “repl”. reply agambrahma 2 hours agoprevSounds like he'd like Coalton reply colingw 2 hours agoparentCL-embedded langs like Coalton and April are mentioned in the article. reply LAC-Tech 10 hours agoprevThe big thing I took away from LISP is that having everything be prefix solves a lot of problems. Seriously, the arguments people have in languages like Zig about whether to have \"operator overloading\" just seem a bit silly. Operators are just functions, it's very silly to make them syntastically different. But I never got much into REPL driven development. Probably because I just couldn't wrap my head around emacs. reply mpweiher 9 hours agoparentSmalltalk solves that by having everything be infix. With fewer parentheses. ¯\\_(ツ)_/¯ reply LAC-Tech 9 hours agorootparentYeah that's another approach to it, one I think could definitely be borrowed (though I wouldn't do it exactly how smalltalk did). And then of course there's the \"concatenative\" family where everything is postfix. reply mpweiher 53 minutes agorootparentHow would you make it different? One change that I made in ObjS is that I added the pipe so you don't need to parenthesize chained keyword messages. ((receiver msg1:arg1) chained1:arg2) chained2:arg3 vs. receiver msg1:arg1chained1:arg2chained2:arg3 Having colon keywords does get in the way a little, as I also need them for schemes (URIs are part of the language) and I would also like them for type-specifiers, as type-after really seems the better way. I can probably pull all that off syntactically, not sure about aesthetically. reply beepbooptheory 10 hours agoprevDoes anyone know how to get a lisp job? Or have any experiences to share? Its been a dream for a while for me, and I think I am possibly ready (at least with common lisp, have started doing more clojure recently though). It just seems so impenetrable to me, I don't even know how to begin to search for it. reply neilv 9 hours agoparentI got mine by contributing to some niche communities, and developing open source packages. A great foot in the door is when someone is using and likes some software that you have written. If they know it came from you, and can easily find out you're available when they're ready to hire. Of course, that's not an option for everyone -- some people simply haven't yet had the free time or paid opportunity to make open source -- and we should try not penalize people for that. But it's much better positive signal than a job-seeker approach of \"I spent person-months memorizing Leetcode medium-difficulty answers, and rehearsing whiteboard interview stage presence like a rockstar.\" Beware that good Lisp-family jobs are rare, and don't tend to be highest-paying. Outside of relative pay within categories like enterprise Java-shop programmer. (Aside: From a hiring perspective, using some beloved niche language with few jobs available is also a great way to pick up mythical \"10x\" hires, and retain them for a long time. :) But, more seriously, it's ethical to make sure that prospective hires for unpopular niche keywords realize what they're getting into, in the current software job environment that often assumes fad-following & job-hopping strategy, and is skeptical/derisive of people whose resumes don't look like that.) reply colingw 7 hours agorootparentIf the Stackoverflow Dev Survey is to be believed, CL and Clojure devs actually make decent money. reply neilv 6 hours agorootparenthttps://survey.stackoverflow.co/2023/#work-salary This chart I'm looking at might be broken, because mouseover is showing median salary of exactly $96,381 for all of Scala, Elixir, Clojure, \"Lisp\", and F#. (But somehow OCaml and Haskell didn't get jumbled in with those.) BTW, I'm keeping in mind that people into the fringe power-user tech might be more capable than your average bear. For example, your typical person who, somehow, got years of experience with CL, IME, is a lot more capable overall than your typical person with the same number of years using Python. So, someone taking home $150K doing a Lisp at a company that lets them might have comparable skills to someone making $500K at a FAANG. (Excluding the blip when Google acquired ITA Software, which I guess brought a bunch of CL people there.) reply reikonomusha 7 hours agoparentprevIt's been in Who's Hiring threads, r/lisp, etc. There's a Lisp job advertised on Reddit [1,2] right now even. Another good source is [3]. [1] https://www.reddit.com/r/ProgrammingLanguages/s/AZmouaoARl [2] https://jobs.lever.co/dodmg/af802f7f-4e44-4457-9e49-14bc47bd... [3] https://github.com/azzamsa/awesome-lisp-companies reply colingw 10 hours agoparentprevThe Clojure Slack has active channels for both job posters and job seekers. reply Jach 9 hours agoparentprevHaven't had an exclusively lisp job, so maybe I shouldn't comment, but... I did use CL and Clojure on the job for a few smaller things at my last two places. It's easier to find Clojure companies (and them to find you) than Common Lisp ones. You might want to peruse https://github.com/azzamsa/awesome-lisp-companies from time to time and see if any have openings. There's other resources linked too and of course there's the reddit and discord community (such as there is) hubs. You can also see if there are any meetups in your area, that's how I almost ended up at a Clojure startup some years back. I should have taken strategy notes after talking to a guy at my last job who got management buy-in to rewrite a lot of Java code (for android) to Kotlin and have all new code for android be in Kotlin (before that was considered the sensible default). I think that's in general a better approach for a lot of would-be paid lispers: don't wait for or look for the lisp job, make the lisp job. Whether that's doing work where the customer doesn't care what language the thing is made in, or introducing it (some have even snuck it in -- the original clojure.jar got a lot of early success that way) to an existing work place. What I somewhat remember from my conversation was that if you can make a good technical case and have at least one other person supporting you (ideally your entire dev team as was his case), it's a lot easier to sell. No one raised bogus concerns about increasing the hiring difficulty or effort learning the new system. (I say bogus because engineers are learning all the time, and huge swathes of the industry have already had to do things like migrate from ObjC to Swift, or the various versions of JavaScript and later TypeScript + all the framework churn, switching IDEs; learning and change are quite common and a non-issue.) From other Lisp company reports, getting a new hire up to speed to be productive with the team using Common Lisp is a matter of a week or two, a small portion of the overall onboarding time a lot of new jobs have. Mastery takes longer, of course, but that's different. If I had stayed longer at my last job I would have continued to flesh out a better demo for interactive selenium webdriver tests for our main Java application after injecting ABCL into it, it seemed like the easiest vector to get more interest from my team and other teams. It kind of sucks when you're debugging a broken test and finally hit an exception but now you have to start over again (especially if you stepped too far in the debugger), especially with heavy webdriver tests that can take a long time. The Lisp debugging experience is so much better... And when writing the test from scratch, it's very interactive, you type code and execute it and verify the browser did what you intended. When you're done you run it again from scratch to verify. reply sph 12 hours agoprevI believe this article is selling Guile short. I think it is the most pragmatic Lisp around. I love Lisp's ideas but I find all modern implementations terrible. I would go so far as to say Lisp aren't popular because what we have today is not good enough. Common Lisp is the most advanced one, but it is like C++: it does everything and the kitchen sink. Design by committee. You want functional programming AND imperative? You want documentation that reads like an IBM mainframe manual? You want a standard library with names as cryptic as ANSI C? We got all that. But frankly, it is the one to build serious production software with. Racket is the best for a beginner, but it keeps having that academic, \"we made it for the kids\" feel of being easy to understand but not very pragmatic for a seasoned programmer. And it is one of a kind, so a good Racket developer might never know Lisp itself. Schemes are fun, but the only standard everyone agrees to relegates to small embeddable languages almost like Lua. They are not made to live on their own almost. Since the standard is very simple, there are half a million implementations that are not very practical. But of these, there is Guile, which has a pretty decent documentation AND standard library (even a built in PEG parser!), actively developed and it is my opinion that is it very underrated and the only Lisp worth my time these days. To be honest, the most popular Lisp in the world is Emacs Lisp bar none. How's that Guile port coming along anyway? reply nerdponx 11 hours agoparentThe problem with CL is not that it does everything, but that it does everything with a mish-mash of inconsistent idioms, thick layers of jargon, and implementation-specific behavior in places where you wouldn't really expect, leading to a combination of implementation lock-in and dependence on 3rd-party libraries that is stronger than one might expect at first. My (least) favorite example: (nth needle haystack) (aref haystack needle) Ugh! But then again, what other language provides both AREF and ROW-MAJOR-AREF? And of course CL tooling is just weird if you aren't used to it. Quicklisp is amazing! But it has no CLI and doesn't use HTTPS for package downloads. Swank and Slynk is amazing! But clients other than Slime and Sly are second-class citizens. SBCL can generate a compiled binary, cool! But the routine is called \"save-lisp-and-die\". ASDF expects you to either put all of your Common Lisp projects in a single directory, or hard-code project locations in a config file. Does Go still do that too? Ancient alien technology for sure, but at least moderately damaged upon crash-landing. Ironically, the only piece of CL tooling that feels mostly not-weird is called Roswell. Of course all of this stuff is completely free-as-in-beer and developed almost entirely by volunteers in their spare time. Hard to criticize: they built it, and I didn't. But there is definitely a cumulative oddball feeling to it. reply floren 11 hours agorootparentSome day I'll do something useful with the debugger in SLIME but so far it's mostly \"look at the error message, find the one frame in the stack that tells me something useful, hit either continue or abort\" reply vindarel 10 hours agorootparentQuick help: look at Emacs/your editor menu for commands, in Emacs type \"v\" to go to the buggy line, fix your bug, compile the function (C-c C-c), type \"r\" on the buggy frame to restart it from where it failed, and voilà. If you were processing something costly, you didn't have to restart it from zero. reply vindarel 10 hours agorootparentprevQlot works well for project-local dependencies. https://qlot.tech/ https for QL: https://github.com/rudolfochrist/ql-https another package manager: https://github.com/ocicl/ocicl (\"built on the world of containers\") Can't really refute the weird feeling. But there's often a reason! (like, it makes totally more sense to install a library from within the Lisp REPL and not from the terminal, we can use it right away) reply sph 11 hours agorootparentprevYou hit the nail on the head. And I like the comparison with alien technology: in some ways it feels centuries ahead than our mainstream languages, but in other it feels they have never developed stuff we take for granted in 2024, like first-class hashmaps, Unicode and decent date/time functions. reply gmfawcett 5 hours agorootparentSBCL has excellent Unicode support, for one, and hash tables are first class objects in all CL implementations. Won't disagree about date and times. reply schemescape 8 hours agorootparentprevGood list! I, too, have many complaints about Common Lisp. But for a language whose standard hasn’t been updated in roughly 30 years, it holds up impressively well! reply Capricorn2481 10 hours agorootparentprev> SBCL can generate a compiled binary, cool! But the routine is called \"save-lisp-and-die\" ...And? reply kazinator 11 hours agorootparentprev> what other language provides both AREF and ROW-MAJOR-AREF C? :) reply velcrovan 11 hours agoparentprev> Racket is the best for a beginner, but it keeps having that academic, \"we made it for the kids\" feel of being easy to understand but not very pragmatic for a seasoned programmer. And it is one of a kind, so a good Racket developer might never know Lisp itself. As someone trying to write definitively about cold hard reasons Racket remains in relative obscurity, I would be interested to know specifics you can provide for this view. Is it just academia vibes? Or are there specific aspects of Racket that in your view repel “seasoned programmers”? I notice you praise Guile for having decent documentation and standard library. How do you believe Racket’s documentation and standard library compare to Guile’s? reply sph 11 hours agorootparentSorry, I don't have anything more concrete than \"general vibes.\" Like, there's thousands of libraries and modules in Racket, sometimes competing with each other, that feel like they have been developed for a school project and kept around. And it is no secret that Matthew Flatt, one of the lead developers, is actually a professor at Utah University. In my experience, it is pretty easy to tell when a language and ecosystem is developed in academia, or developed by software engineers by trade. They have two very different goals and approaches to the same problem. The former might focus on educational purpose for newbies, the latter on shipping production-ready software for professionals. (In my humble opinion, this is one of the reasons Smalltalk never went very far in the real world. Too much focus on the educational aspect of it.) reply a1369209993 10 hours agorootparentprev> Or are there specific aspects of Racket that in your view repel [competent programmers]? This is anecdotal and over a decade ago, but quite specific: my first exposure to Racket was with a version of the language (I think it was some pragma-type thing like `#lang`, but it's been a while) allegedly intended for CS classes, and included something along the lines of [exact spelling and phrasing almost certainly differ]: > (cons 'foo 'bar) error: cons: 'bar is not a list I immediately deleted the Racket installation and added it to the same set of blacklists as the Java Virtual Machine (and hypothetically any COBOL implementations I ever encounter). That any version of the language would behave that way, much less one purportedly intended for people who are only just learning LISP in the first place, is a insult to everyone who ever bothered to actually learn LISP in the first place. This is the sort of \"Dangling by a Trivial Feature\"[0] thing that there's just no excuse for, like trying out a new text editor and discoving that pressing backspace inserts the text \"^H\", except it was clearly deliberately aimed at people who didn't know any better and would 'hopefully' not realise there was a problem, rather than just being a local idiosyncracy[1], because cons did work correctly under other dialect settings. Actively trying to fuck over newcomers who are only just learning LISP (or programming in general) deeply offends me. 0: https://prog21.dadgum.com/160.html 1: In which case I still wouldn't use Racket myself, but I wouldn't be actively opposed to anyone else doing so. reply samth 7 hours agorootparentIt is indeed the case that the student languages in Racket use cons only for lists. Improper lists are an extra complexity that people just learning to program don't need. reply drekipus 9 hours agorootparentprevI've dabbled in a few lisps and definitely a noobie but I have absolutely no idea what you're talking about reply logicprog 6 hours agorootparentThat language behavior fundamentally misrepresents how a proper cons should actually work. It's a link between any two values, represented as essentially two joined pointers, one to each value. It represents a linked list when cons cells are nested, but that's just something that falls out of the far more beautiful and fundamental core axiom of the cons cell, not all of what cons cells are. Thus claiming cons cells must be nested (which is what requiring the second argument of cons to be a list (I.e. more nested cons cells or nil) implies) is actually bastardizing what a cons cell is, reducing it to a menial operation for producing lists that means the same thing as \"prepend\" instead of an elegant core abstraction you can derive further things from. Bastardizing it like this, honestly especially in a language for teaching CS, in the name of talking down to students, is unforgivable. I learned Common Lisp at 12 partially from Land of Lisp and partially from a Lisp 1.5 manual and experimentation. Properly learning the core abstractions was vital. reply kazinator 6 hours agorootparent> requiring the second argument of cons to be a list A language called ISLisp (ISO Standard Lisp) also does this nasty thing. reply a1369209993 4 hours agorootparentTo be scrupulously fair, that seems to fall under > just being a local idiosyncracy albeit a stupid one; whereas Racket actively targets beginners in particular. Also, this is ISO we're talking about here; we probably shouldn't expect anything technically competent from them that isn't just a rubber-stamped version of someone else's specification, and they didn't call it \"ISO Standard Common Lisp\" or \"ISO Standard Scheme\", presumably for good reason. reply nsm 7 hours agoparentprevI would like to disagree with this view of Racket. I'm a fairly experienced professional programmer with a lot of systems experience, and I find Racket an extremely well thought out, competently implemented and high performance (particularly with the switch to Chez Scheme) language. I wrote a bit about it at https://nikhilism.com/post/2023/racket-beyond-languages/ and Bogdan Popa and Alex Hirsanyi have done amazing stuff with the language https://defn.io/ https://alex-hhh.github.io/index.html including a sophisticated web framework, a Kafka GUI client and so on. I think the only deficiency for a \"pragmatic for a seasoned programmer\" (beyond the standard \"no FAANG is sponsoring it\" related lack of resources and presence in popular discussion) is the lack of good editor integration beyond emacs. That said, I've found DrRacket perfectly reasonable for my uses, although clunky. I'm not saying it can replace something like Python, but that is more because of the smaller ecosystem and contributor base than anything wrong with the language itself. reply hajile 10 hours agoparentprevIt's here that I'll again get on my soapbox about Scheme. SRFIs suck really badly. The language isn't useful without them, but every implementation uses a different subset. You have to find out which SRFI does what you want (there may be more than one) then find out what number it is then look up the SRFI itself because the spec is pretty much the only documentation you're going to get. This entire situation sucks for experienced devs and simply kills most beginners before they even do anything. Things were supposed to get better with R7RS. It was supposed to have a tiny core language to keep the R5RS crowd happy, but add the R6RS features in the large edition that are needed to get real work done. R7RS-small released in 2013. That's TEN YEARS ago and it STILL doesn't have a standardized library because apparently nobody wants to work on R7RS-large. The whole thing is an unusable mess for no good reason and it's killing the language. reply zilti 9 hours agorootparentPeople are working on R7RS-Large, a lot of the standard is decided upon by now. I suspect, though, it'll take another five years... https://github.com/johnwcowan/r7rs-work/blob/master/WG2Docke... reply natrys 1 hour agoparentprev> How's that Guile port coming along anyway? It's just not. It was someone's GSoC project, and they basically stopped working on it in 2015 and nobody picked up the slack since: https://git.hcoop.net/?p=bpt/emacs.git We only got Nativecomp in elisp because a very talented and persistent compiler engineer from ARM saw through it. With the basics done, I think they want to tackle doing more aggressive optimisations with libgccjit. But best not to take things for granted when bus factor is 1. reply evdubs 7 hours agoparentprev> Racket is the best for a beginner, but it keeps having that academic, \"we made it for the kids\" feel of being easy to understand but not very pragmatic for a seasoned programmer. And it is one of a kind, so a good Racket developer might never know Lisp itself. I find Racket to be more pragmatic than Java, Python, and JavaScript. So much so that I trade options with it. Edit: \"pragmatic\" not for all things, but for many things, including GUIs and charts. Second edit: whoops. I see I've already pointed out this Racket program to you. https://github.com/evdubs/renegade-way reply nequo 5 hours agorootparentThis is a very cool project! Have you considered using Typed Racket for it? If yes, then how do you see the tradeoffs? reply evdubs 2 hours agorootparentI sort of evaluated Typed Racket before I started writing lots of Racket code, and my experience was that \"Contract\" Racket (regular non-Typed Racket) was more ergonomic and idiomatic. I think the contract system is great and I use it, for example, when sending messages to Interactive Brokers: https://github.com/evdubs/interactive-brokers-api/blob/maste... This file just includes request message definitions and to-string (->string) implementations. The overall program's performance is adequate for me, so if the contract system is causing overhead, I don't particularly care about it. Maybe that's a scalability concern for more performance-demanding programs which could benefit from Typed Racket. reply davexunit 11 hours agoparentprevYeah Guile should get more love! Very practical lisp that punches above its weight. I've had the fortune of writing Guile as my full-time job for the past year and I've been a user for a lot longer so I can attest that you can get stuff done with it. reply NeutralForest 11 hours agoparentprevLol, a bit scathing but mostly fair from my experience. I haven't used Lisps a lot but I'm a pretty big Emacs user and I've played around with the languages your mentioned and I feel somewhat the same. I'll just add that the ideas and many libraries around Racket are super cool. There's really a bunch of documentation and tutorials for a lot of things, it's fun. reply neilv 11 hours agoparentprev> Racket is the best for a beginner, but it keeps having that academic, \"we made it for the kids\" feel of being easy to understand but not very pragmatic for a seasoned programmer. And it is one of a kind, so a good Racket developer might never know Lisp itself. I know why you got that reasonable impression, but there's more to it, which can make it much more interesting to practitioners... The gang-of-professors reasons for Racket are for research platform and education platform. However, at the same time, Racket (then called PLT Scheme) attracted a disproportionate share of a user community of high-powered programmers and software engineers, like you also see with Common Lisp and some other languages. And one of the professors, Matthew Flatt, happens to be a great systems programmer, with good software engineering sensibility. There's some really solid stuff in Racket, and I've used it on important systems with hyper-productive teams (you'd think they had a hundred engineers, when it was only a few). The impression you get might be for two reasons: 1. The gang-of-professors decides most of the customer-facing image for Racket, in various ways, such as writing some introductory books, promoting their zero-prior-experience-student-oriented IDE as the Racket IDE, odd language on the Web site, etc. 2. At times they've also made direction decisions that suit either their education&research goals or, secondarily, their idea of what professional practice wants. On the latter, of course, people who've been professors for decades aren't going to have all the insights of the best people who've been practicing in industry that same period. If you want to approach Racket as a serious and skilled practitioner, one way is to focus on the Guide and Reference books (I wish these were consolidated), and expect that you'll have to creatively build much of the ecosystem bits you need from scratch. That can actually be a great situation to be in, if you're up for it. The governance model, last I checked, is a benevolent dictatorship by the gang-of-professors, and the industry practitioner user community is currently small, so just be aware of that going in. But, there's an open ecosystem for packages, more empowering than most languages, due to all the language-extension mechanisms of Racket, so you can \"build out\" Racket in a decentralized way, to some extent. If you do that, you'll want to keep on top of what the gang-of-professors are doing, and give them a heads-up on things you're doing, to minimize unpleasant conflicts. I'm not going to make a sales pitch for it; just wanted to add some info for serious practitioners who're already interested, on what to expect, and how to approach it. reply FrustratedMonky 11 hours agoprev [–] Not even an honorable mention for F#? Why does Haskell get a mention, since it is also just another ML based language. Seems like F# is trying to solve the same problems as LISP. Lot of same features. Edit: Did like article. Is serious question LISP-> Clojure-> F#. There are similarities. Edit2: Guess it is a stretch. But not insultingly so. Have seen articles comparing Lisp macros to F# Type Providers. reply nerdponx 11 hours agoparent [–] F# isn't a Lisp, so I think it's fair to consider it out of scope. I think the only reason Haskell got mentioned is because the author has experience with it and was using it as a personal point of reference. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author shares their experience exploring different Lisp dialects such as Guile, Common Lisp, Fennel, Clojure, and Emacs Lisp in 2023.",
      "They discuss the strengths and drawbacks of each dialect and praise Clojure for its tooling and community support.",
      "The author expresses their preference for a Lisp language that combines features from Clojure and Common Lisp and encourages newcomers to embrace Lisp, seek help from online communities, and build real projects."
    ],
    "commentSummary": [
      "The summaries cover discussions on various programming languages within the Lisp family, including Guile, Gauche, Common Lisp, Scheme, Clojure, Elisp, Hy, Fennel, Janet, and Racket.",
      "Users share their experiences, opinions, and concerns about these languages, focusing on documentation, performance, compatibility, and job opportunities.",
      "The advantages and challenges of using Lisp languages are explored, alongside the popularity and limitations of programming languages like Python and Java. Other topics covered include immutability, debugging, syntax, and the application of Lisp in different contexts."
    ],
    "points": 255,
    "commentCount": 182,
    "retryCount": 0,
    "time": 1706562936
  },
  {
    "id": 39179031,
    "title": "Former Air Force Officer Discovers Possible Amelia Earhart Plane",
    "originLink": "https://www.businessinsider.com/sonar-image-pilot-amelia-earhart-plane-found-pacific-ocean-2024",
    "originBody": "Transportation A former US Air Force officer spent $11 million searching for Amelia Earhart's long-lost plane — and may have found it Katherine Tangalakis-Lippert and Rebecca Rommen 2024-01-28T04:58:18Z Share Facebook Email Twitter LinkedIn Copy Link Save Read in app Amelia Earhart seated atop the Lockheed-Electra monoplane and a sonar image of what researchers believe may be the wreckage of her ill-fated flight. Getty Images/Deep Sea Vision Redeem now Tony Romeo believes he's discovered Amelia Earhart's long-lost aircraft. Romeo said he captured an image of an aircraft-shaped object on the floor of the Pacific Ocean. Experts say the location seems roughly correct, but clearer images are needed. Advertisement A pilot and former US Air Force intelligence officer believes an image he captured using sonar on a high-tech unmanned submersible may have finally answered one of America's most baffling mysteries: What caused the disappearance of the iconic pilot Amelia Earhart at the height of her fame? Tony Romeo is one of a long line of researchers and hobbyists to have taken up the search for Earhart's distinctive Lockheed 10-E Electra plane, which disappeared over the Pacific Ocean along with its famous pilot and its navigator, Fred Noonan, during an attempt to circumnavigate the globe in July of 1937. Amelia Earhart, 40, standing next to a Lockheed Electra 10E, before her last flight in 1937 from Oakland, California. AP Photo The mystery surrounding Earhart's disappearance has long puzzled researchers and spurred conspiracy theories over the years, including the Japanese taking her prisoner and her being a government spy. But Romeo, a former real-estate investor who sold commercial properties to raise the $11 million needed to begin funding the search, returned in December from a roughly 100-day voyage at sea with a sonar image that he believes shows the lost plane in the ocean's depths. Advertisement A high-tech search at sea His expedition, which was carried out using a $9 million high-tech unmanned submersible \"Hugin\" drone manufactured by the Norwegian company Kongsberg and a research crew of 16, started last September in Tarawa, Kiribati, covering 5,200 square miles of the ocean floor, The Wall Street Journal reported. It was a dream Romeo had for years before making it a reality. \"This has been a story that's always intrigued me, and all the things in my life kind of collided at the right moment,\" Romeo, whose father and brothers are also pilots, told Business Insider. \"I was getting out of real estate and looking for a new project so even though I really started about 18 months ago, this was something I've been thinking and researching for a long time.\" Amelia Earhart took off from the airport in her £10,000 Flying Laboratory for Honolulu on the first leg of her round-the-world flight. AP Photo Roughly a month into the trip, the team captured a sonar image of the plane-shaped object about 100 miles from Howland Island — but didn't discover the image in the submersible's data until the 90th day of the voyage, making it impractical to turn back to get a closer look. Advertisement Experts have shown interest in the finding, with Dorothy Cochrane, a curator at the Smithsonian Institution's National Air and Space Museum, telling The Journal that the reported location where the image was taken was just about right, geographically, compared with where Earhart's flight is believed to have gone down. A map of where Earhart's plane is believed to have gone missing along her presumed flight path. Google Maps But others say they need clearer views and more details, such as the plane's serial number. \"Until you physically take a look at this, there's no way to say for sure what that is,\" Andrew Pietruszka, an underwater archaeologist at the Scripps Institution of Oceanography, told The Journal. Romeo, who said the search might be \"the most exciting thing I'll ever do in my life,\" added that he planned to return to the area to try to capture better images using autonomous or robotic submersibles equipped with cameras and sonar to get closer to the object, which rests more than 16,500 feet beneath the surface. Advertisement Romeo told BI that if it wasn't Earhart's plane, the object he found could be a different missing aircraft lost in the Pacific or — less interestingly, perhaps — another manmade object that fell off a shipping container. But as of now, he's feeling confident he's made a groundbreaking discovery due to the distinctive shape of the fuselage, tail, and wings. Romeo and his company, Deep Sea Vision, discovered an object of similar size and shape to Amelia Earhart's iconic plane deep in the Pacific Ocean. Deep Sea Vision \"The next step is confirmation — we've got to go back out with different sorts of sensors and really photograph it well and take a look at how the artifact is sitting on the seabed,\" Romeo told BI. \"Once that step is done, lots of people will be involved. The Smithsonian, the family, there'll be some investors involved because it'll be an expensive operation, but then we're thinking: 'How do we lift the plane? How do we salvage it?'\" He added: \"I don't think we're there yet. But I do think Americans want to see this in the Smithsonian; that's where it belongs. Not the bottom of the ocean.\" A decadeslong mystery Hopeful explorers have pumped millions of dollars into expeditions to find Earhart's lost plane over the years, but her last known location has made the searches difficult. Advertisement \"It's very deep water, and the area that she could've possibly been in is huge,\" Tom Dettweiler, a sonar expert, told The Journal. One team who searched for Earhart's aircraft in 2009 said on Twitter that following its 2,500-square-mile search near Howland Island, close to where Romeo conducted his search, it only knew where the aviator wasn't. We're confident we know where #Earhart isn't... our #sidescan #sonar #mosaic is simply amazing -> http://searchforamelia.org/resultsintro — Waitt Institute (@WaittInstitute) January 12, 2010 Earhart, who was the first woman to fly solo across the Atlantic and the US, was declared legally dead on January 5, 1939, two years after she vanished. But her legacy has lived on, and she continues to fascinate people worldwide. \"It was one of the great mysteries of the 20th century and still now into the 21st century,\" Cochrane told The Journal. \"We're all hopeful that the mystery will be solved.\" Advertisement The dateline theory Romeo believes he's taken a massive step toward answering vital questions surrounding the famous pilot's disappearance after scouring decades of clues and potential leads to her location, including the \"dateline theory.\" The theory, which Romeo relied on partly to guide his search, suggests that when Earhart crossed over the international dateline during her 20-hour flight, her navigation system became inaccurate and misdirected her by about 60 miles, potentially leading to a tragic end. Earhart after landing in Honolulu after a speedy flight from Oakland, California, in March 1936. AP Photo Romeo said if he received confirmation that he'd found Earhart's plane, hopefully during another voyage planned for later this year, the company he created as part of the search would continue trying to solve other mysteries held in the ocean. \"There's lots of cool stuff in the Pacific — WWII aircraft and flight MH370 are still out there, and maybe we can make a run at that at some point,\" Romeo told BI. \"I'm not announcing yet that we are, but I'd love to collaborate with other folks on other projects since we've got the state-of-the-art equipment. There's only a couple of these in the world, and finding these things out is in demand.\" Sign up for notifications from Insider! Stay up to date with what you want to know. Subscribe to push notifications Read next Watch: Amelia Earhart's disappearance may finally be explained with one photo Aviation History Advertisement",
    "commentLink": "https://news.ycombinator.com/item?id=39179031",
    "commentBody": "An Air Force officer who spent $11M searching Earhart's plane may have found it (businessinsider.com)200 points by rntn 16 hours agohidepastfavorite156 comments CommieBobDole 16 hours agoThe interesting thing about this is that it doesn't appear to involve TIGHAR at all. TIGHAR, for those unfamiliar, are the people who pop up every few years with tantalizing new evidence that certainly will prove once and for all some novel theory about Earhart's disappearance and raises a couple of million dollars to mount an expedition which of course reveals nothing but some tantalizing new evidence that requires a new expensive expedition to investigate. So, there might be something to this. reply hn_throwaway_99 15 hours agoparentFor others curious about the acronym: The International Group for Historic Aircraft Recovery, https://en.wikipedia.org/wiki/TIGHAR reply RajT88 15 hours agoparentprevI seem to recall these guys. Last time I read an article about them, it read like they were in cahoots with the government of Kiribati to promote tourism. Fun read though! reply Sniffnoy 14 hours agoprev> The theory, which Romeo relied on partly to guide his search, suggests that when Earhart crossed over the international dateline during her 20-hour flight, her navigation system became inaccurate and misdirected her by about 60 miles, potentially leading to a tragic end. How would this happen? It's not like her plane would have contained a misprogrammed computer. reply jameshart 13 hours agoparentJournalistic misinterpretation of what an expert told them leading them to write something dumb. Earhart’s “navigation system” was Fred Noonan, an almanac, and a chronometer. Fred looking on the wrong page of the Almanack because he forgot which side of the dateline they were going to be on is what’s implied here. reply wolverine876 11 hours agorootparent'Navigation system' could be paper, maps, compass, etc. It's modern bias to assume that means 'computer' or 'electronic'. reply pests 7 hours agorootparentExactly. Just like I have an organization system for my clothes. That doesn't mean I have a computer controlled robot organizing my clothes. It just means socks go in the top left drawer. reply jameshart 6 hours agorootparentRight, but if a reporter is trying to write a story why all your t-shirts turned pink, and after you explain that system to them, they write: \"When the clocks went back at the end of daylight savings, pests' organization system became inaccurate and misdirected a red sock into the whites wash,\" ... they might have misunderstood what you were trying to say. They certainly aren't doing a good job of explaining to their reader how your clothing organization system works. reply wolverine876 2 hours agorootparentWhat evidence is there for your accusation against the journalist? I don't see any at all but your personal interpretation of 'system' (which many disagree with) and your personal imagination of how it got into the article. reply Rebelgecko 13 hours agoparentprevNavigators used tables to determine the longitude that depend on the day. The theory[0] is that because their flight involved multiple day changes, Noonan might've done some on the fly calculation that plugged star positions into the wrong day's table, putting them off by 1 degree of longitude. [0]: http://www.datelinetheory.com/, no idea how plausible this is reply bb611 13 hours agoparentprevNo computer (device), but definitely a computer (person who computes) the location of the plane and destination in a formula that relies on the correct date for accurate long distance navigation. reply Jgrubb 14 hours agoparentprevGoogle brings me here - http://www.datelinetheory.com/p/time-and-celestial-navigatio... reply chernevik 14 hours agoparentprevProbably a polite way of saying she screwed up. She had a reputation as an indifferent navigator and aviator which is consistently downplayed. reply bb611 13 hours agorootparentFred Noonan was hired as the navigator specifically for this leg of the flight because of his navigation skills, which he proved by establishing Pan Am's transpacific routes, so it's not obvious to me why a navigational error would rest on Earhart's lack of skills in that area. reply ciscoriordan 14 hours agoparentprevAn electrical issue could cause a gauge to misreport. E.g. vibrations cause a bad ground. reply Sniffnoy 14 hours agorootparentAn electrical issue wouldn't be caused by crossing the international date line. reply dtgriscom 12 hours agorootparentMaybe the Bermuda Triangle is on walkabout? reply WalterBright 11 hours agorootparentprevCrossing the equator causes the sun to flip around and go the other way in the sky. Source: When I went to Australia, the sun went the wrong way. It was curiously unsettling. reply justinclift 6 hours agoprevDoesn't the mention of titanium here seem wrong? If the Electra had been built of other metals, say aluminum and titanium, there wouldn’t be anything left but free atoms floating around. Titanium is supposed to be resistant to corrosion in seawater due to the oxide layer that forms: https://journals.sagepub.com/doi/full/10.5301/jabfm.5000387 Titanium is reported to be highly resistant to general corrosion in seawater. ... These properties make titanium suitable for a variety of applications involving seawater, starting from thin-walled heat exchanger tubing, with consequent good heat transfer properties, up to submarine hulls, ships, platforms, desalination plants, salt production evaporators and water jet propulsion systems. reply Kon-Peki 16 hours agoprevI have no idea how to interpret that sonar image. Am I supposed to see what looks like an airplane when viewed from above? Because the Lockheed Electra didn't have swept-back wings. I don't think any airplane had swept-back wings until the 1950s. reply jncfhnb 15 hours agoparentThere’s literally a side by side diagram showing you how to look at. Not that your wings comment is wrong. reply entangledqubit 11 hours agorootparentStill, seeing as how I don't look at these kinds of images all day it's really hard for me to gauge if the sea floor is littered with false positive images that may seem to be a good match. I could also imagine a true match to also have some other confounding blob attached to it. It'd be nice to have some kind of score to summarize that aspect. reply Kon-Peki 15 hours agorootparentprevHa, I clearly didn't scroll far enough. Thanks :) reply ZiiS 12 hours agoparentprevPost crash many planes have swept-back wings reply WalterBright 11 hours agorootparentSwept wings wasn't a thing until after WW2. The Me262 had swept wings because the engines came in heavier than expected, and sweeping the wings back was the easiest fix. reply dralley 8 hours agorootparentRead their comment again with a bit more irony :) reply WalterBright 2 hours agorootparentI meant to reply to the parent of that. reply Cthulhu_ 15 hours agoparentprevCould it be either sonar distortion or that the wings were damaged in the crash? Speaking of which, it looks pretty intact, which means (armchair air crash specialist here) it wouldn't have crashed but done an emergency landing on the water. reply jlbooker 11 hours agorootparentThat would match with the leading mis-navigation theory. She and her navigator were fine and healthy, as was the plane. They were looking for the island to land on, but had messed up the navigation and were sufficiently off-course. There were US Navy boats in the area of the island that heard their radio calls. There was no emergency -- they just reported to be searching for the land that they should've found. Presumably they flew until they ran out of fuel and likely set it down in the water as gently as possible. reply karaterobot 15 hours agoprevRelevant search: this presumed plane is at 16,500 feet of depth, and the record for deep sea salvage is, according to a quick search, about 19,000 feet. I had wondered whether it was feasible on the face of it to try salvaging this plane, and it may be. No comment on the advisability. reply maplet 14 hours agoprev> Roughly a month into the trip, the team captured a sonar image of the plane-shaped object about 100 miles from Howland Island — but didn't discover the image in the submersible's data until the 90th day of the voyage, making it impractical to turn back to get a closer look. To be a skeptic, this sounds like confirmation bias. reply gleenn 14 hours agoparentOr maybe the computer they had analyzing the data takes a while to search and it had nothing to do with people at all. reply CSMastermind 14 hours agoprevTo quote Brian Dunning of Skeptoid: > The biggest secret of the Amelia Earhart mystery is that there is no mystery, and never has been. The USCG Itasca was on station at Howland and in partial radio contact with Earhart when she and Fred Noonan ran out of fuel and ditched after having slightly overshot the island in the dawn lighting conditions. The USN and USCG analyzed their data and identified this area as where the plane went down... > So the best news about Tony Romeo’s find is that he’s looking in the right place, unlike the TV networks and the random crackpots whose claims they promote. Romeo hasn’t divulged the exact location, for obvious reasons; but his location has the endorsement of Dorothy Cochrane, a curator in the aeronautics department of the Smithsonian Institution’s National Air and Space Museum. She knows where the USN and USCG have said where the plane is, so that tells me Romeo is probably right. https://briandunning.substack.com/p/i-remain-very-guarded-ab... reply Apocryphon 14 hours agoparentSo like the fate of the Roanoke colony, the cause of the Tunguska explosion, the Dyatlov Pass incident, the explanation is what's right in front of everyone? reply bhickey 13 hours agorootparent> the fate of the Roanoke colony What is the prosaic explanation for the Roanoke colony? reply runjake 13 hours agorootparentGreat Wikipedia article: https://en.wikipedia.org/wiki/Roanoke_Colony Being a west coaster, I don't recall ever hearing about it. Colonial era Europeans observed that many people removed from European society by Native Americans for substantial periods of time – even if captured or enslaved – were reluctant to return; the reverse was seldom true. Therefore, it is reasonable to postulate that, if the colonists were assimilated, they or their descendants would not seek reintegration with subsequent English settlers. reply fwip 13 hours agorootparentprevThe colonists, lacking food and supplies to get by on their own, went over and joined the local Native American tribe, the Croatoan. Their descendants are likely now part of the Lumbee tribe. reply nocoiner 13 hours agorootparentSo weird that they all disappeared! And carved “Croatoan” into a tree before that happened! I cannot imagine where they might have gone. reply Apocryphon 12 hours agorootparentYeah, regardless of what might have happened to them there, it's pretty obvious that it was the destination of the colonists. reply tialaramex 13 hours agorootparentprevIntegration. reply rconti 14 hours agoparentprevI was curious, but the link doesn't elaborate on how much is known about the \"ran out of fuel and ditched after having slightly overshot the island\" -- whether that's all fact, or partially fact, or all speculation. reply CSMastermind 14 hours agorootparentHere's the US Navy's full report: https://catalog.archives.gov/id/305240 Which references the radio logs from the day of the disappearance: https://catalog.archives.gov/id/6210268 tl;dr we know they overshot the island and were running a search pattern for it while running out of fuel because they told the radio operator on the other end as much. The only question is what specifically happened (did they manage to get life rafts out, etc. reply saalweachter 12 hours agoparentprevAnd to quote his article with the bit that matches my first impressions: > I am not super gung-ho about the image. Yes, it is vaguely airplane-shaped (though not a great match for the Lockheed Electra). It’s also vaguely anchor-shaped, or the shape of most any random pile of rocks on the ocean floor — you can see at the bottom left of the image there’s another object right next to it, which would be improbable if this was indeed a lone aircraft sitting on the ocean floor. It could be anything. reply hondo77 11 hours agorootparentWhat? That image looks just like Bigfoot! reply WalterBright 11 hours agoparentprev> Romeo hasn’t divulged the exact location, for obvious reasons The most obvious reason being he doesn't know the exact location. reply francisofascii 13 hours agoparentprevthe exact location is a big part of the mystery reply qxfys 14 hours agoprevCan we do the same with MH370? I've got a classmate inside that plane. His family has been deprived of any form of closure or peace regarding his fate. reply stirlo 14 hours agoparentAround $150 million was spent searching including with high resolution sonar[1]. But obviously nothing was found. The area they're searching is far larger unfortunately. [1] https://en.wikipedia.org/wiki/Search_for_Malaysia_Airlines_F... reply dralley 8 hours agoparentprevThe precise location of the wreckage nonwithstanding, the fate is pretty clear. Several pieces of MH370 have been discovered washed up along the coast of Africa and Madagascar over the past few years. reply wkat4242 5 hours agorootparentThe fate is clear, the how and why is not. Of course that's important for families. And $150M is less than the plane itself would have cost to buy. reply ianburrell 13 hours agoparentprevA company, Ocean Infinity, is continuig searching with robots. reply chasd00 13 hours agoparentpreviirc when it was missing there was a lot of wild speculation and then someone (maybe it was on twitter) discretely said it was on the bottom of the ocean at some location i can't remember and nothing else. The conclusion was this person was from some three letter agency and knew exactly where it was because of all the listening devices in the various oceans hunting for submarines. They(agency) will never say where it is because it exposes their capability. I think a similar thing happened when that submarine imploded on the way to the titanic. reply codezero 1 hour agorootparentThey readily shared the implosion detection data with search and rescue teams. reply wolverine876 11 hours agorootparentprev> The conclusion Whose conclusion? On what basis? reply GenerWork 12 hours agorootparentprevIs there a screenshot of this Twitter post? reply pfdietz 13 hours agoprevThis is likely Earhart's plane, as it has a fuselage and two wings. Also, I am likely the Pope, because like the Pope I have two arms and two legs. reply 0xDEADFED5 12 hours agoparenthere you go, you earned it! https://discordia.fandom.com/wiki/Pope_cards reply Izkata 11 hours agoprevSo around 400 miles* from where (likely) her bones and shoes were found? https://www.npr.org/1998/12/02/1032135/bones-shoes-may-have-... https://www.npr.org/sections/thetwo-way/2018/03/08/591950171... * Eyeballing it on Google Maps, distance from Howland island to Nikumaroro island reply fred_is_fred 15 hours agoprevHow would/could you distinguish this from any of the hundreds or maybe thousands of planes that went down during WW2 in the Pacific? Assuming it's even a plane. reply rconti 14 hours agoparentFrom the source linked in another comment: (edit, ugh, I hate HN formatting, can never get it right even after all these years) ---- One piece of good news for Romeo’s search is that there are probably very few other planes anywhere near Howland. An airstrip was built on Howland in the 1930s in anticipation of commercial trans-Pacific flights, but Earhart was going to be the first to actually use it. During the war it was bombed by the Japanese to prevent its use, and that’s the extent of its aviation history. None of the WWII air-sea battles were fought in the vicinity, and it’s much too remote for general aviation planes to ever go near. ---- https://briandunning.substack.com/p/i-remain-very-guarded-ab... reply NikkiA 11 hours agorootparentHe's wrong because that is an area that the USN actively did carrier training in from 1945 to today, and lots of planes ended up in the ocean after missing a landing, a take-off failure, or just because the pilot had to bail for whatever reason. Heck, during the evacuation of Hanoi they were pushing perfectly good jets into the ocean just to clear enough room for helicopters and the odd cessna, but that would have been further west. https://theaviationgeekclub.com/that-time-a-south-vietnamese... reply iudqnolq 8 hours agorootparentNot sure why you're bringing up the evacuation of Hanoi. Further west is a hell of an understatement. Howland Island is closer to San Francisco than Vietnam. It's thousands of miles from that anecdote. reply wolverine876 11 hours agorootparentprevWho is Brian Dunning and what do they know about the subject? reply jandrese 14 hours agoparentprevI think the author is excited because he found the plane in roughly the area he expected to find it if she was thrown off course by an instrumentation fault that occurs when crossing the international date line. Also, it was about the right shape. Also, it wasn't near the fighting in WWII, and the shape doesn't match that of WWII carrier aircraft. I'll be more excited when they get out there with a deep sea probe and take some photographs. reply RcouF1uZ4gsC 16 hours agoprevIn a surprise twist, when they explore it in detail, it turns out to be MH 370. reply bell-cot 13 hours agoparentI - Howland Island (near which they seemingly found this wreckage) is in the Central Pacific Ocean. Vs. MH370 seems sure to have gone down in the Central-ish Indian Ocean. II - If they have even the vaguest sense of scale from their image - MH370 was a https://en.wikipedia.org/wiki/Boeing_777-200ER. Vs. Earhart was flying a https://en.wikipedia.org/wiki/Lockheed_10E. There is a ~10X difference in wing area, and ~50X difference in weight. reply georgeecollins 15 hours agoparentprevThat's what I was thinking. Someday I or my kids will read about finding MH370. Will they put that in a museum? reply rand1239 14 hours agorootparentNo because it's not mysterious anymore. Also not many museums in the world that can fit one. reply dreamcompiler 13 hours agoprev> the object, which rests more than 16,500 feet beneath the surface So 4000 feet deeper than the Titanic. reply whycome 15 hours agoprevTitle reaction: How much does an 'Air Force Officer' make?! Answer: > But Romeo, a former real-estate investor who sold commercial properties to raise the $11 million needed to begin funding the search, returned in December from a roughly 100-day voyage at sea with a sonar image that he believes shows the lost plane in the ocean's depths. (Also, article title uses 'former air force officer') reply wharvle 15 hours agoparent20 years in + take retirement + also work for a government contractor at 3x+ your former pay rate (while collecting retirement!), can equal a lot of money in a hurry at a relatively young age (say, money to fund a real estate investment venture without risking being penniless in old age). reply FireBeyond 15 hours agorootparent> government contractor at 3x+ your former pay rate And then some. Friend of mine rode backseat in F-15s. After retirement, he spent two-three years as a contractor for Boeing training allied military in the Middle East and paid off his mortgage, and much much more (I want to say at least 1, maybe 2 investment properties owned effectively outright). reply wharvle 15 hours agorootparentYeah, the \"+\" is doing some heavy lifting there :-) 3x is what you can basically just fall into after military retirement without trying very hard, with a bad network, and with poor luck. Another factor is that your expenses can be quite low while you're in, even with a family. Put an officer's salary on top of that—which isn't amazing, but isn't terrible either—and you can save a fair bit of cash even before you even start the contractor + retirement double-dipping. Though there are real costs—the schools are generally very good, but you may have to move every couple years. The bureaucracy is hellish (though, for all their dysfunction, they've got a lot of shit figured out way better than your average bigco). You'll probably serve under and with some real assholes at some point. And there's ever the temptation to get out early and start taking those sweet contractor dollars before you secure the government retirement check. Plus, you know, you may have to go kill people or get killed or what have you. [EDIT] Oh, another downside: it can be really hard for your spouse to have a career, due to all the moving and often living in places without much economic opportunity. reply warner25 13 hours agorootparentAs an active duty officer approaching 20 years of service myself, I think this is a good comment except for your first couple of statements. I'm very skeptical. I think that an enlisted service member who's good enough, and lucky enough, to learn and do some technical things can easily 3x their pay by working as a contractor after their initial enlistment. But their starting point is low as an E-3, E-4. They can go from making $40k to $120k, sure, especially if they're willing to work in a combat zone. I don't know a single O-4, O-5, O-6 (and I'm talking aviators and IT/cyber officers) who has \"fallen into\" making 3x as a civilian. Their pay in uniform is $160-240k. Many of them seem to opt for civil service jobs as a GS-14, GS-15, which will match their previous pay at best (but be in addition to the $50-90k pension). I hope you're right, but I'm certainly not counting on a $600k job offer when my time comes. reply wharvle 10 hours agorootparentI could have this figured as more of an exponential curve when it’s really as S-curve with your E-8s and your mid-scale officers on roughly the same flattish (but fairly high!) part. Could be I’ve got that bit wrong. I do think the really high paid ones usually require living in one of a few areas (NOVA, say) and some travel, which isn’t in a lot of folks’ life plan for their 40s and 50s. But yeah, I may just have the “graph” wrong. [edit] my more narrow point was just that an ex-officer with enough money to safely invest in a real estate business that might plausibly grow to $11m value under even moderate success, isn’t that weird a notion. reply warner25 9 hours agorootparentTo your last point, I agree. I think that most people (including most people in uniform) don't realize how well it actually pays to be an officer. The corollary is that it's not easy to make much more by transitioning to the civilian world. But yes, one can leave active duty in their early 40s in a really good position - a couple million in savings, the pension, the retiree healthcare, the GI Bill for the kids - and keep making similar money in a new job or launch a new venture (having plenty to fall back on if it fails). Ending up with $11M later in life is not out of the question. I'm still thinking about the exponential curve vs. S-curve. I do think there are a lot of jobs, regardless of someone's final military rank like you said, that will pay low six-figures just for the security clearance and knowledge of certain systems. The few jobs that pay much more - getting into the world of corporate executives, board members, consultants, lobbyists, media personalities, guest speaking - might be accessible to some retired flag officers, but there aren't a lot of retired flag officers to begin with. reply actionfromafar 14 hours agorootparentprevThanks for adding that last one - it's really important to remember. (It's even a movie trope.) reply mschuster91 13 hours agorootparentprev> [EDIT] Oh, another downside: it can be really hard for your spouse to have a career, due to all the moving and often living in places without much economic opportunity. And it may be really hard to get an actual spouse instead of gold diggers of either gender. reply jjackson5324 14 hours agorootparentprevCould you give a range if possible? I've always been curious as to what those salaries are like. Are we talking 300-600k / yr? More? Thank you reply tzs 12 hours agorootparentHere's a page that gives the range for each rank [1] to give an idea what they can make before they leave and start contracting. Click on a particular rank and you'll get a page with a slider for years to see how pay progresses within that range over time. A general makes the most, $221 900 per year. That site also has pay for non-military jobs. Lots of interesting stuff there. For example here's their page for computer science [2]. Average government computer scientists makes $142k. There's a graph showing the distribution and it is very uneven--looks like you can make pretty good pay or pretty terrible pay. 5 times as many computer scientists at the FAA (250) than at the IRS (53). 180 at the FBI. Nearly 1500 computer scientists total. Some surprised me. The government has a little over 1000 archeologists [3]. I guess that explains where all those other thing came from in the warehouse they stored the Ark in! [1] https://www.federalpay.org/military/air-force/ranks [2] https://www.federalpay.org/employees/occupations/computer-sc... [3] https://www.federalpay.org/employees/occupations/archeology reply warner25 12 hours agorootparentThat chart for pay levels at different military ranks is not a good reference. It's only showing \"basic pay.\" I recommend instead playing with this: https://militarypay.defense.gov/calculators/rmc-calculator/ You also have to understand how career progression works. Like it's nonsense to look at the basic pay for an O-5 with less than two years of service, even though there's a number on the chart for that. Outside of exceptionally rare circumstances, an O-5 will have at least 15 years of service. reply jjackson5324 12 hours agorootparentprevVery useful, thank you very much for sharing. reply wharvle 12 hours agorootparentprevI'd say $200k is about the floor, including enlisted, not just officers. Assuming they did 20 years in, they should be able to land at least that much for a boring office job in the US and if they didn't have something notable working for them (good network, what kinds of work they did, that sort of thing). That'll mean an NCO rank of some sort, for enlisted, and probably at least a bachelor's degree (it's strongly encouraged past a certain point) plus the all-important clearance. It goes up from there—way up, at the higher officer's ranks (O-6 and up, say), where whole new tiers of job open up, like high-level lobbying jobs, think tanks, C-suite positions, and even media. Details of your service, who you know, what you're willing to do (travel, say) can start adding fives and even sixes of figures to your comp in a hurry (each!), from there. [EDIT] This is for post-\"retirement\" former military folks in general, not that specific case. You're probably not getting one at all for under $200k/yr unless they just don't want to do one of the kinds of jobs that pay a premium for retired military. And that's, again, way on the low end. [EDIT EDIT] Also if you account for all benefits, the multiplier on military comp may not be as large. These places tend to offer good bennies, too, but not... you know, housing and such. reply warner25 12 hours agorootparentThis comment has a lot more qualifiers than your previous comment, but I still think you're wildly overestimating things. Maybe everybody I know \"just [doesn't] want to do one of the kinds of jobs that pay a premium for retired military\" but it seems like someone I know would at least try the $500k+ job for a while if that were an immediately available option for them. I don't think they're ending up in GS-14 or GS-15 supervisory positions in a big headquarters because the office politics, meetings, and emails are so much fun. reply FireBeyond 13 hours agorootparentprevMy rough guess would be around $400,000. He mentioned \"A thousand dollars a day, including weekends\". Atop this, effectively zero living expenses (family still at home, to be sure), plus a whole host of stipends and per diems. reply HumblyTossed 16 hours agoprevAir Force pays goooood[0]. But seriously: >> but then we're thinking: 'How do we lift the plane? How do we salvage it?'\" Don't. Just leave it. Be happy it was found, but just be respectful and leave it. [0] Yes, I RTFA. reply s_dev 15 hours agoparent>Don't. Just leave it. Be happy it was found, but just be respectful and leave it. I don't see anything inherently disrespectful about rasing the plane from the seabed. I recall people telling James Cameron that he should leave the Titanic alone but he made a great movie and brought the ship back in to the public imagination and didn't disrespect the fact many souls were lost tragically. The ship was deemed not economically viable to salvage but this plane is much much smaller and it may be worth putting in a museum. I'm presuming you are offended on behalf of other people but if not can you elaborate why you find the idea so offensive or disrespectful? reply aeturnum 15 hours agorootparent> I recall people telling James Cameron that he should leave the Titanic alone I think HumblyTossed is suggesting this group do exactly what Cameron did: investigate and document but not remove. If this is Earhart's plane, it's probably also her grave. We generally frown on disturbing the resting places of the dead for commercial purposes. What \"worth\" do you see in placing it in a museum? reply cortesoft 14 hours agorootparent> If this is Earhart's plane, it's probably also her grave. Only because no one knew where she was to bury her properly. I assume you don't say that we shouldn't recover bodies from ANY plane crash; most people would expect bodies to be moved from a plane crash to be buried properly. Why is this one different? The length of time it took to find the crash, I assume. So how long before we can't move the body because it is a grave? 3 months? A year? 10 years? I don't think I would consider this a grave, and recovering the body and burying it properly is not grave robbing. reply aeturnum 14 hours agorootparentThat's super fair - I think recovery and reburial is probably a respectful option. My impression is that our decisions about what to do with human remains often relies on finding a living member of the family to speak about preferences. To go back to the titanic example - I don't know of any families who agitated to have the remains in the ship reburied. One of the ways this situation is different is that there would be no ambiguity about remain identification. > I don't think I would consider this a grave If by not a \"grave\" you mean not an \"intentional burial site\" I agree. But it would be a final resting place and, to the question I was answering (\"why would people object\") that's a thing that people care about being handled with the proper care and consideration. reply cortesoft 13 hours agorootparentYeah, this one is always a tricky topic for me because I have literally zero emotional response to human remains and really don't care at all what is done with my remains or the remains of my loved ones. Once a person is dead, the body is just random matter to me. Trying to guess how other people would feel about things is a complete academic exercise in this case for me. reply bogantech 13 hours agorootparentprevThere are no bodies to recover. The crabs ate them long ago reply jncfhnb 15 hours agorootparentprevWe don’t frown on that at all, nor do we apply any sacred virtue to graves comprised of accidental wrecks. reply executive 15 hours agorootparentprevSo people can know about it Example: https://www.facebook.com/RoyalAviationMuseumofWesternCanada/... reply lmm 12 hours agorootparentprev> What \"worth\" do you see in placing it in a museum? The same value as anything else in a museum? It's historically important, and also really cool, the kind of thing that inspires kids and even adults. (I saw Halifax W1048 at an impressionable age and am very glad I did) reply malermeister 15 hours agorootparentprevWhile I agree with you, there's sarcophagi in museums all over the world. reply markstos 15 hours agorootparentAnd on the tour these days, you are more likely to year \"Yeah, maybe we shouldn't have moved it\". reply HumblyTossed 15 hours agorootparentprevBecause there's little to be gained save morbid curiosity. She did amazing things alive, celebrate that. Putting a wreck in a museum will emphasize her death. Enjoy disagreeing... reply BriggyDwiggs42 15 hours agorootparentThat’s such an odd opinion. Her death is already emphasized whenever the disappearance is mentioned because it’s super interesting. Being concerned that someone who died doing something interesting will have their death emphasized is just super weird imo. Everyone in history is dead. reply HumblyTossed 15 hours agorootparentHer disappearance is emphasized. Her death is not. There is a difference. reply BriggyDwiggs42 15 hours agorootparentBut like, who cares? It kinda just doesn’t seem like a big deal to me. I really don’t get it. reply TylerE 15 hours agorootparentprevThe big problem with Titanic is how deep it is. Very, even by ocean standards. reply tomjakubowski 15 hours agorootparentTitanic (12,500ft) is pretty close to average ocean depth (12,080ft). reply TylerE 15 hours agorootparentThe Pacific is several thousand feet deeper on average than the Atlantic, though. Especially the parts of the Atlantic near the US mainland are quite shallow, with the continental shelf extending outwards well offshore. I guess it would have been more accurate to say that for a ship in the major Atlantic shipping lanes, Titanic is quite deep. Even looking for Titanic was a cover story in the first place. They were actually looking for a sunken Russian sub that was sorta in the area. It was only when they had a bit of left over time at the end of the mission that they ACTUALLY found Titanic, almost accidentally. reply not-my-account 15 hours agoparentprevIf we raise it, it will almost certainly end up in a museum where many many people will see it and connect with Earhart and her story. Earhart would near deification, I’d guess. Plus the inspiration of many young boys and girls to adventure. I don’t quite see where the lack of respect would come from. Or, if it is disrespectful to raise the crash, that disrespect would be repaid many times over with honour in a museum, no? reply digging 15 hours agorootparent> Earhart would near deification, I’d guess. Let's not get hyperbolic. She's already well known. Agreed though that displaying her plane in a museum would seem respectful to me. reply VBprogrammer 14 hours agorootparentI can't imagine what would be recovered of a crash of a light aluminium aircraft after a century at the bottom of the ocean would bear much resemblance to an aircraft. I think it's a great thing to locate and document the discovery but I don't see much to be gained from raising it. reply GolfPopper 9 hours agorootparentPlus, it would be a violation of the Benthic Treaty with the Deep Ones. reply jncfhnb 15 hours agorootparentprevFrankly I think her story is a bit less inspiring without the mystery reply whartung 14 hours agoparentprevIndeed. Just leave it. Document it, photograph it, leave it. There's no reason to lift it, to \"restore it\", etc. It's a wreck. Leave it alone. Go find a \"modern\" Electra, and make that as \"close to Earharts\" plane for your display. But there's no reason to drag this thing up. If this plane were shot down, it would be considered a war grave and left untouched. No reason to not treat this the same. Rejoice if she has, indeed, been found. Peace for the families, finally. reply paulddraper 15 hours agoparentprev> just be respectful IDK if littering wrecks is more respectful than putting them in a museum. reply whycome 15 hours agoparentprevGood use case for Apple Vision Pro, etc. Capture the plane in situ and then have it available to view as an AR model (at home, or in a 'museum' space). Let users 'walk' or 'swim' around the wreck. reply TylerE 15 hours agorootparentNo so sure. Titanic is essentially unique, her two sisters not lasting much longer, the last going to the scrappers in 1935. There were quite a few model 10 Electras built. Over a dozen survive, with several still flying. The Earhart museum already has a 10E, even. reply Cthulhu_ 15 hours agorootparentprevIt looks like there's something similar for the Titanic already, so, not a weird suggestion to make. reply jdawg777 16 hours agoparentprevSalvaging the plane could give interesting clues about what led to the crash. reply poulsbohemian 14 hours agorootparentEvery time Earhart is mentioned here, someone pops up with authoritative info that she and Noonan were not well prepared / equipment was faulty / more Hollywood salesmanship than navigation skills / that island was always going to be a stretch on gas... point being that while in the collective memory they are heroes, my lay understanding is that scholarship on the subject has already determined there were major problems with their flight plan reply filleduchaos 13 hours agorootparentprevIt's been several decades in saltwater at unbelievable pressure - I somehow doubt that. reply HumblyTossed 15 hours agorootparentprevThat's just morbid curiosity... reply quatrefoil 15 hours agorootparentI find it odd to see geeks, on a site essentially devoted to idle curiosity, speaking out against idle curiosity. There is value in showing respect for the dead, mostly for the benefit of those near death and the grieving families, but the arguments made here seem weird to me. Do we seal homes and turn them into tombs when a person dies inside? Do we leave car wrecks on the side of the road? reply genocidicbunny 10 hours agorootparent> Do we leave car wrecks on the side of the road? If they get lost for 80+ years, yeah, we do. There's a state park not far from me that has a trail that uses an old highway's right of way for a bit. The highway is long long gone, but if you look down from the trail into the gully nearby, you can still make out a few rusting hulks of cars from the 1920's and 1930's that went off the road and were just left there. reply alpaca128 14 hours agorootparentprevCuriosity is great but that doesn't mean it should always take priority. Would it be nice to know how that plane crashed? Sure, but we also figured out how the Titanic sank without lifting it out of the water. > Do we leave car wrecks on the side of the road? No, but we also don't need 80 years to find car wrecks on the side of the road, and there are various reasons to remove them. This comparison is ridiculous. reply lp0_on_fire 12 hours agorootparentprevI think the \"risks\" of disturbing a grave site are greater than any \"rewards\" for doing it. A home in which someone died can be used again by another person after proper cleaning and what not. Wrecks on the side of the road are usually removed because they are a danger to other motorists. In the case of Earhart's plane the air frame and navigation systems are 75ish years out of date so it's not like there is a pressing need to solve a potential safety issue. IMO there is nothing of value to be gained besides \"solving the mystery\" and perhaps gathering a few personal artifacts that could be displayed in a museum or returned to the families. reply quatrefoil 3 hours agorootparentAnd nothing of practical consequence is learned from exploring the tombs in Egypt. We might be able to decipher some tidbits about everyday life thousands of years ago, but one would be hard-pressed to explain what tangible benefits it offers, beyond \"well, it's a part of our heritage / it's cool to know\". Just like many other human activities, it's glorified idle curiosity. reply deadbabe 16 hours agoparentprevIt can be raised respectfully. reply yardstick 16 hours agorootparentProbably can. Should it though? Like the Titanic, maybe we just let it rest in peace? Send a sub down to confirm what it is. Then maybe update a few of the monuments to her to indicate her final resting place has been found… My only concern would be looters. If it is quite feasible to access “easily” then yeah maybe we should recover it just to protect against looters and pillagers. reply Kon-Peki 15 hours agorootparentIt appears that there would be significant questions about ownership and whether maritime salvage laws apply [1]. Remember that Earhart was a faculty member at Purdue University, the aircraft was owned by the university, and it was filled with student experiments. They're going to claim that it still belongs to them. You're going to spend millions to recover it and then immediately get hit with a lawsuit demanding you turn it over to them. By an entity that will have no problem collecting enough donations to fund the best lawyers for as long as it takes. [1] https://www.perthilj.com/blog/2019/2/19/aircraft-salvage-in-... reply reaperman 14 hours agorootparentSeems like a situation where it would be best to get all parties with any claim that wouldn’t be summarily dismissed to agree on details of what should happen with the salvage before committing to recovering it. reply MeImCounting 15 hours agorootparentprevAside from the practicality of the two, the Earheart wreck seems much more like something our society would want to honor. An explorer and adventurer who's legacy and life pushed equality forward for women across the west vs a failed engineering project which was mostly a status symbol for rich people. reply not-my-account 15 hours agorootparentprevThe titanic is massssive, making raising it a much more difficult challenge compared to raising a single aircraft. reply notbeuller 14 hours agorootparentThere was a rather silly 1976 thriller “Raise the Titanic!” predating it’s actual discovery by about 10 years. They made a movie from it too in 1980. Based on a premise that there was something valuable the government wanted on board) reply RegBarclay 14 hours agorootparentThen Arthur Clarke wrote \"The Ghost from the Grand Banks\" in 1990 (after Ballard discovered it was in pieces) with a slightly more plausible approach to raising the bow. The wreck has deteriorated now to the point where raising it would probably destroy more than what would be recovered. reply paulddraper 15 hours agorootparentprevIf it were a feasible to raise the Titanic as a small aircraft, it would have been done. reply stronglikedan 15 hours agorootparentprev> Should it though? Yes, it's a plane not a person. We can learn from it, and Earhart won't care. reply yardstick 4 hours agorootparentWhat could we learn from taking it out of the water vs leaving it in its grave? There will be no FAA safety notices (or whatever they are called) based on the findings. reply deadbabe 15 hours agorootparentprevYes absolutely, there’s no reason for the plane to rest at the bottom of an ocean forever. The bottom of the ocean is the opposite of rest, it’s hell. reply Beldin 14 hours agorootparentTo you, there's no reason. To others, there is. That by itself shows the need for discussion (where either side ought to listen to the other). With respect to the argument of putting the plane rests in a museum: I do not see how that would be better than being able to visit the actual place where the crash occurred, with the plane still somewhere below the waves. reply deadbabe 13 hours agorootparentThe needs of the many outweigh the needs of the few. reply dclowd9901 14 hours agorootparentprevLooters are exactly why we should raise it. Protect it from unscrupulous people. reply cozzyd 15 hours agorootparentprevYeah, but then amateur submariners can't offer expensive tours in their homebuilt craft. reply kylebenzle 16 hours agoprevThere would be hundreds of WWII planes in that area that would be indifferentiable from what Mr. Tony Romeo says he's looking for. More likey he's just looking to fund his retirement hobby. reply OJFord 16 hours agoparentThat's what I thought, not even WW2 specifically, but (and I'm no expert) the image supposedly showing 'the distinctive shape of the fuselage, tail, and wings' is not that compelling? To paraphrase Jaws: it certainly looks like an aircraft, but not the aircraft. It could be! Wings broken off/up a bit. But is it that likely? It could also be some other aircraft. Seems a bit sensationalist until they go back for a better look. reply wharvle 16 hours agoparentprevMaybe. Looks to be a bit (... by Pacific standards, so, hundreds of miles) off the Easternmost part of the Pacific that would have seen lots of air traffic in the war, which should leave it relatively clear, but I may be wrong. [EDIT] I still doubt it's the plane, but this area may be far less-cluttered with 1930s-40s aircraft wreckage than others, is all I mean. reply ianburrell 15 hours agorootparentHowland island was pretty far from the fighting in WW2. The Phoenix Islands were not used by the military. All the fighting was to the west or the north (Hawaii). reply zelos 16 hours agoprevSurely to search a plane have to have already found it? reply paulddraper 16 hours agoparentThe article title currently says \"for\" reply Arainach 16 hours agoparentprevNonsense. For instance, I do a search every morning for \"tomorrow's winning lottery numbers\" on several search engines and haven't found them yet. reply echelon 16 hours agorootparentTangent - wouldn't it be absolutely fantastical (and perhaps scary) if we could break the deterministic universe? Time travel movies about sending people and things never made sense to me. To receive information from the future alone would be all you needed. Not only could you get stock prices or a lead on innovation, but you could also perhaps perform instantaneous, constant time computation of any function. And you'd have perfect foresight of anyone trying to disrupt you and take away your advantage. But then maybe the future sends instructions for a machine that destroys you. Or it could be even simpler - it knows where to send you to die. So maybe you can't trust what it tells you at all. It has all the time to plan around your choices, and if you're still listening in any capacity it can coerce you to do its bidding. Wild science fiction ideas searching for novelization, I suppose. reply Sunspark 13 hours agorootparentThe reason time travel movies don't make sense, while absolutely lots of fun to watch, is because the Sun is travelling through space. If you were to hypothetically build a time machine and set it for any given year, you would materialize in a hard vacuum. So, you need more than a time machine, you also need a spacecraft to be able to fly to where the Earth's previous position is. Chemical propulsion won't do it, you need something with significant velocity and energy. Unless of course, you are able to enter specific astronomical coordinates to appear in and match velocities. Would suck if you got the coordinates right, but were off on the velocity by 1000 km/hr. reply asystole 15 hours agorootparentprevThe 2004 indie film Primer is a very good take on this. reply mikestew 14 hours agorootparentprevReminds me of DirecTv’s Black Sunday[0]: the future machine gives you all the pieces you need to build the machine that, as the final puzzle piece is put into place, explodes and kills you. [0] https://blog.codinghorror.com/revisiting-the-black-sunday-ha... reply p1mrx 12 hours agorootparentprev> you could also perhaps perform instantaneous, constant time computation of any function See the first section of HPMOR chapter 17: https://hpmor.com/chapter/17 reply dllthomas 13 hours agorootparentprevNot quite the same thing, but https://qntm.org/causal is related. reply 1024core 15 hours agorootparentprevBruce Willis starred in one such movie, \"Looper\". reply lb1lf 15 hours agorootparentArguably in two - 12 Monkeys, too... reply lainga 16 hours agorootparentprevfor reply BarbaryCoast 3 hours agoprev [–] If he's \"searching Earhart's plane\", doesn't that imply that he's found it? Or is he searching a plane he _thinks_ was Earhart's, trying to find confirmation from the contents of it? On the other hand, if he's \"searching FOR Earhart's plane\", this contradiction resolves itself. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Former US Air Force officer, Tony Romeo, claims to have found what may be Amelia Earhart's missing plane using sonar technology on an unmanned submersible.",
      "The aircraft-shaped object was detected on the floor of the Pacific Ocean, about 100 miles from where Earhart's plane is believed to have crashed.",
      "Further investigations and clearer images are required for concrete confirmation, but if confirmed, this discovery could potentially solve the long-standing mystery of Earhart's disappearance."
    ],
    "commentSummary": [
      "There is ongoing discussion about the possible discovery of Amelia Earhart's missing plane.",
      "There is debate about whether to salvage the plane or leave it as a historical artifact.",
      "The financial benefits of a military career are being discussed.",
      "Various theories and opinions on historical mysteries are being shared."
    ],
    "points": 200,
    "commentCount": 156,
    "retryCount": 0,
    "time": 1706548834
  },
  {
    "id": 39176877,
    "title": "Why Walmart Pays Truck Drivers Six Figures",
    "originLink": "https://www.freightwaves.com/news/how-walmart-uses-trucking-to-dominate-american-retail",
    "originBody": "Home/News/Why Walmart pays its truck drivers 6 figures NewsTop StoriesTrucking Why Walmart pays its truck drivers 6 figures Walmart's stores are bare-bones — but their truckers' pay stubs aren't Rachel Premack · Tuesday, January 23, 2024 Walmart associates have the chance to become a truck driver for the retailer’s elite fleet. (Photo: Jim Allen/FreightWaves) One Walmart truck driver says he has 15 years left of working, and he intends to spend them hauling loads for Walmart. “Barring a lottery win or marrying a sugar mama, I don’t see myself going anywhere,” said the Texas-based driver, who asked not to have his name included as he is not authorized to speak on behalf of the company. Such loyalty to a single company is unusual in trucking, an industry notorious for massive turnover. And the Texas trucker isn’t alone in his dedication to Walmart. One of the best jobs you can get in trucking is at Walmart. The uber-retailer says truck drivers can make up to $110,000 in their first year at the company. That’s twice the nationwide median pay of a truck driver, and certainly above the $17.50 an hour that the average Walmart associate earns. Home time, paid vacation and good health insurance are also guaranteed for Walmart company drivers. These offerings are elusive in the trucking world. It’s not out of the goodness of Walmart’s corporate heart that it pays truck drivers a truckload. Rather, truckers are key to Walmart’s retail dominance — and they have been from the start. Without a highly engaged trucking workforce, it’s not likely that the company would have flourished in the way it has. The Fortune 1 company prioritized supply chain long before it became a buzzword. “At Walmart, we believe in offering our drivers a competitive compensation package to attract the best drivers in the industry,” a Walmart spokesperson told FreightWaves in an emailed statement. Walmart company drivers are historically some of the most elite in the industry. (Photo: Jim Allen/FreightWaves) Walmart employs some 14,000 drivers, which makes it comparable to some of the largest for-hire fleets in the U.S. It’s added 5,800 drivers to the company in the past five years alone. Recently, Walmart has shifted some of its strategies around recruiting those new drivers. In 2018, Walmart changed its truck driving recruitment program to allow more drivers to pass its program. A senior vice president at Walmart told Yahoo! Finance at the time it was because of a “shortage” of truck drivers. (Those who study the trucking industry dispute that such a shortage exists, concluding that drivers leave the industry for jobs with better pay and hours.) Last year, Walmart announced another key pivot. The company said in January 2023, building on a pilot it launched the year before, that it would allow Walmart associates living in participating locations to apply to a 12-week CDL program. For perhaps the first time, a Walmart company driver doesn’t need years of experience to get behind the wheel of its branded 18-wheelers. It may be that Walmart’s tack on trucking is changing. Operating in the boondocks forced Walmart to become a distribution whiz Decades before Walmart became the biggest company in the world — raking in $611 billion in revenue last year — its leadership team couldn’t find anyone to haul freight to its first stores. Walmart operated mostly in the boondocks, far from where most trucking companies wanted to go. “We couldn’t find anybody who wanted to run their trucks 60 or 70 miles out of the way into these little towns where we were operating,” Don Sonderquist, an early Walmart executive, explained in an interview published in 1992. “We were totally ignored by the distributors and the jobbers. That’s not only how we came to build our own distribution system, it’s also how we got used to beating the heck out of everybody on prices.” This forced founder Sam Walton to build up his own network of suppliers, too. Walmart elected to work directly with the brands it sells in-store, which still allows the company unusual control over the minutiae of the products of some very large companies, according to journalist Charles Fishman, the author of the 2006 book “The Wal-Mart Effect.” Walmart employed 2.1 million associates, per its 2022 financial statement. It’s the largest private employer in the world. (Photo: Jim Allen/FreightWaves) This hyperfocus on supply chain and distribution shaped key decisions from the top to the bottom of Walmart’s operations. That’s according to two people who have closely studied Walmart: Fishman and historian Nelson Lichtenstein, author of the 2006 book “Wal-Mart: The Face of Twenty-First-Century Capitalism” and professor at the University of California, Santa Barbara. When Walmart sought to open a new store, Fishman and Lichtenstein explained, it built the distribution center. Then, it built stores within a one-day drive of that distribution center. This might seem like an obvious strategy in 2024, but it was somewhat revolutionary in the mid-20th century. Kmart, for example, targeted the same blue-collar Americans that Walmart did. However, Kmart would simply plop stores into suburbs that had plenty of customers. Distribution was an afterthought. “Walmart has become the largest company in human history by doing something that was already being done, better than anyone else did it,” Fishman said. “Logistics and transportation is one of the things that made Walmart, Walmart, and allowed them to outcompete.” So … why is Walmart so obsessed with its truckers? Kroger, Home Depot, Target and the like all operate huge supply chains and obviously manage to get their shelves robustly stocked — without the front-and-center obsession on supply chain. Walmart’s supply chain, though, is different for a few key reasons. Professor Brian Gibson, executive director of the Center for Supply Chain Innovation at Auburn University, laid it out: Walmart is just, well, bigger than any other retailer. It has 4,616 stores, compared to fewer than 2,000 for Target, around 2,200 for Home Depot and 2,750 for Kroger. Walmart also has a larger average square footage per store, too. Walmart has a mix of grocery and general merchandise, which adds complexity to its trucking network. As a result of its large, private fleet, Walmart has unusual clout among equipment manufacturers and other trucking service providers. It’s the type of status that’s usually reserved for companies that only do trucking. “Walmart’s mission is to save people money so they can live better,” the Walmart spokesperson said in an emailed statement. “Managing our own distribution and trucking networks helps us better serve our customer and manage costs.” Walmart employs around 14,000 truck drivers. The retailer hired nearly 6,000 in the last five years. (Photo: Jim Allen/FreightWaves) The importance of distribution is perhaps incredibly obvious. If stuff is not on the shelves, customers aren’t going to be buying that stuff. Customers would ultimately buy less during that visit and, if they get fed up by a consistent lack of stuff, eventually not at all. The stuff has to be moved safely and on time across the country. If paying top dollar makes that happen, then it’s sensible for Walmart to agree to do that. “They wanted to pay them good money because it was the absolute core of their, of their business — to get this stuff from the distribution center to the store at precisely the right time with no screw-ups,” Lichtenstein said. “That was crucial.” Paying truck drivers top dollar also makes sense because Walmart doesn’t employ that many of them. The company has about 14,000 truck drivers and 1.6 million associates. Each of those truck drivers holds a lot of power over the shopping experience of a Walmart store. “One associate here or there can have a positive impact, but it’s not going to change the economics of the store,” Fishman said. “A truck driver is going to really matter. They have an outsized impact on the way the company runs.” Paying six figures to 14,000 employees may seem reasonable enough for Walmart. “It’s not even 1% of their staff,” Fishman said. Walmart is remodeling some of its trucker policies Walmart is now changing its truck driver hiring policies. Until 2022, the company required 30 months of driver experience before one could be considered for the company driver role. That year, the company began piloting a program that allowed Walmart associates to go to a 12-week driver training program and become fleet drivers. Walmart expanded the program nationwide. (Outside applicants still need 30 months of training, and not every associate who applies is admitted to the program.) “We started the Associate-to-Driver program because we wanted to tap into our own talent pool of incredible associates and give them opportunity to develop their career,” the Walmart spokesperson said in a written statement. “It’s been a great opportunity for our associates to continue to grow their careers without having to leave the company.” The spokesperson said the company requires all trainees to pass the same skills assessment as external hires. Then, they’re partnered with a mentor for six weeks of continued training. It’s a sensible move for Walmart to train from within; its current CEO, Doug McMillon, started as an hourly associate. “I think it’s a recognition that [you value] your own employees better than somebody walking in off the street,” GIbson said. The truck driver shortage debate appears… again Some Walmart drivers aren’t delighted. The Texas-based Walmart truck driver, who joined the company two years ago, said the retailer could attract more drivers by raising its pay. “Raise the driver’s pay and you’ll retain and attract [experienced drivers],” he said. Another Texas-based truck driver, who joined Walmart seven years ago, said he fears it’s a sign that Walmart is approaching trucking in the same way as large for-hire fleets, which see typical turnover rates around 94%. “Back in the day, you used to need a decade before you’re even looked at to get on with Walmart,” he said. (The driver asked not to have his name included as he is not authorized to speak on behalf of the company.) Both complaints get at the heart of an ongoing debate in the trucking industry: the so-called truck driver shortage. Trucking employers maintain that they’re unable to hire drivers due to a persistent shortage — caused largely by demographic issues and the lifestyle of trucking. However, researchers (and truck drivers themselves) disagree. Studies suggest the massive turnover seen by large fleets keeps them scrambling to hire new workers; one March 2019 study published by the Bureau of Labor Statistics concluded that “price signals” would lead to a more stable trucking workforce. A company, like Walmart, that pays six figures and offers good benefits should not struggle with turnover. On the other hand, Walmart needs to hire more drivers as the retailer expands operations and current truckers retire. Walmart has hired nearly 6,000 new drivers in the past five years. Experts believe Walmart wants more control over training its truckers From the perspective of these supply chain experts, it doesn’t seem like the associate-to-driver program is necessarily a way to cut costs. Gibson said Walmart has been “very aggressive” in recruiting drivers in recent years. It may have simply tapped out of the current supply of drivers. “I think this is just the latest in the evolution of the hiring process for Walmart,” Gibson said. “Going internal has been proven to be a good strategy by other organizations.” Many trucking fleets hire new drivers to save money on wages, but that might not be Walmart’s tactic here. (Photo: Jim Allen/FreightWaves) What’s more, the associate-to-driver program could be a way to better mold the Walmart truck driver. “You take somebody who’s been with another company, they’ve developed habits, they’ve developed styles, they know certain systems – for the good and the bad,” Gibson said. “If they’ve developed any bad habits over time, you can train your new drivers the way you want, the way you need on your systems and try to focus on the skills, capabilities and safety issues that are directly of importance to your organization.” Fishman agreed. An associate-turned-driver might not bring years of trucking experience, but they certainly get Walmart. “It’s possible in this wave of hiring that [outside trucking hires] are diluting this Walmart culture,” Fishman said. “Truck drivers are famously independent.” What do you think of Walmart’s trucking fleet? Email rpremack@freightwaves.com with your thoughts. And don’t forget to subscribe to MODES. Trucking",
    "commentLink": "https://news.ycombinator.com/item?id=39176877",
    "commentBody": "Why Walmart pays its truck drivers 6 figures (freightwaves.com)193 points by Gigamouse 19 hours agohidepastfavorite308 comments subsubzero 14 hours agoThere is a narrative that Wal-mart pays low wages, I think its quite apt as most positions do pay quite low, I worked at Wal Mart while in college and saw it first hand. A few things to note, certain positions do pay well, store managers for one, I know for a fact that over 20 years ago certain store managers would see over $300k in yearly compensation. In addition once I got into tech after college I worked with a few walmart.com ex-employees, and these employees stated that part of their bonus was tied to a percentage of sales walmart.com made and they said the bonus pay was wildly lucrative. So Wal Mart does pay well in fact if you are in the right position. Considering keeping your shelves(and online distribution centers) stocked is the lifeblood of the company it behooves them to have a healthy circulatory system and not have drivers who do not deliver on time or get into accidents. reply swozey 14 hours agoparentStore Managers are a very different type of pay structure though. They're more like someone from Corporate vs Retail. They get paid massive bonuses based on numbers their stores hit. I worked at a few Home Depots and IIRC the store managers were clearing 150-250 while the assistant managers I think were in the 60-80k range. Then us floor staff were 9-15, maybe 20. This was 2008ish. I remember the biggest HD in my state or somewhere the manager was getting I think 500k-1mil+. You can infer it from the numbers the store does and their bonus percentage whatever that was. My mom was an HR director there so corporate as well and she made somwhere in 130-200+ not sure exact. reply filmgirlcw 13 hours agorootparent+1 to everything you say here. The store manager track is very much a Corporate vs Retail (hourly) thing at least at the big chains. I worked retail in high school and college in the same era as you mentioned and even 20 years ago, a store manager at a Best Buy or a Costco was clearing $250k or more, at least in the wealthier-Atlanta area where I lived (I know a lot of HD employees being from Atlanta and my knowledge of corporate and sales manager salaries at HD aligns with yours too), as you said, based in part on bonuses related to store revenue (the Best Buy stores I worked at when I was 16 - 22 were two of the top-ranked stores in the southeast region by sales, with some departments being among the top 10 in the whole company). For some of the chains, I think that has decreased as in-store sales have shifted but places like Costco notoriously pay their store managers extremely well. Best Buy desperately wanted me to quit college and be fast-tracked into store or sales management (I would spend my summers as a manager of a department and they agreed to keep me at that elevated hourly wage even when I was part-time during the school year), trying to lure me with tales of just how much money the top of the foodchain people make, but I never took it seriously. Given the ebbs and flows of the economy (I graduated from college in 2008) and the collapse of the retail market from its early 2000s high in favor of ecommerce/Amazon, that was almost certainly the right decision -- but it was an instructive lesson for me at a young age about the very disparate differences in compensation between the hourly workers and the people at the top of those stores. reply eastbound 12 hours agorootparentThe Freakinomics book has a great chapter on how the police discovered accounting books of a NYC drug cartel, and its pay scale from the street watcher to the top lord was the same as at… McDonnalds (just the ratio, not the amounts). 20 years later, I reckon there was a lot of storytelling here advantaging the arguments of the right-wing, but I’d still be curious what are the true figures. reply AtlasBarfed 10 hours agorootparentI thought freakonomics also showed most foot soldiers made less than minimum wage. reply FirmwareBurner 13 hours agorootparentprev>They get paid massive bonuses based on numbers their stores hit. Isn't that ... good? The demographic in every region or neighborhood is different and has different tastes, incomes and budgets, so it's your job as store manager to identify the needs of that demographic, and order merchandise and organize it on shelves, run sales, etc. in order to move as much of it as possible to maximize sales and profits. Sort of like targeted advertising but IRL and less privacy invasive. Paying based on this seems like the only way to incentivize this performance. reply LegitShady 13 hours agorootparentIn university before co-op, many years ago, I worked for home depot part time. the line employee profit share was really bad. the management profit share was better. They understaffed my department for long periods of time to make their numbers look better. They were making $15k/day in profit on my department but they wouldn't schedule a second employee so that there was someone there to help people when person #1 was on the saw, on the forklift, or covering lunch for the next department over too. I quit mid june after I realized they weren't hiring/scheduling more staff for the summer even though every day was very busy, constantly running between helping customers and taking care of other things. There are a lot of ways to make profit and show good metrics that aren't good for line employees. reply FirmwareBurner 13 hours agorootparent>There are a lot of ways to make profit and show good metrics that aren't good for line employees. Sure, but this is in line with every major (European) tech company I ever worked at. Pay employees peanuts and pay senior mangers amazing, therefore you were incentivized not to do well as an IC, but to put up with bullshit and fight your way to climb the corporate ladder to management, and then exploit the other poor schmucks who come below you with the same perverse incentive structure. Yes this leads to a huge turnover, unhappy workers, and to a lot of arguments, back-stabbings and fights. But their stocks were always rock solid and managers were always wealthy so I guess they saw that culture as a net win. It seems to be the M.O. with ever major company everywhere, I think it's called the \"GE model\". Paying ICs at the bottom great wages and incentivizing them with stock, is mostly unique to US/SV and new-age software start-ups from the Google/Facebook era, but rare everywhere else. reply rightbyte 12 hours agorootparentYe it is strange that such anti-intuitive management style pays off. I have no clue why. I got this feeling that what's most important for a Big Bad Company is to be able to get well of with management of other Big Bad Companies to be able to land contracts. reply ethbr1 3 hours agorootparent> I have no clue why. Because it's cheaper to compensate one manager 10x average (and get the best candidate) than pay more to hire an entire team of more skilled ICs. FAANG gets away with it because they amortize their engineering costs over an extremely customer base. Which also multiplies the value in output quality difference. Most companies are not building EC2 or BigTable. And so... it makes more sense to hire minimal cost engineers, then corral them with a single high-cost manager. reply FirmwareBurner 12 hours agorootparentprev>Ye it is strange that such anti-intuitive management style pays off. I have no clue why. It works because it prays and exploits several flaws in our society and in human psychology like greed, wealth inequality, conformity of the masses to cultural norms, fear of missing out/of being left out, and the crabs in the bucket mentality, which leads to a rat race to the bottom form which it's difficult to check yourself out as an individual if the majority does it and is heavily vested in it. This isn't something big bad corporations have created, it s just something that has existed for hundreds of years, they're just exploiting. The same mechanism and behaviors were in place during the communist rule in my home country 35+ years ago, when people were fighting to climb up the party ranks, because that's what gave you the best QoL. Now it's climbing the corporate ranks. reply tpm 11 hours agorootparentprevhttps://www.ribbonfarm.com/2009/10/07/the-gervais-principle-... reply acchow 10 hours agorootparentprevAren't stronger labor union protections supposed to lead to higher wages? How is it that European tech workers can't bargain for higher wages? reply FirmwareBurner 9 hours agorootparent>Aren't stronger labor union protections supposed to lead to higher wages? Only for those at the lower end, usually at the expense of growth and opportunities for those at the higher end, but since those are fewer in number their votes are less important. >How is it that European tech workers can't bargain for higher wages? Because investors there don't have as much money, and those who do don't risk it starting tech companies. And also because Europe is not a single country with one law and one language, but 27+ with different laws and different languages, means scaling a business beyond the original country is difficult and expensive. It's the US who's the exception in tech success, not EU for failing to match it. reply AnthonyMouse 8 hours agorootparentprev> How is it that European tech workers can't bargain for higher wages? The leverage a company has and the leverage a union has both come from the same place: A lack of alternatives. But then that's the problem. If you have a competitive industry, the union can't demand much because the company's margins will be slim and they won't be able to give the union much without raising prices, losing customers to competitors and going out of business. This is actually fine because in that case workers still get the money in the form of lower consumer prices, but then it's not the union which is getting you the benefit. Conversely, if the industry isn't competitive then the company will be a huge conglomerate with a lot of leverage against workers/consumers. They'll be big enough to exert political influence in the government and also big enough for the company's union to become infested with politics. In particular, the union knows where its leverage comes from, and it's workers with skills in high demand, so any union that doesn't keep them happy and keep them from moving to another company/industry is going to have to contend with a pissed off conglomerate with a lot of political influence. So the money goes to the people with high demand skills (or political connections) and the rank and file still get the scraps, but now it's even worse because lack of competition and high levels of organizational inefficiency means high prices when they go to buy things. reply danielscrubs 1 hour agorootparentprevI haven’t seen any managers that worked themselves up from the bottom, in eu. A manager starts with an economic background and step in from the side. That coupled with free university means that programmers is not something that you look up too. reply EnigmaFlare 5 hours agorootparentprevMaybe that was fine? It's possible the customers were prepared to wait and would buy regardless, in which case why have extra staff to cover short gaps? It sounds like your personal complaint is that you had to spend your whole time at work working instead of standing around? reply klyrs 11 hours agorootparentprev> Isn't that ... good? For whom? reply FirmwareBurner 11 hours agorootparentFor everyone? reply Shared404 10 hours agorootparentFWIW I've seen managers burn down stores by pushing their employees to far in chase of those numbers. Perhaps it can work situationally, but I've not seen it anywhere I or any of my friends have worked. reply FirmwareBurner 9 hours agorootparentManagers burning out people chasing KPIs and promotions of their own careers, isn't anything new or exclusive to Walmart. I've seen it in every tech company I've ever worked at. reply olddustytrail 9 hours agorootparentThen why on earth did you suggest it was good for everyone? reply FirmwareBurner 9 hours agorootparentSlipped my mind. reply golergka 7 hours agorootparentprevIf the manager burns the store down, wouldn't his KPI plummet and he get the boot eventually? reply achenet 54 minutes agorootparenteventually... the delay between manager doing stupid action chasing KPI and store burning down may be large enough that the average manager is likely to burn the store down. If your goal is long-term success, those incentives might be misaligned. reply klyrs 9 hours agorootparentprevWhat do you mean by \"everyone?\" Do you include the floor workers who are paid so little that they need food stamps? How are they benefitting? reply FirmwareBurner 8 hours agorootparentWould they be better off if the managers were worse paid? reply klyrs 8 hours agorootparentI can't help but notice that you shifted the goalpost subtly from \"better for everyone\" to \"not worse for everyone\" there. If the pie was shared more fairly, then yes they would be better off and I daresay that their incentives would be better aligned with getting those numbers up. And yes, that would mean managers not pulling down millions. reply anjel 4 hours agorootparentprevRadio shack store managers easily hit 60 (Strip Malls) to over 100k a year (Shopping malls) in the late 1970s, again thanks to performance bonuses so this is nothing new. Same long hours as retail, with way more reason to get out of bed each day. (60k in 1978 would be 280k in 2023 thanks to inflation) reply raydev 12 hours agorootparentprev> while the assistant managers I think were in the 60-80k range. Don't know how it was at Home Depot, and my experience at Walmart was ~20 years ago, but asst managers were in-building 10-12 hours/day, usually there 6 days a week for various reasons, and at that time were only making $50k salary to start. Unless there were other bonuses I was unaware of, it was absolutely not worth it. I wasn't making a living wage as an hourly associate, but they were super strict about me working beyond 40 hours, and when they \"requested\" beyond that (it was rare but never truly optional if I wanted to keep climbing) I was well compensated because they were terrified of lawsuits by that point. Small blessings. reply swozey 11 hours agorootparentYeah, asst managers lived a brutal life walking back and forth through the stores on walkie talkies all day long. I have a feeling its a lot of peoples first salaried jobs, they move up from front line to asst mgr so willing to take more abuse to not make $15/hr anymore. I was there at many points. I did that leaving HD, 12.50 to 40k in ops. I can picture the faces of all of my managers, ASMs. I can't picture the face of either of my store managers. I'm not sure I actually ever saw them. They definitely weren't in the store often. I loved my first store and HATED my second store. If you're full-time at HD you have no control of your hours at all. They'd schedule me to close 11pm-12am then open at 5am the next day at the second store. I just quit one day. I constantly got sick from getting no sleep. If you're part-time there you have 100% control of your hours. Tons of people in college who worked 2-4 hours a day. It was miserable watching them come and go. reply subsubzero 12 hours agorootparentprevYou are 100% spot on. Shrink was a huge factor for bonus payout(huge pay lever for mgmt in stores) as if you lived in a area that had high theft you never hit your bonus(shrink is the delta of actual vs. recorded inventory). That and the bonus also factored in sales volume, but shrink was a giant component of it. I remember when a few employees from a rural store came to help out they mentioned they had never not gotten a bonus, whereas my store - a urban store never received a bonus due to excessive shrink. But what I was getting at is there are positions that pay well if you hit your numbers, as a sales associate/dept. manager your pay was capped to really low numbers no matter what happened. reply m463 12 hours agorootparentprevHome Depot stock has been very lucrative. Take a look at the historical values. reply missedthecue 12 hours agorootparentHome Depot is the best performing stock in the S&P500 going back to 1981 reply throwaway2037 9 hours agorootparentLiterally, I Googled for: best performing stock in the S&P500 going back to 1981 The best I could find: Home Depot (HD) had the highest return in 1981 by a US stock in the S&P 500 (GSPC), returning 66.9%. [1] I think you are mistaken: That was the best single year return in 1981. It is not easy to find a single stock with the highest return since 1981. The best I could find was 20 years: Monster (energy drinks) [2] Other research shows the highest single year return on a stock ever was Qualcomm: +2620% in 1999 [3] [1] https://www.statmuse.com/money/ask/best-performing-stocks-in... [2] https://www.cnbc.com/2023/11/08/sp-500-stock-has-the-highest... [3] https://www.visualcapitalist.com/top-sp-500-stocks-by-annual... reply missedthecue 2 hours agorootparentHome depot has returned over a million percent since 1981. Compare that to Apple who is up about 150,000% since IPO and less since their S&P inclusion. reply hunter-gatherer 11 hours agorootparentprevI worked at Lowe's circa 2013 and this was my experience as well. One thing to note is that if you're ambitious and not incompetent, you can get pretty far career wise in retail. That was the impression I got anyways. reply swozey 11 hours agorootparentI'm the op that worked at HD, I got out at like 20, but I have an ex who's climbing the ladder at Abercrombie (or whoever owns it) for the last 5-6 years and it seem so miserable. She's moving every 1-3 years because they transfer her to a new store, and because she's running the higher profit stores they send her to trash stores to try and fix them. She has no practically no vacation, has to fill in staff shortages, works every weekend. I think she's around 30-40k. So she's doing all this to eventually (6 years in now) break into the corporate world, where she'll then have to move 1000 miles away to I think Georgia. To... probably make 60k. I'm really curious where she'll be in 5-10 years, she's managed so many stores I feel like she'd go regional manager route or something. OR training would keep her at corporate instead. reply wahnfrieden 7 hours agorootparentDo you think she’s meeting the bar of ambitious and not incompetent? If so then why isn’t she reaping the rewards? Are the people working under her unambitious and incompetent? reply blackgirldev 10 hours agorootparentprevI worked retail in the early 90s at a huge chain of appliance/electronic stores. The managers were paid very well. If you survived the politics of being moved from store to store, never knowing your schedule, and blatant, miserable sexual harassment (\"hey let's go out for drinks and discuss how I might let you sell large appliances instead of small electronics\"...oh wait...\"you didn't go to my apartment...we have this opening an hour away that you have to take with no advancement opportunity\". Okay, assuming the sexual harassment is gone/obsoleted by solid company policy, there's the \"chasm\" from peon to management that you can only aspire to if you never make a mistake or make them quietly. Or never leave the job to get a degree. Or get lucky somehow. Those positions at management level above peon required a lot of sweat equity and risk on the part of the worker bee. If it doesn't pan out, well, that's what...3-15 years of your real actual life flushed down the toilet. I'm with GenX. Screw that noise. reply throwaway2037 9 hours agorootparentThank you to share your experience. While I am not experienced in this area, I feel too many other posts are overlooking the _path_ to store manager. It is a brutal grind, and getting to the top is so much luck. reply packetlost 14 hours agoparentprevWalmart is somewhat notorious in the OTR trucking industry for having high wages, but very high barrier to entry. You need a near perfect driving record among other things. OTR trucking overall pays pretty well, but it's an extremely physically and emotionally taxing job that doesn't really get the credit it deserves. How would you feel if you were \"home\" for a weekend every 2 weeks and living in 20sqft the rest of the time? reply jzb 12 hours agorootparentIn college I had a friend whose father was an OTR trucker. Absolutely destroyed his back. He had back problems when I first met the family and a few years later he was on medical leave and unable to work, and he was maybe in his late 50s. He wasn't driving for Walmart of making that kind of money then, so I hope he was able to move into a different career that didn't do further damage. It's not a job I'd want to do. Ergonomics aside, I find driving to be very unpleasant in large doses. Having to put up with traffic, shitty drivers, weather, and everything else 8-12 hours per day is not my idea of a great time. reply packetlost 12 hours agorootparentMy mother was an OTR trucker during most of my teenage years, including driving for Walmart under contract (so not paid amazingly well, but regularly interacted with the Walmart employees at the DC in Tomah, WI). It's not always back breaking work, but access to (healthy) food is a major issue. For Walmart specifically, there's not much physical effort involved besides lowering the trailer landing gear, which is almost always done with a manual crank. The Walmart backroom staff handle unloading and the DC staff handle loading, which would be very very taxing to do constantly. reply fuzztester 11 hours agorootparentwhy is access to healthy food an issue? because truckers, being on the road, have to eat in poor quality restaurants, such as diners, maybe? reply packetlost 10 hours agorootparentIn a truck you can't just... drive anywhere. There are designated legal routes that you can't deviate from under most circumstances. The vast majority of food you have access to as a trucker is gas station/truck stop food and fast food attached to truck stops. You have limited cooking options in a truck, maybe a small waffle iron, rice cooker, or microwave. Most won't have room or power for something like a refrigerator (some might, it depends) so keeping non-dry food isn't really an option. reply biggestdummy 10 hours agorootparentprevFirst, your work day is almost completely sedentary. Second, you are not near friends, gyms, fields, or other systems which promote healthy activity outside of work. While diet is important for all of us, it is especially important in this circumstance. \"Healthy foods\" are going to be largely inaccessible through long sparsely-populated stretches of the country. Even where they are available - in markets, fruit stands, etc. pulling your huge truck off the road to park is expensive (in terms of time) and difficult (in terms of space). Truck stops have the infrastructure for your truck. But their food selection is mostly junk. reply genocidicbunny 10 hours agorootparentprevThey generally don't have access to a kitchen or a large refrigerator, so meals tend to either be from a restaurant (where most restaurants, good and bad, are arguably not healthy unless you severely portion-limit) or something that can be reheated in a microwave or portable air fryer. In combination, this results in a not quite healthy diet. reply fuzztester 4 hours agorootparentprevthanks to all for the good answers. reply throwaway2037 9 hours agorootparentprevWhat is it in particular about truck driving that is so bad for your back? I hear this so often. Please don't reply \"sitting down all day\". I have been doing that for more than 20 years in a comfortable office (mostly Hermann Miller Aeron), and my back is fine. My guess: The vibrations of the truck are awful for your bones. reply massysett 7 hours agorootparentDo you ever get up? Maybe turn your head from side to side, or stretch your legs out under your desk? Or go to the bathroom whenever you want? Driving freezes your body into one position for hours on end. Doing it in a car for a few hours on a road trip makes my neck hurt. Doing it every day would be deleterious very quickly. I work at a computer at a desk, but the ability to stand up at will just to use the toilet makes a big difference. Can’t do that driving a truck. reply WalterBright 13 hours agorootparentprev> You need a near perfect driving record among other things. Absolutely. If a driver is at fault in an accident, Walmart will get sued for $zillions. I once knew a middle aged man laying tile. I asked him why was he laying tile for crap wages. He said he lost his trucking license because he had a DUI (not while trucking). Doing construction was the only job he could get. He was quite bitter about it. reply packetlost 12 hours agorootparentPretty sure a DUI bars you from holding a CDL of any kind, not specifically a trucking license. I don't see a problem with that at all. reply throwaway2037 9 hours agorootparentWow, if true, that is harsh. If you made a mistake at 18 or 19 years old, then you are condemned for life to never hold a CDL (commercial driving license). That doesn't see fair. reply stavros 8 hours agorootparentI'm assuming that's if you get the DUI while you have a trucking license. reply packetlost 8 hours agorootparentprevIn WI, you lose it for a year on first offense. Second offense is life. Source: https://wisconsindot.gov/Documents/dmv/shared/cdl-disq.pdf reply beaeglebeached 12 hours agorootparentprevI built my own house because almost every step was more expensive hourly labor than me writing software for an hour instead. Even the lower rate such as concrete laborers wanted 100+ an hour. I suspect the low pay of many of these trades is just a symptom of housing and construction being the number one money laundering industry and thus tradesman are always saying they're making nothing while simultaneously asking 100+ an hour cash and working full weeks. Id bet the BLS income reports are off by like 50+%. reply bernawil 11 hours agorootparentI think you mean tax dodging instead of money laundering there. Though, if they are really succesful dodging taxes, they might need to do some money laundering later, too. reply evilduck 6 hours agorootparentIt's a bit of both. It's not too hard to get cash-only \"customers\" to buy services you don't have to actually perform. It's also pretty easy to write two invoices, feign a mixup, and skim some off the top of a payment. reply hellotomyrars 5 hours agorootparentprevWhere is your house? Those rates are not typical nationwide but I know a general contractor who is involved in construction on the islands near Bellingham and the rates there are like what you describe. I agree that in my experience a lot of contractors would rather deal with cash and it isn’t exactly a secret why. I got a significant all cash discount on a new roof. reply datavirtue 8 hours agorootparentprevAnd the isolation. Sociable people claim they just look at people and can't get words to come out after being in a truck for six months. reply Spooky23 12 hours agoparentprevRetail is always a flat pyramid. GMs get paid. The guy who ran the CompUSA that I worked in in college cleared $250k in the mid 90s. Even a CVS will pay the GM well if they hit targets. And if they miss the targets, they get purged and there's a thousand people to replace them. reply pavel_lishin 14 hours agoparentprev> So Wal Mart does pay well in fact if you are in the right position. This feels like a globally true statement, regardless of the employer, no? reply resolutebat 13 hours agorootparentMost companies don't have 14,000 of these \"right\" positions. reply filmgirlcw 13 hours agorootparent14,000 out of their total number of employees is such a drop in the bucket though. If you can get hired as a Wal-Mart trucker, from what I understand, you're the best of the best. Also, consider there are like 3.5 million truck drivers in the US - only a tiny percentage of those can get jobs at places that pay as well as Wal-Mart. And Wal-Mart has 1.6 million US employees alone, so you're talking less than 1% of total jobs when you say 14,000. reply jonlucc 13 hours agorootparentprevMost companies don't have 2.1M employees. reply fuzztester 10 hours agorootparentprevyes. or of the industry. it's called supply and demand. econ 101. reply shmatt 12 hours agoparentprevWalmart Global Tech is above average tech pay, just a step or 2 behind FAANG. One downside is annual stock grants instead of 4 year, so there is a lot less upside reply washadjeffmad 8 hours agoparentprevA friend went to Harvard to join Waffle House (corporate). These are obviously lucrative ventures that yield reliable returns, such as voting blocs to hold over your (not-so) local congresspersons' heads during election seasons. reply bawolff 9 hours agoparentprevI feel like that is true of all companies. The people with limited responsibilities who are easily replacable get paid nothing. Jobs that are hard to hire competent people for get paid $$$$. Supply and demand in action. reply colechristensen 12 hours agoparentprevI worked in tech somewhere behind walmart.com and the pay/bonuses were standard silicon valley, non FAANG. More or less median. reply sct202 18 hours agoprevA lot of people aren't factoring in that truckers can work 60 hours a week and are exempted from overtime pay and a lot of companies paying the big bucks in trucking are paying that much because they are maximizing the hours to minimize driver count (health insurance, benefits costs don't scale by hour). reply pavlov 15 hours agoparentAnother example of how the American healthcare system is working against society’s best interests. You don’t want all truck drivers putting in the maximum hours humanly possible. It’s a safety hazard. But having to pay for health insurance makes corporations try to optimize for this at the expense of human lives. reply idlewords 14 hours agorootparentTruck drivers don't put in the maximum hours possible; their maximum allowed driving time is heavily regulated by law and increasingly enforced by remote monitoring. reply jimt1234 9 hours agorootparentJust a side-note: My step-dad was a long-hauler when I was a kid, before any of the \"maximum hours\" regulations. The stories he used to tell me are terrifying. Basically, back then, all the long-haulers were alcoholics who drank coffee and popped pills to stay awake and driving for as long as they could. Dudes used to have contests to see who could drive the longest. Driving coast-to-coast on a straight shot was totally common. There were union rules around \"maximum hours\", but dudes wanted to get paid, so they just kept driving. Drunk, and on pills. reply pkulak 13 hours agorootparentprevMaximum hours possible is the maximum allowed driving time. reply jopsen 11 hours agorootparentprevDo truck drivers count the hours they spend waiting to be loaded or unloaded? Sure they have breaks, but is it really spare time of you're stuck in a truck in the middle of nowhere waiting for time to pass so you can drive again? reply MisterTea 13 hours agorootparentprevThe ELD (electronic logging device) reports this data to the trucking company. It is up to them to enforce driving time. However, law enforcement can request these logs during a stop or inspection and the ELD must display this on screen or a print out. https://eld.fmcsa.dot.gov/ reply missedthecue 11 hours agorootparentIt's pretty hard to cheat. Much to the irritation of truckers. reply KennyBlanken 11 hours agorootparentThe opposite could not be further from the truth. Trucker forums are full of ways to cheat the devices like simply pulling a fuse or replacing it with one that's been burned out, and police can't / won't check electronic logbooks because it's too much of a hassle for them. The companies installing the electronic logbook devices are the same people who have incentive to cheat the system. So do you think anyone notices, complains, or gets disciplined when \"the fuse keeps burning\"? reply stavros 8 hours agorootparent> The opposite could not be further from the truth. This means that what you're replying to was absolutely true. reply KennyBlanken 11 hours agorootparentprevYou mean like how hospitals were told to stop overworking their residents by the goverment, and told the residents to fake their timecards? The industry shifted to electronic logbooks and while there was initial resistance, truckers quickly learned they could just pull fuses, or cover GPS antennas...and that police officers would look at paper logbooks but didn't have the equipment or interest to pull electronic logs. Truckers in the US do whatever they please, and the rest of us pay the price when they crash and kill others or dump toxic crap everywhere. reply nonethewiser 15 hours agorootparentprev> You don’t want all truck drivers putting in the maximum hours humanly possible. It’s a safety hazard. But having to pay for health insurance makes corporations try to optimize for this at the expense of human lives. ... what? First, I don't understand how paying for health insurance means they are trying to get the most hours out of their employees. I guess you could say thats somewhat true for literally every overhead cost including social security payroll taxes, training, heating buildings, etc. At best, it seems a little true in a very indirect way so why the laser focus on health insurance? Second, it its truly a safety hazard then it disincentivizes employers from doing it. reply EForEndeavour 14 hours agorootparent> First, I don't understand how paying for health insurance means they are trying to get the most hours out of their employees I'm not OP, but my take: the health insurance cost of hiring a driver is fixed, regardless of how many hours per week they drive. Therefore, the more hours they drive, the more the employer's \"bang for their buck\". > Second, it its truly a safety hazard then it disincentivizes employers from doing it. Health and safety regulations are written in blood, and exist precisely because employers need to be forced by law to maintain safe working conditions for their employees. reply scholarofgolb 13 hours agorootparent> employers need to be forced by law to maintain safe working conditions for their employees. Probably less true in trucking, where the leading cause of worker injury is road accidents. If, say, a roofer falls off a roof, the only damage is to the worker, but if a trucker crashes badly enough to get injured, the truck and cargo are likely damaged as well. Truckers and their employees should be more naturally aligned on safety issues. reply mcguire 12 hours agorootparentThis is one of those situations where a theory reasoned from first principles fails to match reality. Back in the 20th C., drivers frequently took amphetamines in order to spend more time driving. Forged log books are a significant incentive for the development of electronic logs. I've seen many stories of drivers told to break hours-of-service regulations by dispatchers. And the hours of service regs are insane (https://www.fmcsa.dot.gov/regulations/hours-service/summary-... - keep in mind \"on duty\" != \"driving\" but waiting for loads and such, which the driver is not paid for) largely to maximize the number of hours driven. reply scholarofgolb 9 hours agorootparentThat doesn't necessarily mean there's a disagreement about safety between workers and supervisors, it could just as well mean they both disagree with the regulators about the acceptable level of risk. This would make sense from first principles, because unsafe truckers impose externalities beyond the direct risks to truckers and their supervisors, by making the roads more dangerous to the public. reply beaeglebeached 12 hours agorootparentprevWhy isn't it illegal to force someone to log/elog overage hours? Isn't that a fifth amendment violation as you're forcing them to incriminate themselves? Seems elog should be challenged. reply happyopossum 11 hours agorootparentBy that logic any law requiring disclosure of any kind would be a fifth amendment violation. You are not required to participate in trucking, nor is it a right to do so. Choosing that career (and it's associated licensing) comes with responsibilities... reply beaeglebeached 10 hours agorootparentHere's an example. You don't have a right to a short barreled shotgun (per US v. Miller). But you can get one by buying a stamp. If I'm a felon applying for the stamp would be incriminating, thus courts ruled felon cannot be guilty of not disclosing they have the shotgun. So they get to have it and never 'log' it even though everyone else must. It's only violating the fifth for the part where you must admit to crimes, not all logging. I'm talking about hours logged past the maximum. reply lazyasciiart 11 hours agorootparentprevSame reason you can go to jail for not reporting your illegal income. reply beaeglebeached 9 hours agorootparentThe IRS can't make you say where the income is from, though, so unlike logging it's at best a hint of a crime rather than an admission of crime. reply sarchertech 13 hours agorootparentprevFor employers who are big enough maybe. A big percentage of freight though is handled by small employers and owner operators. If a particular unsafe practice produces an extra $1 million dollar accident every 200k hours of driving time, a small one man operation isn’t likely to see that happen over the course of a career. And even if they do, they may not have enough data to notice the correlation. A company with 1000 drivers will likely deal with it once per month. reply mcguire 12 hours agorootparentBut keep in mind, even the least-safe driver will get 99% of the loads to their destination. Dealing with accidents is a cost-of-doing-business issue for large companies. reply IntelMiner 13 hours agorootparentprev> Second, it its truly a safety hazard then it disincentivizes employers from doing it. When have employers ever cared about safety except when mandated by law? These things were all legally codified because people got hurt or died, not because companies wanted to be \"nice\" reply m463 12 hours agoparentprevOn the other hand, there is a maximum number of hours a driver can drive by law, and it has to be logged. I think it is 11 hours per day, with a minimum rest time of 10 consecutive hours. reply asimpleusecase 18 hours agoprevHere is the money quote that makes the most sense to me: “They wanted to pay them good money because it was the absolute core of their, of their business — to get this stuff from the distribution center to the store at precisely the right time with no screw-ups,” Lichtenstein said. “That was crucial.” I am sure that WalMart demands full compliance with all regulations and is very exacting in pick up and delivery times. Likely drives are doing the same routes all the time. A lot of drivers would find that boring. reply lupire 17 hours agoparentDo drivers care about route variety? I don't think it's like flying where you can get a little mini vacation on your route. reply SoftTalker 17 hours agorootparentMany drivers like driving a familar route because they know what to expect. They know where the low bridges are, they know which roads are designated truck routes, where there are difficult turns, they know which exits have fuel/good food/shopping/parking or are a safe place to stop to get out and take a walk, etc. reply onlyrealcuzzo 15 hours agorootparentMy uncle was a truck driver. Driving a new route (in his work) was generally not considered \"exciting\" - but taxing. reply bluGill 17 hours agorootparentprevMost drivers are running a fixed route. You (or often you company) has contracts to make regular deliveries. Most companies that ship something ship the same thing every day to the same customers. that said, there are a lot of deliveries that are not the same thing to the same place. while they are a minority overall, there are still a lot. If you work for a big national shipping company (WalMart is not this even though they are national) you can ask for a delivery to anywhere - they always have a single delivery to some random location. Though if you want to go to rural states - many other drivers grew up on a farm and truck driver is one of the few well paying jobs they can get without a degree on so there is a lot of others wanting that. (If you want to go to a big city there are plenty of delivers and less drivers with reason to want them) reply buffington 5 hours agoparentprevIn the late 90's, my dad was a screen printer and did a bunch of big runs of garments for Walmart stores. He'd built a good size business, doing regular jobs for Disney, action sports, NFL, etc, building a shop that usually ran two shifts of hundreds of employees. It's not an exaggeration to suggest that if it had a screen printed image on it, we probably printed it. > very exacting in pick up and delivery times Well, for us, not exactly. For our last contract with Walmart, there was a lot of language about timing. If the driver arrived and the order wasn't ready for pickup, we'd be penalized for every hour of delay. The penalties were such that you could end up losing crazy money on the job if you weren't confident about your abilities to deliver on time. Not a big deal for us since we were tight, and could easily handle it. Then Walmart arrived a day early. The driver had made a choice to stop for us first, which, after pulling up the contract, we realized it allowed for that. Our lawyer had determined it was vague enough that the agreed delivery date would prevail, contractually. Which is probably why we didn't notice that the contract also said the pickup clock started the minute the driver arrived, and that was also the official pick up time. Had we caught that during contract negotiations, we'd have refused the terms. But we missed it. We weren't a small shop. We already ran two shifts, and quickly needed to pull in a third to be able to finish in time to get any profit out of the job. We almost did that last contract with zero profit, and refused all business from Walmart after that day. Thing is, I've heard of similar stories where Walmart would do similar tricks with produce from farms. Whether myth or not, I don't know, but I know with certainty how they treated us. There were probably incentive structures in place for drivers to show up early, and some of them had the balls to do it a lot earlier than others. In this case, they could give the driver an bonus for his better than on-time averages, then \"buy\" goods worth a retail $5M+ for the cost of the driver's salary and bonus, plus whatever the supplier (us) managed to keep after penalties. reply didgetmaster 17 hours agoprevIsn't 6 figures the new 'living wage' in many areas? reply 0xbadcafebee 13 hours agoparentOnly in the wealthiest cities and neighborhoods, which is a very small share of the total. There's almost 20,000 cities in the USA and only about six of them have an average living wage above $100K. reply s1artibartfast 8 hours agorootparentwhat is the actual median wage? living wage is an aspirational goal. it doesnt tell you much about if 100k is a huge step up or down from typical jobs. reply jklinger410 11 hours agorootparentprevAnd debt is at its highest ratio in modern history, along with all cost of living metrics. Most of those sub-$100k zips are dying. reply refurb 10 hours agorootparentBelieve it or not but there is a whole world beyond San Francisco and NYC. And those cities are thriving. reply mcguire 12 hours agoparentprevMedian household income in the US is $75,000. (https://www.census.gov/library/publications/2023/demo/p60-27...) reply replwoacause 17 hours agoparentprevYes reply wolverine876 11 hours agoprevHow does Amazon pay its truck drivers? They seem like the closest comparison, with their own distribution system. Amazon's reputation is treating employees poorly, but maybe truck drivers are valued. reply massysett 7 hours agoparentDoes Amazon even have truck drivers? Every time I see a Prime trailer, it’s got a random company pulling it. reply Jeff_Brown 10 hours agoprevThis article somehow ignores the burning question it raises: What distinguishes an okay trucker from a great one? Why are they worth so much more? reply cjmcqueen 6 hours agoparentReliability. Truckers aren't just drivers, they're mechanics and logistics strategists. The assignment is to get from point A to point B, but there a lot of decisions in between. Source: two good friends that drive truck for a living. One in Iowa and the other in Michigan reply throwaway2037 9 hours agoparentprevZero accidents in your career. reply kotaKat 18 hours agoprevWalmart is one of the few trucking companies IIRC that slipseats (shares) their sleeper cabs, which means you're continually going to be changing the truck you drive and live in weekly. That kinda sucks. reply slow_typist 14 hours agoparentMalcom McLean, the founder of Sea-Land (sold to Maersk in 1999), who commercialised container shipping, believed strongly in that concept. You cannot run an efficient operation if drivers are allowed to have personal trucks. reply mschuster91 12 hours agorootparent> You cannot run an efficient operation if drivers are allowed to have personal trucks. I'd be interested in actual data. Employee satisfaction/retention and associated hiring costs and increased maintenance/higher failure rates because no one has \"ownership\" over the truck aka diffusion of responsibility happens are where I could see that calculation break apart. reply slow_typist 12 hours agorootparentI think part of the philosophy is the idea that „personal ownership“ will conceal structural (maintenance) problems. It’s similar to lowering inventory. Sure you can run into problems with smaller buffers, but at least you see the problems and have a chance to act accordingly. reply bonestamp2 18 hours agoparentprevI agree the alternative would be nice, but for years I lived out of a different hotel room every week (for work). I didn't mind it. I assume it's similar in that you pack light, and only bring what fits in your luggage so you can pack up and move on quickly at the end of the week. reply brnt 6 hours agorootparentEverybody knows trucks come with room service and daily housekeeping. reply 1letterunixname 11 hours agorootparentprevUsername checks out. Jack Reacher spotted. reply UncleEntity 17 hours agoparentprevI had a gig for a while where we slip-seated the trucks and, yes, it did kind of suck. Mostly because nobody cared about the trucks and they got tore up pretty fast. Plus, you always have that one person who does stuff like leaving trash all over the cab -- the chicken bones all over were my final straw on that one, we had words... reply gooseyman 7 hours agorootparentWait - there isn’t maid service between stays?! reply Ekaros 18 hours agoprevI also think part of equation is retention. You want to keep the drivers as them leaving might disturb deliveries. Specially if they are JIT. Inventory not at store does not sell. reply JeremyNT 12 hours agoprevHere was a recent and (somewhat) related post about the trucking industry and training for a CDL in Texas [0]. A very interesting read about both the macro environment and the particulars of several people looking to enter this career. [0] https://news.ycombinator.com/item?id=39046904 reply kraig911 6 hours agoprevBetween this news and the other bit how store managers make 400k someone is doing good PR work for Walmart somewhere! reply renewiltord 14 hours agoprevAnswer: They don't https://www.glassdoor.com/Salary/Walmart-Truck-Driver-Salari... Looks like top reported are at $75k. reply bena 18 hours agoprevBecause WalMart knows it is primarily a logistics company. They know that if they get the goods to stores efficiently, everything else will follow. reply sumtechguy 18 hours agoparentalso they have union that knows that. reply mixdup 15 hours agorootparentWalmart drivers are not unionized reply silenced_trope 11 hours agorootparentThis makes sense. If they're paid higher than average, then how could a union fight to get them higher wages when they aren't doing it for the unionized competitors that have lower? And also per another comment here, they require an unblemished driving record. So if the worst concern of unions came true, that they can make it hard to fire bad performers and require hiring \"their guys\", then drivers with blemished records can make it in and stick around. That means the good drivers currently employed may see their value drop comparatively and likewise their ability to demand better pay. As it stands it seems like the current drivers then have the benefit of being considered best in industry and may prefer to gate-keep and not unionize as a result in order to protect themselves. I wonder if other companies that want to fight unionization should do similar: hire top performers and pay them well :) reply mcmoor 10 hours agorootparentThis is exactly what union usually work for though. High barrier of entry combined with high salary. It's just that union fought to keep it that way instead of at the whim of company. But still, looks like this is why software engineers are very allergic of union. The same environment already guarantees those properties and adding union on top only makes it sour. That environment's disappearance lately make union possible once more. reply mixdup 10 hours agorootparentprev>I wonder if other companies that want to fight unionization should do similar: hire top performers and pay them well :) I mean this also applies to the rest of Walmart, or at least it did In high schoool and college (98 through early 00s) at age 16 I initially worked at a local grocery store chain that was unionized making minimum wage. They went out on strike for a month and came back to just a ten cent raise. Not long after that I went to work at Walmart making about 40% more, with a 10% discount on anything I wanted to buy in the store Working conditions between the two were not much different, both were pretty transparent on scheduling, job duties, and the like Now, I'm pretty left wing and I support the right to organize without any restriction, but I probably would've voted against a union if the UFCW had come to my Walmart store asking for signatures based on my prior experience with them Does that mean unions have a place? Absolutely. We would not have the meager protections we have today without unions, and they serve a deterrent against employers running amok, but yeah treating your employees somewhat decently is plenty to keep them at bay reply caycep 14 hours agoprevOne playbook in how to survive against Amazon? reply softwaredoug 18 hours agoprevThis is pretty grueling work requiring you to operate heavy machinery, be away from your family for days, and drive mundane hour after hour. Why wouldn’t it be highly paid? reply manicennui 18 hours agoparentWorking at Long John Silvers was also grueling work and I was always covered in grease and received minor burns from time to time. The job can be learned by almost anyone in a couple weeks though. Physical difficulty has almost nothing to do with compensation. reply bdcravens 17 hours agoparentprevMy father was an elementary school janitor and worked harder in a day than I work in a month. However, I make multiples of what he made (even adjusted for inflation, though jobs at the bottom of the pay scale haven't risen like that over the years) All that said, I think the question isn't whether the pay is acceptable, but why Walmart pays so much relative to other companies, where the drivers do pretty much the same job. reply emptybits 18 hours agoparentprevYour reasoning is somewhat sound. History has not been so generous. Source: son of a trucker. Also consider that minimal academic education and minimal specialized training is often sufficient to do the work. So the supply of those willing to do the work has often exceeded demand, so there's naturally downward pressure on pay. reply philomath_mn 18 hours agoparentprevThat isn't how wages are set. It's a combination of supply, demand, and the business value of services rendered. Trucking is a hard job which affects the supply of drivers, but this is offset by the fairly low barrier to entry to trucking. reply randomdata 11 hours agorootparent> It's a combination of supply, demand, and the business value of services rendered. Just supply and demand. Demand is a function of that business value (among other attributes). reply dewey 18 hours agoparentprev> Why wouldn’t it be highly paid? There's many jobs that are probably even worse and paid less. The only reason it's highly paid is because the competition for it is higher: \"at the time it was because of a “shortage” of truck drivers.\" reply throwaway44773 18 hours agorootparentThe barrier to entry to driving a truck is a few months of training so the amount of people that can become truck drivers is quite large. However, the social status of driving a truck is quite low and there is a constant threat of automating the job away. Not to mention the terrible working conditions. reply Teever 18 hours agorootparentThe barrier to entry for driving a truck is greater than just the time it takes to do the training. To pass the training you need to be a competent driver. It isn't as easy as you make it out to be and the pool of people who can become truck drivers is smaller than you think it is. reply throwaway44773 15 hours agorootparentHaving a driver's license is basically table stakes for having a job in the United States. There are exceptions if you live in dense areas with good public transit, or you can work from home. reply madars 14 hours agorootparentCDL != regular driver's license. Does an average driver know how to operate air brakes or prevent jack-knifing or what to do with a runaway engine? Trucking is a highly specialized job and requires more aptitude than ordinary driving. reply vel0city 11 hours agorootparentprevLoads of people with driver's licenses can barely operate a sedan. Passing a CDL exam and not constantly getting into accidents driving a massive truck is a completely different thing. Loads of drivers fret parallel parking. Imagine the difficulty of moving a 54' trailer perfectly straight into a bay at the bottom of a narrow ramp and not crushing anything in comparison to regular parallel parking. reply DiggyJohnson 17 hours agoparentprevI don't mean to sound snarky, but do you actually believe that's how wages are often set, or are you describing an ideal? These sorts of comments confuse me because they seem to be making an argument by explicitly confusing is from ought as an intentional rhetorical technique. reply randomdata 11 hours agorootparent> do you actually believe that's how wages are often set He'd be wrong if he thought anything else. Obviously supply and demand always determines price. There is no question on that front. The only problem with the previous comment, perhaps, is thinking that this particular gruelling/isolating work leads to fewer people being willing to drive the trucks than actually happens. The reality is the average trucker running typical routes isn't really experiencing anything all that gruelling/isolating in the grand scheme of things, so it is a more attractive career than suggested. It's not a cushy programming job by any stretch, but it's still quite comfortable compared to a lot of work out there. The general premise is valid, though. There are clear examples of other, albeit arguably more, gruelling/isolating work that can't get anyone to even consider the job without big dollar signs. He is just overestimating how undesirable trucking in particular is. For a lot of people, trucking is their ideal career. But, in fairness, the comment was in reply to an article that suggests, at least for some routes, that there is trouble attracting drivers. So it wasn’t exactly off-base. The original article may be. reply ecf 18 hours agoparentprevBecause it’s a task that teenagers are expected to do safely? Driving as a profession does not require skill. reply epiccoleman 18 hours agorootparentThe kind of driving that teenagers are expected to do is wildly different from trucking. The biggest vehicle I've ever driven was a U-Haul and even that requires more focus and care than regular driving. Add another 25 feet of length, a trailer which can rotate independently of the vehicle, and tight timetables into the mix, and you're certainly looking at more skill than the average driver needs. reply eropple 18 hours agorootparentprevForget eighteen-wheelers, which are rolling geometry problems: ever driven a solid-axle flatbed? Come back and tell us that that doesn't \"require skill\". reply ecf 14 hours agorootparentI learned from a young age how to drive with a trailer, and I have experience with 30ft fifth-wheels. reply brnt 6 hours agorootparentIf it requires no skill, you can do it as well as I (I've never driven either). Wanna bet? reply kamaal 3 hours agorootparentprevThis reminds me of the time when coding was popular among high schoolers and people were saying coding jobs will be 0 as kids will be building mainframes in days. It turns out its one thing to do a thing for fun or small periods totally a different ball game when you have to do it at scale. Im guessing a lot of people who can play a chord or two like to imagine it can't be that hard to be a concert musician either. Everything changes at scale and when it has to be done seriously. reply bdcravens 17 hours agorootparentprev> ....as a profession does not require skill Some might say the same about gluing together boilerplate in modern development frameworks. At the end of the day, skill is only relevant for competition within a given industry. The pay scale for any given field is based on the value they provide the employers (which is why grown men playing children's games are paid millions of dollars) reply imgabe 18 hours agorootparentprevTeenagers are not expected to drive 18-wheelers for 12 hours a day. reply selimthegrim 12 hours agorootparentMy dad drove coal trucks from Cleveland to Toledo at 16 in the 70s so yes, some are. Was it fully legal? I doubt it. reply imgabe 12 hours agorootparentIf it’s illegal, then it’s not something that’s expected of them. reply selimthegrim 6 hours agorootparentYou live a charmed life. reply imgabe 6 hours agorootparentI think we're talking past each other. I took \"expected\" to mean \"expected by society in general\". Teenagers are generally expected to get regular driver's licenses and learn how to drive a car (although many of them don't even do that anymore). It is not common for people to expect a teenager to have a commercial driver's license and know how to operate an 18-wheeler. Now, is it possible that in the 1970s your dad had an unhinged boss who expected a 16 year old to reliably operate commercial trucks without any training? Is it possible your dad is an exceptional driver who managed to do this? Sure, both of those things are possible. Is it a reasonable thing to expect from any randomly selected teenager? No. reply kome 18 hours agorootparentprev> Driving as a profession does not require skill. how can say this with a straight face? reply Symbiote 18 hours agorootparentDidn't everyone play Euro Truck Simulator (or whatever it was), when it went viral as a daft video game of the year? I got stuck going round the first corner, then got lost, then gave up when I had to back the trailer into the loading bay. https://eurotrucksimulator2.com/ reply maxsilver 18 hours agorootparentprevCome back and say this, after you've gotten a CDL Class A reply ecf 14 hours agorootparentMy best friend’s brother in law recently paid a CDL mill to get a license. Took him less than a month. Miss me with the demanded respect for an industry that will be replaced in a couple decades. reply selimnairb 18 hours agoprevBecause they are professionals who do work that can be dangerous to them and the general public? reply bdcravens 17 hours agoparentTrue, but that begs the question of why every trucking company doesn't pay as much as Walmart. reply randomdata 4 hours agorootparentBecause they operate under a different business model that is willing/able to risk hiring amateurs. Not dissimilar to why FAANG will pay a software developer many hundreds of thousands of dollars each year all while Mom & Pop are hiring some kid to develop their software for $10 per hour. reply kome 18 hours agoparentprevEh, it doesn't work like this unfortunately; pay is never a mirror of the dangerousness, the utility, or the dirtiness of the job. I would say that usually it's the contrary: the more essential a job is, the less it is paid. reply Infinity315 18 hours agorootparentNo, pay corresponds with the skillset and how replaceable you are (moreso how replaceable). Walmart truck drivers are highly compensated because trucking is a lonely and relatively uninteresting job to do. Doing that for many hours requires a certain type of person and would mentally wear down most people. Walmart requires a consistent level of output which they hope can be maintained by keeping truck drivers happy with high pay. Other trucking companies can afford to have some drivers leaves and thus pay them less because they have some leniency for delays which Walmart cannot/will not tolerate reply FredPret 18 hours agorootparentprevIt's all supply and demand. Dirty and dangerous work attracts fewer workers. High-utility work is worth more to the employer so there will be more job offers. When you see dirty, dangerous, high-utility work that pays poorly, it's because there's a ton of people who are willing to do it. reply notShabu 15 hours agoprevI wonder if this has the effect of increasing \"market rate\" so that only big companies with economies of scale can be competitive reply Ericson2314 14 hours agoprevFordism reply hyperpape 18 hours agoprevAlready, several comments saying how being a truck driver is a tough job (it is!) or \"that's what they have to pay to get decent drivers\". But whether the job is hard, isn't the question, and the second comment is sort of vaguely accurate, but just ignores the interesting part. From the article: > One of the best jobs you can get in trucking is at Walmart. The uber-retailer says truck drivers can make up to $110,000 in their first year at the company. That’s twice the nationwide median pay of a truck driver The interesting question is: why does Walmart pay so much more than is typical? Unfortunately, the article gives a pretty superficial analysis. Edit: to forestall helpful comments, I am aware that businesses often make an effort to do things based on a kind of sophisticated analysis where they compare the upside of doing the thing (\"benefits\") to the downsides (\"costs\"), and then do the thing if the upsides seem to exceed the downsides. The question is, why is it that this significantly above market rate is what Walmart thinks maximizes its benefits compared to costs? reply PaulHoule 15 hours agoparentThe general standard in the industry is pretty bad. My brother-in-law, who has a CDL, tells me somebody wedges a tractor-trailer into the Tompkins Street bridge in Binghamton about once a month because they ignore the clearance warning signs. My understanding about Wal-Mart is that trucking for them is a good job because you get to sleep in your own bed. That is, the distribution center is close enough to the store that you can drive for them and live a normal life, it's not like driving trucks all the way across from the US. Normal trucking companies are always having workers say \"take this job and shove it\" and then struggling to get their cargo moved, truckers know they are in demand enough they can quit without notice, take a few weeks off, then start up at the next place. reply WheatMillington 14 hours agorootparent>My understanding about Wal-Mart is that trucking for them is a good job because you get to sleep in your own bed. This could justify Walmart paying UNDER the median wage, but why is Walmart paying considerably ABOVE the median wage? reply ace2358 14 hours agorootparentBecause possibly they are aware that their shops need the goods for them to stay operational. They may also realise there are other areas of the business they can squeeze. They may realise that having a few missed shipments costs a lot more than the wages saved. reply soared 14 hours agorootparentprevWouldn’t that enable Walmart to pay less, since the job itself is better? reply akira2501 14 hours agorootparentOutside of Silicon Valley, not a lot of companies will pay you less under the auspice that the fringe benefits somehow \"make up the difference\" and Walmart is about as far from startup culture as you can get. Aside from that in order to drive one of these trucks, you require a CDL, which you have to get on your own, and maintain on your own, all at your own expense. The pay is there to attract talent from a limited pool of available workers and to ensure they show up to work on time and make deliveries on time. It would be against their own interests to try to cut corners and pay drivers less based upon these types of \"benefits.\" reply ElevenLathe 14 hours agorootparentprevThe point is that a delayed truck is very expensive, so it ends up being cheaper to pay efficiency wages and have good working conditions so people aren't constantly quitting and therefore delaying your trucks. reply PaulHoule 14 hours agorootparentWal-Mart was also a pioneer in just in time delivery for retail. If your supply chain is reliable you don't need to pile merchandise as high in your stores and your warehouses. People will drive an hour to Wal-Mart(s) in the most rural locations and in that case it is a real bummer to be out of stock. I'd contrast that to K-Mart where I went to get 3 digits for my mailbox and could only get 2 of them so I didn't buy any. reply azemetre 14 hours agorootparentprevNot if high turnover means slower deliveries which likely effect everything downstream (or upstream). When you don't pay well, you probably miss out on the hires that learned all the skills, what corners to cut, what corners to not cut, what actual lead times are, etc when hiring. reply HPsquared 14 hours agorootparentprevIt must contribute. Their pay would probably be even higher otherwise. reply eschneider 18 hours agoparentprevThe cost of not having a reliable supply chain is much more than they're paying their drivers. It's just good economics to pay more to reduce driver turnover. reply manicennui 18 hours agorootparentProbably takes away a lot of drivers from their competitors too. reply savanaly 14 hours agorootparentprevWhy is it not good economics for their competitors to do so as well? reply adventured 13 hours agorootparentWalmart has basically always been superior to its competitors at logistics. It has been one of their key competitive advantages for many decades. They have been a data monster heavily focused on logistics for most of their existence (I've been reading about their various logistics advantages for 30 years now). Even if doing a thing is good for a competitor, that obviously doesn't mean that competitor will do said logical thing. Companies fail all the time because they're incompetent in one form or another (or many forms). reply hanniabu 14 hours agorootparentprevLess scale, easier to fill gaps, less impact reply wharvle 18 hours agoparentprevComparing “up to” with median is setting off my bullshit alarm. reply olyjohn 14 hours agorootparentThen it's always \"... well including the health insurance and retirement package which we estimate costs us $75,000/yr.\" So actual take home wage is never near $110,000. reply lupire 17 hours agorootparentprevThe Quota consensus is that the thesis of the OP is simply misinformation, as expected. https://www.quora.com/Walmart-is-offering-new-truck-drivers-... reply mathgradthrow 15 hours agorootparentMisinformation or advertising? reply elil17 14 hours agorootparentOften the same thing reply nonethewiser 15 hours agorootparentprevGood catch. That's a bullshit comparison indeed. reply paxys 18 hours agoparentprevAny reputed tech company pays entry level engineers 2-3x more than the industry average. There's no complicated reason behind it. They just want the best talent. There are plenty of $50K/yr programmers out there but they aren't going to write the kind of code you want. I imagine it's the same for truck drivers. reply mettamage 15 hours agorootparentThis is not a thing in NL unless you do HFT programming. One of the reasons why the US pays so much seems to simply be that the economies of scale are there. reply HPsquared 14 hours agorootparentIt's the same in all skilled professions. Pay scales are much steeper in the USA than Europe/UK. reply badpun 14 hours agorootparentprevI don't think it's economies of scale. Top money in US is paid by tech companies (FAANGs and similar) who have virtual monopolies on their markets, and thus are printing money. They overpay for programmers to make sure they're not outcompeted on tech and retain their monopoly status. Second crucial factor is the VC ecosystem unlike anything else in the world. VC-fueled startups (even late-stage ones, like Uber) will also often pay way above market for programmers, because they believe programming talent is the way for the startup to dominate the competition and become a money-printing monopoly. Europe and rest of the world does not have anything like that. There's no serious software industry (aka \"tech\") there, neither large and estabilished nor VC-fueled. Programmers are seen as a commodity and not a source of competitive advantage. And, even if there are exceptions, they're not common enough to start bidding wars for top talent which in the US elevated the salaries of top ICs to $300k-$400k. reply filleokus 13 hours agorootparentI think it's even more complicated than that. If you compare salaries for physicians (doctors) or lawyers there will be a huge gap as well. Even though there is no VC-funding, economy of scale or monopolies to speak of. On the supply side of high salaries I think the economy of scale actually have some impact by creating a higher \"floor\". For example I would almost wager that every European country have their own Salesforce company (non-SAP web based CRM from early/mid 00's), but the e.g Latvian alternative never broke out to the wider EU. Whilst Salesforce conquered US and then the globe. This puts upward pressure on the virtual monopolists / VC-fueled companies that don't really exist outside the US. Imagine if almost every single B2B software company in the US spent their first 5 years only operating within a single state, and many never expanding outside. But I think the \"demand side\" is even more important. Most European countries have lower take-home salaries for large parts of the salary spectra, definitely the top quartile. The reason for that are also numerous. But we generally have high taxes on an individual level, high payroll taxes, high VAT etc. In exchange for that we have large welfare states with larger redistributions both across individuals and across time for the same individual (paid parental leave, pension etc). reply lebean 15 hours agorootparentprevThere are plenty ofThere are plenty of A shortage of truck drivers is unlikely. Surely a transient shortage is possible due to licensing requirements. You could offer ten million dollars a year, but there are only so many people with a current CDL and finite capacity exists to grant new licenses. reply randomdata 17 hours agorootparentThe cool thing is that if we really did see ten million dollar per year offers, then most people wouldn't want to use the services of truckers anymore (too expensive), so you'd still have the right number of truckers, if not too many, among those currently able to drive, without the need for more. No need to wait on anyone else getting licensed. You have done well to highlight why a technical shortage is defined the way it is. reply thfuran 16 hours agorootparentThat's begging the question. reply randomdata 16 hours agorootparentDoes the question not find begging to be a tad bit pathetic? reply CydeWeys 16 hours agorootparentprevThe licensing requirements for being a truck driver just aren't that onerous. It's way way harder to become a doctor, lawyer, hell, even hairdresser in many states. reply thfuran 16 hours agorootparentA shortage of doctors could definitely stick around for longer, but it's still external influence limiting market actors. reply randomdata 16 hours agorootparentA shortage of doctors is quite likely as many jurisdictions place price ceilings on doctor services. In fact, where I live, it is quite illegal for a doctor to accept any amount of payment from a patient. We as a society believe that people, no matter how rich or poor, should have equal access to healthcare, so a doctor giving priority to a billionaire who can offer a handsome reward to get his common cold looked at over someone suffering something much worse is considered abhorrent. As such, we rely on a mixture of first-come, first-serve and needs-based priority instead of a price-based mechanism. Never heard of any attempts to place similar restrictions on truckers, though. If you want to try and charge a billion dollars per mile, go nuts! reply nightpool 17 hours agorootparentprevNot everything is a perfectly elastic market. In fact, nothing is. For example, if I wanted to go out and hire 250,000 US-based computer programmers tomorrow, I would not be able to even if I wanted to pay them a million dollars each, since there are only 132,740 computer programmers across the entire US. So that would constitute a shortage of computer programmers. It would take many years to create the number of educational institutions, training programs, etc necessary to have that many programmers available to hire. The same could be said for many industries on a much smaller scale—how many people are willing to relocate to driving trucks in northern Canada? Probably not as many as you'd like, even if you had an unlimited budget. How many qualified airplane pilots are there in the world? Too few—we don't have enough training programs for them and mandatory \"aging out\" requirements have removed a lot of the pilots we already had. A lot of new training and education programs for pilots have been started in the past year, but they're going to take 4-5 years until the supply has increased enough that we're no longer in a shortage. (And although this is a heavy topic of debate between union leaders who say that there's no shortage and airlines who say that there is, I think the amount of investment in slow and costly supply increase programs is a good sign that there's at least some amount of supply constraint/bottleneck affecting the industry. It's hard for there not to be when safety and training requirements make the supply pool so slow to respond to changes in demand). reply sokoloff 17 hours agorootparent> there are only 132,740 computer programmers across the entire US Source? That feels extremely low, maybe even by an order of magnitude. reply nightpool 15 hours agorootparentAh, yeah, I was just going off a simple Google. Probably ended up on the wrong BLS page. I agree it does seem low, a search for \"Software Developer\" turns up on this BLS page with much higher stats—1.5M developers. reply medvezhenok 16 hours agorootparentprevYeah, that's either out of date or only counts people that report their title as \"Computer Programmer\" rather than \"Software Engineer\" or one of the myriad of other ways to describe people who develop/maintain software. The recent estimates are closer to 4.4 million software engineers in the U.S. (~2.75% of the working population) reply rightbyte 13 hours agorootparent> The recent estimates are closer to 4.4 million software engineers in the U.S. (~2.75% of the working population) Heh, we are probably more than the secretaries we replaced. I honestly believe most companies are worse off using computers at all. At some size, you'd probably want some computer system for pay slips. But I am not too sure about that ... reply andy99 18 hours agorootparentprevAnd if you were a big business, you'd hire lobbyists to petition the government for Ferrari subsidies and buy news articles talking about the Ferrari shortage, and tell all your friends that the only reason you don't drive a Ferrari is purely because of the shortage and has no relationship to your finances or priorities. reply zeroonetwothree 18 hours agorootparentprevIn economics you can’t have a shortage in a perfect market as you note. Of course most (all?) markets are imperfect so various factors can cause shortages (eg rent controls is a common example). For labor markets imperfect mobility can cause a shortage. In common use it means that something costs more than many people are willing to pay for it. So your Ferrari example would be a valid use under this meaning. reply tbihl 16 hours agorootparent>In common use it means that something costs more than many people are willing to pay for it. More likely it costs far less than people are willing to pay for it. When the Colonial Piepline was shut down for ransomware, the shortage occurred because a reasonable populace correctly assessed that the local value of gasoline had just skyrocketed far beyond its steady state price, so the demand became limited only by time and social costs (e.g. you don't want to be the guy whose picture is shown filling trash bags of gasoline) plus ways to store it. reply grotorea 17 hours agorootparentprevMaybe we can still call it a shortage if there's some unexpected and/or temporary reduction in supply or increase in demand. You could still buy oil during the 1970s oil crisis but it still was a shortage. Or it can mean that there's insufficient supply to meet demand at a \"reasonable\" price. reply solidsnack9000 17 hours agorootparentprevIn most contexts, a shortage is when there is a gap in supply due to external factors. This results in resource allocation not responsively adapting to price signals. A commonly used example is milk, yogurt, cream and ice cream. Every year, the farmers produce milk. Some people want yogurt, some want milk, some want cream, &c. Say that one year, everyone wants yogurt for some reason. They are lining up to pay 200%, &c, for it. Then the allocation of resources adapts -- more milk is used to make yogurt. If demand for other products does not go down -- people still want just as much milk, ice cream, &c -- then their prices may go up. Over a reasonable time frame, the effect of this is that more land will be turned over to pasturage and more milk will be produced. Gradually the prices will come down. More people have more yogurt than before; more people are working on that and less on something else (whatever it was). The workers and consumers live happily ever after and resource allocation has adapted to price signals. One important consideration in the above example is that if people want more yogurt and are willing to pay more, they must also be willing to go without something else. They only have so much money (so much of their own resources to allocate). That is why the resource allocation has to change. Now that might not be milk, per se, but it is probably something adjacent to it, like tofu. We can see how if the shift is from tofu to yogurt, the reallocation of resources might take longer -- people can't just take the milk consumers don't want and turn it into yogurt, they have to get more milk first, which means feeding more soybeans to cows and less to tofu machines -- but it still happens. A simple example of a shortage is a situation where a virus affects a certain fruit tree, preventing it from growing (or, at least, preventing it from growing the desired fruit). Although people are willing to pay a lot for this fruit, and by extension are willing to consume less of other fruits or other foods, there is no way to turn resources over from the production of other goods to the production of this fruit. It might be that only a small number of areas are isolated enough to produce this fruit; they might all be turned over to it; but after that, there would be no elasticity in the supply of this fruit. Given what we know about the pricing, demand, ordinary cost of inputs for producing the fruit, the supply of this fruit before the virus, we must conclude that there is considerable demand which is not being met. (Something else notable about the second example is that most of the new, elevated price of the fruit would go into some form of rent. The ground on which the fruit could be produced would be exceedingly rare and valuable, but the material inputs, labor, skill, &c needed for the fruit are the same as before; the new high price would mostly go to the ground rent or financing costs.) reply jvanderbot 18 hours agoparentprev> drivers leave the industry is basically the same thing as > there's a shortage If anyone wants to work on this problem with me without solving the self-driving truck / Aurora problem outright - I think the answer is something like remote operation, level 3-4 autonomy for highways, and nice comfy air conditioned driver hubs in cities so nobody has to be away from home for days at a time. Boom - all the drivers come back and get paid decent wages to drive trucks. Ideally the per-truck subscription fee for the service is an add-on to the cost of doing driver business. reply wil421 18 hours agorootparentHow realistic are the trucking sims? Do you actually get gas and unload? Highway stuff and regular driving sounds good but backing a trailer up to Walmart video game style sounds frightening. reply bluGill 17 hours agorootparentPretty good for the expensive ones. They are not video games though, and so they are not making the shortcuts games do to make them fun. I've driven a few (snow plow, excavator) and they are very hard to control - then I see someone who drives that machine professionally get on: they get a near perfect score and comment about how real it feels. Note that part of the sim is the controls: they take the cab from a real machine and replace the windshield with a screen (for version 1, often version 2 starts to take away parts, but everything you touch including the seat is the real thing). I've long thought that to renew your drivers license you should have to spend a day every few months in a simulator to prove you can drive safely. Do you maintain a safe following distance? Can you maintain the speed limit? Do you stop for pedestrians? Do you zipper merge correctly? Here is a situation where someone else screwed up and you will crash, can you choose the best crash? Here is an icy road, can you control the car? there is a lot that modern simulators can do but few people have used them. reply gooseyman 7 hours agorootparentI agree with you on the DL renewal based on a sim. Interestingly: The base CDL is a one and done skill test. There are no CPEs like with other regulated industries (nurses/teachers/investment advisors) - but there is a health and fitness check. reply Taylor_OD 17 hours agorootparentprevThe difficult part is actually the last mile/few miles in trucking. Open highway driving is a significantly less complicated problem than navigating down side streets then backing into a warehouse to complete a delivery. Freight tech companies have been talking about automating every BUT the last mile/last few miles for years. Have a stable of drivers who you have on hand to take over the last hour of a delivery. Automate everything else. reply Yujf 17 hours agorootparentThe nice thing about this is also that truckers would be able to go home everynight which would make the job more attractive reply jvanderbot 18 hours agorootparentprevWe can do that scary bit autonomously already: https://www.outrider.ai/ I'm not referring to \"sim\", I'm referring to remote piloting. Real time sensor and controls feedback from a truck that is only autonomous when it makes sense to be. We can pilot drones over 100s of yards in real time without a hitch. We just need a bit more range (few kms) via, say, the internet, and you can do this fairly easily. I don't see a research problem, I see a funding problem and an aversion to autonomous trucking b/c people think it's either completely infeasible, or completely solved by Aurora. I'm not sure of either. reply SoftTalker 17 hours agorootparentAnd what does the driverless truck do when it loses its internet connection at 70mph on a freeway? reply jvanderbot 17 hours agorootparentThe use case does not include operating remotely at 70mph on freeways, esp because internet connectivity is hard over most the USA. That bit is mostly lvl 3/4, and is something that is addressed well by other companies. It's the long tail between lvl3/4 and all the scenarios that remain in which a remote driver solves the problem. Esp in logistics yards, which are chaotic, fast paced, and driven by voice communication. reply ikari_pl 18 hours agorootparentprevOTOH we dock manned spaceships to the ISS video game style reply foxyv 17 hours agorootparentThat is a much easier problem than backing a truck up to a loading bay. Even landing an airplane is a lot easier if you have an ILS. Space and the air are mostly empty. A warehouse loading area is cluttered with other trucks, debris, and cargo. Although I could totally see this being done remotely with VR and a ton of cameras. reply jvanderbot 17 hours agorootparentAgree with you. Just to add spice the convo: Backing into a loading bay is quite easy, autonomously. Once you stage the truck you can push a button and just watch it go. See: https://outrider.ai (which is where I work, and we do all the staging _also_, but I'm actually interested in \"everything else\" as well). There's a lot of very human-ish problems to get to that state, that I think remote operators can really help with. reply tbihl 16 hours agorootparentprev>If anyone wants to work on this problem with me without solving the self-driving truck / Aurora problem outright In the US, we un-solved this problem decades ago when we abandoned railroads for subsidized auto development. reply nradov 18 hours agorootparentprevRemote operation on public highways is a dead end. There is no wireless data network with the latency and reliability guarantees necessary for safety. Particularly when it comes to tunnels, canyons, and heavy precipitation. And let's not have any ridiculous claims about how in case of a network connection failure the truck can just autonomously pull over and stop; that's not going to work safely for descending I-70 in a snowstorm. There may be some limited use cases for remote operation on specific local routes where sufficient network access points can be provisioned. reply jvanderbot 17 hours agorootparentYou can't do it with autonomy, and you can't do it with remote piloting, but you can do it with a mix of both. That's my assertion. Spending 20 hours looking at Nebraska / Illinois farm fields is not the use case for remote operation - and as you say, you lack the infra for that anyway. And navigating traffic or interacting with distribution center logistics is not the case for autonomous operation - it's an infra nightmare to get a autonomous vehicle to integrate with all that radio-voice-comms madness. Even OTR drivers just want to get that part of the journey over with and get back to the hotel. Having a pod with local drivers near major hubs, for example, means the drivers take over when the trucks get close enough for it to matter. It wont' work for delivery to, say grocery stores (which can be local driving anyway), but it will work for center-to-center transport ala between Walmart hubs. It's the long haul OTR trucking that has high attrition / people shortages, because you're away from family for so long. That little bit can be somewhat autonomous, with handoff of remote operators only near hubs. Think air-traffic-control for trucks and hubs, except probably more hands on than just telling it what to do. reply datadrivenangel 17 hours agorootparentLike harbor pilots for ships, where someone comes out to join the ship to help navigate through potentially dangerous waters. reply tstrimple 14 hours agorootparentprevEven without full autonomy, truck platooning has been shown to be effective in places where they have been tested. One human driver and \"autonomous\" followers. These truck platoons could be the solution to long haul deliveries while individual drivers still handle the last mile delivery and city navigation. https://projects.research-and-innovation.ec.europa.eu/en/hor... reply newsclues 18 hours agorootparentprevIt’s not a dead end, it’s a solvable problem but the issue is that no one wants to pay for infrastructure. reply LiquidSky 17 hours agorootparentprev\"> drivers leave the industry is basically the same thing as > there's a shortage\" No, it's not. It means the drivers are not being paid enough to stay, as they explicitly say is the reason they're leaving. >If anyone wants to work on this problem with me Sure, here's the solution: pay the drivers more. Problem solved. reply jvanderbot 17 hours agorootparent> drivers are not being paid enough to stay therefore > drivers leave the industry therefore > there's a shortage ... at the going wage I don't see how we can argue over the problem statement. We can argue over the solutions, of which there are infinite, but at least two: 1. increase wages 2. decrease shittiness of the job via ideas like mine Or why not both? The only dichotomy here is a false one you've introduced. reply LiquidSky 17 hours agorootparentBecause there's no problem. There's not \"shortage\", it's not that there just aren't enough people to be drivers. There are plenty, employers just don't want to pay enough to attract them. >We can argue over the solutions No, we can't, because there's no solution needed to this non-problem artificially created by employers unwilling to pay enough to attract and retain employees. Your ideas aren't needed except by the employers trying to perpetuate their false self-made \"shortage\", all to avoid just paying employees more. The only actual problem is uncritically accepting the narrative of the employers as you are doing here. reply jvanderbot 17 hours agorootparent> There's not \"shortage\", it's not that there just aren't enough people to be drivers. There are plenty, employers just don't want to pay enough to attract them. OK, well, let's not mince definitions. For the purposes of my thought process, a shortage is defined as \"Number of people we want to do a job\" minus \"Number of people who do that job\". The semantic difference between that and what you said is so small as to be negligible to me and doesn't preclude \"increase wages\" as a solution. I think that's fine for you to have a preferred solution or even a different preferred definition. reply LiquidSky 16 hours agorootparentNo, you're just wrong. You're carrying water for the employers who want to squeeze their employees. Again, there is no actual problem here, just an artificial one created by employers unwilling to pay their workers and crying about it. Don't buy into it. reply medvezhenok 16 hours agorootparentIt's not about employers in this case. If we think being a truck driver is an important job we can subsidize truck drivers federally, via taxes. At the end of the day, society (in the form of government regulation) gets to decide what is and isn't an important job (i.e. government subsidies), and we the people can certainly put our thumb on the scale. People collectively decided truck drivers were overpaid [relatively] in the 1970s, hence the deregulation of the trucking industry which led to a collapse in trucker's salaries. No use blaming employers here - it was decided by the government (representing the people of the U.S.) reply randomdata 15 hours agorootparent> It's not about employers in this case. In this case it is. The discussion is about how employers have a dream that isn't being realized. They are metaphorical 10 year old-boys drooling over the idea of owning a Ferrari, without the capability to actually buy one. The only \"problem\" is that their dream is remaining a dream. Which isn't actually a problem. Dreams aren't supposed to be realized. They are meant to be just dreams. Who gives a shit if a 10 year old can't own a Ferrari? And that is what the parent is pointing out – that the '10 year-old cries' of business are meaningless. Let them dream, but it ends there. If people of the U.S. want to give truckers more money, they are free to do so, but that is well beyond the topic at hand. reply jvanderbot 16 hours agorootparentprevI completely understand and agree with your point. Just wanted to be clear. Not disagreeing at all. reply unethical_ban 14 hours agorootparentprevTalk about rhetorical self harm. The person you're screaming at isn't far from you, but you refuse to acknowledge a premise. Notably, you ignore their autonomous driving pitch is supposed to increase working quality of like, like many instances of automation (in theory). What if someone can think drivers deserve better pay and ethical treatment and that self driving is a long term solution to a grueling job? reply LiquidSky 14 hours agorootparent>but you refuse to acknowledge a premise. Why would I acknowledge a flawed premise? >like many instances of automation (in theory). That (in theory) is doing so much work here it ought to be paid. >What if someone can think drivers deserve better pay and ethical treatment and that self driving is a long term solution to a grueling job? What if mythical beasts like unicorns existed? That would be pretty cool, I guess. Sadly, we'll never know. reply gadders 18 hours agoparentprevLike any other \"good\", there is only ever a shortage of a particular employee category at a particular salary rate (obviously there may be a lag between supply and demand). reply randomdata 18 hours agorootparentA shortage, as it pertains to other \"goods\", is defined as a situation where an external mechanism prevents price from rising. A common example of a shortage is where price gouging laws are in effect. When a shortage occurs, a non-price based mechanism must be used to distribute \"goods\" instead, such as first-come, first served, a lottery, needs-based selection, etc. Only labour gets the special shortage definition seen here. Seemingly because shortage was originally used with respect to healthcare practitioners, where true shortages are possible with price ceilings often imposed by government in order to allow equal healthcare access to all citizens regardless of how much they can afford to spend, and then misconstrued onto other fields. reply lupire 17 hours agorootparentSince physical limitarions exist, including time, shortages exist. We can't simply pay more money to get a personal chef, doctor, and pilot for everyone, even if there was a magical infinite source of compensation to pay them. reply randomdata 17 hours agorootparent> We can't simply pay more money to get a personal chef, doctor, and pilot for everyone, even if there was a magical infinite source of compensation to pay them. Remember, money is debt – an IOU, a promise to deliver something later. It says \"You do this for me now and I will give you this token (money) that you can redeem for what you want from me in the future.\". 'Compensation' is just the other side of the transaction – you offering the same in kind. If there was a magical infinite source of compensation then there would necessarily be an infinite ability to provide chef, medical, etc. services. In the real world, you only have so much to offer others, which limits how much you can receive from them. As before, transactions must be balanced. That is what it means 'to compensate'. In the real world, you have a pick what you want from the limited amount of value you can offer in kind. There is no such thing as an insatiable demand for chefs or doctors. People will stop considering those services when they become too expensive. The more expensive, the fewer people needed. This only becomes a problem when price is prevented from rising (e.g. due to government intervention) – and that is when you can encounter a shortage. reply gadders 16 hours agorootparentprevWell, no. But we also shouldn't say (EG) \"There is a shortage of Java Developers\" without also specifying at what salary level. reply bluGill 17 hours agorootparentprevBuy you could have all that for yourself - if you are a billionaire. Everybody cannot have that, but somebody could. Economics is about managing limited resources. I end up being my own personal chef. I share my neurosurgeon with 100,000 people (most of whom never need one). Because I can share my neurosurgeon there is no shortage. reply DiggyJohnson 18 hours agorootparentprevCan we not simply say a shortage is when there's not enough of something. In this case, available drivers willing to work for the compensation they're willing to pay? reply randomdata 18 hours agorootparent> Can we not simply say a shortage is when there's not enough of something. More or less, but \"how much is needed\" is dependent on price. You need an infinite number of truck drivers if they are driving for free. And you need no truck drivers if they are driving for $100,000,000,000 per day. In a functioning market, price will rise until you have exactly the right number of truck drivers. Remember that the dreamers not willing to pay the price are simply not in the market. You don't have a shortage of Ferraris when 10 year old boys with Ferrari posters on their walls can't have the real thing. However, if something stops the price from rising – such the government stepping in and saying: \"It is now illegal to pay truck drivers more than $30,000 per year\" – then you have a disconnect. The market wants more drivers and are willing to pay more to get them, but are not allowed to pay them more. That disconnect is what the shortage defines. reply UncleEntity 16 hours agorootparentYeah, but that's not why there's a \"shortage\" of drivers. There is absolutely no upper limit to driver pay and no nameless bureaucrat filling out some permissible wage table but only \"what the market will bear\". From experience I can tell you the main two culprits are it's kind of a shitty job and it's kind of a shitty job that can turn into a really shitty job (or no job) really, really fast. No serious solutions ever treat it like the quality-of-life problem that it really is -- parking is a major problem, shippers/receivers suck up large quantities of unpaid time and living in a truck/being away from home for weeks at a time is not for everyone are probably the top 3 complaints I would hear. Well, and all the crazy shit the \"4-wheelers\" get up to but that's usually entertaining. reply randomdata 16 hours agorootparent> Yeah, but that's not why there's a \"shortage\" of drivers. Right, because there isn't a shortage of drivers. There isn't even a lack of drivers. We have the right number of drivers – along with a whole lot of metaphorical 10 year olds with Ferrari posters who like to talk big with their friends about how they want truck drivers, but when push comes to shove, they really don't. reply LiquidSky 17 hours agorootparentprev>Only labour gets the special shortage definition seen here. Isn't this backwards? This is the colloquial plain-language use of \"shortage\" (though intentionally misleadingly and wrongly used by employers), you're talking about a technical economics-only definition. reply mixdup 15 hours agoparentprevWa",
    "originSummary": [
      "Walmart pays its truck drivers significantly higher salaries compared to the national average, with salaries in the six-figure range.",
      "The company relies heavily on its trucking workforce to maintain a robust supply chain and ensure timely delivery of products to its stores.",
      "Walmart has recently implemented changes in its hiring policies, allowing associates to join a training program to become fleet drivers, aiming to retain talent and maintain control over driver training and culture."
    ],
    "commentSummary": [
      "The discussion explores wages, compensation disparities, job satisfaction, and challenges across various industries, with a focus on truck drivers and Walmart.",
      "Key points include the perception of low wages at Walmart and the hierarchical pay structure in European tech companies.",
      "The challenges faced by truck drivers, the impact on their health, and concerns about automation and remote piloting in the trucking industry are also discussed."
    ],
    "points": 193,
    "commentCount": 308,
    "retryCount": 0,
    "time": 1706539907
  },
  {
    "id": 39176797,
    "title": "Design Patterns and Considerations of Query Optimizers: Exploring Intermediate Representation (Part 1)",
    "originLink": "https://xuanwo.io/2024/02-what-i-talk-about-when-i-talk-about-query-optimizer-part-1/",
    "originBody": "What I Talk About When I Talk About Query Optimizer (Part 1): IR Design I recently came across an insightful article on SQL Query Optimizers by @leiysky on Zhihu, and I must say it was excellent! To make it accessible to a wider audience, I have translated the piece into English. Enjoy reading! Please note that @leiysky deserves full credit for the quality of this article. Any mistakes are solely due to my inadequate translation skills. Alright, let's begin! During our recent conversation, Mr. Chi and I discussed his latest project at CMU called optd, which is a query optimizer library developed using the Cascades framework. We ended up griping together about various design and implementation aspects of database optimizers. It was at that moment when I realized the intriguing nature of certain technical subjects and decided to take note of them. So, I've made the decision to launch a series where we'll cover everything about query optimizers—ranging from algorithm basics to engineering techniques, technological evolution to project implementation, and even some insider industry gossip. The inspiration of the title comes from Haruki Murakami's book, which I highly recommend. The book is a collection of essays that Murakami wrote about his experience as a runner. The title is a reference to a collection of short stories by Raymond Carver, \"What We Talk About When We Talk About Love\", which Murakami translated into Japanese. Software development is a skill just as running is, everyone can do it by practising but not everyone will have the same feelings about it. I'd like to share some personal thoughts and experiences about query optimizers, and I hope that you will find them interesting. Today, let's begin by discussing the topic of IR Design, focusing on common design patterns in optimizers and the underlying considerations. What is a Query Optimizer? Before we officially start the discussion, I'd like to clarify the definition of a query optimizer. In general, a query optimizer is a database component that optimizes the execution plan of queries. However, different databases have various methods for optimizing queries. For instance, some may directly rewrite the AST, perform transformations during AST lowering, or dynamically rewrite during query execution. To unify the concept, I will refer to all parts from the SQL parser to the SQL Executor collectively as the Query Optimizer. What is IR? Friends familiar with compilation technology should be very familiar with the term IR. IR stands for Intermediate Representation, which is commonly used in compilers for different programming languages like Rust's HIR & MIR and LLVM's LLVM IR. IR serves as a structural representation of programming languages, enabling the compiler to conduct various analyses and optimizations more effectively. If SQL is considered a programming language, then relational databases function as virtual machines that execute SQL, similar to how the JVM executes Java. The query optimizer is responsible for translating SQL statements (Java code) into execution plans (Java bytecode) for the executor (Java runtime) to execute. Consequently, it is essential to design different IRs for SQL when developing a query optimizer. What does SQL IR look like? Typical database projects are divided into several modules: Parser, Analyzer/Binder, Optimizer, and Executor. These components process SQL statements sequentially to transform them into query results. In our context, the Optimizer includes both the Analyzer and Optimizer modules mentioned earlier. AST SQL is a declarative language that mimics natural language syntax. It is based on relational algebra and can describe operations on sets, mapping them to queries on tabular data (tables). To facilitate processing, like the vast majority of compilers, we will first parse SQL language into an AST (Abstract Syntax Tree). A typical SQL AST is illustrated as follows: In the SQL AST, we generally divide nodes into two types: Statement (Stmt) and Expression (Expr). The root node of each SQL AST is always a Statement, which may contain some Clauses as well as Expressions. An Expression is a recursive structure that includes various operators and function calls, and even nested Statements (Subqueries). One interesting aspect of SQL is the blurred boundary between Statements and Expressions in SELECT Statements. This occurs because SELECT Statements are recursive and must address operator precedence issues (UNION/EXCEPT/INTERSECT). Additionally, only SELECT Statements can interact with Expressions recursively, which should be considered when designing an SQL AST. Relational Algebra The theoretical foundation of the SQL language is relational algebra, and every query statement corresponds to a representation in relational algebra. For example: Since the expression of relational algebra is also a recursive tree structure, many systems naturally convert SQL AST into an execution plan similar to the one shown below. We refer to each node as an operator, and we call the entire operator tree a query plan. Of course, there are exceptions among the many systems. For example, IBM, as a pioneering, introduced the Query Graph Model (QGM) in its Starburst system. This representation is quite abstract and hardcodes many properties into QGM, making it exceptionally difficult to understand. Its claimed extensibility is also questionable. Due to space limitations, I won't elaborate here; if interested, you can read the related papers Extensible Query Processing in Starburst and Extensible/Rule Based Query Rewrite Optimization in Starburst. Currently, mainstream databases have essentially adopted the representation of relational algebra (such as IBM's System R and DB2, Oracle's various database products, Microsoft's SQL Server series, open-source PostgreSQL and MySQL 8.0). Based on this foundation, they have developed numerous optimization frameworks and execution frameworks. Therefore, choosing to use the abstraction of relational algebra when designing SQL IR is a foolproof decision. By utilizing the various axioms and theorems of relational algebra, we can perform a variety of transformations on SQL IR to achieve optimization while ensuring correctness. Specific optimization rules and algorithms will be discussed in subsequent articles. (My) Best Engineering Practices There is a cognitive bias known as the curse of knowledge, which occurs when one assumes that others possess the same level of knowledge during communication. This phenomenon is quite common in software development. People who have experience writing certain types of code and those who don't often struggle to communicate effectively, even if they share the same theoretical foundation (algorithms, programming languages, or domain knowledge). The reason for this lies in the significant flexibility of software engineering; there are multiple ways to implement the same functionality, each with its own set of challenges. To eliminate such communication barriers, various technical fields have developed their own set of idioms or design patterns. New projects built on these practices can avoid a lot of unnecessary trouble. The same is true for the field of databases; however, due to its niche nature and high degree of commercialization, knowledge circulated among the public is very scarce, and engineering practices are scattered across various open-source projects. In this article, I will build a SQL IR from scratch based on my own best practices, which will facilitate the progressive sharing of some design considerations. Due to personal preference, I will use Rust to write code. Friends who are not familiar with Rust need not worry; as long as you have a basic understanding of C/C++, you can comprehend the logic behind Rust's code. Hello, world! When we learn a new programming language, the first program we generally encounter is \"hello world\". fn main() { println!(\"Hello, world!\"); } Therefore, we will also start by building our IR from the SQL version of \"hello world\". create table t(a int); select * from t; Translating this SQL statement into relational algebra is very straightforward; we denote it as Get(t), which means to return all the data in set t. To represent such a query, we can define a simple struct called Get. pub struct Get { pub table: String, } fn plan() -> Get { // select * from t; Get { table: \"t\".to_string(), } } This simple SQL IR is now complete. With the Get, we can represent all queries similar to select * from xxx. Isn't it very straightforward? Select & Project Next, we can add more features to this IR, supporting additional SQL clauses. For example: create table t(a int, b int); select a from t where a = 1; This SQL query, when translated into relational algebra, can be denoted as Project(Select(Get(t), a = 1), a). The Select operator can filter data based on the provided predicate, while the Project operator can trim the set to obtain the required attribute. To represent such a query, we need to add more struct definitions. pub struct Get { pub table: String, } pub struct Select { pub get: Get, pub predicate: String, } pub struct Project { pub select: Select, pub project: String, } fn plan() -> Project { // select a from t where a = 1; Project { select: Select { get: Get { table: \"t\".to_string(), }, predicate: \"a = 1\".to_string(), }, project: \"a\".to_string(), } } Upon arriving here, we are confronted with several questions: According to the theorem of relational algebra, can Project act as a child of Select? Given that Select is optional for an SQL query, how should this be reflected in the code? To address these issues, we can introduce some features of dynamic dispatch. In C++/Java, inheritance is commonly used to represent an Operator, for example: class Operator {}; class Get : public Operator {}; class Select : public Operator { Operator* _child; }; class Project : public Operator { Operator* _child; }; In Rust, we have a more convenient option that allows us to enjoy the benefits of both static typing and dynamic dispatch, which is enum. Rust's enum is an ADT (Algebraic Data Type), also known as tagged union, and it can represent our operators very conveniently: pub enum Operator { Get { table: String, }, Select { child: Box, predicate: String, }, Project { child: Box, projects: String, }, } fn plan() -> Operator { // select a from t where a = 1; Operator::Project { child: Box::new(Operator::Select { child: Box::new(Operator::Get { table: \"t\".to_string(), }), predicate: \"a = 1\".to_string(), }), project: \"a\".to_string(), } } With this, we can freely represent operator trees of various shapes, and the design of the IR begins to get on the right track. Scalar expression Although we have introduced the operators Select and Project, the Select Predicate and Project Expression still exist in the form of strings, which cannot meet the requirements for analysis and optimization. Therefore, we need to design an IR for these expressions as well. Looking back, after being processed by the Parser, SQL strings are transformed into AST, and the expressions within them become Expr nodes, roughly like this: pub enum Expr { ColumnRef(ColumnRef), Literal(Literal), Function(Function), BinaryOp(BinaryOp), UnaryOp(UnaryOp), Subquery(SelectStmt), } The expression itself is a recursive structure, and the Expr node of AST is also a recursive structure. Can we lazily use the Expr node directly as part of our SQL IR? Let's give it a try first. After replacing string with Expr, we can obtain: pub enum Operator { Get { table: String, }, Select { child: Box, predicate: Expr, }, Project { child: Box, projects: Vec, }, } Next, let's try some common analysis using the given SQL statement to see if it works well: select a from t where exists (select * from t1 where t.a = t1.a) Q: Which tables and columns does Expr in Project depend on? A: It uses a column called a, but I don't know which table it belongs to, maybe this column doesn't even exist. Q: What is the return type of Expr in Project? A: I don't know, there is no type information included in Expr. Q: Is the subquery in Select a correlated subquery? A: I don't know, the subquery in Expr is just an unprocessed AST. Ok, it seems that Expr is not as useful as we imagined. In order to conduct the above analysis, we need to design a more informative IR. To distinguish it from Expr, we will name it ScalarExpr. Summarizing the above analysis, our requirements for ScalarExpr are as follows: All identifiers must be resolved to fully qualified names. Type information needs to be injected and undergo type check. All subqueries need to be transformed into SQL IR form. Combining the above requirements, along with some desugar, ScalarExpr would look something like this: pub enum ScalarExpr { ColumnRef(Vec, Type), Literal(Value, Type), Function(Signature, Vec), Subquery(Quantifier, Box), } In this way, the design of the expression's IR is also formed. Let's integrate the entire set of SQL IR together. The IR After the above design, we have: Operator, a tree structure capable of flexibly expressing various SQL queries. ScalarExpr, providing rich semantic information. Although some key operators are still missing, such as Join, Union, Aggregate, etc. However, since the overall framework is already very clear, we can follow the same pattern and add them as well. After integration, we have a fairly perfect SQL IR. pub enum ScalarExpr { ColumnRef(Vec, Type), Literal(Value, Type), Function(Signature, Vec), Subquery(Quantifier, Box), } pub enum Operator { Get { table: String, }, Select { child: Box, predicate: ScalarExpr, }, Project { child: Box, projects: Vec, }, Join { kind: JoinKind, condition: ScalarExpr, left: Box, right: Box, }, UnionAll { left: Box, right: Box, }, Aggregate { group_by: Vec, aggr_exprs: Vec, child: Box, }, } Because it is too perfect, I have decided to give this IR an imposing name - The IR. Property Derivation When we want to analyze and optimize IR, we always need to obtain some properties of the IR. We can calculate these properties by writing an analyzer that traverses the entire IR, but this requires a lot of effort to maintain the state of the context in which the IR is located. Fortunately, SQL as a declarative query language for data flow is quite simple, and we can use its features to calculate properties. The data flow and parent-child relationships between operators in The IR are closely related and presented as a DAG (directed acyclic graph), where all data flows from child nodes to parent nodes. Under this characteristic, it is simple to compute the property of a certain IR node. It only requires recursively computing the property of each child node and then calculating its own property based on these properties. We refer to this process as property derivation. pub struct Property; fn derive_property(op: &Operator) -> Property { // Calculate the properties of the children operators. let children_property: Vec = op .children() .map(derive_property) .collect(); // Calculate property with the children properties. op.calculate_property(&children_property) } In SQL optimization, commonly used properties can be divided into two categories: relational/logical properties that describe the characteristics of a data set and physical properties that describe the physical characteristics of the data. Common relational properties include: Information about the attributes/columns contained in the dataset Cardinality of the dataset, indicating the number of records in the dataset Statistics, representing the data distribution of attributes Constraints, representing constraints on attributes, such as NOT NULL Functional dependency, indicating the functional dependency relationship between attributes Common physical properties include: Order Degree of parallelism (DOP) Data distribution Data partition Combining the properties of relational algebra, we can describe the differences between types of properties. Assuming there are relations R and S: the relational property of R is RP_R, and the physical property is PP_R; the relational property of S is RP_S, and the physical property is PP_S. We can obtain: It is not difficult to see that the equivalence relationship between two relations can determine the equivalence relationship of relational properties, but the equivalence relationship of physical properties is not affected by the equivalence relationship of relations. The content about combining properties with specific query optimization algorithms will be discussed in detail in subsequent articles. With property derivation, we can use theorems from relational algebra to optimize The IR while ensuring correctness. So the next question is, what should the property look like? Relational properties The most important part of relational property is the representation of attributes. In naive relational algebra, each relation is composed of sets of tuples, and each attribute in a tuple has its own unique name. It is natural for us to directly consider using the tuple schema as the representation of attributes. Let's first recall how a table is created. create table t(a int); In SQL, we use DDL (Data Definition Language) to create and manage various tables. When creating a table, we need to specify its table schema, which includes the specific definition of each column in the table, corresponding to attributes in relational algebra. The structure of the table schema might look something like this: pub struct TableSchema { pub name: String, pub columns: Vec } pub struct ColumnDefinition { pub name: String, pub column_type: Type, pub not_null: bool, } Since ColumnDefinition and attribute have a one-to-one correspondence, can we directly use ColumnDefinition to represent the property of attribute? We can try adding support for attributes in The IR first. fn derive_attributes(op: &Operator) -> Vec { // Calculate the attributes of the children operators. let children_attributes: Vec> = op.children().iter().map(derive_attributes).collect(); // Calculate attributes with the children attributes. op.calculate_attributes(&children_attributes) } First, we need to make some modifications to The IR and add table schema information for the Get operator. pub enum Operator { Get { table: String, schema: Vec, }, // Nothing changed for other variants } Then we implement attribute derivation for the Operator. impl Operator { fn calculate_attributes(&self, children: &[Vec]) -> Vec { match self { Operator::Get { schema, .. } => { let attributes = schema.clone(); attributes } Operator::Select { .. } => children[0].clone(), Operator::Join { .. } => { let mut attributes = children[0].clone(); attributes.extend(children[1].clone()); attributes } Operator::UnionAll { .. } => children[0].clone(), Operator::Project { .. } => todo!(), Operator::Aggregate { .. } => todo!(), } } } Most of the operator implementations went smoothly, but it can be seen that Project and Aggregate have been marked as todo. At this point, we will find that Project and Aggregate cannot directly generate their own attributes using children attributes. Going back to relational algebra, the purpose of Project is to trim the shape of tuples or modify the name of attributes. This kind of SQL expression like SELECT a + 1 AS b FROM t cannot be expressed as a naive Project; as for Aggregate, it is not even present in basic relational algebra, it is an extension to relational algebra. The theory of relational algebra no longer exists! However, despite this, the project still needs to continue. We need to introduce some village rules to expand the definition of relational algebra. Here we provide the formal definitions of Project and Aggregate in The IR. Project represents the attributes in relationship R as input, output a tuple consisting of n function mappings f_1 to f_n. Aggregate represents grouping the tuples in relationship R according to m attributes k_1 to k_m, and applying n function mappings f_1 to f_n on each group, finally outputting the grouped tuples. The biggest change in this village rule is the introduction of derived columns. For columns directly from tables in SQL, we call them base table columns; for columns calculated through Project/Aggregate, we call them derived columns. Before introducing the concept of derived columns, we could ensure that all data sources would ultimately point to the Get operator. However, after its introduction, this convention was broken and a concept similar to scope in programming languages emerged. We need to be more careful when optimizing. After having village rule, we can also achieve attribute derivation for Project and Aggregate. However, at the same time, we also need to make some modifications to the structure of The IR. pub enum Operator { Project { child: Box, projects: Vec, }, // Others } impl Operator { fn calculate_attributes(&self, children: &[Vec]) -> Vec { match self { Operator::Project { projects, .. } => { let attributes: Vec = projects .iter() .map(|(expr, alias)| ColumnDefinition { name: alias.clone(), column_type: expr.column_type(), not_null: expr.nullable(), }) .collect(); attributes } Operator::Aggregate { group_by, aggr_exprs, .. } => { let mut attributes: Vec = group_by .iter() .map(|expr| ColumnDefinition { name: expr.name(), column_type: expr.column_type(), not_null: expr.nullable(), }) .collect(); attributes.extend(aggr_exprs.iter().map(|expr| ColumnDefinition { name: expr.name(), column_type: expr.column_type(), not_null: expr.nullable(), })); attributes } // Others } } } In this way, we can calculate the attributes property for all operators. Come and try it out! First, let's take a look at the most common and effective optimization in SQL - predicate pushdown. This optimization can reduce the computational workload of other operators by pushing down the Select operator into other operators, while ensuring that the overall query result remains unchanged. It is very concise and elegant. Let's try to implement this optimization on The IR. The idea is very simple, just swap the positions of Select and Project based on the relational algebra theorem. However, since we introduced derived columns, we must check if the predicate in Select depends on the column generated by Project. fn push_down_select_project(op: &Operator) -> Option { match op { Operator::Select { child: project @ box Operator::Project { child, projects }, predicate, } => { let project_attributes: Vec = derive_attributes(&project); let predicate_used_columns: Vec = predicate.used_columns(); // Check if the predicate uses any column from the project. let used_derived_columns = predicate_used_columns.iter().any(|used_column| { project_attributes .iter() .any(|attr| attr.name == *used_column) }); if used_derived_columns { None } else { Some(Operator::Project { child: Box::new(Operator::Select { child: child.clone(), predicate: predicate.clone(), }), projects: projects.clone(), }) } } _ => None, } } It seems to be basically usable now, which is delightful. Let's try a more complex example, such as trying SQL with Join: Because Join does not generate additional derived columns like Project, the logic for checking will be relatively simpler. Let's first implement an optimization that attempts to push Select down to the left child of Join: fn push_down_select_join_left(op: &Operator) -> Option { match op { Operator::Select { child: join @ box Operator::Join { left, right, .. }, predicate, } => { let left_attributes: Vec = derive_attributes(&left); let predicate_used_columns: Vec = predicate.used_columns(); // Check if the predicate only uses column from left. let only_left = predicate_used_columns .iter() .all(|used_column| left_attributes.iter().any(|attr| attr.name == *used_column)); if only_left { Some(Operator::Join { left: Box::new(Operator::Select { child: left.clone(), predicate: predicate.clone(), }), right: right.clone(), ..join.clone() }) } else { None } } _ => None, } } Everything looks great, but the devil often hides in the details. Let's take a look at the output of this example in PostgreSQL: leiysky=# create table t(a int); CREATE TABLE leiysky=# create table t1(a int); CREATE TABLE leiysky=# insert into t values(1); INSERT 0 1 leiysky=# insert into t1 values(1); INSERT 0 1 leiysky=# select * from t, t1 where t.a = 1; aa ---+--- 11 (1 row) The final result returned has two attributes called a. In the current implementation of The IR, we cannot know which side this Select should be pushed down to. Because when we check which side the predicate that depends on a can be pushed down to, we will find that both sides of the Join can satisfy it. Although it is not allowed to have multiple columns with the same name in the same table, there is no such restriction between different tables. As the open-source database product with the highest support for ANSI SQL, PostgreSQL naturally handles this kind of problem very well. Through the EXPLAIN statement, we can see that it pushes down the Select to the correct place. leiysky=# explain(verbose) select * from t, t1 where t.a = 1; QUERY PLAN ---------------------------------------------------------------------- Nested Loop (cost=0.00..491.78 rows=33150 width=8) Output: t.a, t1.a -> Seq Scan on public.t1 (cost=0.00..35.50 rows=2550 width=4) Output: t1.a -> Materialize (cost=0.00..41.94 rows=13 width=4) Output: t.a -> Seq Scan on public.t (cost=0.00..41.88 rows=13 width=4) Output: t.a Filter: (t.a = 1) (9 rows) As a perfect SQL IR, The IR must also have its own solution. If we carefully observe this query, we will find that the predicate of Select is represented by a qualified name. If an unqualified name is used, PostgreSQL will throw such an error: leiysky=# select * from t, t1 where a = 1; ERROR: column reference \"a\" is ambiguous LINE 1: select * from t, t1 where a = 1; Because in the current context, a is ambiguous, but t.a is not. Let's try using qualified name to represent attribute property to solve this problem. For this purpose, we need to make some code changes. pub struct QualifiedName(pub Vec); impl QualifiedName { /// If the current name can be used to refer another name pub fn can_refer(&self, other: &Self) -> bool { self.0.len()Option { let candidates: Vec = attributes .iter() .filter(|attr| attr.name.can_refer(name)) .collect(); if candidates.len() == 1 { Some(candidates[0].clone()) } else if candidates.len() > 1 { panic!(\"Watch out, ambiguous reference found!\") }else { None } } fn push_down_select_join_left(op: &Operator) -> Option { match op { Operator::Select { child: join @ box Operator::Join { left, right, .. }, predicate, } => { let left_attributes: Vec = derive_attributes(&left); let predicate_used_columns: Vec = predicate.used_columns(); // Check if the predicate only uses column from left. let only_left = predicate_used_columns .iter() .all(|used_column| resolve_attribute(&left_attributes, used_column).is_some()); if only_left { Some(Operator::Join { left: Box::new(Operator::Select { child: left.clone(), predicate: predicate.clone(), }), right: right.clone(), ..join.clone() }) } else { None } } _ => None, } } In this way, the above problem is solved, and we have the ability to handle complex attribute references. However, there is still a long way to go before achieving a perfect solution. Let's take another example: leiysky=# select * from (select * from t1) as t, t1 where t.a = 1; aa ---+--- 11 (1 row) Although SQL does not allow the use of multiple identical table names in the same FROM clause, we can bypass this by using an inlined view or CTE. According to our current implementation, when processing t.a = 1, we have two t1.a attributes instead of t.a because we did not handle the alias of the inlined view. Therefore, we need to add a Project specifically for renaming attributes. So the problem arises again, because we only renamed some columns and treated them as derived columns, which added a lot of burden to our Select pushdown. Therefore, we must modify the definition of The IR and various related codes to serve the mapping of names. pub enum Operator { Project { child: Box, // (Expression, Source name, Alias) projects: Vec, }, // Others } These problems can be solved by writing a little more code, but take a look at the next example. I believe that most people will go crazy just like me: leiysky=# select a from t natural join t1; a --- 1 (1 row) leiysky=# select t.a from t natural join t1; a --- 1 (1 row) leiysky=# select t1.a from t natural join t1; a --- 1 (1 row) leiysky=# select * from t natural join t1; a --- 1 (1 row) leiysky=# select a from t join t1 on t.a = t1.a; ERROR: column reference \"a\" is ambiguous LINE 1: select a from t join t1 on t.a = t1.a; Of course, we can add all kinds of strange restrictions to the code, create difficult-to-maintain loopholes to maintain this property, and ensure the correctness of these properties while optimizing. But for lazy programmers, finding a simpler design is a better choice. Welcome to the Deep Water Zone. The IR made simple The initial version of The IR was very concise and elegant, but in order to achieve more functionality and support more complex requirements, we added a lot of information that we don't want to focus on. In general, the ideal state of The IR should be: Having a concise algebraic structure Operator nodes being completely independent from each other Not having to deal with names (only for debugging and display purposes) Let's take a moment to reflect, does IR really rely on name? We initially used name to represent attributes mainly based on intuition and reused the table schema. However, there is a lot of useless information embedded in the name, which is of no help to our optimization. It's similar to various symbol names in programming languages that eventually become memory addresses and register numbers during program execution. Without a name, attributes cannot be distinguished. Is a name really such an inconvenient thing? NOTE: the above image means \"Is a name really such an inconvenient thing?\" Ultimately, what we need is to assign a unique id to each attribute, whether it is an integer or a string. Our sole purpose is to differentiate and reference attributes using these ids. All name resolution will be handled in AST lowering; I only want the attribute id! After the redesign, we have changed the way attributes are represented and also made some changes to The IR's definition. By default, we use int64 as the attribute id type. pub type Id = i64; pub struct Attribute { pub id: Id, pub column_type: Type, pub nullable: Type, } pub enum ScalarExpr { ColumnRef(Id), // Others } The design of the id generally cannot be separated from the corresponding context. In SQL IR, the common design methods for attribute id can mainly be divided into two categories: One is based on the abstraction of tuple attribute that we have used before, using the index of attribute in tuple as its id. We call this kind of id as local id. The characteristic of this design is that the id of the same logical attribute will change with different operators it belongs to. The advantage of this design is that it can be inferred from the operator tree without relying on external states for maintenance. However, a disadvantage is that frequent remapping of ids is required when converting operators. Another method is to maintain a global id generator and assign a unique id to all attributes in SQL IR. We call this kind of id as global id. The advantage of this design is that it decouples attributes from tuple schema and allows representation using an unordered collection structure like HashMap. It also helps property derivation through set operations and reduces maintenance complexity. However, a disadvantage is that operator trees using global ids depend on external states and cannot exist independently. The use of these two different designs will have a significant impact on the specific implementation of the optimizer. For example, regarding this optimization: When there are suitable indexes available, this optimization can avoid full table scans and improve performance. If using the local id design, implementing this optimization is very simple, just need to copy the entire operator tree and finally connect them with UnionAll. But if using the global id design, this is a non-trivial operation, even can be said to be very painful. In order to distinguish different attributes, we must generate new IDs for all attributes while copying the operator tree at the same time, and then replace all places that reference these attributes with new IDs. This will cause many troubles when the query is more complex. For example, when optimizing join order: According to the commutative law of Join operators, we can legally exchange the left and right child of a Join. When using global id design, because attributes can be represented as an unordered set, this operation has no impact on property derivation. However, when using local id design, this operation becomes extremely painful. Apart from optimization-related parts, there are also significant differences in representing correlated subqueries. Correlated subquery is a special type of subquery that can access attributes outside its own scope. We refer to accessing such special attributes as outer reference. Many programming languages also support similar operations, which allow accessing variables that are not defined within the function by binding them to a specific environment. This special type of function is called a closure. fn main() { let a = 1; let f = || { let b = a; // a is captured from outside println!(\"{}\", b); }; // f is a closure f(); // stdout: 1 } The design using global id can determine whether the subquery is correlated through attribute property calculation. However, when using local id design, we generally need to maintain an additional scope id in the ColumnRef of scalar expression, which is very cumbersome to implement. Correlated subquery is a very big topic, and we may discuss it in subsequent articles. It can be seen that both designs have their own advantages and disadvantages. In engineering practice, we need to choose a suitable design based on our own needs. Personally, I think global id is a better design because it can easily solve problems in most cases. After the transformation using global id, the code of The IR can be greatly simplified. pub type Id = i64; pub struct Context { pub id_gen: Id, } pub struct Attribute { pub id: Id, pub column_type: Type, pub nullable: Type, } pub type AttributeSet = HashMap; pub enum ScalarExpr { ColumnRef(Id), Literal(Value, Type), Function(Signature, Vec), Subquery(Quantifier, Box), } pub enum Operator { Get { table: String, output_columns: AttributeSet, }, Select { child: Box, predicate: ScalarExpr, }, Project { child: Box, projects: Vec, }, Join { kind: JoinKind, condition: ScalarExpr, left: Box, right: Box, }, UnionAll { left: Box, right: Box, }, Aggregate { group_by: Vec, aggr_exprs: Vec, child: Box, }, } After transferring the complexity to the AST lowerer, we can confidently say that The IR is now a production-ready SQL IR. It supports all SQL operations and common optimizations, has a user-friendly API, and is also very easy to understand. What's more important is that no one understands The IR better than the readers of this article, and any reader can easily extend The IR according to their own needs. Afterword Finally, we have reached the end of this article. As the opening of the series, in this article I simply discussed some focal points in SQL IR design without delving into the details of various algorithms. However, sharing the design process of IR is an interesting thing. Understanding multiple IRs is like understanding why a roadside tree grows crooked. To someone seeing it for the first time, the tree's unusual shape is puzzling. However, locals who have lived around it are aware of its backstory: when it was young, the tree became bent due to the habit of hanging preserved meat on its branches. This little thing is an important reason for the final result, but it is too insignificant to be voluntarily shared by those who know - of course, in reality, no one often cares about the reasons behind it either. Database development is a niche field with many engineering practices and experiences. These experiences are rarely circulated among people and I don't want them to disappear with changing times like America's moon landing technology; hence my original intention to write this series of articles came about. In the next article, I will share related content about optimizer architecture; please stay tuned. Categories code Tags sql",
    "commentLink": "https://news.ycombinator.com/item?id=39176797",
    "commentBody": "What I talk about when I talk about query optimizer (part 1): IR design (xuanwo.io)184 points by xuanwo 18 hours agohidepastfavorite42 comments refset 16 hours ago> the Query Graph Model (QGM) representation is quite abstract and hardcodes many properties, making it exceptionally difficult to understand. Its claimed extensibility is also questionable. I don't know much about the context, but it was interesting to note that Materialize scrapped their QGM code last year: https://github.com/MaterializeInc/materialize/pull/17139 Also, a couple of interesting projects in the IR space: - https://substrait.io/ is a cross-language serialization for Relational Algebra - https://www.lingo-db.com/ is an MLIR-based (LLVM) query engine described extensively in this paper https://db.in.tum.de/~jungmair/papers/p2485-jungmair.pdf?lan... reply leiysky 7 hours agoparentA fun fact, I was there when I first see them going to put it in use. That’s a pity they finally gave it up. https://github.com/MaterializeInc/materialize/issues/8073 reply cmrdporcupine 14 hours agoparentprev> - https://substrait.io/ is a cross-language serialization for Relational Algebra This is awesome. reply simicd 11 hours agorootparentAgree, substrait is a really cool project! Related: if you like substrait you might want to check out datafusion too. The project is a query execution engine built on top of Apache Arrow (incl. SQL parser, query planner & optimizer, execution engine, extensible user defined functions, among others) and it implements a substrait provider and consumer: https://github.com/apache/arrow-datafusion/tree/main/datafus... reply cmrdporcupine 8 hours agorootparentIn general I'm not interested in analytics/OLAP, but in more OLTP type workloads, so the Arrow stuff and that whole world is of not much interest to me. reply alamb 16 hours agoprevBTW you can see a version of what an industrial strength query optimizer / execution engine looks like in Rust https://arrow.apache.org/datafusion/ (can also use it in your own projects) It is quite similar to what is described in this post reply Sesse__ 11 hours agoparentAfter a quick look, I'm not sure if I would call this “industrial strength”. In particular, the join optimizer (typically the heart of a large-scale SQL optimizer) looks very rudimentary? And the statistics it uses have zero idea about correlation, no histograms beyond min/max… reply menaerus 1 hour agorootparentI was wondering about the same claim. However, I believe that JOIN's are a common weakness among OLAP database engines, and DataFusion is built on top of a columnar storage format - Apache Arrow. reply sanxiyn 1 hour agorootparentBy being columnar, I guess you could say DataFusion has a good executor, but no, not a good optimizer. reply menaerus 21 minutes agorootparentNot that I was trying to make any of those claims but just trying to correlate the domain with what appears to be a common problem in it. reply chrisjc 16 hours agoparentprevSomewhat similar, see https://substrait.io/ So for example using DuckDB with the Substrait extension, if you create a table create table t(a int); and then query it as in the article, you can see something similar to what is described in the article CALL get_substrait_json('select * from t'); {\"relations\":[{\"root\":{\"input\":{\"project\":{\"input\":{\"read\":{\"baseSchema\":{\"names\":[\"a\"],\"struct\":{\"types\":[{\"i32\":{\"nullability\":\"NULLABILITY_NULLABLE\"}}],... DuckDB extension doesn't seem to cover any DDL operations though. https://duckdb.org/docs/extensions/substrait Some other related discussions and links that i've collected over the years https://news.ycombinator.com/item?id=37415494 https://news.ycombinator.com/item?id=34233697 https://news.ycombinator.com/item?id=31981568 https://datastation.multiprocess.io/blog/2022-04-11-sql-pars... https://tomassetti.me/parsing-sql/ reply biggestdummy 10 hours agoparentprevThere's also this pretty detailed article on StarRocks' query optimizer. (StarRocks is open source - focused on OLAP) https://medium.com/starrocks-engineering/starrocks-inside-sc... reply muizelaar 13 hours agoparentprevSpecifically: https://github.com/apache/arrow-datafusion/tree/main/datafus... reply zachmu 14 hours agoprevWe implemented a query optimizer with a flexible intermediate representation in pure Go: https://github.com/dolthub/go-mysql-server Getting the IR correct so that it's both easy to use and flexible enough to be useful is a really interesting design challenge. Our primary abstraction in the query plan is called a Node, and is way more general than the IR type described in the article from OP. This has probably hurt us: we only recently separated the responsibility to fetch rows into its own part of the runtime, out of the IR -- originally row fetching was coupled to the Node type directly. This is also the query engine that Dolt uses: https://github.com/dolthub/dolt But it has a plug-in architecture, so you can use the engine on any data source that implements a handful of Go interface. reply throwaway81523 18 hours agoprevFirst installment is mostly introductory but gets more interesting near the end, and the rest of the series sounds promising. I'm looking forward to it. I also recommend the sqlite.org docs \"overview of the optimizer\" and \"vbdb bytecode\" which you could see as an IR. reply leiysky 17 hours agoparentThanks for your comment. I'm the author of this post. I have been thinking about sharing something about database internals for a long time, but none of my writing ideas was out of cliché then(e.g. introduce some algorithms, summarize some papers). But I just realized that it maybe an interesting thing to talk about \"how\" and \"why\" instead of \"what\". The rest posts are coming soon, hope you can see them on hacker news again. reply Sesse__ 15 hours agorootparentAn honest question, since you're there: Why do you consider MySQL 8.0's optimizer to be based on relational algebra? It's true that the new executor introduced in 8.0 is Volcano-style, but the optimizer is pretty much ad-hoc as I see it. reply leiysky 7 hours agorootparentI agreed with you on the ad-hoc part. The thing is Oracle guys have made significant improvement in MySQL 8.0, including the Volcano-style executor. They also implemented DPhyp join reorder algorithm but with an ad-hoc IR. The join optimizer IR(they call it RelationalExpression) is based on relational algebra. I think this is a good start. But it’s not easy to migrate a project lived for decades, especially one with poor design. reply Sesse__ 1 hour agorootparentYeah, I know, I wrote the new executor and the hypergraph join optimizer (the one based on DPhyp), that's why I wondered :-) It's true that if you are using the hypergraph optimizer, you will get a rewrite from the array of tables into RelationalExpression. But I find it hard to call that relational algebra; in particular, it only supports joins and tables as operations. Filters are pushed down ad-hoc, and things like grouping or windowing operators are simply not representable in this structure at all. Columns are not dealt with at all either (projection is unavailable). And perhaps more importantly; RelationalExpression is hardly used. Most of the optimizer works on the old array-of-tables structure, then it briefly becomes RelationalExpression for condition pushdown, then the hypergraph is created and RelationalExpression is never to be seen again. The entire hypergraph optimizer works by inducing subgraphs of a hypergraph; it does not use relational algebra. Also, notably, MySQL 8.0 does not actually _use_ the hypergraph optimizer by default. You need to explicitly compile it in (it's off in release builds), and then enable it using an optimizer switch. So unless you go to fairly great lengths to enable it yourself, RelationalExpression and friends is never used. I agree that using a relational algebra IR would be a good idea; it's a better structure than what's in there right now (which comes all the way from MySQL 3.x, and is extremely unflexible to work with). It's just that I don't think MySQL 8.0 does it. :-) (I obviously don't speak for Oracle, not the least because I haven't worked there in a while) reply mattgreenrocks 16 hours agoprevThanks for the writeup. As a compiler dev, I'm interested in databases as of late. Is there a good introductory text on the field? reply alamb 16 hours agoparentCMU's database courses are online and excellent: https://15445.courses.cs.cmu.edu/spring2024/ https://15721.courses.cs.cmu.edu/spring2023/ reply nraynaud 13 hours agorootparentTo complete, the youtube link is here: https://www.youtube.com/playlist?list=PLSE8ODhjZXjYzlLMbX3cR... the advanced course is wonderful, it explains how everything is implemented. reply mattgreenrocks 14 hours agorootparentprevPerfect, will take a look. Thanks! reply rx987 16 hours agoparentprevdatabase internals by alex petrov comes to mind. reply mattgreenrocks 14 hours agorootparentThank you! reply cmrdporcupine 12 hours agoparentprevIf you're into languages and compilers and PL theory, you might really enjoy the \"Alice\" book (Foundations of Databases): http://webdam.inria.fr/Alice/ reply 62951413 16 hours agoprevIt's a pity calcite.apache.org has never been known for extensive documentation. reply luizsantana 16 hours agoprevAs a Haruki Murakami fan, I appreciate this title reply Jun8 16 hours agoparentThe title format \"What We Talk About When We Talk About X,\" which today perhaps would be called a meme started with a famous short story by Raymond Carver, part of his short story collection published in 1981 with the same title (https://en.wikipedia.org/wiki/What_We_Talk_About_When_We_Tal...). It's a great story, strongly recommend it. This format is now very widely used in blogs and other pieces, almost to the point of being overdone. TIL from (https://lithub.com/what-we-talk-about-when-we-talk-about-thi...) that Murakami \"asked Tess Gallagher, Carver’s widow, for permission to use the title form.\" for his memoir published in 2009. He's probably an important factor in the resurgence of the format. reply atombender 14 hours agorootparentAlso worth mentioning that the title was made up by Carver's editor Gordon Lish, who significantly altered the story (originally called Beginners). Lish made relatively extensive changes to many of Carver's stories, and is widely accepted as having improved them, though not everyone agrees. reply Jun8 5 hours agorootparentGreat point! Lish was a tight controller who was universally liked, to say the least. He’s generally insufferable (judge for yourself don’t this PM interview: https://www.theparisreview.org/interviews/6423/the-art-of-ed....) A sexier version of Maxwell Perkins with better hair, perhaps? Yet he did teach writing and edited well. reply shoelessone 11 hours agorootparentprevThank you for sharing. My mind went right to the Murakami book as well, so it's nice to have the additional information you provided. reply sfpotter 16 hours agorootparentprevWay, way past the point of being overdone! And I'm sure most of the people who keep overdoing it are referencing the Murakami book, rather than the great Carver short story. reply replwoacause 17 hours agoprev [–] Hmm, no credit to Haruki Murakami, who wrote the memoir of the title you yanked “What I Talk About When I Talk About Running”? Murakami credited Raymond Carver's collection of short stories called “What We Talk About When We Talk About Love” for inspiring his title. Might be nice for you to include a small mention. reply whateveracct 16 hours agoparentIt's pretty standard to not cite cute references like this. Nobody puts Dr Strangelove in their blog bibliography when they do a \"How I stopped worrying and learned to love X\" post. reply replwoacause 13 hours agorootparentThis is a good point. Maybe I missed the mark here. I'm not a writer, just a reader/appreciator so if my advice is bad from a writer's perspective then I hope the author ignores it altogether. reply whateveracct 12 hours agorootparentIt's fine either way tbh! reply leiysky 17 hours agoparentprevThanks for reminding, I will update the post later. By the way, the storytelling of Haruki Murakami influenced my a lot, I should have mentioned it when the title first occured to me. reply pvg 17 hours agorootparentYou can of course do what you want in your own writing but the GP's suggestion that you need to 'credit' or explain every allusion is not good writing advice - it would mean not having allusions at all. reply replwoacause 12 hours agorootparentprevI enjoyed the article and learned some things reading it. Re-reading my comment, it could have been worded better so I'm sorry about that. As I mentioned in another comment here, I'm not a writer, just an admirer of Murakami's, so if this suggestion contradicts what is common or best practice for authors I hope you'll ignore it. reply bigbillheck 14 hours agoparentprev [–] I think that being free to use, and expect your audience to get, basic references like this is just part of being in an educated society. reply replwoacause 13 hours agorootparent [–] It’s not about the “expectation to get it”. Disagree if you like, that’s fine, but my opinion is that I’d give a nod if it were me. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "This article delves into query optimizers and the design patterns and considerations of intermediate representation (IR) in relational databases.",
      "It explains how query optimizers translate SQL statements into execution plans and introduces an example of building a SQL IR using Rust programming language.",
      "The article explores the importance of property derivation for analyzing and optimizing the IR, the use of SQL for calculating properties, and the modifications required for relational algebra. It also discusses the benefits of using global identifiers in the IR and emphasizes the significance of sharing experiences in database development."
    ],
    "commentSummary": [
      "The thread revolves around query optimizers and the design of the intermediate representation (IR).",
      "The Query Graph Model (QGM) is critiqued, and alternative projects like Substrait, lingo-db.com, and Datafusion are suggested.",
      "The discussion touches on the optimizer in MySQL 8.0 and its reliance on relational algebra, and recommendations are shared for online database courses, a YouTube playlist, and a book on databases. The importance of citing references, including the inspiration from Haruki Murakami, is also highlighted, and the author agrees to update the post accordingly."
    ],
    "points": 184,
    "commentCount": 42,
    "retryCount": 0,
    "time": 1706539508
  },
  {
    "id": 39176899,
    "title": "Apple's Vision Pro AR Headset Faces Missing Apps Challenge",
    "originLink": "https://stratechery.com/2024/the-apple-vision-pros-missing-apps/",
    "originBody": "The Apple Vision Pro’s Missing Apps Posted onMonday, January 22, 2024Thursday, January 25, 2024 Author by Ben Thompson Om Malik has been observing, writing about, and investing in technology for going on three decades; that’s one reason I find his unabashed enthusiasm for the Apple Vision Pro to be notable. Malik wrote on his blog: Apple touts Vision Pro as a new canvas for productivity and a new way to play games. Maybe, maybe not. Just as the Apple Watch is primarily a health-related device that also does other things, including phone calls, text messages, and making payments. Similarly, the primary function for Vision Pro is ‘media’ — especially how we consume it on the go. Give it a few weeks, and more people will come to the same conclusion. In 2019, I wrote an essay about the future of television (screen): With that caveat, I think both, the big (TV) and biggest (movie theater) screens are going to go the way of the DVD. We could replace those with a singular, more personal screen — that will sit on our face. Yes, virtual reality headsets are essentially the television and theaters of the future. They aren’t good enough just yet — but can get better in the years to come as technologies to make the headsets improve. Apple has made that headset. Apple Vision Pro has ultra-high-resolution displays that deliver more pixels than a 4K TV for each eye. This gives you a screen that feels 100 feet wide with support for HDR content. The audio experience is just spectacular. In time, Apple’s marketing machine will push the simple message — for $3,500, you get a full-blown replacement for a reference-quality home theater, which would typically cost ten times as much and require you to live in a McMansion. Malik expounded on this point last week in a Stratechery Interview: But the thing is you actually have to be mobile-native to actually appreciate something like this. So if you’ve grown up watching a 75-inch screen television, you probably would not really appreciate it as much. But if you are like me who’s been watching iPad for ten-plus years as my main video consumption device, this is the obvious next step. If you live in Asia, like you live in Taiwan, people don’t have big homes, they don’t have 85-inch screen televisions. Plus, you have six, seven, eight people living in the same house, they don’t get screen time to watch things so they watch everything on their phone. I think you see that behavior and you see this is going to be the iPod. The iPod was a truly personal device, which was not only what people wanted, but also a great business: why sell one stereo to a household when you can sell an iPod to every individual? You can imagine Apple feeling the same about the long-term trajectory of the Vision Pro: why sell a TV that sits on the wall of the living room when you can sell every individual a TV of their own? You can be sure that Apple isn’t just marketing this device to people who live alone: the EyeSight feature only makes sense if you are wearing the Vision Pro around other people. I already commented about the dystopian nature of this vision when the Vision Pro was announced; for now I’m interested in the business aspects of this vision, and the iPod is a good place to start. The iPod and the Music Labels The iPod story actually starts with the Mac, and Apple’s vision of a “Digital Hub.” The company released iMovie in 1999, iDVD and iTunes two years later, and iPhoto a year after that. The release order is interesting: Apple thought that home movies would be the big new market for PCs, but the emergence of Napster in 1999 made it clear that music was a much more interesting market (digital cameras, meanwhile, were only just becoming a thing). That laid the groundwork for the iPod, which was released in the fall of 2001. I documented this history in Apple and the Oak Tree and noted: One of my favorite artifacts from the brief period between the introduction of iTunes and the release of the iPod was Apple’s “Rip. Mix. Burn.” advertising campaign. What is particularly amazing (that is, beyond the cringe-inducing television ad) is that Apple was arguably encouraging illegal behavior: it was likely legal to rip and probably legal to burn, presuming the CD that you made was for your own personal use. It certainly was not legal to share. The iPod was predicated on the reality of file-sharing as well: And yet, as much as “Rip. Mix. Burn.” may have walked the line of legality, the reality of iTunes — and the iPod that followed — was well on the other side of that line. Apple knew better than anyone that the iPod’s tagline — 1,000 songs in your pocket — was predicated on users having 1,000 digital songs, not via the laborious procedure of ripping legally purchased CDs, but rather via Napster and its progeny. By the spring of 2003 Apple had introduced the iTunes Music Store, a seamless and legal way to download DRM-protected digital music, but particularly in those early days the value of the iTunes Music Store to Apple was not so much that it was a selling point to consumers, but rather a means by which Apple could play dumb about how it was that its burgeoning number of iPod customers came to fill up their music libraries. That description of the iTunes Music Store is perhaps a touch cynical, but it is impossible to ignore the importance of music piracy in Apple’s original deal with the record labels. Apple was able to make a deal in part because it was offering the carrot of increased digital revenue, but it was certainly aided by the stick of piracy obliterating CD sales. Over the next few years the record labels would become increasingly resentful of Apple’s position in the market, but they certainly weren’t going anywhere; by 2008 iTunes was their biggest source of revenue, and it’s all but impossible for an ongoing business to give up revenue just because they think the arrangement under which they make that revenue is unfair. The App Store The iTunes Music Store does still exist, although its revenue contribution to the labels has long been eclipsed by streaming. It’s more important contribution to modern computing is that it provided the foundation for the App Store. The App Store didn’t exist when Apple launched its iPhone in 2007; Apple provided a suite of apps that made the iPhone more capable than anything else on the market, and assumed the web would take care of the rest. Developers, though, wanted to build apps; in September 2007 Iconfactory released Twitterific, a Twitter client that ran on jail-broken iPhone devices, and more apps followed. The following year Apple gave its eager developers what they wanted: an officially supported SDK and an App Store to distribute their apps, for free or for pay; in the case of the latter Apple would, just as it did with songs, keep 30% of the purchase price (and cover processing fees). This period of the App Store didn’t require any sticks: the capability of the iPhone was carrot enough, and, over the next few years, as the iPhone exploded in popularity, the market opportunity afforded by the App Store proved even more attractive. A better analogy to what Apple provided was gas for the fire, particularly with the release of in-app purchase capabilities in 2009. Now developers could offer free versions of their apps and convert consumers down the line, or sell consumables, a very profitable approach for games. That, though, is where App Store innovation stopped, at least for a while. By 2013, when I started Stratechery, I was wondering Why Doesn’t Apple Enable Sustainable Businesses on the App Store?, by which I meant trials, paid updates, and built-in subscription support. The latter (along with associated trials) finally showed up in 2016, but at that point developer frustration with the App Store had been growing right alongside Apple’s services revenues: productivity apps shared my concerns about sustainability, while “reader” apps like streaming services were frustrated that they couldn’t sign up new users in the app, or even point them to the web; game developers, meanwhile, hated giving away 30% of their revenue. It’s fair to note that an unacknowledged driver of much of this frustration was surely the fact that the app market matured from the heady days of the early App Store. No one is particularly worried about restrictions or missing capabilities or revenue shares when there is a landgrab for new users’ homescreens; by the end of the decade, though, mature businesses were locked in a zero sum game for user attention and dollars. In that environment the money Apple was taking, despite the fact the lack of flexibility entailed in terms of business model, was much more of an irritant; still, it’s all but impossible for an ongoing business to give up revenue just because they think the arrangement under which they make that revenue is unfair. The Epic Case I keep saying “all but impossible” because Epic is the exception that proved the rule: in August 2020 Epic updated Fortnite to include an alternative in-app purchase flow, was subsequently kicked out of the App Store by Apple, and proceeded to file an antitrust lawsuit against the iPhone maker. I documented this saga from beginning to end, including: Apple, Epic, and the App Store, which provided a history of the App Store and Epic’s lawsuit at the time it was filed. App Store Arguments, which I wrote at the conclusion of the trial, explained why I expected Epic to lose, even as I hoped that Apple would voluntarily make pro-developer changes in the App Store. The Apple v. Epic Decision, which reviewed the judge’s decision that favored Apple in 10 of the 11 counts. The 11th count that Epic prevailed on required Apple to allow developers to steer users to a website to make a purchase; while its implementation was delayed while both parties filed appeals, the lawsuit reached the end of the road last week when the Supreme Court denied certiorari. That meant that Apple had to allow steering, and the company did so in the most restrictive way possible: developers had to use an Apple-granted entitlement to put a link on one screen of their app, and pay Apple 27% of any conversions that happened on the developer’s website within 7 days of clicking said link. Many developers were outraged, but the company’s tactics were exactly what I expected: To that end, I wouldn’t be surprised if Apple does the same in this case: developers who steer users to their website may be required to provide auditable conversion numbers and give Apple 27%, and oh-by-the-way, they still have to include an in-app purchase flow (that costs 30% and includes payment processor fees and converts much better). In other words, nothing changes — unless it goes in the other direction: if Apple is going to go to the trouble to build out an auditing arm, then it could very well go after all of the revenue for everyone with an app in the App Store, whether they acquire a user through in-app purchase or not. The reason not to do so before was some combination of goodwill, questionable legality, and most importantly the sheer hassle of it all. At this point, though, it’s not clear if any of those will be deterrents going forward… Apple has shown, again and again and again, that it is only going to give up App Store revenue kicking-and-screaming; indeed, the company has actually gone the other way, particularly with its crackdown over the last few years on apps that only sold subscriptions on the web (and didn’t include an in-app purchase as well). This is who Apple is, at least when it comes to the App Store. The crackdown I’m referring to was pure stick: Apple refused to approve upgrades to SaaS apps that had been in the App Store for years unless they added in-app purchase; developers complained but this time the reality of it being impossible for an ongoing business to give up revenue meant they didn’t have any choice but to do extra work so that Apple could have a cut. Vision Pro’s Missing Apps The Apple Vision Pro started pre-sales last week, but the biggest surprise came via two stories from Bloomberg. First: Netflix Inc. isn’t planning to launch an app for Apple Inc.’s upcoming Vision Pro headset, marking a high-profile snub of the new technology by the world’s biggest video subscription service. Rather than designing a Vision Pro app — or even just supporting its existing iPad app on the platform — Netflix is essentially taking a pass. The company, which competes with Apple in streaming, said in a statement that users interested in watching its content on the device can do so from the web. Second: Google’s YouTube and Spotify Technology SA, the world’s most popular video and music services, are joining Netflix Inc. in steering clear of Apple Inc.’s upcoming mixed-reality headset. YouTube said in a statement Thursday that it isn’t planning to launch a new app for the Apple Vision Pro, nor will it allow its longstanding iPad application to work on the device — at least, for now. YouTube, like Netflix, is recommending that customers use a web browser if they want to see its content: “YouTube users will be able to use YouTube in Safari on the Vision Pro at launch.” Spotify also isn’t currently planning a new app for visionOS — the Vision Pro’s operating system — and doesn’t expect to enable its iPad app to run on the device when it launches, according to a person familiar with matter. But the music service will still likely work from a web browser. These are a big loss: Malik made the case about why the Vision Pro is the best TV ever, but it will launch without native access to the largest premium streaming service and the largest repository of online video period. I myself am very excited about the productivity use cases of the Vision Pro, which for me includes listening to music while I work; no Spotify makes that harder. There are, to be sure, valid business reasons for all three services to have not built a native app; the latest prediction from Apple supply chain analyst Ming-Chi Kuo put first-year sales at around 500,000 units, which as a tiny percentage of these services’ user bases may not be worth the investment. Apple’s solution, though, is to simply use a pre-existing iPad app; that all three companies declined to do even that is notable. Nebula CEO Dave Wiskus observed on X: 2003: Steve Jobs brings the big five record labels together in a landmark deal to sell their songs digitally for $0.99 each on the iTunes Store. 2024: Apple can’t convince streaming video companies to check the “allow iPad app” box. — Dave Wiskus (@dwiskus) January 19, 2024 The Apple Vision Pro app shelves will not be bare in terms of video content; the company says in a press release: Users will also be able to download and stream TV shows, films, sports, and more with apps from top streaming services, including Disney+, ESPN, NBA, MLB, PGA Tour, Max, Discovery+, Amazon Prime Video, Paramount+, Peacock, Pluto TV, Tubi, Fubo, Crunchyroll, Red Bull TV, IMAX, TikTok, and the 2023 App Store Award-winning MUBI. Users can also watch popular online and streaming video using Safari and other browsers. It’s not clear how many of these apps are truly native versus iPad apps with the Vision Pro check box, but the absence of Netflix and YouTube do stand out, and their absence is, without question, a total failure for Apple’s developer relations team. The blame, though, likely goes to the App Store: Apple has been making Netflix in particular jump through hoops for years when it comes to precisely what language the service can or cannot present to customers who can’t sign up in the app, and also can’t be directed to the web. The current version’s language is fairly anondyne (although it has been spicier in the past): Apple may be unhappy that Netflix viewers have to go to the Netflix website to watch the service on the Vision Pro (and thus can’t download shows for watching offline, like on a plane); Netflix might well point out that that going to the web is exactly what Apple makes Netflix customers do to sign up for the service. 1 Developers On Strike It’s certainly possible that I’m reading too much into these absences: maybe these three companies simply didn’t get enough Visions Pro to build a native app, and felt uncomfortable releasing their iPad versions without knowing how useful they would be. YouTube in particular, given that much of its usage is free, likely has less of a beef with Apple than Netflix or Spotify do, and it’s easy enough to believe that Google just isn’t a company that moves that fast these days. Still, there’s no question that the biggest beneficiary of these companies being on the Vision Pro — and, correspondingly, the biggest loser from their absence — is Apple. The company is launching an audacious and ambitious new product, and there are major partners in its ecosystem that aren’t interested in helping. This is the consequence of fashioning App Store policies as a stick: until there is a carrot of a massive user base, it’s hard to see why developers of any size would be particularly motivated to build experiences for the Vision Pro, which will make it that much more difficult to attract said massive user base. Apple was happy to remind users that, when it came to the iPhone, there’s an app for that; in the case of the Vision Pro, there may not be: this is the one and only chance for developers to go on strike without suffering an Epic-like fate, and some of them are taking it. For now, Apple appears to be so supply-constrained that it doesn’t matter; the company will likely sell as many units as it can make. I would guess that Apple’s strategy with regards to developer hold-outs will be to wait them out, trusting that it can sell enough devices that developers can’t go on strike forever. I certainly think this approach is more likely than offering any sort of concessions to developers, on any of its platforms. A Disney Double-Down? The other option may be an even greater investment in content by Apple itself. This could take the form of more Apple TV+ shows and sports deals like MLS, but the most interesting possibility is deepening its partnership with Disney. The entertainment giant is looking for a tech partner to invest in its ESPN streaming service, and the Vision Pro makes Apple a compelling candidate. From an Update last summer: What does seem notable was Iger’s call out of Apple’s headset; I can attest that the sports experience on the Vision Pro is extraordinary, and remember that Iger appeared on stage at the event to say that Disney would be working with Apple to bring content to the device; here is the sports portion of the video he played at WWDC: I have to say, one almost gets the impression that the Apple Vision sports-watching experience might have single-handedly convinced Iger to keep ESPN! What does seem likely is that Apple is probably Iger’s preferred partner, and there certainly is upside for Apple — probably more upside than any other tech company — primarily because of the Vision Pro. The single most important factor in the Vision Pro’s success will likely be how quickly entertainment is built for it, and as Cook noted while introducing Iger, “The Walt Disney Company is the world’s leader in entertainment.” I heard from a lot of people after that Update who were very skeptical that any sort of deal would be struck, in large part because Apple is so difficult to partner with (the company seems continually surprised that not everyone negotiates like the record labels under siege from Napster). And, it should be noted, Disney is showing up on Day One for the Vision Pro launch; why partner if the content is already there? And yet, Apple’s most potent response to ecosystem intransigence may be to double down: Disney with a war chest (via an Apple partnership) would be a far more formidable competitor to Netflix, and ESPN with a VR camera at every game it televises would, in my estimation, make the Vision Pro an essential purchase for every sports fan. I once argued that Apple Should Buy Netflix the last time the two companies were at odds, but the weakness in that argument is that simply having money another company needs isn’t a compelling enough case; when it comes to Disney the payoff is the Apple Vision Pro having that much more great content that much sooner, not only making the headset a success but also making it impossible for other streaming businesses to not serve their customers just because they think the arrangement under which they operate is unfair. There is an exception for Netflix specifically: if you download a Netflix game you can sign up with in-app purchase, which the company would almost certainly prefer not to offer but, thanks to Apple’s aforementioned crack-down on SaaS app sign-ups, requires. ↩ Share Facebook Twitter LinkedIn Email Related",
    "commentLink": "https://news.ycombinator.com/item?id=39176899",
    "commentBody": "The Apple Vision Pro’s missing apps (stratechery.com)183 points by mooreds 19 hours agohidepastfavorite432 comments idopmstuff 18 hours agoI think this whole thing is fairly overwrought - after all, the iPhone launched with zero third party apps (maybe there were a few, but I don't recall that being the case). Given the volume of VP sales, it feels more like a production-ready dev kit and tool for enthusiasts. It's really the version that people will use to build apps on, so that v2, v3, etc., which will be lighter/cheaper/whatever else is needed to make them more mass-market, won't have the same problem when they launch. If you're Netflix et al, why support the VP now? Even if it's just checking a box, it still means you'll get some support load. You'll also have to make some design and implementation decisions that will be less-informed than if you wait a while and see what other apps are doing on there. At the end of the day, is having Netflix on the VP going to generate any incremental revenue? Tough to imagine that everybody buying it doesn't already have a Netflix subscription. Building for it right now is zero upside and some downside. We'll see a Netflix app eventually, though. reply jandrese 18 hours agoparentThe iPhone was even worse, it launched without support for third party apps. Steve Jobs said that web apps would take their place. The original iPhone was also overpriced and underspecced in some areas and forced you on a crappy phone provider, but it had a significant price drop shortly after release and the iPhone 3G fixed most of the performance issues (mostly around the slow cell modem). Plus Apple released the app store and it was an enormous success, far more than the existing app stores on the Symbian phones. reply Retric 17 hours agorootparentIt forced you in a crappy phone provider with unlimited data. Having a better than average phone browser wasn’t such a big deal, having a browser you could use without thinking about it was huge. reply altcognito 16 hours agorootparentThe browser itself was ground breaking imho. Everything seemed to require a mobile specific browser and those versions of websites looked like garbage. The iPhone was (as far as I know) the first device that really and truly let you have real websites on your phone (even if it was just a tiny version of it) I had been waiting for such a device for a long time, and this was the thing that got me over to Apple in any real sense. reply Retric 16 hours agorootparentThere was a wide range of mobile browsers. From what I recall Windows Mobile 5/6 smart phone’s browsers were arguably better than the 1st gen iPhone. Though the phones where overall worse. reply 0x457 15 hours agorootparentAbsolutely not. Even without App store iPhone was significantly better than Windows Mobile 5/6. This is 5: https://en.wikipedia.org/wiki/Windows_Mobile?useskin=vector#... this is 6: https://en.wikipedia.org/wiki/Windows_Mobile_6.0?useskin=vec... Devices were much worse compared to even first gen iPhone. Ironically, it only for worse because the industry decided that using their shitty resistive touchscreen with UI even worse than previous models - menus had large buttons, but apps were built for stylus. reply pjmlp 3 hours agorootparentSymbian was quite good, the only thing was the strange Symbian C++ dialect, which wasn't something sorted out by iPhone until the store finally came to be. reply gandalfian 13 hours agorootparentprevI had an HTC TyTN II windows mobile 6 CE mobile phone. Basically same year as original iPhone. IE was rapidly abandoned and unusable. But with Gmail + activesynch it was like a blackberry with instant email. Opera mobile worked as well as modern browsers, probably better than iPhone. 3G. GPS. Load a multitude of apps self contained onto the huge SD card. It was a much more powerful capable device than an iPhone almost even up to today's standards. But it was clunky as hell and setting up the mobile internet was an insanely complicated process. I'm not suprised the iPhone won out. reply Retric 14 hours agorootparentprevI agree the devices were worse overall, however IE on Windows Mobile in 07 could render more websites well at the time including Flash support etc. The browser’s UI was also worse thus the ambiguity. reply 0x457 13 hours agorootparentOh, I remember what flash on a mobile looked like in 07. In fact, I remember how taxing flash was on PC in 07. I also remember how vulnerable flash was. Yeah, it could render more, but it was unusable because no one designed for mobile back then. IMO mobile world was sad until Apple came in. reply Hamuko 16 hours agorootparentprevYeah, the iPhone was the first phone where it felt like it was actually possible to browse the web on it. The Symbians I had previously didn't feel like usable browsers. reply mlindner 15 hours agorootparentprevFunny how now the internet is now regressing itself to be \"mobile-first\" and almost all websites have become \"websites that look like garbage\". Even on my phone I still often switch to the desktop versions of websites as they're easier to use. Let alone trying to use a mobile website on a desktop. reply matwood 17 hours agorootparentprev> having a browser you could use without thinking about it was huge This right here. People forget things like roaming and data charges. $500+ phone bills were not uncommon for business travelers. Data charges made smartphones almost a non-starter for the average person. Apple broke that logjam. reply pjmlp 3 hours agorootparentHardly in Europe, the contracts with data plans were mostly the same for iPhone, Blackberry, Symbian, Java feature phones, Windows CE/Pocket PC. Worse even, as while the others had pre paid plans, iPhone required a 2 year contract. reply myko 17 hours agorootparentprevand they weren't crappy everywhere, though admittedly they were extremely bad in Silicon Valley until iPhone opened up to other providers, so they get that reputation among tech enthusiasts from sites like HN that are SV-centric reply matwood 15 hours agorootparentIt was Edge, ie 2.5g. It was very slow everywhere it worked at all. But, as the poster said, internet all the time with a flat rate plan was huge. For me, it really changed how I thought about the internet when I had it always with me with zero cost concerns. reply Exoristos 16 hours agorootparentprevThey worked great in NYC. reply scarface_74 17 hours agorootparentprevIt was 2G. It was crappy everywhere. My dumb phone on Sprint had faster internet speeds reply mrcwinn 17 hours agorootparentprevUnderspeced? What? Compared to what? This is not even remotely defensible. In its spec it offered a full color multi-touch gesture display —instead of a 132x65 pixel black and white screen. Like Steve Jobs said, products are a series of choices. reply jandrese 15 hours agorootparentIt was EDGE and didn't have enough memory to run the browser well, which caused significant slowdowns. Granted, it was still better than literally every other phone browser at the time, but that didn't mean it was a good as it could have been. The iPhone 3G was a significant upgrade. reply simondotau 1 hour agorootparent> but that didn't mean it was a good as it could have been Nothing is, and nothing can be. Not even if you eliminate cost from the equation. Not even if you eliminate manufacturability, or supply chain availability. You're holding the original iPhone to an impossible standard. The iPhone was arguably the best product Apple could have made for that price, to be released on that date. Sure it could have been better and more expensive. It could have been better and further delayed. It could have been better and also worse in some other way. reply snowwrestler 17 hours agorootparentprevThe original iPhone had 2G cellular connection which was slow for the time, and especially silly since, as you point out, it was well powered otherwise. The iPhone 3GS caught it up to other smart phones in the market. It also launched without cut and paste available in the UI, which again, was silly given how it was otherwise an impressive mobile computer. reply ghaff 15 hours agorootparentWhile there were obviously people who bought the iPhone as soon as it became available, it was probably the 3GS when it really took off. That's when I replaced my Treo. (I also think I was starting a new and more secure job by then.) reply mvdtnz 8 hours agorootparentprevLots of Windows devices pre-dating the iPhone had comparable full colour touch screens. Its competition certainly wasn't the \"132x65 pixel black and white screen\". reply simondotau 56 minutes agorootparentLots of Windows devices pre-dating the iPhone had similar sized full colour screens. None had comparable screens, and Windows certainly didn't have an interface worth touching with a finger. reply FireBeyond 4 hours agorootparentprevThe N95 outspecced it in essentially every way: First available March 2007, three months before the iPhone was introduced. * Memory - 160MB, versus iPhone: 128MB. * CPU - Dual CPU, 332 MHz Texas Instruments OMAP 2420 (ARM11-based), versus Samsung 32-bit RISC ARM 1176JZ(F)-S v1.0 412 MHz * Display - iPhone: 320x480 18-bit 3.5\", versus N95: 240x320 24-bit, 2.6\" * Network ability: HSDPA 3.5G, versus iPhone: GSM/EDGE * Camera: 5MP, versus iPhone 2MP reply mongol 17 hours agorootparentprev> Steve Jobs said that web apps would take their place Yes I remember this. But he must have known their plans and basically lied about it. reply justinator 17 hours agorootparentSounds convenient, but I remember the backlash was so bad from this news, they rushed out a dev SDK to add that support in. Different time. \"No Flash\" sounded like a WTF, too. reply jdewerd 17 hours agorootparentYeah, \"webapps are all you need\" is the one time I can recall Steve Jobs getting booed on stage. reply scarface_74 17 hours agorootparentAnd now developers (not end users) are whining about not being able to do good [sic] PWAs Edit: Users may not know what a PWA is. But they do know what web apps based on Electron and I have never heard any user say that they love Electron apps and they would really love to have the same experience on mobile reply maxsilver 15 hours agorootparentTo be fair, they're only whining about PWAs, because Apple artificially locked down all native apps. Apple forced the developer shift, their own preferences didn't necessarily change. If you could install + sell iPhone apps freely, like you can with OSX / macOS apps, 99% of developer complaints about PWAs would vanish. reply scarface_74 14 hours agorootparentNo because the reason that developers want PWAs do that they can have the same shitty experience across all platforms - much like Electron apps reply DANmode 4 hours agorootparentThe developers on HN clamoring for PWA support are not the same devs making \"one layout works fine on all screen sizes\" webapps. I'm sad you're equating the two groups. reply dnissley 16 hours agorootparentprevI mean users do complain about not being able to sideload apps, which is an issue that is at least partially/minimally addressed through PWAs. reply threeseed 15 hours agorootparentI imagine users care far more about how bloated and slow Slack, Teams etc are. Rather than whether they can side-load some apps that don't already exist on the App Store. Which apart from gambling, porn and crypto I can't exactly think what they are. reply fauigerzigerk 14 hours agorootparentPorn is a significant share of device/internet usage though (not sure how big gambling is). reply threeseed 14 hours agorootparentAnd plenty of people watch porn on iPhones today. This is more about having specific apps for it. reply fauigerzigerk 14 hours agorootparentThis is about whether or not people might complain about not being able to side-load apps. As a significant content category is excluded from the app store, it seems very likely that some people would like to side-load those apps and complain about not being able to do so. [Edit] Another reason why people might want to side-load is to circumvent restrictions or censorship in authoritarian countries (e.g VPNs). reply scarface_74 13 hours agorootparentI really hate to ask this question because it may lead down a road that I don’t want to go. But what could a porn app do that you can’t do by going on a website? reply fauigerzigerk 12 hours agorootparentIf you can answer this question for Youtube and TikTok, you have already answered it for porn tube sites as well. Other categories would probably be erotic themed chat apps, social networks and of course games. And the reasons are just as valid or invalid as for the respective non-porn equivalents. As far as I'm concerned, almost all apps could be websites. reply scarface_74 9 hours agorootparentAnd what do you actually get from the YouTube app that you can’t do from the website especially now that Safari finally supports notifications from the web? The same with chat apps? I prefer using the Facebook website over the native app because the in app webview doesn’t support ad block. reply fauigerzigerk 2 hours agorootparentI often prefer webapps as well, for instance because I can open multiple pages to keep state I meant to come back to. But all the same reasons why some apps need native features and why some people prefer native apps are just as valid for porn. It’s not my personal preference. reply scarface_74 15 hours agorootparentprevHow many users outside of geeks complain about side loading? reply dnissley 12 hours agorootparentWhenever a family member bitches to me about not being able to buy kindle books through the kindle app I count that as a non-geeky complaint about sideloading :) reply nottorp 16 hours agorootparentprev> \"No Flash\" sounded like a WTF, too. No Flash saved a lot of power worldwide. Until people started to do javascript apps. reply ok123456 16 hours agorootparentprevHackers figured out how to put together a GCC toolchain and make their own apps, and that jumpstarted it. reply OnlyMortal 16 hours agorootparentI remember Cydia on the 2G iPhone. I was living in Belgium at the time so had to jailbreak a UK O2 iPhone. Worked fine for the time. reply ynx 13 hours agorootparentprevI worked with a couple of people who worked on the original iPhone and they basically corroborated this: the writing was on the wall but the phone wasn't quite ready for it. They needed an out to ship the phone first and the SDK later. reply lm411 14 hours agorootparentprevApple used to push web apps pretty hard. E.g. if I recall correctly, in the early days of the App Store, the guidelines were that if your app could easily be a web app, it should be, and you could have your app submission rejected for that reason. reply olliej 15 hours agorootparentprevThe plan was always web apps, they're vastly more secure than any other option. That's a significant part of why safari for windows ever existed. The problem was developers just wanted to write native apps, presumably believing that the only secure apple platform would be converted to the Mac-like experience, complete with malware, etc. So with no one making actual web apps, in part because the web standards of 15+ years ago were not as powerful, and in part because developers all thought web apps were bad, and ongoing \"we want real apps\" noise, you got the initial UIKit APIs, which lacked _a lot_ of functionality, and made a lot of things very hard. Or you can just go all conspiracy theory and say \"they would never change their plans in response to market pressure, who would do such a thing?\" reply jandrese 14 hours agorootparentAlso, although the original iPhone had a full browser, it was pretty slow. Not only was the data slow, but the phone itself was hamstrung by insufficient RAM which caused way too much swapping on complex webpages. Making a webapp that performed adequately was difficult to say the least. reply grumpyprole 12 hours agorootparentFurther evidence for this is that Apple ported their widgets, like stocks and weather, to native apps instead of using the web-based ones from OSX. reply idopmstuff 18 hours agorootparentprevIs that right? Man, did not recall that... funny how the state of apps today has colored my recollection. reply mh- 17 hours agorootparentMatches my recollection of it. iOS 3.0, which launched alongside the iPhone 3GS, is when it got copy-paste functionality. Apple has historically been very comfortable shipping devices without \"table stakes\" functionality - waiting until they have an implementation they like. Or sometimes never shipping it, and leaving it to third parties. My iPads still don't have a calculator app. reply izacus 15 hours agorootparentIt also kinda needs to be said that iPhone really took off after AppStore (and 3G and copy/paste and other stuff) became a thing. Before that the market share wasn't that great. reply threeseed 15 hours agorootparentThe market share wasn't great because it wasn't sold globally. Once that happened market share really took off. reply jdminhbg 14 hours agorootparentAnd in the markets it did exist, the carrier options were limited, usually to the worst carriers, because those were the carriers who were desperate enough to accept Apple's terms. reply rahoulb 17 hours agorootparentprevEven John Gruber (Daring Fireball) described it as a shit sandwich from Apple. reply mandeepj 17 hours agorootparentprev> Is that right? Absolutely! I also remember those chain of events exactly that way. They are also documented in Steve Jobs biography as well. reply SkyPuncher 17 hours agorootparentprevSure, but that was at a time when there weren't really apps reply CharlesW 17 hours agorootparentThere definitely were, but feature phone apps weren't great (for reasons involving hardware, app platforms, and carriers), and smartphone (Windows Mobile, Palm OS, Symbian) app distribution was mostly decentralized. Apple's innovation in the mobile app space was the App Store ecosystem. reply scarface_74 17 hours agorootparentprevThe original iPhone though did come with the YouTube app built in and it’s major use cases were according to Jobs were - a phone, an internet device and an iPod. The day the iPhone was introduced, the CEO of Google was on stage. One of the major use cases for the Vision Pro is video and there ars no native apps for the two most popular video sites. The day that the iPad was introduced. The CEO of Netflix was on stage. reply shawnc 16 hours agorootparentWasn't the CEO of Disney on stage when the Vision Pro was introduced/announced? From an IP standpoint having Disney on stage over Netflix seems like the better choice... reply jwells89 16 hours agorootparentprevI wish so much that YouTube would allow fully featured, non-hacky third party clients again, even if only for paid users (similar to how Spotify used to). The clean platform-convention-respecting straightforwardness of the Apple iOS YouTube app is missed. reply ribosometronome 15 hours agorootparentprevApple directly competes with Netflix, and to some extent Google, considerably more today than they did then. reply scarface_74 13 hours agorootparentAnd they compete with Disney in streaming. The fact is, Google doesn’t really compete with Apple. The amount of money that Google makes from Android, pales in comparison to what they make from YouTube. reply curiouscavalier 17 hours agoparentprevThe iPhone launched with a phone, email and an internet browser (not limited to the mobile internet) in your pocket. I think those were the killer apps, where each one had already proved their value in other form factors. I think part of the focus on apps is because while portable connectivity is an easier feature for most people to understand and appreciate (though I appreciate this may not have been quite as clear in 2007), wearability is less so, especially where the form factor is socially and ergonomically awkward. Working in AR/VR I know my team is constantly asking variations of \"how can we convince someone to put this on their face?\" My interpretation of pieces like this is often that they echo underlying concerns over the efficacy of use-cases, and decisions by players like Netflix to port their apps is some (albeit noisy and incomplete) data around it. reply pwthornton 16 hours agorootparentJust having a full Web browser in your pocket was huge. Also, a touchscreen iPod! That was another massive killer app. I used to carry around a phone, PDA, and an iPod. Day 1, Apple allowed me to carry one device. And then add in a great Web browser, and you can see why it was great even before 3rd party apps. I am not convinced Apple has come up with any killer 1st party apps for the Vision Pro, and that's concerning. reply pjmlp 3 hours agorootparentI was already doing that for quite some time with Symbian phones. reply cloogshicer 17 hours agorootparentprevAnd Maps. Don't forget about Maps. That was one of the killer apps too. reply aeturnum 15 hours agoparentprevI think this is somewhat overwrought, but I think the iphone comparison obscures more than it reveals. The Vision Pro isn't in a new hardware category. It's actually entering a somewhat crowded field of companies offering headsets of various kinds. The VP is different in a few ways of course, but it's not clear exactly what prevented previous offerings to get wide adoption. If this product is going to be \"successful\" (whatever that ends up meaning) you would expect something about the VP to diverge from the trajectory of previous offerings. So far, at least as far as apps go, it seems like it's doing worse. > v2, v3, etc., which will be lighter/cheaper/whatever else I understand what you're gesturing at here, but this isn't v1 at all. This would be the equivalent of entering the smart phone market in 2015. If Apple isn't able to offer a noticeably more compelling experience with this realease, it does not speak well for their overall vision. reply roughly 15 hours agorootparentAt the time of the iPhone release, Blackbirds were widely available, as were high-spec phones from Nokia and others. We don’t recognize them as smartphones today because the iPhone redefined the category, but at the time, we did. reply EduardoBautista 17 hours agoparentprevNetflix hasn't updated the Meta Quest's app in a while. The video quality is awful. Netflix is ignoring this market completely, it's not specific to the VP. reply pwthornton 16 hours agorootparentI used the Netflix app on my Meta Quest 3 and never tried it again. It's a bad app, the video quality isn't good, etc. Apple is going to have to prove to Netflix to support them (or build the app for them). If Apple sells several million of these and watching video is big on it, Netflix will change their tune. reply 1000100_1000101 17 hours agorootparentprevHaven't bothered with PSVR2 either, and have not updated the original PSVR app to the point where it no longer functions. Netflix is clearly saying \"We don't want to be a 3D/VR movie service\". Perhaps Apple will pay for 3D licenses and serve them as part of your AppleTV subscription, which would suck, because it would mean no 3D movie support on any other headset. reply taeric 15 hours agorootparentI'm curious what they have to do to support PSVR2? You can watch videos with the standard app, right? Is there a market for 3d movies? I'm intrigued... reply 1000100_1000101 12 hours agorootparentThis is a sore spot. On PSVR1, Sony's \"Media Player\" app's only VR option was to display 360 degree mono videos with head-looking. There was no option for stereoscopic playback. YouTube did add a PSVR capable player, but streamed the videos at a very low res. A 3rd party player, Littlstar was capable for playing various 3D video formats (full 360 degree, VR180, and 3D bluray style 3D video), but demanded a subscription, even for local files... It also only worked on one or two codecs with very specific encoding options... and the decoding was poorly optimized, so large videos (only a subset of which is visible at a time, so you need a high-res video), didn't really work. Not ideal. PSVR1 could support 3D blu rays. On PSVR2, Sony still has no built in VR video playing app at all. Littlstar has rebranded as Rad, and has been struggling to get playback working. They also plan to make their subscription based on payments via some proprietary blockchain crap. PS5/PSVR2 Sony dropped 3D BluRay support. It's not clear if it will ever return. reply taeric 10 hours agorootparentI am amused that I was completely unaware that there were 3d blurays. Any that are worth trying to get ahold of to play with? reply expensive_news 4 hours agorootparentIf you have a 3D capable setup Gravity is a must! Avatar and Life of Pi are also some favorites. There’s also a few gems and lesser known films that were released on 3D Blu-Ray like Hitchcock’s Dial M for Murder, which was shot in 3D but wasn’t originally released in the format because the 3D fad died by the time the it came out. They’re hard to watch because 3D TVs were so unsuccessful. You often need a special Blu-Ray player, and I don’t know of any streaming service that supports 3D. 3D TVs were too far ahead of their time. 3D works far better on 4K than on 1080 since each eye only gets half the image, (though the 4K 3D format doesn’t exist) so it would be nice to see it make a comeback on VR. While 4K and HDR are much better I still miss the occasional 3D film. reply 1000100_1000101 8 hours agorootparentprevI've really only tried Ready Player One. I've heard the Marvel ones are decent. Anything animated, like a Pixar should be good, since they can just re-render from the 2nd camera location for the other eye. Be aware 3D BluRays won't play without a 3D display, like a 3D TV, or a PSVR... For PSVR it will be lower res than ideal, and due to the BluRay player not using the camera for tracking the headset, will drift slowly to the side. (As I said, Sony's not been good for non-game PSVR support). Finding 3D BluRays is the trickier part. I've found it easier to order region free disks from amazon.co.uk than trying to find anything in Canada. reply taeric 7 hours agorootparentIt occurs to me that I did, in fact, know of 3d movies. I was thinking something more VR focused than 3d, for some reason. Still, I share the curiosity that the psvr2 doesn't support 3d movies, now. reply raydev 14 hours agoparentprevYou are underselling the original iPhone, it entered a very different cell phone market, where the same quality tech existed but was hard to find, and the UX on all smartphones was, to be generous, poor, especially for the mass market. Even beloved Blackberry was in need of improvements that RIM didn't even have on their roadmap until the iPhone showed up. Despite all its initial shortcomings, the iPhone still raised the bar for the entire market, set a new standard that every other manufacturer/developer would be chasing for years. The Vision Pro is being watched by the mass market and it will be competing against Meta for better or worse. Apple has put in zero effort into removing that appearance. They are not selling a devkit, they are advertising a mass market device at an absurdly high price. reply bradgessler 16 hours agoparentprevYeah, this reminds me of the Apple Watch launch. I think Twitter & Slack launched watch apps, but they performed poorly on v1 and they learned people really just want notifications on their watch, so they pulled their apps. Today we just have notifications and it’s completely fine. That and dev teams are running leaner these days, so it makes sense to wait-and-see for companies with content leverage like YouTube and Netflix. reply sleepybrett 12 hours agoparentprevWatch was the same way. They weren't generally available so any third party that wanted to make an app either had to have an in with Apple to get access to even a simulator under heavy NDA or just wait until the thing hit the streets. I think you'll see the same thing here. Of course in this case with the price point being where it is it will probably take a bit to explore the possibility space and get some killer apps that propel the platform. reply JumpCrisscross 16 hours agoparentprev> If you're Netflix et al, why support the VP now? It’s a nice beachhead they’re exposing. There might be a niche for someone to build an AI that converts flat video into immersive content, and then plant a bunch of patent land mines around it. reply asadotzler 10 hours agorootparentNetflix will have the patents and decide not to do anything with them. They're not idiots. Not making an app and not investing in R&D for future display formats are quite different. The first is reasonable but the second would be unacceptable for a strong video platform. reply JumpCrisscross 10 hours agorootparent> Netflix will have the patents and decide not to do anything with them. They're not idiots …they don’t. They should. Which is why I’m calling it a beachhead. reply manquer 17 hours agoparentprevNew sales is not only source of revenue Netflix sells plans for higher resolutions for example If some just have say a laptop screen and a phone you consume content on there is little need for higher resolution plans. However let's say you buy VP then it is possible you would consider upgrading to higher resolution. reply Kluggy 17 hours agoparentprevThe thing I don’t get is why Netflix won’t allow their iPad app to be used. They had to go out of their way to disable the platform from using their existing app. I get not building a new one, but reusing the existing one makes more sense imho. reply cloogshicer 17 hours agorootparentProbably because support volume still increases even if it's not \"officially\" supported. And then you can't just tell those people to go away, because they'll be pissed off. reply crazygringo 14 hours agorootparentprevFor whatever technical reasons, it might just not work as well as the website. E.g. maybe it will only display video on an emulated \"iPad screen\" whereas the website uses Safari's video player that might have many more VR-friendly playback features. reply pxeboot 16 hours agorootparentprev> why Netflix won’t allow their iPad app to be used Support costs? Bad PR when it doesn't work well? There are a lot of reasons they may not want it to be used yet. reply xu_ituairo 15 hours agorootparentprevMaybe if Vision Pro takes off they’ll charge a higher fee for it like they do for higher resolution subscriptions. reply evilduck 14 hours agorootparentIf they wanted to start producing 3D content and charging extra for that, I'd see the reason but I'd balk at them charging more for delivering the same 2D content to a new system. I wouldn't expect a Vision Pro app delivering a 2D content experience to be dramatically more expensive to build and maintain than their vast array of apps for set-top boxes, hotels, streaming sticks, or the Meta Quest they've already supported but not charged extra for. Why would they draw the line here? reply fauigerzigerk 14 hours agorootparentCompanies often charge as much as they think customers will pay, regardless of costs. Apple's RAM upgrade prices are a prime example. But I doubt that Netflix would do it on this occasion. reply Someone 17 hours agoparentprev> the iPhone launched with zero third party apps (maybe there were a few, but I don't recall that being the case). I think Google Maps was the only one, sort-of. It shipped with the OS, but used Google’s data, and, I expect, needed Apple to negotiate with Google. reply georgeecollins 15 hours agoparentprevI completely disagree. There are a bunch of mixed reality headsets and I think the one with the best apps will win. Good news for APPL, META is messing up their store as well. https://www.reuters.com/legal/transactional/meta-platforms-s... Disclosure: I own APPL / META stock. reply samstave 17 hours agoparentprevI am sure there are plenty of companies we are not aware of who have been building apps for the platform in partner with Apple Marketing Dollars for some time. If Autodesk and Blender don't have a whole team pointed at this, I would be highly disappointment. reply phreeza 17 hours agoparentprevThe situation is completely different from the iPhone launch though, 99% of mobile devices at the time probably had zero or at most one crappy Symbian app installed. The concept of an App store wasn't even really invented yet. reply pjmlp 3 hours agorootparentIt was available via each mobile operator, and app listings on magazines, which would provide a download link as reply to a SMS with app code. reply screye 17 hours agoparentprevThe Vision Pro is Apple selling a combination of a tech demo, fashion item and dev kit. It isn't meant to be useful. Rather, it gives them bragging rights for being first to market (in AR) and they now control the narrative around it. reply Nullabillity 17 hours agorootparent> Rather, it gives them bragging rights for being first to market (in AR) and they now control the narrative around it. Magic Leap says hello. reply ftio 17 hours agorootparentMagic what? Nobody outside of a very small, niche bubble has any idea what that is. I'm in the bubble and had to look it up. reply OJFord 15 hours agorootparentI think Google Glass would've been a better example. Magic Leap doesn't seem to be targeting the same market at all. Fwiw though AVP had somehow passed me by (I think I'd heard of it, but didn't know what it was) until this submission. I'm not sure the general awareness is that high, outside of people that follow Apple news/WWDC/product launches/etc., which is also a bubble. reply sleepybrett 12 hours agorootparentGoogle glass was the most anemic product i've even experienced. reply OJFord 7 hours agorootparentI'm not really sure what you mean by that, but I don't think it's relevant - it clearly wasn't exactly a success, but it did come first, and it was the same kind of idea, what was possible at the time. reply sleepybrett 4 hours agorootparentthe constraints they put on it , specifically fitting all of it into a normalish looking pair of glasses, meant that they really weren't going to be able to give it enough functionality to actually make anyone .. want it. reply strict9 18 hours agoprevMuch of this discussion is centered on how the headset will replace theaters and TV. I may be an outlier but in our house the only time we watch shows or movies is a social context--usually Friday or Saturday nights as a family. I'm struggling to see how the headset could replace what is in my experience a mostly social activity. Will it ever be comfortable enough for the binge watcher to watch it all day? I have my doubts. I think it will be a coexistence and replacement of flat screens will be a very long time away. reply jillesvangurp 18 hours agoparentThere are lots of people living by themselves or at least spending a lot of time consuming content by themselves. As for comfort wearing this for extended periods of time, we'll have to see. I think that will be the key deciding factor. There was a lot of marketing speak in the original announcement suggesting that they did work quite hard on this issue and made some progress. A light enough device that can provide the experience of having a ginormous screen in front of your nose without inducing headaches, motion sickness, etc. could be a nice thing to have. If somebody delivers such a thing, people will be reluctant to turn them off possibly. As for replacing existing things, there's a long history of people thinking about new products in terms of how the old one worked. The more interesting question to ask is what new content will emerge for this thing. reply sho_hn 18 hours agorootparent> There are lots of people living by themselves or at least spending a lot of time consuming content by themselves. Is this great? It's a market, maybe, but it is it one we want to incentivize and grow? Increasingly I feel like we, as an industry, lean too hard into providing lesser-resistance substitutes for mentally and socially healthier lifestyles, and that \"we are not making the choice for them, the root problem has to be solved somewhere else\" is a cop-out. The massive amount of effort we spend on boyfriend/girlfriend replacement tech, meet-up-with-friends replacement tech, experience-interesting-places replacement tech is starting to worry me. reply crazygringo 14 hours agorootparent> but it is it one we want to incentivize and grow? Why on earth not? Reading books is solitary. Programming is usually solitary. A lot of hobbies are solitary. But you can watch a show on your own and then talk about it the next day with your friends. reply notamy 17 hours agorootparentprevIt’s depressing. It feels like we keep making technological solutions to maximise profit at the expense of people’s mental/emotional health. reply LoganDark 17 hours agorootparentThat's exactly what they're doing. And as a side effect, everyone wanting to maximize profit means that everything costs money, and that means everyone wants money, and that means money is hard to get! At least, it's hard to get for people who don't happen to already be at the top of some empire. reply jmull 15 hours agorootparentprevWhy assume people spending a lot of time consuming content by themselves is bad? It can certainly be done at an unhealthy level but you can say that about a lot of things. If it is bad, then you'd have to discourage things like reading books. And I guess, depending on what you think is wrong with it, maybe discourage other mainly solitary pursuits. reply jillesvangurp 13 hours agorootparentprevWho is the royal \"we\" here? You don't have to buy this thing if you don't want to. Why are you judging others that might decide otherwise? I think it's very simple. If this thing works more or less as advertised, lots of people might buy it. I don't think it's a given Apple has another winner here but I wouldn't exclude the possibility. And I was kind of impressed with what they announced last year. reply pwthornton 18 hours agoparentprevIf you live with other people, the appeal is limited. I can see the appeal for my friends who live alone in small apartments. They can get this and a nice pair of headphones and have a great home theater experience. This is something that would be difficult to do in their current situations. I also see the appeal for people who travel a lot for work. This is probably an amazing way to watch movies on a plane or in your hotel room at night. I have a wife and two small kids, so I will not be buying this to watch movies at home. reply hydroxideOH- 18 hours agorootparentAre there a lot of people who live alone in a small apartment that are willing and able to buy a $3500 headset? I feel like this entire thread has a blind spot to the fact that this device is very much a luxury item. A 55-inch TV is ~$300. A high-end laptop is ~$1000. High-end noise-cancelling headphones are ~$250. You could buy all of those and still not reach half of the cost of this device. The only people I can see buying the device are rich people looking for another toy, not as a serious competitor to other entertainment tech. reply DetroitThrow 17 hours agorootparent>The only people I can see buying the device are rich people looking for another toy, not as a serious competitor to other entertainment tech. I'm surprised at how consistently people think that a technology that is so expensive and serving such a niche will end up having adoption asides from a few wealthy enthusiasts. iPhone 2 suggested retail price was $300 (~$425 in todays dollars) and provided \"smart\" replacement for your cell phone matching its features 1-to-1 while also providing more than what was available. If someone imagines themselves buying this to watch movies \"on the go\" or at hotels or something, they're part of an extremely exclusive club. reply ac29 16 hours agorootparent> iPhone 2 suggested retail price was $300 (~$425 in todays dollars) This was back when most phones were still carrier subsidized and required long term service contracts. Per Apple's press release, the $299 pricing required \"a new two year contract with AT&T\". Unsubsidized price was about double. https://www.apple.com/newsroom/2008/06/09Apple-Introduces-th... reply ok123456 16 hours agorootparentprevIt's 10x the cost of an entry Oculus device. And no one wants that either. reply pwthornton 16 hours agorootparentMeta Quest's are basically video game devices. It's a much different market than AVP. Meta is selling hundreds of thousands of units a month, so I don't know if I'd say no one wants it either. It seems to be selling pretty well overall, but Meta way overinvested in some of the stuff and is having a hard time making enough money. reply ok123456 16 hours agorootparentThey're both primarily for media consumption. Just because one device is from Apple doesn't make media consumption any more virtuous or productive. reply LordDragonfang 16 hours agorootparentprevThe Quest 2 has sold 5-10x as many units as the first iphone did. Apples to oranges, but that's not no one. reply threeseed 15 hours agorootparentprev> $3500 headset? Nobody is buying the Pro version of the headset just for watching movies other than the early adopters, YouTubers etc we have today. The idea is that when a normal version launches for $999 it will be a far more compelling proposition. reply pwthornton 17 hours agorootparentprevSure, there are in any of the global cities. I'm in the DC area, so I know plenty of people who have money and live in apartments/condos. NYC is the same way. There are plenty of people in the Bay Area that this describes as well. And then you talk about Western Europe and Asia, and home theater setups are a lot less common. Even if you have a lot of space in your apartment, it's hard to justify much of a home theater setup, as you will be really limited by sound issues. reply jimbokun 17 hours agorootparentprevThe price will come down. This version is for wealthy early adopters. reply kllrnohj 18 hours agorootparentprevAt $3500 though you can easily put together a decent-to-great 75-80\" home theater system with the latest gaming console(s) and have plenty of money left over for multiple rounds of pizza & beer with friends over. 77\" OLED TVs from Samsung & LG are only around the $2000 mark these days, after all. Compromise even just a smidge to a full array local dimming mini-LED QLED TV and you're now down to under $1k for the TV. Surround-sound soundbars that sound fantastic are plentiful below the $1k mark as well and require next to zero setup. It seems like you need to live alone and be rich enough to afford it such that you likely already have a great home theater anyway and still be interested in the Vision Pro. Or you actually don't want a TV at all for some reason but still want a home theater experience. That seems like a quite small market? reply filoleg 17 hours agorootparentI can afford a 75-80” home theater system, but I cannot afford an apartment in NYC large enough to justify buying such a system. Most people who rent also move at least every few years, so having that large of a thing to move is a bit annoying. Not even mentioning people who travel for work and obviously cannot bring such a system with them. It isn’t just all about the money when it comes to device purchases. reply Kirby64 17 hours agorootparentPresumably in a smaller apartment setting, you don't have any need for such a large TV (or sound system). It should scale appropriately and provide the similar experience at an even cheaper price. A 48\" OLED TV, for instance, is much much easier to move. reply bfeynman 18 hours agorootparentprevI don't think those are really comparable. Spending a ton of money on a home theater is a large and static purchase. You put it in your home, and that's it, you can't move it easily or adapt it much. The Vision Pro, on top of having other functionalities, lets you have that home theater experience anywhere. Want o watch in bed? on the couch? outside on porch? waiting at the airport? People spend a lot more on phones because of the portability aspect. reply LoganDark 17 hours agorootparentAnother HN commenter[0] speculated that the R1 chip could be part of the path towards an Apple self-driving car. Since they're no longer trying to do level 5 FSD, I think one of the absolute coolest things could be using the Vision Pro to see straight through the car into what would otherwise be blind spots. Obviously there are implications to wearing Vision Pro while driving, especially since if the device loses power you will entirely lose the ability to see. The idea still fascinates me though. [0]: https://news.ycombinator.com/item?id=39108792 reply brandall10 17 hours agorootparentprevSomething I've noticed in the past few years is the number of homes I've been in without a TV and the use of smart speakers placed throughout instead of a fixed stereo/surround system. Seems quite a few folks are opting to randomly binge-stream on their laptop as vegging out on the TV is a thing of the past for them, so no need to have their living space dominated by these types of electronics. reply kllrnohj 17 hours agorootparentAre those folks going to suddenly find it appealing to strap on a headset with a battery pack for $3,500 but don't want to try that out today for $500? The appeal of the laptop/tablet/phone veg is the speed at which you can plop and start watching. Futzing with VR headset is kinda not that. The \"personal home theater!\" angle has been tried, and failed, so many times now. It doesn't seem like the Vision Pro changes anything to suddenly make it a motivating feature on its own. Which I think is the more telling thing about Netflix's response here. They've been pushing VR movie watching since day 1. And yet now they aren't doing so for the AVP. That seems less likely to be a snub of Apple and more likely to be an admission of \"this just isn't something people do or want currently\" reply brandall10 15 hours agorootparentIt's too early right now and I feel the device needs to make strides for it to be viable for most. The various youtubers pointing out neck strain issues underscore this. We can't gauge much from early gripes on a low production device that hasn't launched yet. Netflix's engineering staff is expensive and I imagine they learned some interesting lessons from experiments like Bandersnatch. No need to make bets for the sake of innovation, let things play out first. I'm sure there is some signaling involved for the sake of investors showing they're prudent here. All I'm saying is - to your point - the market may be shifting to where personal consumption devices like this make sense. It's not replacing a TV so much as a laptop. For some early adopters the experience may be far superior, and if so, a next generation device that cuts the cost and size in half may be the thing that launches things forward. Like the iPhone 3G. Or it could be sort of a wash and go the way of the 3d TV craze from 15 years ago. reply pwthornton 17 hours agorootparentprevThe $500 headset can't really do this well. They may have demoed one in a store and realized this isn't a great experience. You will also not find a lot of people online recommending it either. I have a Quest 3, and I would much rather watch movies on a laptop or iPad than it. The screen quality is not great. You can see the pixels due to the low-ish resolution. The contrast ratio is not great. The screen quality is generally good enough for VR games and other VR-specific experiences, however. reply simmerup 18 hours agorootparentprevYou’re forgetting just how much space that all takes up reply kllrnohj 18 hours agorootparentIt takes a wall. Most people that can afford a $3,500 VR toy usually have at least one wall - often at least 4 of them, even! reply pwthornton 16 hours agorootparentA lot of homes are not built with an obvious spot for a TV, especially a big one. Hence all of the people sticking TVs over their fireplaces (which is way too high for proper viewing). reply macintux 15 hours agorootparentprevA wall. And power. And furniture to hold the 50 cables and 5 auxiliary devices that go with the TV. And a place to comfortably sit that works with all of the above. reply FemmeAndroid 18 hours agorootparentprevI'm hoping there is a better selling point than a really good home theater for single people. When I was single, having a TV and couch was valuable so that even though they were 95% used alone, they could be shared with guests. If I was into media enough to spend $3,500 to have a great experience, it would be a bummer if I couldn't watch a movie with someone else occasionally. reply solardev 18 hours agorootparentYou can just share the headset and each look into one eye. reply crazygringo 18 hours agorootparentprevI mean, you can still have the TV to watch things with guests. Or if you're both into the big screen VR experience, you can both wear headsets. Watching movies in a shared virtual movie theater is already a thing and works great. Even better is that you can be in your home and your romantic partner can be in their home (or traveling) and you can still watch together. reply jacksontheel 16 hours agorootparentIf I'm watching a movie with my romantic partner, I'd like to be in the same room as them lol. Not really interested in a VR Chat relationship. reply Philpax 16 hours agorootparentYes, that's preferable when it's an option - unfortunately, that's not always possible, especially when life intervenes. Wouldn't you still want a way to share a space with your partner and watch a movie with them? reply simiones 17 hours agorootparentprev> Or if you're both into the big screen VR experience, you can both wear headsets. For the low low price of 7000$, vs 1000$ for a TV. reply crazygringo 15 hours agorootparentI mean, right now it's for the low low price of $500 for a pair of Oculus Quest 2's. Just as big screen, but you'll effectively get 1080p quality on the virtual screen rather than 4K. Obviously for the Vision we'll wait for prices to come down. reply worldsayshi 18 hours agorootparentprevI'm somewhat of a VR enthusiast. But my headset is mostly gathering dust. There's one use case that I do think I would actually use once it's good enough though. It's using glasses as replacement for computer monitors when doing work. But I'd prefer the Nreal/Xreal Air form factor and price point. My inner VR enthusiast thinks the Apple Vision is cool but my inner realist wonders if anyone is really going to use anything beyond smart glasses. reply filoleg 17 hours agorootparent> My inner VR enthusiast thinks the Apple Vision is cool but my inner realist wonders if anyone is really going to use anything beyond smart glasses. My personal prediction - smart glasses as a smartphone replacement, AR/VR headsets as a poweruser workstation machine replacement. The market segments map out nicely with your prediction as well - almost everyone has a smartphone of some kind (just like i expect almost everyone to have some sort of smart glasses in the future), while a relative minority (even though it is a large one) has poweruser workstation machines (just like with AR/VR headsets in my prediction). That is, until at least the tech gets insane enough to allow packing full functionality of an AR/VR headset into the form factor of glasses, with an insane battery life to boot. I don’t see that happening in any foreseeable future though, sadly, barring some transformational and unexpected battery chemistry breakthroughs. reply sho_hn 17 hours agorootparent> My personal prediction - smart glasses as a smartphone replacement I still struggle with seeing smart glasses as a viable smartphone replacement unless paired with some sort of peripheral to perform input privately. Doing everything by voice or expressive gestures around others isn't going to work for people. reply zonkerdonker 17 hours agorootparentWrist wearables that can track micro-muscle movements in your fingers (pinching, scrolling, etc) are in development as a pair to these devices reply filoleg 17 hours agorootparentprevThat’s a really good point I totally forgot about. I would expect it to be controlled by a combo of gestures using eye tracking + some auxiliary input device, like a ring or a smartwatch or something like that. I agree, for now we have no good or even barely established UX/HCI paradigms for hypothetical standalone AR glasses. Not that we even have those types of paradigms established for currently existing AR/VR devices, but we are getting there slowly. With each year since I first tried the original HTC Vive, every new device and update slowly but surely made the interactions better, simpler, and feel more “worked out”. What gives me hope is seeing how the touch-only UIs have changed since the original iPhone release. At first, everyone was scoffing big time at touch-only interfaces ever becoming functional, viable, and widely used. The first third party apps on the App Store were also extremely disjointed and had almost nothing in common between each other in terms of UI/UX. Felt like everything was just spliced together and stamped with “we think this should work.” Not casting shade at devs of those apps, everyone was in that position back then, as there were no established UI/UX for touch-only interface smartphones. In 2024? While there are still continuing changes, things slowed down overall as the general cohesive UI/UX principles for touch-only smartphones have been established. And they indeed became functional, viable, and widely used devices. reply proamdev123 17 hours agorootparentprevAgree, but it’s trivial to add support for bluetooth controllers or keyboards. reply filoleg 17 hours agorootparentThis isn’t for in-home use, I believe they were talking about use cases similar to using smartphones outside of home. I am not pulling out a bluetooth keyboard out of my pocket on the street when I need to navigate using GPS or look something up. Btw, pretty much every AR/VR headset I am aware of these days already supports bluetooth controllers and keyboards. For some keyboard models, you can even have them visible and physically tracked in your VR space (I tried it with Quest 2 and apple’s wireless keyboard, worked like a charm). reply ImPostingOnHN 17 hours agorootparentprevPupil tracking is in consumer VR devices, I can see it being further miniaturized, especially with advances in waveguiding. In fact, this might be a great use for Zeiss's holocam tech [0]: high resolution, low definition, grayscale \"window\" that waveguides some of the light passing through, down to one of the edges, where an image sensor picks it up and decides it. 0: https://news.ycombinator.com/item?id=38881981 reply worldsayshi 14 hours agorootparentprev> That is, until at least the tech gets insane enough to allow packing full functionality of an AR/VR headset into the form factor of glasses, with an insane battery life to boot. I don’t see that happening in any foreseeable future though, sadly, barring some transformational and unexpected battery chemistry breakthroughs. It's not that far fetched if you move most of the hardware to a fanny pack or similar. You can probably get pretty close with current smart glasses (or Bigscreen Beyond) and a (next gen) Steam Deck. reply nradov 17 hours agorootparentprevVR glasses aren't very practical if much of your work consists of Zoom meetings with webcams. reply dylan604 17 hours agorootparentprevIf you live with other people, it could still be appealing. Not everyone wants to watch the same thing at the same time. Also, living together does not mean everyone is family. reply lm28469 17 hours agorootparentprev> They can get this and a nice pair of headphones and have a great home theater experience. This is something that would be difficult to do in their current situations. For that price you can get 3 of your friends a nice 1080p short throw projector and studio monitors which will outlive the vision pro and won't cause neck/head pain after an hour. reply makeitdouble 18 hours agorootparentprevThis is indeed a great use case for people living alone. But then they they'll also get way cheaper options if it's just to watch streaming services. I'd assume devices like the xreal are lighter and cheaper, and the Vision Pro's extra resolution is less an appeal if it's just to watch Netflix. reply whstl 18 hours agorootparentprevPutting on my doomer glasses: with the so-called loneliness epidemic and the rising prices of property (forcing people to co-habit with non-family/friends), I can definitely see something like this becoming as popular and perhaps as necessary as mobile phones. reply unusualmonkey 16 hours agorootparentprevThe other problem here is that the is very little moat here - all you need is a good screen and decent headphones, both of which are commodity. The AVP is vastly overpriced and overspecced for basic media consumption. reply sunnybeetroot 12 hours agorootparentMoat like water around a castle? reply raydev 12 hours agorootparentprev> have a great home theater experience We'll have to see how people handle 1.5lbs of headgear for long periods. Maybe it'll be fine, but the Quest is already heavy enough. reply Remedwme 18 hours agorootparentprevWe have a projector at home. Still my wife watches her stuff on a phone. People actually don't care about quality more often than not. But the quest 3 should be cheap enough already for this and good enough and still is not the mass market reply greedo 16 hours agorootparentWe too have a nice 4k projector. Wife prefers to watch in the living room on an old LCD. She doesn't even mind watching DVDs (no upscaling on this TV). That makes my eyes bleed, but she doesn't care one bit. reply jayd16 17 hours agorootparentprevQuest 3 doesn't get you a 4k image and you can get a 4k Tv for less than the Quest. It's almost just sharp enough for text but not quite. I don't think you can really make the claim that the Quest 3 is good enough. Probably the Quest 4 though. reply nsxwolf 17 hours agorootparentIt provides immersion. Your brain eventually filters out what little bit of screen door effect there is when watching a movie. Sometimes when I go to the movie theater I forget to bring my glasses, but I still have a good time. I'd be surprised if anyone thought the Quest 3 didn't render sharp text. I find everything very readable. reply nsxwolf 17 hours agorootparentprevI am curious to see the image quality of the VP, because the Quest 3 already does a pretty good job as a home theater replacement for $500. reply jayd16 17 hours agorootparentprevWith the AVP we can actually have the game on while the wife and kids control the living room TV. reply trjordan 18 hours agoparentprevI don't think you're an outlier, but on the other side, \"I watch Netflix on my laptop / iPad\" is a _huge_ market. Yes, there's plenty of social watching experiences to be figured out, but solo watching is an entire industry unto itself. I'm a bit like you. I don't watch TV by myself, and the idea of plugging into a VR device to do so seems weird. But I carry my AirPods everywhere and mostly listen to music solo. It's not unreasonable to think that the same happens with TV/movies. reply 0x457 14 hours agorootparentI have a 75\" OLED in the living room where I watch things while wearing AirPods Max. Image quality and sound is superb, but I also often watch movies on my iPad Pro in the bedroom (no place to put tv there). I still get Dolby Atmos and Dolby Vision on both devices and with iPad being much closer than TV screen size is alright. People that watch movies on their phones and smaller ipads are wild. I tried watching movies on Quest 2 - too bulky. reply curiouscavalier 18 hours agoparentprevI'm in the same boat. The idea of everyone watching TV in isolation defeats the purpose for me. Even (maybe especially) for sports, where for large swaths of a game it is essentially background content to a cultural gathering. Not to mention issues around comfort and battery life. reply agumonkey 18 hours agorootparentSoon they'll add a `presence` feature so that you can see others watching stuff at the same time too reply nixgeek 18 hours agorootparentIt kinda already exists via SharePlay but who knows if the Vision Pro supports that today or will in the future. reply kemayo 17 hours agoparentprevPerhaps worth considering that the Nintendo Switch is the best-selling game console of the last several console generations (#3 all-time!), and a core part of its appeal is that you don't have to take over the TV when you're using it. In fact, it really shines as a \"people are watching TV, but I want to do something else\" device. (Particularly for kids, of course, which isn't a market that works well with the $4k price of the current Vision Pro. But a hypothetical cheaper future generation...) I'll admit that some of my perspective here comes from being extremely not a TV-watcher, so \"the social joy of watching a show\" mostly fails to motivate me. :D reply jandrese 17 hours agoparentprevI'm not sure what the killer app for this headset will be, but watching movies/tv shows on it is not it. The most obvious reason being that a passive movie/tv watching experience is already possible on a much cheaper Quest headset and it has not been a success. For me a killer app needs to make use of the AR capabilities of the headset to justify the cost. reply prng2021 18 hours agoparentprevI agree with it coexisting with a flat screen. We go movie theaters with friends and family and watch in silence. Simply having people around you rather than being alone seems to be enough. I think VR will be something in between. In your situation, you’re not planning a night out but you’re also not just flipping on the tv because you’re bored. You have a family ritual to watch a show/movie at a designated time. I do something similar and would be fine with putting these on if it makes watching something more immersive. reply crazygringo 18 hours agoparentprevOf course it's not going to replace your family's shared viewing experience. But a lot of people watch a lot of TV solo. And headsets are actually extremely comfortable for binge-watching because you can lie back in your reclining chair or lie totally flat on your bed or long couch. You just drag the virtual screen up to your ceiling. You can also loosen the headband in these cases. There's no reason to sit upright for viewing unless you want to. reply criddell 18 hours agoparentprevI don't think anybody believes it will generally replace theaters and TV, just that it will carve out some portion of that. I think it's the same as how in some contexts, watching Netflix on your phone makes a lot of sense. You wouldn't do it on a Friday night with your family, but it isn't absurd to think some TV and movies are watched on a phone screen. reply strict9 18 hours agorootparentThat's the vision I see. But I was specifically responding to the article which opens with this quote by Om Malik: >With that caveat, I think both, the big (TV) and biggest (movie theater) screens are going to go the way of the DVD. We could replace those with a singular, more personal screen — that will sit on our face. Yes, virtual reality headsets are essentially the television and theaters of the future. And the rest of the article lays out a mostly supporting case with the missing apps argument. reply criddell 17 hours agorootparentI'm a fan of Om's work, but I don't really believe that's what he really thinks. He also says he doesn't understand why people own televisions. Om has been on many hours of video podcasts (TWiT). Apparently he understands why somebody would watch a podcast but not why they would own a television? I don't buy it. reply rickdeckard 17 hours agorootparentprevI think that describes the dream-scenario of Apple: Making a controlled AR/VR in front of your eyes so ubiquitous that not only TV-sales and cinemas disappear, but companies no longer advertise on screens and billboards and instead pay Apple to rent advertising real-estate on \"personal screens\" of their target-group... The scary thing is that noone is better equipped to achieve this dystopian goal than Apple, already entering the space with a fully protected walled garden... If I'd be Netflix (or even Disney), I wouldn't rush to support such a future... reply FireBeyond 17 hours agorootparentprevFamily movie night being replaced by everyone sitting plugged in to their headsets sounds very dystopian, something out of Snow Crash. And that’s after you buy a VP for you, your partner and your 2.2 children… only $14,000! reply dwaite 17 hours agoparentprevI think if the price comes down there will be a low double-digit percentage of people who have a Vision Pro and no television. This would not be too surprising based on the portion of people who do not own a TV today, watching media on laptop, tablet or desktop computers or on their phone. I suspect there will be a larger portion of people who have both a Vision Pro and a television, and use the Vision Pro for some portion of their viewing. The \"replaces TV\" is likely a mental connection between the cost of a higher end TV and the cost of a higher end AR headset being in a similar range where media consumption is a major feature of both. One could maybe justify the price of a 85\" OLED or justify the price of a Vision Pro, but not both. reply duped 18 hours agoparentprevI used to work in the space about 6-7 years ago (not directly on XR hardware, but software that was adjacent to it in the content creation world). The phrase I kept hearing from film studio/director types was that they didn't know how to tell a story with VR that could only be told with VR (profitably). Note that didn't stop them from trying and there wasn't a shortage of ideas. The point was that they hadn't figured out what would actually resonate with audiences (and make more money than doing what they already knew). But things are different now. Back then, the problem was that VR in your home was a dead-end. What seemed promising was using it for theme park rides and people at Universal/Disney were super excited about it. So the \"story\" they were trying to design was something that was mass marketable with IP tie-ins, short enough not to tire the average American and maximize throughput while also minimizing floor space in the places they'd install. That's a very different kind of experience than a film or video game. I think with cheaper/lighter hardware the profit model could be different and the kind of content you consume with it would become different. I don't think people have figured it out yet, but with hardware changes there's a different kind of story that can work and it just needs the right storyteller to figure it out. All that said, the problem all these tech bro led companies forget is that content is king in this space and you can't just make some cool gizmo and expect people to buy it when no one knows how to create for it. For all their flaws, Magic Leap was actually smarter about that than their competition. reply PaulHoule 18 hours agorootparentI've been thinking there could be a market for place-based experiences with XR headsets. Like you go to the Paleontology museum and it gets turned into something like \"Jurassic Park\". Something like that ought to be a lot cheaper to develop and deploy than a typical theme park ride. I was a little disappointed when I got the MQ3 and found it didn't have the same persistent SLAM https://en.wikipedia.org/wiki/Simultaneous_localization_and_... that Hololens does, mainly on the pretext of privacy, but that does make it a little harder to make an app bound to a specific place. I see that kind of thing having a window of opportunity between when it is possible and when everybody has an XR headset and there is nothing special about it. The slower XR is to catch on, the wider the window. reply duped 17 hours agorootparent> I've been thinking there could be a market for place-based experiences with XR headset You're not far off, the industry buzzword is \"LBE\" for \"location based experiences.\" There was a startup that was killed by COVID/mismanagement called the Void that was building some great LBE stuff with tie-ins to Disney IP (Star Wars, Marvel, etc) and had locations in their parks. There is definitely a market for it, and even \"cheap\" LBE like escape rooms have kind of proven that there's a market for entertainment that works like a ride. reply dylan604 17 hours agorootparentprev> The phrase I kept hearing from film studio/director types was that they didn't know how to tell a story with VR that could only be told with VR (profitably). This was my exact same complaint within my group creating VR content around the same time. Gaming is a no-brainer. It was the live action that was the blocker. Directors want to control the narrative with various techniques like blocking, depth of field, framing, etc. Within VR, all of that control is lost. Within VR means that it is possible the viewer isn't even looking in the right direction as the director intended. Putting the camera in motion is weird for the viewer since the viewer did not initiate it (unless it was something obvious like being in a car/roller coaster/etc). reply duped 16 hours agorootparentI think if you ask the question, how do you make a compelling narrative when there's one camera, it can move anywhere on set, and it's sentient? One answer is \"that's a video game.\" And there's definitely a lot of ways to tell a story through gameplay. But a lot of filmmakers don't want to make video games, and finding new answers to that question is something that VR struggles with today. reply dylan604 16 hours agorootparent> But a lot of filmmakers don't want to make video games, and finding new answers to that question is something that VR struggles with today. Part of me flippantly says that most directors are making nothing but a demo of a video game since it's all CG anyways. Another part of me says this is also where the generative AI characters will not be something a traditional director will even be interested in. Part of being a director is working with actors and getting them to deliver the performance they want. There are different types of directors, and the type that are an \"actor's\" director will not be interested in it at all. Those that are more technical and just want tech to tell a cool story might be interested since it takes that weird human interaction out of the equation. reply jacksontheel 16 hours agorootparentprev> I think if you ask the question, how do you make a compelling narrative when there's one camera, it can move anywhere on set, and it's sentient? There's live theater too. Could be an interesting way to experience front-row tickets for a play. But what could AVP provide that a live play couldn't? Maybe putting the viewer in the middle of the stage, but it would be a pain to keep rotating to witness the action. reply duped 16 hours agorootparentMy understanding is the economics don't work out for live theater. Most productions struggle to fill seats and have pricing issues. The productions that don't struggle (eg: Broadway) don't want to lower the demand for seats. That said, there is a financing issue with Broadway where productions are getting more expensive but audiences are price sensitive after some point, and with the finite number of seats available there's essentially a cap on the revenue they can bring in. That's also ignoring the artistic issues with convincing actors/directors to design and conduct performances for audiences in a completely new way, which is the problem I was alluding to earlier. > Maybe putting the viewer in the middle of the stage, but it would be a pain to keep rotating to witness the action. This has been done before (I've even been to a few local productions where this is the norm) but you have to keep in mind the production is designed for the venue its performed in and where the audience is located. I think there's a kind of theater production where you could use VR as a tool to a lot of success but I think the work has to be written for it, a production team that's down with it, a cast that can be trained to do it, and pricing model that makes it profitable. It's a very hard problem domain that isn't technical. It's artistic, social, and economic. --- I think ballet would be a much better fit than theater, for what its worth. reply 1over137 10 hours agoparentprev>I'm struggling to see how the headset could replace what is in my experience a mostly social activity. Going out to eat at restaurants used to be a social activity, now people just sit together but stare at their phones. reply SquareWheel 18 hours agoparentprevFor you, VR likely wouldn't be able to replace that social context. However for those kilometers apart, it might allow them to simulate it. See for example Big Screen[1] which lets you enjoy a group watching experience. It might not be as good as sharing real popcorn, but it can be a surprisingly convincing imitation. [1] https://store.steampowered.com/app/457550/Bigscreen_Beta/ reply aranelsurion 12 hours agorootparentWe have tried this with a friend recently with that app, and I can tell you it was miles better (in terms of feeling natural) compared to, say having a video call or Discord. We played a few flat (not VR) games, on a super large screen in a cozy virtual room, with directional audio that is always on. With your attention being taken on what is happening within the game, it's -almost- as convincing as sitting together and playing the same game. I can already say that this is my favourite way of playing couch co-op remotely. Haven't tried movies, yet I wouldn't expect the experience to be any different. reply ano-ther 18 hours agoparentprevFully agree. Just using it for consumption is also very unimaginative. I look forward to seeing developers explore the potential of spacial interactions that are different than just strapping a 2d display on your head. reply gwbas1c 17 hours agoparentprev> I may be an outlier but in our house the only time we watch shows or movies is a social context--usually Friday or Saturday nights as a family. There's a very clear cultural divide here. You missed a very important point in TFA: > If you live in Asia, like you live in Taiwan, people don’t have big homes, they don’t have 85-inch screen televisions. Plus, you have six, seven, eight people living in the same house, they don’t get screen time to watch things so they watch everything on their phone. (Emphasis mine.) Basically, the point is that watching TV in VR will probably be very popular in Asian markets. reply Towaway69 16 hours agoparentprevI assume everyone said the same about television, especially as the VCRs came out - no more theater and cinema. But: the iPhone did replace the rotatry phone! /s reply lapcat 18 hours agoprev> The iTunes Music Store does still exist, although its revenue contribution to the labels has long been eclipsed by streaming. It’s more important contribution to modern computing is that it provided the foundation for the App Store. I'm glad that Ben mentioned this. I discussed it at length in my own blog post \"App Store is neither console nor retail but jukebox\": https://lapcatsoftware.com/articles/jukebox.html The App Store was a carbon copy of the iTunes Music Store, thrown together very quickly, as shown in evidence and testimony from the Epic trial, but a store for selling 99 cent songs is an extremely bad fit for selling computer software. A lot of the problems with the App Store today stem from its origins in the iTunes Music Store, and unfortunately Apple has done very little to reshape the App Store to be more suitable for software. As a software developer myself, I have very little interest in Vision Pro right now. One major problem, especially for indie developers of paid apps, is that customers have come to expect free support for new platforms. Last month I finally gave in and made my Mac and iOS apps a universal purchase instead of separate purchases, but to me it's ridiculous to give away a new version for free on Vision Pro when consumers are giving $3500+ to Apple for a new device. Consumers are resentful if they have to give third-party developers any extra money. The level of consumer entitlement for free software is over the top. Apple gets to make all the money from expensive hardware, and we're supposed to be indentured servants supporting any and all new Apple hardware for no profit. reply rickdeckard 18 hours agoparent> One major problem, especially for indie developers of paid apps, is that customers have come to expect free support for new platforms This is an interesting aspect indeed. Not just the increased customer expectation but also the resulting increased dev-cost. It looks alot like Apple aims to repeat what they did on the iPhone: Deliver a solid barebone experience, watch and observe what the dev-community does, build your app/service feature-backlog / adjust your revenue-share model based on the 3rd party apps that succeed. But now the ramp-up complexity to make a good app is much higher than it was back then for the $1 Flashlight App, the $2.99 iBeer App or Fruit Ninja. The question is whether there are again enough devs who are eager to do all the upfront invest to \"throw stuff against the wall and see what sticks\" on behalf of Apple... reply TheJoeMan 15 hours agorootparentIt’s very funny you mention Fruit Ninja, because I got a email that seems like Apple specifically ensured Fruit Ninja is ported to VP. BeatSaber alternative? reply pm 18 hours agoparentprevConsumers find it easy to justify a hardware purchase: it's tangible, and save for requiring an electrical outlet (which is ubiquitous in our society), runs anywhere. Software is, in some regards, intangible, and is constrained to certain platforms. It makes it easier to dismiss, even though it's no less work to make great software. reply skydhash 18 hours agorootparentAnd that has not been helped by freemium and ads supported platforms. When I first got into computing (around windows XP SP2) if something was free and not open source, it was viewed with suspicions. Piracy was rampant, but they’re already not your market. It’s easy to buy software when it’s fulfilling a real need. reply dwaite 15 hours agorootparentprevIMHO this has been one of the reasons the push for subscription-based pricing has been successful - it helps you pitch services that provide value, rather than software the user can leverage to create their own value. reply dwaite 15 hours agoparentprev> Last month I finally gave in and made my Mac and iOS apps a universal purchase instead of separate purchases, but to me it's ridiculous to give away a new version for free on Vision Pro when consumers are giving $3500+ to Apple for a new device Depends on your model. If you are charging individually for new features, it wouldn't make sense to have an entirely new platform as a free thing. If you are rolling features into incremental paid version upgrades, it could make sense to have AVP support be one of the features of a new version. Product v1 has an iOS app, v2 is a universal app with iOS and Mac support, v3 includes iOS, Mac and AVP. If you are charging for ongoing maintenance which includes new features, it makes sense to give your entire user base an ever-increasing value. AVP support may just gets rolled in as a feature their subscription gives them. To do it the other way and require two product purchases or split subscriptions for AVP is a tiered model. Tiered models IMHO are more something that comes from necessity, when the price to support development with a single tier increases the base price to the point where you are pricing out too much of your market. You justify customers paying more by giving them more features, which in turn gives you more revenue to support development for all users. Tiering is often more of a large development team problem than an indie problem, but for workplace-oriented apps (where you may have personal or corporate buyers who are willing to pay different amounts) it still winds up being a pricing consideration for indies. I suspect tiered pricing to be a thing for AVP for a while for this reason - developers deciding an AVP owner will have extra spending capacity. I'd recommend pricing though to recognize that the platform success will be defined by it managing to cannibalize some iPad sales, and that on a five-year timeline you may again be pricing out sales by requiring a \"Pro\" AVP purchase. Plan the pivot. reply dpkonofa 18 hours agoparentprevI don't know that this is entirely true. Statistically, Apple users are far more likely to spend money on apps and in-app purchases than on any other application platform. Mainstream users may feel entitled to free apps but Apple users typically don't fit that mold. Also, you just kinda made the same case as the author in that shareware and other trial apps are probably what's needed here. reply lapcat 18 hours agorootparent> Statistically, Apple users are far more likely to spend money on apps and in-app purchases than on any other application platform. Mainstream users may feel entitled to free apps but Apple users typically don't fit that mold. Yes, Apple users are willing to pay for software... once. But then they want that one payment to apply to every Apple device: iPhone, iPad, Mac, Watch, Vision Pro, etc. They don't want to pay separately for each Apple platform. reply dpkonofa 17 hours agorootparentWhat's your source for that? Apple subscriptions also dwarf the next closest provider. I don't think Apple users care about paying once or more than once so long as they have access on all their devices. reply lapcat 17 hours agorootparent> What's your source for that? My source is myself! I am an App Store developer. Did you miss the part where I said, \"Last month I finally gave in and made my Mac and iOS apps a universal purchase instead of separate purchases\"? This was because customers were constantly confused and complaining about it. reply tcmart14 15 hours agorootparentI wonder if this could be changed some if Apple allowed some different pricing models. I've never pushed an app to the app store, but I imagine it isn't granular enough for this. But if you could offer 3 price points. One price point for the software on each platform then perhaps like a bundled price point. Say, $5 for the mac version and $5 for the phone version. But $8 for both platforms. I can see some frustration with idea of paying for the app full cost separately. But I think I would be less annoyed if I can pay for both in a single transaction. Even if it wasn't at a discount for the bundle, $10 for both. reply lapcat 15 hours agorootparent> One price point for the software on each platform then perhaps like a bundled price point. Say, $5 for the mac version and $5 for the phone version. But $8 for both platforms. Apple doesn't support this: https://lapcatsoftware.com/articles/2023/12/4.html reply saagarjha 17 hours agorootparentprevApple sells the story that with their technologies it's easy to port apps between platforms. Sometimes they do the work themselves to make it seem like that's the case, too. So the reason that users expect it is that Apple has conditioned them to think that it is the case. reply dclowd9901 18 hours agoparentprevIt was my understanding that Apple bridged this gap with Apple Arcade. But maybe you aren’t in the gaming space? reply mayoff 15 hours agorootparentHe's not in the gaming space. You can find his apps here: https://underpassapp.com reply PaulHoule 18 hours agoprev(1) All of these video apps, including Netflix, are on Meta Quest, which makes it all the more of a snub. (It's a dirty little secret that apps are highly portable between VR and AR headsets because they are almost all written in Unity anyway) (2) If you're thinking about buying an AVP and not thinking about buying an MQ3 at 1/7 the price you're not thinking or at least you're not an technology enthusiast you're an Apple enthusiast. (3) So far all the video apps (not to mention remote desktop apps like Immersed) I've seen have a poor user interface for situating and controlling the virtual screens. It doesn't seem like an insurmountable problem but it's somewhat startling that it hasn't been addressed. Maybe AVP will point the way to something better. (4) I tried the \"VR camera\" view of NBA games on Xtadium. I've lately taken a shine to sitting in the front row at college games (Newman Auditorium is rarely full so usually I can sit courtside with a $8 ticket) so the \"courtside view\" was appealing to me but: (a) my 20Mbps DSL connection is not slow enough to support it (though every normal streaming service works fine) and (b) the perception of space around the camera is really strange. I just can't say it is really any better than watching an NBA game on an ordinary TV. Once you get your AVP (or if you \"think different\" and get an MQ3) you will realize there are some troubles when you try to synthesize views out of multiple cameras in different spots and even a camera like https://us.kandaovr.com/products/obsidian-pro that shoots great pano video will get strange results when people get close. The problems I am having w/ it have to do with the network and camera so I don't see it being better on AVP. reply pwthornton 16 hours agoparentThis is true, but also these apps tend to be poor quality. I have a Quest 3, and I suspect a lot of the people who are thinking AVP and not Quest 3 haven't liked Meta's pitch. A lot of the metaverse stuff is silly. It was a poor pitch, and it wasn't remotely ready. Beyond that, the Meta Quest 3 doesn't have the best screens, so things like watching video aren't actually very good. The passthrough is comically bad, so any AR stuff is really a no go. The only things the Meta Quest 3 does well is video games and video game fitness experiences. The reason to consider the Quest 3 is almost 100%, \"do you want to play VR video games without breaking the bank?\" That's it. The Vision Pro is not making that pitch at all, and doesn't even support motion controllers. I don't think there is a lot of cross shopping between the two, and I don't think people looking at the Vision Pro are just Apple enthusiasts. They simply aren't that interested in video games, but are interested in the other experiences Apple is touting. reply BlindEyeHalo 27 minutes agorootparentDo you own a Quest 3? I disagree on a few points here. > Beyond that, the Meta Quest 3 doesn't have the best screens, so things like watching video aren't actually very good. Is it as good as a 4k OLED TV? Obviously no, but it is the by far the best screen watching experience I had away from a TV. So if you want to chill in bed or a travelling for work and spend a lot of times alone in hotels it is by far the best media consumption device. Another point are immersive 3D VR videos which is a completely new dimension and let me tell you there are a few really great things to watch, which you cannot replicate on a TV. > The reason to consider the Quest 3 is almost 100%, \"do you want to play VR video games without breaking the bank?\" That's it. see above about watching stuff. Another point are social experiences like VR chat, which I would not consider games. reply PaulHoule 14 hours agorootparentprevHard to say about the passthrough. The MQ3’s passthrough is terrible from an eye chart perspective but the latency and spatial perception are good enough you can throw and catch a ball without any trouble. The Apple Vision has cameras far away from the eye centers to support the front screen so it’s going to have to work harder to reproject images and it may be functionally worse. We’ll have to see. reply sebzim4500 18 hours agoparentprev>(2) If you're thinking about buying an AVP and not thinking about buying an MQ3 at 1/7 the price you're not thinking or at least you're not an technology enthusiast you're an Apple enthusiast. I don't think this follows. I haven't used an AVP myself but apparently they are much nicer to use than an MQ3. If $3.5k is not a significant expense for you then you might as well get the premium product. reply jsheard 17 hours agorootparentIf your primary interest in VR is for gaming then the AVP is mostly a downgrade from the MQ3, due to the limited input options. There's no proper VR controllers with IMUs, sticks, buttons and triggers, just your bare hands. You're not going to be able to play something like Beat Saber or Half Life Alyx on the AVP to any satisfactory level. reply madeofpalk 15 hours agorootparentMaybe your primary interesting in VR isnt gaming. reply jsheard 15 hours agorootparentIf you want to do spreadsheets in VR then that's your prerogative, but it's still not accurate to say the AVP is a more premium substitute for the Quest in general. reply threeseed 15 hours agorootparentSo the only options are spreadsheets and gaming. Even though it's obvious to everyone that content consumption will be the killer app. reply makeitdouble 18 hours agorootparentprevThe weird part is the MQ3 does more. I'd assume people with this kind of purchasing habits are just buying both and some more anyway. reply PaulHoule 17 hours agorootparentHard to say. I think the most interesting feature of the AVP is the eye tracking which could not only project your avatar to the front of the device but might be able to make an avatar good enough that you could jump into a Zoom call from an AVP. That's one of those things that would make it possible to travel and pack an XR headset instead of a laptop That eye tracking though also supports dystopian scenarios such as being able to change the world right from under you by only changing things you're not looking at, \"reading your mind\" by seeing what catches your eye, etc. The kind of thing that makes people afraid that Facebook is involved with this. The Quest controllers work great for a range of applications. Hand tracking has come a long way since the painful experience of holding your arms up high so the Hololens 1 can see them but I don't know if AVP's hand tracking will really be as versatile as the Quest controllers and Apple's the sort of company that will go down with the ship because they think there is something unclean about a design... But on that other hand they've got that battery pack. The Quest has really good VR games and also the kind of media apps that this article is talking about. MQ3 came with a great MR demo game: they opened up the API for MR apps as soon as the MQ3 hit the street but they have been slow to get third-party MR apps through the app store, I just got my first one the other day. The Quest's graphics capabilities aren't that great by modern standards, certainly if you are a serious gamer with a powerful gaming PC you have seen games that are much more detailed and impressive... I would say that the graphics of Asgard's Wrath 2 are pretty similar to those of Metroid Prime. On the other hand there is something really special about being in a space. The AVP is a lot more powerful and on paper could synthesize better graphics but it's not so clear to me how it works out in practice. I think how the 2004 game Grand Theft Auto: San Andreas lets you travel in a huge world with no loading screens in a Playstation 2 with 36MB of RAM... An experience which is still uncommon despite having phones with 1000x the RAM because it is a lot of work to tune up graphics. Similarly you see most of the games that are on Xbox and Playstation and PC are also on the much less powerful Switch. Part of the reason Horizon Worlds has failed is that they were trying to make an easy authoring experience that would be accessible to people who don't use traditional 3D tools which comes with all sorts of limitation, not just in the amount of geometry you can have or the number of players, but that you can't bring in your own textures. In the end it all has to fit in RAM in the headset. Maybe you pay 7x as much for 7x the capacity on an AVP (so a world could have 140 max players instead of 20) but often you give people more resources and they just waste them... Look how cloud gaming never developed exclusive titles that did anything that you could only do with cloud gaming. reply jsheard 17 hours agorootparent> That eye tracking though also supports dystopian scenarios such as being able to change the world right from under you by only changing things you're not looking at, \"reading your mind\" by seeing what catches your eye, etc. For better or worse Apple isn't allowing this kind of thing, the eye tracking \"cursor\" is only ever known to the OS itself and apps only receive \"click\" events with a snapshot of where you're looking when the OS detects the relevent hand gesture. Apps are never allowed to know what you're gazing at passively. It's a good move for privacy, but it's very limiting for games since they will only have (accurate) head and (not so accurate) hand tracking data to work with. reply sebzim4500 15 hours agorootparentI suspect there will be ways of getting this information though, it would be incredibly hard to design the foveated renderer such that you can't figure out where the eye is by e.g. positioning different amounts of geometry on the screen and then timing frames. reply jsheard 15 hours agorootparentApple hasn't made it exactly clear AFAICT, but I suspect that foveated rendering might only work in the managed RealityKit environment where Apple controls the entire rendering pipeline, and not inside applications which implement their own rendering from the ground up using Metal, for exactly that reason. There's nothing in the documentation about foveated rendering in Metal apps, and it is something that engines would have to be explicitly aware of if they do any kind of off-screen rendering. reply dwaite 15 hours agorootparentprevI don't believe you get mouseover/\"glanceover\" events; you have to define your focus visual behavior declaratively. Rendering of that behavior is then not visible to an app with default entitlements. Of course, Apple's store means they can just forbid gaming the system. reply makeitdouble 17 hours agorootparentprevThanks ! BTW I find Apple/Meta's focus on meetings so fascinating. As a mere employee I can't imagine being excited to do meetings in a more immersive way, when we collectively agreed to disable cameras on most of our calls for stress reduction. That would be better with family, I guess, but then $3500 of material and getting kids and elderlies in VR is quite an hurdle. On AVP's performances, I fully agree. Currently, running the Quest as a PCVR headset, and thus aleviating the base computing part, still requires a pretty beefy PC to run the games at full quality. And even laptops have a hard time getting enough power and cooling to run at decent speeds for sustained periods. While the AVP has an M2, I wonder how far that would go when it comes to games that actually push the envelope (or apps that are as underoptimized as VRChat ?). reply Philpax 16 hours agorootparentRegarding the meetings: VR meetings are much less fatiguing because you aren't staring at 12 people who are staring back at you. The spatiality and body language make a huge difference. The corporate implementations are bad, but they'll eventually take some lessons from VRChat. reply makeitdouble 11 hours agorootparentI think we're not talking about the same thing. The main sources of \"zoom fatigue\" (camera on) I see are: - having to show you pay attention to people speaking when you're actually looking at documents (or doing something completely different if you didn't need to pay attention). - being stuck where you are as you can't just go to the kitchen or feed your cat while someone else is presenting. VR solves none of that, and having your whole body captured makes it worse (to note, the AVP doesn't allow moving past some small boundary I think ?). We're still in the fantasy that meetings are something you should be 100% focused on, and double down in a \"it doesn't work because we aren't doing it enough\" cross training way. I truely believe the appropriate future of meetings are holographic slabs floating in space representing each participant audio only, Evangelion style. reply asadotzler 9 hours agorootparentprevThis can be done without VR though. Mozilla Hubs meetings, for example, right on my ThinkPad screen, don't suffer from the zoom brady bunch problem and it didn't require a dedicated device that's twice as expensive. reply PaulHoule 14 hours agorootparentprevVRChat stands out as one of very few “multiplayer” experiences in VR that are successful. Sure you might have somebody jump into tutorial island yelling “I am a furry! I am a furry! I am a furry” but I also had positive interactions with people right off the bat whereas there was no way I was going to get somebody in Horizon Worlds how to work the stupid fishing rod. reply creaturemachine 17 hours agorootparentprevMeta has captured the home and family segment with the Quest devices, as evidenced by the number of kids you hear in social spaces and games. No parent is going to hand a $3.5k device to their kids when a $300 Quest will do the same job. reply PaulHoule 17 hours agorootparentNote that that consumer is still buying the Quest 2 instead of the Quest 3 so save a few hundred $. https://mixed-news.com/en/quest-2-vs-quest-3-amazon-sales-fi... Although Asgard's Wrath 2 is a pack-in game for the MQ3 it plays fine on the MQ2 and doesn't take advantage of the more powerful processor of the MQ3 and only includes a tiny amount of MR gaming as an afterthought. reply Remedwme 18 hours agorootparentprevBased on what? The Q3 is a 100x better device for market entry than the avp. It has also very good resolution. For just watching movies etc it would be a game changer for a lot of people and still it's not a thing everyone just owns reply mellosouls 18 hours agoparentprevAll of these video apps, including Netflix, are on Meta Quest, which makes it all the more of a snub. Yep, its a bit of an elephant in the room in both this and the linked blogpost that the authors seem completely unaware that this is an already well-established medium use case that Apple is very late to. reply empath-nirvana 18 hours agorootparentI don't think it really needs to be stated that everything Apple does is an already well established medium that they're late to. They didn't event smart phones or smart watches or set top boxes or smart speakers or the personal computer or or or reply titanomachy 17 hours agorootparent…or mp3 players or wireless headphones. But all those markets became much bigger after apple entered them. reply jayd16 17 hours agoparentprevPorting a Unity game is way harder than you think, especially on a VR headset where Netflix would have had to tune things to specific hardware to get video frames syncing properly. The entire UX has to be redesigned. Not to mention that Unity apps are second class citizens until they're rewritten as \"immersive apps.\" It's not trivial. reply skazazes 17 hours agoparentprev> (2) If you're thinking about buying an AVP and not thinking about buying an MQ3 at 1/7 the price you're not thinking or at least you're not an technology enthusiast you're an Apple enthusiast. I bought the Quest 1 when it was an Occulus product and stopped using it the moment they started enforcing Meta anything within the device. I could not care less if it is a 1:1 hardware equivalent as long as it has anything to do with the Meta empire. The last I checked, my Quest 1 refuses to function on my home network because of the filtering I enforce on my router... The \"technology enthusiast\" crowd is highly heterogeneous reply ozten 17 hours agoparentprev> (2) If you're thinking about buying an",
    "originSummary": [
      "Apple has introduced the Vision Pro AR headset, which has the potential to replace traditional home theaters.",
      "The article discusses Apple's business strategy, tracing the evolution from the iPod to the iTunes Music Store and the App Store.",
      "Apple is facing an antitrust lawsuit with Epic Games, concerning its control over App Store revenue.",
      "Notably, popular streaming services like Netflix and YouTube have chosen not to develop native apps for the Vision Pro, which is seen as a setback.",
      "There is speculation that Apple may partner with Disney to strengthen their streaming services and compete with Netflix."
    ],
    "commentSummary": [
      "The article and discussion cover various topics related to Apple's VR headset, the potential of VR and AR technology, and the limitations and challenges of VR devices.",
      "The conversation includes discussions on the lack of third-party apps and features on the Apple Vision Pro and the drawbacks of Windows Mobile compared to the iPhone.",
      "There is mention of the potential use of VR in platforms like Netflix, concerns about the high cost and practicality of VR headsets, and the potential impact of VR on shared viewing experiences."
    ],
    "points": 183,
    "commentCount": 432,
    "retryCount": 0,
    "time": 1706540031
  },
  {
    "id": 39175393,
    "title": "Build Native iOS and Android Apps with Svelte Native",
    "originLink": "https://svelte-native.technology/",
    "originBody": "The Svelte Mobile Development Experience Powered by Svelte Svelte Native is a mobile application framework powered by Svelte — build mobile apps using the friendly web framework you already know learn more Using NativeScript Build cross-platform, native iOS and Android apps without web views. Get truly native UI and performance while sharing skills and code with the web learn more Fully featured Use the full power of Svelte including Transitions, Stores, and Reactivity. One of the smoothest development experiences available for mobile learn more Svelte Native is a new approach to building mobile applications using NativeScript. Where other JavaScript mobile development frameworks like React Native and NativeScript-Vue do the bulk of their work on the mobile device, Svelte Native shifts that work into a compile step that happens when you build your app. Instead of using techniques like virtual DOM diffing, Svelte writes code that surgically updates the native view widgets when the state of your app changes.npm install -g nativescriptns create my-mobile-app --sveltecd my-mobile-appns run android See the quickstart guide for more information. Learn Svelte Native Svelte Native Grocery Grocery app example. Manage and sync grocery items. Repository Svelte Native Realworld A Realworld implementation app in Svelte Native. Repository Svelte Native Hacker News Reader Simple HackerNews client in Svelte Native. Repository",
    "commentLink": "https://news.ycombinator.com/item?id=39175393",
    "commentBody": "Svelte Native (svelte-native.technology)169 points by thunderbong 21 hours agohidepastfavorite72 comments oneseven 17 hours agoWhatever the merits of this project -- and it does seem to be useful -- they should be a little clearer about the fact that it's not actually part of the \"official\" svelte framework. I'm sure they consider it a tribute but it fees a little deceptive that they're cloning the svelte docs look and feel without any kind of \"who we are\" statement. reply kevinak 11 hours agoparentBack when Svelte Native was created it was pretty much endores by the maintainers, there's even a channel for it on the official Svelte Discord :) reply LinguaBrowse 8 hours agorootparentMoreover, it was made by one of the folks who brought TypeScript support to Svelte (https://github.com/halfnelson/svelte2tsx), so although it may not be explicitly pushed by the core team, it was still made by a significant core contributor. I'll note that React is the only web renderer that gives official blessing to a native framework, and that's because it's also the only web renderer that built one themselves rather than leaving it to the community. reply kevinak 1 hour agorootparent100% agree reply imgabe 12 hours agoprevI’ve done a couple React Native apps and never again. They’re fine at first but if you let one sit for a few months there is a combinatorial explosion of complexity among shifting incompatibilities of different versions of javascript libraries, the framework, android/iOS versions, build systems, etc. it becomes a nightmare. I’d much rather just learn Kotlin/Swift and have only one ecosystem to maintain. reply wharvle 12 hours agoparentAt the time I used it, it was almost worth it just to use RN for Android even if you did iOS in Swift, just because styling on Android was so hellish (\"There must be some non-crazy way to set a default typeface for my app. Right?\" LOL wrong; \"setting this color on this Material input field must be easy\" LOL nope, welcome to Java metaprogramming just to set a goddamn color), the documentation-suggested way to structure apps was so bad that you'd be reaching for other solutions anyway if you valued your sanity (I wanna say Square was at the forefront, here? It was some company like that. Like your best bet was just to find their libraries and blog posts and do exactly what they did, ignore the official docs), and in some cases the native libraries for Google APIs were less-capable than the Javascript ones anyway (Maps was one, I remember—god, what a surprise that was to find out). But that was like 5 years ago so maybe it's gotten better. reply jamil7 10 hours agorootparentAlthough I haven't used it, Jetpack Compose exists now, so that and SwiftUI are pretty similar. On a previous project I was contracting on, the CTO managed to bash out a basic Android version of the iOS app I was working on without any Kotlin experience by going through the SwiftUI views one by one and implementing them in Jetpack Compose. reply rareitem 11 hours agorootparentprevI agree with you. RN still sucks on some aspects, but it's quite an amazing technology and I'm grateful it exists. reply fakedang 11 hours agorootparentNah, among cross-platform, RN has stagnated while Flutter has sped past. The key issue I see these days is that it's just not worth it to build a mobile app these days unless your payment flow happens outside of the app. reply willsmith72 8 hours agorootparent> it's just not worth it to build a mobile app these days unless your payment flow happens outside of the app not true at all, 85% of something is better than 100% of nothing. This is coming from someone who hates mobile apps and apple's grip on them reply jitl 8 hours agorootparentprevI think most users would prefer an RN app over a Flutter app, especially on iOS, because the RN app will feel much more native because it renders using native views, instead of a Skia framebuffer. reply meowtimemania 10 hours agorootparentprevI had bad experiences with it 7 years ago, but really good experiences with it recently. If you stick to native modules managed by expo you'll be in a great place! You can run into dependency hell if you use a random react-native npm module that was last updated 5 years ago. reply vietvu 6 hours agoparentprevMaybe it got better but when I was using RN, it felt fine at the first 3 months; then we found out that it is a hell to work with JS and React Native dependency. To the point we needed to forked multiple project to make it worked. And in the end, we still need a considerable native code for our app. It might just be better to start from scratch with native iOS/Android. reply conradfr 11 hours agoparentprevPeople should just use Ionic & Capacitor. You can still use your React knowledge with it. reply factormeta 9 hours agorootparentCapcitor with solid or marko would be better, more lightweight. reply apatheticonion 6 hours agoparentprevWriting a Windows app, MacOS app, Linux app, iOS app and Android app is a nightmare because of the insanity and divergence of the SDKs. They could all have a common API with OS-specific extensions - but instead of that, everyone is trying to make their own API. As a result, developers just write everything in electron/WebKit/WebView. I wish there was an open standard API for writing native desktop/mobile apps that OSes could adhere to (that the EU forces them to adhere to, haha). reply 0x6c6f6c 6 hours agorootparentI've seen Dart filling this gap for many apps lately reply ex3ndr 9 hours agoparentprevThis is a very weird thing to say when expo is literally one line installation and it does not require you anything to be installed (even XCode) on machine for most projects. It is literally 10x times easier than just writing in Swift. reply matharmin 1 hour agorootparentIf you don't need any native plugins other than those provided by Expo Go, then yes. If you do need other libraries, or add native code yourself, the package system becomes a nightmare. It is NPM packages on top of Cocoapods on top of a XCode/Swift/Objective-C compile process. Only slightly better on Android. Want to make a change to a library? Good luck getting linking working on your local setup. Cordova had exactly the same issues. I'm not sure if Ionic Capacitor is better, but this is one area where Flutter is miles ahead. reply jamil7 7 hours agorootparentprev> It is literally 10x times easier than just writing in Swift. I feel Swift as a language is a lot more productive than TS/JS personally. I've found things like React Native and Expo etc. more of a liability than a help for businesses that are heavily reliant on mobile. reply imgabe 9 hours agorootparentprevIf you want to submit it to the app store you'll need to install XCode anyway to build it and submit it. Might as well just start there and have it configured right from the beginning. reply ex3ndr 8 hours agorootparentNo, you do not, you can use a free service by expo that would build in the cloud. Literally requires zero setup. Local build is needed only if you are sensitive to security, but many banks started to use it. Also native plugins became so easy to use it is a joke how simple it is comparing to cocoapods and some cryptic libraries. Expo's workflow is so perfect comparing to pure native, it is not even comparable. To do iOS development you need: Swift Packages, Cocoapods and XCode at least. Whils in general it is simple, but cocoapods constantly fights with apple and xcode to not crash anything. While expo also uses cocoapods, it is much more stable because you can always just delete a native project and regenerate it from scratch and it will work. I am not even starting to discuss how complicated is development for Android - literally everything now have 5+ different APIs for a simple things like \"please encrypt this string\" or \"take a photo\". React Native and Expo has perfect packages that solves real problems and work with a few keystrokes. reply imgabe 8 hours agorootparentI'm glad it works for you. One thing I've learned from bitter experience is \"don't swim upstream\". Meaning, you're likely to have problems when you're using tech outside the way it was intended. Apple intends apps to be developed in Swift with XCode, so that's what they support and when they develop new features that's what they have in mind. They don't care about Expo and if they do something that breaks Expo or Flutter or any other third-party service, that's not their problem and they aren't going to do anything about it. Expo et al are always going to be second class citizens to Apple. Cocoapods are on their way out. I use Swift Package Manager on new projects and it's literally a couple clicks to install whatever package you want. Haven't had any problems yet. reply jd3 11 hours agoparentprevWe recently threw out our React Native app and have gone all in on PWAs. reply lakomen 10 hours agorootparentTWA as well? Because I find, the app stores are good for discovery. reply hardwaresofton 6 hours agoparentprevReally important to note, this isn’t react native, its NativeScript. The two have some fundamental differences — React Native is complex because the React ecosystem is full of complexity. NativeScripts biggest problem is more ecosystem size. reply woahitsraj 15 hours agoprevWhile I'm a huge Svelte fan, this project hasn't really gotten very much care and attention over the last few years. Which is honestly fine. React and React Native I think do a much better job of filling the niche of people who want to build native apps with web tech. That being said, I think with the progress that Safari has made in implementing PWA support, the increased hostility of Apple toward native developers, and browser improvements like WebGPU coming out soon, I really hope that we no longer have to build native apps for like 95% of use cases. The only major hurdle to this is Apple continuing to treat web apps as second class citizens on iOS and only begrudgingly adding APIs to Safari to make good native experiences. reply sacrosanct 15 hours agoparent> The only major hurdle to this is Apple continuing to treat web apps as second class citizens on iOS If you add a site to iOS' homescreen it automatically becomes a PWA. The best example I found of a site fully leveraging this feature is Cryptee[0]. They talk about the PWA thing here: https://crypt.ee/download [0] https://crypt.ee/ reply throwitaway1123 14 hours agorootparentThe whole add to home screen process is needlessly convoluted. It would be nice if there was something similar to Smart Banners for PWAs: https://developer.apple.com/documentation/webkit/promoting_a... reply ezfe 12 hours agorootparentYou just click \"Share -> Add To Home Screen.\" I understand that that's technically more clicks than a smart banner but hardly \"convoluted\" reply throwitaway1123 11 hours agorootparentI think it's convoluted relative to a Smart Banner. Smart Banners give users a clear call to action, and they're not buried in a menu somewhere. It's hard to make this point without linking to a screenshot, but the share menu is incredibly bloated. To get to the add to home screen button a user has to know it's in the share menu (which is just an unlabeled icon), and then scroll past the following menu items: - An options button (which leads to another menu) - Air Drop - Share via text message (with several different contacts listed individually to share with) - Copy - Add to Reading List - Add Bookmark - Add to Favorites - Add to Quick Note - Find on Page reply oh_sigh 8 hours agorootparentprevAdding to your own home screen under a \"share\" menu is something probably 90% of users would never think of. reply woahitsraj 8 hours agorootparentprevI agree it's MUCH better than it used to be (and huge credit to Jen Simmons and her team for making this possible). However Safari APIs are still WAY behind Chrome/Android and I think this is probably intentional to push developers into using the App Store so Apple can collect their 30% tax https://fugu-tracker.web.app/ reply willsmith72 8 hours agorootparentIt's absolutely intentional, but going to slowly get better even as they drag their feet reply dangerlibrary 9 hours agorootparentprevFastmail does this too, and it works extremely well. reply tootie 14 hours agoparentprevWe've been working with Capacitor recently which is the current iteration of webview-to-native. Basically you just build a website and run it through a compile step to output iOS and Android. The ergonomics of the build process and integration with native features is definitely a bit lumpy, but we've been very successful at building an all-in-one codebase that will power web, ios and android with minimal drift. reply kevinak 14 hours agorootparentNot sure what you built but for anyone interested in going down this venue there's a blog post by a Svelte Ambassador that you can read here: https://khromov.se/how-i-published-a-gratitude-journaling-ap... reply roldyclark 13 hours agorootparentprevCapacitor is great! Did Svelte + Capacitor for my last app. Highly recommend. reply dickersnoodle 12 hours agoparentprev> increased hostility of Apple toward native developers Chortle. reply papa_bear 16 hours agoprevMy company tried using this a few years ago to build a cross platform app (and hopefully share a lot of code with our svelte web interface). We made a lot of progress in 3-4 months of trying, but we ended up running into too many odd bugs and edge cases that were difficult to debug. We eventually switched to Flutter for the mobile codebase. No doubt this and nativescript have improved since then, but from my early impressions, I wouldn't have recommended nativescript for anything other than the simplest of interface needs. reply scosman 16 hours agoparentThis is the story I’ve heard about almost every react native project. Get to proof of concept 10x faster. Spend 10x more time in the weeds on weird bugs. reply rhodysurf 15 hours agorootparentNativeScript is not react-native reply whizzter 10 hours agorootparentprevDid they have JS _AND_ React experience beforehand? Or did they just pick RN because it was the cool thing on the block? I foundered on my first attempt at an React app (side project) coming from a C,C++,Java and non-React JS background, but the second one(professionally) after I've had exposure to some React got released and an fork of it is still in use today. And the company I work with has 2-5 other apps out for customers using RN/Expo (thanks to the positive experience I had). We're also strict about using TypeScript though so we're not chasing basic type bugs (since debugging is a tad weaker with RN/Expo even if it's usable if shit the fan only occasionally) and we have plenty of React experience outside of it, biggest \"quirks\" are often RN/Expo upgrades if we haven't maintained things in a while (the oldest app used a relatively early version of Expo though) or platform specific notification crap (that I've heard is painful regardless of Expo/RN). reply mixmastamyk 14 hours agorootparentprevAlso the old Cordova equivalent and Kivy. Take mobile development and try to shoehorn a browser-based solution (they don't want) into it. Now you've got two problems and not enough expertise available to help. reply TSiege 14 hours agoparentprevHow has Flutter compared in your experience? reply gmaster1440 17 hours agoprevThis is not so much the Svelte equivalent of React Native as it is just one of the wrappers for NativeScript (https://nativescript.org). reply FlyingSnake 13 hours agoprevPowered by NativeScript Hell no. It’s better to stick with RN if you want cross platform. Writing an app in Kotlin and Swift is easier compared to this contraption. reply ca-tech-run 12 hours agoparentnext [2 more] [flagged] sixstringtheory 11 hours agorootparentPlease stop spamming the same link. If you have something you'd like everyone to see, just make it a top-level comment on the submission. And a bit more substantial commentary on why we should visit it would be nice. reply lawgimenez 17 hours agoprevNative is such an abused word. reply jbverschoor 15 hours agoparentThey're making native calls.. Either through message passing, or through creating stubs. You can literally use all of the iOS SDK directly. Not sure why I'm downvoted.. https://old.nativescript.org/native-api-access/ It's the basis of their entire product. On top of that they've built cross-platform wrapper for many UI things. And on top of that, they've integrated several toolkits. They just market it the other way around, which i.m.o. is not helping them with traction. Right now they're always competing with many other products (react native for example), even though they have a very unique product. reply eximius 16 hours agoparentprevTrue. I think the intent here is \"we translate your stuff into non-webviews\" which is... pretty much what native means in a mobile context. reply kvgr 17 hours agoprevThe example app is 5 years old. Is anybody using this? reply joshdajosh 8 hours agoparentIt's being used here: https://github.com/Akylas/OSS-DocumentScanner reply la_fayette 15 hours agoprevI have built many mobile apps in the last years. I always used web technologies rendered inside webviews, using tools like capacitorjs. Since I am very concerned with user experience, I always considered the perceivable UI rendering performance as highly important. For contemporary smartphones I cannot see any benefit in using tools like react native, flutter, nativescript in contrast to rendering inside a webview. I think UI libraries like ionic are indistinguishable from native UI components. At least, to the best of my knowledge, there is no real user study, which proves the contrary. So why should one use such cross-platform tools and not use svelte directly with a webview? reply crowdyriver 15 hours agoparentHave you experienced slow scrolling issues? https://github.com/ionic-team/capacitor/issues/4187 reply refulgentis 14 hours agoparentprevI very much doubt Ionic is visually indistinguishable from native UI components, but I'd love to try if you have a good demo link. My guess is it crosses a \"good-enough\" threshold for usability, and it feels objective, ex. maybe scroll performance is the same. I strongly believe users notice things feeling \"off\" in a way that's either subconcious or hard to communicate, and that's why it matters. The reason why I use Flutter instead of JS is for a number of reasons beyond perf / native controls. The shortest version is iOS / Safari makes web apps extremely obviously a worse version of a native app. But I'm picky. I just upgraded macOS and got a \"you must accept new T&C\" dialog. Accidentally scrolled on it vertically. It did the rubberband effect showing a white background, and I said to myself: \"lo, how the mighty Apple has fallen!\" reply iamandoni 12 hours agoprevAn alternative I’ve been enjoying is @tommertom’s Ionic port (+ Capacitor) for native integration: https://github.com/Tommertom/svelte-ionic-app reply miohtama 17 hours agoprevSuper, I love Svelte and I hope its unique component model fits well for the native mobile app development. Is the tool chain very different from React native? reply woahitsraj 15 hours agoparentIt is. This is mostly a Svelte wrapper around NativeScript so it's really not equivalent reply solarkraft 16 hours agoprevWhy does NativeScript seem so much worse than React Native? reply TomatoTomato 16 hours agoparentFunding? Plugin ecosystem like Expo modules? Or maybe use both? https://blog.nativescript.org/nativescript-with-react-native... https://twitter.com/wwwalkerrun/status/1700719038392078607 Then throw in some SwiftUI too https://twitter.com/NativeScript/status/1624895622540328961 reply ex3ndr 9 hours agoparentprevBecause they run JS on main thread while react native tries to minimise it at all costs (only keeping rendering and animations, but not layouts for example). reply 8 hours agorootparentnext [2 more] [dead] ex3ndr 1 hour agorootparentNo, even when you are writing directly in swift/kotlin you avoid main thread at all costs. Any overhead here is a death sentence for performance. reply OJFord 17 hours agoprevI think if you're starting today you'd be better off with Tauri (and Svelte as the framework)? reply CharlesW 17 hours agoparentTauri Mobile seems far from production-ready, but more importantly, the reason to use something like Svelte Native instead of Tauri Mobile is to avoid web views in favor of a platform-native experience. reply pjmlp 16 hours agoparentprevI would rather stick to either pure mobile Web, or native views + C++, but that is me. No added tooling, only the SDKs from each platform, no late nights debugging integration layers, lack of support on IDEs,... reply ultra_nick 16 hours agorootparentSDKs for 4+ platforms is added tooling... reply pjmlp 15 hours agorootparentIt is when it represents debugging the same code 4+ times. That is the fallacy of all these leaky abstractions, the need to actually know the native APIs from each platform doesn't go away. Someone needs to take care of integration problems. reply mixmastamyk 10 hours agorootparentYes. Want to ship on N platforms, with the cross platform lib now you need to know N+1 platforms. reply hd4 17 hours agoprevIs the goal here to reach a minimum subset of basic Android UI functionality? I don't see any push to try and replicate a MD3 feature-set here for example. reply PolCPP 16 hours agoprevYuck nativescript reply tamimio 7 hours agoprev [–] Docs returned 404 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Svelte Native is a mobile application framework powered by Svelte, enabling developers to create native iOS and Android apps without web views.",
      "It offers a comprehensive development experience, leveraging the capabilities of Svelte such as transitions, stores, and reactivity.",
      "Svelte Native optimizes the mobile device's performance by compiling the app and efficiently updating the native view widgets. It also provides sample projects and repositories for grocery management, real-world applications, and a Hacker News reader."
    ],
    "commentSummary": [
      "The conversation centers around different mobile app development frameworks, including Svelte Native, React Native, Ionic/Capacitor, and NativeScript.",
      "Participants share their positive experiences and discuss concerns, limitations, and alternative options of these frameworks.",
      "Topics discussed include ease of use, compatibility, native development, third-party services, web-based apps, and performance."
    ],
    "points": 169,
    "commentCount": 72,
    "retryCount": 0,
    "time": 1706530438
  }
]
