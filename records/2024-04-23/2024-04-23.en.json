[
  {
    "id": 40115554,
    "title": "Meta Horizon OS Revolutionizes Mixed Reality Ecosystem",
    "originLink": "https://www.meta.com/blog/quest/meta-horizon-os-open-hardware-ecosystem-asus-republic-gamers-lenovo-xbox/",
    "originBody": "A New Era for Mixed Reality Meta Quest Blog|Apr 22, 2024| Meta Quest Blog Apr 22, 2024 Today we’re taking the next step toward our vision for a more open computing platform for the metaverse. We’re opening up the operating system powering our Meta Quest devices to third-party hardware makers, giving more choice to consumers and a larger ecosystem for developers to build for. We’re working with leading global technology companies to bring this new ecosystem to life and making it even easier for developers to build apps and reach their audiences on the platform. Introducing Meta Horizon OS This new hardware ecosystem will run on Meta Horizon OS, the mixed reality operating system that powers our Meta Quest headsets. We chose this name to reflect our vision of a computing platform built around people and connection—and the shared social fabric that makes this possible. Meta Horizon OS combines the core technologies powering today’s mixed reality experiences with a suite of features that put social presence at the center of the platform. Meta Horizon OS is the result of a decade of work by Meta to build a next-generation computing platform. To pioneer standalone headsets, we developed technologies like inside-out tracking, and for more natural interaction systems and social presence, we developed eye, face, hand, and body tracking. For mixed reality, we built a full stack of technologies for blending the digital and physical worlds, including high-resolution Passthrough, Scene Understanding, and Spatial Anchors. This long-term investment that began on the mobile-first foundations of the Android Open Source Project has produced a full mixed reality operating system used by millions of people. Developers and creators can take advantage of all these technologies using the custom frameworks and tooling we’ve built for creating mixed reality experiences, and they can reach their communities and grow their businesses through the content discovery and monetization platforms built into the OS. These include the Meta Quest Store, which contains the world’s best library of immersive apps and experiences—we’re renaming it to the Meta Horizon Store. The Horizon social layer currently powering Meta Quest devices will extend across this new ecosystem. It enables people’s identities, avatars, and friend groups to move with them across virtual spaces and lets developers integrate rich social features into their apps. And because this social layer is made to bridge multiple platforms, people can spend time together in virtual worlds that exist across mixed reality, mobile, and desktop devices. Meta Horizon OS devices will also use the same mobile companion app that Meta Quest owners use today—we’ll rename this as the Meta Horizon app. A New Generation of Hardware The growth of the mixed reality market and the rising popularity of use cases like gaming, entertainment, fitness, productivity, and social presence have created new opportunities for specialized hardware. As we’ve seen with the PC and smartphone industries, consumers are best served by a broad hardware ecosystem producing both general-purpose computing devices and more specialized products, all running on a common platform. Leading global technology companies are already working on new devices built on Meta Horizon OS: ASUS’s Republic of Gamers will use its expertise as a leader in gaming solutions to develop an all-new performance gaming headset. Not an actual product render. Lenovo will draw on its experience co-designing Oculus Rift S, as well as deep expertise in engineering leading devices like the ThinkPad laptop series, to develop mixed reality devices for productivity, learning, and entertainment. Not an actual product render. Xbox and Meta teamed up last year to bring Xbox Cloud Gaming (Beta) to Meta Quest, letting people play Xbox games on a large 2D virtual screen in mixed reality. Now, we’re working together again to create a limited-edition Meta Quest, inspired by Xbox. Not an actual product render. All of these devices will benefit from our long-term collaboration with Qualcomm Technologies, Inc., which builds the Snapdragon® processors that are tightly integrated with our software and hardware stacks. Qualcomm Technologies’ latest Snapdragon XR2 Gen 2 Platform launched with Meta Quest 3 and delivered breakthrough performance that pushed the boundaries of what’s possible in mixed reality. Companies building hardware for this new ecosystem can also leverage the benefits of these chipsets and custom software enhancements. A More Open App Ecosystem As we begin opening Meta Horizon OS to more device makers, we’re also expanding the ways app developers can reach their audiences. We’re beginning the process of removing the barriers between the Meta Horizon Store and App Lab, which lets any developer who meets basic technical and content requirements ship software on the platform. App Lab titles will soon be featured in a dedicated section of the Store on all our devices, making them more discoverable to larger audiences. Some of the most popular apps on the Store today, like Gorilla Tag and Gym Class, began on App Lab. We’re excited to make it even easier for developers to quickly ship their titles on our platform. Not an actual product render. We're also developing a new spatial app framework that helps mobile developers create mixed reality experiences. Developers will be able to use the tools they’re already familiar with to bring their mobile apps to Meta Horizon OS or to create entirely new mixed reality apps. Developers can apply for access here. Along with a more open app store, Meta Horizon OS will continue to give people more choice in how to access apps. Because we don’t restrict users to titles from our own app store, there are multiple ways to access great content on Meta Horizon OS, including popular gaming services like Xbox Game Pass Ultimate, or through Steam Link or our Air Link system for wirelessly streaming PC software to headsets. And we encourage the Google Play 2D app store to come to Meta Horizon OS, where it can operate with the same economic model it does on other platforms. Not an actual product render. The Future of Mixed Reality Consumers and developers alike stand to benefit the most from an ecosystem where multiple hardware makers build on a common platform. Our partners are excited to begin this journey with us. “We’ve been inspired by the incredible gaming community that has formed around virtual and mixed reality, and we know that the most passionate gamers want high-performance hardware. With Meta Horizon OS, ASUS and Republic of Gamers will build the gaming headset of the next generation.” — S.Y. Hsu, Co-CEO, ASUS “Mixed reality is transforming how people interface with computers by integrating digital experiences and physical spaces to reach new levels of productivity, learning and play. Building from our past successful partnership, Lenovo is bringing together Meta Horizon OS with our leadership and innovation in personal computing to accelerate adoption of new user scenarios in mixed reality like virtual screens, remote presence, content consumption, and immersive training.” — Yuanqing Yang, Chair & CEO, Lenovo “The convergence of physical and digital spaces is accelerating, and we see virtual reality, mixed reality, and augmented reality becoming the next computing platform. To enable this future, these high-performance devices require an entirely new class of Snapdragon processors. Our collaboration with Meta has produced extraordinary experiences that have become the benchmark of the industry, and we’re excited to see this new ecosystem taking shape.” — Cristiano Amon, President and CEO, Qualcomm Inc. RELATED ARTICLES See all articles A Decade on the Road to the Next Computing Platform Apr 3, 2024 Today, we’re celebrating the last 10 years of metaverse and AI work at Reality Labs—and looking ahead toward what the future holds.... Read article The Magic Under the Hood: How AI Is Powering Meta’s Technologies Today + in the Future Jan 25, 2024 There’s a well-worn saying that magicians never reveal their secrets. But today, we’re taking you behind the scenes for a look at some of the... Read article Ring in the New Year With New, Lower Prices on Meta Quest 2 & Accessories Dec 31, 2023 We’re permanently lowering prices on Quest 2 and its accessories, effective January 1.... Read article",
    "commentLink": "https://news.ycombinator.com/item?id=40115554",
    "commentBody": "Meta Horizon OS (meta.com)690 points by ahiknsr 18 hours agohidepastfavorite566 comments pavlov 16 hours agoShades of Apple licensing the Mac OS to clone hardware vendors in 1995 - 1997. It seems like Meta will be controlling the software experience quite closely, just like Apple did back in the day. There’s an important difference though: Apple made all their money on Mac hardware margins which the nimble clone vendors could undercut. Whereas for Meta, the Quest hardware has always been sold at breakeven or even as a loss leader (a few years ago they actually raised the price of the Quest 2 to cut further losses). So there’s no kind of existential danger to Quest here, but it remains to be seen if the hardware licensees can bring anything relevant to the table either. reply bonton89 14 hours agoparent> So there’s no kind of existential danger to Quest here, but it remains to be seen if the hardware licensees can bring anything relevant to the table either. Why would I, as a potential hardware maker, wish to compete in a market where the existing main producer is a mega wealthy entity that is already dumping product below cost and is capable of doing so indefinitely? reply distortedsignal 14 hours agorootparentThe low end of the market is taken - but (from an outsider perspective) the high end appears free. The people who want to buy a Honda aren't the same people who want to buy an Acura, even though it's (essentially) the same parts. Let's say that, tomorrow, Gucci or Dolce and Gabanna (or however you spell that) want to make a VR headset (why? who knows?). They don't have the tech acumen to compete with Facebook on experience, but they have the brand to compete on \"people who want to be seen in Gucci.\" Is there a market for that? I don't know. But this opens Facebook up to the possibility of making that deal. reply theptip 14 hours agorootparentDon’t agree with this take. I’m sure Meta would be fine with other vendors taking over the low-end, if that meant there was a vibrant platform they controlled (ie the OS). They will lead with flagship models to push the boundaries of what the OS/tech can do. Why would they want to sell at/below cost forever? The reason they do this now is to make the platform viable. The only reason they are focusing on cheaper devices now is to build the platform and try to get more users, to in turn get more data on what the killer usecases will be. Think of this as a play like Android. Google doesn’t care what goes on in the commodified end of the spectrum, as long as there is one. Google does ship flagship phones (in competition with eg Samsung) and that is fine. reply distortedsignal 14 hours agorootparentMy point is that there is more to \"VR Headset Market\" than just \"low end\" - low end is one part of the market, but (right now) Facebook has that part locked up. It may be that there are more places to compete in the VR Headset Market that people on HN don't know about. Like you said, this is probably something like an Android play. Everyone was talking about the Apple Vision Pro as the VR Market's \"iPhone moment\" when it came out - maybe Meta Vision OS (or whatever they're calling it) is Facebook's Android moment. And yeah, Facebook would probably be ok with others taking the low end of the market if they do it well. Right now, Facebook is the only company willing to take a loss on their own platform. So they do. reply asrael_io 9 hours agorootparentNot really sure if the Vision Pro could be called the iPhone of VR. It's not nearly as popular and its demand has ground to a halt. reply Nevermark 1 hour agorootparentIt is a wonderful indispensable device for a very special market. Particular Mac users who happen to particularly like the physical and mental ergonomics, and locational and furniture freedom, of arrangeable virtual screens and a beautiful visual isolation chamber on demand. (And in my case, who have dramatically customized the light shield and straps to be super comfortable for long periods. I would buy the next version at twice the price if it came out tomorrow. And give up a lot to do it. But that is a VERY niche market. There are only three of us happy campers, after the return wave. It is definitely not an iPhone - yet. Personally, I think they should lean into it as a MacBook Pro killer. Make it a first class pro computing device. That is a good rationale for keeping around a high end spec, high priced version. Then have Air versions when it becomes possible to ship a cheap enough iOS-computing level version for the masses. reply mcmcmc 9 hours agorootparentprevI think it really depends on if you think Meta wants to be a hardware company or if they want to continue being a data/advertising company. reply williamcotton 12 hours agorootparentprevDon’t agree with this take. Is this a command or an opinion that left out the subject of the sentence? reply boomlinde 3 hours agorootparentThe inference engine between your ears should be able to answer that. reply imagineerschool 4 hours agorootparentprev'take' as in 'hot take' intended meaning : 'I disagree with your assessment and conclusions' reply potatolicious 14 hours agorootparentprevIt feels less like low vs. high-end and more like specialized vs. general hardware. For example, if you're selling VR headsets for the purposes of industrial training, you may not want the consumer-grade hardware Meta is selling. You may need weather-sealing to allow outdoor operation. You may need vastly higher-resolution screens for industrial applications. The list of specializations goes on. The specialized businesses tend to have wider moats and bigger margins. The TAM is smaller - too small for a mega-cap company like Meta to care about, but nonetheless can contribute to the health of the ecosystem. This play gives influence over these niche, specialized uses of AR/VR without having to commit the entire company to it. For example think of a medical instruments company that trains on VR headsets. Their choices right now are to use consumer-grade hardware which may not hit all of their needs, or become a full-on AR/VR company with all the requisite R&D that involves. This allows these companies to exist in the middle ground - having the core R&D being done by another party, but having sufficient control to ship specialized hardware. reply stronglikedan 13 hours agorootparentprev> The people who want to buy a Honda aren't the same people who want to buy an Acura, even though it's (essentially) the same parts. Nit, but as an Acura driver (chooser), I can tell you that they are definitely not essentially the same parts. The irrelevant parts are the same, but everything that matters to the driver (suspension/drivetrain, interior materials, technology, etc.) all all different and better in the Acura. I get what you're trying to say, but that was not a good metaphor. reply 0x457 11 hours agorootparentprevYes, but also there are currently so few components to pick from. SoC is definitely some kind of variation of Snapdragon XR2 unless you're Apple. Can't go into high-end because for a device to make sense either an existing ecosystem around it or high confidence in one appearing. If you tell me that I can buy a 3k dollar vr headset that can run current quest library, I would pretend you're joking. Mid-end is where we're at right now has/had very small margins because despite it being mid-end - you still have to use high-end components due to lack of options. I can see someone like Porsche Design making a \"high-end\" headset (in terms of price, components would be the same). The only option for low-end is to use components previously used in mid-end that would need to compete with used previous gens since they would be nearly identical on the hardware level. reply mdasen 14 hours agorootparentprevI feel like this is a bit off. There have been things like Porsche phones, but those are so niche that I don't think they're really worth considering. They happened, but they haven't been a long-standing product. They were a cash grab where they licensed a brand. Now, Hondas and Acuras are different products. You can say \"oh, they're essentially the same\" and if you truly believe that, I'll sell you a Core i3 processor for the price of a Core i7. Yea, they're essentially the same, but it's the differences that make one better than the other. The point is that the high end isn't about branding. The high end is about capability. Apple has shown that their iPhone will outsell any luxury-branded Android phone to rich people because some things are about capability, not a logo. Samsung's flagships will way outsell some luxury logo smartphone too. The high end here is really about devices with better capabilities and it allows companies with good hardware businesses (like ASUS and Lenovo) to build something in the Meta VR ecosystem. It's also possibly a way for Meta to stop dumping Quest devices. They'd rather just own the ecosystem rather than doing the hardware. If they can get ASUS, Lenovo, and others to do the low-margin hardware work and pick up the tab for a lot of the marketing, that's a win for Meta. Maybe Meta simply backs out of hardware over the next 5 years if a nice third party hardware ecosystem arises. But I think this is going to be tough with VR. When you're trying to make an immersive experience, you need a baseline of hardware. It's also easier when you know the hardware you're trying to target. Android development can be frustrating because there's so much variance in speed and capabilities. One of the reason gaming consoles exist is that targeting a small set of hardware/capabilities makes things easier. That's not to say that PC gaming doesn't exist, but it can be hard because gamers need to spend a lot of money on hardware and there's a variance in capabilities that you need to account for - and who you might simply exclude. With a phone, it's less of an immersive experience for most apps which are just displaying something. They might display it slower, the UX might be laggier, etc. but it works. VR can't be laggy. In some ways, it feels like Meta is trying to become a game console company without having to subsidize the console. That would be big if they can pull it off. I guess in many ways this is what Steam pulled off on the PC - taking a 30% cut without having to subsidize any hardware. We'll see if Meta can do the same for VR. reply zztop44 10 hours agorootparentI don’t disagree with your core point, but branding is about a lot more than a logo. I think iPhones do sell to rich people because of branding, because iPhone’s brand basically is “the best possible phone you can get, plus it integrates seamlessly with your other Apple devices”. And Samsung flagships’ brand is “the best Android phone”. Porsche, or whatever, have a great and meaningful brand when it comes to cars, but that doesn’t translate to smartphones. So when you see a phone with the Porsche logo the brand isn’t really gonna do much for you. reply schneehertz 9 hours agorootparentprevThe previous company that gave up hardware and only focused on software is called SEGA. reply v01d4lph4 4 hours agorootparentprevI think their RayBay Meta Glasses is a great example of a product like that! reply rokkitmensch 12 hours agorootparentprevI strongly disagree that the low end of the market is taken. XReal's Air/2 are awesome and Moore suggests we'll see awesome displays in that form factor, not even necessarily from XReal. reply s0rce 13 hours agorootparentprevReminds me of the Vertu cell phone, which the iPhone basically killed. reply MichaelZuo 14 hours agorootparentprevOr more like those Lamborghini and Porsche branded phones. reply distortedsignal 14 hours agorootparentprevSomeone teach this man how to spell \"Gabbana\". reply mkozlows 8 hours agorootparentprevBecause you want to compete in a different market segment. To put this very concretely, look at Pimax. Their Crystal headset allegedly has standalone capability. All the hardware is there for it, they make this thing, they sell it -- but they don't have a standalone platform, and have no hope whatsoever of being able to develop one. For PCVR, they just use regular standard SteamVR and OpenXR, but for standalone usage, they suddenly have to be an amazing software company and they definitely are not. Using Horizon makes a ton of sense, and makes their product more attractive and better than it is without it. reply ryanbrunner 13 hours agorootparentprevGoogle did this early on with android - originally the Google devices (Nexus) were lower end, and high end devices were left to other manufacturers. They've flipped around recently, but I think the Nexus line was a decent enough idea at the time. reply ryukoposting 13 hours agorootparentprevAre they capable of selling hardware loss leaders indefinitely? If so, can they do it at a scale that matches the company's digital presence? The Google Daydream headset sitting on my shelf is skeptical. Regardless of whether they can sell the hardware at a loss forever, they probably won't need to. Third-party hardware is engineering labor that Meta doesn't have to pay for. In fact, it's engineering labor that will pay Meta through royalties. Cost-cutting measures developed by those third parties can easily be copied by Meta's own product, reducing the cost of future versions of the hardware. Cheaper options in the marketplace also help Meta gain market penetration without their own hardware developing a reputation for poor quality. After a couple generations, vendor lock-in will start to set in, and they'll be able to charge more without losing customers. The aforementioned cost-cutting techniques start to pile up, too. reply wvenable 11 hours agorootparentprevSeems like without some kind of software revenue sharing model it doesn't make sense. These things are consoles -- sold a cost or below cost in order to make it back on software sales. reply richardw 13 hours agorootparentprevits partly in the article. Companies like Xbox and Lenovo want different experiences for their clients. This allows them to share the basic platform but specialise for their customer segments, eg professional or Xbox owners or whatever. Architects and civil engineers and doctors might not want normal game controllers and the default resolution. Cheap devices used in developing countries can have a lower resolution and no hand tracking. Meta wins because they can’t build 100 versions themselves, all they care about is growth of the market. Nothing is locked up yet, think of a market 100x bigger. reply threeseed 9 hours agorootparentprevBecause Meta will be focused on the consumer market. Which leaves a huge opportunity for professionals and businesses. reply __s 14 hours agorootparentprevBecause you want to use it as a basis for military VR or something reply lttlrck 9 hours agorootparentprevPCVR maybe. There are latency optimizations to be made (preferably, for me, by adding DisplayLink) reply samspenc 16 hours agoparentprevLike other comments have pointed out, I think of this as similar to how Google controls the Android OS. While theoretically open, the real useful stuff on top of Android (Google services etc) require a license from Google along with their Play store, so Google makes revenue from there. Certainly more open (edited) than the Apple ecosystem, but still controlled by one (big) player, with a little bit of flexibility but not a whole lot. reply talldayo 15 hours agorootparent> A bit more open than the Apple ecosystem It is not \"a bit more open\" than iOS, it is a completely different approach to OS development that enables wildly different results. > While theoretically open, the real useful stuff on top of Android That stuff is by no means necessary; I've run Android without Google services for years and it just feels like a normal tablet OS. Again, it is not \"theoretically open\" but in fact practically usable without any first-party services, unlike iOS. reply samspenc 15 hours agorootparent:D Fair, I didn't want to be yelled at by the Apple fanbase that comes out in defense to comments like this, have edited my original post with a note. reply bsharper 16 hours agorootparentprevAmazon used AOSP to create tons of their products. Even if most Android devices have the Play Store, there are successful variants that don't. And I'd even include Meta's Quest line here: every headset since the Go has the ability to sideload apks using standard Android tools. reply hadlock 12 hours agorootparentMy quest (1) I bought on launch day got an OS update and now I have to find my old credentials to log in (and thus accept some (probably) draconian EULA, just to enable developer mode to side load apps on there. You can't just plug a USB stick in the side and copy paste them to \"side_loaded_apps\" dir it's still a ridiculous task of doing backflips through flaming hoops to use it as an open device. And it may yet again reset my login. reply ec109685 16 hours agorootparentprevYou’re discounting the huge number of android devices that use it as a base OS. E.g. Amazon and Peloton use android without Google services for consumer devices. reply babypuncher 15 hours agorootparentThese are devices built with a specific use case in mind that need an operating system that is easy to develop for. Non-Google versions of Android have struggled to make much headway in markets like smartphones. Even Amazon's own phone flopped pretty hard. reply sangnoir 15 hours agorootparent> Non-Google versions of Android have struggled to make much headway in markets like smartphones You may have forgotten about China: Huawei smartphones are doing very well on an AOSP-based OS. Edit: Google was concerned about a Chinese fork of Android flourishing, they wrote to congress at the time the tech-transfer ban was being condidered reply rchaud 15 hours agorootparentprevThe thing about Android is that a device doesn't have to make headway by selling 100 million units plus to continue being adapted to many different types of devices. In any case, the Play store can be sideloaded on every Android device even if it isn't officially supported. reply indymike 15 hours agorootparentprevKindle tablets on the other hand have done well. reply kernal 10 hours agorootparentprev>Non-Google versions of Android have struggled to make much headway in markets like smartphones. Even Amazon's own phone flopped pretty hard. Not much headway? Pretty much all Android phones used in China do not come with GMS. reply giuseppe_petri 12 hours agoparentprev> So there’s no kind of existential danger to Quest here, but it remains to be seen if the hardware licensees can bring anything relevant to the table either. How does Meta make its money? Advertising. Licensees bring actual eyeballs to the table and eat the hardware costs fighting amongst themselves selling commodity hardware and Meta re-position Quest as a 'premium' product that they might actually make a little money on. reply _giorgio_ 12 hours agoparentprevWhy should it be different than Google and Android? Google makes medium and high cost devices, but actually doesn't compete too much on margins. If the OS catches up, meta will do the same move, allowing other vendors to proliferate. reply chiefalchemist 9 hours agoparentprevThe other important difference between Apple then and Meta now is that Apple wasn't in a position that subjected it to possible (government) regulation. Meta has to try to play nice. Perhaps few vendors bite? But when the regulation noise increases - and it will - Meta can say it tried. reply nvy 10 hours agoparentprev>Whereas for Meta, the Quest hardware has always been sold at breakeven or even as a loss leader (a few years ago they actually raised the price of the Quest 2 to cut further losses). Indeed, and that means they'll seek a return on their investment from continued invasive surveillance, emotional manipulation, and cramming ads everywhere. reply modeless 17 hours agoprevOfficial blog post: https://about.fb.com/news/2024/04/introducing-our-open-mixed... Short video from Zuckerberg: https://twitter.com/NathieVR/status/1782436898654273981 This is an interesting move and feels like a response to complaints that Meta is hypocritical when complaining about closed platforms while running one themselves. But this isn't open source. I don't know why any OEMs would want to compete with Meta's hardware subsidized by app store revenue when they continue to own the store. Maybe there's an app revenue share involved? Wait, at the end of the video he says \"We're also as part of this going to be opening up our store to give you even more options to use whatever experiences you want. So whether they're on Steam, Xbox Cloud Gaming, our own App Lab, or even Google Play, if they're up for it.\" The blog post doesn't mention Steam or Google Play. It's not really clear what that means. Will they allow Steam to sell native Quest apps? Edit: There's a better blog post that has more detail. https://www.meta.com/blog/quest/meta-horizon-os-open-hardwar... This one seems to suggest that being \"open\" to Steam just means allowing game streaming which they already do, while being \"open\" to Google Play means that they would allow Google to install the actual Play Store app on the headset, for 2D apps only. But Google doesn't want to. In any case it seems like they would specifically not be open to alternative app stores selling native 3D apps directly on the headset itself. reply ejj28 17 hours agoparentI think this is at least better than nothing for companies who want to build standalone headsets and not just headsets that are dependent on PCs. Up until now everyone's had to make their own OS and store and hope that people care enough to port over apps and games. At the very least, this could lead to more high-end standalone headsets being available. Not every 3rd party headset has to be competing with the Quest line of headsets, so the lack of revenue from the store might not matter to some companies. reply Mindwipe 16 hours agorootparentIt seems to suggest they're limited to Qualcomm chipsets, and Qualcomm don't make a higher end VR chipset so it's hard to see where a high end headset would come from. reply throwthrowuknow 13 hours agorootparentDisplays and optics are a big one, lots of competing technologies there like OLED vs LED, pancake vs fresnel, movable optics, laser based displays, video pass through vs semitransparent or even HUD style for lower res overlays. They can also compete on audio quality for microphones and headphones, different battery solutions like hot swappable packs, corded or built in. Maybe different form factors that can distribute weight or higher quality head straps for comfort. Tracking options like more cameras for inside out or a different system for pairing with controllers or full body trackers. Even external dedicated compute that works with Air Link. If they’re making the hardware they can add whatever extra chips or sensors they want. reply entropicdrifter 12 hours agorootparentThat's not even considering the possibilities of using eye tracking for foveated rendering to combine high resolution and long battery life reply zmmmmm 8 hours agorootparentprev> Qualcomm don't make a higher end VR chipset I don't think that's a given long term. Even already the chip in the Quest 3 is rated at the same GPU grunt as an nVidia 1060, the minimum supported chipset originally for PCVR. The next gen is already announced and is significantly more powerful again, able to power 4k displays. I would project in 3 years from now we have something that can actually be considered at least moderately high end in stand alone form factor from Qualcomm. reply mrmetanoia 13 hours agoparentprevThis seems kind of interesting to me, it's a shame it's from Meta so I'll likely never touch it because of the toxicity of their core products, but I'm glad to see this, as it seems like it could stimulate some competition. reply philo23 10 hours agoparentprevI think the mention of Steam will be to do with the Steam Link app they released for Meta quest devices fairly recently that lets you stream your VR games from your desktop computer to the quest wirelessly https://help.steampowered.com/en/faqs/view/0E2C-406B-9135-38... reply Mindwipe 16 hours agoparentprevYeah, if it was \"we acknowledge that is bad for hardware to have one installation path\" and they were allowing itch.io to have an indie VR store that might be cool but they are very much not. reply mebazaa 17 hours agoprevMeta has a lot of work to do on DevEx for non-gaming experiences. Say what you want with the Vision Pro, but it comes with a lot of niceities like SwiftUI. When you develop with the Quest, you're stuck with Unity or Unreal Engine -- it's almost too much freedom to develop simple productivity apps. reply bschmidt1 13 hours agoparentIt's all going to end up in JavaScript anyway. I say this as a joke, yet: https://www.npmjs.com/package/react-unity-webgl Simply rendering your Unity Application within your React Application is just the beginning! The Unity Context exposes a lot more fun functions and properties to play around with such as two way communication or requesting fullscreen or a pointerlock. The possibilities are endless, what's next is up to you! Love it or hate it. Everything ends up in JavaScript! reply zmmmmm 12 hours agorootparentA video about their upcoming spatial SDK (\"augments\") already leaked [0] and you are correct, it's based on JavaScript, using their Spark toolkit [1] which is hardly surprising - when the company already ships a production AR dev kit, why would they not use it? [0] https://www.youtube.com/watch?v=svlL_ndNdj0 [1] https://spark.meta.com/ reply ccheney 3 hours agorootparentprevAtwood's law reply tstrimple 11 hours agorootparentprevI don't hate it. JS has its worts, but so does every language. I don't think there is a single language even close to JS for lowering the bar of entry to software development. All of the bad things about it that \"real computer science\" folks complain about are the exact same features which give it a far more broad reach than Rust or Golang. Software development experiences the exact same problem that the internet at large has suffered. The lower and lower bar of entry invites less and less competent folks to create things they never before would have been able to create. Some of these things are genuinely useful and we benefit greatly by lowering the bar, and most of these things end up being shit because the people who made them really had no clue what they were doing. Lowering the bar of participation greatly increases the amount of shit that is created, but also increases the amount of exceptional examples which can come from a domain. If 80% of everything created is almost complete garbage, then lowering the bar of entry for participating will exponentially increase the amount of shit out there. But it will also enable a few really great apps which wouldn't have existed otherwise. I think we need better filtering than to raise the bar so only true experts can participate. reply mjburgess 41 minutes agorootparentOr it just happened to monopolise the browser at a time when the web was being invented, and has nothing to do with the features of the language. As a language for new developers is terrible: 100 different ways to do anything, most of them a muddle of paradigms that's inexplicable to anyone without 2 decades of experience, and so on. Imagine a new developer using ChatGPT to generate: python, C, go, etc. vs. generating javascript. Most of the generated js is incomphrensible to newbies, but for the others generally obvious. reply btown 12 hours agoparentprevReposting a comment I wrote on a comparison with Vision Pro here, which is very relevant here: https://news.ycombinator.com/item?id=39830713 > The biggest innovation with Vision Pro is visionOS. visionOS provides native app frameworks, so developers can build apps for it. That sounds ridiculously obvious, and yet its something Meta have failed to offer for years. Every app on Quest has to reinvent how buttons work, how a scroll view works, how far away from the user the content should be etc.. and every app works differently. On visionOS, all of this is handled by Apple, and every app looks and feels the same. Meta does have standardized utilities for translating movement to touch/drag/etc. interactions on arbitrary virtual surfaces: https://developers.facebook.com/blog/post/2022/11/22/buildin... https://developer.oculus.com/documentation/unity/unity-isdk-... But it doesn't seem (AFAIK) to answer the other side of this, which is the UI design system so apps have a consistent look and feel. Which is perhaps more common coming from a game development perspective, but ever since the Mac OS shareware days, Apple's understood that it's empowering to a certain kind of developer if you make it easy/the default path for them to build experiences that match a standardized look and feel. I'm honestly surprised that Meta didn't at least make an optional SDK for this. reply atrus 12 hours agoparentprevYou can also use Godot (although def has the too much freedom issue) and a-frame as well. The latter might be more attractive to webdevs reply gorbypark 12 hours agoparentprevAt one point in time the React/React Native teams put out a blog post devoid of any actual details about “multi platform” support and mentioned VR in it. I’m surprised I haven’t hear anything else about it since. reply gr__or 42 minutes agorootparentYeah it's kind of mind-boggling to me that the creators of React (which SwiftUI is ofc heavily inspired by) and the most successful cross-platform quasi-native library (React Native), have not managed to bring those powers to their biggest investment. I truly don't get it. reply pavlov 17 hours agoparentprevOpenXR Mobile SDK is the native development option for the Quest devices: https://developer.oculus.com/documentation/native/android/mo... reply vineyardmike 17 hours agoparentprevDon’t they also have a web view wrapper for 2D tools? reply gryn 16 hours agorootparentfor 2D you can also plain old android apps, they work here. the point though is that there's not that much room for 3D stuff without going through either unity or unreal or writing everything yourself from scratch. if your goal is to make some sort of `spatial computing` tool, well there nothing here you can use. each app is it's own little silo that has little room for interaction. I'd love to be able to write my own custom apps that can exist in the home screen/environment and that can interact with each other in non trivial ways. it would make it feel more like a personal space rather than a 3D slideshow that I can use to launch games. reply ThinkBeat 11 hours agoprevAs usual I get quite hopeful when I hear about a new operating system. We so absolutely need paths to move away from what we have today. I didn't even know what \"mixed reality operating system\" was so that made me hopeful. May include some nifty quantum processing. Alas, it is just stuff built on top of Android, which is built on top of the Linux kernel. Perhaps \"Meta Horizon SDK for Android\" would be a better label?` reply pjmlp 3 hours agoparentAndroid is already quite cool compared with what traditional OSes happen to be. For starters it is what Microsoft has failed to do with Longhorn, because contrary to having internal sabotage from the kernel team, the Android team doubled down on their efforts to make a managed userspace actually work in a mainstream product. Dalvik sucked balls against Sony and Nokia J2ME implementations, with its DEX interpreter, yet they managed to create ART with quite cool mix of JIT/AOT/PGO, and a very modern GC. Then it follows up, that for all the praise Plan 9 gets on hacker circles, Android's approach to user space closely follows on Inferno with Limbo, Plan 9's successor that many of its praisers tend to forget. Followed by being able to encapsulate Linux kernel into a micro-kernel like stack, where most drivers are process based using Android IPC to talk to the kernel, and can be written in a mix of Java, C++, and now Rust. If only they handn't screw Sun, and stepped away when the ship went down, maybe they could have saved themselves some headaches, however Google isn't really a language designer company, so maybe that was for the greater good nontheless. Ah, and using AIDL is so much better developer experience than dealing with the primitive COM tooling on Windows. reply sircastor 3 hours agoparentprevI was amused and interested when I found out that HaikuOS is not Linux, nor even the Linux Kernel, but a reimplementation of BeOS (concepts mostly) on the NewOS kernel. I've been dwelling in some nostalgia recently and miss some of the earlier days when everything wasn't so homogenous. This is all rose-colored glasses though, as I don't miss when everything was also extremely incompatible. And slow. And expensive. reply dfex 2 hours agorootparent> I've been dwelling in some nostalgia recently and miss some of the earlier days when everything wasn't so homogenous. You are not alone in this. I see announcements like this one from Meta and others like Oxide - companies out there having a swing in a different direction and it makes me glad that there are people out there thinking outside the monoculture that computing feels like it's become sometimes. Sure, standardisation has been good in some respects, but I feel like the incentive to really innovate (as opposed to incrementally improve) in the operating system and compute space has really dried up. reply llm_trw 10 hours agoparentprev> May include some nifty quantum processing. While we're at it they might do a bit of dabbling in reversing entropy and solving the halting problem - it's only impossible in flat space-time after all. reply BizarreByte 10 hours agoparentprev> We so absolutely need paths to move away from what we have today. Why though? What necessitates our move away from what we have? reply hanniabu 9 hours agorootparentDiversity = freedom reply aporetics 8 hours agorootparentNope reply Panini_Jones 7 hours agorootparent\"Nope\" doesn't really contribute to the discussion. Why does diversity not equal freedom? reply snvzz 10 hours agoparentprev>Alas, it is just stuff built on top of Android, which is built on top of the Linux kernel. Look into HarmonyOS, Fuchsia, LionsOS, Managarm, Haiku and Genode. reply rainsil 10 hours agoparentprevWhat unexplored space do you think is ripe for a totally new operating system? reply whywhywhywhy 0 minutes agorootparentDreaming of a world where normal PCs could have an OS on the quality level of MacOS with the software ecosystem but Linux is just too fragmented that you can't build anything coherent or reliable on an interface level in the way MacOS is. This isn't saying Linux is bad, its great at what it does and being modular but how can something like MacOS spell check where it works on any text field coherently across any app work in a modular world like that. Windows just feels like bad decisions made decades ago hold it back, it can't even open a folder of 20 thumbnails without choking, MacOS can handle 10,000 like a knife through butter. Can't even find a file opened yesterday when you search for it in the start menu by exact name, MacOS manages in a fraction of a second. Maybe the right team could rewrite all the things causing this jank but you'd essentially have to replace so much of the company that caused it anyway that it doesn't seem feasible. Still use all 3 for different tasks but MacOS is the only one that feels like an operating system should feel in 2024. reply klabb3 10 hours agorootparentprevIronically, general purpose computing. Like servers, smartphones, etc. The premise of Fuchsia, as a prominent example, is to build an OS that operates more like a sandboxed/capabilities-based microservices platform, with structured IPC across processes. All major platforms both cloud and clients (even browsers) have gone to great lengths to deliver precisely that on top of existing OSs, with many expensive layers and hacks. It would be a lot nicer to have it built into the OS. Does that mean it’s truly ripe? I don’t know. reply marcus_holmes 1 hour agorootparentUnikernels. Throw away the OS completely, and just run the server application on the hardware/hypervisor. That would be my answer for servers. You kinda don't need an OS for them. reply refulgentis 10 hours agorootparentprevDumb, probably overly broad Q: I'm looking to build a hardware project off a flutter app. Linux on Raspberry Pi is the straightforward option. But I want an excuse to build on Fuchsia. Let's pretend Fuchsia is just as easy to deploy on (I'm 99% sure it won't be). Does Fuchsia buy me any cool stuff long-term that isn't possible on Linux? Like, what's my excuse for going with Fuchsia other than \"(handwaves) it'll be more secure\"? (to be clear, im teasing myself and my understanding of Fuchsia, not your explanation) reply klabb3 8 hours agorootparent> Let's pretend Fuchsia is just as easy to deploy on (I'm 99% sure it won't be). To stress this beyond a doubt: don’t do this unless you’re willing to become an early-masochistic-martyr adopter. My knowledge is outdated by years though. I don’t know how far you’d get today. > Does Fuchsia buy me any cool stuff long-term that isn't possible on Linux? I don’t think so. Fuchsia shines with capability-based multitenancy, and basically anywhere you would have different processes from different vendors communicating together (like a mobile OS with multiple partially trusted apps that may need to talk to one another). If you own/audit all code on your device, especially in a single app, you don’t gain anything from that. Some auxiliary stuff like content addressable file system and OTA deployments may be attractive, but I have no idea if those things are actually supported or even around these days. Oh, one thing that may be worth tracking in your domain is how fuchsia is progressing in terms of power management. If you’re on arm and battery power, and they prioritize it, it may beat Linux & android in terms of low power devices. But that’s pure speculation. reply lp0_on_fire 10 hours agorootparentprevA space where operating systems exist to service the operator and not the OS vendor. One that isn’t just a vehicle to push ads and subscriptions to -as-a-service or otherwise act as a siphon to send telemetry up to the mothership. Yes, I know *nix exists but it’s so fractured and every little variant has its own quirks making it difficult to be a general purpose OS that the masses could adopt, IMO. reply klabb3 10 hours agorootparent> Yes, I know *nix exists but it’s so fractured and every little variant has its own quirks making it difficult to be a general purpose OS that the masses could adopt, IMO. Yes, but to be clear: the main hurdle for adoption is fragmentation across GUIs/installs/package management across distros, which is out of scope (for better or worse) from the POV of Linux. Linux is a technological marvel, and if “they” could sort out these issues it would be the shortest path to mainstream appeal, by far. I’m certain this is technically feasible but also extremely challenging to pull off from a leadership perspective. Unfortunately, I don’t think there is a solution the majority would embrace. There is a ton of flame wars to overcome, hills people will die on. reply rainsil 9 hours agorootparentprevIt seems like if you created a new operating system to solve these problems and it gained some traction, you'd fracture the landscape even more. Unless your operating system happened to be superior to all the existing solutions in all aspects with no tradeoffs. Relevant xkcd: https://xkcd.com/927/ reply meiraleal 10 hours agoparentprev> Perhaps \"Meta Horizon SDK for Android\" would be a better label?` Yes, the same as Android SDK for Linux. But it became just Android. reply BlueFalconHD 6 hours agorootparentWhat about \"Meta Horizon SDK for Android SDK for Linux?\" reply jessriedel 10 hours agoparentprev“Operating system” is being used to refer to the UI for running and managing apps (not the original under-the-hood sense of the word for how app share resources, manage security, etc.). For better or worse, that is how consumers understand it. reply classified 3 hours agoparentprevAnd you seriously expect an ad company to improve anything? What's Google missing? reply w10-1 11 hours agoprevIf hardware manufacturers actually wanted this, Meta would be announcing a licensing deal. This is a threat to Apple: if Apple doesn't relent on advertising/privacy in VisionOS, then Meta will do to VR what Google did for smartphone's: sell the market to maintain advertising access. Meta doesn't care about money or mindshare on VR. They just want ad access. reply zmmmmm 8 hours agoparentMeta isn't even trying to get anything on Vision OS at all, I'm sure they have no interest or care about that until and unless it becomes a mass market product. They don't want to be locked out of freedom to do what they want, and that's a primary motivation here it's true. But the obsessive focus on ad revenue that people assume they have is pretty far from their thoughts in the VR/AR space at this point. They have complete faith that being the dominant owner of the dominant platform that they think the future of computing will revolve around (I'm not arguing this, but I'm satisfied it's what Zuckerberg believes) will yield sufficient value to be worthwhile and they aren't worried about how it happens at this point. reply EchoReflection 10 hours agoparentprevthey want ad access because they care about money and mindshare (a term I hadn't heard of until reading your comment, so thank you for that!) reply hooloovoo_zoo 10 hours agoparentprevApple and to a lesser extent Google have high trust. Nobody trusts Meta at all. So who would adopt their OS? reply m463 6 hours agorootparentI think apple has high trust only because they repeat their message all the time. However, just buy an apple device and try to use it. You cannot use it without apple allowing it. You must connect to apple and activate your device. And as part of activation, you get a chance to read apple's privacy policy. Literally thousands of pages of privacy policy without the ability to DO anything, like say \"No\". And everything in the os phones home, continuously. If you are a european and disable location services, and use a european vpn, apple still won't let you use the european app store because the phone knows your location. sigh. reply genidoi 4 hours agorootparent> apple still won't let you use the european app store because the phone knows your location. The national app store you get to use (the region of your iphone) is based on your billing address + card details (which obviously are validated when you enter them), not your phone location. reply m463 3 hours agorootparentHere is the comment that said different? https://news.ycombinator.com/item?id=40069509 reply simondotau 2 hours agorootparentprevActivation makes your phone significantly less valuable to thieves. That’s an incredible service that Apple is doing for me. Curious, are you saying that if I buy a Samsung Galaxy or Google Pixel phone, I don't have to click “agree” on any license? reply wraptile 1 hour agorootparentI've never had my phone or tech device stolen in general. This anti theft stuff is a red herring or a distraction from real societal issues. Something along the lines of \"sell us your rights and the device that doesn't really belong to you will not be stolen from us, I mean you.\" reply simondotau 1 hour agorootparentI'm glad to hear you've never had anything stolen and aren't planning to have anything stolen in the future. That's a great plan and I'm kicking myself for not thinking of it myself. Oh and here's a great money saving tip: if you've never been in a car accident, don't waste your money on car insurance. Nobody is forcing you to buy anything from Apple. I do buy Apple products fully aware of the relationship I'm entering into and I'm very happy with it. I consider activation lock to be a positive feature which increases the value of an iPhone for me. An article from 2015: https://techcrunch.com/2015/02/11/apples-activation-lock-lea... reply TiredOfLife 2 hours agorootparentprevAnd that is weird. Apple gives Chinese government direct access to user data. Apple obeys every demand from China and Russia. Meanwhile Meta communications director Andy Stone just got 6 years in russian prison for allowing people to say mean things about russia. reply dustypotato 51 minutes agorootparent\"Got\" as in got convicted in absentia. He's not in prison! reply ceroxylon 8 hours agorootparentprevAs a Quest user: mostly younger devs. I see 18-22yo developers who do not care about the history of Meta making very impressive games that attract large amounts of their peers, adding micro-transactions along the way to fund their development, and it works, they make a large amount of money and have very active communities on Discord. reply Panini_Jones 7 hours agorootparentWhat are the names of the games? reply rmbyrro 8 hours agorootparentprevApple and Google have high trust? In what world? In terms of privacy, Apple has more trust, yes, but the daily stream of zero days on iPhones hurt it by a good margin. I won't even start talking about Google's privacy issues... reply genidoi 4 hours agorootparentI fully trust apple photos / g photos, on an iphone. Idk/idc about the \"daily 0 days\" that you referenced, I do care about being able to know with absolute certainty that my photos will be there 20 years from now. reply threeseed 9 hours agorootparentprevNot sure why you wouldn't put Google and Meta in the same basket. Both are advertising companies with a history of working to undermine pro-privacy initiatives. reply hooloovoo_zoo 5 hours agorootparentIt's an inherently subjective assessment obviously, but I can justify it with e.g. survey results https://theharrispoll.com/partners/media/axios-harrispoll-10.... reply marcus_holmes 1 hour agoprevHas anyone actually tried the Metaverse yet? As I understood it, this was basically a massively expensive failed experiment so far. No-one is using it for reals (yet?). Who's the market for developing apps for it? reply esskay 52 minutes agoparentTried it, was very unimpressed, it feels very much like a tech demo even as recent as this year. Certainly wouldn't waste my money on it. reply modeless 16 hours agoprev> And we encourage the Google Play 2D app store to come to Meta Horizon OS, where it can operate with the same economic model it does on other platforms. It sounds like they are specifically not open to third party app stores selling native AR/VR (\"3D\") apps. I can see why Google might not want to participate if they're not allowed to compete. This feels like a response to complaints that Meta is hypocritical when complaining about closed platforms while running one themselves. But they aren't open sourcing the OS. I don't know why any OEMs would want to compete with Meta's hardware subsidized by app store revenue when they continue to own the exclusive store for native AR/VR apps. Maybe there's an app revenue share to sweeten the deal for hardware partners? reply modeless 13 hours agoparentSorry for the double post. One of my comments was moved here from a different submission so now there are two top level comments from me, and I can't edit or delete them because the edit window has closed. reply TiredOfLife 2 hours agoparentprevhttps://sidequestvr.com/ exists. Also Google is pushing their new AndroidXR os behind the scenes (after they killed Daydreams). reply cmiles74 13 hours agoparentprevThis seems like trying to come up with an answer to Apple Vision's ability to run iOS apps. If you can install apps from Google Play then you could, if you wanted to, check your email from inside your headset. I'm not sure many people would want to do that, but if Apple thinks it's a good idea, blah blah blah. reply jayd16 10 hours agorootparentIts not a killer feature, but having to take off the headset to read mail or slack or whatever is a real drag. It will quickly become table stakes. reply bsimpson 15 hours agoparentprevI think you're overindexing on \"2D.\" The Play store is full of flat apps, designed for phones and tablets. There are a gajillion of them, and FB wants them in their headsets. It's not about not wanting competition - it's about wanting people to have more than 100 apps available when they use a Meta headset. reply modeless 13 hours agorootparentThat's what it's about for Meta, sure. But for Google it would be about selling apps on a headset, and being prohibited from selling the native type of app for the platform would be pretty bad! Meta wants to have their cake and eat it too. If it's their intention to allow third party stores to sell native Quest AR/VR apps on the headset in competition with their own store, they should state that explicitly because what's written here pretty carefully doesn't imply that. I don't think we can just assume what isn't stated here. reply int_19h 14 hours agorootparentprevBut why would I as a user want to use those flat apps in VR, when I already have a phone and a tablet? reply modeless 13 hours agorootparentOn current headsets, just because it's annoying to have to take the headset off to do things, and also to be able to use those apps in conjunction with other services like meetings in Horizon Workrooms. On future headsets with higher resolution and better comfort, because it would be a legitimately better experience in many cases. reply wvenable 11 hours agorootparentprevIs that what the majority of Vision Pro users are doing in VR? Projecting flat monitors around. reply int_19h 10 hours agorootparentI can see the point of projecting a huge flat monitor in VR to, say, write code on it. A mobile app, not so much. reply 0x457 7 hours agorootparentWell, kinda required for proper usage IMO. Let's forget display limitations and pretend I can project a giant code editor straight onto my eyeballs. I don't care about it if I have to take it off, to do any of the following: - Watch YouTube as I do something else - Listen to music and have control over the player I can see myself wearing a headset playing TFT, watching YouTube and have a build guide open in a browser. I can currently do it on an iPad and Vision Pro - https://www.reddit.com/r/leagueoflegends/comments/1akjfpj/le... (different game, but you get the idea). reply chaostheory 12 hours agorootparentprevHave you seen the Apple Vision demo(s)? Now imagine being able to have wall sized versions of Android apps like Netflix or even word processing apps like Google Docs. reply hatthew 11 hours agoprevJoel Spolsky's \"Commoditize your Complements\" [0] seems relevant here. From that perspective, it seems like Meta is trying to commoditize the hardware and monopolize the OS (and likely the app store and payment system), similar to Android. [0] https://www.joelonsoftware.com/2002/06/12/strategy-letter-v/ reply 999900000999 17 hours agoprevThis is awesome, I'm waiting for a VR manufacturer to add an HDMI input. Then I could use it as a monitor without jumping through a bunch of hoops. reply TrueDuality 17 hours agoparentYou're not going to get HDMI (or at least not directly), but you can now get DisplayPort with many of the XR headsets. They're primarily using the alt-mode of USB-C as the cable is re-used for power as well. reply 999900000999 16 hours agorootparentWhich headsets support this? reply jsheard 17 hours agoparentprevThe XREAL glasses are trying to fill the niche of a headset that just mirrors a HDMI input on a big virtual screen. I don't think anyone is currently making an all-purpose VR headset which can also do HDMI mirroring though. reply m463 6 hours agorootparentI thought about getting some xreal glasses, but I think you have to activate them to use them, which annoys me. reply 999900000999 17 hours agorootparentprevI understand, but it seems trivial to add HDMI input here. Technically you can just remote desktop into a computer from your VR headset, but that won't work in every scenario. reply int_19h 14 hours agorootparentFor Quest 3 specifically, \"remote desktop into a computer\" actually works surprisingly well if you 1) avoid the stock Meta software and use Steam Link instead, and 2) use wired connection to maximize bandwidth and minimize latency. The second part needs some explaining. One undocumented feature of Quest 3 is that it supports (some) USB-C Ethernet adapters. There isn't really any UI for it that I know of; things just work so long as DHCP is there. This then gives you a direct wired 1GBps link to the PC, which Steam Link will happily utilize. reply RockRobotRock 10 hours agorootparentI have xreal glasses and a Quest 3. There's so much friction in using Steam Link/Quest Link/Virtual Desktop that it's barely worth using over the xreal airs. You need to use controllers to turn them on, and if you lose tracking or exit your guardian, the display turns off and you have to grab your controllers again to make any adjustments. reply int_19h 10 hours agorootparentI agree; I use Goovis G3 Max myself when I need this kind of thing (which is bulkier and not AR, but has better FOV and higher resolution). But for people who already have a Quest 3, it can still be a useful trick sometimes. reply StrauXX 13 hours agorootparentprevImo a good WAP connected directly to your computer works just as well as with a high end cable. I have both and didn't notice a quality difference either way. reply jayd16 10 hours agorootparentprevEven the stock one is pretty good these days. reply jsheard 17 hours agorootparentprevIt's fairly trivial to add a HDMI/DP input which directly drives the panels in the headset through a mux (e.g. the Pico Neo3 Link could run standalone or from DP input), but that's probably not what you want, because in that case the HDMI source has to perform all of the 3D rendering and lens correction, using software that probably only supports Windows. If you want to be able to plug in any random HDMI source and have that rendered on a virtual screen then the headset needs a SoC with a low-latency HDMI receiver built in, so it can ingest the video and process it onboard before displaying it, and HDMI input isn't very common on these mobile SoCs. Maybe you could convert the HDMI input into MIPI and feed that into the SoCs camera interface, but I think headsets like the Quest are already pretty much maxing out the SoCs camera capabilities just to read in all the actual cameras used for inside-out tracking. There's no bandwidth left to shove an extra HD video feed in as well. tl;dr HDMI input that turns the device into a dumb PCVR headset: easy. HDMI input that mirrors arbitrary video: hard. reply numpad0 15 hours agorootparentprevYou don't want it. It's disorienting and uncomfortable. XREAL as well as some drone FPV goggles support non-/partially-head-tracked HDMI input, albeit with much smaller FOV for comfort reasons. reply int_19h 14 hours agorootparentThere are quite a few options for non-head-tracked wearable display type headsets. Those generally get pitched as \"portable cinema\" though, e.g.: https://goovis.net/products/g3max (This particular one uses USB-C for video input, but they also sell an HDMI adapter for it.) Of course, in practice it's just a display and can be used for any purpose. I do appreciate the fact that you don't have to mess around with all the usual VR setup chores with these - it's really very plug and play. reply RockRobotRock 10 hours agorootparentI would be worried about forking over $1000 to a fly by night company for a product which could easily break. (xreal is also in this category imo but at least its a bit cheaper) reply int_19h 9 hours agorootparentFWIW despite the weird name that invokes cheap Amazon noname brand vibes, these guys have been around for a few years now. They do have other headsets that are in the same ballpark price-wise as XREAL, too - e.g. GOOVIS Young, which is pretty decent for on-the-go use with a smartphone etc. I've owned that one for three years and used it many times with no issues before splurging on G3 Max. And yeah, $1K is quite a lot, but the 65 degree FOV and 2560x1440 (per eye) OLED that you get for it really does look amazing - I haven't regretted that purchase in the slightest. It's rather bulky, though, although still nowhere near as heavy as VR headsets. Still, not something I'd want to carry on the go. I hope we'll get 4K per eye some day with this tech. Whichever brand does it first for reasonable $$$, it'll completely replace my primary display. reply RockRobotRock 6 hours agorootparentThank you! that detail helps a lot. I wish these companies were better at giving off a professional vibe when they make genuinely good products reply m463 6 hours agoparentprevHmmm... I wonder if FPV goggles (from drones) could do what you want? lots of them have HDMI inputs... https://oscarliang.com/fpv-goggles/ reply ShamelessC 17 hours agoparentprev> Then I could use it as a monitor without jumping through a bunch of hoops. bold of you to assume your hdmi cable won't naturally form some hoops. reply banish-m4 2 hours agoparentprevLOL. The problem is the shitty software. SteamVR and Oculus apps are worthless and flaky, but when they work, there's zero need for any damn cables. PS: I have a barely used Meta Quest Pro for sale. ;) reply superkuh 14 hours agoparentprevThe original Vive has HDMI input. reply juliendorra 8 hours agoprev'Pivot to software' has been done several times in computing history: Go Corporation, SGI, Sun, Blackberry, Palm, Sega, and oh, NeXT. These are interesting precedents. I don't know of a 'pivot to software' that actually worked (NeXT ended up going back to hardware, thanks to the reverse acquisition!) There's also the question of the licensees: so they choose Meta’s OS as the base for their hardware, maybe because they don't have anything else? Quest OS is neither a best in class user experience (it's still full of bugs, frictions and is a very inconsistent UI), and it's clearly not a good dev experience at all. So your hardware now inherit all of Meta's tech and UX debts, and you don't even have a guarantee of daily recurring usage or of a killer application user will flock to. Is that the great win for their hardware that they think it is? Of course, as several of us predicted when the number of Vision Pro apps quickly surpassed Quest Store Apps, Meta is now forced to react and change their weird two-tier approach: they are opening a (small) marketing presence for App Lab apps and are creating a \"brand new\" spatial app framework, to start and get out of the Unity/Unreal game engine third-party pipeline trap. That they only now see that giving developers a native app UI kit is a basic of any OS is… not promising from a supposedly now long term OS owner. reply bemmu 2 hours agoparentPromoting App Lab apps was the most exciting part of this announcement. I had been put off porting anything to Quest since I'd need to get official Meta blessing to actually get store presence to have enough users to pay for such an effort, but now if you can publish in App Lab and actually get some in-store promotion, it starts to sound way more compelling. I do agree with their initial approach to keep the perceived quality of the first apps high. But reality is that people want to play what they find fun/funny, even if less polished, and this change might open up some store promotion for such apps that solo devs like me are more likely to be able to make. reply jimmySixDOF 8 hours agoparentprevThe 'pivot to software' is more like a pivot back to software because Meta already disbanded a previous OS team at Reality Labs and indeed platform software was exactly what motivated the thinking behind buying Oculus as expressed in Zucks internal deal memo on the subject [1] where he ranks Apps and Platform OS (eg Android/PlayStore) both above hardware. They also recently passed on an offer from Google who are working on a new XR android build. 6DOF 3D is a unique surface area and so far all we have seen are OS ports from 2D pancake land so these efforts can't come soon enough. [1] https://news.ycombinator.com/item?id=33538742 reply banish-m4 2 hours agoprevAndroid tried it and the results were closed-source fiefdoms and crappy hardware. There's not a big enough market for this to sustain itself because VR is a flying car category that no one needs. reply crmd 17 hours agoprevI wonder if this \"opening\" of the operating system is their way of putting the metaverse project out to pasture - analogous to donating it to the Apache Foundation - without admitting that the company burned $36 billion on a misadventure. reply romanhn 17 hours agoparentI'm guessing it's the opposite. Meta is trying to establish the same OS-level foothold/control that Microsoft, Apple and Google have. reply AzzyHN 16 hours agorootparentHaving a personal computer at home was a game-changer, though. \"The Metaverse\" has been around for a couple years now, and yet consumer VR (which has been around for eight years now) is still just a \"gimmick\", rather than a must-have. The IBM Personal Computer released in 1981. By 1989... yeah. iPhone came out in 2007. By 2015, smartphones ruled the world. reply throw310822 16 hours agorootparent> iPhone came out in 2007. By 2015, smartphones ruled the world. Advanced phones with proper os, apps, camera had been around for years, and personal digital assistants before that. Tablets, too. iPhone got the form factor and ui exactly right and triggered an explosion, but it was far from the first. We might still be in the \"smartphone, pre-iPhone\" years. reply crowcroft 15 hours agorootparentI don't know what exactly is the right analogy for this, but two other points of context which make me discredit this line of thinking. 1. Feature/smart 'Phones' were around before the iPhone *and* were already pretty much ubiquitous. VR headsets don't do much but sit on shelfs (either in people's houses or in distribution centres not being sold). 2. VR has arguably existed in some ways before the Quest, Nintendo Virtual Boy was from the mid-90s. Maybe the iPhone comparison isn't right, but if we're decades into developing this technology and still very early in development I think we should assume we're a LONG way off these things becoming mainstream consumer devices and we should be wary of any company that brings them to the consumer market. reply ricardobeat 14 hours agorootparent> VR headsets don't do much but sit on shelfs (sic) Quest has 6+ million monthly active users. Steam 2-3 million. Sony doesn't publish numbers but a good guess is 3-4 million active players. If you allow for some overlap, that's roughly ten million monthly users, and in sales VR is already more successful than a lot of computer platforms of the past. reply herculity275 13 hours agorootparent> Sony doesn't publish numbers but a good guess is 3-4 million active players. I find it really hard to believe that 3 million people put on PSVR2 every month. That thing gets basically no content. reply crowcroft 9 hours agorootparentprevYea it’s not nothing, but that’s a rounding error on the number of smartphone users. reply Seb-C 9 hours agorootparentprev> Quest has 6+ million monthly active users. Steam 2-3 million. There is no way that those numbers are right. EDIT: Yeah, Steam has 120 million active users per month. And the 6+ million users for Quest is from 2022, during the pandemic and metaverse bubble. Would be curious to know the trends for 2023 and 2024 thought. reply mnahkies 16 hours agorootparentprevBefore the iPhone we had palm pilots, blackberries, etc. I prefer to think of it as consumer VR simply hasn't had its iPhone moment yet reply baby 16 hours agorootparentIt’s crazy because if you try the Quest it’s quite insane how good it is already. If I were to guess what could give it an iPhone moment: - lighter/more comfortable - faster to get started when you put the headset on - more social experiences and event organized in VR - shorter time from headset on to hanging out with your friends in VR A number of years ago I convinced a bunch of my friends to buy the Quest after being blown away by board games in VR, but turns out Catan only worked for the Go and it was a lot of work to do something together in VR. IMO there needs to be some sort of lobby that does not take you away from hanging out with your friends when you’re in between games. I should be able to easily join a lobby or pause a game to go to a lobby and wave at my friend who’s playing to pause and join me in the lobby reply Capricorn2481 15 hours agorootparent> - lighter/more comfortable It's this and one other point: Games that people aren't bored of in an hour. To me, very few games have come out for VR that don't feel like gimmicky experiences. Even Half Life Alyx, as advanced as it was, kinda felt like a theme park ride after a while. I'm not sure if there's technical reasons for it, but it feels like nobody is taking VR development seriously. It's hard to justify strapping a TV to my face and feeling uncomfortable for one-off experiences. Even if there was a game with some depth and replayability, I would be even more annoyed to play it on such an uncomfortable headset. Almost everyone I know is not using their VR headset anymore. I'm not sure it will ever move past that phase, because people want it to be smaller and, simultaneously, more technically immersive. So we're in some weird in between zone where it's neither. reply zooq_ai 15 hours agorootparentSkills based games -- baseball, cricket, golf, tennis almost have infinite game play reply vundercind 15 hours agorootparentprevI game quite a bit and had access to multiple headsets at home because of the work my wife did, for a couple years. Official permission to use the hardware for whatever. I tried beat saber for like 10 minutes and never bothered with anything else. The headset’s just too big a hassle, and blocking out the world sucks a lot. Plus I can’t help but think of the VR headset guy from the Pearl Jam video “Do the Evolution” when I look at the damn things. Kinda like how I think of the dad from Serial Experiments Lain any time one of my kids walks in and I’m in front of a glowing screen. Gross. reply christianqchung 14 hours agorootparentSpeaking of Serial Experiments Lain, there is also the guy walking around the street in the AR headset which everyone thought was weird. Funny that it's still weird 27 years later. I have access to a Vive headset for school project right now and do not find it very fun to use, Beat saber remains the only VR game that is at least on the same tier of replayability as osu. reply mrguyorama 15 hours agorootparentprev>I'm not sure if there's technical reasons for it, but it feels like nobody is taking VR development seriously. The \"technical\" reason for it is very very very simple: Nearly no video games are actually improved by \"increased immersion\" to an extreme. Chess won't be more fun because you have to physically move digital chess pieces around a virtual board, people playing Call of Duty do not want to physically move their arms around to aim, and don't want to jump around to move, and if you aren't doing those things you don't want the downsides that are inherent to a VR system, like extreme seclusion of wearing a headset, physical ability being an inherent filter, clunky UI, nausea etc. The TWO areas where VR is useful, flight simulators and driving simulators, haven't even fully adopted VR simply because it's too much hassle. VR is only a gimmick unless you can benefit from that extra immersion, and most things cannot. The Wii sold gangbusters because everyone and their grandma could understand \"swing remote to swing tennis racket\", but you couldn't actually build a hyperaccurate tennis sim off of that because a Wiimote is NOT a tennis racket and you cannot get beyond that. VR is the same way. Everyone can experience the \"Oh VR is soooo coool\" gimmick but very few genres inherently benefit from what VR provides. reply wvenable 15 hours agorootparentWhere VR shines, in my opinion, is in fitness. Where the goal is ultimately to move around in a gamified way. That's effectively how I use my Quest 2 and I'm not alone. Recently I've been trying to increase my table tennis skills. reply LordShredda 12 hours agorootparentBut why does it need a heavy screen attached to your head? Just get some shorts and go outside, and if you can afford a quest 2 then surely you can afford a tennis table reply wvenable 10 hours agorootparentIt's not that heavy. I don't have the room for a tennis table nor am I close enough to my friends to play it for 30 minutes every night like I do in VR. reply scheeseman486 3 hours agorootparentprev>The TWO areas where VR is useful, flight simulators and driving simulators, haven't even fully adopted VR simply because it's too much hassle. I see this come up, over and over again. It's so obviously wrong based on even a basic reading of the market. The Quest is unambiguously the most popular VR headset and it's store has barely any cockpit simulators at all. As for tennis, one of the perks of Quest is that you can take it anywhere, including a real tennis court: https://www.youtube.com/watch?v=atuIRf59hzc At a smaller scale, Eleven Table Tennis is an extremely accurate VR Table Tennis game that supports paddle attachments. It is extremely close to the real thing, professionals use it for practice. reply int_19h 14 hours agorootparentprevI think part of the problem is this weird insistence that VR means having to physically move arms around etc. For most games, the visual experience of VR can vastly improve immersion, but control schemes nearly universally suck. Simulators work so much better largely because they don't fall into the same trap - if you're playing a flight sim, say, you're still probably using the same stick/throttle/pedals as you would without the headset. For space sims, I find that headset + mouse combo works amazingly well (End Space is a good showcase of what can be done there). And so on. But for some reason there's practically no uptake on any of this outside of sims. I would love to see a first-person shooter that is fully VR enabled while still allowing me to use WASD + mouse. In fact, I already kinda sorta do that by using 2D theater mode with games like Insurgency: Sandstorm, but that doesn't give you the actually useful VR stuff like the ability to turn your head to look around etc. If somebody were to make an FPS that did all that, they'd have my money in a heartbeat. reply numpad0 12 hours agorootparentThat some reason is motion sickness. There has to be consistency with your perception, else it develops into compounding vection feelings. It tend not to apply for vehicular controls hence sim usage. reply int_19h 9 hours agorootparentThat varies from person to person. I have played games with keyboard and mouse in VR (e.g. Polynomial 2, or the unofficial GTA 5 VR mod), and it works great for me. reply maxsilver 14 hours agorootparentprevThe Quest is insanely good, for a single person in isolation, once it's up and running. But there's a ton of friction that shouldn't exist before that happens, and Meta hasn't nailed most of the UX here yet. For example: - App sharing / libraries doesn't work properly yet. (The owner has to secretly log in to each individual app themselves, before anyone else can use it on the device. There is no documentation informing anyone of this requirement) - Add/ons or DLC also don't work properly yet. (You have to 're-unlock' each individual DLC, for it to share to anyone else on device in something like Beat Saber, for example) - Child permissions don't work properly yet. (The notification does work, but a parent is not allowed to approve an app from that notification, the child has to entirely shut down and restart the whole device, before an approval takes effect) - Screen sharing doesn't work, at all. (If you have a child, you just can't ever mirror their view onto a TV or Tablet -- full stop, no exceptions. Which also means, there's no way to help a child who is wearing a headset -- ever). Note that \"taking the headset off\" triggers a state reset, so a child can't hand the headset over to their parent for help, since the face sensor will kill state the second a face is removed. - Windowing UI doesn't really work yet. (You can have windows, but only three, and only side-by-side, and only for a select few apps) -- it's more usability-restricted than even stage manager on an iPad. You can tell the Quest is designed around the expectation that you will be in one-and-only-one full-screen game, pretty much the entire time your wearing the headset. - Online sharing is app-dependent, a bunch don't work. Many more don't work at first, you have to spend 30 to 60 minutes \"unlocking\" the right to match-make. (making the online/networking more seemless is critical because of the nature of the device -- you can't both look at it the way you might with a TV or PC or Laptop or Tablet, since it's a worn device) None of this is dealbreaking stuff, none if it needs any kind of \"new invention\" or anything to fix. But as a product, friction is still really high here, and I can see why it's not necessarily super popular outside of techie/gaming scenes yet. reply romanhn 8 hours agorootparent> Screen sharing doesn't work, at all This is definitely not true. The Meta Quest app can connect to a headset and show what the player is seeing, both video and audio. Been watching my kid play various games for years. Also not sure what state reset you're talking about. I've definitely grabbed the headset from another person and continued with the game. reply maxsilver 6 hours agorootparent> The Meta Quest app can connect to a headset and show what the player is seeing, both video and audio. Been watching my kid play various games for years. You don’t have your kid on a child account, you have your kid using an adult account. Child accounts can’t screen share —- it’s acknowledged in the Quest FAQ - https://www.meta.com/help/quest/articles/in-vr-experiences/o... “only the primary account can cast” > Not sure what state reset you’re talking about. Setup a child account. Let your kid login. Have the kid take the headset off. Congrats, everything just got killed, and it’s now PIN-locked to the adult account. When the adult types in a PIN, it trashes the child’s session, and starts an Adult session. (We work around this by gently holding the face sensor with a finger while moving the headset around between people, to trick the headset into thinking it’s still being worn -— but this is ridiculously broken, no one should have to do that just to get it to work) reply romanhn 6 hours agorootparentOof, that sounds rough. You're right, I'm just sharing my own account and haven't seen these issues. reply jayd16 15 hours agorootparentprevThey actually do have cross game party chat these days, just FYI. Just make the party and then hop into the game. Support is a little inconsistent as games are not forced to support the feature, though. reply jimmySixDOF 16 hours agorootparentprevThere is a nee version of Catan for Quest 2/3 reply imzadi 14 hours agorootparentThere is, but it is kind of crappy. It's crossplay, but not in a meaningful way. You can't create a room and share a room code. The best you can do is invite a friend, but only if they are on the same platform. They've never done anything to improve it or make it more player friendly. They released it and forgot about it. reply scrame 14 hours agorootparentprevI'm still a little nonplussed. i don't like apple stuff, but did a couple demos at work with the vision elite or whatever its called. Came away very impressed with the technology, but really didn't like having the damn thing strapped to my head for 15-20 minutes. it reminds me of getting the original 3ds that could do some cool AR stuff, and could do 3d without glasses, but ultimately was an impressive tech demo that I mostly didnt use. I already spend too much of my day in front of phones and monitors, I'm not sure if the answer is moving the screens closer to our eyes and shutting out more of the world. industrial applications for sure can have a niche with this, but as a mass market device I think there's a long way to go, even if the experience looks good. reply skhameneh 14 hours agorootparentprevConsumer VR hasn't had it's Blackberry moment yet! Coincidentally, someone I interacted with mentioned \"I never thought I'd get rid of my Blackberry\" in passing, which reminded me of the term once popular term \"Crackberry\". reply babypuncher 15 hours agorootparentprevEven as a kid before the iPhone came out, it wasn't hard to see the appeal of a smartphone. People loved their Blackberries and Palm Treos. Having internet access wherever you go was incredibly appealing even before the hardware, software, and infrastructure were ready to make that mass marketable. VR makes a ton of sense for video games, but I just don't see how it could enhance the rest of my day-to-day life. I don't see it becoming a good general purpose computing platform that most people use all day. I see it being useful for specific niche tasks like CAD, but I'll never put on a headset just to send an email, file my taxes, or browse the internet. reply HDThoreaun 14 hours agorootparentI see tons of appeal for headsets in day-to-day life. Maybe Im unique but I spend a solid hour a day lying in bed reading or on my tablet, I think this experience of using a computing device while lying down could be vastly improved with the right headset and thats an hour every day straight away. reply TulliusCicero 16 hours agorootparentprevVR headsets can be fun for some games, but the hardware and software still have a lot of maturing to do, it's not like smartphones where it feels very developed and there's not much more room for obvious growth/improvement. reply MyFirstSass 15 hours agorootparentYesterday i was close to buying a Pico 4 (Cheaper non meta Quest 3 equivalent), then i realised there has been zero fully triple a games since my friend blew me away with a Half Life Alyx demo 4 years ago. I find it incredible there's still only 1 actual \"serious\" VR game - lots of people then recommended Skyrim VR, a game from 2011. Is VR gaming in an absolute standstill? reply numpad0 13 hours agorootparentYes. Meanwhile:[0] 0: https://medium.com/@nemchan_nel/vrchat-breaks-records-with-9... reply jamilton 14 hours agorootparentprevI think there are few to zero new AAA games, but I don't think that means VR gaming is at a standstill, or that there are no great, fun games. reply TulliusCicero 15 hours agorootparentprevIt's not at all, no. There's plenty of compelling games, there just aren't any AAA games (not ones built for VR only anyway) because the market simply isn't big enough to justify the incredible production costs. If you're dead set on only AAA games then yeah, it's not a useful purchase, but that doesn't mean it's \"standing still\". reply rvba 15 hours agorootparentprevIMHO when you talk about PCs \"becoming the big thing\" - it is more Windows 95 time. In 1989 market was still fragmented and PCs were weak (286, amiga, mac + old 8bits like atari and commodore). reply r00fus 14 hours agorootparentprevIt's both. Meta can be \"giving it away\" and \"hoping to establish an OS foothold\" but if there is no major interest in playing in this space, it's going to be a very empty metaverse. reply nicce 16 hours agorootparentprevIs there better place to place ads than Metaverse ;) reply exe34 16 hours agorootparentIt's great isn't it! All ads should go there. In fact ads should be banned everywhere else! reply bevekspldnw 16 hours agorootparentprevExactly, at the root a lot of this about ATT. reply babypuncher 15 hours agorootparentprevThe difference here though is I don't think AR/VR will ever become as ubiquitous for general purpose computing as laptops and smartphones. reply araes 15 hours agoparentprevHow does any company burn $36 billion on a headset they got handed a prototype for? At a reasonable $100k/yr and 50% overhead, that's 240,000 years of labor. ~5000 human lifetimes. At a .gov labor rate of 2080 hrs/yr, that's 500,000,000 hrs of work wasted? For mediocre \"not a product rendering\" that looks like 90's Second Life? I'm not usually the graphic resolution crowd, yet that was rather underwhelming. Could'a just taken a picture of the inside of the Quest view and it would have been better. Trying to avoid humble bragging, yet last year I put in four government proposals (one 20-pager, rest were 5-10), wrote a web app, converted a NIST matrix package to a different language, and wrote a mixed Android / Windows app for cross-communication. I may have observer bias, and not be representative. However, that was one year, not 5000 lifetimes... You'd think they would have more than a single game as their killer app. Not even Pokemon Go or similar? It's such an obvious previous idea. reply azinman2 15 hours agorootparent100k/yr is not reasonable for Bay Area, let alone top talent. There are people working on it making $1M/yr+. Junior developers straight out of college are making more at Meta. You're also assuming everything went to just engineering payroll, which is obviously not true. reply araes 14 hours agorootparentThen FB/Meta's throwing money out the door on people who demonstrably do not deserve $1M+/yr. And on that topic, same with Wikipedia, why \"must\" you have your development base in the most expensive place on the West Coast? Per https://www.gamedevmap.com/ there are Many other, less expensive, locations. America has a bunch, even Africa has gamedevs. They're an International megacorp, with 3 billion monthly active user (probably still a lot of dupes). India has the largest FB audience (366 million, 2024), not America (100 million). Will an Indian developer make you a launch app for less than $1M+/yr? New Dehli has 17 game studios (including Riot Games) and Mumbai has 33 (Ubisoft Mumbai and Pune). reply danielmarkbruce 13 hours agorootparentHow is it demonstrable that they aren't worth $1m per year? While in many cases folks are overpaid in big tech, some of them are insanely talented people who can do things others simply cannot. reply bobsomers 12 hours agorootparent> How is it demonstrable that they aren't worth $1m per year? Probably because of this: > How does any company burn $36 billion on a headset they got handed a prototype for? reply danielmarkbruce 10 hours agorootparentThat's confusing the decision to work on something with the quality of the folks working on it. Brilliant engineers often work on things which are risky from a market perspective. It doesn't make them less brilliant at engineering when it turns out there isn't a market for the thing. John Carmack appears to be brilliant. I'd guess he would admit the VR market didn't turn out as hoped. reply lostmsu 3 hours agorootparent> he would admit the VR market didn't turn out as hoped That's not what he said though. What he said is that Meta is basically incompetent. reply throwup238 14 hours agorootparentprevThe numbers aren't all that much more palatable at 24,000 man-years, assuming $1M average TC. That's equivalent to the amount of labor it took to build some of the minor Egyptian pyramids. reply bee_rider 15 hours agorootparentprevI want to make a snarky comment about how any reasonable person would want 10x that to work for Facebook but 500 human lifetimes is still a wild amount of time for what they’ve gotten. reply boloust 4 hours agorootparentprevIn terms of difficulty/complexity, nothing you've listed there comes anywhere close to the r&d required to go from the original oculus prototype to volume shipping the meta quest 3. reply TiredOfLife 15 hours agorootparentprevThey didn't and they didn't. reply vineyardmike 17 hours agoparentprev1. Zuck always wanted to own a platform. He was a developer at heart and wants a product that developers can build on. He’s personally invested in this. 2. I’m pretty sure a lot of the cost quoted for their “Metaverse” stuff included their CapEx for a ton of GPUs which probably have a lot of other uses within the company. reply TeMPOraL 16 hours agorootparentI wonder how 1. works. Won't any developer tell you that platforms are traps, to be avoided unless necessary (or unless you're prepared in advance to jump off it at any time)? I feel platforms are only interesting to business folks, particularly those selling access to them. reply vineyardmike 16 hours agorootparentPlatforms are a trap, except Windows launched a revolution. Platforms are a trap, but iOS made companies (like Facebook) billions. reply abraae 15 hours agorootparentAnimals usually realise there is something off about a trap. They interact with it extremely cautiously, sometimes leaving it for a few nights and then coming back. Eventually their desire for whats in the trap overcomes their caution and they put their head in. reply zem 16 hours agorootparentprevas a developer at heart he should have thought back to how interested he would have been in sharecropping on someone else's locked down platform! reply TiredOfLife 15 hours agorootparentprevMeta reality does a TON of research. reply swatcoder 17 hours agoparentprevIt reads more like they're smartly stepping away from the hardware game they're not really optimized for and focusing on the software and connectivity features that they are. I'm not keen on more headsets having a Meta data vacuum built-in, but this isn't the opposite of putting the metaverse stuff to pasture. They're just shifting from an Apple strategy of full-control vertical integration to a Android/Windows strategy of platform ubiquity. reply sgift 17 hours agorootparent> It reads more like they're smartly stepping away from the hardware game they're not really optimized for and focusing on the software and connectivity features that they are. Which would be weird cause the hardware (Quest 3/Quest Pro) is top notch, while Metas software for it is garbage. Everything good is provided by 3rd party companies. reply swatcoder 16 hours agorootparentPixel phones are great too, but Google would be a radically different company if they tried to saturate the demand for Android hardware on their own. Making flagship/reference hardware on the Oculus legacy is a much better strategy for Meta and lets them focus on platform vision and data collection, which is exactly the company they spent the last 15+ years building. reply andybak 13 hours agorootparentprev\"Garbage\" is harsh. It's flawed but they are streets ahead of Pico or Vive. Ironically Google Daydream was also very polished and now Google is starting again but with a gigantic dent in their credibility. reply baby 16 hours agorootparentprevI really hope Meta keeps making hardware. I want a Quest 4 reply sgift 15 hours agorootparentYeah, me too. It would be really sad if Meta stopped hardware development and left it to other companies. reply drdaeman 15 hours agoparentprev\"Opening\" what? I can't think of anything here that even remotely resembles opening something to a public or donating a project to any foundation. They realized they have an asset, and they made some money by licensing it. Sales department did their job, story at 11. But it would've been a boring non-story, so a copywriter used the corporate brandbook - and \"open\" is the buzzword of the last few years when it comes to the technology. Someone need to make an LLM SaaS to de-bulshittify the news. reply soared 17 hours agoparentprevSeems more aligned with trying to achieve what android is to mobile phones but with mixed reality. reply bee_rider 15 hours agorootparentWhich, I guess, makes sense. I think it is absolutely nuts that people would buy an OS developed by an ad company that relies on user profiling for their whole business. But then again it works for Google. reply persolb 16 hours agorootparentprevExactly this. Facebook makes money by network effects. They are incentivized to grow network engagement, more than they are to make direct money off new network members. reply PaulHoule 15 hours agorootparentprevIt actually is Android, just customized for XR. reply graypegg 17 hours agoparentprevUsing the name « horizon » without showing Horizon Worlds at all definitely hints at Horizon Worlds being a side social feature of the Horizon OS, versus this being an OS specifically FOR the horizon worlds metaverse. Meta/Facebook really has trouble with focus, I hope they can pick 1 vision for VR. reply vineyardmike 17 hours agorootparentI think they have the most focus out of any big tech company. They have like 4 products. They’re trying to build a platform to build VR experiences on. That’s clearly their goal. Horizon Worlds is “just an app” to show that off. It’s a “hero use case”. reply PaulHoule 16 hours agorootparentI looked at it seriously for content authoring but gave it up. The big problem is you cannot import images, textures, 3-d models and such from ordinary tools. You have something like constructive solid geometry to work with but only so much and there is a slider you can use to set the number of players and the more players the less geometry you can use. I want to make worlds based on photographs (particularly pano and stereo) and art. McDonalds needs to put a Coca-Cola logo on the side of the cup. Either way it is a non-starter. HW supports collaboration (more than one person shares the world) but https://aframe.io/ lets me make the content I want. If I have to choose one or the other I am going to pick the second. My take on Meta Quest is that it seems highly successful as a gaming environment based on an app store but is skews towards single-player experiences. Like a lot of AAA games, the excellent Asgard’s Wrath 2 has some multiplayer tacked on but it is all meaningless like leaderboards and the occasional ghost that shows up in a procedurally generated dungeon. Of course, Meta wants to make multiplayer experiences but somehow they just can’t do it. reply pnw 15 hours agorootparentThe most popular gaming experiences on Quest are all social - Gorilla Tag, Rec Room, VR Chat, Population One, Contractors etc. It makes sense that expensive AAA experiences like Asgard's Wrath are single player since that's a fairly dominant model in gaming. The Quest doesn't have the player base to support a AAA multiplayer model at this point. reply mvkel 14 hours agoparentprevAgreed. Zuck on Dwarkesh's podcast definitely seemed to be doing some aggressive retconning, making it seem like AI was always the plan, and the metaverse never was. Of course the opposite was true. reply baby 16 hours agoparentprevWhy would they shut down the Metaverse? It’s clearly the future and Zuck brought it up again in the last podcast that people are linking to. Apple just release a bad headset that just confirmed the bet that Meta took reply Culonavirus 16 hours agoparentprev> the company burned $36 billion on a misadventure Watch out, the VR mafia is gonna get ya! Seriously though, any well informed and level headed person could see this coming a mile away. Apparently, such people are in short supply at Meta. reply imzadi 14 hours agorootparentSpeaking from the perspective of a person who is very into VR. There are a lot of things that have gone wrong. First, Facebook/Meta pushing hard with low-end hardware that caused the existing VR gaming to take leaps backward. PCVR was progressing fine before Zuckerberg intervened. Now the VR space is just cluttered with so many low effort, low res games. None of the big players want to get involved, because everyone is so convinced it is too \"niche.\" Meanwhile, you have people who really want to spend money on VR and there's nothing worth spending the money on. reply wahnfrieden 16 hours agorootparentprevThe number of employees who can see failure coming does not matter when they are organized by hierarchy and coerced to work toward failure under threat of losing their wage. reply bsimpson 15 hours agorootparentPlenty of people inside Google also thought that splitting(/replacing) Hangouts into Allo and Duo was monumentally stupid. reply smm11 14 hours agorootparentThat was the thing that turned me off Google permanently. I deleted my G account and have never looked back. reply nradov 16 hours agorootparentprevMeta kind of doesn't have a choice. The major platforms are now owned by Apple, Google, and Microsoft (and also to a lesser extent IBM and Amazon). The strategic risks of being dependent on other companies' platforms are huge. Meta is desperately hunting for a disruptive innovation that will allow them to control the next major platform. A lot of people are betting that will be AR/VR but it could be something completely different. reply burnte 17 hours agoparentprevThis is them doing that but also trying to do what Android did, capture the bulk of the market and leave the ultra-ridiculously-high-end to Apple. reply criddell 16 hours agorootparentSo create a big market where nobody makes much money to compete with Apple's smaller market that captures absurd margins? reply bbarnett 16 hours agorootparentDo devs really capture absurd margins? Or does Apple, leaving the dev with a pittance? reply awad 15 hours agorootparentWhile we can debate plenty on what the right amount of App Store fees might be, it is objectively true that developers absolutely care about the market of available consumers on the high end platform. reply criddell 16 hours agorootparentprevI thought ultra-ridiculously-high-end was referring mostly to Apple hardware and bundled software. reply rbanffy 17 hours agoparentprevNot really. There is no money in VR/AR headsets. All the money is in the services that back them. Even further, the less money is in making headsets, the more money is in the services. To say nothing about your data, which is Facebook's primary revenue driver. reply b_d98 16 hours agoprevWhile good, Meta is likely to attempt to form a walled-garden in the vein of IOS and even Android to an extent. What I’d really hope for is a general-purpose computing platform with root access for the user, As I believe that mixed reality headsets have the potential to overtake both the smartphone and the PC as the default compute device of choice. reply xyst 9 hours agoparentlikely this. I fell for Apple once. I definitely won't fall for a Apple clone with one of the most untrustworthy companies since the turn of the century. reply squigz 15 hours agoparentprevIs there any such solution in the VR space, even if not state-of-the-art? reply b_d98 14 hours agorootparentThere's SimulaVR. https://simulavr.com/ reply pavlov 17 hours agoprev> “Meta Horizon OS devices will also use the same mobile companion app that Meta Quest owners use today—we’ll rename this as the Meta Horizon app.” Churning through so many VR brands. The app that used to be called Oculus and is now called Meta Quest will soon be called Meta Horizon. If Meta really believes in the Metaverse, why do they need the Horizon brand? Why not just the Meta OS? reply what_ever 16 hours agoparentBecause their whole company is called Meta? reply sangnoir 15 hours agoparentprev> If Meta really believes in the Metaverse, why do they need the Horizon brand? Why not just the Meta OS? Perhaps for the same reason why Apple OS, Google OS or Microsoft OS don't exist as customer OSes - each OS is scoped to a specific function/device-class, and maybe superceded in the future. reply not_your_vase 17 hours agoprevIs it based on some prior art (BSD, Linux...), or a new proprietary OS written from scratch? The post is not too rich in actual information, beside that now other tech-giants can use it too... reply asveikau 17 hours agoparentI can't recall when I first heard the name \"OS\" used to mean just another linux distro, whereas my increasingly old-man brain expects the term OS to mean a unique kernel, not a repackaging of a different one. Certainly by the 2010s that usage was common. I feel like these days some would even call something an \"OS\" if it's running in a docker container, without providing any kernel at all. Which is to say the meaning of the term is expanding. reply dmayle 16 hours agorootparentFrom personal experience as far back as the 80's (and from my understanding going back before that as well), OS has never meant kernel. An Operating System is the collection of software that allows you to operate a computer, so that means kernel, program loader, simple text editor, simple disk management, etc. As computer users became more savvy, and hardware became more powerful, more and more functionality was included in the OS (graphical interfaces, utility apps, etc.). I don't think many people would have trouble calling Android an operating system, and that's just the Linux kernel with utility apps, loader, and app libraries, yet very different from something like Redhat. I don't think it's a stretch in the least to call Horizon an OS. reply asveikau 15 hours agorootparent> From personal experience as far back as the 80's (and from my understanding going back before that as well), OS has never meant kernel. I definitely heard it used to literally mean only the kernel. Circa 20 years ago and earlier. For example, if we look at \"Operating system\" on Wikipedia from 2006: > https://en.wikipedia.org/w/index.php?title=Operating_system&... > Most current usage of the term \"operating system\" today, by both popular and professional sources, refers to all the software that is required in order for the user to manage the system and to run third-party application software for that system. Note it says \"most current usage\". That is because the usage was changing at that time, or had only recently changed. (I picked 2006 because I remember it changing around then.) If we go back another 2 years: > https://en.wikipedia.org/w/index.php?title=Operating_system&... > In computing, an operating system (OS) is the system software responsible for the direct control and management of hardware and basic system operations ... Sure sounds like that doesn't include userland. Definitions which include userland are marked as \"colloquial\". Famously in the 1990s, Microsoft tried to argue in court that an OS included a web browser, and that discussion is cited in these old articles... Many reasonable people at the time thought that position was bullshit. reply wzdd 13 hours agorootparent> I definitely heard it used to literally mean only the kernel. Circa 20 years ago and earlier. You may have, but it was a nonstandard usage. Even your 2004 Wikipedia article distinguishes between OS and kernel. Userland is certainly part of it. AmigaDOS, 1991, manual p22: \"Each AmigaDOS process represents a particular process of the operating system— for example, the filing system [...] AmigaDOS provides a process that you can use, called a Command Line Interface or Shell. (https://archive.org/details/1991-baker-jesup-et-al-the-amiga...) MS-DOS 6.22 (1994) concise user's guide consistently refers to the entire thing including command.com as the operating system (the kernel here is named msdos.sys.) (https://ia801204.us.archive.org/33/items/msdos_manual_622/ms...) Hell, the whole Linux vs GNU/Linux thing, which has been around since 1992 (https://en.wikipedia.org/wiki/GNU/Linux_naming_controversy), was explicitly about the fact that \"Linux\" is just the name of the kernel. reply bradjohnson 14 hours agorootparentprev> I definitely heard it used to literally mean only the kernel. Circa 20 years ago and earlier. You're correct that peop",
    "originSummary": [
      "Meta Quest is revolutionizing mixed reality by allowing third-party hardware manufacturers to utilize their operating system, Meta Horizon OS, fostering a more diverse ecosystem for consumers and developers.",
      "Renowned tech giants such as ASUS, Lenovo, and Xbox are partnering with Meta Quest to develop new devices running on Meta Horizon OS, emphasizing social interaction and connectivity.",
      "The collaboration with Qualcomm Technologies guarantees high-performance hardware, aiming to establish a more flexible and inclusive platform for mixed reality experiences on a range of devices."
    ],
    "commentSummary": [
      "The focus is on Meta Horizon OS and its impact on the VR headset industry, compared to competitors like Apple Vision Pro, along with discussions on niche markets within AR/VR.",
      "Importance is placed on branding, hardware partnerships, and software revenue sharing in the market, highlighting advancements in VR technology and challenges in VR gaming and technology.",
      "The debate includes concerns over VR technology direction, Meta's challenges in the VR/AR industry, platform vision, data collection, user profiling, multiplayer experiences, and competition strategies with industry giants, showcasing a mix of optimism and skepticism for future breakthroughs in VR technology."
    ],
    "points": 690,
    "commentCount": 566,
    "retryCount": 0,
    "time": 1713801270
  },
  {
    "id": 40117599,
    "title": "Voyager 1 Sends Engineering Updates to Earth Again",
    "originLink": "https://blogs.nasa.gov/voyager/2024/04/22/nasas-voyager-1-resumes-sending-engineering-updates-to-earth/",
    "originBody": "NASA’s Voyager 1 Resumes Sending Engineering Updates to Earth After receiving data about the health and status of Voyager 1 for the first time in five months, members of the Voyager flight team celebrate in a conference room at NASA’s Jet Propulsion Laboratory on April 20. Credit: NASA/JPL-Caltech For the first time since November, NASA’s Voyager 1 spacecraft is returning usable data about the health and status of its onboard engineering systems. The next step is to enable the spacecraft to begin returning science data again. The probe and its twin, Voyager 2, are the only spacecraft to ever fly in interstellar space (the space between stars). Voyager 1 stopped sending readable science and engineering data back to Earth on Nov. 14, 2023, even though mission controllers could tell the spacecraft was still receiving their commands and otherwise operating normally. In March, the Voyager engineering team at NASA’s Jet Propulsion Laboratory in Southern California confirmed that the issue was tied to one of the spacecraft’s three onboard computers, called the flight data subsystem (FDS). The FDS is responsible for packaging the science and engineering data before it’s sent to Earth. The team discovered that a single chip responsible for storing a portion of the FDS memory — including some of the FDS computer’s software code — isn’t working. The loss of that code rendered the science and engineering data unusable. Unable to repair the chip, the team decided to place the affected code elsewhere in the FDS memory. But no single location is large enough to hold the section of code in its entirety. So they devised a plan to divide the affected code into sections and store those sections in different places in the FDS. To make this plan work, they also needed to adjust those code sections to ensure, for example, that they all still function as a whole. Any references to the location of that code in other parts of the FDS memory needed to be updated as well. The team started by singling out the code responsible for packaging the spacecraft’s engineering data. They sent it to its new location in the FDS memory on April 18. A radio signal takes about 22 ½ hours to reach Voyager 1, which is over 15 billion miles (24 billion kilometers) from Earth, and another 22 ½ hours for a signal to come back to Earth. When the mission flight team heard back from the spacecraft on April 20, they saw that the modification worked: For the first time in five months, they have been able to check the health and status of the spacecraft. During the coming weeks, the team will relocate and adjust the other affected portions of the FDS software. These include the portions that will start returning science data. Voyager 2 continues to operate normally. Launched over 46 years ago, the twin Voyager spacecraft are the longest-running and most distant spacecraft in history. Before the start of their interstellar exploration, both probes flew by Saturn and Jupiter, and Voyager 2 flew by Uranus and Neptune. Caltech in Pasadena, California, manages JPL for NASA. News Media Contact Calla Cofield Jet Propulsion Laboratory, Pasadena, Calif. 626-808-2469 calla.e.cofield@jpl.nasa.gov Author Naomi HartonoPosted on April 22, 2024April 22, 2024",
    "commentLink": "https://news.ycombinator.com/item?id=40117599",
    "commentBody": "NASA's Voyager 1 Resumes Sending Engineering Updates to Earth (nasa.gov)624 points by jonathankoren 15 hours agohidepastfavorite150 comments divbzero 5 hours agoThe substance of this news release is incredible, but its style is also admirable. The authors managed to convey in seven paragraphs a concise and comprehensible explanation of how the team resolved the technical issue. reply fforflo 2 hours agoparentThat's what happens when you're freed from \"SEO-optimized content\". It's also a culture thing I'd probably put under military philosophy. I've worked with ex-military engineers, and you can tell from how they communicate. Writing technical reports and memos is a skill. reply Night_Thastus 13 hours agoprevThere's something very beautiful about Voyager's journey so far. I hope one day when we're a true interstellar species we'll still keep tabs on it. The data may not be useful anymore, but it would be cool to imagine a year 3000 society with a little \"Look at where Voyager is now :)\" tool that you can see its path and where humans have colonized by comparison. reply Buttons840 12 hours agoparentPerhaps our descendants will build a museum around it without ever changing its velocity. reply el-duderino42 3 hours agorootparentI think a 19 year old yobbo, whose dad is a successful interstellar logistics businessman, who out of a guilty conscience for never having had time for his son and having bought him an overpowered spacecraft, will either put grafitti on it or misjudge his afterburner and half burn it while trying to fly a very tight corner around it, in order to impress the 2 girls he has on board. reply justinclift 6 hours agorootparentprevSomeone (something?) will probably make a religion out of it. ;) reply RheingoldRiver 3 hours agorootparentThere is no destination; there is only the voyage. There was no beginning; there is only the voyage. In life, we voyage together, and in death we shall voyage alone. Now, then, and forever voyaging. reply beeeeerp 9 hours agorootparentprevI’ve never thought about it this way. What a cool idea! reply divbzero 5 hours agoparentprevI’ve imagined a scene playing out, in sci-fi or for real in the distant future, where astronauts test out a new propulsion system by flying out towards Voyager 1 and catch up to it with ease. As they approach, they see the ancient probe grow larger and larger in their window until… reply ChrisMarshallNY 12 hours agoparentprevWasn't there a Star Trek enemy that was based on Voyager colliding with some kind of doomsday machine? reply ioblomov 12 hours agorootparent[SPOILER ALERT] It was the surprise ending for the first Star Trek film... https://en.m.wikipedia.org/wiki/Star_Trek:_The_Motion_Pictur... reply hnlmorg 11 hours agorootparentThat was Voyager 6, a fictional probe based on the real life probes of the same name. Earth probe gone bad is a common trope in Sci-Fi. Star Trek alone has a few episodes: - https://memory-alpha.fandom.com/wiki/Nomad - https://memory-alpha.fandom.com/wiki/Friendship_1 Also the Doomsday machine was a very different type of probe to V’ger https://memory-alpha.fandom.com/wiki/The_Doomsday_Machine_(e... reply layer8 11 hours agorootparentStrangely, this isn’t listed as a trope here: https://tvtropes.org/pmwiki/pmwiki.php/Film/StarTrekTheMotio... reply fbdab103 3 hours agorootparentprevAlso a Futurama episode where a probe merged with God. reply boffinAudio 3 hours agorootparentprevMy favourite, the ANDROMEDA STRAIN .. the idea that the US government would experiment with weaponizing upper-atmosphere organisms is just too real .. https://en.wikipedia.org/wiki/The_Andromeda_Strain reply rob74 31 minutes agorootparentAh yes, the first Michael Crichton novel I read - and also the book that first introduced me to binary code (https://imgur.com/N4IjIYq)! And after watching (and then reading) Jurassic Park some 10 years later, it took a while until I realized that it was from the same author... reply boffinAudio 23 minutes agorootparentYes, I too had the same disparity in recognizing the author as a young 'un, reading CONGO and SPHERE and so on .. Mr. Crichton sure had his finger on the pulse of the technological world we live in. What a wild series of stories he has created .. he was my favourite author until Messrs. Stephenson and Gibson came along .. Staying on subject, I wonder if we will see a new adaptation of ANDROMEDA STRAIN some time. As a story it seems topical and relevant. reply batch12 11 hours agorootparentprevI think the Klingons destroyed pioneer 10 too. reply INGSOCIALITE 10 hours agorootparentprevV-GER reply iampivot 11 hours agoparentprevDo we know how long the power source will last? reply retrac 10 hours agorootparentThe plutonium 238 decays according to a curve, and the thermocouples are degraded as well by heat and radiation according to a curve. So the power output drops rather predictably: \"The radioisotope thermoelectric generator on each spacecraft puts out 4 watts less each year.\" [1] The Voyagers will soon no longer have enough power to operate any of their instruments. They'll have enough power to continue operating the transmitter (which serves as a science experiment of its own) into the 2030s. The power of the signal will drop before the electronics and control brown out (if it works as designed), and it the signal might become too weak to detect before the probe completely stops operating. Such a fate befell Pioneer 11, who may yet still be warbling away at low power no longer pointed at Earth; its carrier was last detected in 2003. [1] https://voyager.jpl.nasa.gov/frequently-asked-questions/ reply wolverine876 9 hours agorootparent> [1] https://voyager.jpl.nasa.gov/frequently-asked-questions/ Also: Even if science data won't likely be collected after 2025, engineering data could continue to be returned for several more years. The two Voyager spacecraft could remain in the range of the Deep Space Network through about 2036, depending on how much power the spacecraft still have to transmit a signal back to Earth. That FAQ covers a lot of interesting ground (though it talks about 2020 in the future tense). After Voyager 1 took its last image (the \"Solar System Family Portrait\" in 1990), the cameras were turned off to save power and memory ... I didn't realize that was the last image. ... it is very dark where the Voyagers are now. While you could still see some brighter stars and some of the planets with the cameras, you can actually see these stars and planets better with amateur telescopes on Earth. reply exitb 2 hours agorootparent> The two Voyager spacecraft could remain in the range of the Deep Space Network through about 2036 It would be quite depressing if it was us failing to receive a fainter signal, rather than Voyager failing to send it. reply xattt 8 hours agorootparentprev> the cameras were turned off to save power and memory Since it’s powered by an RTG, how does the power get “used up”? I assume that this refers to the available power budget at a given moment versus some sort of expendable power reserve. reply 8bitsrule 2 hours agorootparentIt's the first question at the top of that ^ FAQ page. One of their comments is : \"Mission managers removed the software from both spacecraft that controls the camera.\" Makes me wonder if that unused RAM came in handy lately! reply Yossarrian22 6 hours agorootparentprevProbably a physical relay to deal with parasitic power draw reply xcv123 5 hours agorootparentprevYes the power budget is decreasing as the generator output decreases over time. reply dmurray 2 hours agorootparentprev> the transmitter (which serves as a science experiment of its own) into the 2030s. One of the longest-running scientific experiments, too. It's already about as old as the Queensland pitch drop experiment was when Voyager I was launched. reply Fatnino 10 hours agorootparentprevIt has a few years left as best. Supposed to run out in the 2020s reply echelon 11 hours agoparentprev> when we're a true interstellar species If we can harness all of the energy and mass available in our solar system, we [1] can likely compute more than several galaxies full of classical humans. We might even begin to test the edges of physics. Maybe we don't need to go anywhere at all. Maybe we [1] have all we need right here to become literal gods. [1] Our digital descendants. Humans are very much fit to the gas exchange and metabolism envelope of our gravity well. reply science4sail 9 hours agorootparent> compute more than several galaxies full of classical humans Unfortunately the simulated classical humans in your Matrioshka Brain will want to mine Bitcoin, which means that our digital descendants will have to become a true interstellar species anyway in order to convert the Laniakea Supercluster into coin-mining computronium. reply eigenket 1 hour agorootparentThis is essentially the plot of a book called Accelerando, published way back in 2005! reply eru 2 hours agorootparentprev> If we can harness all of the energy and mass available in our solar system, we [1] can likely compute more than several galaxies full of classical humans. We might even begin to test the edges of physics. I grant that. But why would that keep us from pressing on? If we have more resources in general, we will also have more resources for interstellar adventures. reply datavirtue 10 hours agoparentprevOh, you sweet summer child... reply tempestn 7 hours agorootparentThis patronizing phrase should really be scrubbed from the modern lexicon. reply jojobas 6 hours agorootparentEven if you construe this phrase as unequivocally negative, you can't scrub negative emotions and intentions. If you were able to actually scrub some word or phrase from the language, another would take its place, like with all the racial and intellectual disability slurs. reply Anotheroneagain 5 hours agorootparentprevIt is the modern lexicon, it's from the game of thrones. reply iancmceachern 10 hours agoprevVoyager has been an inspiration for generations of engineers. Bless you all that worked on it. Thank you. Recently I designed in a Voyager inspired secret Easter egg into the surgical robot I designed. I put a gold (plated) plate with everyone's signature engraved on it. Gave everyone one as a surprise Christmas gift. reply 14 9 hours agoparentCool! What else can you tell us about this robot you built? reply iancmceachern 9 hours agorootparentIt's the Maestro from Moon Surgical, it's done over 200 surgeries, all successful. So far (no whammy) its had 100% reliability with the first 6 systems we built. We designed it (hardware wise) with only 3 engineers, including me, and we hand built the first ones right here in San Carlos, CA. The company is based in Paris and has a whole interesting history there as well. reply phinnaeus 8 hours agorootparentHold on, is that the robot that did surgery on a grape? reply iancmceachern 7 hours agorootparentNo, but I worked on one of those too. I worked on J&Js ottava, its designed to compete with the one doing the grape surgury, Intuitive's Da Vinci. I've had the pleasure of using one (Da Vinci XI) to play around with, it's truly amazing. reply drtgh 14 hours agoprevHow they manage to squeeze all the resources of the probe and keep it working year after year is an astounding achievement, pleasantly mind-blowing. It is important that all the know-how about this type of maintenance never disappear. I hope the designs in electronics that this team would have wanted to have available in the probe are implemented in the new designs. reply JackFr 12 hours agoparentI forget where I saw the headline, but it's still funny \"Voyager: Please let me die. NASA: No.\" reply shadowgovt 13 hours agoparentprevI should see whether there's documentation of what they moved and what they replaced. I imagine there's \"plenty\" of room to do that (in the sense that there's probably some programs that are no longer mission-relevant because they controlled systems that have been shut down), but I'd love to know what got tossed. Heck of a job. reply anotherhue 14 hours agoprevIncredible to see the same faces in that photo as in the excellent documentary: https://www.itsquieterfilm.com/ Voyager might make it to 2027. reply kibwen 13 hours agoparent> Voyager might make it to 2027. With some amount of luck, Voyager might last ten more years beyond that: \"Even if science data won't likely be collected after 2025, engineering data could continue to be returned for several more years. The two Voyager spacecraft could remain in the range of the Deep Space Network through about 2036, depending on how much power the spacecraft still have to transmit a signal back to Earth.\" https://voyager.jpl.nasa.gov/frequently-asked-questions/ (And then, 15,000 years later, maybe this happens: https://www.sbnation.com/a/17776-football ) reply chungy 37 minutes agorootparent> With some amount of luck, Voyager might last ten more years beyond that: Oh, good, it won't have to suffer a year 2038 problem :) Also: what's with that last link? Definitely didn't expect something to make my browser slow to a crawl. reply singleshot_ 12 hours agorootparentprevCan’t possibly say enough good things about this Jon Bois fellow. reply temp0826 12 hours agorootparentrip centennial bulb I hope the 3rd installment comes out someday. Seems to be on permanent hiatus. reply Fatnino 10 hours agorootparentThat bulb in Livermore is still lit. Or are you talking about something else? reply temp0826 10 hours agorootparentReferencing 17776 by Jon Bois when the bulb (spoiler alert?) meets its demise. A tragic day for sentient satellites and immortal football fans alike. reply doctor_eval 12 hours agorootparentprevThat was remarkably good! reply cancerboi 12 hours agorootparentprevWhat is this sbnation link? The text explodes and a calendar pops up. reply peddling-brink 12 hours agorootparentKeep reading. It’s a story. reply an1sotropy 10 hours agoparentprevWhat a sweet film. Thank you for the link. This whole time when I heard about work on the Voyager mission I assumed there was a larger team, with fewer single points of failure. reply IncreasePosts 13 hours agoparentprevThe documentary only came out less than 2 years ago, one would imagine most of the same people would be there - though I don't see Jefferson Hall. reply Keyframe 11 hours agoparentprevno way to purchase/watch this outside of certain regions (I presume US-only or similar). What a shame. reply userbinator 7 hours agoprevIf anyone is curious about the computers they use: https://en.wikipedia.org/wiki/Voyager_program#Computers_and_... 16/18-bit custom architecture, 4k/8k words (around 9/18KB total). Other sources I could find indicate a 250kHz clock frequency and around 8KIPS. reply taylorius 4 hours agoparentCould get doom running on that, no problem. reply grosswait 8 hours agoprev> A radio signal takes about 22 ½ hours to reach Voyager 1, which is over 15 billion miles (24 billion kilometers) from Earth, and another 22 ½ hours for a signal to come back to Earth. It has taken 46 years to get 22.5 light hours away from Earth! reply sllabres 4 hours agoparentYes, this is why I always liked 'planet walks' like this [1] If you see Images the planets in the solar system, the solar system itself etc. are mostly not to scale. If you walk such path one get a bit of a feel how large everything is, I think especially if the plantes on the walk are to scale too. Even our solar system is mostly empty. And the distances to moon a microscopic. Walking such a path is is not the same, but a bit like [3] if one is patient (which is difficult on the internet) and doesn't scroll manually but only via the little \"c\" in the lower right corner, wich scrolls with light speed. [1] https://en.wikipedia.org/wiki/Sagan_Planet_Walk (there are several other listed at the bottom of the page) [2] https://en.wikipedia.org/wiki/Solar_System_model [3] https://joshworth.com/dev/pixelspace/pixelspace_solarsystem.... reply jlaneve 12 hours agoprev> The team started by singling out the code responsible for packaging the spacecraft’s engineering data. They sent it to its new location in the FDS memory on April 18. A radio signal takes about 22 ½ hours to reach Voyager 1, which is over 15 billion miles (24 billion kilometers) from Earth, and another 22 ½ hours for a signal to come back to Earth. Talk about a slow feedback loop! And I get frustrated when I need to push code to a repo to test things in CI... reply HeckFeck 46 minutes agoparentSo there is something with a worse ping than IP over Avian Carriers. reply z500 11 hours agoparentprevMy internet is operating at about 1/100 normal speed today. It feels a bit like I've been remoting into a machine on Mars. reply LAC-Tech 12 hours agoparentprevHow much data can you send in a single radio single? I assume it's not TCP/IP reply akira2501 10 hours agorootparentDownlink from Voyager to Earth is currently 40 bits/s but can be up to 160 bits/s. The signal is received at -160dBm or around 100 _zeptowatts_ (1e-22 kW). reply dgfitz 11 hours agorootparentprevHave you investigated this or are you just asking? I imagine if you wish to learn the answer it is a few simple searches away. And by “imagine” I mean, it is. reply huytersd 7 hours agorootparentPosting a question in a forum has its benefits though. A bunch of drive by folks end up picking up information they would never have gone through the trouble of researching themselves. reply Dylan16807 7 hours agorootparentprevYou can get raw bitrate numbers, not so much about the way transmissions are formatted. reply harles 10 hours agoprevPretty incredible feat of engineering (both back when it was launched and now). Does anyone know its current purpose? I’m curious if there’s anything we’re actively using it for or if it’s just a matter of “look for surprises”. reply retrac 9 hours agoparentThe magnetometers and charged particle detectors are still operating. So they are measuring the galactic magnetic field, and cosmic rays and the gas in interstellar space. The results are more or less what was predicted, though the exact boundary of the sun's influence was only discovered when Voyager 1 and 2 crossed over it in 2012 and 2018 respectively. Beyond that, yes, they're basically assuring us the sun is still there and that space is very empty. I don't think anyone expects the interstellar gas to vary in density on the timeframe that the Voyagers will be able to observe it but, I guess we'll find out! Edit: I spoke way too soon. It varies, as discovered with Voyager measurements recently: https://www.jpl.nasa.gov/news/as-nasas-voyager-1-surveys-int... reply harles 6 hours agorootparentExactly what I was looking for - thanks! Not earth shattering, but it’s neat to fill out our map of the neighborhood. reply AstroJetson 13 hours agoprevYep, 45 years old hardware, still getting software updates. Hey Apple, JPL is close to you, can you get someone to bike over and see how they do it? Thanks! reply somenameforme 5 hours agoparentPlanned obsolescence [1] in the problem. Apple could certainly create modular devices designed to be used and piecemeal modified/updated indefinitely. But it's way more profitable to sell you a brand new phone every couple of years than it is to sell you a modular device that you could update as desired, especially if the parts/connections are standardized meaning you could foster a [normalized] third party market for batteries, screens, etc. Devices would last way longer, prices would be much lower, and it'd be unimaginably better for the environment. But Apple wouldn't make as much money, and the government's GDP growth figures wouldn't be as high. So clearly it's a terrible idea. [1] - https://en.wikipedia.org/wiki/Planned_obsolescence reply Salgat 13 hours agoparentprevI bet for a billion they could make a single device that lasts decades too. reply lolive 9 hours agorootparentFYI, my original iPad is still working fine. reply dylan604 13 hours agorootparentprevand a nuclear battery to last decades. oh, and stagnant software that doesn't get updates meant to work for other hardware reply dx4100 13 hours agorootparentThe average vape has more processing power than Voyager, and the iPhone is orders of magnitude more complex. With that said, it takes skilled engineers to squeeze perfectly crafted code into such a tiny platform from the 70s. reply treesknees 12 hours agorootparentI understand what you're getting at, but the 'average' vape pen is essentially a disposable battery and temperature sensor with no additional inputs or features. After reading some details about the Voyager, I have my doubts that a disposable vape has more computation power [1]. Maybe the higher end devices with programable displays and temperature settings? [1] https://www.eejournal.com/article/voyagers-1-and-2-take-embe... reply dekhn 10 hours agorootparentA Pinecil (digital soldering pen) is probably a better example. BL706 MCU, \"a low-power, high-performance IoT chip that supports BLE/Zigbee wireless networking, ... BL702 has built-in RISC-V 32-bit single-core processor with FPU, the clock frequency can reach 144MHz, has 132KB RAM / 192KB ROM / 1Kb eFuse storage resources, supports external Flash, and optional embedded pSRAM.\" The Voyager had a custom-designed processor (well, several) that were basically computers made out of basic logic chips (74xx series); see details here https://www.eejournal.com/article/voyagers-1-and-2-take-embe... Either way, it's clear that we (well, JPL) can build extremely powerful and sophisticated systems with relatively small computers, suggesting that resource constraints can sometimes be a source of stability and creativity. reply shiroiushi 5 minutes agorootparent>The Voyager had a custom-designed processor (well, several) that were basically computers made out of basic logic chips (74xx series); This was not unusual at the time: many early arcade games were made exactly the same way. They've even been emulated by the MAME project. nojvek 11 hours agorootparentprevNew vapes have Bluetooth on a small chip that tracks # of puffs remaining. Being sold for $40 bucks a at store. Crazy that we have disposable one use electronics now. reply layer8 11 hours agorootparentprevI wonder if some billionaire ordered one. reply MadnessASAP 13 hours agoparentprevMaybe some notes on how to handle degrading power supplies while they're at it. reply jonathankoren 13 hours agorootparentThey turn stuff off. It's not a secret. The vast majority of the sensors are off. They're simply not needed in interstellar space. reply tnmom 12 hours agorootparentGood lord could you imagine the meltdown HN would have if Apple had taken this option to solve the old-batteries-support-lower-peak-current physics problem? “Your device battery no longer supports the camera. Or the backlight on the top third of the screen. But it runs at full speed otherwise!” reply wkat4242 12 hours agorootparentIf this happened after 45 years, I don't think people would mind. reply wongarsu 8 hours agorootparentVoyager's cameras were shut off after only 13 years to conserve power. Still a long time, but some people might mind if their phone did that. reply cjk2 12 hours agorootparentprevMeh just buy a new Voyager! reply LAC-Tech 11 hours agorootparentOMG the new one is black!! reply cjk2 3 hours agorootparentI actually just bought a new black iPhone so this burns :) reply dotnet00 7 hours agorootparentprevBut it drops the golden record for a more modern, hardware locked bluetooth speaker :( reply MadnessASAP 5 hours agorootparentprevI mean yeah, it was just a little bit of humor at Apples expense. Obviously the Voyager plan of turning stuff off wouldn't be applicable to an iPhone. reply GeekyBear 3 hours agoparentprevIn 2016, Apple released the original iPhone SE and Google released the original Pixel phone. The Pixel stopped getting security updates at the end of 2019. The OG iPhone SE is still getting security updates eight years later. Maybe Google should try seeing how Apple does it? reply nolist_policy 2 hours agorootparentThe Google Pixel 8 series receives 7 years of updates. And unlike Apple they are upfront about it, which is important. Maybe your iPhone gets 8 years of updates, maybe 6 noone knows. reply cdelsolar 8 hours agoparentprevJPL is in Altadena and Apple is in the sf Bay Area reply its_ethan 13 hours agoparentprevThe current iOS 17 is compatible with the iPhone XS, which is from 2018. That's 6 years for a piece of tech that the majority of people replace after The team discovered that a single chip responsible for storing a portion of the FDS memory — including some of the FDS computer’s software code — isn’t working. No one thought of having backup computers on the spacecraft? reply noir_lord 59 minutes agoparentMost of the computer hardware is duplicated but much of it has already failed/failed earlier - remember these are 50 year old hardware - a lot of the logic is TTL and discrete components - which are far larger than modern equivalents would be. reply shiroiushi 1 minute agorootparentIt's not just 50-year-old hardware: it's hardware that's been subjected to cosmic radiation for 50 years, part of that outside the solar system (so presumably even higher). reply qwertox 12 hours agoprevHackers at heart. I wish media would report about it to illustrate what hacking used to mean. reply nirav72 8 hours agoprevAll I can say is wow! This probe refuses to die. Despite being built with almost 50 year old technology. Amazing engineering by the people that designed and built it. Even more amazing is the people that continue to debug software problems from 2 light days distance. reply eh_why_not 1 hour agoparent> from 2 light days distance The distance is currently 22.5 light hours (and increases by half-hour per year.) But it is indeed about 2 days of round-trip time for debugging. reply hujun 7 hours agoprevDark Forest Hypothesis: https://en.wikipedia.org/wiki/Dark_forest_hypothesis reply NKosmatos 1 hour agoparentAlthough that’s one of the many possible explanations of the Fermi paradox [0], I prefer to think that the real reason we haven’t discovered (or we haven’t been discovered) is the fact that we’re limited by the speed of light. The distances are so vast, almost unfathomable, that we need Faster Than Light means of traveling. Perhaps I’m being naive or romantic, but I prefer to think this is the real reason :-) [0] https://en.wikipedia.org/wiki/Fermi_paradox reply ngneer 3 hours agoprevMust be difficult debugging a system with a 45 hour round trip each step of the way. And here I thought debugging a system on customer premises was tough. Hats off! reply orlp 3 hours agoparentWorse than the round-trip is that there's no second chances in some scenarios. If you mess up the wrong part(s) of the system, it's bricked with no way to recover it. reply nxobject 7 hours agoprevI would give my left nut for a Voyager operations + programming sim, complete with live patches and day plus communications round-trip time. reply botanical 6 hours agoprevThe Voyager project is an amazing feat for humanity. But I wonder, does NASA take into account the repercussions of sending a probe deep into space? I know space is big but I can only think of the dark forest hypothesis. What's the plan for further space exploration? reply T-A 6 hours agoparentVoyager 1 is currently 2.4e10 km from Earth. Trisolaris - sorry, I mean Proxima Centauri - is 4e13 km away [2], so in the (almost) 47 years since its launch, Voyager 1 has covered a whopping 0.06% of the distance. And it's not even headed that way. Meanwhile, radio, TV and radar have been advertising the presence of a new technological civilization on Earth for more than a century. That means any entity worth worrying about within a 100+ light year radius - a distance 44+ times longer than to Proxima - already knows about us. And that sphere keeps growing in all directions at the speed of light, or roughly 18000 times faster than Voyager 1 is moving. If you really want to worry about the Dark Forest, it would be more justified to ask if your local radio station takes into account the repercussions of sending commercials and TOS reruns into deep space. [1] https://theskylive.com/voyager1-info [2] https://en.wikipedia.org/wiki/Proxima_Centauri reply wumeow 6 hours agorootparent> repercussions of sending commercials and TOS reruns into deep space. There’s actually a documentary about this: https://www.wikipedia.org/wiki/Galaxy_Quest reply somenameforme 5 hours agorootparentprevInverse square law. With the power we're transmitting at, signals become just background noise relatively quickly. They're nowhere near strong enough to be detectable at e.g. Proxima Centauri. This is what makes the radio signals we do detect, like fast radio bursts, so interesting. So for instance, the furthest signal we've detected is called FRB (fast radio burst) 20220610A, and that millisecond length signal came from a source with output energy equivalent to decades of the Sun's entire output. reply nuccy 32 minutes agorootparentThis \"law\" is only for point (or spherical) sources, i.e. those emitting evenly in all direction - the area is increasing as a square of distance and thus signal power drops accordingly. With lasers, directional antennas, phased array antennas [1] the signal won't decay that fast. For instance with lasers it will be just a matter of alignment of internal elements to obtain a parallel light beam which doesn't lose power over distance (obviously there are other factors - atmosphere, particles in the vacuum, et al which will result in diveregence anyway). In fact some billionaires [2] invest into using telescopes with fast sampling cameras (in this case IACT [3] telescopes used normally to detect gamma-rays by their interaction with the atmosphere) to detect flashed of extraterrestrial lasers. [1] https://en.wikipedia.org/wiki/Phased_array (see the animation of the radiation pattern) [2] https://www.space.com/are-aliens-flashing-laser-beams.html [3] https://en.wikipedia.org/wiki/IACT reply techbuttman 6 hours agoparentprevWell they put a golden record on voyager (https://voyager.jpl.nasa.gov/golden-record/) with a lot of our info. so I think they were betting on friendly aliens, if any. reply nomercy400 3 hours agoprevThis is cool. Does anybody know how they identified the non-working regions of the chip, without being able to probe the chip? reply project2501a 12 hours agoprevsorry, i just have a silly question: what would it take to send new probes out there? voyager 3 and 4 for example to follow the same path (more or less, sans planet alignment) V1 and V2 followed, but with better hardware of course. reply Rinzler89 12 hours agoparentYou can't. Voyager's launch date coincided with a planetary alignment allowing for gravitational slingshotting out of our galaxy. We have to wait for the next alignment. reply cbhl 12 hours agorootparentIf you only want to get a gravity boost from Jupiter and Saturn (like V1) I wonder if you wouldn't have to wait as long, say, every 20 years instead of every 176? https://voyager.jpl.nasa.gov/mission/timeline/#event-a-once-... But you'd still have to fly for ~40 years to get to where they are now, and they'll keep on flying during those 40 years. reply skykooler 8 hours agorootparentYou don't even need to fly by Saturn - New Horizons only used a Jupiter gravity assist, and those are available about once a year. reply layer8 11 hours agorootparentprevI think you mean solar system. reply vikingerik 11 hours agoparentprevThere's basically no point scientifically for doing the four-planet flyby again. Since Voyager, we've already done much better at Jupiter and Saturn with years-long orbiter missions. A quick flyby wouldn't get us anything we don't already know at those. Voyager gets the mindshare because it was first and the four-way slingshot is fun to visualize, but the reality is that Galileo and Cassini and Juno delivered thousands of times more data. We could use more investigation at Uranus and Neptune, but we'd get much more out of extended orbiter missions to those rather than another quick flyby. A Uranus orbiter is currently one of the higher priority missions in planning, and there's a launch window for a Jupiter-Uranus slingshot in 2034 or 2035. (What I wonder is, how much planning do these things need? Why can't we just launch another copy of Cassini to Uranus and skip all the expensive design? You'd need some changes to antennas and power supply for a more distant planet, but the scientific instruments and computing platform should just be reusable designs.) reply wongarsu 8 hours agorootparentGoing out further than Voyager might be interesting just for studying the conditions outside the inner solar system. Though I'm not sure how much there is to observe unless you go an order of magnitude faster than Voyager to try to reach the oort cloud reply vikingerik 7 hours agorootparentThere are proposals for this, here's one: https://en.wikipedia.org/wiki/Interstellar_Probe_(spacecraft... reply floxy 11 hours agoparentprevIf you just want to fling something out there fast, I think this is a pretty cool way to do it: https://youtu.be/NQFqDKRAROI?si=28yrQW8B2IfbGvqj&t=883 reply pkaye 12 hours agoparentprevMostly need money to fund them. There are various proposals in US/EU/China: https://en.wikipedia.org/wiki/Interstellar_probe#Proposed_mi... reply dotnet00 7 hours agoparentprevIf you mean just to beat them on distance, we could probably launch something capable of it in a decade or two by getting more serious about nuclear propulsion and using some slingshots to pick up speed further. reply idiotsecant 13 hours agoprevWell goddamn done. reply taco-hands 10 hours agoprevI don't know about anyone else, but that box of assorted pastries/doughnuts should been rapidly consumed after hearing the news! reply hoseja 1 hour agoprevI find it sad they haven't been overtaken by much faster and better probes yet. reply squarefoot 9 hours agoprev\"The team discovered that a single chip responsible for storing a portion of the FDS memory — including some of the FDS computer’s software code — isn’t working. The loss of that code rendered the science and engineering data unusable. Unable to repair the chip, the team decided to place the affected code elsewhere in the FDS memory.\" Just another proof that we may have gained a lot, but also lost something in our pursuit of modernity: on modern systems direct memory access is discouraged if not prevented by the underlying OSes, and this hack would not have been possible. reply InvisibleUp 4 hours agoparentThe modern equivalent to this would be an embedded system with an RTOS, where you do get full control of memory, because you are the OS. We just have nice abstractions on top of that for the most common use cases, since you very rarely need that precise of control over system timings or memory allocation. reply drivebycomment 7 hours agoparentprevModern systems can automatically detect bad memories and map those hardware pages out. SSDs can do it at the firmware. ECC are also self correcting. The \"hack\" wouldn't be necessary, or can be done natively in many modern systems. reply lolive 9 hours agoparentprevI want no remote hacker to do this kind of hack on my devices. reply fsniper 12 hours agoprevGood old segmentation and goto's at work! Are goto's still considered harmful? reply jacoblambda 8 hours agoparentWhat's the paper title? \"goto considered harmful considered harmful\" I think? Even in modern C programming goto is still pulling its weight for handling unrolling and cleanups. reply Covzire 12 hours agoprevCould NASA send another probe after them to act as a kind of message relay? reply Dalewyn 13 hours agoprevHell yeah! reply golem14 5 hours agoparentImagine you are the alien finding the probe 10k years from now and have to analyze and reverse engineer the now quite complex logic :) reply cjk2 13 hours agoprev [–] Next time I bitch about debugging something in a container I'm going to look at this and stop bitching. Great job! reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "NASA's Voyager 1 spacecraft has recommenced transmitting engineering updates to Earth following a five-month pause caused by a malfunctioning chip in its onboard computer.",
      "Engineers at NASA's Jet Propulsion Laboratory have fixed the affected code, enabling Voyager 1 to send scientific data once more.",
      "Voyager 2, its companion spacecraft, remains operational and unaffected, while both probes hold the record as the farthest and longest-running spacecraft, exploring multiple planets before reaching interstellar space."
    ],
    "commentSummary": [
      "NASA's Voyager 1 spacecraft faced recent technical challenges but has reestablished communication with Earth, sparking debates on its journey's importance, potential issues, and religious connotations.",
      "Discussions cover various topics like science fiction scenarios, Voyager probes' longevity, power management, future experiments, planned obsolescence, and implications for deep space exploration and potential extraterrestrial encounters.",
      "Participants express admiration for the Voyager project while contemplating the future of space exploration, emphasizing its significance in understanding extraterrestrial life."
    ],
    "points": 624,
    "commentCount": 151,
    "retryCount": 0,
    "time": 1713812390
  },
  {
    "id": 40126751,
    "title": "Introducing KRAZAM OS: A Collaborative Open-Source Platform",
    "originLink": "https://www.krazam.tv/",
    "originBody": "KRAZAM videos contact gallery merch patreon terminal syllabus brochure.pdf security safe space recycle bin Start site dreamt + built by lowercase 5:59 AM Copyright (C) 2024 - Infinity Hardware: MMX2 In Collaboration With: FaceGoog, Inc. Stakeholders Pleased Features Released Portfolio Blocked By Value Delivered Circadian Rhythm :0 :0.75 :All Red :Container Team and Omega Star Team :Positive :Not Good Shoutout To :98.css (and all our open source friends) Website Team :Gooch Lawrence, Kurt Schoenfeld, Neil, JJ Freedump, Steve, Dale, Makro, Vikas, Gianni Roberto, Sajib Moskowitz, Sanuel Darnuld, Greg Moss the F.O.S.S. Boss, Orange DeLonge, Ross Elephant, Tim Thomasworth 01101000 01100101 01101100 01101100 01101111 00100000 01110111 01101111 01110010 01101100 01100100 00100000 01110011 _ Press ENTER to do nothing",
    "commentLink": "https://news.ycombinator.com/item?id=40126751",
    "commentBody": "Krazam OS (krazam.tv)576 points by zdgeier 10 hours agohidepastfavorite122 comments swyx 9 hours agotheir \"merch store\" https://merch.krazam.tv/ is losing out on a lot of potential revenue by not even making a reference to their most viral videos. Who wouldn't want to rock a Galactus shirt. can we crowdsource merch ideas for these guys pls reply PyWoody 7 hours agoparentOpen the terminal and enter \"yahoo_cd\" for a 3 part scavenger hunt for 15% off in the store! Might need to pick myself up a BALLMERCON 2019\" shirt. Maybe one day Makro will be redeemed. reply metadat 6 hours agorootparentAt first I thought it was \"BALLMERCOIN 2019\", which would also be a kind of cool shirt. reply HPsquared 2 hours agorootparentThat would be an alternate history where Ballmer went rogue. reply Nevermark 1 hour agorootparentHe would have called it WINDOWSCOIN even if Microsoft had already excommunicated him! reply lIl-IIIl 2 hours agoparentprevAgreed. Please give me a mug with the microservices diagram. reply sph 2 hours agorootparentI need one with the daily DevOps affirmations to get through my mornings. reply andrewflnr 8 hours agoparentprevI think Galactus is Marvel IP, so I don't think they can do that one. reply Ygg2 4 hours agorootparentGalactus is deprecated. It's been replaced by Omnious all knowing all seeing user provider, made by famous xz contributor NotCIA. reply SeanAnderson 8 hours agoparentprevRight? Where is the \"Take this offline\" t-shirt :D https://www.youtube.com/watch?v=1RAMRukKqQg reply kaycey2022 7 hours agoparentprevThey can do omega star with iso timestamps reply DustinBrett 7 hours agorootparentIf Omega Star ever gets its shit together reply scoot 2 hours agorootparent“gets its shirt together”? reply VHRanger 6 hours agoparentprevThey'd get a cease&desist instantly if they sold a T-shirt with Galactus (an IP they don't own) reply HammadB 7 hours agoparentprev+1 I was ready to drop $$$ reply teaearlgraycold 9 hours agoparentprevThe hustle mug is a good idea. reply kebman 38 minutes agoprevI like the Safe Spaces. Such heart warming contents! Also, the joke I told in the meeting WAS really funny! reply smnscu 3 minutes agoparentEnjoy https://www.srenity.online/ reply croon 16 minutes agoparentprevCame to say the same thing (about your joke of course). The Safe Space was genuinely both very funny and insightful. reply redbell 58 minutes agoprevSomehow, unrelated.. With the rise of such cool OS projects like Puter[1], DaedalOS [2] et al. [3], with Krazam being one of them, I can't help but dream of the day when the .os TLD for operating systems is introduced. I believe it would make more sense to use/read krazam.os rather than krazam.tv. About a year ago, new TLDs were introduced [4], with .zip and .mov being the most popular. However, the .os TLD wasn't on the list, which was disappointing for me! I'm uncertain about the criteria that determine whether a TLD is a good candidate, and I may be the only one emotionally attached to the idea of a .os TLD. Nevertheless, the introduction of the .zip TLD faced numerous complaints[5],[6], particularly regarding security concerns. ____________________________ 1. https://news.ycombinator.com/item?id=33838179 2. https://news.ycombinator.com/item?id=38830132 & https://news.ycombinator.com/item?id=29779753 3. https://simone.computer/#/webdesktops 4. https://fieldeffect.com/blog/what-you-should-know-about-the-... 5. https://www.bleepingcomputer.com/news/security/new-zip-domai... 6. https://www.malwarebytes.com/blog/news/2023/05/zip-domains reply redsolver 36 minutes agoparentAfaik all 2-letter TLDs are ccTLDs, which means they must be a country code of some country and are managed by it. There's no country with \"os\" yet, so to make the TLD available one would need to found a new country first! reply redbell 6 minutes agorootparentOh, thanks for the clarification! I didn't thought about this. reply throwaway11460 17 minutes agorootparentprevLet's call it the Oasis of Servers. I guess we could find an old oil platform or something... reply SigmundurM 35 minutes agoparentprevI may be wrong, but I believe all 2 letter TLDs are ccTLDs. All none ccTLDs would have to be 3 characters or longer. reply cosmic_quanta 9 hours agoprevKrazam is definitely the highlight of my YouTube subscriptions. Its specificity makes it even funnier. reply coffeebeqn 9 hours agoparentThere aren’t many software engineering comedy channels but Krazam is hilarious no matter what scale you’re rating on reply MikeDelta 2 hours agorootparentI find \"Programmers are also human\" quite funny as well. Interviews with stereotypes. https://www.youtube.com/@programmersarealsohuman5909 reply asimpletune 1 hour agorootparentprevI’ve tried to show it to my gf, who is v funny btw, and she had no idea what parts were supposed to be funny. reply dkarl 6 hours agoparentprevI often think about their Senior Engineer video before interviews. I simultaneously aspire to match that character's ancient wisdom and am terrified that I might accidentally resemble him personally. reply smrtinsert 5 hours agorootparentI actually have stopped telling stories thanks to that skit. reply sph 2 hours agoparentprevFunny? \"You were born to deploy Kubernetes clusters.\" I watch KRAZAM and I get into a deep existential crisis. I love them. reply dgellow 7 hours agoparentprevTheir delivery is so good reply boppo1 8 hours agoparentprevKrazam does tech humor better than I've ever seen it. Absolutely dunks on silicon valley & big bang theory. reply colecut 8 hours agorootparentIs big bang theory really \"tech humor?\" I haven't watched much because the laugh track and general cheesiness turned me off.. but I thought it was just jokes about him being a \"nerd\" in the ways mainstream people think of nerds.. reply chrisfosterelli 8 hours agorootparentI'm convinced the big bang theory is intended for older audiences that see their grand children or children in the main characters; they relate to watching smart young people who can do things they don't understand learn to handle basic social conflict; and the trouble is never serious and is typically resolved nicely within 30 minutes so you know everyone will be OK. reply pavlov 1 hour agorootparentprev> \"jokes about him being a 'nerd' in the ways mainstream people think of nerds\" \"Big Bang Theory\" was initially written that way, but the original pilot episode was a famous flop. Test audiences hated the characters. (The unaired pilot can be found with a Google search.) Even though the pilot failed, the studio liked the concept enough that the showrunners were given the rare opportunity to reshoot the pilot with a new script. They introduced the character of Penny and balanced the scenes carefully around emotional connection, to make it clear to audiences when the characters are connecting or failing to connect. And that's probably closer to why millions of people love the show — it's not the tech jokes or laughing at nerds but the empathy. reply saagarjha 44 minutes agorootparentGive me the tech jokes I don’t need empathy reply getwiththeprog 1 hour agorootparentprevNo, it is advertising. reply Ygg2 4 hours agoparentprevIt's more absurdist. Programmers are also human is way more on tech point in his jokes. reply bavell 9 hours agoprevI clicked on safe space and all was right in the world once more reply timnetworks 8 hours agoparenthttps://www.youtube.com/watch?v=ia8Q51ouA_s reply throwup238 8 hours agoparentprev\"You were born to deploy kubernetes\" \"Your feedback is actionable and important\" \"There will never be another outage again\" \"Your tests are reliable and have appropriate coverage\" reply kaycey2022 7 hours agorootparentpfft.. I have no tests. Still need to figure out how to write them. reply mondobe 6 hours agorootparentYou start by choosing between multiple choice, fill-in-the-blank, or an essay question... reply alex23478 2 hours agorootparentprevWell, technically all of your tests are very reliable then. reply archon810 1 hour agorootparent100% of the tests are passing. reply brennopost 7 hours agoparentprevReminds me of Severance Wellness Sessions reply sph 2 hours agorootparentPlease try to enjoy all your Jira tickets equally. reply akoboldfrying 9 hours agoparentprevSRENITY reply russellpekala 9 hours agoprevHow much do we need to pay this guy to quit software and just make jokes? This guy is an icon. reply swyx 9 hours agoparenti feel like their humor would drop if they lost touch with their real jobs. but also they prob need to do a lot of work to reassure their coworkers that specific parodies of them are off limits or something reply dannyobrien 9 hours agoparentprevNot sure he's actually /in/ software, but you can still pay them here: https://www.patreon.com/KRAZAM reply tehsauce 9 hours agorootparentNot sure if he still is, but definitely was. Many of his videos are filmed in the Amazon NYC office. reply catlover76 8 hours agorootparentI'm surprised he got the go-ahead from Amazon to film there reply mike_d 7 hours agorootparentHaha. You're also looking to make the leap from engineering to comedy? reply handojin 6 hours agorootparentYou made me laugh. I think I love you. reply cdchn 3 hours agoparentprevI saw he was doing a standup set in New York and I was very tempted to undertake the trip to see it. reply the_af 9 hours agoparentprevIt's actually two guys as far as I can tell, but yes, they are definitely awesome. reply SatvikBeri 9 hours agorootparentYeah, from their Patreon \"We’re Ben and Shiva, two friends/mid-level software engineers who have been making dumb stuff together for over a decade.\" reply ashton314 9 hours agoprevClick on “safe space”. DEVOPS is a meaningful term. YOU WERE BORN TO DEPLOY KUBERNETES CLUSTERS! reply kreelman 8 hours agoparentThis had me laughing too. Very cool. Kind of neat that this one sits right next to the meta OS article... Cue music from the lion King ... Can you feel the irony tonight... It's so obvious.. Huge meta corps writing a spurious OS.. What a hideous mess... reply andrewflnr 8 hours agorootparentIt makes me happy that it sits right above the Meta OS post (at this writing). reply panqueca 9 hours agoparentprev\"Your family understands what you do...\" reply johnisgood 9 hours agorootparentThis makes me chuckle. reply makach 2 hours agoprevI clicked on trash and found exactly what I was looking for reply aftergibson 2 hours agoparentNicely done. reply arittr 9 hours agoprevstill one of my all time faves: https://www.youtube.com/watch?v=y8OnoxKotPQ reply fabianholzer 3 hours agoparentI have an extract out of it bookmarked, ready to be sent to unruly product managers, sales people and ux designers: https://www.youtube.com/embed/y8OnoxKotPQ?start=138&end=148 reply adamredwoods 8 hours agoparentprevOne of the most brilliant things ever to be born from the internet uterus. I show this every time our PM asks \"why\". reply tippytippytango 8 hours agoprevWow, I fully expected that to destroy my browser history. Imagine my shock when I clicked back and ended up back on HN. Great work. reply average_r_user 34 minutes agoprevI'm still using picchi 1.0 to improve my deployment pipeline by 20% reply jmward01 9 hours agoprevFun 15 mins. They really should have made the prize code make merch cost more, not less. I think that would have fit well with the theme! reply agocin 7 hours agoprevhttps://www.srenity.online/ reply OisinMoran 43 minutes agoparentOh wow, this is giving very strong Severance vibes! reply Panoramix 2 hours agoparentprevStraight to the bookmarks reply rc_mob 9 hours agoprevThat SRE video accurately depicted my life in DevOps. I felt that one. reply beefsack 1 hour agoprevOpen the Terminal and type help to start a puzzle, not much to it but it was a bit of silly fun! reply divbzero 9 hours agoprevThe top two HN posts are currently: 1. Krazam OS (krazam.tv) https://news.ycombinator.com/item?id=40126751 2. Meta Horizon OS (meta.com) https://news.ycombinator.com/item?id=40115554 Both posts feature OS but with rather different look and feel. reply jeffcox 3 hours agoparentEven odds on which one will still be going in 10 years time. reply peterkelly 8 hours agoprevDoes it support ISO timestamps? reply cdchn 3 hours agoparentWE'RE BLOCKED reply joneil 21 minutes agoprevThe class name on the HTML element definitely makes me want to start using more expressive class names in my code. (Doesn't seem I'm able to copy/paste it into a Hacker News comment) reply yolkedgeek 1 hour agoprevyay go Krazam! absolutely love their videos, their videos are THE inside jokes between me and my co-workers. reply Apocryphon 10 hours agoprevThey truly speak to the soul of a new industry reply Etesam 9 hours agoprevIf you want to run something that looks like windows xp/98 on your browser startpage you can try this extension that I created: https://chromewebstore.google.com/detail/xp-newtab/ncfmlogae... GitHub repo: https://github.com/Etesam913/xp-newtab/ reply bsnnkv 6 hours agoprevThis really is the peak of user interface design. I will never be convinced otherwise. reply pnw 8 hours agoprevThe affirmations video is great. I'm still waiting for a Makro comeback as well! reply tschwimmer 9 hours agoprevKrazam really captures the feeling of working for a mid-sized startup in SF perfectly. The pedantry, the braggadocio, and most of all the absurdity and the alienation one feels working on this stuff. reply indrora 1 hour agoparentBecause it was started by ex-FAANGers, Amazon specifically. reply nine_zeros 8 hours agoparentprev> The pedantry, the braggadocio, and most of all the absurdity and the alienation one feels working on this stuff. Also at FAANG and FAANG wannabes. The absurd obsession over BS and pedantic conflicts is just yucky. reply ashton314 9 hours agoparentprev> braggadocio Thank you for showing me a new word today! reply eptcyka 3 hours agoprevMuch better than HorizonOS. reply whatever1 9 hours agoprevbuttery smooth scrolling + windows 98 = perfection reply an_aparallel 5 hours agoprevim having a moment - i'm an absolute dead ringer for Ben Burke..its freaking me out XD Also they are outrageously funny! reply sph 2 hours agoprevI would be surprised if the KRAZAM guys did not have an HN account. They are so plugged in the Silicon Valley startup life. Now I want to see a video from them that references or is about Hacker News. reply data_ders 8 hours agoprevterminal wants me to FIND THE FIRST CODE IN THE SYLLABUS. where?!?! reply satvikpendem 9 hours agoprevObligatory microservices video: https://youtu.be/y8OnoxKotPQ They're hilarious, I think only matched by Austin Nasso who does a tech roast show: https://youtube.com/@austinnasso/videos reply swyx 9 hours agoparentthats a tall order given he has 3 orders of magnitude less views. i'll give it a shot but burden of proof is on you... reply justech 7 hours agorootparentHe's pretty funny, but suffers from having to pump out more content for tiktok IMO. Krazam rarely uploads but it's always a banger reply satvikpendem 9 hours agorootparentprevThe latter is mostly on TikTok, not YouTube, but I know HN is not too amenable to watching TikTok so I linked his YouTube. reply devX3 1 hour agoprevlegendary reply nxobject 4 hours agoprevIn a world of people with side projects on GitHub, be the guy with a satire YouTube page. reply cdchn 3 hours agoparentWhich one looks better on your resume? reply elwell 9 hours agoprevCould Krazam be behind the HN spam today? reply Apocryphon 7 hours agoparentI forgive them reply spxneo 6 hours agoprevmaybe im too old but not sure why this is on the front page reply imp0cat 5 hours agoparentBecause it's funny and nostalgic. Seriously, who doesn't love those two guys and their videos (I wanted to write \"parodies\", but damn, some of those really hit hard). reply satchlj 6 hours agoprevspent way too long looking for that code in the syllabus reply Physkal 9 hours agoprevWhat happens with the recycle bin reply seeingnature 9 hours agoparentIt's a video feed of yourself with clippy in the bottom right corner saying \"It's you\" reply INTPenis 1 hour agorootparentI'm glad I checked the comments first, not worth it. reply sanex 9 hours agoparentprevGive it access and find out reply ahyc 8 hours agoprevbanger reply Crier1002 8 hours agoprevi just love the details on this one. thanks for sharing! reply smrtinsert 5 hours agoprevI always get one enter away from posting the \"I've brought value but at what cost\" video in my company slack but then I chicken out. reply cdchn 3 hours agoparentPost the \"Leadership Sync\" video after an executive shakeup. reply Terr_ 8 hours agoprevWeird, twice now it has locked up Firefox when I try to open it. Maybe it's somehow trying to access my clipboard, and that's triggering a separate problem I've been having when RDP'ing to a certain Linux machine in another window... reply mixmastamyk 7 hours agoprevHa, I made a boot-up sequence on my site in the mid-90s. Brings back the memories. reply hypertexthero 5 hours agoprev“You were born to deploy Kubernetes.” LOL reply Xunxi 8 hours agoprev [–] I saw a similar design theme on Dan Brown's personal site recently and was wondering if it was one of those trends to feed off nostalgia or just a coincidence https://danb.me/ reply timnetworks 8 hours agoparenthttps://jdan.github.io/98.css/ reply tracerbulletx 7 hours agoparentprev [–] This has been a trend for at least 15 years. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The discussion explores potential merchandise ideas for Krazam OS online store, including a desire for a .os top-level domain and criticism of the .zip TLD.",
      "It highlights the success of \"Big Bang Theory\" attributed to empathy, featuring a comedy duo of software engineers and relatable tech content.",
      "The conversation also includes comparisons with other operating systems, nostalgic web design trends, and Kubernetes deployment."
    ],
    "points": 576,
    "commentCount": 122,
    "retryCount": 0,
    "time": 1713829123
  },
  {
    "id": 40119958,
    "title": "Challenges in Book Industry Amid Rise of Self-Publishing",
    "originLink": "https://www.elysian.press/p/no-one-buys-books",
    "originBody": "Share this post No one buys books www.elysian.press Copy link Facebook Email Note Other Discover more from The Elysian Studying utopia + writing about a better future through essays and fiction. Over 14,000 subscribers Subscribe Continue reading Sign in No one buys books Everything we learned about the publishing industry from Penguin vs. DOJ. Elle Griffin Apr 22, 2024 284 Share this post No one buys books www.elysian.press Copy link Facebook Email Note Other 181 Share In 2022, Penguin Random House wanted to buy Simon & Schuster. The two publishing houses made up 37 percent and 11 percent of the market share, according to the filing, and combined they would have condensed the Big Five publishing houses into the Big Four. But the government intervened and brought an antitrust case against Penguin to determine whether that would create a monopoly. The judge ultimately ruled that the merger would create a monopoly and blocked the $2.2 billion purchase. But during the trial, the head of every major publishing house and literary agency got up on the stand to speak about the publishing industry and give numbers, giving us an eye-opening account of the industry from the inside. All of the transcripts from the trial were compiled into a book called The Trial. It took me a year to read, but I’ve finally summarized my findings and pulled out all the compelling highlights. I think I can sum up what I’ve learned like this: The Big Five publishing houses spend most of their money on book advances for big celebrities like Brittany Spears and franchise authors like James Patterson and this is the bulk of their business. They also sell a lot of Bibles, repeat best sellers like Lord of the Rings, and children’s books like The Very Hungry Caterpillar. These two market categories (celebrity books and repeat bestsellers from the backlist) make up the entirety of the publishing industry and even fund their vanity project: publishing all the rest of the books we think about when we think about book publishing (which make no money at all and typically sell less than 1,000 copies). But let’s dig into everything they said in detail. Did you know that 96% of books sell less than 1,000 copies? That’s why I write here instead 👇🏻 Subscribe Bestsellers are rare In my essay “Writing books isn’t a good idea” I wrote that, in 2020, only 268 titles sold more than 100,000 copies, and 96 percent of books sold less than 1,000 copies. That’s still the vibe. Q. Do you know approximately how many authors there are across the industry with 500,000 units or more during this four-year period? A. My understanding is that it was about 50. Q. 50 authors across the publishing industry who during this four-year period sold more than 500,000 units in a single year? A. Yes. — Madeline Mcintosh , CEO, Penguin Random House US The DOJ’s lawyer collected data on 58,000 titles published in a year and discovered that 90 percent of them sold fewer than 2,000 copies and 50 percent sold less than a dozen copies. In my essay “No one will read your book,” I said that publishing houses work more like venture capitalists. They invest small sums in lots of books in hopes that one of them breaks out and becomes a unicorn, making enough money to fund all the rest. Turns out, they agree! Every year, in thousands of ideas and dreams, only a few make it to the top. So I call it the Silicon Valley of media. We are angel investors of our authors and their dreams, their stories. That’s how I call my editors and publishers: angels… It’s rather this idea of Silicon Valley, you see 35 percent are profitable; 50 on a contribution basis. So every book has that same likelihood of succeeding. — Markus Dohle, CEO, Penguin Random House Those unicorns happen every five to 10 years or so. We’re very hit driven. When a book is successful, it can be wildly successful. There are books that sell millions and millions of copies, and those are financial gushes for the publishers of that book, sometimes for years to come… A gusher is once in a decade or something. For instance, I don’t know if you know the Twilight series of books? Hachette published the Twilight series of books, and those made hundreds of millions of dollars over the course of time. Right now the novels of Colleen Hoover are topping the bestseller lists in really, really huge numbers and the publishers of those books are making a lot of money. You probably remember The Girl With the Dragon Tattoo… Or the Fifty Shades of Grey series. So once every five years, ten years, those come along for the whole industry and become the industry driver that’s drawing people into bookstores because there is such a commotion about them. — Michael Pietsch, CEO, Hachette Big advances go to celebrities They spent a lot of the trial talking about books that made an advance of more than $250,000—they called these “anticipated top-sellers.” According to Nicholas Hill, a partner at Bates White Economic Consulting, 2 percent of all titles earn an advance over $250,000. Publisher’s Marketplace says it’s even lower. Top-selling authors were defined as those receiving advances (i.e., guaranteed money) in excess of $250,000. Far fewer than 1 percent of authors receive advances over that mark; Publishers Marketplace, which tracks these things, recorded 233 such deals in all of 2022. — ken whyte , Publisher at Sutherland House Hill says titles that earn advances over $250,000 account for 70 percent of advance spending by publishing houses. At Penguin Random House, it’s even more. The bulk of their advance spending goes to deals worth $1 million or more, and there are about 200 of those deals a year. Of the roughly $370 million they say PRH accounts for, $200 million of that goes to advance deals worth $1 million or more. This chart shows that as advances go up, more of them come from Penguin Random House which has the deepest pockets. Most of those are deals with celebrities. And Penguin gets most of them. Books by the Obamas sold so many copies they had to be removed from the charts as statistical anomalies. There are giant celebrities Michelle Obama where you know it’s going to be a top seller. — Jennifer Rudolph Walsch, Literary Agent Because they are so lucrative, Gallery Books Group focuses its efforts on trying to get celebrities to write books. 75 percent [of our] acquisitions come from approaching celebrities, politicians, athletes, the “celebrity adjacent,” etc. That way, we can control the content…. We are approaching authors and celebrities and politicians and athletes for ideas. So it’s really we are on the look out. We are scouts in a lot of ways… — Jennifer Bergstrom, SVP, Gallery Books Group Bergstrom said her biggest celebrity sale was Amy Schumer who received millions of dollars for her advance. We’ve had a lot of success publishing musicians, I mentioned Bruce Springsteen. We’ve also published Bob Dylan and Linda Ronstadt, a lot of entertainers through the years… There was a political writer, Ben Shapiro, who has a very popular podcast and a large following. We also competed with HarperCollins for that. — Jonathan Karp, CEO, Simon & Schuster Penguin Random House US has guidelines for who gets what advance: Category 1: Lead titles with a sales goal of 75,000 units and up Advance: $500,000 and up Category 2: Titles with a sales goal of 25,000-75,000 units Advance: $150,000-$500,000 Category 3: Titles with a sales goal of 10,000-25,000 units Advance: $50,000- $150,000 Category 4: Titles with a sales goal of 5,000 to 10,000 units Advance: $50,000 or less Is anyone else alarmed that the top tier is book sales of 75,000 units and up? One post on Substack could get more views than that….. Franchise authors are the other big category Franchise authors are the other big category. Walsch says James Patterson and John Grisham get advances in the “many millions.” Putnam makes most of its money from repeat authors like John Sandford, Clive Cussler, Tom Clancy, Lisa Scottoline, and others. Q. Putnam typically publihses about 60 books a year. Correct? A. 60, 65, sort of on naverage… I will say of those 65, though, a good portion of those are repeat authors… franchise authors that we regularly publish every year, sometimes twice a year. — Sally Kim, SVP and Publisher, Putnam Publishing houses want a built-in audience The advantage of publishing celebrity books is that they have a built-in audience. In some of the cases, the reason they are paying big money is because the person has a big platform. And if that platform is there for the advertising, then the spend might be lower. — Jennifer Rudolph Walsh, former Agent Macmillan agrees. Q. Would you agree that those type of authors, meaning the ones with the built-in audience, are also authors who would command a high advance if they went to a traditional publisher like Macmillan or PRH? A. That’s a broad brush. But, yes… Q. And you’re willing to pay more if they have a significant following? A. Yes. — Donald Weisberg, CEO, Macmillan Publishers They give some examples: The Butcher and the Wren… this particular author has a big following, and with a single post on Instagram, she presold over 40,000 books. So, I mean, that’s just staggering from a per copy perspective, and it pretty much guarantees a number one spot on the New York [Times] best seller list when it’s published in September. — Jennifer Rudolph Walsh, former Agent A big audience means publishing houses don’t have to spend money on marketing These big advances, the authors have quite a bit of their own infrastructure with them. They have their own publicists. They have their own social media people. They have their own newsletters. So they actually are able—we are able to offload a good amount of the work, not all the time, but that is actually a factor in why we sometimes pay these big advances, because the authors are actually capable of helping us a lot. — Jonathan Karp, CEO, Simon & Schuster For example: Q. Who is the best selling Simon & Schuster author currently? A. Right now it’s Colleen Hoover. Q. Does she have the highest marketing budget that Simon & Schuster pays? A. No. Q. Why is that? A. She’s the queen of TikTok, and so she has a huge following on TikTok. — Jonathan Karp, CEO, Simon & Schuster Related: [One author wrote] paranormal, so it’s sexy vampires. This book was probably her 21st book. So she’s what I would call a franchise author. She’s very established. Though we spent $1.2 million on the book, we spent about $62,000 on the marketing and publicity because she had such an established fan base… [Another author is] a celebrity-adjacent author, but also her platform was on social media. So we paid $450,000 for her book, and we spent $36,000 on the marketing and publicity. We didn’t need to spend more than that because she already booked at that point on Good Morning America, The Today Show. So publicity drove that, and that didn’t cost us. — Jennifer Bergstrom, SVP, Gallery Books Group Just goes to show that the main thing an author gets from a publishing house is an advance! Publishing houses pay for Amazon placement Every second book in America, ballpark, is being sold via e-commerce…Amazon.com has 50 million books available. A bookstore, a good independent bookstore, has around 50,000 different books available… an algorithm decides what is being presented and made visible and discoverable for an end consumer online. It makes a huge difference. — Markus Dohle, CEO, Penguin Random House Publishing houses try to game the algorithm and even pay to get ahead of it. Q. Penguin Random House has hired data scientists to try and figure out these algorithms so that its books get better presented on Amazon than its competitors’ books? A. One of the many efforts that we pursue, correct. Q. And Penguin Random House pays Amazon to improve its search results? A. There is something that is available to our publishers, it’s called Amazon Marketing Services, AMS, and all publishers can spend money and give it to Amazon to have hopefully better search results. — Markus Dohle, CEO, Penguin Random House But even celebrity books don’t sell… Ayesha Pande , president of Ayesha Pande Literary, says that 20 percent of her authors earn out their advance—if she’s being generous. The single most important contract term is the advance…Because in a large number of cases, it may be the only compensation that the author will receive for their work. — Ayesha Pande, President, Ayesha Pande Literary Even celebrity books flop. There are plenty of books that we spend $1 million on the advance and published them last year and they did not even make the top 1,000 on BookScan… Less than 45 percent of those books [that we spend a million dollars on] end up on that thousand best seller list. — Madeline Mcintosh, CEO, Penguin Random House US Just because the publisher pays $250,000 or $500,000 or $1 million for a book does not guarantee that a single person is going to buy it. A lot of what we do is unknowable and based on inspiration and optimism.” — Michael Pietsch, CEO, Hachette Even celebrities, though sometimes you think it’s going to be a big best seller, it flops. It happens… I mean, Andrew Cuomo’s book was sold at the height of his being America’s governor during the COVID crisis. I mean, that book was sold for $5 million, I believe. I don’t know for a fact. But by the time it came out, the nursing home scandal had happened, the Me Too issues, and the book didn’t do any business. Sometimes it’s just a timing issue, like Marie Kondo. She did a book about Joy at Work, about making your office sparked with joy because it’s not cluttered. It published in March of 2020. — Jennifer Rudolph Walsch, Literary Agent Having a lot of social media followers or fame doesn’t guarantee it will sell. The singer Billie Eilish, despite her 97 million Instagram followers and 6 million Twitter followers, sold only 64,000 copies within eight months of publishing her book. The singer Justin Timberlake sold only 100,000 copies in the three years after he published his book. Snoop Dog’s cookbook saw a boost during the pandemic, but he still only sold 205,000 copies in 2020. Here’s a few more: Representative Ilhan Omar, a Democrat from Minnesota, is no global pop star, but she has a significant social-media presence, with 3 million Twitter followers and another 1.3 million on Instagram. Yet her book, This Is What America Looks Like: My Journey from Refugee to Congresswoman, which was published in May 2020, has sold just 26,000 copies across print, audio and e-book formats, according to her publisher. Tamika D. Mallory, a social activist with over a million Instagram followers, was paid over $1 million for a two-book deal. But her first book, State of Emergency, has sold just 26,000 print copies since it was published in May, according to BookScan. The journalist and media personality Piers Morgan had a weaker showing in the United States. Despite his followers on Twitter (8 million) and Instagram (1.8 million), Wake Up: Why the World Has Gone Nuts has sold just 5,650 U.S. print copies since it was published a year ago, according to BookScan. —The New York Times It’s pretty common. The worst day of a life of an agent and an author is when they’ve gotten a large advance and you go on BookScan and you see their first few months’ of sales and it says 4,000 copies or something like that. It happens. It happens more than any of us would like. — Gail Ross, Literary Agent Books don’t make money If I look at the top 10 percent of books… that 10 percent level gets you to about 300,000 copies sold in that year. And if you told me I’m definitely going to sell 300,000 copies in a year, I would spend many millions of dollars to get that book. — Madeline Mcintosh, CEO, Penguin Random House US Publishing houses pay millions of dollars for a book that sells only 300,000 copies??? Well, because books don’t sell a lot of copies, they don’t make a lot of money. According to Hill, 85 percent of the books with advances of $250,000 and up never earn out their advance. (Meaning the royalties earned never covered the cost of the advance.) Very, very frequently, the winning bid in our calculation is a money loser. — Michael Pietsch, CEO, Hachette A chart from Penguin Markus Dohle, CEO, Penguin Random House, says the top 4 percent of titles drive 60 percent of the profitability. That goes for the rest of them too: It would be just a couple of books in every hundred are driving that degree of profit… twoish books account for the lion’s share of profitability. — Madeline Mcintosh, CEO, Penguin Random House US Around half the books we publish make a profit of some kind. — Michael Pietsch, CEO, Hachette About half of the books we publish make money, and a much lower percentage of them earn back the advance we pay. — Jonathan Karp, CEO, Simon & Schuster Many publishers have realized that maybe those big advances aren’t worth it. We have a report that we colloquially call ‘The Ones That Got Away.’ And it’s a report on the books where we bid $500,000 or more as an advance and did not succeed in acquiring the book… this report stands as a kind of caution against the high risk of big advances because the lesson we take away again and again is: Thank goodness we stopped bidding when we did because even at the advance we offered, we would have lost money… Very frequently, the winning bid in our calculation is a money loser. — Michael Pietsch, CEO, Hachette It’s all about the backlist If new books typically don’t sell well, well that’s why publishing houses make their revenue from their backlist. I would actually expect a book that is selling 300,000 units in a year is probably going to sell at least 400,000 or 500,000 over its life once you get backlist in there too. Our backlist brings in about a third of our annual revenues, so $300 million a year roughly, a little less. — Michael Pietsch, CEO, Hachette The backlist includes all of the books that have ever come out. Brian Murray, CEO of HarperCollins, points out that their backlist includes bibles (an $80 million business), coloring books, dictionaries, encyclopedias, magic trick books, calendars, puzzles, and SAT study guides. It also includes perennial bestsellers like Don Quijote, Steven King’s Carrie, and Tolkien’s Lord of the Rings—these books continue to sell year after year. Popular children’s books are cash cows selling huge amounts of copies year after year and generation after generation. Sometimes children’s books will be three generations, people have been buying them over and over again, and so that backlist catalog is really, really important to pay for the overhead of your publishing teams and then also to take the risks on the new books. So without a backlist I think it’s very hard to compete with these big books. — Brian Murray, CEO, HarperCollins For instance, Penguin Random House owns Eric Carle’s Very Hungry Caterpillar intellectual property. The book has been on Publisher Weekly’s bestseller list every week for 19 years. Children’s books comprised 27 percent of PRH’s sales in 2021. That’s about $725 million—so roughly double the size of Scholastic’s trade division, and more or less equal on its own to all of Macmillan or HBG. Christian books accounted for 2 percent. —The Trial Backlist titles like The Bible and Very Hungry Caterpillar and Lord of the Rings make up a disproportionately large percentage of the publishing industry. Amazon is the biggest threat to the industry Q. Are you concerned that Amazon will favor Penguin Random House Simon & Schuster in terms of promotion and distribution and discoverability? A. Yes. — Donald Weisberg, CEO, Macmillan Publishers With Amazon’s data, they could immediately beat out all the publishing houses if they wanted to. I think Amazon as a publisher of books is underestimated. They have about 50 editors… Obviously, given the number of people searching on Amazon for products, that gives them a huge advantage because when people go onto Amazon, they—if the book isn’t there for what they are searching for, they could create that book. That’s one theory I have. But even if that doesn’t happen, they know what people are buying and they have access to that data. Their bestseller list, in my view, is more important than The New York Times best seller list because it’s in realtime. It’s hourly. And I look at that Amazon best seller list regularly, every day. — Jonathan Karp, CEO, Simon & Schuster A “Netflix of Books” would put publishing houses out of business Wouldn’t it be great if you could pay $9.99 a month and read all of the books you want? Just like you get all the movies you want from Netflix? Or all the music you want from Spotify? Technically, it does exist. Kindle Unlimited is the largest, followed by Scribd. Audible isn’t quite all-access, but then Spotify got into audiobooks and made them so. But none of these players have quite taken off the way Netflix or Spotify has. That’s for one reason: The Big Five publishing houses refuse to let their authors participate. Q. No books are found on Kindle Unlimited? Because you think that’ll be had for the industry?” A. We think it’s going to destroy the publishing industry. — Markus Dohle, CEO, Penguin Publishing House He’s right. No one would purchase a book again. We all know about Netflix, we all know about Spotify and other media categories, and we also know what it has done to some industries… The music industry has lost, in the digital transformation, approximately 50 percent of its overall revenue pool. — Markus Dohle, CEO, Penguin Publishing House There’s one reason. Around 20 to 25 percent of the readers, the heavy readers, account for 80 percent of the revenue pool of the industry of what consumers spend on books. It’s the really dedicated readers. If they got all-access, the revenue pool of the industry is going to be very small. Physical retail will be gone—see music—within two to three years. And we will be dependent on a few Silicon Valley or Swedish internet companies that will actually provide all-access. — Markus Dohle, CEO, Penguin Publishing House The publishing industry would die, that’s for sure. But I’d be willing to bet writers would get their books read way more. And I think it’s on its way. Spotify has already started publishing audiobooks, and my money is on Substack for eventually publishing written books! Authors are getting more independent If publishing houses make minimal investment in marketing their authors and focus largely on celebrity books and their backlist, authors who can’t snag a large advance might have better luck building their own audience and publishing elsewhere. I think really from the advent of online—really, once the internet became popular, you know, we heard the phrase disintermediation. And I don’t understand why that wouldn’t be a possible prospect for any best selling author, to just disintermediate, to go straight to the internet and sell directly if you have a following… Colleen Hoover has published with both Amazon and Simon & Schuster. And her Amazon book was on the independent book sellers’ best seller list. So what that says to me is that a Rubicon has been crossed. — Jonathan Karp, CEO, Simon & Schuster The romance category has already gone independent. Many of those heavy readers of romance novels at that time switched to self-published stories. A very different price point. 99 cents, $1.99, away from what we call mass-market trade paperbacks… The mass-market trade paperback is the sort of small-format mass-market book, like it is a trade paperback, but a smaller format. It has been declining for the last 25 years. But we had a step change around ’14, ‘15, with this trend that so many consumers went away from mass-market books into electronic ebooks in particular and self-published books.” — Markus Dohle, CEO, Penguin Random House Gallery author Anna Todd moved to self-publishing (though Todd began her career writing on Wattpad, and recently returned to set up an imprint at Wattpad Books). — Jennifer Bergstrom, SVP, Gallery Books Group And of course, we have to talk about Kickstarter MVP Brandon Sanderson. There is a New York Times best selling author in the science fiction and fantasy category. His name is Brandon Sanderson. I believe he’s published by both Macmillan and Penguin Random House. He went onto Kickstarter and announced that he would be offering four of his novels to anybody who wanted them if they wanted to donate to Kickstarter. And he raised over $42 million… I have subsequently become aware of Good Night Stories for Rebel Girls, which is a series of books. It’s now actually become a whole company. And these are stories to give young girls confidence. And it’s been very successful, and it’s actually resulted in an entire company. — Jonathan Karp, CEO, Simon & Schuster Another publishing house bites the dust After the Judge denied the merger, Penguin went through a massive round of layoffs and Simon & Schuster was sold to a private equity company instead. Private equity tends to have one game plan: buy a company, load it with debt, wring out costs to improve its financials, sell at a profit. Dealing Simon & Schuster to private equity, The New Republic warned at the time with some slight hyperbole of its own, would mean “absolute devastation and wholesale job loss.” — ken whyte The publishing houses may live to see another day, but I don’t think their model is long for this world. Unless you are a celebrity or franchise author, the publishing model won’t provide a whole lot more than a tiny advance and a dozen readers. If you are a celebrity, you’ll still have a much bigger reach on Instagram than you will with your book! Personally, I could not be more grateful to skip the publishing houses altogether and write directly for my readers here, being supported by those who read this newsletter rather than by a publishing advance that won’t ultimately translate to people reading my work. But I’d love to know your thoughts 👇🏻 Leave a comment Thank you for reading and being here, Elle Griffin P.S. If you enjoyed this post please consider sharing it. That’s how I meet new people and earn a living as a writer! ✨ Share 284 Share this post No one buys books www.elysian.press Copy link Facebook Email Note Other 181 Share",
    "commentLink": "https://news.ycombinator.com/item?id=40119958",
    "commentBody": "No one buys books (elysian.press)433 points by AlbertCory 13 hours agohidepastfavorite399 comments grepLeigh 6 hours agoFrom the article: >>> \"Wouldn’t it be great if you could pay $9.99 a month and read all of the books you want?\" I sincerely hope nothing \"disrupts\" public libraries in my lifetime. As a California resident, I can walk into ANY public library in the state and get a free library card with access to physical books, audiobooks, ebooks. Some branches have laptops, hotspots, tablets, e-readers available to borrow. My local branch even has a Makerspace with 3D printers, laser cutters, sewing machines, and other misc tools. reply yard2010 2 minutes agoparentSure but why would I pay for something I can get for free from the library - either a physical one or genesis? reply yellow_postit 6 hours agoparentprevIt’s funny seeing this comment and the article about Seattle libraries reducing digital copies both on HNews front page at the same time. I love libraries. It also feels like all our public institutions in the US are crumbling under assault from all sides. reply ddingus 3 hours agorootparentThey are. Every single public resource is seen as a potential market opportunity being squandered away, or as competition other players do not want to see in play. reply atoav 2 hours agorootparentWell a library is a place where a population can spend the day getting smarter while not spending money. If you're rich, powerful and an unethical asshole those are two things you definitely do not want. Sadly many people seem to think they are really showing the folks up there by retreating from any form of self-education once they are out of school. The opposite couldn't be more true. reply Zenzero 1 hour agorootparent> Well a library is a place where a population can spend the day getting smarter while not spending money. If you're rich, powerful and an unethical asshole those are two things you definitely do not want. This is conspiracy level thinking. Nobody wealthy is sitting around scheming how to keep the proles dumb and subdued. The assault on public institutions like libraries comes from 2 places: 1) Stop wasting my tax dollars on stuff I don't use or care about. Or 2) I bet there is an untapped commercial market that can be built, and people will love my solution more than public libraries. reply vik0 1 hour agorootparentprev>If you're rich, powerful and an unethical asshole those are two things you definitely do not want. This smells like a half-baked conspiracy theory. It's not like the overwhelming majority of people want to get smarter in their free time. I'm not sure how it is in the rest of the world, but in American and all (individual) European cultures, most people don't pursue self-education. I would not be surprised if this applies to most other cultures as well You might say that's how a lot of people on this website got their knowledge, but this website is statistically insignificant compared to the world population. Plus, it's not like every single member here pursues self-education. I'm willing to wager most people come on here just to be entertained reply atoav 1 hour agorootparentI didn't say rich, powerful and unethical people actively need to work against public libraries. They just don't need to support them, which is enough, given the fact that they often land in or near places of political power. Just don't increase the funding while inflation increases and you're basically cutting funding. And then when things eventually start to look like shit tear them down. And these are not even things a rich, powerful and unethical person needs to do willingly or on purpose. It is just a thing that happens, because it isn't high on the priority list. And it isn't high on the priority list because it doesn't make them direct money. Now if you're a politician that profits from your constituents being not too critical why should you actively support a thing that has the potential to make them more critical? Not that you actively have to be against it, it just isn't that high on your list of priorities.. This is how incentives work. And while there are people who will support measures which won't directly profit them short term, it seems to me they are an endangered species, at least in the US. reply grepLeigh 3 hours agorootparentprevOverDrive (and Libby) were acquired by KKR, the behemoth PE firm responsible for bankrupting Toys R Us. KKR also acquired one of the Big Five publishing houses last year. I'm worried about public library funds being squeezed by an extractive PE playbook. I wonder what kind of negotiating power a regional library has in that situation. Do state library agencies have more leverage than regional libraries? Do large states like California have more leverage in negotiating digital licenses than smaller states? Would a national library system have even more leverage? I'll ask a librarian tomorrow and report back. reply lstamour 1 hour agorootparentOverDrive is a strong player but they’re still a commodity. Publishers and libraries can use other lending platforms if and when they emerge as stronger competitors. This article serves as a good overview: https://openeducationalberta.ca/ciicm/chapter/public-library... The problem is the publishers - they don’t always sign on to provide books to every new platform and don’t always release to every platform. But other platforms do exist: https://bookriot.com/comparing-public-library-ebook-platform... reply j45 4 hours agorootparentprevLimiting or undermining literacy remains important to those who benefit from it. reply nox101 3 hours agorootparentwhat logic leads to this conclusion? I tend to lean to the rising tide lifts all boats. who benefits from illiteracy? reply grepLeigh 2 hours agorootparentUNESCO published a report on the beneficiaries of illiteracy at global, national, and regional levels (1989): https://unesdoc.unesco.org/ark:/48223/pf0000085962 reply Root_Denied 3 hours agorootparentprev> who benefits from illiteracy? Those already on top. They can afford to educate their own line and hoard resources across generations doing so. They can fail as many times as it takes before they finally succeed because of this. They throw the bodies of the world at their problems until they are solved, which necessitates an uneducated populace that doesn't realize they're chattel and canon fodder to this purpose. A populace that fear collective power, that fears community beyond immediate family, that shies away from actions that would better the world in favor of actions that are less risky personally. reply animex 2 hours agorootparentprevPeople who want to control the illiterate and subjugate their voting power to their own ends. reply Karellen 43 minutes agorootparentprevThe number of vastly unequal societies throughout history who have banned the lower classes from learning how to read, or from any kind of education at all - beyond whatever narrow training will make them better perform their specific duties for those on top. ...and not just historically. Some societies where significant minority groups are banned from getting an education, in order to further perpetuate their marginalisation, still exist today. reply yunohn 2 hours agorootparentprevElites are always worried about the labor crisis - “nobody wants to work”. Usually this is about blue collar work, and their desire to underpay and overwork service/laborers. Globally, this class of people definitely wants to reduce literacy to keep a stream of manual labor inflowing. reply eeskildsen 2 minutes agoparentprevSince going full-time on my startup, I often work remotely at my local library. Over my time there, my eyes have been opened to what an absolute treasure public libraries are. Besides the physical books everyone knows about, which are a treasure by themselves, there are many other valuable resources, as you mentioned, including: - The Adobe suite, even including Character Animator - Udemy - Digital access to The New York Times, The Wall Street Journal, and others - Free notary services - Print magazines and newspapers - Puzzles - Music and films - eBooks and audiobooks ...not to mention community activities like classes, groups, and concerts. Just being physically present in the library has benefited me. When I take breaks, I sometimes pick up a book at random. It may be about sales, health, politics, history. I sometimes come away with new ideas for my business, sometimes am just inspired or informed or amused in general. I encourage anyone reading to stop by a library if you haven't been in a while. See what they offer. You might be surprised. And please, do what you can to support them in whatever way you can, even if that's just to use them, demonstrating the need for them by your patronage. We need these institutions, and they won't be around forever without our support. reply initplus 5 hours agoparentprevIt's not disruption to make me wait 5 weeks to read a digital file through my local library's Libby service. It's an outdated business model using artificial scarcity that isn't effective at getting books onto readers devices. Spotify doesn't make me take out a hold on the artist I want to listen to. Netflix doesn't make me queue to watch the latest release. reply grepLeigh 2 hours agorootparentNetflix has licensing restrictions too, but the limitation is time/exclusivity (rather than number of copies). You can only watch whatever Netflix owns distribution rights to. Although Netflix does charge more per device, so that's a bit like charging per \"copy\" of their service on top of the limited distribution. I'm wondering if you'd really prefer a library system where you could get some titles instantly, but the majority of content is unavailable because some other digital service provider owns distribution in your region for the next 12-16 months. I'd hate that, personally. reply rockemsockem 3 hours agorootparentprevI don't think it's fair to compare Netflix and Spotify to libraries; you pay for the first two. I agree that it's artificial scarcity and it's hard to feel bad for the publishing companies reply seanp2k2 3 hours agorootparentWe pay for libraries through taxes, it’s just that they’re considered a public good so important that everyone has to contribute to pay for them. reply thayne 2 hours agorootparentprevI do pay for my library card, because my city doesn't have a library (unless you count bookmobile), and the nearby cities that do have libraries charge you to get a library card if you aren't a resident. And it is approx. $9.99/mo. reply dredmorbius 2 hours agorootparentprevPublic libraries are paid for through public taxation. Which suggests a possible solution the the greater problem. reply tasuki 57 minutes agorootparentprev> It's not disruption to make me wait 5 weeks to read a digital file That seems very much like a disruption to me. In the \"interruption to the regular flow or sequence of something\" meaning of the word. reply rfarley04 2 hours agoparentprevThis! I'm from Oregon but live in Thailand, where there are ZERO libraries. My absolute favorite thing about going home is just maxxing out my library card and seeing how much I can get through before heading home (I can do ebooks and audiobooks while abroad with Libby but I'm one of those obnoxious people that always prefers physical) reply closetkantian 2 hours agorootparentThis is an exaggeration. I also live in Thailand, and I just got a library card at the very stately Neilson Hayes library last week. A bit pricey (3000 THB/year) but amazing ambience since the library was built in 1860. reply wrp 1 hour agorootparentI haven't been to Thailand, but I assume there are also libraries at the universities. The parent appears to be referring to a tradition of public libraries, so these are not really counterexamples. I've used private English libraries in various countries of the Middle East and East Asia. For the expat community, they were really a treasure before the internet. reply ar_lan 5 hours agoparentprevIf anything did, it's Libby, except that directly just worked alongside libraries instead of disrupting them. reply lannisterstark 5 hours agorootparentI sincerely hope one day they get off their asses and implement casting that their predecessor, overdrive, had. speaking of, btw, your library likely has 'partner' libraries where you can borrow stuff too using overdrive/libby. You can often see a list on top right of the overdrive page of your respective library. reply sleepycatgirl 2 hours agoparentprevYeah... Libraries are very much lovely places. Truly bliss how they can exist... And about books themselves, I hate subscriptions, and all the DRM garbage, and if I can't own the epub without DRM, then I won't buy it. To be honest, the silver lining is that DeDRM exists, so I can actually obtain Japanese novels for my cute e-ink devices, but.. bleh. reply coding123 3 hours agoparentprevIt would be the most expensive megabyte. reply paxys 11 hours agoprevCan't help but feel for the publishing industry considering how shafted they got by tech. You distribute books all over the country? Amazon can do it better and cheaper. You print books? Well we have e-books now. You have a massive back catalog? Google just scanned all of it. You do marketing? Authors now have their own followings on social media and can reach them directly. You give cash advances? Fans on Kickstarter give 10x more. What's sad though is that publishers have historically had one power that would have been unassailable – editorial judgement. They could have sustained their brands on quality. Imagine a world where you wanted something good to read, and among all the garbage out there saw a title and went \"this one is by Simon & Schuster, it has to be good\". Instead they went all in on pulp bestsellers and celebrity memoirs at the expense of actual good authors, and here we are. reply devjab 4 hours agoparentI can. Book publishers are terrible and they always have been. At least that is the case here in Denmark. They game our library system, so that books they know won’t sell well but will be lent a lot at the libraries, like a lot of children books are priced ridiculously. These books are never actually put in stores, because why bother? But our libraries, have, to buy them and then pay fees to the publishing houses based on the individual book prices. I guess part of the blame goes to our politicians though. The worst part is how they pay artists. Authors are paid poorly as touched upon in this article. But illustrators are paid criminally low rates. A children book with 50 pages of breathtaking art can earn you as little as $500 for months for work. Publishers pay your royalties by the size of your printed name on the front of the book. If your name isn’t there you get 0 royalties. If your name is smaller than the other names you get significantly less royalties. If your name is smaller, and, italic, you get almost no royalties and so on. In general the model has never really worked for anyone except a few authors. There are the famous ones who sell a lot of books and then there are the ones who write a billion children’s books who actually don’t get the majority of their money from publishers but from the government compensation programs. But for the most part publishers have always run a business model akin to music streaming services where only a handful of Danish authors can actually make a living just by selling books. Mean while our publishing houses have been able to employ thousands and keep investors happy. What a lot of artists and authors have done instead is to form smaller independent publishers. Which means you get 0 advertising and 0 product placement and so on, but ironically often will make you far more money than using the big publishers. These smaller indie publishers, and other creative ways of publishing like the article mentions, aren’t doing poorly. The big publishers are suffering. Thanks to the digital age. But I won’t miss them when they are gone. reply bazoom42 1 hour agorootparent> books they know won’t sell well but will be lent a lot at the libraries This sounds weird. Wouldn’t the books which are lent more also sell more? Childrens books are a big market for book stores and of course they want to sell the most popular books. Are you referring to some particular book which cannot be purchased? Also libriaries does not have to buy any book. reply yunohn 2 hours agorootparentprev> I guess part of the blame goes to our politicians though. There is no universe in which public funds are leeched without political complicity and corruption. Most likely the publishers benefitted are either related to or provided kickbacks to the influential parties. reply bazoom42 1 hour agorootparentSorry, is publishing a book which gets a lot of readers through the public libraries now “leeching” and “corruption”? reply serial_dev 1 hour agorootparentWhat a strawman! Depending on how they got the deal with the library, and how much they get compensated for that, yes it is. reply bazoom42 1 hour agorootparentWhat do you mean “the deal with the library”? What deal are you referring to? Libraries does not make individual deals with publishers or authors. They buy the books at market price and pay authors some additional royalties determined by objective criteria according to an agreement with the organization for authors and illustrators. It is all public information. If you want to make an accusation of corruption you need to be more specific. reply Anon4Now 11 hours agoparentprevBusinesses that remain stagnant don't get shafted so much as outmaneuvered. Instead of investing and pivoting, they cling to the old business model. Short term, their decisions make financial sense, but long term, it's a death sentence. Meanwhile, Amazon is looked at as the behemoth in the industry, yet it probably isn't thought of as an online book store / publisher by most people. I think of so many things before I get to, \"Oh yeah, they also sell books.\" The one piece of information that I wish the article had mentioned is the age demographics of avid book readers. My gut tells me the market has dropped significantly in the last ~25 years, but I'd like to see the data. reply huijzer 2 hours agorootparent> The one piece of information that I wish the article had mentioned is the age demographics of avid book readers. My gut tells me the market has dropped significantly in the last ~25 years, but I'd like to see the data. I read a lot of non-fiction books and don't think much has changed over time. For non-fiction you always had groups of people, typically academics and successful business people and politicians, who read tons and tons of books. Winston Churchill and Dwight Eisenhower, for example, both read many books when they were young in around 1900 to 1930. How many people who worked in factories at the time would have read books? Not many I think. Today it's more or less the same. You have a few people like David Senra or Stephen Kotkin, who read 100-200 books per year, and you have the average person who reads maybe one. Just like book sales, it's a power law. reply leoedin 2 hours agorootparentI suspect I’d read a lot more non-fiction books if I couldn’t access the short form equivalent on my phone readily. But instead I read articles shared on HN or Reddit, with a sprinkling of non fiction books mixed in. Compared to my dad, who devoured non fiction books in the 80s and 90s, my lifetime spend is going to be far lower. reply bigthymer 8 hours agorootparentprevThe best survey data I've found for this information is linked here [1]. It breaks data down by age range for those who read at least one book in the prior year. It defines avid readers as having read 50 books/year but doesn't do an age breakdown for this subset. 1 - National Endowment for the Arts - \"U.S. Patterns of Arts Participation: A Full Report from the 2017 Survey of Public Participation in the Arts\" - , page 44 reply treflop 8 hours agorootparentprevI don't think a company that publishes books is going to be able to pivot to building an online book selling platform that easily. They just don't have anyone with the knowledge nor the capital to hire the people who then know who else to hire. Just like I don't see someone working in construction picking up programming or some software engineer picking up construction. It happens, but generally both are just not built the same. reply Ray20 6 hours agorootparent>They just don't have anyone with the knowledge nor the capital to hire the people who then know who else to hire. Doesn't seem to be true. They pay celebrities (for their boring bs books that no one read) enough to build a dozen online platforms reply mike_hearn 2 hours agorootparentprevWell, Amazon started out with almost no capital, so apparently they could have built an online book platform pretty easily. And the capital requirements of selling books online only fell over time as the tech got more widespread. Businesses like that are everywhere, but the root cause is always the same: they're run by people who do not like or have any interest in technology. And they hire people just like themselves. reply youngtaff 2 hours agorootparentIn the early days Amazon relied on huge amounts of trade credit from the publishers / distributors Seem to remember that if the publishers hadn’t been lax about Amazon’s delinquent debt the publishers could have forced Amazon into bankruptcy (Worked for a major US technical / educational publishing house in the late 90s / early 00s) reply GeoAtreides 3 hours agoparentprev> there saw a title and went \"this one is by Simon & Schuster, it has to be good\" Meet Fitzcarraldo, the publisher house with four Nobels so far: https://www.theguardian.com/books/2022/oct/10/four-nobels-an... reply NegativeLatency 11 hours agoparentprevIMO there are still quality publishers, but they are small and tend to focus on specific topics. Two that I'm aware of, although I'm sure there are more out there: - https://lostartpress.com - https://tinhouse.com reply vram22 11 hours agorootparentAnother one is Other India Press. https://en.m.wikipedia.org/wiki/Other_India_Press https://www.otherindiabookstore.com/about I got to know about them through coming across the book Tending the Earth, by Winin Pereira. https://www.banyantreebookstore.com/product-page/tending-the... I have some background in organic gardening, and think it is a very good book. It is about more than just organic gardening, though. reply atlasunshrugged 2 hours agorootparentI would add Stripe Press to this list. https://press.stripe.com/ reply tazu 8 hours agorootparentprevhttps://www.foliosociety.com/ is one of my favorites, the books are works of art. reply vidarh 1 hour agoparentprev> You print books? Well we have e-books now. The vast majority of sales is still paper books. > You do marketing? Authors now have their own followings on social media and can reach them directly. Some do. Most don't. Especially new authors. Publishers are playing a risky game of trying to figure out the tradeoff of how much to invest in developing an authors brand in the hope that future books will cost less to sell. Too much, and you have a cashflow problem today. Too little, and lose out on future returns. > You give cash advances? Fans on Kickstarter give 10x more. Most authors have no fans yet, and most never will - most authors write only a few books in their lifetime. From what I see, most authors (myself included) would rather not deal with most of what publishers do. If anything, most authors would prefer it if publishers did more of the sales effort, but it's not economically viable in most cases. reply nitwit005 7 hours agoparentprev> You print books? Well we have e-books now. It's often the same publishers publishing the e-books. They did successfully make that transition. > Instead they went all in on pulp bestsellers and celebrity memoirs at the expense of actual good authors If they're best sellers, as you note, that was clearly a good choice. They were selling what people actually wanted. reply fragmede 7 hours agorootparentAre they? Mark Dawson was caught buying 400 copies of his own book in 2020, but he's not the first or last author to have done that. reply paxys 7 hours agorootparentprevYes people are buying John Grisham and JK Rowling and the latest celebrity memoir, but eventually (1) interest will run out and people will want the next new thing and (2) these big brands will realize that they don't need the publishers to market for them (and this is already happening, as the article says). What then? reply nitwit005 3 hours agorootparentRegardless of them struggling in the future, publishing books that sell is clearly superior to publishing one's that don't. reply greenie_beans 4 hours agoparentprev> Imagine a world where you wanted something good to read, and among all the garbage out there saw a title and went \"this one is by Simon & Schuster, it has to be good\". this still exists, but the presses are harder to find and mostly independent. FSG books are always good. also, books these indie presses are always good: - tyrant (rip) - graywolf - new directions - NYRB and some more, but idk your taste so not gonna recommend them. reply makeitdouble 10 hours agoparentprevWhen you say \"publishing industry\" you're really saying \"paper book industry\", right ? Nothing stops a publisher from being an ebook publisher, and that's a tried and true successful model adopted all around the world. You'd have made the same argument for vinyl record publishers saying they're cornered into a niche, but no, as you point put artists will still want editorial power and support, so digital music is thrieving. reply kasey_junk 10 hours agorootparent> Nothing stops a publisher from being an ebook publisher Except that the biggest platform for ebooks (by a big stretch, they own ~70% of the market) is a company that you have a direct adversarial relationship with in most other parts of your business. reply makeitdouble 9 hours agorootparentBusiness is business. If a publisher is refraining from going into ebooks altogether because of Amazon, when they get the option to direct sell and also access the other ebook platforms as well, I feel they're not long for this world either way. reply kasey_junk 7 hours agorootparentIm not suggesting they are refraining from going into ebooks due to Amazon I’m suggesting it’s not a tried and true business model the world over. It’s mostly not a going business model outside of the profits Amazon brings in from it. reply makeitdouble 6 hours agorootparentIt's difficult to prove a negative. Ebooks are a growing market in the US, the EU and SEA (though TBH I have no idea about China amd India) It's been more than a decade that it's mainstream and takes about 10~20% of book sales depending on the country. Amwzon's profits are still split with the publishers and publishers have their own venues + competitors (in particular outside of the US) so I'm not sure why Amazon's presence is a blow against the industry as a whole. What more would be needed to see it as a validated business area? reply tannhaeuser 3 hours agoparentprevRather than feeling for the publishing industry, you should ask yourself what has gone wrong with the web as a medium for self-publishing. reply elorant 3 hours agoparentprevPenguin is that publisher for me. Pretty much everything I've read from them is rock solid. reply DrBazza 35 minutes agoparentprevIn the UK many years ago, there was the Net Book Agreement, which was price fixing and Amazon, BooksOnline/StreetsOnline comprehensively broke that. Unfortunately, book shops didn't really learn, and we are where we are. Back to the title. I still buy books. I've stopped buying them from Amazon, and just buy them from my LBS. Though one thing still makes me irrationally angry - \"SciFi + Fantasy\" as \"a genre\" in bookshops. No. It. Isn't. It's two separate. Two loved-up vampires, isn't sci-fi. At least on Amazon I can filter that. Browse on Amazon, buy in the LBS. Which is the reverse of what I did a decade ago. https://en.wikipedia.org/wiki/Net_Book_Agreement reply joe5150 10 hours agoparentprevI can't think of any publishers that are also in the business of printing and distribution. Those are entirely separate industries. There is almost no way for publishers to lose money because of tech that makes it cheaper and easier than ever to get books to consumers while retaining unprecedented control over pricing (fixed, never discounted or remaindered), resale (prohibited), and lending (also prohibited). Publishers sell eBooks for the same price as print books but pay almost nothing to produce them. That's a big bump in profits. I don't see how any publisher is losing money on their back catalog due to scanning by Google, either. Google doesn't sell or even offer access to the full text of most books they scan, certainly not any that are under copyright and still being sold by publishers. Out of print books are by definition not earning money for publishers, so it wouldn't make any difference there. reply kaedroho 2 hours agorootparent> I can't think of any publishers that are also in the business of printing and distribution You're right that publishers don't handle printing, but they do need to handle distribution. Amazon won't help if you want to get books on shelves of retailers, and printers don't distribute. Smaller publishers will usually use the distribution network of a larger publisher. For example, HarperCollins and PRH both handle distribution in the UK for themselves and others. reply TheCoelacanth 10 hours agorootparentprevIf you think that publishers spend almost nothing on producing ebooks, then you clearly understand nothing about the publishing industry. reply joe5150 9 hours agorootparentNotwithstanding what I understand about the publishing industry, I'm talking about the marginal cost of distributing a .mobi or whatever versus manufacturing a printed book, not the entire cost of publishing an eBook. reply TheCoelacanth 7 hours agorootparentThat's only a dollar or two of difference per unit in costs to the publisher. The majority of their costs are exactly the same for ebooks and printed books. reply joe5150 6 hours agorootparentSure, I never said they weren't. reply Aeolun 4 hours agoparentprevI don’t think it would have been possible for publishers to deal with the absolutely massive amount of books they’d have to vet to do what you’re saying. That works if the absolute number of books written is very small, so everyone interested in books pulls from that pool. The publishers could try to find a bunch of good books and get absolutely nothing. reply yunohn 2 hours agorootparentCuration is a multi-stage filtering pipeline. Such brands do not look at a firehose of content - they rely on existing networks and markers of quality before even considering content. reply marcus_holmes 1 hour agoparentprevSorry, but no. Authors have been complaining about publishers for ever. Publishers treated authors like crap when they controlled the market. No sympathy for them at all. If they had any integrity they'd move to be the Spotify of books and serve their customers better. Instead (as the article says) they're spending their efforts trying to bail the sinking boat and resist any such move. The world will be a better place without them. reply mcphage 11 hours agoparentprev> You distribute books all over the country? Amazon can do it better and cheaper. That just makes it Easter for publishers. The major problem with Amazon is monopsony, not easier distribution. > You print books? Well we have e-books now. Which have to be purchased ultimately from the publishers, so again, they’re still there making money, they just don’t need to bother printing books anymore. You’ll note they’re not any cheaper. > You do marketing? Authors now have their own followings on social media and can reach them directly. I think this is confusing cause and effect. Authors turned to social media because publishers weren’t doing a great job marketing, not because it makes publishers unnecessary. reply zeroonetwothree 9 hours agoparentprevAren’t print books still the vast majority of sales? reply paxys 7 hours agorootparentEbooks are touching or have crossed 50% of sales in certain combinations of markets and genres. Overall they make up 30-50% of all book sales. So no, print books aren't still the vast majority. reply kaedroho 2 hours agorootparentAlso worth mentioning that there are some genres that will never make sense as ebooks. When was the last time someone bought you an ebook as a gift? This is is the main market for cookbooks, for example. reply nottorp 3 hours agorootparentprevI remember reading on Charles Stross' blog that the paperback is kind of dead in the US. At least for his niche. I think his last book only went out in hard cover. reply vram22 11 hours agoparentprevI saw the second part of your comment coming, while reading the first part - in my mind's eye :) Uncanny valley, or maybe I just read the signals and interpreted them right. reply huytersd 7 hours agoparentprevThere are very few people go this is by Simon and Schuster etc. Most Americans don’t read and the ones that do read a bunch of junk. reply throwaway2037 3 hours agorootparent> Most Americans I would suggest an edit: Most humans reply Turing_Machine 11 hours agoparentprev> Can't help but feel for the publishing industry considering how shafted they got by tech. Publishers have been shafting authors for centuries. I'll shed no tears on their behalf. reply Spivak 11 hours agorootparentPrior to the digital age where it's possible for the author to self-publish the publisher was handling 99% of the actual business of selling books and giving authors 15%. So while I think it's been hard to make a living as an author I don't think it's necessarily the publisher's fault. Better to be the one's selling shovels than the ones mining for gold. reply thfuran 11 hours agorootparent>the publisher was handling 99% of the actual business of selling books It seems to me that you're rather underestimating the importance of having something to put in the book. reply vidarh 2 hours agorootparentHaving written two novels, I will tell you it's far less work for a lot of writers to write the thing than getting decent sales. It varies - some do slave over their manuscripts for years, while others churn them out in a week, but selling books is hard work. reply AtlasBarfed 10 hours agoparentprevCopyright law is ridiculous and abusive (thank you supreme court). There is some role for domain knowledge, curation, and editing, but the article shows that's not what they make bucjs on: celebrity drivel, bibles, sat prep, and the copyright monopoly. It's just a cartel, just like the music labels are. reply tryptophan 8 hours agorootparentMan can we just kill these industries? Copyright law is so annoying and its supposed to promote quality...which we get none of. Free youtube tutorials are the best learning resource these days. The GDP hit would be insignificant too, likeI am kind of the opposite; I have found it much easier to read on Kindle simply because I can make the text as big as I want. This and the fact that the kindle is _exactly_ where I left it, no need for physical bookmarks. reply kaashif 4 hours agorootparentprev> I realized that the only really different variable is the font size. Didn't you say you're 13 years older than last time? reply iamacyborg 12 hours agoparentprevPaper books are also just orders of magnitude easier to flip through and re-read specific chapters, paragraphs or sections. I can skim through a shelf full of books in a way that I’m just not able to with ebooks, even with stuff like full text search available in Calibre. reply SubmarineClub 11 hours agorootparentAnother benefit to paper books, in my experience, is it's a lot easier to remember the rough location of a particular passage (towards the front, middle, near the end, etc.) than with digital A progress bar really doesn't replace the context of the stack of pages behind and ahead of your current page. reply dhosek 8 hours agorootparentMy own personal anecdata on this comes from being able to find a passage I read in a 500-page book in 1992 for a class I was taking in 1998. I could probably even find that same passage now if I were to walk across the room to the bookcase where I have that book. Although on the other hand, I was also able to turn up an article I’d read online a couple years later when it was relevant to a friend’s relationship with her newly out trans kid, but it was definitely a different sort of recall and lookup happening in the latter case. reply maximilianthe1 9 hours agorootparentprevWhile physical \"progress bar\" is much easier to remeber than digital one (on the side of the screen for pdfs), because you actually interact with it the whole time, i don't find (ha) finding a particular passage hard without it. I personally usually can remeber a phrase or just a word and search for them in a PDF to find the needed passage. reply UncleOxidant 10 hours agorootparentprevIt's pretty easy to highlight passages with most e-reader software. Some even let you write a note that goes with the highlight. You can then look through a list of all highlighted passages. reply Tor3 6 hours agorootparentIt's when something comes up and you remember that you read something relevant in the past. In most of those cases it's not about \"Ah, this is something I may want to check again at a later time, so let's highlight it\". So, there's no list of highlighted passages to search through. Indeed this is one of those things I miss about paper books. When the above happens I have a terrible time finding that on my Kindle. Most of the time I'm not able to, while with a paper book I didn't have that problem, mostly. reply skydhash 11 hours agorootparentprevI classify skimming as a \"distracting\" activity. I always ask myself: What information do I need? And then it's very easy to get to the relevant passage. And outlines are there for a reason (PDF Expert can edit them) so navigating between the same file is not that cumbersome for me. I do agree that the experience is more pleasant with a paper book, but in a focused session, the result is pretty much the same. Skimming is great for building a mental map, but that is a separate activity than reading to learn. And it can be done digitally too, just differently. reply Ferret7446 9 hours agorootparentprevThat's a software choice issue. You can definitely flip through digital texts more easily than physical. With a decent monitor, you can flip through multiple books at the same time which is quite difficult to do with physical books, assuming you haven't summoned eldritch appendages to help. reply wolverine876 10 hours agorootparentprev> Paper books are also just orders of magnitude easier to flip through and re-read specific chapters, paragraphs or sections. Whatever works for you, of course. PDFs have bookmarks, which instantly take me to a specific spot on a specific page. Also, I can open multiple instances of the PDF, viewing multiple pages of the same book (or some PDF readers allow split screens to do the same thing). Edit: And if I don't recall where something is, I can search for it. reply Tor3 6 hours agorootparentI prefer PDFs for informative material (various documentation), though they're less useful if it's just a photocopied old manual (no OCR). It's when reading books (literature, not documentation) that the e-readers have something going for them. I read nearly all my books on the Kindle, but I never use it for documentation - I've tried, that's just cumbersome. PDFs on a PC are fine. reply wenc 6 hours agoparentprevI read paper books because I can flip back and forth without friction, and I can underline and scribble. I don't know why eReader interfaces make these two functions -- critical for deep reading -- so difficult. Kindle is optimized for linear reading only. So beach reading and long romances. But if you do the kind of reading that Mortimer Adler's \"How to Read a Book\" proposes (skimming, inspectional reading, writing notes), you pretty much can't with Kindles. reply kstrauser 6 hours agorootparentI truly don’t mean to be pedantic, but there’s a giant implicit “for me” in there. I do not want or need to scribble or write in my books for “deep reading”. It’s totally fine if you want to, but that’s your preference and not a general requirement everyone has. I just finished “Moby Dick” on a Kobo. It’s no “Ulysses”, thank heavens, but I’ve never heard anyone describe it as a long romance or a beach read. reply steve1977 4 hours agorootparentprevThis is interesting, I'm pretty much the opposite. I only read paper books if there are no electronic versions available. Fiction I read on a Kindle. My deep reading all happens on a PC actually. Not even a tablet. I find it much easier to highlight and annotate reading material with mouse and keyboard and look up references with a proper multi-window screen. Also, while you certainly can underline and write notes on paper, this information then stays on paper and in this book. No way to index it easily, no way to cross-reference etc. reply chrismcb 5 hours agorootparentprevWhile you can scribble, you can easily highlight and comment on books. Going back and forth is maybe slightly harder than a book, but it isn't \"so difficult.\" I'm not saying reading on a Kindle is better than a paper book. But I didn't think it is as bad s you claim it is. I personally like it because I can read anywhere. I don't need to carry a large heavy book around. reply nottorp 3 hours agorootparentprevLooks like you don't read books for entertainment? I believe the original article wasn't about technical/self improvement etc books. reply astura 5 hours agorootparentprevWTF is \"deep reading?\" reply vunderba 4 hours agorootparentIt's likely some arbitrary jargon that someone made up. I have my own method of reading, which since we're throwing out pretentious flowery prose, I'm going to call \"predictive perusal\". For each chapter or titled section, I read the first sentence or title and then mentally position myself as the writer of that section and attempt to predict the contents. For convergence I attempt to traverse further plies. For divergence, I attempt to figure out the root of the difference. EDIT: For the record, I have never felt the need to scribble in the margins of a book. It's far better to take notes on separate media to prevent \"preloading\" your existing thoughts at the time should you choose to re-read the book years later. reply WolfeReader 4 hours agorootparentprevThis link will help you understand: https://lmddgtfy.net/?q=deep%20reading You can take a similar approach for other words and phrases you don't know, too! reply echelon_musk 12 hours agoparentprevThe older I get the less I trust myself to know how to administer or even to care how to work any current or future reading e-device. A printed book requires light as just about it's only dependency! I definitely agree with you about the distractions of digital devices. Switching to a book is a focused mode. reply criddell 11 hours agorootparentThe older I get, the more I appreciate being able to select a larger font with my Kindle. I love books, but the accessibility features of a modern ereader are pretty great too. reply echelon_musk 11 hours agorootparentI've been putting off glasses for as long as I can. I suspect I'll return to my Kindle again in the future for this reason. It's good to be reminded that both things can coexist. reply Tor3 6 hours agorootparentprevThe ability to select a font size I like is one of my main three reasons for using a Kindle. reply ethbr1 12 hours agorootparentprevWhen I'm on vacation, I always read paper books. Because I want minimal complexity. (Usually in the tropics, often with rum and a hammock) Dead tree books' short term failure modes are water and fire, and once failure is confined to emergencies-that-are-already-emergencies they impose no additional cognitive burden. Also, I suspect the physical friction involved in \"swiping away from\" paper books helps reset my dopamine baseline / memory, but that's just suspicion from having lived pre-smartphone. PS: Also, re: environmental impact. eBay and Amazon sell tons of used books. Sure you're shipping them around, but I really buy new these days. reply AlotOfReading 10 hours agorootparentYou forgot the third and fourth failure modes: weight and bag space. Books take a lot of it. An e-reader is thinner than any book and weighs the same no matter how many it's holding. I used to take my e-reader on field expeditions when I was an archaeologist. Never had one die or break even after months in places like Siberia. The number of notebooks that were ruined during those same expeditions is non-zero, usually from condensation or the physical trauma of a backpack. reply TheCapeGreek 4 hours agorootparentConsidering you're on HN, it's likely a safe bet to say you also travel with a laptop. In which case, an e-reader is still a waste of space because your laptop can just open ebooks too. Or if you don't, then your phone is still a better space saver for reading books with. Yet, you will likely prefer to take your e-reader with anyway (and its charger if it isn't the same as other devices). This is why I find \"saving space during travels\" a very relative argument to make, because it's clearly down to preference. reply BoringTimesGang 35 minutes agorootparentMy laptop and phone can't last through a long-haul flight. My e-reader doesn't need charging until I come home, fits in a jeans pocket and is a similar weight to my passport. reply TheCapeGreek 35 minutes agorootparentThat's kind of my point - your preferences are to the advantage of the e-reader. For others, the same might apply to physical books instead, despite the disadvantage of bulk. reply theshrike79 36 minutes agorootparentprevThe ergonomics on any laptop is dogshit compared to a dedicated e-ink reader. Just the amount of brightness the display beams to your face makes it suboptimal for reading in low light. And the \"charger\" I need with my ancient Kindle is a standard micro-USB cable, not exactly exotic. And as soon as this one breaks down, the next one will be USB-C like a good 90%+ of my devices I travel with. reply dripton 11 hours agorootparentprevI'm just the opposite. One of the worst things about vacations was having to carry along all those books or take a detour to a bookstore (if there were any) to buy more. Now I just carry a single small e-reader. It's a great savings in terms of cargo space. I don't consider an e-ink reader very complex. You charge it once in a while, download books to it once in a while, and otherwise it's basically a book except it remembers what page you were on. reply hollandheese 10 hours agorootparentprevE-readers are often very simple devices. Their main audience is older people. Light being a dependency of reading a printed book is one of the biggest plus for e-ink readers. You don't need light, they already come with it! It's so much easier reading with a Kindle (in dark mode) at bed time than it is a paper book. reply BoringTimesGang 33 minutes agorootparentAnd when you read lying on your side, no awkward holding for the nearside pages! reply Tor3 6 hours agorootparentprevWell, yes and no.. I do use the backlight on my Kindle if it's dark, but this changes how it looks. I have a Paperwhite Kindle. With the backlight on it's more like reading on LCD, though not as bad. The Kindle looks much nicer if I can use a bedside lamp instead of the backlight, just as I would with a paper book. reply nottorp 3 hours agorootparentYou still can't turn off the light on a Kindle completely can you? Always felt like a waste of battery on a beach... reply Tor3 1 hour agorootparentThere's a tiny amount of light, yes. I don't know why Amazon made it that way. It's not really visible.. not to me, at least. But it's probably one reason the battery is slowly drained, over weeks, even if not in use. reply nottorp 56 minutes agorootparentIt does turn off when you're not using the kindle. But you can't turn it completely off while reading. I suppose the use case is picking up the kindle at night in a dark room while you disabled the light when you were reading outside during the day. reply duderific 10 hours agoparentprevI have a slightly different problem. What I miss about the physical book is its context - that is to say, the cover art, its particular weight, the font face, how the chapter numbers are printed, etc., which give a book its certain \"feel.\" With an e-book, you don't get any of that, so all books have a kind of sameness. reply beautron 4 hours agorootparentI feel this strongly. Physical books make a deeper impression in my memory. There are all sorts of particularities of how the book was bound: it's size, it's smell, the texture of the paper, etc. It's like the book exists in its own unique space (vs all digital books existing in the single shared space of my computer/reader). I think a book's reading environment loses some richness when all these subtle little details are homogenized. reply hollandheese 10 hours agorootparentprevWell you can get the cover art, but yeah. I mean you can get the differing font faces and etc. if the publisher built that into the file, but most don't go to that amount of effort on their e-books. reply skydhash 11 hours agoparentprevI use my iPad and my Kobo for digital reading. I have paper version of \"important\" books (stuff I'd like to read if I don't have power), but some are unwieldy (Algorithms by Cormen et Al). I prefer e-ink for reading, but it's too slow for my learning workflow. I highlight and mark interesting stuff, that I export later to condense and reflect upon. And that is cumbersome on an e-reader. There's also the matter of size. I generally like the PDF version of technical books as they're typeset well, but my e-reader is too small for them. So I use my iPad for those (Distraction is handled by the fact that I read those in short focused sessions). But for fiction books, the e-reader is perfect. I don't highlight text in those books, I just read. Any other operation is slow enough that I just can't fiddle with it. And it's perfect for long sessions of reading as it does not project light in my eyes. It's light, so I just bring it with me everywhere. reply alexpetralia 11 hours agorootparentI recommend the Fujitsu Quaderno. Great for marking up textbooks on A4 size pages, and far lighter than a book itself. reply wolverine876 10 hours agorootparentWill you be able to read those textbooks and markups in 10 or 20 years, if Fujitsu stops making Quadernos? reply Koshkin 11 hours agorootparentprevAn Apple-esque expensive stuff. (A pen sheath, $32.) reply antod 5 hours agoparentprev> Yes, it's not particularly ecological If it makes you feel better, just think of your bookshelf as a carbon sequestration facility. reply dotnet00 5 hours agoparentprevI haven't had much trouble focusing on reading on computers, my only expectation is for the device to be touch screen and a pressure sensitive pen. The touch screen makes moving through pages trivial (for casual reading on say, my kindle, I have the habit of laying back and just tapping with my nose to flip pages lol) and the pen makes it trivial to mark up more serious reading material. With paper books I tend to not be near them right when I need them, and tend to be too wary of just marking them up. With digital stuff, and all the syncing we have between devices these days, I pretty much always have multiple devices at hand where the material is accessible. I do still prefer physical books for things like art, I find it easier to study it and understand how it's put together that way (maybe because a physical copy forces me to look at the piece as a whole, rather than getting distracted by and overfocusing on minute details that turn out to not matter until I grasp the whole structure). However, even there, once I do understand how to look at the art, I can go with the digital version just fine. reply endgame 10 hours agoparentprevOn top of all of that, nobody's going to revise the book out from under me while I'm reading it, or prevent me from loaning it to a friend, or even yanking it entirely from my device. reply BoringTimesGang 24 minutes agorootparentReading ebooks doesn't mean submitting to Amazon's ecosystem. reply Tor3 6 hours agorootparentprevAs far as I know (in the sense that I've never experienced anything like it in a decade of using a Kindle) that the book's getting revised from under me while I'm reading it. There's an occasional update for some books, but I have to actively choose to download the updated version, otherwise my original stays the same. I think I've only downloaded an updated version a single time. reply nate 10 hours agoparentprevSame. I recently got one of these wirecutter book lights: https://www.nytimes.com/wirecutter/reviews/glocusent-reading... Does an amber color for bedtime reading, and now I' tearing through books again. It's so much easier to skim, skip, re-find something (yes, even without fancy search) and I just find myself reading faster with real books again. It's rekindled :) a love of books again I feel like I lost somehow only doing it on e-readers. reply hintymad 10 hours agoparentprevSame here. I'm not sure if it's psychological. When reading my Kindle, I have to constantly fight my urge to do something else, maybe except for reading a page-turner novel. reply jprete 8 hours agorootparentThat's exactly why I get paper books. An e-reader is too much like a smartphone and I start circling the UI looking for distractions. reply 127 3 hours agoparentprevPaper is a carbon sink. As long as you don't burn them, the carbon will stay in the book. reply acchow 12 hours agoparentprevThese sound like common traits for ADHDers. reply zymhan 5 hours agoparentprevWho cares about the ecological impact of buying a book? You have missed the forest for the trees. reply datascienced 5 hours agoparentprevYes plus the emotion of sitting on a couch with a paper book vs. a device is very different. The feeling of flicking pages is better than the UI of book readers or smart phones. reply maxerickson 7 hours agoparentprevPaper is probably not the place you are going to be able to make your most significant marginal impact. reply bhhaskin 4 hours agoparentprevHonestly it is likely more ecological than ebooks after you factor in everything. Think about how many e-readers are sitting in the bottom of a landfill somewhere. Screens, batteries, plastic shells. The energy to not only power the device but all of the infrastructure for transmitting and storing the book. reply m463 10 hours agoparentprevI found that I change the font size/lighting depending on time of day, which is harder to do with a paper book :) reply jszymborski 6 hours agoparentprevI've been buying used books from local booksellers and betterworldbooks.com when I can't find them locally. Seems like a fair compromise to me re: ecological impact. reply zem 7 hours agoparentprevexact opposite for me - the more i got into ebooks, the harder i found it to handle the ergonomics of paper books, and today my reading is pretty much 100% on my phone or kindle. reply petre 6 hours agoparentprev> Yes, it's not particularly ecological As if food packaging is particulary ecological, but we continue to order food, eat takeaway and processed food reliant on packaging. I also buy paper books. Cheap paperback ones. Books can easily be recycled as opposed to food packaging which mostly ends up in landfills or litters the environment. reply jhatemyjob 7 hours agoparentprevThis was my opinion until I got the Kindle Scribe, turns out the Kindle was too damn small reply hoseja 1 hour agoparentprevWhat goes on in your head that you even consider the \"ecologicity\" of a book. It has no impact. Nada. Zilch. Zero. Stop feeling guilty about meaningless things. reply globular-toast 2 hours agoparentprevI'm the opposite. I stopped reading books because I was accumulating too many and it was difficult to find the ones I like without paying full price for new copies (which I'd eventually just discard). Also I find them cumbersome. Now I have a Kobo I read a lot. I can read on my side in bed. It's great. However, paper is still the best format by far for text books and references. I can't stand having to use a screen for that. If I'm using a reference sheet I always print it out. reply Brajeshwar 7 hours agoprevI used to buy and read a lot of physical books. Around 2013-204, I started moving to eBooks, and Amazon Kindle played a significant role in my journey. But a couple of years later, I had an epiphany while discussing books with my daughter, “I never see you read books. You are just reading on your computer (kindle) or the phone and are not reading.” Since then, I have returned to reading physical books with an approximate ratio of 1:5 against digital ones. I have kinda rule/understanding with my daughters -- you have unlimited access to books that you can read, and you also get extra pocket money for reading them. Of course, it is OK not to finish a book, but in that case, a few cool-down days before buying another one. Yes, buy and read physical books. I believe, the return is a magnitude higher for everyone than the loss it does to anything. If you have kids, read more physical books in front of them. Encourage them to read physical books. They are less distracting and help the kids focus. Five years ago, I wrote a story about it. https://story.oinam.com/2018/why-physical-books-matter/ reply Aeolun 4 hours agoparent> If you have kids, read more physical books in front of them. I think this part is so important. I want to show my kids that reading is fun, but instead what they see is me looking at my phone all the time. There’s nothing differentiating reading on my phone from watching youtube videos from their perspective. Ereaders help to some extend, but only a bit. reply kstrauser 5 hours agoparentprev> They are less distracting and help the kids focus. That’s such an incredibly personal preference though. It’s far easier for me to read with an ereader when I don’t have to have 2 hands holding a heavier hunk of paper wants to close itself if I let go for a moment. I have a huge book collection but I’ve switched almost entirely to ebooks because I find the ergonomics so much better. Physically fighting with dead tree media is more distracting than my ereader. reply The_Colonel 2 hours agorootparentIt happened to me multiple times that I bought a real paper book, but the ergonomics was so bad, I've pirated the book and read it on my kindle instead (since I bought it I feel entitled to get the e-variant). Especially paperbacks are bad - tiny letters, yellow low-contrast paper, difficult to hold open. Reading good night stories to children is even more challenging, since it's usually dark. Bringing some extra light is annoying and distracting for the child. reply twelvedogs 1 hour agoparentprevi used to read a lot but buying paper books was a real pain for me, i read hundreds of books on the kindle but figured out i could just slap a video on my phone and actually sleep. since then i've read maybe 15 books in the last couple years and my kids never see me reading, however we've read to them and had them read to us for their whole life and they now both read independently for fun. i don't think leading by example is anywhere near as important as reading with your kids reply cm2012 6 hours agoprev319 comments and I don't see the simple fact that the article title is dead wrong. We are going through a massive book boom right now: https://pbs.twimg.com/media/GLSGuGOWoAArd-K.jpg:large. Book sales are the highest they've been in 20 years lol. So all the comments mourning books can relax. reply shkkmo 6 hours agoparentThe title is provocative clickbait, but the actual point is this: > These two market categories (celebrity books and repeat bestsellers from the backlist) make up the entirety of the publishing industry and even fund their vanity project: publishing all the rest of the books we think about when we think about book publishing (which make no money at all and typically sell less than 1,000 copies). Given that spending per capita, adjusted for intlation is only a couple of percent higher than they were 20 years ago, the \"boom\" isn't particularly impressive. reply outop 3 hours agorootparentBut there are more books published than ever before. Most of the books that sell hundreds of copies wouldn't have been published in the past. If you write a niche book about something and a few hundred people read it, that seems great. If you're (stochastically) subsidized by a few huge hits and bestsellers, that also seems great. People are reading the Bible, Harry Potter and celebrity memoirs? Great too, I'm sure they are enjoying them. Possibly many of those people have not chosen those books in preference to other more \"worthy\" books but in preference to other pastimes. The only problem I can see is if you wrote a book and had it published hoping it would be this year's runaway success, and it wasn't. Fine, but someone's book was. By its nature not everyone can achieve this. reply ar_lan 4 hours agorootparentprev> The title is provocative clickbait, but the actual point is this. It's an aside, but this is just so frustrating we have succumbed to this level. In principle I don't even want to read the article because the title is ridiculous, but on the other hand, they wouldn't generate any traffic if they don't because our brains are too fixated on \"drama\" and \"controversy\" that clickbait titles generate. reply CaptainFever 4 hours agorootparentWe need DeArrow but for the Internet. reply nottorp 3 hours agorootparentprevNobody cries about the poor movie industry only making money from superhero movies, do they? reply theshrike79 31 minutes agoprevFor \"linear fiction\"[0] I use an e-ink reader exclusively. I also re-read books pretty much never. There are way too many excellent books I haven't read yet, so why would I spend time redoing something I have already experienced? I can easily hi-light stuff on the Kindle and check them out from Goodreads if I need to later. For books I read for work and other knowledge stuff it's either PDF on an iPad or a web page. In some rare cases a physical book, but stuff tends to move so fast paper reference books expire too fast to have any advantage compared to asking a large language model. If I get the urge to make any notes from this category, they go into Obsidian. [0] Books that are read from beginning to end with no need to jump around to check stuff you read before. Could be a biography too, which isn't fiction per se. reply bombcar 12 hours agoprevThe entire book industry rides on the backs of bibles, hobbits, and extremely ravenous caterpillars. Seems almost poetic, somehow. reply vundercind 12 hours agoparentI gotta admit: I rarely make time for any recent fiction. Too busy catching up on the last 5,000 years. I don’t expect that to change before I die. Film has a similar problem—there are plausibly low-thousands existing films worth my time, a whole lot of them 30+ years old. I could never watch a film made after 2000 and not run out of good entertainment in my lifetime. They’re damn lucky the Mouse got copyright extended to a century or more. reply bombcar 11 hours agorootparentIt’s also a highly effective sorting algorithm - anything still talked about ten years after it came out is probably worth some time. I will say that it seems entirely possible to relatively quickly see all US animated kids movies … reply satvikpendem 11 hours agorootparentThe Lindy Effect: https://en.wikipedia.org/wiki/Lindy_effect reply vundercind 5 hours agorootparentprev> I will say that it seems entirely possible to relatively quickly see all US animated kids movies Decent or better ones… maybe. There’s a deep bench of poor-to-terrible animated kids’ movies that were straight to VHS/DVD/streaming. For god’s sake, there are like nine Land Before Time movies alone. reply pests 8 hours agorootparentprevThere are gems coming out today that will be considered great works by our descendants. By avoiding current authors or media you miss out on the cultural story and critique/commentary that influences all our media. Sure, the last 5000 years are interesting, but isn't right now pretty amazing too? You might accidently read the best book ever written but you won't know until history plays out. But I think it's important to see how recent history has shaped the narrative of the worlds authors in today's world. Weve lived through a lot of major events. Everyone does. But these are our events. reply vundercind 7 hours agorootparentI’m not avoiding them, but “rando probably-just-ok new thing” versus any of hundreds of major classics I haven’t made it to yet… it’s tough to pick the new thing very often. [edit] or, hell, not even major. Odds that a recommended-by-people-online got-some-hype-in-review-rags new book is gonna make me happier to have read it than, say, a 2nd-tier Maugham or Faulkner or something? Not great. reply greenie_beans 11 hours agorootparentprevread \"the sarah book\" or \"crapalachia\" by scott mcclananhan if you want to read fiction from a living author that should be included in your reading list for the past 5k years of language arts. reply matwood 11 hours agorootparentprevAnd if you add in TV shows, there's more great content out there than I'll ever have time to watch. reply vundercind 10 hours agorootparentVideo games, too. I live through the 90s yet my backlog includes thousands of hours of reputedly-amazing games from that decade that I didn’t get to at the time. reply bombcar 9 hours agorootparentTo be fair the awesomeness of many blockbuster games could be condensed way down with little harm to the game (e.g., Final Fantasy without all the random battles). reply ZhadruOmjar 6 hours agoparentprevI'm glad my recent bible purchase can fund everyone else's hobby books. reply fmajid 9 hours agoparentprevMaybe Gutenberg was right on his market positioning after all. reply ginko 2 hours agoparentprev> The entire book industry rides on the backs of bibles, hobbits, and extremely ravenous caterpillars. Have to say I'm kind of surprised about the bibles part. Aren't those usually printed by smaller church-associated publishers? reply jackstraw14 12 hours agoparentprevhow many years of print dominance? welcome our overlords. reply joshuamcginnis 12 hours agoprevI'm currently writing a book about independent thinking and I have zero expectations that it will sell even a single copy. However, the net gain I will get from having completed the goal of writing the book is invaluable to me personally. reply bjelkeman-again 12 hours agoparentI had this idea for a hard science fiction story that I have been thinking about for years. Nobody else is going to write it, so I started. 20 000 words in my beta readers say it is good, so I think I will continue. And I don’t expect to sell it, but I want to know what happens and so do the beta readers. So on we go. reply vidarh 1 hour agorootparentKeep at it. I've self-published my first two (and beaten the odds - somewhat - judging by this article, while still paying substantially below minimum wage for my time, but that's ok), and the experience is \"interesting\". The first time I had someone somewhat accusingly message me on twitter to ask when I'll get the next one out (I'm way overdue) was exciting. reply mysteria 11 hours agorootparentprevYou really have to write it for yourself if you want to keep going - unless you're lucky or have a large following already it's hard to get a lot of readers. I've written stories and serials and posted them on social media, my personal site, etc. (I release them as Creative Commons works) and few people read them. Self-published authors who want to make a living typically have to do a lot of promotion to get a chance. Also there are times when I decide I want to read my own stuff for entertainment instead of a published novel. And the joy I get from that keeps me writing. reply wolverine876 10 hours agorootparentMany successful artists in any medium say they create things for themselves; it's more a compulsion than a business activity. Some are lucky that others like it; some are even luckier that others like it during the creator's lifetime; the luckiest even make money from it! reply mysteria 8 hours agorootparentIf you want to make money you're often creating for your audience rather than for yourself too. For instance I know professional artists who don't enjoy their work that much since they end up having to please the client rather than make what they really want. Even for my own CC fiction my themes and language are affected by current trends and what I think my readers will like. Of course if you're lucky you can create whatever you like and your fans will eat it up, but how many people can do that? As an aside though I feel that the science/engineering heavy HN crowd would bring some life to science fiction. We have the ability to write things that make technical sense and I'm sure many of you internally roast the handwaviness seen in some franchises. I don't know if I'm the only one that does this but I plan out the computer networks my characters use, draw cutaways and chip architecture diagrams for new devices I come up with, read scientific papers for research, and so forth. Sometimes that's even more fun than the actual writing! reply jimbokun 7 hours agorootparentCixin Liu was a computer engineer working at a power plant before writing 3 Body Problem and its sequels. He very clearly embeds his love for science into his books. reply Aeolun 4 hours agorootparentprevI think the best motivation I’ve ever gotten for writing was finishing another story and being sad about it. The best part about writing the story yourself is that it never ends unless you want it to (or you end, I suppose). reply bhaney 11 hours agorootparentprevCan I be a beta reader? Always looking for more good hard sci-fi. reply mbbbackus 12 hours agorootparentprevCurrently in the same boat! When you say beta readers, do you mean other friends/writers or like people on twitter reply greenie_beans 11 hours agorootparentprevhell yeah, keep on writing! reply datascienced 4 hours agoparentprevOr in other words you are doing research, a bit like a PhD and the book is a byproduct! Like how HN was recently talking about 8 hours to code the first time, but half hour to retype the same code from memory the next day. The code is almost an small (but critical) artefact. reply gizmo 12 hours agoparentprevI have read a couple of books on that subject and they all disappointed. I will gladly buy your book if it's original work. reply joshuamcginnis 11 hours agorootparentThat's encouraging. It's definitely not fluff and original. The feedback would be valuable even if you don't end up liking it. reply IlliOnato 10 hours agorootparentprevSame here reply deadbabe 6 hours agoparentprevI love the idea of owning books you can’t find anywhere else. One of the main reasons I don’t bother going to bookstores is because I know everything there is just gonna be on Amazon anyway, and with reviews. But the idea of coming across a strange book few people have read, and finding unconventional wisdom and writing inside that no publisher would ever put out, sounds exciting! I think more people should write books and publish them in small quantities and just give them away for others to discover some day. Let the books circulate through the world, growing old and more mysterious with time. It’d be better than keeping some static blog site that will disappear after a decade. reply Aeolun 4 hours agorootparentWhat I want is some easy way to print any website as a book. Some web serials would be best read as a physical book, but it’s often hard or impossible. reply keiferski 5 hours agorootparentprevPretty much all of the physical books I own are like this: rare and impossible to find online, in digital format or otherwise. People really overestimate how many books have been digitized. reply echelon_musk 12 hours agoprevAnecdotally I'm buying and reading more books now than at any other point in my life. Reading gets me away from a computer screen, which I would otherwise stare at from sunrise to sundown. I tend to be a late adopter. Once things are close to obsolete I usually get involved! Although books have the disadvantage of being a similar focal distance as a screen and for this reason is still bad for my eyesight! reply hyperman1 2 hours agoprevI read this and think about Sanglard's book: https://fabiensanglard.net/gebbdoom/ He has 0.77 dollar profit on an 51 dollar book. A minimal campaign with some organisational backing on HN alone should be able to get 1000 books sold, no? Every programmer who lived trough the doom craze is interested. It would place him above the average number of books sold. reply keiferski 3 hours agoprevBooks are functionally a type of merch (low end) or art object (high end) at this point. The companies that sell the most understand this, whereas those that adhere to the old model of focusing on the words/information inside are struggling. For example: celebrity books like David Goggins’ sell extremely well at a lower price point. At the other end, Taschen, who makes expensive art books and partners with celebrities often, also does well. reply hdivider 7 hours agoprevMain value prop of physical books for me: guaranteed zero interruptions from the book itself. Worst case, the pages stick together, or pages are missing, or damaged, or other very infrequent causes of distraction. All modern digital systems are by comparison, distraction machines. You never know when the next interruption ('notification') gets surfaced, the next update, the next forced restart, the next bug, the next battery problem, the next broken or lost charger cable. Physical books require light, the book, and your mind. All else fades away, and your mind is given free reign to ponder, to consider, to deliberate. No wonder civilization flourished whenever the physical written form was embraced at sustained scale. reply tomgp 1 hour agoprevOK, I haven't read the whole thing yet but this bit raised an eyebrow... >These two market categories (celebrity books and repeat bestsellers from the backlist) make up the entirety of the publishing industry and even fund their vanity project: _publishing all the rest of the books we think about when we think about book publishing (which make no money at all and typically sell less than 1,000 copies)._ Why do we view this as a vanity project and not the purpose of the whole endeavour? The celebrity books are just to raw material that allow them to dot heir real job; to make books those fragments of human connection widely available to the people who might need them, a service to readers and to future generations that isn't just about making money. reply rossdavidh 10 hours agoprevAlmost everything this post describes is not new, not even 21st century new. E-books market share seems to be stuck at about 20% of the total market, where it's been since the early teens. The decline of the print book is more or less like the decline of vinyl albums, except that it never actually declined that much. However, a lot more people want to be a best-selling author, than there are spots available, so most books don't sell much, and there are lots more authors who cannot get any advance at all. It was ever thus. reply AlbertCory 12 hours agoprevI've asked a number of agents online for two numbers: 1) How many unsolicited queries have you received, in whatever time period you like: month, quarter, year, forever? 2) How many turned into published books? None will provide that data, but I strongly suspect that the answer to #2 is \"zero.\" They'll happily quote you BS quantities, like \"a bunch\" or \"a handful.\" So as this article says, they spend their time chasing celebrities, online influencers, and friends of friends. And not making a whole lot of money anyway. reply tptacek 10 hours agoparentI don't think this is what the article says at all. It says that publishers make all their money celebrities and the backlist, and that those titles pay for all the midlist titles, most of which don't recoup their advances. It's not like publishers can simply decide to focus on new and midlist authors. The books that sell, sell. The top line of this article is well described by its title. reply AlbertCory 10 hours agorootparentNo, you're quite mistaken. The agents try to divine what the publishers want, which is, as you said, \"celebrities [and the backlist], and that those titles pay for all the midlist titles, most of which don't recoup their advances.\" > It's not like publishers can simply decide to focus on new and midlist authors who said anything about what they should or shouldn't do? But since you brought it up: the same applies to the music business, as Ted Gioia (my original source). At one time in history, everyone in the music business was focused on selling more music, so nurturing and promoting new artists was good business. They didn't simply look at \"what are people buying now?\" reply tptacek 10 hours agorootparentI have what the article says to go on, and the article contradicts you. But I know a bit more about the music industry, and it is absolutely the case that midlist music acts don't --- and never did --- generate enough record sales to cover their nut. There's a famous Steve Albini zine article that indicts the recording industry for never recouping an advance, and a less famous David Lowery article that explains why nobody could reasonably expect any of these titles to recoup. It may be that we don't disagree at all, and are just talking past each other? At any rate: my only point here is, like startup investing and the music industry, we're talking about a hits business. And they aren't hits businesses because a cabal of cigar-chomping executives wants them to be that; they're that way because the median investment never recoups, and so the winners have to pay for the losers. reply kyleyeats 12 hours agoparentprevYou used to pitch your book to publishers. Now you pitch your audience. reply greenie_beans 4 hours agorootparentthis is true for non-fiction for sure reply AlbertCory 10 hours agorootparentprevI think that's true, actually. reply x0x0 4 hours agorootparentprevThat's always been the case; it's merely a proxy for likely sales. It used to be that audience was demonstrated by previous sales; now it can be proved by being a celebrity, twitch/twitter/instagram/whatever followers, etc. reply vundercind 12 hours agoprevYep. No money in it short of not just getting optioned for TV or film, but an actual released production. Even then, a hit or two likely won’t pay as much as you’d think. Some pretty damn successful authors have boring day jobs, or stopped after a handful of “successful” books that didn’t make them much money and went back to a normal corporate career. The other option is to crank out (ha, ha) short romance novels like a machine and market like mad. The money’s in hard work at writing porn, or in big broad-audience grand slams, probably written at a junior high reading level, with a small niche supporting a handful of authors in writing based-on-a-true-story stuff (bonus if “true crime” connection) aimed directly at getting optioned for film or tv (but that one’s very hard to break into, studios have some go-to authors for that stuff and you ain’t one of them) reply vunderba 4 hours agoprevFrom the article: \"The Big Five publishing houses spend most of their money on book advances for big celebrities like Brittany Spears and franchise authors like James Patterson and this is the bulk of their business. They also sell a lot of Bibles\" Well that was a depressing read (no pun intended). reply nikolayasdf123 5 minutes agoprevI do reply lrvick 12 hours agoprevMost books today are sold with DRM and when the DRM servers are deactivated the books stop working. This happened with the Microsoft ebooks store and has happened with countless other DRM media. Sometimes books are automatically revised, or censored. Sometimes accounts holding DRM licenses are deleted or deactivated by mistake. The only way to actually own a book these days is to buy it on paper, or from one of the rare few publishers that do not use DRM. If you want a DRM free e-book collection you will need a book scanner or send it to a scanning service. reply npilk 11 hours agoparent> If you want a DRM free e-book collection you will need a book scanner or send it to a scanning service. Well, technically there's a third option... reply tmtvl 10 hours agorootparentYeah, but if I _could_ write my own, I wouldn't be in the market to buy one, right? reply TheCapeGreek 4 hours agorootparentI think the post is talking about sailing the high seas... reply dredmorbius 3 hours agorootparentprevFourth option, then. reply WolfeReader 11 hours agoparentprevKobo and Google Play both tell you up-front if a book has DRM or not. And very often - not \"rare\"-ly as you claim - buying directly from the publisher results in a DRM-free ebook. reply stubish 3 hours agoparentprevMost Kindle books are not sold with DRM, for many years now. I don't know about other platforms. Readers yelled at publishers until they stopped enabling it, and turned out it didn't make enough difference to keep annoying their core customer base. reply doug_durham 11 hours agoparentprevThe books sold through Apple Books are all sold without DRM. That's pretty standard now. reply daveoc64 6 hours agorootparentThat's not at all standard. Most books Apple sells come with DRM. This section of the Apple documentation explains how a publisher can control whether DRM is used for their book: https://itunespartner.apple.com/books/support/30-manage-book... All of the major eBook stores sell books with and without DRM (including Amazon Kindle, Rakuten Kobo, Apple Books, Google Play Books). Whether an individual book includes DRM on all of these stores is down to the publisher, and you won't typically see books from the major publishers offered without DRM. reply II2II 11 hours agoparentprevI am fairly certain that most, if not all, mass market ebooks use DRM tied to the software. If the servers go, the books will be fine for as long as the software continues to work. There are also authors and publishers who offer books without DRM. Please note: I do not like DRM. I simply view disinformation as a good way to loose good will from those who would otherwise support a cause. reply lrvick 3 hours agorootparentThere exist select few major publishers that sell without DRM, particularly when looking at fiction. Microsoft is a noteworthy example of a major ebook seller shutting down ebook accs: https://www.vice.com/en/article/3k3wkk/microsoft-ebooks-will... DRM being tied to the software is effectively still the same end result. Software maker for any number of reasons stops publishing new versions of the app and they become unlisted from appstores eventually for failing to use recent SDKs or patching security flaws and users still cannot read their books. So you depend on a company paying someone to run a DRM server or you depend on them paying someone to maintain an app and a CI/CD system. Sooner or later when the DRM content supplier stops paying for the infrastructure, your book stops working one way or the other. reply 140 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The post explores the antitrust battle between Penguin Random House and the Department of Justice, focusing on the success of books from famous authors and franchises.",
      "It highlights the struggles of traditional publishers in staying profitable, the rise of self-publishing, and the competition from Amazon, signaling potential industry shifts.",
      "Additionally, it mentions the decreasing significance of traditional publishing advances, the surge in direct-to-audience writing, especially in romance and children's genres."
    ],
    "commentSummary": [
      "The discussion highlights challenges in the publishing industry, such as technology's impact, marketing roles of publishers, and struggles faced by new authors in a celebrity-driven market.",
      "It examines the evolving book publishing landscape towards digital formats and the value of public libraries as knowledge sources.",
      "The conversation also addresses the ecological impact of e-readers, the debate between physical books and e-readers, and emphasizes the significance of reading for personal enjoyment and creativity."
    ],
    "points": 433,
    "commentCount": 399,
    "retryCount": 0,
    "time": 1713818201
  },
  {
    "id": 40127806,
    "title": "Phi-3 Mini: Powerful Language Model for Mobile Devices",
    "originLink": "https://arxiv.org/abs/2404.14219",
    "originBody": "Computer Science > Computation and Language arXiv:2404.14219 (cs) [Submitted on 22 Apr 2024] Title:Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone Authors:Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Olatunji Ruwase, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yunan Zhang, Xiren Zhou View PDF HTML (experimental) Abstract:We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench). Comments: 12 pages Subjects: Computation and Language (cs.CL); Artificial Intelligence (cs.AI) Cite as: arXiv:2404.14219 [cs.CL](or arXiv:2404.14219v1 [cs.CL] for this version)https://doi.org/10.48550/arXiv.2404.14219 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Sebastien Bubeck [view email] [v1] Mon, 22 Apr 2024 14:32:33 UTC (3,072 KB) Full-text links: Access Paper: View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CLnewrecent2404 Change to browse by: cs cs.AI References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers?Disable MathJax (What is MathJax?)",
    "commentLink": "https://news.ycombinator.com/item?id=40127806",
    "commentBody": "Phi-3 Technical Report (arxiv.org)268 points by varunvummadi 7 hours agohidepastfavorite88 comments modeless 5 hours agoEveryone needs to take these benchmark numbers with a big grain of salt. According to what I've read, Phi-2 was much worse than its benchmark numbers suggested. This model follows the same training strategy. Nobody should be assuming these numbers will translate directly into a high ranking on the LMSYS leaderboard, or usefulness in everyday tasks. Let's not dethrone Llama 3 until some real world testing can be done. That said, I don't think it's impossible for a small model to be very good. I see their \"synthetic data\" as essentially a way of distilling GPT-4 into smaller models. It would be exciting if a large fraction of the performance of huge models could be transferred to small ones! If true, then Chinchilla-optimal training could make sense again, as you could optimally train a ginormous model and then distill it afterward for efficient inference. reply ankit219 2 minutes agoparentNot trying to disparage them, but their models always give a feeling that it is overfitted on benchmarks hence they perform so well. On everyday tasks, it's much worse - chat or simple completion tasks. Distilling can work and there are papers which suggest it does, but we still do not have a reliable mechanism which can distill knowledge from larger teacher models to smaller student models. reply bt1a 5 hours agoparentprevThis won't dethrone Llama 3, but it's equally impressive. They mention this model's relative weakness in the TruthfulQA eval, since it's more lossy trying to pack 'knowledge' into a small model relative to problem-solving skills (which shine on MMLU) Regardless - still a very useful thing to have offline and on the fly. Those scores are nothing to scoff at. Given that these pipelines are likely harder harder to imitate than new architectures like Transformers, I assume there has been and will be an intense focus on synthetic data generation and cleansing. Llama 3 used 15T of tokens in its training corpus vs 4.8T in the \"scaled-up\" version of phi-3. If you made it to the end of this disjointed ramble I'm sorry reply Grimblewald 5 minutes agorootparentEven llama3 has its issues. Ive been quite impressed so far but if the context gets a little long it freaks out, gets stuck repeating the same token or just fails to finish an answer. This is for the full f16 8B model, so it cant be put down to quantization. It also doesnt quite handle complex instructions as well as the benchmarks would imply should. reply IvanAchlaqullah 3 hours agorootparentprev> TruthfulQA Wait, people still use this benchmark? I hear there's a huge flaw on it. For examples, fine-tuning the model on 4chan make it scores better on TruthfulQA. It becomes very offensive afterwards though, for obvious reasons. See GPT-4chan [1] [1] https://www.youtube.com/watch?v=efPrtcLdcdM reply andy99 45 minutes agorootparentNot sure I understand your example? It's not an offensiveness benchmark, in fact I can imagine a model trained to be inoffensive would do worse on a truth benchmark. I wouldn't go so far as to say truthfulQA is actually testing how truthful a model is or its reasoning. But it's one of the least correlated with other benchmarks which makes it one of the most interesting. Much more so than running most other tests that are highly correlated with MMLU performance. https://twitter.com/gblazex/status/1746295870792847562 reply thomashop 1 hour agorootparentprevCouldn't it be that training it on 4chan makes it more truthful for some reason? reply nurumaik 43 minutes agorootparentprev>scores better >very offensive Any cons? reply hoseja 3 hours agorootparentprevLooks like a good and useful benchmark. reply refulgentis 5 hours agoparentprevPhi-2 wasn't chat/instruct tuned, so it didn't act good in chat, it was a base model. But the benchmark #s were real. reply nl 4 hours agorootparentI had a lot of issues trying to get Phi-2 to perform as well as the benchmarks indicated on non-chat tasks. It felt a lot like it was overfitted to the exact type of tasks (ie, not a data leak) in the benchmarks but if you were trying something a bit off track if didn't know what to do. At the time my hypothesis was that the small model just didn't have the capacity to generalise well enough, but since then Gemma 2B has come out and seems to be ok. So now I have no idea why, but yes: the benchmarks for Phi-2 didn't represent how it worked for me on real world tasks where you'd expect it top be ok. reply irjustin 5 hours agorootparentprevI'm pretty naive so please forgive it's a stupid question. To me, what the parent comment is saying is that even though the benchmarks are cool, it's not super helpful to the every day person. Because if you can't chat with it very well (even for a narrow context) what utility does it have with great benchmarks? reply svnt 4 hours agorootparentBoth are saying the same thing: in order for the base model that is phi to perform well as a chat agent, it would need to be tuned for that purpose before its benchmark results could have real-world value. reply imjonse 4 hours agorootparentFrom this report. Phi-2 was not instruct tuned indeed. \"Our models went through post-training with both supervised instruction fine-tuning, and preference tuning with DPO. We have worked on generating and curating various instruction and preference data. This has improved the model chat capabilities, robustness, as well as its safety.\" reply oersted 6 hours agoprevIncredible, rivals Llama 3 8B with 3.8B parameters after less than a week of release. And on LMSYS English, Llama 3 8B is on par with GPT-4 (not GPT-4-Turbo), as well as Mistral-Large. Source: https://chat.lmsys.org/?leaderboard (select English in the dropdown) So we now have an open-source LLM approximately equivalent in quality to GPT-4 that can run on phones? Kinda? Wild. (I'm sure there's a lot of nuance to it, for one these benchmarks are not so hard to game, we'll see how the dust settles, but still...) Phi-3-mini 3.8b: 71.2 Phi-3-small 7b: 74.9 Phi-3-medium 14b: 78.2 Phi-2 2.7b: 58.8 Mistral 7b: 61.0 Gemma 7b: 62.0 Llama-3-In 8b: 68.0 Mixtral 8x7b: 69.9 GPT-3.5 1106: 75.3 (these are averages across all tasks for each model, but looking at individual scores shows a similar picture) reply jxy 6 hours agoparentThis inductive logic is way overblown. > Incredible, beat Llama 3 8B with 3.8B parameters after less than a week of release. Judging by a single benchmark? Without even trying it out with real world usage? > And on LMSYS English, Llama 3 8B is on par with GPT-4 (not GPT-4-Turbo), as well as Mistral-Large. Any potential caveat in such a leaderboard not withstanding, on that leaderboard alone, there is a huge gap between llama 3 8B and Mistral-Large, let alone any of the GPT-4. By the way, for beating benchmark, \"Pretraining on the Test Set Is All You Need\" reply oersted 6 hours agorootparentIt's easy to miss: select English in the dropdown. The scores are quite different in Overall and in English for LMSYS. As I've stated in other comments, yeah... Agreed, I'm stretching it a bit. It's just that any indication of a 3.8B model being in the vicinity of GPT-4 is huge. I'm sure that when things are properly measured by third-parties it will show a more sober picture. But still, with good fine-tunes, we'll probably get close. It's a very significant demonstration of what could be possible soon. reply saretup 5 hours agorootparentFirstly, English is a highly subjective category. Secondly, Llama 3 usually adds first sentences like ‘What a unique question!’ or ‘What an insightful thought’, which might make people like it more than the competition because of the pandering. While Llama 3 is singular in terms of size to quality ratio, calling the 8B model close to GPT4 would be an overstretch. reply YetAnotherNick 3 hours agorootparentYes, I don't know how people don't realize how much cheap tricks works in Chatbot Arena. A single base model produces 100s of ELO difference depending on the way it is tuned. And on most cases, instruction tuning heavily slightly even decreases reasoning ability on standard benchmark. You can see base model scores better in MMLU/ARC most of the times in huggingface leaderboard. Even GPT-4-1106 seems to only sounds better than GPT-4-0613 and works for wider range of prompt. But in a well defined prompt and follow up questions I don't think there is an improvement in reasoning. reply imtringued 3 hours agorootparentWhen I tried Phi2 it was just bad. I don't know where you got this fantasy from that people accept obviously wrong answers, because of \"pandering\". reply YetAnotherNick 2 hours agorootparentObviously correct answer matters more but ~100-200 elo points could be gained just for better writing. Answer would be range of 500 elo in comparison. reply ignoramous 6 hours agoparentprev> Phi-3-mini 3.8b: 71.2 Per the paper, phi3-mini (which is english-only) quantised to 4bit uses 1.8gb RAM and outputs 1212 tokens/sec (correction: 12 tokens/sec) on iOS. A model on par with GPT-3.5 running on phones! (weights haven't been released, though) reply coder543 6 hours agorootparent> (weights haven't been released, though) Phi-1, Phi-1.5, and Phi-2 have all had their weights released, and those weights are available under the MIT License. Hopefully Microsoft will continue that trend with Phi-3. > outputs 1212 tokens/sec on iOS I think you meant \"12 tokens/sec\", which is still nice, just a little less exciting than a kilotoken/sec. reply jph00 5 hours agorootparentWeights will be realised tomorrow, according to one of the tech report authors on Twitter. reply ignoramous 5 hours agorootparentprev> you meant 12 tokens/sec Thanks! The HTML version on archive.is has messed up markup and shows 1212 instead: https://archive.is/Ndox6 reply intellectronica 3 hours agorootparentprevWeights are coming tomorrow. reply karmasimida 1 hour agoparentprevWhere did you get this from? > So we now have an open-source LLM approximately equivalent in quality to GPT-4 that can run on phones No, not even close ... Even Gemini has huge UX gap comparing to GPT4/Opus, 8B I won't even attempt this argument. reply alecco 1 hour agoparentprevAt a glance, it looks like Phi-3 was trained on an English only, STEM-strong dataset. See how they are not as strong in HumanEval, Trivia, etc. But of course it's very good. reply crakenzak 6 hours agoparentprevCan’t wait to see some Phi-3 fine tunes! Will be testing this out locally, such a small model that I can run it without quantization. Feels incredible to be living in a time with such neck breaking innovations. What are chances we’ll have aWhat are chances we’ll have a And on LMSYS English, Llama 3 8B is well above GPT-4 Source? reply oersted 6 hours agorootparentRight thanks for the reminder, I added it reply moralestapia 6 hours agorootparentThanks, I don't see them being \"well above GPT-4\", merely 1 point? Also, no idea why one would want to exclude GPT-4-Turbo, the flagship \"GPT-4\" model, but w/e. I also don't think they \"beat Llama 3 8B\"; their own abstract says \"rivals that of models such as Mixtral 8x7B and GPT-3.5\", \"rivals\" not even \"beats\". Great model, but let's not overplay it. reply oersted 6 hours agorootparentIn the English category: GPT-4-0314 (ELO 1166), Llama 3 8B Instruct (ELO 1161), Mistral-Large-2402 (ELO 1151), GPT-4-0613 (ELO 1148). You are right, I toned down the language, I got a bit overexcited, and I missed the difference in the versions of GPT-4. And LMSYS is a subjective benchmark for what users prefer, which I'm sure has weird inherent biases. It's just that any signal of an 3.8B model being anywhere in the vicinity of GPT-4 is huge. reply moralestapia 6 hours agorootparentYeah, GPT3.5, in a phone, at ~1,000 tokens/sec ... nice! reply mlyle 5 hours agorootparent> at ~1,000 tokens/sec 12 tokens per second. reply zone411 6 hours agoparentprev> So we now have an open-source LLM approximately equivalent in quality to GPT-4 that can run on phones? No, we don't. LMsys is just one, very flawed benchmark. reply ukuina 5 hours agorootparentWhy is LMsys flawed? Many people treat LMsys as gospel because it's the only large-scale, up-to-date qualitative benchmark. All the numeric benchmarks seem to miss real-world applicability. reply oersted 6 hours agorootparentprevAgreed, but it's wild that even one benchmark shows this. Based on what we knew just a few months ago, these models should be so far from each other in every benchmark. reply mythz 5 hours agoprevI'll believe it till I try it for myself, Phi-2 was the clear worst of the 20 LLMs we evaluated (was also smallest so was expected). But it was slow for its size, generated the longest responses with the most hallucinations, as well as generating the most empty responses. It was also the model ranked with the lowest quality answers. reply visarga 5 hours agoprevThis shows the power of synthetic content - 3.3 trillion tokens! This approach can make a model even smaller and more efficient than organic text training, and it will not be able to regurgitate NYT articles because it hasn't seen any of them. This is how copyright infringement claims can be placated. reply brcmthrowaway 6 hours agoprevIf I was Apple I'd be quaking in my boots. They are getting too far behind to ever catch up. Nokia in 2010 vibes. reply PedroBatista 6 hours agoparentThey'll just do what they have been doing for ~20 years, they wait, pick the \"winner\", polish the \"user experience\", call it Apple magic and incorporate that into their product cycles. Some day will be the day their joke book becomes so mediocre it will not stick anymore, but I think they are safe on this one, for now.. reply mirekrusin 35 minutes agorootparentConsidering that experiments cost tens to hundreds of millions of dollars a pop this may be not that bad strategy. reply fauigerzigerk 1 hour agorootparentprevTrue for hardware, but their record on software is far less convincing. reply vessenes 6 hours agoparentprevI don't think MS has a special sauce here, just a willingness to publish. To the extent MS has disclosed the bulk of what they are doing with Phi, it's a combination of really nice initial idea \"Use written texts + GPT-4 to generate high quality prompts where we know the answer is great because it's written down\" and engineering. To me this is advancing the state of the art as to the impact of data quality, but it doesn't look to me like the phi series have some magical special sauce otherwise. Data of quality and synthetic data creation are not magical moats that Apple can't cross. I'll say too that I'm psyched to try Phi-3; the sweet spot for me is a model that can be a local coding assistant and still answer random q&a questions with some sophistication. I'm skeptical that 3-8b parameter models will bring the high-level of sophistication sometimes needed in this cycle; there's still a very large gap with the larger models in daily use, despite some often close benchmark scores. Anyway, Apple-Phi-3 is in no way an impossibility. reply bt1a 5 hours agoparentprevI tore my hair out developing a SwiftUI app that could run llama.cpp and whisper.cpp simultaneously. Was able to run a Q3_K Mistral 7B along with a smaller whisper model eventually, but grinding through XCode is a nightmare. They're working on MLX but it only recently got swift bindings. They just don't have the DEVELOPERS DEVELOPERS DEVELOPERS coked out attitude i guess reply esafak 6 hours agoparentprevDid they ever claim to be a powerhouse in foundation models? Did your MacBook or iPhone become obsolete or stop working? They use the models, they don't release them because they don't hoard data. reply WanderPanda 6 hours agoparentprevThe opposite is the case, with all the advancements, even by doing nothing, Apple (like everyone, including hobbyists) is moving closer to the frontier. Hopefully this trend stays alive! reply oersted 6 hours agoparentprevIf anything this is good for them. Apple's play here has always been getting their devices ready for running LLMs locally. This makes it way easier. reply Deverauxi 6 hours agoparentprevThey have something like 140 billion dollars in cash. They’ll be fine. reply IncreasePosts 6 hours agoparentprevHow exactly does publicized research lead to them not being able to catch up? I don't think anything in this paper is patentable. reply seydor 4 hours agoparentprevApple's advantage is that their devices are safeguarding people from the dangers of AI reply fauigerzigerk 1 hour agorootparentHow so? And what dangers? reply astrange 5 hours agoparentprevI think that when people release new interesting software products it's good for hardware companies. reply thoughtegting 6 hours agoparentprevIf I were apple, I would be developing something in total secrecy and then release something ahead of the rest of competition when people least expect it. very big ifs but siri can be updated everywhere overnight and I dont see them rushing into anything like this reply golergka 6 hours agorootparentIf I were apple, I would just buy one of the major LLM companies. They have the cash. reply bingbingbing777 5 hours agorootparentThey've been buying AI companies and have nothing to show for it. reply elbear 4 hours agorootparentWhy do you think that is? Do you think their culture is an obstacle or is it something else? reply golergka 3 hours agorootparentprevShowing off work in progress is not really their thing. reply ec109685 6 hours agoparentprevEh, I think it’s showing that this class of model is becoming commoditized given there is a new one launching every week. reply moralestapia 6 hours agoparentprevI don't recall Nokia being a 3 trillion dollar company. Your vibes may vary, though. reply abidlabs 5 hours agoprevHugging Face Paper Page and Discussion: https://huggingface.co/papers/2404.14219 reply whereismyacc 55 minutes agoparentthey're just spamming \"weights or it didn't happen\" i mean, fair reply smartmic 3 hours agoprevHm, roundabout 84 authors of one \"scientific\" paper. I wonder if this says something about (a) the quality of its content, (b) the path were academic (?) paper publishing goes to, (c) nothing at all, or (d), something entirely else. reply lysecret 1 hour agoparentJust means you need a big machine and a lot of capital to make advancement. Take a look at any paper coming out of cern. reply a_bonobo 2 hours agoparentprevI have been on far larger author lists :) There's probably a whole team for the training data generation and assessment, a whole team for the safety assessment (section 4), that stuff adds up. reply samus 1 hour agoparentprevIt's a tech report. Fair enough to include the whole lab. reply simonw 6 hours agoprevI'm getting a bit skeptical of MMLU at this point. As far as I can tell it's a set of multiple choice questions that hasn't been updated since 2020. We have to trust the model providers not to deliberately or accidentally train on it for those scores to be useful. reply minimaxir 6 hours agoparentAt the least, there's multiple benchmarks noted in the paper (21!) and the results are consistent across all of them. I'd trust Microsoft to do decontamination testing, although the paper doesn't explicitly mention it other than \"The prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for the phi-3 models.\" reply blackoil 6 hours agoprevHas anyone used these/similar with fine tune and RAG? How is the performance over a narrow domain for simple queries? Is it good enough for say an informational chat bot? reply ur-whale 5 hours agoprevThat's a whole lot of Zhangs! reply maximsicora 1 hour agoprevinsane reply hackerlight 6 hours agoprev [–] Less tokens than Llama 3 (3.3T vs 15T) yet better outcome. No doubt more information dense training data. The interesting thing is the use of synthetic data which they don't talk about. reply vessenes 6 hours agoparentActually the original Phi papers did talk about their synthetic data strategy, and it's very cool -- essentially invert high quality textbook text using GPT-4 to create prompts, where the textbooks supply the answers. There may be more undisclosed, but it remains in my mind as one of the best ideas of the last twelve months -- so smart, and interesting, and apparently, it works well. reply astrange 5 hours agorootparentI feel like literal dictionaries would make good training data; wonder if any of them have done that. LLMs are good at faking so it's hard to tell by asking them. reply torginus 4 hours agorootparentprevExcept everything that comes out of an LLM (like GPT4) is highly suspect (at least in my experience). reply samus 1 hour agorootparent1. They need it for style and language, not necessarily for the facts 2. Since GPT-4 is seen as the very best general-purpose LLM in existence, it makes sense to emulate its performance with less resources. 3. Phi models are also trained with other high-quality data reply xarope 5 hours agorootparentprevperhaps that's the best path forward? Text and reference books (hopefully unbiased) for answers, and web scraped data for conversational tone. reply YetAnotherNick 4 hours agorootparentprevNo they don't use textbook text at all despite the paper title. They just asked GPT-4 to generate \"textbook quality\" content, which doesn't even exactly looks like textbook. reply minimaxir 6 hours agoparentprev [–] Yes, \"chinchilla optimal\" is a meme, but 15T might turn out to be too many tokens. reply wrsh07 6 hours agorootparent [–] My understanding from this tweet thread [1] is that chinchilla probably overspecified some of the hyperparameters to the model tl;dr I'm looking forward to having lots of models (ideally models) trained with a wide range of parameters to narrow down \"what is actually optimal\" I think there is an interesting tradeoff of data quality and data volume, though (Eg if we train with the highest quality 10% of our data, does the model improve if we use the other 90%? What if we increase our data size by 10x?) [1] https://twitter.com/tamaybes/status/1780639257389904013 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The phi-3-mini language model, with 3.8 billion parameters trained on 3.3 trillion tokens, competes with larger models like Mixtral 8x7B and GPT-3.5, offering high performance while being deployable on a phone.",
      "The model's training dataset is an enhanced version of that used for phi-2, incorporating extensively filtered web and synthetic data, prioritizing robustness, safety, and chat format.",
      "Results also reveal the scalability of phi-3-small and phi-3-medium models, proving their superior abilities compared to phi-3-mini in initial parameter-scaling experiments."
    ],
    "commentSummary": [
      "The Phi-3 Technical Report delves into benchmark figures and practical performance of a new model, addressing overfitting concerns and everyday task performance.",
      "It compares the model with others like Llama 3 and GPT-4, emphasizing their strengths and weaknesses.",
      "Discussions tackle benchmark efficacy, data quality worries, and synthetic data utilization in model training, shedding light on AI industry competition and innovation, alongside skepticism and hope regarding language model advancements from tech giants such as Apple and Microsoft."
    ],
    "points": 268,
    "commentCount": 88,
    "retryCount": 0,
    "time": 1713838912
  },
  {
    "id": 40112958,
    "title": "OpenOrb: Curated Search Engine for RSS Feeds",
    "originLink": "https://openorb.idiot.sh/search",
    "originBody": "Alternative search engines are neat, as are RSS feeds. OpenOrb is a self-hosted app which allows visitors to search over a list of blogs you love. If you put your 10 favourite blogs in there, it&#x27;ll search just those blogs and not show you any sponsored content or machine-generated garbage (unless... you follow blogs written by machines?)Personal RSS feed readers can usually do this sort of thing, but RSS readers aren’t meant to be shared, so you can think of the search engine as a &#x27;curated feed list as a public service&#x27;.I wrote a longer blog post about OpenOrb here: https:&#x2F;&#x2F;raphael.computer&#x2F;blog&#x2F;openorb-curated-search-engine&#x2F;",
    "commentLink": "https://news.ycombinator.com/item?id=40112958",
    "commentBody": "OpenOrb, a curated search engine for Atom and RSS feeds (idiot.sh)245 points by lowercasename 23 hours agohidepastfavorite50 comments Alternative search engines are neat, as are RSS feeds. OpenOrb is a self-hosted app which allows visitors to search over a list of blogs you love. If you put your 10 favourite blogs in there, it'll search just those blogs and not show you any sponsored content or machine-generated garbage (unless... you follow blogs written by machines?) Personal RSS feed readers can usually do this sort of thing, but RSS readers aren’t meant to be shared, so you can think of the search engine as a 'curated feed list as a public service'. I wrote a longer blog post about OpenOrb here: https://raphael.computer/blog/openorb-curated-search-engine/ gbrindisi 23 hours agoI really like the idea! At some point I put up a miniflux instance and it has surprisingly been a breath of fresh air for my content consumption. What miniflux and my setup lacks is a way to retrieve stuff I read and this OpenOrb might fit the use case... I will try it out! reply swyx 17 hours agoparenthttps://github.com/miniflux/v2 in case anyone else was also wondering reply freetonik 21 hours agoparentprevWhat do you mean by \"retrieve stuff I read\"? reply gbrindisi 20 hours agorootparentsometimes I stumble into stuff I’m sure I’ve already read something about in an article but if I didn’t bookmarked or made a note of it it’s very hard to find again (miniflux flushes content after a certain age) reply corney91 15 hours agorootparentI've set the following in Miniflux to stop it deleting things: CLEANUP_ARCHIVE_READ_DAYS=-1 CLEANUP_ARCHIVE_UNREAD_DAYS=-1 reply gen220 20 hours agorootparentprevYou can set the age to arbitrary points in the past, if storage isn't a concern. I've actually found miniflux's search feature fairly solid for dredging up old stuff I've read! reply freetonik 20 hours agorootparentprevAh, I see, thanks for the explanation! I'm asking because I'm working on a similar project, which allows to both search and save blog posts permanently. reply flir 20 hours agorootparentSpeaking for me personally, I've always felt \"search my history\" should be implemented in the browser, not as an external tool. \"Search and save blog posts\" seems like a subset of the real problem. reply b1nj0y 1 hour agoprevIf you'd like to read RSS in you new tab: https://tabhub.github.io/ reply beaugunderson 7 hours agoprevI really like the idea of feed/entry search but it seems to not return very relevant results... if I search for \"software defined radio\" with or without quotes I get lots of results that don't have those terms in them reply INGSOCIALITE 12 hours agoprevbut does this filter out the rss feeds that are just a headline and then a \"click here to read the whole story\" link? that's what killed rss it wasn't google reader going away, it was the ad-weaponizing of the feeds themselves reply yakkomajuri 21 hours agoprevSomeone compiled this a while ago which is a pretty good starter list for content discovery: https://github.com/outcoldman/hackernews-personal-blogs I've imported most of them into https://app.recessfeed.com/ and found some nice ones to follow through that reply lbhdc 20 hours agoprevThis is a cool idea. When I search for \"history\" it returned only technical articles, and heavily favored dan luus website. Are technical blogs the primary focus? reply freetonik 20 hours agoparentI believe the instance currently has very few blogs indexed: https://openorb.idiot.sh/feeds But you can deploy your own instance and add any blogs you want. reply marginalia_nu 20 hours agoparentprevGiven that techy people have a strong disposition to have a blog, more so than other demographics, there's an implicit bias toward the technical within the blogosphere, especially in its diminished state. reply marginalia_nu 8 hours agorootparent@ reply marginalia_nu 22 hours agoprevTangentially on this note, if anyone is interested, I can produce a list of every RSS feed known to the marginalia search crawler. It's a pretty noisy list, but any thing I can do to help the spread, discovery and adoption of RSS I'm happy to help with so just let me know. I a tool in place to export this data to help power the experimental RSS preview feature[1], but haven't had the inspiration to do much with that yet. [1] e.g. https://search.marginalia.nu/site/jvns.ca --edit-- Ok so there was interest. Give me a moment, I'll need to run an extraction script. Check back in a few hours or bookmark https://downloads.marginalia.nu/exports/ reply petercooper 22 hours agoparentI would be very keen to have access to that list and to, ideally, have a go at cleaning it up and producing a topical subset for broader use in certain fields I'm interested in (e.g. all the \"developer blogs\", say). I offer an OPML file of several hundred engineering/dev related blogs at https://engineeringblogs.xyz/ but I'm starting to think a little bigger. reply marginalia_nu 19 hours agoparentprevAlright, about half a million RSS feeds available at: https://downloads.marginalia.nu/exports/ [select feeds.csv] The data is, as mentioned, pretty noisy. It's a best-effort guess as to which is the canonical RSS feed for the particular domain. There doesn't appear to be any convention for specifying this, so when there's multiple a fair bit of guesswork is involved. Expect a fair number of dead URLs, lots of spam from CRMs that generate uninteresting feeds. reply yakkomajuri 21 hours agoparentprevI'd be keen on this and would import all of them into Recess (https://app.recessfeed.com) - also working on RSS adoption and discovery! reply mariusor 20 hours agoparentprevIsn't there a way to integrate this type of info into the actual search engine? Ie, search for type:rss or atom and return the links to the RSS feeds? [edit] I mean, to have it closer to what OP showed. reply 8organicbits 18 hours agorootparentI know the Google search console lets you upload a site map, which can be an RSS feed, so the information is readily available. I suspect Google isn't incentivised to promote RSS, especially after they killed Google Reader. reply hactually 21 hours agoparentprevI started a submission based platform ( bao.social but not currently resolving) as a side project because I missed the accessibility for RSS. would be keen on the list or even just connecting with you and OP reply marginalia_nu 14 hours agorootparentFeel free to shoot me an email if you want to have a chat, bounce ideas or whatever. That goes for other people as well ;-) I'm a bit busy with finalizing my grant-funded work in the immediate future so reply times may be a bit slow, but such is life. reply lowercasename 22 hours agoparentprevThat would be _so_ cool! What an amazing resource that would be. reply djoldman 22 hours agoparentprevI think the community would be interested in list and you'd get a lot of downloads if you offered it up. reply freetonik 21 hours agoprevNice! I was thinking about the same kind of tool a while back, and developed a community-based curated feed reader with full-text search. It's not public yet (sign ups are behind an invitation code), but search works for guests: https://minifeed.net/global reply lowercasename 20 hours agoparentThis is super nice, and it looks like it's going to have some really great features, well beyond OpenOrb's! Excited to keep an eye on this. reply freetonik 20 hours agorootparentThanks for the kind words! I'm still a bit hesitant to make a \"Show HN\" at this time, but there are indeed potentially interesting implemented and planned features, like: - full text search across all blogs (implemented) and across blogs user subscribed to (planned) - subscribing to users to see the blogs they follow in your \"friendfeed\" (implemented) - favorites, with contents saved to permanent storage (implemented) - custom lists of blogs and posts (planned) - comments (not sure about this one yet) reply renegat0x0 22 hours agoprevYou can find many RSS feeds, links in my repository https://github.com/rumca-js/Internet-Places-Database/tree/ma... It contains also domain lists, that include tag indicating, if it is personal, or not. reply toastal 19 hours agoprevGreat to it’s hosted on a free software forge too not locking in contributions! Not sure I always agree that feeds should have the full post tho. This not only (obviously) bloats the size of the feed, but there are valid reasons to want to drive users to your site--especially if you have demos or you write about code & have your code blocks syntax highlighted (statically, never do this with a JavaScript) as it provides a better reading experience. You can put styling technically in Atom/RSS but even then, a lot of readers won’t be applying the styling. That said, I definitely appreciate the full post if your site is full of trackers, ads, marketing garbage or other bloat since I can skip the site. Is this some site engineers giving us the nod on a better UX? I read a gridiron football news site & boy does that feed become take a site from unusable to pleasant (good photography). reply fabianholzer 17 hours agoparentAs a feed consumer I am always happy if a feed contains the full content, but I am not sure if the feed must also include all articles that a site ever published. That would basically make the feed a serialized version of the whole website (which is indeed what a few feeds that I subscribe to do by including sections that are common on personal sites like about/contact/now as items of their feed - but those are the minority). That would actually be fine as long as the archive is small or at certain size, when the feed is paginated. But I am under the impression that most feed generators do not have pagination in mind, also I don't know how well the individual aggregators and readers handle it on the consuming end. reply tiffanyh 20 hours agoprevFor a brief moment, I thought this was related to https://OrbStack.dev reply keepamovin 22 hours agoprev [–] I wonder when RSS will experience its \"Google Search in 1997\" moment? Right now it's beginning to nibble at Yahoo Directory days reply tl 22 hours agoparent [–] That would be 2005 when Google Reader launched. RSS for people who didn't know what RSS was. reply keepamovin 22 hours agorootparent [–] No, I mean: \"Google\" moment as in what Google originally was. Let me rephrase in edit my original comment to \"Google Search\" moment. Basically, when Google came on the scene in 1997, it blew away Yahoo Directory. Do I have my dates right? Hahaha :) reply cykros 22 hours agorootparent [–] RSS if anything is in decline, rather than its ascent, because of the fact that in many ways it offers access to content in a way that diminishes ad views. It's not impossible that it could come back from this state, and indeed, outside of this issue, there's nothing wrong with it as a system, and podcasts make heavy use of it. But it's worth being aware of this headwind. reply throwaway14356 20 hours agorootparentrss is the advertising. It allows me to conveniently keep track of tens of thousands of websites. If you don't have a feed, no problem. Ill just read something else. With few exceptions I can't be bothered to keep looking at a web page hoping something new has happened. reply toyg 19 hours agorootparentRSS advertises the content, but not the actual sponsors of such content (i.e. commercial ads). It's also pretty hard to make it track readers. That's why the likes of Meta and Google just don't like it. reply keepamovin 21 hours agorootparentprev [–] No I totally know it's in ascent, that's my point! Haha! :) Hmm, how to express what I'm saying more clearly -- seems it's been missed? Haha! :) I mean, like RSS seems like where the web was in 1996 - on the ascent! - waiting for its \"Google Search\" moment, whereas these types of RSS curations in this product and others like it recently, a little bit like Yahoo Directory! reply tl 21 hours agorootparent> No I totally know it's in ascent, that's my point! Haha! :) How do you \"know\" this? Show some proof! RSS has two well-known use cases: news and podcasts. It is fighting a pitched battle against players with deep pockets who want you to consume content where they can monetize it with ads. Google Reader survived for as long as it did because such a service is incredibly cheap to run. Google only ended it to push people to Google+. Many of the various competing providers that popped up during that period are still around, but I would not say it is flourishing. This is what Google thinks of RSS: https://trends.google.com/trends/explore?date=all&geo=US&q=R... Note a rise and plateau centered around 2005 and a brief peak in 2013 (when Google killed Reeder). reply toyg 19 hours agorootparentI agree with your view, but if we put down our old greybeard hats for a minute - isn't it nice to see a new generation of people potentially getting excited about RSS? The parent comment is clearly by an optimistic youngster, who has just discovered an awesome technology that (he thinks) could change the world. And maybe it can! Just because we've seen it beaten once (well, a few times), it doesn't mean it's dead, and maybe, just maybe, there is something we can't see that will be the real RSS killer app. Take podcasting - when RSS was first devised, nobody thought of such a use-case; it just happened that the media-attachment hacks tacked on top of it merged, at a particular time and place, with some other emerging tech (the iPod), creating something so good that it's still around. reply tl 18 hours agorootparent> The parent comment is clearly by an optimistic youngster My only objection is when the \"youngster\" had his viewpoint questioned, his response was \"no I totally know it's in ascent\". Objective evidence points in the opposite direction. reply keepamovin 5 hours agorootparentAhahaha, yeah the original cykros comment was \"If anything RSS is in ascent\" because he took my parent comment to mean it was going down somehow, then I replied and sometime later he changed his comment. Then you guys came along hahahaha@! So funny hahaha :) You know mercury is retrograde right now so there's a lotta confusion on here hahahah! :) reply toyg 14 hours agorootparentprevYouth be youth, innit ;) reply keepamovin 5 hours agorootparentHahah yeah based on your misinterpreted comment thread hahaah! :) reply keepamovin 5 hours agorootparentprevIt's so funny how people take this stuff, it's so funny. Yeah I was resopnding to that vibe but it's more like the greybeards who love RSS who keep it alive, but what I see is it keeps building over time. Hahahaah and it's so funny to see contrarian or animosity against RSS on HN because usually it's the total opposite. I guess it's just 'cause I'm saying it people love to disagree, right? Hahahah! :) so hilarios hahah omg :) reply keepamovin 5 hours agorootparentprevHahaha, this is so funny because the original cykros comment was like \"RSS is in ascent\", and I was just agreeing with that in the course of the discussion. And then he changed hahaha! :) So like the whole conversation became a non sequitur after that so I understand if you're confused. Hahahaha! :) The issues with async I guess hahaha so funny :) reply pantulis 21 hours agorootparentprev [–] I think you are being slightly over enthusiastic here. reply keepamovin 5 hours agorootparent [–] Hahah! :) yeah in the context of the negativity on this thread and the changes in the comments it might seem like that but in a different context it will look right hahaah! :) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "OpenOrb is a self-hosted app enabling users to search a curated list of blogs without sponsored or machine-generated content.",
      "It provides a curated feed list as a public service, distinguishing it from personal RSS feed readers.",
      "For more details, refer to a detailed blog post on raphael.computer."
    ],
    "commentSummary": [
      "OpenOrb is a specialized search engine catering to Atom and RSS feeds, offering users access to curated blog lists free of sponsored content.",
      "The platform addresses the decline of RSS caused by advertising and technological biases, exploring methods to incorporate RSS feeds into search engines to boost usage.",
      "Discussions on OpenOrb highlight the advantages of RSS for user experience and speculate on its future importance, coupled with a lighthearted tone involving amusing disputes and misunderstandings."
    ],
    "points": 245,
    "commentCount": 50,
    "retryCount": 0,
    "time": 1713781580
  },
  {
    "id": 40117510,
    "title": "North Korean-themed CSS styling for website design",
    "originLink": "https://www.38north.org/2024/04/what-we-learned-inside-a-north-korean-internet-server-how-well-do-you-know-your-partners/",
    "originBody": "Just a moment...*{box-sizing:border-box;margin:0;padding:0}html{line-height:1.15;-webkit-text-size-adjust:100%;color:#313131}button,html{font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji}@media (prefers-color-scheme:dark){body{background-color:#222;color:#d9d9d9}body a{color:#fff}body a:hover{color:#ee730a;text-decoration:underline}body .lds-ring div{border-color:#999 transparent transparent}body .font-red{color:#b20f03}body .big-button,body .pow-button{background-color:#4693ff;color:#1d1d1d}body #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}}body{display:flex;flex-direction:column;min-height:100vh}body.no-js .loading-spinner{visibility:hidden}body.no-js .challenge-running{display:none}body.dark{background-color:#222;color:#d9d9d9}body.dark a{color:#fff}body.dark a:hover{color:#ee730a;text-decoration:underline}body.dark .lds-ring div{border-color:#999 transparent transparent}body.dark .font-red{color:#b20f03}body.dark .big-button,body.dark .pow-button{background-color:#4693ff;color:#1d1d1d}body.dark #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjZDlkOWQ5IiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.dark #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI0IyMEYwMyIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjQjIwRjAzIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}body.light{background-color:transparent;color:#313131}body.light a{color:#0051c3}body.light a:hover{color:#ee730a;text-decoration:underline}body.light .lds-ring div{border-color:#595959 transparent transparent}body.light .font-red{color:#fc574a}body.light .big-button,body.light .pow-button{background-color:#003681;border-color:#003681;color:#fff}body.light #challenge-success-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSIgdmlld0JveD0iMCAwIDI2IDI2Ij48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJNMTMgMGExMyAxMyAwIDEgMCAwIDI2IDEzIDEzIDAgMCAwIDAtMjZtMCAyNGExMSAxMSAwIDEgMSAwLTIyIDExIDExIDAgMCAxIDAgMjIiLz48cGF0aCBmaWxsPSIjMzEzMTMxIiBkPSJtMTAuOTU1IDE2LjA1NS0zLjk1LTQuMTI1LTEuNDQ1IDEuMzg1IDUuMzcgNS42MSA5LjQ5NS05LjYtMS40Mi0xLjQwNXoiLz48L3N2Zz4=)}body.light #challenge-error-text{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIzMiIgaGVpZ2h0PSIzMiIgZmlsbD0ibm9uZSI+PHBhdGggZmlsbD0iI2ZjNTc0YSIgZD0iTTE2IDNhMTMgMTMgMCAxIDAgMTMgMTNBMTMuMDE1IDEzLjAxNSAwIDAgMCAxNiAzbTAgMjRhMTEgMTEgMCAxIDEgMTEtMTEgMTEuMDEgMTEuMDEgMCAwIDEtMTEgMTEiLz48cGF0aCBmaWxsPSIjZmM1NzRhIiBkPSJNMTcuMDM4IDE4LjYxNUgxNC44N0wxNC41NjMgOS41aDIuNzgzem0tMS4wODQgMS40MjdxLjY2IDAgMS4wNTcuMzg4LjQwNy4zODkuNDA3Ljk5NCAwIC41OTYtLjQwNy45ODQtLjM5Ny4zOS0xLjA1Ny4zODktLjY1IDAtMS4wNTYtLjM4OS0uMzk4LS4zODktLjM5OC0uOTg0IDAtLjU5Ny4zOTgtLjk4NS40MDYtLjM5NyAxLjA1Ni0uMzk3Ii8+PC9zdmc+)}a{background-color:transparent;color:#0051c3;text-decoration:none;transition:color .15s ease}a:hover{color:#ee730a;text-decoration:underline}.main-content{margin:8rem auto;max-width:60rem;width:100%}.heading-favicon{height:2rem;margin-right:.5rem;width:2rem}@media (width Enable JavaScript and cookies to continue(function(){window._cf_chl_opt={cvId: '3',cZone: \"www.38north.org\",cType: 'managed',cNounce: '90022',cRay: '878d054839b8819f',cHash: 'e54bf248d239395',cUPMDTk: \"\\/2024\\/04\\/what-we-learned-inside-a-north-korean-internet-server-how-well-do-you-know-your-partners\\/?__cf_chl_tk=x8b8UlYv1_aB3u6asq6xoq__r1YN.Pyq7DBw5cn09qo-1713866574-0.0.1.1-1749\",cFPWv: 'b',cTTimeMs: '1000',cMTimeMs: '375000',cTplV: 5,cTplB: 'cf',cK: \"visitor-time\",fa: \"\\/2024\\/04\\/what-we-learned-inside-a-north-korean-internet-server-how-well-do-you-know-your-partners\\/?__cf_chl_f_tk=x8b8UlYv1_aB3u6asq6xoq__r1YN.Pyq7DBw5cn09qo-1713866574-0.0.1.1-1749\",md: \"7vOxAbcKqzZrpJiWaDU4jbPjwLM0WuR6IRz3yZxL470-1713866574-1.1.1.1-EdhYvMKF9P2dbgjIh8l6Kdheg0BYyaAkRMlSd5EbXeMpK8zV7Zkf_6WtzZf5W7yKV9DSyZZxOax5IZSz6h8n_8_9Sv54wt_8LJo_3EEkTHuBMC1tkrXxov.S8pjBBomdSJxfMQcxweRp_YAzXucW8bpMaGK_WXZ7jk2P18U2iDDK4FgBE7A86.Iakox0CWeh6cVr7LUFHRA1fNnGx8Smj2HHZ.pq4Ue_annDvVy_d54a7hS6dbvGfoazGQPFsjXuiPU6H4uEmtlajbvGaWTh_fxehCZzAqraTbEsBfDJ1iiVGZQtsnFrApfb4BLTMa5hyE11_uAJ._e0tmmicMCdCGSCvkAWA.G4xhLWTD.VpeMiBwkDjX3dFjMw8BksiEQ1D4Q0p5qou6VIBGai38dq1IUOpuXE5BTMyVuMlyq2QphuIuiqJ9Vzqn3TBR0n.12jszcvPoLRnVY6iYWJG982uqty84sImmjltVNuzNwYTTG5wsWnhjX3vBzX9ErfqiY7ecP3bNRuWnbMPF6Lo0PJUDIiT07zwS3RtnwroIlZ9EqYPovVtnyYfzrvM3lDMYC_H6_ZbtRIR2C5xTsw9hI26qFw.YVHEw.Qo.dB0KaJexVyuvFgpri.Wq27aDGGph3EkwORcrVcJvt4qOYqaPcOXlL1xk8sr1h5n9Ub4h9nJh1fLlDe_pyaNlIC0XdNgG4MqcOzVLQEfvF_TNqhzlNlbVRxG6DvbajpJbqaymn4ZexcW.JwdqNqkk5LkJyNtGJBTTXbd5b47n.5sBUcJrE7LbJP6kUVWhck1SPkOWnS5xetc94RKiTuP4LqcJGBuevfx0T.F9r6my2Rieix1JVD6.v4kjjZIXd0.JCNWOEia4XAC0tPkQzxp7zdmJbC2WomDAZpv28ZLeEqoKtjjF8H5IBXYix3me5z8JGxWGkUrw1ocTnZqEOILAkAW5KXBAdC9o4MZTAtz700byZ.LORSZSFtZKT_sksb2PyJIH.xhmex6Br5X_5Ls941q5GRvW48hzaMbsUivBqHN5JtcxIa..2jmEAVRia9fZfIx97PQVOxH_XRUlLKgAJVZdsqOZ9QYtIkiz0DPRQZ7xf7MwRw5Mta1D9GUchpM138JFQ.MSwvaVzzab_R8rgwY6r2s0ODRD0dY4iTtgm6_L5gMcUCodlFLsgECU.fD0i4vu1yrTY2vazLfQghXo9ii35gU3FZAk9zy_Kqn6pjO7S.DTCFW8taweJtpRigAsxdDBbuhVuVzwEDmNX3_xUruHwYwVZiEKsbKEIOZWCC6MLFkkUonAMHpOITDkC8FbLNWLuCDt6W2cmrQQIDG7iYMyo09h7Ayvm3o.zpGuFE.VnpUbJZAOFenTd4l.1_i7D2U4tfDPq4snVShPvBOJoHBGaQUMPnFOPsm9643SkoiaPz6hKbgkQflbRcU_WCb_dEZsPRtuSTIyioWhBvcld1uTSzvxmBjoukHs_GyTu8muH9RPkI_U.NUcEKDmeU0047PIHPan0VSPK_0NSP0UnxwdC2iLbR8WOvuFk8rqzltPhf535UDwHSek99UwcjeiwP0w._DAIIFhdVz0nvYJawIr18KS0nNKUZOLc_2ApiOS5NhC1v_2ttbhjhHsI_ULHNsAu8BQrIutJ4gf3S3nbygDAC_14dSJFDiq8Nnx_xunvwS38mJQ\",mdrd: \"rn4OGI2t6RkvkOzN2wUivDhWzhDYFBkgeTBuH4kLhJw-1713866574-1.1.1.1-bZyjfn3lEZPUWx4wE7nX.oJgpUxOSwxiyumo4VJlvoxLFrI8LNwLM38mI0o0taYNDDVehVXngMKCI8UKVs7PB3gAceRkqxYd6KM72rYYrPsutFZXrkKpKo7Lg_AHEqQU8wzvCq5jkOCBkhgx.u9.dplNAsH8VVg3Pn7aLP1_vjeo7FfzmfX0e2CMX8.6LCNLBdzoo5RgJ0Vxh.tcwD_Q_5M3yUIJJk2U8O23JhXmXEGR7LA5acExgrLpg1feTofp6XJOLf7MPfZ26GKDwmWfO0flzKZobxPr6WDPJC2coPu3jLecoj5YoWz3SZsNTAfhz3C6QqpBubFFNfYhFE44EQoiZRR3eVVGPG1RFcOj3WPZ4hakKwutk0U_0joY8shmhyMsd_xx4PsNfy2nhmkcm69cUW_8qhUASdGXvPH8CGP7nQvkDHuyELlxzMtXFN9VwxwNcIxN1oMs3Zx3Qcm4SicKEZh8j75iTKDJUcCXDNIME.Bup7FRb8BSXL_4L.3zeuT.sDdOTFC3eHfMPTgIe3634pPanBflFAzZEf.InCnFdpNC8hxHCCurcRZmT9LqMVhQ1L107zf5_ob8yRfrACGkqQ1QNN0PQuW3T.gJrX28td6TcL2lVKO.dQYnatxdWurK3RCbzmQsO4IyvV00hjytC0w7q.eByb5HcemMy8RjdPXD.itoJ3OwXIYDwgMlxVEcVNAjd_TTQmf07NgZyuGbZZZO8Epc99vaaUQgr_h5nhgtKUIskzGhZumWcuJ5VsIgtntwCEb_Uw_XC_r4rjqd42oazgP__2wGUaO1SoxUQY0K6Dcv4Lp._CMlBSyOaR8PIQU7_MueGCW_P2dpXdKHTq6BTd_2YPcJaofiFIBZoHawhxHL.szCKduGSZvyKOOyyn5HsDOW2KniYhgtCQXExL9ajkYuwed9FD1YKqstsDkw0Hm9fd0s39MXxUoAckb_BGfAsgZWXjWm_EifoFEdhh9jTumQvdYRDdU9qjVM8zbWEGQVVp2PJqR8MZpcNVYi0rgzPUph7YtlW0g7dxX5h9L8dY97jYu9aizZtDyDzJtJZppqbMMD8sosvAXgtJCSEyNdLn1fngyYluqXxbl5keu1j4xhOfX1FgGz37e0zh0TCbW9YPg9Qaj8D8OSUUzTOsNbdqi0fUuVVhqASpj5Xa7zCoz8mRPhiUUphs1.7egsd54wwVK.Nkoe5SQiQPuuHwj2PRfiHrBE0u43VFs1DY.4kcnz4AnBzIFbtzW.7mkKL4V6D_HkDfVYLb5lqddF3WQHSBaz_if5ivnHrkwcGH50DZsazHsAR9E0Z.390qKZ43NRnuFy0aOR0c6mG4yKmexvxlDSVMScGthwPLoSsyx8tM4XlF0Yichb9PETnIvdc_gHO0Q6raLKIgtHhYzwoYOY2IySbpHVrmpz4ndib2X2X0Bix_F4a42Q05IgzSVd9RoYsn1cQDrAKxANMo8jroPArb3UzaywuR1SddOF.JILtyODf2w5R9BacKz0bVauL4TgvOXNLqYwgFQ9A8SgicWtJQyLlYp9srkKd7GrapugR.8EmbUjWovw5hrH3K0kj5pHAPr9sjQzJulMt_uukZT3bO1CfmooyjLGSfJf9qBoNVvhz6606a541nVVKurL_SzDUqyqMR2WUnLxEaAOqpfwVBLTLwk1S3QpPTaunmBGLyTN4JIfvdS6D5OYjk_LTcGfxzoR0g0FlOv6mGUfdDUGmrdTYyoi9R0hbnHmaHIBC07wx2swRnUT89enNAReO6gtgVivOtbGe.fgKdiApWbnbylL1w_idtWPEO0HfIEMLZI3W6bc60bzGIES.fwbyWnV21Uubb4bR5ZVhvpRKFeQ3AliECURvtyYoJFeobXyS38gqt2RRJKV1HUw863dbnw66i.sZnK_bVVUm2QO4ICjDUG3.UxHDFEkMvAb0qpLtex4xW4zALWOUfpE6KHDagHM5RnXlQrcSiG2rusi6tfEprH2CxOBlm_MNMNcHtvGtxNsBDZ.keXcvaCPQNjdsr1Jtz8gtAlqmv1WqNF2kBEXsd8balkSW9En96hvlQcBsmoJTMdCwZ2XBkP_TDEre7opAQG2fAGN2RO8dP7677srrt58DQ2sn.mWpiSBihKUbniUBS_lPBGuIWYS2wIGMS3NeEY5S5l0WzmaClDf3mCUrHo_I5K7HrB.lwG7S4eGrarH_rADNaQkOsH9IDVlNyU.KAPPuJmSSsYowCqFGP8RH9DV9LgFNryYnCmoF7gy6FW7oLkXYPG2DiFmElQ9.RwXfLLMGO1fmFNm\",cRq: {ru: 'aHR0cHM6Ly93d3cuMzhub3J0aC5vcmcvMjAyNC8wNC93aGF0LXdlLWxlYXJuZWQtaW5zaWRlLWEtbm9ydGgta29yZWFuLWludGVybmV0LXNlcnZlci1ob3ctd2VsbC1kby15b3Uta25vdy15b3VyLXBhcnRuZXJzLw==',ra: 'TW96aWxsYS81LjAgKGNvbXBhdGlibGU7IEdvb2dsZWJvdC8yLjE7ICtodHRwOi8vd3d3Lmdvb2dsZS5jb20vYm90Lmh0bWwp',rm: 'R0VU',d: 'wSzGZYCWNS+7Cr9Kw2L7JgLMTPs8w7JjphMhJq9NI3sTUnN7agBFEJayJWv6smKfFFnh3UPrNrR0W6oze+ESIlYS/lNKFAIjtDUfrzjalREk3+bUI54R0UQWZppbDLnjAkZ66TWWs5nQn5C5DLn4tYhEtZHZd5bi5WwnaZmRMv/u8Vkg06HbXHwqihsPu7vMke2YkfZttj8g6veBhv9sdNtbi26A3QJq6rEbw34prc43xyj+i8XWnatUAr2hhfp6IPSpA8I2XHouarGSqE3/LlGUpXjwQKF4vO96xUuqMsebgGO8mSnT/QtICZLDMX9Pt9tbZdffAvg2yQ9wxHrRcRtolWOU9ErERiUIW+RcuPUfXgYh+XBt+DIuMz3bGRoDvUKywSYE2XErFiVCgrBx9wLPanYrl/gaQ0D0qXw0Ka8xLaVKDNhjJkTtA4qhAs1OORwYgyBJufKVGwBfcfNNuY6rvJY3rzw6BMhfvApkU3TfdhMB5ok4fF2I0jnaHo+nLoX9GpPhSg0N7/R4nqqZWAaaEopp+glKfLBVKU2AEMg=',t: 'MTcxMzg2NjU3NC4xMTUwMDA=',cT: Math.floor(Date.now() / 1000),m: 'MxPh/BZWo8vaIxBwF/S/DGNB6u0KNWITyZA956qH5oM=',i1: 'W/gn4WQHbza/3QourE4EYQ==',i2: 'rXk7CosDsj06PK5FzDKR8w==',zh: '7d/SCcE9mnVnev5Wr7Scyr+yALZyyI9MJJVhU/ovUCw=',uh: 'idqvltDEaw6z1eUpAaUFY/6rIUCphTJo6GMHGHVnQbg=',hh: 't7TyuUidhV5ERJ6KDwWwA9xq6a2NfXGMErG87YLgaHo=',}};var cpo = document.createElement('script');cpo.src = '/cdn-cgi/challenge-platform/h/b/orchestrate/chl_page/v1?ray=878d054839b8819f';window._cf_chl_opt.cOgUHash = location.hash === '' && location.href.indexOf('#') !== -1 ? '#' : location.hash;window._cf_chl_opt.cOgUQuery = location.search === '' && location.href.slice(0, location.href.length - window._cf_chl_opt.cOgUHash.length).indexOf('?') !== -1 ? '?' : location.search;if (window.history && window.history.replaceState) {var ogU = location.pathname + window._cf_chl_opt.cOgUQuery + window._cf_chl_opt.cOgUHash;history.replaceState(null, null, \"\\/2024\\/04\\/what-we-learned-inside-a-north-korean-internet-server-how-well-do-you-know-your-partners\\/?__cf_chl_rt_tk=x8b8UlYv1_aB3u6asq6xoq__r1YN.Pyq7DBw5cn09qo-1713866574-0.0.1.1-1749\" + window._cf_chl_opt.cOgUHash);cpo.onload = function() {history.replaceState(null, null, ogU);}}document.getElementsByTagName('head')[0].appendChild(cpo);}());",
    "commentLink": "https://news.ycombinator.com/item?id=40117510",
    "commentBody": "North Korean animation outsourcing for Amazon, HBO Max series (38north.org)225 points by zdw 15 hours agohidepastfavorite154 comments karaterobot 14 hours agoThe HN headline is more clickbaitey than the original article, which has a different headline and goes out if its way to point out that Amazon and HBO are likely not the ones doing the outsourcing to North Korea. > There is no evidence to suggest that the companies identified in the images had any knowledge that a part of their project had been subcontracted to North Korean animators. In fact, as the editing comments on all the files, including those related to US-based animations, were written in Chinese, it is likely that the contracting arrangement was several steps downstream from the major producers. reply exhilaration 13 hours agoparentBut that's the most interesting part of the article - that Amazon and HBO projects are being worked on by NORTH KOREAN workers!!! I totally agree with the HN headline and I'm glad it got me to click and read the entire article. reply x0x0 12 hours agorootparentIt's also wild that it somehow makes financial sense to outsource a core input into your product. A company that makes animations outsourcing animation makes as much sense to me as a software company outsourcing engineering. Though we do have a plane company that outsources making planes, but that's going some sort of way right now... reply dannyphantom 11 hours agorootparentOooh I can try tackling a bit of this (I'll try not to ramble... ) But anyway studios have been outsourcing animation work to Asian countries for a while now. You might have seen some of the work product from one of the larger Japanese animation studios: TMS Enterainment who has worked on things like Batman: The Animated Series, Batman Beyond, The New Batman Adventures in addition to shows like Tiny Toon Adventures and Transformers in the 80-90s. Some more recent(ish) examples are Cartoon Network outsourcing the animation for Steven Universe to Rough Draft Korea located South Korea and Nickelodeon outsourced some of the work for Korra to a Japanese company called Pierrot. I'm just rambling at this point so I'm going to just leave a few links below that can do a better job of illustrating than I'm able to. Wikipedia, Outsourcing of Animation: https://en.wikipedia.org/wiki/Outsourcing_of_animation [DCAU Fandom Wiki, TMS Entertainment: https://dcau.fandom.com/wiki/TMS_Entertainment,_Ltd. Reddit thread, Stylistic differences between two studios: https://www.reddit.com/r/TheLastAirbender/comments/1m4wbn/st... AnimeNewsNetwork, American animation outsourced to Japan (2015): https://www.animenewsnetwork.com/answerman/2015-11-02/.94920 reply jhbadger 7 hours agorootparentHeck, The Simpsons has been largely animated in South Korea since the beginning over thirty years ago! They've even made jokes about that in the show itself. https://www.today.com/popculture/simpsons-rides-seoul-train-... reply XajniN 8 hours agorootparentprevI guess everyone remembers those strange Tom & Jerry episodes that felt like watching Twilight Zone. They were made in Czechoslovakia. https://www.awn.com/animationworld/tom-jerry-produced-prague reply bathtub365 11 hours agorootparentprevAnimation has been outsourced for decades, typically with in-house artists providing the key frames and outsourced animators doing the “in between” frames. https://en.m.wikipedia.org/wiki/Outsourcing_of_animation reply spamizbad 9 hours agorootparentprevFrom the prospective of these companies, the \"core input\" of the product is already in their hands: The intellectual property of the stories/characters. Media productions from this core IP is down the value chain. reply mitthrowaway2 10 hours agorootparentprevI can see how one company might focus only on developing its technical skill in animation, rather than other aspects that are important to a production (like writing, music, marketing, etc), and do contract work for anyone who wants to make an animated production, no matter the story / market / etc. Sort of like how TSMC mostly focuses on semiconductor manufacturing excellence and leaves the design to other companies, with only a small amount of its production used to manufacture its own component designs. But for studios where mastery of a certain animation style is critical to their work, vertical integration makes sense too. reply cnasc 10 hours agorootparentprevThis is fairly standard for mature products. In the early days the whole production process is tightly integrated, but as components and processes mature there is less room for there to be some “secret sauce.” Once that happens it makes sense to outsource it to other firms who compete on cost while you focus on parts where you can differentiate. That said, companies frequently misstep determining where they can safely outsource and where they must keep things integrated. reply laborcontract 11 hours agorootparentprevTo be fair, the outsourced tasks seem to be for edits, and not for original artwork. There’s a lot you can hide with outsourced software engineering. With animation it’s all on display. reply metalspoon 10 hours agorootparentprevSoftware companies don't outsource engineering in your country? reply x0x0 4 hours agorootparentAlmost never the entirety of it, no. back of office / LOB apps, sure. reply Dalewyn 9 hours agorootparentprevConsider that Intel, Nvidia, AMD, Qualcomm, and Micron, all manufacturers of semiconductors, outsource their semiconductor manufacturing. Consider that most game development studios outsource most development on a per-project basis. Consider that most car manufacturers outsource most of their parts manufacturing to other manufacturers. Basically: It's more unusual for a business to not be outsourcing their line of business. reply karaterobot 13 hours agorootparentprevIf the headline is factually wrong, it doesn't matter whether you agree with it or not. reply zdw 13 hours agorootparentOriginal title \"buried the lede\" and was too long for HN, so I edited it. It may be useful to see how others linked to this news - the original place I found this was at Engadget, which had this title: \"Some Amazon and Max cartoons may have been partially animated in North Korea\" https://www.engadget.com/some-amazon-and-max-cartoons-may-ha... Which linked to Reuters: \"North Koreans may have helped create Western cartoons, report says\" : https://www.reuters.com/world/asia-pacific/north-koreans-may... I linked the original report, which makes no mention of animation, but is the obvious focus of the article. The point isn't who did the outsourcing, but what was done, IMO. The length limit on HN titles sometimes makes nuance difficult - maybe I should have added \"may have been outsourced\" instead. I was not going for clickbait/misrepresentation. reply anigbrowl 13 hours agorootparentprevBut it's not. The original headline was just vague, and HN tends to ignore headlines that are too abstract or vague. reply constantcrying 13 hours agoparentprevI see nothing wrong with the headline. It accurately reflects what happened. That it was unintentional should be quite obvious, anything else would have been a major scandal. reply rangerelf 12 hours agoparentprevIt doesn't matter if it was intentional or not, the headline represents exactly what happened. reply wnevets 10 hours agoparentprev> he HN headline is more clickbaitey than the original article, which has a different headline and goes out if its way to point out that Amazon and HBO are likely not the ones doing the outsourcing to North Korea That is the beauty of the subcontractor shell game and one of the top reasons why companies do it, no one has to take responsibility. reply bcherry 12 hours agoparentprevthere's a difference between \"click bait\" (a misleading title specifically crafted to drive instinctual interest but which is not an accurate summation of the content) and titles accurately describing something truly interesting or surprising with more details inside. This is a case of the latter (and whatever the opposite of \"click bait\" is, that's how I'd describe the original title!) reply Waterluvian 10 hours agoparentprevI wish that all people who exist in the business world would learn to appreciate that levels of indirection do not absolve anyone of wrongdoing. The law might give them a pass but that doesn’t make them innocent. reply yourapostasy 9 hours agorootparentIt will take awhile before regulatory requirements catch up and leverage technology that is already available today, but automated lineage capture of Relationship Bill Of Materials (RBOM) is already possible. We lack the data interchange standards and compliance compulsion to reify. That is in turn possibly due to regulatory and legislative capture? Other than the fleets of white shoe law firms that would be deployed against passage of such regulation, I do not know why structuring laws are in place for \"trifling\" $10K USD transfers, and not for these larger, more economically devastating cases of indirection. reply geraldwhen 13 hours agoparentprevNot really. Outsourcing and then claiming ignorance is not a defense against financing North Korea. reply Arrath 13 hours agorootparentClassic 'two steps removed' syndrome. \"I told X to get it done. What X did to get it done is not my fault/responsibility.\" reply Yeul 13 hours agorootparentThe EU recently introduced a law to counter that. Companies are responsible for their entire production chain. reply godelski 12 hours agorootparentprevYou're oversimplifying the issue, which does nothing to bring us closer to a solution. If you tell X to get something done and ALSO give X rules that they must follow and X gets the thing done but __breaks the rules__ then this is different. It is also different if you have reasonable suspicion to expect them to break the rules or are using subversive language to tell them to break the rules. These are different things and have different consequences. If you are truely acting in good faith, then yes, it is a defense. But determining if that's true is not an easy task. There is good faith, coercion, negligence, and willful negligence. These are different things. reply yxwvut 12 hours agorootparentI'd go further to say that rules without any verification aren't really rules. You don't make a rule without the suspicion that it'd be more efficient to break them, and if you're not verifying their adherence to those rules, your rule is meaningless. This is the iterated game that morally bankrupt manufacturers (IE the vast majority) play to insulate themselves in these sort of scandals: - First, they get caught doing A,B,C, so they pass rules about A,B,C - Then they outsource to someone who is willing to do A,B,C, then they get caught outsourcing to violators - Then, they impose rules about A,B,C on these firms, but do no verification of the firms adherence to those rules. It insulates them of liability without ever increasing costs (because the firms still get to break the rules and the company gets to say \"I'm Shocked! I told you not to do that!\") reply godelski 12 hours agorootparentI'm more of the position of \"trust but verify.\" The verification process is exceptionally difficult. We're on HN and I think it should be rather common knowledge that attackers almost always beat defenders because the game is asymmetric. Attackers only need to find a single flaw while defenders need to find a large number of defenses. There is a huge difference in the resource expenditures between these two groups. This is related to the reasons why one single person can fuck shit up (e.g. a bad driver can impact tens of thousands of other drivers) but it is difficult for a single person to fix things. It is the nature of unstable equilibria. A society, of any form, depends on trust. Like it or not, there are no trustless systems available to us. Certainly not at any meaningful scale. This does not mean one should be negligent, but rather I'm saying that it isn't easy and the best intentioned can still be taken advantage of. We should recognize this and accommodate this fact when approaching solutions or we will end up with many undesired results. reply beaeglebeachh 13 hours agorootparentprevIf it's possible to accidentally pay north Korea that should be the fault of whatever financial institution (which is held to far more KYC and AML than animators) caused that to happen, presuming it went through the financial system. HBO likely had no mens rea beyond cash flow out animation flow in. If I can't trust the bank to know more about the UBO of some rando after KYC what the hell are we doing this for? reply acdha 9 hours agorootparentThat’s kind of the point: these companies are using outsourcing to avoid the expense of obligations they would have if they did things in house. It’s the same way we end up seeing workers being mistreated or placed in risky situations on behalf of major American companies who would never risk that in a domestic factory with their name on it. A good outcome of this would be more restrictions on outsourcing to China since it’s been repeatedly shown that they will not maintain meaningful restrictions on North Korea, Russia, Iran, etc.: > All three cities are known to have many North Korean-operated businesses and are main centers for North Korea’s IT workers who live overseas. reply beaeglebeachh 8 hours agorootparentHell don't stop there. You pay HBO for a subscription? Sanction violation, should have known, straight to jail. reply carlosjobim 13 hours agorootparentprevWhat do you mean with \"syndrome\"? Of course business customers can not be held responsible for what other businesses do. If Amazon buys floor tiles for their factories from a company that provides floor tiles to many other companies, how can Amazon be held responsible for any of their unethical practices? Or is it the faith of hackers that any misdoing in the world should always be traced back and blamed on a tech giant? From the article: \"There is no evidence to suggest that the companies identified in the images had any knowledge that a part of their project had been subcontracted to North Korean animators.\" If it's a subsidiary, that's another case. reply kalleboo 6 hours agorootparentIf Amazon sells me floor tiles full of asbestos, they should sure as hell be held responsible, they can't just absolve it with \"oh but we bought it from another company, go complain to them\" So why is it different if they sell me floor tiles made with slave labor? Global retailers like H&M and IKEA do do audits of their supply chain to try to stop labor abuse (although how effective they are at that is up to debate) reply s1artibartfast 13 hours agorootparentprevIt absolutely is a defense. We outsourced to company X, who is bound by law and contract not to engage in illegal actions. Company X, without our knowledge or approval subcontracted illegally. reply VS1999 11 hours agorootparentIf this is true, we just need to make companies liable for their subcontractors. They apparently know they can escape responsibility by farming out work they're not supposed to do. reply s1artibartfast 11 hours agorootparentI dont think it is worth it. Existing law already punishes companies operating in bad faith, and I would want companies operating in good faith to remain protected. The Amazon and HBO are the victims here. They were the ones harmed and deceived. reply VS1999 10 hours agorootparentI don't think they were victims or deceived. It's normal for companies to set metrics that can't be legitimately achieved and then closing their eyes and pretending they don't know what's going on. A waste disposal company in Canada had a quota for \"overflow fines\", which ended with employees overfilling dumpsters themselves. Call centers set unachievable metrics that require employees to redirect and mark issues as resolved instead of following protocol. Samsung recently had a contractor destroy customer property to void the warranty. Companies like amazon are extremely powerful. They set the terms of the agreement, they set the project requirements and metrics, and then pretend that whatever happens has nothing to do with them. reply s1artibartfast 10 hours agorootparentIf you are coming from the default position that companies inherently and always bad, I dont think we will be able to have a conversation about it. reply VS1999 9 hours agorootparentI'd be willing to give some companies the benefit of the doubt, but not amazon of all things. reply constantcrying 13 hours agorootparentprevIt is a defense. Even in a court of law not knowing about something can potentially protect you from punishment. It is just a part of the nature of outsourcing. Your supplier might just choose to outsource himself and have the work done by companies you couldn't possibly work with. For a digital good this is extremely hard to monitor. How would a Californian studio possibly know what company their supplier outsources to? reply geraldwhen 13 hours agorootparentDon’t outsource. If you outsource, have boots on the ground ensuring working conditions and no re outsourcing. Pick one. Pretty straightforward. reply godelski 12 hours agorootparent> Don’t outsource. If this were codified into law I would suspect that it would quickly lead to monopolization. I can't imagine how the world would work without contractors. Imagine if Apple had to operate the mines, build the machines to mine the materials, to build the machines to build those machines, build the silicon foundry, and all the things along from dirt in the ground to the iPhone. Boy, you'd get nothing done. Because that's the world with no \"outsourcing.\" It is too broad of a word. reply constantcrying 12 hours agorootparentSure, having suppliers is completely inevitable for any company. \"We only do X\" is one of the best tools to manage complexity. I think what the other poster is referring to is specifically the practice of companies to have work done by workers in areas of the world where labor is cheap. That particular practice is something very different than e.g. buying chips from a foundry. Specifically buying parts from q supplier involves very little management, outsourcing work means you have to actively involved in the management of the offshore labor. reply godelski 12 hours agorootparent> I think what the other poster is referring to is specifically the practice of companies to have work done by workers in areas of the world where labor is cheap. Sure, but part of what I wanted to (albeit indirectly) convey is the difficulty of creating a meaningful rule about this. Even if you locked work within a country's borders (I imagine this would have terrible consequences too), this concept scales. I understand the complaint, but I think we need to also recognize the complexity of the issue if we're to do anything meaningful about it. Trivializing it will get us nowhere and often leads to bad laws that get more abused than the original ones. reply constantcrying 12 hours agorootparentI agree, you actually always have to work with other companies. The alternative is not possible. In general it is very easy and safe to work with companies in the same country as you, as they are bound by identical laws to you and litigation and control is relatively easy. They also can't legally re-outsource to companies you can't outsource to. Similar things are true if the company is in a broadly aligned country. E.g. the US and Germany. The further away, physically, ideologically, linguistically and legally away the country of the other company is the worse it becomes and the harder it is to effectively control them. reply godelski 11 hours agorootparent> The further away Yeah, I agree that this is definitely a weighting factor. But neither do I think it is good to return to isolationism. Globalism, despite its many flaws, has clearly contributed to the long peace. Encourages negotiations at the table rather than on the battlefield given that in the end, wars are primarily economically driven. Better to destroy economies than people, even if the former can indirectly result in the latter. (wish there were better solutions and a larger gain, but that's a whole other conversation fraught with far more complexity) reply CaptainOfCoit 11 hours agorootparentWhat direct evidence exists to say that globalism clearly contribute to long peace? As far as I know, there is only proof of globalism failing to contribute to peace, where invading nations continue their invasions even after heavy sanctions. reply godelski 8 hours agorootparentI mean what constitutes \"proof\" for you? Does it need to be void of abstraction and indirect causal chains? Because if so, good luck. If you allow for this I think there's plenty of strong evidence for it and many others have written about this. I mean we see these same trends through scale and history, so it is not as far of a stretch as I think many of those that are quick to dismiss the hypothesis claim. Absolutely nothing in our modern world will be adequately explained without abstraction and consideration of complex interacting chains (this even includes the conflicts in Ukraine and Gaza. While there's often claimed a clear aggressor the truth is that aggressions have been taken by all parties over a long time. To be clear, I'm not saying any specific party is justified in their actions nor am I saying all parties are equal. But the ambiguity is simply because I don't want to start a long complex off topic thread over these current hot topics, not because of a lack of alliance). But given your response I think there maybe confusion as to what is being referred to when discussing \"Long Peace.\" It is not the absence of war, it is the absence of __direct__ war between __major powers__. This may not be clear since being born into long peace (as presumably you arebut among those that were not VPN-related was an IP address in Spain and three in China I wonder if the IP in Spain was related to Alejandro Cao de Benós https://en.m.wikipedia.org/wiki/Alejandro_Cao_de_Ben%C3%B3s reply LarsDu88 14 hours agoprevWhen I saw the title, I immediately thought of the show \"Invincible\" which had abysmal animation, and lo and behold, its right there are the top of the list. reply Rinzler89 13 hours agoparentOur of curiosity why did you think Invincible had abysmal animation? Which is ironic since in the last seasons they even broke the fourth wall and did a tongue in cheek poke at their audience who criticize their animation quality explaining how they're under crunch and what techniques they use to cut corners. Quite clever actually. Didn't think they were covering up their North Korean animators though lol. reply jsheard 13 hours agorootparentThey didn't do themselves any favors by putting out a nicely animated teaser for season 2, which was made by a studio that otherwise didn't work on season 2 at all (they were busy animating Captain Laserhawk for Netflix) https://www.youtube.com/watch?v=vjDOpHuUppU reply Rapzid 11 hours agorootparentI mean, that shaky cam execution is not so great. It feels super unnatural. reply LarsDu88 12 hours agorootparentprevGreat writing. Great voice acting. Why the hell are all the non-pivotal scenes barely animated? Like in one episode an alien spaceship blows up and a static gif of an alien goes spinning around. Just compare that to the animation in Xmen '97 which has a similar episode count. reply vundercind 13 hours agorootparentprevI’ve only seen season 1 but it’s barely even animation. It looks like “motion comics”. I enjoyed it anyway (I liked the comic, and this is playing out like a “second draft” with some stuff tightened up, so that’s really cool to see) but it’s one of the worst-looking animated anythings I’ve seen. It’s on par with the bottom half of amateur Flash animation in the ‘00s. reply booleandilemma 10 hours agoparentprevI felt that way watching season 1 but I was drawn in by the great story regardless. reply constantcrying 13 hours agoprevIf you are outsourcing over the entire globe things like this tend to happen. There is very little you can actually do to verify how the work you require really gets done, especially if the format is purely digital and there is no physical process you could monitor. Companies building things in China have been caught in that trap multiple times and somewhere down the line the work was allegedly done by highly mistreated populations, potentially in slave like conditions. Certainly no company would want this to happen, as it is obviously a major PR disaster, but it is just very hard to oversee. reply iaaan 12 hours agoparentIt's really easy though, actually, isn't it? Just don't toss problems over the fence to China. i.e. don't outsource things. reply Bayart 10 hours agorootparent> i.e. don't outsource things. i.e. don't be efficient. Outsourcing always gets a bad reputation, but holistically it's the best way for the greatest number. reply constantcrying 12 hours agorootparentprevSure. You can avoid all of this by doing the work in house or even just working with companies bound by the same laws as you are. Obviously management doesn't see it your way though. reply s1artibartfast 11 hours agorootparentprevthat is indeed an easy solution, but that doesn't mean it is a good solution, or even better than no solution. reply acdha 8 hours agoparentprev> If you are outsourcing over the entire globe things like this tend to happen. There is very little you can actually do to verify how the work you require really gets done There’s very little you can do at minimal cost. They could have on-site auditors or collocated employees, but that cuts into the margins. That doesn’t mean it’s a bad thing: each poorly-monitored subcontractor is a security risk and likely hiding labor law or environmental violations so an outcome of less outsourcing to China would be a net-win. reply peppertree 14 hours agoprevThe real question is does North Korean let their animators work remote in a LOCL area. reply hawkice 14 hours agoparentNot sure \"cost of living\" exists in NK the same way it does in the rest of the world. reply gwbas1c 12 hours agoparentprevThe economics of where you live are very different in a communist (Or whatever NK calls itself now) state. I would assume that the people with these jobs don't have an American-style commute. reply beaeglebeachh 14 hours agoparentprevHonestly if N Korea had a cheap/low-barrier remote visa it might be attractive. I would imagine having an oppressive authoritarian regime looking at you as a prime tax slave might mean none of the prols would risk getting their head chopped off to mess with you. Meanwhile labor/rent/food gotta be hella cheap. reply RajT88 13 hours agorootparentLet's game it out. You go there to work remotely. Food/labor/rest are super cheap. Everyone is starving to death there, to the point where meth is casually used by most people to stave off hunger pangs. Not to mention, if they decide they may just get some fun leverage out of a foreign hostage, they may just decide to claim you committed a crime and beat you half (or 3/4's) to death. This is just the news stories I can recall off the top of my head as well... reply booleandilemma 9 hours agorootparentDo you really believe that everyone over there is starving? reply RajT88 9 hours agorootparentNot everyone all the time of course. There would just be barren earth instead of a country. The food supply is prioritized for the military. Food insecurity is frequent for civilians. A pattern we have seen play out many times: NK makes a threat of some sort. The West and its allies try to talk them down. As part of negotiations they demand food. Sometimes they get it. If a nation demanding food not to blow up its neighbor is not dealing with food insecurity in its society at large I will eat my hat. reply krisoft 13 hours agorootparentprevHard to look at the Otto Warmbier story and say “hey, you know what? I wish i could also be there” reply beaeglebeachh 12 hours agorootparentI see it as a much worse version of Dubai. The economic arbitrage version of fishing for king crab in a lethal bering Sea. Not saying I'd do it. But it might be attractive to the right person. reply bobthepanda 13 hours agorootparentprevNorth Korea is no stranger to famine, so if you want food it's probably not the best place to go. Even for tourists who they are trying to extract lots of hard currency from, the food quality is notoriously poor. reply josephstalinOk 10 hours agorootparentWhat a strange thing to say. No country is stranger to famines. Nowadays the country has a normal and modern economy. reply ch4s3 13 hours agorootparentprevNorth Korea doesn't have a great history with respect to foreign \"guests\"[1]. Here's a choice quote: >The four lived together in a two-bedroom house outside of Pyongyang, where they were forced to study the writings of then-leader Kim Il Sung and were subject to regular beatings. They were also featured prominently in propaganda magazines and movies. [1] https://www.npr.org/2023/07/19/1188656665/travis-king-north-... reply beaeglebeachh 13 hours agorootparentTotally agree, but I'll point out they entered as technically enemy combatants, not with a visa. And dresnok said the opposite and retired fat and with alcohol cirrhotic liver and a nice stolen wife and downtown apartment, which is far more than he would've got in USA as such a lazy, stupid, criminal that he was. He even become a local celebrity as a movie star playing as a white devil. reply ch4s3 8 hours agorootparentIt’s not like people are regularly entering as anything other than defectors and the one guys wife said she was tricked into going there and held against her will. reply beaeglebeachh 7 hours agorootparentYes \"stolen\" wife. Great for dresnok as his worst fear was broken homes (he came from one and experienced divorce) so the held against will was a huge selling point for him. From watching docs in his final days I sincerely believe the stolen wife who couldn't divorce was his biggest selling point in staying. reply otikik 13 hours agorootparentprevWow you might want to document yourself a little bit. reply numpad0 11 hours agorootparentprevI kind of see the idea but... I don't think it's worth the risk. Authority figures in dictatorships just aren't always rational. They're by definition not capitalists anyway. reply hobs 13 hours agorootparentprevMost North Koreans face starvation on a regular basis, and being excited about the prospect of having a cowed population that serves you is either sociopathic or psychopathic, or both. Seek some help. All of that being said that when those in power think you made a single mistake, you're dead. reply dangerboysteve 13 hours agoprevLet's not forget the use of pirated software to produce the animations. reply tumsfestival 11 hours agoparentOh no, not the pirated software!! reply 1123581321 14 hours agoprevGenerally, how far downstream does the US State Department expect companies to vet vendors for sanctions violation? Due diligence this many layers deep is expensive, especially if hostile (investigative work to proactively discover dishonest sourcing reports.) I would think it would vary by industry--e.g. animation is obviously less stringent than medical devices so would have fewer reporting and certification structures already set up. Does anyone have experience dealing with this? reply jsiepkes 14 hours agoparentSo all that's required would then just be to outsource it to a bunch of companies who then outsource it and then claim you have: \"no knowledge\"? You're the one outsourcing, so it's your responsibility. The entire chain. reply aembleton 1 hour agorootparentDo you have to check just for the product that you're selling or for the inputs into that product? Do you have to check that the tablets, keyboards and mice used by your employees and those of outsourcing companies were not made in North Korea? Do you have to check that the ink used on the keyboards weren't made in North Korea? Do you have to check that the coal for the power station for the keyboard factory didn't come from North Korea? reply 1123581321 14 hours agorootparentprevYes, of course, but I was asking what level of due diligence is expected to verify that the chain does not violate sanctions. In other words, when a problem like this is discovered, the US State Department will assign more blame to the company if their attempts to avoid violating sanctions fell below a threshold; what is that threshold for the arts industry. reply KptMarchewa 13 hours agorootparent> Yes, of course, but I was asking what level of due diligence is expected to verify that the chain does not violate sanctions. The same as you were employing them directly. reply toasterlovin 14 hours agorootparentprevYeah, but \"outsource it to a bunch of companies who then outsource it\" is literally how the economy works. The most mundane product you can imagine has a network of upstream suppliers that is essentially incomprehensible in its complexity. reply mhsred5 13 hours agorootparentAt some point maybe stop outsourcing and just do the work. Boeing used to make airplanes. Now they outsource the work of \"make the airplanes\" and all it cost them was their reputation. Less outsourcing, more just doing the work please. reply toasterlovin 12 hours agorootparentEven when you “stop outsourcing and just do the work”, you’re just subtracting a ~4 bit integer from a ~16 bit integer. reply johngladtj 12 hours agorootparentprevOk, so you expect the airline to mine the ore to make the tools needed to mine the ore needed to make the aluminum used in the packaging of the snacks they give out on board? Think about exactly where this ends. reply jsiepkes 12 hours agorootparentThere is a difference between ordering specialist work (i.e. someone makes something to your exact specifications) and buying ore on a global market. But sure, even in case of ore you have a responsibility to make sure it isn't being delved by slave labour. reply colechristensen 14 hours agoparentprevThe entire chain all the way down is sanctioned, the US can and will climb up and down that chain to punish sanctions violations. A company like Disney will have to have a Sanctions Compliance Program and like any other compliance regime, there are standards, external auditors, etc. to make sure enough is being done, and \"enough\" can be a bit of a moving target. If you get caught having sanctioned suppliers, those standards and auditing get kicked up a notch, if you did a really bad job maybe fines or criminal charges. There isn't ever a sense of \"I'm doing enough and therefore the sanctions violations happening are no longer my fault\". It's somewhat up to you to determine your risk and tailor your compliance program to address them and to adjust if you're ever wrong. How guilty you are is a function of how good a job the state department thinks you're doing trying to avoid sanctions violations. reply 10000truths 13 hours agorootparent> How guilty you are is a function of how good a job the state department thinks you're doing trying to avoid sanctions violations. Is the standard for this codified in clear language anywhere, or is it merely based on the whims of some federal prosecutor/judge? If I make digital watches, and I buy coin cell batteries from a supplier who buys battery precursors from a supplier who buys LiCoO2 from a manufacturer who buys lithium-rich brine from a supplier who buys lithium mining equipment from a sanctioned entity, how much of the full brunt of Uncle Sam's retribution can I expect to come crashing down on me? reply beaeglebeachh 13 hours agorootparentFeds have 99+% conviction rate and infinite money and time, meanwhile you sit in cage and deal with frozen accounts while trying to pay your attorney. They imprisoned weev for doing arithmetic on wget'ing a public website. Worst sin is angering the gods. I would imagine most the time theyll probably just ask nicely for you to stop, then bury you if you don't, but for political or convenient targets they seem fine going straight for the throat. reply acdha 8 hours agorootparentThe hyperbole isn’t helping your argument here. Weev spent time brute-forcing collection of personal information which they discussed using for phishing, manipulating AT&T’s stock price, and potentially shorting the stock: https://arstechnica.com/gadgets/2011/01/goatse-security-trol... They were too bumbling to actually do anything that serious but being too broke to run a stock scam isn’t exactly a great character testimonial, and all of that is well outside of professional ethical boundaries in the infosec community. reply beaeglebeachh 8 hours agorootparentIts not hyperbole, you're just using misguided rhetoric of the prosecution. Weev has escaped to Ukraine in any case, those same bumbling idiots making your argument had their conviction dropped on appeal. reply acdha 8 hours agorootparentHis appeal was decided on a technicality over where the case was brought: https://arstechnica.com/tech-policy/2014/04/appeals-court-re... It’s certainly possible that a court might have found his actions did not violate the relevant laws, but that was not the case in the first trial where he had full opportunity to challenge the prosecution’s claims. You might characterize that as “misguided rhetoric” but that should be your cue to ask whether it’s really true that you understand the situation better than his real legal team. reply beaeglebeachh 8 hours agorootparentThe \"real\" prosecuting legal team didn't even prosecute in the right venue. That's how far off the mark they were. reply colechristensen 8 hours agorootparentprevThe main reason the Feds’ conviction rate is so high is they’re lazy cowards who only try cases that are extremely easy to convict. It’s not that they do nothing but there are shitty incentives in place to only put cases in front of judges that are certain. reply blackhawkC17 13 hours agorootparentprevMore based on the whims of a prosecutor. Many small companies violate sanctions (usually unknowingly) and don't get prosecuted. But stick out too much, and you'll likely get hammered. reply 1123581321 11 hours agorootparentprevThanks; this is helpful. Looking up what sanctions compliance/export control professionals proactively do yields a ton of additional information. reply thriftwy 13 hours agorootparentprevQuite contrary I would say. An American megacorp will always know how much profit from the US sanctions it can do and get away with it. Sometimes a slap on the wrist can happen, but in general... You do want those campaign donations flowing, do you? Then there is no reason to rock the boat. Smaller companies from other countries may not be so lucky so they may actually refrain from such activities. reply tiahura 14 hours agoprevSo it looks like Season 3 of Invincible is a go. reply candiddevmike 14 hours agoparentIt's a little ironic considering part of season 2 has some meta commentary on how hard it is to create animated shows. Guess the easy way is to indirectly outsource it to NK! reply GordonS 12 hours agorootparentAnd even more ironic given it was the story that made season 2 such a disappointing bore-fest - the animation was superb! reply epiccoleman 10 hours agorootparentThat season definitely was a little slow, but it left me wanting more, so I read through the comics. Based on the quality of what the show has done so far and of the source material I'm still excited for the next season whenever it finally comes reply the_real_cher 11 hours agoprevWhy is the USA sanctioning the citizens of North Korea? It's not like theyre happy about a dictator running the country doing international crazy stuff and oppressing them. I feel like we should be supporting any capitalistic effort they make so that they can build up resources to combat their dictator from within. reply VS1999 11 hours agoparentThis is normal behavior. We feed our allies and starve our enemies. reply unobatbayar 10 hours agorootparentMore like invading, and starving innocent countries under pretext. The enemies are consuming more than ever before in history. reply johnea 14 hours agoprevIsn't North Korea working on something like cartoon animation, a good thing? reply loudmax 13 hours agoparentThese aren't North Korean entrepreneurs building businesses in a free market. Any enterprise in North Korea should be understood as slave labor to provide support for a criminal regime. reply johnea 12 hours agorootparentIsn't that the same thing as working for a publicly traded company? reply ses1984 14 hours agoparentprevMaybe, but what if it’s basically slave labor and the money goes to the regime? reply fy20 8 hours agorootparentAre you describing Europe? Our taxes definitely feel like they are going to the \"regime\", making politicians and the wealthy richer, not the everyday worker. reply markus_zhang 14 hours agoparentprevBut then they get paid. We don't want them get paid. reply evan_ 14 hours agorootparentand to clarify- the \"they\" who gets paid is probably not the person doing the animating reply markus_zhang 14 hours agorootparentYeah the majority of the $$$ probably goes to we know who. reply micromacrofoot 14 hours agoparentprevIt depends on who is defining \"good\" reply godelski 14 hours agoprevSeems like a difficult problem to solve. It shows the importance of paying close attention. NK has shown to be quite good at bypassing sanctions but it seems that the link is almost always through China. It would seem that the best way to go about this would be through stricter negotiations with them, since they are already acting as a significant intermediary. Either they know about this or the great firewall is not so great (I suspect a bit of both). Edit: Interesting to see that this particular thread is getting heavy traffic and attacked. I'm not sure I've seen this happen on HN before, at least not a front page post. @Dang, I guess we can add a signup filter to prevent similar usernames being generated within a timeframe, since presumably these come from different IPs. Should be a simple regex filter and provide some warning system? Anyone else know? https://i.imgur.com/ngexngJ.png reply noodlesUK 13 hours agoparentIt’s certainly interesting seeing the thread get attacked so obviously - that’s a first as far as I’ve seen on HN. In terms of the actual story I think that we should be careful not to introduce insane KYC for contractors just to avoid the NK boogeyman. If such measures were introduced, that would seriously restrict the ability to work with people from around the world. I also fear that scammy companies such as id.me will lobby for such measures in order to extract profit from companies who want to contract abroad, all the while not actually stopping sophisticated threat actors. reply godelski 13 hours agorootparentYeah that is something I'd be worried about as a potential \"solution.\" It should not involve placing spyware on contractor's systems. And it should not involve bureaucracy dependency hell either. reply anigbrowl 12 hours agoparentprevEdit: Interesting to see that this particular thread is getting heavy traffic and attacked. It isn't, the spam is spread across multiple front page stories. There might be some IP address rotation but I'm not sure why it's allowed to get through when it would be so easy to filter. reply koito17 14 hours agoparentprev@dang is a no-op. You should send an e-mail to hn@ycombinator.com. I'm writing one as I write this comment. reply godelski 14 hours agorootparent@dang is a signifier that makes it easier for dang to visually see his name (or search and differentiate from the more common word) in comments. I was writing an email but I won't send if you got this covered. Thanks reply pvg 13 hours agorootparentis a signifier that makes it easier for dang to visually see his name It isn't. That's been explained in many threads of his comments, I feel reasonably sure some as previous replies to you. reply godelski 13 hours agorootparent> It isn't. Well __I__ can visually distinguish a username more easily with @ in front of it. Just the same way as I, and many others, use various typographical marks to indicate various things. It does also make a *manual* thread search easier. I feel reasonably confident that the vast majority of people doing this are not expecting @dang to be pinged, but are just using it either due to habit and/or a visual indicator. Either way, I'm not sure why this is such a big deal and worth more than a single exchange. Potentially someone doesn't know, it is okay to inform them, but after \"I know\" or \"I didn't know\" there is no more to be said. reply pvg 13 hours agorootparentIt's not that big of a deal, the main problem with it is people assume this is actually a way to get moderator attention for something. It's great that you don't but plenty of users don't know that nor are they aware of the reliable method of emailing hn@ycombinator.com. The other, probably more important reason not to do it is that it gums up threads with pointless meta which runs against the site conventions. If a comment starts with @dang, it probably doesn't belong in the thread. Just like that meeting, it could have been an email. reply godelski 12 hours agorootparent> The other, probably more important reason not to do it is that it gums up threads with pointless meta which runs against the site conventions. It seems like we are complaining about the same issue. Again, why does this conversation exist since it has clearly been established that I am aware and that anyone reading is aware. If you got a problem with how I use typographical indicators, sorry, I'm going to keep doing it. You can keep starting these metas if you want, but it seems hypocritical to me. I'll just stop responding to prevent more metas, because I've been given no indication that anyone thinks it actually pings @dang other than people who get upset at people using \"@\". Seems like a classic assumption, where people try to solve a problem that doesn't exist (or exists in a very small percentage). And as you can read, I did not start with @dang. It was an edit, and into the edit. And as you can read, I was going to send an email but then saw several users note they did, so wish to not spam the email any more. I think we're done here and have derailed the thread enough. I don't think anyone's opinion is changing, and that's perfectly fine. reply pvg 12 hours agorootparentthat anyone reading is aware That's the thing, they aren't. I've been given no indication that anyone thinks it actually pings @dang You can find lots of comments by people who think that and replies by dang explaining it does nothing. The idea that we just have no clue what the effects of this are and why moderators think it is best avoided is just an odd one. The busybodies repeat this because the moderators do. Well, that and they're busybodies. I did not start with @dang. It doesn't matter, editing your comment to add meta is the thing that ends up derailing comments and threads. It's spamming your own comments, effectively - such comments are regularly moderated to the bottoms of threads. reply Arrath 13 hours agoparentprevTo be fair its not only this thread. reply icepat 13 hours agorootparentThe dodgy AI product spam attacks have been escalating recently, or so I've noticed. I don't think I've seen this with any other product class here. reply godelski 13 hours agorootparentI think I've only seen it once before and that too was on a politically contentious thread. Definitely is rare and I don't blame anyone for being suspicious given that the spam started quite quickly after this post was created (and how all comments got initially downvoted). Who is definitely within question, but without a doubt the HN sight is under attack and is getting significant traffic that is slowing it down. reply ChrisMarshallNY 14 hours agoprevSeems like this thread is being DDoSed (maybe by the Norks). > video interviews From what I hear, the live deepfakes are getting good enough to make these near worthless. reply vajrabum 14 hours agoprev@dang looks like we have a troll who's creating multiple accounts. reply pronoiac 14 hours agoparent@'s don't do anything here. I've emailed the mods. reply wesselbindt 14 hours agoprevThey were animating stuff? For money? The thing that hits me the most about this, I think, is the depravity of it all. reply tchbnl 13 hours agoparentThe issue is the money is piped into the NK regime. The problem isn't that a North Korean did the work, but that it was done by a state-owned firm. reply iamleppert 13 hours agoprev [–] This more than anything says why we need SORA AI now, and to replace these animators with AI. We cannot be allow to have reputable US firms engaging with a foreign adversary. Replacing these workers with AI should do the trick in this compliance snafu. reply tchbnl 13 hours agoparent [–] And of course here's the \"AI solves everything\" comment. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "The article explores outsourcing animation work to countries such as North Korea, South Korea, and Japan, emphasizing ethical concerns and challenges linked to subcontracting.",
      "Topics include responsibility, sanction compliance, monitoring supply chains, product quality impact, and risks tied to collaborating with nations like North Korea.",
      "The discussion extends to globalism's implications, pirate software usage, human rights issues, and suggestions to use AI technology instead of human animators."
    ],
    "points": 225,
    "commentCount": 154,
    "retryCount": 0,
    "time": 1713811860
  },
  {
    "id": 40116644,
    "title": "The Performance of C++'s `final` Keyword in Ray Tracing",
    "originLink": "https://16bpp.net/blog/post/the-performance-impact-of-cpp-final-keyword/",
    "originBody": "Mon Apr 22nd, 2024 – 08:25 AM EST The Performance Impact of C++'s `final` Keyword If you're writing C++, there's a good reason (maybe...) as to why you are. And probably, that reason is performance. So often when reading about the language you'll find all sorts of \"performance tips and tricks\" or \"do this instead because it's more efficient\". Sometimes you get a good explanation as to why you should. But more often than not, you won't find any hard numbers to back up that claim. I recently found a peculiar one, the final keyword. I'm a little ashamed I haven't learned about this one earlier. Multiple blog posts claim that it can improve performance(sorry for linking a Medium article). It almost seems like it's almost free, and for a very measly change. After reading you'll notice something interesting: no one posted any metrics. Zero. Nada. Zilch. It essentially is \"just trust me bro.\" Claims of performance improvements aren't worth salt unless you have the numbers to back it up. You also need to be able to reproduce the results. I've been guilty of this in the past (see a PR for Godot I made). Being a good little engineer with a high performance C++ pet project, I really wanted to validate this claim. I keep on finding myself unable to get away from my pandemic era distraction, PSRayTracing. But I think this is actually a VERY good candidate for testing final. It has many derived classes (implementing interfaces) and they are called millions of times in normal execution. Side note: when publishing this (April 22nd, 2024), GitHub's rendering of reStructuredText is broken. So if you want to look at the README, please use the GitLab mirror of this project. For the (many) of you who haven't been following this project, the quick and skinny on PSRayTracing: it's a ray tracer implemented in C++, derived from Peter Shirley's ray tracing minibooks. It serves mainly an academic purpose, but is modeled after my professional experiences writing C++. The goal is to show readers how you can (re)write C++ to be more performant, clean, and well structured. It has additions and improvements from Dr. Shirley's original code. One of the big features I have in it is the ability to toggle on and off changes from the book (via CMake), as well as being able to supply other options like random seeds, multi-core rendering. It is somewhere 4-5x faster than the original book code (single threaded). How This Was Done Leveraging the build system, I added an extra option to the CMakeLists.txt:option(WITH_FINAL \"Use the `final` specifier on derived classes (faster?)\" OFF)# ...if (WITH_FINAL)message(STATUS \"Using `final` spicifer (faster?)\")target_compile_definitions(PSRayTracing_StaticLibrary PUBLIC USE_FINAL)else()message(STATUS \"Turned off use of `final` (slower?)\")endif() view raw CMakeLists.txt hosted with ❤ by GitHub Then in C++ we can use (ab)use the pre processor to make a FINAL macro:#ifdef USE_FINAL#define FINAL final#else#define FINAL#endif view raw Common.hpp hosted with ❤ by GitHub And easily it can slapped onto any classes of interest:$ rg FINALRandomGenerator.hpp185:class RandomGenerator FINAL : public _GeneralizedRandomGenerator {Materials/Lambertian.hpp8:class Lambertian FINAL : public IMaterial {...Materials/SurfaceNormal.hpp7:class SurfaceNormal FINAL : public IMaterial {...PDFs/HittablePDF.hpp7:class HittablePDF FINAL : public IPDF {...Objects/Sphere.hpp19:class Sphere FINAL : public IHittable { view raw FINAL_being_used.cpp hosted with ❤ by GitHub Now, we can turn on & off the usage of final in our code base. Yes, it is very hacky and I am disgusted by this myself. I would never do this in an actual product, but it provides us a really nice way to apply the final keyword to the code and turn it on and off as we need it for the experiment. final was placed on just about every interface. In the architecture we have things such as IHittable, IMaterial, ITexture, etc. Take a look at the final scene from book two, we've got quite a few 10K+ virtual objects in this scenario: And alternatively, there are some scenes that don't have many (maybe 10): Initial Concerns: For PSRT, when testing something that can boost the performance, I first reach for the default scene book2::final. After applying final enabled the console reported: $ ./PSRayTracing -n 100 -j 2 Scene: book2::final_scene ... Render took 58.587 seconds But then reverting the change: $ ./PSRayTracing -n 100 -j 2 Scene: book2::final_scene ... Render took 57.53 seconds I was a tad bit perplexed? Final was slower?! After a few more runs, I saw a very minimal performance hit. Those blog posts must have lied to me... Before just tossing this away, I thought it would be best to pull out the verification test script. In a previous revision this was made to essentially fuzz test PSRayTracing (see previous post here). The repo already contains a small set of well known test cases. That suite initially ran for about 20 minutes. But this is where it got a little interesting. The script reported using final slightly faster; wtih final it took 11m 29s. Without final it was 11m 44s. That's +2%. Actually significant. Something seemed up; more investigation was required. Big Beefy Testing Unsatisfied with the above, I created a \"large test suite\" to be more intensive. On my dev machine it needed to run for 8 hours. This was done by bumping up some of the test parameters. Here are the details on what's been tweaked: Number of Times to Test a Scene: 10 → 30 Image Size: [320x240, 400x400, 852x480] → [720x1280, 720x720, 1280x720] Ray Depth: [10, 25, 50] → [20, 35, 50] Samples Per Pixel: [5, 10, 25] → [25, 50, 75] Some test cases now would render in 10 seconds, others would take up to 10 minutes to complete. I thought this was much more comprehensive. The smaller suite did around 350+ test cases in 20+ minutes. This now would do 1150+ over the course of 8+ hours. The performance of a C++ program is also very compiler (and system) dependent as well. So to be more thorough, this was tested across three machines, three operating systems, and with three different compilers; once with final, and once without it enabled. After doing the math, the machines were chugging along for a cumulative 125+ hours. 🫠 Please look at the tables below for specifics, but the configurations were: AMD Ryzen 9: Linux: GCC & Clang Windows: GCC & MSVC Apple M1 Mac: GCC & Clang Intel i7: Linux GCC For example, one configuration is \"AMD Ryzen 9 with Ubuntu Linux using GCC\" and another would be \"Apple M1 Mac with macOS using Clang\". Not all versions of the compilers were all the same; some were harder to get than others. And I do need to note at the time of writing this (and after gathering the data) a new version of Clang was released. Here, is the general summary of the test results: This gives off some interesting findings, but tells us one thing right now: across the board, final isn't always faster; it's in fact slower in some situations. Sometimes there is a nice speedup (>1%), other times it is detrimental. While it may be fun to compare compiler vs. compiler for this application (e.g. \"Monday Night Compiler Smackdown\"), I do not believe it is a fair thing to do with this data; it's only fair to compare \"with final\" and \"without final\" To compare compilers (and on different systems) a more comprehensive testing system is required. But there are some interesting observations: Clang on x86_64 is slow. Windows is less performant; Microsoft's own compiler is even lagging. Apple's silicon chips are absolute powerhouses. But each scene is different, and contains a different amount of objects that are marked with final. It would be interesting to see percentage wise, how many test cases ran faster or slower with final. Tabling that data, we get this: That 1% perf boost for some C++ applications is very desirable (e.g. HFT). And if we're hitting it for 50%+ of our test cases it seems like using final is something that we should consider. But on the flip side, we also need to see how the inverse looks. How much slower was it? And for how many test cases? Clang on x86_64 Linux right there is an absolute \"yikes\". More than 90% of test cases ran at least 5% slower with final turned on!! Remember how I said a 1% increase is good for some applications? A 1% hit is also bad. Windows with MSVC isn't faring too well either. As stated way above, this is very scene dependent. Some have only a handful of virtual objects. Others have warehouses full of them. Taking a look (on average) how much faster/slower a scene is with final turned on: I don't know Pandas that well. I was having some issues creating a Multi-Index table (from arrays) and having the table be both styled and formatted nicely. So instead each column has a configuration number appended to the end of its name. Here is what each number means: 0 - GCC 13.2.0 AMD Ryzen 9 6900HX Ubuntu 23.10 1 - Clang 17.0.2 AMD Ryzen 9 6900HX Ubuntu 23.10 2 - MSVC 17 AMD Ryzen 9 6900HX Windows 11 Home (22631.3085) 3 - GCC 13.2.0 (w64devkit) AMD Ryzen 9 6900HX Windows 11 Home (22631.3085) 4 - Clang 15 M1 macOS 14.3 (23D56) 5 - GCC 13.2.0 (homebrew) M1 macOS 14.3 (23D56) 6 - GCC 12.3.0 i7-10750H Ubuntu 22.04.3 So this is where things are really eye popping. On some configurations and specific scenes might have a 10% perf boost. For example book1::final_scene with GCC on AMD & Linux. But other scenes (on the same configuration) have a minimal 0.5% increase such as fun::three_spheres. But just switching the compiler over to Clang (still running on that AMD & Linux) there's a major perf hit of -5% and -17% (respectively) on those same two scenes!! MSVC (on AMD) looks to be a bit of a mixed bag where some scenes are more performant with final and others ones take a significant hit. Apple's M1 is somewhat interesting where the gains and hits are very minimal, but GCC has a significant benefit for two scenes. Whether there were many (or few) virtual objects had next to no correlation if final was a performance boon or hit. Clang Concerns Me PSRayTracing also runs on Android and iOS. Most likely a small fraction of apps available for these platforms are written in C++, but there are some programs that make use of language for performance reasons on the two systems. Clang is the compiler that is used for these two platforms. I unfortunately don't have a framework in place to test performance on Android and iOS like I do with desktop systems But I can do a simple \"render-scene-with-same-parameters-one-with-final-and-one-without\" test as the app reports how long the process took. Going from the data above, my hypothesis was that both platforms would be less performant with final turned on. By how much, I don't know. Here are the results: iPhone 12: I saw no difference; With and without final it took about 2 minutes and 36 seconds to perform the same render. Pixel 6 Pro: final was slower. It was 49 vs 46 seconds. A difference of three seconds might not seem like much, but that is a 6% slowdown; that is fairly significant. (clang 14 was used here BTW). If you think I'm being a little silly with these tiny percentages, please take a look at Nicholas Ormrod's 2016 CppCon talks about optimizing std::string for Facebook. I've referenced it before and will continue to do it. I have no idea if this is a Clang issue or an LLVM one. If it is the latter, this may have implications for other LLVM languages such as Rust and Swift. For The Future (And What I Wish I Did Instead): All in all this was a very fascinating detour; but I think I'm satisfied with what's been discovered. If I could redo some things (or be given money to work on this project): Have each scene be able to report some metadata. E.g. number of objects, materials, etc. It is easily doable but didn't seem worth it for this study of final. Have better knowledge of Jupyter+Pandas. I'm a C++ dev, not a data scientist. I'd like to be able to understand how to better transform the measured results and make it look prettier. A way to run the automated tests on Android and iOS. These two platforms can't easily be tested right now and I feel like this is a notable blindspot run_verfication_tests.py is turning more into an application (as opposed to a small script). Features are being bolted on. Better architecture is needed soon. Saving and loading testing state was added, but this should have been something from the start and feels like more of a hack to me I wish the output of the results were in a JSON format first instead of CSV. I had to fuddle with PyExcel more than desired. PNGs are starting to get kinda chunky. One time I ran out of disk space. Lossless WebP might be better as a render output. Comparing more Intel chips, and with more compilers. The i7 was something I had lying around. Conclusions In case you skimmed to the end, here's the summary: Benefit seems to be available for GCC. Doesn't affect Apple's chips much at all. Do not use final with Clang, and maybe MSVC as well. It all depends on your configuration/platform; test & measure to see if it's worth it. Personally, I'm not turning it on. And would in fact, avoid using it. It doesn't seem consistent. For those who want to look at the raw data and the Jupyter notebook I used to process & present these findings, it's over here. If you want to take a look at the project, it's up on GitHub (but the active development is done over on GitLab). Looking forward to the next time in one year when I pick up this project again. 😉 Tags: Computer Graphics, Projects, C/C++, Ray Tracing PSRayTracing is now available on the Apple App Store",
    "commentLink": "https://news.ycombinator.com/item?id=40116644",
    "commentBody": "The Performance Impact of C++'s `final` Keyword (16bpp.net)196 points by hasheddan 16 hours agohidepastfavorite190 comments mgaunard 16 hours agoWhat final enables is devirtualization in certain cases. The main advantage of devirtualization is that it is necessary for inlining. Inlining has other requirements as well -- LTO pretty much covers it. The article doesn't have sufficient data to tell whether the testcase is built in such a way that any of these optimizations can happen or is beneficial. reply chipdart 4 hours agoparent> What final enables is devirtualization in certain cases. The main advantage of devirtualization is that it is necessary for inlining. I think that enabling inlining is just one of the indirect consequences of devirtualization, and perhaps one that is largely irrelevant for performance improvements. The whole point of devirtualization is eliminating the need to resort to pointer dereferencing when calling virtual members. The main trait of a virtual class is it's use of a vtable that requires dereferencing virtual members to access each and every one of them. In classes with larger inheritance chains, you can easily have more than one pointer dereferencing taking place before you call a virtual members function. Once a class is final, none of that is required anymore. When a member is referred, no dereferencing takes place. Devirtualization helps performance because you are able to benefit from inheritance and not have to pay a performance penalty for that. Without the final keyword, a performance oriented project would need to be architected to not use inheritance at all, or in the very least in code in the hot path, because that sneaks gratuitous pointer dereferences all over the place, which require running extra operations and has a negative impact on caching. The whole purpose of the final keyword is that compilers can easily eliminate all pointer dereferencing used by virtual members. What stops them from applying this optimization is that they have no information on whether that class will be inherited and one of its members will either override any of its members or invoke any member function implemented by one of its parent classes. With the introduction of the final keyword, you are now able to tell the compiler \"from thereon, this is exactly what you get\" and the compiler can trim out anything loose. reply simonask 3 hours agorootparentAn extra indirection (indirect call versus direct call) is practically nothing on modern hardware. Branch predictors are insanely good, and this isn't something you generally have to worry about. Inlining is by far the most impactful optimization here, because it can eliminate the call altogether, and thus specialize the called function to the callsite, lifting constants, hoisting loop variables, etc. reply silvestrov 1 hour agorootparent\"is practically nothing on modern hardware\" if the data is already present in the L2 cache. Random RAM access that stalls execution is expensive. My guess is this is why he didn't see any speedup: all the code could fit inside the L2 cache, so he did not have to pay for RAM access for the deference. The number of different classes is important, not the number of objects as they have the same small number of vtable pointers. It might be different for large codebases like Chrome and Firefox. reply Negitivefrags 15 hours agoparentprevSee this is why I find this odd. Is there a theory as to how devirtualisation could hurt performance? reply phire 13 hours agorootparentJumps/calls are actually be pretty cheap with modern branch predictors. Even indirect calls through vtables, which is the opposite of most programmers intuition. And if the devirtualisation leads to inlining, that results in code bloat which can lower performance though more instruction cache misses, which are not cheap. Inlining is actually pretty evil. It almost always speeds things up for microbenchmarks, as such benchmarks easily fit in icache. So programmers and modern compilers often go out of their way to do more inlining. But when you apply too much inlining to a whole program, things start to slow down. But it's not like inlining is universally bad in larger program, inlining can enable further optimisations, mostly because it allows constant propagation to travel across function boundaries. Basically, compilers need better heuristics about when they should be inlining. If it's just saving the overhead of a lightweight call, then they shouldn't be inlining. reply a_e_k 5 hours agorootparentAnother for the pro side: inlining can allow for better branch prediction if the different call sites would tend to drive different code paths in the function. reply phire 4 hours agorootparentThis was true 15 years ago, but not so much today. The branch predictors actually hash the history of the last few branches taken into the branch prediction query. So the exact same branch within a child function will map different branch predictors entries depending on which parent function it was called from, and there is no benifit to inlining. It also means that branch predictor can also learn correlations between branches within a function. Like when a branches at the top and bottom of functions share conditions, or have inverted conditions. reply qsdf38100 13 hours agorootparentprev\"Inlining is actually pretty evil\". No it's not. Except if you __force_inline__ everything, of course. Inlining reduces the number of instructions in a lot of cases. Especially when things are abstracted and factored with lot of indirections into small functions that calls other small functions and so on. Consider a 'isEmpty' function, which dissolves to 1 cpu instruction once inlined, compared with a call/save reg/compare/return. Highly dynamic code (with most functions being virtual) tend to result in a fest of chained calls, jumping into functions doing very little work. Yes the stack is usually hot and fast, but spending 80% of the instructions doing stack management is still a big waste. Compilers already have good heuristics about when they should be inlining, chances are they are a lot better at it than you. They don't always inline, and that's not possible anyway. My experience is that compiler do marvels with inlining decisions when there are lots of small functions they _can_ inline if they want to. It gives the compiler a lot of freedom. Lambdas are great for that as well. Make sure you make the most possible compile-time information available to the compiler, factor your code, don't have huge functions, and let the compiler do its magic. As a plus, you can have high level abstractions, deep hierarchies, and still get excellent performances. reply grdbjydcv 11 hours agorootparentThe “evilness” is just that sometimes if you inline aggressively in a microbenchmark things get faster but in real programs things get slower. As you say: “chances are they are a lot better at it than you”. Infrequently they are not. reply EasyMark 6 hours agorootparentprevdoesn't the compiler usually do well enough that you really only need to worry about time critical sections of code? Even then you could go in and look at the assembler and see if it's being inlined, no? reply usefulcat 5 hours agorootparentI find that gcc and clang are so aggressive about inlining that it's usually more effective to tell them what not to inline. In a moderately-sized codebase I regularly work on, I use __attribute__((noinline)) nearly ten times as often as __attribute__((always_inline)). And I use __attribute__((cold)) even more than noinline. So yeah, I can kind of see why someone would say inlining is 'evil', though I think it's more accurate to say that it's just not possible for compilers to figure out these kinds of details without copious hints (like PGO). reply jandrewrogers 5 hours agorootparent+1 on the __attribute__((cold)). Compilers so aggressively optimize based on their heuristics that you spend more time telling them that an apparent optimization opportunity is not actually an optimization. When writing ultra-robust code that has to survive every vaguely plausible contingency in a graceful way, the code is littered with code paths that only exist for astronomically improbable situations. The branch predictor can figure this out but the compiler frequently cannot without explicit instructions to not pollute the i-cache. reply variadix 10 hours agorootparentprevIt basically never should unless the inliner made a terrible judgement. Devirtualizing in C++ can remove 3 levels of pointer chasing, all of which could be cache misses. Many optimizations in modern compilers require the context of the function to be inlined to make major optimizations, which requires devirtualization. The only downside is I$ pressure, but this is generally not a problem because hot loops are usually tight. reply hansvm 14 hours agorootparentprevThere's a cost to loading more instructions, especially if you have more types of instructions. The main advantages to inlining are (1) avoiding a jump and other function call overhead, (2) the ability to push down optimizations. If you execute the \"same\" code (same instructions, different location) in many places that can cause cache evictions and other slowdowns. It's worse if some minor optimizations were applied by the inlining, so you have more types of instructions to unpack. The question, roughly, is whether the gains exceed the costs. This can be a bit hard to determine because it can depend on the size of the whole program and other non-local parameters, leading to performance cliffs at various stages of complexity. Microbenchmarks will tend to suggest inlining is better in more cases that it actually is. Over time you get a feel for which functions should be inlined. E.g., very often you'll have guard clauses or whatnot around a trivial amount of work when the caller is expected to be able to prove the guarded information at compile-time. A function call takes space in the generated assembly too, and if you're only guarding a few instructions it's usually worth forcing an inline (even in places where the compiler's heuristics would choose not to because the guard clauses take up too much space), regardless of the potential cache costs. reply cogman10 14 hours agorootparentprevThrough inlining. If you have something like a `while` loop and that while loop's instructions fit neatly on the cache line, then executing that loop can be quiet fast even if you have to jump to different code locations to do the internals. However, if you pump in more instructions in that loop you can exceed the length of the cache line which causes you to need more memory loads to do the same work. It can also create more code. A method that took a `foo(NotFinal& bar)` could be duplicated by the compiler for the specialized cases which would be bad if there's a lot of implementations of `NotFinal` that end up being marshalled into foo. You could end up loading multiple implementations of the same function which may be slower than just keeping the virtual dispatch tables warm. reply bandrami 5 hours agorootparentprevIf it's done badly, the same code that runs N times also gets cached N times because it's in N different locations in memory rather than one location that gets jumped to. Modern compilers and schedulers will eliminate a lot of that (but probably not for anything much smaller than a page), but in general there's always a tradeoff. reply samus 14 hours agorootparentprevDevirtualization maybe not necessarily, but inlining might make code fail to fit into instruction caches. reply masklinn 14 hours agorootparentprevCode bloat causing icache evictions? reply neonsunset 11 hours agorootparentprevPractically - it never does. It is always cheaper to perform a direct, possibly inlined, call (devirtualization != inlining) than a virtual one. Guarded devirtualization is also cheaper than virtual calls, even when it has to do if (instance is SpecificType st) { st.Call() } else { instance.Call() } or even chain multiple checks at once (with either regular ifs or emitting a jump table) This technique is heavily used in various forms by .NET, JVM and JavaScript JIT implementations (other platforms also do that, but these are the major ones) The first two devirtualize virtual and interface calls (important in Java because all calls default to virtual, important in C# because people like to abuse interfaces and occasionally inheritance, C# delegates are also devirtualized/inlined now). The JS JIT (like V8) performs \"inline caching\" which is similar where for known object shapes property access is shape type identifier comparison and direct property read instead of keyed lookup which is way more expensive. reply ynik 2 hours agorootparentCaution! If you compare across languages like that, not all virtual calls are implemented equally. A C++ virtual call is just a load from a fixed offset in the vtbl followed by an indirect call. This is fairly cheap, on modern CPUs pretty much the same as a non-virtual non-inlined call. A Java/C# interface call involves a lot more stuff, because there's no single fixed vtbl offset that's valid for all classes implementing the interface. reply neonsunset 2 hours agorootparentYes, it is true that there is difference. I'm not sure about JVM implementation details but the reason the comment says \"virtual and interface\" calls is to outline it. Virtual calls in .NET are sufficiently close[0] to virtual calls in C++. Interface calls, however, are coded differently[1]. Also you are correct - virtual calls are not terribly expensive, but they encroach on ever limited* CPU resources like indirect jump and load predictors and, as noted in parent comments, block inlining, which is highly undesirable. [0] https://github.com/dotnet/runtime/blob/5111fdc0dc464f01647d6... [1] https://github.com/dotnet/runtime/blob/main/docs/design/core... (mind you, the text was initially written 18 years ago, wow) * through great effort of our industry to take back whatever performance wins each generation brings with even more abstractions that fail to improve our productivity reply bdjsiqoocwk 40 minutes agoparentprevWhats devirtualization in C++? Funny how things work. From working with Julia I've built a good intuition for guessing when functions would be inlined. And yet, I've never heard the word devirtualization until now. reply i80and 15 hours agoparentprevIf you already have LTO, can't the compiler determine this information for devirtualization purposes on its own? reply ot 15 hours agorootparentIn general the compiler/linker cannot assume that derived classes won't arrive later through a shared object. You can tell it \"I won't do that\" though with additional flags, like Clang's -fwhole-program-vtables, and even then it's not that simple. There was an effort in Clang to better support whole program devirtualization, but I haven't been following what kind of progress has been made: https://groups.google.com/g/llvm-dev/c/6LfIiAo9g68?pli=1 reply Slix 1 hour agorootparentThis optimization option isn't on by default? That sounds like a lot of missed optimization. Most programs aren't going to be loading from shared libraries. Maybe I can set this option at work. Though it's scary because I'd have to be certain. reply wiml 15 hours agorootparentprevIf your runtime environment has dynamic linking, then the LTO pass can't always be sure that a subclass won't be introduced later that overrides the method. reply gpderetta 14 hours agorootparentYou can tell the compiler it is indeed compiling the whole program. reply i80and 15 hours agorootparentprevAha! That makes sense. I wasn't thinking of that case. Thanks! reply samus 14 hours agorootparentprevThis is one of the cases where JIT compiling can shine. You can use a bazillion interfaces to decouple application code, and the JIT will optimize the calls after it found out which implementation is used. This works as long as there is only one or two of them actually active at runtime. reply nickwanninger 15 hours agorootparentprevAt the level that LLVM's LTO operates, no information about classes or objects is left, so LLVM itself can't really devirtualize C++ methods in most cases reply nwallin 12 hours agorootparentYou appear to be correct. Clang does not devirtualize in LTO, but GCC does. Personally I consider this very strange. $ cat animal.h cat.cpp main.cpp // animal.h #pragma once class animal { public: virtual ~animal() {} virtual void speak() = 0; }; animal& get_mystery_animal(); // cat.cpp #include \"animal.h\" #includeclass cat final : public animal { public: ~cat() override{} void speak() override{ puts(\"meow\"); } }; static cat garfield{}; animal& get_mystery_animal() { return garfield; } // main.cpp #include \"animal.h\" int main() { animal& a = get_mystery_animal(); a.speak(); } $ make clean && CXX=clang++ make -j && objdump --disassemble=main -C lto_test rm -f *.o lto_test clang++ -c -flto -O3 -g cat.cpp -o cat.o clang++ -c -flto -O3 -g main.cpp -o main.o clang++ -flto -O3 -g cat.o main.o -o lto_test lto_test: file format elf64-x86-64 Disassembly of section .init: Disassembly of section .plt: Disassembly of section .plt.got: Disassembly of section .text: 00000000000011b0 : 11b0: 50 push %rax 11b1: 48 8b 05 58 2e 00 00 mov 0x2e58(%rip),%rax # 401011b8: 48 8d 3d 51 2e 00 00 lea 0x2e51(%rip),%rdi # 401011bf: ff 50 10 call *0x10(%rax) 11c2: 31 c0 xor %eax,%eax 11c4: 59 pop %rcx 11c5: c3 ret Disassembly of section .fini: $ make clean && CXX=g++ make -j && objdump --disassemble=main -C lto_test|sed -e 's,^, ,' rm -f *.o lto_test g++ -c -flto -O3 -g cat.cpp -o cat.o g++ -c -flto -O3 -g main.cpp -o main.o g++ -flto -O3 -g cat.o main.o -o lto_test lto_test: file format elf64-x86-64 Disassembly of section .init: Disassembly of section .plt: Disassembly of section .plt.got: Disassembly of section .text: 0000000000001090 : 1090: 48 83 ec 08 sub $0x8,%rsp 1094: 48 8d 3d 75 2f 00 00 lea 0x2f75(%rip),%rdi # 4010109b: e8 50 01 00 00 call 11f010a0: 31 c0 xor %eax,%eax 10a2: 48 83 c4 08 add $0x8,%rsp 10a6: c3 ret Disassembly of section .fini: reply ranger_danger 8 hours agorootparentWhat if you add -fwhole-program-vtables on clang? reply adzm 15 hours agorootparentprevMSVC with LTO and PGO will inline virtual calls in some situations along with a check for the expected vtable, bypassing the inlined code and calling the virtual function normally if it is an unexpected value. reply bluGill 15 hours agorootparentprevnot if there is a shared libray or other plugin. Then you coannot determine until runtime if there is an override. reply tombert 15 hours agoprevI don't do much C++, but I have definitely found that engineers will just assert that something is \"faster\" without any evidence to back that up. Quick example, I got in an argument with someone a few years ago that claimed in C# that a `switch` was better than an `if(x==1) elseif(x==2)...` because switch was \"faster\" and rejected my PR. I mentioned that that doesn't appear to be true, we went back and forth until I did a compile-then-decompile of a minimal test with equality-based-ifs, and showed that the compiler actually converts equality-based-ifs to `switch` behind the scenes. The guy accepted my PR after that. But there's tons of this stuff like this in CS, and I kind of blame professors for a lot of it [1]. A large part of becoming a decent engineer [2] for me was learning to stop trusting what professors taught me in college. Most of what they said was fine, but you can't assume that; what they tell you could be out of date, or simply never correct to begin with, and as far as I can tell you have to always test these things. It doesn't help that a lot of these \"it's faster\" arguments are often reductive because they only are faster in extremely minimal tests. Sometimes a microbenchmark will show that something is faster, and there's value in that, but I think it's important that that can also be a small percentage of the total program; compilers are obscenely good at optimizing nowadays, it can be difficult to determine when something will be optimized, and your assertion that something is \"faster\" might not actually be true in a non-trivial program. This is why I don't really like doing any kind of major optimizations before the program actually works. I try to keep the program in a reasonable Big-O and I try and minimize network calls cuz of latency, but I don't bother with any kind of micro-optimizations in the first draft. I don't mess with bitwise, I don't concern myself on which version of a particular data structure is a millisecond faster, I don't focus too much on whether I can get away with a smaller sized float, etc. Once I know that the program is correct, then I benchmark to see if any kind of micro-optimizations will actually matter, and often they really don't. [1] That includes me up to about a year ago. [2] At least I like to pretend I am. reply jandrewrogers 14 hours agoparentA significant part of it is that what engineers believe was effectively true at one time. They simply haven't revisited those beliefs or verified their relevance in a long time. It isn't a terrible heuristic for life in general to assume that what worked ten years ago will work today. The rate at which the equilibriums shift due to changes in hardware and software environments when designing for system performance is so rapid that you need to make a continuous habit of checking that your understanding of how the world works maps to reality. I've solved a lot of arguments with godbolt and simple performance tests. Some topics are recurring themes among software engineers e.g.: - compilers are almost always better at micro-optimizations than you are - disk I/O is almost never a bottleneck in competent designs - brute-force sequential scans are often optimal algorithms - memory is best treated as a block device - vectorization can offer large performance gains - etc... No one is immune to this. I am sometimes surprised at the extent to which assumptions are no longer true when I revisit optimization work I did 10+ years ago. Most performance these days is architectural, so getting the initial design right often has a bigger impact than micro-optimizations and localized Big-O tweaks. You can always go back and tweak algorithms or codegen later but architecture is permanent. reply tombert 13 hours agorootparentYep, completely agree with you on this. Intuition is often wrong, or at least outdated. When I'm building stuff I try my best to focus on \"correctness\", and try to come up with an algorithm/design that will encompass all realistic use cases. If I focus on that, it's relatively easy to go back and convert my `decimal` type to a float64, or even convert an if statement into a switch if it's actually faster. reply neonsunset 14 hours agorootparentprev.NET is a particularly bad case for this because it was a decade of few performance improvements, which caused a certain intuition to develop within the industry, then 6-8 years of significant changes each year (with most wins compressed to the last 4 years or so). Companies moving from .NET Framework 4.6/7/8 to .NET 8 experience a 10x average performance improvement, which naturally comes with rendering a lot of performance know-how obsolete overnight. (the techniques that used to work were similar to earlier Java versions and overall very dynamic languages with some exceptions, the techniques that still work and now are required today are the same as in C++ or Rust) reply throwaway2037 8 hours agorootparent.NET 4.6 to .NET 8 is a 10x \"average\" performance improvement. I find this hard to believe. In what scenarios? I tried to Google for it and found very little hard evidence. reply neonsunset 8 hours agorootparentIn general purpose scenarios, particularly in codebases which have high amount of abstractions, use ASP.NET Core and EF Core, parse and de/serialize text with the use of JSON, Regex and other options, have network and file IO, and are deployed on many-core hosts/container images. There are a few articles on msft devblogs that cover from-netframework migration to older versions (Core 3.1, 5/6/7): - https://devblogs.microsoft.com/dotnet/bing-ads-campaign-plat... - https://devblogs.microsoft.com/dotnet/microsoft-graph-dotnet... - https://devblogs.microsoft.com/dotnet/the-azure-cosmos-db-jo... - https://devblogs.microsoft.com/dotnet/one-service-journey-to... - https://devblogs.microsoft.com/dotnet/microsoft-commerce-dot... The tl;dr is depending on codebase the latency reduction was anywhere from 2x to 6x, varying per percentile, or the RPS was maintained with CPU usage dropping by ~2-6x. Now, these are codebases of likely above average quality. If you consider that moving 6 -> 8 yields another up to 15-30% on average through improved and enabled by default DynamicPGO, and if you also consider that the average codebase is of worse quality than whatever msft has, meaning that DPGO-reliant optimizations scale way better, it is not difficult to see the 10x number. Keep in mind that while particular regular piece of enterprise code could have improved within bounds of \"poor netfx codegen\" -> \"not far from LLVM with FLTO and PGO\", the bottlenecks have changed significantly where previously they could have been in lock contention (within GC or user code), object allocation, object memory copying, e.g. for financial domains - anything including possibly complex Regex queries on imported payment reports (these alone have now difference anywhere between 2 and >1000[0]), and for pretty much every code base also in interface/virtual dispatch for layers upon layers of \"clean architecture\" solutions. The vast majority of performance improvements (both compiler+gc and CoreLib+frameworks), which is difficult to think about, given it was 8 years, address the above first and foremost. At my previous employer the migration from NETFX 4.6 to .NET Core 3.1, while also deploying to much more constrained container images compared to beefy Windows Server hosts, reduced latency of most requests by the same factor of >5x (certain request type went from 2s to 350ms). It was my first wow moment when I decided to stay with .NET rather than move over to Go back then (was never a fan of syntax though, and other issues, which subsequently got fixed in .NET, that Go still has, are not tolerable for me). [0] Cumulative of https://devblogs.microsoft.com/dotnet/regex-performance-impr... https://devblogs.microsoft.com/dotnet/regular-expression-imp... https://devblogs.microsoft.com/dotnet/performance-improvemen... reply rerdavies 6 hours agorootparentCheating. All of the 6x performance improvement cases seem to be related to using the .net based Kestrel web server instead of IIS web server, which requires marshalling and interprocess communication. Several of the 2x gains appear to be related to using a different database backend. Claims that regex performance has improved a thousand-fold.... seem more troubling than cause for celebration. Were you not precompiling your regex's in the older code? That would be a bug. Somewhere in there, there might be 30% improvements in .net codegen (it's hard to tell). Profile Guided Optimization (PGO) seems to provide a 35% performance improvement over older versions of .net with PGO disabled. But that's dishonest. PGO was around long before .net Core. And claiming that PGO will provide 10x performance because our code is worse than Microsoft's code insults both our code and our intelligence. reply ygra 4 hours agorootparentNot sure about the 10×, either, and if true it would involve more than just the JIT changes. But changing ASP.NET to ASP.NET Core at the same time and the web server as well as other libraries may make it plausible. For certain applications moving from .NET Framework to .NET isn't so simple when they have dependencies and those have changed their API significantly. And in that case most of the newer stuff seems to be built with performance in mind. So you gain 30 % from the JIT, 2× from Kestrel, and so on. Perhaps. With a Roslyn-based compiler at work I saw 20 % perf improvement just by switching from .NET Core 3.1 to .NET 6. No idea how slow .NET Framework was, though. I probably can't target the code to that anymore. But for regex even with precompilation, the compiler got a lot better at transforming the regex into an equivalent regex that performs better (automatic atomic grouping to reduce unnecessary backtracking when it's statically known that backtracking won't create more matches for example) and it also benefits a lot from the various vectorized implementations of Index of, etc. Typically with each improvement of one of those core methods for searching stuff in memory there's a corresponding change that uses them in regex. So where in .NET Framework a regex might walk through a whole string character by character multiple times with backtracking it might be replaced with effectively an EndsWith and LastIndexOfAny call in newer versions. reply neonsunset 3 hours agorootparentRoslyn didn't have much of changes in terms of optimizations - it compiles C# to IL so does very little of that, save for switches and certain new or otherwise features like collection literals. You are probably talking about RyuJIT, also called just JIT nowadays :D (the distinction becomes important for targets serviced by Mono, so to outline the difference Mono is usually specified, while CoreCLR and RyuJIT may not be, it also doesn't help that JIT, that is, the IL to machine code compiler, also services NativeAOT, so it gets more annoying to be accurate in a conversation without saying the generic \".net compiler\", some people refer to it as JIT/ILC) reply ygra 2 hours agorootparentNo, I meant that we've written a compiler, based on Roslyn, whose runtime for compiling the code has improved by 20 % when switching to .NET 6. And indeed, on the C# -> IL side there's little that's being actually optimized. Besides collection literals there's also switch statements/expressions over strings, along with certain pattern matching constructs that get improved on that side. reply neonsunset 2 hours agorootparentInteresting! (I was way off the mark, not reading carefully, ha) Is it a public project? reply neonsunset 4 hours agorootparentprevNo. DynamicPGO was first introduced in .NET 6 but was not mature and needed two releases worth of work to become enabled by default. It needs no user input and is similar to what OpenJDK Hotspot has been doing for some time and then a little more. It also is required for major features that were strictly not available previously: guarded devirtualization of virtual and interface calls and delegate inlining. Also, IIS hosting through Http.sys is still an option that sees separate set of improvements, but that's not relevant in most situations given the move to .NET 8 from Framework usually also involves replacing Windows Server host with a Linux container (though it works perfectly fine on Windows as well). On Regex, compiled and now source generated automata has seen a lot of work in all recent releases, it is night and day to what it was before - just read the articles. Previously linear scans against heavy internal data structures (matching by hashset) and heavy transient allocations got replaced with bloom-filter style SIMD search and other state of the art text search algorithms[0], on a completely opposite end of a performance spectrum. So when you have compiler improvements multiplied by changes to CoreLib internals multiplied by changes to frameworks built on top - it's achievable with relative ease. .NET Framework, while performing adequately, was still that slow compared to what we got today. [0] https://github.com/dotnet/runtime/tree/main/src/libraries/Sy... reply wvenable 15 hours agoparentprevIn my opinion, the only things that really matter are algorithmic complexity and readability. And even algorithmic complexity is usually only an issue a certain scales. Whether or not an 'if' is faster than a 'switch' is the micro of micro optimizations -- you better have a good reason to care. The question I would have for you is was your bunch of ifs more readable than a switch would be. reply tombert 14 hours agorootparentYeah, and it's not like I didn't know how to do the stuff I was doing with a switch, I just don't like switches because I've forgotten to add break statements and had code that appeared correct but actually a month down the line. I've also seen other people make the same mistakes. ifs, in my opinion at least, are a bit harder to screw up, so I will always prefer them. But I agree, algorithmic complexity is generally the only thing I focus on, and even then it's almost always a case of \"will that actually matter?\" If I know that `n` is never going to be more than like `10`, I might not bother trying to optimize an O(n^2) operation. What I feel often gets ignored in these conversations is latency; people obsess over some \"optimization\" they learned in college a decade ago, and ignore the 200 HTTP or Redis calls being made ten lines below, despite the fact that the latter will have a substantially higher impact on performance. reply jpc0 1 hour agorootparentprev... really matter are algorithmic complexity ... This is not entirely true either... Measure. There are many cases where the optimiser will vectorise a certian algorithm but not another... In many cases On^2 vectorised may be significantly faster than On or Onlogn even for very large datasets depending on your data... Make your algorithms generic and it won't matter which one you use, if you find that one is slower swap it for the quicker one. Depending on CPU arch and compiler optimisations the fastest algorithm may actually change multiple times in a codebases lifetime even if the usage pattern doesn't change at all. reply doctor_phil 15 hours agorootparentprevBut a switch and an if-else *is* a matter of algorithmic complexity. (Well, at least could be for a naive compiler). A switch could be converted to a constant time jump, but the if-else would be trying each case linearly. reply bregma 13 hours agorootparentBut what if, and stick with me here, a compiler is capable of reading and processing your code and through simple scalar evolution of the conditionals and phi-reduction, it can't tell the difference between a switch statement and a sequence of if statements by the time it finishes its single static analysis phase? It turns out the algorithmic complexity of a switch statement and the equivalent series of if-statements is identical. The bijective mapping between them is close to the identity function. Does a naive compiler exist that doesn't emit the same instructions for both, at least outside of toy hobby project compilers written by amateurs with no experience? reply saurik 14 hours agorootparentprevWhile I personally find the if statements harder to immediately mentally parse/grok--as I have to prove to myself that they are all using the same variable and are all chained correctly in a way that is visually obvious for the switch statement--I don't find \"but what if we use a naive compiler\" at all a useful argument to make as, well, we aren't using a naive compiler, and, if we were, there are a ton of other things we are going to be sad about the performance of leading us down a path of re-implementing a number of other optimizations. The goal of the compiler is to shift computational complexity from runtime to compile time, and figuring out whether the switch table or the comparisons are the right approach seems like a legitimate use case (which maybe we have to sometimes disable, but probably only very rarely). reply adrianN 5 hours agorootparentprevBoth the switch and the if have O(1) instructions, so both are the same from an algorithmic complexity perspective. reply cogman10 14 hours agorootparentprevYup. That said, the linear test is often faster due to CPU caches, which is why JITs will often convert switches to if/elses. IMO, switch is clearer in general and potentially faster (at very least the same speed) so it should be preferred when dealing with 3+ if/elseif statements. reply tombert 14 hours agorootparentHard disagree that it's \"clearer\". I have had to deal with a ton of bugs with people trying to be clever with the `break` logic, or forgetting to put `break` in there at all. if statements are dumber, and maybe arguably uglier, but I feel like they're also more clear, and people don't try and be clever with them. reply cogman10 14 hours agorootparentUpdates to languages (don't know where C# is on this) have different types of switch statements that eliminate the `break` problem. For example, with java there's enhanced switch that looks like this var val = switch(foo) { case 1, 2, 3 -> bar; case 4 -> baz; default -> { yield bat(); } } The C style switch break stuff is definitely a language mistake. reply wvenable 14 hours agorootparentC# has both switch expressions like this and also break statements are not optional in traditional switch statements so it actually solves both problems. You can't get too clever with switch statements in C#. However most languages have pretty permissive switch statements just like C. reply tombert 14 hours agorootparentYeah, fair, it's been awhile since I've done any C#, so my memory is a bit hazy with the details. I've been burned C with switch statements so I have a pretty strong distaste for them. reply neonsunset 14 hours agorootparentprevC# has switch statements which are C/C++ style switches and switch expressions which are like Rust's match except no control flow statements inside: var len = slice switch { null => 0, \"Hello\" or \"World\" => 1, ['@', ..var tags] => tags.Length, ['{', ..var body, '}'] => body.Length, _ => slice.Length, }; (it supports a lot more patterns but that wouldn't fit) reply gloryjulio 11 hours agorootparentprevThis is just forcing return value. You either have to break or return at the branches. To me they all look equivalent reply neonsunset 14 hours agorootparentprevAny sufficiently advanced compiler will rewrite those arbitrarily depending on its heuristics. What authors usually forget is that there is defined behavior and specification which the compiler abides by, but it is otherwise free to produce any codegen that preserves the defined program order. Branch reordering, generating jump tables, optimizing away or coalescing checks into branchless forms are all very common. When someone says \"oh I write C because it lets you tell CPU how exactly to execute the code\" is simply a sign that a person never actually looked at disassembly and has little to no idea how the tool they use works. reply cogman10 14 hours agorootparentA complier will definitely try this, but it's important to note that if/else blocks tell the compiler that \"you will run these evaluations in order\". Now, if the compiler can detect that the evaluations have no side effects (which, in this simple example with just integer checks, is fairly likely) then yeah I can see a jump table getting shoved in as an optimization. However, the moment you add a side effect or something more complicated like a method call, it becomes really hard for the complier to know if that sort of optimization is safe to do. The benefit of the switch statement is that it's already well positioned for the compiler to optimize as it does not have the \"you must run these evaluations in order\" requirement. It forces you to write code that is fairly compiler friendly. All that said, probably a waste of time debating :D. Ideally you have profiled your code and the profiler has told you \"this is the slow block\" before you get to the point of worrying about how to make it faster. reply tombert 13 hours agorootparentI agree with what you said but in this particular case, it actually was a direct integer equality check, there was zero risk of hitting side effects and that was plainly obvious to me, the checker, and compiler. reply cogman10 13 hours agorootparentAnd to your original comment, I think the reviewer was wrong to reject the PR over that. Performance has to be measured before you can use it to reject (or create...) a PR. If someone hasn't done that then unless it's something obvious like \"You are making a ton of tiny heap allocations in a tight loop\" then I think nitpicking these sorts of things is just wrong. reply Gazoche 1 hour agorootparentprevIt's linear with respect to the number of cases, not the size of inputs. It's still O(1) in the sense of algorithmic complexity. reply yau8edq12i 4 hours agorootparentprevUnless the number of \"else if\" statements somehow grows e.g. linearly with the size of your input, which isn't plausible, the \"else if\" statements also execute in O(1) time. reply saghm 15 hours agoparentprev> But there's tons of this stuff like this in CS Reminds me of the classic https://stackoverflow.com/questions/24848359/which-is-faster... reply sgerenser 14 hours agorootparentNever saw that before, that is indeed a classic. reply BurningFrog 15 hours agoparentprevEven if one of these constructs is faster it doesn't matter 99% of the time. Writing well structured readable code is typically far more important than making it twice as fast. And those times can rarely be predicted beforehand, so you should mostly not worry about it until you see real performance problems. reply apantel 14 hours agorootparentThe counter-argument to this is if you are building something that is in the critical path of an application (for example, parsing HTTP in a web server), you need to be performance-minded from the beginning because design decisions lead to design decisions. If you are building something in the critical path of the application, the best thing to do is build it from the ground up measuring the performance of what you have as you go. This way, each time you add something you will see the performance impact and usually there’s a more performant way of doing something that isn’t more obscure. If you do this as you build, early choices become constraints, but because you chose the most performant thing at every stage, the whole process takes you in the direction of a highly-performant implementation. Why should you care about performance? I can give you my personal experience: I’ve been working on a Java web/application server for the past 15 years and a typical request (only reading, not writing to the db) would take maybe 4-5 ms to execute. That includes HTTP request parsing, JSON parsing, session validation, method execution, JSON serialization, and HTTP response dispatch. Over the past 9 months I have refactored the entire application for performance and a typical request now takes about 0.25 ms or 250 microseconds. The computer is doing so much less work to accomplish the same tasks, it’s almost silly how much work it was doing before. And the result is the machine can handle 20x more requests in the same amount of time. If it could handle 200 requests per second per core before, now it can handle 4000. That means the need to scale is felt 20x less intensely, which means less complexity around scaling. High performance means reduced scaling requirements. reply tombert 14 hours agorootparentBut even that sort of depends right? Hardware is often pretty cheap in comparison to dev-time. I really depends on the project, what kind of servers you're using, the nature of the application etc, but I think a lot of the time it might be cheaper to just pay for 20x the servers than it would be to pay a human to go find a critical path. I'm not saying you completely throw caution to the wind, I'm just saying that there's a finite amount of human resources and it can really vary how you want to allocate them. Sometimes the better path is to just throw money at the problem. It really depends. reply apantel 14 hours agorootparentI think it depends on what you’re building and who’s building it. We’re all benefitting from the fact that the designers of NGINX made performance a priority. We like using things that were designed to be performant. We like high-FPS games. We like fast internet. I personally don’t like the idea of throwing compute at a slow solution. I like when the extra effort has been put into something. The good feeling I get from interacting with something that is optimal or excellent is an end in itself and one of the things I live for. reply tombert 13 hours agorootparentSure, though I've mentioned a few times in this thread now that the thing that bothers me more than CPU optimizations is not taking into account latency, particularly when hitting the network, and I think focusing on that will generally pay higher dividends than trying to optimize for processing. CPUs are ridiculously fast now, and compilers are really really good now too. I'm not going to say that processing speed is a \"solved\" problem, but I am going to say that in a lot of performance-related cases the CPU processing is probably not your problem. I will admit that this kind of pokes holes in my previous response, because introducing more machines into the mix will almost certainly increase latency, but I think it more or less holds depending on context. But I think it really is a matter of nuance, which you hinted at. If I'm making an admin screen that's going to have like a dozen users max, then a slow, crappy solution is probably fine; the requests will be served fast enough to where no one will notice anyway, and you can probably even get away with the cheapest machine/VM. If I'm making an FPS game that has 100,000 concurrent users, then it almost certainly will be beneficial to squeeze out as much performance out of the machine as possible, both CPU and latency-wise. But as I keep repeating everywhere, you have to measure. You cannot assume that your intuition is going to be right, particularly at-scale. reply apantel 13 hours agorootparentI absolutely agree that latency is the real thing to optimize for. In my case, I only leave the application to access the db, and my applications tend not to be write-heavy. So in my case latency-per-request == how much work the computer has to do, which is constrained to one core because the overhead of parallelizing any part of the pipeline is greater than the work required. See, in that sense, we’re already close to the performance ceiling for per-request processing because clock speeds aren’t going up. You can’t make the processing of a given request faster by throwing more hardware at it. You can only make it faster by creating less work for the hardware to do. (Ironically, HN is buckling under load right now, or some other issue.) reply oivey 12 hours agorootparentprevIt almost certainly would require more than 20x servers because setting up horizontal scaling will have some sort of overhead. Not only that, there is the significant engineering effort to develop and maintain the code to scale. If your problem can fit on one server, it can massively reduce engineering and infrastructure costs. reply neonsunset 14 hours agorootparentprevPlease accept a high five from a fellow \"it does so little work it must have sub-millisecond request latency\" aficionado (though I must admit I'm guilty of abusing memory caches to achieve this). reply apantel 13 hours agorootparentCaches, precomputed values, lookup tables — it’s all good as long as it’s well-organized and maintainable. reply tombert 14 hours agorootparentprevI mostly focus on \"using stuff that won't break\", and yeah \"if it actually matters\". For example, much to the annoyance of a lot of people, I don't typically use floating point numbers when I start out. I will use the \"decimal\" or \"money\" types of the language, or GMP if I'm using C. When I do that, I can be sure that I won't have to worry about any kind of funky overflow issues or bizarre rounding problems. There might be a performance overhead associated with it, but then I have to ask myself \"how often is this actually called?\" If the answer is \"a billion times\" or \"once in every iteration of the event loop\" or something, then I will probably eventually go back and figure out if I can use a float or convert it to an integer-based thing, but in a lot of cases the answer is \"like ten or twenty times\", and at that point I'm not even 100% sure it would be even measurable to change to the \"faster\" implementations. What annoys me is that people will act like they really care about speed, do all these annoying micro-optimizations, and then forget that pretty much all of them get wiped out immediately upon hitting the network, since the latency associated with that is obscene. reply neonsunset 14 hours agorootparentprevThis attitude is part of the problem. Another part of the problem is having no idea which things actually end up costing performance and how much. It is why many language ecosystems suffered from performance issues for a really long time even if completely unwarranted. Is changing ifs to switch or vice versa, as outlined in the post above, a waste of time? Yes, unless you are writing some encoding algorithm or a parser, it will not matter. The compiler will lower trivial statements to the same codegen and it will not impact the resulting performance anyway even if there was difference given a problem the code was solving. However, there are things that do cost like interface spam, abusing lambdas writing needlessly complex wokflow-style patterns (which are also less readable and worse in 8 out of 10 instances), not caching objects that always have the same value, etc. These kinds of issues, for example, plagued .NET ecosystem until more recent culture shift where it started to be cool once again to focus on performance. It wasn't being helped by the notion of \"well-structured code\" being just idiotic \"clean architecture\" and \"GoF patterns\" style dogma applied to smallest applications and simplest of business domains. (it is also the reason why picking slow languages in general is a really bad idea - everything costs more and you have way less leeway for no productivity win - Ruby and Python, and JS with Node.js are less productive to write in than C#/F#, Kotlin/Java or Go(under some conditions)) reply tombert 14 hours agorootparentI mean, that's kind of why I tried to emphasize measuring things yourself instead of depending on tribal knowledge. There are plenty of cases where even the \"slow\" implementation is more than fast enough, and there are also plenty of cases where the \"correct\" solution (from a big-O or intuition perspective) is actually slower than the dumb case. Intuition helps, you have to measure and/or look at the compiled results if you want to ensure correct numbers. An example that really annoys me is how every whiteboard interview ends up being \"interesting ways to use a hashmap\", which isn't inherently an issue, but they will usually be so small-scoped that an iterative \"array of pairs\" might actually be cheaper than paying the up-front cost of hashing and potentially dealing with collisions. Interviews almost always ignore constant factors, and that's fair enough, but in reality constant factors can matter, and we're training future employees to ignore that. I'll say it again: as far as I can tell, you have to measure if you want to know if your result is \"faster\". \"Measuring\" might involve memory profilers, or dumb timers, or a mixture of both. Gut instincts are often wrong. reply klyrs 13 hours agoparentprev> A large part of becoming a decent engineer [2] for me was learning to stop trusting what professors taught me in college When I was taught about performance, it was all about benchmarking and profiling. I never needed to trust what my professors taught, because they taught me to dig in and find the truth for myself. This was taught alongside the big-O stuff, with several examples where \"fast\" algorithms are slower on small inputs. reply TylerE 13 hours agorootparentHow do you even get meaningful profiling out of most modern langs? It seems the vast majority of time and calls gets spent inside tiny anonymous functions, GC allocations, and stuff like that. reply klyrs 12 hours agorootparentI don't use most modern langs! And especially if I'm doing work where performance is critical, I won't kneecap myself by using a language that I can't reasonably profile. reply neonsunset 11 hours agorootparentprevThis is easy in most modern programming languages. JVM ecosystem has IntelliJ Idea profiler and similar advanced tools (AFAIK). .NET has VS/Rider/dotnet-trace profilers (they are very detailed) to produce flamegraphs. Then there are native profilers which can work with any AOT compiled language that produces canonically symbolicated binaries: Rust, C#/F#(AOT mode), Go, Swift, C++, etc. For example, you can do `samply record ./some_binary`[0] and then explore multi-threaded flamegraph once completed (I use it to profile C#, it's more convenient than dotTrace for preliminary perf work and is usually more than sufficient). [0] https://github.com/mstange/samply reply leetcrew 15 hours agoparentprevagreed, especially in cases like this. final is primarily a way to prohibit overriding methods and extending classes, and it indicates to the reader that they should not be doing this. use it when it makes conceptual sense. that said, c++ is usually a language you use when you care about performance, at least to an extent. it's worth understanding features like nrvo and rewriting functions to allow the compiler to pick the optimization if it doesn't hurt readability too much. reply jollyllama 14 hours agoparentprevI've encountered similar situations before. It's insane to me when people hold up PRs over that kind of thing. reply zmj 12 hours agoparentprev.NET is a little smarter about switch code generation these days: https://github.com/dotnet/roslyn/pull/66081 reply trueismywork 13 hours agoparentprevThere's not yet a culture of writing reproducible benchmarks to gage these effects. reply mynameisnoone 7 hours agoparentprevYep. \"Profiling or it didn't happen.\" The issue is that it's essentially impossible for even the most neckbeard of us to predict with a high degree of accuracy and precision the performance on modern systems impact of change A vs. change B due to the unpredictable nature of the many variables that are difficult to control including compiler optimization passes, architecture gotchas (caches, branch misses), and interplay of quirks on various platforms. Therefore, irreducible and necessary work to profile the differences become the primary viable path to resolving engineering decision points. Hopefully, LLMs now and in the future will be able to help build out boilerplate roughly in the direct of creating such profiling benchmarks and fixtures. PS: I'm presently revisiting C++14 because it's the most universal statically-compiled language to quickly answer interview problems. It would be unfair to impose Rust, Go, Elixir, or Haskell on an interviewer software engineer. reply pjmlp 2 hours agorootparentI would say it would be safer to go up to C++17, and there are some goodies there, specially for better compile time stuff. reply KerrAvon 10 hours agoparentprev> `if(x==1) elseif(x==2)...` because switch was \"faster\" and rejected my PR Yeah, that's never been true. Old compilers would often compile a switch to __slower__ code because they'd tend to always go to a jump table implementation. A better reason to use the switch is because it's better style in C-like languages. Using an if statement for that sort of thing looks like Python; it makes the code harder to maintain. reply dosshell 14 hours agoparentprev> I can get away with a smaller sized float When talking about not assuming optimizations... 32bit float is slower than 64bit float on reasonable modern x86-64. The reason is that 32bit float is emulated by using 64bit. Of course if you have several floats you need to optimize against cache. reply jcranmer 13 hours agorootparentUm... no. This is 100% completely and totally wrong. x86-64 requires the hardware to support SSE2, which has native single-precision and double-precision instructions for floating-point (e.g., scalar multiply is MULSS and MULSD, respectively). Both the single precision and the double precision instructions will take the same time, except for DIVSS/DIVSD, where the 32-bit float version is slightly faster (about 2 cycles latency faster, and reciprocal throughput of 3 versus 5 per Agner's tables). You might be thinking of x87 floating-point units, where all arithmetic is done internally using 80-bit floating-point types. But all x86 chips in like the last 20 years have had SSE units--which are faster anyways. Even in the days when it was the major floating-point units, it wasn't any slower, since all floating-point operations took the same time independent of format. It might be slower if you insisted that code compilation strictly follow IEEE 754 rules, but the solution everybody did was to not do that and that's why things like Java's strictfp or C's FLT_EVAL_METHOD were born. Even in that case, however, 32-bit floats would likely be faster than 64-bit for the simple fact that 32-bit floats can safely be emulated in 80-bit without fear of double rounding but 64-bit floats cannot. reply dosshell 12 hours agorootparentI agree with you. It should take the same time when thinking more about it. I remember learning this in ~2016 and I did performance test on Skylake which confirmed (Windows VS2015). I think I remember that i only tested with addsd/addss. Definitely not x87. But as always, if the result can not be reproduced... I stand corrected until then. reply dosshell 36 minutes agorootparentI tried to reproduce it on Ivybridge (Windows VS20122) and failed (mulss and muldd) [0]. single and double precision takes the same time. I also found a behavior where the first batch of iterations takes more time regardless of precision. It is possible that this tricked me last time. [0] https://gist.github.com/dosshell/495680f0f768ae84a106eb054f2... Sorry for the confusion and spreading false information. reply tombert 14 hours agorootparentprevSure, I clarified this in a sibling comment, but I kind of meant that I will use the slower \"money\" or \"decimal\" types by default. Usually those are more accurate and less error-prone, and then if it actually matters I might go back to a floating point or integer-based solution. reply sgerenser 14 hours agorootparentprevI think this is only true if using x87 floating point, which anything computationally intensive is generally avoiding these days in favor of SSE/AVX floats. In the latter case, for a given vector width, the cpu can process twice as many 32 bit floats as 64 bit floats per clock cycle. reply dosshell 14 hours agorootparentYes, as I wrote, it is only true for one float value. SIMD/MIMD will benefit of working on smaller width. This is not only true because they do more work per clock but because memory is slow. Super slow compared to the cpu. Optimization is alot about cache misses optimization. (But remember that the cache line is 64 bytes, so reading a single value smaller than that will take the same time. So it does not matter in theory when comparing one f32 against one f64) reply andrewla 15 hours agoprevI'm surprised that it has any impact on performance at all, and I'd love to see the codegen differences between the applications. Mostly the `final` keyword serves as a compile-time assertion. The compiler (sometimes linker) is perfectly capable of seeing that a class has no derived classes, but what `final` assures is that if you attempt to derive from such a class, you will raise a compile-time error. This is similar to how `inline` works in practice -- rather than providing a useful hint to the compiler (though the compiler is free to treat it that way) it provides an assertion that if you do non-inlinable operations (e.g. non-tail recursion) then the compiler can flag that. All of this is to say that `final` can speed up runtimes -- but it does so by forcing you to organize your code such that the guarantees apply. By using `final` classes, in places where dynamic dispatch can be reduced to static dispatch, you force the developer to not introduce patterns that would prevent static dispatch. reply sixthDot 3 hours agoparent> I'd love to see the codegen differences between the applications There are two applications, dynamic calls and dynamic casts. Dynamic casts to final classes dont require to check the whole inheritance chain. Recently done this in styx [0]. The gain may appear marginal, e.g 3 or 4 dereferences saved but in programs based on OOP you can easily have *Billions* of dynamic casts saved. [0]: https://gitlab.com/styx-lang/styx/-/commit/62c48e004d5485d4f.... reply bgirard 15 hours agoparentprev> The compiler (sometimes linker) is perfectly capable of seeing that a class has no derived classes How? The compiler doesn't see the full program. The linker I'm less sure about. If the class isn't guaranteed to be fully private wouldn't an optimizing linker have to be conservative in case you inject a derived class? reply GuB-42 15 hours agoparentprev\"inline\" is confusing in C++, as it is not really about inlining. Its purpose is to allow multiple definitions of the same function. It is useful when you have a function defined in a header file, because if included in several source files, it will be present in multiple object files, and without \"inline\" the linker will complain of multiple definitions. It is also an optimization hint, but AFAIK, modern compiler ignore it. reply fweimer 15 hours agorootparentGCC does not ignore inline for inlining purposes: Need a way to make inlining heuristics ignore whether a function is inline https://gcc.gnu.org/bugzilla/show_bug.cgi?id=93008 (Bug saw a few updates recently, that's how I remembered.) As a workaround, if you need the linkage aspect of the inline keyword, you currently have to write fake templates instead. Not great. reply lelanthran 4 hours agorootparentprev> It is useful when you have a function defined in a header file, because if included in several source files, it will be present in multiple object files, and without \"inline\" the linker will complain of multiple definitions. Traditionally you'd use `static` for that use case, wouldn't you? After all, `inline` can be ignored, `static` can't. reply pjmlp 2 hours agorootparentNo, because that would make it internal to each object file, while what you want is for all object files to see the same memory location. reply lelanthran 2 hours agorootparent> No, because that would make it internal to each object file, while what you want is for all object files to see the same memory location. I can see exactly one use for an effect like that: static variables within the function. Are there any other uses? reply pjmlp 1 hour agorootparentGlobal variables and the magic of a build system based on C semantics. reply jacoblambda 14 hours agorootparentprevThe thing with `inline` as an optimisation is that it's not about optimising by inlining directly. It's a promise about how you intend to use the function. It's not just \"you can have multiple definitions of the same function\" but rather a promise that the function doesn't need to be address/pointer equivalent between translation units. This is arguably more important than inlining directly because it means the compiler can fully deduce how the function may be used without any LTO or other cross translation unit optimisation techniques. Of course you could still technically expose a pointer to the function outside a TU but doing so would be obvious to the compiler and it can fall back to generating a strictly conformant version of the function. Otherwise however it can potentially deduce that some branches in said function are unreachable and eliminate them or otherwise specialise the code for the specific use cases in that TU. So it potentially opens up alternative optimisations even if there's still a function call and it's not inlined directly. reply ack_complete 8 hours agorootparentprev> \"inline\" is confusing in C++, as it is not really about inlining. Its purpose is to allow multiple definitions of the same function. No, its purpose was and is still to specify a preference for inlining. The C++ standard itself says this: > The inline specifier indicates to the implementation that inline substitution of the function body at the point of call is to be preferred to the usual function call mechanism. https://eel.is/c++draft/dcl.inline reply lqr 14 hours agorootparentprev10 years ago it was already folklore that compilers ignore the \"inline\" keyword when optimizing, but that was false for clang/llvm: https://stackoverflow.com/questions/27042935/are-the-inline-... reply wredue 15 hours agorootparentprevI believe the wording I’ve seen is that compilers may not respect the inline keyword, not that it is ignored. reply lanza 14 hours agoparentprev> Mostly the `final` keyword serves as a compile-time assertion. The compiler (sometimes linker) is perfectly capable of seeing that a class has no derived classes That's incorrect. The optimizer has to assume everything escapes the current optimization unit unless explicitly told otherwise. It needs explicit guarantees about the visibility to figure out the extent of the derivations allowed. reply wheybags 15 hours agoparentprevWhat if I dlopen a shared object that contains a derived class, then instantiate it. You cannot statically verify that I won't. Or you could swap out a normally linked shared object for one that creates a subclass. Etc etc. This kind of stuff is why I think shared object boundaries should be limited to the lowest common denominator (basically c abi). Dynamic linking high level languages was a mistake. The only winning move is not to play. reply mgraczyk 15 hours agoprevThe main case where I use final and where I would expect benefits (not covered well by the article) is when you are using an external library with pure virtual interfaces that you implement. For example, the AWS C++ SDK uses virtual functions for everything. When you subclass their classes, marking your classes as final allows the compiler to devirtualize your own calls to your own functions (GCC does this reliably). I'm curious to understand better how clang is producing worse code in these cases. The code used for the blog post is a bit too complicated for me to look at, but I would love to see some microbenchmarks. My guess is that there is some kind of icache or code side problem. where inlining more produces worse code. reply cogman10 14 hours agoparentCould easily just be a bad optimization pathway. `final` tells the compiler that nothing extends this class. That means the compiler can theoretically do things like inlining class methods and eliminate virtual method calls (perhaps duplicating the method)? However, it's quite possible that one of those optimizations makes the code bigger or misaligns things with the cache in unexpected ways. Sometimes, a method call can bet faster than inlining. Especially with hot loops. All this being said, I'd expect final to offer very little benefit over PGO. Its main value is the constraint it imposes and not the optimization it might enable. reply lpapez 1 hour agoparentprev> For example, the AWS C++ SDK uses virtual functions for everything. When you subclass their classes, marking your classes as final allows the compiler to devirtualize your own calls to your own functions (GCC does this reliably). I want to ask, and I sincerely mean no snark, what is the point? When working with AWS through an SDK your code will spend most of the time waiting on network calls. What is the point of devirtualizing your function calls to save an indirection when you will be spending several orders of magnitude more time just waiting for the RPC to resolve? It just doesn't seem like something even worth thinking about at all. reply mgraczyk 1 hour agorootparentYeah that's was just the first public C++ library with this pattern that popped into my head. I just make all my classes final out of habit and don't think about it. I remove final if I want to subclass, but that almost never happens. reply akoboldfrying 11 hours agoprevI would expect \"final\" to have no effect on this type of code at all. That it does in some cases cause measurable differences I put down to randomly hitting internal compiler thresholds (perhaps one of the inlining heuristics is \"Don't inline a function with more than 100 tokens\", and the \"final\" keyword pushes a couple of functions to 101). Why would I expect no performance difference? I haven't looked at the code, but I would expect that for each pixel, it iterates through an array/vector/list etc. of objects that implement some common interface, and calls one or more methods (probably something called intersectRay() or similar) on that interface. By design, that interface cannot be made final, and that's what counts. Whether the concrete derived classes are final or not makes no difference. In order to make this a good test of \"final\", the pointer type of that container should be constrained to a concrete object type, like Sphere. Of course, this means the scene is limited to spheres. The only case where final can make a difference, by devirtualising a call that couldn't otherwise be devirtualised, is when you hold a pointer to that type, and the object it points at was allocated \"uncertainly\", e.g., by the caller. (If the object was allocated in the same basic block where the method call later occurs, the compiler already knows its runtime type and will devirtualise the call anyway, even without \"final\".) reply koyote 10 hours agoparent> (perhaps one of the inlining heuristics is \"Don't inline a function with more than 100 tokens\", and the \"final\" keyword pushes a couple of functions to 101). That definitely is one of the heuristics in MSVC++. We have some performance critical code and at one point we noticed a slowdown of around ~4% in a couple of our performance tests. I investigated but the only change to that code base involved fixing up an error message (i.e. no logic difference and not even on the direct code path of the test as it would not hit that error). Turns out that: int some_func() { if (bad) throw std::exception(\"Error\"); return some_int; } Inlined just fine, but after adding more text to the exception error message it no longer inlined, causing the slow-down. You could either fix it with __forceinline or by moving the exception to a function call. reply Maxatar 10 hours agorootparentSince the inlining is performed in MSVC's backend, as opposed to its frontend, and hence operates strictly on MSVC's intermediate representation which lacks information about tokens or the AST, it's unlikely due to tokens. std::exception does not take a string in its constructor, so most likely you used std::runtime_error. std::runtime_error has a pretty complex constructor if you pass into it a long string. If it's a small string then there's no issue because it stores its contents in an internal buffer, but if it's a longer string then it has to use a reference counting scheme to allow for its copy constructor to be noexcept. That is why you can see different behavior if you use a long string versus a short string. You can also see vastly different codegen with plain std::string as well depending on whether you pass it a short string literal or a long string literal. reply koyote 10 hours agorootparent> std::exception does not take a string in its constructor You're right, I used it as a short-hand for our internal exception function, forgetting that the std one does not take a string. Our error handling function is a simple static function that takes an std::string and throws a newly constructed object with that string as a field. But yes, it could very well have been that the string surpassed the short string optimisation threshold or something similar. I did verify the assembly before and after and the function definitely inlined before and no longer inlined after. Moving the 'throw' (and, importantly, the string literal) into a separate function that was called from the same spot ensured it inlined again and the performance was back to normal. reply akoboldfrying 10 hours agorootparentprevWow, I had no idea. And I thought I knew about most of C++'s weirdnesses. reply simonask 3 hours agoparentprevActually, the compiler can only implicitly devirtualize under very specific circumstances. For example, it cannot devirtualize if there was previously a non-inlined call through the same pointer. The reason is placement new. It is legal (given that certain invariants are upheld) in C++ to say `new(this) DerivedClass`, and compilers must assume that each method could potentially have done this, changing the vtable pointer of the object. The `final` keyword somewhat counteracts this, but even GCC still only opportunistically honors it - i.e. it inserts a check if the vtable is the expected value before calling the devirtualized function, falling back on the indirect call. reply akoboldfrying 47 minutes agorootparentFascinating, though a little sad. Are there any important kinds of behaviour that can only be implemented via this `new(this) DerivedClass` chicanery? Because if not, it seems a shame to make the optimiser pay such a heavy price just to support it. reply ein0p 15 hours agoprevYou should use final to express design intent. In fact I’d rather it were the default in C++, and there was some sort of an opposite (‘derivable’?) keyword instead, but that ship has sailed long time ago. Any measurable negative perf impact should be filed as a bug and fixed. reply leni536 15 hours agoparentC++ doesn't have the fragile base problem, as members aren't virtual my default. The only concern with unintended inheritance is with polymorhpic deletion. \"final\" on class definition disables some tricks thag you can do with private inheritance. Having said that \"final\" on member functions is great, and I like to see that instead of \"override\". reply pjmlp 2 hours agorootparentAll OOP languages have it, the issue is related to changing the behaviour of the base class, and the change introducing unforceen consequences on the inheritance tree. Changing an existing method way of calling (regular, virtual, static), changing visibility, overloading, introducing a name that clashes downstream, introducing a virtual destructor, making a data member non-copyable,... reply jstimpfle 1 hour agorootparentprevNow try a regular function, you will be blown away. No need to type \"final\"... reply josefx 15 hours agoparentprevIntent is nice and all that, but I would like a \"nonwithstanding\" keyword instead that just lets me bypass that kind of \"intent\" without having to copy paste the entire implementation just to remove a pointless keyword or make a destructor public when I need it. reply cesarb 15 hours agoparentprev> In fact I’d rather it were the default in C++, and there was some sort of an opposite (‘derivable’?) keyword instead Kotlin (which uses the equivalent of the Java \"final\" keyword by default) uses the \"open\" keyword for that purpose. reply jbverschoor 15 hours agoparentprevIn general, I think things should be strict by default. Way easier to optimize and less error prone. reply ndesaulniers 10 hours agoprevAs an LLVM developer, I really wish the author filed a bug report and waited for some analysis BEFORE publishing an article (that may never get amended) that recommends not using this keyword with clang for performance reasons. I suspect there's just a bug in clang. reply mastax 12 hours agoprevChanges in the layout of the binary can have large impacts on the program performance [0] so it's possible that the unexpected performance decrease is caused by unpredictable changes in the layout of the binary between compilations. I think there is some tool which helps ensure layout is consistent for benchmarking, but I can't remember what it's called. [0]: https://research.facebook.com/publications/bolt-a-practical-... reply alex_smart 5 hours agoprevOne thing that wasn't mentioned in the article that I wished it did was the size of the compiled binary with and without final. Only reason I would expect the final version to be slower is that we are emitting more code because of inlining and that is resulting in a larger portion of instruction cache misses. Also, now that I think of it, they should have run the code under perf and compared the stats. reply MathMonkeyMan 9 hours agoprevI think it was Chandler Carruth who said \"If you're not measuring, then you don't care about performance.\" I agree, and by that measure, nobody I've ever worked with cares about performance. The best I'll see is somebody who cooked up a naive microbenchmark to show that style 1 takes fewer wall nanoseconds than style 2 on his laptop. People I've worked with don't use profilers, claiming that they can't trust it. Really they just can't be bothered to run it and interpret the output. The truth is, most of us don't write C++ because of performance; we write C++ because that's the language the code is written in. The performance gained by different C++ techniques seldom matters, and when it does you have to measure. Profiler reports almost always surprise me the first few times -- your mental model of what's going on and what matters is probably wrong. reply scottLobster 9 hours agoparentIt matters to some degree. If it's just a simple technique you can file away and repeat as muscle memory, well that means your code is that much better. From a user perspective it could be the difference between software that's pleasant to use and software that's annoying to use. From a philosophical perspective it's the difference between software that functions vs software that works well. Of course it depends on your context as to whether this is valued, but I wouldn't dismiss it. Once person's micro-optimization is another person's polish. reply bluGill 15 hours agoprevI use final more for communication. Don't look for deeper derived classes as there are none. that it results in slower code is an annoying surprise. reply juliangmp 1 hour agoprev>Personally, I'm not turning it on. And would in fact, avoid using it. It doesn't seem consistent. I feel like we'd have to repeat these tests quite a few times to get to a decent conclusion. Hell small variations in performance could be caused by all sorts of things outside the actual program. reply kreetx 1 hour agoparentAFAIU, these tests were ran 30 times each and apparently some took minutes to run, so it's unlikely that you'll get any different conclusions. reply leni536 13 hours agoprevThis is the gist of the difference in code generation when final is involved: https://godbolt.org/z/7xKj6qTcj edit: And a case involving inlining: https://godbolt.org/z/E9qrb3hKM reply pklausler 9 hours agoprevMildly related programming language trivia: Fortran has virtual functions (\"type bound procedures\"), and supports a NON_OVERRIDABLE attribute on them that is basically \"final\". (FINAL exists but means something else.). But it also has a means for localizing the non-overridable property. If a type bound procedure is declared in a module, and is PRIVATE, then overrides in subtypes (\"extended derived types\") work as usual for subtypes in the same module, but can't be affected by overrides that appear in other modules. This allows a compiler to notice when a type has no subtypes in the same module, and basically infer that it is non-overridable locally, and thus resolve calls at compilation time. Or it would, if compilers implemented this feature correctly. It's not well described in the standard, and only half of the Fortran compilers in the wild actually support it. So like too many things in the Fortran world, it might be useful, but it's not portable. reply JackYoustra 15 hours agoprevI really wish he'd listed all the flags he used. To add on to the flags already listed by some other commenters, `-mcpu` and related flags are really crucial in these microbenchmarks: over such a small change and such a small set of tight loops, you could just be regression on coincidences in the microarchitecture scheduler vs higher level assumptions. reply j_not_j 11 hours agoparentAnd he didn't repeat each test case 5 or 9 times, and take the median (or even an average). There will be operating system noise that can be in the multi-percent range. This is defined as various OS services that run \"in the background\" taking up cpu time, emptying cache lines (which may be most important), and flushing a few translate lookaside entries. Once you recognize the variability from run to run, claiming \"1%\" becomes less credible. Depending on the noise level, of course. Linux benchmarks like SPECcpu tend to be run in \"single-user mode\" meaning almost no background processes are running. reply gpderetta 15 hours agoprev1% is nothing to scoff of. But I suspect that the variability of compilation (specifically quirks of instruction selection, register allocation and function alignment) more than mask any gains. The clang regression might be explainable by final allowing some additional inlining and clang making an hash of it. reply teeuwen 3 hours agoprevI do not see how the final keyword would make a difference in performance at all in this case. The compiler should be able to build an inheritance tree and determine by itself which classes are to be treated as final. Now for libraries, this is a different story. There I can imagine final keyword could have an impact. reply connicpu 3 hours agoparentBut dynamically loaded libraries exist, so even if it knows the class is the most derived version out of all classes that exist in all of the statically-linked code through LTO or something, unless it can see the instantiation site it won't be able to devirtualize the function calls without the class being marked as final. reply pjmlp 2 hours agoparentprevOnly if the complete source code is available to the compiler. reply lanza 14 hours agoprevIf you're measuring a compiler you need to post the flags and version used. Otherwise the entire experiment is in the noise. reply fransje26 12 hours agoprevI'm actually more worried about Clang being close to 100% slower than GCC on Linux. That doesn't seem right. I am prepared to believe that there is some performance difference between the two, varying per case, but I would expect a few percent difference, not twice the run time.. reply sfink 15 hours agoprevtldr: sprinkled a keyword around in the hopes that it \"does something\" to speed things up, tested it, got noisy results but no miraculous speedup. I started skimming this article after a while, because it seemed to be going into the weeds of performance comparison without ever backing up to look at what the change might be doing. Which meant that I couldn't tell if I was going to be looking at the usual random noise of performance testing or something real. For `final`, I'd want to at least see if it changing the generated code by replacing indirect vtable calls with direct or inlined calls. It might be that the compiler is already figuring it out and the keyword isn't doing anything. It might be that the compiler is changing code, but the target address was already well-predicted and it's perturbing code layout enough that it gets slower (or faster). There could be something interesting here, but I can't tell without at least a little assembly output (or perhaps a relevant portion of some intermediate representation, not that I would know which one to look at). If it's not changing anything, then perhaps there could be an interesting investigation into the variance of performance testing in this scenario. If it's changing something, then there could be an interesting investigation into when that makes things faster vs slower. As it is, I can't tell what I should be looking for. reply akoboldfrying 10 hours agoparent>changing the generated code by replacing indirect vtable calls with direct or inlined calls It can't possibly be doing this, if the raytracing code is like any other raytracer I've ever seen -- since it must be looping through a list of concrete objects that implement some shared interface, calling intersectRay() on each one, and the existence of those derived concrete object types means that that shared interface can't be made final, and that's the only thing that would enable devirtualisation -- it makes no difference whether the concrete derived types themselves are final or not. reply sgerenser 15 hours agoparentprevThis is what I was waiting for too. Especially with the large regression on Clang/Ubuntu. Maybe he uncovered a Clang/LLVM codegen bug, but you’d need to compare the generated assembly to know. reply drivebycomment 7 hours agoparentprev+1. On modern hardware and software systems, performance is effectively stochastic to some degree, as small random perturbations to the input (code, data, environments, etc) can have arbitrary effects for the performance. This means you can't draw a direct causal chain / mechanism from what you changed to the performance change - when it matters, you do need to do a deeper analysis and investigation to find the actual and full causal chain. I.e. a correlation is not a causation, and especially more so on modern hardware and software systems. reply magnat 13 hours agoprev> I created a \"large test suite\" to be more intensive. On my dev machine it needed to run for 8 hours. During such long and compute-intensive tests, how are thermal considerations mitigated? Not saying that this was case here, but I can see how after saturating all cores for 8 hours, the whole PC might get hot to the point CPU starts throttling, so when you reboot to next OS or start another batch, overall performance could be a bit lower. reply lastgeniusua 13 hours agoparenthaving recently done similar day-and-night long suites of benchmarks (on a laptop in heat dissipation conditions worse than on any decent desktop), I've found that there is no correlation between the order the benchmarks are run in and their performance (or energy consumption!). i would therefore assume that a non-overclocked processor would not exhibit the patterns you are thinking of here reply kookamamie 4 hours agoprev> And probably, that reason is performance. That's the first problem I see with the article. C++ isn't a fast language, as it is. There are far too many issues with e.g. aliasing rules, lack of proper vectorization (for the runtime arch), etc. If you wish to have a relatively good performance for your code, try ISPC, which still allows you to get great performance with vectorization up to AVX-512, without turning to intrisics. reply chipdart 4 hours agoparent> That's the first problem I see with the article. C++ isn't a fast language, as it is. There are far too many issues with e.g. aliasing rules, lack of proper vectorization (for the runtime arch), etc. That's a bold statement due to the way it heavily contrasts with reality. C++ is ever present in high performance benchmarks as either the highest performing language or second only to C. It's weird seeing someone claim with a straight face that \"C++ isn't a fast language, as it is\". To make matters worse, you go on confusing what a programming language is, and confusing implementation details with language features. It's like claiming that C++ isn't a language for computational graphics just because no C++ standard dedicates a chapter to it. Just like every engineering domain,you need to have deep knowledge on details to milk the last drop of performance improvements out of a program. Low-latency C++ is a testament of how the smallest details can be critical of performance. But you need to be completely detached from reality to claim that C++ isn't a fast language. reply kookamamie 3 hours agorootparent> That's a bold statement due to the way it heavily contrasts with reality. I'm ready to back this up. And no, I'm not confusing things - I work in HPC (realtime computer vision) and in reality the only thing we'd use C++ for is \"glue\", i.e. binding implementations of the actual algorithms implemented in other languages together. Implementations could be e.g. in CUDA, ISPC, neural-inference via TensorRT, etc. reply jpc0 53 minutes agorootparent\"We use extreme vectorisation and can't do it in native C++ therefore the language is slow\" You a junior or something? For 99% of use cases C++ autovectorisation does plenty and will outperform the same code written in higher level languages. You are literally in the 1% and conflating your use case for that of the general case... reply indigoabstract 15 hours agoprevIf it does have a noticeable impact, that would be surprising, a bit like going back to the days when 'inline' was supposed to tell the compiler to inline the designated functions (no longer its main use case nowadays). reply headline 6 hours agoprevre: final macro > I would never do this in an actual product what, why? reply jcalvinowens 15 hours agoprevThat's interesting. Maybe final enabled more inlining, and clang is being too aggressive about it for the icache sizes in play here? I'd love to see a comparison of the generated code. I'm disappointed the author's conclusion is \"don't use final\", not \"something is wrong with clang\". reply ot 15 hours agoparentOr \"something is wrong with my benchmark setup\", which is also a possibility :) Without a comparison of generated code, it could be anything. reply pineapple_sauce 15 hours agoprevWhat should be evaluated is removing indirection and tightly packing your data. I'm sure you'll gain a better performance improvement. virtual calls and shared_ptr are littered in the codebase. In this way: you can avoid the need for the `final` keyword and do the optimization the keyword enables (de-virtualize calls). >Yes, it is very hacky and I am disgusted by this myself. I would never do this in an actual product Why? What's with the C++ community and their disgust for macros without any underlying reasoning? It reminds me of everyone blindly saying \"Don't use goto; it creates spaghetti code\". Sure, if macros are overly used: it can be hard to read and maintain. But, for something simple like this, you shouldn't be thinking \"I would never do this in an actual product\". reply sfink 15 hours agoparentMacros that are giving you some value can be ok. In this case, once the performance conclusion is reached, the only reason to continue using a macro is if you really need the `final`ity to vary between builds. Otherwise, just delete it or use the actual keyword. (But I'm worse than the author; if I'm just comparing performance, I'd probably put `final` everywhere applicable and then do separate compiles with `-Dfinal=` and `-Dfinal=final`... I'd be making the assumption that it's something I either always or never want eventually, though.) reply jandrewrogers 13 hours agoparentprevIn modern C++, macros are a viewed as a code smell because they are strictly worse than alternatives in almost all situations. It is a cultural norm; it is a bit like using \"unsafe\" in Rust if not strictly required for some trivial case. The C++ language has made a concerted effort to eliminate virtually all use cases for macros since C++11 and replace them with type-safe first-class features in the language. It is a bit of a legacy thing at this point, there are large modern C++ codebases with no macros at all, not even for things like logging. While macros aren't going away, especially in older code, the cultural norm in modern C++ has tended toward macros being a legacy foot-gun and best avoided if at all possible. The main remaining use case for the old C macro facility I still see in new code is to support conditional compilation of architecture-specific code e.g. ARM vs x86 assembly routines or intrinsics. reply sgerenser 11 hours agorootparentBut how would one conditionally enable or disable the “final” keyword on class members without a preprocessor macro, even in C++23? reply jandrewrogers 10 hours agorootparentMacros are still useful for conditional compilation, as in this case. They've been sunsetted for anything that looks like code generation, which this isn't. I was more commenting on the reflexive \"ick\" reaction of the author to the use of macros (even when appropriate) because avoiding them has become so engrained in C++ culture. I'm a macro minimalist but I would use them here. Many people have a similar reaction to the use of \"goto\", even though it is absolutely the right choice in some contexts. reply bluGill 15 hours agoparentprevMacros in C are a text replace and so it is hard to see from a debugger how th code got like that. reply pineapple_sauce 14 hours agorootparentYes, I'm well aware of the definition of a macro in C and C++. Macros are simpler than templates. You can expand them with a compiler flag. reply bluGill 11 hours agorootparentwhen things get complex templete error messages are easier to follow. nobody makes complex macros but if you tried. (template error messeges are legendary for a reason. nested macros are worse) reply p0w3n3d 15 hours agoprevI would say the most performance impact would give `constexpr` followed by `const`. I wouldn't bet any money on `final` which in C++ is a guard of inheritance, and C++ function invocation address is resolved the `vtable` hence final wouldn't change anything. Maybe the author was mistaken with `final` keyword in Java reply adrianN 15 hours agoparentIn my experience the compiler is pretty good at figuring out what is constant so adding const is more documentation for humans, especially in C++, where const is more of a hint than a hard boundary. Devirtualization, as can happen when you add a final, or the optimizations enabled by adding a restrict to a pointer, are on the other hand often essential for performance in hot code. reply lelanthran 4 hours agorootparent> In my experience the compiler is pretty good at figuring out what is constant so adding const is more documentation for humans, In the same TU, sure. But across TU boundaries the compiler really can't figure out what should be const and what should not, so `const` in parameter or return values allows the compiler to tell the human \"You are attempting to make a modification to a value that some other TU put into RO memory.\", or issue similar diagnostics. reply bayindirh 14 hours agorootparentprevSince \"const\" makes things read-only, being const correct makes sure that you don't do funny things with the data you shouldn't mutate, which in turn eliminates tons of data bugs out of the gate. So, it's an opt-in security feature first, and a compiler hint second. reply Lockal 1 hour agorootparentHow does const affects code generation in C/C++? Last time I checked, const was purely informational. Compilers can't eliminate reads for const pointer data, because const_cast exists. Compilers can't eliminate double calls to const methods, because inside function definition such functions can still legally modify mutable variables (and have many side effects). What actually may help is __attribute__((pure)) and __attribute__((const)), but I don't see them often in real code (unfortunately). reply jey 15 hours agoprevI wonder if LTO was turned on when using Clang? Might lead to a performance improvement. reply jeffbee 14 hours agoprevI profiled this project and there are abundant opportunities for devirtualization. The virtual interface `IHittable` is the hot one. However, the WITH_FINAL define is not sufficient, because the hot call is still virtual. At `hit_object |= _objects[node->object_index()]->hit` I am still seeing ` mov (%rdi),%rax; call *0x18(%rax)` so the application of final here was not sufficient to do the job. Whatever differences are being measures are caused by bogons. reply akoboldfrying 10 hours agoparentAn interface, like IHittable, can't possibly be made final since its whole purpose is to enable multiple different concrete subclasses that implement it. As you say, that's the hot one -- and making the concrete subclasses themselves \"final\" enables no devirtualisations since there are no opportunities for it. reply gpderetta 14 hours agoparentprevI haven't looked at the code, but if you have multiple leaves, even marking all of them as final won't help if the call is through a base class. reply jeffbee 14 hours agorootparentYeah the practical cases for devirtualization are when you have a base class, a derived class that you actually use, and another derived class that you use in tests. For your release binary the tests aren't visible so that can all be devirtualized. In cases where you have Dog and Goose that both derive from Animal and then you have std::vector, what is the compiler supposed to do? reply kccqzy 7 hours agorootparentThe compiler simply knows that the actual dynamic type is Animal because it is not a pointer. You need Animal* to trigger all the fun virtual dispatch stuff. reply froh 4 hours agorootparentI intuit vector is what was meant... reply kasajian 5 hours agoprevI'm surprised by this article. the author genuinely believes that a language construct to benefit performance was added to the language without anyone ever running any metrics to verify. \"just trust me bro\", is the quote. It's is an insane level of ignorance about how these things are decided by the standards committee. reply kreetx 1 hour agoparentAnd yet, results from current compilers show that results are mixed, in summary not making programs faster. reply manlobster 7 hours agoprevThis seems like a reasonable use of the preprocessor to me. I've seen similar use in high-quality codebases. I wonder why the author is so disgusted by it. reply jeffbee 15 hours agoprevIt's difficult to discuss this stuff because the impact can be negligible or negative for one person, but large and consistently positive for another. You can only usefully discuss it on a given baseline, and for something like final I would hope that baseline would be a project that already enjoys PGO, LTO, and BOLT. reply chris_wot 8 hours agoprevSurely \"final\" is a conceptual thing... in other words, you don't want anyone else to derive from the class for good reasons. It's for conceptual understanding, surely? reply LorenDB 14 hours agoprev [–] Man, I wish this blog had an RSS feed. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The author examines the performance implications of using the `final` keyword in C++ within a ray tracing project, implementing it through preprocessor directives and testing it across different classes and interfaces.",
      "Results vary across platforms and compilers, with advice against using `final` specifically with Clang and MSVC, emphasizing the importance of tailoring decisions based on application requirements and performance factors.",
      "GitHub links are available for those interested in delving deeper into the project."
    ],
    "commentSummary": [
      "The article explores the impact of C++'s `final` keyword on devirtualization and inlining, noting how inlining can remove function calls while devirtualization removes pointer dereferencing.",
      "Emphasizing the complexities, trade-offs, and the necessity for improved compiler heuristics in modern hardware regarding inlining and devirtualization.",
      "Transitioning to new .NET versions offers notable performance boosts, with insights on C++ optimizations, using switch statements for clarity, and the significance of prioritizing performance optimization from the beginning."
    ],
    "points": 196,
    "commentCount": 190,
    "retryCount": 0,
    "time": 1713807164
  },
  {
    "id": 40118778,
    "title": "Hacker News Faces Onslaught of Suspicious Comments",
    "originLink": "https://news.ycombinator.com/item?id=40118778",
    "originBody": "Seeing a lot of spam comments in the last few minutes from accounts that all have similar names. Omitting name since it is NSFW.",
    "commentLink": "https://news.ycombinator.com/item?id=40118778",
    "commentBody": "Is Hacker News under attack from spam bots?194 points by joeyhage 14 hours agohidepastfavorite131 comments Seeing a lot of spam comments in the last few minutes from accounts that all have similar names. Omitting name since it is NSFW. geerlingguy 14 hours agoIt sure looks like it; every front page post has a dozen or so comments from unique bot accounts. Hopefully we don't see a 'I created a spam bot service to advertise on every HN post' soon. reply rjbwork 13 hours agoparentHave also seen them. They all have the same name with numbers at the end. Also getting a lot of \"sorry we can't service your request responses\" this past half hour or so. reply A_Duck 13 hours agoparentprevAlready was : https://news.ycombinator.com/item?id=39981911 Claimed at the time to be working on HN support reply mtmail 9 hours agorootparentI remember that website. I reported it to the moderators the same day and all related accounts got suspended. reply icepat 13 hours agoparentprevThis very thread is under attack by spam bots... reply mysteria 10 hours agoprevWhat a mess, this is literally the first time I saw something like this on HN. They've even started posting on this thread! HN has been running slow since the flood started and I wonder if it's causing a mini-DDoS effect. The usernames of the spammers are \"2genders\", \"SEXMCNIGGA\", and \"indianmilf\"; for some strange reason they keep the same prefix and just alter the number so it should be easy for admins to block them. Some of them are posting Twitter links as well. reply rockpods 1 hour agoparentThe reason is that they are just trying to cause chaos. They do not have any real goal other than that. reply troydavis 13 hours agoprevYup. The site being advertised is proxied through Cloudflare, and they're also using Supabase. Anyone from Cloudflare or Supabase care to remove your abusive customer? Also reported. reply fragmede 13 hours agoparentUnfortunately there's no proof that the spam accounts are linked to said site. If I were a competitor to the linked account and wanted to cause then damage, I could run a bot campaign purporting to be from them in order to get them kicked off their provider. reply troydavis 13 hours agorootparentThat’s possible, and is why the providers investigate (using the account history that we don’t have access to). Often, other customer data - or a 5 minute phone call - is enough for the provider to tell the difference. reply remram 4 hours agorootparentYou say this from experience? As a spammer or service provider investigator? reply umvi 13 hours agoparentprevAny other possible actions we can take for punishing these sorts of bad actors? reply esafak 10 hours agoparentprevThe founders are \"John Smith\" and \"Jane Doe\", and they're incorporated in Malta. Anyone who does business with this outfit has it coming. reply johnasmith 10 hours agorootparentNot all John Smiths are bad, promise. reply esafak 10 hours agorootparentI'm partial to the John Smith in The Man in the High Castle. I'm sure you are fine too but this spammer is besmirching your family's good name. reply madcow2011 10 hours agoprevI laughed pretty hard when I noticed the same issues and clicked the 'discuss' link and found that your post had been inundated with the comments you are referring to XD reply elwell 13 hours agoprevHas been loading slow for me. Also reddit seems to be down. And Google login on Twitter hung for me. reply ricopags 10 hours agoprevMight not be a 'coordinated attack' so much as the consequence of a referral[0] program in the age of AI [0] https://docs.google.com/forms/d/e/1FAIpQLSe52_7L-JqY6OqhL0FJ... reply MrFoof 10 hours agoprevWould love to see a postmortem once it's dealt with. reply krapp 10 hours agoparentThis forum is built with early 2000s technology. It doesn't even use two-factor authentication or captchas for creating accounts. This was honestly bound to happen sooner or later. reply mysteria 10 hours agorootparentI don't want them to require a email/phone for privacy's sake, but they should definitely have a captcha of sorts to limit bot accounts. reply huytersd 6 hours agorootparentRelying on security through obscurity goes away pretty quickly once you’re popular. reply EasyMark 8 hours agorootparentprevthat seems reasonable to me reply krapp 10 hours agorootparentprevI don't know... Hacker News keeps complaining that they want the old 90's web experience back, now it feels like some shitty PHP messageboard from back in the day getting pwned by script kiddies. Enjoy the nostalgia while it lasts I guess. reply mysteria 9 hours agorootparentI clicked on the Discord link in the spam message for fun, entered a random username, and immediately got asked to verify with my phone number before I could even read the messages in the chatroom. HN doesn't need to do that and I don't want them to do that, but a simple captcha or a proof of work algo like what Cloudflare uses would at least slow down the flood of bot accounts. reply Nuzzerino 8 hours agorootparentThat’s the Orwellian way that discord cracks down on users that it deems, for whatever reason, “suspicious”. It is tied to the IP address and anything else associated with it, but if you had an existing account before the flag then that account won't be flagged, only new accounts. There is no appeal process and they won’t even tell you what your offense was. Unfortunately I’ve had to pay for an extra cell phone line just to use the app for work. VOIP numbers are rejected and must be unique per account. In my case it was likely because I had the audacity to back up my chat messages with a script. After a few years I can make new accounts again but I feel like I’m playing Russian roulette every time I do. If you don’t use separate accounts for privacy someone can dump a list of potentially any known server you’ve ever been in. I knew it would be only a matter of time until something like this would happen: https://www.reddit.com/r/privacy/s/A5nvuZBLab reply raxxorraxor 2 hours agorootparentThis \"suspicious activity\" can even be triggered if you click the wrong invite link, although you have no way to tell where it leads you anyway. Discord sadly was pretty successful to lure in users and even a lot of devs build their community there. I think it is a bad choice because of lacking discoverability and the proprietary nature of the platform. It feels lively because it is a chat. But otherwise most projects are better hosted elsewhere. reply mysteria 8 hours agorootparentprevI don't use Discord anymore but the phone number thing seems new, in the past I was able to visit as a guest and be able to read messages but not chat. Then again Twitter and Reddit are doing the same thing now and forcing people to log in, so I'm not surprised. Considering how many community groups and open source projects now use a Discord in place of a public forum this looks like a disaster going forwards since all the information in there will become locked up. And of course the chats and internal discussion threads aren't indexed by search engines. reply alisonatwork 8 hours agorootparentI tried to join Discord during the pandemic when I lived in China and they forced phone number verification both on and off VPN (presumably because both VPNs and China IPs are considered untrustworthy, which annoyingly defeats the point of using VPN). Then I went to Canada and bought a local prepaid SIM, but the area code was not recognized as a valid phone number so I still couldn't sign up. It's very frustrating as a user to be region-locked on the supposedly open internet, but the real feeling of violation happens when companies layer phone number requirements on top of the region lock, which in many countries means that your government ID is now linked to the account, because you cannot buy a SIM without linking it to your ID. Truly a cyberpunk dystopia. reply satokema 6 hours agorootparentprevIt's a per-server setting. Ranges from phone verification to email verification to minimum account age. reply Nuzzerino 2 hours agorootparentThat isn’t what is being discussed. This is a separate account-wide lockout. reply throwaway598 8 hours agorootparentprevAs a non-Discord user, I'm glad I'm a non-Discord user. That sounds hellish. Is whatever's being gate-kept worth it? reply busymom0 8 hours agorootparentprevPretty sure I remember hacker news having cloudflare captchas some time ago. Maybe they enabled the \"attack mode\" back then? Not sure why it wasn't enabled today. @dang could answer maybe. reply wumeow 10 hours agoprevIt’s been happening for hours and is killing site performance. It’s all from brand new accounts. I don’t why account creation hasn’t been turned off yet. reply varenc 10 hours agoprevAnyone have some insight into the motivation of spam bot behavior? It doesn't make sense to me that they'd intentionally re-post the same link on a story 100+ times. Perhaps repeating the same link is good for SEO farming? Or somehow there's a belief that 100+ identical comments is more effective than just a few? Also the comments all seem to end with a 15 character random string, which I assume is just there to add entropy and avoid identical comment detection. reply troydavis 8 hours agoparentJust a guess: a teenager who learned to script and thought this was novel. This isn't an organized marketing campaign, it's a DoS with some links for amusement. Other than the volume, it's not even script kiddie level. Presumably they have access to proxy servers, though. reply gertop 8 hours agorootparentA teenager who just learned to script bested a forum for (and by!) the Silicon Valley elite? Shameful if true. But unsurprising. reply maximusdrex 14 hours agoprevClearly, I'm surprised there isn't a spam filter that detects this obvious attack. reply motoxpro 10 hours agoparentSeeing as there is usually no obvious spam attacks, there IS a filter. Assume competence :) https://paulgraham.com/spam.html reply fragmede 13 hours agoparentprevor the bot is taking advantage of holes in the existing spam filter that haven't been exploited before reply anigbrowl 10 hours agorootparentObviously you can't filter for every possibility in advance, but with a hands-on moderator and some regex it should be super-easy to throttle this. And as more than one person has pointed out, just shutting down new account creation/posting for 24 hours would be equally effective. I'm perplexed at how a mature site full owned and catering to network technologists is vulnerable to such a laughably crude attack. reply fragmede 10 hours agorootparentbut then you've shut down account creation for 24 hours. The site operators get to choose how they want to play it, but it seems they don't want to do that just yet. You're right that it's laughably crude though. Says a lot about things that this hasn't happened until now. reply anigbrowl 8 hours agorootparentbut then you've shut down account creation for 24 hours. But so what? The impact of that would be negligible, almost certainly less than that of having site performance go through the floor/become temporarily unreadable. It's not like a B2C product launch, and the target audience of HN is more or less optimally positioned to understand why one might deliberately interrupt service. reply TechDebtDevin 10 hours agoprevThere are apps currently make multi six figures a month with \"AI girlfriend services\". Not for me but it apparently is worth paying for to some people. But hell, one time I was scrolling through this hot person's Instagram and it took me a good minute or two to realize the whole account was a generative AI account, almost tricked me. Give it another decade and we can reevaluate. reply johnisgood 5 hours agoparenthttps://microsoft.com/en-us/research/project/vasa-1/ with AI-generated people! reply water-data-dude 8 hours agoprevOh my god, you aren’t kidding. As of right now, there’s 350 (plus or minus a few) dead spam comments at the bottom of this page. Someone obviously misplaced a decimal somewhere - you obviously don’t want to flood a forum with THAT many bot messages. reply Turing_Machine 8 hours agoparentI don't think we're dealing with the cream of the crop here. They're not even smart enough to have it make up new, plausible user names. reply renk 4 hours agorootparentTheir IPs and emails might tell a different story and this could also be a threat. But to what end? reply macintux 13 hours agoprevPer https://news.ycombinator.com/newcomments the flood stopped 2 minutes ago. reply notRobot 13 hours agoparenthttps://hn.algolia.com/?dateRange=all&page=0&prefix=true&que... For historical purposes Edit: nope, it's still ongoing, there are spam comments on this very thread from 2 minutes ago. The new comments link doesn't show dead comments. reply joeyhage 13 hours agoparentprevIt seems to be going in waves, and it also appears they are getting removed in batches. reply busymom0 13 hours agoparentprevOn this post alone, there are several of those comments after that. So, it's not stopped. reply owlninja 13 hours agoprevYep, I am sure it happens but this is the first time I've actually seen it! reply wumeow 12 hours agoprevKind of strange this is still going on. They’re all new accounts so why not just disable account creation? reply Xenoamorphous 14 hours agoprevhttps://paulgraham.com/spam.html reply thrwaway1337 10 hours agoprevIt's the day after the YC application deadline, so my hypothesis is resources that would otherwise be dealing with these script kiddies spamming HN are spread thin at the moment... reply skilled 14 hours agoprevInteresting that this wasn’t baked in as a preventative method for repeat usernames. Which is also ironic because why would this guy reuse the same username for his little spam campaign when it can be nuked in one line of code… Amateur stuff. Never seen it happen before though! reply riedel 13 hours agoparentReally wonder, if this kind of spam is the perfect application for an LLM based agent. reply marginalia_nu 13 hours agorootparentTo be fair, this kind of spam seems like you could output it with a shellscript. reply ataru 13 hours agorootparentDo we lack a captcha here? reply omoikane 10 hours agorootparentI just tried logging in and looks like there is a captcha now. There wasn't one before. reply krainboltgreene 3 hours agorootparentprevFor what, the pure joy of burning money? reply hawkice 10 hours agorootparentprevI mean, at least LLMs would have different text in different messages. reply joeyhage 13 hours agoparentprevI’m also surprised that slurs/slang/foul language in usernames is allowed unless the server is overwhelmed and things are slipping past the validation. reply hobs 12 hours agorootparenthttps://en.wikipedia.org/wiki/Scunthorpe_problem It's almost definitely them not filtering anything and letting the community manage it. reply krapp 10 hours agorootparentprevI think the \"validation\" is the mods happening to notice (usually because people who create such accounts get flagged, and dang gets notified of flags) and politely telling the person such usernames aren't allowed. reply gertop 8 hours agoparentprevI like how all of you keep demeaning the spammer as being amateur or less than script kiddie. And yet, he bested you, the supposedly experts at web dev and hyperscaling. You create trillions of dollars of value. And yet, your social hot spot is beyond laughably bad at handling that \"incompetent\" attacker. reply renk 1 hour agorootparentIs that you? yt/@gertop6402 Just guessing.. anywho, your nick signals SEO knowledge and reasons to hide. reply renk 4 hours agorootparentprevSince you are othering the parties concerned, mind sharing more info on the attacker? reply buildbot 13 hours agoprevInterestingly, reddit seems have gone down about 30-40 minutes ago too. reply freedomben 10 hours agoprevHN also seems to be responding very slowly, and in a couple of cases timing out on the request. It may be under a heavy load. reply jimmySixDOF 5 hours agoprevThoughts and Prayers with Dang during this attack ! reply nojvek 10 hours agoprevIMO there is likely huge demand for bots that are witty and can write occasional put a useful comment with a link every now and then. It’s going to be interesting how spam evolves. At-least spammers who aren’t lazy. Already many of the recruiting emails I get sound a lot human. They are bots though since they send at 9am everyday reply fragmede 9 hours agoparentGmail's got a send at 9am feature for humans to use tho reply AndrewOMartin 10 hours agoparentprevhttps://xkcd.com/810/ reply jacobrussell 10 hours agoprevI thought the same thing! Very interesting. I wonder if this is happening on other sites like X/Reddit. reply throwaway598 10 hours agoprevhttps://hn.algolia.com/?dateRange=all&page=0&prefix=false&qu... reply lwansbrough 10 hours agoprevBrave of this guy to link his Twitter. Quick way to get blackballed from every startup in the country. reply krainboltgreene 10 hours agoparentWe have no idea if the author of the link and the author of the twitter account and the person in the image of the twitter account are in any way related. reply EasyMark 8 hours agoparentprevit's likely just some randomly selected account that's real reply 47thpresident 13 hours agoprevAt the end of each spam message there is a unique 15 character string. Anyone know what purpose the string is supposed to serve? reply snthd 12 hours agoparentIt's a hash buster. https://en.wikipedia.org/wiki/Hash_buster reply joeyhage 13 hours agoparentprevPoor attempt at trying to make the URL unique possibly and prevent it from being blocked. Someone could easily block the domain or use regex to block comments with that domain. reply Bilal_io 10 hours agoprevAre you lonely and want to do something? Flag those spam comments. Yeah, I was surprised by the amount, it feels like an attack rather than spam. I hope this didn't interrupt Dang from something more important. reply paranoidrobot 10 hours agoparentFor anyone like me, unable to figure out how to do that - you click Reply, and then there's a 'flag' link. reply Bilal_io 10 hours agorootparentI tried at first. It's a losing game against this number of spammy comments. The temporary solution is to shadow ban the comments, the usernames seems to follow 2 naming schemas. Banning them completely will alert the attacker to change the naming schema, or to make it more random, which will make stopping them even more difficult. reply anigbrowl 10 hours agorootparentFor a bot-based DDOS attack it's easier to to do blanket shutdowns then it is for the attacker to whip up new randomization code. If it gets really bad just shutting down new account creation for 24 hours or making a timelag beween creation and posting is effective. reply consp 10 hours agorootparentprevOr click the timestamp. reply anigbrowl 10 hours agorootparentAll these approaches increase server load. You're likely not the only one doing so, so the server has to respond to a read and flag request for every person. I'm not sure how many flags it takes to kill a post, something like 6 iirc. When you multiply that by the number of spam posts it adds up to a lot. I'm honestly perplexed that HN doesn't have any kind of string filtering facility considering its centrality in the tech ecosystem. reply fragmede 9 hours agorootparentit's never needed it before, and it's not being run by a huge team. it's hard to make time to prepare for a thing that hasn't happened before when you're busy dealing with everything else. reply anigbrowl 8 hours agorootparentThis isn't the first spam/DDOS attack on HN, although it's one of the more severe. This is basic stuff for any kind of user-driven forum, going back to the BBS days. You are going to have periodic problems with trolls and spammers so you need to have some sort of mitigation process in place. That doesn't need to be some sort of top-heavy new technology, it can be as simple as the ability to add a few new scripting rules or wildcard matches on a few minutes notice, or hit the pause button on new account creation. reply cabbageears 8 hours agoparentprevfunction modifyElements(pSel, cSel, rxStr) { const regex = new RegExp(rxStr, 'i'); const pEls = document.querySelectorAll(pSel); pEls.forEach(pEl => { const fEl = pEl.querySelector(cSel); if (fEl && regex.test(fEl.textContent)) { pEl.style.display = 'none'; } }); } let rx = /(hi are u lonely|want (an )?ai gf?)/i; modifyElements(\".athing.comtr\", \".comment\", rx); People can add onto the regex as needed, I guess. I haven't seen enough of the comments to be more specific since that seemed to get them. :-/ reply mtmail 9 hours agoparentprevI flagged several hundred and can't find anything more to flag right now. Massive. reply fragmede 9 hours agoparentprevHopefully after this is all over, Dan can tell us if I'm wrong, but I don't know that flagging is actually useful, since it's all so obvious. it causes additional db hits to load the page and then flag it, when a search on the back end will easily find them without us doing anything. reply tithe 10 hours agoparentprevI'm doing so, but sometimes I can still see/reply to the comment despite me flagging it. Is a certain threshold of users required to flag a comment before it's removed? reply teensydata 13 hours agoprevReminds me of when I was working for a university in early 2000s. I set up WebBB for a student organization to use and after checking back a week later it was thousands of spam posts. reply Ancalagon 10 hours agoprevShould’ve used the AI to write better comments reply laborcontract 10 hours agoprevSite is effectively getting ddos’d right now reply anonzzzies 14 hours agoprevYep, guess the admins will have a busy day. Seems 10000s of accounts being created and used to spam ai sex bots. reply geerlingguy 14 hours agoparentLuckily the accounts are all prefixed with the exact same phrase, which should make cleanup easier. reply rich_sasha 13 hours agoprevWhat we need now is the bot to post here and demonstrate a total lack of sense of irony. reply herpdyderp 13 hours agoparentThis has happened now. I count 20 spam comments at the moment, though I expect they'll get removed shortly. reply crackercrews 10 hours agoprevGives new meaning to \"show hn\" reply EveryPizza 11 hours agoprevThey seem to also be spamming posts. reply mtmail 10 hours agoprev> Is it just me No, 1000s of bot accounts commenting 30+ per minute are quite obvious > Is it some kind of coordinated flood attack? Looks like it > And is an AI girlfriend really a feasible idea? It's the new penis enlargement and viagra spam reply nojvek 10 hours agoprevhttps://news.ycombinator.com/item?id=40115155 Yeah this thread is full of spam. reply slater 11 hours agoprevThey're back. At this point it might be worthwhile switching off new account registrations for a while? reply WarOnPrivacy 13 hours agoprevStill going on btw. We're getting fresh hot new spams as I write this. Diff link in the text. reply flemishgun 13 hours agoprevIs the GNAA alive and well??? reply bediger4000 10 hours agoprevThis is a old, very effective move from the spammer's playbook. If some entity protests effectively (penetrates the spammer's own anti-spam, anti-communication precautions), threaten to spam them harder. Then follow through. We're seeing some follow through, I reckon. reply zoklet-enjoyer 10 hours agoprevYeah, nobody here is going to go for that reply kertoip_1 13 hours agoprevSo if it is possible with comments, does it mean it is possible with voting? I'm wondering how many posts recently came to main page upvoted by bots reply Kikawala 13 hours agoparentBelow submission had over 1,000 votes before being flagged. https://news.ycombinator.com/item?id=40117443 reply pvg 13 hours agoparentprevIt's surely possible but it's not quite that easy, otherwise you'd see it daily in comments. It's similar for front page posts but harder since both users and moderators nuke spam-looking things as they are highly visible. reply Arnavion 13 hours agoparentprevHN doesn't have any integrity about voting in general. Look at the \"about\" text of https://news.ycombinator.com/user?id=pwdisswordfishc for example, and then look at the username. There is a whole family of pwdisswordfish* accounts btw. The \"b\" account's \"about\" text even has a holier-than-thou attitude about it. reply gus_massa 7 hours agorootparentIf you go to https://news.ycombinator.com/newest , from time to time there apears stories with 50 upvotes in 30 minutes, and perhaps a few sockpuppets/shills comments. If you do the math, they should be in the front page, but misteriously they aren't. So the conclussion is that HN has a secret feature that detect (some of) the tricks. I think I read some coment from pg or dang about voting rings, but it was a long time ago, but it has no details. The details are part of the secret sause. Also people flag strange threads, so the detection is not only automatic. If you notice something strange, you can send an email to dang: hn@ycombinator.com reply fragmede 13 hours agorootparentprevyou're assuming votes from those accounts are being included in vote counts reply Arnavion 13 hours agorootparentYou're missing the point that I didn't need to assume it. reply fragmede 13 hours agorootparentif you have a way to map votes to the account that made them, I'm all ears. reply tgv 13 hours agorootparentprevThat looks like a person, or a pretty good HNGPT. It's not your average spam. (Unless most comments have been deleted.) reply Arnavion 13 hours agorootparentWhen I said \"Look at the username\", I meant that. You're not going to get anything by analyzing the posting style of a shared account. reply wood_spirit 13 hours agoparentprevnext [2 more] [flagged] snakeyjake 13 hours agorootparentnext [2 more] [flagged] pvg 13 hours agorootparentWhatever the techbro religion is, it's seems to be lot less obvious, boring and common (at least on HN) than grandiose fearless-truthteller-of-strident-truths-the-sheeple-refuse-to-hear delusions. reply leovander 10 hours agoprevI assumed the spam was trying to bury this via DDoS: https://news.ycombinator.com/item?id=40117510 reply gnabgib 10 hours agoparentDoubtful: https://www.wired.com/story/north-korea-amazon-max-animation-exposed-server/ https://www.cnn.com/2024/04/22/politics/us-animation-studio-sketches-korean-server/index.html reply tadfisher 14 hours agoprevObviously, yes. reply spambot_12345 12 hours agoprevYep reply ergocoder 13 hours agoprevwell ARE YOU LONELY? It might be a lot of spams, but it seems to come from a single account using a single sentence. Spammers are getting lazy these days. reply omoikane 12 hours agoparent> single account I think that's actually multiple similarly named accounts with the same prefix. I believe there are rate limits on how fast a single account can post. reply Hamuko 12 hours agoparentprevImpossible to be lonely with this many bots keeping me company. reply elwell 13 hours agoprev [–] This is why we can't have nice things. reply SushiHippie 9 hours agoparent [–] > Certainly we can have nice things https://news.ycombinator.com/item?id=30387562 reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [],
    "commentSummary": [
      "Hacker News is facing a spam bot attack, with users pointing to a suspected bot service posting NSFW comments, sparking discussions on security measures, Discord, and AI-generated spam.",
      "Suggestions to fight the spam include reporting usernames, using captchas, and flagging suspicious activities, while users discuss the effectiveness of moderation, string filtering, and cleanup efforts.",
      "Users express concerns over platform integrity due to bots and sockpuppets influencing votes, analyzing posting styles and usernames to detect potential spam accounts amid a conversation mixing skepticism with humor."
    ],
    "points": 194,
    "commentCount": 131,
    "retryCount": 0,
    "time": 1713815590
  },
  {
    "id": 40114567,
    "title": "Py2wasm Boosts Python Performance on WebAssembly",
    "originLink": "https://wasmer.io/posts/py2wasm-a-python-to-wasm-compiler",
    "originBody": "Products Developers BlogTemplates SDK'S CLI Tools Visual Studio Code Extension 17k Sign in Back to articles Announcing py2wasm: A Python to Wasm compiler py2wasm converts your Python programs to WebAssembly, running them at 3x faster speeds syrusakbary Syrus Akbary Founder & CEO April 18, 2024 Back to articles Since starting Wasmer five years ago we've been obsessed with empowering more languages to target the web and beyond through Webassembly. One of the most popular languages out there is Python, and while it is certainly possible to run Python programs in WebAssembly, the performance is not ideal to say the least. *benchmark below Today we are incredibly happy to announce py2wasm: a Python to WebAssembly compiler that transforms your Python programs to WebAssembly (thanks to Nuitka!) avoiding the interpreter overhead, allowing it to run 3 times faster than with the baseline interpreter! Here is how you can use it: $ pip install py2wasm $ py2wasm myprogram.py -o myprogram.wasm $ wasmer run myprogram.wasm Note: py2wasm needs to run in a Python 3.11 environment. You can use pyenv to set Python 3.11 easily in your system: pyenv install 3.11 && pyenv global 3.11. Benchmarking Lets try to get the famous pystone.py benchmark running to compare native Python, regular WebAssembly and py2wasm. Note: you can check the code used to benchmark in https://gist.github.com/syrusakbary/b318c97aaa8de6e8040fdd5d3995cb7c When executing Python natively (387k pystones/second): $ python pystone.py Pystone(1.1) time for 50000 passes = 0.129016 This machine benchmarks at 387549 pystones/second When executing the CPython interpreter inside of WebAssembly (89k pystones/second): $ wasmer run python/python --mapdir=/app:. /app/pystone.py Pystone(1.1) time for 50000 passes = 0.557239 This machine benchmarks at 89728.1 pystones/second When using py2wasm via Nuitka (235k pystones/second): $ py2wasm pystone.py -o pystone.wasm $ wasmer run pystone.wasm Pystone(1.1) time for 50000 passes = 0.21263 This machine benchmarks at 235150 pystones/second In a nutshell: using py2wasm gets about 70% of the Native Python speed… and is about 2.5~3x faster than the baseline! So, how does this black magic work? Let's first analyze all the possible strategies that we can think of to optimize Python workloads in WebAssembly. How to speed up Python in WebAssembly There are many ways to optimize runtime speed: Use a Python subset that can be compiled into performant code Use JIT inside of Python Use Static Analysis to optimize the generated code It's time to analyze each! Compile a Python subset to Wasm Instead of supporting the full Python feature set, we may want to only target a subset of it that can be optimized much further since not all features need to be supported and we can afford to do some shortcuts: ✅ Can generate incredibly performant code ❌ Doesn’t support the full syntax or modules The most popular choices using this strategy are: CPython, RPython (PyPy) and Codon. Cython Cython has been around for many years, and is probably the oldest method to accelerate Python modules. CPython is not strictly a subset, since it supports a syntax closer to C. The main goal of Cython is to create performant modules that run next to your Python codebase. However, we want to allow creating completely standalone WebAssembly binaries from our programs. So unfortunately Cython will not work for speeding up Python executables in Wasm. RPython RPython transforms the typed code into C, and then compiles it with a normal C compiler. PyPy itself is compiled with RPython, which is able to do all the black magic under the hood. def entry_point(argv): print \"Hello, World!\" return 0 def target(*args): return entry_point $ rpython hello_world.py (→ hello-world-c.c ) → hello-world (assembly binary) $ rpython helloworld.py $ ./hello-world \"Hello, World!\" However, RPython has many restrictions when running Python. For example, dictionaries need to be fully typed, and this severly limits the programs that we can use it for. Codon Codon transforms a subset of Python code into LLVM IR. While Codon is one of the most promising alternatives and the one that offers the most speedup (from 10 to 100x faster), the subset of Python they support still has many missing features, which prevents using it for most Python code. Python JITs Another strategy is to use a JIT inside of Python, so the hot paths of execution are compiled to WebAssembly. ✅ Really fast speeds ❌ Needs to warm up ❌ Not trivial to support with Webassembly (but possible) One of the most popular ways (if not the most popular) is PyPy. PyPy PyPy is a Python interpreter that can execute your Python programs at faster speed than the standard CPython interpreter. It speeds up the execution with a Just In Time (JIT) compiler that kicks in when doing complex computation. Running a JIT in WebAssembly is not trivial, but is possible. About five years ago, the project pypyjs.org showcased this possibility by creating a new backend for PyPy that targeted Javascript/Asm.js (instead of x86_64 or arm64/aarch64). You can check the PyPy Asm.js backend implementation here: https://github.com/pypyjs/pypy/tree/pypyjs/rpython/jit/backend/asmjs For our case, we would need to adapt this backend from outputting Javascript code to Webassembly instead. It should be totally possible to implement a Wasm backend in PyPy as Pypy.js demonstrated, but unfortunately is not trivial to do so (it may take from a few weeks to a month of work). Static Analysis There’s one last strategy that we can try to speed up Python execution speed inside of WebAssembly: static analysis. Thanks to static analysis, we can analyze/autodetect the typings of our program ahead of time, so the code can be transpiled into more performant specializations (usually through Python to C transpilation). ✅ Mostly compatible with any Python code and applications ❌ Only 1.5-3x faster ❌ Complex to get right (from the static analyzer perspective, many quircks) ❌ Larger binaries mypy & mypy-c Mypy is probably the most popular static analyzer for Python. The Mypy team also created a mypy-c , which gets all the typing information from Mypy and then transforms the Python code into equivalent C code that runs more performantly. mypy-c is currently specialized on targeting Python modules that can run inside native Python. In our case, we want to allow creating new standalone WebAssembly binaries from our programs, so unfortunately it seems that mypy-c couldn’t work for our use case. Nuitka Nuitka works by transpiling the Python calls that the programs does into C, using the inner CPython API calls. It supports most Python programs, as it transpiles Python code into the corresponding CPython calls. It can even work as a code obfuscator (no one will be able to decompile your program!) After a deep analysis of all the options we realized that probably the fastest option to get Python running performantly in WebAssembly was using Nuitka. Using Nuitka to compile Python to WebAssembly Nuitka seemed like the easiest option to speed up to Python in WebAssembly contexts, mainly because most of the hard work was already done to transpile Python Code into the underlying CPython interpreter calls, so we could probably do some tweaks to get it working to compile to WebAssembly. Nuitka doesn't work (yet) with Python 3.12, so we had to recompile Python to 3.11 to WASI and use the generated libpython.a archive, so Nuitka could use this library when targeting WebAssembly and WASI to create the executable. And things started working... kind of. Once we tried to run the generated Wasm file we realized another issue: because the Nuitka transpiler is executing in a 64 bit architecture, but the generated code is running in a 32 bit architecture (WebAssembly), things were not properly working. Nuitka uses a serialization/deserialization layer to cache the values of certain constants (and accelerate the startup), and while the code was being serialized in 64 bits, the deserialization was done in 32 bits, so there was a bit of mismatch. Once we fixed this two issues, the prototype was fully working! Hurray! 🎉 We have created a PR to upstream the changes into Nuitka, feel free to take a look here: https://github.com/Nuitka/Nuitka/pull/2814 ℹ Right now py2wasm is using a fork of Nuitka, but once changes are integrated upstream we aim to make py2wasm a thin layer on top of Nuitka. We worked on py2wasm to fulfill our own needs first, as we want to accelerate Python execution to the maximum, so we can move our Python Django backend from Google Cloud into Wasmer Edge. py2wasm brings us (and hopefully many others) one step closer to running Python backend apps on Edge at an incredible performance providing a much cheaper alternative for hosting these apps than the current cloud providers. Future Roadmap In the future, we would like to publish py2wasm as a Wasmer package, so you can just simply execute the following command to get it running. Stay tuned! wasmer run py2wasm --dir=. -- myfile.py -o myfile.wasm We hope you enjoyed the article showcasing py2wasm and we can’t wait to hear your feedback on Hacker News and Github! This article is based on the work I presented in the Wasm I/O conference on March 15th, 2024. You can view the slides in SpeakerDeck, or watch the presentation in Youtube: https://www.youtube.com/watch?v=_Gq273qvNMg About the Author Syrus Akbary is an enterpreneur and programmer. Specifically known for his contributions to the field of WebAssembly. He is the Founder and CEO of Wasmer, an innovative company that focuses on creating developer tools and infrastructure for running Wasm Syrus Akbary Syrus Akbary Founder & CEO Benchmarking How to speed up Python in WebAssembly Compile a Python subset to Wasm Cython RPython Codon Python JITs PyPy Static Analysis mypy & mypy-c Nuitka Using Nuitka to compile Python to WebAssembly Future Roadmap Read more Making software universally accessible TwitterDiscordGitHub Explore Packages Blog Products Runtime Registry Edge Company About Values & Culture Imprint Privacy Terms",
    "commentLink": "https://news.ycombinator.com/item?id=40114567",
    "commentBody": "Py2wasm – A Python to WASM Compiler (wasmer.io)179 points by fock 19 hours agohidepastfavorite45 comments wdroz 19 hours ago> py2wasm converts your Python programs to WebAssembly, running them at 3x faster speeds This is clearly written in the article, but I hope that the impatient readers will understand that this is 3 times faster than the CPython wasm, not the native CPython. reply 5- 17 hours agoparenteven more specifically, 3 times faster than wasmer's build of cpython (whatever that is), running on their runtime. i'd be curious to see this benchmark extended, as in my own experience toying with python-like interpreters, you get ~2x slowdown (like their end result) from just compiling to wasm/wasi with clang and running in any 'fast' runtime (e.g. node or wasmtime). reply syrusakbary 16 hours agorootparentHey, I'd love to reproduce the ~2x slowdown you commented from running the workload in Native CPython compared to Wasm CPython (in any runtime, browser or outside). Any tip would be helpful so we can debug it. If your claims are accurate, we can easily get py2wasm even running faster than native CPython! Note: we benchmarked in a M3 Max laptop, so maybe there's some difference there? reply kelp 12 hours agorootparentHere is a trivial example, also on a M3 Max: cat hello.py print(\"Hello, Wasm!\") time python3 ./hello.py Hello, Wasm! ________________________________________________________ Executed in 26.86 millis fish external usr time 16.37 millis 0.13 millis 16.24 millis sys time 7.25 millis 1.14 millis 6.11 millis time wasmer hello.wasm Hello, Wasm! ________________________________________________________ Executed in 84.77 millis fish external usr time 50.26 millis 0.14 millis 50.12 millis sys time 28.97 millis 1.21 millis 27.76 millis time wasmtime hello.wasm Hello, Wasm! ________________________________________________________ Executed in 141.72 millis fish external usr time 120.86 millis 0.13 millis 120.72 millis sys time 16.65 millis 1.20 millis 15.45 millis reply IshKebab 19 hours agoparentprevYeah more honest would have been \"2x slower\"! reply williamstein 19 hours agorootparentEven more honest would be “on one particular microbenchmark that everybody optimizes for”. That said, this seems like a really cool project that could have some real value! It still fully depends on having the full CPython runtime environment, but that means it could work correctly with most existing code and libraries (including numpy and script). reply wasiwin 30 minutes agorootparentFrom what I could tell, Nuitka dynamically loads extension modules so this wouldn't work with Webassembly modules which are statically linked. If I could be enhanced to automatically build wheels of packages like numpy and statically link them, then that could bring extensions to Wasm and be very cool. IIRC, Google does this with their Python apps so the concept does exist but don't think I've seen any OSS tooling trying that. reply boomskats 18 hours agoprevSo I had a look at the repo/branches at https://github.com/wasmerio/py2wasm. This might be a nit, but in the spirit of OSS: if I'd done this work, I'd have contributed it to upstream Nuitka. I definitely would not have forked a whole new GitHub repo and given it a completely different name. What's the rationale for creating a new project and calling it py2wasm? Am I missing something? reply syrusakbary 18 hours agoparentThanks for the feedback! I'm Syrus, main author of the work on py2wasm. We already opened a PR into Nuitka to bring the relevant changes upstream: https://github.com/Nuitka/Nuitka/pull/2814 We envision py2wasm being a thin layer on top of Nuitka, as also commented in the article (not a fork as it is right now, although forking got us into the proof of concept a bit faster!). From what we gathered, we believe that there's usefulness on having py2wasm as a separate package, as py2wasm would also need to ship the precompiled Python distribution (3.11) for WASI (which will not be needed for the other Nuitka use cases), apart of also shipping other tools that are not directly relevant for Nuitka reply theanonymousone 2 hours agorootparentOn a different note, will there some technical documents on how exactly to use it, e.g. exporting functions etc...? By the way is it an alpha release, usable for testing, or something in between? reply spencerflem 18 hours agoparentprevReading the blog post- they are contributing it upstream reply boomskats 18 hours agorootparentWell, reading the blog post, they're announcing a whole new compiler. I'd encourage you to have a look at the PR their post links to. reply Ennea 18 hours agoparentprevMy first thought upon reading Nuitka's name in the article was along the lines of \"I hope they've contributed some of this to Nuitka\". Not a very nice trend. reply __s 10 hours agorootparentWhat trend? Forking happens all the time. It's one of the benefits of open source. I've often made PRs on github to projects while shifting projects I work on to my fork since upstreaming can take months or often never happen Granted, Nuitka is an active project, but wasmer.io is going to have a much more focused desire with their changes that they can deploy, meanwhile getting it upstream will have to go through rounds of review & adjustments https://github.com/rustwasm/wasm-pack/pull/937 this small change took over 2 years to merge. Open source takes time reply ubj 19 hours agoprevNice--it's great to see more options for interoperability between Python and WASM. I was aware of Pyodide [1], but Py2wasm seems to be a tool for compiling specific scripts into WASM rather than having an entire Python distribution in the browser. Please correct me if I'm misunderstanding the difference between the two however. [1]: https://pyodide.org/en/stable/ reply theanonymousone 18 hours agoparentThat's my understanding as well. And Pyodide being a great piece of engineering, is still lagging behind compiled Wasm languages in terms of footprint at least and performance sometimes, unfortunately. reply umvi 13 hours agoprevWhat's up with the botspam in this thread? Looks like hundreds of sex bot accounts are being created. Pages seem really slow to load currently... someone should notify dang. reply spankalee 18 hours agoprevWith WasmGC finalized, I hope we see more compilers that target it instead of including their own GC. It's could be a new interpreter, or maybe a version of CPython that uses WasmGC structs for all Python objects, or a compiler similar to this but that targets WasmGC directly. reply Jasper_ 17 hours agoparentWasmGC is not really what you think it is. Think of it as a full-on replacement for the linear memory model, not as a way to add GC to it. It's exceptionally difficult to port a language runtime over to WasmGC -- it doesn't even offer finalizers, so __del__ can't implemented correctly (even ignoring the issues with existing __del__ semantics in Python leading to compatibility issues). It doesn't support interior pointers, so you can't have a pointer into the middle of an array. There's no easy way to port Python to WasmGC. reply kevingadd 15 hours agoparentprevCurrent WasmGC is missing a lot of critical features for production grade GC, like finalization and interior pointers. It's really promising though, so I think once the gaps get filled in you'll see a bunch of runtimes start moving over - maybe .NET, etc. reply vimota 11 hours agoprev> We worked on py2wasm to fulfill our own needs first, as we want to accelerate Python execution to the maximum, so we can move our Python Django backend from Google Cloud into Wasmer Edge. I was under the impression that Django was stateful and not meant to be run in a serverless/edge cloud. Is that not the case or are you planning to do a special setup to support that? reply grondilu 19 hours agoprevNice. Maybe a pie in the sky but considering how prevalent python is in machine learning, how likely is it that at some point most ML frameworks written in python become executable on a web browser through WASM? reply ilyaraz90 19 hours agoparentYou can already run inference of many modern ML models in-browser via, e.g., https://huggingface.co/docs/transformers.js/en/index . reply lmeyerov 18 hours agoparentprevLast we checked for one of our use cases around sandboxing, key pydata libraries were slowly moving there, but it takes a village. At that time, I think our blockers were Apache Arrow, Parquet readers, and startup time. There were active issues on all three. The GPU & multicore thing is a different story.as that is more about Google & Apple than WASM wrt browsers, and I'd be curious about the CUDA story serverside. We didn't research the equivalent of volume mounts and shmem for fast read-only data imports. For Louie.AI, we ended up continuing with serverside & container-based sandboxing (containers w libseccomp, nsjail, disabled networking, ...). Hard to motivate wasm in the server for general pydata sandboxing in 2024 given startup times, slowdowns, devices, etc: most of our users who want the extra protection would also rather just self-host, decreasing the threat model. Maybe look again in 2025? It's still a good direction, just not there yet. As a browser option, or using with some more limited cloud FaaS use cases, still potentially interesting for us, and we will keep tracking. reply miohtama 18 hours agorootparentPyarrow is giving a headache trying to get it compiled with Emscripten reply lmeyerov 18 hours agorootparentAn irony here is that we donated the pure JS/TS implementation of arrow to apache reply Beretta_Vexee 18 hours agoparentprevPython in ML acts as a glue language for loading the data, the interface, api, et. The hard work is done by C libraries running in parallel on GPUs. Python is quite slow, and handles parallelization very badly. This is not very important for data loading and conditioning tasks, which would benefit little from parallelization, but it is critical for inference. reply miohtama 19 hours agoparentprevDepending on the underling machine learning activity (GPU/no-GPU) it should be already possible today. Any low level machine learning loops are raw native code or GPU already and Python’s execution speed is irrelevant here. The question is do the web browsers and WebAssembly have enough RAM to do any meaningful machine learning work. reply westurner 18 hours agorootparentWebGL, WebGPU, WebNN Which have better process isolation; containers or Chrome's app sandbox at pwn2own how do I know what is running in a WASM browser tab? JupyterLite, datasette-lite, and the Pyodide plugin for vscode.dev ship with Pyodide's WASM compilation of CPython and SciPy Stack / PyData tools. `%pip install -q mendeley` in a notebook opened with the Pyodide Jupyter kernel works by calling `await micropip.install([\"mendeley\"])` IIRC. picomamba installs conda packages from emscripten-forge, which is like conda-forge: https://news.ycombinator.com/item?id=33892076#33916270 : > FWIU Service Workers and Task Workers and Web Locks are the browser APIs available for concurrency in browsers sqlite-wasm, sqlite-wasm-http, duckdb-wasm, (edit) postgresql WASM / pglite ; WhatTheDuck, pretzelai ; lancedb/lance is faster than pandas with dtype_backend=\"arrow\" and has a vector index \"SQLite Wasm in the browser backed by the Origin Private File System\" (2023) https://news.ycombinator.com/item?id=34352935#34366429 \"WebGPU is now available on Android\" (2024) https://news.ycombinator.com/item?id=39046787 \"WebNN: Web Neural Network API\" https://www.w3.org/TR/webnn/ : https://news.ycombinator.com/item?id=36159049 \"The best WebAssembly runtime may be no runtime\" (2024) https://news.ycombinator.com/item?id=38609105 (Edit) emscripten-forge packages are built for the wasm32-unknown-emscripten build target, but wasm32-wasi is what is now supported by cpython (and pypi?) https://www.google.com/search?q=wasm32-wasi+conda-forge reply movpasd 19 hours agoparentprevThe actual underlying models run in a lower-level language (not Python). But with the right tool chain, you already can do this. You can use Pyodide to embed a Python interpreter in WASM, and if you set things up correctly you should be able to make the underlying C/FORTRAN/whatever extensions target WASM also and link them up. TFA is compiling a subset of actual raw Python to WASM (no extension). To be honest, I think applications for this are pretty niche. I don't think Python is a super ergonomic language once you remove all the dynamicity to allow it to compile down. But maybe someone can prove me wrong. reply calebkaiser 19 hours agorootparentWe implemented an in-browser Python editor/interpreter built on Pyodide over at Comet. Our users are data scientists who need to build custom visualizations quite often, and the most familiar language for most of them is Python. One of the issues you'll run into is that Pyodide only works by default with packages that have pure Python wheels available. The team has developed support for some libraries with C dependencies (like scikit-learn, I believe), but frameworks like PyTorch are particularly thorny (see this issue: https://github.com/pyodide/pyodide/issues/1625 ) We ended up rolling out a new version of our Python visualizations that runs off-browser, in order to support enough libraries/get the performance we need: https://www.comet.com/docs/v2/guides/comet-ui/experiment-man... reply thangngoc89 19 hours agoparentprevFor popular ML/DL models, you could already export model to ONNX format for interference. However, glue codes are still python and you may need to replace that part with the host’s language reply ptx 17 hours agoprevHow come the CPython interpreter is so slow when compiled to WebAssembly? According to the figures in the article, it runs at 23% the speed of the native version. Wasmer, which they're using in the benchmarks, should run code \"at near-native speeds\" according to the docs[0]. Apparently it has multiple compiler backends, so maybe the choice of backend affects the result? [0] https://docs.wasmer.io/runtime reply theanonymousone 18 hours agoprevIt may be a stupid question, but is there already some DOM access or a dedicated package that allows writing web applications in this compiled python? reply miohtama 18 hours agoparentYes. Here is an example how to integrate Python to React front end https://blog.pyodide.org/posts/react-in-python-with-pyodide/ Similar examples for raw DOM manipulation should be available for Pyodide as well. It has bindings to all JS code, so it can do everything that JS can do. reply theanonymousone 17 hours agorootparentThanks. I know about the one for Pyodide. Is it confirmed that it works for this new py2wasm compiler? reply lenerdenator 16 hours agoprevWould it be too late to call it PyToWasm? I know Python 2 has been EOL for over four years now but you still see it (much to the chagrin of decent engineers everywhere) in the wild; could generate confusion. reply weinzierl 17 hours agoprevI want the opposite. The Python VM is everywhere. Can't I just run my WASM on Python? (The WASM runtime must be pure Python, installing C/C++ extensions is against the spirit of this.) reply hathawsh 17 hours agoparentHmm, what I want is to run WASM using a CPython extension. CPython extensions are vital to Python. I'm curious why you say an extension is against the spirit of it. reply Rucadi 17 hours agoparentprevI mean, the performance would be abysmal. reply lagt_t 14 hours agoprevWhat's the hello world wasm file size? reply kelp 11 hours agoparentls -lh hello.wasm -rw-r--r--@ 1 tcole staff 24M Apr 22 14:18 hello.wasm This is on macOS. That is hello.py compiled to wasm with Py2wasm. reply syrusakbary 0 minutes agorootparentIndeed, this can be way further optimized. For example, you can probably do a wasm-strip and wasm-opt passes that would leave the wasm file being ~5-10Mb. Still very big, but a bit more reasonable. The good thing is that thanks to Nuitka you could actually do some tree shaking and only include the imported files and not everything in between (although this might break other behavior such as eval and so). reply theanonymousone 2 hours agorootparentprevWow! 24M makes it even less practical than interpreted pyodide. reply spxneo 15 hours agoprev [–] so does this mean we can run FastAPI inside a browser? reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The release of py2wasm, a Python to WebAssembly compiler, significantly boosts the performance of Python programs in a WebAssembly environment, running three times faster than with a traditional interpreter.",
      "Various strategies like compiling a Python subset to Wasm, employing JITs, and static analysis are discussed to accelerate Python in WebAssembly, enhancing its speed.",
      "Syrus Akbary outlines the development process and future plans for py2wasm, aiming to make it a widely accessible tool for optimizing Python backend applications to run efficiently on the Edge."
    ],
    "commentSummary": [
      "Py2wasm is a Python to WebAssembly compiler designed to boost the performance of Python programs in web browsers.",
      "Users have conflicting outcomes in benchmark tests, with some seeing a decrease in speed compared to native CPython.",
      "The project shows promise in improving Python and WebAssembly compatibility, with talks of integration with tools like Nuitka, offering exciting prospects for web development, especially in machine learning applications."
    ],
    "points": 179,
    "commentCount": 45,
    "retryCount": 0,
    "time": 1713795056
  },
  {
    "id": 40115155,
    "title": "Introducing Parquet-WASM: Rust for WebAssembly Parquet I/O",
    "originLink": "https://github.com/kylebarron/parquet-wasm",
    "originBody": "WASM Parquet WebAssembly bindings to read and write the Apache Parquet format to and from Apache Arrow using the Rust parquet and arrow crates. This is designed to be used alongside a JavaScript Arrow implementation, such as the canonical JS Arrow library. Including read and write support and all compression codecs, the brotli-compressed WASM bundle is 1.2 MB. Refer to custom builds for how to build a smaller bundle. A minimal read-only bundle without compression support can be as small as 456 KB brotli-compressed. Install parquet-wasm is published to NPM. Install with yarn add parquet-wasm or npm install parquet-wasm API Parquet-wasm has both a synchronous and asynchronous API. The sync API is simpler but requires fetching the entire Parquet buffer in advance, which is often prohibitive. Sync API Refer to these functions: readParquet: Read a Parquet file synchronously. readSchema: Read an Arrow schema from a Parquet file synchronously. writeParquet: Write a Parquet file synchronously. Async API readParquetStream: Create a ReadableStream that emits Arrow RecordBatches from a Parquet file. ParquetFile: A class for reading portions of a remote Parquet file. Use fromUrl to construct from a remote URL or fromFile to construct from a File handle. Note that when you're done using this class, you'll need to call free to release any memory held by the ParquetFile instance itself. Both sync and async functions return or accept a Table class, an Arrow table in WebAssembly memory. Refer to its documentation for moving data into/out of WebAssembly. Entry Points Entry point Description Documentation parquet-wasm, parquet-wasm/esm, or parquet-wasm/esm/parquet_wasm.js ESM, to be used directly from the Web as an ES Module Link parquet-wasm/bundler \"Bundler\" build, to be used in bundlers such as Webpack Link parquet-wasm/node Node build, to be used with synchronous require in NodeJS Link ESM The esm entry point is the primary entry point. It is the default export from parquet-wasm, and is also accessible at parquet-wasm/esm and parquet-wasm/esm/parquet_wasm.js (for symmetric imports directly from a browser). Note that when using the esm bundles, you must manually initialize the WebAssembly module before using any APIs. Otherwise, you'll get an error TypeError: Cannot read properties of undefined. There are multiple ways to initialize the WebAssembly code: Asynchronous initialization The primary way to initialize is by awaiting the default export. import wasmInit, {readParquet} from \"parquet-wasm\"; await wasmInit(); Without any parameter, this will try to fetch a file named 'parquet_wasm_bg.wasm' at the same location as parquet-wasm. (E.g. this snippet input = new URL('parquet_wasm_bg.wasm', import.meta.url);). Note that you can also pass in a custom URL if you want to host the .wasm file on your own servers. import wasmInit, {readParquet} from \"parquet-wasm\"; // Update this version to match the version you're using. const wasmUrl = \"https://cdn.jsdelivr.net/npm/parquet-wasm@0.6.0/esm/parquet_wasm_bg.wasm\"; await wasmInit(wasmUrl); Synchronous initialization The initSync named export allows for import {initSync, readParquet} from \"parquet-wasm\"; // The contents of esm/parquet_wasm_bg.wasm in an ArrayBuffer const wasmBuffer = new ArrayBuffer(...); // Initialize the Wasm synchronously initSync(wasmBuffer) Async initialization should be preferred over downloading the Wasm buffer and then initializing it synchronously, as WebAssembly.instantiateStreaming is the most efficient way to both download and initialize Wasm code. Bundler The bundler entry point doesn't require manual initialization of the WebAssembly blob, but needs setup with whatever bundler you're using. Refer to the Rust Wasm documentation for more info. Node The node entry point can be loaded synchronously from Node. const {readParquet} = require(\"parquet-wasm\"); const wasmTable = readParquet(...); Using directly from a browser You can load the esm/parquet_wasm.js file directly from a CDN const parquet = await import( \"https://cdn.jsdelivr.net/npm/parquet-wasm@0.6.0/esm/parquet_wasm.js\" ) await parquet.default(); const wasmTable = parquet.readParquet(...); Debug functions These functions are not present in normal builds to cut down on bundle size. To create a custom build, see Custom Builds below. setPanicHook setPanicHook(): void Sets console_error_panic_hook in Rust, which provides better debugging of panics by having more informative console.error messages. Initialize this first if you're getting errors such as RuntimeError: Unreachable executed. The WASM bundle must be compiled with the console_error_panic_hook feature for this function to exist. Example import * as arrow from \"apache-arrow\"; import initWasm, { Compression, readParquet, Table, writeParquet, WriterPropertiesBuilder, } from \"parquet-wasm\"; // Instantiate the WebAssembly context await initWasm(); // Create Arrow Table in JS const LENGTH = 2000; const rainAmounts = Float32Array.from({ length: LENGTH }, () => Number((Math.random() * 20).toFixed(1)) ); const rainDates = Array.from( { length: LENGTH }, (_, i) => new Date(Date.now() - 1000 * 60 * 60 * 24 * i) ); const rainfall = arrow.tableFromArrays({ precipitation: rainAmounts, date: rainDates, }); // Write Arrow Table to Parquet // wasmTable is an Arrow table in WebAssembly memory const wasmTable = Table.fromIPCStream(arrow.tableToIPC(rainfall, \"stream\")); const writerProperties = new WriterPropertiesBuilder() .setCompression(Compression.ZSTD) .build(); const parquetUint8Array = writeParquet(wasmTable, writerProperties); // Read Parquet buffer back to Arrow Table // arrowWasmTable is an Arrow table in WebAssembly memory const arrowWasmTable = readParquet(parquetUint8Array); // table is now an Arrow table in JS memory const table = arrow.tableFromIPC(arrowWasmTable.intoIPCStream()); console.log(table.schema.toString()); // Schema }> Published examples (These may use older versions of the library with a different API). GeoParquet on the Web (Observable) Hello, Parquet-WASM (Observable) Performance considerations Tl;dr: When you have a Table object (resulting from readParquet), try the new Table.intoFFI API to move it to JavaScript memory. This API is less well tested than the Table.intoIPCStream API, but should be faster and have much less memory overhead (by a factor of 2). If you hit any bugs, please create a reproducible issue. Under the hood, parquet-wasm first decodes a Parquet file into Arrow in WebAssembly memory. But then that WebAssembly memory needs to be copied into JavaScript for use by Arrow JS. The \"normal\" conversion APIs (e.g. Table.intoIPCStream) use the Arrow IPC format to get the data back to JavaScript. But this requires another memory copy inside WebAssembly to assemble the various arrays into a single buffer to be copied back to JS. Instead, the new Table.intoFFI API uses Arrow's C Data Interface to be able to copy or view Arrow arrays from within WebAssembly memory without any serialization. Note that this approach uses the arrow-js-ffi library to parse the Arrow C Data Interface definitions. This library has not yet been tested in production, so it may have bugs! I wrote an interactive blog post on this approach and the Arrow C Data Interface if you want to read more! Example import * as arrow from \"apache-arrow\"; import { parseTable } from \"arrow-js-ffi\"; import initWasm, { wasmMemory, readParquet } from \"parquet-wasm\"; // Instantiate the WebAssembly context await initWasm(); // A reference to the WebAssembly memory object. const WASM_MEMORY = wasmMemory(); const resp = await fetch(\"https://example.com/file.parquet\"); const parquetUint8Array = new Uint8Array(await resp.arrayBuffer()); const wasmArrowTable = readParquet(parquetUint8Array).intoFFI(); // Arrow JS table that was directly copied from Wasm memory const table: arrow.Table = parseTable( WASM_MEMORY.buffer, wasmArrowTable.arrayAddrs(), wasmArrowTable.schemaAddr() ); // VERY IMPORTANT! You must call `drop` on the Wasm table object when you're done using it // to release the Wasm memory. // Note that any access to the pointers in this table is undefined behavior after this call. // Calling any `wasmArrowTable` method will error. wasmArrowTable.drop(); Compression support The Parquet specification permits several compression codecs. This library currently supports: Uncompressed Snappy Gzip Brotli ZSTD LZ4_RAW LZ4 (deprecated) LZ4 support in Parquet is a bit messy. As described here, there are two LZ4 compression options in Parquet (as of version 2.9.0). The original version LZ4 is now deprecated; it used an undocumented framing scheme which made interoperability difficult. The specification now reads: It is strongly suggested that implementors of Parquet writers deprecate this compression codec in their user-facing APIs, and advise users to switch to the newer, interoperable LZ4_RAW codec. It's currently unknown how widespread the ecosystem support is for LZ4_RAW. As of pyarrow v7, it now writes LZ4_RAW by default and presumably has read support for it as well. Custom builds In some cases, you may know ahead of time that your Parquet files will only include a single compression codec, say Snappy, or even no compression at all. In these cases, you may want to create a custom build of parquet-wasm to keep bundle size at a minimum. If you install the Rust toolchain and wasm-pack (see Development), you can create a custom build with only the compression codecs you require. The minimum supported Rust version in this project is 1.60. To upgrade your toolchain, use rustup update stable. Example custom builds Reader-only bundle with Snappy compression: wasm-pack build --no-default-features --features snappy --features reader Writer-only bundle with no compression support, targeting Node: wasm-pack build --target nodejs --no-default-features --features writer Bundle with reader and writer support, targeting Node, using arrow and parquet crates with all their supported compressions, with console_error_panic_hook enabled: wasm-pack build \\ --target nodejs \\ --no-default-features \\ --features reader \\ --features writer \\ --features all_compressions \\ --features debug # Or, given the fact that the default feature includes several of these features, a shorter version: wasm-pack build --target nodejs --features debug Refer to the wasm-pack documentation for more info on flags such as --release, --dev, target, and to the Cargo documentation for more info on how to use features. Available features By default, all_compressions, reader, writer, and async features are enabled. Use --no-default-features to remove these defaults. reader: Activate read support. writer: Activate write support. async: Activate asynchronous read support. all_compressions: Activate all supported compressions. brotli: Activate Brotli compression. gzip: Activate Gzip compression. snappy: Activate Snappy compression. zstd: Activate ZSTD compression. lz4: Activate LZ4_RAW compression. debug: Expose the setPanicHook function for better error messages for Rust panics. Node <20 On Node versions before 20, you'll have to polyfill the Web Cryptography API. Future work Example of pushdown predicate filtering, to download only chunks that match a specific condition Column filtering, to download only certain columns More tests Acknowledgements A starting point of my work came from @my-liminal-space's read-parquet-browser (which is also dual licensed MIT and Apache 2). @domoritz's arrow-wasm was a very helpful reference for bootstrapping Rust-WASM bindings.",
    "commentLink": "https://news.ycombinator.com/item?id=40115155",
    "commentBody": "Parquet-WASM: Rust-based WebAssembly bindings to read and write Parquet data (github.com/kylebarron)158 points by kylebarron 18 hours agohidepastfavorite14 comments nickfs 2 minutes ago;8y aiu;khjbvnvxzg;o9 reply m_d_ 9 hours agoprevI'd like to point out that fastparquet has been built for wasm (pydide/pyscript) for some time and works fine, producing pandas dataframes. Unfortunately, the thread/socket/async nature of fsspec means you have to get the files yourself into the \"local filesystem\" (meaning: the wasm sandbox). (I am the fastparquet author) reply jasonjmcghee 16 hours agoprevSeeing as the popular alternative here would be DuckDB-WASM, which (last time I checked) is on the order of 50MB, this is comparatively super lightweight. reply leeoniya 16 hours agoparenti think duckdb-wasm is closer to 6MB over wire, but ~36MB once decompressed. (see net panel when loading https://shell.duckdb.org/) the decompressed size should be okay since it's not the same as parsing and JITing 36MB of JS. reply FridgeSeal 10 hours agoprev@dang we have a mass spam incursion in this comment thread. reply seanw444 4 hours agoparentIt's site-wide. reply leeoniya 16 hours agoprevin my [albeit outdated] experience ArrowJS is quite a bit slower than using native JS types. i feel like crossing the WASMJS boundary is very expensive, especially for anything other than numbers/typed arrays. what are people's experiences with this? reply kylebarron 13 hours agoparentArrow JS is just ArrayBuffers underneath. You do want to amortize some operations to avoid unnecessary conversions. I.e. Arrow JS stores strings as UTF-8, but native JS strings are UTF-16 I believe. Arrow is especially powerful across the WASMJS boundary! In fact, I wrote a library to interpret Arrow from Wasm memory into JS without any copies [0]. (Motivating blog post [1]) [0]: https://github.com/kylebarron/arrow-js-ffi [1]: https://observablehq.com/@kylebarron/zero-copy-apache-arrow-... reply lmeyerov 7 hours agorootparentYeah, we built it to essentially stream columnar record batches from server GPUs to browser GPUs with minimal touching of any of the array buffers. It was very happy-path for that kind of fast bulk columnar processing, and we donated it to the community to grow to use cases beyond that. So it sounds like the client code may have been doing more than that. For high performance code, I'd have expected overhead in %s, not Xs. And not surprised to hear slowdowns for any straying beyond that -- cool to see folks have expanded further! More recently, we've been having good experiences more recently here in PerspectiveLoaders, enough so that we haven't had to dig deeper. Our current code is targeting I think it is based on the Kepler.gl / Deck.gl data loaders that go straight to GPU from network. reply rubenvanwyk 14 hours agoprev [–] Can this read and write Parquet files to S3-compatible storage? reply kylebarron 13 hours agoparent [–] It can read from HTTP urls, but you'd need to manage signing the URLs yourself. On the writing side, it currently writes to an ArrayBuffer, which then you could upload to a server or save on the user's machine. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "WASM Parquet WebAssembly bindings enable reading and writing Apache Parquet files in Apache Arrow via Rust parquet and arrow crates.",
      "The library offers synchronous and asynchronous APIs, emphasizing small bundle size and tailored builds for distinct compression codecs.",
      "Users can acquire parquet-wasm from NPM for ESM, bundler, and Node builds, with debugging, performance optimization, compression support, and reduced bundle size features, aiming to add pushdown predicate and column filtering capabilities in the future."
    ],
    "commentSummary": [
      "Parquet-WASM is a WebAssembly binding written in Rust for reading and writing Parquet data efficiently.",
      "It is lightweight compared to alternatives like DuckDB-WASM, making it a good choice for handling operations across the WASMJS boundary.",
      "Users can read data from HTTP URLs, but they are required to handle signing on their own, and the tool currently writes data to an ArrayBuffer for uploading or saving."
    ],
    "points": 158,
    "commentCount": 14,
    "retryCount": 0,
    "time": 1713798600
  },
  {
    "id": 40121318,
    "title": "Dify: Visual Workflow for LLM App Development",
    "originLink": "https://github.com/langgenius/dify",
    "originBody": "Dify Cloud · Self-hosting · Documentation · Enterprise inquiry Dify is an open-source LLM app development platform. Its intuitive interface combines AI workflow, RAG pipeline, agent capabilities, model management, observability features and more, letting you quickly go from prototype to production. Here's a list of the core features: 1. Workflow: Build and test powerful AI workflows on a visual canvas, leveraging all the following features and beyond. optimized_workflow_intro.mp4 2. Comprehensive model support: Seamless integration with hundreds of proprietary / open-source LLMs from dozens of inference providers and self-hosted solutions, covering GPT, Mistral, Llama2, and any OpenAI API-compatible models. A full list of supported model providers can be found here. 3. Prompt IDE: Intuitive interface for crafting prompts, comparing model performance, and adding additional features such as text-to-speech to a chat-based app. 4. RAG Pipeline: Extensive RAG capabilities that cover everything from document ingestion to retrieval, with out-of-box support for text extraction from PDFs, PPTs, and other common document formats. 5. Agent capabilities: You can define agents based on LLM Function Calling or ReAct, and add pre-built or custom tools for the agent. Dify provides 50+ built-in tools for AI agents, such as Google Search, DELL·E, Stable Diffusion and WolframAlpha. 6. LLMOps: Monitor and analyze application logs and performance over time. You could continuously improve prompts, datasets, and models based on production data and annotations. 7. Backend-as-a-Service: All of Dify's offerings come with corresponding APIs, so you could effortlessly integrate Dify into your own business logic. Feature comparison Feature Dify.AI LangChain Flowise OpenAI Assistants API Programming Approach API + App-oriented Python Code App-oriented API-oriented Supported LLMs Rich Variety Rich Variety Rich Variety OpenAI-only RAG Engine ✅ ✅ ✅ ✅ Agent ✅ ✅ ✅ ✅ Workflow ✅ ❌ ✅ ❌ Observability ✅ ✅ ❌ ❌ Enterprise Feature (SSO/Access control) ✅ ❌ ❌ ❌ Local Deployment ✅ ✅ ✅ ❌ Using Dify Cloud We host a Dify Cloud service for anyone to try with zero setup. It provides all the capabilities of the self-deployed version, and includes 200 free GPT-4 calls in the sandbox plan. Self-hosting Dify Community Edition Quickly get Dify running in your environment with this starter guide. Use our documentation for further references and more in-depth instructions. Dify for enterprise / organizations We provide additional enterprise-centric features. Schedule a meeting with us or send us an email to discuss enterprise needs. For startups and small businesses using AWS, check out Dify Premium on AWS Marketplace and deploy it to your own AWS VPC with one-click. It's an affordable AMI offering with the option to create apps with custom logo and branding. Staying ahead Star Dify on GitHub and be instantly notified of new releases. Quick start Before installing Dify, make sure your machine meets the following minimum system requirements: CPU >= 2 Core RAM >= 4GB The easiest way to start the Dify server is to run our docker-compose.yml file. Before running the installation command, make sure that Docker and Docker Compose are installed on your machine: cd docker docker compose up -d After running, you can access the Dify dashboard in your browser at http://localhost/install and start the initialization process. If you'd like to contribute to Dify or do additional development, refer to our guide to deploying from source code Next steps If you need to customize the configuration, please refer to the comments in our docker-compose.yml file and manually set the environment configuration. After making the changes, please run docker-compose up -d again. You can see the full list of environment variables here. If you'd like to configure a highly-available setup, there are community-contributed Helm Charts which allow Dify to be deployed on Kubernetes. Helm Chart by @LeoQuote Helm Chart by @BorisPolonsky Contributing For those who'd like to contribute code, see our Contribution Guide. At the same time, please consider supporting Dify by sharing it on social media and at events and conferences. We are looking for contributors to help with translating Dify to languages other than Mandarin or English. If you are interested in helping, please see the i18n README for more information, and leave us a comment in the global-users channel of our Discord Community Server. Contributors Community & contact Github Discussion. Best for: sharing feedback and asking questions. GitHub Issues. Best for: bugs you encounter using Dify.AI, and feature proposals. See our Contribution Guide. Email. Best for: questions you have about using Dify.AI. Discord. Best for: sharing your applications and hanging out with the community. Twitter. Best for: sharing your applications and hanging out with the community. Or, schedule a meeting directly with a team member: Point of Contact PurposeBusiness enquiries & product feedbackContributions, issues & feature requests Star history Security disclosure To protect your privacy, please avoid posting security issues on GitHub. Instead, send your questions to security@dify.ai and we will provide you with a more detailed answer. License This repository is available under the Dify Open Source License, which is essentially Apache 2.0 with a few additional restrictions.",
    "commentLink": "https://news.ycombinator.com/item?id=40121318",
    "commentBody": "Dify, a visual workflow to build/test LLM applications (github.com/langgenius)155 points by mountainview 12 hours agohidepastfavorite32 comments jeswin 4 hours ago> Dify is licensed under the Apache License 2.0, with the following additional conditions ... I am totally fine with closed-source/commercial licenses, but please don't do a \"Like Apache 2.0 but not really\" type of license. It just confuses everyone. You can pick from SSPL, BSL, Elastic license among others if you don't want to roll out your own. > 2. As a contributor, you should agree that: a. The producer can adjust the open-source agreement to be more strict or relaxed as deemed necessary. b. Your contributed code may be used for commercial purposes, including but not limited to its cloud business operations. This is not very contributor-friendly. You could consider keeping an open-source core, and extensions for paid features. reply ivanhoe 27 minutes agoparent> This is not very contributor-friendly. You've put it way too politely IMHO. It's a license designed to attract contributors to help build their product for free, but leaves space for them to just re-license the product and sell it when the opportunity arises. It's not like we haven't seen this play already, and it hurts both contributors and users... reply sandGorgon 4 hours agoparentprevgenuine question - out of the closed-source/commercial licenses, which ones are the most developer friendly ? reply jeswin 4 hours agorootparentBy commercial licenses, I meant those written by lawyers specifically for the product. But usually you could just go with something like SSPL - which has sufficient protections for the developer (but not the user). reply huevosabio 9 hours agoprevHow does it compare with MagickML? https://github.com/Oneirocom/Magick reply jacobheller 11 hours agoprevVery slick and potentially very powerful. After a few minutes playing with it, I have a few recommendations: - Variables should have more types, like an array of objects - Prompting should incorporate Jinja2/Nunjucks - For every prompt, I should be able to create many different test examples, along with an answer key, and measure how well it does across many tests - It should auto-save. I did a lot of prompting work and then clicked another icon. When I came back, all my work was gone. (In fact, I don't see where to save at all! Maybe I'm just missing it.) reply mjos 5 hours agoprevHow does this compare to n8n? https://github.com/n8n-io/n8n reply thomasfromcdnjs 11 hours agoprevWhat kind of people are using this AI dev platforms? When do they become better then just rolling your own custom code? reply guchenhe 7 hours agoparentI work on Dify. what we're trying do with Dify currently is to let people put together prototypes quicker and either get to production or fail at a faster rate. we've seen it being helpful for non-technical folks to collaborate on a project well (e.g. importing documents for knowledge base, creating no-code workflow apps, etc) reply choppaface 10 hours agoprevWow I've never seen so many fake accounts on a HN post before. So then is it fair to say the Github stars for this project could also perhaps be artificially inflated? This month they started to go exponential: https://github.com/langgenius/dify?tab=readme-ov-file#star-h... reply loginx 9 hours agoparentWell, I think a lot of the uptick happened last week because that's when it was published in the Toughtworks Tech Radar for this quarter. The audience is large, presumably larger than HN, and that's how I found out about it and have been toying with it since then. I have no idea what I'm doing, but as far as I can tell, this seems like a legitimate project. reply sdesol 8 hours agorootparentThis aligns with what you said: https://devboard.gitsense.com/langgenius?id=4d1dec9067&r=lan... I created insights for the last 4 weeks and number of new contributors and stars peaked last week. This project has all the signs of a successfully organically grown project. reply choppaface 4 hours agorootparentprevnever heard of tech radar, interesting thanks! reply nerdponx 10 hours agoparentprevThe AI Girlfriend posts are on a lot of threads today, not just this one. But you never know. reply jsunderland323 10 hours agoparentprevYup, this reeks of sock puppets reply Havoc 10 hours agoparentprevHad someone mention it to me yesterday in an organic convo so there are definitely people out there using it (seemingly happily). Haven’t tried it yet - still evaluating autogen. reply sdesol 9 hours agoparentprev> Github stars for this project could also perhaps be artificially inflated? Maybe, but I don't think so. The number of people engaging with the project is what I would expect from the number of stars received. https://devboard.gitsense.com/langgenius?r=langgenius%2Fdify... I've seen projects with 5,000+ stars in a month and only have 20-30 people interacting with the project. Full disclosure: This is my tool reply moklick 11 hours agoprevDify looks super powerful! Always nice to see a React Flow app in the wild :) reply atleastoptimal 10 hours agoprevSorry about all these spam comments lol reply Bilal_io 10 hours agoparentI flagged a few, then realized the problem is way bigger than me reply mdaniel 10 hours agoprev> https://github.com/langgenius/dify/blob/main/LICENSE everyone is apparently a license pioneer reply dudus 5 hours agoparentThis is exactly the type of license Redis tried to move to when people got upset and forked it twice. \"Essentially Apache with restrictions\" reply waldrews 9 hours agoparentprevYeah, that's kind of weird - Apache, except totally not Apache. They probably meant to do a source-available license with free non-commercial use allowing inbound contributions - makes sense for a startup, but... please get a lawyer and clean it up :). reply guchenhe 7 hours agoparentprevteam member here - totally see where you're coming from, we'll be relaxing this soon. reply bashtoni 11 hours agoprev\"AI means the end of coding\" didn't age well. It turns out to get the most out of LLMs you need to program them. reply teaearlgraycold 9 hours agoparentLLMs mean now you can write simple imperative code to perform NLP tasks. reply ekianjo 11 hours agoparentprevsql used to be the end of coding to retrieve data as well. the circle continues reply gryn 11 hours agoprevwtf is up with all these bots ? it's ironic they decided to do this on a post about LLMs. Are they feeling threatened that LLMs are taking their jobs ? reply sdesol 10 hours agoparentMy theory is, there is an AI frameworks turf war going on and the bots are to ensure discussion is impossible or extremely difficult. reply layman51 10 hours agoparentprevI’m seeing them in a bunch of other threads. This is the first time I have seen HN being spammed like this. reply tomrod 11 hours agoparentprevYeah super weird. reply 2genders33756 11 hours agoprev [2 more] [flagged] nilsherzig 11 hours agoparent [–] @dang (I hope it's okay to just that you, my nh reader doesn't seem to have a report button) reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Dify is an open-source LLM app development platform with features like AI workflows, model support, RAG pipeline, and agent capabilities.",
      "Users can deploy AI applications easily on Dify, which offers self-hosting or usage on Dify Cloud, with enterprise and community editions.",
      "Support for Dify is available through GitHub, email, Discord, and Twitter, with options to customize and contribute to the platform under the Dify Open Source License."
    ],
    "commentSummary": [
      "Dify is a visual workflow for creating and testing LLM applications under Apache License 2.0 with extra conditions, sparking concerns among contributors about the licensing terms compared to other AI platforms.",
      "The rise in GitHub stars for Dify leads to debates on the credibility of star counts and user interaction levels, raising questions on potential inflation.",
      "Discussions also touch on the involvement of bots in the comment section and the impact of AI on programming within the post."
    ],
    "points": 155,
    "commentCount": 32,
    "retryCount": 0,
    "time": 1713821550
  },
  {
    "id": 40119417,
    "title": "Diving into Tech Distractions: An Endless Loop",
    "originLink": "https://news.ycombinator.com/item?id=40119417",
    "originBody": "You get nerd-sniped. Assigned a bug to squash. Some new tech or gadget arrived, to familiarize yourself with.While researching &#x2F; reading up &#x2F; debugging, you stumble upon something interesting. Upon looking into that, yet another subject catches your attention.You know how this goes. So... (see title). Bonus questions: what intermediate steps did you pass along the way? What stuck in your mind the most?",
    "commentLink": "https://news.ycombinator.com/item?id=40119417",
    "commentBody": "What rabbit hole(s) did you dive into recently?153 points by RetroTechie 13 hours agohidepastfavorite261 comments You get nerd-sniped. Assigned a bug to squash. Some new tech or gadget arrived, to familiarize yourself with. While researching / reading up / debugging, you stumble upon something interesting. Upon looking into that, yet another subject catches your attention. You know how this goes. So... (see title). Bonus questions: what intermediate steps did you pass along the way? What stuck in your mind the most? jl6 12 hours agoSo, I decided to install Linux on my formerly-Windows-only laptop, and thought it was cool enough to go full time and ditch Windows completely. The downside was the lack of access to top tier games. No problem though, my plan was to take a break from gaming, figuring that by the time Linux had caught up with compatibility, computers would also be much more powerful and I'd be able to resume gaming at some point in the future on better kit, and not have to worry about janky framerates on struggling hardware. Linux proved interesting enough that I kept finding all sorts of cool new rabbit holes to go down - shell scripting, filesystems, Python, databases. It was side-quests within side-quests! Plus, having kicked my gaming habit, I had plenty of time to explore these. Anyway, to cut a long story short, that was 23 years ago. I ended up getting a career in tech, relocated, got married, had kids, lived the American Dream... The \"life\" rabbit hole kind of got in the way of my plans, so I can't wait to finally get back on track and play GTA III on a decent box. reply eddd-ddde 9 hours agoparentThe old-laptop -> install-linux-because-windows-wont-run -> tech-career pipeline is absolutely real. It got me to learn C, graphics programming, operating systems, networks and firewall, literally everything I wanted to do required a couple of days deep in arch linux wiki learning about all kinds of inter-connected systems. reply captainkrtek 9 hours agorootparentWhen I ditched Windows and installed Ubuntu 8.04 on my personal computer was my gateway into further tech (building PCs, using a terminal, programming, etc.) and set the foundation for my career. reply emestifs 12 hours agoparentprevIs a router a decent box? https://kittenlabs.de/real-gaming-router/ reply jhanschoo 1 hour agoparentprevYou'll have to do some research to figure out how to get a decent copy of the old, moddable GTA III, and not the remastered, ported ones. reply pictureofabear 9 hours agoparentprevIt was a big moment for me too when I realized that the real world has infinitely many interesting things to do and explore. The real world is incredibly detailed. reply mmh0000 9 hours agorootparentHard disagree. The real world only has pain and suffering. Endless trials and never a payout. Games on the other hand and very detailed and have a well defined path to success. reply hirvi74 8 hours agorootparentThis oddly enough has been quite a big issue in my life as of lately. I need to get off my ass and start working towards better things in real life in order to (potentially) better my situation e.g. new job, better hobbies, etc.. However, I have always had issues with gaming on and off throughout my life (perhaps a lot more on than off). I seriously think that a lot of my issues with gaming is that games are preferential to life in many regards. In a game, I know if I work hard and follow the steps/guides/quests, then I will be rewarded. Goals are obtainable in that if I fail to achieve them it is my fault -- because I did something incorrectly. Sadly, when I take breaks from gaming, I am not a productivity machine. I just find something else to waste the time with. In the back of my mind, I want to believe that if I work hard and better my situation, then will finally be rewarded. But I have worked hard to get where I am, and I am still awaiting the reward, so to speak. So, I think a part of my brain has taken the shortcut to destroy my motivation because I know that Sisyphus isn't the only one rolling the boulder up the hill thinking, \"maybe this time will be different?\" reply bitfilped 2 hours agorootparentBut you won't be rewarded? It's a game and has no bearing on your real life, I don't follow this train of thought at all. reply jmopp 4 hours agorootparentprevBut when you succeed in the game, then what? Yes, the real world is full of endless trials and never a payout. But the entire point is that there is no point. The fun is in the journey. The real treasure is the friends you make along the way, to quote the meme. reply wdh505 9 hours agorootparentprevThe hardest part of life is to reward the process over the outcomes. Videogames don't have that constraint. reply tharkun__ 10 hours agoparentprevThis is the one! Same here. Life happened. What I told my dad would never happen (not using Linux From Scratch or Gentoo - like how can he use Windows and like just get things done instead of digging deep and solving problems all the time?) did happen. I now use Debian derived but more up-to-date and easy to use for mainstream people distros (like shudder Ubuntu) and they're relegated to server and TV duty and I hope nothing breaks when I upgrade from LTS to LTS version coz \"I don't have time for this!\" and I play a few games here and there on my Windows laptop. But I think it's important to have gone through the \"rabbit hole\" in the middle. All the digging and understanding I did I still do all the time at work. I just no longer spend the other half of my life on it. I spend it digging into other things. reply farseer 1 hour agoparentprevA lot of the top tier games run fine on Linux using Steam/Proton Compatibly layer or just pure Wine. Do explore that. reply umvi 11 hours agoparentprevLet's hear your distro journey! What did you start with, what do you currently use? reply jl6 3 hours agorootparentI started with Mandrake (ironically for the game ban, I bought a retail version that came bundled with a copy of The Sims that had been rigged to run with wine), then tried Linux From Scratch (very educational), then a brief spell on Gentoo (because my Dell laptop needed some extra config to get the wifi working anyway, so I figured why not compile all the packages - then I figured out why). Then I liked KDE so moved to openSUSE, but then I stopped liking KDE when 4 came out and broke everything, so I moved to Ubuntu. Then I stopped liking GNOME when 3 broke everything, and at any rate I didn’t have so much time to fiddle any more, so I moved to Xubuntu, and there I remain. reply perihelions 9 hours agoprevPDF files, and why the heck they are so slow to read. Hours upon hours of perf(1) and fiddling with ugly things in C. My main takeaway is everyone in the world is doing things HORRIBLY wrong and there's no way to stop them. (Digression: did you know libpng, the one everyone uses, is not supposed to be an optimized production library—rather it's a reference implementation? It's almost completely unoptimized, no really, take a look, anywhere in the codebase. Critical hot loops are 15 year old C that doesn't autovectorize. I easily got a 200% speedup with a 30-line patch, on something I cared about (their decoding of 1-bit bilevel to RGBA). I'm using that modified libpng right now. I know of nowhere to submit this patch. Why the heck is everyone using libpng?) The worst offender (so far) is the JBIG2 format (several major libraries, including jbig2dec), a very popular format that gets EXTREMELY high compression ratios on bilevel images of types typical to scanned pdfs. But: it's also a format that's pretty slow to decompress—not something you want in a UI loop, like a PDF reader is! And, there's no way around that—if you look at the hot loop, which is arithmetic coding, it's a mess of highly branchy code that's purely serial and cannot be thread- nor SIMD- parallelized. (Standardized in 2000, so it wasn't an obvious downside then). I want to try to deep-dive into this one (as best as my limited skill allows), but I think it's unlikely there's any low-hanging optimization fruit, like there's so much of in libpng. It's all wrong that everyone's using this slow, non-optimizable compression format in PDF's today, but, no one really cares. Everyone's doing things wrong and there is no way to stop them. Another observation: lots of people create PDF's at print-quality pixel density that's useless for screens, and greatly increases rendering latency. Does JBIG2 support interlacing or progressive decoding, to sidestep this challenge? Of course it doesn't. Everyone's doing PDF things wrong and there is no way under the blue sky to make them stop. reply sparkie 6 hours agoparent> The worst offender (so far) is the JBIG2 format (several major libraries, including jbig2dec), a very popular format that gets EXTREMELY high compression ratios on bilevel images of types typical to scanned pdfs. But: it's also a format that's pretty slow to decompress—not something you want in a UI loop, like a PDF reader is! And, there's no way around that—if you look at the hot loop, which is arithmetic coding, it's a mess of highly branchy code that's purely serial and cannot be thread- nor SIMD- parallelized. Looking at the jbig2dec code, there appears to be some room for improvement. If my observations are correct, each segment has its own arithmetic decoder state, and thus can be decoded in its own thread. The main reader loop[1] is basically a state machine which attempts to load each segment in sequence[2], but it should not need to. The file has segment headers which contains the segments offsets and sizes. It should be possible to first decode the header and populate the segment headers, then spawn N-threads to decode N-segments in parallel. Obviously, you don't want the threads competing for the file resource, so you could load each segment into its own buffer first, or mmap the whole file into memory. [1]:https://github.com/ArtifexSoftware/jbig2dec/blob/master/jbig... [2]:https://github.com/ArtifexSoftware/jbig2dec/blob/master/jbig... reply perihelions 6 hours agorootparent- \"If my observations are correct, each segment has its own arithmetic decoder state, and thus can be decoded in its own thread.\" Yeah, but real-world PDF JBIG2's seem to usually have one segment! One of the first things I checked—they wouldn't have made it that easy, the world's too cruel. It's sort of a generic problem with compression formats—lots of files could easily be multiple segments that decompress in parallel, but aren't—if people don't encode them in multiple segments, you can't decompress them in multiple segments. Most formats support something like that in the spec, but most tools either don't implement that, or don't have it as the default. e.g. https://news.ycombinator.com/item?id=33238283 (\"pigz: A parallel implementation of gzip for multi-core machines\" —fully compatible the gzip format and with gzip(1)! No one uses it). reply sparkie 5 hours agorootparentYikes! Doesn't seem like there's anything that can be done to solve that then. I guess the only way to tackle it would be to target the popular software or libraries for producing PDFs to begin with and try to upstream parallel encoding into them. Or is it possible to \"convert\" existing PDFs from single-segment to multi-segment PDFs, to make for faster reading on existing software? reply perihelions 5 hours agorootparentConversion's a very good solution for files you're storing locally! I'm working on polishing a script workflow to implement this—I haven't figured out which format to store things into yet. I don't consider it a full solution to the problem—more of a bandaid/workaround. The downside is that any PDF conversion is a long-running batch job, one that probably shouldn't be part of any UX sequence—it's way too slow. Emacs' PDF reader does something like this: when it loads a pdf, its default behavior is to start a background script that converts every page into a PNG, which decodes much more quickly than typical PDF formats. (You can start reading the PDF right away, and the end of the conversion, it becomes more responsive). I think it's a questionable design choice: it's a high-CPU task during a UI interaction, and potentially a long-running one, for a large PDF. (This is why I was profiling libpng, incidentally). https://www.gnu.org/software/emacs/manual/html_node/emacs/Do... reply 698969 9 hours agoparentprev> a very popular format that gets EXTREMELY high compression ratios on bilevel images of types typical to scanned pdfs Funny you say that, https://en.wikipedia.org/wiki/JBIG2#Character_substitution_e... reply cocodill 9 hours agorootparentI can still remember this cool talk about that. https://www.youtube.com/watch?v=7FeqF1-Z1g0 reply eddd-ddde 9 hours agoparentprevQuestion since you are probably knowledge-able about it right now. > Another observation: lots of people create PDF's at print-quality pixel density that's useless for screens, and greatly increases rendering latency. Is this relevant to text in the PDF? I would assume text is vectorized, meaning resolution is not relevant until you _actually_ print it? Or is it just relevant to rasterized content like embedded images? reply perihelions 9 hours agorootparentYour understanding's right: PDF's that are text + fonts are easy and fast. I'm concerned about the other kind, that's scanned pages. Any sheet music from Petrucci / imslp.org for one example. That kind is a sequence of raster images, stored in compressed-image formats which most people aren't familiar with, because they're specialized to bi-level (1-bit, black and white) images. A separate class from photo-type images. The big two seem to be JBIG2 [0], and CCITT Group 4 [1], which was standardized for fax machines in the 1980's (and still works well!) [0] https://en.wikipedia.org/wiki/JBIG2 [1] https://en.wikipedia.org/wiki/Fax#Modified_Modified_READ (You can examine this stuff with pdfimages(1)—or just rg -a for strings like /JBIG2Decode or /CCITTFaxDecode and poke around). reply ale42 54 minutes agorootparentPersonally I have the impression that CCITT group 4 compressed PDFs are displayed very quickly, unless they are scanned at 3000 DPI... Can't say the same for JBIG2 or JPEG/JPEG2000 based ones. reply jancsika 8 hours agorootparentprev> A separate class from photo-type images. I'd assume that the photo-type image decoder is optimized, right? If so, how does the optimized photo-type decoder compare to the apparently unoptimizable JBIG2 decoder? reply perihelions 8 hours agorootparentI'm not knowledgeable to speak to that, but just to clarify—the low-hanging fruit in libpng I mentioned is in simple, vectorizable loops—conversions between pixel formats in buffers. Not in its compression algorithm (which isn't part of libpng—it calls out to zlib for that). reply masteruvpuppetz 2 hours agoparentprevis there any ffmpeg like command line program for pdfs? Creating, Appending/Removing pages, viewer, etc? reply pentacent_hq 52 minutes agorootparentFor appending, removing, merging pages, there’s pdftk: https://www.pdflabs.com/tools/pdftk-server/ reply emeril 9 hours agoparentprevsumatrapdf seems better than most at reading them? reply user_7832 12 hours agoprevFor me, it's DIY audio. The thing about diy'ing audio (primarily speakers but also amps, DACs etc) is that you can get top of the line performance for a fraction of the market price. A $50,000 speaker setup that would bring tears to your eyes could be made for perhaps $5000. A DIY $500 kit can perform similar to a $2-3000 set of speakers. Open source amps with gerber files on github are amazing. The biggest reason it's so easy to get amazing value is because that $600 speaker only has $150 of materials. Upgrading its $25 woofer to a $80 one would help a lot, but no company would do that and not sell it now for $1000 if they could. However the biggest allure for me is not beating commercial systems on cost, but making what I want. A small speaker with deep base? Easy. Speakers with quasi-active noise cancellation behind them? Sure, why not. Speakers that'll make the most overpowered/fancy beach-boombox sound like a crappy toy? Simple. The only limit is your imagination and time/money. I'd very much recommend diyaudio.com, but be warned, parts of this field are mature while others are still in effective infancy. Also, being an engineer (electrical/mechanical) helps a lot, there's a ton of signals processing and electrical/mech oscillation. reply wdfx 25 minutes agoparentThis for me, for quite a while now. I've built my own speakers, microphones and now I'm in a DIY synth module hole. Latest at https://fourays.lon.dev I've just got to the point where I think I know what the module is going to be, but last night found out that PCB manufacture puts additional constraints on the PCB design, so I have to go back and re-do a lot of it, including probably dropping some features to make it simpler. The learning never ends. reply greensoap 10 hours agoparentprevAll that build up and not a link to be seen?!?! Where do I go to spend $ on a new hobby I didn't need? reply user_7832 16 minutes agorootparentThe easiest \"take my money\" approach would be to look at some Troels Gravesen speakers on his website and/or find kits being sold on sites like parts express (if in the US). Jeff Bagby and Paul Carmody are two other well known designers, the latter having more budget-friendly builds. Additionally, sites like diyaudio.com are better when you want a specific thing built and are looking to learn more about techniques, new parts etc. reply all2 10 hours agorootparentprevCheck out Hexabase on YouTube, and then check out DIYAudio.com. Those will whet your appetite, I hope. reply pyinstallwoes 3 hours agoparentprevHow hard to make something better than my hs8’s? I want an upgrade but next tier is like 3k reply user_7832 23 minutes agorootparentHow much are you willing to spend? Troels Gravesen has many builds on his website and I'm sure some/many of them would be better than the HS8s. You could also search for \"hs8\" on diyaudio.com and see posts of people in a similar position. reply luisrudge 11 hours agoparentprevAll I want is a small stereo speaker to replace my awful monitor speakers. Like a small soundbar that doesn't suck. I might dive into this rabbit hole reply user_7832 25 minutes agorootparentCheck out this speaker kit call C-Notes. They're really well rated and a pretty simple/budget friendly option, especially if you're okay applying some eq on them. Paul Carmody has many good designs. reply pjc50 1 hour agorootparentprev\"Small\" and \"decent audio response in bass\" don't really go together. But you can go surprisingly far with a pair of \"bookshelf\" speakers. reply user_7832 21 minutes agorootparentWith passive speakers, I agree, but with active/DSP speakers you can do ludicrous things. There's this build on diyaudio called something like \"compact active 3 way\", that'll give you an idea of how decently powerful a small speaker can go (and that's despite some design flaws in the build like the choice of a passive radiator). reply all2 10 hours agorootparentprevCheck out Hexabase on YT. He has really punchy small speakers that he has released designs for. reply sowbug 10 hours agorootparent> Check out Hexabase on YT. https://www.youtube.com/@HexiBase in case you're having trouble finding it with that spelling. reply all2 3 hours agorootparentThank you. Mea culpa. reply peteforde 10 hours agoprevI did not realize that learning Fusion 360 was going to be such a huge chapter in my current journey. Looking back, I'm kind of stumped at how I avoided it as long as I did. I would now put learning CAD in the same category of mandatory life skill as learning to code. The ability to translate what you see in your mind to something that can be repeatably fabricated is an incredible power move, akin to learning how to communicate complex ideas with empathetic language. My advice is to start by following this tutorial step-by-step. It's a 90 minute video that took me ten days to get through. Step two is to take an existing project and change it in a significant way. Step three is to create something from scratch which solves a problem that you have. https://www.youtube.com/watch?v=mK60ROb2RKI reply adius 4 hours agoparentCan second this! However, I would recommend the open source https://solvespace.com! It hits a sweet spot between features vs complexity/learning effort. (And as a programmer I dig the terminal aesthetics) reply all2 10 hours agoparentprevIf you don't know about Plasticity 3D, you should know about Plasticity 3D. It is different, but still parametric, and CADish. reply swah 10 hours agoparentprevThis was super cool. I guess if you want to \"simulate\" its a whole different world? reply nwiswell 9 hours agorootparentNot really! Many of the major CAD packages have built-in or add-on packages for FEA. Some of them will even do dynamic multiphysics simulations -- so for example you could model an internal combustion engine, simulate the combustion cycle, and measure the torque. (That's not a beginner-level project, to be clear) example: https://www.youtube.com/watch?v=dejend_kx94 reply huytersd 7 hours agoparentprevI picked it up to do CNC hobby work and I can’t get enough of it. reply brcmthrowaway 9 hours agoparentprevDo NOT use this proprietary nonsense, learn FreeCAD reply mft_ 22 minutes agorootparentFriends don't tell friends to learn FreeCAD. I'm a decently able self-taught CAD user now, of the level where I can reasonably quickly pick up a new piece of software. And yet... I've lost count of the number of times I've reinstalled FreeCAD thinking \"this time it will be different\"... and then quickly removed it again. Compared to anything reasonable it's just an awful hot mess to try and figure out, with huge quirks, a weird interface, and unhelpful error messages. Given the reasonable pricing, I'm interested to try Plasticity, although it's not strictly CAD in the sense of Fusion360/Solidworks/etc - it's currently more of a modelling program. It's also doesn't have the parametric + history features that are really valuable in other products. The truth is, we're crying out for a decent open-source CAD program. Everything currently available (FreeCAD, OpenSCAD, SolveSpace, CadQuery, etc.) has huge usability and/or feature deficits compared to the commercial offerings. reply wdfx 23 minutes agorootparentprevTo quote myself from a previous blog post of mine: > I tried and tried and tried to get into [FreeCAD]. It promises so much, but there are two fatal flaws in my opinion. First, the UI is a nightmare. I have no idea which \"workbench\" I am supposed to be using, and there are so many similar choices available, each with subtly different tools and ... I gave up trying to make sense of it. Secondly, even when following tutorials to get some basic modelling done, I found lack of sensible keyboard control and having to click almost everything a real distraction. Not a good experience. It's an absolute nightmare to use. Really the worst UX I've ever seen. reply mianos 8 hours agorootparentprevI am a developer, FreeCAD is a bit of fun to wrangle a model out as a puzzle but it's not even in the same universe as Fusion360. I'd love to use something like KiCAD is in the EDA world. I highly expect, at any moment, Audodesk to pull the rug from under the hobbyist licence and I'll not be able to use it anymore. While I feel kinda 'dirty' when I use it, I have been using it for maybe 8 years and the thing is a masterpiece. It's like a third arm for me. Once your brain knows the whole constraints workflow it is so natural. I can very quickly model up pretty much anything I can think of to make. reply brcmthrowaway 7 hours agorootparentFusion360 was good until it became SaaS reply adipose 9 hours agorootparentprevmind elaborating on this? as an outsider to CAD software i don't know the landscape — i'm finding some concerning things about fusion360 when i search for pros/cons but i'm not sure how to weigh the opinions, e.g how relevant the commercial lock-in could be for a hobbyist reply rich_sasha 3 hours agorootparentOne time FreeCAD user: FreeCAD couldnt solve a very basic constraint. Every time it got stuck and said it's too hard. Fusion360 had 0 issues. Licensing is a thing with FreeCAD but last time I looked it was free (as in beer) for hobbyists. The grey zone is where you turn your hobby into a small scale business. reply steve_adams_86 10 hours agoprevI’ve fallen into the plant tissue culturing rabbit hole. I was selling excess trimmings of aquarium plants locally on Facebook marketplace and made a surprising amount of money, and my kids really enjoyed it (they got a cut for helping me out). I thought hmm, this could be a great excuse to make a little business around this, teach them some skills, get them thinking more constructively and feeling a sense of agency and ability, etc. Plus earning money is really nice when you’re a teenager. The challenge is that in an aquarium, plants grow reasonably fast but not fast enough to sell regularly for a decent income. You need ways to produce more plants faster, more reliably, and without taking up too much space. That’s where tissue culturing comes in. It has reaaaally sucked me in. I’m culturing everything I can find. I’m also propagating aquatic plants through more typical means, and that’s fun too. I’m out of space though. Tissue culturing is a really fascinating science and practice. I love keeping track of the media recipes, results, growth rates, etc. I’m too early to have had meaningful results, but I look forward to tracking those as well. reply virtuscience 9 hours agoparentThis is really cool. I work in the plant industry, and most people don't know this but huge numbers of the plants bought in the U.S. are grown from tissue culture. I would guess it could be in the ballpark of half of all small and medium-sized plants (The biggest dumbest problem is when blinds accelerate to the point that the minimum bet is greater than what Kelly would recommend. Some online poker sites are aggressive about this, to the point that you are forced to make irrational choices pretty quickly or get blinded out. A way to juice the house advantage I guess. It's almost enough to make me give up and find the next project. The house does not have an advantage with poker. With player games, they earn money from hosting by taking vig(orish). But rapidly rising blinds in tournament poker is a way for them to get more games played, hence introducing turbos in tournament poker and Zone-type cash games. Better understanding common betting patterns is more useful than hand simulators or whatever. Being able to accurately update your opponent's range throughout the hand is essentially how to play well. reply brcmthrowaway 9 hours agoparentprevDid you write an algorithmic betting software? reply tunesmith 6 hours agorootparentNah. When playing solitaire I just use a Kelly calculator. I plug in estimated equity and pot odds, and get a recommendation of what percentage of my stack to bet. It's just to get a sense of how to make riskier bets when the pot is large. reply unrealp 1 hour agoprevI tried creative coding. Sonic Pi, Processing, p5js, there are so many frameworks built around creative coding that I didnt know even existed. It was really fun. reply TheBozzCL 4 hours agoprevOoh, another one I have: I cosplay as a sysadmin. A while back I had bought a domain for my email, and I thought “I should write a blog about creating a blog”. At first I hosted it in GitHub Pages, but then I realized I have a perfectly good Raspberry Pi. It’s not like I’m ever gonna get a lot of traffic… so why not self-host? That sent me into a very deep rabbit hole. How do I make sure my website doesn’t go down if my IP address changes (no static IPs for me, sadly)? How do I create and automatically renew a certificate? How do I achieve high availability? A few years passed, and now I have a cluster of a few Raspberry Pis running Docker Swarm, managed by Portainer, with stacks running multiple websites and services I self-host. I’ve learned a lot! My next move is going to be a full overhaul: Docker Swarm is blocking me from setting up some things the way I want to, so I want to build a new cluster using Kubernetes. I’ll use the opportunity to overhaul the network layout as well. The funniest part is that I haven’t written a single blog post in 3 years. I wanted to add responsive images so I could add diagrams and photos. Somewhere along the way I realized I shaved too many yaks. reply eddd-ddde 3 hours agoparentWhat's your stance on using something like cloudflare to proxy your pie fleet? Cloudflare tunnel would solve the issue of static IPs and you also get DDoS mitigation and caching. Caching on the edge would be especially beneficial for something like a blog which is likely to be fully static or SSG at most. Although it won't help you with the writing part (; reply jvolkman 9 hours agoprevI got into cross-compiling Python wheels (e.g., building macos wheels on linux and vice versa). Zig's `zig cc` does much of the heavy lifting, but one step in building a portable wheel is the \"repair\" process which vends native library dependencies into the wheel, necessitating binary patching (auditwheel does this for linux, delocate for macos). I wanted to be able to do this cross platform, so I re-implemented ELF patching and Mach-O patching and adhoc signing in Python, and wrapped them into a tool called repairwheel: https://github.com/jvolkman/repairwheel reply meter 12 hours agoprevThis past month, I’ve been reevaluating my dev environment and workflow. My goals are to reduce RSI, be more efficient, as well as learn all my tools as deeply as possible. And have fun! - I’ve ditched VSCode and gone all-in on NeoVim. I’ve spent a bunch of time watching Primeagen, etc., tweaking my vid config and learning how to navigate as efficiently as possible. - Switched from QWERTY to Colemak-DH to hopefully reduce RSI. I’m at about 70wpm with decent accuracy after 4 weeks. My QWERTY skills are gone. I like Colemak, but we’ll see how I feel in another month or two. - Finished my custom hot swappable Sofle keyboard, and spent many hours customizing the layout. I think I’m pretty close to feeling comfortable. I’m using home row mods, which I love. Currently using Kailh box whites (clicky). Might switch to Gateron Brown Pros. - Been going through a “Build your own git” course, to understand git as deeply as possible. reply lycopodiopsida 4 hours agoparentFor my issues, the combination of apple trackpad and a proper ergonomic keyboard with a keywell did it: at first Kinesis Advantage 2, then Glove80. Model 100 is also quite good. reply peteforde 10 hours agoparentprevStrongly urge you to acquire a Logitech MX Vertical mouse. It takes a few days to get used to; now you'd have to pry it from my dead hands. reply auto 9 hours agorootparentI went down this path and tried various vertical mice, and settled on the MX V for a few months in the pursuit of reducing wrist pain. After about 4-5 months or so, I started getting strong wrist pain again, and switch backed to a standard mouse. At that point I started looking elsewhere, specifically on strengthening my wrists and joints. I've been doing this about 5 times a week for probably 5 months now, and most of my mouse hand wrist pain has subsided: https://www.youtube.com/watch?v=iVum3vWlh4Q reply meter 8 hours agorootparentThanks for the video. I’ll definitely try it out. In my original post, I should’ve emphasized that I’ve also been focusing on strengthening/mobilizing my wrists. It’s only been a month or two of concentrated effort, but I think they’re improving a bit. reply seabass-labrax 11 hours agoparentprevCongratulations on learning Colemak! I made that journey myself and haven't regretted it once since. I did have to relearn QWERTY, frustratingly, but luckily it isn't nearly as difficult as learning a layout for the first time, and I can now switch between them relatively easily. (For me, a few years on, I now type at 100-120 WPM on Colemak; I was also at 70 WPM four weeks after starting.) reply meter 10 hours agorootparentThanks! That’s encouraging to hear that you can switch between the two. Awesome! I’m afraid to start practicing QWERTY too soon, and risk losing my progress with Colemak. Maybe I’ll attempt it in a few months. reply inhumantsar 9 hours agoparentprevoooh the sofle. i'm curious about your layout! been using a Moonlander for a few years and while I like it, it's just too big. ordered a sofle variant recently and I've been thinking about switching back to a dvorak or trying colemak when it arrives. reply meter 8 hours agorootparentThe Sofle has been decent for me. I’m not the biggest fan of thumb key positions (especially the outer one), but I’m getting used to them. There are only two thumb keys per side. I’ve had to get a bit creative with my layout. One trick I’ve discovered is Mod-Tap. This lets me use my space bar as a layer key (when held), or a normal “space” when tapped. Two functions on a single key. Awesome. I’ve also been reading this person’s blog to improve my symbol layer and vim navigation (I’m tempted to try the Engram layout, but I’ll stick with Colemak for now): https://sunaku.github.io/engram-keyboard-layout.html reply vbezhenar 9 hours agoprevOur company decided that they want to make an electronic device. They've found an electronics engineer who devised a schematics and wrote some kind of PoC firmware. I wrote \"driver\" for it (really just wrapper around serial port) for our software to consume it. That guy was busy with other tasks, so iterating on firmware was too slow. So I decided to dive in. I mean I knew C a bit. So I had to learn STM32 arm, I had to learn low level C, I had to learn assembly, I had to get some understanding of those electronics things to get some sense of it, I had to read tons of manuals and datasheets. Long story short, I rewrote this PoC firmware into something I could bear. It's so nice to control all software from the start to the end. Now our company wants to rework this device into \"smart\", add display with touchscreen and stuff. So I'm digging into embedded Linux programming, LoL. I'm generally consider myself full stack developer, so I can write frontend, backend, kubernetes, setup servers, deal with cloud stuff. However digging that deep feels like testing my limits. reply jacknews 4 hours agoparent\"Now our company wants to rework this device into \"smart\", add display with touchscreen and stuff.\" please don't do that. Add some kind of interface, eg bluetooth, and an open-source app, or open-source/documented protocol, to control it. reply gnarcoregrizz 12 hours agoprevMetal-air batteries/fuel cells. Made a mini aluminum air battery (you can easily DIY one with household items). It seems that most people consider metal-air batteries to be a dead-end, since they aren't green and are generally non-rechargeable, and air cathodes are tricky (sluggish, exotic materials, expensive catalysts). I dove into \"alternative\" battery and fuel cell research after looking into how to extend the range of my electric motorcycle. I love the electric drivetrain, especially on motorcycles, but lithium ion isn't up to the task as far as capacity for anything beyond an hour or two of high performance fun. If I could get a compact metal air battery or hydrogen fuel cell to output just 1kw for a hybrid drivetrain, range issues could be solved. reply jpt4 9 hours agoparentI would be interested in discussing your project further, for use on my e-bike. reply poyu 12 hours agoprevUSB! I’ve tried and failed couple times in understanding how USB works under the hood, from electrical, to protocol, then classes, and also Power Delivery. This time around things seem to make more sense now. It started out as an ambitious goal to emulate an FTDI USB DMX converter with the ESP32-S2/S3, but realizing that might be too big a goal, so I’m starting small. I want to be able to make a custom device class on the ESP32, and write a driver with libusb. reply umvi 11 hours agoparentUSB is a frighteningly deep rabbit hole. I thought it was going to be easy when I first dipped my toes into the USB stack but boy was I in for a shock. reply datavirtue 10 hours agorootparentI had a peek at the USB spec recently, no desire. I thought it would be trivial to even just enumerate USB devices on a Windows box. It is not. reply nph278 9 hours agoprevGroup theory. Something is incredibly beautiful to me about classifying the kinds of symmetry things can have. I’m trying to understand where the sporadic simple groups come from. Starting with the Matthieu groups. So far it seems to be due to some anomaly in Pascal’s triangle, but I’m still trying to put it together. “Another Roof” on YouTube has a good video about this. reply newusertoday 3 hours agoparenti was interested in signal processing with group theory it was quite interesting to see how matrix multiplication corresponds to modulo over finite group.(I am not a math major). reply scubbo 9 hours agoparentprevI'm over a decade out from my maths undergrad degree, but I still remember how beautiful Group Theory felt. Enjoy it! reply dvno42 11 hours agoprevGoLang and Azure APIs. I've been attempting to add a oauth2 device code flow to a Tacacs server with the goal of extending Azure accounts to access network device management planes. Pretty neat, I can get a \"enter this device at URI\" from the router/switch and let Azure do it's 2fa/compliance etc. Currently trying to get token validation working on the tacacs server =). Ultimate goal is have a reverse proxy web front end kind of like Apache Guacamole that does the Oauth for the user and when they click on a network device, the JWT is passed through to the network device over SSH and thus the tacacs server which is relatively local to the network device which will validate it and let the user into the network device. Playing around with GPT4/Opus a lot lately and man... I have feelings. They've been a great learning tool to learn the basics of Go though so I'm thankful. It's going swimingly /s but I seem to be making progress. Slowly, I'll bake this into my bigger network management tool if it an be secure and make sense to do so... reply K-Wall 12 hours agoprevI've been bitten by the Meshtastic / LoRa bug. It's fascinating to see how far these little inexpensive units can reach. This weekend I was able to reach my home node from a state park 8.2 km away and have been giddy since. reply ajxs 12 hours agoprevI was writing an article on reverse-engineering vintage synthesisers[0], and I ended up getting majorly sidetracked trying to find out exactly what year the Hitachi HD44780 LCD controller was first manufactured, or try and find any background information on it. The earliest reference to it I can find online is a 'preliminary' user's manual[1], dated March 1981. I know it's a bit of a weird rabbit hole to go down, but I figured I'd get to the bottom of this mystery. It's a shame that there's so little background information available about one of the best-known ICs in history. 0: https://ajxs.me/blog/Introduction_to_Reverse-Engineering_Vin... 1: https://archive.org/details/Hitachi-DotMarixLiquidCrystalDis... reply bjourne 10 hours agoprevHistory of early Christianity. I found the gospel if Judas to be extremely fascinating. reply smackeyacky 12 hours agoprevRebuilding a Ford Cleveland V8 - the Australian specific 302 cubic inch version. One broken engine and one non operational one and turning them into a single good motor. American thin-wall cast V8 engines are fairly similar, but different enough that if you don't get them built you have to do a bit of puzzle solving (especially in the timing case). Plenty of youtube videos and forum posts on the Cleveland and it's been fun piecing it back together and learning about new things like installing cam bearings. reply nyjah 10 hours agoprevGolf simulation. I found myself in a unique situation where I had the space, computer and projector. So I got a mat, screen and the key piece, a launch monitor. Now I’m playing 9-36 holes of golf simulation almost every day. The tech is great, the setup is never ending. Golf simulation tech is blowing up right now too. I’m not even necessarily a golfer fwiw. But I love the simulator. reply JamesSwift 9 hours agoparentI'll never set it up but I'd love to hear about the tech choices/options. reply nyjah 8 hours agorootparentThe software that I am using to simulate the courses is GSPro. There are hundreds of courses on there. It's got just about every course you could imagine. Launch monitors come with their own software too and there are a few other options for simulating courses. E6 is one and GolfClub2019 is another. AwesomeGolf. GSPro is hard to beat though. For launch monitors, there are two main types: camera based and radar. Garmin offers the most affordable radar based option with R10 for around $600. Bushnell offers an all camera model and for $2000 you get all the ball data. For a subscription fee you can play GSPro and other 3rd party golf apps using the Bushnell and for another fee you can get all the club data. This model sits beside the ball. FlightScope has a launch monitor that operates with radar and/or camera and sits behind the player. For about $1800 you can get the ball data and its free to connect it to GSPro. For another $1200 you can get club data, with impact location. It's unreal how accurate this thing is. I've had mine for about a year, and since then they have pushed some incredible updates, including what they call \"Fusion\", which is combining the camera and radar for readings. Its how their really expensive 20k+ unit works. At the very top end there are monitors that go on the ceiling and give you readings from there. And then there are commercial simulators where the floor will move up and down. It really never ends. One company showcased lighting from above that shows you where you should putt. It never ends. . . https://gsprogolf.com/ https://www.garmin.com/en-US/p/695391 https://www.bushnellgolf.com/products/launch-monitors/launch... https://flightscope.com/ https://www.foresightsports.com/pages/gchawk https://uneekor.com/ reply bambax 10 hours agoprevWas into cine drones for a while; then discovered FPV. Spent dozens of hours on simulators, now flying actual FPV drones in manual mode. Incredibly addictive. I'm having a hard time focusing on anything else right now. reply bigiain 8 hours agoparentHeh. I was building/programming/racing FPV drones in 2013/14, back before teenaged reflexes and rich dads paying thousands for top line gear became a thing - and easily started beating 50 year old eyesight and reflexes with self imposed budgets of only a few hundred bucks... reply bilsbie 9 hours agoparentprevAny tips on getting started? Is it possible to start super cheap just to see if I like it and then upgrade? reply bambax 3 hours agorootparentFor starting on a simulator you need a remote control (gamepads can technically work but are not recommended). The Jumper T-Lite V2 is around $60. Liftoff is one of the most versatiles simulators and costs around $20 on Steam (there are lots of others). Then to start in real life, complete kits from BetaFPV or GepRC are around $200 (including drone, rc and analog goggles); you can find them used for about half that, in excellent condition. But there is NO POINT in trying to fly an actual drone before doing plenty of hours on a simulator: you would crash constantly and destroy the drone before you even get started. So just start on a simulator. 10 hours is the absolute minimum you'll find everywhere, but I'd recommend around 50 (you can listen to podcasts at the same time). If you want to go the extremely cheap route you can start with a cheap simulator (FPV Freerider, $5) or even a free one (FPV SkyDive?) and use an existing gamepad -- but gamepads really are confusing and don't work like RCs (the throttle joystick should not center automatically). reply sunir 13 hours agoprevI switched to Neovim from Sublime Text after trying copilot in Sublime, feeling sad, and then watching The Primeagen and his glorious mustache for too long. Ostensibly I wanted to be able to code on the production server like a miscreant with the same tools as my laptop. However I just wanted to regain command of my dev environment after years not coding. I also reorganized the furniture in my office and got weirder lighting to make it hacker friendly. I bought a new desk to solder electronics. Most people know me as a partnerships marketer or product manager but I am a compsci at heart. This made me happy. reply contact9879 12 hours agoparenti've been attempting to switch to neovim off and on for about a year now. VS Code is so much much easier to get started with though. And adding support for a new language is just an extension-install away. reply sunir 12 hours agorootparentThat’s fair. I can’t use vs code on the server was my logic. But also it was a hacking challenge. reply SonOfLilit 11 hours agorootparentWhy not really? Remote editing through ssh is vscode's superpower! reply sunir 11 hours agorootparentThere’s no reason. I just wanted to be cool and use neovim! Lol :) reply akdor1154 12 hours agoparentprevNeovim — as good for a mid-life crisis as a Porsche, and a fair bit cheaper. reply sunir 12 hours agorootparentTrue. My friend is selling electrified retromodded Porsches. I want one but I am poor. However I can salve my ego spending a day flipping through neovim colour themes. reply fransje26 12 hours agoparentprevSo, how was the switch to Neovim? Which plugins did you settle on? reply sunir 12 hours agorootparentI used nvchad and I am configuring it from there. Here’s my fork. https://github.com/sunir/NvChad Overall I still think I am faster in sublime text. I get stuck in the different modes. I find shift select and grep to be pretty frustrating. However I will muscle through this. Every challenge is another set of vim stuff to learn. I have faith I will love it later. reply cwp 10 hours agoprevI got a ZSA Voyager split keyboard and then spent weeks exploring custom layouts. The first question was QWERTY vs something better. Then there was layers and layer navigation. And should I swap out the key switches? And Keyboard Maestro. Now I'm trying to abandon 30 years of muscle memory and typing at 4 wpm while I learn Colemak-DH. Maybe what I should really do is build a custom 34-key board... reply pronoiac 6 hours agoprevMine all feel like \"we do these not because they're easy, but because we thought they would be easy\". Turning a thousand-page book - PAIP - into a stack of Markdown files in a git repo, readable online. The print book received more editing and revisions than the ebook. I converted an ebook's ... odd formatting ... into Markdown, remade diagrams, generated new ePub and pdf files, and had the spine cut off a print copy to make a fresh scan. Working on that scan, I made Scantailor, an X program, easier to access from a Mac, via Docker. I tried different OCR engines, and pored over the diffs, incorporating dozens (hundreds?) of improvements. I got to find so many differences between Markdown engines. I have ideas on how to make Pandoc links between chapters. There's still a lot to do! My current WIP: Lars Wirzenius posted about file systems with a billion (empty) files. I started exploring because I was curious, if I was remembering correctly, how well a mostly empty image file would recompress - like, drive_image.gz.gz. Lars offered a Rust program; I was curious about how other methods compared. Like, how about nested shell loops, tar, and touch? And, hey, how well can we archive and compress them? I've gotten to see some issues, bottlenecks, and outright failure modes with SMR hard drives, Samba re: sparse files, and parallel gzip compression. I've accumulated some shell script boilerplate to make it easier to go back and verify my processes, and harder to accidentally wipe out past work if I rerun it. reply batch12 11 hours agoprevI've been researching and planting fruit trees and edible plants. Looking at paw paws, peaches, pears, berries, persimmons and tea bushes. reply qup 5 hours agoparentI'm in a paw paw/persimmon zone, and figs grow nicely. One of the lowest-maintenance trees that don't have much insect pressure. I thought they were notably absent from your list. The variety I chose tastes much like a peach. reply Yenrabbit 12 hours agoprevFun exercise to try and list them out! My last couple of weeks: - 3D-printable parts storage solutions (via: I found some part storage bins in the discard pile at a local hackerspace) - MITM proxy to snoop on Github Copilot API requests (via: we're building an jupyter AI assistant thing and got curious how other players do it). - DIY robot arms (via: I'm making several for a nested 'you pass butter' joke, via a casual conversation about robotics being accessible now. YouTube is amazing at surfacing smaller makers once you start watching a few videos on a given topic) - Learning about Oauth and JWT (via: 'why is auth still a pain?') - Invertebrate UV fluorescence (via: that millipede is glowing under my UV torch!) (a small subset of these end up documented https://johnowhitaker.dev/all.html eventually if you're curious to see a longer historical list) I like rabbit holes where following the curiosity gradient to a satisfying conclusion is possible. \"How does X work\" leads eventually to code that does X. I'm less happy when they lead into a tangle of complexity, like digging into a library only to find weird abstractions 6 layers deep or trying to compare 18 different alternatives in a field I don't know very well. OP I'd also like to hear yours! reply RetroTechie 11 hours agoparent> OP I'd also like to hear yours! Today I gave some thought to what would be a fitting name for my boat (if I were to rename it). One option: the glider pattern from Conway's Game of Life. Instantly recognizable by true hackers, just a weird symbol to others. Of course a quick check on Wikipedia. Know that I'm always interested in things small / simple / computing, so... cellular atomata. Which led me to varieties used to simulate or help understand biological systems (\"systems biology\" - if only that field had even existed back when I left high school). From there on: artificial life, Core Wars & co, self-replicating machinery, and... Astro-chicken (deserves a HN post of its own, imho). Btw. it's amazing to see how many big, open questions there still are, related to the origins of (biological) life, and evolution. Eg. full simulation of a single cell organism: never been done (too complex). Next up: a cup of hot chocolate. reply iamhamm 12 hours agoprevVisiting and documenting abandoned mine sites across the U.S. desert southwest. reply keyle 10 hours agoparent+1 for literally rabbit hole'ing :) reply 7thaccount 8 hours agoparentprevIs hantavirus a possibility? reply feydaykyn 3 hours agoprevMiniature painting I wanted to be able to easily distinguish between the 60+ figurines of the Zombicide board game, so I figured I could paint them \"quick and dirty\". Well 8 months later, I'm not finished because I \"had to\" learn about paint, color theory, paint mixing, human vision, brush types... Being colorblind I gave no attention to colors around me, but I have since discovered I can see more shades than I was aware, and I'm having a blast just looking at the foliage... Which does not speed up the painting! reply rramadass 5 hours agoprevFormal Methods; and i don't think i am going to get out of this rabbit hole anytime soon in the future. More than just using some formal language/tool i had wanted to learn about the Mathematics/Ideas behind formal methods and how they are embodied in TLA/Z language/B method/etc. After a survey of available books i zeroed in on Understanding Formal Methods by Jean François Monin hoping to get an overall idea of how everything comes together. But what i got was a fire-hose/mishmash of so many different sub-fields/notations/abstractions used in the field that it is quite a struggle to get a good grasp on anything. The author's style of writing is obtuse/challenging and the contents are more of a survey/introduction than detailed explanations. The result is that i am now interested in figuring out a whole lot of mathematical/logical sub-fields which i suspect is going to occupy a lot of my time in the future. reply JeremyNT 6 hours agoprevHam radio! I got into it through wanting some cheap radios to keep in my house, so I wouldn't have to go all the way upstairs when I needed to communicate with my daughter. Well let me tell you Baofeng radios are extremely cheap but really flexible. I got these things for the simplest possible use case but after realizing their potential I just had to learn more about the space. You can adjust their configuration with a tool called Chirp and you're off to the races! I attended a local severe weather awareness event where I met some hams who were part of an emergency response network. It's really cool to learn about how these communities operate. It's legal to receive even without a license - you only need the license to transmit. I plan to take the technician test soon and get my license so I can help out at a nearby bike event. The area is incredibly rural so there's no cell coverage and the ham operators are really helpful in coordinating things. Anyway, I feel like the hobby is a bit of a dying art, but it's something that seems like it would have a lot of appeal to the programmer crowd. reply chris_st 10 hours agoprevGot a ARTinoise Re.corder (an electronic wind instrument, basically shaped like a recorder, but MIDI) for my wife. Works with an iOS app for basic sounds. She's spent very little time with it :-) The REAL rabbit hole is the astounding amount, and quality, of AUv3 plugins for iOS. Sounds, effects, looping tools, MIDI things, just... wow. And almost all of them are under $20, and many are free! I've spent less on a dozen software toys than on the first two guitar pedals I got. And infinitely more powerful. Check out this video of someone doing the looping thing way way way better than I'll ever be able to (but it's fun to work towards a goal). Software she's using is called Loopy Pro, another amazing thing: https://www.youtube.com/watch?v=T1O0pwUMbnw reply niccl 12 hours agoprevTrying to _really_ understand the postgres query planner's `EXPLAIN` output. We have long-running embarrassingly parallel processes where the throughput will sometimes completely tank. Got worse when we upgraded to PG16. Trying to compare good query plans with bad ones, and then work out what changes we need to make to the slow queries is ... interesting. reply datascienced 11 hours agoprevI haven’t had time to even look but etcd is on my list. It is a distributed key/value for node configuration (I think!) The reason why I want to learn more about it is I feel it is like a base building block of distributed systems and may be easier to grok and even write a toy version than a bigger thing like kubernetes or a leaderless distributed datastore. I would also learn some go and know how a critical piece of kubernetes works. What led me there is practicing for a damn system design interview. As much as this whole topic is controversial on HN the grinding has really got me curious about the tech that runs at larger scales and how it works under the hood. reply ak_111 12 hours agoprevMH370. There was no intermediate step as I saw somewhere it was the 10th anniversary of its disappearance and decide to get a quick update on the current status of all the evidence and theories. Ended up spending the entire day reading about radars, pings and aviation controls. reply naiv 12 hours agoparentWhat did you find is the most reasonable explanation? reply ak_111 12 hours agorootparentPilot suicide by a long shot. There is no other alternative theory that comes even close. What was interesting is to see them piece the theory together from the very fragmentary and little evidence (what is even more mind-blowing to know how little evidence there was to play with given we are talking about a 747). The true mystery left is how did he execute the suicide and why. How did he dispatch the co-pilot? (There was a very small window to do so). What did he do in the final hours (was he alive for the entire duration)? reply wiresurfer 11 hours agorootparentdid you chance upon @MentourPilot on youtube and his breakdown of the MH370 investigation + recent update? If not, I am afraid you may have another hour to spend, but its very likely you have already seen this. reply 698969 6 hours agorootparenthttps://www.youtube.com/watch?v=Y5K9HBiJpuk reply friggeri 12 hours agoprevTwo very very deep rabbit holes in the last 6 months: - Designed/built a small USB controlled pan/tilt camera head to control the mirrorless I use as a webcam (couple of servos, gears, belts), and then designed/built a custom ortholinear keyboard with a joystick to control the camera (custom PCB, CNC'd aluminum case, etc) - I'm a pretty big runner, built my own web based calendar UI that integrates with Google Calendar where I can type in workouts like \"1 mile warmup @z2 + 5x(30 seconds @ 6:00/mile + 0.5 miles recovery) + 1 mile cooldown\" and this gets parsed/total weekly mileage gets tallied. The next step down this rabbit hole is building a small iOS app to automatically generate Apple Watch Workouts using WorkoutKit. reply hurtuvac78 12 hours agoparentVery interested in your first rabbit hole. Which servos did you use? Which gears? For me, it would be to use with an action camera. How many hours did you spend on it before you were satisfied? I've seen some arduino-based projects to do that, but servos look quite bulky... and with the right gears, very little torque / power should be necessary. But i have not spent the time yet. reply whoopsie 8 hours agoprevBluetooth coffee mugs. I disliked Ember’s app, particularly on Mac, so I built a substitute for the menu bar. While expanding to knockoff mugs on Amazon, it was entertaining to see how bizarre and inefficient some BLE implementations were. Some write every second unnecessarily. Ember starts the heater during cooldown, even though they could do so closer to target temp and save 5-10% battery. reply irongeek 11 hours agoprevDue to all the recent BSD posts I have spent the past month exploring NetBSD and OpenBSD. Really enjoying the journey and finding that I can do everything I am currently doing easily on both. reply captainvalor 12 hours agoprevSuno and Udio. Spent WAY too much time adapting a Buddhist sutra into a heavy metal banger: https://www.youtube.com/watch?v=H-5Y9Z7DK4s reply tithe 10 hours agoparentYeah, digging this! reply thepuppet33r 9 hours agoprevWhy doesn't my Powershell terminal work in Visual Studio Code? What do you mean my launch.json file is missing? It was there yesterday? Wait, I can set up custom launch settings in my launch.json? What else? Ok, so I've got seven different launch settings in there, and now to see if I can have one used for markdown for my markdown word editor. Oh, neat, lots of extensions for markdown. Wait, you can install vim? An hour later, and I've completely re-broken my VSC and am reinstalling from scratch. reply rurban 2 hours agoprevJpeg-xl encoding. Its API is almost openssl-style confusing. I found no way yet how to encode a image buffer to a file. ChatGPT gave some nice code, but it wouldn't work at all, good ideas though reply chakerb 9 hours agoprevReverse engineering android apps. I wrote a bit about it in [0]. In the weekend I also started doing another one. It's interesting to see how these apps behave. [0] https://github.com/benhamad/blog/blob/main/2024-04-12-dramal... reply rapfaria 13 hours agoprev\"What if I were to gather these 5 five recipes that really worked in a future... book?.\" Bookbinding has fascinating details. reply geostupid 9 hours agoprevAfter the post the last week about the DIY GPS receiver, I decided to get out my RTL-SDR and set it up. Took a couple of days of fiddling around with it, but now I've got SDRtrucking setup. Spent one morning listening to the public safety radio traffic. That was a wild ride on it's own with all sorts of things going on in this metropolitan area. reply mianos 8 hours agoprevSo many great rabbit holes in this list. There are several that have blown years of my spare time. My one, at the moment is precision time keeping. Time nuts. I have a pile of oven controlled crystal oscillators, GPSs (even the full Ublox time specific LEA M8). I got a BG7TBL counter and multiple cheap GPSDOs to test my own. I have a DAC1220 20 bit DAC on an ESP32 disciplining a TCXO from an old phone base station by counting the 10Mhz using the PCNT and gated off the GPS 1PPS. Meantime, I learnt the esp-idf so I could have more control over things. Everything done with the esp-idf is way way more stable than using the Arduino wrapper, no idea why, maybe later versions? The disciplining/tracking parameters are exposed by http and mqtt and put into influx. I have a 5.5 digit multiple meter (I repaired the classic HP 3478A). Maybe I need another digit, there goes another rabbit hole. Voltnuts. reply stevekemp 2 hours agoprevA while back I wrote a game in assembly, for CP/M. Since I have a single-board Z80-based computer on which I can run it. I later ported the game to the ZX Spectrum, because that was a fun challenge, and I only needed a few basic I/O operations - \"write to screen\", \"read a line of input\", etc, etc. It occurred to me that I could reimplement the very few CP/M BIOS functions and combine those implementatiosn with a Z80 emulator to run it \"natively\". So I did that, then I wondered what it would take to run Zork and other games. Slowly I've been reimplementing the necessary CP/M BDOS functions so that I can run more and more applications. I'm not going to go crazy, anything with sectors/disks is out of scope, but adding the file-based I/O functions takes me pretty far. At the moment I've got an annoying bug where the Aztec C-compiler doesn't quite work under my emulator and I'm trying to track it down. The C-compiler produces an assembly file which is 100% identical to that produced on my real hardware, but for some reason the assembler output from compiling that file is broken - I suspect I've got something wrong with my file-based I/O, but I've not yet resolved the problem. TLDR; writing a CP/M emulator in golang, and getting more and more software running on it - https://github.com/skx/cpmulator reply zubairq 4 hours agoprevIn browser Time travelling debugging in Javascript.. trying to implement it for an open source hobby project reply Twirrim 12 hours agoprevA quick and dirty, shallow one, that I just opted to brute force out of curiosity. An online game I play includes an optional two player Russian Roulette type feature (non-fatal). I got to wondering if there was an optimal betting percentage to use, if you set aside some money as a betting seed. So I spent time coding up a really ugly brute force \"just run lots of games and see\". Pretty much the answer is you'll lose more often than you win, looks like your best bets are around 2% of whatever money you have left of your betting money. If you play 75 games, at 2% of your betting pool, you'll come out ahead only about 49.8% of the time. There's more efficient ways of working that out than I bothered to do, which was to create a basic abstraction for a gun. For example, your odds of winning is essentially 50%, given two players. For every \"game\" I simulated, I could have just picked a random integer between 0 and 1 instead. Faster and the same effect. As best as I could find, there are no good betting strategies on a coin toss (which is what this really is) reply bormaj 12 hours agoparentYou might find the Kelly Criterion interesting and/or useful for optimal bet sizing. This rabbit hole goes deeeep https://en.m.wikipedia.org/wiki/Kelly_criterion reply Twirrim 10 hours agorootparentOh boy, further down the rabbit hole! reply Twirrim 7 hours agorootparentOkay, unless I'm missing something, Kelly Criterion puts it at 0%, which is about what I'd expect. p = 0.5 q = 0.5 b = 1.0 0.5 - (0.5 / 1.0) = 0.0 reply FrojoS 12 hours agoparentprev> a really ugly brute force \"just run lots of games and see\". This is usually called https://en.wikipedia.org/wiki/Monte_Carlo_method reply wiresurfer 12 hours agoprevSo, I was looking at some performance issues which seemed to be stemming from linux networking stack. We eBPF with docker/k8s a fair bit as a PaaS, and I ended up getting into the weeds with linux sys/procfs and kernel tracing. One thing led to another and after quickly dispatching the perf issue, I was supposed to be on a planned week long holiday which turned into a deep dive into linux kernel + networking. Intermediate step - I feel pretty confident with the gory details of the kernel code now. can possibly build a custom kernel, boot qemu with both a simple C+assembly bare metal kernel, or the self compiled kernel. I feels like the clouds have cleared and I can see the sun. - Incidentally the kernel source code is pretty well documented, but one thing which is missing is a much smaller list of files which are most important. true pareto here. 20% files carry the weight. You also need to know the subsystem you want to touch. Chances are that subsystem is much lesser number of files. Finally - Got to reading about kernel packet handling. at the L2/L3/L7 level. from nic hardware to userspace. Turns out that eBPF [hello old friend!] has a networking avataar called XDP which is pretty recent [ Dream (ocaml) -> axum (rust) -> Django. I feel like Phoenix probably perfectly suits what I'd like to do with this app (long running tasks and collaborative editing) but I'm at the point where I want to support this app long term and I don't see me not being familiar with python anytime soon. reply mtsolitary 2 hours agoparentInteresting, what’s the scheduling problem? reply sneed_chucker 12 hours agoprevInstalling and exploring V7 unix (1979) on a PDP11 emulator. Crazy how familiar and yet different things are. reply Newtonip 8 hours agoparentAre you a reader of virtually fun? That's exactly the kind of topics they explore. reply sneed_chucker 8 hours agorootparentI wasn't, but I am now. Thanks for the tip! reply cameron4 12 hours agoprevI take photos for fun in my spare time and recently started shooting on film again. This time around I've been interested in how film actually functions. SmarterEveryDay has a fantastic 3-part YouTube series of the Kodak manufacturing process as he tours through the facility. I'm now amazed that film is still even being manufactured today, and can't even imagine what the production line was like during the heyday of film. I've developed my own film in the past but knowing so little about chemistry myself, it's still pretty much magic to me even after digesting all of the info from the series. reply all2 9 hours agoparentThis is only partially relevant, but there is some analog film theory buried about halfway through. https://www.youtube.com/watch?v=YE9rEQAGpLw reply keyle 10 hours agoparentprevI wish I could do this, but the financial aspect just isn't working where I am. I've been looking at digital backs for old cameras such as the blad I have taking dust. Sadly, either they're completely impractical or they're way above my budget. Hopeful that some day, something better than a polaroid back can be used to resurrect my old hassleblads. reply yesguidance 10 hours agoprevI've been interested in making a homebrew playstation 3 (childhood console :D) game for a while, and finally got an environment setup yesterday. Ended up getting a simple rust function built, with some slight miscomplilations via wasm + wasm2c. Now I'm going to try to get graphics working. There is a surprising amount of public code containing calls to sony's licensed SDK, if you know what to search for (Not to mention the SDK, which was, \"obtained\" dubiously). Fascinating stuff reply jamesy0ung 8 hours agoparentThis is very cool, do you have a place where you post updates? I was playing with w2c2 last week to try and get some stuff running on Mac OS X Leopard, but I couldn't quite get it working :(. reply yesguidance 3 hours agorootparentNot yet, there is a monumental amount to learn before I can make anything cool :( If I get something cool working, maybe i'll start a blog reply pizzaminded1 10 hours agoprevI have bought recently a bunch of ICs from 8080/8086 computers (mostly peripherals like PIC, I/O, Timers, memory) for next to nothing and i am trying to build a \"retro\" synth. It is way more fun and challenging than arduino or esp32, and not really complicated! A lot of instruments of the (Roland Juno, Oberheim Matrix 6, Akai AX-80) are using these chips too, so when in doubt i can take a peek at the schematics and move on with construction. reply atonse 11 hours agoprevThe wonderful world of research paper identifiers. DOI (Digital Object Identifiers) are used by many modern research papers as sort of a UUID for papers, run by doi.org. But they're discipline-specific. So they're used widely by certain disciplines. But others use different databases. So for biology-related papers, NIH's PubMed ID. Or for Astronomers, Bibcode. All are \"global\" identifiers and each has some kind of consortium that's trying to make theirs the One ID. DOI seems to be the closest. reply IanCal 10 hours agoparentDoi is the main one. Most published papers and many other things have them. They're newer so often older things hang around too, I'd wager most new pubmed articles have dois too. There are several registries, crossref is the big one in the west but it's not the only one. They have probably the best access to the data out of all of the larger registries though. Dois are pretty good, though not persistent and there's no versioning built in so people have their own formats. I spent a lot of time working with these as part of https://dimensions.ai for a decade or so. Happy to chat if you want to delve in more. reply atonse 9 hours agorootparentCool! I know you guys work with OrcID too right? We work with CrossRef to get data but if a DOI is missing, then things get harder to find in CrossRef in our experience. reply kromem 12 hours agoprevI started looking into the Gospel of Thomas right before the start of the pandemic, which led to a number of rabbit holes: * Turns out the work is not 'weird' or 'Gnostic' but is directly addressing details from Lucretius, including paraphrasing his view of evolution and atomism, but refuting the claim there's no afterlife by basically appealing to the idea we're in a simulated copy of an original physical world where the spirit doesn't actually depend on a body, because there is no actual body. * As I dug more into the various mystery religions the followers of the work claimed as informing their views, I saw a number of those were associated with figures various Greek historians were saying came from the same Exodus from Egypt as Moses. * Turns out a lot of the ahistorical details in the Biblical Exodus narrative better fit the joint sea peoples and Libyan resistance who end up forcibly resettled into the Southern Levant latter on. In the past decade we've also started finding early Iron Age evidence of Aegean and Anatolian settlement and trade previously unknown in the area, including in supposed Israelite settlements like Tel Dan, lending support to the theory that Dan were the Denyen sea peoples. * Also turns out that in just the past few years a number of Ashkenazi users have been puzzled by their genomic similarity to ancient DNA samples, where the closest overall match in a DNA bank was 3,500 year old Minoan graves sequenced in 2017 or that they have such a high amount of Neolithic Anatolian (which the 2017 study found was effectively identical to Minoan). * The G2019S LRRK2 mutation that's almost only found among the Libyan Berbers and the Ashkenazi appears to have originated with the former but appeared in the ancestry of the latter ~4,500 plus/minus 1k years. Which is a window that predates the emergence of the Israelites in the first place, but is on the cusp of the sea peoples/Libyan alliance. * There's also been discovery of endogamy among some of the Minoan populations. Did the Ashkenazi endogamy evidenced from their emergence in Europe and the bottleneck in the first millennium CE actually go back much further than we've been thinking? Maybe Tacitus wasn't so off base when he talked about how some claimed the Exodus involved people from Crete hiding out in Libya. Anyways, that's a very rough summary of some of the rabbit holes I was going down. Bonus: Herodotus's description of Helen of Troy spending the whole time in Egypt has two datable markers to the 18th dynasty, which is when Nefertiti, \"beautiful woman who arrived\" is around during a complete change to Egyptian art and religion while she's the only woman in history to be depicted in the smiting pose, with her only noted relatives being a sister and wetnurse. reply therealcamino 9 hours agoparentIs that the one where Jesus sells Thomas into slavery because he didn't go obediently to minister to the people in India? reply kromem 8 hours agorootparentNo, that's the mythology that develops around the travels of an apostle Thomas much later on. I'm pretty sure that there was no 'Thomas.' My guess is that the philosophy of being in a twin universe and a twin of an original humanity ends up anthropomorphized by or before \"doubting Thomas\" in John and ends up credited with the tradition making those philosophical claims which was also denying the physical resurrection. In the Gospel of Thomas itself, there's only two mentions of a 'Thomas,' both likely later additions. Moreso the work features him having female disciples and discussions directly with them, and the only later tradition following it claimed a female teacher named Mary as the starting point of their sect. The Gospel of Thomas is a collection of sayings, and that core may have gone by different names before the 2nd century when it's rolled up in a more secretive context as attributed to 'Thomas' (despite the core itself seemingly being more anti-secretive than any other texts in the early Christian tradition). reply greentxt 12 hours agoparentprev> appealing to the idea we're in a simulated copy of an original physical world where the spirit doesn't actually depend on a body Sounds a bit gnostic no? reply kromem 11 hours agorootparentIt was what resulted in Gnosticism, not the other way around. You had this first century response to Epicureanism's naturalism as a foundation. In that paradigm, the Platonist demiurge recreating the physical world before it was an agent of salvation, liberating the copies from the certainty of death from the Epicurean original. What happens is that Epicureanism falls from popularity over the second century, so in parallel to the increased resurgence of Platonism, Plato's forms becomes the foundation instead. For Plato, there was a perfect world of the blueprints of everything, the corrupted physical versions of those forms, and then the worst of all was the images of the physical. So the Thomasine salvation by being in the images of physical originals is through that lens corruptive. So as the foundation shifted from the Epicurean original world of evolution (Lucretius straight up described survival of the fittest in book 5) to Plato's perfect forms, a demiurge creating a copy of what predated it shifted from being a good thing to trapping people in a corrupted copy. For the first 50 years of the discovery of the Gospel of Thomas, it was mistakenly thought to be Gnostic. This changed at the turn of the 21st century with the efforts of Michael Allen Williams and Karen King, and it's now labeled as \"proto-Gnostic.\" It's absent a lot of the features typically associated with 'Gnosticism' though that term in general should be retired as it's turned out that there isn't any single set of beliefs to be considered 'Gnostic' in the first place (this was the chief realization of scholars over the past twenty years). reply smnplk 10 hours agoprevI installed NixOS and went down into that black hole. reply kylemart 10 hours agoparentTell me more. What was your use-case for installing NixOS? I’ve been thinking about putting together a new media server and saw a recommendation to try NixOS. (I’ve previously used Unraid). That was a couple weeks ago, and it was the first time I’d heard about it reply smnplk 8 hours agorootparentMy use-case for NixOS : Got tired of breaking Arch and never ever wanted to solve package dependencies issues again, so I installed it on my workstation laptop. I havent tried it yet for home server stuff. I am still running containers on a Proxmox host. Nix documentation is bad/incomplete, so I helped myself with some Youtube videos to get started. This might get you started https://jvns.ca/blog/2024/01/01/some-notes-on-nixos/ reply Animats 5 hours agoprevThe Rust 3D rendering stacks. I just want to use them, not work on them. But there are low-level problems and I had to open the lid on Rend3->WGPU->Vulkan. reply rav3ndust 12 hours agoprevOver the last little while, I've fallen down the Nix rabbit hole. I don't currently use it in any serious projects aside from tinkering about with it, but it has been a lot of fun to learn and study. Between the Nix package manager, the associated language, etc., there has been a lot to learn about, and it's been good fun. I have nixOS on my spare Thinkpad for toying with, and I have Nix on my main Debian systems, if I want to pull something from nixpkgs. reply jensenbox 10 hours agoprevI was Youtubing this last weekend and ran down quite the rabbit hole. This all seems like mumbojumbo to me. I could not derive a single piece of solid science in any of it. It was remarkable how much content there was on this subject with little to no actual information - enjoy: https://www.youtube.com/@MFMP reply keyle 12 hours agoprevWriting a low level C code editor, going down the rabbit hole of gap buffers and piece tables. Fun though. I first wrote it the dumbest way possible, one big array with padding at the back. Worked fine actually for most modern use cases, but as this is also a learning experience, I want it to be best in class in performance. I think I'll settle on gap buffer because the performance is great and it doesn't hurt my head. reply Havoc 10 hours agoprevI’ve been trying to fit something into GCP free tier while also designing it to be scalable. Been tinkering with that for an absurdly long time vs just throwing some python onto a $5 dollar VPS but it’s been fun & learned a decent bit about over engineering pitfalls along the way reply SimianLogic 12 hours agoprevI couldn't find any containers for paper filters for the AeroPress XL that didn't look like cheap 3d-printed garbage, so I've been going down a rabbit hole of how to build bronze articulated joints. I'm building a paperweight inspired by vintage brass table lamps to hold the papers in place on a wooden platform. reply busterarm 12 hours agoprevBeen nerdsnipped and diving down the rabbit hole on a few topics in the past few months: Some history podcasts had me digging into the Napoleonic Wars and Israel/Palestine. Also a recent interest in human health and diseases has basically sent me down the path of self-study equivalent to a Kinesiology/Exercise Science/Sports Physiology degree. reply 2rsf 1 hour agoparent> Israel/Palestine This is more than a rabbit hole, it's a fractal that changes as you zoom in and out reply teabee89 10 hours agoprevCollatz conjecture reply 8xeh 9 hours agoparentI've been looking at that particular rabbit hole since a professor of mine mentioned it in 2003 or something. Once or twice a year, I'll read about some theorem or something and think it can be applied to Collatz somehow and dive back in. I've actually proved it several times...except for the insignificant detail that I glossed over that didn't seem important but tanks the proof. Someday I'll have to publish my \"book of lemmas that don't prove the collatz conjecture.\" reply pleb_nz 9 hours agoprevEverything. Everything I look at or touch I soon realise it’s way bigger and deeper than I thought. You could choose nearly any topic and turn the study of it into a phd if you really want. reply glokta 10 hours agoprevI've been fascinated with the VESC project (https://vesc-project.com). I had no idea that controlling a BLDC could be so complicated. reply frakt0x90 12 hours agoprevI watched Lex Fridman interview Richard Wolff and have spent 2 weeks going hard into marxist and anarchist theories and practice. Working through 2 books, a dozen browser tabs, interviews, etc. It's rare something catches my interest like this (especially non-technical). But I'm really enjoying all the different perspectives and formulating my own fantasy scenarios. reply cameron4 12 hours agoparentIf it hasn't made it onto your list yet, Ursula K. Le Guin's book titled \"The Dispossessed\" is a great exploration of anarchism in practice through a sci-fi lens reply hangonhn 12 hours agoparentprevDo you have any recommendations on books that talks about Leninism like it's foundational ideas, etc. and how it departs from Marxism? Thanks in advance. reply dvh 13 hours agoprevI'm trying to beat level 19 in terminator dash: https://www.atarimania.com/pgesoft.awp?version=37332 reply imzadi 13 hours agoprevLocal brewery is doing a cinco de mayo event and we started talking about pre-gaming with margaritas: Margaritas -> Jello Shots -> Chimoy/Tajin rim/topper -> Pop Rocks -> History of Pop Rocks reply 0xWTF 12 hours agoprevStarted on the Nagoya Protocol, then PCR of wastewater on airplanes, mechanical engineering of lavatory fittings, then metagenomic shotgun sequencing, and now Bloom filters. reply kevindamm 11 hours agoparentIt is such a good feeling to deploy a bloom filter in production. There aren't many times it will help but when it helps it helps a TON. reply Alexito 12 hours agoprevWhat is the difference between sony camera models. reply wiresurfer 11 hours agoparentNow that's one tricky rabbit hole. If you are planning on buying a camera, the deliberation seems to be a part of the shopping process. Post buying though, the differences seem either very important, or don't matter at all. Depends on how serious you want to get with the cameras. reply Dowwie 9 hours agoprevSound event recognition is so interesting and seems to be lagging the other ML domains reply kokizzu5 1 hour agoprevvtuber? reply jrjarrett 8 hours agoprevTracing the lineage of the British royal family back to Elizabeth I. reply api 9 hours agoprevUnrelated to work: the black hole rabbit hole. It started with an article about the hypothesis that planet nine may be a primordial black hole with 3-6 Earth masses. What’s a primordial black hole? It’s one that formed in the first seconds after the Big Bang. We don’t know for sure they exist but many theories and simulations predict them. They’re an excellent dark matter candidate. Could it be that simple? Could at least a lot of the missing mass be tied up in little baseball sized embers from the birth of the universe that rarely interact with anything so we don’t see them? They’d be small, would rarely interact, and unless they are sucking in mass (causing a hot accretion disk) would be dark. Then I got onto Hawking radiation and whether micro black holes could exist. Along the way I read about loop quantum gravity (LQG) which looks to me like a decent stab at unifying QM and GR that’s much less baroque and more testable than string theory. That then led to the LQG “bounce” hypothesis for black holes. See LQG does away with true infinite mass singularities. Instead a black hole would be matter packed to its theoretical maximum density (which is still insane). From there it would quickly “bounce” and become a white hole. So wait… how do black holes persist then? Time dilation! From a the hole’s frame of reference it collapses and then instantly bounces and goes kaboom. From our frame of reference though all that gravity slows it to such a crawl that the black hole phase at or near max density looks like it’s stable. The bounce takes billions to even trillions of years! Last but not least I learned about the black hole starship idea. It’s a set of ideas about how far future intelligences could use black holes as mass energy converters to reach relativistic velocities. Might be somewhat easier (for crazy sci-fi values of “easy”) than handling antimatter. This also gives SETI yet another wild extreme technosignature to look for. … and back to the beginning I found a post about how if planet nine were a PBH we could use it to yeet probes to the stars at meaningful fractions of c… at least if we could make them able to survive insane g forces. Unlike the black hole starship this would be feasible today. It’d just be a gravity assist off a ludicrous gravity well. Here I thought black holes were dull. Turns out they’re the most extreme objects in the universe and a whole lot of the most amazing physics intersects around them. If there is any way we could tap into the phenomenon we could potentially access sci-fi levels of energy too. reply bilsbie 9 hours agoparentNeat! Is the bounce idea incompatible with hawking radiation? reply api 8 hours agorootparentNo. Unless I didn’t get it Hawking radiation is part of what this looks like to an outside observer. But I would lean on a physicist more familiar with the loop quantum gravity model to explain how these relate. The bounce idea is super neat because it feels less “magical” than a true singularity. A black hole is just a whole lot of mass stuck in a time dilation tar pit… from our frame of reference. There are other implications too. From what I read LQG may allow stable micro black holes due to quantum effects dominating at small mass, naked singularities (well not true singularities but regions of off the charts mass energy concentration not hidden behind an event horizon), and Hawking radiation subject to quantum spectral effects similar to how emission spectra work. It also resolves the black hole information paradox. All the information just bounces back out. Easy. reply pcdoodle 10 hours agoprevLots of really cool answers here. Electric Bikes: Hub Motors and Mid drives are a really great spring / summer rabbit hole to go down. So many form factors of ride and you can also kill two birds with one stone by going for 60V lawn care equipment (There's adapters on eBay to connect them to your bike). reply andrewinardeer 11 hours agoprevThe Night The Stars Fell reply m1n1 8 hours agoprevSaw all the CSPAN videos for the hearings on the January 6 2021 attack on the US Capitol, listened to Trump's call with Brad Raffensperger, and The Trump Tapes with Bob Woodward, and Liz Cheney's Oath and Honor. reply greentxt 12 hours agoprevSan Francisco politics in the late 19th century. reply geostupid 9 hours agoparentDid the phrase 'knife fight in a phone booth' still apply to SF politics back then? reply anon115 12 hours agoprev----esp32 module ----gaussian splatting ------ reply drivers99 12 hours agoprevDifferent table top roll playing game systems reply interestica 9 hours agoprevToday in Toronto, Canada, a man that was charged with First Degree murder of a police officer was acquitted. I'd only really absorbed the initial news story in passing. The initial news releases relayed the police account of the incident (a man running over a police officer with his car) that it was a \"deliberate and intentional act\". During the trial, and with today's acquittal and subsequent lifting of a publication ban, we learned how different the public perception of the initial events were compared to what actually transpired. Thus, it's interesting to use technological tools (search, or wayback machine) to see how the story was originally framed and how subtle differences can affect perception. My hope is that the event at least makes people question their own assumptions borne of limited or misleading information in the earliest publications of any news event involving the death of a police officer. Even today, I'm seeing conflicting reporting of quotes. For example the CBC has one article that quotes the police chief: > Outside court, however, Toronto Police Chief Myron Demkiw struck a different tone. \"While we respect the judicial process and appreciate the work of everyone involved in this difficult case, we were hoping for a different outcome,\" he said. https://www.cbc.ca/news/canada/toronto/umar-zameer-acquittal.... However, the video of the Chief's statement is slightly different (and quoted correctly in another article): > \"While we respect the judicial process and appreciate the work of the 12 citizens who sat on a very difficult case, I share the feelings of our members who were hoping for a different outcome,\" Demkiw said. https://www.cbc.ca/news/canada/toronto/umar-zameer-verdict-1... The first quote cuts out essential information and does not note that the quote was not verbatim (as presented). It's especially problematic as the quotes might have subtle but different interpretations now that his suggestion of \"hoping for a different outcome\" is being addressed. (There's a subtle difference between 'sharing feelings with someone that hopes for a certain outcome' and 'hoping for a different outcome' -- and this could be major if it's seen as the police services suggesting the wanted the man, who was declared innocent, to have been convicted.) (One other possibility is that the Chief did in fact mention both lines, and that they are so similar because it was based on his official public statement). reply doctor_eval 12 hours agoprevI spent the last week evaluating drag and drop form designers and came to the reluctant conclusion that I’d be better off building my own. Long story. So now I’m building a drag and drop form designer, quite the deviation from the road I need to travel down. Its almost certainly a rabbit hole but at least I’ve forewarned myself. reply jimhefferon 10 hours agoparentI'd be interested in code if you produce something you like. reply doctor_eval 6 hours agorootparentLet’s see how I go. There are quite a few out there - formio and formily came closest. I would probably have chosen formily, but the main docs were all in Chinese and I couldn’t get it to work. I also looked at vueforms and surveyjs, the builders are not free though. reply 11 more comments... GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "Getting caught up in new technologies, you may find yourself easily sidetracked from fixing a bug, falling into a never-ending cycle of distractions and interests.",
      "The process involves researching, debugging, and delving into various topics, primarily focusing on what piques your interest the most.",
      "This phenomenon is commonly known as being \"nerd-sniped,\" where one gets hooked on different subjects, delaying the initial task at hand."
    ],
    "commentSummary": [
      "The discussion entails diverse topics like technology, coding tools, Linux setups, PDF compression formats, CAD expertise, DIY audio ventures, Raspberry Pi clusters, keyboard configurations, firmware modifications, and numerous technical endeavors.",
      "Participants also talk about hobbies such as simulators, ham radio, music plugins, and converting Buddhist sutras into heavy metal tracks.",
      "Emphasis is placed on in-depth exploration of various subjects, fostering learning and curiosity across different fields of interest."
    ],
    "points": 153,
    "commentCount": 261,
    "retryCount": 0,
    "time": 1713816652
  },
  {
    "id": 40122999,
    "title": "Exploring Email Thread Themes: Work and Personal Matters",
    "originLink": "https://www.franken-ui.dev/",
    "originBody": "🎉 Early Preview Early Preview Franken UI HTML-first, framework-agnostic, beautifully designed components that you can truly copy and paste into your site. Accessible. Customizable. Open Source. Get Started Github Mail Dashboard Cards Tasks Playground Forms Music Authentication Alicia Koch alicia@example.com alicia@yahoo.com alicia@cloud.com Inbox 128 Draft 9 Sent Junk 23 Trash Archive Social 972 Updates 342 Forums 128 Shopping 8 Promotions 21 Inbox All mail Unread William Smith 6 months ago Meeting Tomorrow Hi, let's have a meeting tomorrow to discuss the project. I've been reviewing the project details and have some ideas I'd like to share. It's crucial that we align on our next steps to ensure the project's success. Please come prepared with any questions or insights you may have. Looking forward to our meeting! Best regards, William meeting work important Alice Smith 6 months ago Re: Project Update Thank you for the project update. It looks great! I've gone through the report, and the progress is impressive. The team has done a fantastic job, and I appreciate the hard work everyone has put in. I have a few minor suggestions that I'll include in the attached document. Let's discuss these during our next meeting. Keep up the excellent work! Best regards, Alice work important Bob Johnson 6 months ago Weekend Plans Any plans for the weekend? I was thinking of going hiking in the nearby mountains. It's been a while since we had some outdoor fun. If you're interested, let me know, and we can plan the details. It'll be a great way to unwind and enjoy nature. Looking forward to your response! Best, Bob personal Emily Davis 6 months ago Re: Question about Budget I have a question about the budget for the upcoming project. It seems like there's a discrepancy in the allocation of resources. I've reviewed the budget report and identified a few areas where we might be able to optimize our spending without compromising the project's quality. I've attached a detailed analysis for your reference. Let's discuss this further in our next meeting. Thanks, Emily work budget Michael Wilson 6 months ago Important Announcement I have an important announcement to make during our team meeting. It pertains to a strategic shift in our approach to the upcoming product launch. We've received valuable feedback from our beta testers, and I believe it's time to make some adjustments to better meet our customers' needs. This change is crucial to our success, and I look forward to discussing it with the team. Please be prepared to share your insights during the meeting. Regards, Michael meeting work important Sarah Brown 6 months ago Re: Feedback on Proposal Thank you for your feedback on the proposal. It looks great! I'm pleased to hear that you found it promising. The team worked diligently to address all the key points you raised, and I believe we now have a strong foundation for the project. I've attached the revised proposal for your review. Please let me know if you have any further comments or suggestions. Looking forward to your response. Best regards, Sarah work David Lee 6 months ago New Project Idea I have an exciting new project idea to discuss with you. It involves expanding our services to target a niche market that has shown considerable growth in recent months. I've prepared a detailed proposal outlining the potential benefits and the strategy for execution. This project has the potential to significantly impact our business positively. Let's set up a meeting to dive into the details and determine if it aligns with our current goals. Best regards, David meeting work important Olivia Wilson 6 months ago Vacation Plans Let's plan our vacation for next month. What do you think? I've been thinking of visiting a tropical paradise, and I've put together some destination options. I believe it's time for us to unwind and recharge. Please take a look at the options and let me know your preferences. We can start making arrangements to ensure a smooth and enjoyable trip. Excited to hear your thoughts! Olivia personal James Martin 6 months ago Re: Conference Registration I've completed the registration for the conference next month. The event promises to be a great networking opportunity, and I'm looking forward to attending the various sessions and connecting with industry experts. I've also attached the conference schedule for your reference. If there are any specific topics or sessions you'd like me to explore, please let me know. It's an exciting event, and I'll make the most of it. Best regards, James work conference Sophia White 6 months ago Team Dinner Let's have a team dinner next week to celebrate our success. We've achieved some significant milestones, and it's time to acknowledge our hard work and dedication. I've made reservations at a lovely restaurant, and I'm sure it'll be an enjoyable evening. Please confirm your availability and any dietary preferences. Looking forward to a fun and memorable dinner with the team! Best, Sophia meeting work Daniel Johnson 6 months ago Feedback Request I'd like your feedback on the latest project deliverables. We've made significant progress, and I value your input to ensure we're on the right track. I've attached the deliverables for your review, and I'm particularly interested in any areas where you think we can further enhance the quality or efficiency. Your feedback is invaluable, and I appreciate your time and expertise. Let's work together to make this project a success. Regards, Daniel work Ava Taylor 6 months ago Re: Meeting Agenda Here's the agenda for our meeting next week. I've included all the topics we need to cover, as well as time allocations for each. If you have any additional items to discuss or any specific points to address, please let me know, and we can integrate them into the agenda. It's essential that our meeting is productive and addresses all relevant matters. Looking forward to our meeting! Ava meeting work William Anderson 6 months ago Product Launch Update The product launch is on track. I'll provide an update during our call. We've made substantial progress in the development and marketing of our new product. I'm excited to share the latest updates with you during our upcoming call. It's crucial that we coordinate our efforts to ensure a successful launch. Please come prepared with any questions or insights you may have. Let's make this product launch a resounding success! Best regards, William meeting work important Mia Harris 6 months ago Re: Travel Itinerary I've received the travel itinerary. It looks great! Thank you for your prompt assistance in arranging the details. I've reviewed the schedule and the accommodations, and everything seems to be in order. I'm looking forward to the trip, and I'm confident it'll be a smooth and enjoyable experience. If there are any specific activities or attractions you recommend at our destination, please feel free to share your suggestions. Excited for the trip! Mia personal travel Ethan Clark 6 months ago Team Building Event Let's plan a team-building event for our department. Team cohesion and morale are vital to our success, and I believe a well-organized team-building event can be incredibly beneficial. I've done some research and have a few ideas for fun and engaging activities. Please let me know your thoughts and availability. We want this event to be both enjoyable and productive. Together, we'll strengthen our team and boost our performance. Regards, Ethan meeting work Chloe Hall 6 months ago Re: Budget Approval The budget has been approved. We can proceed with the project. I'm delighted to inform you that our budget proposal has received the green light from the finance department. This is a significant milestone, and it means we can move forward with the project as planned. I've attached the finalized budget for your reference. Let's ensure that we stay on track and deliver the project on time and within budget. It's an exciting time for us! Chloe work budget Samuel Turner 6 months ago Weekend Hike Who's up for a weekend hike in the mountains? I've been craving some outdoor adventure, and a hike in the mountains sounds like the perfect escape. If you're up for the challenge, we can explore some scenic trails and enjoy the beauty of nature. I've done some research and have a few routes in mind. Let me know if you're interested, and we can plan the details. It's sure to be a memorable experience! Samuel personal Mark as unread Star thread Add label Mute thread WS William Smith Meeting Tomorrow Reply-To: williamsmith@example.com Oct 22, 2023, 9:00:00 AM Hi, let's have a meeting tomorrow to discuss the project. I've been reviewing the project details and have some ideas I'd like to share. It's crucial that we align on our next steps to ensure the project's success. Please come prepared with any questions or insights you may have. Looking forward to our meeting! Best regards, William Mute this thread Send",
    "commentLink": "https://news.ycombinator.com/item?id=40122999",
    "commentBody": "HTML-first, framework-agnostic implementation of shadcn/UI – franken/UI (franken-ui.dev)149 points by sveltecult 11 hours agohidepastfavorite46 comments lenkite 2 hours agoWhy is this advertised as HTML-first and framework-agnostic ? It directly leverages tailwind and UIKit libraries. That is NOT framework agnostic by any means. reply codetrotter 5 minutes agoparentWhen they say framework-agnostic they are talking about not being dependent on using React. > Who is Franken UI for? > Franken UI is tailored for small teams and solo developers seeking a beautiful, reliable CSS framework without the complexity of React, Vue, or Svelte. It's designed with an \"HTML-first\" approach, making it accessible to developers at any skill level. https://www.franken-ui.dev/docs/introduction As opposed to the shadcn ui > Which frameworks are supported? > You can use any framework that supports React. > Next.js, Astro, Remix, Gatsby etc. https://ui.shadcn.com/docs reply mubu 2 hours agoprevIt doesn't seem HTML-first though. Also the Github description says it's a library of web components, this may be misleading because this project doesn't use Web Components. reply bibstha 3 hours agoprevThis is exactly what I was looking for. I love Stripe's UI and I was looking for a library of unified components that I can copy paste with my Rails app. Then I saw shadcn and was disappointed that it was only for React. I'm mostly a backend developer and TailwindCSS really enabled me to be bold with working with frontend CSS. So far I've been copy pasting examples from Tailwind or Flowbite. But \"franken\" really looks much closer to what I was looking for, a cleaner opinionated unified interfaces that I can put together, works with Tailwind and can be used with Rails. Thanks for the work on this. reply camillovisini 3 hours agoparentMaybe this is also interesting for you: https://github.com/aviflombaum/shadcn-rails reply afavour 9 hours agoprev> framework-agnostic > Franken UI is a Tailwind CSS plugin :| reply jillesvangurp 3 hours agoparentI guess they mean Javascript framework agnostic. We use kotlin-js with Tailwind for our frontend. Actually works really nicely. So, this might be interesting for us. reply tipiirai 5 hours agoparentprevIndeed. I clicked to see an HTML-first UI library but entered a utility-first Tailwind library. reply bschmidt1 5 hours agoparentprevHad the same thoughts, I like the tailwind className approach enough but you'd think agnostic means it would work with sites that use regular CSS. reply oddevan 9 hours agoparentprevAs someone using Svelte for a project, I'm always happy to see something less React-specific. But yeah, I'm kinda over Tailwind. You whipper-snappers think CSS sucks so much; back in my day* we had font tags and tables! *high school reply afavour 7 hours agorootparentYeah I know it’s crotchety old man territory but I find CSS absolutely fine to work with these days, now that we have variables etc. Every time I’ve added PostCSS, Tailwind or whatever I’ve found my build times jump a ton and I don’t get a whole lot of use out of it. Plain CSS and containerisation provided by Svelte (or the forthcoming scope stuff) is more than enough for me. reply prisenco 7 hours agorootparentCSS is a dream these days. I understand why it’s intimidating for devs because it’s not meant for engineers it’s meant for designer so it has a wildly different set of assumptions and expectations. But learning it is 100% worth it. Tailwind is like an ORM. I get the appeal but if you know SQL /CSS it’s just going to get in the way. reply bschmidt1 5 hours agorootparentI like the SQL/CSS analogy, with the full-stack spectrum being like: SQL - Backend - Frontend - CSS Most people just want to stay in the middle doing backend/frontend (\"full-stack\") work because it's just writing functions. Or because it's building the core of a feature, where things like database work and styling is pushed off or abstracted in libraries. But on each end is where all the interesting stuff is really happening, and a full-stack dev who embraces both SQL and CSS is a lot more useful than a full-stack dev who stays in the middle. reply nullandvoid 2 hours agorootparentprevFor me it's not intimidating, it's just time consuming. I can create / iterate twice as fast when I'm styling inline with my HTML. It could be with tailwind, or other utility based libraries - tailwind has just done it the best from my experience. reply balls187 5 hours agorootparentprevIn addition to intimidating—there is the rote repetitiveness: how many times do you want to style the same components? Or adding the same custom components. Sometimes I just wanna work on my app and not write css. reply ffsm8 3 hours agorootparentprevThis is ridiculous. Tailwind css is a utility \"framework\", which generates configured css classes to set 1-2 css attributes each. The only reason it adds a build step is that it wants to tree-shake all unused classes to keep your bundle size as small as possible and let you configure which classes are available for generation. Even the documentation makes it extremely clear which properties are set by which class (the default ones anyway). As an actual example: how completely braindead do you have to be to not know which properties are set from looking at the official documentation? https://tailwindcss.com/docs/overscroll-behavior Your comparison to an ORM would work for things like Bulma, bootstrap and pretty much all component libraries, including TailwindUI. It's completely nonsensical for tailwindcss You've either never actually looked into tailwind css and are purely talking out of your ass or are just repeating this opinion from someone that did such. Disliking tailwind css is fine, and there are perfectly fine reasons to not use it. But your argument is just plain dumb. reply naasking 9 hours agoprevI was just looking at shadcn and thinking it's a shame it was react specific. Looking forward to seeing if this is viable to use in a server rendered site with htmx! reply TheRealPomax 9 hours agoprevThis is definitely not HTML first, though. This is TS first, JS second, CSS third, and only then, HTML last. I'm all for clean frameworks, but this is very far removed from the idea of putting HTML first. reply tbeseda 8 hours agoparentI'd really like a set of elements like this that is truly HTML + CSS. Vanilla. Layer on some _optional_ JS for some enhancements. But this frankenelement requires JavaScript to even render. Same for the tooltip, leader, cover, totop, ++ components; all of which can be done with HTML and CSS. Fine, write your HTML is JS, but that isn't \"HTML first\". edit: I stand corrected. the form will render without JS. The tab element in the docs wrapping the form doesn't render. reply Frotag 7 hours agorootparentDaisyUI offers zero-JS components https://daisyui.com/ I used it for a small form + search result list recently and it works well enough for simple / static stuff. But I think I'll still be reaching for a JS lib first since I'd miss things like inputs-with-autocomplete too much. reply eddd-ddde 3 hours agorootparentDaisyUI is a blessing when working on qwik projects, since there is an emphasis on eliminating hydration. It would seem that all other libraries want js for something as simple as a drop-down. reply sveltecult 7 hours agorootparentprevThe form does not need JS. There are two labels black and red \"Requires JS\". The black one should've been \"May Require JS\". It is use for the \"custom controls\" provided by UIkit. But it is not required. Cover, leader, dropdowns and even modals, you can always use the CSS class instead of UIkit custom attributes and toggle it manually via server. reply tbeseda 7 hours agorootparentAh so the tab component doesn't render without JS. Because I can't see the form render in the docs. Which is in a tab. Tabs can also be plain HTML + CSS. Even easier now with :has reply rzmmm 3 hours agoparentprevThe extreme version of HTML first: classless CSS libraries. IMO every web developer should be aware that these are sometimes a great option https://github.com/dbohdan/classless-css reply blooalien 3 hours agorootparentI'ma huge fan of these. They often have a built in CSS \"reset\" and then nice base styling for all the HTML elements most folks typically use in a website. Overlay a tiny little bit of custom CSS to \"make it your own\", and you're off to the races! reply sveltecult 7 hours agoparentprevLet me disagree. This is HTML-first because you can always grab the compiled CSS and optionally, compiled JS and reference it in your plain HTML file viaand it will work just fine. The TypeScript and Tailwind insanities are just there for easier development and my convenience. ;) Maybe soon, someone will publish the compiled CSS. IDK reply lelanthran 2 hours agorootparent> This is HTML-first because you can always grab the compiled CSS and optionally, compiled JS and reference it in your plain HTML file viaand it will work just fine. But this is true for almost every web thing, and every web framework - you can just grab the final outputs and stick it into your web page. So either your thing is \"HTML-first\", but so is everything else, or your thing is not \"HTML-first\", and some other things are. > Maybe soon, someone will publish the compiled CSS. So, until then, not only is this not HTML-first, it's not HTML at all. To me (and maybe others), there's a difference between web development and Node/npm. They are not one and the same. reply hanniabu 8 hours agoprevFoolish of me to think \"HTML-first\" meant it wouldn't require nodejs/npm and that dependency hell. reply 708733454927516 6 hours agoparentThanks for that. Good to know I wasn't the only one... reply lelanthran 2 hours agorootparent> Thanks for that. Good to know I wasn't the only one... I'm also glad I'm not the only one. I may, however, be the only one to be annoyed at what looks like a rug-pull: \"Here, come see this thing I made that doesn't require $FOO!\" > Requirements: $FOO reply andrethegiant 6 hours agoprevHow about some color? Or in other words, how much longer until the Vercel-inspired brutalism fad wears off? reply spiderfarmer 4 hours agoparentThe idea is that this is neutral enough to gain an impression and that you theme it yourself. That has been the idea for each UI library ever since bootstrap. Doesn't mean that people take the time to actually customize it. reply begueradj 5 hours agoprevIt looks similar to Tailwind UI and other free and paid alternatives which components you can also reuse and modify at will. But it's always good to have other options like this one. reply kevincox 10 hours agoprevThe demo at the very least appears to assume a lot about font sizes or shapes. It is very broken on my setup with everything escaping the bounds that it is presumably supposed to live in: https://pasteboard.co/6Z1nLEWhPHRg.png However I do like the dark theme. It manages to separate different areas without too much noise which can be hard to do without shadows. reply sveltecult 9 hours agoparentHello, thank you for feedback. Mind sharing what device are you using? That will definitely help me I'd be happy to fix it. Right now, I'm cheating a little and it only supports tablet (landscape) and up. Below that will just hide everything and show a static PNG. reply kevincox 9 hours agorootparentFirefox on desktop Linux 2k monitor with 22px font size. reply Traubenfuchs 1 hour agoprev> I mean you do not install it as a dependency. It is not available or distributed via npm. > You can use the diff command to check for updates against the registry. Isn't that a contradiction? So they do offer automatic importing & updating but refuse to use the infrastructure everyone else is using and instead copy/paste their files directly into your project with their cli? reply kewp 6 hours agoprevThis is great, have been looking for something like this. reply squigglydonut 8 hours agoprevThis made me want to unlearn react. reply canadiantim 10 hours agoprevLooks cool! reply ivanjermakov 10 hours agoprev [–] Mods are asleep and someone bombarding HN with spam.. reply CharlesW 9 hours agoparentTaking 30 seconds to send an email to Hacker Newsis shockingly effective and a nice thing to do for the HN community. reply datascienced 1 hour agoparentprevFor clarification I think my parent comment is referring to the now dead comments, not the submission. reply slavetologic 10 hours agoparentprevFree ai girlfriend sounds amazing reply Traubenfuchs 1 hour agorootparentNobody said anything about \"free\". reply cebert 10 hours agoparentprev [–] What is going on with the spam. It’s not even good spam. reply GuidelinesFAQListsAPISecurityLegalApply to YCContact Search:",
    "originSummary": [
      "The email thread covers work-related topics like project updates, budget approvals, meeting agendas, vacation plans, and team-building events.",
      "Key themes revolve around collaboration, feedback, project progress, and future event planning.",
      "The thread concludes with a meeting invitation from William Smith regarding a project discussion."
    ],
    "commentSummary": [
      "Franken/UI is an HTML-first, framework-agnostic CSS implementation using Tailwind and UIKit, ideal for small teams and solo developers, emphasizing clean integration with Rails.",
      "The discussion evaluates CSS efficiency versus PostCSS and Tailwind, debating HTML-first strategies, DaisyUI, and worries regarding reliance on Node.js/npm.",
      "Feedback covers UI design, customizability, troubleshooting device data, limitations of tools, and spam concerns on the platform."
    ],
    "points": 149,
    "commentCount": 46,
    "retryCount": 0,
    "time": 1713824322
  }
]
